Forecasting Counts of User Visits for Online Display Advertising with Probabilistic Latent Class Models 

Suleyman Cetintas
Dept. of Computer Sciences Purdue University
scetinta@cs.purdue.edu

Datong Chen
Yahoo! Labs Santa Clara, CA
datong@yahoo-inc.com

Luo Si, Bin Shen, Zhanibek Datbayev
Purdue University
{lsi,bshen,zdatbaye} @cs.purdue.edu

ABSTRACT
Display advertising is a multi-billion dollar industry where advertisers promote their products to users by having publishers display their advertisements on popular Web pages. An important problem in online advertising is how to forecast the number of user visits for a Web page during a particular period of time. Prior research addressed the problem by using traditional time-series forecasting techniques on historical data of user visits; (e.g., via a single regression model built for forecasting based on historical data for all Web pages) and did not fully explore the fact that different types of Web pages have different patterns of user visits.
In this paper we propose a probabilistic latent class model to automatically learn the underlying user visit patterns among multiple Web pages. Experiments carried out on real-world data demonstrate the advantage of using latent classes in forecasting online user visits.
Categories and Subject Descriptors: H.4 [Information Systems Applications]: Miscellaneous General Terms: Experimentation, Performance, Theory Keywords: Forecasting, User Visits, Display Advertising
1. INTRODUCTION
Online display advertising is a multi-billion dollar industry where advertisers buy user visits from publishers in order to promote their products by displaying advertisements (ads) on popular Web pages. An important problem in display advertising is forecasting the count of user visits for a Web page during a particular period of time (e.g., day, hour, etc.). Over-forecasting or under-forecasting leads to undesired ad delivery outcomes, such as missing an advertiser's goal or revenue loss due to unsold user visits [1].
Prior research has mainly adopted traditional time-series forecasting techniques [1, 3, 4]. Forecasting models are trained from historical user visits as a single regression model for all Web pages (assuming independence among different Web pages). In real online world, user visits among Web pages are not independent; and this results in groups of Web pages that have similar user visit patterns. For instance, one ob-
This research was partially supported by the NSF research grants IIS-0746830, CNS-1012208, IIS-1017837, and a research grant from Yahoo!.
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

vious factor shaping user visits is the physical structure of Web pages. A large proportion of users follow the navigation from a parent Web page, and visit the children Web pages step-by-step. Hence, some of the Web pages will have similar user visit patterns with each other, while having different user visit patterns than some other Web pages. To our best knowledge, prior research has not differentiated the groups of Web pages with similar user visit patterns.
In this paper, we learn user visit patterns hidden behind the online traffic of a large number of Web pages by using a novel probabilistic latent class model. In particular, different types/classes of Web pages that share similar patterns of user visits are automatically identified from historical data, and a regression model is built for each type/class of Web pages for making accurate prediction. The detailed formulation of the model is presented in Section 2. We then evaluate the performance of the model against typical traditional solutions using real-world data from Yahoo! in Section 3.

2. METHOD
For forecasting user visit volume vst for a Web page s (or bucket of web pages defined by a publisher) at time t, the proposed probabilistic latent class model can be described as follows:

Nz

P (s, t, vst) = P (s)P (z|s)P (vst|z)

(1)

z=1

where P (s) is assumed to be uniform distribution, P (z|s) denotes the conditional probability of a latent class z given Web page s, and the Nz is the number of latent Web page classes that is empirically set to 5. The visit pattern in a class P (vst|z) can be modeled with a Laplace distribution as follows:

P (vst|z, f st, )

=

exp(- |vst- 2

) K
i

zi fist |



(2)

where fist is the ith feature for a Web page s and time t pair (more information about the features can be found in Section 3), zi is the weight of latent class z for the ith feature, and K is the number of features.
The parameters of the model in Eqn.(1) (P (z|s), ) can be estimated by the EM algorithm [2]. The E-step can be derived by computing the posterior probability of z, i.e. P (z|s, t, vst). By optimizing the auxiliary Q-function, we can derive the following M-step update rules:

P (z|s)  P (z|s, t, vst)

(3)

t

1217

K

z·



arg max
z·

s,t

P (z|s)

- |vst -

i

zi fist |

(4)

Eqn.(4) is differentiable, and can be solved with gradient descent solvers. In particular, we use the Quasi-Newton method. An extreme case of the proposed latent class model, referred as Latent S M od, is using only one latent class Nz = 1. In this case only the Laplace regression power is employed. We particularly report this case as Laplace Regr in the experiments as one of the baselines. It should be noted that the Latent S M od model and the Laplace Regr model are run on the log-scaled (base-e) count data, and the estimated count is rescaled back for comparison with the raw user visit counts during evaluation.

3. EXPERIMENTS
Experiments are conducted on 1 month user visit logs of tens of millions users from Yahoo!. Data from the second and third weeks is used for training, and the last week is used for testing. Features are extracted for each Web page from its past week history. User visits in the same hour are aggregated together. Starting from the first hour of the second week, we extract the first 4 features as the average of visit volumes of the same hour-of-the-day in the past 1, 3, 5, 7 days. For instance, the features for the hour, 9:00pm-9:59pm on Jan.10th, are extracted from the visits during 9pm-9:59pm on days between Jan.9th, 8th,..., Jan.3rd). Note that the first week data is only used while extracting this first set of features for the second week. We extract the second 4 features as the average number of visits in the past 1, 3, 6, 9 hours. The number of user visits for the most visited i) 500, and ii) 1000 properties are computed (for each hour), and the corresponding datasets are referred as Top500Prop and Top1000Prop respectively. The Top500Prop and the Top1000Prop datasets have around 156K and 306K training, and 83K and 164K test data instances respectively.
The proposed model is compared to 3 types of baselines. The first baseline follows a simple forecasting approach that uses the average of past visit volume as the forecast of the coming hour. We use the 8 features as a set of 8 Webpage-independent baseline forecasts B LastN Day for N  (1, 3, 5, 7) and B LastN Hour for N  (1, 3, 6, 9). The forecasting error is measured by the absolute percentage error between forecast and truth: |vsftorecast - vst|/vst, where vsftorecast is the forecast and vst is the actual visit count. In Table 1, we reported the average the forecasting error for all 8 baselines for the Top500Prop and Top1000Prop datasets. In order to protect the confidential information from the company, the actual errors are normalized with (i.e., divided by) the error of B Last1Hour baseline on the Top1000Prop dataset (note that the actual error is in the range 0.5-1), and only the normalized errors are reported for relative comparison. It can be seen that B Last1Hour is the best performing model out of all other 7 baselines, showing that the user visits in the last hour is the most relevant to the current hour, which is totally consistent with the common sense.
The second baseline of this work, namely BB P ropSpec, is similar to the first one, but allows each web page to have its own best model selected from the 8 features. The best feature is selected in the training data, and tested on the testing data. Interestingly, BB P ropSpec performs better than all other methods except B Last1Hour. Selecting the best model for each Web page overfits the training data, and

Table 1: (Normalized) Results of the proposed prob-

abilistic latent class model (i.e., Latent S Mod) in comparison to several baselines.

Methods

Top500Prop Top1000Prop

B Last7Days

2.293

2.385

B Last5Days

2.288

2.451

B Last3Days B Last1Day

2.091 1.195

2.318 1.794

B Last9Hours

3.007

3.493

B Last6Hours B Last3Hours

2.116 1.077

2.577 1.493

B Last1Hour

0.614

1

BB PropSpec

0.987

1.229

Laplace Regr

0.545

0.893

Latent S Mod

0.499

0.829

generates more forecasting errors even in comparison to the simple baseline B Last1Hour. Potential improvements can be achieved by following a direction between these two types of baseline approaches.
The third baseline is a traditional time-series regression model. We added a binary flag indicating weekends as an additional feature into the existing 8 features, and perform regression with Laplace distributions. This model is referred as Laplace Regr. It can be seen in Table 1 that the Laplace Regr outperforms all previously introduced approaches. This shows that combining the different information from past user visits intelligently is more effective than using only a specific type of information. Table 1 also shows that the proposed model (i.e., Latent S M od) outperforms all the presented approaches by modeling the latent Web page classes that provide much higher modeling flexibility leading to its superior performance. This explicitly shows that differentiating the Web pages with different user visit patterns, and specializing the forecast model for different classes of Web pages that share similar patterns of user visits is important for achieving higher forecast accuracy.
4. CONCLUSIONS
Forecasting the number of user visits is an important task for display advertising. Different Web pages have different user visit trends, and it is important to learn specialized forecasting models for properties with different user visit trends. This paper proposes a probabilistic latent class model that identifies the latent classes for Web pages with similar user visit trends, and learns a separate forecasting model for each type/class of Web pages. Experiments on real-world data from a major internet company show the effectiveness of the proposed probabilistic latent class model.
5. REFERENCES
[1] D. Agarwal, D. Chen, L.-j. Lin, J. Shanmugasundaram, and E. Vee. Forecasting high-dimensional data. In ACM SIGMOD Conf., 2010.
[2] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society., 1977.
[3] R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Applications. Springer, 2007.
[4] A. Zellner and J. Tobias. A note on aggregation, disaggregation and forecasting performance. Journal of Forecasting, 1999.

1218


Measuring Assessor Accuracy: A Comparison of NIST Assessors and User Study Participants

Mark D. Smucker
Department of Management Sciences University of Waterloo
mark.smucker@uwaterloo.ca

Chandra Prakash Jethani
David R. Cheriton School of Computer Science University of Waterloo
cpjethan@cs.uwaterloo.ca

ABSTRACT
In many situations, humans judging document relevance are forced to trade-off accuracy for speed. The development of better interactive retrieval systems and relevance assessing platforms requires the measurement of assessor accuracy, but to date the subjective nature of relevance has prevented such measurement. To quantify assessor performance, we define relevance to be a group's majority opinion, and demonstrate the value of this approach by comparing the performance of NIST assessors to a group of assessors representative of participants in many information retrieval user studies. Using data collected as part of a user study with 48 participants, we found that NIST assessors discriminate between relevant and non-relevant documents better than the average participant in our study, but that NIST assessors' true positive rate is no better than that of the study participants. In addition, we found NIST assessors to be conservative in their judgment of relevance compared to the average participant.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Experimentation
Keywords: Relevance, assessors, signal detection theory
1. INTRODUCTION
Whether it is assessors judging the relevance of documents or users performing document triage in the midst of a search, humans are faced with a speed accuracy trade-off. When accuracy must be traded off for speed, relevant documents will be missed and judged non-relevant, and non-relevant documents will be judged to be relevant. Better user interfaces (UIs) will allow assessors and users to judge faster and more accurately than poorer interfaces.
To build better relevance assessing UIs, we need to be able to measure the performance of the users of these UIs. While it is easy to measure the rate at which assessors make judgments, the subjective nature of relevance makes measuring the quality of the judgments difficult.
To sidestep the issue caused by the subjective nature of relevance, and inspired by the work of Alonso and Mizzaro [1], we define relevance as the majority opinion of a group of qualified assessors. Alonso and Mizzaro [1] found
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

Assessor Relevant Non-Relevant

Gold Standard Judgment Relevant (Pos.) Non-Relevant (Neg.) T P = True Pos. F P = False Pos. F N = False Neg. T N = True Neg.

Table 1: Confusion Matrix. "Pos." and "Neg." stand for "Positive" and "Negative" respectively.

that the majority vote of a group of 10 non-NIST assessors correctly judged the relevance of 28 of 29 documents for a single TREC topic, and that the non-NIST assessors discovered mistakes made by the NIST assessors. We expand on Alonso and Mizzaro's work by considering multiple topics and by using more than twice as many documents.
We demonstrate the usefulness of this approach by comparing NIST assessors to a group of user study participants and in doing so gain a better understanding of both groups.

2. MATERIALS AND METHODS

We use a group's majority opinion to locate instances

where NIST assessors may have made mistakes and establish

a gold standard for a set of relevance judgments.

In making relevance assessments, an assessor has to make

a classic signal detection yes/no decision. Once viewed as a

signal detection task, it becomes clear that we should ana-

lyze the performance of assessors in terms of their true pos-

itive rate (TPR) and their false positive rate (FPR), where

TPR = |T P |/(|T P |+|F N |) and FPR = |F P |/(|F P |+|T N |)

(see Table 1).

Signal detection theory says that an assessor's relevance

judging task may be modeled as two normal distributions

and a criterion [2]. Given this model of the signal detection

task, we can characterize the assessor's ability to discrimi-

nate as d = z(T P R) - z(F P R), and compute the assessor's

criterion

c

=

-

1 2

(z(T

P

R)

+

z(F P R)),

where

the

function

z

is the inverse of the normal distribution function [2]. A pos-

itive c represents a conservative judging behavior where the

assessor misses relevant documents to keep the false positive

rate low, and a negative c is liberal, respectively.

As part of a study that we conducted [3], 48 participants

judged the relevance of documents for 8 different topics. The

topics were from the 2005 TREC Robust track, which used

the AQUAINT collection of newswire documents. All the

results here are based on phase 1 of the study.

An intentional by-product of our study was that we would

obtain a large number of judgments on the same documents.

1231

Topic 336 336 362 362 362 383 427 436

Docno NYT19980817.0234 NYT20000927.0154 NYT19990313.0150 NYT19980805.0250 XIE20000821.0238 NYT19991214.0159 NYT19990614.0216 XIE19980303.0229

Participant RN% 10 1 91 8 4 67 11 1 92 9 1 90 2 8 20 7 5 58 5 7 42 7 4 64

NIST qrel N N N N R N R N

Gold qrel R R R R N R R R

Table 2: Documents on which participants disagreed with NIST assessors. R=relevant. N=non-relevant.

After conducting our study, we found that some documents were duplicates of each other. For both NIST assessors and study participants, we found cases where the same assessor had conflicting judgments for the same document.
After removing documents found to be duplicates, we had 71 documents with 10 or more judgments. There were 8 documents where the majority opinion of relevance differed from the NIST assessors' and 2 documents where there was a tie. For these 10 documents, we and an undergraduate assistant examined each document to determine whether the majority opinion or the NIST assessor was correct. We in effect acted as "swing votes" to determine the final majority in these few cases.
Both ties were from topic 426 and the NIST assessor had judged them relevant. At first, the relevance of these two documents was not obvious to us, but after looking at other examples of the topic's relevant documents, we set the gold standard judgment to agree with the NIST assessor.
For the remaining 8 of 10 documents, we determined that the majority opinion was correct in 7 of the 8 cases. Table 2 shows the 8 documents and the count of judgments by the participants. For each of the 6 documents judged non-relevant, we found relevant material in the document.
The one case where we found NIST assessors to have made a false positive mistake, it was because the on-topic document failed to describe a specific incident, which was required by the description of relevance for topic 362.
For the seemingly off-topic document where the majority was wrong, the final paragraph contained relevant content.
For both the computation of d and c, a false positive or true positive rate of 0 or 1 will result in infinities. Rates of 0 and 1 are most often caused by these rates being estimated based on small samples. To better estimate the rates and avoid infinities, we employ a standard correction of adding a pseudo-document to the count of documents judged. Thus, the estimated TPR = (|T P | + 0.5)/(|T P | + |F N | + 1) and the estimated FPR = (|F P | + 0.5)/(|F P | + |T N | + 1).
Of the 8 total search topics, each study participant worked on different sets of 4 topics. As such, it is possible that one participant could have worked on easier topics or sets of documents than another participant. To correctly compare between participants and NIST assessors, we compute the NIST assessors' performance on 48 sets of documents. Each set of documents corresponds to the set of documents that a participant judged. We measure statistical significance using a paired, two-sided, Student's t-test. The measures for each participant are based on an average of 16.8 judgments.

Average TPR FPR d c

Participants 0.77 0.18 1.9 0.10

NIST Assessors 0.77 0.07 2.3 0.37

p-value 0.73
< 0.001 < 0.001 < 0.001

Table 3: Results. Please see text for explanation.

3. RESULTS AND DISCUSSION
The averages reported in Table 3 all use estimated TPR and FPR. For the NIST assessors, we average performance over the 48 sets of documents determined by the 48 participants (see Section 2). The NIST performance on the 71 documents is similar with an overall TPR of 0.81 (26/32) and a FPR of 0.03 (1/39).
While our results are not based on a random sample of documents or topics, two observations stand out.
First, our study participants did as well as NIST assessors at judging relevant documents to be relevant. Both study participants and NIST assessors had an average a TPR of 0.77. We hypothesize that the TPR of human assessors is limited by the amount of time that can be devoted to searching a document for relevant material. Unless an assessor reads documents in their entirety, relevant material will be missed and a TPR of less than 1 will result.
Second, NIST assessors rarely judged non-relevant documents as relevant (a false positive). NIST assessors had a much lower FPR of 0.07 compared to the participants' 0.18. There are many possible reasons for this difference including different training, instructions, and user interfaces.
Taken together, the NIST assessors are better able to discriminate relevant from non-relevant documents than the study participants (d of 2.3 vs. 1.9). While there is considerable variation in the criterion used by participants, on average the participants show little bias with a criterion c = 0.10. The NIST assessors were found to have a conservative bias with a c = 0.37.
4. CONCLUSION
By defining relevance to be a group's majority opinion, we were able to quantitatively compare the relevance judging performance of NIST assessors and user study participants. We found that while both groups effectively have the same true positive rate, NIST assessors have a significantly lower false positive rate.
5. ACKNOWLEDGMENTS
Special thanks to Gordon Cormack, Ian Soboroff, and Ellen Voorhees for their helpful feedback. As a URI, Michael Tatham did a preliminary analysis of this data. This work was supported in part by NSERC, in part by Amazon, and in part by the University of Waterloo.
6. REFERENCES
[1] O. Alonso and S. Mizzaro. Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment. In Proc. SIGIR'09 FIRE Workshop, pages 15­16, July 2009.
[2] N. Macmillan and C. Creelman. Detection theory: a user's guide. Lawrence Erlbaum Associates, 2005.
[3] M. D. Smucker and C. Jethani. Human performance and retrieval precision revisited. In SIGIR'10. ACM, 2010.

1232


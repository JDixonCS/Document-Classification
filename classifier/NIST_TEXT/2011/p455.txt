Authorship Classification: A Discriminative Syntactic Tree Mining Approach

Sangkyum Kim, Hyungsul Kim, Tim Weninger, Jiawei Han, Hyun Duk Kim
Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801 {kim71, hkim21, weninge1, hanj, hkim277}@illinois.edu

ABSTRACT
In the past, there have been dozens of studies on automatic authorship classification, and many of these studies concluded that the writing style is one of the best indicators for original authorship. From among the hundreds of features which were developed, syntactic features were best able to reflect an author's writing style. However, due to the high computational complexity for extracting and computing syntactic features, only simple variations of basic syntactic features such as function words, POS (Part of Speech) tags, and rewrite rules were considered. In this paper, we propose a new feature set of k-embedded-edge subtree patterns that holds more syntactic information than previous feature sets. We also propose a novel approach to directly mining them from a given set of syntactic trees. We show that this approach reduces the computational burden of using complex syntactic structures as the feature set. Comprehensive experiments on real-world datasets demonstrate that our approach is reliable and more accurate than previous studies.
Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing--Text Analysis; H.3.3 [Information Search and Retrieval]: Clustering; H.2.8 [Database Applications]: Data Mining
The work was supported in part by the Blue Waters sustained-petascale computing project under the National Science Foundation (award number OCI 07-25070) and the state of Illinois, NDSEG Fellowship, NSF IIS-0905215, NSF-CCF-0905014, U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR '11 July 24-28 2011, Beijing, China Copyright 2011 ACM 978-1-4503-0757-4/11/07 ...$10.00.

General Terms
Algorithms, Experimentation
Keywords
Authorship Attribution, Text Mining, Text Categorization, Authorship Discrimination, Authorship Classification
1. INTRODUCTION
In computational linguistics and text mining domains, there are three classical classification problems: topic classification, genre classification, and authorship classification. Among these three problems, arguably the most difficult is the classification of documents in terms of their authorship (known as authorship classification, authorship attribution and/or authorship discrimination). This problem can be thought of as classifying documents based on the writing styles of the authors. This is a nontrivial problem even for humans: while a human can easily identify the topic and genre of a given document, identifying its authorship is harder. If the documents are in the same topic and genre, the task becomes much harder.
In the era of excessive electronic texts, authorship classification has become more important than ever before with a wide variety of applications. Besides the early works of analyzing the disputed plays of Shakespeare(1887) [22] or anonymous documents of The Federalist Papers(1964) [24], it could also be used to identify authors of short `for sale' messages in a newsgroup [36] and even for forensic investigations by identifying authorship of e-mail messages [2]. Detecting plagiarism or copyright infringement of unauthorized reuse of source code by establishing a profile of an author's style is another important application of authorship classification [5].
Existing approaches to authorship classification use various methods to extract effective features, the most common of which include style markers such as function words [9, 33, 1, 12] and grammatical elements such as part of speech (POS ) tags [3, 11, 35]. Function words are common words (e.g. articles, prepositions, pronouns) that have little semantic content of their own but usually indicate a grammatical relationship or generic property. Recently, there have been several papers that claimed function words are more effective than other types of style markers [33, 35, 34].
Unfortunately, research on more complex syntactic structures has not been practical because of the lack of a reliable, automatic tool which retrieves syntactic structures, and be-

455

S NP VP

PP VBD PP

IN

NP

IN

NP

Pattern t

S ­ simple declarative clause NP ­ noun phrase PP ­ prepositional phrase IN ­ preposition VP ­ verb phrase VBD  verb, past tense

Syntactic Tree S

Example. The major indexes fell more than 2 percent, and the surge that had lifted the troubled indexes by more than 20 percent in the last month showed signs of stalling as the reporting period for the first fiscal quarter of the year began.

Figure 1: A 2-ee subtree t is mined from two The New York Times journalists Jack Healy and Eric Dash who worked in the same business department. On average, 21.2% of Jack's sentences contained t while only 7.2% of Eric's sentences contained t.

cause of the high computational cost associated with syntactic structure-based algorithms. Instead, several variations of POS tags [9, 11, 15] and rather simple syntactic structures like rewrite rules [3, 11, 15] have been proposed. Among them, bigram POS tags and rewrite rules showed reliable performance in various dataset configurations.
Recently, several advanced techniques have been developed which greatly improved the performance of Natural Language Processing(NLP ) tools1 enabling reliable, highly accurate sentence parsing into a syntactic tree of POS tags. A syntactic tree is a rooted and ordered tree that is labeled with POS tags that represent the syntactic structure of a sentence. Based on the syntactic trees parsed by these tools, we propose a novel syntactic feature set of tree fragments allowing at most k-embedded edges (in short, a k-ee subtree). We say there is an embedded edge between two nodes if and only if they are in an ancestor-descendant relationship but not in a parent-child relationship. Compared with previous feature sets that consist of parts of distinct connected subtree components, our new feature set captures the relationship between k+1 connected subtree components of a syntactic tree, which leads to a better representation of datasets consisting of long and complex sentences. Figure 1 gives an example of a k-ee subtree t for k = 2. Pattern t is composed of three smaller subtrees, which are connected by two embedded edges (S,NP) and (VP,PP). The differences in pattern distributions between two authors suggest that a set of k-ee subtrees can be utilized as a good feature set for authorship classification.
To reduce the number of features, we only mine a set of frequent and discriminative k-ee subtrees, which results in higher accuracy by avoiding overfitting to the training data and by not generating non-discriminative features that often degrade the performance. This task is commonly referred to as pattern-based classification. The original pattern-based classification technique employed a two-step procedure called generate-and-test which generates all frequent and closed
1We used Stanford Parser (http://nlp.stanford.edu/ software/lex-parser.shtml), but there are more tools available like Natural Language ToolKit (NLTK) package (http://www.nltk.org).

candidate patterns and then selects the discriminative patterns among them [20]. Unfortunately, it is still intractable to use this generate-and-test methodology to get discriminative patterns because there are simply too many candidate patterns.
For this reason, there have been quite a few works which directly mine discriminative patterns without generating all candidates [7, 37, 29]. Yet, these existing works cannot be directly applied to our problem setting because they require the feature values to be binary. Instead, we require numeric feature values because a (syntactic) feature can occur multiple times in a document and usually the number of occurrences implies its importance. Existing works are all based on binary-valued features and their theorems and proofs are not easily extendable to numeric-valued features. A recent work ([18]) showed that it has more gain to use numeric values than to discretize them into binary values. It also proposed a new way to directly mining discriminative numeric features by solving a linear programming optimization problem. But all these previous works mine top-1 pattern iteratively until the mined patterns cover the entire data. To cope with this issue, we derive an upper bound of a discriminative score of numeric-valued features, and develop an efficient algorithm that mines in one iteration a set of discriminative patterns to be used for classification purpose.
To validate the utility of our new feature set compared to others, for fair comparisons, we apply the same SVM classification algorithm using various feature sets on several real data collections. Because of its high and reliable performance, SVM has commonly been used to compare the effectiveness of feature sets [11, 35, 15]. Experimental results demonstrate the effectiveness of the proposed k-ee subtree features in comparison to the well-known existing feature sets of function words, POS tags, and rewrite rules. We demonstrate that by using k-ee subtrees as the feature set we outperform the existing feature sets by 8.23% on average and show that it is significantly better from other approaches by t-test with 95% confidence level.
In summary, the contributions of this paper are as follows:
· We propose a new feature set of k-ee subtrees for authorship classification.

456

· We develop an efficient algorithm to directly mine discriminative k-ee subtrees, which are not binary but numeric valued features, in one iteration.
· Through comprehensive experiments on various datasets, we demonstrate the utility of our proposed framework to provide an effective solution for the authorship classification problem.
The rest of the paper is organized as follows. Section 2 presents an overview of the related works. In Section 3, we introduce various preliminary concepts and define our new feature set of k-ee subtrees. Section 4 explains a branchand-bound framework of discriminative k-ee subtree mining. We report experimental results in Section 5, followed by discussions in Section 6 and conclusions in Section 7.
2. RELATED WORKS
There are two main steps involved in any authorship classification algorithm: (1) the feature extraction step and (2) the classification step based on features extracted from the first step.
For the feature extraction step, since the earliest works of the authorship attribution on the plays of Shakespeare(1887) [22] and The Federalist Papers(1964) [24] that used a small number of common words such as `and ', `to' as a feature set, nearly 1,000 different feature sets have been studied including sentence length, chi-square score, lexical richness [17], vocabulary richness [8], function words [1], word n-grams [27], character n-grams [13], and rewrite rules [3] with lots of controversy on their effectiveness. Even though there is an issue of fair comparison among feature sets because previous works conducted experiments based on their own datasets with different classification methods [33, 28], function words and rewrite rules are generally considered to give reliable and good results. In [35, 34], the authors compared function words with other feature sets (even combination of those feature sets) to show that function words are better than the other feature sets.
Even though many new features have been explored for authorship classification, most of the classification algorithms are simply adapted from other domains' well-known classification algorithms such as PCA [16], k-nearest neighbor, decision tree, bayesian networks [33], and SVM [9, 11, 35, 15]. In [35], a language model based authorship classification framework was proposed, which showed comparable performance to SVM. Because of its high and reliable performance, SVM has commonly been used to compare the effectiveness of feature sets [11, 35, 15], so in this paper we also use SVM for fair comparison between our new feature set and existing feature sets.
While most of the earlier works focused on binary authorship classification problem (classifying documents from two authors) which showed significantly good results, recent works including [2, 19, 21, 33, 35] have brought up the problem of multiple authorship classification (classifying documents from more than two authors). Thus, in this paper, we also show the effectiveness of our proposed k-ee subtrees feature set by solving both binary and multiple authorship classification problems.
Our proposed k-ee subtrees feature set can be considered to be a variation of tree patterns. In the data mining domain, there have been several studies on tree pattern mining [30, 31, 4]. TreeMiner [30] is one of the pioneering frequent

tree pattern mining algorithms. For tree classification, rulebased classifiers (XRules [31]) and a decision tree based classifier (Tree2 [4]) were proposed. To the best of our knowledge, mining k-ee subtrees has never been discussed before.
3. PRELIMINARIES
Previous authorship attribution approaches adopted function words, POS tags, and rewrite rules as a feature set to build a classification model. Even though they achieved good accuracy, there still exists room for a more meaningful feature set to improve the performance. In this section, we describe rewrite rules which are somewhat complex syntactic structures that hold more syntactic information than the other two feature sets. Also, we define our new feature set of k-ee subtree patterns.
3.1 Rewrite Rule
In [3], rewrite rules were considered to be building blocks of a syntactic tree, just as words are building blocks of a sentence. Here, a syntactic tree is a rooted and ordered tree which is labeled with POS tags that represents the syntactic structure of a sentence. Its interior nodes are labeled by nonterminals of the grammar, and the leaf nodes are labeled by terminals.
Compared to previous approaches that utilized function words and POS tags, rewrite rules can hold functional structure information of the sentence. In linguistics, a rewrite rule is in the form of "X  Y " where X is a syntactic category label and Y is a sequence of such labels such that X can be replaced by Y in generating the constituent structure of a sentence. For example, "NP  DT +JJ +JJ +NN " means that a noun phrase (NP ) consists of a determiner (DT ) followed by two adjectives (JJ) and a noun (NN ).
There is a limit when using rewrite rules as features of a classification model. First, because of the restriction that the entire rule cannot be broken into smaller parts, no similarity between rules are considered. A large number of slightly different rules are all counted as independent features. For instance, a rewrite rule "NP  DT +JJ +NN ", missing one JJ from the above example, becomes a separate rewrite rule. Second, the expressibility of rewrite rules is limited because they must adhere to a very strict two-level tree structure, which does not allow the entire rule to be broken into smaller parts. For example, the relationships between rewrite rules are missing, which can hold more refined syntactic information. For these reasons, we developed a new feature set of k-ee tree patterns that are flexible and complex enough to represent the syntactic structure information of a sentence.
3.2 k-Embedded-Edge Subtree
To overcome the drawbacks of simple syntactic feature sets used in previous approaches, we explore more complex syntactic features. Induced subtrees of a syntactic tree are one of the candidate feature sets whose features are multilevel tree fragments used to model the complex syntactic structure of a sentence. Here, we define a tree t to be an induced subtree of a tree s if there exists an identity mapping from t to s preserving all parent-child relationships between the nodes of t. Our pilot experiments showed that a small number of combinations of those induced subtrees could achieve even higher accuracy, which motivated us to define k-ee subtrees for our new feature set. Based on this

457

A

A

BB
Pattern t

BBBB
Syntactic Tree S

Figure 2: Example of overcounting overlapped k-ee subtree occurrences

motivation, we designed a new tree pattern that can capture this phenomenon.

Definition 1. We define an embedded edge e of a tree s to be a pair of two nodes with an ancestor-descendant relationship. We define a k-embedded-edge subtree (shortly, k-ee subtree) t of a tree s to be a set of induced subtrees of s that can be connected by at most k embedded edges (not with parent-child relationships) for a user specified value k.

The number of k-ee subtrees would be exponential on the number of trees and their sizes. We define a minimum support  to ensure we only mine general common patterns that will be applicable to test data thus avoiding overfitting. We define the support of a feature t (denoted by sup(t)) to be the total number of sentences in training data that contains t. We say t is frequent if and only if sup(t)   for a userspecified minimum support threshold .
3.3 Document Representation based on Discriminative k-ee Patterns
The frequency of a pattern in a document (or a set of syntactic trees) is quite important in the sense that it can be a good measure to discriminate the writing styles of different authors. Well-known features like function words, and the POS tag-adapted bag-of-words approach use the number of occurrences in a document as their frequency measure. However, unlike function words and POS tags, k-ee subtrees cannot simply adapt the same frequency measure because it generates overlapped occurrences, which would lead to an exaggerated frequency value. Figure 2 is an illustration of this overcounting problem. The syntactic tree S has only one A and four B s, but the number of occurrences of pattern t becomes 6. More generally, if A has n B s as its children in S, then the occurrence count of pattern t becomes O(n2). Since we allow k embedded edges for a k-ee subtree, this overcounting problem will be even more amplified.
Our observation that a document is parsed into a set of syntactic trees (of sentences) gave us an insight to define the frequency measure of a k-ee subtree in a more natural way by counting the number of syntactic trees of a document that contain the pattern.

Definition 2. We define the frequency of a k-ee subtree t in a document d (denoted by f req(t, d)) to be the number of syntactic trees (i.e., parsed sentences) in d that contain t over the total number of sentences in d.

We will discuss how to mine discriminative k-ee subtree patterns in the following section (Section 4). For here, suppose we already have them in a set P = {t1, · · · , tn}. Then, we can express a document d as a vector of their frequencies as d = (f req(t1, d), · · · , f req(tn, d)).

A

A

t1 A

BC S1

BC S3

t2 A

t4 A

A

A

B

C

B

C

t3

A

S2

S4

BC

d1

d2

(a) A toy database D with 2 documents. Each document has 2 syntactic trees.

(b) Pattern growth of k-ee subtrees with  = 0.5 and k=0

Figure 3: Database D and its frequent k-ee subtrees

4. DISCRIMINATIVE K-EE SUBTREE MIN-
ING
In the previous section, we introduced k-ee subtrees as a new feature set for authorship classification. These patterns hold more expressive syntactic information than other features and are flexible enough to consider partial matchings of syntactic trees, but the number of k-ee subtrees is above our control. Therefore, we need to directly mine a small number of discriminative patterns not only to reduce the number of features but also to mine significant patterns which has been shown to improve classification accuracy [6]. In this section, we present a branch-and-bound framework to solve this problem.
4.1 Mining Frequent k-ee Subtrees: PatternGrowth Approach
We do not generate candidate k-ee subtrees and check for frequent attributes. Instead, we find a frequent k-ee subtree and extend it by adding a node that is guaranteed to be frequent in a depth-first manner, which enables several pruning techniques for frequent and discriminative pattern mining. We first introduce how to efficiently mine frequent patterns based on pattern-growth approach by using projected database [26, 38], and then explain pruning techniques to mine discriminative patterns.
We illustrate the procedure for pattern-growth approach as follows. First, find a size-1 frequent k-ee subtree t in the training dataset D. Second, project the postfix of each occurrence of t in the syntactic trees of D into a new database Dt. A postfix of an occurrence of t in a syntactic tree s is a forest of the nodes of s appearing after the occurrence of t in a pre-order scan of s. Third, find a frequent node v in Dt that can be attached to the rightmost path of t that forms a k-ee subtree. Once v is frequent in Dt, it ensures that the extended pattern is also frequent, so we do not need to scan the whole database D again. Note that, in this study, we consider a node v attached to t by an (induced) edge different from the one attached by an embedded edge. Fourth, recursively go back to the second step with the extended pattern for every frequent node we find. Note that the projected database of a pattern t keeps shrinking as the mining process moves on and t becomes a bigger superpattern.

Example 1. Figure 3 shows an example of the patterngrowth approach to mine 0-ee subtrees from a database D of four syntactic trees when minimum support threshold is

458

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

FW

POS

BPOS

RR

0ee

1ee

2ee

Figure 4: Binned information gain score distribution of various feature sets

0.5. Each pattern is indexed in pattern-generation order. We first search for size-1 frequent patterns, which are t1, t5 and t6. We choose t1 as a starting point, and find frequent nodes that can be attached to t1 from its projected database. We find that nodes B and C are frequent, and we extend t1 to t2 by adding a node B. Similar procedures are recursively performed until we mine all frequent patterns.

4.2 Binned Information Gain Score
In previous subsections, we presented a pattern-growth method to mine frequent patterns, but the resulted patterns may still be too many. Based on the study that the patterns with high discriminative score can improve the classification performance [6], we first evaluate the discriminative power of a k-ee subtree. Note that most of the well-known discriminative scores (e.g. information gain, fisher score) have upper bound on binary feature values not on numeric feature values [6, 7, 29, 25]. In this subsection, we define a new discriminativeness score, binned information gain, and derive its upper bound on the numeric feature values to enable a branch-and-bound framework to mine discriminative patterns on numeric feature values.

Definition 3. For a user specified number n, we divide

range [0, 1] of the relative sentence frequency per document

of t into a partition p of equi-width n bins:

p1

=

[0,

1 n

),

p2

=

[

1 n

,

2 n

),

···,

pn-1

=

[

n-2 n

,

n-1 n

),

pn

=

[

n-1 n

,

1].

For a

given partition p and m classes C1, · · · , Cm, we define the

binned conditional entropy of t by

n

m

H(C|X) = - P (X  pi) P (Ck|X  pi) log p(Ck|X  pi)

i=1

k=1

and binned information Gain of t by IG(C|X) = H(C) -

H(C|X) where H(C) = -

m k=1

p(Ck

)

log

p(Ck

).

A pattern t will have a large binned information gain score if the frequency distribution imbalance between the classes becomes bigger for each bin, which means t is significant to discriminate classes.
Figure 4 presents binned information gain score distributions of various feature sets such as function words (FW), POS tags (POS), bigram POS tags (BPOS), rewrite rules (RR), and k-ee subtrees for k=0, 1, and 2 (0-ee, 1-ee, and 2-ee, respectively). We can easily see that the highest scores are mostly from k-ee subtrees, which implies that they can be more meaningful than other features ­ an assertion we later test in the experiments section.

For a tree pattern t, we denote binned information gain of t by IG(t) and information gain upper bound of t and its superpatterns by IGub(t). Given a k-ee subtree t and a partition p, we define (A, B, p) to be a frequency distribution of t where A = (A1, . . . , An) and B = (B1, . . . , Bn) with Ai and Bi being the number of documents in class C1 and C2 respectively for each bin pi of a partition p. Denote (A, B, p) as a frequency distribution of a super pattern t of t. The following two lemmas describe the properties of (A, B, p) and (A, B, p) that will be used to prove the main theorem to derive the upper bound of binned information gain.

Lemma 1. For any k = 2, . . . , n, the following four in-

equalities hold for a k-ee subtree t and its superpattern t:

n i=k

Ai



n i=k

Ai,

k-1 i=1

Ai



k-1 i=1

Ai,

n i=k

Bi



n i=k

Bi,

and

k-1 i=1

Bi



k-1 i=1

Bi.

Proof. Since t is a superpattern of t,

n i=k

Ai



n i=k

Ai

for k  2. Therefore,

k-1 i=1

Ai

=

|C1|

-

n i=k

Ai



|C1| -

n i=k

Ai

=

k-1 i=1

Ai

where

|Ci |

is

the

number

of

documents

in class Ci. Similar proof for Bi.

The following lemma shows the condition to get the upper bound of binned information gain for a special case when only the first two bins of frequency distribution are different.

Lemma 2. For a given frequency distribution (A, B, p),

let (A, B, p) be a frequency distribution with A1 = A1 +

x, A2 = A2 - x (0  x  A2) and the rest unchanged.

If

A1 A1 +B1



, A2
A2 +B2

then

(A, B, p) achieves

its

minimum

conditional entropy when x = A2. Otherwise, it achieves its

minimum conditional entropy when x = 0.

Proof. Let f (x) be the conditional entropy of (A,B,p) and N be the total number of documents. Then,

f (x) = A1 + B1 + x - A1 + x log A1 + x

N

A1 + B1 + x A1 + B1 + x

-

B1

log

B1

A1 + B1 + x A1 + B1 + x

+ A2 + B2 - x - A2 - x log A2 - x

N

A2 + B2 - x A2 + B2 - x

-

B2

log

B2

A2 + B2 - x A2 + B2 - x

n

2

+ P (X  pi) P (Ck|X  pi) log p(Ck|X  pi)

i=3

k=1

f (x) = 1 log A1 + B1 + x · A2 - x

N

A1 + x

A2 + B2 - x

If

A1 A1 +B1



, A2
A2 +B2

f  (x) 

0.

Otherwise,

f  (x) > 0.

The following theorem describes that the binned information gain upper bound exists and is determined by the frequency distribution of the first two bins.

Theorem 1. Given a tree pattern t, its super patterns

including itself have a conditional entropy lower bound in the

frequency distribution (A, B, p) of one of the following two

forms: (i = 2,

(1) ...,

A1 = A1 + A2, n) and Ai = Ai

B2 = (i = 3,

.

n i=2

Bi

. . , n)

, B1 = (2) B1

B1, Bi = B1 +

=0 B2,

A2 =

n i=2

Ai,

A1

=

A1,

Ai

=

0

(i

=

2,

.

..,

n)

and

Bi

=

Bi

(i = 3, . . . , n).

Proof. Suppose (A¯, B¯, p) is a frequency distribution of a superpattern t¯ of t with minimum conditional entropy

459

whose form is in neither cases.

Denote Pi

=

A¯i A¯i +B¯i

and

Qi =

B¯i A¯i +B¯ i

(i = 1, . . . , n).

By generalizing Lemma 2, ei-

ther Pi < Pi+1 or Pi+1 = 0 (i = 1, . . . , n - 1). Symmet-

rically, either Qi < Qi+1 or Qi+1 = 0 (i = 1, . . . , n - 1).

Then, for all i = 2, . . . , n, either Pi = 0 or Qi = 0. ( As-

sume Pi = 0 and Qi = 0 for some i. Then, Pi-1 < Pi and Qi-1 < Qi. But, 1 - Pi-1 = Qi-1 < Qi = 1 - Pi which

is a contradiction.) Therefore, either P2 = 0 or Q2 = 0.

Without loss of generality, say P2 = 0. Then, we can get

another distribution (A¯,B¯,p) where B¯2 = for (i = 3, . . . , n), and the rest unchanged

fnir=o2mB¯i(,A¯B¯,Bi¯=,p)0.

Since its conditional entropy at each bin pi (i = 2, . . . , n)

becomes 0, it has smaller or the same conditional entropy with (A¯,B¯,p). By the assumption that (A¯,B¯,p) has the min-

imum conditional entropy, their conditional entropy are the same. By Lemma 1, A¯1  A1 + A2 and B¯1  B1 ( A¯2 = 0 since P2 = 0). If either A¯1 > A1 + A2 or B¯1 > B1, then the conditional entropy of (A¯,B¯,p) becomes higher than the conditional entropy of (A,B,p) in the first form of the

theorem which is a contradiction to our assumption that the conditional entropy of (A¯,B¯,p) is minimum. Similar contra-

diction can be derived when Q2 = 0.

4.3 Modified Sequential Coverage Method
The binned information gain measure and its upper bound described in Section 4.2 enables a branch-and-bound framework, and we can simply perform the feature selection procedure in a traditional sequential coverage way as follows ([7, 29]). First, we mine the most discriminative k-ee subtree and add it to the feature set. Second, we remove trees that contain the extracted pattern and compute binned information gain scores of the remaining patterns on the updated database. In this way, redundant patterns will have a small chance to be selected. Third, we go back to the first step until either the dataset becomes empty or no more patterns are mined. Once the feature selection procedure is complete, we get a small number of discriminative k-ee subtrees. Based on the feature set F of these patterns, we use the document representation described in Section 3.3 to train a classification model.
But this procedure is inefficient when many discriminative patterns need to be mined because the sequential coverage method described above is based on iteratively mining one discriminative pattern for each iteration. We observe that the object of iterative approach is to find non-repetitive discriminative patterns. For this purpose, previous works simply applied the decision tree scheme of feature selection either (1) to a sequential coverage method to be used for SVM classification model [7, 29] or (2) to a decision tree classification model directly [10]. The difference between them is that the former recursively mines the dataset that does not contain the pattern, and the latter recursively mines both datasets containing and not containing the pattern. But both approaches need to recompute discriminativeness scores of the patterns on the updated database paying an expensive computational cost, which does not really involve removing repetitive patterns. We propose to use a modified sequential coverage method which does not recompute the binned information gain scores.

4.4 Direct Discriminative k-ee Subtree Mining
In this section, we design a novel algorithm to efficiently

mine discriminative patterns in a single iteration. We compute the binned information gain score only once, and apply the sequential coverage method without recomputing the binned information gain scores. Moreover, we propose an efficient way of mining the discriminative patterns in one iteration.
Here, we define some terms and symbols that will be used for the rest of the section. We denote t |= s when a k-ee subtree t is contained in a tree s. We define St = {s  D|t |= s} to be a set of trees in a tree dataset D that contain t. Also, we define At = {p : k-ee subtree|s  St, p = argmaxp|=sIG(p)} to be a set of patterns that achieve the highest discriminative score among all patterns in some trees that contain t, and Bt to be a set of arbitrary patterns from each tree of St. We denote F to be a set of discriminative kee subtrees in D mined by the modified sequential coverage method.
The following lemma characterizes discriminative patterns mined by sequential coverage.
Lemma 3. For a given tree dataset D,
F = {t|s  D such that t = argmaxp|=sIG(p)}.
Proof. By the definition of the modified sequential coverage method mentioned in Section 4.3.
Lemma 3 explains that the discriminative patterns mined by the modified sequential coverage method are indeed the most discriminative patterns for some trees of D. Based on this observation, we derive a pruning method by branch-andbound approach in the following proposition.
Proposition 1. (Branch-and-Bound (BB) Pruning) If IGub(t) < minpAt IG(p), then no superpattern t of t is in F .
Proof. Since St  St , IGub(t) < minpAt IG(p)  minpAt IG(p). That is, t cannot be the most discriminative pattern for any tree in St .
Corollary 1. If IGub(t) < minpBt IG(p), then no superpattern t of t is in F .
Proof. By definition of At, IGub(t) < minpBt IG(p)  minpAt IG(p).
In case IGub(t) = minpBt IG(p), we also skip mining Dt since any tree containing a superpattern t of t will also contain another pattern that has higher or the same discriminative score.
Once we know an upper bound of the discriminative score of t's superpatterns, we can use the BB pruning method described in Proposition 1. Unfortunately, as alluded to earlier, this is a nontrivial task because the feature values are numeric instead of binary. In Section 4.2, we partitioned the numeric range [0, 1] into a finite number of bins and derived the upper bound of binned information gain score by checking a constant number of cases (at most 2 cases) regardless to the number of bins.
In the mining process, since we do not know At, we set Bt to be the set of current best patterns of St and apply Corollary 1 as a BB pruning condition. For that reason, we maintain current best patterns for each tree.
Example 2. Consider the example from Figure 3. Suppose class c1 has a document d1 and class c2 has a document

460

d2 from a database D. Let the number of bins for binned information gain be 3 (i.e. n = 3). We first mine t1, compute its discriminative score (IG(t1) = 0) and update current Bt1 (Bt1 = ) by checking t1. Now, Bt1 = {t1}. Since IGub(t1) = 1 > minpBt1 IG(p) = 0, we move on to next pattern t2 without pruning. We compute t2's discriminative score (IG(t2) = 1), and update Bt2 = {t1} to be Bt2 = {t2}. Since IGub(t2) = 1 = minpBt IG(p), we can skip generating t3.
Following the original sequential coverage methodology mentioned in Section 4.3, when a k-ee subtree t is generated the trees containing t are removed. But in real classification tasks, we may want to generate multiple patterns to represent a tree to improve accuracy. To address this issue, we use a minimum feature coverage threshold  introduced in [7], i.e., a tree is removed when it is covered by at least  discriminative patterns. Lemma 3 and Proposition 1 can easily be adapted with the feature coverage parameter  by maintaining top- patterns for each tree and using -th highest discriminative score as a cut-off threshold for each tree.
In summary, we proposed a branch-and-bound framework of authorship classification. During the process, the algorithm retains and updates the most discriminative patterns Opt(s) of each tree input, and at the end they become F . The basic framework is to expand the patterns from small to large sizes in pattern-growth approach. Before we expand current pattern t into a larger one, we compute the upper bound of the binned information gain of all superpatterns of t. Based on BB pruning described in Corollary 1, if the upper bound value is not greater than the current minimum Opt(s) from all trees (s) containing t, then we can safely skip exploring superpatterns of t.
5. EXPERIMENTS
In this section, we present an empirical evaluation in order to validate the performance of our k-ee subtree based authorship classification. We also analyze the effect of the parameters of k-ee subtree patterns presented in this paper. The experiments are designed to test the usefulness of k-ee subtrees, as a new feature set, for authorship classification.
5.1 Datasets
For the following experiments, we used public data collections extracted from the TREC corpus [14] and The New York Times2.

Table 1: Characteristics of data collections Data # Authors Doc Doc/Author Sentence Word

NTNews Movie TREC

4

400

100

19,161 381,450

4

2,177 415 ­ 598 51,086 1,299,682

7

6,336 804 ­ 1,003 169,767 3,964,865

From The New York Times we collected two different types of datasets: news articles and movie reviews. For the news articles, we randomly selected two journalists from
the business department, and two other journalists from the health department who were the main contributors in their departments.3 We collected datasets assuming that the jour-
2http://www.nytimes.com 3Eric Dash and Jack Healy from the business department, and Denise Grady and Gina Kolata from the health department.

nalists in the same department are likely to write articles on the same topic and genre using similar words.
For the movie reviews, we used four movie critics from the The New York Times. It has three main critics whom we used. We added another randomly selected critic who is one of the major contributors.4 We collected this data because most of the movies reviewed by the critics overlapped. We assumed movie reviews of the same movie will be on the same topic and genre using similar words.
We also used news articles from the Associated Press (AP) subcollection of the public TREC corpus. The AP collection has over 200,000 documents by more than 2,380 distinct authors. We followed the same experimental configurations as previous works [33, 35] did by using the same datasets from the same seven authors5 they used. The statistics of each data collection are described in Table 1. Note that the class distributions (or the number of documents per author) are mostly balanced, and in this way we do not have to consider the effect of skewed data.
5.2 Evaluation Methodology
To evaluate the performance, we performed multiclass classification on each data collection using SVM with linear kernel. Specifically, we decomposed the multiclass problem into binary problems via one-versus-one method, and paired the authors of each data collection and conducted binary classification on these pairwise datasets. For each dataset, we conducted 5-fold cross validation, and averaged the accuracy as a measure of the performance. For each fold, training data was used to mine the syntactic features and to get a classification model while test data was only used for evaluation purposes. For each training data, we used another 5-fold cross validation to determine appropriate parameter values for the classification model (linear SVM). In this way, our evaluation ensured that there is no information leak from the test data for the classification task.
We used the number of occurrences of each feature as a feature value for the syntactic features except k-ee subtrees which used a new frequency measure defined in Definition 2. For the fair comparison, we used the same classifier. In [9, 35], it is shown that SVM achieves reliable performance with high accuracy for authorship classification and the choice of the SVM kernel has little or no effect on the performance.
5.3 Comparison Feature Sets
To show how effectively our new feature set of k-ee subtrees works, we compared the authorship classification performance with other syntactic features such as function words (FW), unigram POS tags (POS), bigram POS tags (BPOS), and rewrite rules (RR). As for function words, we took the list of 308 function words from [23]. We used 74 POS tags from from the stanford parser. 1,088 Bigram POS tags were identified from the leaves of syntactic trees. Rewrite rules and k-ee subtrees were generated by mining parsed sentences of syntactic POS -tagged trees.
In the table 2, we show the average sizes of feature sets for each data collection. To get the number of features of rewrite
4The three main critics of The New York Times are A. O. Scott, Manohla Dargis, and Stephen Holden. The other critic we used is Jeannette Catsoulis. 5The authors are Barry Schweid, Chet Currier, Dave Skidmore, David Dishneau, Don Kendall, Martin Crutsinger, and Rita Beamish.

461

Table 2: Number of features for FW, POS, RR and k-ee feature sets
Data FW POS BPOS RR 0-ee 1-ee 2-ee
NTNews 308 74 1088 3929 119.2 257.8 453.1 Movie 308 74 1088 9029.2 306.2 575.1 1015.6 TREC 308 74 1088 8278 254.4 570.5 1107

Table 3: Accuracy Comparison on Different Number

of Authors and Various Data Collections Data # Authors FW POS BPOS RR k-ee

2

92.25 86.67 90.42 89.75 94.25

NTNews

3

87.08 78.17 83.97 82.17 90.83

4

82.75 71.25 79.45 75.25 87.75

Movie

2

93.18 88.99 84.17 92.88 95.62

3

88.03 81.77 82.17 88.45 92.89

4

84.00 76.23 80.25 85.11 91.30

TREC

2

93.33 92.43 93.95 95.07 96.04

3

88.63 87.12 89.64 91.49 93.43

4

85.10 83.03 86.30 88.67 91.50

5

82.24 79.71 83.51 86.31 89.95

6

79.80 76.87 81.10 84.26 88.56

7

77.62 74.53 78.92 82.46 87.37

Average

86.14 81.40 84.45 86.87 91.62

rules and k-ee subtrees, we computed the average value of the number of distinct features of 5-fold training data for each feature set and dataset. As expected, rewrite rules generated much larger number of features than all the other feature sets. It is noticeable that the number of k-ee subtrees are far less than the number of bigram POS tags and rewrite rules, and sometimes even less than the number of function words. For the rest of the section, we will show that our small sized new feature set of k-ee subtrees outperforms all the other feature sets.
5.4 Performance Evaluation
We first show accuracy comparison on various feature sets and then analyze the effect of the parameters of k-ee subtree approach. For the accuracy comparison with other feature sets, we conducted binary authorship classification as well as multiple authorship classification tasks. Table 3 shows the accuracies of those authorship classification tasks for various comparison feature sets and for three different data collections. By default, we used the number of embedded edge k = 1, minimum support threshold  = 0, the number of bins n = 10, and minimum feature threshold  = 3 for discriminative k-ee subtree mining. In Tables 3 and 4, boldface denotes the best result for each dataset.
5.4.1 Overall Effectiveness
Based on the accuracy results in Table 3, our new feature set of k-ee subtrees achieved the highest performance of the comparison feature sets. Overall, most feature sets showed high accuracy on binary authorship classification tasks. But when the number of authors was increased, the performance gaps between k-ee subtree feature set and all the others became larger.
It is true that bigram POS tags and rewrite rules catch deeper insights of an author's writing style since they are more complex and have much larger number of features than POS tags. But we conclude that a feature set of k-ee subtrees can characterize an author's writing style even better since (1) it allows even more complex syntactic structures

Table 4: Accuracy Comparison on binary authorship classification of The New York Times news articles. Two journalists Dash and Healy from the business department are denoted by B1 and B2, and two journalists Grady and Kolata from the health department are denoted by H1 and H2 respectively.
Author Pair FW POS BPOS RR k-ee

(B1,B2) (B1,H1) (B1,H2) (B2,H1) (B2,H2) (H1,H2)

91.5 87

95

94

94

94 85

92

91

95

95.5 92.5 95

96

94

95 92.5 94.5 92.5 97.5

97 95.5 96.5 97.5 98

80.5 67.5 69.5 67.5 87

Average 92.25 86.67 90.42 89.75 94.25

than rewrite rules as features, (2) its size is much smaller than the feature set of bigram POS tags and rewrite rules, and (3) it achieved better accuracies. Note that the feature set of function words reliably showed reasonable accuracies as previous works mentioned [33, 35, 34]. It achieved better than POS tags and sometimes even better than bigram POS tags and rewrite rules. This is because function words have two different aspects together (syntactic and lexical) while POS tags only have a syntactic aspect. But complex syntactic structures can complement the lack of lexical aspect of the features, since the feature sets of rewrite rules and k-ee subtrees showed higher accuracies than function words.
On average, the feature set of k-ee subtrees improved performance over the other feature sets about 8.23% (overall), 6.36% (function word), 12.56% (POS ), 8.49% (bigram POS ) and 5.50% (rewrite rule).
We also performed a significance test on the feature sets over k-ee subtrees. We used two-tailed t-test on the accuracy results in Table 3, and all their t values (FW:3.18, POS:5.02, BPOS: 4.49, RR: 2.69) indicated that the performance of k-ee subtree patterns are significantly different from (or, better than) all the others (95% confidence interval, threshold:2.07).
Note that we could mine k-ee subtrees even for minimum support  = 0, a task rarely done in previous works because too many patterns were generated from the mining process. Further discussions are described in Section 6.1.
5.4.2 Problem Difficulty Analysis
As we explained in Section 5.1, the datasets of The New York Times news articles were collected to identify the difficulty of classification problem. We assumed that the journalists from the same departments will be hard to classify because they might use similar terms on the same topic and genre. As expected, classification results in Table 4 show that classifying journalists from different departments was easier than journalists from same departments.
Note that the last row of Table 4 shows extremely worse performance than other cases. We manually analyzed the news articles of H1 and H2, and found that their writing styles were quite informal using several quotations which made it the hardest dataset. Even for this hard task, our approach got the highest accuracy with a big gap.
5.4.3 Parameter Analysis
In Figure 5, we analyze the role of each parameter used

462

100

100

100

0-ee 1-ee 2-ee

0-ee 1-ee 2-ee

0-ee 1-ee 2-ee

95

95

95

Accuracy (%)

90

Accuracy (%)

90

Accuracy (%)

90

85

85

85

80

80

0.5

0.4

0.3

0.2

0.1

0.0

Minsup

(a) Accuracy w.r.t. the minimum support threshold 

4

6

8

10

12

14

16

Number of Bins

(b) Accuracy w.r.t. the number of bins n

1

2

3

4

5

Coverage

(c) Accuracy w.r.t. the minimum coverage threshold 

800

800

0-ee 1-ee 2-ee

0-ee 1-ee 2-ee

0-ee 1-ee 2-ee

600

600

400

Running Time (sec)

400

Running Time (sec)

Running Time (sec) 0 2000 4000 6000 8000 10000 12000

200

200

0

0

0.5

0.4

0.3

0.2

0.1

0.0

Minsup

(d) Running time w.r.t. the minimum support threshold 

4

6

8

10

12

14

16

Number of Bins

(e) Running time w.r.t. the number of bins n

1

2

3

4

5

Coverage

(f) Running time w.r.t. the minimum coverage threshold 

Figure 5: Performance Comparisons on Different Parameter Settings

to mine discriminative k-ee subtrees. All experiments were conducted for binary classification of two movie critics Stephen Holden and Jeannette Catsoulis. Similar trends could be found from other datasets. For default values, we used  = 0.3, n = 10, and  = 3. Overall, we found that 1-ee subtree feature set showed the best performance. It could be mined with almost in a constant time even with no minimum support threshold. But, when the number of embedded edges increased (e.g. k = 2), k-ee feature set showed worse accuracies because it tended to overfit to the training data. Moreover, it took exponential time to run when minimum support threshold gets smaller. It is good to know that we do not need too complicated syntactic structures (with a high k), because the computation would be too expensive to make our proposed feature set useful.
There are two parameters, n and , which are related to our binned information gain score. Based on Figure 5, they did not significantly affect the running time, but somehow affected the accuracy. However, since they achieved the peak within a small range, it was not difficult to optimize their values in our experiments.
6. DISCUSSION
6.1 Minimum Support Threshold
In the experiments, we could put minimum support  to be 0 to mine discriminative patterns regardless to any minimum support, which is almost impossible for generate-and-

test methodology that has to generate all patterns. We know that mining frequent pattern is good to avoid overfitting effect, but it is hard to set a correct minimum support threshold value. The authors in [6] show that features with low support have low discriminative power. That is, the discriminative patterns we mine even with no minimum support can be considered to be frequent enough to overcome the overfitting problem.
6.2 Performance of NLP Tools
All current approaches using syntactic features have the same limit of utilizing imperfect natural language processing (NLP ) tools. For example, to obtain syntactic features (including k-ee subtrees), we rely on the performance of NLP parsers. It would be a good follow-up work to apply our approach to other informal datasets such as blogs and newsgroups where NLP parsers tends to show low performance. We believe our approach will still show reasonably good performance even on those messy datasets with informal wording because the frequent pattern mining will not succumb to parsing errors as shown in Table 4 for (H1, H2) author pair.
6.3 Feature Combination
In this paper, we did not consider feature combinations since the main target of this paper is to introduce a new feature set and its effect in authorship classification. In fact, feature combination for authorship classification itself is a different topic with some previous work [32]. Simply applying fixed weights for different feature sets to classify various

463

datasets might not achieve good results because some authors have their own stylistic habits that may be captured in one feature set that is more distinctive than others [35].
7. CONCLUSION
In this paper, we proposed a new syntactic feature set of kee subtrees to classify documents based on their authorship. To mine k-ee subtrees, we developed a direct discriminative k-ee subtree mining algorithm via a branch-and-bound approach. Our novel algorithm could perform a discriminative score based feature selection procedure to mine discriminative patterns in one step, not iteratively. To directly mine discriminative patterns, we theoretically derived an upper bound of binned information gain score of the numeric feature values.
An experimental study has been performed on public real data collections, which were purposefully chosen to contain the same topics and genres and thus use similar terms. Our k-ee subtree-based classification achieved the best results compared to other feature sets. Overall, we conclude that k-ee subtrees are meaningful features for authorship classification that achieved high accuracy across various data collections.
8. REFERENCES
[1] S. Argamon and S. Levitan. Measuring the usefulness of function words for authorship attribution. In ACH/ALLC, 2005.
[2] Shlomo Argamon, Marin Sari´c, and Sterling S. Stein. Style mining of electronic messages for multiple authorship discrimination: first results. In KDD, 2003.
[3] H. Baayen, H. van Halteren, and F. Tweedie. Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguist Computing, 11(3):121­132, 1996.
[4] Bj¨orn Bringmann and Albrecht Zimmermann. Tree2 decision trees for tree structured data. In PKDD, 2005.
[5] Steven Burrows, Alexandra L. Uitdenbogerd, and Andrew Turpin. Application of information retrieval techniques for source code authorship attribution. In DASFAA, 2009.
[6] Hong Cheng, Xifeng Yan, Jiawei Han, and Chih-Wei Hsu. Discriminative frequent pattern analysis for effective classification. In ICDE, 2007.
[7] Hong Cheng, Xifeng Yan, Jiawei Han, and Philip S. Yu. Direct discriminative pattern mining for effective classification. In ICDE, 2008.
[8] O. de Vel, A. Anderson, M. Corney, and G. Mohay. Mining e-mail content for author identification forensics. SIGMOD Record, 30(4):55­64, 2001.
[9] Joachim Diederich, J¨org Kindermann, Edda Leopold, and Gerhard Paass. Authorship attribution with support vector machines. Applied Intelligence, 19(1-2):109­123, 2003.
[10] Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng Yan, Jiawei Han, Philip Yu, and Olivier Verscheure. Direct mining of discriminative and essential frequent patterns via model-based search tree. In KDD, 2008.
[11] Michael Gamon. Linguistic correlates of style: authorship classification with deep linguistic analysis features. In COLING, 2004.
[12] Antonio Miranda Garc´ia and Javier Calle Mart´in. Function words in authorship attribution studies. Literary and Linguistic Computing, 22(1):49­66, 2007.
[13] Jack Grieve. Quantitative authorship attribution: An evaluation of techniques. Literary and Linguistic Computing, 22(3):251­270, 2007.
[14] Donna Harman. Overview of the second text retrieval conference (trec-2). Information Processing & Management, 31(3):271­289, 1995.

[15] Graeme Hirst and Ol'ga Feiguina. Bigrams of syntactic labels for authorship discrimination of short texts. Literary and Linguistic Computing, 22(4):405­417, 2007.
[16] D. L. Hoover. Statistical stylistics and authorship attribution: an empirical investigation. Literary and Linguistic Computing, 16(4):421­444, 2001.
[17] D. L. Hoover. Another perpective on vocabulary richness. Computers and the Humanities, 37(2):151­178, 2003.
[18] Hyungsul Kim, Sangkyum Kim, Tim Weninger, Jiawei Han, and Tarek Abdelzaher. Ndpmine: efficiently mining discriminative numerical features for pattern-based classification. In ECML PKDD, 2010.
[19] Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Eran Messeri. Authorship attribution with thousands of candidate authors. In SIGIR, 2006.
[20] David Lo, Hong Cheng, Jiawei Han, Siau-Cheng Khoo, and Chengnian Sun. Classification of software behaviors for failure detection: a discriminative pattern mining approach. In KDD, 2009.
[21] Kim Luyckx and Walter Daelemans. Authorship attribution and verification with many authors and limited data. In COLING, 2008.
[22] T. C. Mendenhall. The characteristic curves of composition. Science, 11(214):237­246, 1887.
[23] Roger Mitton. Spelling checkers,spelling correctors and the misspellings of poor spellers. Information Processing and Management, 23(5):495­505, 1987.
[24] Frederick Mosteller and David L. Wallace. Inference & Disputed Authorship: The Federalist. Addison Wesley, 1964.
[25] Siegfried Nijssen, Tias Guns, and Luc De Raedt. Correlated itemset mining in roc space: a constraint programming approach. In KDD, 2009.
[26] Jian Pei, Jiawei Han, Behzad Mortazavi-asl, Helen Pinto, Qiming Chen, Umeshwar Dayal, and Mei chun Hsu. Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth. In ICDE, 2001.
[27] Conrad Sanderson and Simon Guenter. Short text authorship attribution via sequence kernels, markov chains and author unmasking: an investigation. In EMNLP, 2006.
[28] Efstathios Stamatatos. A survey of modern authorship attribution methods. Journal of the American Society for Information Science and Technology, 60(3):538­556, 2009.
[29] Xifeng Yan, Hong Cheng, Jiawei Han, and Philip S. Yu. Mining significant graph patterns by leap search. In SIGMOD, 2008.
[30] Mohammed J. Zaki. Efficiently mining frequent trees in a forest. In KDD, 2002.
[31] Mohammed J. Zaki and Charu C. Aggarwal. Xrules: an effective structural classifier for xml data. In KDD, 2003.
[32] Ying Zhao and Phil Vines. Authorship attribution via combination of evidence. In ECIR, 2007.
[33] Ying Zhao and Justin Zobel. Effective and scalable authorship attribution using function words. In AIRS, 2005.
[34] Ying Zhao and Justin Zobel. Searching with style: authorship attribution in classic literature. In Proceedings of the thirtieth Australasian conference on Computer science, 2007.
[35] Ying Zhao, Justin Zobel, and Phil Vines. Using relative entropy for authorship attribution. In AIRS, 2006.
[36] Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang. A framework for authorship identification of online messages: Writing-style features and classification techniques. Journal of the American Society for Information Science and Technology, 57(3):378­393, 2006.
[37] Albrecht Zimmermann and Bjorn Bringmann. Ctc -- correlating tree patterns for classification. In ICDM, 2005.
[38] Lei Zou, Yansheng Lu, Huaming Zhang, Rong Hu, and Chong Zhou. Mining frequent induced subtrees by prefix-tree-projected pattern growth. In WAIMW, 2006.

464


Tossing Coins to Trim Long Queries

Sudip Datta
Language Technologies Research Centre International Institute of Information Technology
Hyderabad 500 032
sudip.datta@research.iiit.ac.in

Vasudeva Varma
Language Technologies Research Centre International Institute of Information Technology
Hyderabad 500 032
vv@iiit.ac.in

ABSTRACT
Verbose web queries are often descriptive in nature where a term based search engine is unable to distinguish between the essential and noisy words, which can result in a drift from the user intent. We present a randomized query reduction technique that builds on an earlier learning to rank based approach. The proposed technique randomly picks only a small set of samples, instead of the exponentially many subqueries, thus being fast enough to be useful for web search engines, while still covering wide sub-query space.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation
General Terms
Algorithms, Experimentation, Performance
Keywords
Verbose Queries, Randomized Algorithm
1. INTRODUCTION
Recent studies indicate a marked growth in the share of long queries[1] - containing more than 5 terms. Though, a user's intent might be well represented in a long query, `bag of words' based search engines tend to fail on them, as they are unable to capture the complex structure of queries. This may result in query drift and consequent poor performance.
Existing approaches to address this problem can be classified into two categories. Query term weighing based techniques attempt to determine the importance of a term and associate a weight with each term in proportion to its perceived importance in the query. Query reduction techniques, on the other hand, focus on identifying the best performing sub-query of the original query by pruning away unimportant terms. The current work belongs to the latter category.
Kumaran et al. [5] (henceforth referred to as `ExpRed') propose an approach that views identification of best representative sub-query as a ranking task and apply learning to rank approach using query quality predictors as features. Since an exponential number 2n - 1 of sub-queries can be generated from an n-word query, computationally it is very expensive to train a model and evaluate it on all sub-queries.
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

Figure 1: Sub-query scores for a typical query.
They propose a number of heuristics to reduce this set of sub-queries. Balasubramanian et al. [2] (henceforth referred to as `SingleRed') extend this work by adopting regression based formulations and only consider sub-queries reduced by a single term from the original query, as candidates subqueries, thus making their system web-scalable.
Our technique for query reduction uses a randomization mechanism to prune terms from original query and builds on Kumaran et al. [5] while considering only a small, configurable number of candidate sub-queries instead of the exponential number in baseline system. Our technique is webscalable as it achieves substantial improvements over original query and the gains increase with larger samplings.
2. MOTIVATION
Performance improvement on verbose queries can be attained by removal of noisy terms such that the resultant shorter query performs better. Moreover, to be useful for web-search, the technique should minimise overhead on the retrieval process. ExpRed achieves the first objective by evaluation of all sub-queries, but is slow because of the exponential number 2n -1 of sub-queries involved in an n word query. SingleRed trades off time for performance by confining the search to sub-queries with one fewer term than the original query. Figure 1, shows the performance of all subqueries for a representative long query, where y-axis measures their Mean Average Precision (MAP) scores while xaxis indicates sub-query length. Thus, the original query, with largest number of terms is the sub-query with highest x co-ordinate (xmax) in this plot. It can be observed that maximum achievable performance (indicated by highest y score for an x value) increases till optimal sub-query and decreases on further addition of terms. As evident from the fig-

1255

ure, employing SingleRed's choice of evaluating sub-queries with one fewer term than the original query, only sub-queries with x = xmax -1 are considered, while the rest are ignored, leaving substantial scope for improvement. We propose an efficient method to explore this space. Another property of the curve is that it is steeper before it peaks while reducing slowly as the original query approaches. This indicates that removal of terms from optimal query, results in greater performance degradation than addition of extra terms. We employ this observation in our sampling technique.
3. OUR APPROACH
Since, evaluation of all sub-queries is infeasible in a web scenario, we propose a randomized reduction technique that samples only a small number of candidate sub-queries and yet efficiently explores the sub-query space. This method removes each query term of the original query, by an `optimal query length' dependent probability. We argue that in small number of iterations of this sub-query generation method, with high probability, it is likely to pick at least one `good' sub-query. Formally, to generate a sub-query from an n term query, a set of n random numbers R[1..n] between 0.0 and 1.0 are generated. The kth term of this query is discarded if R[k] > p, where p = lopt/lq. Here, lopt is the optimal length of queries and lq is the length of query q in words.
Theoretically, this problem can be modeled as a geometric distribution where each new sample can be treated as a Bernoulli trial, where success is defined by selection of one `good' query. Thus, if probability of a good sample being picked in a trial is m, then expected number of samples for an efficient sub-query to be picked is 1/m with a deviation of (1 - m)/(m2). As an illustrative example, among the 9 word queries in our corpus, 5.84% of their sub-queries scored within 10% of their best performing sub-query. Considering these sub-queries as `good', 18 samples would be required to pick one efficient sample, with a standard deviation of 17.
Effectively, a learning to rank formulation is used, where each sub-query is represented by a set of query quality predictors and its MAP acts as the label. The proposed technique is used for generation of sub-queries on train and test data. The model is learned on a large number of candidate sub-queries. Testing requires generation of only a small number of sub-queries which are ranked according to their scores generated by the model and the top positioned version is used as representative of the original query. Unfortunately, the overall performance of our system is dependent not just on the ability to pick effective sub-queries as candidates, but also on the discriminative capabilities of the learning formulation. Despite this constraint, as show in Section 4, significant gains in performance were attained.
4. EXPERIMENTS
We use an experimental setup similar to [5]. Robust 2004 dataset consisting of around half million documents is used for experiments, with the `description' field of topics acting as long queries. We use the definition of long query described in [3], 5  lq  12, where lq is the length of query q. Thus, 184 of the 250 judged queries associated with this data, are used. A set of 30 query quality predictors such as mutual information, query clarity, query scope, are used to describe each sub-query. RankSVM implementation of SV M rank[4] is used for ranking sub-queries.
The set of queries are partitioned into 4:1 split of train and test queries. At each iteration a set of twenty mod-

Figure 2: Random pruning with lopt-train = 6
els, with different seeds, are learned from a large number of samples generated from the training set of queries. These models are used to predict rankings of an equal number of sets prepared from the test queries and the performance is averaged over the 20 sets. A linear `multiplication factor' x determines the number of sub-queries generated for each query and is in proportion to its length. Since, according to [5] the optimal query length lies between 3 and 6, experiments were performed by varying lopt between these values, for both train and test data.
Due to space constraints, only the run with best results is depicted in Fig.2. For this run lopt-train = 6 is used and plots for various lopt-test values with different `multiplication factors' are shown. We believe that performance peaks for lopt-train = 6 and lopt-test = 4 because adequate training requires a large number of sub-queries with decent scores. Thus, as argued in Section 2, a higher average performance among candidate queries can be achieved with bias for longer queries. Testing, on the other hand, relies on a single `good' sub-query and thus peaks closer to the true optimal length of 3.54. Overall, it is seen that irrespective of the choice of lopt-test, performance gains over the original query and ExpRed are achieved with a sampling of just three times the query length. Moreover, a steady co-relation is observed between the number of samples drawn and the improved performance.
5. CONCLUSION
In this paper, we presented a randomization based sampling technique for reduction of long queries. We present results which confirm that this technique is more efficient than earlier techniques of selecting candidate queries during prediction. As future work, we plan to apply this sampling technique on more semantically rich units of information such as phrases and clauses.
6. REFERENCES
[1] http://weblogs.hitwise.com/alan-long/2009/11/ searches_getting_longer.html.
[2] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Exploring reductions for long web queries. In SIGIR'10.
[3] M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. WSCD '09.
[4] T. Joachims. Optimizing search engines using clickthrough data. In KDD '02.
[5] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In SIGIR '09.

1256


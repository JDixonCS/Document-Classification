Utilizing Minimal Relevance Feedback for Ad Hoc Retrieval

Eyal Krikon krikon@tx.technion.ac.il

Oren Kurland kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management Technion -- Israel Institute of Technology Haifa 32000, Israel

ABSTRACT
Using relevance feedback can significantly improve (ad hoc) retrieval effectiveness. Yet, if little feedback is available, effectively exploiting it is a challenge. To that end, we present a novel approach that utilizes document passages. Empirical evaluation demonstrates the merits of the approach. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models; Relevance feedback
General Terms: Algorithms, Experimentation
Keywords: language models, passages, relevance feedback
1. INTRODUCTION
The effectiveness of ad hoc (query-based) retrieval can significantly improve if relevance feedback is available and exploited. Often, terms that are common in the relevant documents -- but not very common in the corpus -- are used for query expansion [7]. Since a document can be deemed relevant even if only a small part of it contains query-pertaining information, utilizing the commonalities between relevant documents is important. Indeed, using very few relevant documents, which reflects practical settings wherein relevance judgments are scarce, can sometimes fall short [8].
We present a novel retrieval method that addresses the challenge of utilizing very little relevance feedback; specifically, a single relevant document. This is a query-by-example task performed in a query-dependent context. We tackle the uncertainty about what makes the document relevant, and more specifically, which terms can represent the underlying information need, by using document passages in two capacities. First, rather than treat the relevant document as one unit, its passages are used so as to better focus on the document parts that presumably contain query-pertaining information. Second, to enrich the basis for performing query expansion, passages from documents in an initially retrieved list that are similar to the relevant document (and to its passages) are used as pseudo relevant units. Empirical evaluation demonstrates the merits of our method with respect to various reference comparisons.
2. RETRIEVAL FRAMEWORK
Suppose that some search algorithm is employed in response to query q so as to rank documents d in corpus D by their presumed relevance to information need I expressed by
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

q. Let Dinit denote the list of documents that are the highest

ranked by the induced initial ranking (initial in short), and

drel denote the highest ranked relevant document in Dinit.

Naturally, we can employ a relevance-feedback-based (RF)

method using q and drel to create an expanded query form

[7]. Since only a single relevant document is available, we opt

to enrich the basis for creating this form; e.g., by employing

a pseudo-feedback-based (PRF) approach that regards all

documents in Dinit as pseudo relevant. Indeed, recent work

[6, 10] has demonstrated the potential merits of integrating

true and pseudo relevance feedback (RF+PRF).

A potential drawback of such approaches is that they often

treat a document as one unit. For example, it could be

that all the information in the relevant document drel that

pertains to I is confined to a single short passage g of drel.

Moreover, the terms in g need not be the most dominant

(e.g., frequent) in drel, and hence, might not be assigned

with high enough weight by the relevance-feedback-based

method. The same observations hold for documents in Dinit\

{drel} for which no (positive) feedback is available. Thus,

we use the passages in documents in Dinit (the set of which

is denoted G) as pseudo relevant units for performing query

expansion. To that end, we estimate p(g|I), the probability

that passage g ( G) contains information pertaining to I.

As q and drel are the only "signals" about I, we estimate p(g|I) by p(g|drel) + (1 - )p(g|q);  is a free parame-

ter. Assuming a uniform prior for passages, we can score

g by P

P g

p(drel |g) G p(drel|g

)

+

(1

-

) P g

p(q|g) G p(q|g

).

In addition,

g drel p(q|g )p(g |g) is used as an estimate for p(q|g).

The goal of using this estimate is addressing the potential

vocabulary mismatch between passage g (which could be

part of drel) that pertains to I and q by using drel's pas-

sages (g ) as proxies for g; the impact of g is based on its

"direct match" with q, which potentially reflects to some ex-

tent the likelihood that g contains information pertaining to I as it is part of drel.1 Using the estimates just described

results in our PsgF method that scores g ( G) by:

P

S (g) d=ef

P p(drel|g) g G p(drel|g

+

(1 P

-

) P

g

)

g G g

drel p(q|g drel p(q|g

)p(g )p(g

|g) |g ) .

(1)

The highest scoring passages are used for query expansion.

3. EVALUATION
1Inter-passage relations were also utilized, for example, for text summarization [3], finding documents similar to a given document [9], and re-ranking search results [4].

1099

Experiments were conducted with TREC corpora (disks, queries): AP (1-3, 51-150), TREC8 (4-5, 401-450), WSJ (1-

AP MAP p@5

TREC8 MAP p@5

WSJ MAP p@5

WT10G MAP p@5

2, 151-200), WT10G (WT10G, 451-550). Titles of TREC initial 21.8 40.9 23.0 40.8 27.2 43.2 15.6 25.1

topics served for queries. Tokenization, Porter stemming RF

32.3 61.1 26.2 54.7 35.3 58.4 23.2 41.3

and language model induction were performed using the

Lemur toolkit (www.lemurproject.org). Half overlapping

150-terms We use

wexinpd"o-wDs i"npdDxoirc[u0m](·e)ntspsDyerirv[e]f(o·)r"p"as[s4a]gfeosr

[4]. p(x|y)

in Equation 1; D is the KL divergence; pDz ir[](·) is the

PRF RF+PRF TopPDrel AllPDrel
PsgF

30.0 50.8r 32.9p 63.4rp 29.3r+ 59.4p+ 32.3t 63.0rt p 33.4rtap 61.9p

26.2 44.9r 26.0 53.5p 25.2 54.7p 26.3t 55.5p
27.4rt + 55.1p

33.8 35.6 34.8 35.5 36.5rt

50.4 57.2 58.8p 56.8
56.8

18.0r 29.2r 22.6p 41.5p 21.9p 42.0p 23.8r+pt 43.3p 23.4pt 42.0p

Dirichlet-smoothed unigram language model induced from z with smoothing parameter .
The initial list, Dinit, is set to the 50 highest ranked documents d in the corpus by p(q|d) with  set to optimize MAP (at 1000) so as to have an initial list of reasonable quality.

Table 1: Performance numbers; 'r', 'p', '+', 't' and 'a' mark statistically significant differences with RF, PRF, RF+PRF, TopPDrel and AllPDrel respectively. The best result in a column is boldfaced.

We use relevance model number 3 (RM3) [1] as a query

e(1x-pan)sP ionxmXepthJxoMd[t]h(ta)tWas(sxig)ntso

the probability pJq term t; X is a set of

M[0](t) + passages

or documents; pJxM[](·) is the Jelinek-Mercer smoothed lan-

guage model induced from x with smoothing P parameter ;  is a free parameter; and, W (x) is x's weight ( xX W (x) = 1). Methods that use drel assign it with either a weight 1

(RF) or  (RF+PRF), which is a free parameter; in the lat-

ter case, the weight (1 - ) is distributed among the pseudo-

relevant documents. For any method, except PsgF, that

uses

pseudo-relevant

units

f

(

F



X ),

P f

p(q|f ) F p(q|f

)

serves

as f 's relative weight; for PsgF the (normalized) scores as-

signed in Equation 1 are used. The set X in RM3 is set to

(i) drel for RF, (ii) k highest ranked documents in Dinit for

PRF, (iii) drel and the k - 1 highest ranked documents d in Dinit (d = drel) for RF+PRF, and (iv) the k highest ranked

passages by Equation 1 for PsgF.

We use two additional passage-based baselines that utilize

RM3. TopPDrel sets X d=ef {g} (W (g) d=ef 1); g is drel's passage with the highest p(q|g) [2]. AllPDrel uses all drel's

passages (X d=ef {g|g  drel}) as pseudo relevant units.

For evaluation, drel is not considered (i.e., the residual

corpus approach is employed); MAP(@1000) and p@5 are

reported. The two-tailed paired t-test (95% confidence level)

serves for determining statistically significant differences.

The following free-parameter-values' ranges are used:  =

2000 except in pDd ir[](q) where we use the value used to create Dinit to maintain consistency with the initial ranking;

k  {5, 10, 20, 30, 40, 50};   {0, 0.1, . . . , 1};   {0.2, 0.4, 0.6,

0.8}; the number of terms used by RM3 is set to {25, 50, 75,

100, 500, 1000, 5000, ALL} ("ALL": all terms in the vocab-

ulary);   {0, 0.1, 0.3, 0.5, 0.7, 0.9};   {0, 0.1, . . . , 0.9}.

To give the "best chance" to the reference comparisons RF,

PRF, TopPDrel and AllPDrel, their free-parameter values

outperforms RF (often, statistically significantly), which is the standard approach of using drel as a whole unit. PsgF also outperforms RF+PRF in most relevant comparisons. As both methods integrate true and pseudo feedback, we see that using passages (by PsgF as opposed to RF+PRF) is of merit. We also see that TopPDrel is often inferior to AllPDrel and PsgF, which suggests that using several (weighted) passages rather than a single one is beneficial. Finally, except for WT10G, PsgF is MAP-superior to AllPDrel and posts more statistically significant MAP improvements over the other methods than AllPDrel does. This implies that using passages not in, as well as in, drel can be more effective than using just those in drel. However, in terms of p@5, PsgF is often outperformed by AllPDrel. Thus, integrating these two methods, along with utilizing term-proximity models [6, 5], is an interesting future venue.
Acknowledgments This paper is based upon work supported in part by the ISF under grant no. 557/09, and by IBM's SUR award. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsors.
4. REFERENCES
[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proceedings of TREC-13, 2004.
[2] J. Allan. Relevance feedback with too much data. In Proceedings of SIGIR, pages 337­343, 1995.
[3] G. Erkan and D. R. Radev. LexPageRank: Prestige in multi-document text summarization. In Proceedings of EMNLP, pages 365­371, 2004. Poster.
[4] E. Krikon, O. Kurland, and M. Bendersky. Utilizing inter-passage and inter-document similarities for reranking search results. ACM Transactions on Infortmation Systems, 29(1):3, 2010.
[5] H. Lang, D. Metzler, B. Wang, and J.-T. Li. Improved latent concept expansion using hierarchical markov random fields. In Proceedings of CIKM, pages 249­258, 2010.

are set to optimize MAP over all queries per corpus. For RF+PRF and PsgF, RM3's parameters are set to the values used by RF and AllPDrel, respectively; the other free parameters are set using leave-one-out cross validation per-

[6] M. Lease. Incorporating relevance and pseudo-relevance feedback in the markov random field model. In Proceedings of TREC 2008, 2008.
[7] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System:

formed over queries (MAP is the optimization criterion). Results, conclusions, and future work. Table 1 shows that all methods outperform the initial ranking used to create Dinit. (Most of these improvements are statistically significant; hence, they are not marked to avoid cluttering.) The MAP performance of our PsgF method is superior (often, statistically significantly so) for most corpora to that of the reference comparisons. Specifically, PsgF always MAP-

Experiments in Automatic Document Processing, pages 313­323. Prentice Hall, 1971.
[8] E. L. Terra and R. Warren. Poison pills: harmful relevant documents in feedback. In Proceedgins of CIKM, pages 319­320, 2005.
[9] X. Wan, J. Yang, and J. Xiao. Towards a unified approach to document similarity search using manifold-ranking of blocks. Information Processing and Management, 44(3):1032­1048, 2008.
[10] L. Zhao, C. Liang, and J. Callan. Extending relevance model

for relevance feedback. In Proceedings of TREC 2008, 2008.

1100


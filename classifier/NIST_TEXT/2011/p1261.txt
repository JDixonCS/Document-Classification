Formulating Effective Questions for Community-based Question Answering

Saori Suzuki
College of Knowledge and Library Sciences
University of Tsukuba
s0711607@u.tsukuba.ac.jp

Shin'ichi Nakayama
Graduate School of Library, Information and Media Studies
University of Tsukuba
nakayama@slis.tsukuba.ac.jp

Hideo Joho
Graduate School of Library, Information and Media Studies
University of Tsukuba
hideo@slis.tsukuba.ac.jp

ABSTRACT
Community-based Question Answering (CQA) services have become a major venue for people's information seeking on the Web. However, many studies on CQA have focused on the prediction of the best answers for a given question. This paper looks into the formulation of effective questions in the context of CQA. In particular, we looked at effect of contextual factors appended to a basic question on the performance of submitted answers. This study analysed a total of 930 answers returned in response to 266 questions that were formulated by 46 participants. The results show that adding a questionnaire's personal and social attribute to the question helped improve the perceptions of answers both in information seeking questions and opinion seeking questions.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation; H.3.5 [On-line Information Services]: Webbased services
General Terms
Experimentation, Human Factors
Keywords
Question formulation, context, CQA
1. INTRODUCTION
Taylor's 1968 model is perhaps one of the earliest work to point out that people's expression of their information needs can be compromised by a given environment [9]. Short queries frequently submitted to search engines show that Taylor's model is also evident on the Web. One goal of Information Retrieval (IR) research is to retrieve relevant information in response to such compromised expressions of people's information needs. In this regard, communitybased Question Answering (CQA) sites are an interesting venue for information seeking on the Web, since the expression of information needs does not have to be compromised as much as typical search engine queries. People use natural language to formulate a query and it is common to use multiple sentences to elaborate the information needs in CQA
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

services. Meanwhile, Yahoo! Answers alone are reported to have 200 million visitors worldwide in 20091, making CQA an important research area in IR. However, existing studies on CQA have been focused on the prediction of the best answers for a given question [1, 2, 8]. Investigations on how to formulate effective questions have been limited.
To address the problem, this paper examined effect of contextual factors in question formulation in the context of CQA. Appending a contextual factor has been shown to be effective in query formulation [7] in IR. Therefore, we hypothesised that appending a contextual factor to a basic question could improve the performance of submitted answers. To examine the hypothesis, we asked participants to formulate genuine questions with different contextual factors, submitted them to a CQA site, and assessed the answers responded to the questions.
2. METHOD
2.1 Contextual Factors
The first task was to identify the contextual factors to be examined in our investigation. Our approach was twofold. First, we revisited exiting context models proposed in IR. More specifically, we synthesised the context models proposed in Interactive IR [6], QE [7], and personalisation [3]. Second, we manually investigated a CQA archive to identify contextual factors that have been used in questions. We used the CQA data of Yahoo Japan Corporation which contains over 3 million questions posted during 2004 and 2005. 1,400 questions were randomly chosen from the data collection and manually analysed by one of the authors.
As a result of this process, we have identified five groups of contextual factors, namely, task (reasons, motivations, aims, and goals), thought (own thought, answer prediction), situation (circumstances, environment, experience, knowledge, and familiarity), attribute (personal attribute, social status), and limit (conditions on answers and answerers). The category of task, situation, and attribute had overlap with existing models in IR, while the category of thought and limit were found to be distinctive in CQA.
It should be noted that we appended only one contextual factor to basic questions once at a time in this study.
2.2 Question Types
We were also interested in how effect of contextual factors differed across the types of questions. We decided to look
1http://yanswersblog.com/index.php/archives/2009/ 12/14/yahoo-answers-hits-200-million-visitors-worldwide/

1261

at two types of questions based on the investigations by [1, 5, 4]. One type was called information seeking questions while another type was called opinion seeking questions. As the name suggests, the former type was generally looking for information such as facts and methods, while the latter type was looking for opinions and suggestions.
2.3 User Study
46 participants (27 females and 19 males) were recruited for the study. The majority of participants were the undergraduate and postgraduate students of our institution, with varied backgrounds (e.g., Management, Physics, Mathematics, Linguistics, Engineering, Medicine, Cultural Study, Media Study, Computer Science, Information Science, Chemistry, Art, Robotics, Biology, etc).
Participants were asked to formulate six genuine questions. Three of them were information seeking questions while others were opinion seeking questions. Participants were asked to include one of the five contextual factors in their question formulation. The remaining question was formulated without any contextual factor. We rotated the order of contextual factors to append across participants.
The formulated questions were then submitted to one of the major CQA sites in Japan by the experimenter, and recorded any answers given to the questions for 7 days. Participants were called again and asked to assess the answers from eight perspectives. They were Overall satisfaction, Topicality, Novelty, Situational relevance, Attitude, Trustiness, Promptness, and Length. A 5-point Likert scale was used to capture participants' assessment of answers.
266 questions were formulated and 930 answers were collected and assessed by participants.
3. MAIN RESULTS
We examined effect of five contextual factors used in question formulation on the perception of answers, by comparing to the questions formulated without using contextual factors. This section reports only on a subset of analysis we carried out during this study.
First, we examined the number of answers submitted to the questions. Although opinion seeking questions tended to receive more answers than information seeking questions (3.9 vs. 2.9, p  .005 by T-test), there was no significant effect of contextual factors on the number of answers.
Second, we examined participants' perceptions on the answers. The result pattern differed across the question types. In information seeking questions, the questions with Attribute and Thought had a higher average score of Attitude of answers (Attribute: p  .001, Thought: p  .003, by T-test). In opinion seeking questions, the questions with Attribute had a higher average score of Novelty of answers (p  .003, by T-test). On the other hand, the factors such as Task and Situation had a negative impact on the perceptions of their answers.
Overall, we gained some evidence suggesting that appending a contextual factor is a promising way to formulate effective questions in a CQA site. At the same time, the results suggest that some contextual factors should be avoided in a CQA site. Also, our results suggest that effective contextual factors can differ across the question types. A comprehensive description of this study and analysis of the results will be reported elsewhere.

4. CONCLUSION
This paper explored effect of contextual factors as a means of formulating effective questions in the context of communitybased question answering services. Genuine questions were formulated by our participants and some questions were presented with a contextual factor and others without. Based on the analysis of over 900 answers submitted to participants' questions, we gained evidence to support the following findings. First, including a contextual factor to the question can improve a questioner's assessment of the answers. Second, affective contextual factors can differ across the question type (information seeking and opinion seeking type). These findings can be a good starting point in guiding how beginners use CQA services to formulate their questions.
5. ACKNOWLEDGEMENTS
The authors thank to Yahoo JAPAN Corporation and National Institute of Informatics who gave us access to Yahoo! Chiebukuro Data (Version 1).
6. REFERENCES
[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. Knowledge sharing and yahoo answers: everyone knows something. In Proc. of the 17th international conference on WWW, pages 665­674. ACM, 2008.
[2] E. Agichtein, Y. Liu, and J. Bian. Modeling information-seeker satisfaction in community question answering. ACM Transactions on Knowledge Discovery from Data, 3:10:1­10:27, 2009.
[3] A. G¨oker and H. I. Myrhaug. User context and personalisation. In ECCBR Workshop on Case Based Reasoning and Personalisation, Aberdeen, 2002.
[4] F. M. Harper, D. Moy, and J. A. Konstan. Facts or friends?: distinguishing informational and conversational questions in social q&a sites. In Proceedings of the 27th international conference on Human factors in computing systems, pages 759­768, New York, NY, USA, 2009. ACM.
[5] F. M. Harper, D. Raban, S. Rafaeli, and J. A. Konstan. Predictors of answer quality in online q&a sites. In Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, pages 865­874, New York, NY, USA, 2008. ACM.
[6] P. Ingwersen and K. J¨arvelin. The turn: integration of information seeking and retrieval in context. Springer, 2005.
[7] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: a document-independent source of terms for query expansion. In Proc. of the 28th annual international ACM SIGIR conference, pages 457­464, 2005.
[8] Y. Liu, J. Bian, and E. Agichtein. Predicting information seeker satisfaction in community question answering. In Proc. of the 31st annual international ACM SIGIR conference, pages 483­490, 2008.
[9] R. S. Taylor. Question-negotiation and information seeking in libraries. College& Research Libraries, 29(3):178­194, 1968.

1262


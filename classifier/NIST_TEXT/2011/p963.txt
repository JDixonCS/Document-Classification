Posting List Intersection on Multicore Architectures

Shirish Tatikonda
IBM Almaden Research San Jose, CA, USA
statiko@us.ibm.com

B. Barla Cambazoglu
Yahoo! Research Barcelona, Spain
barla@yahoo-inc.com

Flavio P. Junqueira
Yahoo! Research Barcelona, Spain
fpj@yahoo-inc.com

ABSTRACT
In current commercial Web search engines, queries are processed in the conjunctive mode, which requires the search engine to compute the intersection of a number of posting lists to determine the documents matching all query terms. In practice, the intersection operation takes a significant fraction of the query processing time, for some queries dominating the total query latency. Hence, efficient posting list intersection is critical for achieving short query latencies. In this work, we focus on improving the performance of posting list intersection by leveraging the compute capabilities of recent multicore systems. To this end, we consider various coarse-grained and fine-grained parallelization models for list intersection. Specifically, we present an algorithm that partitions the work associated with a given query into a number of small and independent tasks that are subsequently processed in parallel. Through a detailed empirical analysis of these alternative models, we demonstrate that exploiting parallelism at the finest-level of granularity is critical to achieve the best performance on multicore systems. On an eight-core system, the fine-grained parallelization method is able to achieve more than five times reduction in average query processing time while still exploiting the parallelism for high query throughput.
Categories and Subject Descriptors
H.3.3 [Information Storage Systems]: Information Retrieval Systems
General Terms
Algorithms, Design, Performance, Experimentation
Keywords
Web search engines, multicore architectures, query processing, posting list intersection, intra-query parallelism
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'11, July 24­28, 2011, Beijing, China. Copyright 2011 ACM 978-1-4503-0757-4/11/07 ...$10.00.

1. INTRODUCTION
Large-scale Web search engines run under strict performance constraints: they must operate at a high target peak throughput and achieve a short response time (i.e., query latency) for each query [8]. Given the size and extent of the Web, these tight constraints must be obtained by exploiting parallelism over very large search clusters. The Web collection is document-based partitioned, and each node in a cluster stores a subset of the collection and the corresponding index. Queries are processed in parallel, i.e., the same query is evaluated concurrently over all local indexes.
To increase the peak throughput, one may replicate resources of the search engine. Replicas of a search cluster serve different queries independent of each other, increasing the number of queries that can be concurrently processed [8, 12]. High throughput, however, is not the only indicator of the performance of a search engine; a low query latency is also important. Recently, it has been empirically shown that high query latencies not only degrade user satisfaction but may also lead to losses in revenues of a search engine [26]. Hence, it is critical to carefully devise optimizations to improve query latencies. If the time constraint cannot be satisfied, queries may have to be processed in a degraded mode, i.e., only partial search results are returned [14]. Degradation causes the quality of search results to drop.
Typically, the response time of a search node to a query is engineered to be under one hundred milliseconds. In order to keep processing times within these bounds, a number of optimizations are employed in the search nodes. These include caching [4, 20], multi-threading [11], index compression [2, 32], and early termination of processing [1, 13, 15, 23, 29]. Emerging multicore architectures provide a new opportunity to decrease query latencies further. Multicores, primarily motivated by energy and power constraints, pack two or more cores on a single die. They typically share an on-chip L2 cache as well as the front-side bus to main memory. As these systems become more popular, the general trend has been from single-core to many-core: from dual-, quad-, eight-core chips to the ones with tens of cores. So far, however, very little has been done to exploit the full potential of these chips in our context.
In this paper, we focus on the posting list intersection problem, where the objective is to identify the set of documents that appear in all posting lists corresponding to the query terms, as efficiently as possible. We consider various coarse-grained and fine-grained parallelization strategies for improving the efficiency of list intersection on multicore systems. While there has been some efforts in parallel query

963

processing, existing strategies fail to exploit the fine-grained parallelism, which is critical to achieve good performance on multicore systems. To this end, we propose a framework, based on the producer-consumer model, that generates finegrained processing tasks to achieve some form of intra-query parallelism so that the same query can be concurrently processed on multiple cores. For task generation, we propose an algorithm that uses the skip information in the lists to create a set of intersection tasks. We evaluate our techniques on a system with eight cores, using a large document collection and a query log obtained from AltaVista Web search engine. When compared to the baseline approach that runs on a single core, we are able to speed up the intersection operation by up to a factor of 5.75 while keeping the throughput degradation to be as small as 3.3% ­ previous approaches experience at least 36% degradation.
This paper is organized as follows. Section 2 gives an overview of query processing on a typical search node. In Section 3, we describe different models for parallelizing the list intersection process on multicore architectures. In Section 4, experimental results are presented. Section 5 summarizes the related work. We finish the paper with the conclusions in Section 6.
2. BACKGROUND
In this section, we summarize the basic steps of query evaluation in a search engine, illustrating alternative options. Due to the embarrassingly parallel nature of query processing in search clusters [8], we restrict the discussion to an individual search node. Interested readers may refer to previous work in the literature on parallelization at the level of a search cluster [24, 25].
Query processing in a search node typically contains two phases [15]. In the first phase, the posting lists that correspond to the terms in the query are intersected to identify the documents that contain all query terms. Identified documents are ranked by means of a simple yet effective scoring function, such as a linear combination of the BM25 score with a query-independent score (e.g., a link analysis metric). In the second phase, a subset of the documents that are top-ranked by the first phase are re-ranked by a more accurate, but costly ranking function (e.g., a machine-learned ranker). Depending on the properties of the query, either of these phases may dominate the query processing time.
In this work, we focus on the efficiency of the first phase ranking and, in particular, posting list intersection, which dominates the processing cost, compared to the cost of scoring. The main reason search engines compute the first phase document scores using the intersection of posting lists (i.e., conjunction), instead of the union of the lists (i.e., disjunction), is that intersection leads to shorter query latencies as the document collections in Web search are fairly large. Moreover, intersection leads to higher result quality as most Web queries are very short. We note that, herein, we omit a discussion on relevance as it is out of scope. We also omit discussions on some optimization techniques (e.g., document id reassignment, list processing order) as they are not directly related and they complement our work.
A typical posting list is composed of a sequence of postings, each keeping some useful information about a document that contains the respective term for the list. In its simplest form, this information contains the document identifier and the frequency of the term in the document. Al-

though there are other options, postings in a list are typically sorted by decreasing frequency or increasing document id. The first option enables early-termination optimizations [1, 13, 23, 29], some of which speeds up processing while preserving the result quality for the top k results. The second option enables compression of postings based on d-gaps. Compressing posting lists [2, 32] leads to higher cache hit rates and fewer disk accesses, in exchange of increasing processing costs due to decompression overheads. A technique used to speed up processing of compressed posting lists is to place skip pointers at regular intervals between postings [28]. This provides a form of random access to postings in the list, which is not possible, otherwise, due to compression.
In our work, we assume that posting lists are compressed and contain skip information, and that the postings within lists are sorted by increasing document id. Typically, largescale search engines dedicate significant portions of server memory to cache most frequently and/or recently accessed posting lists to prevent expensive disk accesses [32]. Hence, we assume that disk accesses do not form a bottleneck. Note that none of these assumptions are enforced by the proposed techniques as our methods are quite general. Selection of this particular setting is mainly because it reflects the current design practices in Web search.

3. QUERY PROCESSING ON
MULTICORE ARCHITECTURES
Multicore systems constitute an important opportunity for developing efficient and scalable applications. The key to realize true benefits of these parallel systems lies in effective fine-grained parallelization of workloads. We now present various models for parallelizing the list intersection process (see Fig. 1). We consider two important metrics to compare these alternative models: throughput and average query latency. Throughput is measured as the number of queries processed per unit of time. Average latency is measured as the average time a query spends in the entire system. The latency of a query is defined as the difference between the time it arrives at the system and the time its processing is complete. Since all queries have to wait in the entry queue (associated with the search node) until their processing starts, the query latency includes both the waiting time in the queue and the processing time. Formally, we compute the latency (q) of query q by

(q) = wait time(q) + processing time(q) = (s(q) - a(q)) + (f (q) - s(q)) = f (q) - a(q),

where a(q) denotes the time q arrives at the system while s(q) and f (q) are the times at which processing of q is started and finished, respectively. Given a query workload W , the throughput T (W ) and the average query latency L(W ) under W are given respectively by:

T (W ) = |W | ,

tP(W )

L(W ) =

qW
|W

|

(q)

.

where |W | is the number of queries processed, and t(W ) is the total wall-clock time spent in processing W .
Web search engines are typically hosted on systems comprising a number of clusters, where each cluster node is pos-

964

q1

q2

P1

P2

qi

result
aggregator qn

qi

qi

qi

Pn

P1

P2

Pn

qi

result aggregator

qi

qi

qi

P1

P2

Pn

task creator
task pool

qi
result aggregator

P1

P2

Pn

inverted

index

I

I

I1

I2

In

I

(a)

(b)

(c)

(d)

Figure 1: Parallel Models for List Intersection: (a) Inter-query, (b) DOCP-UI, (c) DOCP-PI, (d) FGQP

Algorithm 1 ThreadFN(W, L)
Require: Query workload W Require: Set of posting lists L
{Each core executes this function in parallel} 1: while W is not empty do 2: q = GetNextQuery(W ) 3: D = Process(q, L) 4: R = Score(q, D) 5: Output(R) 6: end while
sibly a multicore system. Here, we primarily focus on leveraging the parallelism within a single cluster node. Next, we present two paradigms for parallel list intersection: interquery and intra-query. While the former exploits the parallelism across different queries, the latter focuses on the parallelism that is present within a single query.
3.1 Inter-query Processing
The simplest way to leverage multicore systems is to partition the query workload across different cores. Each incoming query is scheduled for execution on the next available processing core. All cores are kept busy by dispatching the queries from the entry queue to different processors using a simple first-come-first-serve strategy. This model explores the parallelism present among different queries. It relies on the fact that all queries are independent of each other in the sense that the output of one query does not affect the processing of any other query. Hence, we refer to this model as the inter-query processing model. This model is shown in Figure 1a ­ Pi's in the figure refer to different processing cores present in the multicore system; and I denotes the inverted index that hosts all the posting lists.
Algorithm 1 illustrate the way queries are processed in the inter-query-parallel model. In this model, all threads execute the function ThreadFN in parallel. Each thread picks up a query from the workload W (or from a stream of queries), processes it using the posting lists L, and outputs a result set R. We process P lists to compute their intersection using a modified version of the Small Adaptive algorithm, proposed by Demaine et al. [17]. An eliminator e is initially

set as the first item (document id) of the first list. The algorithm then repeatedly cycles through all lists searching for the eliminator. If e is present in all lists then its relevance score is computed, and the next eliminator is then chosen from the first list. If e is not present in some list L then the algorithm selects the smallest item from L that is larger than e as the next eliminator.
In a real-world scenario, the loop at line 1 is an infinite loop because the query stream is typically unbounded as queries continuously arrive at the system. For the parallel implementation, only the function call at line 2 forms a critical section, which requires synchronization. At any given time point, only one thread can exist in the GetNextQuery method. Since the critical section is very small, the parallelization overhead incurred in this model is minimal. It is likely that this model achieves good parallel efficiency, resulting in high system throughput as the number of cores in the system is increased.
3.2 Intra-query Processing
Although the inter-query model improves the number of queries answered in a given time period, the time it takes to process an individual query does not change irrespective of the number of processing units in the system. This is because a query is always processed by a single core. However, as mentioned earlier, most commercial search engines are typically driven by the query latency, and improving this is critical for user satisfaction. We next describe three different models that leverages the intra-query parallelism present within a single query.
3.2.1 Document Partitioning (DOCP)
This strategy partitions the document collection present on the search node equally among all processing cores. Each core is responsible for identifying matching documents for the given query in the allocated partition of documents. To form the final answer, we aggregate the partial sets of results produced, one from each core. Recall that the search nodes summarize the information about all documents in the form of an inverted index. Hence, this strategy corresponds to partitioning the accesses over different parts of the index. The partitioning can be done in two ways ­ logical and physi-

965

cal. According to logical partitioning, all cores share a single unified index but each core accesses a different portion of the index. Such a strategy is shown as DOCP-UI in Figure 1b. Each processing core P1 to Pn gets a copy of the query qi, and they process the query against disjoint portions of the index I. The partial results produced by each core are then combined in the result aggregator, which is typically implemented as a shared data structure. DOCP-UI method of parallelization has been suggested by Frachtenberg [19].
The alternative choice to implement the DOCP is to physically partition the index I into n equal parts I1 to In, where n is equal to the number of cores. This strategy is shown as DOCP-PI in Figure 1c. As it is evident from the figure, both DOCP-UI and DOCP-PI differ only in the way index is partitioned and used among processing cores.
For a simpler design, we implement these strategies using a master-slave model. The master (a designated core) is responsible for several tasks ­ to fetch the next query from the query stream; to send a copy of the query to all the other cores; to process the query against the allocated index partition; and to send the combined results out of the search node. While more decentralized methods are plausible, they introduce various control overheads. It is important to note that these document partitioning based parallel strategies require some form of barrier synchronization at the end of processing each query.
3.2.2 Fine-Grained Query Partitioning (FGQP)
Both DOCP-UI and DOCP-PI strategies divide the index accesses into n parts where n is equal to the number of cores. These coarse-grained strategies suffer from load imbalance, and they often provide suboptimal performance. Several earlier studies [30] have also shown that it is imperative to leverage the fine-grained parallelism to achieve good performance on multicore systems. We now present a mechanism that divides the work associated with a single query into several small and independent tasks (see Fig. 1(d)). These tasks are then processed in parallel to produce the result set. Small-sized tasks allow us to exploit the fine-grained parallelism within the query, and as a result, they enable mechanisms that guarantee good load balance among threads. It must be noted that very small tasks increase the parallelization overhead incurred in creating and managing these tasks. Independent tasks, on the other hand, reduce the need for synchronization and thereby help in realizing better parallel efficiency.
The framework of our intra-query model is shown in Fig. 2. This framework adopts a producer-consumer model, where one of the threads acts as the producer and remaining threads act as consumers. The producer takes a query from the entry queue, fetches associated posting lists, and produces multiple, independent posting list intersection tasks. Generated tasks are pushed into an intersection task pool. Each consumer thread iteratively obtains a task from this intersection task pool and processes its task. Processing a task mainly involves decompression and intersection of posting lists. Documents that are common in all lists are added to the scoring task pool. They are later picked up by different threads for scoring in order to produce the final, ranked result set. The producer thread also participates in processing of intersection and scoring tasks, whenever possible. In contrast to the producer-consumer model, one can consider a setup where all threads are capable of both producing and

Posting lists

CC

PC

C

Thread pool

Query

Intersection task
creation

Intersection task
processing

Scoring task
processing

Results

Intersection task pool

Scoring task pool

Figure 2: Framework of the intra-query model.

Algorithm 2 Producer(W, P )

Require: Query workload W

Require: Set of posting lists P

Require: Intersection task pool IT P

Require: Scoring task pool ST P

Require: Pool threshold 

1: while W is not empty do

2: while |IT P |   do

3:

q = GetNextQuery(W )

4:

T = CreateTasks(q, P )

5:

ITP = ITP  T

6:

NotifyConsumers()

7: end while

8: {Act as a consumer}

9: while |IT P | >  do

10:

t = GetNextTask(IT P )

11:

D = ProcessTask(t)

12:

ST P = ST P  {D}

13: end while

14: end while

consuming tasks. Such a decentralized approach, however, requires complex synchronization methods, and it is likely to incur higher control overhead.
The scoring phase requires the threads to iteratively pop a document from the scoring task pool and compute a score, which is then used to rank the document. Evidently, the scoring step is an embarrassingly parallel problem that requires a simple data-parallel implementation.
Algorithms 2 and 3 show the steps of producer and consumer threads in the intra-query model, respectively. By Algorithm 2 (lines 2­7), the producer can choose to temporarily pause the task creation process and act as a consumer. If the size of the task pool is smaller than a threshold  (line 9), the producer resumes the task creation duty. The producer relies on the pool threshold  to decide whether or not to generate tasks for new queries. Higher values of  delays the task generation, and generated tasks are likely to spend more time in the task pool before they get picked up for processing. Further discussion on the impact of  is presented in Section 4.2. Note that only the producer process is responsible for task creation in this approach. Other decentralized variants are possible, but they are likely to incur high control overhead.

966

Algorithm 3 Consumer()
Require: Intersection task pool IT P Require: Scoring task pool ST P 1: while IT P is not empty do 2: t = GetNextTask(IT P ) 3: D = ProcessTask(t) 4: ST P = ST P  {D} 5: end while

a1 L1 20
b1 L2 10

a2 ... 35
b2 ... 50

a3 ... 90
b3 ... 90

a4 null
... 175

b4 ... 145

b5 null
... 205

Tasks: t1 = (a1, a2)  (b1, b2), t2 = (a2, a3)  (b1, b3), t3 = (a3, a4)  (b3, b5)
Figure 3: Example skip lists
3.2.3 Task Creation
The producer thread receives a new query from the workload and partitions the work associated with the query into tasks by using the posting lists required by the query (lines 3­5 in Algorithm 2). We partition posting lists so that each task is responsible for intersecting some portion of the lists. Unfortunately, posting lists are stored in compressed format to save space and this limits the ability to perform random accesses required while creating tasks. To facilitate effective work partitioning while retaining the benefits of compression, we store postings in the form of skip lists. A skip list is a sorted list of document identifiers (also referred to as items), denoted using a linked list that connects sparse subsequences of items [22]. Each pointer in the list forms a link i  j between two non-consecutive items i and j. The number of items skipped between two skips1 i and j is referred to as the skip block size or skip size. Two example skip lists are shown in Fig. 3, where ai's and bj's are the skips in lists L1 and L2, respectively.
We compress all document ids between a pair of skips, but store the skips without compression. For example, in L1 of Fig. 3, the ids 20, 35, 90, and 175 are stored as uncompressed values, but the ids between these skips are compressed. For convenience, we also store the highest document id in the list as a separate skip pointing to null. More formally, the posting list for a query term a is denoted as L(a) = (Sa, Ca), where Sa is the sequence of skips in the skip list and Ca is the sequence of document ids in compressed format.
The producer thread generates tasks by operating on these compressed skip lists. Without loss of generality, we assume that the lists are sorted in increasing order of their size. The main idea is, for each skip block in the first (or the shortest) list, to find the relevant skip blocks from the remaining lists. A task is then formed by the sequence of skips obtained from all lists in the query. The total number of tasks generated for a given query is equal to the number of skip blocks present in the shortest posting list.
Consider a query q with two terms a and b, whose posting lists are L(a) = (Sa, Ca) and L(b) = (Sb, Cb) with |Sa|  |Sb|. For each skip pointer in L(a), we create a task with one
1We use the term "skip" interchangeably to indicate a link or the end points of a link.

Algorithm 4 CreateTask()

Require: Set of posting lists P

Require: Intersection task pool IT P

Require: Query q = (q1, q2, . . . , qm){Assume that query terms are sorted in increasing order of posting list size}

1: n = |S(q1)| {The number of skips in L(q1)} 2: for i = 1 to n - 1 do

3: InitTask(t)

4: d1 = d[L(q1), i] 5: d2 = d[L(q1), i + 1] 6: add (i, i + 1) to t

7: for r = 2 to m do

8:

find j and k in L(qr) = (Sqr , Cqr ) s.t.

i  j < k  |Sqr |, d1  d[L(qr), j], and

d2  d[L(qr), k]

9:

add (j, k) to t

10: end for

11: IT P = IT P  {t}

12: end for

or more skips from L(b). We generate a set of independent tasks {t1, t2, . . . , t|Sa|}, where
ti = [(si, si+1), (sj, sk)],
si, si+1  Sa, for 1  i  |Sa|, and
sj, sk  Sb, for 1  j  k  |Sb|.
Note that si+1 is undefined for i = |Sa|. Moreover, for a given si and si+1 in L(a), the skips from L(b) are chosen such that si  sj and si+1  sk. In other words, all documents in L(a) within a skip pointer si  si+1 fall in the id interval given by [sj , sk] in L(b). If the query contains more than two terms, for each skip block in the shortest list, a sequence of skips is found from all remaining lists. The entire procedure is summarized in Algorithm 4 for the general case of a query with m terms. Here, d[L(a), i] denotes the document id at the ith skip in L(a). The skips j and k in line 7 can be found either by a linear scan or by a binary search. Each task t that is generated by the algorithm has the following form:

t = [(s1i1 , s11+i1 ), (s2i2 , s2i2 ), . . . , (sm im , sm im )],

(1)

where m = |q| and sjk indicates the kth skip in the jth posting list. It is important to note that each task encapsulates

the complete information required to intersect the relevant

portions of all posting lists. Hence, a processor can inde-

pendently process a task without requiring any information

about other tasks. For the skip lists in Fig. 3, Algorithm 4

produces three tasks by considering the sequence of skips in

L1 and in L2.

3.2.4 Task Processing
All threads iteratively dequeue tasks from the task pool and process them to determine the common documents. Every entry (sjk, sj), for 1  j  m, in a task can be treated as a sublist having a sequence of skips (sjk, sj) and the corresponding compressed skip blocks, as seen in Eq. (1). The problem of processing a task with m entries is then reduced to that of intersecting m sublists. Hence, any of the existing list intersection algorithms [5, 6, 7, 16, 17] can be used. As mentioned in Section 3.1, we use a modified version of the Small Adaptive algorithm [17] to intersect all sublists in a given task. Our task processing algorithm first decompresses a single skip block from all sublists. The intersection process is initiated on these decompressed blocks. Whenever

967

the search for an eliminator overshoots the current block, the next skip block from the sublist is decompressed. This process is terminated either when there are no skip blocks for decompression or after all items in the shortest sublist are considered by the intersection process.
4. EMPIRICAL EVALUATION
4.1 Dataset and Setup
We evaluate different parallel query processing models using a subset of the AltaVista query log (about a quarter million queries).2 Posting lists are obtained from a large crawl of the UK domain, obtained in May 2006.3 All lists are compressed with skip list information as described in Section 3.2.3. Skip blocks are compressed using the PForDelta [33] algorithm, which is recently shown to be efficient in terms of both compression quality and decompression time [32]. The default skip block size is set to 512, but is varied in some experiments. We assume that all posting lists are loaded into the main memory before the queries are processed. In other words, we do not account for the time spent in disk I/O while evaluating our models (see the discussion in Section 2).
In our experiments, we consider that the query arrival process follows a Poisson distribution with an intensity of  query/sec. We evaluate our models using the two metrics defined in Section 3: query throughput and average query latency. The parallel performance of our models is evaluated by considering the fastest sequential version as the baseline. This baseline is given by the inter-query model that runs on a single processor (denoted as INTER-1), where the queries are processed one after the other, on a first-come-first-serve basis. Therefore, the data points that correspond to running intra-query models on a single core are not shown in any of our performance plots.
All experiments are conducted on a multicore system with dual quad-core Clovertown E5345 2.33GHz processor (essentially, a total of eight cores) with 6GB main memory. The system has 8MB aggregated L2 cache space that is shared among eight cores. The parallel code is implemented in C++ using the pthreads library.
4.2 Performance Comparison
The performance differences between the two parallel query processing paradigms are shown in Figs. 4(a) and 4(b). We measure how throughput and average query latency vary with the number of cores used for query processing. For FGQP, we show two trends that differ in the value of pool threshold . Recall from Section 3.2.2 that the value of  affects the producer process in generating tasks for new queries. The throughput achieved by INTER and FGQP models increases almost linearly with the number of cores. This indicates that these models effectively make use of the available computational resources. When using eight cores, the INTER is able to serve 850 query/sec while the FGQP (=150) model is able to serve up to 826 query/sec. The marginal throughput difference between INTER and FGQP is primarily due to additional parallelization overhead incurred in FGQP. This includes the time spent in creating and managing tasks,
2There are a total of 47, 278 unique query terms. 3Available from the University of Milan: http://law.dsi. unimi.it/webdata/uk-2006-05/.

and aggregating partial results from individual cores. On the other hand, the overhead incurred in INTER is minimal since each core operates independently on a different query, and thereby reduces the need for synchronization ­ see Section 4.5 for further details on parallelization overhead.
When compared to FGQP, the query throughput achieved by other intra-query models that rely on document partitioning is small across the chart. This is mainly because DOCP-UI and DOCP-PI target coarse-grained parallelism among large document partitions, whereas FGQP exploits fine-grained parallelism among small-sized tasks. For a given query, the skew among the finish times of different cores is very high in DOCP-UI and DOCP-PI when compared to that in FGQP. Consequently, document partitioning based strategies suffer from load imbalance, and as a result, they deliver poor parallel efficiency. When compared to the sequential baseline (inter on a single core), INTER and FGQP (=150) improve the throughput by a factor of 7.9 and 7.6 when all eight cores in the system are used. In contrast, DOCP-UI and DOCP-PI models are able to improve it only by a factor of 5.1 and 4.8, respectively.
In terms of average query latency (see Fig. 4(b)), the interquery model is unable to leverage extra processing capabilities in the multicore system to reduce latency. In fact, the average query latency achieved by INTER is almost constant across the board since queries are always executed on a single core irrespective of the number of available processing units. On the other hand, strategies that leverage intraquery parallelism are able to reduce the query latency since any given query is processed by multiple cores, in parallel. Note, however, that the reduction is not linear with the number of cores. For example, the average query latency from the sequential baseline INTER-1 is about 9.27ms. In contrast, the latency achieved by DOCP-UI and DOCP-PI models has improved from 5.4ms to 1.9ms as the number of cores increases from 2 to 8. Similarly, the latency of FGQP(=5) has reduced from 5.4ms to 1.61ms. When compared to INTER-1, this amounts to an improvement by a factor of 5.75.
To summarize, INTER model gives the best query throughput as the parallelization overhead is minimal but it does not offer any improvement in query latency. The performance of both document partitioning based strategies (DOCP-UI and DOCP-PI) are very similar to each other. While they provide good improvements in query latency, their query throughput is significantly affected by load imbalance issues from coarse-grained parallelism. In contrast, FGQP leverages a fine-grained intra-query parallelism, and it delivers query throughput that is very close to INTER while providing excellent latency improvements that are similar to document partitioning strategies.
The performance of FGQP, however, depends on the value of pool threshold , which is illustrated in Fig. 4(c). We consider two metrics as we vary the value of : percent degradation in query throughput (PDTP ) and improvement in average query latency (IAQL) due to task-based intra-query processing. These metrics are computed, respectively, as
PDTP = ((T PINTER-1 -T PFGQP)/T PINTER-1 )×100, and
IAQL = TINTER-1 /TFGQP,
where T P(·) and T(·) refer to the realized query throughput and total wall-clock time spent in processing the entire query workload, respectively. We would like the throughput degradation to be as small as possible and the latency

968

Query Throughput (queries/sec)

Average Query Latency (msec)

(a)
900 INTER
800 DOCP-UI
700 DOCP-PI FGQP(=150)
600 FGQP(=5)

500

400

300

200

100

0

1

2

4

6

Number of Cores

(b) 12

Average Query Latency (msec)

10

8

6

4 INTER

DOCP-UI

2 DOCP-PI FGQP(=150)

FGQP(=5)

0

8

1

2

4

6

Number of Cores

(c) 20
Query Throughput Degradation (%) Query Latency Improvement
15

10

5

0

8

5 10 20 50 75 100 150

Magnitude

106 105 104 103 102 101 100
200

Figure 4: (a, b) Comparison of different parallelization models, and (c) impact of  on FGQP.

(a)

INTER-8 DOCP-UI-8 FGQP(=5)-8
INTER-4 DOCP-UI-4 FGQP(=5)-4
400 600 800 1000 1200
Query arrival rate ()

1400

Average Query Latency (msec)

(b) 80
INTER 70 FGQP(=5)
DOCP-UI 60
50
40
30
20
10
0
2 3 4 5 6 7 8 9 10 Query Size

Query Throughput (queries/sec)

(c)

1400 INTER

1200

FGQP(=5)

DOCP-UI

1000

800

600

400

200

0
2 3 4 5 6 7 8 9 10
Query Size

Figure 5: (a) Impact of  on average query latency, and (b, c) performance as the query length varies.

improvement to be as high as possible. As the value of  increases, the producer process creates new tasks more aggressively. This increases the average query latency as tasks spend more time in the task pool before they are consumed for processing. Aggressive task generation also reduces the chance of encountering an empty task pool. This increses the query throughput since the consumer processes are always kept busy processing the tasks. Therefore, higher  values result in higher query throughputs and higher query latencies, and lower  values give lower query throughputs and lower query latencies. Such a trade-off between throughput degradation and latency improvement is evident in Fig. 4(c). As the value of  is increased from 5 to 150, the degradation in throughput (when compared to INTER-1) drops from 16% to 3.3%, while the latency improvements drop from 5.75-fold to 3.7-fold. As a comparison, we observed the degradation in throughput for DOCP-UI and DOCP-PI when running on eight cores to be 36% and 41%, respectively.
The value of  can be tuned based on the performance requirements set by end applications. For a given upper bound on the query latency, the value  can be adjusted to maximize the query throughput while keeping the latency within the given limit. On both extremes, we observed the best query throughput at  = 150, and similarly the average query latency can be improved by as much as 5.75-times by setting  = 5. The rest of the results in this section, unless otherwise mentioned, are obtained with  = 5.
4.3 Effect of System Load
To investigate the effect of system load, we model a Web search engine as a queue with  queries arriving and 

queries getting served per second.4 The system will be in a stable state as long as   . When  > , the average number of queries present in the system (both waiting and in processing) will increase over time. In this experiment, we analyze the performance in such overloaded conditions by measuring how the average query latency varies as we increase the query arrival rate . The service rate , i.e., the rate at which queries are processed is directly proportional to the throughput of the system, which in turn relies on the number of cores used to process the queries.
We now consider a system with a fixed number of cores (equivalent to a fixed ) and analyze the effect of increasing the query load (see Fig. 5(a)). We vary the value of  from 100 query/sec up to 1400 query/sec and observe the change in the average query latency. As  increases, the latency stays constant until the system saturates, after which the latency increases steeply with . The saturation point of the system is a point beyond which the arrival rate dominates the service rate. In case of INTER running on four cores (denoted as INTER-4), the system saturates around 400 query/sec. Any further increase in  beyond 400 query/sec results in up to three orders of magnitude increase in query latency. In the figure, it can also be observed that the saturation point shifts towards the right as the number of cores increases (INTER-8 saturates around 800 query/sec). This highlights the fact that a system that is provisioned ith more computational resources can serve more requests without saturating. This is a significant result given the architectural trend that promises an increasing number of cores in
4The service rate  in our models is primarily determined by the number of cores that are used in query processing.

969

105

Average Query Latency (msec)

104

103

102

101

100 10-1
100

INTER,length=4 DOCP-UI,length=4

FGQP(=5),length=4 INTER,length=6
DOCP-UI,length=6 FGQP(=5),length=6

200 300 400 500 600 700 Query arrival rate ()

800

Figure 6: Performance with varying query arrival rate and query length.

future multicore server systems. We also observed similar trends for models that leverage intra-query parallelism. Furthermore, the performance of DOCP-PI is very similar to the one of DOCP-UI, and hence we do not include DOCP-PI in rest of the experiments.
While INTER and FGQP saturate around the same value of , the saturation point of DOCP-UI is notably much smaller. This can be attributed to the fact that document partitioning based strategies provide lower query throughputs (see Section 4.2). At small values of  when the system is not saturated, the performance of FGQP is superior to that of INTER due to its task-level parallel processing. However, in a saturated system, this difference is small because the benefit from task-level processing is overshadowed by large queuing times experienced by the queries.
4.4 Effect of Query Length
Query length is known to be positively correlated with the performance of a search task [9]. A recent study showed that, in the past year, there has been a 22% growth in the number of queries with eight or more terms.5 There are also estimations that the average query length has a tendency to increase over time.6 Moreover, search backends are more likely to receive long queries as most short queries are caught by the result cache [27]. Given these observations, we conduct experiments to analyze the performance of our models as the query length is varied. For this experiment, we separate the AltaVista query log into nine bins according to query length (lengths are between two and ten). Each bin contains 10, 000 queries of a particular length. This experiment is conducted on eight compute cores.
As shown in Fig. 5(b), the query length directly affects the average query latency, and it is indirectly related to the system throughput. Longer queries involve intersection of a larger number of lists, and hence they lead to higher query processing times. The average latency of queries with ten terms is approximately an order of magnitude greater than that of queries with only two terms. This accounts for a 11-fold drop in system throughput when processing longer queries. The intra-query models consistently provide better
5 http://www.itfacts.biz/average- us- query- size- injanuary-2009/12782 6http://www.beussery.com/blog/index.php/2008/02/ google- average- number- of- words- per- query- haveincreased/

query latencies when compared to INTER, and FGQP performs marginally better than DOCP-UI. Fig. 5(c) indicates that the throughput of DOCP-UI is consistently lower than that of INTER and FGQP, and the difference between INTER and FGQP is only marginal. An interesting observation here is that the throughput difference among different models is higher when the queries are short. This is because the parallelization overhead is relatively significant when compared to the actual time spent in query processing.
The combined effect of query arrival rate  and query length on the performance is studied in Fig. 6. We consider queries of length 4 and 6 in the experiment, and we change query arrival rate from 100 to 800 query/sec. For a given number of cores, the saturation limit is indirectly correlated to the query length ­ longer queries lead to earlier saturation. For a given query length, the trends are similar to those observed earlier in Fig. 5(a). In a saturated system, the queries suffer from large latencies, accounting for an increased waiting time spent in the queue. The amount of change in latency increases with the query length. It can again be noticed that DOCP-UI saturates prior to INTER and it experiences marginally higher latency at high values of . On the other hand, FGQP has a lower latency than INTER when the system is not saturated. They both saturate around the same value of , and beyond which they behave approximately in a similar manner.

4.5 Task Creation Overhead in FGQP

In this experiment, we measure the overhead incurred in FGQP model due to task-level processing. The task size not only affects the processing time but also affects the amount of synchronization that is required among cores. The synchronization overhead (due to accumulation, accessing the task pool etc.) is high when a given query is partitioned into a large number of tasks. We measure this overhead by defining two quantities: Savg and Oavg. While Savg is the average number of skips per list per task, Oavg measures the average number of overlapped skips per list, per task. They indirectly indicate the amount of work required and the amount of work that is repeated per task.
Recall that a task is denoted as the sequence of skips from each posting list involved in the query. For a query with m terms, a task t can be denoted as shown in Eq. (1). The average number St of skips in task t can be computed as

St = 1 + (i2

- i2) + · · · + (im m

- im) .

The overall Savg is computed as

P

Savg =

tT
|T |

St

,

where T is the set of all tasks generated for the entire workload of queries. Due to the way the tasks are generated, it is possible for two consecutive tasks t1 and t2 to share at most one skip per list. We quantify this overlap as follows:

t1 = [(s1i1 , s11+i1 ), (s2i2 , s2i2 ), . . . , (sm im , sm im )], t2 = [(s1j1 , s11+j1 ), (s2j2 , s2j2 ), . . . , (sm jm , sm jm )],

where j1 = 1 + i1 and

Ot2 =

P 1<km(jk - ik m-1

+ 1)+ ,

where (·)+ indicates that the entry is retained only if the

970

20

18

Number of skips per list Number of tasks per query

16

Number of tasks Task creation time

Savg

14

Oavg

12

Value

10

8

6

4

2

0

512

256

128

64

Skip block size

Figure 7: Overhead in task creation (8 cores).

argument is positive. Then, the quantity Oavg is computed

as the average taken over all tasks in the workload.

P

Oavg =

tT Ot |T |

Note that the first task that is generated for every query does not contribute to the summation.
Fig. 7 shows different quantities for various skip block sizes (B) by considering the values for B = 512 as the baseline. As B is reduced by half, both the number of skips per list and tasks per query double. Also, the amount of time spent in generating the tasks increases significantly as the value of B is decreased. Surprisingly, the values of Savg and Oavg remain almost constant across the board. For each value of B, we observe Savg and Oavg to be equal to 10.2 and 0.97, respectively, i.e., out of 10.2 skips/list/task, 0.97 skips overlap with the previous task. In other words, approximately 10% of the skips are processed twice. Overall, we observed that the time spent in task creation is only 2% of the total wall-clock execution time. The overhead due to task creation is therefore very small when compared to the actual amount of work that is needed for posting list intersection.

5. RELATED WORK
In the literature, there are various techniques for parallel query processing on distributed architectures [3, 21, 24, 25], where the inverted index is partitioned over a number of nodes in a search cluster. These techniques are a form of high-level intra-query parallelism because parallelization is at the cluster-level and a query is processed concurrently by many index servers. Our work is complementary to these in that intra-query parallelism is employed further in every search node at the level of cores.
The problem of intersecting sorted lists has also attracted some attention [5, 6, 7, 16, 17]. Most of the proposed algorithms assume that the lists are already sorted. Typically, one of the lists is used as an eliminator and some randomaccess lookups are performed (via binary search, galloping search, or interpolation search) in the remaining lists for a target element in the eliminator list. Due to random accesses, these algorithms are not applicable to compressed posting lists unless skips are employed.
So far, the only work that considers the list intersection problem on multicore architectures is that of Tsirogiannis et al. [31]. The authors propose two algorithms called Dynamic Probes and Quantile-based. The idea in the first algorithm is to restrict the search space by using a micro-index that

keeps a history of previous lookups. The second algorithm statically partitions the posting lists for the sake of improved load balancing, by using summaries of lists obtained via order statistics. Both algorithms require random accesses to the lists. The partitioning overhead is observed to become a bottleneck for high number of cores if the cache is not shared between all cores (see Figs. 11 and 12 in [31]).
Ding et al. [18] propose a framework for query processing on graphics processors (GPUs). Queries are first preprocessed on a CPU and then selectively scheduled on the GPU. Performance of their list decompression and intersection techniques developed for GPUs provide only marginal improvements over CPU-based implementations (see Table 2 in [18]). This is because the accesses to main memory from GPUs incur significant overhead, and more importantly, the way they parallelize the workload does not match the access pattern best suited to GPUs (sequential scans starting from some aligned memory locations).
Bonacic et al. [11] use a strategy similar to Inter that groups queries into batches and processes them sequentially on multicores. While their method is targeted at increasing query throughput, we focus also on reducing query latencies by exploiting the intra-query parallelism. Strohman and Croft [28] present a preliminary evaluation of in-memory query processing techniques on multicore systems. Their techniques exhibit modest performance improvements on a four-core system. Frachtenberg [19] also explores the performance differences between inter-query and document partitioning based strategies. In fact, the best approach suggested by Frachtenberg (referred to as FGMT in [19]) is same as DOCP-UI in Fig. 1(b). This work assumes that hits on document ids are evenly distributed. However, in practice, hits have a skewed distribution due to reassignment of document ids [10]. Our fine-grained FGQP model is immune to these kinds of biases in posting lists as intersection tasks are dynamically created. Moreover, compared to [19], we provide experimental results on a broader set of parameters.
6. CONCLUSION
We explored two different paradigms for parallel posting list intersection on multicore systems. While the inter-query model can scale system throughput with the number of cores by executing independent queries in parallel, the intra-query models are able to scale system throughput and reduce the average latency as we increase the number of cores. Our key contribution is in the design of a work partitioning strategy that divides a single query into multiple small and independent tasks, to enable fine-grained parallelism that is critical for effective parallel performance on multicores. When compared to the baseline approach that runs on a single core, this approach is able to speed up the intersection operation by up to a factor of 5.75 while keeping the throughput degradation to be as small as 3.3% ­ previous approaches experience at least 36% degradation. We also evaluated the influence of query length and arrival rate on latency and throughput, showing that the fine-grained intra-query model does not introduce any unexpected behavior compared to the inter-query approach. Finally, our analysis of skip block size indicated that smaller skip blocks increase the number of tasks and hence the overhead to create tasks. However, the fraction of redundantly processed skip blocks per task is almost constant irrespective of the skip block size.
We considered a hybrid strategy that switches between

971

intra-query and inter-query approaches according to query traffic. Our preliminary results did not show much improvement, and we are currently analyzing them carefully. As part of future work, we would like to investigate methods that leverage our parallel query processing models for improving the search quality, effective server resource provisioning, and designing power-aware algorithms that adaptively adjust the performance to stay within the budgeted power envelopes.
7. ACKNOWLEDGMENTS
This work has been partially supported by the COAST Project (ICT-248036), funded by the European Community.
8. REFERENCES
[1] V. N. Anh, O. Kretser, and A. Moffat. Vector-space ranking with effective early termination. In Proc. 24th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 35­42, 2001.
[2] V. N. Anh and A. Moffat. Compressed inverted files with reduced decoding overheads. In Proc. 21th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 290­297, 1998.
[3] C. Badue, R. Baeza-Yates, B. Ribeiro-Neto, and N. Ziviani. Distributed query processing using partitioned inverted files. In Proc. 8th Symp. String Processing and Information Retrieval, pages 10­20, 2001.
[4] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, V. Plachouras, and F. Silvestri. The impact of caching on search engines. In Proc. 30th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 183­190, 2007.
[5] R. A. Baeza-Yates. A fast set intersection algorithm for sorted sequences. In Proc. 15th Annual Symp. Combinatorial Pattern Matching, pages 400­408, 2004.
[6] J. Barbay. Optimality of randomized algorithms for the intersection problem. In Proc. 2nd Int'l Symp. Stochastic Algorithms: Foundations and Applications, pages 26­38, 2003.
[7] J. Barbay, A. L´opez-Ortiz, and T. Lu. Faster adaptive set intersections for text searching. In Proc. 5th Int'l Workshop on Experimental Algorithms, pages 146­157, 2006.
[8] L.A. Barroso, J. Dean, and U. Holzle. Web search for a planet: the Google cluster architecture. IEEE Micro, 23(2):22­28, 2003.
[9] N.J. Belkin, D. Kelly, G. Kim, J.Y. Kim, H.J. Lee, G. Muresan, M.C. Tang, X.J. Yuan, and C. Cool. Query length in interactive information retrieval. In Proc. 26th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 205­212. ACM New York, NY, USA, 2003.
[10] R. Blanco. Index compression for information retrieval systems. PhD thesis, University of A Corun~a, 2008.
[11] C. Bonacic, C. Garcia, M. Marin, M. Prieto, F. Tirado, and C. Vicente. Improving search engines performance on multithreading processors. In Proc. 8th Int'l Conf. High Performance Computing for Computational Science, pages 201­213, 2008.
[12] Eric A. Brewer. Lessons from giant-scale services. IEEE Internet Computing, 5(4):46­55, 2001.
[13] C. Buckley and A. Lewit. Optimizations of inverted vector searches. In Proc. 8th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 97­110, 1985.
[14] B.B. Cambazoglu, F.P. Junqueira, V. Plachouras, S. Banachowski, B. Cui, S. Lim, and B. Bridge. A refreshing perspective of search engine caching. In Proc. 19th Int'l Conf. World Wide Web, pages 181­190, 2010.
[15] B.B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. Early exit

optimizations for additive machine learned ranking systems. In Proc. 3rd ACM Int'l Conf. Web Search and Data Mining, pages 411­420, 2010.
[16] E. Demaine, A. Lo´pez-Ortiz, and J. I. Munro. Adaptive set intersections, unions, and differences. In Proc. 11th ACM-SIAM Symp. Discrete Algorithms, pages 743­752, 2000.
[17] E. Demaine, A. Lo´pez-Ortiz, and J. I. Munro. Experiments on adaptive set intersections for text retrieval systems. Lect. Notes Comput. Sc., 2153:91­104, 2001.
[18] S. Ding, J. He, H. Yan, and T. Suel. Using graphics processors for high performance IR query processing. In Proc. 18th Int'l Conf. World Wide Web, pages 421­430, 2009.
[19] E. Frachtenberg. Reducing query latencies in web search using fine-grained parallelism. World Wide Web, 12(4):441­460, 2009.
[20] Q. Gan and T. Suel. Improved techniques for result caching in web search engines. In Proc. 18th Int'l Conf. World Wide Web, pages 431­440, 2009.
[21] A. Moffat, W. Webber, J. Zobel, and R. Baeza-Yates. A pipelined architecture for distributed text query evaluation. Inf. Retr., 10(3):205­231, 2007.
[22] A. Moffat and J. Zobel. Self-indexing inverted files for fast text retrieval. ACM Trans. Inf. Syst., 14(4):349­379, 1996.
[23] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency-sorted indexes. J. Am. Soc. Inf. Sci., 47(10):749­764, 1996.
[24] D. Puppin, F. Silvestri, and D. Laforenza. Query-driven document partitioning and collection selection. In Proc. 1st Int'l Conf. Scalable Information Systems, 2006.
[25] B. Ribeiro-Neto and R. A. Barbosa. Query performance for tightly coupled distributed digital libraries. In Proc. 3rd ACM Conf. Digital Libraries, pages 182­190, 1998.
[26] E. Schurman and J. Brutlag. Performance related changes and their user impact. Velocity ­ Web Performance and Operations Conf., 2009.
[27] Gleb Skobeltsyn, Flavio Junqueira, Vassilis Plachouras, and Ricardo Baeza-Yates. ResIn: a combination of results caching and index pruning for high-performance web search engines. In Proc. 31st Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 131­138, 2008.
[28] T. Strohman and W. Croft. Efficient document retrieval in main memory. In Proc. 30th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 175­182, 2007.
[29] T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In Proc. 28th Int'l ACM SIGIR Conf. Research and Development in Information Retrieval, pages 219­225, 2005.
[30] S. Tatikonda and S. Parthasarathy. Mining tree-structured data on multicore systems. Proc. VLDB Endow., 2(1):694­705, 2009.
[31] D. Tsirogiannis, S. Guha, and N. Koudas. Improving the performance of list intersection. In Proc. 35th Int'l Conf. Very Large Data Bases, pages 838­849, 2009.
[32] J. Zhang, X. Long, and T. Suel. Performance of compressed inverted list caching in search engines. In Proc. 17th Int'l Conf. World Wide Web, pages 387­396, 2008.
[33] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-scalar RAM-CPU cache compression. In Proc. 22nd Int'l Conf. Data Engineering, 2006.

972


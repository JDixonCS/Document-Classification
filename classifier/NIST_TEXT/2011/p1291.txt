A Tool for Comparative IR Evaluation on Component Level
Thomas Wilhelm, Jens Kürsten, and Maximilian Eibl
Chemnitz University of Technology Straße der Nationen 62, 09111 Chemnitz, Germany
{thomas.wilhelm, jens.kuersten, eibl}@cs.tu-chemnitz.de

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software ­ performance evaluation
General Terms: Experimentation, Measurement, Perfor-
mance
1. MOTIVATION
Experimental information retrieval (IR) evaluation is an important instrument to measure the effectiveness of novel methods. Although IR system complexity has grown over years, the general framework for evaluation remained unchanged since its first implementation in the 1960s. Test collections were growing from thousands to millions of documents. Regular reuse resulted in larger topic sets for evaluation. New business models for information access required novel interpretations of effectiveness measures. Nevertheless, most experimental evaluations still rely on an over 50 year old paradigm.
Participants of a SIGIR workshop in 2009 [1] discussed the implementation of new methodological standards for evaluation. But at the same time they worried about practicable ways to implement them. A review about recent publications containing experimental evaluations supports this concern [2]. The study also presented a web-based platform for longitudinal evaluation. In a similar way, data from the past decade of CLEF evaluations have been released through the DIRECT1 system. While the operators of the latter system reported about 50 new users since the release of the data [3], no further contributions were recorded on the web-platform introduced in [2].
In our point of view archiving evaluation data for longitudinal analysis is a first important step. A next step is to develop a methodology that supports researchers in choosing appropriate baselines for comparison. This can be achieved by reporting evaluation results on component level [4] rather than on system level. An exemplary study was presented in [2], where the Indri system was tested with several components switched on or off. Following this idea, an approach to assess novel methods could be to compare to related components only. This would require the community to formally record details of system configurations in connection with experimental results. We suppose that transparent descriptions of system components used in experiments could help researchers in choosing appropriate baselines.
2. DEMONSTRATION
Our demo2 aims to record and visualize the actual configuration of components used in modern IR systems. A flexible interface
1 http://direct.dei.unipd.it/ 2 http://sachsmedia.tv/compeval
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

was developed for the import of experimental results. Visual presentation allows comparison on component level. Interactively selected configurations are provided for export. A largescale experimental study on several test collections served as data source to introduce the concept. In contrast to the study presented in [2], numerous commonly used implementations of IR system components were tested here. The tool was applied to analyze relations and interactions of system components across test collections w.r.t. retrieval effectiveness.
Figure 1: Visual representation of a subset of 450 experiment configurations evaluated on two document collections Another intention of this demo is to stimulate the discussion of current issues and future directions in experimental evaluation:
· To what extent are common baselines needed? · How and where to implement such baselines? · How to persuade/convince IR researchers to use them? We suggest that a simple but formal description of system components should be incorporated and archived at the level of evaluation campaigns. This could be presumably achieved by integrating this demo into existing evaluation platforms.
REFERENCES
[1] Jaap Kamps, Shlomo Geva, Carol Peters, Tetsuya Sakai, Andrew Trotman, and Ellen Voorhees. 2009. Report on the SIGIR 2009 workshop on the future of IR evaluation. SIGIR Forum 43, 2, pp. 13-23.
[2] Timothy G. Armstrong, Alistair Moffat, William Webber, and Justin Zobel. 2009. Improvements that don't add up: ad-hoc retrieval results since 1998. In Proceedings of the 18th ACM CIKM conference, pp. 601-610.
[3] Maristella Agosti, Giorgio Maria Di Nunzio, Marco Dussin, and Nicola Ferro. 2010. 10 years of CLEF data in DIRECT: Where we are and where we can go. In Proceedings of the 3rd EVIA workshop, pp. 16-24.
[4] Allan Hanbury and Henning Müller. 2010. Automated component-level evaluation: present and future. In Proceedings of the 2010 international conference on Multilingual and multimodal information access evaluation. Springer-Verlag, Berlin, Heidelberg, pp. 124-135.

1291


Learning to Rank From a Noisy Crowd

Abhimanu Kumar
Department of Computer Science
University of Texas at Austin
abhimanu@cs.utexas.edu

Matthew Lease
School of Information
University of Texas at Austin
ml@ischool.utexas.edu

ABSTRACT
We study how to best use crowdsourced relevance judgments learning to rank [1, 7]. We integrate two lines of prior work: unreliable crowd-based binary annotation for binary classification [5, 3] and aggregating graded relevance judgments from reliable experts for ranking [7]. To model varying performance of the crowd, we simulate annotation noise with varying magnitude and distributional properties. Evaluation on three LETOR test collections reveals a striking trend contrary to prior studies: single labeling outperforms consensus methods in maximizing learner accuracy relative to annotator effort. We also see surprising consistency of the learning curve across noise distributions, as well as greater challenge with the adversarial case for multi-class labeling.1
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]
General Terms
Algorithms, Design, Experimentation, Performance
Keywords
Crowdsourcing, learning to rank, active learning
1. INTRODUCTION
Crowdsourcing platforms like Amazon Mechanical Turk2 are changing the dynamics of how we train our learners. While labeled data is no longer as difficult to obtain, individual labels tend to be noisier and require greater quality assurance, e.g. by requesting redundant labels from multiple annotators and resolving disagreements automatically via consensus [5, 3]. When annotation is noisy, how do we best utilize labeling effort to maximize learning? Do we label additional examples (improve coverage), or request more labels for already labeled examples to reduce label noise [5]? How should we compute consensus with such multi-labeling? For learning to rank [1, 7], how sensitive is the learner to different quanitities and distributions of label noise?
1Following acceptance to SIGIR'11, this paper underwent secondary review and was accepted to HCOMP 2011 [2] as a previously published work. The paper appears in the proceedings of SIGIR'11 only.
2https://www.mturk.com
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

Prior work compares single labeling (SL) each example to multi-labeling for binary classification [5]. Given a fixed seed set of N singly-labeled examples and an infinite pool of unlabeled examples, SL grows this set of labeled examples with each new label (to increase example coverage), whereas multi-labeling requests additional labels for examples in the seed set (to increase label accuracy). With 2N labels, SL covers 2N examples with a single label while multi-labeling covers the N examples with two labels each in round-robin fashion. Simple majority vote (MV) is used for consensus.
Our prior work [3] on binary classification compared SL to multi-labeling with Naive Bayes (NB) [6] as well as MV. We studied effects of modeling vs. ignoring worker accuracy and saw across methods that modeling worker accuracy significantly improved classifier accuracy, indicating a limitation of the oft-used simple majority vote with noisy annotation. While a variety of crowd behaviors and noise may arise in practice, both prior studies [5, 3] assumed assumed uniform noise, as well as each label coming from a unique annotator. Other worker behaviors and noise characteristics may be observed in practice and could be usefully modeled.
Prior work by Yang et al. [7] studied learning to rank (with graded judgments) rather than binary classification, evaluating SL, MV, and other consensus algorithms for ranking with LambdaRank. They assumed labels come from reliable experts and provided limited analysis of the relationship between consensus method and the resulting learning curve.
This paper extends our earlier study from binary classification to learning to rank, and we consider learning under different noise quantities and distributions. We compare SL, MV, and NB for consensus, and we measure resulting ListNet [1] ranking accuracy on three LETOR [4] collections: OHSUMED, MQ2007 and MQ2008. We observed similar results across all three and so present results on OHSUMED only due to space constraints. We respect LETOR's standard 5-fold partition with 3 training folds and the others for validation and testing. While training labels come entirely from the crowd, we make a significant assumption of having expert labels for the entire validation fold ( 3500 examples). Note this validation data is used only by ListNet, not by consensus methods. We also use expert labels as ground truth for evaluation. This reflects a scenario in which more costly expert annotation suffices for validation and testing, but larger volumes of more affordable data is desired for training.
We use a seed set size of N = 800 (potentially noisy) singly-labeled examples, reflecting a minimal training size to obtain stable results. The learning curve is then measured as a function of adding L additional labels. For each setting

1221

L No noise (SL) Distribution
N (0.7, 0.2)
N (0.5, 0.2)
N (0.4, 0.2)
ln N (0.4, 0.2)
U (0.2, 0.6)

0 30.9
L
0 800 1600 3200 6400 Average
0 800 1600 3200 6400 Average
0 800 1600 3200 6400 Average
0 800 1600 3200 6400 Average 0 800 1600 3200 6400 Average

800
33.5
SL Rank 21.2 23.5 29.0 33.0 35.3 30.2
21.1 23.7 28.8 31.7 36.1 30.1
17.0 19.5 25.8 30.4 34.0 27.4
18.4 21.1 26.9 27.9 28.6 26.1 17.7 19.0 26.6 26.7 26.4 24.7

1600 3200 36.4 36.9
MV Rank Label
27.6 61.8 27.3 63.6 27.0 70.2 26.4 77.0 27.1 68.1
24.5 52.6 28.0 54.2 24.5 55.1 26.5 58.4 25.9 55.0
22.6 46.8 24.0 44.7 22.7 44.4 21.9 39.5 22.8 43.85
22.3 40.6 23.4 40.1 19.8 35.8 19.7 30.5 21.3 36.7
21.3 37.4 21.1 33.8 18.8 29.6 17.3 23.9 19.6 31.1

6400 Avg 38.3 36.3
NB Rank Label
25.2 62.3 28.6 75.3 30.6 90.6 30.4 96.9 28.7 81.2
22.2 54.7 25.6 66.4 29.1 81.3 31.0 90.1 27.0 73.1
22.9 49.0 23.9 61.4 28.4 76.1 27.0 85.7 25.6 68.0
21.7 41.1 22.2 40.0 21.7 37.1 21.8 36.7 21.8 38.7
20.5 38.5 17.1 35.6 19.8 35.2 23.2 32.4 20.1 35.4

Table 1: Label accuracy and ListNet rank accuracy (%)

achieved by SL vs. MV and NB consensus methods for vary-

ing L and quantity and distribution annotation noise (nor-

mal N (µ, ), log normal ln N (µ, ), and uniform U (min, max).

Expected label accuracy for SL is defined by noise parame-

ters

(mean

µ

or

min+max 2

;

we

report empirical accuracy

for

MV and NB. L additional labels are added to the seed set

of N = 800 singly-labeled examples. We also report average

accuracy of each method across L = {800, 1600, 3200, 6400}. We

repeat experiments 5 times and average for stability.

of L, we compute consenus labels (no-op for single labeling) and then train ListNet using them. We report label accuracy achieved as well as the resultant ranking accuracy achived by ListNet. We measure this across different noise settings.
We simulate noisy annotation via a fixed-size pool of 100 annotators who select between C = 3 possible labels (ternary graded relevance classes: non-relevant, relevant, or highly relevant). Each annotator i has a unique parameter pi denoting the probability he will produce the correct label for a given example. Otherwise he produces one of the other two possible labels (uniformly) at random. New labels are generated by selecting an annotator i from the pool at random and then generating a label according to pi as just described.
Results without annotation noise and for five possible noise settings are shown in Table 1. Noiseless ranking accuracy with N = 800 L = 0 provides an approximate upperbound for MV and NB consensus results across settings of L since perfect consensus would restore us to the noiseless condition. While level of noise clearly impacts the learning curve (Figure 1), we see relatively little impact of different noise distributions on ranking accuracy. Overall, it seems

when average accuracy exceeds 50%, sufficient "good" anno-

tators exist to overcome the noise of their less reliable peers.

Between N = 800 and N = 1600, SL begins to con-

sistently outperform NB and MV across noise distribu-

tions, with greater example coverage apparently more im-

portant than label accuracy. Effects here may be task-

specific or learner-specific, and having expert validation la-

bels may benefit SL more than MV and NB since SL label-

ing accuracy on training examples is lowest. We aso see NB

typically outperform MV across noise distributions.

We define an adversarial annotator for multi-class anno-

tation with C

classes as one whose pi

<

1 C

.

In such cases,

a simple way fix is to randomly pick one of the other C - 1

classes. We saw little benefit from doing so. Suppose an an-

notator has accuracy 0.2. Assuming a uniform prior over

remaining classes, each has probability 0.4, so not much

higher than the class originally labeled. We expect more

benefit from handling adversarial labeling when accuracy is

extremely low (i.e. strongly anti-correlated), or when we

have a better prior for selecting between remaining classes.

Figure 1: Consensus label accuracy vs. ranking accuracy of
the ListNet learner shows a strong linear relationship across
consensus methods and noise distributions considered (not
shown). This suggests one can simply optimize for label accu-
racy with confidence of improving rank accuracy as a result.
Future work includes: a similar study with real crowd workers and data, developing more representative models for simulation, studying additional consensus methods and noise settings, and dynamic example selection for labeling.
Acknowledgments. We thank the anonymous reviewers for their valuable feedback. Eunho Yang provided the ListNet implementation. This work was partially supported by a John P. Commons Fellowship for the second author.
2. REFERENCES
[1] Z. Cao, T. Qin, T.-Y. Liu, M. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML, 2007.
[2] A. Kumar and M. Lease. Learning to rank from a noisy crowd. In 3rd Human Computation Workshop (HCOMP) at AAAI, 2011.
[3] A. Kumar and M. Lease. Modeling annotator accuracies for supervised learning. In WSDM Workshop on Crowdsourcing for Search and Data Mining, 2011.
[4] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR Learning to Rank Workshop, 2007.
[5] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In KDD, 2008.
[6] R. Snow, B. O'Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast--but is it good? In EMNLP, 2008.
[7] H. Yang, A. Mityagin, K. Svore, and S. Markov. Collecting high quality overlapping labels at low cost. In Proc. SIGIR, 2010.

1222


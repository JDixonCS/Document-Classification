A Site Oriented Method for Segmenting Web Pages

David Fernandes
Dep. of Computer Science Fed. Univ. of Amazonas Manaus, AM, Brazil
david@dcc.ufam.edu.br

Edleno S. de Moura
Dep. of Computer Science Fed. Univ. of Amazonas Manaus, AM, Brazil
edleno@dcc.ufam.edu.br

Altigran S. da Silva
Dep. of Computer Science Fed. Univ. of Amazonas Manaus, AM, Brazil
alti@dcc.ufam.edu.br

Berthier Ribeiro-Neto
Dep. of Computer Science Fed. Univ. of Minas Gerais Belo Horizonte, MG, Brazil
berthier@dcc.ufmg.br

Edisson Braga
Dep. of Computer Science Fed. Univ. of Amazonas Manaus, AM, Brazil
edisson@dcc.ufam.edu.br

ABSTRACT
Information about how to segment a Web page can be used nowadays by applications such as segment aware Web search, classification and link analysis. In this research, we propose a fully automatic method for page segmentation and evaluate its application through experiments with four separate Web sites. While the method may be used in other applications, our main focus in this article is to use it as input to segment aware Web search systems. Our results indicate that the proposed method produces better segmentation results when compared to the best segmentation method we found in literature. Further, when applied as input to a segment aware Web search method, it produces results close to those produced when using a manual page segmentation method.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval: Information Search and Retrieval]: Search process
General Terms
Algorithms, Experimentation
Keywords
Page Segmentation, Segment Class
1. INTRODUCTION
Recent work has shown that Web pages can be sub-divided into distinct segments or blocks [5, 10, 21] (such as main content, service channels, decoration skins, navigation bars, copyright and privacy announcements), and that information about these segments can be used to improve informa-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'11, July 24­28, 2011, Beijing, China. Copyright 2011 ACM 978-1-4503-0757-4/11/07 ...$10.00.

tion retrieval tasks such as ranking [1, 6, 13, 14, 28], Web link analysis [4], and Web mining [8, 19, 22].
Most of previous segmentation methods we found in literature segment each Web page based on its content only, not considering the content of other pages present in the same Web site. However, since many pages belonging to a same Web site share a common structure and look and feel, we hypothesize that we can achieve a more accurate segmentation by taking all pages of a same Web site into account when dividing them into segments. Based on this idea, we propose and evaluate in this article a segmentation method which segments pages according to properties of the whole Web site, instead of just information obtained from a single page.
Our segmentation method adopts a DOM tree alignment strategy previously proposed in literature to solve the problem of template detection [31, 33]. However, we find no application of such strategy in literature to address the problem of segmenting Web pages. While segmentation of pages and template detection may appear as similar tasks at a first glance, there are important differences between these two tasks. Segmentation consists in dividing a Web page into its structural blocks, each structural block may contain templates or not and may also be part of a template. Further, in a segmentation process an area that does not contain templates may be divided into several blocks.
Our segmentation method is particularly useful to segment the so called data-intensive Web sites, such as digital libraries, Web forums, news Web sites, electronic catalogs, or institutional sites, whose main focus is providing access to a large quantity of data and services [9]. These sites usually contain a large set of web pages which can be organized in a few tens of groups according to the regularity of their structure. Our segmentation method takes advantage of such regularity to automatically segment data intensive web sites.
Besides segmenting the Web pages, our algorithm is able to cluster the segments into segment classes, which are set of segments that play the same role in a group of pages of a site. Knowledge about the segment classes can be used to estimate a weight for each segment in a Web collection, indicating its importance for ranking purposes [14, 13]. In [13], the authors show how to compute these weights automatically, and then how to estimate term weights according to the weights of the blocks where they occur. The ranking

215

method they proposed is particularly useful for data intensive web sites, where search is an important way of accessing data, given the large amount of information usually provided to their users. This type of Web site is also the main focus of our work here.
Even though this article focus on the use of the segments and their classes as input to segment aware Web search systems, the segmentation method here proposed may be applied to many other applications. For instance, segmentation algorithms can be used to improve the quality of results in many Web mining applications [23]. As another example, segmentation techniques can be also useful for the contextual advertisement problem, in which advertisers often wish to place (or avoid placing) distinct ads on specific regions of a same Web page [35].
To evaluate the performance of our segmentation algorithm, we performed experiments with four distinct Web collections to compare its results with those produced by the Rule-based Block Fusion algorithm -- a recently proposed automatic segmentation algorithm that achieves good performance [21]. The results indicate that our algorithm outperforms the baseline for all four Web sites tested and that the results are far superior for two of the sites tested.
We also studied the impact of using our automatic segmentation method with a segment aware ranking search system. For this, we ran experiments to compare such ranking strategy with (i) a traditional ranking function that does not use segment information, (ii) a traditional ranking function applied to pages after template removal, and (iii) a segment aware ranking search system applied to manually segmented Web pages. The experiments indicate that our segmentation method, when combined with the segment aware ranking system, leads to improved results in comparison to the two first baselines. Further, the loss in ranking quality in comparison to the third ranking baseline is not significant in most cases and is small even in the worse results.
The remaining of this paper is organized as follows. Section 2 covers related work. Section 3 presents the model we adopt here to represent Web sites. Section 4 presents the proposed automatic Web page segmentation method. Section 5 reports results we obtained from experiments we run to evaluate the effectiveness of our method in generating suitable segments and the impact of the segmentation it generates in segment aware ranking systems. Finally, Section 6 concludes this article.
2. RELATED WORK
The process of analyzing a Web site to detect structural divisions of its pages has been addressed in several previous research articles [3, 5, 10, 16, 21]. One of the best solutions found in the literature, called Vision-based Page Segmentation algorithm (VIPS) [5], segments a page by simulating the visual perceptions of the users about that page. The VIPS algorithm requires a considerable human effort, since it is necessary to manually set a number parameters to each page to be segmented.
Chakrabarti et al [10] also consider the problem of segmenting Web pages into visually and semantically cohesive pieces. This work uses approximation algorithms for clustering problems based on weighted graphs. In the case of the segmentation problem proposed by [10], the weights of edges of the graph capture the likelihood that two regions of the Web page are placed together or apart in the segmen-

tation, and the clusters are the segments occurring in the pages.
Kohlschu¨tter et al [21] examined the problem of Web page segmentation from a textual perspective. The key assumption in their work is that the number of tokens in a text fragment (or more precisely, its token density) is a valuable feature for segmentation decisions. Based on this assumption, the authors presented a Segment Fusion algorithm for Web page segmentation using text density metrics. The authors show that their method outperforms the method proposed by Chakrabarti et al. Given its quite strong results, we decided to use it as the baseline in our experiments. Notice that this method uses less information than ours when segmenting pages, since it segments each page individually and we segment a page based on web site statistics.
Many authors have argued that information present on page templates is noisy and should be disregarded to avoid deteriorating search precision [20, 31]. Such an idea has motivated the proposal of methods for identifying noisy contents or templates based on the structure of the Web pages [3, 15, 20, 24, 31, 33]. However, template extraction is a task different from segmentation. Segmentation consists in dividing a Web page into its structural blocks and each structural block may contain templates or not. Further, in a segmentation process an area that does not contain templates may be divided into several blocks.
Our method aligns the DOM trees of Web pages of a site in order to uncover their implicit structure. Aligning the DOM trees to extract structural information from of Web sites and Web pages is not a new strategy. This idea has been used previously for Web site template identification [31, 33] and web mining [27, 34]. However, we find no application of such strategy in literature to address the problem of segmenting Web pages. The adaptation and heuristics described by us to adopt the DOM tree alignment to the problem of segmenting Web pages constitute the major contribution of our work when compared to the previous application of such strategy. As we show through experiments, the DOM tree alignment strategy is an excellent alternative to solve the problem of segmenting Web pages.
Bar-Yossef and Rajagopalan have also proposed a method that uses the DOM tree of pages in a site to guide the segmentation process [3]. While their proposal can be considered as a segmentation method, their main goal was to detect noisy information in Web pages. As the main heuristic to identify segments, they count the number of links found in each HTML element. If an element contains more than K links it is considered as a segment, otherwise it is counted as part of the element containing it. We have considered the possibility of including their method in our experiments as a segmentation method for sake of completeness, but the method would produce a quite low quality segmentation and we have not reported experiments with it.
In Cai et al. [6], the authors proposed a method for taking advantage of the segments generated for pages in a search task. They present a ranking strategy where the segments are processed as semantic passages and then apply previously proposed passage-level ranking strategies [7] to evaluate the possible improvements obtained in ranking quality with their method. Results are superior to other previously studied types of passages, such as paragraphs or fixed size windows.
The method proposed by Song et al. in [28] also adopted

216

the vision based segmentation proposed by [5]. The authors used such a segmentation to compute an importance rank of segments found in a page through a learning approach. Spatial features, such as position and size of segments, and content features, such as the number of images and links on each segment, are extracted from each segment. Two learning algorithms, neural network and SVM, are used to rank the segments according to their importance.
In de Moura et al. [13], the authors propose a method for automatically computing segment importance factors to improve Web search. Such a method uses statistical information available in the Web page collection to assign an importance value to all segments of each page found in the collection. Their method produces a significant improvement in the quality of search results when compared to several baselines. However, it requires a complex segmentation mechanism and the authors proposed no fully automatic strategy to generate it.
The segmentation methods found in the literature aim to identify the segment structure of a single page given as input. The method presented in [13] requires the identification of a common structure shared by a large quantity of pages. The purpose of such an identification is to cluster the segments into segment classes. A segment class is a set of segments that share a common role on a group of pages of a same site. Although the authors proposed the use of VIPS [5] to guide the segmentation process, the clustering of segments into segment classes requires a manual process even when using VIPS, making the application of the method unfeasible in large Web page collections.
We here propose a novel method for automatically divide Web pages from a collection into segments. This method can be considered as a significant improvement in the proposal of de Moura et al., since our experiments indicate that their segment aware Web search method produces results almost similar in terms of quality either using our new segmentation approach or using the manual approach they originally use.
3. REPRESENTING WEB SITES AS SEGMENT COMPONENTS
Unlike plain text documents, Web pages usually encompass multiple regions such as navigation bars, decoration stuffs, copyrights, advertisements and contact information. Each of these regions play a particular role within the pages and are generically referred to as segments. In this Section we present some basic definitions which are useful for better characterizing the problem of dividing the Web pages into segments. Our discussion is based on [13].
Definition 1. A segment s is a self-contained logical region within a Web page that (i) is not nested within any other segment and (ii) is represented by a pair (l, c), where l is the label of the segment, represented as the root-to-segment path in the Web page DOM tree, and c is the portion of text of the segment.
It follows that a Web page n might be represented as a finite set of (non-overlapping) logical segments n = {s1, ..., sk}, with k varying according to the page structure. The definition of segments we adopt here allows several types of partitions for a given Web page, subject to the interpretation of a human analyst or to the behavior of the segmentation method adopted. Since our segment definition admits that

any arbitrary subtree of a DOM structure be assigned as a segment, the decision of how to divide a page into segments should be done by the segmentation algorithm. In this work, we adopt the criterion that the division of a Web page into a set of non-overlapping segments must be performed according to the perception of users about the best logical division of the page.
A Web site S is represented here as a set of Web pages S = {1, ..., m}, each of them composed of a set of segments. Given this representation, we can now define the concept of segment class. Segments of distinct pages can be clustered into classes as follows:
Definition 2. A segment class C is a set of segments that belong to distinct pages of a given Web site and that share a same label, i.e.,
C = {s1, s2, ..., snC }
s1 = (lC , c1) s2 = (lC , c2)
... snC = (lC , cnC ) where nC is the total number of segments in class C, lC is the common label, si is the ith segment in class C, and ci is its corresponding portion of text. To illustrate, consider the two news pages depicted on Figure 1. Notice that each page contains a news title, which we refer to as title segments. Since these title segments are in the same position in both pages, they have a common rootto-segment path and thus, a same label. As a consequence, they are part of a same segment class. We also notice that segments with the same label tend to have the same role within their respective pages. That is, a segment class is usually composed of segments that belong to distinct pages of a Web site and that play the same role within the site. Various examples of segments that belong to the same class occur in the news pages depicted in Figure 1, such as segments that represent the menu, the news body, the summary of the news in the page, etc.
Figure 1: News pages 1 and 2, extracted from CNN Web site.
4. WEB PAGE SEGMENTATION
Algorithm 1 describes the automatic segmentation process, which tries to segment the pages close to the perception of users about how each page should be divided into segments, and how these segments should be clustered into segment classes. It takes the set of pages S found in the Web site as

217

input and produces as output a set of segment classes. Our segmentation algorithm is divided into three phases, that will be discussed in the following sections.
4.1 Phase 1: Pre-processing Web Pages
The first phase of our algorithm takes as input the pages of the site. In this phase, the DOM trees of the pages are pre-processed to obtain a structural representation that is closer to our definition of segments and segment classes. For instance, we use this phase to identify and remove nodes of the DOM tree that may produce nested segments (that are not allowed by our model). It is also used to aggregate regions of regular structures, such as menus or lists of items, which would be seen by a user as a single segment, but that would be broken into several small segments by the heuristics we adopt in the second phase.

Algorithm 1 Segment(S)

1: { Phase 1: Pre-processing Web Pages}

2: for each pi  S {i is the number of a page} do

3: i  preProcess(pi);

4: end for

5: { Phase 2: Constructing the SOMtree}

6: SOMtree  ;

7: for each i  S {i is the number of a page} do

8: for each node N  i traversed in pre-order do

9:

if Ns  SOMtree | Ns.label = N.label then

10:

Ns.counter  Ns.counter + 1;

11:

if N.f lag then

12:

Append i in Ns.pageList;

13:

end if

14:

else

15:

insertNewNode(S OMtree ,N ,i);

16:

end if

17: end for

18: end for

19: {Phase 3: Refining the SOMtree }

20: for each node Ns  SOMtree traversed in pre-order do

21: if Ns is an internal node AND Ns.pageList =  then

22:

for each node ns  descendants(Ns) traversed in pos-order

do

23:

if distance(Ns,ns)<  AND ns is a leaf node then

24:

Append ns.pageList to Ns.pageList;

25:

Remove ns from SOMtree;

26:

end if

27:

end for

28: end if

29: end for

30: for each node Ns  SOMtree traversed in pos-order do

31: if ns.counter < , ns  children(Ns) then

32:

for each ns  children(Ns) do

33:

Remove ns from SOMtree;

34:

Add ns.pageList to Ns.pageList;

35:

Add ns.counter value to Ns.counter;

36:

end for

37: end if

38: end for

39: {Each node Ns of SOMtree represents now a segment class, where

Ns.pageList contains the list of segments of Ns, and Ns.label

contains the label of the segments}

40: Return the SOMtree

Further, the pre-processing also is used to aggregate further information about each node and adjusts the DOM trees to be used in the second phase of the algorithm. Figure 2 depicts a segment of HTML code and the pre-processed version of its DOM tree. To simplify, only the leftmost node of the DOM tree depicts the additional information introduced by the pre-processing phase. This example is used in the following sections to show how each individual page is pre-processed (see Lines 1­4). The function P reP rocess of our algorithm consists of four main operations which are described on the following sections.

Figure 2: An example of HTML code and the preprocessed version of its DOM tree.
Creating the node labels
Given a page , its DOM tree is a structure that represents the hierarchical relationship between the tags found in the page. Each tag is represented by a node N that contains information about the name of the tag (N.name) and a list of its attributes (N.attr), where each attribute is composed by a pair of name (N.attr.name) and value (N.attr.value). We use this information to recursively define the label of each node N (N.label) as the concatenation of its N.name, N.attr.name field values and the label value of its parent in the DOM tree (considering it as empty when the node is the root of the tree), separating each token in the concatenation by a slash. Whenever two sibling nodes get equal values assigned by this strategy, we distinguish them by adding a sequential number to their label values, so that each node has a unique label.
For instance, in the leftmost node of Figure 2, the value of name is "div" and the only value of attribute name is "class", thus attr.name={"class"}. Its label is "1/div/class/body/html". The value "1" is added because there is a second node that would get the same label (see the rightmost div tag). This node is the identified by "2/div/class/body/hmtl" in our representation. For the node labelled as "img" in the Figure, the label is "img/body/html".
Although information about textual content is not included in DOM tree, it is useful for our segmentation algorithm. Thus, we add to each node N a boolean value (N.f lag), which indicates whether it has textual content assigned to it (N.f lag = true) or not (N.f lag = f alse).
Dealing with nested content
Our segmentation algorithm assumes that only the leaf nodes of a DOM tree may have textual content assigned to them. This assumption is important in order to avoid the generation of nested segments during the future phases of our algorithm. Thus, whenever we find internal nodes with textual content, we do not represent their children in our DOM tree representation, and thus it becomes a leaf node. The textual content associated to the removed tags is then associated to this new leaf node.
For instance, let us consider the second tag <div> in DOM tree of Figure 3. Notice that such a tag contains a text directly attached to it (Last year in Orlando...). Besides, inside this text there is a tag <a>, that also has a textual content (anchor text). In this case, we say that the content of <a> is nested to the content of <div>. As nested contents can generate nested segments in the future phases of our algorithm, the tag <a> is removed from our DOM tree representation, and its textual content is associated to the content of <div>.

218

<div>

<div class=fullcontent>

Last year in Orlando...

<a href=link>

Iphone Killer

</a>

Rather, a Sprint spokeswoman

called it... </div>

Nested content

</div>

Anchor text

Figure 3: Example of tag with nested content.

Dealing with recurrent regions
Another change we perform in the DOM trees is to remove sequences of tags disposed in a recurrent (or regular) way. In the DOM tree of a typical Web page, it is very common to find regions with a quite regular structure. Let us consider the Web page depicted in Figure 4. Notice that the menu signalized by the Arrow 2 is formed by a set of links disposed in a vertical bar. As the set of tags that separate two adjacent links is always the same, we can say these tags are disposed in a regular or recurrent way.

2

1

3

Figure 4: Recurrent structures: the regions pointed by the arrows contain sets of items (links or books) disposed in a regular or recurrent way.
Regions with recurrent structure commonly contain a list of items highly related to each other such as list of links, products, or paragraphs (notice that the regions signalized by the arrows 1 and 3 are also examples of regions with recurrent structures). Thus, each list has a single purpose within their respective pages and then would probably be seen by an user as a single segment. In this way, the DOM subtree of these regions is represented by a single node in our DOM tree representation, in order that all textual content of the subtree with recurrent structure become directly linked to this node. This process is depicted in Figure 5.
Identifying recurrent structures in DOM trees is a procedure commonly executed in methods for extracting data from HTML pages [25, 34]. The motivation is that the information to be extracted is often placed in a particular order, and repetitive patterns can be found in these Web pages when multiple records are aligned together.

Figure 5: Recurrent substructures of the DOM trees are not represented in our DOM tree representation.
4.2 Phase 2: Constructing the SOMtree
The second phase of the Algorithm 1 creates an auxiliary hierarchical structure named SOMtree (SOM is an acronym to Site Object Model ) that summarizes the DOM trees of all pages found in a Web site (see Lines 5­18).
Each node Ns of the SOMtree contains the same information found in a node N of our DOM tree representation, but has two extra fields: Ns.counter, with the number of pages where it occurs in the site (i.e., the number of pages that contain a node N such that N.label = Ns.label); and Ns.pageList, with the list of pages where it occurs associated to textual content in the site. Notice that the size of Ns.pageList may be smaller than the value of Ns.counter, since only tag occurrences associated with textual content are inserted in Ns.pageList. A single tag may occur in a page associated to content and appear in a second page without any content.
As an example of a SOMtree construction, consider a very simple Web site S formed by only two pages, 1 and 2, depicted in Figure 6. A SOMtree for this site can be constructed by merging 1 and 2 in a single structure. Phase 2 of our algorithm starts by reading page 1, through a preorder traversal (Lines 8­17). For each node N of 1, the algorithm checks if the SOMtree already have a node Ns such that Ns.label = N.label (Line 9). However, since the SOMtree is created empty (Line 6), all nodes N found in this first page are merged into SOMtree through the function insertN ewN ode, that is called on Line 15, and detailed in Algorithm 2. The insertion point is guided by the value of N.label. Function insertN ewN ode assigns the value of N.label to Ns.label, and assigns 1 to Ns.counter. If N has textual content (N.f lag = true), then function insertN ewN ode also adds the id of page 1 (value 1) to the pageList of the node Ns.
After that, the algorithm reads the second page of S, 2, to merge it into SOMtree. In Figure 6, we can see that this page starts with the tag <html>. In the Line 9, it is verified that the SOMtree has already a node Ns with the label html, and thus the algorithm only updates the information of Ns (Lines 10­12): its counter value is incremented.
In Figure 6, we see that the representations of 1 and 2 diverge below the nodes div on right. Below this node, page 1 has the nodes h1 and pre, and page 2 has the nodes h1 and p. As the nodes h1 have the same path in the DOM tree of both pages, then they result in a single node in the SOMtree. On the other hand, the nodes pre (in 1) and p (in 2) occur only once in their respective pages. Thus, each of them results in a different node in the SOMtree. The counter of these two nodes is 1 in both cases.
4.3 Phase 3: Refining the SOMtree
The third phase of our algorithm has the goal of removing nodes of the SOMtree that would be considered by a human

219

Figure 6: An example of a SOMtree construction. The pageList values are depicted below each node of this structure.
Algorithm 2 insertNewNode(SOMtree, N, i)
1: Create an empty node Ns; 2: Ns.label = N.label; 3: Ns.counter = 1; 4: if N.f lag then 5: Ns.pageList = {i}; 6: end if 7: Insert Ns in SOMtree according to Ns.label

The distance adopted is the difference in the depth between the two nested nodes analyzed. We noticed that as this distance increases, also increases the chance that nodes represent different segment classes of the site. On the other hand, when this distance is small, it is mostly due to small variations in pages, and thus the nodes probably represent the same segment class.
The ideal value for this distance, represented by the threshold  in Algorithm 1, can be empirically set through experiments. An alternative to avoid the necessity of such threshold is to use a clustering algorithm to group the pages of the site according to their templates [11, 12, 30]. After clustering the pages, we can run the algorithm 1 for each cluster. This procedure would avoid the presence of pages with large differences in their structure as input to the segmentation algorithm, and then would allow all nested content to be joined, without a risk of joining segments of pages with distinct structure. On the other hand, the use of clustering would make the segmentation process more expensive and the results we obtained by using our heuristic were good enough for avoiding such extra computational cost.
As an example, consider the SOMtree segment depicted in Figure 7(a). Notice that this segment contains two nodes, div and p, and that the p node is descendant of the div node. As one can see, there is a small variation in the structure in this region of the pages, since a set of pages contains textual content connected to the first node (div), and another set of pages contain textual content associated to the second node p.

as internal parts of one or more segments of the site. By removing these nodes, that we refer to as noisy nodes, the SOMtree becomes a structure formed only by nodes that belong to the label (root-to-segment path) of one or more segments, and each each leaf node of this new structure will refer to a distinct segment class of the site.
To better illustrate what is a noisy node, let us consider that there is a table inside the body of a news page. This table would probably be considered by a human as part of a single segment containing the body of the news. However, when including this page in the SOMtree, the content of the table would create new nodes in the structure, and thus would let the segmentation system to consider the table as a separate segment.
When analyzing the results produced by Phase 2 of our segmentation algorithm for several samples, we realized that in general, these noisy segments occur when a given set of pages has a region with pure textual content, while another set of pages has textual content mixed with other type of information in the same region, such as a table containing text, or a different textual style. When humans see such variations, they can easily realize that the region has the same function in both sets of pages. However, the small structural differences introduced affect the representation of the site in the SOMtree. The main goal of the third phase of our algorithm is to remove the noisy nodes, what is done by applying two different heuristics.
The first heuristic performs a join of nested nodes that have textual content and that occur close in the SOMtree.

Figure 7: (a) A SOMtree segment example and (b) its pruned version, considering a  threshold equal to 6. Notice that, instead of presenting the list of documents in each leaf element node, this figure presents only the number of documents in each list.
Line 23 of the algorithm 1 verifies whether the distance between these two nodes is smaller than the threshold  or not. If it is smaller, then the pageList of the descendant node is appended to the ancestor, and the descendant node is deleted from the SOMtree(Lines 24­25). On the other side, if this distance is greater than the threshold , then we assume that these nodes refer to distinct segment classes. In our experiments, we empirically set the value 6 for this threshold based on training experiments. Considering this value for the threshold , the node p depicted in Figure 7(a) must be pruned during the refining process, and its pageList merged to the pageList of the node div. The result of this process is depicted in Figure 7(b).
Notice that, although the join can fail in some cases, it does not cause a significant change in the final result of the method, since if we set threshold  too high, the only change is that a few segments with different classes may be joined in a single class. On the other hand, if we set it too low, some small differences in the structure of pages may cause the split of segments considered by humans as single segments. While these changes may contribute to result in a set of segment classes different from the ones produced by humans,

220

the impact of these changes for ranking purposes is expected to be small for Web sites with regular structure, since only a few pages would have a few changes in our proposed term weighting scheme.
The second heuristic joins all the children of a node N to it whenever all of them have counter value smaller than a threshold  (Lines 30­38). This parameter is related to the minimum quantity of elements in a segment class in order to allow safe use of the segment aware ranking method we adopt in the experiments. This parameter is proposed and studied by Moura et al [13], where authors suggest to set it as 8.
Notice that the children of a node Ns can be joined to their parent only when all of them have counter value smaller than the threshold . The purpose of this rule is to avoid the generation, during this stage of the refining process, of new nested nodes with textual content. Thus, even after this second phase of the refining process, some nodes might remain with counter value smaller than the threshold .
The result of the algorithm after the refining is a set of segment classes that are described by the SOMtree. Each leaf node N of the final SOMtree represents a segment class, with label N.label and occurring in pages N.pageList. From the structure of the SOMtree it is possible to define how each page is partitioned into segments by performing a matching between the SOMtree and the DOM tree of the page.
5. EXPERIMENTS
To validate our automatic segmentation method, we performed experiments on 4 real Web collections namely IG, CNN, CNET, and BLOGs. The results for these experiments are discussed in this section.
5.1 Data Sets
The IG collection, which contains 34,460 pages crawled from one of the largest Brazilian Web portals (see www.ig.com.br), is composed of a recipe site, a forum site, and a news Web site, as detailed in Table 1. The set of test queries used in ranking experiments for this collection is composed of 50 popular queries extracted from a log of queries submitted to the IG Web search service. Relevance assessments for these queries were made by 85 volunteers from 5 different Brazilian universities. A pooling method [17] was used to collect relevance judgments. For each of the 50 queries, we composed a query pool formed by the union of the top 20 documents retrieved by each ranking method considered in our experiments, and then presented those documents in random order to the users. To avoid noise and erroneous judgements, each document retrieved by a given query was evaluated by three distinct volunteers. We considered a document as relevant to a query when at least two of the volunteers considered it as relevant to that query.
The second collection is a crawling of the CNN Web site composed of 16,257 Web pages. The queries used for experimentation with this collection were proposed by 50 volunteers, such that each volunteer wrote a single query. We then inspect each one of the proposed queries to ensure that all of them are representative. As in the IG collection, for each query submitted we computed a pool formed by the union of the top 20 results of all the methods we considered.
The third collection was obtained by crawling four Web sites affiliated to the CNET Web portal (see www.cnet.com): CNET News, that provides news, blogs, and special reports

about technology; CNET Download, composed by a large set of pages containing free downloads; CNET Shopper, which is a virtual shop of tech products; and CNET Reviews, composed by a set of pages containing reviews of products. This collection contains a total of 352,770 Web pages. The 50 test queries were obtained in similar way as described for the CNN collection.
The fourth collection was obtained by crawling 9 popular blogs from the top popular list presented in Technorati Blog 1. Table 1 presents the list of the crawled blogs. This collection contains a total of 54,055 blog posts and 50 test queries, which were generated as in the CNN collection.
5.2 Evaluation Metrics and Baselines
To evaluate the results achieved by our segmentation method, we adopted the Adjusted RAND index (AdjRAND) metric [18, 21, 32]. The Adjusted RAND index between two partitions measures the fraction of pairs of terms that are either grouped together (in a same segment) or placed in separate segments in both partitions. Hence, the higher the Adjusted RAND index between the segmentation output (obtained by the algorithm we want evaluate) and a manually labeled segmentation, the better the segmentation quality of the algorithm.
We also have performed experiments with a second measure, named as Average Mutual Information [29]. Mutual information is a measure of the quantity of information shared between two partitions, and provides an indication of the similarity between the segmentation output of an automatic method and the manual segmentation of a same page. However the conclusions obtained when using this second measure were similar to the ones obtained with AdjRAND and we decided to not include them due to space restrictions.
To conduce this evaluation, we manually segmented the pages of the experimented collections. First, we clustered the pages of each collection according to their internal structure. We asked volunteers to mark one example for each distinct structure of pages they found in the web site, and then clustered the pages based on their structural distance from each structure pointed out by the users.
In the second step, we used the VIPS [6] algorithm to segment the pages of each cluster through a manual selection of parameters. Notice that a single set of parameters can be used to segment all pages of a same cluster, given that these pages have the same internal strucuture. While this fact allows us to perform a human segmentation to produce the reference sets, still the number of structures to be found is high. Further, to assert the quality of the segmentation, we selected three volunteers to analyze the quality of segmentation performed by VIPS. For each distinct structure found in the web sites segmented, we selected one page to be analyzed by a volunteer. Whenever the volunteer did not agree with the segmentation found in a page p, he was able to adapt the segmentation to its perception about how the page p could be segmented. In these cases, we then changed the segmentation of all pages with the same structure of p. Given the necessary manual intervention in the process, we named this approach as manual segmentation.
We performed experiments to compare the quality of the segments found by our automatic segmentation algorithm with those found by the algorithm proposed by Kohlschu¨tter and Nejdl [21], referred to as Rule-based Blockfusion in
1http://technorati.com/blogs/top100

221

the experiments. The algorithm identifies the segments of Web pages from a textual perspective. It uses the number of tokens in a text fragment (or more precisely, its token density) to segment Web pages (instead of DOM-structural information, as in our method). The segmentation algorithm is based on a merge strategy called Block-Fusion, where adjacent text fragments of similar text density (interpreted as "similar class") are iteratively fused until the blocks' densities (and therefore the text classes) are distinctive enough. Using various settings, including a rule-based approach, authors show that the resulting block structure closely resembles a manual segmentation, achieving a segmentation performance better than those achieved by [10], which is another recent work that approached the Web page segmentation problem.
We also studied the impact of using our automatic segmentation method as input to a segment aware ranking system proposed by de Moura et al. [13]. This method changes the weight of terms in BM25 ranking model [26] according to the segments where they occur with the goal of improving the search quality. We compared the ranking results obtained when segmenting the Web pages with our method to the results obtained when manually segmenting the pages.
We also report search results obtained when using the BM25 ranking applied to full pages (using no segment information), and the BM25 ranking applied to pages after templates removal. Templates were manually removed to assure a high quality template removal, thus avoiding doubts which could arise when using an automatic method. This last baseline was included to show that using segment information is better than removing templates in search tasks.
To compare the ranking strategies, we adopted two evaluation metrics. The first metric is the traditional mean average precision (MAP) and the second is precision at 10 (P@10), which measures the amount of relevant documents in the top 10 answers provided on each system [2].
5.3 Experimental Results
Table 1 presents the total number of segments and segment classes found by the automatic segmentation approach. The Table also shows the number of segments found in classes with less than  elements. We can see that, despite the number of pages in CNN collection be smaller than the other collections, the number of segment classes found in this collection is relatively high, if compared with those found in other collections. This happens because segments with the same role in the CNN collection may have two or more versions of labels (root-to-segment path), i.e., it is possible to exist two or more segment classes formed by segments with the same function in such collection. For instance, news pages found in the CNN collection usually contain tables, graphics, photos and other illustrative components in different parts of their text. Such characteristic of the CNN collection does not significantly interfere in the results obtained by a segment aware Web search results, since each segment class continues to be formed by segments with the same function.
On the other hand, notice that the number of segment classes found in BLOGS collection is quite small, when compared with those found in other collections. This happens because such collection is formed only by posts of their respective blogs, and the posts of a same blog tend to have the same structure, i.e., they tend to have the same set of

segments. Thus, since there is not a big variety of segments from post to post, the number of segment classes found in each blog of this collection tend to be quite small.
An important point to observe in Table 1 is that in all collections just a few segments were grouped into classes with less than  (i.e., 8) elements. For instance, from the total of 1, 404, 512 segments found in IG, only 1, 109 were grouped into classes with less than  elements, which is less than 0.08% of the total number of segments found. In CNN the number is less than 0.03%, in CNET less than 0.3%, and in BLOGS it is less than 0.05% of the segments found.

adjRand

Adjusted Rand Index: IG COLLECTION

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

SOM Tree Rule-based Blockfusion 5 10 15 20 25 30 35

1000 Pages

adjRand

Adjusted Rand Index: CNN COLLECTION

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

SOM Tree Rule-based Blockfusion 2 4 6 8 10 12 14 16 18

1000 Pages

adjRand

Adjusted Rand Index: BLOGS COLLECTION

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

SOM Tree Rule-based Blockfusion 10 20 30 40 50 60

1000 Pages

adjRand

Adjusted Rand Index: CNET COLLECTION

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

SOM Tree Rule-based Blockfusion 50 100 150 200 250 300 350 400

1000 Pages

Figure 8: Adjusted RAND Index graphs found for IG, CNN, BLOGs and CNET collections. Due to space constraints, the values in x-axis were divided by 1000.

The results for Adjusted RAND Index is depicted on Figure 8. To compose each curve of the Figure (for instance, the curve SOM Tree in IG collection graph), we computed the Adjusted RAND index achieved by the method of the curve for each page of the collection, and then plotted these values in increasing order. For instance, considering the graphs for IG collection, the kth page of the curve SOM Tree refers to a page segmented by our automatic segmentation method that achieved the kth smaller Adjusted RAND index.
As one can see, these graphs show that our method achieved a segmentation performance better than those achieved by Rule-based Blockfusion [21] in all collections. The graphics show that our method outperformed the baseline at all intermediate points, being closer to 1 in all levels. Observing the graphs, we notice that the CNET collection presented the smaller agreement between the manual and the automatic approaches. When checking the results, we realized that this is due to irregularities in the page structures found in CNET which affects the automatic identification of recurrent structures (see Section 4.1). Such irregularities do not affect the manual method since a human can easily deal with them. However, even for this collection, our automatic approach presented a segmentation performance better than Rule-based Blockfusion.
To better illustrate the results, in IG collection we have just a few pages with AdjRAND below 0.6, while almost

222

Site (Domain)
IG - Total News (ultimosegundo.com.br) Forum (jornaldedebates.com.br) Recipe (panelinha.com.br)
CNN - Total News site (cnn.com)
CNET - Total News (news.com) Downloads (downloads.com) Reviews (reviews.cnet.com) Shopper (shopper.com)
BLOGs - Total B. Boing (boingboing.net) CNET (news.cnet.comtech-blogs) Engadget (engadget.com) Gizmodo (us.gizmodo.com) Google (googleblog.blogspot.com) Hacker (lifehacker.com) Mashable (mashable.com) S. Film (slashfilm.com) T. Crunch (techcrunch.com)

Pages
34,460 26,466
6,389 1,605
16,257 16,257
352,770 131,474
99,186 64,142 57,968
54,055 14,173
8,054 6,343 4,454 1,050 3,997 7,410 5,376 3,198

Classes
1,095 797 256 42
1,945 1,945
7,587 2,155
916 1,563 2,953
1,383 59
194 69 77 14 54
500 213 203

Segments
1,404,512 1,017,960
366,202 20,350
597,301 597,301
25,458,565 6,287,577 3,182,811 6,320,501 9,667,676
2,149,039 421,074 415,108 282,681 119,728 14,503 93,046 560,079 145,151 97,669

S. Small Classes
1,109 887 212 10
1,911 1,911
7,678 5,568
800 620 690
1,029 9
85 16 37
0 56 367 283 176

Table 1: The set of Web sites crawled for each document collection, and the number of segments and segment classes found by the automatic segmentation process. Table also shows the number of segments found in classes with less than  elements (S. Small Classes).

10,000 are below this threshold when using Rule-based Blockfusion. In CNN our method produces less than 4,000 pages with AdjRAND below 0.8, while the baseline produces more than 8,000 pages below this threshold. When taking the average AdjRAND values obtained by each method we can see that our method obtained an average value of 0.83 when processing pages of IG, while the Rule-based Blockfusion achieved an average value of 0.72. In CNN the average AdjRAND value obtained by our method was 0.85, while Rule-based Blockfusion achieved 0.58.
We also performed some experiments to check whether these results are uniform for Web sites with a small number of pages. For this, we randomly divided the IG collection into subsets containing 1%, 5%, and 10% of the original collection, and then used our method to segment the pages of each subset. Using the AdjRAND metric, we compared the segmentation of each page of the subsets to the segmentation obtained for these pages when processing the whole collection. When checking the results, we realized that the segmentation output obtained for each subset was similar to that obtained for the whole collection (for instance, the average AdjRAND value achieved by the subset containing 1% of the pages of IG was 0.98), which lead us to conclude that our method is able to obtain good segmentation quality even for small Web sites.
Table 2 presents the results obtained when using the segment aware model proposed by de Moura et al. [13] with our automatic segmentation method, combination referred to as AutS, and with the manual segmentation approach, combination referred to as ManS. Notice that we do not perform ranking experiments with Rule-based Blockfusion because the search model adopted requires information about segment classes that is not provided by the Rule-based Blockfusion method. The Table also shows the ranking quality obtained when using BM25 ranking strategy [26] with no block information, and when using BM25 ranking applied to pages after templates removal. This last baseline method

is referred to as BM25NT, which means BM25 without templates.
The first important observation is that the segment aware ranking method yields improved results relative to the BM25 and BM25NT baselines, for both segmentation methods. Further, the results obtained when using our segmentation algorithm are quite close to the ones obtained when using the manual segmentation. When applying the Wilcoxon statistical test with a 95% confidence level, we notice that the differences between our method and ManS are not significant in most cases, except for all metrics in CNET and MAP in IG. The differences in the scores obtained by the two methods are not high, being smaller than 1.3% in most cases. Further, the fully automatic method has the advantage of not requiring manual intervention at any point of the segmentation process.

Metric BM25 BM25NT ManS AutS
Metric BM25 BM25NT ManS AutS

IG P@10 MAP 0.594 0.621 0.571 0.649 0.667 0.749 0.659 0.733
BLOGs P@10 MAP 0.584 0.644 0.594 0.632 0.604 0.678 0.602 0.677

CNN P@10 MAP 0.612 0.691 0.602 0.704 0.642 0.786 0.630 0.779
CNET P@10 MAP 0.476 0.458 0.534 0.481 0.552 0.498 0.512 0.470

Table 2: Ranking quality obtained in four collections when using the BM25 formula, the BM25 without no templates (BM25NT), and the segment aware model applied to manual segmentation (ManS) and to our automatic segmentation (AutS).

6. CONCLUSIONS
In this paper we proposed and evaluated a site oriented segmentation algorithm for Web pages. The method com-

223

bines the DOM trees of all pages in a site into a single data structure which is called SOM tree. By inserting the individual pages into the SOM Tree, the algorithm performs a clustering-like procedure. By investigating regularities among the pages associated with the leaves of the SOM tree, the algorithm naturally breaks them into segments.
The experiments performed with our segmentation approach indicate it produces quite competitive segmentation results, which opens an opportunity to the use of the method in several Web site segmentation applications.
We show that the method is particularly useful to produce input to a previously proposed segment aware ranking method, since it not only segments the Web pages, but also is able to cluster them into classes. Further, the results obtained by our method in segment aware Web search are close to the ones obtained when using a segmentation approach based on manual intervention.
As future work we plan to study the impact of using our method in large scale Web search systems, where there are usually just a few sample pages of each Web site in the collection. Also, we intend to investigate the application of our segmentation method to mobile applications, including its use in alternative ways of presenting Web search results in mobile devices.
7. REFERENCES
[1] K. Ahnizeret, D. Fernandes, J. M. Cavalcanti, E. S. de Moura, and A. S. da Silva. Information retrieval aware web site modelling and generation. In ER '04, pages 402­419, Berlin, Heidelberg, 2004. Springer.
[2] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999.
[3] Z. Bar-Yossef and S. Rajagopalan. Template detection via data mining and its applications. In WWW '02, pages 580­591, New York, NY, USA, 2002. ACM.
[4] D. Cai, X. He, J.-R. Wen, and W.-Y. Ma. Block-level link analysis. In ACM SIGIR '04, pages 440­447, New York, NY, USA, 2004. ACM.
[5] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. Vips: a vision-based page segmentation algorithm. Microsoft Technical Report, 2003.
[6] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. Block-based web search. In ACM SIGIR '04, pages 456­463, New York, NY, USA, 2004. ACM.
[7] J. P. Callan. Passage-level evidence in document retrieval. In ACM SIGIR '94, pages 302­310, New York, NY, USA, 1994. Springer-Verlag New York, Inc.
[8] Y. Cao, Z. Niu, L. Dai, and Y. Zhao. Extraction of informative blocks from web pages. In ALPIT '08, pages 544­549, Washington, DC, USA, 2008. IEEE Computer Society.
[9] S. Ceri, M. Matera, F. Rizzo, and V. Demald´e. Designing data-intensive web applications for content accessibility using web marts. Commun. ACM, 50(4):55­61, 2007.
[10] D. Chakrabarti, R. Kumar, and K. Punera. A graph-theoretic approach to webpage segmentation. In WWW'08, pages 377­386, 2008.
[11] V. Crescenzi, P. Merialdo, and P. Missier. Fine-grain web site structure discovery. In ACM WIDM '03, pages 15­22, New York, NY, USA, 2003. ACM.
[12] V. Crescenzi, P. Merialdo, and P. Missier. Clustering web pages based on their structure. Data Knowl. Eng., 54(3):279­299, 2005.
[13] E. S. de Moura, D. Fernandes, B. Ribeiro-Neto, A. S. da Silva, and M. A. Gon¸calves. Using structural information to improve search in web collections. JASIST, 61:2503­2513, December 2010.

[14] D. Fernandes, E. S. de Moura, B. Ribeiro-Neto, A. S. da Silva, and M. A. Gon¸calves. Computing block importance for searching on web sites. In CIKM '07, pages 165­174, New York, NY, USA, 2007. ACM.
[15] D. Gibson, K. Punera, and A. Tomkins. The volume and evolution of web page templates. In WWW '05, pages 830­839, New York, NY, USA, 2005. ACM.
[16] G. Hattori, K. Hoashi, K. Matsumoto, and F. Sugaya. Robust web page segmentation for mobile terminal using content-distances and page layout information. In WWW '07, pages 361­370, New York, NY, USA, 2007. ACM.
[17] D. Hawking and N. Craswell. Overview of TREC-7 very large collection track. In Proc. of the Seventh Text Retrieval Conf., pages 91­104, 1998.
[18] L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2(1):193­218, December 1985.
[19] J. Kang and J. Choi. Detecting informative web page blocks for efficient information extraction using visual block segmentation. pages 306­310, 2007.
[20] C. Kohlschu¨tter, P. Fankhauser, and W. Nejdl. Boilerplate detection using shallow text features. In WSDM '10, pages 441­450, New York, NY, USA, 2010. ACM.
[21] C. Kohlschu¨tter and W. Nejdl. A densitometric approach to web page segmentation. In CIKM '08, pages 1173­1182, New York, NY, USA, 2008. ACM.
[22] L. Li, Y. Liu, A. Obregon, and M. Weatherston. Visual segmentation-based data record extraction from web documents. In IRI'07, pages 502­507, 2007.
[23] S.-H. Lin and J.-M. Ho. Discovering informative content blocks from web documents. In Proc. of ACM SIGKDD '02, pages 588­593, New York, NY, USA, 2002. ACM.
[24] A. K. Marco Baroni, Francis Chantree and S. Sharoff. Cleaneval: a competition for cleaning web pages. In LREC'08, Marrakech, Morocco, may 2008.
[25] D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laender. Automatic web news extraction using tree edit distance. In WWW'04, pages 502­511, 2004.
[26] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In ACM SIGIR '94, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.
[27] L. Shi, C. Niu, M. Zhou, and J. Gao. A dom tree alignment model for mining parallel data from the web. In ACL'06, pages 489­496, Morristown, NJ, USA, 2006. Association for Computational Linguistics.
[28] R. Song, H. Liu, J.-R. Wen, and W.-Y. Ma. Learning block importance models for web pages. In WWW '04, pages 203­211, New York, NY, USA, 2004. ACM.
[29] A. Strehl and J. Ghosh. Cluster ensembles -- a knowledge reuse framework for combining multiple partitions. J. Mach. Learn. Res., 3:583­617, 2003.
[30] K. Vieira, A. L. da Costa Carvalho, K. Berlt, E. S. de Moura, A. S. da Silva, and J. Freir. On finding templates on web collections. World Wide Web, pages 171­211, 2009.
[31] K. Vieira, A. S. da Silva, N. Pinto, E. S. de Moura, a. M. B. C. Jo and J. Freire. A fast and robust method for web page template detection and removal. In CIKM '06, pages 258­267, New York, NY, USA, 2006. ACM.
[32] N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In ICML '09, pages 1073­1080, New York, NY, USA, 2009. ACM.
[33] L. Yi, B. Liu, and X. Li. Eliminating noisy information in web pages for data mining. In KDD '03, pages 296­305, New York, NY, USA, 2003. ACM.
[34] Y. Zhai and B. Liu. Web data extraction based on partial tree alignment. In WWW '05, pages 76­85, 2005.
[35] Y. Zhang, A. C. Surendran, J. C. Platt, and M. Narasimhan. Learning from multi-topic web documents for contextual advertisement. In SIGKDD'08, pages 1051­1059, New York, NY, USA, 2008. ACM.

224


Temporal Latent Semantic Analysis for Collaboratively Generated Content: Preliminary Results

Yu Wang
Emory University
yu.wang@emory.edu

Eugene Agichtein
Emory University
eugene@mathcs.emory.edu

ABSTRACT
Latent semantic analysis (LSA) has been intensively studied because of its wide application to Information Retrieval and Natural Language Processing. Yet, traditional models such as LSA only examine one (current) version of the document. However, due to the recent proliferation of collaboratively generated content such as threads in online forums, Collaborative Question Answering archives, Wikipedia, and other versioned content, the document generation process is now directly observable. In this study, we explore how this additional temporal information about the document evolution could be used to enhance the identification of latent document topics. Specifically, we propose a novel hiddentopic modeling algorithm, temporal Latent Semantic Analysis (tLSA), which elegantly extends LSA to modeling document revision history using tensor decomposition. Our experiments show that tLSA outperforms LSA on word relatedness estimation using benchmark data, and explore applications of tLSA for other tasks.
Categories and Subject Descriptors
H.3.1 [Content Analysis and Indexing]: Indexing methods
General Terms
Algorithm, Experimentation
1. OVERVIEW
Mapping words to semantic concepts has been an active area of research in Information Retrieval (IR) and Natural Language Processing (NLP). Many approaches, such as LSA [3], pLSA, and LDA, have been introduced to identify semantic concepts by analyzing static documents. However, with the proliferation of Collaboratively Generated Content (CGC), such as the questions and answers posted in Yahoo Answers, blog posts and comments, as well as pages in Wikipedia, additional temporal information about the document evolution is available. This information has been shown to be helpful for fundamental IR and NLP tasks, such as ranking [1] and word similarity analysis [4]. It is then natural to ask, whether temporal document evolution information, such as revision history, can be used to enhance the identification of semantic concepts, by extending LSA to take advantage of the temporal document evolution.
We propose a new hidden-topic modeling algorithm, tLSA, to utilize the content authoring process of CGC. tLSA rep-
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

resents the collection as a tensor, with the dimensions corresponding to the documents, words, and time, respectively, and uses tensor decomposition to identify latent concepts in a process inspired by LSA. We show that incorporating temporal information in tLSA can be valuable for word similarity computation, which could in turn improve search quality over CGC. We also explore another application of tLSA, tracing the change of hidden-topic importance over time, which cannot be accomplished by static analysis.
Specifically, our contributions include: Introduction of a novel temporal latent semantic analysis method, tLSA (Section 2); Improved performance over LSA for predicting word relatedness, and an exploration of applying tLSA for temporal topic evolution analysis (Section 3).

2. TEMPORAL LATENT SEMANTIC ANA-

LYSIS WITH TENSOR DECOMPOSITION

Our tLSA algorithm extends LSA, which we briefly review, and use to provide intuition for our approach. A wordby-document matrix can be built to represent the collection of static documents. Matrix decomposition algorithms, such as Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF), can be performed onto the matrix to obtain semantic representation. By considering the generation process of the collection, we have time dimension upon the flat word-by-document matrix. Therefore, the matrix becomes a word-document-time cuboid, more precisely, an 3rd-order tensor (as shown in Figure 1). Intuitively, tensor decomposition algorithms could be applied to this 3rdorder tensor, with the goal of identifying important latent topics similar to the idea of LSA or SVD.
LSA decomposes the word-by-document matrix into the sum of rank-1 matrices, therefore captures the correlation/cooccurrence information between words. CANDECOMP/PARAFAC (CP) decomposition introduced in [2] decomposes a tensor in the same way. CP-decomposition factorizes worddocument-time tensor into the sum of weighted rank-1 tensors. Each rank-1 tensor is associated with one hidden-topic and can be rewritten as the outer product of three vectors (word, document and time vector respectively). More formally, for CP-decomposition, we have:
R

T = r × (wr  dr  tr )

(1)

r=1

T is the 3rd-order tensor, r is the r-th "singular value", wr,dr and tr are the word, document and time vector respectively. R is the number of rank-1 tensors used to recover the original tensor T . We can also construct a matrix by aggregating the vectors, for example, [t1, t2, . . . , tR]. This is illustrated in Figure 1. Tensor T is decomposed into three

1145

0.035 0.02

Figure 1: Tensor decomposition for the word-documenttime tensor describing a CGC collection like Wikipedia.

matrices and a core tensor, the core tensor  is diagonal and the non-zero entries have the value of 1, 2, . . . , R.
The original CP-decomposition could be done by iteratively solving a least square problem. However, we notice that the time dimension is different from document and word dimension. The continuity of time dimension should be considered when decomposing the tensor, while document and word dimension does not have this issue. Intuitively, two adjacent time slices in the tensor should have similar weights if we assume the weight of hidden-topic consistently changes over time. The loss function of time vector is redefined as:

Ltime(t) = T - T 2 + 1tLtT + 2 t 2

(2)

F

Basically, loss function (2) adds regularization on time dimension when solving the least square problem, resulting in
a smooth time vector. T is the recovered tensor as defined in (1). L is the Laplacian matrix which penalizes the difference between "neighborhoods" along the time vector t.

3. EXPERIMENTAL RESULTS
We use WordSimilarity-353 (WS-353) benchmark dataset1 to evaluate tLSA. To construct the tensor, we retrieve top 100 articles through Wikipedia default search engine for each word in WS-353 and 39183 articles in total are obtained. For each retrieved article, the revisions from Jan 1st, 2008 till Jan 31st, 2011 are collected. We take weekly snapshot for those articles since Jan 1st, 2008, resulting in 161 time slices. The baseline method, LSA, uses the last snapshot of this collection only. Word relatedness computed by LSA and tLSA are compared on WS-353. Pearson Correlation is employed here to estimate the performance of these models.

Algorithm LSA
tLSA with time regularization tLSA without time regularization

Pearson Correlation 0.4398
0.4593 (+4.4%) 0.3752

Table 1: Correlation with WS-353 word relatedness.
Table 1 shows that tLSA with time regularization performs better than LSA by 4.4% on word relatedness estimation. However, the words in the WS-353 dataset are relatively common, and primarily related to static concepts, such as "car" and "love". This may explain the relatively small absolute improvement of tLSA over LSA.
We now explore another application of tLSA - tracing hidden-topic evolution and importance over time (which is not possible with LSA or other static analysis methods). Furthermore, to show the applicability of tLSA, we run this algorithm on another corpus, 20-year New York Times articles. A section in this corpus is considered a "document",

1http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/

Aug. 23, 2006

Jun. 19, 2007

Figure 2: Periodic topic: A topic of weekly news asso-

ciated with words week, monday and church.

0.07

0.04

Aug. 23, 2006

Nov. 7, 2006

Jun. 19, 2007

Figure 3: Event associated topic: A topic of U.S. Senate

Election associated with words senate and control.

and all the articles of that section on a single day form a "revision" of this "document". This pseudo-document reflects the daily changing topics while these topics are connected by the theme of the section.
After the CP-decomposition, a time-by-topic matrix is obtained and the topic trend can be observed. We find two interesting patterns in the topic trend of New York Times corpus. One is periodic pattern as shown in Figure 2. The period of this topic is exact one week. The representative words for this topic are "week", "monday" and "church" etc. We can interpret this topic as weekly news, such as weekly report and weekly events. The other topic trend pattern is "event driven" topic as shown in Figure 3. Nov 7th, 2006 is the date of Unite States Senate Election and the Democrats regained control of the House. The top words for the topic in Figure 3 are "senate" and "control" etc.

4. CONCLUSION
In this study, we introduced a novel algorithm, tLSA, which extends latent semantic analysis by incorporating temporal information using tensor decomposition. The results of our preliminary experiments using tLSA for word relatedness estimation and for analyzing topic evolution in collections over time are encouraging and suggest promising directions for future work.

5. ACKNOWLEDGMENTS
This work was partially supported by the NSF grant IIS-1018321 and the Yahoo! Faculty Research Engagement Program.

6. REFERENCES
[1] A. Aji, Y. Wang, E. Agichtein, and E. Gabrilovich. Using the past to score the present: Extending term weighting models with revision history analysis. In CIKM, 2010.
[2] J. D. Carroll and J. J. Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition. Psychometrika, 35:283­319, 1970.
[3] S. Deerwester, S. T. Dumais, G. Furnas, T. Landauer, and R. Harshman. Indexing by latent semantic analysis. In JASIST, 1990.
[4] K. Radinsky, E. Agichtein, E. Gabrilovich, and S. Markovitch. Word at a time: Computing word relatedness using temporal semantic analysis. In WWW, 2011.

1146


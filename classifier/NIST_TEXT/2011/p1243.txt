A Large-scale Study of the Effect of Training Set Characteristics over Learning-to-Rank Algorithms

Evangelos Kanoulas, Stefan Savev, Pavel Metrikov, Virgil Pavlu, Javed Aslam e.kanoulas@shef.ac.uk, {savev, metpavel, vip, jaa}@ccs.neu.edu
 Information School, University of Sheffield, Sheffield, UK  College of Computer & Information Science, Northeastern University, Boston, MA, USA

ABSTRACT
In this work we describe the results of a large-scale study on the effect of the distribution of labels across the different grades of relevance in the training set on the performance of trained ranking functions. In a controlled experiment we generate a large number of training datasets wih different label distributions and employ three learning to rank algorithms over these datasets. We investigate the effect of these distributions on the accuracy of obtained ranking functions to give an insight into the manner training sets should be constructed.
Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 Information Search and Retrieval:Retrieval models
General Terms: Experimentation, Measurement, Theory
Keywords: Learning-to-Rank, Document Selection Methodologies
1. INTRODUCTION
Learning to rank has become one of the most prominent frameworks for constructing ranking functions. While much thought and research has been placed on the development of sophisticated learning to rank algorithms, relatively little research has been conducted on the construction of training datasets, nor on the effect of this on the effectiveness of learning to rank algorithms.
Aslam et al. [1] considered the problem of constructing effective training datasets with low cost by investigating a number of methods for selecting only a handful of documents to be labeled per query. A post-hoc analysis of the effectiveness versus the characteristics of the constructed datasets revealed that the ratio of relevant to non-relevant documents in the dataset was one of the most influential factors. The dataset used by Aslam et al. was based on TREC 6, 7 and 8 ad-hoc corpora. It contained 150 queries with binary labels and 22 features per query-document pair. Hence, even though this dataset allowed Aslam et al. to quantify performance as a function of the constructed training sets, the conclusions remain limited to small TREC collections
We gratefully acknowledge the support provided by the European Commission grant FP7- PEOPLE-2009-IIF-254562 and the NSF grant IIS-1017903.
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

and text-derived query features. Further, the construction methodology of the datasets did not allow Aslam et al to observe the performance of ranking functions over the entire range of relevant/non-relevant ratios.
In this work, we reconsider the effect of the distribution of "positive" and "negative" examples in the training set in a large-scale study. In a controlled experiment we construct a large number of training sets over the entire possible range of "positive" and "negative" instances and employ different learning to rank algorithms over these datasets. Understanding how the distribution of labels in the training set affects the quality of the trained ranking functions can provide a better insight into the manner researchers and practitioners should construct effective and efficient training data.
2. DATA AND METHODS
In this work we use the WEB30k dataset provided by Microsoft Research 1. This dataset comprises more than 30,000 user-issued web queries. For each query-url pair 136 widely used in the research community features are provided. Relevance judgments (labels) are on a 5-grades scale, from 0 (completely irrelevant) to 4 (perfectly relevant). In order to control the distribution of "positive" and "negative" instances in the training set and given that the total number of labeled urls per query and the distribution of labels across the 5 relevance grades varies widely across the 30K queries we only select those queries that have at least k labeled urls in each of the relevance grades. Further, we combine the two highest relevance grades into a single one, since typically labels in these grades are sparse in the collection. For k = 16 there are 655 queries that have at least 16 judged urls in each of the four grades, while for k = 8 there are 3676 such queries and so on. For each one of these datasets we randomly select 3/5 of the queries to construct the training set, 1/5 the validation and 1/5 the test set. There is no further processing of the test set. For the training and validation sets we first select a total number of judgments per query n, with n  k. In the case of 655 queries (k = 16) n  {4, 8, 16}. Then we generate as many training and validation sets as the possible distributions of the n judgments over the four grades (judgment sets). For instance, for n = 8, an example distribution would be (4,1,1,2) meaning that each query will have 4 completely irrelevant documents, 1 marginally relevant, 1 relevant and 2 highly/perfectly relevant.
Three learning-to-rank algorithms are run over the thousands of the aforementioned datasets, RankBoost [2], Lin-
1http://research.microsoft.com/en-us/projects/mslr

1243

RankBoost

ear Regression and Ranking SVM [3]. The performance of each of the algorithms is measured by nDCG at cut-off 10 (nDCG@10). We tuned the algorithms carefully by prior investigation of the effect of feature normalization and on the validation sets corresponding to each of our derived training sets. In the case of SVM we found prior to our study that feature normalization greatly affects the learning ability of SVM. We took the log of pagerank and similar static document features to put those features on the same scale as text-based (e.g. language model derived) features. Further, we applied zero-mean unit variance normalization in order to make the features scale-free. RankBoost was not sensitive to feature normalization and we ran it without transforming the features in any way.

RankingSVM

3. RESULTS
To quantify the distribution of labels over the four relevance grades we use two summaries which were found to be particularly informative about the effectiveness of learningto-rank algorithms as measured by nDCG@10, (a) the normalized cumulative gain of the judgment set, and (b) the variance over the judgment set.
Figure 1 demonstrates the interplay between the distribution of the labels over the different grades in the training sets and the three learning to rank algorithms. In Figure 1 (left column), the normalized cumulative gain of the grades of the labeled urls in the training set per query is computed. For instance, for the example distribution (4,1,1,2) above with 4 completely irrelevant documents, 1 marginally relevant etc., the cumulative gain is, 40 + 11 + 12 + 23. The larger the normalized cumulative gain the more skewed is the distribution of labeled documents towards perfectly relevant documents; the smaller it is, the more it is skewed towards completely irrelevant ones. NCG values in the middle of x axis range indicate a balanced training set regarding the "positive" and "negative" instances. As it can be observed from all three plots on the left, the optimal performance is indeed reached when there is a balance of labels in the dataset. Note that this is not a contradiction to Aslam et al. who report a dropping performance when the ratio of "positive" to "negative" instances exceeds some threshold. In that work and due to the manner the datasets were constructed one can only view the right tail of what appears a bell-shape curve in the left-hand side plots of Figure 1.
Obviously, a number of different datasets can be constructed that have a level of balance in the labels. For instance, a uniform distribution is similarly balanced to an equal distribution of labels in the two middle grades (marginally relevant and relevant) or an equal distribution of labels in the two extreme grades (completely irrelevant and perfectly relevant). To distinguish these cases of balanced datasets we also compute the variance of the grades in the datasets. For instance, for the example distribution (4,1,1,2) above, the variance would be the variance of the flattened list of relevance grades: [[0,0,0,0], [1], [2], [3,3]]. The higher the variance the farther the labels are from the middle grades, while the smaller the variance the more uniform the distribution. The effectiveness of the learning algorithms as a function of variance is shown in Figure 1 (right column). As the figure demonstrates the higher the variance the more effective the algorithms are. Hence, optimal performance is reached when instances are mostly taken from the extreme grades of relevance.

Regression

nCG over judgment set Variance over judgment set
Figure 1: RankBoost, Ranking SVM, Regression performance (nDCG@10, y-axis) as a function of the normalized cumulative gain of the training dataset per query (x-axis, left) and the variance of the labels in the training dataset per query (x-axis, right).
4. CONCLUSION
In this paper we studied the effect of relevance grades distribution in training sets for a learning to rank task. We conclude that the relevance grade distribution in the training set is an important factor for the effectiveness of learning to rank algorithms. Based on our analysis, we can give qualitative advise about the construction of learning to rank datasets: distributions with a balance between the number of documents in the extreme grades are to be favored and that the middle relevance grades play less important role than the extreme ones. ANOVA studies not presented here due to space limitations confirm and quantify our findings.
5. REFERENCES
[1] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In Proceedings of the 32nd ACM SIGIR, pages 468­475. ACM Press, July 2009.
[2] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4:933­969, December 2003.
[3] T. Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD, KDD '06, pages 217­226, New York, NY, USA, 2006. ACM.

1244


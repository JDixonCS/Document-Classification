Learning to Rank Using Query-Level Regression
Jiajin Wu, Zhihao Yang, Yuan Lin, Hongfei Lin, Zheng Ye, Kan Xu
School of Computer Science and Technology, Dalian University of Technology No. 2, LingGong Road, GanJingZi District DaLian, China 116024
wujiajin@mail.dlut.edu.cn, yangzh@dlut.edu.cn, yuanlin@mail.dlut.edu.cn, hflin@dlut.edu.cn, zye@mail.dlut.edu.cn, xukan@dlut.edu.cn

ABSTRACT
In this paper, we use query-level regression as the loss function. The regression loss function has been used in pointwise methods, however pointwise methods ignore the query boundaries and treat the data equally across queries, and thus the effectiveness is limited. We show that regression is an effective loss function for learning to rank when used in query-level. We propose a method, namely ListReg, to use neural network to model the ranking function and gradient descent for optimization. Experimental results show that ListReg significantly outperforms pointwise Regression and the state-of-the-art listwise method in most cases.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms, Experimentation, Performance.
Keywords
Learning to Rank, Loss Function, Query-Level, Regression.
1. INTRODUCTION
Learning to rank [3] methods can be categorized into the following three groups: pointwise, pairwise and listwise approaches according to their different input spaces. The listwise approaches have been paid more and more attention for that all IR evaluation measures are defined at the query level. Listwise approaches can be divided into two types: using query-level loss function and directly optimizing evaluation measures. In this paper, we propose a novel listwise approach based on the first type.
In prior work, Cao et al [1] proposed to use Top k Probability based on Plackett-Luce model [5] to represent the listwise instances. Assuming there are documents for a given query , the number of permutation elements of the model is of order of
. They used cross entropy loss to measure the difference of distribution and gradient descent as optimization algorithm proposing ListNet. Similarly, Xia et al [3] used likelihood loss based on Plackett-Luce model proposing ListMLE. Qin et al [4] proposed RankCosine using boosting algorithm to optimize cosine loss.
To train a good ranking model, a robust loss function is needed. In
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

this paper, we show how to use a robust and effective loss

function of order

to improve the retrieval performance.

2. METHODOLOGY
2.1 Query-level Regression
Lan et al. [2] proposed a query-level ranking framework, within which the query-level loss is defined. In the listwise approach, each query is represented by a group of documents and their ground truth labels, and queries are assumed to be i.i.d. random variables.

Let be the input space and be the output space, then

can be regarded as a random variable sampled from the

space

according to an unknown probability distribution.

The query-level (also named listwise) loss is defined as follows:

(1)

Here is the objective ranking function, is the current query,

and is the permutation of given according to their ground

truth relevance. In our approach, we use the query-level

regression (specifically square loss) as listwise loss function.

Regression has been used in pointwise approach, in which the loss

on a single sample

is given by

, and the

training goal is to minimize the total losses of all samples in

which the queries boundaries are obviously ignored. The

query-level regression loss for query is given as follows:

(2)

Here we assume that there are documents for query , and

the ground truth label for document is . We denote the

output for document given by ranking function is

.

2.2 The Learning Approach
The objective ranking function can be obtained by optimizing the query-level regression:

 f (x)  min f F

[ y nq
i 1 i



2
f ( xi )]

(3)

here we use neural network as model and choose linear ranking i.e.

which in practice is a s-dimension function and s is

the features number of document . We use gradient descent for

optimization and refer our method as ListReg. The gradient

of the loss function

with respect to parameter can be

calculated as follows:

(4)

where if we denote the t-th weight of and . Eq. 4 is then used in Gradient Descent for updating weight vector . Algorithm 1 shows the learning algorithm of ListReg.

1091

Table 1. Ranking accuracies on LETOR3.0. Here we use N@k to denote NDCG@k for short. * and  indicate significant
improvement to Regression and ListNet of our approach ("ListReg") respectively according to the LETOR3.0 t-test at the 0.05 level. Best result in each metric on each dataset is marked in bold. Note that ListReg outperforms Regression and ListNet in most
cases.

Dataset OHSUMED
TD2003 TD2004

Method Regression
ListNet ListReg Regression ListNet ListReg Regression ListNet ListReg

P@1 0.5965 0.6524 0.7081* 0.3200 0.4000 0.4044* 0.3600 0.3600 0.4800*

P@2 0.6006 0.6093 0.6684* 0.3000
0.3300 0.3233* 0.3400 0.3467 0.4400*

P@3 0.5768 0.6016 0.6273* 0.2600
0.2933 0.2911* 0.3333 0.3600 0.3733*

P@4 0.5605 0.5745 0.6064* 0.2450 0.2550 0.2589* 0.3200 0.3367 0.3333

P@5 0.5337 0.5502 0.5938* 0.2160 0.2520 0.2316* 0.3120 0.3067 0.3013

MAP 0.4220 0.4457 0.4376 0.2409 0.2753 0.2837* 0.2231 0.2231 0.2356*

N@1 0.4456 0.5326 0.5810* 0.3200 0.4000 0.4067* 0.3600 0.3600 0.4800*

N@2 0.4532 0.4810 0.5447* 0.3200 0.3400 0.3644* 0.3467 0.3467 0.4400*

N@3 0.4426 0.4732 0.5127* 0.3071 0.3365 0.3495* 0.3573 0.3573 0.3920*

N@4
0.4368
0.4561 0.4961* 0.3082
0.3254 0.3504*
0.3469
0.3469 0.3663*

N@5 0.4278 0.4432 0.4913* 0.2984 0.3393 0.3540* 0.3325 0.3325
0.3474

Algorithms 1 Learning Algorithms of ListReg

Input: training instances

Parameter: number of iterations , initial learning rate

Initialize parameter

Output: Neural Network model

for

to do

for

to do

Input of query to Neural Network and compute score

list

with current

Compute gradient

using Eq. 4

Update

end for

end for

The query-level loss of query

is a sum of objects'

regression losses, the time complexity is of order

. Also note

that the calculation of in ListReg is much simpler than that of

ListNet, for more detail, [1] can be referred.

3. EXPERIMENTAL RESULTS
3.1 Experiments Settings
We use three datasets published in LETOR3.0 [4] package: OHSUMED, TD2003 and TD2004 and use Regression and ListNet as baselines. We perform 5-fold cross validation experiments, where in each fold 3/5 is used for training, 1/5 for testing, and the left 1/5 for validation. Models are trained using the training set, tuned on the validation set and tested on the testing set. For evaluation purpose, we use three widely used IR evaluation measures: precision (P@k) and normalized discounted cumulative gain (NDCG@k) and mean-average precision (MAP). The initial learning rate of ListReg is set to be 1E-4 on OHSUMED and 1E-5 on TD2003 and TD2004 respectively, and the dropping rate is set to be 0.5 on all datasets in the experiment.

3.2 Experimental Results
It can be observed from Table 1 that our method ListReg significantly outperforms Regression on all metrics on all three datasets except in terms of P@5 on TD2004 (which is comparative nevertheless). The reason is that there are great diversities between different queries, but Regression is of pointwise style, whose loss function is based on document, treating different queries equally, however, ListReg is a query-level method (of listwise style), and it can capture the

characteristics of different queries. Accordingly ListNet also outperforms Regression in most cases.
It can also be observed that ListReg outperforms ListNet in most cases, especially on the top P@k and NDCG@k categories. For example, ListReg outperforms ListNet with 8.5% in terms of P@1 on OHSUMED and with 33.3% in terms of NDCG@1 on TD2004. This is quite meaningful in the scenario of real search engine where users usually care more about the top-most retrieved documents. Furthermore, the query-level regression loss function is much simpler than cross entropy, and the training time consumption for ListReg is less than ListNet.

4. CONCLUSION

In this paper, we have proposed a

complexity listwise

approach using query-level regression as the loss function and

gradient descent as the optimization algorithm. Experimental

results show that by considering regression at query-level,

ListReg significantly outperforms both the pointwise approach

Regression and the listwise approach ListNet. In the future, we

will investigate other loss functions in query-level style; we hope

to find more appropriate loss functions both in time complexity

and performance for information retrieval.

5. REFERENCES
[1] Z. Cao, T. Qin, and T. Y. Liu, et al. Learning to rank: From pairwise
approach to listwise approach. In Proc. of the ICML, pages 129-136, 2007.
[2] Y. Y. Lan, T. Y. Liu, and T. Q, et al. Query-Level Stability and
Generalization in Learning to Rank. In Proc. of the ICML, pages 512-519, 2008.
[3] T. Y. Liu. Learning to rank for information retrieval. Foundations and
Trends in Information Retrieval, 3(3): pages 225-331, 2009.
[4] T. Y. Liu, J. Xu, and T. Qin, et al. Letor: Benchmark dataset for research
on learning to rank for information retrieval. In Proc. of the Learning to Rank workshop in SIGIR, pages 3-10, 2007.
[5] R. L. Plackett. The analysis of permutations. Applied Statics, 24(2): pages
193-202, 1975.
[6] T. Qin, X. D. Zhang, and M. F. Tsai, et al. Query-level loss functions for
information retrieval. In IP & M, (44): 838-855, 2008.
[7] F. Xia, T. Y. Liu, and J. Wang, et al. Listwise Approach to Learning to
Rank ­ Theory and Algorithm. In Proc. of the ICML, pages 1192-1199, 2008.

1092


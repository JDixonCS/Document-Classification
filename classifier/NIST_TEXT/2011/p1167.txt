Towards an Indexing Method to Speed-Up Music Retrieval
Benjamin Martin, Pierre Hanna, Matthias Robine, Pascal Ferraro
Labri, University of Bordeaux 351, cours de la Libération Talence, France
firstnam .name@labri.fr

ABSTRACT
Computations in most music retrieval systems strongly depend on the size of data compared. We propose to enhance performances of a music retrieval system, namely a harmonic similarity evaluation method, by first indexing relevant parts of music pieces. The indexing algorithm represents each audio piece exclusively by its major repetition, using harmonic descriptions and string matching techniques. Evaluations are performed in the context of a state-of-the-art retrieval method, namely cover songs identification, and results highlight the success of our indexing system in keeping similar results while yielding a substantial gain in computation time.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--Indexing methods
General Terms
Algorithms, Performance, Experimentation
1. INTRODUCTION
With the growing presence of large collections of musical content, methods for facilitating efficient browsing become more and more useful. The research field of Music Information Retrieval (MIR) aims at analysing and comparing music pieces on musical criteria, such as harmony, timbre, rhythm etc. Most MIR comparison systems are based on a common scheme to estimate similarity on audio material. One or several audio features are chosen and computed on signals segmented in small parts, or frames, thus providing feature sequences representing pieces. For instance, harmonic information is often used since it provides a good insight of the architecture of music pieces. Then, adapted algorithms, such as dynamic time warping or alignment techniques, are used to yield similarity scores between sequences. However, algorithms involved in such retrieval systems highly depend on the length of representative sequences. Harmonic sequences, for instance, are usually relatively long and induce a high computational cost to be compared. Hence, many works over the last decades focused on the detection of the most salient parts in musical sequences in order to speed-up music retrieval and analysis (see for instance [6]).
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

In this study, we propose to reduce the size of musical sequences by selecting relevant parts in order to index data involved in retrieval tasks. Our indexing method uses harmonic representations analysed by string matching techniques to isolate specific parts. Then, our indexing process is evaluated in the context of harmonic similarity from a data reduction step preliminary to cover song identification.
2. INDEXING PROCESS
Audio pieces are represented by sequences using Harmonic Pitch Class Profiles (HPCP) [1], frequently used to describe harmonic information. The aim of the indexing step is to extract a specific part in each audio piece, namely the major repetition, that can roughly be described as the longest repeated part perceived in a piece. In western popular music, for instance, this repetition classically consists in the concatenation of a verse and a chorus. The motivation of such a selection is to reduce the audio representation to a summarized, less redundant version, that remains relevant to characterize properly the music piece. To isolate relevant parts, each harmonic sequence is analysed using string matching techniques by considering each HPCP feature as a symbol. We consider the polynomial algorithm proposed by Kannan et al. [2], that mainly relies on a local alignment algorithm [5] to detect approximate repetitions. In order to evaluate the alignment score of approximate repetitions, the latter technique requires scores for substituting each feature by another one and gap penalties. These scores are determined according to the Optimal Transposition Index (OTI) method [4], which ensures a higher robustness to musical variations. The Pearson correlation coefficient is used as a similarity measure for OTI evaluations. For a detailed description of the indexing process, please refer to [3].
3. HARMONIC SIMILARITY
The above process is evaluated as an indexing step for music retrieval, more specifically on harmonic content. One way to evaluate harmonic similarity retrieval systems is to identify cover songs, i.e., every new recording, performance, or rendition of a previously recorded musical piece. In cover songs datasets, music pieces are divided into several classes, each one containing all the different covers of a track. Harmonic baselines of cover songs of a same piece are likely to remain very similar, even in presence of strong musical variations. For our experiments, we focus on a robust cover song identification method, proposed by Serra` et al. [4], and we compare its performances with and without the major repetition indexing step.

1167

Precisio n

100%

80%

60%

40%

20%

0%

A

B

C

D

E

F

G

Figure 1: Distribution of cover identification results. Y-axis shows precision rates. Letters on X-axis correspond to different cover classes. Grey: retrieval without indexing; black: indexing by the major repetition; white: from left to right, naive indexing processes I1, I2, I3.

None MR I1 I2 I3

Results (%)

78.7 74.4 57.5 57.1 59

Retrieval (min) 324 41 41 41 41

Indexing (min) -

23 -

--

Table 1: Mean precision and computing durations for the retrieval system with each indexing method. MR stands for indexing by the major repetition.

We consider datasets chosen from personal music collections that incorporate 7 classes A to G containing each between 14 and 41 different covers songs (average of 32) of an original music piece, along with a "confusing songs" dataset incorporating 1000 songs of the same genres and artists. Each class was constituted as widely as possible, containing for instance an a capella performance, an orchestral version or an electronic remix of the same popular song, Yesterday from The Beatles.
Our method is compared to three naive indexing methods I1, I2 and I3. We denote by m the mean length of the indexed strings. I1 consists in indexing each sequence by its m first elements, I2 in indexing each sequence by its m first elements, where m denotes the length of the indexed string in the corresponding piece, and I3 in indexing each sequence by its m consecutive elements from the central position in the piece. Note that these three indexing methods as well as our method reduce data size with the same rate. On the evaluation database, the indexing algorithm reduces the size of sequences by k = 2.96 on average on every cover classes. Since the local alignment techniques involved in [4] require |u| × |v| operations to compare two strings u and v, the overall number of operations performed by local alignment computations in cover song identifications is divided by k2, i.e., about 8.58 in our case. Tab. 1 summarizes the durations elapsed for each task. In our experimental framework, cover identification lasts 324 minutes without any indexing step, and 41 minutes with our indexing step pre-computed (23 minutes for the pre-computation).
For each cover class, the recall and precision rates of the retrieval process within the first N retrieved songs are computed, where N denotes the number of covers for the current class. Note that in this particular context, precision and recall values are identical. Overall results, presented in Tab. 1 show that the retrieval system correctly identifies cover songs with an average precision of 74.4% with our indexing step and 78.7% without. Hence, the deletion of information seems to slightly affect the accuracy of the system, which highlights a limit of the indexing step pro-

posed. However, naive indexing methods having the same data reduction rate yield 15.4% to 17.3% significantly lower results, which confirms the relevance, for this application, of the parts identified by our method. Fig. 1 presents the distribution of precision scores over 7 cover classes. For each class, all the naive indexing methods (white bars) provide results 5% to 25% worse than the major repetition indexing method (black bars). For instance, the C cover class (38 cover songs) points out close values with our indexing system (88%) and without it (93%), although naive methods yield much poorer scores (63%, 56% and 63% resp. for I1, I2 and I3). This distribution is similar for every classes, with highest precisions for the non-indexed retrieval and with our process applied and significantly lower precisions with naive indexing methods.
4. CONCLUSION
The proposed method allows isolating in music pieces relevant parts, namely major repetitions, using harmonic features combined to string matching techniques. Indexing songs with these specific part in a music retrieval system based on harmonic similarity was tested in a cover song identification task. Results show that the application of our indexing method as a preliminary step to the harmonic similarity evaluation produces significant results, close to the non-indexed approach and much better than naive indexing methods, which highlights the relevance of the algorithm for indexing music retrieval systems. Experiments additionally show a significant gain in computing time, which gives the indexing step a substantial interest for computation issues, above all on large audio databases.
5. REFERENCES
[1] E. G´omez. Tonal Description of Music Audio Signals. PhD thesis, Univ. Pompeu Fabra, pp 63­100, 2006.
[2] S. Kannan and E.W. Myers. An algorithm for locating non-overlapping regions of maximum alignment score. Proc. of CPM '93, pp 74­86, London, UK, 1993.
[3] B. Martin, P. Hanna, M. Robine, and P. Ferraro. Indexing musical pieces using their major repetition. Proc. of JCDL, 2011.
[4] J. Serr`a, E. G´omez, P. Herrera, and X. Serra. Chroma binary similarity and local alignment applied to cover song identification. IEEE Trans. on Audio, Speech and Language Processing, v 16, pp 1138­1151, 2008.
[5] T. F. Smith and M. S. Waterman. Identification of common molecular subsequences. Journal of molecular biology, v 147, pp 195­197, 1981.
[6] A. Uitdenbogerd and J. Zobel. Matching techniques for large music databases. Proc. of ACM International Multimedia Conference, pp 57­66, 1999.

1168


Measuring Improvement in User Search Performance Resulting from Optimal Search Tips

Neema Moraveji
Stanford University Stanford, CA 94305
neema@moraveji.org

Daniel Russell
Google Mountain View, CA 94043
drussell@google.com

Jacob Bien
Stanford University Stanford, CA 94305
jbien@stanford.edu

David Mease
Google Mountain View, CA
dmease@google.com

ABSTRACT
Web search performance can be improved by either improving the search engine itself or by educating the user to search more efficiently. There is a large amount of literature describing techniques for measuring the former; whereas, improvements resulting from the latter are more difficult to quantify. In this paper we demonstrate an experimental methodology that proves to successfully quantify improvements from user education. The user education in our study is realized in the form of tactical search feature tips that expand user awareness of task-relevant tools and features of the search application. Initially, these tips are presented in an idealized situation: each tip is shown at the same time as the study participants are given a task that is constructed to benefit from the specific tip. However, we also present a follow-up study roughly one week later in which the search tips are no longer presented but the study participants who previously were shown search tips still demonstrate improved search efficiency compared to the control group. This research has implications for search user interface designers and the study of information retrieval systems.
Categories and Subject Descriptors
H.3.3 [Information storage and retrieval]: Information Search and Retrieval ­ query formulation, search process, G.3 [Mathematics of computing]: Probability and statistics ­ experimental design.
General Terms
Measurement, Design, Experimentation, Human Factors
Keywords
User studies, search interface, experimental design, effectiveness measures, query reformulation, expertise, tactics, tips, suggestions, assistance, efficiency.
1. INTRODUCTION AND MOTIVATION
Research on improving search performance often focuses on the algorithm, input modality, and visualization improvements. However, substantial improvements may also come from research on improving searcher expertise.
Other researchers share this vision [6, 13, 30], and one approach has been to improve pedagogical methods for search instruction
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'11, July 24­28, 2011, Beijing, China. Copyright 2011 ACM 978-1-4503-0757-4/11/07...$10.00.

[11, 23] and opportunities for modeling expertise of others [28, 31]. Our study tests the idea that educating searchers can produce a large and valuable improvement in search behavior [14].
Search engines have grown increasingly sophisticated and have evolved past the traditional query/response paradigm to include scores of tools and options for specifying queries, targeting certain corpora, visualizing results, browsing result sets, and so on. These tools are often hidden but have the potential to offer tremendous efficiency gains to users who know how to use them appropriately.
Thus, beyond term selection and query formulation, search actions include search techniques, corpus selection, features, and strategies used to solve information needs. We refer to these as search tactics. A tactic is a low-level plan to achieve a search goal, where the goal is simply to solve the present information need [1].
We believe that, in addition to suggesting relevant documents or query reformulations [24], there is high latent value in making relevant search tactics more salient to users. We illustrate this with two studies: first, a study which demonstrates that users can apply search tactics effectively to improve their search performance, and second a study which demonstrates that some improvements are retained when the tactical tips are removed and roughly one week passes. The results of the study are followed by a discussion of implications for designers and researchers of IR systems and expertise.
2. RELATED WORK
Jansen [16] uses the phrase `automated assistance' in IR systems to refer to specific actions taken by the system to improve search behavior according to some metric. However, we have found in our review that there are multiple, ambiguous and overlapping definitions of automated assistance, including strategies that are presented, tactical advice, feedback on search performance [35], and hints. Jansen and McNeese's taxonomy [17] can be expanded to encompass alternative suggestions. For our purposes, we classify automated assistance into three categories: suggestions for query expansion, search strategies, and search tactics instruction.
2.1 Suggestions for Query Expansion
Suggestions for query expansion are the most popular class of suggestions. Mizzaro [27] provides a review of systems with different automated assistance, all of which aim to influence the user to reformulate or refine his/her query. These include thesaurus systems, automatic reformulation based on semantic analysis, translating natural language queries, and dialog systems to disambiguate queries.
Query suggestions to improve the likelihood of success are seen as most useful in exploratory search for beginning searchers,

355

especially on topics for which they have little knowledge or familiarity [18, 20, 22].
Comparing two techniques that ultimately aim to influence query formulation, Belkin, et al. investigated term relevance feedback (meant for manual query reformulation) and Local Context Analysis (meant for automating reformulation), finding that the automatic LCA resulted in fewer user-defined queries, but with no difference in performance for locating document instances within a closed set [3]. Kelly, et al. [20] found that specific query suggestions are preferred by searchers over term relevance feedback.
Kelly, et al. [19] as well as Capra and Pérez-Quiñones [10] studied how the social dimension of such suggestions can influence their usage, drawing upon theories of social influence. The effect on task efficiency was not measured. Showing information about the popularity and quality of a query suggestion was shown to influence usage, with quality being more important than popularity [19].
The suggestions featured in Brajnik, et al.'s [7] FIRE system are primarily aids for query expansion (despite being referred to as strategy suggestions by the authors). They define two classes of such suggestions: "hints" (giving feedback to the user about how the system is interpreting his/her query) and "advice" (executable links to system features to aid query reformulation, such as browsing the thesaurus). In evaluations with 6 users, some found the suggestions in their system "too general" and that, though it sped up their search process, it did not change their search strategy, and participants left believing they would have followed the strategies even without any help.
Like the FIRE system, DAFFODIL [21] was created to give search strategy advice over a closed corpus of scientific literature. It was evaluated with 12 users on an open-ended task. The 16 strategies suggested by DAFFODIL included tactics such as trying alternative spellings of keywords and restricting/broadening the query. Evaluation was done with self-reported Likert assessments that indicated promise for tactical suggestions, but again, task performance was not measured.
Jansen and McNeese [17] found that automated assistance for query expansion resulted in modest performance gains in identifying relevant documents over a closed corpus (using the TREC behavior metric). They found that automated assistance was implemented in 71% of the instances where it was requested (i.e. it was not automatically shown) and that this rate did not depend on expertise.
2.2 Suggesting Strategies
Brajnik, et al. [7] summarize search strategies as "the sequence of search statements that identify, restrict, limit a set of retrieved documents to solve an information problem". Belkin, et al. [5] classify search strategies into four independent dimensions: method, goal, mode, and resource.
Kriewel and Fuhr [21] argue that an ideal search system would provide useful strategies for completing specific tasks ­ either by request or proactively helping users improve their search. The goal is to use what we know about search procedures to suggest search strategies.
The system described by Brajnik, et al. [7] attempts to infer "critical" situations (such as receiving zero results from a query) and uses a knowledge-based approach to provide strategic suggestions for searching a closed set of scientific literature. Suggestions included scanning related topics or restricting a result

set to a particular journal. Again, only 6 participants provided qualitative assessments of how well they thought the system would perform.
2.3 Suggesting Tactics
For search purposes, strategies are longer-term plans that are executed across many moves. By contrast, search tactics are much shorter sequences of moves intended to accomplish a single information goal [2]. Bates defines four types of search tactics [2]: monitoring (keeping the search on track), file structure (identifying desired file type), formulation (aiding query redesign), and term (aiding selection and revision).
In modern search engines, diverse tools are integrated and allow the user to, in effect, use different modes and mediums of search in the same session. Tactical suggestions provide immediate guidance to exploring unfamiliar corpora or visualizations without necessarily focusing on query reformulation.
Examples of modern tactical search suggestions include switching corpora from general webpages to discussion forums or images, changing the input modality from text to voice, changing the visualization of the result set from a list to a map, and changing from keyword to exact phrase terms.
The system presented by Kriewel and Fuhr [21] includes suggestions we would classify as tactical, such as visualizing a social graph of authors, and focusing the query on particular conference proceedings. While these tactics are presented to the user, the description of the system does not include any metric that measures how searcher behavior changes.
For most of the prior work on search suggestions, performance is measured by the TREC standard definition of task success: that is, the proportion of documents identified as relevant to the task, where "relevant" is determined by a group of judges [4]. This is logical because the field is primarily concerned with evaluating IR system performance whereas we are interested in improving user performance. Hence, we instead measure performance in terms of task completion time (TCT) by searchers on the live, unstructured web.
2.4 Summary and Motivation for Present Study
To be sure, query specificity and domain-appropriate term selection remain important components of search expertise. However, this focus on query expansion has neglected other aspects of search expertise, such as task-appropriate tactics based on available search engine features. This area has been understudied for multiple reasons. First, tactical suggestions are not common in closed-corpus systems because query reformulation may be sufficient. Second, IR research systems often test one search engine on one corpus instead of integrating multiple corpora, as is often the case in commercial systems that tie together web, images, video, news, blogs, and other corpora. Third, the tradition of focusing on query reformulation, rather than exploration, has acted as momentum toward features that influence them.
3. EXPERIMENT 1: CAN TACTICAL TIPS IMPROVE SEARCH PERFORMANCE?
A "tactical tip" is a short suggestion for the searcher that suggests a direct action to take to improve the likelihood of successfully solving a search problem. Such a tip does not suggest possible likely documents or query alternatives, but suggests potentially successful approaches to solve the information need. Our

356

experiment seeks to determine if an appropriate tip shown at the moment of need can measurably improve overall searcher performance.
3.1 Experimental Design and Apparatus
To determine this, a controlled user study was conducted where each participant was assigned six directed search tasks. This between-subjects study had one independent variable: availability of a task-relevant tactical tip. The dependent measure was task completion time (TCT), our primary metric for search task performance.
For each trial, the participants in the experiment group were given a search task and were asked to press a `Start' button after they had read both the task and the tip. The participants in the control group were presented only the task but not the tip. The participants performed the task in other windows or tabs and returned to the user study tab to enter their answer. The screen was then cleared and the next task was displayed.
The user interface for the study displayed the task as shown in Figure 1. For the experiment group, static screenshots illustrating how to use the tip in question were also displayed. For the control group (which saw no tips), only the light blue textbox was displayed.
Each participant varied in the amount of time they spent on the study (for a maximum of 5 minutes per task). All instructions were given textually but participants were unable to copy and paste from the question into a search box because that text was represented as an image. We asked participants to "try their best" but an honorarium was paid regardless of performance.
3.2 Experiment Task Design
The experiment was conducted on the `open Web' but with specified (directed) tasks [29]. Participants were instructed to use the Firefox browser and the Google search engine, but were permitted to enter any query and view any pages they chose.
We identified six tactical search skills, each of which is designed to be well-served by a particular search tactic: specific query syntax, a feature of the user interface, or narrowing to a particular corpus. (e.g., Skill-2: knowing how to limit searches to a particular file type) Then, for each skill we created two tasks. Tasks 1A and 1B both are more easily solved if the participant has a good grasp of Skill-1, Task 2A and Task 2B with Skill-2, etc. (The set of 6 tips is available at: http://bit.ly/TacticalSearchTips. See Table 1 for the text of the 6 tips as well as the corresponding 12 tasks.)
The tasks were designed with the following requirements: a) the task was judged to be difficult or required a great deal of effort to solve, b) answers to the task can be easily determined to be correct or incorrect and c) the task is well-specified, leaving very little room for misinterpretation.
Participants had a maximum of five minutes to complete each task. They were instructed to give up if they were not able to find (what they believed to be) the correct answer by that time. The time elapsed was shown on the task webpage.
The tasks were purposefully designed to be both difficult to complete and yet still be considered "lookup" tasks, which are characterized by Marchionini [26] as concerning fact retrieval, known item searches, navigation, verification, and question answering.
As shown in Figure 1, the question text was always shown in a light-blue box and, in the tip conditions, the relevant tip was

shown below. The tip often (but not always) included screenshots illustrating how to use the feature in question, with callouts to make its functionality immediately apparent.
3.3 Participants
The 491 paid participants (59% female, varied professional and educational backgrounds) in this study are part of a larger pool of people around the U.S. who perform search comparison tasks as part of their normal work flow, selecting tasks from a work-task queue. Participants varied widely in profession and computer expertise. Each participant was randomly assigned to either the experiment (tip shown) group or the control (no tip shown) group.
Figure 1: Task 4A shown here with its tactical tip. This is the layout of the actual instructions the study participant would see. The tip depicted here illustrates the skill searching for
videos of certain durations. We conducted this study in a remote setting for multiple reasons. Participants were able to use the browser and monitor with which they were comfortable. The participants also have a great deal of experience completing tasks of this sort, and so were not subject to strong learning or novelty effects. Because they had a substantial amount of prior practice performing similar tasks, we elected not to run practice trials before beginning the experiment.
3.4 Method
There were roughly 50% of the participants in the experiment and control groups, with around 25% of the population in each of the 4 subgroups: Control-task-set-A, Control-task-set-B, Experimenttask-set-A, Experiment-task-set-B. (The actual numbers vary slightly from group to group due to a small number of abandoned task sets by participants.) In addition to answer task questions posed, the participants were also asked to rate how successful they thought they were on each task. Each participant was asked to solve all 6 tasks within a maximum of 5 minutes for each task.

357

Task 1A
1B
2A
2B
3A
3B 4A
4B 5A 5B 6A 6B

Prompt
Imagine you live in Centreville, MD. What library within 15 miles of Centreville has the book The Person and the Situation and how far away is it (in miles)?
Imagine you live in Grand Junction, CO. What is the closest library that carries the book Arenas of Language Use and how far is it (in miles)?
There are many Microsoft Word files freely available on the web about having good standing posture. Find four (4) different ones and paste the URLs to the 4 files.
There are many Microsoft Word files freely available on the web about how to correctly cut a dog's nails. Find four (4) different ones and paste the URLs to the 4 files (any dog breed).
Which month of 2010 has had the most tweets about Demi Moore so far?
Which day in July 2010 were there most tweets about George Clooney?
Paste the URL of a video about Surat Thani, Thailand that is over 21 minutes long.
Paste the URL of a video about Nagaland, India (or people from Nagaland) that is over 50 minutes long.
What is the song that contains the lyric who are you gonna bring the bad things to?
Find a web page that has the speech that contains the phrase go in search of the true justice?
Paste 3 URLs that include the full names of people who have the name "Lebron" ... OTHER than Lebron James.
Paste 3 URLs that include the full names of people who have the name "Mariah" ... OTHER than Mariah Carey.

Search Tip You can search for books in a local library with Google and see if that book is in a local library by clicking the "Find in a library" link.
(same as 1A)
You can have Google return only some kinds of files, like Powerpoint and PDF. Use Advanced Search like below or add filetype:doc or whatever file type into the search box.
(same as 2A)
Press `More' then 'Updates' on the left side of Google search results to see how many Tweets there are that match your keywords.
(same as 3A)
You can search videos on Google and it will automatically include results from YouTube, Vimeo, and other video sites. You can then sort the list of videos by duration, quality, etc.
(same as 4A)
Putting quotes around your query will search for the exact words in the order you type them.
(same as 5A)
Use the minus sign (`-`) to tell Google to REMOVE any results that contain that word.
(same as 6A)

Table 1: Each of the search tasks used. The text of the tips is also shown here, although most tips additionally had screenshots showing the UI (e.g. Figure 1). The complete tips used are available at http://bit.ly/TacticalSearchTips

3.5 Results
Figure 2 shows task completion times for the experiment and control groups for each of the 12 tasks (the box plots were generated using the R package lattice).
In these box plots, the black dot, bottom and top of the boxes display the median, 25% and 75% quantiles, respectively. We added black lines to connect the medians for the control and experiment groups for each task.
Comparing the task completion times for the control and experiment groups in Figure 2 shows substantially faster times for the experiment group for each task. For example, the difference in median task completion time between experiment and control is more than 2 minutes for most tasks. Even for Task 5B, which has the smallest difference between groups, the median time is improved by nearly 30 seconds.
In Tasks 1A, 1B, 3A, and 3B the control group fared particularly poorly, with roughly 75% of the participants taking the maximum allowed time (5 minutes). These 4 tasks are especially difficult without the knowledge taught by the two corresponding tactical tips.

The conclusion is that this data supports our original hypothesis-- we can measure improvement in search efficiency resulting from search feature tips. Formal verification of the statistical significance of the results is presented later in this paper.
3.6 Discussion
Study participants shown tactical search tips consistently outperformed study participants who were not shown any tips for that task. The tactics present strategies and user interface features but do not suggest relevant documents, media, or answers. We believe that improvements in search performance can be achieved not only from improving retrieval algorithms but also from improving search expertise. The results of this study confirm and quantify this belief: showing searchers tactical tips at a moment just before the skill is to be used reduced search task completion time by roughly 35% for our tasks.
The substantial reduction in task completion time is worth noting. The implication is that providing users with the right tactic at the right time can improve performance dramatically. The larger challenge for IR system designers, then, is to enumerate available tactics and match them with search task behaviors.

358

Figure 2: Results of Experiment 1. The experiment group is shown a search tip and the control group is not shown a tip. The taskcompletion time is substantially faster for the experiment group than the control group for all 12 tasks.

A clear question about these results is, "Are participants actually learning to use the tactic presented in the tip, or are they blindly applying it in a rote manner as cued?" To answer this question, we carried out a follow-up study designed to measure if participants were retaining knowledge of when and how to use the tactics.
4. WEEK 2 EXPERIMENT: ARE INITIAL PERFORMANCE GAINS RETAINED?
We conducted a follow-up experiment roughly one week later to determine if the performance gains shown in the first experiment are maintained. We did this by re-testing both the control and experiment groups with the question variants that they had not seen in Week 1.
4.1 Method
To assess learning effects of the tips in question, we re-assessed the participants from Week 1 using the task variant they had not already seen. In other words, a participant who did Task 6B in Week 1 would be assigned Task 6A in this Week 2 study. This ensures that no participant would see the same task twice. (We note that 146 of the 491 participants from Experiment 1 were unable to participate, which is expected in the normal course of these studies.)
Participants were all retested between 5 and 9 days after they completed the first experiment (timing variation depending on when they decided to take part in each experiment). Participants were unaware that there would be a follow-up study, and not informed that the second set of tasks was connected with the first task set in any way.
4.2 Results
Figure 3 displays box plots for the Week 2 study alongside those from Week 1. In nearly every case (Task 5B being the only exception), the performance improvement (task completion time decrease) from seeing the tip was retained: the skill the participants had learned 5 to 9 days earlier was retained even though there was no stimulus to remind the participants. In fact, the differences between the control and experiment group are almost as large as in Week 1. Thus, the data supports the notion that we can measure retention of the skills learned from the tips. The next section validates the statistical significance of this finding for each task.

In addition to this experiment effect, we can also notice in Figure 3 an interesting effect in the control group from Week 1 to Week 2: namely, participants in the control group improved their performance slightly. This can be attributed to practice and is a common finding in studies of this kind. In fact, by virtue of completing the Week 1 tasks many of the control participants may have discovered, learned or remembered the knowledge that would have been conveyed in the (unseen) tip. This improvement in the control group underscores the importance of using an experimental design that retests the control group in Week 2 in addition to the experiment group.
Task 5B is curiously different in that it is the only task for which the improvement from Week 1 did not carry over from Week 2. Looking at the data in Figure 3, we see that the control group was especially fast with respect to this task (compared to other tasks) in Week 1 and even more so in Week 2. We believe that this task was fairly simple for the participants (especially after having had some practice with the complementary Task 5A in Week 1), which may explain why the improvement relative to the experiment group did not carry over.
5. STATISTICAL MODELING AND INFERENCE
In this section we validate the statistical significance of the findings. Specifically, the data from the Week 1 study suggests that the search tips substantially improve performance for all 12 tasks. Further, the data from the Week 2 study suggest that at least some of the experiment group's advantage over the control group is retained for all tasks except 5B. These findings are valid because it is unlikely to obtain such substantial differences between the control and experiment groups simply by chance given the fairly large participant sample size and the fact that we randomly assigned users to the control and experiment groups. In this section, we formally validate this statement using inference in the context of a statistical model.
The model takes into consideration participant skill variation, the difficulty of each task, and the tip effects for the experiment group. In Week 2 the model also takes into account practice effects for each task as well as the fact that some of the tip effects may be diminished or forgotten for some tasks.

359

Figure 3: Results of Experiments 1 and 2 (i.e. Week 1 and Week 2). In Week 2 the participants are retested with the task variant they did not see in Week 1. Neither group is shown a search tip in Week 2. The experiment group performs faster than the control
group for most tasks, ostensibly due to the memory of the tip from Week 1.

Formally, we will write our model as
Log(TCT)ij = Ui + tj + dj + pj + fj +ij
where the response variable Log(TCT)ij is the logarithm of the task completion time of user i when doing task j. The logarithm is used because the task completion times are right-skewed, as is done in analogous models employed in task completion time studies (e.g. [25]).
Because the independent variables in this model consist of both fixed effects and random effects, the type of model is known as a mixed effects model. Specifically, the Ui are random user effects that account for individual skill variation [6, 12, 15] and are included because some users are naturally faster than others. We assume that these user effects are independent, zero-mean normal random variables with a common variance. Similarly, the error terms in the model ij are also assumed to be independent normal variables with mean zero and a common variance.

The remaining terms tj, dj, pj and fj are all fixed effects associated with each task j. The terms tj are the tip effects for the tasks. These are the 12 effects of interest in Week 1. The terms dj are the difficulties of the 12 tasks. These are not of interest but are essential to include in the model to account for inherent variations in task difficulty. The terms pj are the 12 practice effects that measure any benefit from practice for each task in Week 2. These are also not of primary interest but are important to include to model the effects of individual practice. Finally, the terms fj measure the extent to which the benefit of the tips diminishes for each of the 12 tasks for the experiment group in Week 2.
We fit the above model to the data from the 345 participants who completed tasks in both Week 1 and Week 2. We used the R package lme4's implementation of restricted maximum likelihood and the package languageR to generate confidence intervals.

Figure 4: Analysis of Experiment 1 results. The improvement in task completion time (TCT) due to the tip was statistically
significant for all 12 tasks. 95% confidence intervals are indicated.

Figure 5: Analysis of Experiment 2 results. The improvement in task completion time (TCT) due to the memory of the tip was statistically significant for all 12 tasks except Task 5B.
95% confidence intervals are indicated.

360

In Figure 2 we saw that the (Week 1) tip effects appeared to be substantial for all 12 tasks. The model above verifies that indeed all of these effects are statistically significant. Specifically, Figure 4 displays 95% confidence intervals for the percent improvement as a result of the tip for each task. We can see that none of these confidence intervals overlap zero. Again, Tasks 1A, 1B, 3A and 3B show the largest improvement. In particular, Task 3A and 3B are both estimated to have approximately a 60% improvement in efficiency.
In Week 2 the effects of interest are the sum of tj and fj for the 12 tasks. In other words, it is of interest to see whether the tip effect remains for each task after the participants have had roughly a week to forget the tip. Figure 3 suggested that the tip benefit does remain except for Task 5B. Again, using the model presented above we can confirm that these effects are indeed statistically significant. This is evidenced by the fact that the 95% confidence intervals shown in Figure 5 do not overlap zero with the exception of 5B. Tasks 3A and 3B show the strongest persistent tip effect.
That the overall performance gains remain for most tasks is understandable given that the tactic in the associated tip is difficult to discover without the tip and that the tasks are fairly easy using the tactic and very difficult without it.. This was a specific design goal of the experiment. In spite of this, inspection of the tasks (shown in Table 1) shows they are feasible tasks.
6. TASK SUCCESS
Up to this point we have considered only the task completion time as our dependent variable. However, in addition to measuring task completion time we also asked the participants to self-report whether or not they felt they had successfully completed each task. In this section we present a brief analysis using the selfreported success rate as the dependent variable.
Figure 6 compares these success rates for the control and experiment groups in both Week 1 and Week 2. We see in the figure that the success rates for the experiment group are higher than those of the control group for all 12 tasks in both weeks. This result is not surprising since we expect the success rates to mirror the task completion time data by virtue of the nature of our experiment.
Figure 6 also shows that overall the experiment group achieved high absolute success rates for all tasks in both weeks, in excess of 80% in most cases. Conversely, the control group did quite poorly for certain tasks such as 1A, 1B, 3A and 3B. Recall that we noted earlier these 4 particular tasks also had the strongest experimental effects in our task completion time analysis.
In order to verify the statistical significance of the experiment versus control differences observed in Figure 6 we fit a model analogous to our previous model for task completion time. The model uses the log odds of the success probability as the response. From analysis of that model we see in both Week 1 and Week 2 all differences are statistically significant except those for Tasks 5B and 6B.
Overall, our conclusion is that the tips presented substantially improve (self-reported) success rate in addition to task completion time.
We will note that we also carried out the same analysis above but instead using an objective measure of success based on what we had predetermined to be "correct" answers for the tasks. That analysis gave even stronger results; however, we chose to present the self-reported success rates here in favor of those results because our predetermined "correct" answers are arguably biased

in favor of the search tips. Participants who did not use the tips often found different but possibly correct answers by using alternative search strategies, which we had not considered. We were not able to verify the correctness of their answers in every case. Thus, the self-reported success seemed to be a more fair measure to us.
Figure 6: Percent of tasks self-reported as successfully completed. The experiment group consistently reports
successful completion of the task.
7. DISCUSSION
These studies demonstrate that we can measure the efficiency gains resulting from relevant search tips, and that these gains persist and continue to be measurable over at least one week, even when the search tips are no longer shown. In the literature on IR systems, few interventions have been able to show such dramatic and sustainable gains in performance strictly by focusing on improving user performance. A relatively large, diverse sample of users participated in the study, and it was done in an ecologically valid setting. The tasks used in this study were explicitly designed to be both feasible for common use and to show the positive effects of knowing which tactic is appropriate for which task. We can attribute the performance gains of the experiment group to two possible causes: 1) users learned a new tactic and associated it with a type of task or 2) users were reminded of a previouslyknown tactic and associated it with a type of task. Both are valid reasons for why tactical suggestions can be useful. This experiment did not explicitly test the transfer of a learned tactic to another task. Instead, the follow-up study used tasks very closely related to those in the first experiment. The results make no claim of how well users learned to generalize the tactic but rather that certain tactics can be associated with certain, narrow types of tasks. Prior work points to task [8, 9, 10], search [6], and domain [32, 34] complexity and expertise as some of the contextual factors influencing search performance. The results of our study suggest that providing relevant search tactics may be construed as either augmenting an individual's search expertise (i.e. knowing the

361

tactic well enough to match a task to it) or task expertise (i.e. knowing the task well enough to choose a particular known tactic for it). We propose that such tactical tips are improving upon search expertise rather than that of task or domain. Domain expertise (or topic familiarity) is not particularly relevant and task expertise is not being tested because the tasks are directed lookup tasks that do not require multiple steps or a particular strategy to solve.
The tasks used in this study were explicitly designed to draw out the importance of knowing particular tactics. The use of suggestions should likewise be highly selective in order to maintain a consistent expectation of utility in the user.
Research on improving automated assistance in IR systems tends to improve upon methods of suggesting ways of reformulating queries. The tips used in this study influence users to frame their questions in a new way or to become aware of tactics that are supported by the system and may be useful. From a systems perspective, this requires system designers to be intricately aware of what types of tasks are best-suited for the features supported. Already, search engines are attempting to `blend' results from different corpora into the standard result set and offer entry points into those corpora (e.g. integrating a few images into a document result set with a prompt for seeing this query in image search).
It is clear that an understanding of a user's level of sophistication and search knowledge is of importance to information retrieval systems, ostensibly to offer appropriate or adaptive feedback and suggestions. For example, White, et al. [33] used the presence of advanced search operators in queries as a proxy for expertise. Another indicator may be the use of specific search tactics.
The differences between the control and experiment groups in self-reported successful completion points to the idea that some tactics do not merely improve efficiency, they make some queries possible. For example, participants in Task 3A and 3B who were made aware of the tactic to use the micro-blog searching feature were able to consistently answer this very feasible query while participants who remained ignorant of the tactic simply could not finish it (i.e. gave up after reaching the experiment-imposed maximum of five minutes).
Though the primary goal of IR research is to produce the document the user is seeking, recent cases have been made that move study from document retrieval into exploration of relevant corpora or perspectives on the information need [25]. This shift requires a new perspective on the purpose of automated assistance. Instead of inferring the user's information need as he or she iterates on queries, the system can help the user refine the information need itself. For example, when searching for a book to buy, the search engine could suggest a tactic that shows the locations of libraries nearby given the need to understand a topic. Once ignorant of such a possibility, the user's information need has shifted.
As noted earlier, this study used task completion time as a measure of performance rather than recall or precision. This study was conducted on the open web and the users had the freedom to formulate their own queries or browse as they liked. Our goal is to improve user expertise so we focus on measuring user performance.
The visual and interaction design of tactical tips is an interesting domain of further inquiry. The visual design of the tips used in this study was not constrained by size in any significant way. They also did not incorporate animation or interactivity in any way. The display of the tip would also differ substantially

between user-triggered (e.g. [17]) and automatically displayed (as was done in the present study).
7.1 Study Limitations
The following are three major limitations in our study. As explained in [29], we used artificial tasks that were assigned to the participants, not "personal" tasks that were generated by the participants. Second, these results represent a `best case scenario' where the tips are highly relevant to the task at hand. In practice, it is a much more challenging problem to design the actual user interface so that the tips trigger at the correct time and in the correct context. For this reason our observed improvement in efficiency can be thought of as an upper bound for what could be obtained in practice. Third, we largely ignored task accuracy as a metric for this study in favor of efficiency alone. In a sense, efficiency was a proxy for accuracy because participants were instructed to give up if they hit the five-minute limit.
7.2 Experimental Design
Both experiments presented in this paper (Week 1 and Week 2) used what might be called a simple 2 group "between-subjects" or "case/control" design. Because our treatment effect (learning the skill) is irreversible, we did not have the option of using a standard cross-over design. However, an analogue of the four group cross-over design presented in [25] would have been feasible. We chose not to use that particular design due to its complexity and because the experiment in [25] showed that design was actually slightly worse for estimating experimental effects for the individual tasks, which was the primary interest in our study.
8. CONCLUSION
This study demonstrates that providing tactical search tips, appropriately shown to searchers at the right time, can produce a substantial and measurable improvement in their performance. This gives us hope that further instruction can also have a measurable effect on overall searcher behavior, particularly when extended to strategic instruction and longer-term search behavior on more complex tasks.
9. ACKNOWLEDGEMENTS
The authors would like to thank Esther Wojcicki and Roy Pea for their discussions during the course of the research.
10. REFERENCES
1. Bates, M. J. (1979). Idea tactics. JASIS, 30, 280-289.
2. Bates, M. J. (1979). Information search tactics. JASIS, 30, 205-214.
3. Belkin, N. J., Cool, C., Head, J., Jeng, J., Kelly, D., Lin, S.-J., Lobash, L., Park, S., Savage-Knepshield, P. A., and Sikora, C. (1999). Relevance feedback versus local context analysis as term suggestion devices. In TREC Conference: Rutgers' TREC-8 Experience track.
4. Belkin, N.J., Muresan, G. (2004) Measuring Web search effectiveness: Rutgers at Interactive TREC. WWW Conference workshop on Measuring Web Effectiveness.
5. Belkin, N. J., Marchetti, P. G., Cool, C. (1993). Braque: design of an interface to support user interaction in information retrieval. Info. Processing Mgt., 325-344.
6. Bhavnani, S.K. (2001). Important Cognitive Components of Domain-Specific Search Knowledge. TREC, 571-578.

362

7. Brajnik, G., Mizzaro, S., Tasso, C., Venuti, F. (2002). Strategic help in user interfaces for information retrieval. JASIST, 53(5), 343­358.
8. Byström, K. (2002). Information and Information Sources in Tasks of Varying Complexity. JASIST, 53(7): 581-591.
9. Byström, K., Jarvelin, K. (1995). Task Complexity Affects Information Seeking and Use. Information Processing and Management, 31(2): 191-213.
10. Capra, R., Pérez-Quiñones, M.A. (2006). Factors and Evaluation of Refinding Behaviors SIGIR Workshop on Personal Information Management, 16-19.
11. Fernández-Luna, J. M., Huete, J. F., Macfarlane, A., Efthimiadis, E. N. (2009). Teaching and learning in information retrieval. Information Retrieval. 12, 2 (Apr. 2009), 201-226.
12. Grimes, C., Tang, D., Russell, D. M. (2007). Query logs alone are not enough. WWW Workshop on Query Log Analysis: Social and Technological Changes.
13. Gwizdka, J. (2008). Revisiting search task difficulty: Behavioral and individual difference measures. ASIST.
14. Hölscher, C. Strube, G. (2000). Web search behavior of Internet experts and newbies. WWW, 337-346.
15. Hsieh-Yee, I. (1993). Effects of search experience and subject knowledge on online search behavior: Measuring the search tactics of novice and experienced searchers. JASIST 44(3), 161­174
16. Jansen, B. J. (2006). Using temporal patterns of interactions to design effective automated searching assistance systems. Comm. of the ACM. 49(4), 72-74.
17. Jansen, B. J. McNeese, M. D. (2005). Evaluating the effectiveness of and patterns of interactions with automated assistance in IR systems. JASIST. 56(14), 1480-1503.
18. Jones, R., Rey, B., Madani, O. (2006). Generating query substitutions. WWW, 387-396.
19. Kelly, D., Cushing, A., Dostert, M., Niu, X., Gyllstrom, K. (2010). Effects of popularity and quality on the usage of query suggestions during information search. CHI 45-54.
20. Kelly, D., Gyllstrom, K., Bailey, E. W. (2009). A comparison of term and query suggestion features for interactive searching. SIGIR, 371-378.

21. Kriewel, S. Fuhr, N. (2007). Adaptive search suggestions for digital libraries. ICADL, 220-229.
22. Lau, T., Horvitz, E. (1999). Patterns of Search: Analyzing and Modeling Web Query Refinement. UMAP, 119-128.
23. Lazonder, A. W. (2003). Principles for Designing Web Searching Instruction. Education and Information Technologies 8, 2 (Jun. 2003), 179-193.
24. Liu, Y. and Belkin, N. J. (2008). Query reformulation, search performance, and term suggestion devices in questionanswering tasks. IIiX, 21-26.
25. Ma, L., Mease, D., Russell, D. M. (2011). A Four Group Cross-Over Design for Measuring Irreversible Treatments on Web Search Tasks. HICSS.
26. Marchionini, G. (2006). Exploratory search: From finding to understanding. Communications of the ACM, 49(4), p. 41-46.
27. Mizzaro, S. (1996). Intelligent interfaces for information retrieval: A review. Technical Report UDMI/18/96/RR, Department of Mathematics and Computer Science, University of Udine.
28. Moraveji, N. (2010). User interface designs to support the social transfer of web search expertise. SIGIR, 915-915.
29. Russell, D., Grimes, C. (2007). Assigned tasks are not the same as self-chosen Web search tasks. HICSS.
30. Teevan, J., Alvarado, C., Ackerman. M., Karger, D. (2004). The Perfect Search Engine Is Not Enough: A Study of Orienteering Behavior in Directed Search. CHI.
31. Twidale, M. B., Nichols, D. M., Smith, G., and Trevor, J. (1995). Supporting collaborative learning during information searching. CSCL, 367-370.
32. White, R., Dumais, S., Teevan, J. (2009). Characterizing the Influence of Domain Expertise on Web Search Behavior. WSDM, 132-141
33. White, R., Morris, D. (2007). Investigating the Querying and Browsing Behavior of Advanced Search Engine Users. SIGIR, 255-262
34. Wildemuth, B. M. (2004). The effects of domain knowledge on search tactic formulation. JASIST, 246-258.
35. Xie, I., & Cool, C. (2009). Understanding help-seeking within the context of searching digital libraries. JASIST, 477-494.

363


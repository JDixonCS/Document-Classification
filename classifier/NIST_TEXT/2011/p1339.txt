Review of MSR-Bing Web Scale Speller Challenge

1st Author

Kuansan Wang Jan Pedersen

3rd Author

1st author's affiliation

Microsoft Research and Bing

3rd author's affiliation

1st line of address 2nd line of address Telephone number, incl. country code

One Microsoft Way Redmond, WA 98052, USA
+1(425) 882-8080

1st line of address 2nd line of address Telephone number, incl. country code

1st author's email address

http://web-

3rd E-mail

ngram.research.microsoft.com

ABSTRACT
In this paper, we provide an overview of the MSR-Bing Web Scale Speller Challenge of 2011. We describe the motivation and outline the algorithmic and engineering challenges posed by this activity. The design and the evaluation methods are also reviewed, and the online resources that will remain publicly available to the community are also described. The Challenge will culminate in a workshop after the time of the writing where the top prize winners will publish their approaches. The main findings and the lessons learned will be summarized and shared in the Industry Track presentation accompanying this paper.
Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing ­ Language models. H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ query formulation
General Terms
Algorithm, Measurement, Experimentation.
Keywords
Web search, Language model, Spelling Alteration, MSR-Bing Speller Challenge.
1. INTRODUCTION
As a follow-up to the invited talk in SIGIR 2010 on the critical role of query understanding for web search [2], Microsoft Research (MSR) and Bing jointly announced a public contest in December 2010 [3] to promote the algorithmic and engineering research in this area. The focus for the MSR-Bing Web Scale Speller Challenge is on an early stage of query understanding: spelling alteration. Spelling alteration has traditionally been studied in the field of natural language processing (NLP) [1]. As the web has dramatically democratized the content publishing business, it has been noted [4] that traditional NLP techniques that excel at handling editorial quality contents are no longer sufficient for processing the web contents. Specifically for web search, one often observes that search queries sometimes consist of segments from different languages that are difficult to detect using the conventional language identification techniques, and the term usages and the lexicon for the web are rapidly growing and changing over time. Popularized by the short message services and the conflated domain names commonly seen for online marketing, many terms are a mixture of alphabetical of numeral
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

characters that do not follow conventional word boundaries, and even individual terms are well constructed the search queries are often expressed in a "pidgin" language that does not abide by normal linguistic rules. The notion of a "correct" spelling can vary with contexts, and it is not unusual that some relevant documents can themselves contain spelling errors that an overzealous speller in the query processing stage would prevent these documents from being retrieved and ranked high.

In addition to these algorithmic challenges, applications for web search typically need to process a large amount of data and complete the NLP task in a split second. Engineering quality plays as critical a role as algorithmic excellency. As such, a peculiar setting of the Speller Challenge is to require the participants to implement their entries as publicly accessible web services with a REST interface defined in [3], based on which the engineering quality, such as reliability and latency, can also be evaluated by the on-demand, web-based tools developed by MSR (Sec. 2). An intended side effect is the participating teams can easily replicate the results and learn from the submitted spellers among one another. The contest rules allow and encourage teams to pool their resources online and utilize social networks or other collaborative means to improve their spellers for a period of five months with a final entry deadline at noon on May 31, 2010. After the Challenge results are finalized, all the web-based tools and data resources will be reopened to the public so that future researchers can benchmark their systems against the participating entries at high fidelity with minimum effort.

2. SPELLER CHALLENGE FORMAT
The objective of spelling alteration is to produce plausible alternatives for a given query. As laid out in [2], the problem can be formulated as the noisy channel problem in information theory where the optimal solution is known to be

|

(1)

where h and q denote a hypothesized alternative and the received queries, respectively. A realization of (1) is a spelling alternation task when the hypothesized alternatives are limited to lexical manipulations on the received query and no semantic revisions are considered.

In practice, spelling alteration is merely a subcomponent of the query understanding process where multiple spelling alternatives are often desirable and the respective posterior probabilities
| serve as an invaluable cue for the optimization of the overall query processing. The Challenge therefore requires the submissions to not only produce a list of alternatives for each query but also estimate their posterior probabilities.

1339

2.1 Measurements
To take into account the posterior probabilities, the Challenge modified slightly the conventional definition of F1 measure for
evaluating the the algorithmic performance of a speller. First, the Expected Precision (EP) is defined as

| | 

|

(2)

where . is the Boolean indicator function, is the full query

set, and

,

are the list of alternatives generated by the

speller and the set of the desirable alterations for query

,

respectively. The design is to favor spellers that do not over-

generate alteration hypotheses because the total posterior

probability must be distributed among all the hypotheses and

summed up to 1. The corresponding Expected Recall (ER) is

defined as


||

|

(3)

For this Challenge, we only use manually annotated data that does not estimate the relative importance of each plausible alteration that | 1/| | is used. It is hoped that in the future the proper distribution can be estimated for | either through the search behavioral data or cross-validations among a diverse collection of multiple spellers. Finally, the harmonic mean of EP and ER, called the Expected F1 (EF1), is used as the main metric for the Challenge.

MSR maintains a web site [3] to accept the URI of a speller for

evaluation. The evaluator sets a 60 second timeout for the speller

to respond to each query, and treats the response as

if

an HTTP 500 level error or timeout expiration is encountered. In

addition to EF1, the average latency for each query is also

computed as the potential tie-breaker for the Challenge.

2.2 Datasets
The Challenge uses search queries received by Bing in March 2010 as the final test set with | | 1500. The queries are uniformly sampled from the tail section of the query log so that the uniform frequency assumptions intrinsically in both (2) and (3) are valid. The queries are manually annotated with spelling alterations, and are tokenized in the same fashion as described in [5]. After the award contest phase of the Challenge is completed, this dataset will continue to be used by the evaluator (Sec. 2.1) for researchers to benchmark future spelling alteration systems.

Three datasets are made available for the purpose of developing spelling alteration systems. First, the query set from TREC 2008 Million Query Track (1MQ) is annotated with spelling alterations using the same guidelines as the Bing test set and is made available for download [3]. The TREC dataset, with a size of more than 5500 queries, consists of search queries seeking public information published by US government websites. Roughly 10% of the queries contain typographical errors and, although the queries are collected in the EN-US region, the dataset contains a subset of queries in ES-US Spanish. These non-English queries are deliberately preserved because having rudimentary multilingual capability is a necessity for web search query processing. The frequencies of having non-English or spelling errors in TREC query set, however, are lower than the Bing dataset for the final evaluation of the Challenge.

The second dataset is the Microsoft Web N-gram service [5], available exclusively in the web service format. The dataset

includes multiple statistical language models created from text resources on the web, including queries received by Bing over a 10 month period. In addition, the dataset includes language models built from the body, title, and the anchor text of the web documents indexed by Bing. Two snapshots from the Bing index are available: one from June 30, 2009 and April 30, 2010, respectively. These language models are provided as a reference resource for the speller developers to estimate the contextual probabilities of the alterations.
As misspelling is common in the web documents and the cut-off is set low in processing these snapshots, the web N-gram dataset appears to be a valuable resource to discover common patterns of misspelling in a large scale. To that end, the Challenge organizer made available a service to identify tokens appearing in the similar N-gram context that can be used as a supplement material to the TREC data in developing a speller.
3. PRESENT AND FUTURE
At the time of writing, the prize competition phase of the Challenge is still ongoing and there are 339 teams producing over 400 prototypes vying for the top 5 cash reward positions. The Challenge will consummate in July, 2011 when a workshop is held for the Challenge participants to discuss their work and exchange experience. The papers and discussion will be published in an electronic workshop proceedings, with summary shared in the SIGIR-2011 industry track presentation.
The dataset and the evaluation tools will continue to be available as web services at the Challenge web site [3]. It is the hope that researchers can utilize these resources to benchmark future improvements against the competition results.
4. ACKNOWLEDGMENTS
The authors would like to thank Mr. Christopher Thrasher, Nikolas Gloy and Amit Koul for their technical assistance in maintaining the online infrastructure for the Challenge. Drs. BoJun "Paul" Hsu, Evelyne Viegas and Jianfeng Gao contributed to the design of the Challenge.
5. REFERENCES
[1] Gao, J., Li, X., Micol, D., Quirk, C., Sun, X. 2010. A large scale ranker-based system for search query spelling correction. In Proc. 23rd International Conference on Computational Linguistics (COLING-2010), Beijing, China, August, 358-366.
[2] Pedersen, J. 2010. Query Understanding at Bing. Invited talk at the 33rd Annual ACM SIGIR Conference (SIGIR-2010), Industry Track, Geneva, Switzerland, July.
[3] Speller Challenge, http://www.spellerchallenge.com, also at http://web-ngram.research.microsoft.com/spellerchallenge.
[4] Wang, K., Thrasher, C., Hsu, B.-J. 2011. Web scale NLP: A case study on URL word breaking. In Proc. 20th International Conference on World Wide Web (WWW-2011), Hyderabad, India, March, 357-366. DOI=http://doi.acm.org/10.1145/1963405.1963457
[5] Wang, K., Thrasher, C., Viegas, E., Li, X., Hsu, B.-J. 2010. An overview of Microsoft web N-gram corpus and applications. In Proc. NAACL-HLT 2010, Los Angeles, CA, May, 45-48. Also, http://web-ngram.research.microsoft.com.

1340


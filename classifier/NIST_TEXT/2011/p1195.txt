Rating-based Collaborative Filtering Combined with Additional Regularization

Shu Wu and Shengrui Wang
Department of Computer Science, University of Sherbrooke 2500 Boul. de l'Universite, Sherbrooke, Quebec, Canada J1K 2R1
{Shu.Wu, Shengrui.Wang}@usherbrooke.ca

ABSTRACT
The collaborative filtering (CF) approach to recommender system has received much attention recently. However, previous work mainly focuses on improving the formula of rating prediction, e.g. by adding user and item biases, implicit feedback and time-aware factors, etc, to reach a better prediction by minimizing an objective function. However, little effort has been made on improving CF by incorporating additional regularization to the objective function. Regularization can further bound the searching range of predicted ratings. In this paper, we improve the conventional rating-based objective function by using ranking constraints as the supplementary regularization to restrict the searching of predicted ratings in smaller and more likely ranges, and develop a novel method, called RankSVD++, based on the SVD++ model. Experimental results show that RankSVD++ achieves better performance than existing main-streaming methods due to the addition of informative ranking-based regularization. The idea proposed here can also be easily incorporated to the other CF models.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Information filtering
General Terms
Algorithms, Experimentation
Keywords
Collaborative Filtering, Matrix Factorization
1. INTRODUCTION
A collaborative filtering (CF) [1] recommender system, primarily based on the neighborhood approach and the latent factor approach, produces personalized item recommendations to users by relying on past behaviors, e.g. ratings, transactions or even web click streams. A good CF system will not only enhance the satisfaction and loyalty of customers but also promote sales. Some e-commences and web service providers like Amazon and Netflix have adopted the CF-based recommendation to further their businesses.
Previous work on CF methods primarily focuses on improving the predicted function of rating. For instance, the factor models, e.g. SVD, SVD++ [3] and timeSVD++ [4],
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

which are serial models originated from the basic Matrix Factorization (MF) model [1], are augmented by integrating user and item biases, implicit feedback of users, and temporal effects respectively in calculating predicted ratings. The Netflix Prize announced the success of these improvements.
To enhance the effectiveness of CF recommender, we propose to integrate the objective function with additional regularization. In this paper, we address optimization of the classic rating-based metric in combination with a rankingbased regularization. The rating-based metric in [1] receives reward for a predicted rating r^ close to the given rating value r and gets penalty for the predicted rating r^ far away from the given one. The rank-based regularization receives reward for predicting the same pairwise ordering r^i > r^j as the given ordering ri > rj. The idea of this work is that a rating-based metric combined with extra ranking-based regularization helps to restrict the searching of predicted ratings in smaller and more likely ranges. The regularization factor depends on the difference between the given rating and the predicted rating to measure pairwise ranking. This measurement not only can be well unified with the loss function of rating but also make the new objective function a continuous function for convenient optimization.
i1 i2 i3 ... in
r 1 5 5 ... 5
r^1 5 5 5 ... 5 r^2 1.2 5 5 ... 5
Figure 1: Given ratings r and predicted ratings r^1, r^2. The r^1 is near-perfect for rating metric, but with bad ranking relation. The r^2 is a near-perfect prediction of rating-based metric with an additional ranking-based regularization and is better than r^1.
This work addresses the limitation of the conventional rating-based metric of CF. Since the perfect prediction of rating metric provides a perfect ranking scheme, one might think that a good prediction of rating would also provide a good ranking schema. That is not true. Consider the distribution of given ratings r in Fig. 1, there are n - 1 ratings with highest value r = 5 while just one with r = 1. Judged according to the metric of rating, the prediction r^1 provides excellent rating performance, however, it expresses a very poor ranking relation. When we impose the rankingbased regularization into the ranking metric, the new model is able to generate a near-perfect prediction r^2 which is better than r^1. In this work, we want to design such a model that exalts the performance of rating-based CF with added ranking-based regularization.

1195

2. RANKSVD++ METHOD
Let us assume a set of users U and a set of items I in a typical CF scenario. Each user u is associated with a set of items Iu, which contains all the items that the user has rated. All known (u, i) pairs between U and I is denoted as a set R. du and di indicate the deviations of user u and item i from the average rating  of R. f -dimension factor vectors pu  Rf and qi  Rf describe the latent characters of the user u and item i. A second vector yi  Rf of item i is used to characterize users based on the set of items they rated
as a implicit feedback. In SVD++ model [3], the function of predicted rating is written as r^ui =  + di + du + qTi pu, where pu = pu + |Iu|-0.5 jIu yj .
Now we investigate the scenario of combining the metric of
SVD++ with the supplementary ranking-based regulariza-
tion. A simple and successful pairwise ranking [2] is involved
in this work. Similar to other norm terms, we also use the
Frobenius norm to calculate the regularization of pairwise ranking, i.e. (ri - rj) - (r^i - r^j) 2. To combine with the metric of rating, we calculate the sum of associated rankingbased regularization of items in Iu at each (u, i) pair and normalize the sum with coefficient |Iu|-0.5. Meanwhile,  is used to adjust the weight of ranking regularization. The
model with the new objective function is learnt by mini-
mizing the regularized least squares function. The objective
function can be expressed as

SV D++

l (rui, r^ui) +

w

2

+|Iu|-

1 2

(ri - rj ) - (r^i - r^j ) 2

(u,i)R

jIu

3. EXPERIMENTAL RESULTS
For our experiments, we use the Yahoo Music rating data [5]. About 6.4 million ratings of 16,883 users and 2,676 songs between years 2002 and 2006 are chosen, where each user gives at least 20 ratings. Three ratings of each user are randomly chosen as test data, the remaining ones are treated as training data. The experiments are repeated 10 times by sampling new training and test data.
To illustrate the effectiveness of our model, we compare it with a number of factor models and neighborhood methods. The MF model is the basic SVD without the user and item biases. The SVD++ model improves SVD by integrating the implicit feedback of users. Since timeSVD++ needs time-aware factor as extra information, we did not implement it on this data. On the other hand, we implement two neighborhood methods, i.e. user-based and itembased methods, using the Pearson correlation to measure the similarity. The learning rate and hyperparameters of factor models are searched on the first training data. The quality of predictions is measured by the Root Mean Squared Error (RMSE)[1] (smaller is better). The average RMSEs of 10-time tests are listed in Fig. 2.

0.92

RMSE

0.91

User-based 0.9
Item-based

MF

0.89

SVD

SVD++

0.88

RankSVD++

where, the first two terms, i.e. the loss function of rating and regularized parameters of models in a (u, i) pair, compose the least square function of the SVD++ model [3]. The final term is the ranking-based regularization modeled by the loss between two given ratings and two predicted ratings. Finally, the objective function of our proposed RankSVD++ can be written as

min
b.,q.,p.,f .

rui -  - di - du - qTi p u

2
+

w 2+

(u,i)R

|Iu|-0.5

rui - ruj - di + dj - qTi p u + qTj p u 2

jIu

Since the criterion of RankSVD++ is composed of convex functions, it can be solved efficiently by the method of stochastic gradient descent [3]. We loop over all known ratings in R, calculating:

du  du +  (eui - 1du) di  di +  eui - 1du + |Iu|-0.5bui qi  qi +  euipu - 2qi + |Iu|-0.5buipu pu  pu +  euiqi - 2pu + |Iu|-0.5fui
j  Iu dj  dj +  -|Iu|-0.5bui qj  qj +  -|Iu|-0.5buipu yj  yj +  eui|Iu|-0.5qi + |Iu|-1fui - 2yj

where, to simplify the expression, some abbreviates are used such as eui = rui - r^ui, fui = jIu (eui - euj )(qi - qj ), bui = jIu eui - euj . Learning rate is denoted as . Regularization factors 1, 2 and  are used to avoid overfitting.

0.87

0

50

100

150

200

Factor Dimensionality

Figure 2: Prediction accuracy of different methods

measured by RMSE for varying dimensionality f .

The empirical results indicate the factor models outper-

form two neighborhood models, and our RankSVD++ model

performs the best among these factor models. All factor

models benefit from the growing number of factor dimen-

sions f , which can better express the latent characters of the

users and items. The advantage delivered by RankSVD++

over SVD++ is consistently significant, due to the added

constrains of ranking. Compared with the benefit brought

by additional ranking regularization, the gain in accuracy

by adding implicit feedback of users in SVD++ over SVD

model is not as significant, and the gain decreases with

the growing f . The gain in accuracy by integrating user

and item biases in SVD over MF is smaller than the gain

brought by ranking. Further evidence of the importance of

combining ranking-based regularization is the fact that the

RankSVD++ model at f = 10 is already more accurate

than the other factor models at f = 200. The results show

that RankSVD++ is a very effective single model.
4. REFERENCES
[1] F. Ricci, L. Rokach, B. Shapira and P.B. Kantor, Recommender Systems Handbook, Springer, 2011.

[2] C. Burges, T. Shaked, E. Renshaw, et al. Learning to rank using gradient descent, In ICML '05, 2005.
[3] Y. Koren, Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model, In KDD '08, 2008.

[4] Y. Koren, Collaborative Filtering with Temporal Dynamics, In KDD '09, 2009.

[5] Yahoo Music Data, http://webscope.sandbox.yahoo.com/

1196


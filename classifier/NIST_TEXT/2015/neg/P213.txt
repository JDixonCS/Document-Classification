Multiple Social Network Learning and Its Application in Volunteerism Tendency Prediction

Xuemeng Song, Liqiang Nie, Luming Zhang, Mohammad Akbari, Tat-Seng Chua
National University of Singapore
{sxmustc, nieliqiang, zglumg}@gmail.com, akbari@nus.edu.sg, chuats@comp.nus.edu.sg

ABSTRACT
We are living in the era of social networks, where people throughout the world are connected and organized by multiple social networks. The views revealed by different social networks may vary according to the different services they offer. They are complimentary to each other and comprehensively characterize a specific user from different perspectives. As compared to the scare knowledge conveyed by a single source, appropriate aggregation of multiple social networks offers us a better opportunity for deep user understanding. The challenges, however, co-exist with opportunities. The first challenge lies in the existence of block-wise missing data, caused by the fact that some users may be very active in certain social networks while inactive in others. The second challenge is how to collaboratively integrate multiple social networks. Towards this end, we first proposed a novel model for data missing completion by seamlessly exploring the knowledge from multiple sources. We then developed a robust multiple social network learning model, and applied it to the application of volunteerism tendency prediction. Extensive experiments on real world dataset verify the effectiveness of our scheme. The proposed scheme is applicable to many other domains, such as demographic inference and interest prediction.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing
Keywords
Multiple Social Network Learning; Missing Data Completion; Volunteerism Tendency Prediction
1. INTRODUCTION
With the explosion of social network services, more and more people are involved in multiple social networks for various purposes at the same time. It is reported
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767726.

that 52% of online adults concurrently use multiple social media services1. Different aspects of users are disclosed on different social networks due to their different emphasis. In fact, these views are complementary to each other and essentially characterize the same user from different perspectives. As compared with single social network, appropriate aggregation of multiple social networks provides us a potential to comprehensively understand the given users [1, 29]. For example, we can learn descriptive user representation, build predictive models for user profiles, and recommend prescriptive actions based on complete historical behaviors. Hence, an effective technique for multiple social network learning (MSNL) is highly desired. Distinguished from multi-view learning which maximizes the agreement between views using unlabeled data, MSNL works towards supervised learning.
However, integration of multiple sources is non-trivial [27]. The first tough challenge lies in how to fuse users' heterogeneous distributed data from multiple social networks effectively. One naive approach is to concatenate the feature spaces generated from different sources into a unified feature space. Thereby, traditional machine learning models can be further applied. However, this method simply treats the confidence of all data sources equally and may also lead to the curse of dimensionality. Moreover, it ignores two important facts: 1) different aspects of users are revealed in different social networks and are thus distributed in different feature spaces; and 2) all these aspects tend to characterize the same users. In particular, data from multi-sources describes the same user and thus the results predicted by different sources should be similar. Therefore, it is expected to take the source confidence and source consistency into consideration. Another challenge we are facing is the data missing problem. Although some users have social accounts on multiple social networks, generally they are active on only a few of them. One simple approach to address this challenge is to discard all incomplete subjects. It is apparent that this method will dramatically reduce the training size, thereby result in overfitting in the model learning stage. Therefore, accurately completing missing data by jointly utilizing multiple sources is a necessity to enhance the learning performance.
To address these problems, we present a scheme for MSNL, which co-regulates the source confidence and source consistency. Figure 1 shows our proposed scheme comprising of three components. Given a set of users, we first crawl
1According Paw Research Internet Project's Social Media Update 2014: http://www.pewinternet.org/

213

Figure 1: Illustration of our proposed scheme. We first collect and align users' distributed data from multiple social networks. We then jointly infer the block-wise missing data based on the available data. We finally apply MSNL on the complete data. SNi, xj, and yl refer to the i-th social network, j-th user sample, and the l-th corresponding label, respectively.

their historical contents and all social connections. The first component extracts the multi-faceted information cues to describe a given user, including demographic information, practical behaviors, historical posts, and profiles of social connections. To deal with the block-wise missing data, the second component attempts to infer the block-wise missing data by learning a latent space shared by different social networks, achieving a complete input to the next component. We finally use the last component to conduct MSNL on the complete data. Particularly, we model the confidence of different data sources and the consistency among them by unifying two regularization terms into our model.
Based upon our proposed scheme, we introduce one application scenario: volunteerism tendency prediction. Volunteerism was defined in [18] as long-term, planned, prosocial behaviors that occur within organizational settings and can benefit strangers. Persons exhibiting volunteerism are the so-called volunteers, who serve as an important work force in modern society. Traditionally, it is intractable for nonprofit organizations (NPOs) to aimlessly recruit volunteers from the huge crowd. It is thus necessary to develop an automatic volunteerism tendency prediction system to alleviate the dilemma that NPOs are facing. In particular, we take the advantage of users' casually distributed online data, especially from multiple social networks, which can comprehensively reveal users' personal concerns, interests [5, 22] and even personality traits [20, 21]. On the other hand, the real word dataset may contain block-wise missing data due to some users' inactivity in social networks. Moreover, the specific task also brings us another issue in terms of data collection and ground truth construction. Therefore, the proposed scheme naturally fits this application scenario.
Our main contributions can be summarized in threefold: · We propose a novel MSNL model, which is able
to model both the source confidence and source consistency. Specifically, we can obtain a closed-form solution by taking the inverse of a linear system, which has been mathematically proven to be invertible. · We propose an approach to deal with missing data in multiple social networks, which first learns a common

latent subpace shared by different sources [12] and the original missing data can then be derived in turn.
· We empirically evaluate our proposed scheme on the application of volunteerism tendency prediction. In addition, we develop a set of volunteer-oriented features to characterize users' volunteerism tendency. We have released our compiled dataset2 to facilitate other researchers to repeat our experiments and verify their proposed approaches.
The remainder of this paper is structured as follows, Section 2 briefly reviews the related work. Section 3 describes the proposed MSNL model. Missing data completion is introduced in Section 4. Section 5 presents the set of volunteer-oriented features we developed. Section 6 details the experimental results and analysis, followed by our concluding remarks in Section 7.
2. RELATED WORK
Although our work is distinguished from multi-view learning, we can still benefit from their efforts. Zhang et al. [28] proposed an inductive multi-view multi-task learning model (regMVMT). regMVMT penalizes the disagreement of models learned from different sources over the unlabeled samples. Besides, the authors also studied the structured missing data, which is completely missing for a source in terms of a task. In other words, if a source is available for a task, then all samples will have data from this source. However, they overlooked the source weights and did not pay attention to the partially structured missing data, which are both what we are concerned with. Yuan et al. [25] introduced an incomplete multi-source feature learning method, avoiding the direct inference of block-wise missing data. Particularly, the authors split the incomplete data into disjoint groups, where they conducted feature learning independently. However, such a mechanism constrains us to conduct source level analysis. Later, Xiang et al. [24] investigated multi-source learning with block-wise missing
2The compiled dataset is currently publicly accessible via: http://multiplesocialnetworklearning.azurewebsites.net/

214

data with an application of Alzheimer's Disease prediction and proposed the iSFS model. Apart from feature-level analysis, the authors also conducted source-level analysis by introducing the weights of models obtained from different sources. However, ignoring the consistency relationships among different models seems inappropriate. In addition, the authors also adapted the model to handle cases where block-wise missing data exists. Different from their work, we infer the missing data by making full use of the available data before applying MSNL, which is more generalizable to other applications.

3. MULTIPLE SOCIAL NETWORK LEARNING
This section details our proposed MSNL model and derives an analytic solution by solving the inverse of a linear system, whose invertibility is proved rigorously.

3.1 Notation

We first declare some notations. In particular, we use bold

capital letters (e.g. X) and bold lowercase letters (e.g. x) to

denote matrices and vectors, respectively. We employ non-

bold letters (e.g. x) to represent scalars, and Greek letters

(e.g. ) as parameters. If not clarified, all vectors are in

column forms.

Suppose we have a set of N labeled data samples and

S  2 social networks. We compile the S social networks

with an index set C = {1, 2, · · · , S}. Let Ds and Ns

denote the number of features and samples in the s-th social

network, s  C, respectively. Let Xs  RN×Ds denote

the feature matrix extracted from the s-th social network.

Each row represents a user sample. Then the dimension

of Ssf=ea1tDurse.sTehxetrwachtoelde

from all these feature matrix

social networks can be written

is as

D X

= =

{X1, X2, · · · , XS}  RN×D and y = {y1, y2, · · · , yN }T 

{1, -1}N×1 is the corresponding label vector.

3.2 Problem Formulations
Based on a set of data samples with S social networks, we can learn S predictive models, where each model is individually and independently trained on a social network. The final predictive model can be strengthened via linear combination of these S models. Mathematically, we learn one linear mapping function fs for the s-th social network. In addition, we assume that the mapping functions learned from all social networks agree with one another as much as possible. Particularly, we can formalize this assumption using regularization function. Using the least square loss function, we have the following objective function,

1 min fs 2N

y - f (X) 2 + µ  S  2N

fs(Xs) - fs (Xs ) 2

s=1 s=s

+  f 2,

(1)

2

where f (X) is the final predictive model. fs(Xs) is the prediction results generated from data Xs.  and µ are the nonnegative regularization parameters that regulate the sparsity of the solution regarding fs and the disagreement among models learned from different social networks, respectively. If we just treat the confidence of different social networks equally, the final predictive model can be

formalized as follows,

1  S

f (X) = S

fs(Xs).

(2)

s=1

However, in reality, different social networks always have different confidence to the final prediction, and we consider modeling the weights of multiple sources instead of treating all sources equally by introducing the weight vector:  = [1, 2, · · · , S]T  RS×1, where s controls the weight of model learned from s-th social network. Then the final model is defined as follows,

 S f (X) = sfs(Xs)

s=1

subject to eT  = 1,

(3)

where e = [1, 1, · · · , 1]T  RS×1. It is worth mentioning that we do not impose the constraint of s  0, as we want to keep both positive and negative weights. Positive weights indicate the positive correlations of social networks with the final results, while negative weights reflect negative correlations between the given task and different sources, which may contain unreliable and noisy data.
For the s-th social network, we learn a linear mapping function indexed by a model ws  RDs×1. Then the objective function can be rewritten as follows,

1 min ws, 2N

S

2 µ  S 

y - sXsws
s=1

+ 2N
s=1 s=s

Xsws - Xs ws

2

  S +
2

ws

2+  2

 2,

(4)

s=1

where eT  = 1 and  is the regularization parameter, controlling the sparsity of the solution regarding .

3.3 Optimization
We adopt the alternating optimization strategy to solve the two variables  and ws in Eqn. (4). In particular, we optimize one variable while fixing the other one in each iteration. We keep this iterative procedure until the objective function converges.

3.3.1 Computing  with ws fixed
We denote the objective function as . For simplicity, we replace y in Eqn. (4) by yeT , as eT  = 1. With the help of Lagrangian,  can be rewritten as follows,

min 1 yeT  - XW 2 +   2 + (1 - eT ), (5)

 2N

2

where  is the nonnegative Lagrange multiplier and W = diag(w1, w2, · · · , wS)  RD×S. Taking derivative of  with
respect to , we have,

 = 1 (yeT - XW)T (yeT - XW) +  - e. (6)  N

Setting Eqn. (6) to zero, it can be derived that,

 = M-1e,

(7)

where

M = 1 (yeT - XW)T (yeT - XW) + I.

(8)

N

215

Since eT  = 1, we can obtain that,

1

M-1e

=

,

=

.

(9)

eT M-1e

eT M-1e

Obviously, M  RS×S is positive definite and invertible, according to the definition. We thus can obtain the analytic solution of  as Eqn. (9). Moreover, we note that when the prediction results learned from all social networks are equal, where X1w1 = X2w2 = · · · = XSwS, then same weights will be assigned, i.e., 1 = 2 = · · · = S. In addition, Eqn. (9) tends to assign higher weight s, if smaller difference exists between y and Xsws.

3.3.2 Computing ws with  fixed
When  is fixed, we compute the derivative of  regarding ws as follows,

 ws

=

1 N

sXTs

 S (

sXsws

-

y)

s=1

+

µ N

XTs

 S



(Xsws

-

Xs ws )

+

ws

s=1 s=s

=

[ I

+

2s N

XTs

Xs

+

µ(S - N

1)

XTs

Xs

] ws

 S +



1 N (ss

- µ)XTs Xs ws

-

s N

XTs

y,

s=1 s=s

(10)

where I is a Ds × Ds identity matrix. Setting Eqn. (10) to zero and rearranging the terms, all ws's can be learned jointly by the following linear system,

Lw = t



   

L11 L12 L13 · · · L1S w1

t1



L21 L31
...

L22 L32
...

L23 L33
...

··· ··· ...

L2S L3S
...





w2 w3
...



=



t2 t3 ...



,

LS1 LS2 LS3 · · · LSS wS

tS

(11)

where L  RD×D is a sparse block matrix with S × S blocks, w = [w1T , w2T , · · · , wST ]T  RD×1 and t = [tT1 , tT2 , · · · , tTS ]T  RD×1 are both sparse block vectors with
S × 1 blocks. ts, Lss and Lss are defined as follows,

 ts LLssss

=

s N

XTs

y,

=

I

+

2s -µ N

XTs

Xs

+

µS N

XTs

Xs,

=

s

s N

-µ

XTs

Xs

.

(12)

Technically, t can be treated as a constant matrix as  is fixed. It is worth noting that L is symmetric as Lss = LTss. If we can prove that L is invertible, then we can derive the
closed-form solution of w as follows,

w = L-1t.

(13)

We now show L is invertible by proving that L is a positivedefinite matrix. Let h = [hT1 , hT2 , · · · , hTS ]T  RD×1 = 0 be an arbitrary block vector, where hi  RDi×1, i  C. Then

Algorithm 1 Alternative optimization for solving Eqn. (4)

Input: X, y, , , µ

Output: , w

1: Initialize (w)0 by fitting each source individually on the

available

data.

Initialize

()0

=

[

1 S

,

1 S

,

·

·

·

,

1 S

].

2: for k = 1, 2, · · · do

3: Compute each ()k according to Eqn. (9).

4: Update (w)k according to Eqn. (13).

5: if the objective value stops decreasing then

6:

return  = ()k and w = (w)k

7: end if

8: end for

we need to prove that hT Lh

 S  S

=

hTi Lij hj

i=1 j=1

=

h

2 + 1 [  S N

iXihi

2 + µ(S - 1)  S

Xihi

2

i=1

i=1

 S 

 S 

]

+

ihTi XTi j Xj hj - µ

hTi XTi Xj hj , (14)

i=1 j=i

i=1 j=i

is always larger than zero. In fact, given an arbitrary vector bi, we have,

b1 - b2 2 + · · · + b(S-1) - bS 2 + bS - b1 2  0

 S bi 2   S  bTi bj .

i=1

i=1 j=i

(15)

Therefore, as S  2, we have the following inequality,

 S µ(S - 1)

Xihi

2  µ  S

Xihi

2

i=1

i=1

 S 

µ

(Xihi)T Xj hj .

i=1 j=i

(16)

Besides, we know that,

 S iXihi 2 +  S  ihTi XTi j Xj hj

i=1

i=1 j=i

1  S =
2
i=1

iXihi

2+ 1 2

S

2

iXihi  0.

i=1

(17)

Based upon Eqn. (16) and Eqn. (17), we have that,

hT Lh   h 2 .

(18)

As h = 0, hT Lh is always larger than zero. Consequently, L is invertible. The overall procedures for alternating optimization are summarized in Algorithm 1. As each iteration can decrease , whose lower bound is zero, we can guarantee the convergence of Algorithm 1 [7, 16].

4. MISSING DATA COMPLETION
In this section, we deal with a more challenging and realistic situation, where block-wise missing data exists, and propose an approach for multiple social network data completion (MSNDC). In such situations, user samples may

216

not be active in all social networks, which leads to the blockwise data missing.
Suppose we have S data sources in total and each sample has at least one data source available. We employ the subset Ci  C to indicate the presence of each source and the signature of a specific social network combination. Based on these combinations, all the data samples can be split into multiple exclusive sets, where each set corresponds to a combination. Figure 2 illustrates the incomplete data in our dataset. As can be seen, all users have complete features from SN1, while some users miss data in SN2 or SN3. Therefore, our dataset can be split by four exclusive social network combinations: C1 = {1, 2}, C2 = {1, 2, 3}, C3 = {1, 3}, C4 = {1}.
Inspired by [10], we use Non-negative Matrix Factorization (NMF) to explore the latent spaces that are shared by different social networks, and further infer the missing data based upon these latent spaces. It is reasonable to assume that the data from different social networks about the same user shares certain latent features. We employ XCsi  RNCi ×Ds to denote the samples generated from the s-th social network. It only contains samples that are available in the set of social networks Ci, where NCi stands for the number of these samples. We use Us  Rz×Ds to represent the latent basis matrix for the s-th social network, and PCsi  RNCi ×z to denote the corresponding latent representation of feature matrix XCsi . z is the dimension of the shared latent space of different social networks. The intuitive assumption is that for the samples available in both the s-th and s-th social networks, their corresponding latent representations should also be quite similar. In particular, we impose this constraint to NMF as follows,

PCsi = PCsi = PCi ,

(19)

where s = s, s  Ci, and s  Ci. We thus learn the shared subspaces by the following objective function,

min
Us 0 Ps 0

 

X{11} X{11,2} X{11,3}

 

-

 

P{1} P{1,2} P{1,3}

  U1

X{11,2,3}

P{1,2,3}

2
+

+

[

]

X{21,2}

X2{1,2,3}

-

[ P{1,2} ] P{1,2,3}

U2

2
+

+

[

]

X{31,3}

X{31,2,3}

-

[ P{1,3} ] P{1,2,3}

U3

2
+

P1

+
1

P2 1 + 

P3

+
1

U1 1

U2 1

U3

,
1

(20)

where  and  are the nonnegative tradeoff parameters for the regularizations. Similarly, we employ the alternating optimization strategy to solve the optimization in Eqn. (20). To be more specific, we first initialize Us and compute the optimal Ps. Afterwards, Ps is updated based on the computed Us. We keep this iterative procedure until the objective function converges.
The proposed approach differs from [10] in the following three aspects. First, MSNDC is generalized to handle the more challenging scenario where data samples are extracted from more than two social networks. Second, apart from regulating the latent representation matrix, we also incorporate the regularization on the latent basis matrix. Third, we further derive the original missing data from

Figure 2: Illustration of the incomplete data from three sources. XCsi denotes the samples generated from social network s that are only available in the
social network combination of Ci.

the latent representation, where the authors in [10] just apply cluster algorithms directly to the latent representation of data instead of the original data. This is due to two considerations. One is that we believe the value of original known data is higher than the latent representation. The other one is that we need to preserve the heterogeneity among data from different sources to fit the MSNL model.

4.1 Optimization
In order to increase the efficiency of the iterative procedure, we initialize Us by optimizing the following objective function,

min
Us 0
+ +

X{11,2,3} - P{1,2,3}U1 X{21,2,3} - P{1,2,3}U2 X{31,2,3} - P{1,2,3}U3

2
+
2
+
2
+

P{1,2,3} + 
1

U2 1

U3

.
1

U1 1 (21)

We then alternatively optimize Us and Ps until the objective function converges. Specifically, we employ the greedy coordinate descent (GCD) approach [9], which has been proven to be tremendously fast to solve NMF decomposition with L1-norm regularization. Finally, we obtain Ps, Us, s  C, based on which we can infer the missing data as follows,

X^ Csi = PCi Us, s / Ci.

(22)

Algorithm 2 summarizes the overall procedures for alternating optimization.

5. APPLICATION: VOLUNTEERISM TENDENCY PREDICTION
In this work, we apply the proposed scheme to an application scenario: volunteerism tendency prediction. In modern society, volunteers are extremely crucial to NPOs to sustain their continuing operations. The discovery of users' volunteerism tendency can significantly facilitate the recruitment of volunteers for NPOs, which can save considerable cost to find the potential volunteers. In particular, we cast the problem of volunteerism tendency prediction as a user binary classification. If the predicted tendency score of a given user is larger than a pre-defined

217

Algorithm 2 Alternative optimization for solving Eqn. (20)

Input: X1, X2, X3, ,  Output: X^

1: Initialize U(s0) according to Eqn. (21).

2: for k = 1, 2, · · · do

3: for s = 1, 2, · · · , S do

4:

Compute each P(sk) according to Eqn. (20) via GCD

approach.

5:

Update U(sk) according to Eqn. (20) via GCD

approach.

6: if the objective value stops decreasing then

7:

return Us = U(sk) and Ps = P(sk)

8: end if

9: end for

10: end for

11: for j = 1, 2, · · · , S do

12: for Cq  C do

13:

if j  Cq then

14:

X^ Cj q = XjCq .

15: else

16:

Infer X^ Cj q according to Eqn. (22).

17: end if

18: end for

19: end for

threshold , we regard this user as a volunteer. In this work, we explore three popular social networks: Twitter, Facebook and LinkedIn, as they are representative of a public, private, and professional social network, respectively. Besides, it is known that users exhibit different aspects on different social networks [1], and the combination of these three social networks would help to better characterize user behaviors on social platforms.
5.1 Multiple Social Accounts Mappings
To represent the same users with multiple sources, we need to first tackle the problem of "Social Account Mapping", which aims to align the same users across different social networks by linking their multiple social accounts [1]. To accurately establish this mapping, we employ the emerging social services such as About.me3 and Quora4, where they encourage users to explicitly list their multiple social accounts on one profile.
We proposed two strategies to collect data from About.me.
· Keyword search: We searched About.me with the keyword "volunteer" and obtained 4, 151 volunteer candidates.
· Random select: We employed Random API5, provided by About.me, to collect non-volunteers. This API returns a specified number of random user profiles. Finally, we harvested 1, 867 non-volunteer candidates. It is worth mentioning that volunteers may be present in these random users.
To enlarge our dataset, we also collected candidates from Quora by the breadth-first-search method. Particularly, we took advantage of both the follower and followee6 relations
3https://about.me/ 4http://quora.com/ 5http://about.me/developer/api/docs/ 6If A follows B, then A is B's follower and B is A's followee.

provided by Quora. Initially, we selected two popular users as the seed users and then explored all their neighboring connected users. We applied similar exploration approach to all other non-seed users. In the end, we collected 172, 235 users' profiles and only retained those who have accounts in Facebook, Twitter and LinkedIn.
5.2 Ground Truth Construction
Based on these candidates, we launched a crawler to collect their historical social contents, including their basic profiles, social posts and relations. However, the traditional web-based crawler is not applicable to Facebook due to its dynamic loading mechanism. We thus resorted to the Selenium7 to simulate users' click and scroll operations on a FireFox browser and load users' publicly available information. We limited the access rate to one request per second to avoid being blocked by the robot checkers. It is worth mentioning that the data we collected is all publicly available. On the other hand, due to the privacy constraint, we could not access uses' social relations in Facebook and LinkedIn. We hence only collected users' followee relations in Twitter.
In order to improve the quality of our dataset, we employed three annotators to finalize our ground truth. As users tend to provide more complete and reliable profiles in LinkedIn, we guided the annotators to study the LinkedIn profiles of candidate users, and determine whether they are "volunteers" by majority votes. To ensure a uniformly labeling procedure, we provided them a piece of guideline. Given a user's LinkedIn profile, we classified the user as a volunteer if and only if this user lists his/her volunteer experiences in the section "Volunteer experience & Causes" or section "Experience". Candidates who do not satisfy the above two criteria were tagged as nonvolunteers. We focused on LinkedIn to determine whether users are volunteers because the volunteer experiences in LinkedIn are the most straightforward evidence to identify volunteers. It should be noted that those who do not mention their volunteer experiences in LinkedIn are not necessarily classified as "non-volunteers". However, the absence of these mentions, at least, reveals their limited interests and low enthusiasm in volunteerism. Therefore, in our work, we broadly defined users as "non-volunteers" if they do not mention their relevant volunteerism experiences in LinkedIn.
Table 1 lists the statistics of our dataset. We obtained the data for 1, 425 volunteers and 4, 011 non-volunteers according to the aforementioned strategies. The crawling was conducted between 22nd August to 11th September, 2013. Here we only selected a subset of non-volunteer data and made the dataset balanced to avoid the training bias. To facilitate this line of research, this dataset has been released after certain privacy preservation processing.
However, in reality, not all users are active enough on all social networks. To ensure the data quality, we treated those inactive users as missing with respect to a specific social network. Therefore, there exists block-wise missing data in our dataset. In particular, we treated a user as missing in Twitter or Facebook, if this user has less than 10 historical social posts. In addition, due to the absence of social post
7http://docs.seleniumhq.org/download/

218

Data

Table 1: Statistics of our dataset.

Volunteer

Nonvolunteer

Twitter profiles Twitter posts Twitter followees' profiles Facebook profiles Facebook posts LinkedIn profiles

1.5k 559k 902k 1.5k 83k 1.5k

4k 1m 3m 4k 338k 4k

mechanism in LinkedIn, we treated a user as missing8 in LinkedIn if the word count of this user's profile is less than 50. Figure 3 shows the statistics of our incomplete data. As can be seen, about 50% of users have complete data from all three social networks. 1% and 47% of users only miss the data either from Facebook and LinkedIn, while 2% of users miss the data from both of them.

Figure 3: Statistics of the incomplete data. Tw: Users with Twitter data only; Tw+Fb: Users with Twitter and Facebook data only; Tw+In: Users with Twitter and Linkedin data only; Tw+Fb+In: Users without missing data.
5.3 Features
To capture users' volunteerism tendency, we extracted a rich set of volunteer-oriented features.
5.3.1 Demographic Characteristics
The study in [19] reported that some demographic characteristics, such as education and income level, are strong indicators for volunteerism. This study inspires us to extract demographic characteristics from users' profiles, especially the Facebook and LinkedIn profiles. In our work, we explored users' demographic characteristics, including Gender, Relationship status, Education level, and Number of social connections.
5.3.2 Linguistic Features
We also extracted linguistic features, including Linguistic Inquiry and Word Count (LIWC) features, user topics and contextual topics.
LIWC features. LIWC is widely-used to analyze the psycho-linguistic transparent lexicon. It plays an important role in predicting users' personality [2, 15]. The main component of LIWC is a directory which contains the
8Here we exclude the contents of section "Volunteer experience & Causes" and section "Experience".

mapping from words to 72 categories9. Given a document, LIWC computes the percentage of words in each category and represents it as a vector of 72 dimensions.
To capture the key aspects of LIWC features, we selected the top 5 dimensions as the representative LIWC features according to the information gain ratio. Considering that the emotions for individuals may also affect users' volunteerism tendency, we additionally selected two categories from LIWC: positive emotion and negative emotion. Besides, we also utilized the positive-negative emotion ratio to further reflect users' emotional states. Let L(·) represent the percentage of users' words in certain LIWC category. The positive-negative emotion ratio is defined as,

P Nemo

=

L(pos)log

L(pos) L(neg)

+ +

p n

,

(23)

where p and n are introduced to avoid the situation: individuals have no positive or negative emotional word. They are both set as 0.0001. In total, we have 16 dimension LIWC features, extracted from Twitter and Facebook. User topics. According to our observation, volunteers may have, on average, a higher probability of talking about topics such as social caring or giving back, while the non-volunteers may mention other topics more often. This motivates us to explore the topic distributions of users' social posts to identify volunteers. We generated topic distributions using Latent Dirichlet Allocation (LDA) [4], which has been widely found to be useful in latent topic modeling [8, 23]. Based on perplexity [14] metric frequently utilized to find the optimal number of hidden topics, we ultimately obtained 52, 26, 42 dimensional topic-level features over users' Twitter, Facebook and LinkedIn data, respectively. Contextual topics. We define users' contextual topics as the topics of users' connections. We believe that the contextual topics intuitively reflect the contexts of users. "He that lies down with dogs must rise up with fleas" tells us that the context significantly affects a user's tendency. Particularly, we studied followees and retweeting10 connections on Twitter because of their intuitive reflection of topics that users concern. As the bio descriptions are usually provided by users to briefly introduce themselves and may indicate users' summarized interests, we integrated the bios of a user's followees or those whose tweets are retweeted by this user into two kinds of bio documents, on which we further applied LDA model. We utilized the perplexity to fix the dimensions of topiclevel features over followees' bio documents and retweetings' bio documents as 40 and 20, respectively. In this work, we only explored the contextual topics in Twitter, since we were unable to crawl the connections' profiles in LinkedIn and the bio descriptions are usually missing in Facebook.

5.3.3 Behavior-based features
This kind of features is characterized by users' posting behavior patterns and networking behavior patterns. The former focuses on the written style of users' social posts, while the latter captures their egocentric network features.

Posting behavior patterns. Posting behavior patterns have been investigated in many scenarios, spanning from
9http://www.liwc.net/ 10If A broadcasts a tweet posted by B, then B is A's a retweeting user.

219

age estimation to social spammers discovery [3, 13]. These patterns can be used to depict users' participation in information diffusion, which correlates with volunteerism tendency much.
On one hand, we employed the fraction of users' posts containing certain behaviors, including emoticons, slang words, acronyms, hashtags, URLs, and user mentions, to intuitively reflect users' engagement in topic discussion and social interaction. On the other hand, we observed that users' posting behaviors in social networks can be classified into a few categories. For example, posts in Twitter can be classified into two categories, Ctw = {tweets, retweets}, while posts in Facebook can roughly be split into eight types: Cfb = {share link, share sideo, share status, share photo, change photo, repost, post, tagged}. The distributions over users' posts on these categories also reflect their participation in information diffusion, revealing whether a given user tend to share information in social networks. When it comes to Linkedin, we utilized the profile completeness to characterize users' behaviors. Based on our observation, we found that volunteers tend to provide more information for all the sections. This not only reflects volunteers' active participation in LinkedIn but also signals their selfconfidence and openness to public. Profile completeness is defined as a boolean vector over six dimensions to denote the presence of the six common sections in LinkedIn profiles: summary, interest, language, education, skill and honor. We excluded the sections on experience and volunteer experience & causes, because the ground truth is built on these two sections. Egocentric network patterns. We also studied users' social behaviors from their egocentric networks. Intuitively, we believe that users belong to certain class tend to be connected with several class-specific accounts, as it goes for that "birds of a feather flock together". Therefore, volunteers should interact with some typical accounts in social media. The set of typical accounts is denoted as F V . Inspired by [17], we measured the degree of a user's correlation with volunteerism by three features: the frequency and fraction of a user's "friends" that belong to F V as well as the total number of "friends". In particular, we treated both the followees and retweetings as the "friends" of users in Twitter.
To construct the F V , we utilized the Twitter profile repository Wefollow11, which allows us to find the most prominent people given a particular category. By crawling prominent users falling into categories of Nonprofit, Charity, Volunteer, NGO, Community Service, Social Welfare and Christian from Wefollow, we obtained 23, 285 accounts.
6. EXPERIMENTS
We conducted extensive experiments to comparatively verify our proposed scheme from various angles over a system equipped with Intel i72.60 GHz CPU, and 8 GB memory. In particular, we launched 10-fold cross validation for each experiment, and reported the average performance. Each fold involves 2, 249 training and 250 testing samples.
6.1 On Model Comparison
We compared MSNL with four baselines. Before that, the data was completed by MSNDC. We also performed significant test to validate the effectiveness of MSNL.
11http://wefollow.com/

SVM: We chose the learning formulation with the kernel

of radial-basis function. We implemented this method based

on LIBSVM [6].

RLS: Regularized least squares model [11] aims to

minimize

the

objective

function

of

1 2N

y - Xw

2

+

 2

w

2.

In fact, the RLS model can be deduced from MSNL via

the

settings

of



=

[

1 S

,

1 S

,

·

·

·

,

1 S

]T

,

µ

=

0

and



=

0.

iSFS: The third baseline is the incomplete source-feature

selection model proposed in [24]. This model only assigns

weights to models learned from different social networks but

ignores the relationships among them. We can derive iSFS

from MSNL by making µ = 0.

regMVMT: The fourth baseline is the regularized multi-

view multi-task learning model [28]. This model only

regulates the relationships among different views but fails

to take the source confidence into account. We can derive

regMVMT

from

MSNL

by

making



=

[

1 S

,

1 S

,

·

·

·

,

1 S

]T

.

Table 2: Performance of different models(%).

Approaches

F1-measure

P-value

SVM RLS regMVMT iSFS MSNL

83.11 82.82 84.07 84.72 85.59

0.038 0.025 0.173 0.281
-

Table 2 shows the performance comparison between baselines and our proposed MSNL. We noticed that MSNL significantly outperforms the SVM and RLS. This implies that the information on multiple social networks are complementary and characterize users' volunteerism tendency consistently. This also proves that the correlations of different social networks with the task of volunteerism tendency prediction cannot be treated equally. In addition, MSNL achieves better performance, as compared with iSFS and regMVMT, which are the derivations of MSNL. This demonstrates that both the source confidence and the source consistency deserve particular attention.
6.2 On Data Completion Comparison
We further evaluated the component for missing data completion with the following three baseline methods.
Remove: This method eliminates all data samples that are not complete.
Average: This method imputes the missing features with the average values of the corresponding feature items.
KNN: The missing data is inferred by averaging its Knearest neighbors. K is experimentally set as 1.

Table 3: Performance of different models over

different data completion strategies.

Approaches

SVM

RLS

MSNL

Remove Average
KNN MSNDC

74.91 82.09 82.60 83.11

74.66 81.99 82.22 82.82

81.81 85.43 85.55 85.59

Table 3 shows the performance of different models over different data completion strategies. It can be seen that MSNDC outperforms the other strategies. Additionally, removing all incomplete data samples achieves the worst performance, which may be caused by the fact that it introduces training bias, making the dataset unbalanced and

220

reduces the size of training dataset. We found that the percentage of volunteer samples decreases from 50% to 40% after filtering out all incomplete data samples.
6.3 On Feature Comparison
To examine the discriminative features we extracted, we conducted experiments over different kinds of features using MSNL. We also performed significant test to validate the advantage of combining multiple social networks. Table 4 comparatively shows the performance of MSNL in terms of different feature configurations. It can be seen that the linguistic features achieves the best performance, as compared against demographic characteristics and behaviorbased features. This reveals that volunteerism tendency is better reflected by their social contents, including their own social posts and the self-descriptions of their social connections. This also implies that users with volunteerism

Table 4: Performance of different features(%).

Features

F1-measure

Demographic characteristics Linguistic features
User topics Contextual topics LIWC Behavior-based features Posting behavior patterns Egocentric network patterns

68.43 80.06 75.04 78.14 68.48 78.52 69.83 75.91

tendency may talk about related topics and follow or retweet related social accounts. In addition, we found that contextual topics are more discriminative as compared to users' own topics. This may be due to the fact that users' self-descriptions are of more value and contain less noise than users' tweets. Some hot topics discussed by volunteers are given in Table 5. Besides, the egocentric network patterns also play a dominant role in our task. This implies that one's social connections indeed reflect the user's personal concerns to a large extent.

6.4 On Source Comparison
To demonstrate the descriptiveness of multiple social network integration, we conducted experiments over various source combinations. Notably, data from Facebook and LinkedIn is incomplete and we need to infer the block-wise missing data first taking advantage of the complete data samples from Twitter.
Table 6 shows the performance of MSNL over different social network combinations. We noted that the more sources we incorporate, the better the performance can be achieved. This implies the complementary relationships rather than mutual conflicting relationships among the sources. Moreover, we found that aggregating data from all these three social networks can achieve significantly better performance as compared to each of the single source. Additionally, as the performance obtained from different single social networks are not the same, this validates that incorporating the confidence of different social networks to MSNL is reasonable. Interestingly, we observed that MSNL over Twitter alone achieves the much better performance, as compared to that over LinkedIn or Facebook alone. This may be caused by the fact that the

Table 5: Hot topics discussed by volunteers. Followee and retweeting: contextual topics; Self: user topics.
Data source Topic words

Followee Retweeting Self

· public, politics, rights, development · editor, global, journalist, university · global, nonprofit, change, community · health, education, learning, university · woman, help, education, child · volunteer, nonprofit, support

most discriminative features evaluated by Section 6.3 are all extracted from Twitter.

Table 6: Performance of different social network combinations(%). Facebook and LinkedIn both

refer to the complete data, whose missing data is

pre-inferred. F1: F1-measure.

Social network combinations

F1

p-value

Twitter Facebook LinkedIn Twitter+Facebook Twitter+LinkedIn Facebook+LinkedIn Twitter+Facebook+LinkedIn

82.35 73.53 74.49 83.67 83.84 76.29 85.59

4.2e-2 5.0e-7 3.1e-7 1.1e-1 1.4e-1 6.0e-6
-

6.5 Size Varying of Positive Samples
In order to verify the usefulness of our model on real world dataset, where the volunteers should account for a minority portion of user population, we tuned the fraction of volunteer samples in our dataset. In particular, we fed x%, x  [5, 50], of volunteer samples to our model with stepsize 5%. Figure 4 shows the F1-measure with respect to different fraction of volunteer samples of different models. As can be seen, our model can achieve satisfactory performance even when volunteer samples only accounts for 5% of the whole samples. This demonstrates that the proposed MSNL model is not sensitive to the percentage of positive samples. Whereas, SVM and RLS are relatively more sensitive to the fraction of volunteer samples in dataset.

Figure 4: F1-measure at different fraction of volunteer samples.
6.6 Complexity Discussion
In order to analyze the complexity of MSNL, we need to solve the time complexity in terms of constructing M, L and t as defined in Eqn. (8) and Eqn. (12), and computing the inverse of M and L. Assume D  S, the construction of matrix M has a time complexity of

221

O(N DS), and the construction of matrix L has a time complexity of O(N D2). Due to the fact that the cost of matrix multiplications (XTs Xs ) and that of constructing t involved in Eqn. (12) remain the same for all iterations and L is symmetric, we can save much practical time cost. Also, using the standard method, computing the inverse of two core matrices, M and L, has the complexity of O(S3) and O(D3), respectively. Furthermore, using the method of Coppersmith and Winogard, the time cost can be bounded by O(S2.376) and O(D2.376) [26], respectively. We note that the speed bottleneck lies in the number of features and the number of social networks instead of the number of data samples. As S and D are usually small, especially S, MSNL should be efficient in time complexity.
To validate the practical efficiency of the proposed MSNL model, we conducted a set of experiments. The comparison of average time consumption of different models is shown in Table 7. As can be seen, MSNL shows superiority over SVM in terms of the time cost, which takes only 19% of the time that SVM uses. By careful observation, we observed that MSNL converges very fast, which on average takes about 20 iterations. Even though MSNL takes more time than RLS and regMVMT due to the consideration of source consistency and source confidence, it improves the performance in terms of F1-measure.

Table 7: Time cost of different models (%).

Approach

Total (s) Train (s) Test (s)

SVM RLS regMVMT iSFS MSNL

2.0550 0.0639 0.0605 0.5565 0.3936

1.8211 0.0631 0.0595 0.5557 0.3929

0.2339 0.0008 0.0006 0.0008 0.0007

7. CONCLUSIONS AND FUTURE WORK
This paper presented a novel scheme for multiple social network learning. This scheme takes the source confidence and source consistency into consideration by introducing regularization to the objective function. We further demonstrated that the proposed scheme, designed for complete data, is also able to handle the real and more challenging cases where there exists block-wise missing data. In particular, before feeding the data into the proposed MSNL model, we inferred the missing data via NMF technique. Furthermore, we practically evaluated the proposed scheme in an interesting scenario of volunteerism tendency prediction. We developed a set of volunteeroriented features to characterize users' volunteerism tendency. Experimental results demonstrated the effectiveness of our proposed scheme and verified the advantages of utilizing multiple social network over a single source. Currently, we only consider solving a single task in the proposed scheme. In the future, we will extend our work to the context of multiple task learning.

Acknowledgments
This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.

8. REFERENCES
[1] F. Abel, E. Herder, G.-J. Houben, N. Henze, and D. Krause. Cross-system user modeling and personalization on the social web. UMUAI, 2013.
[2] B. Bazelli, A. Hindle, and E. Stroulia. On the personality traits of stackoverflow users. In ICSM, 2013.
[3] F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida, and M. Gonc¸alves. Detecting spammers and content promoters in online video social networks. In SIGIR, 2009.
[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 2003.
[5] D. Carmel, N. Zwerdling, I. Guy, S. Ofek-Koifman, N. Har'El, I. Ronen, E. Uziel, S. Yogev, and S. Chernov. Personalized social search based on the user's social network. In CIKM, 2009.
[6] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. TIST, 2011.
[7] Y. Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu. Visual-textual joint relevance learning for tag-based social image search. TIP, 2013.
[8] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity recognition in query. In SIGIR, 2009.
[9] C.-J. Hsieh and I. S. Dhillon. Fast coordinate descent methods with variable selection for non-negative matrix factorization. In SIGKDD, 2011.
[10] S.-Y. L. Y. Jiang and Z.-H. Zhou. Partial multi-view clustering. In AAAI, 2014.
[11] S. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. A method for large-scale l1-regularized least squares problems with applications in signal processing and statistics. J-STSP, 2007.
[12] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature.
[13] K. Lee, J. Caverlee, and S. Webb. Uncovering social spammers: social honeypots+ machine learning. In SIGIR, 2010.
[14] D. Li, B. He, Y. Ding, J. Tang, C. Sugimoto, Z. Qin, E. Yan, J. Li, and T. Dong. Community-based topic modeling for social tagging. In CIKM, 2010.
[15] D. Markovikj, S. Gievska, M. Kosinski, and D. Stillwell. Mining facebook data for predictive personality modeling. In ICWSM, 2013.
[16] L. Nie, Y.-L. Zhao, X. Wang, J. Shen, and T.-S. Chua. Learning to recommend descriptive tags for questions in social forums. TOIS, 2014.
[17] M. Pennacchiotti and A.-M. Popescu. Democrats, republicans and starbucks afficionados: user classification in twitter. In SIGKDD, 2011.
[18] L. A. Penner. Dispositional and organizational influences on sustained volunteerism: An interactionist perspective. JSI, 2002.
[19] L. A. Penner. Volunteerism and social problems: Making things better or worse? JSI, 2004.
[20] D. Quercia, M. Kosinski, D. Stillwell, and J. Crowcroft. Our twitter profiles, our selves: predicting personality with twitter. In SocialCom, 2011.
[21] D. Quercia, R. Lambiotte, D. Stillwell, M. Kosinski, and J. Crowcroft. The personality of popular facebook users. In CSCW, 2012.
[22] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized search. In CIKM, 2005.
[23] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In SIGIR, 2006.
[24] S. Xiang, L. Yuan, W. Fan, Y. Wang, P. M. Thompson, and J. Ye. Multi-source learning with block-wise missing data for alzheimer's disease prediction. In SIGKDD, 2013.
[25] L. Yuan, Y. Wang, P. M. Thompson, V. A. Narayan, and J. Ye. Multi-source learning for joint analysis of incomplete multi-modality neuroimaging data. In SIGKDD, 2012.
[26] D. Zhai, H. Chang, S. Shan, X. Chen, and W. Gao. Multiview metric learning with global consistency and local smoothness. TIST, 2012.
[27] D. Zhang, F. Wang, and L. Si. Composite hashing with multiple information sources. In SIGIR, 2011.
[28] J. Zhang and J. Huan. Inductive multi-task learning with multiple view data. In SIGKDD, 2012.
[29] X. Zhu, Z.-Y. Ming, X. Zhu, and T.-S. Chua. Topic hierarchy construction for the organization of multi-source user generated contents. In SIGIR, 2013.

222


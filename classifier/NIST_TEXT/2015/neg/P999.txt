LBMCH: Learning Bridging Mapping for Cross-modal Hashing

Yang Wang, Xuemin Lin, Lin Wu , Wenjie Zhang and Qing Zhang
The University of New South Wales, Kensington, Sydney, Australia
Australian Centre for Visual Technologies, The University of Adelaide, SA, Australia Australian E-Health Research Centre, Brisbane, Australia
{wangy,lxue,zhangw}@cse.unsw.edu.au, lin.wu@adelaide.edu.au, qing.zhang@csiro.au

ABSTRACT
Hashing has gained considerable attention on large-scale similarity search, due to its enjoyable efficiency and low storage cost. In this paper, we study the problem of learning hash functions in the context of multi-modal data for cross-modal similarity search. Notwithstanding the progress achieved by existing methods, they essentially learn only one common hamming space, where data objects from all modalities are mapped to conduct similarity search. However, such method is unable to well characterize the flexible and discriminative local (neighborhood) structure in all modalities simultaneously, hindering them to achieve better performance.
Bearing such stand-out limitation, we propose to learn heterogeneous hamming spaces with each preserving the local structure of data objects from an individual modality. Then, a novel method to learning bridging mapping for cross-modal hashing, named LBMCH, is proposed to characterize the cross-modal semantic correspondence by seamlessly connecting these distinct hamming spaces. Meanwhile, the local structure of each data object in a modality is preserved by constructing an anchor based representation, enabling LBMCH to characterize a linear complexity w.r.t the size of training set. The efficacy of LBMCH is experimentally validated against real-world cross-modal datasets.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Hashing; Multi-modal Data; Bridging Mapping
1. INTRODUCTION
Hashing is dramatically efficient for approximate nearest neighbor search while enjoying the low storage cost of loading low dimensional hash codes. It, hence, inspired numer-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767825.

ous hashing methods over single-modal data, which is described within only one feature space.
Figure 1: (a) Conventional methods learn the hashing functions to map all modalities into one common hamming space (b) Our proposed technique learns individual hamming space to well preserve the local structure for each modality.
With the rapid growth of information technology, significant efforts are shifted to learning hashing functions from single-modal hashing to Cross-modal hashing, which is more widely seen in real-life application over multiple data sources as multi-modalities [9, 10, 8, 11, 12, 7], while characterize the same semantics, with each individual data source corresponding to one modality, where a query from one modal is issued to search semantically relevant results from another modal [14]. Typical example is image search via a text query to return the images characterizing the similar semantics as text query. There have been a lot of approaches, where the basic idea of learning cross-modal hashing functions is relying on canonical correlation analysis (CCA) [4]. Specifically, [5] extended conventional spectral hashing [13] from single modality to multi-modal data, and learnt a common hamming space in which the local structure of all modalities can be well preserved. Motivated by anchor graph hashing [6], Zhu et al. [16] proposed a linear cross-modal hashing method against anchor based representation regarding each modality. To effectively preserve local structure, [15] proposed a novel parametric method to learn an individual projection matrix for each data object.
Despite the progress achieved by these approaches, one fundamental limitation is that they learn only one common hamming space, which is unable to simultaneously characterize the flexible and discriminative local structure of each individual modal, or even ignore the local structure preservation, hindering them to achieve better performance.
Bearing the limitation above, we propose a novel technique aimed at Learning Bridging Mapping for Cross-modal Hashing, named LBMCH, to learn an individual hamming

999

space adaptive to local structure of each modal, while automatically learn a bridging mapping to preserve cross-modal semantic correspondence by seamlessly connecting distinct hamming spaces.
Our proposed method is orthogonal to recent state-of-thearts with featured contributions summarized below
· We adaptively learn distinct hamming spaces to characterize discriminative local structure for each modal. The bridging mapping is automatically learned to characterize the cross-modal semantic correlation by connecting these hamming spaces.
· We formulate the problem of learning bridging mapping for cross-modal hashing, and propose a novel twostage optimization technique to achieve the goal, featured with a sub-linear time complexity with respect to the size of training samples.
· Extensive experiments are conducted over real-world datasets, which demonstrate the superiority of LBMCH over state-of-the-art methods.

2. LEARNING BRIDGING MAPPING FOR CROSS-MODAL HASHING
In this section, we focus on two modalities, which can be easily extended to multiple modalities.

2.1 Problem Definition

Assume Nq is the number of training data in the qth modal, denoted as X(q)  RNq×dq , where dq is the dimen-
sions for qth modal. x(iq) is the ith training data object from the qth modal. Npq is the number of observed semantic correspondence between training data objects in the pth and qth modalities. Without relying on semantic labels of train-
ing data, we study a more practical case with training data
unlabeled. Unless otherwise specified, given any matrix A, Aij represents the entry at the ith row and jth column of
A. Given x(iq), we map it to k-dimensional representation zi(q), with its jth dimension encoding the Euclidean distance between x(iq) and the jth anchor, a(jq) (j = 1, . . . , k), denoted
as zi(jq). We make this fit Gaussian distribution, formulated as

p(ijq) =

exp(-zi(jq)/) ,

k l=1

exp(-zi(lq)/)

(1)

where p(ijq) represents the refined representation based on zi(jq),  is the bandwidth parameter controlling the decay rate of zi(jq), which can be defined as the largest distance to all its k anchors. We further represent x(iq) as (iq) = [p(ijq), · · ·, pi(kq)]. We only keep the value for the s nearest anchors (s k)
with others being 0, then we can normalize s positive values to refine the representation. Collecting all (iq)(i = 1, ···, Nq) to be the columns of Z(q), we have Z(q)  Rk×Nq .
Instead of learning only one common hamming space,
LBMCH is to learn hashing functions characterized by Wp and Wq for the pth and qth modalities, which can map train-
ing data objects into distinct hamming spaces with mp and
mq dimensions i.e., code length, respectively, such that mp
and mq may be different. A bridging mapping matrix i.e., M  Rmp×mq , is learnt to map the hash code of query from

the pth modal to search semantically relevant result within the hamming space from qth modal.

2.2 Learning Bridging Mapping to Preserve Cross-modal Semantic Correlation
We rewrite two hashing projection matrices Wp and Wq to be Wp = [wp1, · · ·, wpmp ]T and Wq = [wq1, · · ·, wqmq ]T , where each column of Wp and Wq generates one bit of hash code for the pth and qth modal. Then we attempt to learn a bridging mapping matrix, M, to map the hash codes from mpdimensional hamming space to mq-dimensional hamming space or vice versa, by utilizing the cross-modal semantic correlation as provided by training data objects. That is, the cross-modal semantically related data objects should have similar hash codes after mapping. Besides, we expect the bridging mapping matrix to have a good generalization capacity simultaneously. Then we propose to minimize the following objective function

Npq

mq

Si(jpq)

mp

2

Mhl(wpl )T i(p) - (wqh)T j(q) + ||M||2F ,

i,j

h=1 l=1

(2) where Mhl is the lthentry of the hth row of M. Spq encodes

the semantic correlation between training data objects from

two modalities. Sipjq = 1 indicates that xi(p) is semantically similar to xj(q), it is 0 otherwise. The regularization term ||M||2F is leveraged into Eq. (2) to achieve the good generalization power.

Now we are ready to propose the overall objective function

to learn hash functions Wp and Wq, for both modalities and

the bridging mapping M, simultaneously.

2.3 Overall Objective Function
Given the pth and qth modality, we propose to learn Wp,Wq, and M by minimizing the objective function below.

F (Wp, Wq, M) =

T r(Wj Z(j)L(j)(Z(j))T WjT )

j{p,q}

+

µ 2

Npq

Si(jpq)

mq

i,j

h=1

mp

2

Mhl(wpl )T (ip) - (wqh)T (jq)

(3)

l=1

+

 2

||M||2F

s.t. WpZ(p)(Z(p))T WpT = NpImp

WqZ(q)(Z(q))T WqT = NqImq ,

where µ and  are trade-off parameters, mp and mq denote the hash code length for the pth and qth modal. Imp and Imq represent the identity matrix with the size of mp × mp and mq × mq. Unlike existing methods that narrowly consider only one hamming space, we account for a generalized
case adaptive to different modalities, while bridged by map-
ping M. Next, we will discuss the optimization solution to
Eq. (3).

2.4 Two-Stage Sequential Optimization Strategy
Minimizing Eq. (3) is not jointly convex to Wp, Wq and M. Hence, we turn to propose a gradient descent method [2] to minimize Eq. (3), seeking the local optimization for each variable while fixing others.

1000

Motivation. Unlike conventional methods [15] by conducting one-to-one bit of hash code learning fashion (e.g., learning only the pth bit for two modalities at each iteration) alternatively for two modalities regarding one common hamming space, we consider a more generalized case of learning heterogeneous hamming spaces via many-to-many bit relationship, characterized by bridging mapping M, which is more practical yet challenging. As we observed, the second term in Eq. (3) indicates that one bit of hash code from the pth modal is related to all bits from the qth modal, and vice versa. Hence, without loss of generality, we first initialize all mq bits from qth by using spectral hashing [13], while initializing all Mhl to be 1. Particularly, we conduct a sequential optimization strategy, which comprises of two stages below.
· Sequentially learn each hash bit for the pth modal.

· Sequentially learn each bit of hash code for the qth
modal, and each entry Mhl in M based on all the mp hash bits from the pth modal learned in the first stage

Why learn all bits from qth modal and Mhl at the

second stage? To answer this question, we first derive the

gradient

descent

for

Mhl

by

calculating

F  Mhl

,

formulated

as

F Mhl

Npq

mp

=µ

Si(jpq)(wpl )T (ip)( Mhl(wpl )T (ip)

i,j

l=1

(4)

- (wqh)T (jq)) + Mhl.

Then we update Mhl by one step gradient descent. Remark-

ably,

F  Mhl

relies

on all hash

bits

from the

pth

modal and

the hth hash bit from the qth modal, implied by (wqh)T j(q).

Thus, we can learn each entry Mhl upon the hth bit i.e.,

wqh from the qth modal, and all mp bits of hash code at the

second stage.

2.4.1 Sequentially learning each bit from the pth modal

Assume we have learned the previous l - 1 bits, then we can learn the lth bit by calculating the derivative with respect to wpl , that is,

Npq

mq

El(p)wpl + µ

Si(jpq)

Mhl(ip)((ip))T wpl

i,j

h=1

Npq

mq

=µ

Si(jpq)

(ip)((jq))T wqh + pK(lp)wpl ,

(5)

i,j

h=1

where

· El(p) = Zl(p)L(lp)(Zl(p))T  Rk×k, L(lp) = Dl(p) - Sl(p) and Sl(p) = (Zl(p))T (Zl(p)).

· Zl(p) = Z(p)-

l-1 k=1

wpk(wpk)T Z(p)

and

K(l p)

=

Zl(p)(Zl(p))T .

Remarkably, all the above variables are updated and deter-

mined by Zl(p), and Zl(p) = Z(p) -

l-1 k=1

wpk (wpk

)T

Z (p)

is

formed by removing the projections of Z(p) on the subspace

spanned by wpk(k = 1, ··, l - 1), thus to encourage bits de-

correlation. To learn the optimum wpl w.r.t. the lth bit of

hash code, we have

wpl

Npq

= (El(p) + µ

Si(jpq)

mq

Mhl (ip) ((ip) )T

- p K(lp) )-1 Hq ,

(6)

i,j

h=1

where Hq = µ

Npq i,j

Si(jpq)

mq h=1

(ip) ((jq) )T

wqh.

2.4.2 Sequentially learning each bit from the qth modal and M
Assume we have learned the previous h - 1 bits, and we are ready to learn the hth bit by calculating the derivative with respect to wqh, then we have

Npq

Eh(q)wqh + µ

Si(jpq)j(q)(j(q))T wqh

i,j

Npq

mp

=µ

Si(jpq)

Mhlj(q)((ip))T wpl + q Kh(q)wqh,

(7)

i,j

l=1

where

· Eh(q) = Zh(q)Lh(q)(Zh(q))T , Lh(q) = Dh(q) - Sh(q) and Sh(q) = (Zh(q))T (Zh(q)).

· Zh(q) = Z(q)-

h-1 k=1

wqk(wqk)T Z(q)

and

Kh(q)

=

Zh(q)(Zh(q))T .

The variables above are updated due to Zh(q), and Zh(q) =

Z(q) -

h-1 k=1

wqk (wqk )T

Z (q)

is

formed

by

removing

the

projec-

tions of Z(q) on the subspace spanned by wqk(k = 1, ··, h - 1),

to encourage bits de-correlation. To learn the optimum wqh

w.r.t. the hth bit of hash code, we have

Npq

wqh = (Eh(q) + µ

Si(jpq)j(q)(j(q))T - q Kh(q))-1Hp, (8)

i,j

where Hp = µ

Npq i,j

Si(jpq)

mp l=1

Mhl j(q) (i(p) )T

wpl .

Mean-

while, we learn Mhl via one step gradient descent by Eq. (4)

based on wql and all mp hash bits from the pth modal.

3. COMPLEXITY ANALYSIS AND DISCUS-
SIONS
The time complexity of learning each bit e.g., wpl , from the pth modal of running Eq. (6) comes from the inverse computation, which costs O(k3). The same cost holds for learning each bit e.g., wqh, from qth modality by the inverse computation from running Eq. (8). The cost for updating Mhl is
O(mpNpqk) by implementing Eq. (4). Therefore, the total complexity is O((mp + mq)k3 + mpmqkNpq + kNp + kNq) = O(max{kNp, kNq, (mp+mq)k3, mpmqNpqk}), where k is the
number of anchors. As k Np and k Nq, while mp and mq are the length of hash codes for pth and qth modalities,
such that mp k and mq k. Therefore, we expect m2j(j=p,q) < min{Np, Nq, Npq} and
k2 < min{Np, Nq, Npq}, which leads to mpmqk < k3 <
min{kNp, kNpq, kNq} < max{kNp, kNq}. Having k as a
constant, the time complexity can be rewritten as
O(max{Np, Nq}), which is linear to the number of training
samples on both modalities.

4. EXPERIMENTS
We use Wiki 1 and NUS-WIDE 2 for cross-modal hashing with two tasks: retrieving relevant texts by using images as queries and retrieving images by using texts as queries.
The Wiki dataset consists of 2,866 Wikipedia documents, each of which contains one text-image pair. All documents
1http://www.svcl.ucsd.edu/projects/crossmodal/ 2http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm

1001

are labeled by one of 10 semantic categories. Each image is represented by a 128-dimensional bag-of-visual SIFT feature vector and each text is encoded by a 10-dimensional feature vector generated by Latent Dirichlet Allocation (LDA) [1]. Following [16], 2173 documents are partitioned as training pairs and remaining 693 documents form query set.
The NUS-WIDE database contains 269,648 labeled images crawled from Flickr, and is manually annotated with 81 categories. We prune it via selecting 186,577 image-tag pairs that belong to the ten largest concepts. The images are represented by 500-dimensional bag-of-visual words (BoVW) and the tags are represented by 1000-dimensional tag occurrence feature vectors. We randomly select 300 × 300 = 90, 000 image-tag pairs to be training pairs, with remaining to be query set for two tasks. Besides the training and query set, we also conduct out-of-sample experiments by randomly selecting the 2000 images from the remaining to be query set to search tag modal.
Following [16], we set 300 and 600 anchors i.e., k = 300 and k = 600 in Eq. (1), for both modalities on Wiki and NUS-WIDE datasets. Among k centroids, we set s = 3 nearest anchors to encode the local structure of each data object for both modalities. We set  = 0.01 in Eq. (3).
4.1 Competitors
We consider three state-of-the-art competitors: (1) CrossModal Similarity-Sensitive Hashing (CMSSH) [3] tackles cross-modal hashing by using Adaboost to sequentially construct hash function for each modality whilst only considering cross-modal similarity in one common hamming space without preserving local structures. (2) Cross-View Hashing (CVH) [5] extends spectral hashing to cross-modal hashing via CCA (Canonical Correlation Analysis) in one common hamming space. (3) Linear Cross-Modal Hashing (LCMH) [16] propose a cross-modal hashing using different anchor graphs, while preserving both intra-and-inter modal similarity in one common hamming space.
4.2 Performance Comparison

(a) 32 bits

(b) 64 bits

(c) 32 bits

(d) 64 bits

Figure 2: Precision-recall curve on Wiki dataset ((a),(b)) and NUS-WIDE dataset ((c),(d)) over searching text modality via image query and searching image modality via text query.

Results on NUS-WIDE Dataset. We show the result of two tasks on NUS-WIDE in Fig.2 (c)-(d). Obviously,

we can see the advantage of LBMCH by flexibly learning the bridging mapping for various hamming spaces instead of only one common space characterized by other methods.
Time complexity against training size. We evaluate the time complexity with varied training size on two databases, and report the results in Fig.3. It can be seen that the time cost of our method grows linearly with increasing training samples, which demonstrates the efficiency of LBMCH.

(a)

(b)

Figure 3: Time complexity against training size over two databases.

5. CONCLUSIONS
In this paper, we propose a novel technique by learning distinct hamming space so as to well preserve the flexible and discriminative local structure of each modality. Based on that, a bridging mapping is learned to seamlessly connect these individual hamming spaces for cross-modal hashing. Extensive experimental results over real-world datasets demonstrate the effectiveness and efficiency of our method. Acknowledgements. Xuemin Lin is supported by NSFC61232006, ARC DP120104168, ARC DP140103578, and ARC DP150102728. Wenjie Zhang is supported by ARC DP150103071 and ARC DP150102728.

6. REFERENCES

[1] [2] [3]
[4] [5] [6] [7] [8]
[9]
[10] [11] [12]
[13] [14] [15] [16]

D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. In NIPS, 2001.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
M. Bronstein, A. Bronstein, F. Michel, and N. Paragios. Data fusion through cross-modality metric learning using similarity-sensitive hashing. In CVPR, 2010.
H. Hotelling. Relations between two sets of variables. Biometrika, 28(3/4):321­377, 1936.
S. Kumar and R. Udupa. Learning hash functions for cross-view similarity search. In IJCAI, 2011.
W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing with graphs. In ICML, 2011.
Y. Wang, M. A. Cheema, X. Lin, and Q. Zhang. Multi-manifold ranking: Using multiple features for better image retrieval. In PAKDD, 2013.
Y. Wang, X. Lin, L. Wu, Q. Zhang, and W. Zhang. Shifting multi-hypergraphs via collaborative probabilistic voting. Knowledge and Information Systems, DOI 10.1007/s10115-015-0833-8.
Y. Wang, X. Lin, L. Wu, W. Zhang, and Q. Zhang. Exploiting correlation consensus: Towards subspace clustering for multi-modal data. In ACM Multimedia, 2014.
Y. Wang, X. Lin, and Q. Zhang. Towards metric fusion on multi-view data: a cross-view based graph random walk approach. In ACM CIKM, 2013.
Y. Wang, X. Lin, Q. Zhang, and L. Wu. Shifting hypergraphs by probabilistic voting. In PAKDD, 2014.
Y. Wang, J. Pei, X. Lin, Q. Zhang, and W. Zhang. An iterative fusion approach to graph-based semi-supervised learning from multiple views. In PAKDD, 2014.
Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS, 2008.
L. Wu, Y. Wang, and J. Shepherd. Efficient image and tag co-ranking: a bregman divergence optimization method. In ACM Multimedia, 2013.
D. Zhai, H. Chang, Y. Zhen, X. Liu, X. Chen, and W. Gao. Parametric local multimodal hashing for cross-view similarity search. In IJCAI, 2013.
X. Zhu, Z. Huang, H. T. Shen, and X. Zhao. Linear cross-modal hashing for efficient multimedia search. In ACM Multimedia, 2013.

1002


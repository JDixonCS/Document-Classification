Sign-Aware Periodicity Metrics of User Engagement for Online Search Quality Evaluation

Alexey Drutsa
Yandex Moscow, Russia
adrutsa@yandex.ru

ABSTRACT
Modern Internet companies improve evaluation criteria of their data-driven decision-making that is based on online controlled experiments (also known as A/B tests). The amplitude metrics of user engagement are known to be well sensitive to service changes, but they could not be used to determine, whether the treatment effect is positive or negative. We propose to overcome this sign-agnostic issue by paying attention to the phase of the corresponding DFT sine wave. We refine the amplitude metrics of the first frequency by the phase ones and formalize our intuition in several novel overall evaluation criteria. These criteria are then verified over A/B experiments on real users of Yandex. We find that our approach holds the sensitivity level of the amplitudes and makes their changes sign-aware w.r.t. the treatment effect.
Categories and Subject Descriptions: H.1.2 [User/Machine Systems]: Human information processing; H.5.2 [User interface]: Evaluation/methodology
General Terms: Measurement, Experimentation
Keywords: User engagement; online controlled experiment; periodicity; DFT; sign-aware metric; A/B test
1. INTRODUCTION
Online controlled experiments, or A/B testing, have become the state-of-the-art technique of improving web services based on data-driven decisions [10, 7]. An A/B test compares two variants of a service1 at a time by exposing them to two user groups and by measuring the difference between them in terms of a key metric (e.g., the revenue, the number of visits, etc.), also known as an overall evaluation criterion [8]. Many existing studies were devoted to an invention of new metrics [5, 9, 3] or to improvements of existing ones [2, 4] in order to make them more consistent with the long-term goals of the company [1, 6, 7] and to
1e.g., a current version of the service (the control variant) and a new one (the treatment variant)
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767814.

make their changes more detectable. The ability of the metric to detect the statistically significant difference when the treatment effect exists is referred to as the sensitivity.
User engagement metrics (e.g., the number of sessions per user [9], the absence time [5], etc.) are considered to be the best ones and are popular in the A/B testing practice of many companies, because user engagement reflects how often a user solves her needs (e.g., to search for something) by means of the considered service (e.g., a search engine) [6, 7]. Hence, on the one hand, these metrics are measurable in the short-term experiment period, and, on the other hand, they are predictive of long-term goals of the company [6, 7].
Recently, several metrics of user engagement periodicity were developed and were used in A/B experiments [3]. These metrics are the amplitudes of the discrete Fourier transform (DFT) of daily time series of several standard user engagement metrics (e.g., the daily number of sessions). On the one hand, the amplitude metrics were found to be more sensitive than the state-of-the-art ones, i.e., the average values of the time series (e.g., the number of sessions per day). However, on the other hand, the amplitudes (as considered in [3]) could not be used to determine, whether the treatment effect is positive or negative: if one of these metrics changes (increases or decreases) significantly, it just tells us that a user changes her behavior in response to the treatment variant with no insight on whether it is positive or negative for the user or for the service. Thus, these metrics are sign-agnostic with respect to an evaluated treatment effect.
In our work, we develop a novel approach, which improves several amplitude metrics by taking into account the phases of the DFT. We find that the DFT components with the first frequency detect signals of the UE measure trend along the experiment time period. Hence, these metrics could be profitable for delayed treatment effects (which are often caused by primacy and novelty effects [8, 6]). Our approach provides an experimenter with several intuitively clear guidelines (referred to as symptoms), that allow him to determine whether the treatment effect is positive or negative, when he or she observes changes in the periodicity components of a user engagement metric. Thus, we overcome the sign-agnostic issue of the amplitudes.
We reinforce our theoretical results by applying the proposed approach to evaluation of changes in a search engine. We considered 55 real large-scale online experiments (32 A/B tests and 23 A/A tests) run at Yandex, one of the popular search engines. We find that, on the one hand, the novel approach is more sensitive than the state-of-the-art user engagement metrics (it allows us to detect the treatment ef-

779

Figure 1: The amplitude A1, the phase 1, and the sine wave f 1.
fect in more experiments). On the other hand, the novel approach is sign-aware (it allows us to understand, whether the detected treatment effect is positive or negative), while the classic amplitude metrics in [3] are sign-agnostic.

2. PERIODICITY METRICS

We briefly review the key points of the periodicity metrics,

that were studied in [3]. Let us consider any user engage-

ment (UE) measure calculated for an individual user, e.g.,

the number of sessions [9, 3]. Let x = (x0, x1, .., xN-1) be

a daily time series of N numbers, that represent this UE

measure calculated for N consecutive days (e.g., the daily

number of sessions). Then, we apply the discrete Fourier

transform (the DFT ) to x and obtain the sequence of its coordinates in the harmonic basis {f k}Nk=-01:

N -1

Xk =

xn e-ik n ,

n=0

2k k = N ,

k  ZN ,

where f k = (eikn/N )nZN is the sine wave (harmonic) with the frequency k. From the polar form2 of each complex number Xk = |Xk|eik , the amplitude Ak = |Xk|/N and

the phase k are obtained, k  ZN . The amplitude Ak represents the magnitude of the sine wave f k with the fre-

quency k, presented in the series x, whereas the phase k

represents how this wave is shifted (see Fig.1).

The amplitudes Ak and the normalized ones Ak/A0, k 

ZN , are found to be considerably more sensitive to different

changes in a search engine than the state-of-the-art baseline

metric A03. However, these metrics do not determine the

sign of the treatment effect: whether the evaluated service

change is negative or positive for users (i.e., these metrics

are sign-agnostic). In this study, we will show how several

amplitudes could be refined in order to obtain sign-aware

evaluation criteria, that will also be of high sensitivity.

3. CHANGE IN ENGAGEMENT TRENDS
In A/B testing practice, we are often faced with primacy and novelty effects [8, 6], that cause a delay in the treatment effect, which could not be easily detected by the stateof-the-art metrics. We supposed that the metrics aimed to detect changes in the trend of UE measures may be helpful in such and other cases. For instance, if a user is enjoyed with the treatment version, then the number of sessions should growth during the experiment. Otherwise, if the treatment harms user engagement, the number of sessions should decrease to the end of the period. Such UE measure changes along the whole experiment period should leave traces in the first frequency4 (i.e., 1) of the DFT of the UE measure time series. Therefore, if the treatment effect stands
2i.e., z = rei = r cos  + r sin i 3A0 is the average value of the source time series x of the UE measure (e.g., the average number of sessions per day). 4The other frequencies (i.e., k, k > 1) are responsible for more frequent changes (e.g., the week periodicity, etc.).

Figure 2: A positive, a negative, and neutral trends of the sine wave f 1 w.r.t. the phase 1.
out sharply in the UE trend, then the sine wave with this frequency should change. Moreover, the shift of this sine wave can help to determine the sign of the treatment effect (whether it is positive or negative).
From here on in this paper we will consider only the first amplitude A1 and its normalized version A1/A0. In order to understand the meaning of a change in such amplitude, let us consider it together with the corresponding phase 1 of the sine wave f 1 with the frequency 1. Let us consider the following cases (see Fig.2):
(a) if 1  /2 (or sin 1  1), then the sine wave f 1 oscillates in x with a positive trend;
(b) if 1  -/2 (or sin 1  -1), then the sine wave f 1 oscillates in x with a negative trend;
(c) if 1  0 or  (or sin 1  0), then the sine wave f 1 oscillates in x with a neutral trend.
From these cases, we see that, for instance, an increase of the amplitude A1 may led to both negative, and positive consequences. It depends on how the sine wave is shifted: if sin 1 < 0, then the magnitude of the sine wave with a negative trend increases; on the contrary, if sin 1 > 0, then the magnitude of the sine wave with a positive trend increases. Furthermore, even through the amplitude A1 does not change at all, a change in the phase 1 could shift the type of the sine wave trend: make it more or less negative or positive. Therefore, we conclude that, in order to understand the positiveness or negativeness of changes in the amplitudes A1 and A1/A0 of the sine wave f 1, we should pay attention to its phase 1.
Before presenting in detail our evaluation criteria, let us dwell on two points. First, note that the equality ImX1 = N A1 sin 1 holds (see Fig.1). Therefore, from here on in this paper we will study the novel5 metric ImX1 and its normalized version ImX1/A0. Second, the straightforward approach to determine the trend of the N -day time series x is just a comparison of the first half of the series with the second one. Therefore, we will consider the difference D between the average value of the time series x over the last [N/2] days avg2x and the one over the first [N/2] days avg1x (i.e., D = avg2x-avg1x) as our baseline metric (besides the state-of-the-art metric A0).
Growth and fall symptoms. We refer to a case in an A/B test, when the trend of a considered UE measure becomes more positive (or less negative), as a growth symptom, while we refer to a case, when the trend becomes more negative (or less positive), as a fall symptom. Let avgAM and avgBM be the average values of the metric M over the users, exposed to the variant A and B, respectively, and M = avgBM - avgAM be the difference between the variants. Then, we summarize all symptoms under our study
5The authors of [3] considered only the amplitudes Ak and Ak/A0, k  ZN , as evaluation metrics.

780

Table 1: The baseline (BASE) and the novel (DFT) growth and fall symptoms.

Growth symptoms

Fall symptoms

Baseline (BASE): G0

D > 0

Gn0

D/A0 > 0

F0

D < 0

Fn0

D/A0 < 0

G1

ImX1 > 0 A1 = 0

Gn1

ImX1/A0 > 0 A1/A0 = 0

F1

ImX1 < 0 A1 = 0

Fn1

ImX1/A0 < 0 A1/A0 = 0

1 = 0

1 = 0

1 = 0

1 = 0

Novel (DFT):

G2

avgAImX1 > 0 A1 > 0

Gn2

avgAImX1/A0 > 0 A1/A0 > 0

F2

avgAImX1 > 0 A1 < 0

Fn2

avgAImX1/A0 > 0 A1/A0 < 0

1 = 0

1 = 0

1 = 0

1 = 0

G3 avgAImX1 < 0 Gn3 avgAImX1/A0 < 0 F3 avgAImX1 < 0 Fn3 avgAImX1/A0 < 0

A1 < 0

A1/A0 < 0

A1 > 0

A1/A0 > 0

Table 2: The studied user engagement measures.

S the number of sessions PT the presence time

Q the number of queries CpQ the number of clicks per query

C the number of clicks

ATpS the absence time per session

in Table 1 (where all equalities and inequalities are treated as statistically significant). The growth (fall) symptoms are denoted by the letter G (F). The superscript n denotes the symptoms based on the normalized versions of the metrics.
We will describe only the growth non-normalized symptoms (all other cases are similar). The baseline symptom G0 is simple: the condition D > 0 means that the average value over the second part of the time period tends to growth w.r.t. the one over the first part. The novel DFT symptom G1: the condition A1 = 0 means that the amplitude does not change, hence, the condition ImX1 > 0 infers that the phase 1 changes in the direction of /2 (see Fig. 2), i.e., the sine wave shift changes in the positive direction. The symptom G2: the condition 1 = 0 means that the shift of the sine wave f 1 does not change, the condition avgAImX1 > 0 means that this shift is positive relating to the trend of x, and A1 > 0 means that the magnitude of this sine wave increases. The intuition of the symptom G3 is similar.
Note, that the sign of a symptom (growth or fall) is the sign of a change in the trend of the UE measure during an A/B test. Therefore, it should not be confused with the sign of the treatment effect (positiveness or negativeness). The latter one depends on the UE measure under consideration: whether its increase is preferable or not. For instance, the increase of the number of sessions is a positive effect, but increase in the absence time [5] is negative effect. Thus, the sign of the symptom ought to be properly translated into the treatment effect sign for each UE measure individually.

4. EXPERIMENTS
Experimental setup. In our paper, we consider 32 A/B and 23 A/A experiments conducted on real users of Yandex in order to validate our approach. Each experiment lasted two weeks (N = 14), the user samples used in the A/B tests were all uniformly randomly selected, and the control and the treatment groups were almost of the same sizes (at least, hundreds of thousands of users). Each A/B experiment compared a production version of the search engine with its noticeable deterioration or its clear improvement. We apply a commonly used two-sample t-test with the threshold pval = 0.05 for the p-value (as in [2, 9, 3]). We study the 6 main user engagement measures (Table 2) with the same definitions as in [9, 3, 4].
A/A tests. First of all, 23 control experiments (i.e., A/A tests) were conducted in order to check correctness of the ex-

Table 3: The number of failed A/A tests (out of 23).

Metric A0 D D/A0 A1 A1/A0 ImX1 ImX1/A0

S01

2

1

1

2

2

Q03

2

0

0

1

2

C12

1

1

0

1

1

PT 1 2

3

1

2

2

2

CpQ 1 1

1

1

0

1

2

ATpS 0 2

2

1

1

1

1

Table 4: The number of A/B tests (out of 32) with detected treatment effect (+ the number of those of them, where it is not detected by A0).
Metric A0 D D/A0 A1 A1/A0 ImX1 ImX1/A0
S 2 0 (+0) 1 (+1) 3 (+2) 3 (+2) 0 (+0) 1 (+1) Q 1 2 (+2) 1 (+1) 3 (+2) 3 (+3) 1 (+1) 1 (+1) C 8 2 (+1) 1 (+1) 11 (+3) 2 (+1) 2 (+1) 1 (+1) PT 4 1 (+1) 2 (+1) 4 (+1) 1 (+1) 0 (+0) 2 (+1) CpQ 15 1 (+1) 1 (+0) 12 (+1) 2 (+1) 1 (+1) 1 (+0) ATpS 4 0 (+0) 0 (+0) 2 (+1) 3 (+2) 2 (+1) 0 (+0)

perimentation [8, 1]. An A/A test, which compares the same versions of the service, should be failed about 5% of the time for our p-value threshold 0.05 [8, 1]. The number of failed A/A tests for each metric and each UE measure is reported in Table 3. We found that, first, the results for the metrics A0, A1, A1/A0, ImX1, and ImX1/A0 are acceptable (the two latter ones have the borderline number of failed A/A tests). Second, the metrics D and D/A0 (which correspond to the baseline symptoms) failed an unacceptable number of A/A tests for the vast majority of the UE measures.
A/B tests. Table 4 summarizes the number of A/B experiments (out of 32) whose treatment effect is detected (i.e., pval < 0.05) by each metric and each UE measure (the best result in each row is highlighted in boldface). First, the measures C and CpQ demonstrate the highest sensitivity among all measures. Second, we see that the amplitude A1 and sometimes the amplitude A1/A0 outperform the baseline metric A0 by the number of A/B tests with detected treatment effect (i.e., they are more sensitive). Thus, we reproduce the findings of the study [3]. Moreover, we see that, in almost all cases of the UE measures, the metrics A1 and A1/A0 detect the treatment effect in those A/B tests, where it is not detected by the baseline metric A0 (see the values in brackets in Table 4). Unfortunately, it could be noted that the novel metrics ImX1 and ImX1/A0 are worse than both the amplitudes A1, A1/A0 and the average one A0. Thus, these novel metrics do not improve noticeably the sensitivity, but the main benefit from them will be seen further in detection of the growth/fall symptoms.
Detected symptoms: examples. We start our study from consideration of two A/B experiments as examples. For these A/B tests, the differences of all studied metrics, as well

781

Table 5: Examples of A/B tests with detected symptoms (significant differences (pval < 0.05) are highlighted).

# UE meas.

1

CpQ

ATpS

2

S

CpQ

A0 0.0032
1437 0.0493 -0.0012

D
-0.0009 23.3
0.0012 0.0005

A1
0.0004 -7.78 0.0001 3 · 10-5

ImX1 -0.0034 144.37
0.0008 -0.0039

avgA ImX1 -0.0119 -503.38 -0.0016 0.0085

D/A0 -0.0039
0.0004 0.0057 0.0068

A1 /A0 0.0009
-0.0001 -0.0024 -0.003

ImX1 /A0 -0.0014 0.0002 0.0025 0.0026

avgA ImX1/A0 0.0055
-0.0005 0.0033 0.0042

1 -0.003 0.005 -0.008 -0.017

Symptoms
F3 Fn0 Fn1 G1 Fn2 Fn2

Table 6: The number of growth and fall symptoms found in A/B tests (out of 32) for each UE measure and

the number of A/B tests with detected treatment effect by symptoms from BASE, DFT, or ALL sets.

UE measure

G0

G1

G2

G3

Gn0

Gn1

Gn2

Gn3

F0

F1

F2

F3

Fn0

Fn1

Fn2

Fn3

# of A/B tests with BASE DFT ALL

S0

0

0

0

1

1

1

0

0

0

0

2

0

0

1

0

1

5

5

Q1

0

0

1

1

1

1

0

1

1

2

0

0

0

1

0

3

7

8

C1

0

2

3

0

0

0

0

1

0

1

0

1

1

2

0

3

9

11

PT 1

0

0

1

1

1

0

0

0

0

1

2

1

1

1

0

3

6

7

CpQ 0

0

3

2

0

0

0

0

1

1

2

5

1

1

2

0

2 14 14

ATpS 0

1

1

1

0

0

1

1

0

1

0

0

0

0

1

0

0

5

5

as the average values of the metrics ImX1 and ImX1/A0 over the control group A are presented in Table 5.
The first A/B test evaluates a treatment, which is an artificial deterioration of the ranking algorithm of the search engine. We consider this experiment regarding the clicks per query measure CpQ and the absence time measure ATpS. We see that, first, the state-of-the-art metric A0 detects a significant increase of ATpS and CpQ for an average user (expected effects for this experiment). Second, for the measure CpQ, a statistically significant difference  is observed for several metrics: the baseline metric D/A0, the amplitude A1, and the novel Fourier coefficient ImX1/A0. These differences and the statistically significant positiveness of avgAImX1 yields to a detection of three fall symptoms: F3, Fn0 , and Fn1 (according to Table 1). Therefore, we conclude that the negative trend of the number of clicks per query per day stands out more sharply for an average user of the treatment variant B than of the control one. In this experiment, the novel growth symptom G1 is also observed for ATpS.
The second A/B test evaluates a deterioration of the user interface. The treatment effect of this experiment is not detected by the state-of-the-art metric A0 both for the number of sessions measure S and for the clicks per query measure CpQ (their differences are not statistically significant). However, the amplitude A1/A0 detects (i.e., pval < 0.05) the treatment effect in the both measures. The statistically significant positiveness of avgAImX1/A0 helps us to conclude that we observe the fall symptom Fn2 twice: both the trend of S, and the trend of CpQ are more negative in the treatment variant B than in the control one. Thus, we demonstrated how the novel metrics helped us to understand the sign of the observed change in the amplitudes, even though the state-ofthe-art metrics do not detect any treatment effect.
Detected symptoms: overall. Finally, Table 6 summarizes the number of detected symptoms overall 32 A/B experiments. The last three columns of this table report the number of A/B tests, whose treatment effects were detected by one of the baseline symptoms (i.e., G0, Gn0 , F0, or Fn0 : the col. "BASE"), by one of the DFT symptoms (i.e., Gk, Gnk, Fk, or Fnk, k = 1, 2, 3: the col. "DFT"), or by any symptom (the col. "ALL"), respectively. We conclude that, first, the novel symptoms allow us to detect noticeably more changes in the UE measure trends than the baseline symptoms, i.e. they are more sensitive. Second, being based on the amplitudes A1 and A1/A0, the novel symptoms are more sensitive than the state-of-the-art metric A0 (see Table 4), and, on the contrary to the single A1 or A1/A0 usage, could explain whether the treatment effect is positive or negative.

5. CONCLUSIONS AND FUTURE WORK
In this paper, we considered the problem of amplitude metrics that are well sensitive, but are not sign-aware w.r.t. an evaluated treatment effect. Since the DFT sine wave with the first frequency carries signals of the UE measure trend along the experiment time period, we found out that its phase could shed light on the correct understanding of changes of its amplitudes. We combined the amplitude metrics with the phase ones and formalized our intuition in several novel overall evaluation criteria (consisting of growth and fall symptoms of the UE measure trend). We verified our approach over 55 large-scale A/B experiments on real users of Yandex, one of the popular search engines. We found that, on the one hand, the most novel criteria outperformed the baseline and state-of-the-art metrics in terms of sensitivity. On the other hand, our approach refined the single amplitude metrics by making them sign-aware w.r.t. the treatment effect. As future work we can study the signawareness of the amplitudes with other frequencies.
6. REFERENCES
[1] T. Crook, B. Frasca, R. Kohavi, and R. Longbotham. Seven pitfalls to avoid when running controlled experiments on the web. In KDD'2009, pages 1105­1114. ACM, 2009.
[2] A. Deng, Y. Xu, R. Kohavi, and T. Walker. Improving the sensitivity of online controlled experiments by utilizing pre-experiment data. In WSDM'2013, pages 123­132. ACM, 2013.
[3] A. Drutsa, G. Gusev, and P. Serdyukov. Engagement periodicity in search engine usage: Analysis and its application to search quality evaluation. In WSDM'2015, pages 27­36, 2015.
[4] A. Drutsa, G. Gusev, and P. Serdyukov. Future user engagement prediction and its application to improve the sensitivity of online experiments. In WWW'2015, 2015.
[5] G. Dupret and M. Lalmas. Absence time and user engagement: evaluating ranking functions. In WSDM'2013, pages 173­182, 2013.
[6] R. Kohavi, A. Deng, B. Frasca, R. Longbotham, T. Walker, and Y. Xu. Trustworthy online controlled experiments: Five puzzling outcomes explained. In KDD'2012, pages 786­794. ACM, 2012.
[7] R. Kohavi, A. Deng, R. Longbotham, and Y. Xu. Seven rules of thumb for web site experimenters. In KDD'2014, 2014.
[8] R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne. Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18(1):140­181, 2009.
[9] Y. Song, X. Shi, and X. Fu. Evaluating and predicting user engagement change with degraded search relevance. In WWW'2013, pages 1213­1224, 2013.
[10] D. Tang, A. Agarwal, D. O'Brien, and M. Meyer. Overlapping experiment infrastructure: More, better, faster experimentation. In KDD'2010, pages 17­26. ACM, 2010.

782


Context- and Content-aware Embeddings for Query Rewriting in Sponsored Search

Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic,
Fabrizio Silvestri, Narayan Bhamidipati
{mihajlo, nemanja, vladan, silvestr, narayanb}@yahoo-inc.com
Yahoo Labs 701 First Ave, Sunnyvale, CA, USA 125 Shaftesbury Ave, London, England

ABSTRACT
Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through an issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within a search session. As a result, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search. The method was trained on more than 12 billion sessions, one of the largest corpus reported thus far, and evaluated on both public TREC data set and an in-house sponsored search data set. The results show that the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and monetization potential.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '15, August 09 - 13, 2015, Santiago, Chile Copyright 2015 ACM 978-1-4503-3621-5/15/08. http://dx.doi.org/10.1145/2766462.2767709 ...$15.00.

General Terms
Information Retrieval; Query Rewriting; Algorithms.
Keywords
Query rewriting; word embeddings.
1. INTRODUCTION
In recent years targeted advertising has become one of the largest and most lucrative advertising channels. Despite the fact that traditional offline advertising still accounts for the majority of advertising expenditures [34], future potential of this burgeoning field is clearly exemplified by reported ad revenue of over 20 billion dollars in the first half of 2013 in the US alone, combined with a remarkable growth of around 20% on a yearly basis [21]. The size and importance of the online advertising, as well as the interesting open questions that the scale and variety of the targeting tasks bring, has drawn attention of many researchers from both industry and academia, resulting in a number of novel methods and improvements to the flourishing field [1, 18, 32].
Due to a large diversity of the internet medium, targeted advertising has evolved to encompass many different outlets for the advertisers interested in reaching their target audience. These include behavioral targeting [12] (where users are targeted based on their general browsing behavior), email retargeting [18] (targeting users based on their e-mail interaction patterns), site re-targeting (targeting users based on their historical search queries), to name a few. In this work we consider a task of sponsored search advertising [17, 22], a very popular advertising model that targets users with the most relevant ads by considering an immediate query issued by a user. It has become one of the prevalent means of advertising due to a fact that the current query carries a strong signal about an immediate intent of the user, resulting in a highly effective targeting model [15].
In the sponsored search model each advertiser maintains a list of keywords that they deem relevant to their product (e.g., Nike may maintain a keyword list containing terms such as "running shoes" and "athletics"). In addition to a list of keywords, each advertiser also specifies a monetary bid amount they are willing to pay if their ad is shown and clicked by the user. Then, when a user issues a query in a search engine, the query is compared against each advertisers' keyword list, and all advertisers with the matching keyword are entered into an auction. Finally, according to advertiser's bid amount and the estimated quality and rel-

383

evance of an ad that they wish to show, one advertiser and their corresponding ad are chosen for user targeting. Value of the auction increases when number of matched advertisers and their bids are high, which directly results in a better ad quality and higher revenue for both advertisers and publishers (i.e., websites that host the ads).
Due to a large number of queries that the users could possibly issue, a common occurrence is that an exact query match cannot be found, as the advertisers usually cannot cover all relevant queries related to their product or service. However, even when the exact match does not exist, most often there does exist a non-matching keyword that is still highly related to the query. For example, query "purina one" and bidded keyword "dog food" are strongly related yet a string match would fail to make the connection, which directly translates into lost revenue. To mitigate this problem, query rewriting is used to expand the original query by providing K related ones for which ads are available, and which can be used instead to qualify advertisers for an auction [9, 23, 40]. In the above example, a query rewriting method can be used to rewrite "purina one" into related queries such as "dog food", "cat food", "purina pro plan", and others, thus increasing likelihood of retrieving more high-quality, relevant ads, likely to be clicked by a user that issued the query.
In this paper we address this critical step in sponsored search, and propose a novel query rewriting algorithm motivated by recent advances in distributed neural language models [14, 30, 31]. We explore and expand upon these approaches for the task of query rewriting, resulting in significant performance improvements over the current state-ofthe-art methods. Key contributions are summarized below:
· We describe an application of distributed language models to query rewriting, where we propose to use three novel methods for learning distributed, lowdimensional query representations that compactly capture their semantic meaning;
· We propose and discuss how to add ad clicks and search result clicks to query context, which allows specialization of representations for various tasks of critical interest to search engine companies (e.g., query suggestion, query-to-ad matching, query categorization), indicating even wider applicability of the method in the advertising domain;
· We trained and evaluated the models using more than 12 billion search sessions, resulting in rewriting results of high quality. Empirical results show that the proposed approach significantly outperform the existing state-of-the-art methods.
2. RELATED WORK
In this section we describe related work in the domains of query rewriting and neural language modeling that motivated the approach proposed in this work.
2.1 Query rewriting for sponsored search
Owing to the importance of query rewriting for the success of query-ad auctions, various algorithms have been proposed in the literature to address this critical step. These include graph-based methods such as Query Flow Graph (QFG) [6, 7] that learn from users' browsing behavior, as well as methods that exploit syntactical relationships between queries

[11, 23]. However, disadvantage of existing solutions is that they mostly do not take into account complex semantic relationships between queries, which may lead to suboptimal rewrites. For example, the rewrites typically result in the original queries with added or removed terms, such as "purina one" being rewritten as "purina" or "purina one pro". Thus, they are often obvious and have already been considered by the advertisers that targeted the original query, and do not add a significant value to the auction.
We note that query rewriting task is related to the problem of query suggestion, albeit the goals of the two techniques are quite different. In the case of query suggestion the objective is to provide users with queries that are important for meeting users' information needs [2, 7, 39], while in query rewriting the task is to expand query to increase both the number and quality of retrieved ads. We refer an interested reader to [36] for an overview of query log mining approaches as applied to query suggestion task.
2.2 Neural language models
In a number of natural language processing (NLP) applications, including information retrieval, part-of-speech tagging, chunking, and many others, the specific objective can be generalized to the task of assigning a probability value to a sequence of words. To this end, language models have been developed, defining a mathematical model to capture statistical properties of words and the dependancies among them [3, 27]. Traditionally, language models represent each word as a feature vector using one-hot representation, where a word vector has the same length as the size of a vocabulary, and a vector element that corresponds to the observed word is equal to 1, and 0 otherwise. However, this approach often exhibits significant limitations in practical tasks, suffering from high dimensionality of the problem and severe data sparsity, resulting in suboptimal performance.
Neural language models have been proposed to address these issues, inducing low-dimensional, distributed embeddings of words by means of neural networks [4, 13, 38]. Such approaches take advantage of the word order in text documents, explicitly modeling the assumption that closer words in the word sequence are statistically more dependent. Historically, inefficient training of the neural network-based models has been an obstacle to their wider applicability, being proportional to the size of the vocabulary which may grow to several millions in practical tasks. However, this issue has been successfully addressed with recent advances in the field, particularly with the development of highly scalable continuous bag-of-words (CBOW) and skip-gram (SG) language models [30, 31] for learning word representations. These powerful, efficient models have shown very promising results in capturing both syntactic and semantic relationships between words in large-scale text corpora, obtaining state-of-the-art results on a plethora of NLP tasks. More recently, the concept of distributed representations has been extended beyond word representations to sentences and paragraphs [14, 28], relational entities [8, 37], general textbased attributes [26], descriptive text of images [25], nodes in graph structure [33], and other applications going beyond NLP domain for which they were originally proposed.
3. PROPOSED APPROACH
To address the shortcomings of the existing state-of-theart methods for query rewriting, we propose to take a rad-

384

ically new approach to this task, motivated by the recent success of distributed language models in NLP applications [31, 38]. In the context of NLP, distributed models are able to learn word representations in a low-dimensional continuous vector space using a surrounding context of the word in a sentence, where in the resulting embedding space semantically similar words are close to each other [31]. Our objective is to take advantage of this property for the task of query rewriting, and to learn query representations in a lowdimensional space where semantically similar queries would be close. As a result, and in contrast to rewriting methods commonly used in practice, related queries could have a high similarity score even if they do not have any shared terms. Clearly, such approach would allow us to reduce complex task of query rewriting to a trivial K-nearest-neighbor search in the new embedding space.
However, application of distributed language models to the task of query rewriting is not an easy endeavour. Finding distributed query representation, as opposed to finding word representations, brings very unique challenges quite different from those found in everyday NLP problems. First, there are significant differences between language used in everyday language and web searches [10, 35]. For example, web search users often use summarization when searching for content (e.g., typing "vacations Spain" instead of "vacations in Spain"), thus omitting frequent words. Further, some ngrams that rarely appear in everyday language often appear together in web search [24], and queries are characterized by more spelling mistakes. Moreover, contrary to everyday language where linguistic rules and notions of words and sentences are clearly defined, search queries are composed of terms where there is no existing notion of "sentence of queries" or the surrounding context equivalent to natural language domain.
In this paper we address these issues, and propose three query rewriting methods that bring the state-of-the-art distributed language models closer to the setting of sponsored search: 1) context2vec, where we exploit the fact that user query is recorded with a timestamp, from which we create "query sentences" and apply state-of-the-art language model [31]; 2) content2vec, where we propose to learn distributed query vector representations from its content without considering session information, which is equivalent to paragraph2vec method from [28]; and 3) context-content2vec, where we propose to use a novel two-level architecture [14] that jointly models content of queries (i.e., word tokens that form the query) along with the query context (defined as other temporally close queries within a search session). Empirical results suggest that the resulting rewrites are highly related to the original queries, while being more diverse than rewrites produced by the current state-of-the-art methods. In addition, using a large data base of query-bids collected for thousands of advertisers, we show that the rewrites produced by the proposed method match the highest percentage of bidded queries, providing a strong positive impact on the overall revenue of both publishers and advertisers.
In addition to search query logs, search engines systematically log a number of other valuable signals that can help better explain and model the user intent. For example, ad clicks and clicks on search results are also recorded, and may provide a supplementary intent signal that can be used to improve query representations. To help exploit these two valuable sources of information, we propose to incorporate

both ad clicks and search result clicks into user query sessions, and show how the additional signals are seamlessly handled by the proposed models. The benefits of including the ad and search clicks for query rewriting task are clearly confirmed by the empirical results, showing increased relevance of query rewrites and higher coverage of advertisers' bid terms. Moreover, in addition to improving query representations, in this way we also learn low-dimensional representations of ad clicks and search clicks in the same embedding space, which opens doors for using the proposed methods on a number of important online tasks, such as query-to-ad matching, query suggestion, and query categorization, to name a few.
4. METHODOLOGY
In this section we describe the proposed methodology for the task of query rewriting in sponsored search. As discussed earlier, the goal is to find queries related to the issued one, which would allow us to retrieve relevant ads that were not matched by the original query. To solve this problem, we propose to learn representation of queries in lowdimensional space from historical search logs using neural language models. Query rewriting can then be performed by finding K nearest neighbors of the issued query in the learned embedding space.
More specifically, given a set S of S search sessions obtained from online users, where each session s = (q1, . . . , qMs )  S is defined as an uninterupted sequence of Ms queries, and each query qm = (wm1, wm2, . . . wmTm ) consists of Tm words, our objective is to find D-dimensional real-valued representation vqm  RD of each query qm such that semantically similar queries lie nearby in the new space.
We propose three approaches for learning query representations that address specifics of the web search environment. We first propose context2vec and content2vec methods that consider query context and query content, respectively, motivated by previous work on learning word representations from news articles [30, 31]. We then detail context-content2vec, a two-level architecture for joint modeling of both query context and query content, which results in better, more useful query representations.
4.1 Model 1: context2vec
The context2vec model involves learning low-dimensional representations of queries from search logs by using a notion of a search session as a "sentence" and queries within the session as "words", borrowing the terminology from NLP domain (see Figure 1a). More specifically, context2vec learns query representations using the skip-gram model [31] by maximizing the objective function over the entire set S of search sessions, defined as

L=

log P(qm+i|qm).

sS qms -bib,i=0

(4.1)

Probability P(qm+i|qm) of observing a neighboring query qm+i given the current query qm is defined using soft-max,

P(qm+i|qm) =

exp(vqm vqm+i )

V q=1

exp(vqm

vq

)

,

(4.2)

where vq and vq are the input and output vector representations of query q, b is defined as length of the context for query

385

context  

qm--b   ...   qm--1  

qm+1   ...   qm+b  

Projec,on  

qm  
(a) context2vec

words  within  the  m--th  query  
wt  
Projec'on  

context  

qm--b   ...   qm--1  

qm+1   ...   qm+b  

Projec'on  

words  within  the  m--th  query  
wt  
Projec'on  

qm  

wt--c   ...   wt--1  

wt+1   ...   wt+c  

content  

(b) content2vec Figure 1: Query embedding models

qm  

wt--c   ...   wt--1  

wt+1   ...   wt+c  

content  

(c) context-content2vec

sequences, and V is the number of unique queries in the vocabulary. From equations (4.1) and (4.2) we can see that context2vec models temporal context of query sequences, where queries with similar contexts (i.e., with similar neighboring queries) will have similar vector representations in the projected semantic space.
4.2 Model 2: content2vec
We propose content2vec method to simultaneously learn vector representations of queries and the words contained within them, motivated by the paragraph2vec algorithm [28]. The content2vec architecture is illustrated in Figure 1b. Training data set was derived from user search logs by disregarding the query timestamps, and consisted of queries qm and their containing words qm = (wm1, wm2, . . . wmTm ). During training, query vectors are learned so they predict the words in their content, while word vectors are learned so they predict their context words within the query. More specifically, objective of content2vec is to maximize the loglikelihood over the set S of all query sessions,

L=

log P(qm|wm1 : wmTm )

sS qms

+

log P(wmt|wm,t-c : wm,t+c, qm) ,

wmt qm

(4.3)

where c is length of the context for words within the query. The probability P(wmt|wm,t-c : wm,t+c, qm) is defined using a soft-max function,

P(wmt|wm,t-c : wm,t+c, qm) =

exp(v¯ vwmt )

V w=1

exp(v¯

vw

)

,

(4.4)

where vwmt is the output vector representation of wmt, and v¯ is averaged vector representation of the context including
corresponding qm, defined as

1

v¯ = 2c + 1 (vqm +

vwm,t+j ),

-cjc,j=0

(4.5)

where vw is the input vector representation of w. Similarly, the probability P(qm|wm1 : wmTm ) is defined as

P(qm|wm1 : wmTm ) =

exp(v¯mvqm )

V w=1

exp(v¯m

vw

)

,

(4.6)

where vqm is the output vector representation of qm, and v¯m is averaged input vector representation of all the words

within the query qm, computed as

1 Tm

v¯m

=

Tm

vwmt .
t=1

(4.7)

4.3 Model 3: context-content2vec
Both context2vec and content2vec models have limited modeling power in the light of the available data. This is due to the fact that they are not capable of exploiting all the available query log information as they model only one aspect of the data at hand (i.e., either make use of the context of queries in search sessions or their content, respectively). To overcome this limitation, we propose a two-layer contextcontent2vec [14] specifically tailored for the purpose of modeling query logs. The two-layer architecture of the proposed model is illustrated in Figure 1c. The upper layer models the temporal context of query sequences in search sessions, based on the assumption that temporally closer queries are statistically more dependent. On the other hand, the bottom layer models the content information of word sequences found within queries.
More formally, given S sessions of queries together with their content, objective of the two-layer context-content2vec model is to maximize the log-likelihood of the training data,

L=

log P(qm+i|qm)

sS qms -bib,i=0

+ m log P(qm|wm1 : wmTm )

(4.8)

+

log P(wmt|wm,t-c : wm,t+c, qm) ,

wmt qm

where  weights are hyperparameters that trade off between

minimization of the log-likelihood of query sequences (i.e.,

query context) and the log-likelihood of word sequences (i.e.,

query content). Denoting frequency of the mth query as Km,

we

set

the

hyperparameters

as

m

=

1 log(1+Km

)

,

such

that

rarely seen queries rely more on content and frequently seen

queries more on context.

As can be observed, context-content2vec model combines

the two earlier models, using the context2vec model in the

upper layer and the content2vec model in the lower layer.

Note that the probabilities from equation (4.8) are defined in

equations (4.2), (4.4), and (4.6). In order to learn from both

contextual and content information, the training data set is

a union of context and content training data sets defined

previously in sections 4.1 and 4.2.

386

4.4 Training and inference
The models are optimized using stochastic gradient ascent, suitable for large-scale problems. However, computation of gradients L in (4.1), (4.3), and (4.8) is proportional to the vocabulary size V , which is computationally expensive in practical tasks as V could easily reach hundreds of millions. As an alternative, we used negative sampling approach proposed in [31], which significantly reduces the computational complexity.
When the vector representations of all queries are found, we can perform query rewriting in a straightforward manner. For a given user query q, we generate K query rewrites using K nearest neighbors (K-NN) of query q in the lowdimensional space with respect to a cosine distance [31].
5. EXPERIMENTS
In this section we describe the considered data set and give empirical evaluation of the proposed rewriting methods. We learned embeddings for more than 45 million queries, using one of the largest search data sets reported so far, comprising over 12 billion sessions collected on the US website of Yahoo Search. The vectors were used to produce query rewrites, evaluated in terms of relevance and ad coverage and compared to the state-of-the-art methods from the literature.
5.1 Training data sets
5.1.1 Query content data
For purposes of learning query embeddings from query content, we took 45 million most frequent queries and formed dataset Dcontent = {(qm, wm1, wm2, . . . , wmTm )}, where qm is the m-th query and (wm1, wm2, . . . wmTm ) are the words contained within qm.
5.1.2 Search session data
Search sessions are defined as uninterrupted sequences of web search activity. Following [16], the session ends when a user is inactive for more than 30 minutes. A new session is initiated with the following search query.
To be able to learn query embeddings from interactions of queries within a search session we created dataset Dcontext = {si, i = 1, ..., S}, derived by sessionizing user search logs data into sessions si, that were in turn represented as a set of queries ordered in time, si = (qi1, qi2, ..., qiMi ). In a case of repetitive queries, such as si = (qi1, qi2, qi3 = qi2, qi4 = qi2, qi5), repetitions were de-duplicated to obtain si = (qi1, qi2, qi5). Finally, search sessions that contained only a single search query were discarded.
5.1.3 Ad clicks and search link clicks
In a search session, queries are often accompanied by ad clicks and search link clicks. These events can be used as additional context to improve query representations and specialize them for a specific task. For example, while one of the main goals of query rewriting in sponsored search is to produce relevant alternatives to the original query, another important goal is for rewrites to match as many bid terms as possible to increase the auction value. Thus, to produce more commercial query rewrites, we use ad click events within query contexts when learning vector representations.
For this purpose we extend the session data set by adding ad click events to user sessions, where each ad click is uniquely identified by ad identification number.

atlas_travel cheapoair_airfares_wholesale
southwestern_airlines airports_in_puerto_rico orbitz_com_airfare nited_airlines bookingbuddy

volvo_v70

costamar_com

fare_finder

car_brands nissan_sentra
nissan_leaf mini_usa bmw_x3

hvwioiushasnda_cmirtvsis2u50bivsolhkis_wcaagrosn

merceds hyandai

fiat_cars infinity_cars lexus_com

bmw_328d
nissan_maxima mercedes_usa

compare_cars

kia_hdseyuauthbolneyaydruor_atunaloi_d__ccfagaooiterr_ooensrlsleaat_sne2itrs0a_1_24f0e14 kia_sportage nissan_rogue_2014 tvobsaveahmyosehuldu2ouovhosad2unls0lbeytntolidit0al1_a_e_duxamf1_rh4rcicaqaunyao_4asaa_o_w3sdt2n_r_izcrnk_dei0sastdunaiai_dsod1nia_asrf_ccs3syaeina6_2nnau2_ao_2aas0hriiur0nttshso0l_osaai1se1__sood1u2ra__4r2afeabny4sb0mnxsnnlsed_a13tudi_dstsacrm45vvoeudxta_oah_ay_erclcasko___caiavsr2c2rlnxaeirow0cu03sktdrsc_1oa51_r_iisnaaa203g3y2cltof_0r_eoi_0soinecd1ncnm1tcaikeiart4_fo3y_traidd_notnh_v2il_gxreos_ia0l_xtnuur_eiud12s_dx2a2qt4eogn000su3111o74i34actooryoolltaa_calxua5dnid_k_icaco_rcmuairskeira_fohinnrumitnfseiisdntosiinutaysbc_uiuscbrhaaai__rrvoruodxul_cxvt9loda0_ensddaeuesalbeulreazrru_srkulaio__ccsliuexamx4rtauop2_shrr_0tes2y1x0zud41ancn4_ihvsdi_nsokahaifloaivki_nhnn_soy_dtsihuwtuoaploirkvlc_naanaossddvqtlghhvao_awl5veoofei4iinoxk_0_xnnanuilstaeddkse_ugtt_rqsea_oovrajsewurger_yycoamn_xuoounhatro2atgrsttsagoea_naa0ceazmnon__slp1tuedtc_niaotcgdu4xhaoaar_rzuauaamoorc_dcsamso2n__oav3on_sel20dmrlerd_cmaayrd1oa2re__a_m3n_tx_0ca2_r2_id__s1res02l0otueeps24spo101ssacaro0y41oa3ts_ilonooer142orkhly_ttnrto40vaioosjab_tmeu1c_ont_vm_xark4aaywad_ccbew_i_ovtnza_vaao_c_alftbtdde_eidaoer22nrleeahsxoyse_0il0saatit3uuini1ncsatc1_lc_evs4aczluc3sreiassuuosavrnas

hunda

bmw_lease

subru

nissan_armada

2014_toyota_highlander kia_optima subaru_outback hyundi

car_reviews

qx60 nissanusa

honda_accord_2013 toyota_rav_4

2013_kia_optima

kia_optima_2013

audi_suv

toyota_highlander toyot

honda_cars mazda_cx_9 fiat_500 q50 bmw_com

mitsubishi

scionvolkswagen vw_usa volvo_cars infiniti_qx56
passat
infiniti lexus volvo toyota_rav4 volvo_xc90

yaris

2013_lexus

honda_civic_2013 honda_fit jetta

best_suv
mercedes_benz nissan_versa lexus_suv
vw_golf
vw nissan_rogue hyunda

toyata q7 isuzu

nissan mazda_3 bmw_5_series
truecar acura hondaki_ad_ecoamlerlesxuveloster

vw_tdi

maxima

rav4 prius_c

prius

honda_pilot cr_vtoyota_avalon infiniti_cars

nissan_pathfinder

mazada vw_jetta

fiat mazda_6 sinufibntai ru_legacy

lexus_is_2015

2014_infiniti vw_comaltima toyota_venza

bmw_x5 tayota

venza touota

volvo_s60

toyota_camry

toyota_cars

acura_tl kia_rio

nissan_juke

used_audibmw_cars

azera hyundnissaai

toyota toyota_recall
bmw_usa nissan_dealers

true_carbuik

honda_civic toyota_sienna kia_irvine new_car_deals
used_bmw
hondaedmunds lexus_is_2014 toyota_dealer

svwu_cbaraskriau_tr_ucbk rzjuke nissan_usa

subaru_wrx jaguar_cars

mazda_cx_5

acura_mdx

hyundai_sonata luxus
subaru_com benz

kia_suv scion_tc

lexas

kia_sorento bmw subaru esliacnotnraaudi_q5 range_rover kia honda_accord mazda_cars

scion_frs niss lexus_carsaudie suzuki_cars vw_gti 2014_mazda_3

vw_cc

mazda audi honda_com bmwusa

mercedes kia_soul totota

rav4_2013
toyota_dealers

nissan_frontier

nissan_altima lexus_usahonda_crosstour honda_crx

toyota_com honda_pilot_2013

land_rover nissan_murano ml_350 hundai2013
honda_pilot_2014

mercedes_ctolayototyao_tcao_rtoalclaomtoayotnai_spsraiuns_cars audi_usa mercedes_suv auid toyota_4runner

2013_honda_accord

lexus_es_350 audi_a3

mbusa bmw_x1 infinit

2014_nissan_rogue

2014_honda mercedes_benz_usa

honda_odyssey iinnffiinnitiitui_sasuv

lexus_rx_350

mazda_5 hyndai2013

toyoya hyundai_accent landrover

4runner used_bmw_cars

hyundai_dealers

hundai camry kia_cars toyota_yaris

mazda_com nissan_dealer

mazda_cx_7 2014_honda_crv nissan_quest honda_ridgeline cube_car

hyundai_com 2014_honda_accord true_car_pricing 2014_honda_civic lexus_rx300

mazda_cx9

hondi toyota_used_cars

hyundai_cars

toyota_matrix kia_motors

hornadna_geele_mroenvter_enviosaqnukeias

hona toyato

mazda_cx5

es350 huyndai

mitsubishi_lancer 2014_honda_fit used_subaru

hyundai_elantra nissan_com lexux hyundia bmw_suv

hyundai_veloster fj_cruiser acura_com camery

kia_optima_2014

infiniti_g35

suburu audi_a6 toyota_fj_cruiser

exterior_colors lexus_is_350 kia_soul_2013

acrua

ktoiayo_tcaa_dheignhzlaan_d2e0r1_24014 2014_toyota_corolla blojmtheonotywgtnaaood__ut_atdanddn_le_eiiiifs_siauxts_csulsea2akaas0rnirntass1ol__a_3ystrnpsoovoudaboywgtb_mtaouuhca__tlwref_arbuiau_n_2_eivs5_d20teea2are01trel5clor1e4mc_on3na2ma_ilszu0ln2a_dss1hi0_2saeao31a_ds20nn4t_ic01d_raliua1e3bcnxr_u3ucauttxbre_su_feiarca5mdkct_axtucod_oarzy2amcao0u_t1rahda3__oecsnaaukdvlmiaea_rr_yk_9_olo2dhc0yoa1s4ntsodehriayn_d_tao2chy0roo1vnt4_ad2_am_r0aada1zved4u4aal_dhe_2yruibms_n0mshad1aqwisap3dx7iia3nfiniti_c2too0mys1ous4nutba_imbsa_kasaridazruaudne__a__a_focxoslrpetuoreetvrsri_mrssalttoraeecrkna_citsi2ovks0hiicraay1__nus3u_ihnsssoadcenbaitonodmin_yta_rgwoa_cet__acanr23r_sze0_cs1soies3ro_rkleilceilxaaosu__uss2_pvvo0reoaxwr1lecbnv_3numois2pstrm_u0woaasbc1a__as_u43z5s2nir_dl02aax0_ka8t1si5aiu4v

toyota_fj 2014_crv infiniti_q50 mercedes_benz_cla_2013 toycootsat_croa_va4u_t2o0_1pr4ogrambmuhhsouannd_diaca_ocrm_avutonation_honhvdyoaluvanod_ixglc_64050

air_china_usa airflight

swaiirls_tsaoh_itiawi_railri_linnceesolsatst_f_limignhuttse_fligshptusthwestai_rc_ofrmancaeir_fuas1pshtxt1i_cc2nkhulle2etsap_air_com

alligentairus_airways_tickets www_fly_com cheap_airlines_ticket sidestep_airfare florida_airports_map

yuma_airport aeroplan_com

asa_airlines

jet_airlines air_flights cheap_airline_flights

piceline edreams

expedica expdeia

xpedia

airtrainairlanes

ibc_airways cak_airport

liat star_alliance

iad_dulles souyhwest
kyack blue_jet_airlines

coamirpgaaorierr_i_lltaabrnsvuia_rpaslilniraunainmrg_io_eiertiaen3l__nijsni0spaare_t0_re_oiiirar0tco_fpllli_e_iinif_tnrg_isaefcclgelhiiaisrknglosltioieshrenlbmattiesssnesesstmera_cjitreavhriteaireirfertarigv_lbraneiixsntp_tnlrltearuaao_dipijernareec__iwitaclrs_c_aaoeirybsmalsailnscuaijderi_eershla_iucatnitdenirhmnr_aeaeelieaitanabrsenc_il_pepcsdh__la___ue_nafatala_aaiigiecftr_iprahkrai_a__tenlirrsifiastrnar_eas_llitirarienoslliieinr_ernsi_dslsenieea_nbktrteaiioeercsysitlakt_siekanaas_itcrlisl_aer_ahftiaariresrailrritneanlrinas_ep_enp__assrtiaiir_ccliiakernlieesirnsltisneess trans_air_aiarwiratryasn_uasir__afairrlienseasir_tfrlauignsh__ttaraa_a_iisrvrablcleiinilhnruloeleeienssds_ei_utjysetlieact_ikraefaitrrsfeadsreelta_cahielinpapetilrscaickseek_ta_lnliconaoweimr_eljiaesnitcterb_hlsilfanueameeare__petaariciiicrrklfaiaennrtee_sac_irhcleionamepso_aoifrvf_iiscrtgioicaiunkl_te_swstaosietmtehstewareiirccsliahtna_eeiaarfllipiirngl_iehnmstesrisleitssaemrryaavs_aoriarttt__iihrotflwrainanresneess_st_t_iacakiirrelliitnnsees_com sdeksmeaowbjbbelbyswtwru_ckacakcbstataueyeaiiitah_ro___gt_hrlsupohhl_ixaylsctoi_unaresiaifoeb1p_geytelnllarowpc_ph9eihriiiveuyartmagncaleia0s_oiozatuglicuktltaaieyyi_rr_heietdehor_mansapndtlahaa_paeail_eetatsgotiaiwievsxtrlsvorrratpec_adefr_ppi__daane_a__ieesrplfeststomtlersuaayiilavaa_dtrtli_cil_rttna_ae_aay_fote_einaiipiglt_stcckcrlipiinlnkahcgwaofeselirhsilesrai_tudokaathoioenrimlaprastrwheaehserieteaieoioh_wa_lwyapirtagilrs_riugeaciiltutrratvpm_mattarlnynrtrleswitvhoanste_iohi_eue_lataaaiiim__iaryknngirpnctea_hc_arisetwaatautasydg_waanmarwfranael_yiiustsk_aeisissiwhcomlircnraktstfeisriaeetaioptls_emtprrhanh__aattacbgraeriaphkksia_aeeenimt_luiscdcxia_o_otrrittfiwnrfliihebiiehei_eeogruemdnyrassiratoarlyr_fl_lullidaa_dighfoirsnit_ie_itiwtawicliiiez_gteerndlense__nrssgdnsttrsilteafr_hiot_psaladth_gneisaiastia_assceea_gratitenan_ar_tnwya_p_fwhericiinwot_a__oo_lyefernis_e_iticahrnlarw_ihilrctkacrapaljetltlcbt_gasetsaaaifsiyeeneeiaiteoaaigaaasrace_l_anit_itirngehhroiitisrr_aertastswfa_vcdsgskijrirltintliiireaiearliileirratl_rrsih_optnaecr__yllhmwtlecrtshtiosial_dtntaaonaselfb_ieneklrnlcniaarc_nre_lr_trekontxatao_itbtietk_tpetaatttssaaiihpsriescyrrliysensaioofraab_anje_arishes_etlgraieeereetivrivlxvfirlaetotensrgr_daapea_rpesrsuiietiptpaleafecresei_ollrllti_agioalrppsctplshr_iprirolggnisler_issa_oma__tubencsn_ildiooias_i_srratetfohhbnlieacgrctlyoaarnnalieeuoa_inilf_godalocuvtrnfauncaenwli_eeerurthoeidhnsroeiliiaslririnlrva_nsesq_eteaysiparoni_enimrvesftrihgiuaetrroitnsbifae__d_as__ia_lr_aw_searlnmfuttsstrtaasie_rcapariataaaci_n_ceccmragrgaaan_ent__lx_piocnhiaioc_grsapaoojoclrbfkeapevespciiirietasieroanuaomtt_nlrneewrr2aif_rfniaemgksih_rrartlryw_iagls_futrfy_eiigarl_tidasihis_ibjhsng_elmgr_rlhwafmhiaiai_apaien_ftacaailanbwpesreapinhciptksayeao_hi_ae_goatwstsiebnrrsiprestire_srrndfites_taroltstrhmolntaseal_bliair_a_i_rasti_aniio_grknc_famrinivt_niogrcsavrravspbieailattcortchse_xfioeiieokeraiioecmyai_lreenils_a_ellin_zltcckrnoiau_sairsceaisssorntefiaigaa_tulyitsordiardek_eiriwh_lprac_tsg_uelmpcaiocirh3wit2anesaihytnwrln2rcohiheaittsriruusn1iascatl0mo_etanalteoai_s_toaisnsssnbkiiaeaii1nctaortqe_kdrxecala_aine_peef_rodal3ehaeaa_apeginidiarsertspflirpkmravsueilsaoarsaoj_rietesngrjmdaraanlepiit_cties_icrtcthahn_rctcryfkaahtlea_bkahbiaoaate__gfkyltcchaysenhewalluaa_wllaeneebstudwtiiaroroireli_agroaaigte_liaes_giidansre_sn_esaums_ipfiwa_eabomisabpasrceaa_hilo_msriu_rttiirlriatylp0rr_nalh_ppiibtfuooatneliuseaa_asrrrasieuteanefalssfculeumrlfrsiiaeillirskenertinaiaiicagt_ghrgrwrtyssoheiaia_ealpn_rehwttaik_ihtnnzacewarnaut_d_ac_cteurasi_tyij_heyde_ceronisoatsetdfaaalwlttiiiijslei_maairt_ta__ccrmnliiseiibnlragba_iaadeniaenkwrllkctiurir_iaahe_nn_ry_spee__eioaabserc_aeiakttaisr_eorltamtoipmlsbefais_vbsoeils_sgaknimieiarantligirtaaritirnier_nigrnehgaetrdilulicufetriellstirnrytrepasaixssuknisswmrkna_lnislaoaaagticstymilaniacir_enearkc_a__remlstbhipe_iehnanrlldfuusetiel0foferti_seaeeleelsrriircrinaggaabsltbmdueifaoy_recrw_ihkaoiscioarenaapnatoei_upnatlnnizymyikiqinsattcmaatsnaeoreiirien_siataknnaereoedgeuvomcr_e__a_isrciibiunaaeomrceuiaowtnrtdioa_r_aorhdslunudanikayal_ein_bywrcta_tl_ttniaahaeanorjeios_ciaihereiib_arwmstrsoiaaidbat_lwtrf_lrjeimiaaepor_i_nsneelnltil_ei_tyxne_riatab_nlsi_oeeaaccnsst_tmpshebfuowa_iaod_prai_kltiloiervelemtiraeartraricc_traaleraeyjop_aic_mlceiorsaasnibna_asn_arae_omcmad_nio_lrieirbtiremaefu_vcomtjro_jsu__l_a_laaiu_ihmrenibieseflntttlwdglnraaiaizienalicifn_ortyirm_iawttnzlkjeehnh_geiesidyilreewrtosi_riagi_d_abatttbh_nlcatwr_rl_ttsra_roisbaaiihoalvusminlviltntrernireitreltuloesdsviiltzlnlra_e_caaageesiesetnisiyecrcirkgokranese_heufemttosolessiafnsd_gxueataoiprsbohtseieueipwtoirteodtrrlhei_iiuekuaaeslclew_npbptadtisvichtiarenkekkontidhka_t_wi_oymitalastawbmerieayriaseckitizw_fatrhgeazoiscalrefti__ao_rriltaacosimieak_igdponlrncrniiar_miabuuenrlstestcp_hlzeor_srienanieabnaitorrttitipsueia_zlttn_torymaeeripvtisatlea_rddciula_cco_lin_hh_i_nctirecsoyimnianptaelfaotmohpeirolf_enisitdamnharsewega_iliosewf_1arvalaraduhaiarg_tiiiseiwtierri_chhprtl1f_airl_s_lestingaws2o_eafheelm_celtllssisxnicsasfagsihptiro_ae2jgfdobbiaruheeaciioaaitumeulranmeityiryrneimulnjetyreerpixvna_c_b_nailtnlcrie_iaoeioatarifinelgn_oaelfltiyrslluirbriadonbitetarhgplernees_it_i_tiaeezihojlo_inoealri_si_ttrvrtpntarsnatinc_wehiitrrscltipotibmeenoiosngserkcrasetaa_sdds_eokreyylnefotresefctsn_usldtsi_srahphgtakwivius_rcarep7oepehsauo6ssit_kaossrt7naywtat__rtwshmapi3_itsi_aaa0rpiso_sru0a_yrwg_ucasftseeinaiisaauarlrrraclamhiiinicwlsiraesigrakeiiwec_riknneasnaetar_aaa__hl_ias_iaenlsryaiyniarrjiraatdsslaaulhiantsiniayspnfcrnmseclidiliaeir_vtosiieeerrrsanhfjua_rdahiasetrwlteraohiesliic_i_wgeixsriegrawurtaaenaatsiw__bsf_untar_hthci_l_ehiaalynraislatork_iiwrpahgautiinlowrpvrtts_k_taisswoeaaucacdeejnterhebuacoyedotitijia_mfrmorhlcnko_caiueesat_e_ft_owvirxreardsd_chiptn_pkbezkste_txpedwo_uefwsrtpiial_rhoirpio_zesneitdeoaaattardgfamoeinrvr_irgeaopuutrlceeechautaiaci_iletmparrdoaks_istrthogeohfcermthspslwm_wmfeliiae_mla_ghlltewribseeiia_rieshamlfng_iadaetisdptosakdarrtetolirraeltassethroty_yiayvpflbi_iao__arialreocpwuira_ncsttia_ayczturksrncsrtalacdaieu_tiahlee_iykiir_oaac_scstssfrh_n_swiiialsanaaatelafimarwsslsrhfnmg_aeile_kladaeilns_gialilrilm_hineatpoiiadgnsiilireegaeipa_rri_tbyrsgnrnsa_ilosallhecluaialtleratn_iaiejii_rhiticnahenenpkeftrew_oaasrlnesytokstasspxil_toieoirhtysr_etprrticsfneaiitslnnwa_asdru_kcmnaiw_isaoklfpnat_awri__cealbiasrfeewiai_cromedrlglejitwfhrtlaai_lsm_isprsii_nhr_urrhgken3oclmcsetue_ioraeaa_eht3ttnenieefshheoktl_cot0al_lamvsih_miaspouoduaig__erhtp_cat_emp_hei2sio_areciaanuwmlartpct__oayra0np_cnielsamo_lsrirlbit_0liioddioomeavgoatrprh_fevirekhp_lib_o_tynanaeolls_eomanslauosta_hreiasacissetateelcaaniekwncniirtlohhr_tirttadeiota_kr_rilieeeirle___eyeseamrariumoliubr_ancap_alatclasa_ftpf_osrah_pvhdiastrineatsl8iafaw_n_cppiaeteosvh__ictmrwupusialea_rktmsesg0asaiaiprida_x_iarkiisaeia_lnlwicho_tgspia__iprcerwticaer0eesiailaehiaf_it_recaintanoipi_iiaavalopll_dorrdmihiieroks_aonrynrpiecijialtierwaaeeiretorsrreeiimnkhtl_ruaptcszietrr_snsrtoristtieu_tlthos_ot_asniaigc_lflabrifutpn_itnimtfaransrtfaatoaoralntawnhaefslettelke_iactiyyrw_oisulliveiuslngorrit_ipt_ivueninrdcrtlrercsoasarc_esentcsmgeeiatnaai_ccavaelthtseuihvck_rsositsatlai_vkt_huaelirosssecyclhi_i_isnhigostellrwmswsocntordaoesnngb_raasteiataomisaueectlsfylmfsraatw_uasoareeleiiens_iifeia_sit_wstidt_liiatndtsiynrtintlarr_hctfpj_riasray_tnrelfsrnise_gwoscs__aclaia_rbaxiiigtwtraeeaaeernperiyrriaviia_hlicqd_uc_npilphpnp_pereaienetaeharp_stp_sanrsii_llotipeediassaossnlaatersonieassrrcoiseecci__tra_atmsisil_rnricrre_b__rftloos_rnspol_a_srtrtaiaotaaiiue2ahtdfihacroic_tssnlmoskcmurieu_islid_eacrnnvpatlooier_acitt0rmokokcrinlscrerecteatglductpeteaamneaolbie_eifevukisieihn_hn1esthemshihennlsaaharaneototallotelionrleetossiowhcfwret_r3teri_yefr_ea_csfss1t_iiwrcseoru_a_l_ft_etaaiwtlsptmbsycgtal_aaafiay_tekseieeeantiaobvniacrasg__eoccsairniaaad_drrcia_1prfio_piotpsvsodantrs_see_sliiuhteheipefleidhta_ohayrl2_mteritpiitriostnatyjraa_aediiadornlrrlrrr_es_iaga_nh_lsemenwasiltctfrogats_racsaftilnaitriiieaaamaevf2d_lio_epfwnaiatrcak_atnryl_iciairimeiopuioreneascenr_kbip_iegirgioaly_eknpepgrecrrsube_r_nnsleirijlnfapsshbyml_loatsohlweritonnlttla_s_e_aihluhshio_iirnptoars_atoais_tsisnnenyunsin_aaooattcetietefslacryarin_pli_e_iier_eicrt_natreuekin_ildeimzcwlrapa_ukirhpskvkst_liiosadytacsier_tfa_o_ndseagl_p_ciitssuenyfw_iwdserialiirriao__eyreasrhnmnir_coptayernack_xnaiaolhsimcitlraereioprotvnsregctepeilk_tafsioriwiosetlgin_iaeornolkcmrlrtithoislerlituxehaokerinopasarnrwsiaectisise_sailmsn_tsemsrarlteopgitlr_avarepmlklana_saiasieoslsriirleftaelniiprlsntvoerhtetillpssoirsaaeadpyorsrietetsttotlrlysitirasstas

exprdia

air_line

sprit_air ait_tran

tennessee_airports

flight_deals

copaair lifemiles

one_pass

first_class_air_travel

exploretrip hts missoula_airport
jetblueairlines_com hipmunk

kyak

dely

hnl_airport _com

skymiles 1tpa1_2mci2

united_airlin

cheap_fligts

santa_fe_airport

mci_airport smartfares miami_airport_arrivals dealta

dfw_arrivals

flight_guru

virgin_airways direct_flights

delat_airlines

jetblue priceline_air_ticket

pbia

chepoairline iraqi_airways aligent_air

airtkt_com

direct_air

comair spiri

united_airlines aa_airlines orbitz_com
avp_airport

caribbean_airlines

tran_air dayton_ohio_airport unitd flyjamaica
airport clt_airport_code hotwi
bluejet lowfares national_airlines

travelation_airline_tickets

expeedia www_jet alligaint

ua_airways spokane_airport ryan_airlines

gojet

lga_airport

alligeant_air chepo_air

santa_barbara_airbus

ba_com

northwest_air find_a_flight

tratavael__atiicrlkietnsesvirginair tigchtksets

orbitz_airline_tickets_expedia

jfk_airport southwest_ai insel_airlines

sw_air list_of_airline_companies

alaska_airlines

alligiant_airline

www_emirates_airlines cheapairlines

jet_b_ue

binghamton_airport

spiritairlines kayah

air

tran_airline

smart_fares_airline_tickets

cheap_tickets_airline sanford_airport_orlando parking_orlando_airport md_80_aircraft

tavelzoo 1mia1_2gua2

airlines_in_usa sun_airlines usa_airways pbi_airport

www_usairways lhr_airport

kayac

nc_airports ba_airlines alliant_airlines
emerites ex_edia

kajak

cheeptickets air_france_baggage_allowance
delta_airlines_vacations

avior_airlines ts us_caihreviprtigicnk_etasm_ecrom

southwest_airllines cheap_flights_to_maui

capeair_com

comairfparea_trtoo_rerhbiaaz_wta_ivicaoitermrlafvelaaotsciirtoyu_oetcohsnw_esct_ofairr_mainnescneor_thaiwrsleisotnu_etaishrlsiwnoeeusstti_ahfh_l_iaiwargipsorhrtltti j_noapliailrneplv_igdia_ienarirnptpeoo_rrtsatir

southwest_deals southwest_cheap_flights kyak_flights copa_airlines_colombia

vayama_international american_airlines_reservations jet_blue_airlines_website

southe chorebaitpdz_el_staoiarul_tiphnaerwt_etnistcer_k_eaitarislrilinneec_tsihcekaeptosflig

huntington_wv_airport u_s_air aruba_airport

top_airlines bookingbuusddai_es3_0c0o0m_aikrylianckefl_i penn_air flyfrontier_com green_bay_airport low_airfare
webjet_airfares discount_airfare_tickets

cheap_seats_airline_tickets

acy_airport srq airline_baggage_fees

jet_blue_airline_reservation

northwest_airlines airline_discounts

airlinere_ndoe_aailrsalicgihanetap_flig sahntfor_d_tflioricdak_aireptorts cheap_airfare

airbus_319 allegantairfares

southwest_airlines_confirmation alegent_air_line

military_airfare

spedia

areomexico

expedida

aviacsa_airlines disco luontwe_da_irafiralrien_eti_tcikcektes

cheaptickets_com destin_florida_airport

embraer_175 lowest_fare_airline_tickets

southwst side_step flight_info air_fairs

delta_air

southwest_coupons

southest_airlines bereavement_fares

insanely_cheap_tickets

www_frontier_airlines_com cheap_oair

flyfrontier cheapoair_coauapirolinn_ecomdereican

west_airlines

best_days_to_buy_airline_tickets

usair_airlines rdu_airport

spirit_air blue_jet_travel delta_ai alskaairlnes

expadia delt usa3000 airport_codes

memphis_tn_airport

united_airfares kaakstudent_universe_travel united_states_airlines
delta_airlines parksfo southwest_airlines_number iflyswa
airtran tarom usair_airways spirt_air southwest_om trvelocity best_fares

airway

deltaair flykci_com

air_trans

aitran

flydenver

western_airlines jetble

kayk

orbitiz usair

concord_airport airtran_air_flights_schedule 757_200 airfast

kayack

unitedairways airlines_cheap_fares

budget_air ethiad

us_airways cheap_flights frontier_airliens aaa_flights

kaykay

travel_city kayaka

low_fares_airline

travelone froniter_airlines air_tkt

fiji_air

unite_airlines

air_italia spirti

farebuzz_com nited hotwire_airline_tickets flight_deal

southwest_sirlines

flights_to_italy_from_usa yvr_airport fly_rfd american_arilines

lufthanza

matrix_airfare

pty_airport travalocity

kaiak westjet_com flights

allgiant

frontier_airlines sky_blue_airlines american_airlines_airlines clearwater_airport_floridaord1bna1_2iad2

sothwest

jetblue_airlines blue_airlines soumrbfst_iza__ircpaoomritra_lwaasyksatranvel_oviaty irairfaaiirrleinse_4coledesss
expedia_flights kayak_airfares cheap_o_tickets cheapest_airlines airlines_that_fly_to_hawaii

fronteer

dallas_love_airport

airtrans

transair

deltaa phl

southwes_airlines

jet_blue_flight_tracker sky_scanner aol_travel

kayaak united_ai quatarairlines

fly_kci

flagstaff_airport

my_allegiant_com

airfare_to_philippines fly_com_flights
killeen_airport

sw_airline jet_ble

alligant_air_fare onetravel_cheap_seats_airfare

aligiant_air flights_cheap

united_airlines_discount_code

american_west_airlines

alaskin_airlines

lan_colombia travelocity_airline_tickets allegent
latrobe_airport southwest_airport cheapest_days_to_fly

southwest_promo_codes_2013

travilocity air_france_usa iberia_air

www_delta_com_airlines south_west_airlines

bookiunsga_irbwuadyd_yai_rlfilnigehtskaaiyr tarka_mflaigihrltisn_estiecakrecths ckhaeyaapk__oa_iralinires bkaacueil_irtpedaaorlmdkiaienlyax_gincbapoeiorruliodnkeeainlmtgae_riacdaieernxllpi_tniadaiea_ir_seacl_oirnmlrieenssetraesverlicvityastwioainrlisnes_com

rj_com

airtran_airlines_web kayka arik

air_train allegaint bwi

traveulso_ciatyi_r_aairifarlrienses whatscheaper_com southwest_airlines usa_airlines aaiirrsa_l3osi2fintbua0ae_rerrhraiaaie_rw_blalusiceiarslsionntecdse_ceu_lsoosaco_nwmltiseatsmcsstnto___trauadvetliehrllwitanairee__ms_aattir_iorcclaiknibreleuleit_snsairscrja9e0vi0rv__beatsrit_ratpravovereltl american_ailines

untied us_airways_seating_chart

delat

orbtz

detla aa_co lvia

s_w_airlines air_canda

delta_com etihad_air hawain_airlines travolcity

kiake_travel souy airfair

vueling_com

otwire

finnair oribitz airtran_reservations oregon_airports

jetcost air_malta

lacsa canadair low_air_faresarik_air

twa_airlines trip_com cheapo_flights
one_travel tf_green fnt_airport

capeair rbitz

vuelos

ohare

dealbase_com jetlue

us_airwys

cheapest_day_to_fly

avoca_airport bahama_airlines

jetb

tap_airlines

cheap_fliugnhittse_dt_oa_ifrtw_pmcarihycye_eeaalrispinreelicnbsa_etoam_lhuidnrleaoietnyriten__i_uc_tuseojtwaei_s_cn_akbit_rtiewu_raeawtysea_l_ai__rsayiaawrisatir_r_lilfiiarianprfn_lreyalieigef_rnsatthni_ecretkeassre_sittarsarilariilvnrincetceehlrohsaescaeintyppa__r_sifolacltoioeigaicwlrhuikin__tresteflfhtai_lsni_argeeiarh__fifaatrasilririrelnisneess pkriacyealikn_eau_isar_fiaralriinerwe__scaetoyicanskr_ticenahteisrnwltiaisaytl_s_oafi_rlainhireoliswne_sto__ihnfoi_nwuds__tcoh_efianpd__acihrfeaarep__oanirlsliinneeea_rctihc_kaett_sbtlruaev_ejle_td_eaaucirlhn_lieinltoeaewdp__o_tiaafcanikirrroel_einntassae_iirsrsf_a_totrtreipaccs_knhfeselit_agsaph_iotars_filraifgairhrleitnsse_scom msblegsutujeotdelqofht_c_sueaoa_uuaoiacritanirllsisi_lnrtttnsiaslshpnuaaiuaoehxoenrlaa_wusrlteseardraenl_tottwahfaaiiwefuag_hacnrwilmic_ilsiaifrlbagn_lcmtlesiyrlahaihpaeaice_rcisiv_jwieritwntfeloegkottcdrarslnfti_eia_ei_m_sroaepdgrs_tlrcwertbsnayhmaghitiicdaehaamoaufrstisnra_torkei___lrl_idptiitniiegrchcipvncrcvanso_e_ace_kg_e_aefnccrcieoaaaeknoewatnxsaoesseiatyhito_rrac_mseph_etlp_usrmoe_ynnelorl__astlosiarnhcneirtsit_reenlealnacaeeatiotos_err_ncte_shaspvoretoierilpni_renuaiisani_ar_oicsccgrper_tnsnlndaetaierfcpplihilshratm_nltia_cao_iaiauifnevar_nshanagw_aarenikarnrtpmilccoeii_sapheererarseoeritokde_iltovegnsni_rllttn_cgalminstsasiaes_eo_auereetda_en_csorenrfiisc__oadwipknraa__nrordilteir_eacvm___e_prrltpftfaiw_liioc_airajdsslrlegnhsanrrefrliihtioyaadfoirti_lganteeeinsteoslileirriyao_rrsiwngemsdbwchcmeervgf_aitleanbtks_prssovyht_oesamits_hfhlioospoacecl_seuuciacytiouamsireuogstrmmnk_snew_tkarchgstishhiyiatmhedtcnaijti_strfhwlaeoiouo__a_cceosnitaitrlanar_awesesmttarinada_ufsnatae_nser_iiwadiiotrfzriearrtleewaryb_rattsii_oprsser_s_o_alfsprhiw_teadcelajisotairecu__oarfelnrycscnairrouotpsamswrtpe_lrypa_lrterlw_ifciveohbneriioeticka_nigpvevnahs__raceilaasehmteduauemnrnaaslslaaehsiaeiiosr_oidzvnsrloiu_yvnan_e_tyncira_dlotnraleejraimoitaplipesmewd__atnnscipleve_ifirarittrtelrdnioiobnaitsye_llrrlaaeaeai___iipai_kvuncinrgrteoisag_akalocrleiitdyxrtena_rantrerttfa_sri_amaieflihoaloatil_iosc_lacrgt_eaisicyi_rhprcirrpaaitnhuhiomi_nrnrcaaankoamnihirlsaptnaertirilettnea_kgeeeornsnair_aa_esdir_ccriiirr_seat_otedllsazeilciitawnnnmasiiroisrci_eelr_tomt_nuee_caolsipmdv_lir_iiahfrutcietrnnnlafenalalkddiaeea_nmatdcmtelvsahevtkantlieoae_aiseateserieieaphrrrsr_woe_tee__iilcbe_tr_as_swftoa_allharleinsstaag_nittgcautuvrrhat_ttgnlihssdcsied_a_tatbai_essaassleln_oyigcaalilsolwtieiamporga_vrirse_rka_usilwd_rynrd_nineteipootev_ceewsdhi_odopal_aeusbluetitfwir_remressandfantstaiticdsscy_lhtcra_e_e_iseremmoolgowhgtflueiiir_acsiclirenrnluhpslkhhenwsweaa_iselhrsaewnatseihswstic_asnra__metep_itaaciwf_p_ohl_iita7irreragsftaedd3th_aahiata7a_xrietasetiiras_eipriiem_j_rqcla7rriaieilstyotta_clr0riaofol_fen_bi0nkaal__ioilcaa_tcnfslaitrntrilueirotceossnpn_ncfeolrh_eeeaotinesksidhat_turmoenaoiesuoro_tessmlrsiatauntcaseeavneodtfesaieeishnrlx_issrosntspw_lan_rcatisrvisaietniftl_er_vymfos_aaiarassesacuoicji_rterrsrip_tlnttd_ikihronoteiatatzreoaweliotmewosaieiirmsuireelnrurln_lti_t__iesaiahanc_sealttinegsrel_f_ulhleigieastnrlfs6r_tnoeiayaoecdw9snn_adrscyihaarji__eealeiah_sirteinablribsciltpaisnaealneatucieeethca_lskasd__pseasettsipasra_hltiani_oermefae_lspigtubsbi_ih__arrmttatllb_t_havaiiccnoieetwnioifirlresmyms__jleeasi_lotl_tun_auc_seottoanhuemgiwtit__rci_eitn_taesmdtkbetraaroit_oai_retuimlieien_acengretyxsi_lksdib_crsii_oclbne_to_iu_nmoabettayeasao_ii_cslsras_ii_ts_rpbl_rkasuia_iadkarfnnuhafliiu_aliuitauiciarrfaiatt_eryaphicsegrchaiasplrioko_sn_o_iwrmehwrsaar_ntaltiaat_irstaaaaeaniwpuhcswpiicirerosleirai_inetaaani_rkkrtsilerntmaati_isttl_adsutver__ni_is_casnofnacalaczipo_tgiiboeorit_hpaisrmdokeeoiattos_iie_dmrtrctkestrsbasaairtvhmet_eeksbiarlsacaoutelwsccxcikjsanackec__dnawhpc_ozwttsareebsmafecaiphae_evmub_htoe_tsala_aifeelllieersaiaydtesuried_biptrllsrtgdcilop_ltrlsaipin1nehraoahiaaooti_aeiinngt_rneusp_aruputawaesg_ttoctr1seatr_t_ha_stefhjl_octe2ecan_iotnsfiiwejrrsluowmrbmlenjutltc_wilh_2eagttmaaartieabaaliohsritailrplhopltl_acotw_ie_enyr_irsrtataa_ikaetsxaansgafaiarltrs_iii_ipoi__railrraraartirpairn_tilwenwnnaloriiiorrienanretndctat_ases_riseraaheiiicablarntfrivslel_pnhywiuise_aeenareptelmnl_ifeaasimsctasc_aoaieesrayhimirmarskrpcfpa_ossitaevlllosagereaohiitai__lsrsangiaamsuap_itul_rioii_rtnalrfsrpehvtt_fetefrloetaremeelcrgiahais_ttortla_argugaceessirthtmtnnarmiold_wct_raoh_einihfmbviaal_ctbe_nenihergecntaiaa_cleheeaascooielajrieterk_aicotksrrplebarzoia__iaseitaoioyccinrlrivpsrerajrtttufi1att_atutzisnabpw4itol___r__ir_pnt5yatipreazcnsai_robtaotviicascse_ihrlrkir_uerdnffiywdllcnilhteailil_nluieranosi_tr_fbieegkiaiecaleugl_adllreecashivi_iihsylieiangcrenclna_hoatmtaosl_stpciramtiehaeehd__heopsticissa_amn_rmas_upcservstee__s_kfiaifirlsebts_t_auaa__rdsc_ienaxoatdcpaeacdhldctiilriiuirrcprieialcotbipeooioirretoeottnkla_utreisphllm_aesfralkglsnw_mw_l_olwe_di_etiroetaarllferuaic_nroaent3if_wit_ttc_e__ea3amntislkssou_tnr_a0clie_inatd_f_orssfylgacraioie_2rpgu__apfaiiauria0lmerhi_tcspiopmahsla0ttrnsaaoeenswariiriinrrherol_trithtpeltrlericci_bssaswtaiollsshna_remseaeri_iiue_f_sta_g_oanltdnraali_tuedtiywuspdiirarywsauntsrrpaaa_iekglyfthaseiwsoaraulirceti_ahoriweiirnniahtirw_tot_epsk_rrnrofnloodlo_adwwlsnceetlrlercieaihtarebiinen_lniotretpnhntieaoosfsuiii_yebiatsaeedasafnrt_siheeetcruz_mclaosldslirinitinn_zirisd_lo_u_sagsie_ocrenzl_orsahciftflwajcjillnaemtorphgeualiiapeiieoypwgi_nrncicateosglii_irnh_rltkm_tlurnabsreh_etlob_ffksehtareomilugsnailfinet_islgtxtqeirabcilsuss_iutilrscag_iehaeiedcn_tpngerglet_aettstvaittasxupkoa_ha_fsi__cpiarhsifalrtgrrs_eesr__atiipiditetrdhngktcoaroliwircatiirao_ttgranrire_l_mtuiheetseeobr_icipsusbraednaatimskztifrr__rfkpesuseaeiaoant_sragiictittnrc_rcrfbostp_lsfchmlerslklhjsdopiaiuteahgeugaeairfieco_rhtcavattetaknihtbrahitutfdeps_lsr_irpkipbixgwlpetftaetlhul_rohrese_cati_lii__seuaarldtleie_oalwhdta_m_nctittacscsarao_ulesiuie__ifyii_encoxrretan_re_spoati__t_traafpvorekteaamhaaaaaserhsihdviausctpieirroe_ni__ririigruwlwasporrq_r_rlltesap_diiiucwrwnpislnlnasulealmloliiiaefe_kniluuiimn_reoue_hlapgpeneaprntiytssaicas_hoarghleestorq_ryn_es_rcnoskitdootuefwtaris_hr__uhatsatneleiswlrrsip_wmane_qpikn_traa_urxttsuioiasaswfeerfa_telrnishisflatrtpttnrrao_aalsiiatiidetiid__rrriyavecteglreafrwie_iln_esyihldilroi_rehe_ikrdot_ilslall_nsaaro__iiviai_f_ntraaenan_slpceasilaiifer_oorigiieprelcrtmrtspliito_yhtsigosiosrsnlttroristhanmeneetsls

united_check_in_online

gojet_airlines continental_air

orbitz_com_flights air_watchdog
usairway e175_aircraft

(a) automotive-related ad (b) air travel-related ad

Figure 2: Examples of most similar queries to select ads

The data set Dad = {si, i = 1, ..., S} consists of sessions si comprising both search queries and ad clicks, si = (qi1, qi2, ai3, qi4, ai5..., qiMi ), where qim refers to search queries, and aim refer to ad click events. Moreover, to further improve learned query representations we also considered adding search link clicks to the user sessions (i.e., clicks that a user made on links given as search results, also referred to as organic results), motivated by the idea behind QFG approach. To this end, we expanded data set Dad to obtain data set Dad+link = {si, i = 1, ..., S}, formed by further adding link click events lim to search sessions si.
5.2 Training details

5.2.1 Handling "search stopwords"

Unlike in everyday language, where the most frequent to-

kens are articles and prepositions (e.g., a, the, in), in search

data the most frequent tokens are navigational queries, such

as google, yahoo, facebook, etc. Due to the fact that over

a longer period they occur in a direct neighborhood of ma-

jority of other queries, there is a risk that the embedding

space will shrink. We have empirically observed this phe-

nomenon, and during training the navigational queries tend

to pull all other queries towards them, preventing the vectors

from spreading further away in the hyperspace.

The authors of [31] suggested to deal with the frequent

tokens in news articles by downsampling. This heuristic

involved discarding words with probability P(wi) = 1 -

f

 (wi

)

,

where

f (wi)

is

the

frequency

of

word

wi,

and



is

a

user-set parameter. However, this approach is not applica-

ble in a context of web search which poses quite different re-

quirements. In particular, we aim to learn good representa-

tions of navigational queries while at the same time prevent

them from adversely influencing representation learning for

non-navigational queries. To achieve this objective, we pro-

pose a one-direction learning rule for navigational queries.

In particular, their vectors are being updated by vectors of

queries appearing in the context, but are not used to update

the vectors of other queries in their context. We identified

navigational queries editorially by evaluating the most fre-

quent 3,000 queries for navigational intent.

5.2.2 Training parameters
Models were trained using a cluster of 9 machines with 256GB of RAM memory and 24 cores. Dimensionality of the

387

Table 1: Differences in query rewrites of context2vec and context-content2vec for tail queries

Query

cx2vec

cx-cn2vec

minnesota insurance exam crossword puzzles microwave food safety what to cook in cast iron skillet iphone 6 repair services

satellite tv otego satellite tv menominee satellite tv west end satellite tv townsend satellite tv lake sara staphylococcal enteritis definition salmonella enteritis definition listeria monocytogenes prevention e coli cdc preventing cross contamination steak sauce substitute montreal seasoning ingredients ground turkey breakfast sausage how to cook steak on cast iron skillet reseason cast iron mp3attic music donar ovulos en elche credit a la consommation rapide smart phone repair service social security disability bronx ny

minnesota insurance minnesota insurance license practice exams online insurance exam crossword puzzles colorado insurance exam crossword puzzles online minnesota insurance exam crossword puzzles microwave oven food safety microwave baby food safety microwave food safety studies microwave food safety issues foodsafety.com cast iron skillet recipes how to cook with cast iron skillet how to cook in a cast iron skillet how to cook with a cast iron skillet chicken in cast iron skillet at&t iphone repair service iphone 5c repair service iphone repair iphone service repair iphone repair services

Table 2: Examples of query rewrites with and without ad clicks in training data (bolded rewrites matched bid-terms)

makeup (no ads) makeup (with ads) snowboarding (no ads) snowboarding (with ads) seafood (no ads) seafood (with ads)

makeup tips fashion makeup make up makeup pictures makeup images makeup tutorial

lipstick mac makeup makeup sets eye shadow makeup covergirl makeup items

snowbaording

snowboards

snow boarding

snowboarding gear

snowboarding information burton snowboarding

snowboarding jumps

snowboard deals

snowboard pics

snowboards on sale

shaun white snowboarding snowboarding mountains

sea food crab legs best seafood oysters lobster recipes seafood market

seafood restaurant seafood restaurants crab shack seafood market sea food sea food menu

embedding space was set to D = 300, context neighborhood size was set to 5 and content neighborhood size was set to 7. Finally, we used negative sampling to speed up the training, and used 10 random samples in each vector update.
5.3 Query rewriting models
We considered the following approaches in our empirical analysis, where each resulted in vector representation for 45 million most frequent queries.
1) word2vecnews model was used as a simple baseline. Query vectors were constructed by summing publicly available word vectors for word tokens in a query (whitespace was used as a token separator), trained on Google News data set with English stopwords removed1.
2) word2vecsearch query vectors constructed by summing word vectors for word tokens in a query, trained using Dcontent data set.
3) content2vec model (cn2vec) was trained using Dcontent where queries were used as a global context to the containing words, as illustrated in Figure 1b.
4) context2vec model (cx2vec) was trained using Dcontext, as illustrated in Figure 1a.
5) context-content2vec model (cx-cn2vec) used both Dcontent and Dcontext data to train query vectors, leveraging the two-layer architecture from Section 4.3. Since the model learns from both content and context, one of the motivations behind cx-cn2vec is to improve embeddings for queries that were not seen in many sessions. In Table 1 we give several illustrative examples of such tail queries. Unlike cx2vec, cxcn2vec relies more on content in case of rare queries, and thus provides better rewrites. For example, at the time of creation of our data set the query "iphone 6 repair service" was a tail query, resulting in poor cx2vec rewrites such as "mp3attic music" or "social security disability bronx ny". On the other hand, cx-cn2vec provided more relevant rewrites.
1https://code.google.com/p/word2vec

6) cx-cn2vecad model was trained using Dcontext and Dad (i.e., context data with ad clicks added). Training resulted in additional 8.5 million ad vectors.
As illustrated in Table 2, we can see that addition of Dad resulted in quite different query rewrites than using a data set without ad clicks. For example, rewrites for query "makeup" mainly contain terms that are related to tips, tutorials, and pictures when no ads were used in training. Conversely, rewrites for the same query when clicks on ads are considered contain more commercial search terms.
In addition, given ad and query vectors in the same embedding space, we can easily retrieve similar queries for any given ad. This by-product is extremely useful for suggesting new bid keywords for specific categories of ads (note that in our system ads are categorized into one or more interest categories, such as "sports" or "travel"). As an example, in Figure 2 we show K = 5,000 most similar queries to ads in "automotive" and "air travel" categories. Keywords with higher cosine similarity are shown with larger font sizes. We can observe that the key concepts of the category are well captured with the most similar queries.
7) cx-cn2vecad+link model was trained using Dcontent and Dad+link (i.e., context data with ad clicks and link clicks added). Training resulted in additional 19 million search link vectors and 8.5 million ad vectors.
8) QFGad+link model was trained using a click-flow graph constructed from Dad+link dataset, with additional 19 million search link vectors and 8.5 million ad vectors.
To produce rewrites for out-of-dictionary queries, embedding models generated their vectors by summing the existing vectors of word tokens within queries (excluding stopwords). Unlike the embedding methods, QFG could not produce rewrites for queries that were not seen in the graph.
Our evaluation did not include topic models such as LDA [5] or PLSA [19], as earlier research [20] found that these methods perform poorly on short text documents.

388

Table 3: Mean and standard deviation (in brackets) of query similarity (cosine distance) between pairs of editorially judged query rewrites (top: in-dictionary queries, bottom: out-of-dictionary queries)

grade

pairs

Excellent 1,518

Good

5,531

Fair

4,021

Bad

4,229

avg. p-value

-

Excellent 2,119

Good

11,305

Fair

11,146

Bad

7,849

avg. p-value

-

cn2vec

cx2vec

cx-cn2vec cx-cn2vecad cx-cn2vecad+link word2vecnews word2vecsearch QFGad+link

0.630 (0.136) 0.658 (0.107) 0.669 (0.107) 0.668 (0.114) 0.733 (0.094) 0.818 (0.146) 0.648 (0.151) 0.329 (0.667)

0.599 (0.136) 0.621 (0.125) 0.637 (0.100) 0.632 (0.104) 0.683 (0.097) 0.770 (0.152) 0.614 (0.155) 0.205 (0.574)

0.550 (0.167) 0.565 (0.129) 0.577 (0.124) 0.566 (0.130) 0.605 (0.132) 0.749 (0.190) 0.567 (0.173) 0.114 (0.366)

0.398 (0.196) 0.363 (0.170) 0.349 (0.184) 0.336 (0.187) 0.425 (0.179) 0.517 (0.280) 0.395 (0.201) 0.166 (0.584)

1.39e-15

5.44e-26

8.27e-28

2.99e-30

1e-100

4.121e-07

1.014e-14

0.013

0.791 (0.166) 0.623 (0.141) 0.628 (0.134) 0.623 (0.143) 0.668 (0.147) 0.824 (0.145) 0.790 (0.157)

-

0.752 (0.155) 0.587 (0.135) 0.592 (0.130) 0.584 (0.137) 0.612 (0.141) 0.796 (0.145) 0.756 (0.156)

-

0.715 (0.136) 0.561 (0.145) 0.565 (0.139) 0.558 (0.146) 0.584 (0.147) 0.769 (0.175) 0.707 (0.141)

-

0.635 (0.199) 0.383 (0.211) 0.387 (0.209) 0.382 (0.212) 0.410 (0.208) 0.509 (0.311) 0.602 (0.208)

-

4.99e-26 1.137e-27 4.0423e-32 1.196e-30

2.926e-40

2.038e-16

9.388e-22

-

NDCG @ K

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.51

5

cx-cn2vec ad + link cx-cn2vec ad cx-cn2vec cx2vec cn2vec QFG ad + link word2vec search word2vec news

10

15

20

K

Figure 3: NDCG@K for different competing methods
5.4 Evaluation
In the following two sections we show our main experimental results, where we report performance of the competing approaches in terms of relevance and ad coverage of query rewrites. We also provide examples of rewrites by cx-cn2vecad+link method in an online video2.
5.4.1 Relevance
We used editorial judgments of query rewrites to compare query rewriting methods in terms of relevance.
In-house data. The first data set we used is an inhouse data set consisting of the query and several rewrites produced by current production system that were graded editorially. The editors were instructed to assign the following grades: bad, fair, good, and excellent. In total, the data set includes more than 40,000 (query, rewrite, grade) tuples, such as ("nfl news", "latest nfl news", "excellent"), ("nfl news", "nfl shooting", "fair"), ("nfl news", "nfl rumor", "good"), ("metro transit", "metro pcs", "bad").
Given a query we ranked the rewrite candidates based on a model's output score and retrieved the top K candidates. Next, we computed the NDCG metric using editorial grades of rewrites as labels (0 for bad, 1 for fair, 2 for good, and 3 for excellent) at values of K ranging from 1 to 20. In Figure 3 we report the results for different models.
2http://youtu.be/pvfFQSCYhqI

By considering the reported results, several conclusions can be drawn. Regarding models that did not utilize search session data (i.e., word2vecnews, word2vecsearch, and cn2vec), we can see that the word embeddings specifically tailored for search queries perform better than using word embeddings learned using news data. In addition, learning query vectors as global context of words using cn2vec leads to slightly better results than directly summing the words. However, all three models performed worse than the QFGad+link baseline method, which made use of the co-occurrence of queries, ads, and links in search sessions. Query embeddings trained directly on search sessions, i.e. cx2vec, already outperform QFGad+link. Further improvements were observed when embeddings were learned from both context and content. Finally, incorporating ad and link click events showed incremental boost in relevance. The largest gain was observed when links were added as an additional context.
In addition, in Table 3 we report average similarities of (query, rewrite) pairs in each editorial grade group. It is important to note that scores are comparable only within the same method and not across methods. For conclusive comparison we calculated the level of separation between the groups by p-value of t-test, which tests the hypothesis that the means of two neighboring grades are equal. By comparing p-values reported we can quantify which method does the best job of separating the four grade groups. Findings are similar to the ones from Figure 3. Additionally, we find that QFGad+link method has issues with "bad" grade group, and that without incorporating search context, the embedding models have similar average scores in "good" and "fair" grade groups. As expected, separation improves when embedding models incorporate search context, with the standard deviation between the groups reducing even further when ads and links were considered as an additional context.
In the case when one or both queries from the editorial grade list were not found in a model, we generated the vectors by summing vectors of query tokens. Bottom part of Table 3 shows results for such cases. We can observe that similar conclusion hold even for such out-of-dictionary queries.
TREC data. The second data we used was a publicly available TREC Web Track data set3 from 2009 to 2013, containing a total of 250 queries. Using the competing methods we produced 5 rewrites for each query, and evaluated the result editorially. The editors were given instructions to rate the rewrites in the following way: grade 0 if the rewrite is irrelevant, grade 1 if relevant, and grade 2 if it is excellent.
3http://trec.nist.gov/

389

Table 4: Examples of rewrites obtained using the competing methods (bolded rewrites matched bid-terms)

Original

QFGad+link

cn2vec

cx-cn2vec

cx-cn2vecad

wedding budget

monthly budget calculator

wedding planning checklist wedding budget worksheet

wedding

wedding cost calculator online budget calculator

wedding budget template

wedding vendors

budget

wedding calculator

budget wedding

wedding budget worksheet the knot

calculator wedding cost breakdown

budget calculator free

wedding checklist printable

wedding planning checklist

wedding budget worksheet average wedding budget

wedding costs average

wedding wire

gmat prep

gmat prep online

gmat preparation courses

gmat study books

gmat classes

gmat prep courses

sample gmat tests

gmat test prep classes

gmat prep gmat

gmat online prep

gmat prep class

gmat prep class

classes

kaplan gmat course

online gmat prep

which is easier gre or gmat

kaplan gre courses

kaplan gmat

best gmat prep courses

how much is the gmat

free gmat sample tests

building a fence

how to build fence

how to build a fence on a hill how to build a fence minecraft

build your own fence

build fence

how to fence a yard

fancy fences and gates

how to build how to build a wood fence build a fence

how to build a fence gate

how to build a metal fence

a fence

do it yourself fence

how to build a cheap fence

how to build a brick wall fence home depot com fencing

how to build a privacy fence build your own fence

how to build a fence video back yard fences

solar panels for homes

solar panels for your home

solar power

solar power

solar electric panels

solar panels for residential homes solar energy

solar panels for homes

solar panels solar power

solar panels for

solar panels for homes

solar panels on sale

ebay solar panels

solar panels on ebay

solar power systems

solar panel kits

how to make solar panels

davis solar

solar panel

solar panels for sale

Editorial grades for each method as well as the Levenshtein distance [29] were averaged and reported in Table 5. We can see that the cx-cn2vecad+link query embedding method, learned from query content and search session context including ad and link clicks, returned the most relevant rewrites and outperformed the baseline QFG method. Once more, learning query embeddings from content of queries on its own was not enough to outperform QFG. Interestingly, there was just a small difference between cx2vec and cx-cn2vec models. It can be explained by the fact that cx-cn2vec generally improves rewrites for tail queries, while TREC data set mostly consists of frequent queries.
In addition to providing highly relevant rewrites, cxcn2vecad+link showed the highest diversity, a favorable property for rewrite algorithms. Considering the performance in terms of Levenshtein distance, we can observe that rewrites produced by models which learned from content are less diverse than the ones produced by approaches that modeled query context during training.
Examples of query rewrites obtained by the competing methods are given in Table 4. As can be seen, there exist significant differences between the rewrites of the competing approaches. As discussed earlier, the content-based model cn2vec is sensitive to cases when the query words appeared in different contexts across the query dictionary. For example, words "budget" and "calculator" from the query "wedding budget calculator" mostly appeared in more general financial contexts. For this reason in the first 5 rewrites we have queries that are not related to wedding. The model that utilized both content and context data cx-cn2vec outputs highly relevant, interesting rewrites that capture a number of meanings and contexts of the original query. The model that considered ads during training cx-cn2vecad produced more commercial rewrites, referencing stores and sales. We also bolded queries that matched the actual bidterms in the system, discussed in more detail in the following experiment. Examples of query rewrites produced by cx-cn2vecad+link are shown in an online video4. The video mostly covers queries from TREC 2010 data set.
4http://youtu.be/n5kHKyKQAa8

Table 5: Comparison of query rewrite methods (TREC data)

Method

Editorial grade Levenshtein dist.

QFGad+link word2vecnews word2vecsearch cn2vec
cx2vec
cx-cn2vec
cx-cn2vecad cx-cn2vecad+link

1.0441
0.9189 0.9492 0.9571 1.1273 1.1343 1.2281 1.2457

11.70
10.91 11.32 11.37 13.79 13.13 13.62 13.25

5.4.2 Ad coverage
In the previous section we showed the advantage of proposed methods in terms of relevance. In this section we evaluate how beneficial those rewrites are from the ad matching perspective. It is of fundamental interest to sponsored search to ensure that the rewrites being provided as alternatives to the users' queries match as many additional relevant bidterms as possible. The percentage of rewrites that match bid terms is defined as coverage. We conducted an off-line experiment to compare query rewriting methods: 1) on an in-house dataset of 2,000 editorially selected queries which the editors deemed representative of the query set; and 2) on the TREC data set. For each query in the data sets we generated K = 5 rewrites and look-up bidterms in our current sponsored search demand. We report average coverage (relative improvement over QFG) over the entire set of queries for each of the two data sets. Table 6 summarizes the results of our analysis.
To estimate monetary value of each query rewriting method we used two proxy measures. First, for the query rewrites produced by each method we look-up the bid amounts for the queries that were matched in the bidterm database. We report the total sum of these amounts, and refer to this metric as "revenue potential". In addition, for each rewrite method we calculated effective cost per mille (eCPM). Given a query rewrite q, and an ad a for which q was a bidterm, we calculated eCPM value eqa by multi-

390

Table 6: Relative improvement over the QFG method of different query rewrite methods on query-bid data

In-house data

TREC data

Method

Coverage Revenue potential eCPM Coverage Revenue potential eCPM

QFGad+link word2vecnews word2vecsearch cn2vec

1.00 0.76 0.87 0.89

1.00

1.00

1.00

0.39

0.46

0.32

0.58

0.77

0.57

0.62

0.84

0.59

1.00

1.00

0.66

0.73

0.84

0.75

0.84

0.74

cx2vec

1.16

1.80

1.41

1.41

1.16

1.20

cx-cn2vec

1.18

1.86

1.38

1.44

1.21

1.19

cx-cn2vecad cx-cn2vecad+link

1.20 1.18

1.89 1.88

1.60 1.45

1.52 1.50

1.35 1.28

1.31 1.22

plying cost per click (CPC) dollar amount and the clickthrough rate (CTR) of that (query, ad) pair, computed as number of ad clicks divided by number of ad impressions. Finally, we report the weighted sum of eCPM's, q a wqeqa, where the weight wq is proportional to the number of times the query appeared in the search logs. Considering the results presented in Table 6, we can see that query embedding models trained using news documents and query content achieved lower average coverage than QFG method. However, QFG was in turn outperformed by the coverage of rewrites produced using query embedding approaches.
Moreover, by taking the results from both experiments into account, it can be concluded that cx-cn2vecads+links is the best choice as it achieves the highest relevance while maintaining large ad coverage. When link clicks were added on top of ad clicks we observed a large improvement in relevance, with ad coverage remaining almost the same.
6. CONCLUSION
In this paper we described novel query rewriting methods based on recently proposed neural language models. The methods learn low-dimensional, distributed representations of search queries based on context (context2vec), content (content2vec), or combined context and content (contextcontent2vec) of the queries within search sessions. To specialize the query rewrites for sponsored search application, we further incorporated ad clicks and search link clicks into the training data. We evaluated the proposed methods using both in-house and publicly available TREC data sets. When compared to the current state-of-the-art approaches, we showed that context-content2vec generates the most relevant query rewrites, while at the same time maintains high level of ad coverage. The results clearly indicate significant advantages of context-content2vec over the state-of-the-art query rewrite algorithms, and suggest high monetization potential of the query embedding approach to the task of sponsored search advertising. In our ongoing work, we plan to experiment with more involved search sessionization algorithms and navigational query detection algorithms.
7. REFERENCES
[1] M. Aly, A. Hatch, V. Josifovski, and V. K. Narayanan. Web-scale user modeling for targeting. WWW, 2012.
[2] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query recommendation using query logs in search engines. In Proceedings of the 2004 International Conference on Current Trends in Database Technology, EDBT'04,

pages 588­596, Berlin, Heidelberg, 2004. Springer-Verlag.
[3] R. Baeza-Yates, B. Ribeiro-Neto, et al. Modern information retrieval, volume 463. ACM press New York, 1999.
[4] Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L. Gauvain. Neural probabilistic language models. In Innovations in Machine Learning, pages 137­186. Springer, 2006.
[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993­1022, 2003.
[6] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: Model and applications. In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM '08, pages 609­618, New York, NY, USA, 2008. ACM.
[7] F. Bonchi, R. Perego, F. Silvestri, H. Vahabi, and R. Venturini. Efficient query recommendations in the long tail via center-piece subgraphs. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '12, pages 345­354, New York, NY, USA, 2012. ACM.
[8] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pages 2787­2795, 2013.
[9] D. E. Bowman, M. L. Hamrick, T. R. Kohn, R. E. Ortega, and J. R. Spiegel. Refining search queries by the suggestion of correlated terms from prior searches, Dec. 21 1999. US Patent 6,006,225.
[10] A. Broder. A taxonomy of web search. In ACM Sigir forum, volume 36, pages 3­10. ACM, 2002.
[11] A. Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich, V. Josifovski, and L. Riedel. Search advertising using web relevance feedback. In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM '08, pages 1013­1022, New York, NY, USA, 2008. ACM.
[12] Y. Chen, D. Pavlov, and J. F. Canny. Large-scale behavioral targeting. KDD, 2009.
[13] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493­2537, 2011.

391

[14] N. Djuric, H. Wu, V. Radosavljevic, M. Grbovic, and N. Bhamidipati. Hierarchical neural language models for joint representation of streaming documents and their content. In International World Wide Web Conference (WWW), 2015.
[15] D. C. Fain and J. O. Pedersen. Sponsored search: A brief history. Bulletin of the American Society for Information Science and Technology, 32(2):12­13, 2006.
[16] D. Gayo-Avello. A survey on session detection methods in query logs and a proposal for future evaluation. Inf. Sci., 179(12):1822­1843, May 2009.
[17] M. Grbovic, N. Djuric, V. Radosavljevic, and N. Bhamidipati. Search retargeting using directed query embeddings. In International World Wide Web Conference (WWW), 2015.
[18] M. Grbovic and S. Vucetic. Generating ad targeting rules using sparse principal component analysis with constraints. WWW, 2014.
[19] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50­57. ACM, 1999.
[20] L. Hong and B. D. Davison. Empirical study of topic modeling in twitter. In Proceedings of the First Workshop on Social Media Analytics, pages 80­88. ACM, 2010.
[21] A. K. Jain, L. Hong, and S. Pankanti. Iab internet advertising revenue report: 2013 first six months' results. Technical report, Interactive Advertising Bureau, 2013.
[22] B. J. Jansen and T. Mullen. Sponsored search: An overview of the concept, history, and technology. International Journal of Electronic Business, 6(2):114­131, 2008.
[23] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web, pages 387­396. ACM, 2006.
[24] F. Keller and M. Lapata. Using the web to obtain frequencies for unseen bigrams. Computational linguistics, 29(3):459­484, 2003.
[25] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neural language models. In Proceedings of the 31th International Conference on Machine Learning, 2014.
[26] R. Kiros, R. S. Zemel, and R. Salakhutdinov. A multiplicative model for learning distributed text-based attribute representations. arXiv preprint arXiv:1406.2710, 2014.
[27] V. Lavrenko and W. B. Croft. Relevance based

language models. In SIGIR, pages 120­127. ACM, 2001.
[28] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053, 2014.
[29] V. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet Physics-Doklady, volume 10, 1966.
[30] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.
[31] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111­3119, 2013.
[32] S. Pandey, M. Aly, A. Bagherjeiran, A. Hatch, P. Ciccolo, A. Ratnaparkhi, and M. Zinkevich. Learning to target: what works for behavioral targeting. CIKM, 2011.
[33] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. arXiv preprint arXiv:1403.6652, 2014.
[34] PwC. Global entertainment and media outlook: 2014-2018. Technical report, 2014.
[35] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz. Analysis of a very large web search engine query log. In ACm SIGIR Forum, volume 33, pages 6­12. ACM, 1999.
[36] F. Silvestri. Mining query logs: Turning search usage data into knowledge. Found. Trends Inf. Retr., 4:1­174, Jan. 2010.
[37] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926­934, 2013.
[38] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384­394. Association for Computational Linguistics, 2010.
[39] H. Vahabi, M. Ackerman, D. Loker, R. Baeza-Yates, and A. Lopez-Ortiz. Orthogonal query recommendation. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys '13, pages 33­40, New York, NY, USA, 2013. ACM.
[40] W. V. Zhang and R. Jones. Comparing click logs and editorial labels for training query rewriting. In WWW 2007 Workshop on Query Log Analysis: Social And Technological Challenges, 2007.

392


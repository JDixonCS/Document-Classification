On the Measurement of Test Collection Reliability

Julián Urbano jurbano@inf.uc3m.es

Mónica Marrero mmarrero@inf.uc3m.es

Diego Martín dmartin@dit.upm.es

University Carlos III of Madrid Department of Computer Science
Leganés, Spain

Technical University of Madrid Department of Telematics Engineering
Madrid, Spain

ABSTRACT
The reliability of a test collection is proportional to the number of queries it contains. But building a collection with many queries is expensive, so researchers have to find a balance between reliability and cost. Previous work on the measurement of test collection reliability relied on databased approaches that contemplated random what if scenarios, and provided indicators such as swap rates and Kendall tau correlations. Generalizability Theory was proposed as an alternative founded on analysis of variance that provides reliability indicators based on statistical theory. However, these reliability indicators are hard to interpret in practice, because they do not correspond to well known indicators like Kendall tau correlation. We empirically established these relationships based on data from over 40 TREC collections, thus filling the gap in the practical interpretation of Generalizability Theory. We also review the computation of these indicators, and show that they are extremely dependent on the sample of systems and queries used, so much that the required number of queries to achieve a certain level of reliability can vary in orders of magnitude. We discuss the computation of confidence intervals for these statistics, providing a much more reliable tool to measure test collection reliability. Reflecting upon all these results, we review a wealth of TREC test collections, arguing that they are possibly not as reliable as generally accepted and that the common choice of 50 queries is insufficient even for stable rankings.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation.
General Terms
Experimentation, Measurement, Reliability.
Keywords
Test Collection, Evaluation, Reliability, Generalizability Theory, TREC.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION

The purpose of evaluating the effectiveness of an Informa-

tion Retrieval (IR) system is to assess how well it would sat-

isfy real users. The main tool used in these evaluations are

test collections, which comprise a collection of documents

to search, a set of queries Q, and a set of relevance judg-

ments that contains information as to what documents are

relevant, and to which degree, to the queries [16]. Given

the results returned by a system A for one of the queries

q  Q, an effectiveness measure uses the information in the

relevance judgments to compute a score q,A that represents

the effectiveness of the system for that query. After run-

ning the system for all queries in the collection, the average

Q,A

=

1 |Q|

i,A is usually reported as the main measure

of system effectiveness, representing the expected behavior

of the system for an arbitrary new query. When comparing

two systems A and B, the main measure reported is the av-

erage effectiveness difference Q,AB = Q,A - Q,B. Based

on this difference, we conclude which system is better.

The immediate question to ask is: how reliable are those

conclusions about system effectiveness? Ideally, researchers

would evaluate the system with the set of all possible queries

that a user might request. In such a case, we could be sure

that the true average performance of the system corresponds

to the score we computed with the collection. The prob-

lem is that building such a collection is either impractical

for requiring an enormous amount of queries and relevance

judgments, or just impossible if the potential query set is

not defined, which use to be the case because we can not

account for future queries that do not yet exist. Therefore,

the query set Q in a test collection must be regarded as

a sample from the universe of all queries, and the sample

mean Q,A as an estimate of the true effectiveness mean A.

But because we are estimating this score with a sample of

queries, our estimates are erroneous to some degree. The

results may change drastically with a different query set Q,

so much that differences between systems could be reversed.

An evaluation result is reliable if it can be replicated with

another collection: if the set of queries Q suggests that sys-

tem A outperforms system B, we can be very sure that the conclusion would hold for a different set of queries Q, and in

the end, for the universe of all queries. A simple way to make

a collection reliable is to include many queries; the more we

employ the smaller the variance of the estimates and thus

the more reliable the conclusion. The problem is that more

queries also means more cost to create the collection, so re-

searchers have to find a balance between the reliability of

the results and the cost of the collection. To this end, it is

necessary to develop indicators of test collection reliability.

393

Several works in the last fifteen years have studied the problem of reliability in IR evaluation experiments. The basic methodology consisted in evaluating a series of systems with two different and random sets of queries, computing several reliability indicators that measured how similar those evaluations were. Using different query sample sizes and randomizing query selection, researchers were able to map query set size to reliability and extrapolate results to larger query sets. The data used consisted in runs submitted to several TREC tracks (mostly the Ad Hoc tracks), and the sets of queries employed in each edition. While these approaches are clearly faithful to the data, they are limited in that the full query set had to be partitioned in two disjoint sets to comply with the assumption that they were independent.
In 2007 Bodoff and Li [6] proposed Generalizability Theory (GT) as an alternative [7, 18]. GT is grounded on analysis of variance components, which allows to dissect the variability in effectiveness scores and figure out how much of it is due to system differences, query difficulty, assessors, etc. In an ideal evaluation setting, we would like all variance to be due to actual differences between systems and not due to query variability; if the queries in the collection are too varied, or differences between systems too small, then we need many queries to ensure that our estimates are reliable. From these variance components GT allows researchers to estimate the reliability of a test collection even before it has been created. Based on some previous data, GT can estimate the reliability of a collection with a larger number of queries, more than one assessor providing judgments for each query, etc. GT provides indicators for the stability of both the absolute scores and the relative differences by computing different variance ratios.
The main advantages of GT against the traditional databased approaches are that 1) it is based on statistical theory, 2) it is easy to employ because it does not require tedious and repetitive what if scenarios, and 3) it allows us to estimate the reliability of a collection or experimental design that does not exist yet. But it has disadvantages too: 1) it is unknown the extent to which reliability indicators are affected by the data used to estimate variance components, and 2) it is very hard to interpret them in practical terms.
We address these two problems of GT applied to the measurement of test collection reliability. In the next section we review past work following data-based approaches and the reliability indicators used. We then review the use of GT and discuss the motivation for this work. In Section 3 we show how the initial data used in GT studies has a very large effect on the results, discussing minimum sample sizes and interval estimators. Section 4 reports a study to provide an empirical mapping between GT-based indicators of reliability and the well known data-based ones. Next we discuss the reliability of several TREC collections based on the results from previous sections, presenting conclusions in Section 6.
2. INDICATORS OF RELIABILITY
Several indicators of test collection reliability have been proposed in the literature. This section reviews traditional indicators found in the early data-based studies and the GTbased indicators more recently proposed.
2.1 Data-based Indicators
Given a query set Q and a similar set Q of the same size, we can define the following data-based reliability indicators:

· Kendall correlation ( ), compares the order in which systems are ranked according to Q and Q, regardless of the magnitude of the differences AB. It ranges from 1 (same rankings) to -1 (reversed rankings), counting the number of system pairs that are swapped between the two rankings. For Q to be reliable,  must therefore tend to 1.
· AP correlation (AP ), adds a top-heaviness component to Kendall  , such that swaps between systems towards the top of the rankings are more penalized than swaps towards the bottom [23].
· Power ratio (), is the fraction of pairwise system differences that result statistically significant according to query set Q. If the difference Q,AB between two systems is deemed as statistically significant, it serves as further evidence that the true difference AB has the same sign. For Q to be reliable,  must therefore tend to 100%. In this paper we compute standard 2-tailed t-tests at the 0.05 level [19].
· Minor Conflict ratio (-), is the fraction of statistically significant differences with Q that have a sign swap with Q but are not statistically significant there. - is therefore the fraction of uncertain conclusions when measuring statistical significance, so for Q to be reliable - must therefore tend to 0%.
· Major Conflict ratio (+), is the fraction of statistically significant differences with Q that are also significant with Q but have a sign swap. + is therefore the fraction of incorrect conclusions when measuring statistical significance, so for Q to be reliable + must therefore tend to 0% as well.
· Absolute Sensitivity (a), is the minimum absolute difference Q,AB that need be observed between any two systems such that the differences with Q have the same sign at least 95% of the times. For Q to be reliable, a must therefore tend to 0, meaning that even small differences can be trusted.
· Relative Sensitivity (r), is the minimum relative difference Q,AB/ max Q,A, Q,B that need be observed with Q such that the differences with Q have the same sign at least 95% of the times. For Q to be reliable, r must therefore tend to 0% too.
· Root Mean Squared Error (), measures the difference between the absolute scores with Q and with Q. Thus, for Q to be reliable  must tend to 0 too.
One of the first reliability studies was conducted in 1998 by Voorhees [20], who analyzed the effect of having different assessors provide relevance judgments. Employing a methodology based on randomization, she concluded that the absolute scores could suffer wide variations between assessors, but that the ranking of systems was seldom altered, establishing  = 0.9 as the de facto minimum on ranking similarity. She also studied swap rates as a function of  and suggested a minimum of 25 queries to have a somewhat stable ranking. Also in 1998, Zobel [24] studied the effect of pool depth on absolute system scores, extrapolating trends to larger pool depths. He also compared different statistical procedures in terms of power and conflict ratios.
Buckley and Voorhees [8] compared in 2000 the reliability of various effectiveness measures by mapping effectiveness differences to error rates. Extrapolating to 50 queries, they concluded that   0.05 produced less than 1.5% system swaps when computing Average Precision (AP), while

394

other measures such as Precision at cutoff 10 (P@10) produced 3.6% of swaps. In 2002, Voorhees and Buckley [22] extended their work with other collections and methods, but again extrapolating trends. They concluded that with 50 queries the sensitivity of AP was a = 0.05, while increasing the query set size to 100 would yield a = 0.03. They also reported large differences across collections and effectiveness measures. Lin and Hauptmann [13] showed that the empirical model used by Voorhees and Buckley can be derived theoretically, and that the three factors affecting reliability are query set size, mean effectiveness scores, and variability of scores. Sanderson and Zobel [17] also revisited this work by computing relative sensitivity and incorporating statistical procedures to account for score variability. They concluded r = 10% with AP if coupled with statistical significance, and r = 25% if not. They observed very similar relative sensitivity between AP and P@10, arguing the use of more queries with fewer judgments as previous work suggested that much of the score variability is due to queries [4].
In 2007 Sakai [15] used similar methods to compare the reliability of several effectiveness measures, though he did not extrapolate to larger query sets. He computed  correlations, absolute sensitivity a and a variation of r, and observed that these indicators were not very correlated with statistical significance, arguing the importance of considering score variability rather than just means. Voorhees revisited in 2009 [21] the use of statistical procedures with the TREC Robust 2004 collection, computing reliability indicators with an unprecedented set of 100 queries, therefore avoiding the need to extrapolate to the usual size 50. When using AP, she observed power  = 47% and conflict ratios - = 2.7% and + = 0.04%. She showed again that P@10 is less reliable than AP also in these terms; and that nDCG showed higher reliability (agreeing with Sakai [15]). She also found that minor conflicts were usually coupled with large relative differences, thus suggesting that researchers employ several large collections to draw general conclusions.

2.2 GT-based Indicators
Bodoff and Li [6] proposed Generalizability Theory [7, 18] as an alternative to measure test collection reliability that directly addresses variability of scores rather than just the mean as was common before. GT has two stages: a Generalizability study (G-study) to estimate variance components based on previous data, and a Decision study (D-study) that subsequently computes reliability indicators for a different experimental design. We consider a fully crossed design and decompose variability of scores into three components: variance due to actual differences among systems (s2), variance due to differences in difficulty among queries (q2), and variance due to the system-query interaction effect whereby some systems are particularly good (or bad) for some queries (s2:q). The variance due to other effects, such as assessors, is in our case confounded with the interaction effect.
Using Analysis of Variance (ANOVA) procedures, these variance components can be estimated from previous data:

^s2:q = ^e2 = EMresidual

(1)

^s2

=

EMs - nq

^e2

(2)

^q2

=

EMq - ^e2 ns

(3)

where EM is the expected Mean Square of component , and ns and nq are the number of systems and queries [7, 18]. These estimates can be used to compute the proportion of total variance that is due to each of the effects, such as how much of it is due to actual differences between systems.
In the D-study, we can use the variance estimates from the G-study to compute the reliability of a larger query set. To this end, two reliability indicators are usually employed:

· Generalizability Coefficient (E2), is the ratio of system variance to itself plus relative error variance:

E2

nq

=

s2

s2

+

2 e
n

(4)

q

and it provides a measure of the stability of relative differences between systems . By extension, it measures the reliability of the ranking. For a collection to be reliable, E2 must therefore tend to 1.
· Index of Dependability (), is the ratio of system variance to itself plus absolute error variance:

 nq

=

s2

s2
+ 2+2 qe nq

(5)

and it provides a measure of the stability of absolute effectiveness scores . For a collection to be reliable,  must therefore tend to 1 as well.

The main advantage of these indicators is that they allow us to estimate the reliability of an arbitrary query set size nq, so there is no need to follow the traditional methodologies based on random what if scenarios and extrapolation. From equations (4) and (5) it can be seen that the reliability of the collection increases as nq increases, because the estimates of query difficulty (i.e. average system performance per query) are more precise. These indicators were used by Kanoulas and Aslam [12] to derive the gain and discount functions of nDCG that yield optimal reliability when nq is constant.
With simple algebraic manipulation, we can calculate the minimum number of queries needed to reach some level of relative or absolute stability :

nE2 () =

 · e2 s2 (1 - )

(6)

n () =

 q2 + e2 s2 (1 - )

(7)

which can be used to estimate how many more queries we need to add to our collection for it to be reliable. The main use of this approach can be found in the TREC Million Query Track [2, 1], which set out to study whether many queries with a few judgments yield more reliable results than a few queries with many judgments. The conclusion was that nq  80 queries are sufficient for a reliable ranking, while nq  130 are needed for reliable absolute scores.

2.3 Motivation
The two problems of GT can be clearly spotted at this point. First, equations (1) to (3) show that we do not compute the true 2 variance components, but just estimates ^2 based on some previous data. If we use a different, yet similar set of systems or queries to estimate these variance components, the resulting E^2 and ^ scores might be very

395

Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011

Span covering 95% of ^ observations 0.0 0.2 0.4 0.6 0.8 1.0

Span covering 95% of E^2 observations 0.0 0.2 0.4 0.6 0.8 1.0

Variability due to queries

20

40

60

80

100

Initial number of queries in the G-study

20

40

60

80

100

Initial number of queries in the G-study

Span covering 95% of ^ observations 0.0 0.2 0.4 0.6 0.8 1.0

Span covering 95% of E^2 observations 0.0 0.2 0.4 0.6 0.8 1.0

Variability due to systems

20

40

60

80

100

Initial number of systems in the G-study

20

40

60

80

100

Initial number of systems in the G-study

Figure 1: Variability in E^2 (top) and ^ (bottom) scores as a function of the initial number of queries (left) and number of systems (right) used in the G-study to estimate variance components.

different. In a revised paper, Bodoff [5, §4.6] briefly discussed this issue and argued that differences are marginal. However, he reports the results when randomly selecting only one system per research group instead of all of them, and only one trial of such experiment. We argue that this situation is not representative because the full set of systems and the reduced set after removing runs by the same groups are actually very similar to begin with, so it is expected that reliability scores do not change much. Also, only one such randomly reduced set is compared, so there is really no evidence to support that claim. Likewise, he further suggests that as few as five queries or systems are often enough to provide stable estimates of the variance components in the G-study [5, §3.1]. We further analyze this issue in Section 3.
Second, equations (6) and (7) allow us to estimate the minimum number of queries nq to reach some stability level , but the greater question is: how much is stable enough? Bodoff [5] mentions that in most Social Science applications a stability coefficient of 0.8 is acceptable, but there is no similar standard for Engineering applications. Kanoulas and Aslam [12] set  = 0.95 as the target in their experiments, but this choice is arbitrary. In their analysis of the Million Query Track 2007 [2] and 2008 [1], Allan et al. [1] and Carterette et al. [9, 10] also set E2 = 0.95 as the target. They mention in a footnote that in their experiments E2 = 0.95 approximately corresponded to  = 0.9, but details are omitted. We study this issue in Section 4 by empirically mapping GT-based indicators onto data-based indicators that are easier to understand and use in practice.
3. VARIABILITY OF GT INDICATORS
To measure the effect of the number of queries and number of systems used in the G-study to estimate variance components, we use data from 43 TREC collections covering 12 tasks across 10 tracks, from TREC 3 to TREC 2011 (see Table 1). As in previous studies [22, 17, 6, 21], we remove

the bottom 25% of systems so that our results are not obscured by possibly buggy implementations. For each collection, we randomly selected nq = 5 queries and computed the variance components using the full set of systems. We then calculated E2 and  for the full query set size, and the required number of queries to reach 0.95 stability. This was repeated with increments in nq of 5 queries, up the maximum permitted by the collection or 100. For each query set size, we ran 200 random trials, each of which can be considered as the possible data available for a G-study when analyzing a test collection design. The same process was followed by varying the initial number of systems ns and using the full set of queries instead.
Figure 1 shows the variability in G-study results1. For each collection and initial number of queries used, the y-axis plots the length of the span covering 95% of the E^2 and ^ observations in the 200 random trials. The right hand side plots show the same span lengths, but for different number of systems used in the G-study. As expected, the queries have a larger effect. Most importantly, we see that the average span length with just 5 queries is about 0.5 across collections. That is, the stability estimates could be as low as 0.3 or as high 0.8, for example, just depending on the particular set of queries we use in the G-study. In fact, estimates of the minimum number of queries required can vary in orders of magnitude if not using enough data. For example, with as many as 30 initial queries and all 184 systems from the Microblog 2011 collection, GT may suggest from 63 to 133 queries to reach E2 = 0.95. Similarly, from 40 initial systems and all 34 queries from the Medical 2011 collection, GT may suggest from 109 to 566 queries. In general, at least 50 queries and 50 systems seem necessary for 95% of estimates to be within a 0.1 span. This means that GT may be trusted to measure the reliability of an existing collection, but that
1Given the amount of datapoints displayed in this paper, we recommend to access the full-color version available online.

396

L - nq

1,

U - nq

1

, where

(8)

L

=

Ms Me F:dfs ,dfe

U

=

Ms Me F1-:dfs ,dfe

nsL nsL +

nq

,

nsU nsU +

nq

, where

(9)

L

=

Ms2

- F:dfs,MsMe + (F:dfs, - F:dfs,dfe ) F,dfs,dfe Me2 (ns - 1)F:dfs,MsMe + F:dfs,dfq MsMq

U

=

Ms2

- F1-:dfs,MsMe + (F1-:dfs, - F1-:dfs,dfe ) F1-,dfs,dfe Me2 (ns - 1)F1-:dfs,MsMe + F1-:dfs,dfq MsMq

researchers should be cautious when planning a collection
based on the results of a handful of systems and queries.
These results clearly evidence the need for a measure of
confidence on GT indicators. Bodoff [5] suggests the use of
confidence intervals to account for this variability, but only
computes them for the variance components in the G-study.
Confidence intervals for the ultimately more useful D-study
can be worked out from various variance ratios (see equations (8) and (9)2). Feldt [11] derived exact 100(1 - 2)% confidence intervals for the ratio  = s2/e2 under the assumption of normally distributed scores. The confidence interval on E2(nq) is computed using the endpoints in (8):

E2

nq

=

nq  1 + nq

(10)

Arteaga et al. [3] derived approximate 100(1 - 2)% confidence intervals for the ratio  = s2/ s2 + q2 + e2 , again
assuming a normal distribution of scores. The confidence interval on  nq is computed using the endpoints in (9):



nq

= 1+

nq  nq - 1 

(11)

Brennan [7, §6] discusses different methods to compute confidence intervals in both G-studies and D-studies, showing that the above intervals work reasonably well even when the normality assumption is violated. The right hand side of Table 1 reports the point and 95% interval estimates of the stability of the 43 TREC collections we consider in this paper. These intervals provide a more suitable estimate of test collection reliability because they account for variability in the G-study. For example, researchers could use these intervals to infer the required number of queries to reach the lower endpoint of the interval instead of the point estimate:

nE2 () =

  (1 - )

(12)

n () =

 (1 - )  (1 - )

(13)

4. INTERPRETING GT INDICATORS
To empirically derive a mapping between GT-based and data-based reliability indicators, we again used the 43 TREC collections in Table 1. For each collection we proceeded as follows. Two random and disjoint query subsets of size nq = 10 were selected from the full set of queries; let these subsets be Q and Q. The full set of systems was evaluated with both query subsets, and all data-based reliability indicators in Section 2.1 were computed, along with the two GTbased indicators according to Q and Q. This was repeated
2F:df1,df2 is the quantile function of the F distribution with df1 and df2 degrees of freedom. In our fully crossed design, dfs = ns - 1, dfq = nq - 1, and dfe = (ns - 1)(nq - 1).

with increments in nq of 10 queries, up to the maximum permitted by the collection. For query subset size we ran
50 random trials, each trial providing us with 32 datapoints (E^2 and ^ according to Q and to Q, mapped to ^,^AP , ^, ^-, ^+, ^a, ^r and ^). Theoretically though, E2 is better related to  , AP , , -, + and a because it measures the stability of relative differences, while  is better related to
r and  because it measures the stability of absolute scores.
We thus mapped only these combinations.
Figure 2 shows the mappings. For each collection we fitted
a model with all available datapoints. However, we dropped points for which E^2 < 0.8 and ^ < 0.5 so that the trends
were not affected by mappings with such small stability to
be even practical. These thresholds were chosen based on
the observed stability of the 43 TREC collections; about
85% of them show larger stability scores (see Table 1). This
resulted in over 28,000 points for each plot. In the top three plots ( , AP and ) we fitted the model y = xa, where a is
the parameter to fit. This resulted in the desired theoretical
behavior that limx1 y = 1 and limx0 y = 0, that is, when all variability is due to system differences  should be 1
because the ranking cannot be altered, and if all variance is
due to queries then  should be 0 because the rankings are
completely random. Similarly, in the bottom four plots we fitted the model y = (1 - x)a, such that limx1 y = 0 and limx0 y = 1, that is,  should for example be 0 if there is no variability due to queries.
As the first plot shows, all 43 collections do actually need E2 > 0.95 to reach  = 0.9. In general, E2 = 0.95 corresponds to   0.85, and on average E2  0.97 is needed
across collections to reach  = 0.9. The two clear exceptions
are found in the Million Query Track. The 2008 collection
is the one that reaches the target  = 0.9 with the lowest stability (E2  0.93), while the 2007 collection needs the largest (E2  0.98). Note that these were the two collections for which the E2 = 0.95   = 0.9 correspondence
was established [1, 9, 10]. It should be noted here that these
fits have an exponential-like shape, meaning that it is hard to achieve a mid level of  , but once E2 is large enough
small improvements in stability translate into large improvements in  . However, the relation between nq and E2 has a logarithmic-like shape, meaning that it is increasingly more expensive to improve E2 to begin with. Thus, it should be
considered the required effort for slight improvements in  .
The second plot shows quite high AP scores at these levels of relative stability, but generally below  . This suggests
that the swaps in the rankings are still happening between
systems at the top of the rankings [23]. The third plot shows
that at these stability levels it is expected to observe statis-
tical significance in about 80% of system comparisons. In
the middle right plot we can see that the proportion of con-
flicting results is generally below the  = 0.05 significance level when E2  0.9.

397

Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011



0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Kendall correlation

AP correlation

Power ratio

1.0

1.0

0.9

0.9

0.8

0.8

0.7

0.7



AP

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.80

0.85

0.90

0.95

1.00

E2

Absolute Sensitivity

0.80

0.85

0.90

0.95

1.00

E2

0.80

0.85

0.90

0.95

1.00

E2

Minor Conflict ratio

0.20

0.12

0.15

0.09

0.10

-

0.06

a

0.05

0.03

0.00

0.00

0.80

0.85

0.90

0.95

1.00

0.80

0.85

0.90

0.95

1.00

E2

E2

Relative Sensitivity

RMS Error

0.15

0.10



r 0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.05

0.00

0.5

0.6

0.7

0.8

0.9

1.0

0.5

0.6

0.7

0.8

0.9

1.0





Figure 2: Mapping from GT-based to data-based reliability indicators on a per-collection basis.

Researchers interested in the particular mapping for one of these collections may use the estimates in Table 1 and the plots in Figure 2 to get a better understanding of the evaluation results and draw more informed conclusions. To assess the reliability of future collections and guide in their development process, we fitted a single model using all available data instead of one model per collection. Figure 3 shows these fits, along with 95% and 90% prediction intervals that theoretically cover 95% and 90% of all future observations. In terms of sensitivity, the middle left plots show that a  0.03 for E2  0.9, which is about 60% of what Voorhees and Buckley reported for the Ad Hoc tracks [22]; although the intervals cover their values well. In the bottom

left plot we see that r  20% for   0.75, generally agreeing with Sanderson and Zobel [17]. As to statistical significance, we replicated Voorhees's [21] study with random sets of 50 queries from the Ad Hoc 7-8 topics and Robust 2004 systems. The average relative stability is E^2  [0.81, 0.88], which corresponds to   [37%, 54%], -  [3.9%, 7.8%] and +  [0.38%, 1.3%]. These are again larger than she reported, but the intervals cover her values well.
Overall, these models produce a decent fit on the data, and they fill the gap between data-based methodologies and Generalizability Theory. They provide a valuable tool to rapidly assess and easily understand the reliability of a test collection design.

398

Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011



0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Kendall correlation

AP correlation

Power ratio

1.0

1.0

0.9

0.9

0.8

0.8

0.7

0.7



AP

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.80

0.85

0.90

0.95

1.00

E2

Absolute Sensitivity

0.80

0.85

0.90

0.95

1.00

E2

0.80

0.85

0.90

0.95

1.00

E2

Minor Conflict ratio

0.20

0.12

0.15

0.09

0.10

-

0.06

a

0.05

0.03

0.00

0.00

0.80

0.85

0.90

0.95

1.00

0.80

0.85

0.90

0.95

1.00

E2

E2

Relative Sensitivity

RMS Error

0.15

0.10



r 0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.05

0.00

0.5

0.6

0.7

0.8

0.9

1.0

0.5

0.6

0.7

0.8

0.9

1.0





Figure 3: General mapping from GT-based to data-based reliability indicators, with 95% (dark grey) and 90% (light grey) prediction intervals.

5. DISCUSSION
The last columns in Table 1 report point and 95% interval estimates of the stability of the 43 TREC collections we considered. Collections in the same group correspond to the same tasks, providing a historical perspective on the reliability of the collections used so far since 1994 and for a variety of tasks. For example, the average relative stability in the Ad Hoc collections was E2  [0.86, 0.93], which according to Figure 3 corresponds to   [0.65, 0.81]. For the Web Ad Hoc collections we find E2  [0.8, 0.93], which would correspond to   [0.53, 0.81]. There are large differences within some tasks, such as Web Distillation, Genomics, Terabyte

and Enterprise. This is further evidence of the variability in D-study results due to the data used in the G-study. Except for a few particular cases though, the computation of confidence intervals smooths the problem. Across collections the averages are E2 = 0.88 and  = 0.74, with some tasks having very low scores. According to Figure 3 the expected  correlation is 0.69 with variations from 0.49 to 0.95, that is, much lower than desired.
Figure 4 plots the historical trend of test collection reliability. The left plot shows that relative stability has varied in the (0.8,1) interval for the most part, but most importantly it suggests that the stability of collections has decreased very

399

Track Documents

Query Set

Measure ns nq

Ad Hoc 3 Disks 1 & 2

151-200

AP 40 50

Ad Hoc 4 Disks 2 & 3

201-250

AP 33 49

Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8

Disks 2 & 4 Disks 4 & 5 Disks 4 & 5 Disks 4 & 5

251-300 301-350  351-400  401-450 

AP 94 50 AP 74 50 AP 103 50 AP 129 50

WebAdHoc 8 WebAdHoc 9 WebAdHoc 2001 WebAdHoc 2009 WebAdHoc 2010 WebAdHoc 2011
WebDistillation 2002 WebDistillation 2003 WebDistillation 2004
WebDiversity 2009 WebDiversity 2010 WebDiversity 2011
Novelty 2002

WT2g WT10g WT10g ClueWeb09 ClueWeb09 ClueWeb09
.GOV .GOV .GOV
ClueWeb09 ClueWeb09 ClueWeb09
Disks 4 & 5

401-450 

AP 44 50

451-500

AP 104 50

501-550

AP 97 50

"W1-W50"  AP (MTC) 71 50

"W51-W100" 

AP 56 48

"W101-W150" 

AP 37 50

551-600

AP 71 49

TD1-TD50

AP 93 50

"WT04"

AP 74 75

"W1-W50"  -nDCG@20 48 50

"W51-W100"  -nDCG@20 32 50

"W101-W150"  -nDCG@20 25 50

50 from 300-450 

F 42 49

Novelty 2003 AQUAINT

N1-N50

F 55 50

Novelty 2004 AQUAINT

N51-N100

F 60 50

GenomicsAdHoc 2003 GenomicsAdHoc 2004 GenomicsAdHoc 2005
Robust 2003
Robust 2004
Robust 2005

MEDLINE MEDLINE MEDLINE
Disks 4 & 5 Disks 4 & 5 AQUAINT

"G1-G50"
"G51-G100"
"G101-150" 50 from 301-450 & 601-650 
301-450 & 601-700  50 from 301-700 

AP 49 50 AP 43 50 AP 62 49
AP 78 100 AP 110 249 AP 74 50

Terabyte 2004
Terabyte 2005
Terabyte 2006
TerabyteAll 2006 EnterpriseExpert 2005 EnterpriseExpert 2006 EnterpriseExpert 2007 EnterpriseExpert 2008

GOV2 GOV2 GOV2 GOV2
W3C W3C CERC CERC

701-750  751-800  801-850  701-850 
EX01-EX50 EX51-EX105 CE001-CE050 CE051-CE127

bpref 70 49 bpref 58 50 bpref 80 50 bpref 61 149
AP 37 50 AP 91 49 AP 55 50 AP 42 55

1MQ 2007

GOV2

"MQ1-MQ10000" AP (MTC) 29 1692

1MQ 2008

GOV2

"MQ10001-MQ20000" AP (MTC) 25 784

1MQ 2009 ClueWeb09

"MQ20001-MQ60000" AP (MTC) 35 542

1MQ 2007

GOV2

"MQ1-MQ10000"

statAP 29 1153

1MQ 2008

GOV2

"MQ10001-MQ20000"

statAP 25 564

1MQ 2009 ClueWeb09

"MQ20001-MQ60000"

statAP 35 475

Medical 2011

NLP

"M101-M135"

bpref 127 34

Microblog 2011 Tweets2011

MB1-MB50

P@30 184 49

E^2 (nq ) 0.933 0.893-0.963 0.907 0.847-0.952 0.856 0.804-0.9 0.898 0.855-0.933 0.919 0.891-0.943 0.908 0.88-0.932
0.929 0.89-0.96 0.876 0.833-0.912 0.862 0.813-0.904 0.81 0.729-0.876 0.829 0.746-0.895 0.804 0.685-0.895
0.901 0.858-0.935 0.45 0.249-0.619 0.89 0.844-0.927
0.903 0.852-0.943 0.882 0.803-0.94 0.844 0.725-0.929
0.919 0.873-0.955 0.966 0.949-0.979 0.801 0.708-0.876
0.94 0.909-0.965 0.903 0.848-0.945 0.77 0.664-0.855
0.846 0.784-0.897 0.95 0.934-0.964 0.864 0.807-0.911
0.953 0.933-0.97 0.875 0.815-0.923 0.762 0.668-0.841 0.94 0.913-0.962
0.916 0.864-0.955 0.965 0.952-0.976 0.884 0.827-0.929 0.565 0.315-0.757
0.999 0.999-1 0.998 0.996-0.999 0.96 0.936-0.979 0.992 0.986-0.996 0.978 0.962-0.99 0.96 0.935-0.979
0.774 0.704-0.835
0.92 0.899-0.938

^ (nq) 0.786 0.661-0.88 0.79 0.658-0.89 0.62 0.488-0.732 0.806 0.714-0.875 0.799 0.71-0.864 0.701 0.59-0.787
0.83 0.728-0.904 0.76 0.662-0.835 0.711 0.598-0.801 0.619 0.473-0.744 0.662 0.513-0.787 0.702 0.537-0.835
0.84 0.762-0.898 0.315 0.144-0.492 0.747 0.643-0.832
0.847 0.759-0.911 0.804 0.676-0.899 0.719 0.535-0.865
0.792 0.671-0.883 0.944 0.91-0.967 0.181 0.1-0.301
0.87 0.792-0.925 0.768 0.64-0.868 0.422 0.269-0.586
0.509 0.384-0.636 0.824 0.768-0.872 0.693 0.564-0.797
0.877 0.809-0.924 0.648 0.501-0.774 0.427 0.283-0.575 0.719 0.617-0.812
0.824 0.713-0.905 0.939 0.909-0.96 0.785 0.674-0.87 0.28 0.11-0.498
0.998 0.997-0.999 0.988 0.979-0.995 0.908 0.854-0.951 0.982 0.97-0.991 0.969 0.946-0.986 0.929 0.886-0.963
0.497 0.348-0.628
0.818 0.747-0.869

Table 1: Summary of all 43 TREC collections analyzed. Query sets with  are used in more than one collection. Query numbers in quotes are not official, but arbitrarily named for this paper. The last two columns report the point and 95% interval estimates of the GT-based reliability indicators.

slightly with the years. The clear exceptions are again the Million Query Track collections, which specifically aimed at increasing the number of queries. Within each task it appears that stability tended to decrease as the tasks got older despite that query set sizes were normally unaltered. The second plot shows that this decrease in stability could be due to system variance getting smaller with the years. That is, systems perform more similarly as the tasks get older, indicating that retrieval techniques are generally improved. The right plot shows that query difficulty also varied within tasks. Sudden peaks may be explained by changes in the document set or in the task definition. The general trend suggests that queries are getting more alike with the years, further contributing to the decrease in reliability.
Bodoff [5, §5] discusses the incorporation of the document set as another facet in Generalizability Theory, much like queries and systems, to measure variability due to documents [14]. He argues that it does not make sense in general, because we do no assign performance scores for indi-

vidual documents but for sets of documents (e.g. the first k retrieved when computing P @k). In our case we could compare different editions of the same task but with different document sets to get a (weak) clue of the variability due to documents. For example, the Ad Hoc task of the Web Track shows quite different stability scores in the first three editions (WT2g and WT10g collections) compared to the last three editions (ClueWeb09), given that they all used the standard query set size of 50. Similarly, the Expert Search task in the Enterprise Track shows very different stability levels when using the W3C collection or the CERC collection. We must bear in mind though that these differences might actually be due to the systems and queries used, which varied from year to year.
From the confidence intervals in Table 1, we used the models fitted in Section 4 to provide in Table 2 the estimated data-based reliability scores for all 43 collections. It is evident that expected  correlations are well below the desired 0.9 in most cases. In that line, some collections are clearly

400

Track
Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8
WebAdHoc 8 WebAdHoc 9 WebAdHoc 2001 WebAdHoc 2009 WebAdHoc 2010 WebAdHoc 2011 WebDistillation 2002 WebDistillation 2003 WebDistillation 2004 WebDiversity 2009 WebDiversity 2010 WebDiversity 2011 Novelty 2002 Novelty 2003 Novelty 2004
GenomicsAdHoc 2003 GenomicsAdHoc 2004 GenomicsAdHoc 2005
Robust 2003 Robust 2004 Robust 2005
Terabyte 2004 Terabyte 2005 Terabyte 2006 TerabyteAll 2006 EnterpriseExpert 2005 EnterpriseExpert 2006 EnterpriseExpert 2007 EnterpriseExpert 2008
1MQ 2007 1MQ 2008 1MQ 2009 1MQ 2007 1MQ 2008 1MQ 2009
Medical 2011
Microblog 2011

^
0.725-0.898 0.622-0.87
0.537-0.741 0.641-0.821
0.72-0.846 0.695-0.819
0.718-0.89 0.595-0.77 0.554-0.749 0.406-0.686 0.434-0.729 0.34-0.728
0.647-0.827 0.019-0.255 0.617-0.807
0.633-0.847 0.535-0.839 0.401-0.811
0.679-0.877 0.86-0.941
0.374-0.685
0.762-0.903 0.624-0.852 0.311-0.641
0.5-0.734 0.823-0.902 0.544-0.766
0.82-0.916 0.558-0.795
0.316-0.61 0.772-0.897
0.661-0.877 0.868-0.932 0.582-0.812 0.037-0.453
0.997-0.999 0.989-0.997 0.827-0.942 0.962-0.989 0.896-0.972 0.826-0.941
0.368-0.598
0.74-0.833

^AP
0.637-0.86 0.515-0.823 0.418-0.657 0.537-0.758 0.631-0.791
0.6-0.756
0.629-0.849 0.484-0.694 0.437-0.668
0.283-0.59 0.311-0.643 0.221-0.642
0.544-0.766 0.004-0.148 0.508-0.741
0.528-0.792 0.416-0.782 0.278-0.746
0.582-0.833 0.81-0.919
0.252-0.589
0.684-0.867 0.517-0.799 0.195-0.537
0.379-0.649 0.761-0.865 0.426-0.689
0.758-0.884 0.442-0.725
0.2-0.5 0.696-0.859
0.56-0.831 0.821-0.907 0.468-0.746
0.01-0.33
0.995-0.999 0.985-0.996
0.767-0.92 0.947-0.984 0.858-0.961 0.765-0.919
0.246-0.486
0.656-0.774

^ (%)
58-83 45-79 35-60 47-72 58-76 54-72
57-82 42-65 37-62 22-53 25-59 16-59
48-73 0-10
44-70
46-76 35-74 22-70
52-80 78-90 19-53
63-84 45-76 14-47
31-60 72-84 36-64
72-86 38-68 14-44 65-83
50-80 79-89 40-70
0-26
99-100 98-100
73-90 94-98 83-95 73-90
19-42
60-74

^- (%)
0.6-3.2 0.9-5.6 2.9-8.2 1.6-5.2 1.2-3.3 1.6-3.9
0.7-3.4 2.4-6.4 2.8-7.7 4.1-13.5 3.1-12.2 3.2-17
1.5-5 22.8-64.4
1.8-5.8
1.2-5.4 1.3-8.3 1.7-13.8
0.9-4.2 0.3-1.1 4.1-15.1
0.6-2.5 1.2-5.6 5.2-18.8
3.1-9.6 0.6-1.6
2.5-8
0.5-1.6 2-7.5
6-18.5 0.7-2.4
0.9-4.7 0.3-1
1.7-6.8 11.4-56
0-0 0-0 0.3-1.5 0-0.1 0.1-0.7 0.3-1.5
6.3-15.5
1.4-3

^+ (%)
0.02-0.28 0.03-0.72 0.23-1.38 0.08-0.62 0.05-0.29 0.08-0.38
0.02-0.3 0.17-0.9 0.21-1.22 0.41-3.24 0.27-2.73 0.27-4.81
0.07-0.59 7.89-47.06
0.1-0.76
0.05-0.66 0.06-1.4
0.09-3.35
0.03-0.44 0-0.04
0.42-3.93
0.02-0.18 0.05-0.71 0.62-5.69
0.25-1.78 0.02-0.08 0.17-1.32
0.01-0.08 0.12-1.19
0.8-5.52 0.02-0.16
0.03-0.52 0.01-0.03
0.09-1 2.41-37.02
0-0 0-0 0-0.07 0-0 0-0.02 0-0.08
0.88-4.08
0.07-0.24

^a
0.01-0.03 0.01-0.06 0.03-0.08 0.02-0.05 0.01-0.03 0.02-0.04
0.01-0.03 0.02-0.06 0.03-0.08 0.04-0.13 0.03-0.12 0.03-0.17
0.01-0.05 0.23-0.64 0.02-0.06
0.01-0.05 0.01-0.08 0.02-0.14
0.01-0.04 0-0.01
0.04-0.15
0.01-0.02 0.01-0.05 0.05-0.19
0.03-0.09 0.01-0.02 0.02-0.08
0-0.02 0.02-0.07 0.06-0.18 0.01-0.02
0.01-0.05 0-0.01
0.02-0.07 0.11-0.56
0-0 0-0 0-0.01 0-0 0-0.01 0-0.01
0.06-0.15
0.01-0.03

^r (%)
6-25 6-25 18-42 7-20 8-20 13-31
5-18 10-24 12-31 17-44 13-39 10-37
5-16 41-82 10-26
4-16 5-23 7-37
6-24 1-4
63-87
3-13 7-27 32-67
27-53 7-15
13-34
4-12 15-41 33-65 11-29
5-20 2-4
7-23 41-86
0-0 0-1 2-8 0-1 0-2 1-6
28-57
7-17

^
0.001-0.029 0.001-0.03
0.013-0.112 0.001-0.017 0.001-0.017 0.006-0.054
0-0.014 0.003-0.028 0.005-0.051 0.011-0.122 0.006-0.095
0.003-0.08
0.001-0.009 0.108-0.6
0.003-0.034
0-0.009 0.001-0.025 0.001-0.081
0.001-0.026 0-0
0.309-0.709
0-0.006 0.001-0.035 0.055-0.358
0.036-0.204 0.001-0.008 0.005-0.066
0-0.004 0.008-0.103
0.06-0.336 0.004-0.043
0-0.017 0-0
0.001-0.025 0.104-0.683
0-0 0-0 0-0.002 0-0 0-0 0-0.001
0.039-0.246
0.001-0.011

n^E2(.95) 37-114 47-169
106-233 69-161 58-117 69-130
40-118 92-190 102-220 135-354 107-311 112-438
65-154 585-2862
112-264
58-166 61-234 73-360
44-136 21-52
135-392
35-95 56-171 158-472
218-525 175-336
94-227
30-68 80-217 181-474 111-269
46-149 24-48
73-200 335-2277
11-38 16-59 219-710 88-304 107-421 194-628
129-273
62-105

n^(.95)
130-487 116-484 348-999 136-381 150-389 257-662
102-355 189-484 236-640 327-1058 247-868 188-819
106-292 980-5631
288-791
93-301 107-457 149-826
124-457 33-94
2203-8579
78-250 146-536 657-2528
1087-3043 693-1428 242-733
77-220 279-947 702-2406 657-1761
100-383 39-93
143-459 1053-8458
30-104 81-313 534-1756 196-685 156-616 352-1156
383-1208
141-315

Table 2: Predicted reliability of all 43 TREC collections analyzed. All confidence intervals are based on the fits from Figure 3 at the endpoints of the 95% confidence intervals computed with equations (10) and (11).

not reliable, such as the Web Distillation 2003, Genomics Ad Hoc 2005, Terabyte 2006, Enterprise Expert Search 2008, or the very recent Medical 2011 and Web Ad Hoc 2011. Regarding the expected RMS Error of absolute scores, we can see that collections are somewhat stable, but with clear exceptions such as Web Distillation 2003, Novelty 2004 and Enterprise Expert Search 2008.
The last two columns in Table 2 report intervals on the number of queries, as per equations (12) and (13), required to achieve 0.95 stability. In general the number of queries needs to be at least doubled, and in many cases a few hundred queries seem to be needed. This is particularly interesting for the most recent collections, such as Web Ad Hoc 2010 and 2011, Medical 2011 and Microblog 2011, which stick to the traditional size of 50 queries but need about 200. What becomes clear from these figures is that the ideal size of a collection depends greatly on the task it will be used for, and thus it is not appropriate to fix some acceptable size such as 50 or 100 throughout tasks. Each task has different characteristics and should be analyzed accordingly.

6. CONCLUSIONS
In this paper we discussed the measurement of test collection reliability from the perspective of traditional databased methodologies and of Generalizability Theory. GT is regarded as a more appropriate, easy to use, and powerful method to assess reliability, but it has two drawbacks. First, we showed that GT is very sensitive to the particular sample of systems and queries used to estimate reliability of a larger query set. We showed that about 50 systems and 50 queries are needed for robust estimates of collection reliability. Therefore, researchers should be cautious in using GT when building new collections from scratch. To account for all this variability we discussed a more robust approach based on interval estimates of the stability indicators, which helps in making more appropriate decisions regarding number of queries or different structure in the experimental design. Second, we empirically established a mapping between GT-based and traditional data-based indicators to help interpreting results from GT which, otherwise, do not have a

401

Ad Hoc Web adhoc Web distillation Web diversity Novelty Genomics Robust Terabyte Enterprise 1MQ MTC 1MQ statAP Medical Microblog
Linear trend

E^2 0.5 0.6 0.7 0.8 0.9 1.0

Relative Stability

1995

2000

Year

2005

2010

^s2 (% of total)

0

5 10 15 20 25

Variability due to Systems

1995

2000

Year

2005

2010

^2q (% of total) 30 40 50 60 70 80 90

Variability due to Queries

1995

2000

Year

2005

2010

Figure 4: Historical trend of relative stability (left), variability due to systems (middle) and to queries (right).

clear and easily understandable meaning. Based on these results, we reviewed the reliability of 43 TREC test collections, evidencing that some of them are very little reliable. We show that the traditional choice of 50 queries is clearly not enough even for stable rankings, and in most cases a couple hundred queries are needed. Our results also show that the ideal query set size varies significantly across tasks, suggesting that we avoid the use of some fixed size such as 50 or 100 and that we analyze tasks and collections separately.
There are two clear lines for future research. First, we completely ignored the assessor facet in our study. It is evident that different assessors provide different results, so it would be interesting to include them in the analysis. Second, although we fitted the theoretically correct models, it is clear that they can be improved (see for instance Power and RMS Error in Figure 3). IR evaluation experiments generally violate assumptions of GT, such as normality of distributions and random sampling, so different models and features to better fit the actual data should be investigated.
We created some scripts for the statistical software R that can help researchers perform all these computations to easily assess the reliability of custom test collection designs. They can be downloaded from http://julian-urbano.info.
7. REFERENCES
[1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and E. Kanoulas. Million Query Track 2008 Overview. In Text REtrieval Conference, 2008.
[2] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, B. Dachev, and E. Kanoulas. Million Query Track 2007 Overview. In Text REtrieval Conference, 2007.
[3] C. Arteaga, S. Jeyaratnam, and G. A. Franklin. Confidence Intervals for Proportions of Total Variance in the Two-Way Cross Component of Variance Model. Communications in Statistics: Theory and Methods, 11(15):1643­1658, 1982.
[4] D. Banks, P. Over, and N.-F. Zhang. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval, 1(1-2):7­34, 1999.
[5] D. Bodoff. Test Theory for Evaluating Reliability of IR Test Collections. Information Processing and Management, 44(3):1117­1145, 2008.
[6] D. Bodoff and P. Li. Test Theory for Assessing IR Test Collections. In ACM SIGIR, pages 367­374, 2007.
[7] R. L. Brennan. Generalizability Theory. Springer, 2001.
[8] C. Buckley and E. M. Voorhees. Evaluating Evaluation Measure Stability. In ACM SIGIR, pages 33­34, 2000.

[9] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation Over Thousands of Queries. In ACM SIGIR, pages 651­658, 2008.
[10] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. If I Had a Million Queries. In ECIR, pages 288­300, 2009.
[11] L. S. Feldt. The Approximate Sampling Distribution of Kuder-Richardson Reliability Coefficient Twenty. Psychometrika, 30(3):357­370, 1965.
[12] E. Kanoulas and J. A. Aslam. Empirical Justification of the Gain and Discount Function for nDCG. In ACM CIKM, pages 611­620, 2009.
[13] W.-H. Lin and A. Hauptmann. Revisiting the Effect of Topic Set Size on Retrieval Error. In ACM SIGIR, pages 637­638, 2005.
[14] S. Robertson and E. Kanoulas. On Per-Topic Variance in IR Evaluation. In ACM SIGIR, pages 891­900, 2012.
[15] T. Sakai. On the Reliability of Information Retrieval Metrics Based on Graded Relevance. Information Processing and Management, 43(2):531­548, 2007.
[16] M. Sanderson. Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.
[17] M. Sanderson and J. Zobel. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In ACM SIGIR, pages 162­169, 2005.
[18] R. J. Shavelson and N. M. Webb. Generalizability Theory: A Primer. Sage Publications, 1991.
[19] J. Urbano, M. Marrero, and D. Mart´in. A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation. In ACM SIGIR, 2013.
[20] E. M. Voorhees. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In ACM SIGIR, pages 315­323, 1998.
[21] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR, pages 806­807, 2009.
[22] E. M. Voorhees and C. Buckley. The Effect of Topic Set Size on Retrieval Experiment Error. In ACM SIGIR, pages 316­323, 2002.
[23] E. Yilmaz, J. A. Aslam, and S. Robertson. A New Rank Correlation Coefficient for Information Retrieval. In ACM SIGIR, pages 587­594, 2008.
[24] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In ACM SIGIR, pages 307­314, 1998.

402


Click Model-Based Information Retrieval Metrics

Aleksandr Chuklin
Yandex & ISLA, University of Amsterdam
Moscow, Russia
A.Chuklin@uva.nl

Pavel Serdyukov

Maarten de Rijke

Yandex

ISLA, University of Amsterdam

Moscow, Russia

Amsterdam, The Netherlands

pavser@yandex-team.ru

deRijke@uva.nl

ABSTRACT
In recent years many models have been proposed that are aimed at predicting clicks of web search users. In addition, some information retrieval evaluation metrics have been built on top of a user model. In this paper we bring these two directions together and propose a common approach to converting any click model into an evaluation metric. We then put the resulting model-based metrics as well as traditional metrics (like DCG or Precision) into a common evaluation framework and compare them along a number of dimensions.
One of the dimensions we are particularly interested in is the agreement between oine and online experimental outcomes. It is widely believed, especially in an industrial setting, that online A/B-testing and interleaving experiments are generally better at capturing system quality than oline measurements. We show that oine metrics that are based on click models are more strongly correlated with online experimental outcomes than traditional oine metrics, especially in situations when we have incomplete relevance judgements.

Categories and Subject Descriptors

H.3.3 [

]: Information

Information Storage and Retrieval

Search and Retrieval

General Terms
Human Factors, Verification

Keywords
Click models, evaluation, information retrieval measures, user behavior

1. INTRODUCTION
There are currently two orthogonal approaches to evaluating the quality of ranking systems. The first approach is usually called the Cranfield approach [17] and is done oline. It uses a fixed set of queries and documents judged by

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

trained people (assessors). Ranking systems are then evaluated by comparing how good their ranked lists are--among other things, a system is expected to place relevant documents higher than irrelevant ones.
Another approach described by Kohavi et al. [28] makes use of real online users by assigning some portion of the users to test groups (also called flights). The simplest variant, called A/B-testing, randomly assigns some users to the "control" group (these users are presented with the existing ranking results) and the "treatment" group (these users are presented with the results of an experimental ranking system). Ranking systems are then compared by analysing the clicks of the users in the "control" against those in the "treatment" group. In the interleaving method by Joachims [27] users are presented with a combined list made out of two rankings. Then the system that receives more clicks is assumed to be better.
One of the main advantages of online evaluation schemes is that they are user-based and, as a result, often assumed to give us more realistic insights into the real system quality. Interleaving experiments are now widely being used by large commercial search engines like Bing and Yahoo! [11, 31] as well as studied in academia [22, 32]. However, they are harder to reproduce than oine measurements, whereas in the traditional Cranfield approach one can re-use the same set of judged documents to evaluate any ranking. This makes the use of oine editor-based evaluation methods unavoidable during the early development phase of ranking algorithms. One should take care, however, that the resulting editor-based measurements agree with the outcomes of online experiments--online comparison is often used as the final validation step before releasing a new version of a ranking algorithm.
In order to bring the two evaluation approaches closer to each other, we propose a method for building an oine information retrieval (IR) metric from a user click model. Click models, probabilistic models of the behavior of web search users, have been studied extensively by the IR community during the last five years. The main purpose of predicting clicks, as seen in previous works, is: (1) modeling user behavior when real users are not available (see, e.g., [23]); (2) improving ranking using relevance inferred from clicks (e.g., [10]). We hypothesize that click models can also be turned into oine metrics and the resulting click modelbased metrics should be closely tied to the user and hence should better correlate with online measurements than traditional oine metrics. In addition, there is a growing trend to ground oine metrics in a user model and that is exactly

493

what click modeling does--trying to propose a better user model. So, the question is why not use better user models, based on click behavior, as the basis for oine metrics?
We put our proposal for transforming click models into metrics to the test through a set of thorough comparisons with online measurements. Our comparison includes an analysis of correlations with the outcomes of interleaving experiments, an analysis of correlations with absolute online metrics, an analysis of correlations between traditional oine metrics and our new click model-based metrics, as well as an analysis of the discriminative power of the various metrics. One dimension to which we devote special attention in our comparison framework concerns unjudged documents. As was shown by Buckley and Voorhees [5], having partially-judged result pages in the evaluation pool may result in biased measurements. We examine how dierent oine metrics handle this problem. We also show that in situations when we cannot aord to use only fully-judged data, we can still make good use of the available data by making adjustments, by either a technique called condensation [34] or a new threshold method that we propose.
The main research questions that we address in this work are:
, How do click model-based IR metrics compare to the traditional oine metrics?
, How well do dierent oine IR metrics agree with online experiments? Do click model-based metrics show higher agreement?
, How well do dierent oine metrics perform in the presence of unjudged documents?
, How can we modify oine metrics to enhance agreement with online experiments?
Our main contributions in this paper are a method for converting click models into click model-based oine metrics. Secondly, we present a thorough analysis and comparison of specific click model-based metrics with online measurements and traditional oine metrics.
The rest of the paper is organized as follows. Section 2 presents related work. Section 3 shows how to transform a click model into a model-based oine metric. In Section 4 we examine click model-based and traditional oine metrics and report on their performance. We finish with a conclusion and discussion in Section 5.
2. RELATED WORK
Determining and comparing the quality of information retrieval systems has always been an important task in IR, both in academic and industrial research. In recent years, competition between large commercial search systems has reached the point where even a small improvement can be of great importance. As a result, a broad range of metrics to assess system performance have been proposed: Discounted Cumulative Gain (DCG) by J¨arvelin and Kek¨al¨ainen [26], Expected Reciprocal Rank (ERR) by Chapelle et al. [10], Expected Browsing Utility (EBU) by Yilmaz et al. [39], to name just a few. They have also been assessed from a variety of angles (see, e.g., [6, 10, 33]).
Some IR metrics have an underlying user model (e.g., ERR, EBU) or they can be viewed as such (see [6]). However, there is still a big gap between user models and metrics. For example, some of the widely used click models, such as

the User Browsing Model (UBM) by Dupret and Piwowarski [19] and the Dependent Click Model (DCM) by Guo et al. [21], have so far not been used to develop an oine metric. Moreover, since the introduction of these early click models, many more click models have been developed, not only as improvements to previous models [20, 30], but also to address specific modeling issues, such as click models for diversified search [12, 14], the use of mouse movements along with clicks [24], or to model sessions [41]. We believe that all these models can be converted to evaluation measures.
The creation of traditional test collections against reduced costs has received considerable attention. Carterette and Allan [7] and Sanderson and Joho [35] discuss approaches to building test sets for evaluation at low cost. Azzopardi et al. [3], Berendsen et al. [4] go a step further and describe methods for automatically generating test collections and training material for learning-based rankers, respectively. Other works have extensively examined one particular limitation of traditional test collections: the completeness of judgments [5, 34]. Buckley and Voorhees [5] introduce a new metric called bpref to use in a setup where we have missing relevance judgements. Sakai [34] propose an alternative solution that does not require a new metric. We will also consider this problem when analysing metrics. Apart from evaluation metrics, there are other interesting problems arising when dealing with large query sets; these are addressed by the TREC Million Query Track [2] and further studied by Carterette et al. [8].
Another important group of extremely related studies concerns user-based experiments. Introduced by Joachims [27], the interleaving method is now widely used. Since its introduction, several modifications to the original method have been proposed, notably Team-Draft Interleaving [32] and Probabilistic Interleaving [22]. A thorough overview of interleaving methods can be found in [11]. Radlinski and Craswell [31] analyze and compare the sensitivity of both interleaving and traditional oine IR metrics against each other. They find that the outcomes of interleaving experiments generally agree quite well with oine metrics while data can be collected at a much lower cost. Below, we apply the same type of analysis to evaluate click model-based metrics and to compare them against traditional IR metrics. Ali and Chang [1] show that per-query correlation between oline side-by-side comparisons and online interleaving experiments is low even when query filtering is applied. This finding suggests that aggregating results from multiple queries as was done in [31] is less noisy than computing correlations on a per query basis. Yue et al. [40] propose ways to increase the signal of an interleaving experiment; inspired by this idea we propose to tune oine metrics through two techniques referred to as condensation and thresholding below to enhance the agreement with interleaving (see Section 4.1).
3. CLICK MODEL-BASED METRICS
From an initial focus on precision as a metric, the area of web search evaluation has evolved considerably. An early lesson is that we need to apply some sort of discount to the documents that appear lower in the ranking. One of the first metrics to operationalize this idea was Discounted Cumulative Gain (DCG) [26]. This metric is still widely used in the IR community. However, it has some drawbacks. One is that its discount function is not motivated by a user model. Another important issue with this metric is that it

494

is a static metric, i.e., its discount values are fixed numbers.

As shown in [39], a dynamic metric that dynamically assigns

dierent discount values according to the relevance of the

documents appearing higher in the ranking, more accurately

represents real user behavior.

In this paper we introduce the notion of click model-based

metrics. The main constituent of such a model-based metric

is a click model --a probabilistic model aimed at predicting

user clicks. Apart from click events (C ), a click model k
usually has hidden variables corresponding to events such

as "the user examined the snippet of the k-th document"

(E ). These hidden variables are often used to gain deeper k
insights into users' behavior. For example, Chapelle and

Zhang [9] used a click model (DBN) to predict relevance

and train a ranking function and in [19] the parameters of

the click model were analysed to explain how previous user

clicks influence future clicks. All click models that we study

in this paper assume that users click a document only after

examining the document's snippet, i.e., P pC " 1|E "

k

k

0q " 0.

Following Carterette [6], we distinguish between utility-

based metrics and eort-based metrics. These give rise to

two ways of mapping a click model to a click model-based

oine metric. First, a utility-based metric uses a click model

only to predict the click probability P pC " 1q for the k-th k
document in the ranking. This probability is then used to

calculate the metric value as the expected utility:

ÿN

uM etric " P pC " 1q ¨ R ,

(1)

k

k

"1 k

where R is the relevance of the k-th document. It is comk
mon to use four or five relevance grades, from Irrelevant to

Highly Relevant that are further mapped to numeric values.

For example, the TREC 2011 Web Track [16] uses four levels

of relevance: from 0 for Irrelevant documents to 3 for Highly

Relevant documents.

Second, an eort-based metric requires a click model to

have a notion of "user satisfaction" (S ). A click model must

k

have hidden variables S such that P pS " 1|C " 0q " 0

k

k

k

(the user can only be satisfied by the documents she clicked)

and P pE " 1|S " 1q " 0 for j ° k (after being satisfied

j

k

the user stops examining documents). Having this, we can

define a metric to be an expected value of some eort func-

tion1 at the stopping position:

ÿN rrM etric " P pS

" 1q ¨ 1 " ÿN s P pC

" 1q ¨ 1 ,

(2)

k k"1

k

k k"1

k

k

where s " P pS " 1|C " 1q is a satisfaction probability.

k

k

k

A click model is usually trained using a click log. As a re-

sult we get values of the model parameters that can further

be used to calculate the probability of clicks or satisfaction

events to use in Equations 1 and 2. Some of the parameters

are just constants, some depend on the position(s) in the

ranking and some depend on the document and/or query.

Parameters of the last type are the hardest ones to be used

in a metric, as we want our metric to work even for pre-

viously unseen documents. But, fortunately, parameters of

this type can usually be approximated from the document's

1Following [6] we use reciprocal rank 1 as an eort function.
While we are not doing it here, it wkould be interesting to evaluate metrics with dierent eort functions.

Table 1: Click model-based metrics and their underlying models. Previously proposed models/metrics are followed by the reference.

Underlying click model
DBN [9] DBN [9] DCM [21] UBM [19]

Derived metric

Utility-based Eort-based

uSDBN [10] EBU [39] uDCM uUBM

ERR [10] rrDBN rrDCM ­

relevance. In fact, when training a model we assume that these parameters only depend on the document relevance and not on the document itself. We will demonstrate this procedure for the attractiveness parameters in DBN, DCM, UBM and for the satisfaction parameters in DBN.
If a model meets the requirements listed above, it can be transformed into a click model-based metric. There is no step-by-step algorithm for such a transformation but only general guidelines. In the following sections we demonstrate the idea, using well-known click models as an example. We want to stress, however, that our framework is general enough to be applied to other click models, including those that use additional sources of information, such as recently studied session-based click models [41] or click models for vertical search [12, 14].
In Table 1 we classify previously studied metrics (ERR by Chapelle et al. [10], EBU by Yilmaz et al. [39]) and propose several new click model-based metrics: rrDBN, uDCM, rrDCM, uUBM. The left most column lists click models, the center and right most column denote derived oine metrics, utility-based and eort-based, respectively. As a recipe for naming a metric, we use the name of the underlying model and prefix it with the type metric that we are defining: u- for utility-based and rr- for reciprocal rank eort-based metrics.

3.1 Previously Studied Metrics

In this section we show how two previously proposed met-

rics, ERR and EBU, can be viewed as click model-based

metrics. Despite the fact that they are dierent and were

not in fact proposed as derivatives of a click model, they

can both be viewed as metrics based on special cases of the

Dynamic Bayesian Network click model (DBN) by Chapelle

et al. [10]. In this model, the user examines document cap-

tions one by one and may be attracted by document u with

probability a . If the user is attracted by the document, she u
clicks it and becomes satisfied with probability s . If she u
is not satisfied by the document she proceeds to the next

document with probability and stops otherwise.

The Expected Reciprocal Rank (ERR) metric uses a sim-

plified version of the DBN model [9] (we will refer to this

model as SDBN) in which, as an additional constraint, all at-

tractiveness probabilities are set to 1 (a " P pC " 1|E "

1q

"

1)

and

therefore

all

documents

u k

k

are clicked. This

k
leads

to s " r , i.e., the satisfaction probability is equal to the

u

u

k

k

probability of the document being relevant to the query. By

making this assumption we obtain the probability of clicking

the k-th document

P pC

" 1q "

´1 k

k´1 p1

´

r

q

(3)

k

i

i"1

495

and the probability of satisfaction

P pS " 1q " r

´1 k

k´1 p1

´

r

q,

(4)

k

k

i

"1 i

where r is the probability of relevance of the i-th document i
and is the continuation probability. The probability of

being relevant is usually viewed as a mapping R Ñ r from

the relevance grades to the segment r0, 1s. In the original

ERR paper [10] the authors use a mapping motivated by

DCG: r

"

, 2R ´1
2Rmax

relevance grade (R

where R is the maximum possible max " 3 in the case of TREC 2011 Web

max

Track), but one may also fit this mapping from a click log.

Using probabilities from (3) and (4), we end up with the

ERR and uSDBN metrics (cf. Equations (2), (1)):

ERR "

~ ÿN
r

´1 k

k´1 p1

´

r

¸ q

¨

1

uSDBN

"

k

i

k"1 ~ ÿN

´1 k

i"1
k´1 p1

´

r

¸ q

¨

r

k

i

k

k"1

"1 i

In the original version of the ERR metric, the continuation

probability of the DBN model was set to 1.2 Conversely,

for uSDBN [10, Section 7.2], we set ("one minus the aban-

donment probability") to 0.9, as suggested in [9].

The Expected Browsing Utility (EBU) metric by Yilmaz

et al. [39] is also based on a variation of the DBN model.

Unlike the original DBN model, their modification allows

for dierent continuation probabilities in dierent situations

(p | , p |

, p | ). While these parameters

cont click cont nonrel cont rel

lead to greater flexibility in setting up the metric, they also

represent a di cult choice for a practitioner to make. They

were all set to 1 in the original paper [39] and here we do the

same. By doing so we reduce the EBU model to DBN [9]

with continuation probability " 1. One notable dierence

between the ERR and EBU metrics is that EBU does not

set the attractiveness probabilities to 1. Instead, the attrac-

tiveness probabilities and satisfaction probabilities are both

estimated from a click log using the assumptions that they

are determined by the document relevance:

a « P pC |R q

uk

k uk

s « P pL |R q,

uk

k uk

where C is the random variable corresponding to a click on k
the k-th document, L is the random variable corresponding k
to leaving the result page after clicking the k-th document

and R is the relevance of the k-th document u .

uk

k

3.2 New Click Model-Based Metrics

In this section we propose new oine metrics by introducing an eort-based variant of the EBU metric and also by converting the two popular click models, UBM and DCM, into click model-based metrics. By doing so we show that our framework of click model-based metrics is not only a way of viewing previously studied metrics, but also a way of deriving new metrics in a principled way.
The rrDBN metric uses essentially the same user model as the EBU metric. In fact, the parameters for EBU and

2In the original paper [10, Section 3] an alternative interpretation of the ERR metric as a metric based on the Cascade Model by Craswell et al. [18] was also proposed.

rrDBN are the same. The only dierence is that rrDBN is calculated using Equation (2) instead of (1).
Next, the uDCM and rrDCM metrics can be derived from the Dependent Click Model (DCM, [21]) in a way similar to how EBU and rrDBN are derived from DBN. The only dierence between DCM and DBN is that the satisfaction probability P pS " 1q depends not on the document itself
k
but on its position k in a ranked list. Thus, the DCM model can be described with the following equations:

P pC " 1|E " 0q " 0

k

k

P pC " 1|E " 1q " a

k

k

u k

P pS " 1|C " 0q " 0

k

k

P pS " 1|C " 1q " s

k

k

k

P pE1 " 1q " 1

P pEk`1

"

1|E k

"

0q

"

0

P

pEk`1

"

1|E k

"

1,

Sq k

"

1´S k

As was shown by Turpin et al. [37], the attractiveness of a

document's snippet can be approximated as a function of its

relevance grade. A mapping from grades to attractiveness

probabilities can be inferred from a click log using the click

model (DCM in this case). 3 For this purpose we impose the

constraint that documents with the same relevance have the

same attractiveness, i.e., the attractiveness of a document is

a function of its relevance grade: a " apR q.

u

u

Finally, using the click model and Equations (1), (2), we

can define uDCM and rrDCM metrics as follows:

ÿN

k´1

uDCM "

apR q p1 ´ apR qs q ¨ R

k

ii

k

k"1

"1 i

rrDCM

"

ÿN

s

apR

k´1 q p1 ´ apR

qs

q¨

1

k k"1

k "1 i

ii k

Chen et al. [12] report that the User Browsing Model (UBM) [19] performs better than DBN in terms of click prediction perplexity. We have also evaluated this model using a Yandex click log. We used a sample of clicks collected in November 2012. We then removed pages without clicks and split the remaining data into training and test set. In total we had 1,191,963 training and 1,292,993 test pages. To compare the models we used the perplexity gain [12, Section 5.2] which is a standard way of comparing perplexity values (see, e.g., [12, 20, 41]). On our data UBM outperforms DBN by 16% which is considered a big dierence.
This finding motivates the idea of deriving an oine metric from UBM. In the UBM model the click probability is governed by the attraction bias and the examination bias:

P pC " 1|u, q, r, dq " P pA " 1|u, qqP pE " 1|d, rq " a , uq rd

where C stands for click, A for attraction, E for examina-

tion; u is the document URL, q is the user query, r is the doc-

ument rank (position), and d " r´maxtj  r|C " 1u is the j
distance to the previous click.4 For convenience, we write

a instead of a and pjq instead of , where d " r ´ j.

r

uq

r

rd

Like for the EBU/rrDBN and uDCM/rrDCM metrics, we

assume that the attractiveness probability a is a function of

3The source code for probabilistic inference is freely avail-

able at

.

https://github.com/varepsilon/clickmodels

4As in [19] we use a virtual zero position (which is always

clicked) to simplify our equations.

496

the relevance of the document: a " apR q. The exam-

uq

uq

ination probabilities pjq can be precomputed from click

r

logs during the same model training process. One impor-

tant dierence from the previously studied models is that

the UBM model relies on previous clicks and these are not

available oine. To deal with this problem we factorize the

probability P pC " 1q over the position of previous clicks j: r

rÿ´1

P pC " 1q " r

P pC j

"

1, Cj`1

"

0, . . . , Cr´1

"

0, C r

"

1q.

j"0

By applying Bayes rule we get

rÿ´1

P pC " 1q " P pC " 1q

r

j

j"0

r´1

¨

P pC k

"

0|C j

"

1, Cj`1

"

0, . .

. , C ´1 k

"

0q

k"j`1

¨ P pC r
rÿ´1

"

1|C j

" ~

1, Cj`1 r´1

"

0, .

. . , Cr¸´1

"

0q

"

" P pC " 1q

p1 ´ a pjqq a pjq

j

kk

rr

j"0

k"j`1

Finally, the click probability is given by a recursive formula:

P pC0 " 1q " 1 rÿ´1

~ r´1

¸

P pC " 1q "

P pC " 1q

p1 ´ a pjqq a pjq

r

j

kk

rr

"0 j

k"j`1

where a " apR q, and ap¨q and p¨q are known functions

r

r

r

estimated from clicks. It is important to note, that un-

like Dupret and Piwowarski [19], we used all queries, not

only queries with high clickthrough rate. So our resulting

function is dierent from that analysed by Dupret and Pi-

wowarski, and might be interesting on its own. For example,

pjq is much less than 1 for j ° 0 which corresponds to the
r
fact that most of the users click on only one document.

Given the click probability we can define the metric:

ÿN

uU BM " P pC " 1q ¨ R

(5)

r

r

r"1

The UBM click model does not have a notion of user satisfaction and hence we do not introduce an "rrUBM" metric.

4. ANALYSIS
In this section we analyze the click model-based metrics previously listed, both old and new, along a number of dimensions. We compare click model-based metrics to traditional oine metrics. As traditional metrics we consider precision, with two possible binarizations of four scale judgments (Precision treats the highest three relevance grades (3, 2, 1) as "relevant," while Precision2 only treats the highest two relevance grades (3, 2) as "relevant") as well as DCG. As was shown by Chapelle et al. [10], the NDCG metric is always worse at capturing user satisfaction than DCG. We decided not to include this metric and thus to overcome potential issues with corpus-dependent NDCG normalization.
We start by determining correlations of various oine metrics to the outcomes of interleaving experiments in a way proposed by Radlinski and Craswell [31]. These correlations are then used to compare oine metrics to each other. The metric that shows the best correlation with interleaving out-

comes is assumed to better represent real user behavior. We then move to more traditional comparison techniques, such as metric-to-metric correlations and discriminative power.
4.1 Correlation with Interleaving Outcomes
As was shown by Radlinski et al. [32], absolute click metrics are often unable to determine dierences in IR systems. Moreover, they are always di cult to interpret and may even be misleading, because we cannot know for sure how these metrics are related to user satisfaction.
Fortunately, there is another approach, the pairwise or interleaved comparison techniques mentioned earlier [27, 32]. Following this approach, we compare two ranking systems by presenting a user with an interleaved result page, containing documents from both result lists. The winner is then determined from user clicks. We assess an oine IR metric m in terms of its agreement with the interleaving outcomes. Specifically, we use the Team-Draft Interleaving (TDI) method by Radlinski et al. [32]. In this method each document in the interleaved page is assigned to exactly one of the two ranking systems ("the teams"). We then say that a system wins a comparison if the documents it contributes to the combined list receive more clicks. The system that wins most of the comparisons is assumed to be better.
For the current experiment we used a click log of the Yandex search engine collected in October­December 2012. During this period we focus on five revisions of the core ranking functions (A, B, C, D, E), with each revision being compared to the previous one using TDI, that was run for 5-10 days. For each of our ten experiments we had at least 200,000 impressions as in the work by Radlinski and Craswell [31]. Some ranking function revisions influence more than one market (country), so in total we have 10 pairs of algorithms to compare: 1AB, 2AB, 1BC, 1CD,
2CD, 3CD, 4CD, 1DE, 2DE, 3DE. For each algorithm pair we recorded the interleaving signal value, i.e., the deviation from 50% of the number of cases where the newer system was preferred. For instance, if in the experiment labeled XY system Y was preferred over system X
i
in 51% of all cases, we say that the interleaving signal for the experiment is 1%.
Having interleaving signals, we want to compare them to the signals obtained by the oine IR measures, i.e., the average dierence of the metric values. Unlike in the traditional Cranfield approach we use queries and documents from the query log. When computing an oine metric signal for a particular experiment XY , we extract queries issued
i
by the users assigned to the experimental flight. For these queries we also extract the document lists that would have been produced by each of the systems X and Y if they had not been interleaved. By using click-log-based queries when comparing the signal of an oine metric to the interleaving signal we eliminate the eect induced by the choice of a query set that one needs to compile for a Cranfield-style evaluation. Here it also allows us to perform experiments with historical revisions of a ranking algorithm that is no longer running. Although this approach has some advantages for our research problem, it has some disadvantages for everyday usage. One notable drawback is that we use only part of the judgements available because not all the queries that we have judgements for were submitted by the users of the experimental flight. For each experiment and each metric we keep only the queries that have at least one

497

document judged. Depending on the experiment we have

from 178 to 5,815 queries per experiment (median 573). As

shown in [31], it is usually su cient to have approximately

100 queries to identify the better system in an oine com-

parison.

The amount of data available to the search engine is usu-

ally much larger than a human can handle. Even more im-

portant is the fact that the web corpus is constantly chang-

ing, so we cannot maintain complete judgements even for

a limited set of queries. That is why it seems natural that

some documents returned by the system do not have rele-

vance judgements. In order to analyse the tradeo between

adding noise from unjudged documents and reducing the

noise by allowing more queries we introduce a parameter

#unjudged. We discard queries for which the number of unjudged documents in the top 10 is bigger than this value for

either of the two systems taking part in a TDI experiment.

Below, we vary this bound and see how it influences the

correlation between oine metrics and interleaving.

For each oine metric m and each value of #unjudged from 1 to 9 we compute the weighted Pearson correlation

(similar to [10]) between the metric signal and the interleav-

ing signal. As a weight we use the number of queries partic-

ipating in the calculation of the metric signal (this number

is dierent for each experiment). The results are presented

in Figure 1. We can see that the eort-based metrics rrDBN

and rrDCM are better at dealing with unjudged documents

and are remarkably dierent from their utility-based counter-

parts; we will confirm this dierence in Section 4.3. Another

interesting observation can be made about the Precision and

Precision2 metrics. Their behavior diers and, moreover,

Precision has a negative correlation and this is not the case

for Precision2. This seems to be due to the fact that un-

judged documents are treated in the same way as the lowest

relevance grade 0, whereas in fact they have a higher chance

to belong to one of the top relevance grades: 92% of the

documents in the top 10 have a relevance grade higher than

0, while only 23% have a relevance grade higher than 1.

As we can see from Figure 1, when we increase #unjud-
, the maximum number of unjudged documents, to 4 ged or higher, the correlation drops for all of the metrics stud-

ied. This means that adding queries with highly incomplete

judgements adds noise to the metric signals. The problem of

unjudged documents has previously been studied by Sakai

[34], and his proposed solution is to exclude unjudged doc-

uments from the ranked list and condense the remaining

documents. Despite its heuristic nature, this idea actually

leads to an increase in correlation for most of the metrics as

shown in Figure 2. The exceptions from this rule are rrDBN

and rrDCM that supposedly suer most from the incorrect

eort function values. For example, if we miss a judgement

for the first document than for the second document we ap-

ply

a

1 1

discount

instead

of

1 2

(see

Equation

(2)).

Figure 1: Pearson correlation between oine metrics and interleaving signal. Unjudged documents were treated as irrelevant.
Figure 2: Pearson correlation between oine metrics and interleaving signal. Unjudged documents were skipped (ranked lists were condensed).

Thresholds

Even when we apply condensation, we still have a decrease

in correlation values for high values of

. One way

#unjudged

of dealing with this problem is to choose an optimal value

of #unjudged and use it to get high correlations with interleaving outcomes. We propose a dierent way of dealing

with this noisy data. Comparing systems A and B, we dis-

card all queries with dierences in metric values less than a

Figure 3: Pearson correlation between oine metrics with thresholds and interleaving signal. Unjudged documents were treated as irrelevant.

498

Figure 4: Pearson correlation between oine metrics with thresholds and interleaving signal. Unjudged documents were skipped (ranked lists were condensed).

threshold for each metric m:

m

MetricSignal

"

1 |Q

|

m

ÿ
P

pmpB, qq ´ mpA, qqq ,

qQ

m

where Q " tq P Q | |mpB, qq ´ mpA, qq|· u. This

m

m

means that we use only some portion of the queries we have

(up to 20%), but these are queries that strongly distinguish

between systems. The idea is that by choosing an appropri-

ate threshold we can tune a ranking system to produce m
the best correlation with interleaving outcomes. In order to

test the idea we split our data (ten TDI experiments) into

train and test set: we use the train set to choose the best

threshold and the test set to compute the correlation scores.

While it would be natural to do a time-based train/test

split, it appeared to be impractical with the data we have.

Firstly, it was impossible to get training and test sets of rea-

sonable sizes (either the training or the test set would consist

of only 3 experiments which might give too noisy correlation

values). Secondly, there are only few possible time-based

splits so we are not able to assess statistical significance of

the results. Instead, we used all possible 5/5 splits for our

experiments, i.e., we take a subset of five experiments as

a training set and the remaining five experiments as a test set. In total we had C150 " 252 splits and corresponding cor-
relation values. The correlation values were then averaged

and error bars were computed using the bootstrap test at

95% confidence level and 1000 samples. Results are shown

in Figures 3 and 4.

We can see in Figures 3 and 4 that the confidence intervals

are quite narrow and most of the click model-based met-

rics continue to show high correlation scores when the value

is large. If we look at one of the best performing #unjudged metrics, uUBM, we can see that thresholded variants are a

bit worse for #unjudged lower than 5, while for #unjudged

equal to 5 and higher the thresholded variants start domi-

nating, reaching the highest point for

" 9 (see

#unjudged

Figure 5).

In order to test significance of the dierences in correlation

values we used the 5/5 split procedure described above. Unlike what we did for thresholded and thresholded condensed, for the simple and condensed variants we only use the test

Figure 5: Pearson correlation between uUBM (in dierent variants) and interleaving signal .

set to determine the correlation and just ignored the train-

ing set as there is nothing we need to tune. The correla-

tion values are then averaged and confidence intervals are

computed using the bootstrap method with 1000 samples

and 95% confidence level. Three highest correlation scores

were shown by thresholded condensed variant of uUBM met-

ric (for dierent values of

), while the correla-

#unjudged

tion score for thresholded condensed uUBM (

= 9)

#unjudged

is significantly higher than any other variant (simple, con-

densed, thresholded ) of any metric. From Figures 1­4 we can

also conclude that click model-based metrics in general show

higher correlation values with the outcomes of interleaving

experiments than traditional oine metrics, especially when

we have many incomplete judgements (

° 5),

#unjudged

which confirms the hypothesis formulated in the Introduc-

tion: click model-based metrics are better correlated with

online measurements than traditional metrics. Another in-

teresting observation is that for the simple and condensed

variants there exist optimal values of the

param-

#unjudged

eter (3 and 5 respectively in our case). Conversely, for the

thresholded and thresholded condensed variants it is more

important to pick an appropriate metric and then use any

value of

higher than 5.

#unjudged

4.2 Correlation with Absolute Online Metrics

Following the original work on ERR by Chapelle et al. [10] we also compared oine IR metrics by looking at their correlation with absolute click metrics. In our experiments we used the following metrics:

, MaxRR, MinRR, MeanRR ­ maximal, minimal, mean reciprocal ranks of the click. Following the work of Radlinski et al. [32] we exclude pages with no clicks to avoid correlation with UCTR.

, UCTR ­ binary value representing click (the opposite of abandonment).

, PLC ­ number of clicks divided by the position of the lowest click.

We did not include the Search Success (SS) metric considered by Chapelle et al. [10] as it uses relevances not only clicks. We also confirmed the findings of [10] that QCTR (clicks per session) has negative or close to zero correlation with all the editorial metrics and skipped it as well.

499

A configuration is a tuple that consists of a query and ten URLs of the top ranked documents presented to a user. For each configuration in our dataset we computed the values of absolute online and oine metrics. The vectors of these metric values are then used to compute Pearson correlation (unweighted). For our dataset we used clicks collected during a three-month period in 2012. Because we used a long period and hence had a su cient amount of data, we were able to collect 12,155 configurations (corresponding to 411 unique queries) where all ten documents have relevance judgements.
The results are summarized in Table 2. A similar comparison was previously done by Chapelle et al. [10] for ERR and traditional oine metrics. The numbers they obtained are similar to ours. From the table we conclude that click model-based metrics show relatively high correlation scores while traditional oine metrics like DCG or Precision generally have lower correlations, which agrees with the results of the previous section. Using the bootstrap test (95% significance level, 1000 bootstrap samples) we confirmed that all the click model-based metrics show significantly higher correlation with all the online metrics than any of the traditional oine metrics.
As to the online metrics, we can see that the reciprocal rank family (MaxRR, MinRR, MeanRR) appears to be better correlated with the eort-based metrics (ERR, rrDBN, rrDCM), because the eort function used by these metrics is the reciprocal rank 1 (see Equation 2). The same holds for PLC as it uses recikprocal rank of the lowest click that could be viewed as "satisfaction position" used by an eortbased metric. The dierences between ERR and uSDBN, rrDBN and EBU, rrDCM and uDCM are statistically significant (using the same bootstrap test). Conversely, for the UCTR metric all the utility-based metrics show significantly higher correlation than corresponding eort-based metrics.
We also compared newly introduced click model-based metrics with older metrics: ERR (eort-based) and EBU (utility-based). The result of the comparison is marked as superscripts in the Table 2: the first superscript corresponds to ERR, the second one corresponds to EBU. The first (second) ú means that the metric is statistically significantly higher than ERR (EBU), ù -- significantly lower, "´" -- no statistical dierence can be found (95% signifi-

Table 2: Pearson correlation between oine and absolute online metrics. Superscripts represent statistically significant dierence from ERR and EBU.

-RR

Max- Min- Mean- UCTR PLC

Precision ´0.117 ´0.163 ´0.155

Precision2 0.026 0.093 0.075

DCG

0.178 0.243 0.237

0.042 0.092 0.163

´0.027 0.094 0.245

ERR EBU rrDBN rrDCM uSDBN uDCM uUBM

0.378 0.374 0.384úú 0.387úú 0.322ùù 0.374ùù 0.377´ú

0.471 0.467 0.475úú 0.478úú 0.412ùù 0.466ùù 0.469ùú

0.469 0.464 0.473úú 0.476úú 0.407ùù 0.463ùù 0.467ùú

0.199 0.198 0.194ùù 0.194ùù 0.206úú 0.198´´ 0.198´´

0.399 0.397 0.399´ú 0.400´ú 0.370ùù 0.396ùù 0.398´ú

cance level, bootstrap test). As we see, in most cases our new click metrics appear to be significantly better than the previously known ERR and EBU metrics, expect for UCTR measure, which does not account for clicks (rather for their absence) and hence obviously lacks the source of correlation with click-model based metrics. According to other metrics, rrDBN and rrDCM are better than ERR in 3 of 4 cases and better than EBU in all 4 cases, while uUBM is better than EBU in 4 of 4 cases.
In general, all the absolute click metrics are poorly correlated with oine metrics--the correlation values are much lower than correlation with interleaving outcomes. As was shown by Radlinski et al. [32], absolute click metrics are worse at capturing user satisfaction than interleaving. That is why we propose to use the results of Section 4.1 as the main way to compare oine metrics with user behavior.
4.3 Correlation Between Offline Metrics
In order to compare oine metrics to each other in terms of ranking IR systems we used data from the TREC 2011 Web Track [16]. Participants of the TREC competition were oered a set of queries ("topics" in TREC parlance) and a set of documents for each query to rank. Each document was judged using a 4-grade scale.5 For each metric we can build a list of system runs6 ordered by the metric value averaged over queries. We then compute Kendall tau correlation scores between these ordered lists; they are summarized in Table 3. As was shown by Voorhees [38], metrics with correlation scores around 0.9 can be treated as very similar because this is the level of correlation one achieves when using the same metric but dierent judges. This level of correlation to distinguish equivalent metrics was also used in subsequent papers, for example [5, 7, 35, 37].
In Table 3 such metric pairs are marked in boldface. We see that all click model-based metrics are highly correlated within their group, utility-based or eort-based, while correlations of the two metrics based on the same model (uSDBN and ERR, EBU and rrDBN, uDCM and rrDCM) are lower.
4.4 Discriminative Power
Another measure frequently used for comparing metrics is the discriminative power by Sakai [33]. This measure is a bit controversial, because high values of discriminative power do not imply a good metric. Nevertheless, extremely low values of discriminative power can serve as an indication of a metric's poor ability to distinguish dierent rankings. As was shown in previous work (e.g., [15, 36]) discriminative power is highly consistent with respect to statistical test choice. Given this fact we focus on a bootstrap test as it makes fewer assumptions about the underlying distribution. Results based on the same TREC 2011 Web Track data as used in the previous section are summarized in Table 4. As expected, highly correlated metric pairs (e.g., (rrDBN, rrDCM) and (EBU, uDCM)) have similar discriminative power.
Another observation to be made is that the eort-based metrics ERR, rrDBN and rrDCM have a lower discriminative power than the utility-based metrics uSDBN, EBU and
5Initially, a 5-grade scale was listed on a TREC 2011 description page, but in the end a 4-grade scale was used for evaluation. As in the trec_eval evaluation tool we do not distinguish between Irrelevant and Spam documents. 6In total we have 62 runs submitted by 16 teams.

500

Table 3: Correlation between oine metrics (using the TREC 2011 runs). Values higher than 0.9 are marked in boldface.

Precision Precision2
DCG
ERR uSDBN
EBU rrDBN uDCM rrDCM

Precision2
0.649 ­ ­
­ ­ ­ ­ ­ ­

DCG
0.841 0.785
­
­ ­ ­ ­ ­ ­

ERR
0.597 0.663 0.740
­ ­ ­ ­ ­ ­

uSDBN
0.730 0.780 0.857
0.807 ­ ­ ­ ­ ­

EBU
0.568 0.675 0.711
0.919 0.792
­ ­ ­ ­

rrDBN
0.397 0.526 0.530
0.754 0.585 0.788
­ ­ ­

uDCM
0.562 0.693 0.704
0.902 0.794
0.970 0.786
­ ­

rrDCM
0.442 0.551 0.592
0.826 0.638 0.822
0.917 0.813
­

uUBM
0.537 0.681 0.685
0.888 0.754
0.930 0.807
0.947 0.841

Table 4: Discriminative power of dierent metrics according to the bootstrap test (confidence level 95%).

Metric
Precision Precision2 DCG
ERR uSDBN EBU rrDBN uDCM rrDCM uUBM

Discriminative Power
50.1 % 30.8 % 48.6 %
39.3 % 51.1 % 35.1 % 21.1 % 34.7 % 26.0 % 33.3 %

uDCM, respectively. This is probably due to the fact that "position discount" for the eort-based metrics goes to zero faster than for the utility-based metrics and hence they are less sensitive to changes in the bottom of the ranked list.
5. CONCLUSION AND FUTURE WORK
In this paper we proposed a framework of click modelbased metrics to build an oine evaluation measure on top of any click model. Answering the research questions outlined in the introduction we can say that
, Click model-based metrics generally dier from traditional oine metrics, while they are quite similar to each other. Moreover, utility-based metrics are significantly dierent from eort-based metrics in terms of system ranking.
, All click model-based metrics generally show high agreement with the outcomes of online interleaving experiments and relatively high agreement with absolute click measures. However, correlation with absolute metrics is low for all oine metrics (both traditional and click model-based) compared to the correlation with interleaving outcomes.
, Unjudged documents may decrease correlation with interleaving outcomes but by using thresholds we can overcome this issue for click model-based metrics.
, Condensation and thresholding of oine metrics are effective ways of stabilizing correlations with interleaving outcomes in the presence of unjudged documents.
One natural extension of our framework of click model-based

metrics can be adding more signals from the assessors. For

example, we can ask assessors to judge not only documents,

but their snippets as well (a practice already in place at

commercial search engines). By using this we can drop the

assumption that snippet attractiveness is a function of doc-

ument relevance as was assumed by the click model-based

metrics. While attractiveness is highly correlated with doc-

ument relevance [37], it is essential to use real attractiveness

judgements when we need to evaluate a snippet algorithm,

not only ranking. It might be interesting to incorporate

attractiveness judgements into metrics and re-evaluate our

click model-based metrics using proposed modifications.

Another interesting direction is the area of good abandon-

ments. Li et al. [29] report that some snippets might be

good enough to answer the user query directly on a search

engine result page. As was shown in [13], one can ask human

judges to indicate whether a snippet contains an answer to

the user query (fully or partially). That task appeared to

be relatively easy for assessors. Given such judgements, one

can modify any evaluation metric by adding additional gain

from the snippets that contain an answer to the user's infor-

mation need. To convert this into a metric, we assign some

gain to the documents that were clicked (C " 1) and some k
gain to the documents that were only examined, but did not

attract the user (E " 1, A " 0).

i

k

Adapting click models for the unjudged/unknown docu-

ments is also an interesting direction. For example, we could

modify a click model by adding probability of a document

being skipped because it is unjudged. This question requires

further investigation and we leave it as future work.

In our work we argued that oine metrics should be better

correlated with interleaving outcomes. However, we might

want to have a metric that correlates with user satisfaction.

Some steps towards this problem have been taken in early

work by Human and Hochster [25] where user studies were

performed to analyse the meaning of editorial relevance for

real users. It would be interesting to perform a study of this

type to compare oine metrics.

Acknowledgements

We would like to thank Katja Hofmann, Maria-Hendrike Peetz and our anonymous reviewers for reading the paper and making useful comments and suggestions.
This research was partially supported by the European Community's Seventh Framework Programme (FP7/20072013) under grant agreements nr 258191 (PROMISE Network of Excellence) and 288024 (LiMoSINe project), the Netherlands Organisation for Scientific Research (NWO) un-

501

der project nrs 640.004.802, 727.011.005, 612.001.116, HOR11-10, the Center for Creation, Content and Technology (CCCT), the BILAND project funded by the CLARIN-nl program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), and the Netherlands eScience Center under project number 027.012.105.
REFERENCES
[1] K. Ali and C. Chang. On the relationship between click-rate and relevance for search engines. WIT Trans. on Inform. and Comm. Technol., 1, June 2006.
[2] J. Allan, B. Carterette, J. Aslam, and V. Pavlu. Million Query Track 2007 Overview. Technical report, DTIC Document, 2007.
[3] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated queries for known-item topics: an analysis using six european languages. In SIGIR, 2007.
[4] R. Berendsen, E. Tsagkias, M. de Rijke, and E. Meij. Generating pseudo test collections for learning to rank scientific articles. In CLEF, 2012.
[5] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In SIGIR, 2004.
[6] B. Carterette. System eectiveness, user models, and user utility: a conceptual framework for investigation. In SIGIR, 2011.
[7] B. Carterette and J. Allan. Incremental test collections. In CIKM, 2005.
[8] B. Carterette, V. Pavlu, and E. Kanoulas. If I had a million queries. In Advances in Information Retrieval, 2009.
[9] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW. ACM, 2009.
[10] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM. ACM, 2009.
[11] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. Large-scale validation and analysis of interleaved search evaluation. ACM Trans. Inform. Systems, 2012.
[12] D. Chen, W. Chen, and H. Wang. Beyond ten blue links: enabling user click modeling in federated web search. In WSDM. ACM, 2012.
[13] A. Chuklin and P. Serdyukov. Good abandonments in factoid queries. In WWW, 2012.
[14] A. Chuklin, P. Serdyukov, and M. de Rijke. Using Intent Information to Model User Behavior in Diversified Search. In ECIR, 2013.
[15] C. L. A. Clarke, N. Craswell, and I. Soboro. A comparative analysis of cascade measures for novelty and diversity. In WSDM. ACM, 2011.
[16] C. L. A. Clarke, N. Craswell, I. Soboro, and E. M. Voorhees. Overview of the TREC 2011 Web Track. In TREC 2011. NIST, 2012.
[17] C. W. Cleverdon, J. Mills, and M. Keen. Factors determining the performance of indexing systems. In ASLIB Cranfield project. 1966.
[18] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM. ACM, 2008.

[19] G. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past observations. In SIGIR. ACM, 2008.
[20] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y.-M. Wang, and C. Faloutsos. Click chain model in web search. In WWW. ACM, 2009.
[21] F. Guo, C. Liu, and Y. Wang. E cient multiple-click models in web search. In WSDM. ACM, 2009.
[22] K. Hofmann, S. Whiteson, and M. de Rijke. A probabilistic method for inferring preferences from clicks. In CIKM. ACM, 2011.
[23] K. Hofmann, A. Schuth, S. Whiteson, and M. de Rijke. Reusing Historical Interaction Data for Faster Online Learning to Rank for IR. In WSDM, 2013.
[24] J. Huang, R. W. White, G. Buscher, and K. Wang. Improving searcher models using mouse cursor activity. In SIGIR. ACM, 2012.
[25] S. B. Human and M. Hochster. How well does result relevance predict session satisfaction? In SIGIR, 2007.
[26] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inform. Systems, 20(4), 2002.
[27] T. Joachims. Optimizing search engines using clickthrough data. In KDD. ACM, 2002.
[28] R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne. Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18(1), July 2008.
[29] J. Li, S. Human, and A. Tokuda. Good abandonment in mobile and PC internet search. In SIGIR, 2009.
[30] C. Liu, F. Guo, and C. Faloutsos. BBM. In KDD. ACM, June 2009.
[31] F. Radlinski and N. Craswell. Comparing the sensitivity of information retrieval metrics. In SIGIR. ACM, 2010.
[32] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In CIKM'08. ACM, 2008.
[33] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In SIGIR, 2006.
[34] T. Sakai. Alternatives to Bpref. In SIGIR. ACM, 2007.
[35] M. Sanderson and H. Joho. Forming test collections with no system pooling. In SIGIR, 2004.
[36] M. D. Smucker and C. L. A. Clarke. Time-Based Calibration of Eectiveness Measures. In SIGIR, 2012.
[37] A. Turpin, F. Scholer, K. Jarvelin, M. Wu, and J. S. Culpepper. Including summaries in system evaluation. In SIGIR, 2009.
[38] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval eectiveness. Information processing & management, 36, 2000.
[39] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson. Expected browsing utility for web search evaluation. In CIKM. ACM, 2010.
[40] Y. Yue, Y. Gao, O. Chapelle, Y. Zhang, and T. Joachims. Learning more powerful test statistics for click-based retrieval evaluation. In SIGIR, 2010.
[41] Y. Zhang, W. Chen, D. Wang, and Q. Yang. User-click modeling for understanding and predicting search-behavior. In KDD. ACM, 2011.

502


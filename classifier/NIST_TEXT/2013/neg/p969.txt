The Knowing Camera: Recognizing Places-of-Interest in Smartphone Photos

Pai Peng  Lidan Shou  Ke Chen  Gang Chen  Sai Wu
 State Key Lab of CAD&CG  College of Computer Science and Technology
Zhejiang University Hangzhou, China
{pengpai_sh, should, chenk, cg, wusai}@zju.edu.cn

ABSTRACT
This paper presents a framework called Knowing Camera for real-time recognizing places-of-interest in smartphone photos, with the availability of online geotagged images of such places. We propose a probabilistic field-of-view model which captures the uncertainty in camera sensor data. This model can be used to retrieve a set of candidate images. The visual similarity computation of the candidate images relies on the sparse coding technique. We also propose an ANN filtering technique to speedup the sparse coding. The final ranking combines an uncertain geometric relevance with the visual similarity. Our preliminary experiments conducted in an urban area of a large city show promising results. The most distinguishing feature of our framework is its ability to perform well in contaminated, real-world online image database. Besides, our framework is highly scalable as it does not incur any complex data structure.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
places-of-interest, image retrieval, bag-of-visual-words
1. INTRODUCTION
The global population of smartphone users has hit one billion in the past year. Every day and night, the world is being captured through millions of phone cameras, and then displayed in images via Internet social applications (e.g. Facebook and Flickr) to an enormous audience. These images, ever increasing in numbers at an unprecedented rate, comprise a rich and useful data source for the physical world.
Think of a tourist visiting an unfamiliar city holding a GPS-equipped smartphone. Some of the greatest conveniences she could have are: (1) Upon taking a photo of a place (e.g. a tower or a coffee shop) using her mobile phone, the gadget displays in seconds a pop-up, showing the name
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

of the place, probably with a URL directing to further information about it; (2) Afterwards, her friends would be able to watch her online photos, automatically annotated with respective place names, without requesting for timeconsuming and error-prone tagging. This paper reports our recent work on the Knowing Camera (KC) project, which aims at developing a system for recognizing outdoor Placesof-Interest (POIs) captured in smartphone photos, relying on geotagged photo sharing Web services (e.g. Flickr).
Specifically, the key technical problem of the system can be defined as follows. Given (1) a database of POIs denoted by P , where each POI has a name and geo-location, (2) a set of geotagged images S, and (3) a query photo with its GPS location and camera geometries, find the most prominent POI (target POI) captured in that photo.
Generally, there exist two approaches to this problem, namely the spatial approach and the visual one. The spatial approach usually takes the query location and camera geometries (also known as the field-of-view or FOV) to find the closest and most visible POIs in P . Unfortunately, such solution fails to discriminate POIs in the save FOV, not to mention the errors prevalent in the geotags and camera sensors.
The visual approach requires that each photo in S is associated with one or more POIs. By computing the visual similarity between the query image and those in S, it is possible to find the best matching photos, and subsequently retrieve the target POI which receives the most contributions from the matching photos.
However, the online geotagged photos are known to be both visually and semantically noisy. The "truly" visually similar ones (judged by human perception) comprise only a small percentage, as the photos of the same semantics (a POI) could have very diversified visual appearances. In addition, each POI might be associated with other photos which are just semantically irrelevant, probably due to (i) users' mistakes, (ii) the physical distances between the cameras and the objects, and (iii) humans or events recorded in the photos.
We propose novel techniques to address the above problems. Our method relies on the following observations: First, the camera geometries (FOV) are easy to obtain when a photo is shot by a smartphone. However, those of the online geotagged photos are typically unavailable. Second, the sensors in cameras are erroneous. The sensory readings of phone direction, viewing angles, and camera location may all contain uncertainty. Third, as online photos associated

969

with each POI contain high impurity, simply computing the visual similarity between the query and POI-tagged photos leads to indiscriminating results ­ similarity values which are indistinct among several POIs.
Our first and second observations motivate the design of a probabilistic FOV model. Based on this model, all POIs relevant to the FOV are given a likelihood of being captured by the camera. Our third observation leads to a solution of visual similarity comparison based on sparse coding, a technique rooted in the signal processing domain. This technique allows a query image to be given as a linear function of the "bases", which stand for a few representatives in the original candidate images. The noisy images are not accounted in this case, making the resultant similarity (which we call SC-similarity) reasonably discriminative. As a result, our scheme can tolerate impure images containing multiple human faces and bodies, in contrast to most of the existing works [1, 4, 3] which require clean and uncontaminated image database.
Our method only extracts "bag-of-visual-words" features from each image, and does not require any additional visual data structure. The only structure being used is a conventional spatial index (an R-tree in our implementation), which accesses the visual feature vectors of the photos by their geotag locations. This makes our scheme highly scalable, efficient, and easily adaptable to the existing geotagged photo sharing services. Another desirable feature of the scheme is the possibility to balance the local recognition accuracy and visual similarity computation cost, by varying the range of the probabilistic FOV.
Summary of contributions: · A simple and scalable framework for recognizing POIs from smartphone pictures;
· A probabilistic FOV (pFOV) model to adopt the uncertainty in phone sensor readings;
· A visual similarity computing technique using sparse coding, and an approximate nearest-neighbor filtering procedure to expedite the computation;
· Experiments on real phone-captured photos achieve 92% accuracy for POIs including landmarks and nonlandmarks.
2. THE RECOGNITION FRAMEWORK
Our POI recognition framework consists of (1) a POI database P , where each point p  P has a geo-location p.loc, and (2) an image database S, where each photo s  S has a geotag location s.loc and is also associated with a POI in P . To expedite spatial accesses, we also generate an R-tree which indexes all images in S by their locations s.loc.
Generally, a query q contains a query image q.img, and its camera geometries stored in q.f ov, including its GPS location, the angles of view, the maximum visible distance, and the direction of the camera. Each query is processed in three consecutive phases, namely the spatial phase, the visual phase, and the ranking phase, as described in the following.
2.1 The Spatial Phase
The spatial phase takes the camera location as the center and makes a 2D square range query on the R-tree to retrieve images which are geotagged in the query box. These images are called "local images".

R

v

p

DT

r

q

di

T i

qi

Figure 1: The probabilistic FOV model consisting of 4 parameters, namely the camera location q, the camera direction v, the camera viewing angle , and the maximum visible distance R.
Next, the POIs associated with the local images undergo a probabilistic FOV culling procedure, which removes the POIs that are geometrically impossible to appear in the query image (detailed in Section 2.1.2). Images associated with any culled POI are deleted from the local image set. The remaining local images now become the candidate images, and their associated POIs are considered the candidate POIs.
Subsequently, we compute for each candidate POI p a geometric-relevance value (denoted by geo-relevance), which indicates the geographical probability that p appears in the query pFOV (detailed in Section 2.1.3).
2.1.1 The Probabilistic FOV Model
Given a query q, figure 1 illustrates its probabilistic FOV (pFOV) model, which is derived from the conventional FOV (field-of-view). The conventional FOV model consists of 4 parameters, namely the camera location q, the camera direction v, the viewing angle , and the maximum visible distance R. The first three quantities can be obtained in real-time from the sensory readings while the last parameter R is determined by the camera specification.
It is important to note that the sensory readings may contain an abundance of uncertainty due to device errors. Using the GPS data of the camera location as an example, the actual camera location might be a hundred meters away from the GPS readings. Thus, we use a 2D Gaussian distribution centered at the current location reading q to model the actual camera location. The shaded circle in Figure 1 indicates the probable camera locations, with an uncertain radius of r. Likewise, the direction v, and the viewing angle  are all uncertain variables with uncertainty margins. For clarity of illustration we do not plot these margins in the Figure. Such probability distribution of FOV parameters define the probabilistic FOV of the camera.
2.1.2 Probabilistic FOV Culling
It can be seen that for any POI p appearing in the query FOV, there must exist a probabilistic FOV instance (at qi) which contains p. In other words, the union of all pFOV instances must cover all possible candidate POIs. This observation leads to a pFOV culling algorithm, which relies on the following Lemma. Lemma 1: All candidate POIs whose locations are outside the union of all pFOV instances can be discarded.
A sample process of pFOV culling is given in Figure 2,

970

query box

pd

pb

pe

sb FOV

qc

pa

se q qcc
sd r

o

pf

Camera Direction
pc
sc
sf

Figure 2: Illustration of pFOV culling and candidate images/POIs. The blue markers indicate the geotagged locations of the images {si}, while the red ones {pi} indicate their respective POIs. The circular sectors centered at q, q, and q illustrate the pFOV instances of q.
where POIs located in the shaded region can be culled. Thus, POIs pe, pf can be culled, because their actual FOVs cannot be any pFOV instance for q. However, pd cannot be culled because it is within the reach of a possible pFOV instance (q), even when it is outside the query box.
Given the current camera parameters the pFOV culling algorithm can quickly determine the candidate POIs which may possibly appear in the image. As a result, the cost of subsequent evaluation of geo-relevance and SC-similarity can be reduced significantly.

2.1.3 Computing Geo-Relevance
Now let us evaluate the probability that a POI p is captured by an exact FOV. As shown in Figure 1, given an exact FOV at q and a point p whose distance to q is d and its viewing angle (the angle between v and - qp) is , it is easy to see that p has a higher probability to be captured by the camera if d is smaller or angle  is smaller. Specifically, we model the probability distribution function of POI p being captured by an exact FOV at q as the following function:

P (,

d)

=

e-

2 21 2

· e-

d2 22 2

(1)

where d = pq. 1 and 2 are parameters which ensure that P (, d) is a negligible value ( 0) if p is outside the FOV region. Thus, the probability of a POI p captured by a query FOV is a cumulative distribution function given by

 geo-relevance =

e · e dq -

c2 21 2

-

d2 22 2

(2)

Q

where Q is the circular region for the Gaussian distribution of the camera location with radius r, c > 0 is a factor simulating the 2D uncertainty of the angular parameters, namely  and d. In practice, the geo-relevance is evaluated in a discretized form of Equation 2. This is omitted to save space.

2.2 The Visual Phase
Given a list of candidate images C, we consider each of their respective visual feature vectors as a basis signal, and employ the sparse coding technique to obtain a linear combination of the basis vectors, which maximally reproduces the original signal (the feature vector of the original image).

2.2.1 Computing SC-similarity

To compute the visual similarity, each candidate image is

represented as a bag-of-visual-words column vector. Let D

be the matrix where each column is the vector for each can-

didate image in C. Then the problem can be described as:

Given a query image x, can we represent it as a linear com-

bination of other candidate images(columns in D). This is a

typical problem of sparse coding and the objection function

is given by

min


1 2

Dw

-

x2

+



w1

(3)

where  controls the sparsity in w and w1 is the l1-norm of the parameter vector. Furthermore, the original query

image x can be reconstructed by multiplying D by the sparse

code w.

This optimization problem is called Lasso and can be

solved by the least angle regression(LAR) approach [2]. The

optimization result is a sparse code as w = (0, 1, ..., p)T ,

where i indicates the contribution of the ith candidate

image to reproduce the query image. Weight value i is

then used as the visual similarity between candidate image

i and the query. Finally, these weight values are aggregated

by different POIs, so that each POI receives the sum of

all the weights from its associated candidate images. The

aggregated weights are called sparse coding similarity (SC-

similarity) for each POI.



SC-similarity = diP OI i

(4)

i

2.2.2 ANN Filtering
So far, we have proposed the method of acquiring visual similarity between a query image and a certain candidate image via sparse coding (SC). However, the solution to SC is typically time-consuming when the number of columns of D is large.
To improve the efficiency without compromising the SC effect, we conduct an Approximate Nearest Neighbor (ANN) filtering procedure on the candidate columns before conducting sparse coding. ANN is able to retrieve the top-k approximate nearest-neighbours at significantly reduced cost compared to exact NN search in high dimensions. Since the SC process aims at reconstructing the query image with similar ones in D, columns which are dissimilar to the query mostly do not contribute in the reconstruction. Therefore, the reason for using ANN is to discard the dissimilar columns before invoking the expensive SC process.

2.3 The Ranking Phase
Finally, for each candidate POI, we compute a vote score as a linear combination of the geo-relevance and SC-similarity of each candidate POI
score =  · geo-relevance + (1 - ) · SC-similarity (5)

where 0    1 is a balancing factor. The POI with the top ranking score is taken as the final result.

3. EXPERIMENT RESULTS
Image Dataset Given the same availability of image data, our framework is expected to perform better in sparsely populated regions, as the geo-relevance and visual similarity could be more discriminative. Therefore, we test the framework for urban area only. We download from Flickr 151,193

971

City Hall Hotel *** Mosque ***

Figure 3: Noisy dataset downloaded from Flickr. The red frames indicate the "noisy" images, which do not show outdoor appearance of the places.
Table 1: Results of Computational Cost

Method
KC (no culling) KC (culling, no ANN) KC (culling, ANN) BOW (culling, no ANN) BOW (culling, ANN)

Query time
31.56s 7.03s 0.68s 0.79 s 0.31s

# candidate images
12748 3634 412 3634 412

geotagged images containing 2,256 distinct POIs in a densely populated area of a large city. The POI information are acquired from other sources. Although the dataset has limited size, it can sufficiently simulate the density of photos in urban areas of the world. A larger dataset (spanned over a greater region) is not expected to impact the recognition performance, due to the locality introduced by the spatial range query. It is also important to note that the majority of the dataset are noisy images which do not capture the outdoor appearances of the POIs, as depicted in Figure 3.
Query Set We develop an Android APP to capture query photos of the POIs in the dataset. The APP records the camera FOV parameters at the shooting time. The true POI of each query image (ground truth) is determined by the user when the photo is captured. We employ 4 users who shoot totally 170 POIs, including 48 landmarks and 122 non-landmarks. Each POI is captured for 4 times by different users. The results presented below are averaged among the users.
Baseline Approach Besides our proposed framework KC, we also implement a state-of-the-art approach bag-ofvisual-words(BOW ) as a baseline.
Performance Results
Table 1 shows the results of computational costs with pFOV culling and ANN filtering enabled or disabled. It can be seen that pFOV culls a significant number of images from the original query box. ANN can further reduce the number of candidate images (columns) by almost an order of magnitude. Therefore, these two techniques can effectively reduce the query processing cost.
The POI recognition accuracy of the proposed approach is illustrated in Figure 4, where we vary the spatial query box size and plot the results of different  values. When  = 1, the KC scheme degenerates to a pure spatial method. For comparison, we also plot the accuracy of the BOW. It can be seen that (i) The pure spatial method alone does not perform well; (ii) As the query box expands, the accuracy of all schemes is significantly improved (when box size < 120m) because more relevant photos are added into the candidate

accuracy

1.0

0.9

0.8

0.7

KC( =1) KC( =0)

0.6

KC( =0.5) BOW( =0.5)

0.5

BOW( =0)

0.4

0.3

0.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 query box size(km)

Figure 4: Results of Recognition Accuracy
image set. However, as the query box size increases further, the accuracy of pure SC-similarity ( = 0) declines considerably. This is a clear indication that the recognition problem becomes harder as the search space increases. A more apparent decline can be observed for the BOW method, due to the same reason. Fortunately, the problem in SC-similarity can be compensated by the geo-relevance in our scheme, as when  = 0.5, the accuracy is very stable and remains to be above 90%. Such results confirm the effectiveness of our spatio-visual ranking score.

4. CONCLUSION
We presented the Knowing Camera framework for realtime recognizing places-of-interest in smartphone photos. The framework captured the device uncertainty in a probabilistic field-of-view model, which could be used to cull candidate images from the database. The visual similarity computation of the candidate images relied on the sparse coding technique. We also performed an ANN filtering technique to speedup the sparse coding. The final ranking score combined the geometric relevance and the visual similarity. Our experiments in an urban area showed promising results.
Acknowledgments
The work is supported by the National Science Foundation of China (GrantNo. 61170034).
5. REFERENCES
[1] Y. S. Avrithis, Y. Kalantidis, G. Tolias, and E. Spyrou. Retrieving landmark and non-landmark images from community photo collections. In ACM Multimedia, pages 153­162. ACM, 2010.
[2] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of Statistics, 32(2):407­451, 2004.
[3] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm. Modeling and recognition of landmark image collections using iconic scene graphs. In ECCV, pages 427­440. Springer, 2008.
[4] M. Z. Zheng, Yan-Tao, Y. Song, H. Adam, U. Buddemeier, A. Bissacco, F. Brucher, T.-S. Chua, and H. Neven. Tour the world: Building a web-scale landmark recognition engine. In CVPR, pages 1085­1092. IEEE, 2009.

972


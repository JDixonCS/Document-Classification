Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

ery Performance Prediction for Pseudo-Feedback-Based Retrieval

Haggai Roitman
IBM Research ­ Haifa haggai@il.ibm.com
ABSTRACT
The query performance prediction task (QPP) is estimating retrieval e ectiveness in the absence of relevance judgments. Prior work has focused on prediction for retrieval methods based on surface level query-document similarities (e.g., query likelihood). We address the prediction challenge for pseudo-feedback-based retrieval methods which utilize an initial retrieval to induce a new query model; the query model is then used for a second ( nal) retrieval. Our suggested approach accounts for the presumed e ectiveness of the initially retrieved list, its similarity with the nal retrieved list and properties of the latter. Empirical evaluation demonstrates the clear merits of our approach.
ACM Reference Format: Haggai Roitman and Oren Kurland. 2019. Query Performance Prediction for Pseudo-Feedback-Based Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331369
1 INTRODUCTION
The query performance prediction task (QPP) has attracted much research attention [2]. The goal is to evaluate search e ectiveness with no relevance judgments. Almost all existing QPP methods are (implicitly or explicitly) based on the assumption that retrieval is performed using (only) document-query surface-level similarities [2]; e.g., standard language-model-based retrieval or Okapi BM25.
We address the QPP challenge for a di erent, common, retrieval paradigm: pseudo-feedback-based retrieval [3]. That is, an initial search is performed for a query. Then, top-retrieved documents, considered pseudo relevant, are utilized to induce a query model (e.g., expanded query form) that is used for a second ( nal) retrieval.
Thus, in contrast to the single-retrieval setting addressed in almost all prior work on QPP, here the e ectiveness of the nal result list presented to the user depends not only on the retrieval used to produce it (e.g., properties of the induced query model), but also on the initial retrieval using which the query model was induced. A case in point, if the initial retrieval is poor, it is highly unlikely
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331369

Oren Kurland
Technion ­ Israel Institute of Technology kurland@ie.technion.ac.il
that the nal result list will be e ective regardless of the querymodel induction approach employed. Accordingly, our novel approach for QPP for pseudo-feedback-based retrieval accounts for the presumed e ectiveness of the initially retrieved list, its association with the nal retrieved list and properties of the latter.
Empirical evaluation shows that the prediction quality of our approach substantially transcends that of state-of-the-art prediction methods adopted for the pseudo-feedback-based retrieval setting -- the practice in prior work on QPP for pseudo-feedback-based retrieval [6, 14, 15].
2 RELATED WORK
In prior work on QPP for pseudo-feedback-based retrieval, existing predictors were applied either to the nal retrieved list [6, 14] or to the initially retrieved list [15]. We show that our prediction model, which incorporates prediction for both lists and accounts for their association, substantially outperforms these prior approaches.
The selective query expansion task (e.g., [1, 5, 13]) is to decide whether to use the pseudo-feedback-based query model, or stick to the original query. In contrast, we predict performance for a list retrieved using the query model.
In several prediction methods, a result list retrieved using a pseudofeedback-based query model is used to predict the performance of the initially retrieved list [15, 20]. In contrast, our goal is to predict the e ectiveness of the nal result list; to that end, we also use prediction performed for the initial list.
3 PREDICTION FRAMEWORK
Suppose that some initial search is applied in response to a query q over a document corpus D. Let Dinit be the list of the k most highly ranked documents. Information induced from the top documents in Dinit is used for creating a new query model (e.g., expanded query) used for a second retrieval; i.e., these documents are treated as pseudo relevant. We use Dscnd to denote the result list, presented to the user who issued q, of the k documents most highly ranked in the second retrieval.
Our goal is to predict the e ectiveness of Dscnd . To this end, we appeal to a recently proposed query performance prediction (QPP) framework [16]. Speci cally, the prediction task amounts to estimating the relevance likelihood of Dscnd , p(Dscnd |q, r ), where r is a relevance event1.
We can use reference document lists to derive an estimate for p(Dscnd |q, r ) [16]:
1This is post-retrieval prediction which relies on analyzing the retrieved list. The relevance of a retrieved list is a notion that generalizes that for a single document [16]. At the operational level, a binary relevance judgment for a list can be obtained by thresholding any list-based evaluation measure.

1261

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

p^(Dsc

nd

|q,

r

)

def
=

p^(Dscnd |q, L, r )p^(L|q, r );

(1)

L

L is a document list retrieved for q; herein, p^ is an estimate for p. The underlying idea is that strong association (e.g., similarity) of Dscnd with reference lists L (i.e., high p^(Dscnd |q, L, r )) which are presumably e ective (i.e., high p^(L|q, r )) attests to retrieval e ectiveness.
It was shown that numerous existing post-retrieval prediction methods can be instantiated from Equation 1 where a single reference list is used. Similarly, here we use Dinit as a reference list:

p^(Dscnd |q, r )  p^(Dscnd |q, Dinit , r )p^(Dinit |q, r ).

(2)

That is, by the virtue of the way Dscnd is created -- i.e., using information induced from Dinit -- we assume that Dinit is the
most informative reference list with respect to Dscnd 's e ectiveness. A case in point, an expanded query constructed from a poor

initial list (i.e., which mostly contains non-relevant documents) is

not likely to result in e ective retrieval.

3.1 Instantiating Predictors

Equation 2 can be instantiated in various ways, based on the choice

of estimates, to yield a speci c prediction method. To begin with,

any post-retrieval predictor, P, can be used to derive p^(Dinit |q, r )

[16].

For p^(Dscnd |q, Dinit , r ) in Equation 2, we use logarithmic inter-

polation:

p^(D s c n d

|q,

Dini

t,

r

)

def
=

p^[P](Dscnd |q, r ) p^(Dscnd |Dinit , r )(1- );

(3)

 ( [0, 1]) is a free parameter. The estimate p^[P](Dscnd |q, r ) corre-

sponds to the predicted e ectiveness of Dscnd , where the predic-

tion, performed using the post-retrieval predictor P, ignores the

knowledge that Dscnd was produced using information induced from Dinit .
The estimate p^(Dscnd |Dinit , r ) from Equation 3, of the association between Dscnd and Dinit , is usually devised based on some symmetric inter-list similarity measure sim(Dscnd , Dinit ) [16]. However, as Roitman [11] has recently suggested, a more e ective esti-

mate can be derived by exploiting the asymmetric co-relevance rela-

tionship between the two lists (cf., [10]); that is, p^(Dscnd |Dinit , r )

is the likelihood of Dscnd given that a relevance event has hap-

pened and Dinit was observed:

p^(D s c nd

|Dinit

,

r

)

def
=

p^(D s c nd

|Dinit

)

d

Dscnd

p^(d

|Dscnd

) p^(d

p^(d, r |Dinit

|Dinit ) )p^(r |Dinit

)

;

(4)

d

is

a

document.

Following

Roitman

[11],

we

use

p^(D s c nd

|Dinit

)

def
=

sim(Dscnd , Dinit ). Similarly to some prior work [7, 11], for p^(r |Dinit )

we use the entropy of the centroid (i.e., the arithmetic mean) of the

language models of documents in Dinit . We further assume that p^(d |Dscnd ) and p^(d |Dinit ) are uniformly distributed over Dscnd and Dinit , respectively. Finally, to derive p^(d, r |Dinit ), we follow
Roitman [11] and use the corpus-based regularized cross entropy

(CE) between a relevance model, R [Dinit ], induced from Dinit , and

a language model, pd (·), induced from d:

p^(d, r |Dinit )

def
=

CE(R [Dinit ] ||pd (·)) - CE(R [Dinit ] ||p D (·));

(5)

p^D (·) is a language model induced from the corpus. Further details about language model induction are provided in Section 4.1.

4 EVALUATION
4.1 Experimental setup
4.1.1 Datasets. We used for evaluation the following TREC corpora and topics: WT10g (451-550), GOV2 (701-850), ROBUST (301450, 601-700) and AP (51-150). These datasets are commonly used in work on QPP [2]. Titles of TREC topics were used as queries. We used the Apache Lucene2 open source search library for indexing and retrieval. Documents and queries were processed using Lucene's English text analysis (i.e., tokenization, lowercasing, Porter stemming and stopping). For the retrieval method -- both the initial retrieval and the second one using the induced query model -- we use the language-model-based cross-entropy scoring (Lucene's implementation) with Dirichlet smoothed document language models where the smoothing parameter was set to 1000.

4.1.2 Pseudo-feedback based retrieval. Let cx (w) denote the oc-

currence

count

of

a

term

w

in

a

text

(or

text

collection)

x

;

let

|x

|

def
=

w x

cx (w)

denote

x 's

length.

Let

px[µ ] (w )

def
=

cx (w )+µpD (w ) |x |+µ

de-

note

x 's

Dirichlet-smoothed

language

model,

where pD (w)

def
=

c

D (w |D|

)

.

For

a

query q

and a

set of

pseudo-relevant

documents F



Dinit , pF (·) denotes a pseudo-feedback-based query model.

We use three state-of-the-art pseudo-feedback-based (PRF) query-

model induction methods. All three incorporate query anchoring

as described below. The rst is the Relevance Model [8] (RM):

pF

(w

)

def
=

pd[0] (w )pq[µ ] (d ),

(6)

d F

where

pq[µ

]

(d

)

def
=

pd[µ ] (q ) dF p^d[µ](q)

and

pd[µ ] (q )

def
=

w q cd (w) log pd[µ](w).

The second is the Generative Mixed Model [19] (GMM) which is

estimated using the following EM algorithm iterative update rules:

t

(n)(w

)

def
=

(1-

)p

(n-1) F

(w

)

(1-

)p

(n-1) F

(w

)+

pD

(w

)

,

pF(n)(w )

def
=

d F cd (w )t (n)(w )
wV d F cd (w )t (n)(w ) .

The third is the Maximum-Entropy Divergence Minimization Model [9]

(MEDMM): pF (w)  exp

1 

d F p^q[µ](d) log pd[0](w) -

 

p

D

(w

)

.

We

applied

query

anchoring

[8,

9,

19]

to

all

three

models:

pF

,



(w

)

def
=

pqMLE (w) + (1 - )pF (w); pqMLE (w) is the maximum likelihood es-

timate of w with respect to q and   [0, 1].

We used the n most highly ranked documents in the initial re-

trieval for query-model induction (i.e., inducing pF (·)). Then, a sec-
ond query qf was formed using the l terms w assigned the highest pF (w). We resubmitted qf to Lucene3 to obtain Dscnd .

4.1.3 Baseline predictors. As a rst line of baselines, we use Clarity [4], WIG [20] and NQC [17], which are commonly used post-retrieval QPP methods [2]. These baselines are also used for P in Eq. 3. Clarity [4] is the divergence between a language model induced from a retrieved list and that induced from the corpus.

2 http://lucene.apache.org 3Expressed in Lucene's query parser syntax as: w1^pF (w1) w2^pF (w2) . . . wl^pF (wl ).

1262

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Prediction quality. Boldface: best results per basic QPP method and query-model induction method. Underlined: best results per query-model induction method. '' marks a statistically signi cant di erence between PFR-QPP and either the second best predictor (in case PFR-QPP is the best) or the best predictor (in case PFR-QPP is not).

Method
ListSim
NQC(Dscnd |qf ) NQC(Dscnd |q) NQC(Dinit |q) RefList(NQC) PFR-QPP(NQC)
Clarity(Dscnd |qf ) Clarity(Dscnd |q) Clarity(Dinit |q) RefList(Clarity) PFR-QPP(Clarity)
WIG(Dscnd |qf ) WIG(Dscnd |q) WIG(Dinit |q) RefList(WIG) PFR-QPP(WIG)
WEG(Dscnd |qf ) WEG(Dscnd |q) WEG(Dinit |q) RefList(WEG) PFR-QPP(WEG)

RM
.442
.293 .071 .483 .535 .513
.292 .327 .363 .481 .408
.270 .253 .237 .338 .370
.231 .141 .353 .443 .456

WT10g

GMM MEDMM

.532

.337

.228

.182

.051

.092

.397

.424

.531

.415

.557

.410

.325

.316

.227

.368

.350

.314

.567 .557

.388 .398

.307

.388

.105

.153

.221

.224

.384 .466

.311 .353

.205

.331

.134

.239

.311

.313

.483 .575

.371 .436

RM
.490
.599 .437 .486 .517 .596
.230 .278 .282 .480 .615
.263 .583 .562 .581 .630
.585 .513 .532 .527 .660

GOV2

GMM MEDMM

.432

.410

.545

.353

.418

.283

.414

.414

.486

.457

.549 .550

.157

.130

.200

.084

.261

.264

.469 .497

.414 .490

.301

.448

.424

.276

.498

.498

.562 .603

.480 .575

.548

.432

.504

.390

.470

.409

.481 .562

.427 .481

RM
.543
.653 .475 .635 .654 .671
.450 .412 .452 .582 .589
.424 .651 .649 .660 .665
.661 .566 .635 .654 .688

ROBUST

GMM MEDMM

.528

.436

.637

.622

.492

.620

.605

.602

.631 .661

.621 .642

.393

.409

.350

.349

.441

.401

.575 .607

.535 .566

.361

.381

.455

.430

.618

.578

.638 .682

.637 .648

.656

.693

.571

.674

.619

.616

.633 .664

.632 .688

RM
.537
.655 .574 .550 .607 .670
.313 .236 .320 .589 .652
.159 .414 .554 .639 .650
.627 .560 .526 .580 .675

AP GMM
.343
.617 .479 .536 .530 .640
.408 .350 .456 .519 .585
.281 .281 .614 .580 .634
.562 .491 .474 .467 .552

MEDMM
.407
.454 .530 .502 .572 .650
.339 .270 .308 .511 .651
.285 .226 .505 .608 .643
.575 .575 .518 .555 .664

WIG [20] and NQC [17] are the corpus-regularized4 mean and
standard deviation of retrieval scores in the list, respectively. We
further compare with the Weighted Expansion Gain (WEG) [6] method
­ a WIG alternative which regularizes with the mean score of doc-
uments at low ranks of the retrieved list instead of the corpus.
We use three variants of each of the four predictors described
above. The rst two directly predict the e ectiveness of the nal
retrieved list Dscnd using either (i) the original query q (denoted P(Dscnd |q)), or (ii) the query qf (denoted P(Dscnd |qf )) which was induced from Dinit as described above (cf., [15, 20]). The third variant (denoted P(Dinit |q)) is based on predicting the performance of Dscnd by applying the predictor to Dinit as was the case in [15].
To evaluate the impact of our inter-list association measure in
Eq. 4, we use two additional baselines. The rst, denoted ListSim
[16], uses sim(Dscnd , Dinit ) to predict the performance of Dscnd . The second, denoted RefList(P) [7, 16], treats Dinit as a pseudoe ective list of Dscnd and estimates Dscnd 's performance by:

p^Re

f

Li

st

(r

|Dscnd

,

q)

def
=

sim(Dscnd , Dinit )p^[P](Dinit |q, r ),

where P is one of the four basic QPP methods described above. There are two important di erences between our proposed method and RefList. First, we use the query q in the list association measure in Eq. 3. Second, we use an asymmetric co-relevance measure between the two lists in Eq. 4 compared to the symmetric one used in RefList.

4.1.4 Setup. Hereinafter, we refer to our proposed QPP method from Eq. 2 as PFR-QPP: Pseudo-Feedback based Retrieval QPP.

4To this end, the corpus is treated as one large document.

PFR-QPP(P) is a speci c predictor instantiated using the base predictor P. We predict for each query the e ectiveness of the 1000 documents (i.e., k = 1000) most highly ranked in the nal result list Dscnd . Prediction quality is measured using the Pearson correlation between the ground truth AP@1000 (according to TREC's relevance judgments) and the values assigned to the queries by a prediction method.
Most prediction methods described above incorporate free parameters. Following the common practice [2], we set m  k ­ the number of documents in a given list (i.e., either Dscnd or Dinit ) used for calculating a given predictor's value; with m  {5, 10, 20, 50, 100, 150, 200, 500, 1000}. We applied term clipping with l terms (l  {10, 20, 50, 100}) to the relevance model used in Clarity and PFRQPP. Following [16], we realized the ListSim, RefList and PFRQPP baselines using Rank-Biased Overlap (RBO(p)) [18] as our listsimilarity measure sim(·) (with p = 0.95, further following [16]). For our PFR-QPP method, we set   {0, 0.1, 0.2, . . . , 0.9, 1.0}. Query models are induced from the n = 50 top documents. For the GMM and MEDMM models, we set their free-parameters to previously recommended values, i.e., GMM ( = 0.5) [19] and MEDMM ( = 1.2,  = 0.1) [9]. Unless stated otherwise, the query anchoring and clip-size parameters in all models were xed to  = 0.9 and l = 20, respectively. The prediction quality for other (, l) settings is studied in Section 4.2.
Following [12, 17], we trained and tested all methods using a 2-fold cross validation approach. Speci cally, in each dataset, we generated 30 random splits of the query set; each split had two folds. We used the rst fold as the (query) train set. We kept the second fold for testing. We recorded the average prediction quality

1263

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Pearson

NQC
0.7

0.6

0.5

0.4

0.3 0

0.2 0.4 0.6 0.8 

WIG
0.7

0.6

0.5

0.4

0.3 0

0.2 0.4 0.6 0.8 

Pearson

Pearson

0.7 0.6 0.5 0.4 0.3
0
0.7 0.6 0.5 0.4 0.3
0

Clarity
0.2 0.4 0.6 0.8 
WEG
0.2 0.4 0.6 0.8 

Pearson

Pearson

NQC
0.7 0.6 0.5 0.4 0.3
10 30 50 70 90 l
WIG
0.7 0.6 0.5 0.4 0.3
10 30 50 70 90 l

Pearson

Pearson

Clarity
0.7
0.6
0.5
0.4
0.3 10 30 50 70 90 l
WEG
0.7
0.6
0.5
0.4
0.3 10 30 50 70 90 l

Pearson

(a) Query anchoring ().

(b) Number of terms (l ).

Figure 1: Sensitivity to free-parameter values of the relevance model used for query-model induction.

over the 30 splits. Finally, we measured statistically signi cant differences of prediction quality using a two-tailed paired t-test with p < 0.05 computed over all 30 splits.
4.2 Results
Table 1 reports the prediction quality of our method and the baselines. We can see that in the vast majority of cases, our PFR-QPP approach statistically signi cantly outperforms the baselines.
Applying basic QPP methods. We rst compare the three variants of the four basic QPP methods. We observe that, in most cases, utilizing the PRF-induced query qf for predicting the performance of the nal list Dscnd (P(Dscnd |qf )), yields better prediction quality than using the original query q (P(Dscnd |q)). In addition, predicting the performance of Dscnd by applying the base predictor to the initially retrieved list Dinit (P(Dinit |q)) yields high prediction quality -- sometimes even higher than applying the predictor to Dscnd . These ndings provide further support the motivation behind PFR-QPP: integrating prediction for the initially retrieved list and the nal retrieved list and accounting for their asymmetric co-relevance relation.
PFR-QPP vs. reference-list based alternatives. First, in line with previous work [7, 15, 16], the high prediction quality of ListSim and RefList in our setting shows that the similarity between the two lists is an e ective performance indicator. Moreover, combining prediction for the performance of the initial list with its similarity with the nal list (i.e., RefList) yields prediction quality that transcends in most cases that of using only the similarity (i.e., ListSim). Finally, our PFR-QPP method which uses prediction for both the initial and nal lists, and accounts for their asymmetric co-relevance relationship, outperforms both ListSim and RefList in most cases, and often to a statistically signi cant degree.
Sensitivity to query-model induction tuning. Using the ROBUST dataset and the relevance model (RM), Figure 1 reports the e ect on prediction quality of varying the value of the query anchoring parameter (; while xing l = 20) and the number of terms used after clipping (l; while xing  = 0) in the query model, and hence, in qf . As can be seen, decreasing  or increasing l decreases the prediction quality of all methods. With reduced query anchoring or when using more terms, the induced queries (qf ) tend to become

more "verbose", with less emphasis on the original query q. Indeed, a recent study showed that existing QPP methods are less robust for long queries [12]. Finally, we see that for any value of  and l, PFR-QPP outperforms the baselines.
5 CONCLUSIONS
We addressed the QPP task for pseudo-feedback-based retrieval, where the nal retrieved list depends on an initially retrieved list ­ e.g., via a query model induced from the latter and used to produce the former. Our approach accounts for the predicted e ectiveness of each of the two lists as well as to their asymmetric co-relevance relation. Empirical evaluation showed that our approach signi cantly outperforms a variety of strong baselines.
ACKNOWLEDGEMENT
We thank the reviewers for their comments. This work was supported in part by the Israel Science Foundation (grant no. 1136/17)
REFERENCES
[1] Giambattista Amati, Claudio Carpineto, and Giovanni Romano. Query di culty, robustness, and selective application of query expansion. In Proceedings of ECIR, pages 127­137, 2004.
[2] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of SIGIR, pages 1196­1197, New York, NY, USA, 2012. ACM.
[3] Claudio Carpineto and Giovanni Romano. A survey of automatic query expansion in information retrieval. ACM Comput. Surv., 44(1):1:1­1:50, January 2012.
[4] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query performance. In Proceedings of SIGIR, pages 299­306, New York, NY, USA, 2002. ACM.
[5] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. A framework for selective query expansion. In Proceedings of CIKM, pages 236­237, New York, NY, USA, 2004. ACM.
[6] Ahmad Khwileh, Andy Way, and Gareth J. F. Jones. Improving the reliability of query expansion for user-generated speech retrieval using query performance prediction. In CLEF, 2017.
[7] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri Rom. Back to the roots: A probabilistic framework for query-performance prediction. In Proceedings of CIKM, pages 823­832, New York, NY, USA, 2012. ACM.
[8] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In Proceedings of SIGIR.
[9] Yuanhua Lv and ChengXiang Zhai. Revisiting the divergence minimization feedback model. In CIKM '14.
[10] Fiana Raiber, Oren Kurland, Filip Radlinski, and Milad Shokouhi. Learning asymmetric co-relevance. In Proceedings of ICTIR, pages 281­290, 2015.
[11] Haggai Roitman. Enhanced performance prediction of fusion-based retrieval. In Proceedings of ICTIR, pages 195­198, New York, NY, USA, 2018. ACM.
[12] Haggai Roitman. An extended query performance prediction framework utilizing passage-level information. In Proceedings of ICTIR, pages 35­42, New York, NY, USA, 2018. ACM.
[13] Haggai Roitman, Ella Rabinovich, and Oren Sar Shalom. As stable as you are: Re-ranking search results using query-drift analysis. In Proceedings of HT, pages 33­37, New York, NY, USA, 2018. ACM.
[14] Harrisen Scells, Leif Azzopardi, Guido Zuccon, and Bevan Koopman. Query variation performance prediction for systematic reviews. In Proceedings of SIGIR, pages 1089­1092, New York, NY, USA, 2018. ACM.
[15] Anna Shtok, Oren Kurland, and David Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proceedings of SIGIR, pages 259­266, New York, NY, USA, 2010. ACM.
[16] Anna Shtok, Oren Kurland, and David Carmel. Query performance prediction using reference lists. ACM Trans. Inf. Syst., 34(4):19:1­19:34, June 2016.
[17] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst., 30(2):11:1­11:35, May 2012.
[18] William Webber, Alistair Mo at, and Justin Zobel. A similarity measure for inde nite rankings. ACM Trans. Inf. Syst., 28(4):20:1­20:38, November 2010.
[19] Chengxiang Zhai and John La erty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01.
[20] Yun Zhou and W. Bruce Croft. Query performance prediction in web search environments. In Proceedings of SIGIR, pages 543­550, New York, NY, USA, 2007. ACM.

1264


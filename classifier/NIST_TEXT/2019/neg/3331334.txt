Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Normalized ery Commitment Revisited

Haggai Roitman
IBM Research - Haifa haggai@il.ibm.com

ABSTRACT
We revisit the Normalized Query Commitment (NQC) query performance prediction (QPP) method. To this end, we suggest a scaled extension to a discriminative QPP framework and use it to analyze NQC. Using this analysis allows us to redesign NQC and suggest several options for improvement.

AP@1000

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

0.8

R² = 0.2934

0.7

0.6

AP@1000

0.5

scaled

0.4

0.3

0.2

0.1

0.1 Clarity 0.2

0.3

0

0

0.2

AP@1000 Histogram (Topics 351-400)

0.4 Clarity

R² = 0.3611

0.6

0.8

ACM Reference Format: Haggai Roitman. 2019. Normalized Query Commitment Revisited. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331334

Frequency

20 15 10
5 0
0.0024143990.1092984850.2161825710.3230666560.4299507420.5368348280.643718914 Bin

More

Figure 1: Example of scaling Clarity's values (TREC Robust

1 INTRODUCTION

queries 351-400,  = 0.3)

Query performance prediction (QPP) is a core IR task whose primary goal is to assess retrieval quality in the absence of relevance judgements [1]. In this work, we revisit Shtok et al.'s Normalized Query Commitment (NQC) QPP method [10]. NQC is a state-ofthe-art post-retrieval QPP method [1], based on document retrieval scores variance analysis. Nowadays, the NQC method serves as a common competitive baseline in many QPP works [4, 6, 8, 11].
We rst present the NQC method and shortly discuss the motivation behind it. We then shortly present Roitman et al.'s discriminative QPP framework [8]. Using a scaled extension to this framework, we analyze the NQC method, "deconstructing" it into its most basic parts. Based on this analysis, we revise NQC's design and suggest several options for improvement. Using an extensive evaluation over common TREC corpora, we demonstrate that, by redesigning NQC, where we extend it with more proper calibration and scaling, we are able to signi cantly improve its prediction accuracy.
2 BACKGROUND
2.1 Normalized Query Commitment
Let q denote a query and let D denote the respective ranked-list of top-k documents in corpus C with the highest retrieval scores. Let s(d) further denote document d's ( D) retrieval score with respect to q. The NQC method estimates the query's performance according to the standard-deviation of D's document retrieval scores,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331334

further normalized by the corpus score1 s(C), formally:

def
N QC(D|q) =

1 k

d D (s(d) - µ^D )2

s(C)

,

(1)

where µ^D

def
=

1 k

d D s(d) is D's mean document score. The

key idea behind NQC is the assumption that the mean score µ^D

may serve as a pseudo-ine ective reference (score) point of the re-

trieval. The more the retrieval scores deviate from this point, the

less chance is assumed for a query-drift within D; and hence, a

more qualitative retrieval will be predicted [10]. The corpus score

s(C) further serves as a query-sensitive normalization term, allow-

ing to compare NQC values across di erent queries [10].

2.2 Discriminative QPP

In this work, we build on top of Roitman et al.'s discriminative QPP framework [8]. In [8], the authors have shown that many of the previously suggested post-retrieval QPP methods (e.g., Clarity [2], WIG [12] and SMV [11]) share the following basic form:

W

P

M

(D

|q)

def
=

1 k

s(d) · F (d),

(2)

d D

where

F

(d )

def
=

fj (d) j is a Weighted Product Model (WPM)

j

discriminative calibrator; with fj (d) represents some retrieval qual-

ity feature and j  0 denotes its relative importance [8]. Within

this framework, F (d) calibrates each document d's ( D) retrieval score s(d) according to the likelihood of d being a relevant response

to query q [8]. To this end, F (d) may encode various retrieval quality properties, such as properties of q, C, D and the document d

itself. As Roitman et al. have pointed out, some of such properties

may be complementing each other (e.g., query vs. corpus quality

1Such a score is obtained by treating the corpus as a single (large) document.

1085

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

e ects), and therefore, tradeo s in the design of general QPP meth-
ods should be properly modeled [8]. F (d), therefore, models such tradeo s (i.e., using the weights j ).

2.3 Scaled Weighted Product Model
While most of existing QPP methods are designed to predict a given quality measure (with AP@1000 being the most commonly used measure [1]), the relationship between a given predictor's estimates and actual quality numbers may not be necessarily linear. As an example, the bottom part of Figure 1 depicts an histogram of the AP@1000 values obtained by a query-likelihood (QL) basedretrieval method, which was applied over TREC Robust topics 351400. As we can observe, there is a high variability in quality values, having query di culty non-uniformly distributed.
To address such variability during prediction, we now suggest a simple, yet e ective extension to [8], which scales the calibratedmean estimator de ned in Eq. 2, as follows:



SW

P

M

(D

|q)

def
=

1 k

s(d) · F (d) ,

(3)

d D

where   0 is a scaling parameter. Note that, whenever  < 1, higher variability in prediction values is encouraged. Going back to Figure 1, its upper part further illustrates the relationship between Clarity's [2] predicted values and actual quality numbers before and after applying scaling ( = 0.3). As we can observe, after scaling, Clarity's prediction accuracy has signi cantly improved.

3 NQC REVISITED
We now show that, the NQC method can be derived as a scaled calibrated-mean estimator. To this end, we rst rewrite NQC (de-
ned in Eq. 1), as follows:

N

QC

(D

|q)

def
=

k1 d D s(d)

12 s(C)

s(d) - µ^D s (d )

2  0.5

(4)

Using Eq. 4, it now becomes apparent that, NQC may be treated

as an instance of the generic scaled estimator de ned in Eq. 3. In

NQC's case, we have:

f1 (d )

=

1 s(C)

and 1

=

2,

f2 (d )

=

s(d )-µ^D
s(d )

and 2 = 2, and a scaling parameter  = 0.5.

Reformulating the NQC method as a scaled calibrated-mean es-

timator has two main advantages. First, being represented as a dis-

criminative QPP method, we can potentially improve its prediction

accuracy by applying supervised-learning to better tune its calibra-

tion (i.e., 1 and 2) and scaling (i.e.,  ) parameters. As we shall later demonstrate, such a more ne-tuned calibration may indeed

result in a better prediction quality.

Second, and more important, similar to [8], such a representa-

tion allows to analyze the existing design of NQC's calibration fea-

tures (i.e., f1(d) and f2(d)), potentially even redesigning some of them. Among these two features, f1(d) was previously examined in [8], where it was shown to be utilized as a calibration feature

within the SMV method [11]. We, therefore, next focus our atten-

tion on the second calibration feature of f2(d).

3.1 Analysis of f2(d)

Using the new NQC formulation, we now try to explain the mo-

tivation

behind

its

second

calibration

feature

f2 (d )

=

s(d )-µ^D
s(d )

.

To

this end, we "break" f2(d) into two main parts. The rst, given

by s(d) - µ^D , measures the di erence between a given document

d's ( D) score and that of the pseudo-ine ective reference (score)

point, considered by NQC as the mean document score µ^D . The larger the di erence is, the more will document d's own score be

estimated as an informative evidence for a true view of its rele-

vance (i.e., s(d) > µ^D ) or irrelevance (i.e., s(d) < µ^D ). The second part, given by s(d), basically serves as a scaler, allowing to mea-

sure score di erences with respect to some comparable scale. In

NQC's case, scaling is simply performed using the document score

itself.

3.2 Redesigning f2(d)

We now generalize the basic form of f2(d), and reformulate it as

follows:

f2 (d )

def
=

s(d) - sne

,

(5)

Zd

where sne represents some pseudo-ine ective reference (score)
point for comparison, and Zd denotes some scaler value. Therefore, for the original NQC method we have the con guration of sne =
µ^D and Zd = s(d). We next suggest several other con gurations, based on alterna-

tive sne and Zd instantiations. Starting with Zd , as a rst alterna-

tive, we suggest smax

def
=

maxd D s(d). As a second alternative,

motivated by Ozdemiray et al.'s ScoreRatio QPP method [6], we

also suggest smin

def
=

mind D s(d). As a third alternative, we sug-

gest a range-sensitive scaler: smax

- smin + , where 

def
=

10-10

is used to avoid dividing by zero. Noting that both smax and smin

may actually represent outlier values, as a fourth and nal alterna-

tive, which tries to reduce such outliers, we suggest the following

inter-percentile range scaler Q.95 - Q.05 + . Here, Q.x represents the value of the x% percentile of D's observed score distribution.

Next, we suggest several alternative estimates of sne . As a rst alternative, we suggest smin , which represents the document with the lowest relevance score, and hence, presumably the least e ec-

tive sample reference. As a second alternative, following [4] we

consider sne to be the mean score of the documents at the lower

ranks of D. Formally, for a given l  k, we estimate µ^Dne

def
=

1 k -l

k i=l +1

s(di ),

where di

denotes

the

document

that

is

ranked

at

position i in D.

Our third and nal alternative is motivated by Diaz's Autocorrelation QPP method [3]. Using the opposite of the logic2 that

was suggested in [3], which is based on the Cluster-Hypothesis in

IR [5], we expect an informative document score s(d) to be quite

di erent from those scores of documents that are the most dissimilar to d. Formally, let dist(d, d ) denote the distance between two

documents in D and let KF N (d) be the set of K-farthest neighbors

2The Autocorrelation method treats similar documents as pseudo-e ective reference points, and hence, the score of a document is actually supposed to be similar to the scores of its closest neighbors.

1086

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

of d in D according to dist(d, d ). We then de ne sne as the mean score of documents in KF N (d).

4 EVALUATION
4.1 Experimental setup
4.1.1 Datasets. For our evaluation we have used the following TREC corpora and topics: TREC4 (201-250), TREC5 (251-300), AP (51-150), WSJ (151-200), ROBUST (301-450, 601-700), WT10g (451-550) and GOV2 (701-850). These datasets were used by many previous QPP works [1­3, 7­12]. Titles of TREC topics were used as queries, except for TREC4, where we used topic descriptions instead [8]. We used the Apache Lucene3 open source search library for indexing and searching documents. Documents and queries were processed using Lucene's English text analysis (i.e., tokenization, lowercasing, Porter stemming and stopwords). For retrieval, we used Lucene's language-model-based cross-entropy scoring with Dirichlet smoothed document language models, where the smoothing parameter was set to 1000 [8, 10].

4.1.2 Baseline predictors. We compared the original NQC method

(see again Eq. 1) and its proposed extensions to several di erent

baseline QPP methods. As a rst line of baselines, we compared

with the following state-of-the-art post-retrieval QPP methods:

· Clarity [2]: estimates query performance relatively to the di-

vergence between the relevance model induced from a given (re-

trieved) list and the corpus background model.

· WIG [12]: estimates query performance according to the di er-

ence between the average document retrieval score in a given list

and that of the corpus s(C).

· WEG [4]: a WIG-like alternative that uses µ^Dne as the pseudoine ective reference point instead of s(C).

· Autocorrelation [3]: based on the Cluster-Hypothesis in IR, it

estimates query performance according to the Pearson's-r corre-

lation between D's original document scores and those estimated

by interpolating the scores of each document d's ( D) K-nearest

neighbors relatively to that document's similarity with each neigh-

bor.

· ScoreRatio [6]: simply estimates query performance according

to

the

ratio

smax smin

.

· SMV [11]: is a direct alternative to NQC that also considers score

magnitude and variance, estimated as:

SMV

(D |q)

def
=

1 k · s(C)

s (d )

d D

ln

s (d ) µ^D

.

Using our derivation of NQC as a scaled calibrated QPP method
(see Section 3), we further evaluated various alternatives of NQC,
as follows. We rst evaluated various f2(d) con gurations within NQC, i.e., NQC(sne ,Zd ). To this end, we instantiated sne and Zd according to our proposed alternatives.
Next, using the original NQC con guration (i.e., sne = µ^D and Zd = s(d)), we also evaluated a calibration-only version of NQC (CNQC), where we only tuned its 1 and 2 parameters, while still
xing the scaling parameter to  = 0.5. In a similar manner, we
evaluated a scaled-only version of NQC (S-NQC), where we xed
1 = 2 and 2 = 2 and only tuned the scaling parameter  . We

3http://lucene.apache.org

further evaluated a combined predictor (SC-NQC), where all the three parameters were tuned. Finally, we evaluated a pre-tuned SCNQC employed with the best f2(d) con guration learned for the non-calibrated/non-scaled NQC versions, denoted SC-NQC(best).
4.1.3 Setup. On each setting, we predicted the performance of each query with respect to its top-1000 retrieved documents [1]. We assessed prediction over queries quality according to the correlation between the predictor's values and the actual average precision (AP@1000) values calculated using TREC's relevance judgments [1]. To this end, we report the Pearson's-r (P-r ) and Kendall's (K- ) correlations, which are the most commonly used measures [1].
Most of the methods that we have evaluated required to tune some free parameters. First, following the common practice [1], for each QPP method, we tuned k ­ the number of documents used for prediction4; with k  {5, 10, 20, 50, 100, 150, 200, 500, 1000}. For Clarity, we induced a relevance model using the top-m ( {5, 10, 20, 50, 100}) documents in D and further applied clipping at the top-n terms cuto , with n  {10, 20, 50, 100}. For the Autocorrelation and NQC(KF N (K), ·) baselines, we tuned K  {3, 5, 10, 20, 30}, further using the Bhattacharyya distance between the unsmoothed language models of the documents in D as the (dis)similarity measure dist(·) of choice. To realize µ^Dne in SMV and NQC, we further tuned l  {5, 10, 20, 50, 100, 200, 500}.
To learn the calibration feature weights (i.e., 1 and 2) and scaling parameter (i.e.,  ) of the NQC variants, following [7­9], we used a Coordinate Ascent approach. To this end, we selected the feature weights over the grid of [0, 5] × [0, 5] × [0, 1], in steps of 0.1 per dimension. Following [7­10], we trained and tested all methods using a holdout (2-fold cross validation) approach. On each dataset, we generated 30 random splits of the query set; each split had two folds. We used the rst fold as the (query) train set. We kept the second fold for testing. We recorded the average prediction quality over the 30 splits. Finally, we measured statistical signi cant di erences of prediction quality using a two-tailed paired t-test with p < 0.05 computed over all 30 splits.
4.2 Results
We report our evaluation results in Table 1. As a "stand-alone" QPP method, even the original NQC method (i.e., NQC(µ^D ,s(d))) already provides highly competitive prediction quality results compared to the other state-of-the-art QPP methods. We, therefore, next evaluate the impact of our proposed NQC extensions.
4.2.1 Evaluation of various f2(d) configurations. We start with a qualitative examination of the relative contribution of each of the two parts of f2(d) (i.e., sne and Zd ). To this end, we count the relative number of cases, per dataset and quality measure (i.e., P-r or K- ), in which using a speci c con guration part has resulted in a better prediction accuracy than that of the original NQC's con-
guration. Overall, in 136 out of the 14 × 19 = 266 possible cases, utilizing one of the alternative con gurations has resulted in a better prediction.
Among the three alternative sne options, the relative preference was: KF N (d) (45/70), µ^Dne (38/70) and smin (29/70). This
4All NQC variants were tuned with the same value of k .

1087

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Evaluation results. "greenish" and "reddish" colored values represent an improvement or a decline in prediction accuracy compared to the original NQC method. A statistical signi cant di erence between a given NQC variant and the original NQC are marked with  (p < .05).

Autocorrelation
ScoreRatio
Clarity
WIG
WEG
SMV
NQC(µ^D ,s (d )) (Shtok et al. [10]) NQC(µ^D ,max) NQC(µ^D ,min) NQC(µ^D ,max - min) NQC(µ^D ,Q .95 - Q .05) NQC(µ^Dne ,s (d )) NQC(µ^Dne ,max) NQC(µ^Dne ,min) NQC(µ^Dne ,max - min) NQC(µ^Dne ,Q .95 - Q .05) NQC(min,s(d )) NQC(min,max) NQC(min,min) NQC(min,max - min) NQC(min,Q .95 - Q .05) NQC(K F N (d ),s(d )) NQC(K F N (d ),max) NQC(K F N (d ),min) NQC(K F N (d ),max - min) NQC(K F N (d ),Q .95-Q .05)
C-NQC(µ^D ,s(d )) S-NQC(µ^D ,s(d )) SC-NQC(µ^D ,s (d )) SC-NQC(best)

TREC4 P-r K- .456 .366 .420 .444 .401 .357 .533 .502 .349 .454 .458 .386
.491 .383
.484 .371 .486 .403 .436* .325* .451* .335* .482 .393
.474* .381
.483 .412*
.415* .320*
.443* .344*
.466* .393 .459* .374 .484 .425* .380* .311* .407* .338* .475* .422* .470* .418* .488 .434* .415* .349 .448* .371
.492 .412* .496 .383 .504* .412* .510* .457*

TREC5 P-r K-

.188 .136 .344 .204 .314 .208 .347 .252 .296 .201 .414 .332

.454 .275

.455 .468 .417* .406* .503*

.277 .265 .301* .297* .304*

.495* .314

.518* .306*

.427* .299*

.438* .336*

.506* .498* .522* .430* .442* .524* .519* .530* .474* .479*

.312* .312* .317* .304* .330* .297* .306* .286* .317* .330*

.455 .473* .473* .560*

.310* .275 .310* .317*

AP P-r K-

.348 .226 .231 .166 .413 .265 .613 .417 .499 .357 .536 .379

.619 .405

.628 .592* .645* .643* .625*

.413 .390* .414* .386* .423*

.632* .422*

.615 .407

.635* .409

.678* .406

.602* .617 .591* .630* .674* .638* .643* .635* .646* .688*

.423* .430* .424* .412* .412* .445* .449* .428* .452* .453*

.622* .620 .623* .691*

.412* .405 .412* .454*

WSJ P-r K-

.586 .496 .620 .411 .500 .355 .685 .463 .696 .482 .713 .484

.722 .510

.723 .689* .706* .715 .729*

.494 .486* .460* .496 .504

.734* .513

.705* .499*

.734* .496*

.736* .502*

.715 .723 .714 .706* .715 .730* .739* .691* .760* .744*

.496 .494 .500 .460* .496 .561* .561* .523* .558* .582*

.737* .728* .737* .770*

.510 .502* .510 .567*

ROBUST P-r K-

.385 .321 .405 .370 .404 .329 .560 .399 .562 .400 .534 .391

.580 .422

.582 .540 .585* .440* .567*

.424 .425 .405* .409* .412*

.568* .409*

.541* .420*

.551* .377*

.465* .394*

.538* .393* .536* .388* .529* .408* .493* .329* .457* .353* .580 .413* .583 .414 .556* .422 .572* .394* .504* .403*

.580 .580 .580 .585*

.412 .422 .422 .405*

WT10g P-r K- .299 .198 .226 .275 .289 .203 .221 .323 .462 .383 .466 .279
.522 .330
.518 .324 .538* .337* .407* .237* .384* .274* .523 .333
.516 .325
.539* .334*
.399* .233*
.383* .273*
.406* .277* .391* .265* .429* .291* .245* .176* .255* .205* .506* .333* .500* .324* .528* .351* .385* .253* .380* .272*
.559* .377* .522 .330 .559* .377* .561* .380*

GOV2 P-r K-

.247 .198 .295 .246 .206 .164 .498 .352 .419 .337 .419 .310

.378 .253

.386* .350* .460* .421* .416*

.256 .240* .320* .294* .292*

.427* .300*

.384* .280*

.507* .355*

.504* .363*

.445* .456* .412* .528* .532* .432* .443* .400* .515* .509*

.306* .318* .296* .358* .367* .327* .327* .310* .374* .373*

.507* .398* .527* .566*

.360* .253 .360* .401*

demonstrates that, a better choice of a pseudo-ine ective reference point within NQC should be one that is more sensitive to the document in mind d (i.e., KF N (d)). This in comparison to a point that is more generally estimated (i.e., µ^Dne and smin). Moreover, among the latter two alternatives, considering more than one sample from the lower ranks of D is a better choice. In a similar manner, among the four alternative Zd options, the relative preference was: smax (34/56), smin (29/56), smax - smin (25/56) and lastly Q.95 - Q.05 (23/56). This further demonstrates that, a better choice for a scaler is one that depends on a single point (having smax a better choice than smin), rather than a range of values. Finally, quantitatively, by more proper con guration of f2(d), NQC's prediction accuracy has improved in all datasets, up to 38% and 46% improvement in P-r and K- , respectively. This demonstrates the usefulness of analyzing NQC using our scaled extension to Roitman et al.'s discriminative QPP framework [8].
4.2.2 E ect of calibration and scaling. The four bottom rows in Table 1 further report the e ect of NQC's calibration and scaling. First, we observe that, by better tuning of NQC's calibration features within C-NQC(µ^D ,s(d)), its prediction accuracy has improved in most cases (up to 42% better). Next, scaling (i.e., S-NQC(µ^D ,s(d))) by itself also improves NQC's accuracy (up to 5%). Combining both calibration and scaling (i.e., SC-NQC(µ^D ,s(d))), has resulted in most cases in a further improved accuracy (up to 42%). Finally, further using the best f2(d) con guration together with calibration and scaling (i.e., SC-NQC(best)) provides the best prediction strategy for NQC (up to 60% improvement).

5 CONCLUSIONS
We introduced a simple, yet highly e ective, extension to Roitman
et al.'s discriminative QPP framework [8]. Our main focus was on
the NQC method, where using our proposed extension, we were
able to redesign it and suggest several options for improvement.
REFERENCES
[1] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of SIGIR '12.
[2] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query performance. In Proceedings of SIGIR '02.
[3] Fernando Diaz. Performance prediction using spatial autocorrelation. In Proceedings of SIGIR '07.
[4] Ahmad Khwileh, Andy Way, and Gareth J. F. Jones. Improving the reliability of query expansion for user-generated speech retrieval using query performance prediction. In CLEF, 2017.
[5] Oren Kurland. The cluster hypothesis in information retrieval. In Advances in Information Retrieval, pages 823­826. Springer International Publishing, 2014.
[6] A. M. Ozdemiray and Ismail S. Altingovde. Query performance prediction for aspect weighting in search result diversi cation. Proceedings of CIKM '14.
[7] Haggai Roitman. An extended query performance prediction framework utilizing passage-level information. In Proceedings of ICTIR, pages 35­42, New York, NY, USA, 2018. ACM.
[8] Haggai Roitman, Shai Erera, Oren Sar-Shalom, and Bar Weiner. Enhanced mean retrieval score estimation for query performance prediction. In Proceedings of ICTIR '17.
[9] Haggai Roitman, Shai Erera, and Bar Weiner. Robust standard deviation estimation for query performance prediction. In Proceedings of ICTIR '17.
[10] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst., 30(2):11:1­11:35, May 2012.
[11] Yongquan Tao and Shengli Wu. Query performance prediction by considering score magnitude and variance together. In Proceedings of CIKM '14.
[12] Yun Zhou and W. Bruce Croft. Query performance prediction in web search environments. In Proceedings of SIGIR '07.

1088


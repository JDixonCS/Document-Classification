Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Training Streaming Factorization Machines with Alternating Least Squares

Xueyu Mao
xmao@cs.utexas.edu The University of Texas at Austin
Austin, TX

Saayan Mitra
smitra@adobe.com Adobe Research San Jose, CA

Sheng Li
sheng.li@uga.edu University of Georgia
Athens, GA

ABSTRACT
Factorization Machines (FM) have been widely applied in industrial applications for recommendations. Traditionally FM models are trained in batch mode, which entails training the model with large datasets every few hours or days. Such training procedure cannot capture the trends evolving in real time with large volume of streaming data. In this paper, we propose an online training scheme for FM with the alternating least squares (ALS) technique, which has comparable performance with existing batch training algorithms. We incorporate an online update mechanism to the model parameters at the cost of storing a small cache. The mechanism also stabilizes the training error more than a traditional online training technique like stochastic gradient descent (SGD) as data points come in, which is crucial for real-time applications. Experiments on large scale datasets validate the efficiency and robustness of our method.
CCS CONCEPTS
· Computing methodologies  Factorization methods.
KEYWORDS
Factorization Machines, Online Learning, Alternating Least Squares
ACM Reference Format: Xueyu Mao, Saayan Mitra, and Sheng Li. 2019. Training Streaming Factorization Machines with Alternating Least Squares. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331374
1 INTRODUCTION
Context aware recommender systems [13] are increasingly employed to provide personalized recommendations to users or to predict the reactions of users to a given subject or situation. Many such recommender systems employ factorization machines (FM) [10] which combines the advantages of both a regression model and matrix factorization. In recent years, many variants of FMs
This work was done while the authors were at Adobe Research.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331374

have been proposed. Field-aware FM leverages the field information associated with each feature, and it usually outperforms the original FM in many real-world evaluations [6]. Robust FM utilizes the robust optimization framework, and achieves stable performance under interval uncertainty [9]. FMs are typically trained in batch mode, where the prediction model is trained on large datasets every few hours or days. In these systems, the models can become stale and fail to capture trends as they are evolving in real-time. This can be a particular problem in use cases such as sports or news, where it is important to react quickly to shifting trends to make relevant recommendations based on the current viewing patterns observed over large number of users. Improving the system performance of real-time stream recommendations has become an active research topic from the database perspective, but mainly focuses on the item-based collaborative filtering [5]. Another disadvantage of batch mode training is the significant processing time and storage required to deal with large scale datasets. To this end, it would be desirable to have a system that is capable of training and updating the FM in an efficient, incremental, and continuous fashion, based on each newly received data point from a stream of data, without the need to perform extensive re-calculations based on large volumes of previously received historical data. Such a system can enable the FM to capture dynamic information from streaming data sources in a real-time fashion by performing a single processing pass on each data point as soon as it arrives.
Existing FM training methods include: 1) stochastic gradient descent [11], which can be easily implemented in an online setting, however, the learning rate needs to be tuned and may need multiple epochs to converge; 2) tuning-free coordinate descent, which includes (a) alternating least squares (ALS) [12], (b) efficient training for FM using ANOVA kernel [1] which needs cache scaling with number of data points in the dataset; 3) MCMC [11], which is usually time consuming and it is usually hard to pick a consistently good model as it uses ensembles for prediction. [7] proposed a provable one-pass algorithm for generalized FMs and rank-one matrix sensing using alternating minimization, however, it has strong assumptions that the data are sampled from random Gaussian vectors and is computationally expensive. In [2], the authors propose a fast learning method when FM is used for binary classification, and the objective function is modified to make it multi-block convex. Recently, some methods that combine the merits of deep learning and FM have been proposed, such as DeepFM [3], neural factorization machines [4] and attentional factorization machines [15]. Like many other deep learning approaches, these FM based deep learning methods mainly use gradient descent for model training. In this work, we propose an algorithm based on ALS for fast online update of FM model with streaming data. We give the detailed

1185

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

derivation of the algorithm and validate on large scale simulated and real industrial data.

2 PRELIMINARIES

2.1 Model
We focus on degree-2 FM in this work, which was proposed by Rendle [10]. The model equation is given by:

d

dd

y^(x) = w0 + wi xi +

vi , vj xi xj ,

i =1

i=1 j=i+1

where, x  Rd is a feature vector representation of a data point. w0  R, w  Rd , V = [v1, · · · , vd ]T  Rd×k are model parameters, d is the dimension of the input feature vector, and k  Z+ is the
rank of the factorization. vi , vj  is the inner product of two latent factors, which models the interaction between the i-th and j-th
variable.

2.2 ALS update for FM

As pointed in [12], the objective function of the FM model is linear

in each parameter (w0, wi , Vij ) given the others are fixed. Let   

be a parameter, then the FM model can be written as: y^(x| ) =

( ) (x) + h( ) (x). For example, if  = w0, we have h(w0) (x) = 1

and (w0) (x) =

d i =1

wi

xi

+

d i =1

d j =i

+1vi

,

vj

xi

xj

.

When

we

use

square loss L = (x,y) S (y^(x)-y)2 +  ( ) 2, this linear function

has a closed form optimal solution:

 =-

(x,y) S (( ) (x) - y)h( ) (x) (x,y) S h2( ) (x) + ( )

,

(1)

where S is the set of all the data points and ( ) is the regularization parameter for  . [12] proposed an alternating least squares based
algorithm for FM training using Equation (1), which forms the basis
for our method.

3 ONLINE ALS UPDATE FOR FM
We consider a streaming setting where data points come one by
one: (x|t1 ,y|t1 ), (x|t2 ,y|t2 ), · · · . Let the model parameters at time ti be w0|ti , w|ti , V|ti . Our premise is that the data points can only be used once for training, and then thrown away. We use S |ti to denote all the data points that has been seen at time ti .
In an online setting, for normal ALS training, in each epoch, at
step i (time ti ), it is ideal to update the model using all the data points that have been seen:

 |ti = -

(x,y) S |ti (( ) (x) - y)h( ) (x) (x,y) S |ti h2( ) (x) + ( )

=-

(x,y

)

S

|ti (e (x,
(x,y ) S

y)
|ti

-  |ti h h2( ) (x)

( )
+

(x)  (

)h
)

(

)

(x)

,

(2)

where e (x,y) = y^(x) - y, which is the error term for the prediction of data point x. Note that at step ti-1, we also have:

 |ti-1 = -

(x,y

)

S

|ti-1 (e (x,y)
(x,y ) S |ti-1

-  |ti-1h( h2( ) (x) +

) (x) ( )

)h

(

)

(x)

.

Also, it is easy to show that
(e (x,y) -  |ti h( ) (x))h( ) (x)
(x,y ) S |ti

= - |ti-1

h2( ) (x) + ( ) + e (x|ti ,y |ti )h( ) (x|ti )
(x,y ) S |ti

+

( |ti-1h2( ) (x) -  |ti h2( ) (x)).

(3)

(x,y ) S |ti

Now, combining Equations (2) and (3), we have,

 |ti = |ti-1 -

e (x|ti ,y |ti )h( ) (x|ti ) (x,y) S |ti h2( ) (x) + ( )

-

(x,y )

S

|ti

(

|ti

-1

h

2 (

)

(x)

-



|ti

h2(

)

(x))

(x,y) S |ti h2( ) (x) + ( )

 |ti-1 -

e (x|ti ,y |ti )h( ) (x|ti ) (x,y) S |ti h2( ) (x) + (

)

.

(4)

Equation (4) gives the update rule for any parameter  at time ti
given a new data point (x|ti ,y|ti ). The updates rules for specific parameters of FM models can then be derived as following:

w0 |ti

=w0 |ti-1

-

e (x|ti ,y|ti ) , S |ti + ( )

(5)

wl |ti =wl |ti-1 -

e (x|ti
(x,y ) S

,y
|ti

|ti )xl xl2 +

|ti  (wl

)

,

(6)

Vl f |ti =Vl f |ti-1 -

e (x|ti
(x,y ) S

,y
|ti

|ti )h(Vl f ) (x|ti ) h2(Vl f ) (x) + (Vl

f

)

,

(7)

where |S |ti | denotes the number of data points received at time ti , xl is the l-th element of vector x, and
n
h(Vlf ) (x) = xl Vl f xl - xl2Vl f = xl q(x, f ) - xl2Vl f , (8)
l =1

with
n

q(x, f ) = Vl f xl

(9)

i =l

as shown in [12]. We can see that, in order to update efficiently, we

need to store some cache values. For linear term updates, l  [n], we need al = (x,y) S xl2; for interaction terms, l  [n], f  [k], we need Bl f = (x,y) S h2(Vlf ) (x). With these update rules, our
online FM training algorithm is summarized in Algorithm 1. Note

that there is no learning rate required for this algorithm.

It is easy to see that the memory cost for the cache is (k + 1)n,

which is independent of the number of data points. Note that as x

is typically sparse in real data, we can always check if xl is 0 and skip the iteration if it is so. Hence, the computation cost for each

while loop is O (km), where m is the number of non-zero elements

in x. For batch ALS, as the whole training set will be used for

updating the parameters, the time complexity is O (kmS ), where

mS is the number of non-zero elements in the whole training set.

For large scale datasets, mS is much larger than m, so our algorithm

yields fast implementation and saves a huge computation overhead

compared to batch training.

1186

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Algorithm 1 Online-ALS for FM

Require: Data stream (x|ti ,y|ti ), i = 1, 2, · · ·

Ensure: FM model parameters w0, w, V; cache a, B

1: Initialize w0 |t0 , w|t0 , V|t0 from Gaussian samples;

2: Initialize cache al and Blf to 0, l  [n], f  [k]

3: while data point (x|ti ,y|ti ) arrives do

4: e (x|ti ,y|ti ) = y^(x|ti ) - y|ti

5: Update w0 using Equation (5)

6: e (x|ti ,y |ti ) = e (x|ti ,y |ti ) + (w0 |ti - w0 |ti-1 )

7: for l  [n] do

8:

al = al + xl2 |ti

9:

Update wl using Equation (6)

10:

e (x|ti ,y |ti ) = e (x|ti ,y |ti ) + (wl |ti - wl |ti-1 )xl |ti

11: end for

12: for f  [k] do

13:

Calculate q(x|ti , f ) using Equation (9)

14:

for i  [n] do

15:

Calculate h(Vlf ) (x|ti ) using Equation (8)

16:

Bl f

= Blf

+

h

2 (Vl

f

)

(x|ti

)

17:

Update Vlf using Equation (7)

18:

e (x|ti ,y |ti ) = e (x|ti ,y |ti ) + (Vl f |ti - Vl f |ti-1 )xl |ti

19:

q(x|ti , f ) = q(x|ti , f ) + (Vl f |ti - Vl f |ti-1 )xl |ti

20:

end for

21: end for

22: end while

It is notable that although Algorithm 1 starts with random initialization, if we have a pre-trained FM model, we can skip step 1 in the algorithm (or even step 2 if the cache values have been saved). This is useful in a real world setting because one can always use batch training on a small set of historical data to get a reasonably good model, and then update the model in a streaming setting to capture the real-time trends, and get a good accuracy without re-training using the large dataset. Similarly, a rolling bootstrap dataset from recent past can be used to initialize from time to time.

4 EXPERIMENTS
In this section, we will present our experimental evaluation of Algorithm 1. Besides Online-ALS, we also consider Online-ALSpretrained, which uses 10% of data to pre-train the model in batch fashion, and then do online updating as in Algorithm 1. By default we use ALS for pre-training. We compare Online-ALS, Online-ALSpretrained and SGD with step size 0.01, which performs consistently well across datasets we have used.
Our evaluation metric is root-mean-square error (RMSE), suppose there are N data points for evaluation, then it is defined as

RMSE =

1 N

N
(y^(x|ti ) - y|ti ))2
i =1

We also use two kind of training error to evaluate the model:

· True-training Error: RMSE of training error without reload-
ing the model, i.e., prediction for data point x|ti will not be adjusted after ti .

· Training Error: RMSE for the model at time tN on the entire training set, i.e., use the final model to recalculate the prediction for each data point
Note that True-training Error is more important for a streaming setting, as after updating the model we will dispose of the data point and there is no way to reuse it. It also measures the quality of recommendations in a real-time system.
4.1 Simulations
We first evaluate our method with simulated data. We generate the ground-truth FM model with random samples drawn from a Gaussian, and then simulate data from this FM, with perfect label. The size of the simulated dataset is 1.5M for training, and 171K for testing. The minimum target value is -0.986732, and the maximum is 1.25055. The result is shown in Fig. 1, we can see that SGD has faster convergence on test error, but Online-ALS-pretrained is much better because of the pre-trained procedure. In terms of training errors, Online-ALS converges faster than SGD. And we can see, pre-training is beneficial for both training and true training error.

RMSE RMSE RMSE

Test Error
0.28

0.26

Online-ALS

0.24

Online-ALS-pretrained

SGD
0.22

0.2

0.18

0.16

0.14

0.12

0.1

0.08 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Training Error
0.3

Online-ALS

0.25

Online-ALS-pretrained

SGD

0.2

0.15

0.1

0.05

0

0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

True Training Error
0.28

0.26

Online-ALS

0.24

Online-ALS-pretrained

0.22

0.2

0.18

0.16

0.14

0.12

0.1

0.08 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Figure 1: Simulated Data

4.2 Real data
In this section, we present the results on large scale industrial datasets. We use the feature selection framework in [8] to select about 10 representative features from hundreds of them to accelerate the training procedure. The target (y) of the dataset is session progress introduced in [14], which is a real valued number in [0, 1]. Session progress is an implicit measure of engagement of the user with the video, which measures how much of the video was watched. We also use RMSE to measure the accuracy of the prediction.

Table 1: Statistics of datasets.

Dataset 1 2 3
2-subset

Training size 61K 1.65M 154K 36K

Test size 6.8K 61K 17K 4K

Feature Selection Yes Yes Yes No

We use real industrial datasets which are time-ordered for both training and test. Unlike the simulation experiments, the data points can be dependent with each other and the underlying model may not necessarily be a perfect FM model. The statistics of datasets we are using are shown in Table 1 but we omit the names of the datasets to preserve confidentiality. 2-subset is a subset of dataset 2, but without applying any feature selection technique.

1187

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

RMSE RMSE RMSE

RMSE RMSE RMSE

Test Error
0.85

0.8

Online-ALS

0.75

Online-ALS-pretrained

SGD
0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Training Error
0.9

Online-ALS

0.8

Online-ALS-pretrained

SGD
0.7

0.6

0.5

0.4

0.3

0.2 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Figure 2: Dataset 1

True Training Error
0.85

0.8

Online-ALS

Online-ALS-pretrained

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Test Error
0.6

Online-ALS

0.55

Online-ALS-pretrained

SGD

0.5

0.45

0.4

0.35

0.3 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Training Error
0.65

0.6

Online-ALS

Online-ALS-pretrained

0.55

SGD

0.5

0.45

0.4

0.35

0.3

0.25

0.2 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Figure 3: Dataset 2

True Training Error
0.65

Online-ALS

0.6

Online-ALS-pretrained

0.55

0.5

0.45

0.4

0.35

0.3 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Test Error
90

85

Online-ALS

80

Online-ALS-pretrained

75

SGD

70

Online-ALS-pretrained-w-MCMC

65

60

55

50

45

40

0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Training Error
90

80

Online-ALS

Online-ALS-pretrained

70

SGD

Online-ALS-pretrained-w-MCMC

60

50

40

30

20

0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Figure 4: Dataset 3

True Training Error
85

80

Online-ALS

Online-ALS-pretrained
75
Online-ALS-pretrained-w-MCMC
70

65

60

55

50

45

40

35

0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Test Error

0.7
Online-ALS

0.65

Online-ALS-pretrained

SGD

0.6

0.55

0.5

0.45

0.4

0.35 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Training Error
0.7

0.6

0.5

0.4

0.3

0.2
Online-ALS 0.1 Online-ALS-pretrained
SGD

0

0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

True Training Error
0.6
Online-ALS Online-ALS-pretrained
0.55

0.5

0.45

0.4

0.35 0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

Figure 5: Dataset 2-subset

RMSE RMSE RMSE

RMSE RMSE RMSE

Figs 2, 3, 4 and 5 show the results. We can see that for real datasets, Online-ALS is much more stable than SGD, both for training and test errors. Pre-training is not as advantageous as it is on the simulation data, maybe because the first 10% of training data is not representative and gives a bad initialization, such as Dataset 3 in Fig 4. However, pre-training with MCMC [11] gives a better result. In a real application, we can evaluate which batch training is better for a specific dataset and pre-train to get a better initialization. Also in Fig 5, it is very interesting to see that Online-ALS is more stable when using more context features, where SGD fluctuates a lot. It is also notable to mention that for batch training (ALS, MCMC, SGD) with real industrial data, training with multiple passes of the data does not perform much better than training with one pass of the data (batch or online), if proper features are selected. This is shown in Table 2. Hence, online training in a streaming setting can provide faster results without compromising accuracy in real world applications. We can also see from Table 2 and Figs 2, 3, 4 and 5 that test error of batch training for ALS (one or multiple passes of the training data) is similar or larger than that of Online-ALS with only one pass of the data. This implies that in real world application, Online-ALS outperforms the batch ALS in both speed and accuracy.

Table 2: Test errors for training with multiple passes of the data

Dataset 1 2 3
2-subset

Iteration
1 80
1 80
1 80
1 80

Batch ALS
0.399962 0.409616
0.353463 0.371093
47.6281 54.5406
0.375003 0.370772

MCMC
0.415421 0.382334
0.358274 0.312613
45.9093 43.1982
0.517774 0.344105

SGD
0.407748 0.401594
0.344176 0.316464
42.6463 43.1189
0.549916 0.470411

5 CONCLUSIONS
In this paper, we introduced an algorithm for online FM model
training with streaming data. It has comparable results with batch
training and has much more stable training error than SGD. We val-
idated the efficiency and robustness of our algorithm with extensive
experiments on simulated and industrial datasets.
REFERENCES
[1] Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, and Naonori Ueda. 2016. Polynomial networks and factorization machines: New insights and efficient training algorithms. In 33rd International Conference on International Conference on Machine Learning (ICML), Vol. 48. 850­858.
[2] Wei-Sheng Chin, Bo-Wen Yuan, Meng-Yuan Yang, and Chih-Jen Lin. 2018. An efficient alternating newton method for learning factorization machines. ACM Transactions on Intelligent Systems and Technology (TIST) 9, 6 (2018), 72.
[3] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. AAAI Press, 1725­1731.
[4] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 355­364.
[5] Yanxiang Huang, Bin Cui, Wenyu Zhang, Jie Jiang, and Ying Xu. 2015. Tencentrec: Real-time stream recommendation in practice. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. ACM, 227­238.
[6] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Fieldaware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 43­50.
[7] Ming Lin and Jieping Ye. 2016. A non-convex one-pass framework for generalized factorization machine and rank-one matrix sensing. In Advances in Neural Information Processing Systems. 1633­1641.
[8] Xueyu Mao, Saayan Mitra, and Viswanathan Swaminathan. 2017. Feature Selection for FM-Based Context-Aware Recommendation Systems. In 2017 IEEE International Symposium on Multimedia (ISM). 252­255.
[9] Surabhi Punjabi and Priyanka Bhatt. 2018. Robust Factorization Machines for User Response Prediction. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. 669­678.
[10] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International Conference on Data Mining. IEEE, 995­1000.
[11] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on Intelligent Systems and Technology (TIST) 3, 3 (2012), 57.
[12] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 635­644.
[13] Paul Resnick and Hal R Varian. 1997. Recommender systems. Commun. ACM 40, 3 (1997), 56­58.
[14] Gang Wu, Viswanathan Swaminathan, Saayan Mitra, and Ratnesh Kumar. 2014. Online video session progress prediction using low-rank matrix completion. In Multimedia and Expo Workshops (ICMEW), 2014 IEEE International Conference on. IEEE, 1­6.
[15] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. AAAI Press.

1188


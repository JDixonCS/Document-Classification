Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

One-Class Order Embedding for Dependency Relation Prediction

Meng-Fen Chiang
Living Analytics Research Centre Singapore Management University
mfchiang@smu.edu.sg

Ee-Peng Lim
Living Analytics Research Centre Singapore Management University
eplim@smu.edu.sg

Wang-Chien Lee
The Pennsylvania State University University Park, PA 16802, USA wlee@cse.psu.edu

Xavier Jayaraj Siddarth ASHOK
Living Analytics Research Centre Singapore Management University
xaviera@smu.edu.sg

Philips Kokoh PRASETYO
Living Analytics Research Centre Singapore Management University
pprasetyo@smu.edu.sg

ABSTRACT
Learning the dependency relations among entities and the hierarchy formed by these relations by mapping entities into some order embedding space can effectively enable several important applications, including knowledge base completion and prerequisite relations prediction. Nevertheless, it is very challenging to learn a good order embedding due to the existence of partial ordering and missing relations in the observed data. Moreover, most application scenarios do not provide non-trivial negative dependency relation instances. We therefore propose a framework that performs dependency relation prediction by exploring both rich semantic and hierarchical structure information in the data. In particular, we propose several negative sampling strategies based on graph-specific centrality properties, which supplement the positive dependency relations with appropriate negative samples to effectively learn order embeddings. This research not only addresses the needs of automatically recovering missing dependency relations, but also unravels dependencies among entities using several real-world datasets, such as course dependency hierarchy involving course prerequisite relations, job hierarchy in organizations, and paper citation hierarchy. Extensive experiments are conducted on both synthetic and realworld datasets to demonstrate the prediction accuracy as well as to gain insights using the learned order embedding.
CCS CONCEPTS
· Computing methodologies  Machine learning; Learning to rank;
ACM Reference Format: Meng-Fen Chiang, Ee-Peng Lim, Wang-Chien Lee, Xavier Jayaraj Siddarth ASHOK, and Philips Kokoh PRASETYO. 2019. One-Class Order Embedding for Dependency Relation Prediction. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25,2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331249

(SIGIR '19), July 21­25, 2019,Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331249
1 INTRODUCTION
Motivation. Graph is a widely adopted data representation in diverse real-world scenarios. Learning representation of nodes and edges in graph structures has been well studied for many graph analytics applications [15][27][4]. In form of low-dimensional vectors, embedding representation typically preserves the degree of network proximity [23][22], as various applications, such as node classification and link prediction, requires the learned representations to capture properties of the graph structure. Nevertheless, little attention has been paid to exploring order embeddings that preserve relative order among a given set of nodes in an ordering hierarchy. Such order embeddings are expected to encode rich information of the hierarchical structure, and hence can be used to predict missing dependency links.
Indicative representations of text and partial ordering relations amongst entities, such as hierarchical structure, can capture semantic and structural information embedded among them. Entities with positive or negative ordering relations tend to be semantically related to each other, and thus are comparable. However, not all pairs of entities have an ordering relationship. Specifically, semantically unrelated pairs of entities are not comparable and thus should not have any ordering relations between them. For example, a course "machine learning" depends on several prerequisite courses, including "data structure", "theory of computation", and "artificial intelligence" as they are background to the machine learning course, as indicated in many computer science curricula. Moreover, intuitively, "probabilistic models" is semantically closer to "fundamentals of probability" than it is to "geographical information systems spatial databases". Thus, to establish the relatedness between these courses, we may explore the semantic proximity of title and textual content of these courses. Indeed, most of today's representation learning techniques aim to learn entity representations that are semantics-preserving, i.e., semantically similar entities are mapped into a nearby area in the semantic embedding space. In this paper, we argue that it is also essential for representation of entities to be order-preserving, i.e., antisymmetric relations between two entities are captured in the embedding space. There are relatively very little research on order-preserving embedding. One such order-preserving embedding technique, developed by Vilnis

205

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

artificial intelligence

machine learning

C

AD
data B structure theory of
computation

Dependency

Network

Textual Content
Observed Dependency
? Unobserved Dependency

Pre-Trained Word
Embedding
E+
E-
Negative Samples

WE Word Embeddings
OE
Order Embeddings

Dependency Classifier Negative Sampling

machine learning
A
?
D data structure
Dependent? (Predicted)

Order Embedding

Dependency

Learning

Classifier Learning

Figure 1: Overview of Proposed Framework.

and McCallum, assigns each entity a location in the embedding space such that the relative positions of entities to the origin of the representation space (i.e., zero coordinates) determine the partial ordering among them [26].
*** wlee: In addition to motivating the technical aspect of embedding techniques, I think it's important to motivate the application aspect, i.e., the needs and application of dependency relationsh.
In this paper, we explore a compositional approach to incorporate both semantics- and order-preserving embedding to predict dependency relations of entity pairs. Past research on order embeddings [24] [2] [26] have shown to be effective for word hypernym classification and textual entailment tasks where only entities themselves and their relations are represented. The textual and content features of these entities are largely neglected. To the best of our knowledge, no prior research has attempted to combine both semanticand order-preserving embeddings for prediction of dependency relations. Objectives. This research focuses on two objectives: (1) designing a new framework to accurately recover missing dependency relations, which is essential in automatic completion of knowledge bases; (2) applying the proposed framework on real-world datasets, such as course dependency hierarchy, job hierarchy, and paper citation hierarchy, using the automatically learned order embeddings to gain insights on hierarchical ordering. Both objectives are to be achieved in solving the following fundamental research problem: Problem: (Dependency Relation Prediction). Given a set of observed entity pairs with dependency relations, determine whether an unseen entity pair have a positive dependency relation or not. Overview of Our Proposed Approach. We address the dependency relation prediction problem as a classification task and propose a two-step framework as shown in Figure 1. The Step 1 of the framework learns the order embedding for entity pairs. Here, the order embedding is a low-dimensional vector that can be used to differentiate ordering relations among entities according to their positions in embedding space. Specifically, we learn the ordering hierarchy (consists of order embeddings) for the entire set of entities. However, not every entity pair is comparable. To determine ordering relations for comparable entities, the Step 2 of the framework combines order embedding and word embedding of entities. Examples of the latter include GloVe [21] and Word2Vec [13]. Next, the framework adopts a compositional approach to combine features

from word embeddings and order embeddings for a pair of entities, to train the dependency relation classifiers, which allows us to leverage on both textual content and order embedding information.
*** The paragraph above is a bit confusing to me! Do you mean to learn order embedding as a vector associated with an edge and also learning embeddings for nodes? Or you are actual;ly learning node embeddings only but those nodes should preserve ordering relation? Could you clarify it.
There are two major research challenges in the dependency relation prediction problem and the proposed framework. Firstly, ordering hierarchies are often incomplete as not all ordering relations can be observed in many real applications. Under the open world assumption [14], these non-existing relations can be either negative or unknown [15][27][4]. With only partial ordering relations observed and no knowledge of negative relations, the dependency relation prediction is therefore a one-class problem. To determine whether a dependency relation between two entities holds, we thus need good negative sampling strategies to collect diverse and highly likely negative relations between entities. Hence, the first research challenge lies in selecting a subset of "good" negative samples from a massive set of unobserved relation candidates
The second research challenge is to demonstrate the advantages of fusing both order-preserving and semantics-preserving representations of entities for dependency relation prediction. Our research seeks to look into a detailed evaluation of our proposed compositional approach in comparison with two baseline approaches: (i) One focuses on learning semantic features from textual content, and uses the semantic features to determine dependency relations; (ii) Another approach is to use order embedding as features to determine dependency relations. To the best of our knowledge, such a study has not been conducted before despite its importance to knowledge base completion. Contributions. The contributions of our work are summarized as follows:
· With vector order embedding model [24] used for learning ordering hierarchy among entities, we propose several negative sampling strategies based on graph centralities to effectively address the one-class problem, by using a training set of positive dependency relations and sufficient negative samples for order embedding learning.
· We propose the dependency relation classification framework that fuses both semantic and order embeddings of entities.
· We conduct extensive experiments to demonstrate the effectiveness of proposed framework using both synthetic ordering hierarchies and several real-world datasets.
2 RELATED WORK
Relational Learning. The goal of relational machine learning is to infer relationships between objects and answer queries [15][27][4]. Given a database of partially annotated relations, relational machine learning aims to learn rules or models to answer questions on unseen relations. This task arises in many settings such as biological pathways, analysis of question answering. A variety of techniques from Statistical relational models (SRL) community are proposed to tackle such tasks with partially annotated relations. SRL can be fundamentally divided into two categories: latent feature models and observable models. Markov Logic Nets (MRF) learns a set of

206

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

rules and weights to infer all unobserved relations jointly by maximizing probability. Embedding approaches, e.g., RESCAL, learn embeddings to predict relations. For example, the neural embedding models predict scores based on subjects, objects, and predicate embeddings[6][3][17][11][16]. Graph mining approaches on the other hand explore graph features, common ancestor or random walks, to predict relations. Ultimately, Google knowledge vault projects, automatic knowledge graph construction from the Web, fusion latent and observable models to improve the modeling power. Prerequisite Relation Learning. Recent work has studied prerequisite relations learning among of educational concept map [12][10][19][20][5][1][9]. Prerequisite chains play an important role in curriculum planning and reading list generation [8][7]. Liu et al. focus on learning prerequisite relation for relation prediction among a set of concepts in university courses [12], while Liang et al. focus on recovery of prerequisite relations [10]. Liu et al. address CGL (concept graph learning) for two-level prerequisite learning, course level and concept level. Given observed courselevel prerequisite rations, CGL.Class and CGL.Rank explicitly learn full prerequisite relations between concepts from course pair relations. Pan et al. [19] propose to automatically identify all course concepts from online MOOCs video clips [20]. Chen et al. propose a structural EM to learn an optimal Bayesian Network structure, representing the dependency relations among skills, which best explains the distribution of student performance data [5]. However, the scalability and efficiency of the learning mechanism remain unclear as the data consists of only a handful of students, examine items and skill variables. Liang et al. explore applicability of active learning to address the issue of limited training data for prerequisite classification on pairs of concepts [9]. They adopt pool-based active learning scenario to train a classifier with a substantially small dataset by incorporating four types of "valuable" unlabeled instances in particular prioritized way at each iteration. To the best of our knowledge, there is no existing work on prerequisite relation learning which tackles relation recovery via representation learning from partial orders of structural data. Order Embedding Learning. Order embeddings are shown to be effective for word hypernym classification, image-caption ranking and textual entailment [24][26][2][25]. Vendrov et al. [24] propose to learn asymmetric relationships with deterministic vector order embeddings (VOE) of non-negative coordinates with partial order structure from incomplete data. Due to its limitation of expressiveness of deterministic vector order embeddings, recent work incorporate uncertainty in learning order representation to enrich expressivness and enable prediction with uncertainty, such as probabilistic extensions of order embeddings [26][2] and box lattice representation of order embeddings [25]. Athiwaratkun et al. introduce density order embedding (DOE) to model hierarchical data via encapsulation of probability densities [2]. In particular, they propose a new loss function, graph-based negative sample selections, and a penalty relaxation to induce soft partial orders.
3 ORDER EMBEDDING LEARNING AND NEGATIVE SAMPLING STRATEGIES
3.1 Vector-Based Order Embedding
There are a few order embedding methods proposed in the literature [2, 24, 26]. They all seek to assign a geometric representation

to each entity such that the partially ordered entities are mapped into a latent space where certain geometric relationship between them are preserved. In this paper, we follow [24] to formalize the notion of partially ordered entities as follows.

Definition 3.1. (Partially Ordered entities) A partially ordered
set of entities V has a binary order relation  such that for vi , vj , vk  V , the following properties hold: (1) vi  vi (reflexivity), (2) if vi  vj and vi vj , then vj  vi (antisymmetry), and (3) if vi  vj and vj  vk , then vi  vk (transitivity).

Motivated by Vendrov and others [24], we want to learn a mapping f from V into a Vector-based Order Embedding Space Y where each entity is represented as a geometric point and the ordering relation between two entities is represented as a geometric relationship between the points representing the two entities. For entities with unknown order relations, we can then use their point representations in the embedding space Y to infer their order relations.
A crucial property of the mapping function f , preserving the order relations in the vector-based order embedding space, is defined as follows:

Definition 3.2. (Order preserving) f : (V , )  (Y , ) is orderpreserving if vi , vj  V , vi  vj if and only if f (vi )  f (vj ).

Objective Function. To learn a good order-preserving mapping
f , Vendrov et al. define the geometric relation between two entities
vi and vj in the embedding space based on the conjunction of total order on each dimension d of embedding space Y . That is,
f (vi )  f (vj ) if and only if vi,d > vj,d , 1  d  NY where NY is the dimension size of Y .
The above hard constraints can be implemented using a loss function that penalizes order violations in Y  RNY for the given set of ordered pairs vi  vj :

d(vi , vj ) = || max(0, f (vj ) - f (vi ))||2

(1)

where d(vi , vj ) = 0 if f (vi )  f (vj ) according to the conjunction of total orders; and d(vi , vj ) > 0 if there is an order violation.
Given a set of positively ordered relations E+ and a set of negatively ordered relations E-, the objective function to learn an order-embedding mapping f is defined as a max-margin loss that
encourages positively ordered relations to have zero penalty, and
negatively ordered relations to have penalty greater than a margin:

O=

d(f (vi ), f (vj ))+

max{0,  -d(f (vi), f (vj))}

(vi,vj )E+

(vi,v

 j

)

E

-

(2)

3.2 Negative Sampling Strategies
Problem Analysis. Let E+ = {(vi , vj )} be the set of observed positive dependency relations, and E- be the set of negative depen-
dency relations to be determined. We seek to derive P(vi,vj ), the likelihood of (vi , vj ) E+ being a negative dependency relation. If P(vi,vj ) = 1 or 0, it suggests vi  vj or vi  vj with full certainty. Hence, in negative sampling, two questions should be answered: (1) what is the size of negative samples |E-|? and (2) what is the
sampling distribution P(vi,vj )? For the first question, there are essentially the full approach and
subsampled approach [28]. The full approach determines E- to be

207

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

the set of all unobserved relations, i.e., E- = {(vi , vj )|(vi , vj ) E+}. The full approach can be computationally expensive especially when the observed dependency relations are extremely sparse, i.e., the size of E- can be exponentially greater than the size of E+[28]. Moreover, it may misjudge some unobserved positive relations as negative. The subsampled approach, on the other hand, aims to select a reasonable subset of the entire unobserved relations as negative samples. The subsampled approach is thus a necessary approximation of the full approach. The size of subsampled negative samples is advised to be a constant multiple of the positive samples, i.e., |E-| = O(|E+|) [28]. Simple Sampling Strategies (S1, S2): For the second question, one can assume a uniform distribution for P(vi,vj ) for vi  vj not observed in E+. In other words, for each positive dependency relation (vi , vj )  E+, we generate c negative pairs of either (vi , vk ) E+ or (vk , vj ) E+ where vk 's are randomly selected from V . This is also referred to Simple Strategy 1 (S1). Another simple negative sampling strategy is to reverse each positive dependency relation (vi , vj ) and use the reverse relation as negative sample. We call this the Simple Strategy 2 (S2). Moreover, we explore three aspects of graph structure to derive negative samples: (1) local neighborhood, (2) global neighborhood, and (3) descendant structure. Table 1 gives an overview of the proposed sampling strategies. Local Neighborhood-based Sampling Strategies (L1-L6): The key assumption behind local neighborhood-based negative sampling is that entities with large out-degree should be ones that many other entities may depend on (or precede many other entities), while entities with large in-degree should be ones that depend on many other entities (or be preceded by many other entities). Hence, the likelihood of an unobserved relation (vi, vj) being negative can be estimated by the out-degree and in-degree of vi and vj, respectively. We therefore propose L1-L6 strategies as summarized in Table 1. Given a positive dependency relation (vi , vj ), the strategies select negative samples as follows:

·

L1 gives vi with high in-degree a high probability for (vi, vj )

(e.g.,  (V

a senior-level job × V ) - E+ to be

or course) used as a

negative sample.

·

L2

gives

v

 j

with

high out-degree (e.g.,

an entry-level job, or an

advanced course) a high probability for (vi , vj)  (V × V ) - E+

to be used as a negative sample.

· L3 considers (vi, vj) a high probable negative sample if vi has high in-degree and vj has high out-degree.
· L4-L6 follow the same intuition of L1-L3 respectively except that

high relative out- and in-degrees are used instead.

In the negative sampling process, we follow the sampling with replacement scheme, where the selected negative order relations are not removed from the population V × V - E+. In other words, each relation candidate is independently drawn from the full set of unobserved negative relations according to the sampling distributions controlled by one of L1-L6 until the expected sample size |Et-r | = c × |Et+r | (where c is a constant) is reached. Global Neighborhood Sampling Strategies (G1-G6): Instead of deriving the probability of sampling alternatives of vi and vj to form negative samples using their local neighborhood properties (i.e., in-degree and out-degree), the global neighborhood sampling

Table 1: Specification of the Subsampled Variants.

Strategy Distribution

Population

S1 uniform S2 NA

(vi , vj ) E- (vj , vi ) E+

L1 P(vi ,vj )  ind(vi )

L2 P(vi ,vj )  out d(vj )

L3 P(vi ,vj )  ind(vi ) + out d(vj )

L4

P(vi ,vj )



ind(vi ) 1+out d(vi )

L5

P(vi ,vj )



out d(vj ) 1+ind(vj )

(vi , vj ) E-

L6

P(vi ,vj )



ind(vi ) 1+out d(vi )

+

out d(vj ) 1+ind(vj )

G1 P(vi ,vj )  ancs (vi )

G2 P(vi ,vj )  ancs (vj )

G3 P(vi ,vj )  ancs (vi ) + desc (vj )

G4

P(vi ,vj )



anc s (vi ) 1+d e sc (v )

(vi , vj ) E-

G5

P(vi ,vj )



d e sc (vj ) 1+anc s (vj )

G6

P(vi ,vj )



anc s (vi ) 1+d e sc (v )

+

d e sc (vj ) 1+anc s (vj )

D0 uniform

(vi , vj ) E- 

D1-D6 same as L1-L6 except different sample population vi  T (w ) 

DG1-DG6 same as G1-G6 except different sample population vj  T (w ) - T (vi )

considers global neighborhood properties (i.e., ancestors and de-
scendants). We define the descendants (or ancestors) to be entities that can be reached transitively by following the  relation (or inverse of  relation). These strategies assume that: (i) a node vi with many ancestors suggests that vi is unlikely to be ordered before others. Hence, (vi, vj ) is likely to be a negative order relation; (ii) a node vj with many descendants suggests that it is unlikely to be ordered after others. Hence, (vi , vj) is likely to be a negative order relation. Based on the above assumptions, we derive the sample
strategies G1-G6 in Table 1.
Descendants-Constrained Sampling Strategy (D0-DG6): Other
than measuring the likelihood for a relation to occur, another idea
is to explore the relevance between a pair of entities in the sampling
process. Athiwaratkun et al. introduced a structural constraint on
relations by their relative positions in a network [2]. Specifically, for a node w with at least two descendants, they proposed to randomly select vi  T (w), where T (w) denotes all descendants of w, and then randomly select vj  T (w) - T (vi ). (vi , vj ) is taken as a negative sample if (vi , vj ) E+. In this manner, w is intuitively the common ancestor of vi and vj subject to vj is not a descendant of vi . In essence, this structural constraint suggests a relevant but not close relations between vi and vj . We refer to such structural constraint as descendants constraint. We incorporate such descen-
dants constraint into previously defined sampling strategies. Each
strategy subject to descendants constraint thus has a limited sample
population. Table 1 depicts the specification of D0-D6. D0 is the
original negative selection strategy proposed in [2]. Note that the
set of unobserved relations subject to descendants constraint is a
subset of the full unobserved relations. To sample negative relations
by any of descendants-constrained strategy, we go through every node w with at least two descendants. For each node w, we sample one unobserved relation (vi , vj ) according to P(vi,vj ) among all possible unobserved relations (vi , vj ) that follow the descendants constraint. We repeatedly scan through every node w until the expected sample size |Et-r | is reached. Likewise, the sampling strategies combined with the descendants constraint and global
neighborhood (DG1-DG6) are summarized in Table 1.

208

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

Table 2: Data Statistics.

Synthetic Dataset

Real-World Dataset

Type |V | |E | Density Type |V | |E | Density

Sparse 20 49-50 0.13 Course 654 1,675 3.92×10-3 Moderate 20 83-99 0.26 OrgJob 3,297 755,493 6.95×10-2 Dense 20 129-134 0.34 Citation 5,859 535,608 1.56×10-2

Ensemble Strategies. Each proposed sampling strategy has its merits and deficiencies. For instance, local and global neighborhoodbased strategies can provide a huge quantity of negative samples. Descendants-constrained strategies can provide negative samples of high relevance. We therefore propose to create an ensemble of diverse sampling strategies which may provide a more reliable solution because the blended collection of negative samples from multiple strategies could enrich the sample diversity and quantity. For the ensemble approach, we highlight the top performing strategy in each category. We explore uniform aggregation of them to generate the ensemble scheme A = ki=1Es-i , where Es-i is the set of negative samples by strategy si .

4 EXPERIMENTS ON SYNTHETIC DATASETS
In this section, we conduct two experiments on synthetic graphs in order to achieve the following goals: (1) to understand how negative sampling strategies impact the quality of order embedding, and (2) to explore the usefulness of order embeddings for the entity ranking task and dependency relation prediction task.

4.1 Ground-Truth Dependency Generation

Consider a synthetic graph G=(V ,E) of graph size |V |. We first generate the ordering attributes in a |D|-dimensional ordering space for each entity v  V to entail the order relations. Then, we generate the semantic attributes in another |D|-dimensional semantic space for each entity v  V to entail the semantic relations. Finally, we

generate the dependency relations as ground-truth by aligning both

order and semantic relations entailed in their respective spaces.
Order Relation Generation. To generate a |D|-dimensional vector v  R|D | for each node v  V , the value vi,d in each dimension 1  d  |D| is generated by a uniform distribution between an

arbitrary value range, say, [-2, 2]. According to the relative position to the origin in the |D|-dimensional space, we generate an order relation from entity vi to entity vj if the values of vi in every dimension is smaller than that of vj . Specifically, the adjacency matrix A can be constructed as follows:

Ai, j =

1 0

i f vi,d < vj,d , 1  d  |D | otherwise

(3)

where E = {(vi , vj )|Ai, j = 1} is the set of observed order relations and E¯ = {(vi , vj )|Ai, j = 0} is the full set of unobserved relations. Semantics Relation Generation. We arbitrary select two separated areas in the |D|-dimensional semantic space, e.g., Box1: x [0,2], y [0,2], and Box2: x [30,32], y [30,32] in a 2-dimensional space. To generate a |D|-dimensional vector v  R|D | for each node v  V , we first randomly determine which box v belongs to, then the value vi,d in each dimension 1  d  |D| is generated by a uniform distribution within the value range of the specified box. We
generate 30 synthetic graphs with three density levels (i.e., sparse,
moderate, and dense) as summarized in Table 2.

Train/Test Graph Generation. Given a graph G, we randomly
determine a held-out dataset consisting of 20% relations for evaluation. The 20% held-out relations form a testing graph Gts and the rest 80% relations form a training graph Gtr .

4.2 Entity Ranking

We evaluate the quality of order embedding by the entity ranking

task. Given a training graph Gtr =(V ,E) with observed relations E and unobserved relations E¯, we aim to derive a ranked list of entities

V according to their order embeddings learned from Gtr . Order Embedding Learning. Given a sample size n, we collect a

balanced training set for order embedding learning which consists

of n observed dependency relations as positive training set (denoted ssaceshtEe(t+mdre)enatonotdecnodlulaenscotEbpt-sores)r.ivtWievdeedafenopdlelonnwedgeanthtcievyersedalemaptpieolninndseganswcyniterhgealratetiivpoelnatscr.eaEmint+erinnigst determined by randomly drawn n relations from E in Gtr , whereas Et-r is drawn according to a specified negative sampling strategy from E¯ in Gtr . For instance, we may obtain Et-r using S2 strategy by cf{ro(ovlmlje,cvEtiit+n)r|g(vaenia,dvcEhj )t-rrevgEeivt+rresen}d.eFomibnbsaeeldlryvd,eiWndgereslpleaaatcrioenndoiirmndeeErnfserimoomnbaeEld¯it,dyii.ne|D.g, sE|.ot-TrfhV=e total number of learnable parameters is O = (|V | × |D|).

Ranking Quality Measurement. We compare relative positions

of entities V in the |D|-dimensional original space and the |D|-

dimensional embedding space. To formally measure the ordering

quality, we compute Kendall rank correlation coefficient d  [-1, 1] along dimension d=[1,2]. Given the order relations RGd T as the ground-truth ranking along dimension d and the learned order

embeddings ROE , we compute  coefficient with adjustment for

ties

as

follows.

d

(RGd T

,

RO

E

)=

 nc

+nd

nc -nd +n0 nc

+nd

+n1

,

where nc

is

the number of concordant pairs, nd is the number of discordant

pairs, n0 is the number of ties only in RGd T , and n1 the number of ties only in ROE . Finally, we take the average of rank correlation in

both dimensions µ( ) =

1 |D |

|D | d =1

d

as

the

overall

ranking

quality.

Results. Table 3 gives a full comparison of order embeddings

learned with various negative sampling strategies in terms of av-

erage rank correlations against the ground-truth ordering. Each

point refers to the average rank correlation of one sampling strat-

egy across ten graphs of the same density level, using 5-fold cross

validations, i.e., average of 50 experiments. We compare the rank

correlations of the compared order embeddings using graphs of

three density levels  {sparse, moderate, dense}.For ease of com-

parison, we discretize the average rank correlation µ( ) into four

levels: (i) tier 1 (0.75  µ( )), (ii) tier 2 (0.5  µ( ) < 0.75), (iii) tier

3 (0.25  µ( ) < 0.5), and lastly (iv) tier 4 (µ( ) < 0.25).

We conclude the experimental results with three key observa-

tions. First, order embeddings perform better when the graphs are

denser. For instance, when n=400 the performance (in terms of

µ( )) of most strategies achieve tier 1 quality, compared with the

Table 3: Tier-based Quality of Order Embeddings (n=400).

Sparse Moderate

Dense

Tier 1 None None S2,L1,L4,G2,G4-G6,D0-D3,D5-D6,DG2,DG4-DG6

Tier 2 S1,G3,G6 27 strategies

S1,L2,L3,L5,L6,G1,G3,D4,DG1,DG3

209

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

Table 4: Sample Statistics and Performance Comparison (n=800, 10 Dense Graphs, 5-fold).

Strategy (OE)
S1 S2 L1 L2 L3 L4 L5 L6 G1 G2 G3 G4 G5 G6 D0 D1 D2 D3 D4 D5 D6 DG1 DG2 DG3 DG4 DG5 DG6

OE(d =2)

OE(d =1)

Train (OE) | {Et+r } | | {Et-r

}|

Train/Test(LR) |{Etr }| |{Ets }|

µ( )

WE

OE

OE WE

Train (OE) | {Et+r } | | {Et-r

}|

Train/Test(LR) |{Etr }| |{Ets }|

µ( )

WE

OE

OE WE

63 291.5 140.6 33 .325 .56 .713 .719 76.9 281.4 178 43.9 .473 .54 .677 .681

63

63 140.6 33 .712 .56 .715 .72

76.9 76.9 178 43.9 .573 .54 .697 .716

63 222.2 140.6 33 .401 .56 .709 .718 76.9 232.7 178 43.9 .426 .54 .659 .661

63 218.4 140.6 33 .412 .56 .689 .692 76.9 230.4 178 43.9 .423 .54 .652 .66

63 255.3 140.6 33 .362 .56 .7 .704 76.9 257.9 178 43.9 .465 .54 .662 .678

63 178.1 140.6 33 .378 .56 .697 .703 76.9 186.3 178 43.9 .414 .54 .627 .653

63 179.7 140.6 33 .412 .56 .691 .694 76.9 185.2 178 43.9 .433 .54 .634 .661

63 219.3 140.6 33 .399 .56 .701 .705 76.9 218.9 178 43.9 .408 .54 .652 .671

63 219.9 140.6 33 .383 .56 .698 .709 76.9 231.5 178 43.9 .444 .54 .644 .659

63 217.7 140.6 33 .376 .56 .693 .706 76.9 229.1 178 43.9 .429 .54 .647 .653

63 255.4 140.6 33 .373 .56 .702 .706 76.9 257.4 178 43.9 .435 .54 .645 .668

63 173.8 140.6 33 .391 .56 .705 .711 76.9 178.1 178 43.9 .426 .54 .637 .654

63 175.4 140.6 33 .383 .56 .691 .709 76.9 178.5 178 43.9 .415 .54 .644 .661

63 216.1 140.6 33 .411 .56 .702 .713 76.9 215.5 178 43.9 .413 .54 .627 .657

63 16.1 140.6 33 .568 .56 .5 .611 76.9 20 178 43.9 .465 .54 .497 .578

63

14 140.6 33 .575 .56 .505 .61

76.9 18.9 178 43.9 .451 .54 .485 .561

63 13.3 140.6 33 .559 .56 .507 .614 76.9 18 178 43.9 .46 .54 .493 .582

63 15.5 140.6 33 .549 .56 .491 .596 76.9 19.9 178 43.9 .462 .54 .495 .571

63 13.6 140.6 33 .566 .56 .5 .611 76.9 18.6 178 43.9 .465 .54 .488 .566

63 13.2 140.6 33 .569 .56 .515 .617 76.9 17.6 178 43.9 .455 .54 .487 .581

63 15.3 140.6 33 .551 .56 .505 .605 76.9 19.7 178 43.9 .459 .54 .487 .574

63

14 140.6 33 .554 .56 .503 .61

76.9 18.9 178 43.9 .466 .54 .5 .567

63 13.3 140.6 33 .569 .56 .507 .614 76.9 18.1 178 43.9 .463 .54 .494 .579

63 15.6 140.6 33 .575 .56 .501 .602 76.9 19.9 178 43.9 .461 .54 .497 .571

63 13.7 140.6 33 .567 .56 .499 .609 76.9 18.4 178 43.9 .458 .54 .489 .565

63 13.1 140.6 33 .576 .56 .507 .614 76.9 17.5 178 43.9 .453 .54 .488 .584

63

15 140.6 33 .548 .56 .501 .613 76.9 19.7 178 43.9 .456 .54 .498 .579

A:S1+S2+L1+G6 63 A:S1+S2+L1+D5 63 A:S1+S2+L1+DG2 63 A:S1+S2+G6+D5 63 A:S1+S2+G6+DG2 63 A:S1+S2+D5+DG2 63

261 163.7 236.3 233.9 233.9 181.1

140.6 140.6 140.6 140.6 140.6 140.6

33 .35 .56 .705 .716 33 .52 .56 .698 .716 33 .43 .56 .706 .724 33 .4 .56 .7 .714 33 .42 .56 .705 .718 33 .48 .56 .696 .704

76.9 263.3 178 76.9 237.1 178 76.9 237 178 76.9 238.2 178 76.9 238.2 178 76.9 182.6 178

43.9 .44 .54 .654 .672 43.9 .47 .54 .685 .68 43.9 .46 .54 .657 .667 43.9 .44 .54 .66 .683 43.9 .47 .54 .66 .67 43.9 .48 .54 .682 .686

moderate graphs where all strategies achieve tier 2 quality, and the sparse graphs where only three strategies achieve tier 2 quality. Second, the different sampling approaches showcase their merits and deficiencies through comparisons on graphs of varied densities. For sparse graphs when n=400, some global neighborhood-based sampling approaches achieve the tier 2 quality, whereas all strategies collectively achieve either tier 1 or tier 2 quality. We also obtain better embeddings quality when more samples are provided. We skip the details due to the space limit.
4.3 Dependency Relation Prediction
Given a training graph Gtr =(V , E) with partial-observed relations and a testing graph Gts with a set of held-out relations, we aim to learn a dependency classifier from Gtr to precisely predict dependency and non-dependency relations in Gts . Representation Fusion and Dependency Classifiers. We explore the expressive power of three types of representations: (i) semantic attributed representation (WE); (ii) order embedding (OE); and (iii) fused semantic order embeddings (OEWE), where  is the concatenate operator. Semantic attributed representation is generated during the process of semantics relation generation, which is our simulation of the real-world word embeddings. Order embeddings are learned with respective negative sampling strategies. Given a representation of any type for each entity vi  V , we form the representation for each relation (vi ,vj ) by taking subtraction vj from vi .

Prediction Quality Measurement. To avoid training and testing

sampling in favor of some particular representations, we collect

the observed relations unobserved relations

in in

Gtr Gtr

, ,

denoted denoted

as as

EEt+t-rr,,

and randomly select where |Et+r | = |Et-r |.

We conduct 5-fold cross validation to avoid biased data split for

each training graph Gtr of the same density level. Given a set of
relation representations of particular form {W E, OE, OE  W E} from Et+r and Et-r , we utilize logistic regression (LR) to learn a binary classifier to differentiate positive and negative dependencies.

Specifically, we learn the linear function p(y|e = (vi , vj )), where
y=1 if vi depends on vj , and y=-1 otherwise. We measure the G(pErtt-essd,)i{caetri|eeondeqpEuet+asnlidteypnu(cysyin=(ng1oa|nec-)cdu>erpapec(nyyda=esn-fcoy1l)l|eor)we}lsai.tsi||otTEhnt+PesstiTrEnuNt-stee|| ps, twoinshigteirvgeeraEspet+hst, and {e |e  Et-s  p(y = -1|e) > p(y = 1|e)} is the true negative set. Dimensionality Study (d). Table 4 shows the sample statistics

and prediction accuracy across various negative sampling strategies.

A key observation is that the prediction accuracy using OE with d=2

is consistently better than OE with d = 1 across multiple strategies.

This may suggest that with the right setting of embedding dimen-

sionality (d=2), OE can achieve the best prediction performance.

Representation Study. A key observation in Table 4 is that the

prediction accuracy using blended representation OEWE is con-

sistently better than using either OE or WE alone. Notice that any

testing relation (vi ,vj ) comes from one of four quadrants: (i) vi and vj are ordered and similar in semantics, (ii) ordered but dissimilar

210

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

in semantics, (iii) non-ordered but similar in semantics, and (iv) non-ordered and dissimilar in semantics. The error cases of OEdriven classifier, which is trained without the semantic knowledge, lie under quadrant ii, whereas the error cases of WE-driven classifier, which is trained without the ordering knowledge, lie under quadrant iii. The OEWE-driven classifier can better differentiate cases lies under quadrants ii and iii because it is trained with both ordering and semantic knowledge, and thus outperforms others. Sample Ensemble Study. Another key observation in Table 4 is that sample ensembles can effectively lead to better performance. For instance, S2 alone, which takes the reverse of the observed relations as negative samples, is obviously a wining strategy amongst all. Nonetheless, the overall predictive power can be further optimized with the sample ensemble technique, providing .724 accuracy compared to S2 at .72 in Table 4. This suggests that the negative samples contributed by S1, S2, L1, and DG2, not only increase in quantity (236.3 unique negative samples on average) but also provide diverse and useful signals, resulting in accurate order embeddings.
5 EXPERIMENTS ON REAL DATASETS
We conduct experiments on real-world networks for two purposes: (1) to quantitatively and qualitatively study the quality of order embedding, and (2) to demonstrate the usefulness of order embeddings for entity ranking and dependency relation prediction tasks.
5.1 Datasets
We first describe thee real-world datasets for entity ranking task and dependency relation prediction task. The statistics of each network and their transitive closure is summarized in Table 2. Course Dependency Hierarchy (Course). We use the course dependency dataset1 from 11 cs-related universities in US. There are 654 unique courses, and 861 dependency relations among courses. For each course, we extract the course title (TT) and course description (DE) from course content. After removing stop words, we obtain a vocabulary of 3,987 words. Singapore Organization Job Hierarchy (OrgJob). There are 3,297 standardized job titles and 26,550 immediate reporting lines between supervisor and subordinate job titles from Singapore organization job hierarchy. We develop a job title parser to extract job function from each job title [18]. ACM Citation Network (Citation). There are 1.1M articles with at least one citation in the latest ACM citation network 2. We retrieve those articles in five different research areas: KDD, ICML, NIPS, WWW, and SIGIR, resulting in 5,859 papers and 60,759 citations. For each paper, we extract paper title (TT) as the textual content. After removing stop words, we obtain 6,548 vocabularies.
5.2 Experiment Setup
Representation Baselines. We include several representation baselines for comparison in our experiments.
· Pre-Trained Word Embedding (WE): Glove vectors [21] are global vectors trained for word representations. For fair comparison, we use the vectors of length 50 dimensions trained on 6 billion words from Wikipedia corpus. Another widely used word vectors
1 https://github.com/harrylclc/concept-prerequisite-papers 2 https://aminer.org/citation

are the Skip-Gram-based Word2Vec [13]. We have self-trained
vectors of 44,449 words and of 50 dimensions on our job posts corpus3 using the Skip-Gram model of Word2Vec. The job posts corpus is domain-focused and contains 544,369 unique job posts. · Vector Order Embedding (OE): We adopt vector order embedding
(VOE) [24] to learn order embeddings for entities. The parameter settings of VOE includes learning rate =0.01, batch size = 256,
and 80 epochs for all experiments. · Fusion (OEWE): We represent each entity using both word
and order embeddings learned on each real-world dataset, based
on various negative sampling strategies. The representations of entity vi  V is represented as a concatenation of WEOE. The resulting dimension of vector WEOE consists of word embedding (50 by default) and order embeddings (|D| [1, 3, 5]).

5.3 Entity Ranking

Given a training graph Gtr =(V ,E) with observed relations E and unobserved relations E¯, we aim to derive a ranked list of entities V

according to their order embeddings learned from Gtr .

Course Dependency Hierarchy. We evaluate the quality of order

embedding by the entity ranking task. A valid dependency relation

of a course pair (vi ,vj ) represents course vi depends on course vj ,

in which case vi is the relatively more advanced class and vj is

the fundamental one. A course pair (vi ,vj ) is regarded as positive

if the dependency is observed in Gtr ; otherwise, the course pair

is considered as a negative one. Given the sample size n and a

sampling relations

sEtt-rratteogyle, awrensoamrdpelreenmpboesditdivinegreslafotironeascEht+rcoanudrsne.nTeagbatleiv5e

shows the top-10 fundamental courses and bottom-10 advanced

courses ranked by order embeddings using S2 strategy with the

sample size n=5000. The top-10 fundamental courses tend to have

higher in-degree and out-degree ratio (42.6 versus 0.1 on average),

compared to the top-10 advanced courses (0.0 versus 7.0 on average).

This suggests that top fundamental courses are truly prerequisite

in the course dependency hierarchy and vice versa.

Organization Job Hierarchy. A valid dependency relation of a

job pair (vi ,vj ) represents career progression from job vi to vj , in which case vi is the relatively junior to vj which is more senior. Given the sample size n and a sampling strategy, we sample n positive relations Et+r and n negative relations Et-r to learn order embeddings for each job title. Table 5 shows the top-10 senior

jobs and bottom-10 entry-level jobs in Singapore organization job

hierarchy by order embeddings using S2 strategy with sample size

n=10K. The top-10 senior jobs tend to have higher in-degree and

out-degree ratio (1,579.6 versus 225.5 on average), compared to the

top-10 entry-level jobs (0.8 versus 161.2). This suggests that top

senior jobs are truly high ranking positions in the organization job

hierarchy and vice versa.

Citation Network. A valid dependency relation of a paper pair

(vi ,vj ) represents paper vi cites vj , in which case vi is the relatively

earlier work and vj is the recent one. Given the sample size n

and a sampling strategy, we sample n negative relations Et-r to learn order

positive relations Et+r embeddings for each

and n paper.

Table 5 shows the top-10 earliest work and bottom-10 latest work

in ACM citation network by order embeddings using S2 strategy

with sample size n=175, 000. The top-10 earliest papers tend to

3 https://www.mycareersfuture.sg

211

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

Table 5: Top-10 and Bottom-10 Entities by Order Embeddings (d = 1, S2).

Top Fundamental Courses (n = 5K )

Top Senior Jobs (n = 100K )

Course Title

Job Title

Programming Methodology

Prime Minister

Intro to Computer Science.

Minister

Introduction to Computer Programming

Trade and Industry Minister

Fundamentals of Programming and Computer Science Home Affair Minister

Introduction to Programming Techniques

Infrastructure Minister

Computer Science I: Fundamentals

Defence Minister

Introduction to EECS I

Permanent Secretary

Programming Abstractions

Manpower Minister

Object-Oriented Programming I

Finance Minister

Discrete Structures

Education Minister

Top Earliest Papers (n = 175K )

Paper Title

Year

MARS: A Retrieval Tool on the Basis of Morphological Analysis 1984

A Decision Theory Approach to Optimal Automatic Indexing

1982

Artificial Intelligence Implications for Information Retrieval

1983

A Common Architecture for Different Text Processing Techniques... 1986

Evaluation of The 2-Poisson Model as a Basis for Using Term...

1983

An Approach to Natural Language for Document Retrieval

1987

An Evaluation of Term Dependence Models in Information Retrieval 1982

IR, NLP, AI and UFOS: or IR-Relevance, Natural Language Problems... 1986

The Maximum Entropy Principle in Information Retrieval

1986

The Automatic Indexing System AIR/PHYS

1988

Top Advanced Courses (n = 5K ) Course Title
Collaborative Design (W) Advanced Compiler Construction Projects in Database Systems Topics in Algorithmic Economics Projects in Networking Advanced Computer Security Advanced Networking Advanced Distributed Systems Advanced Computer Networks Array Processing

Top Entry-Level Jobs (n = 100K ) Job Title

Paper Title

Top Latest Papers (n = 175K )

North Mosque Cluster Manager

39 GLAD: Group Anomaly Detection in Social Media Analysis

Deputy Decision Support Service Director SMVC: Semi-Supervised Multi-View Clustering in Subspace...

Asset Service & Finance Director

CatchSync : Catching Synchronized Behavior in Large Directed...

Air Traffic Control Manager

Exploiting Geographic Dependencies for Real Estate. Appraisal: A...

Assistant Policy Director

Gradient Boosted Feature Selection

Senior Assistant Major Director

Network Mining and Analysis for Social Applications

Principal Emergency Preparedness Specialist Grouping Students in Educational Settings

Agency Service Director

Optimal Recommendations under Attraction, Aversion, and Social...

Deputy Pupil Director

Learning Time-Series Shapelets

Assistant Speciality and Service Director Good-Enough Brain Model: Challenges, Algorithms, and...

Year
2014 2014 2014 2014 2014 2014 2014 2014 2014 2014

Table 6: Performance comparison on course dependency hierarchy (n=5K), where TT (DE) denotes course titles (descriptions).

Strategy (OE)
S1 S2 L1 L2 L3 L4 L5 L6 G1 G2 G3 G4 G5 G6 D0 D1 D2 D3 D4 D5 D6 DG1 DG2 DG3 DG4 DG5 DG6

Train (OE) Train/Test(LR) WE

OE(d =1)

OE(d =3)

OE(d =5)

| {Et+r } | | {Et-r } | |Et r | |Et s | TT DE OE OETT OEDE OE OETT OEDE OE OETT OEDE

1675 5000.0 3726.4 847.2 .641 .601 .834 .778 .829 .88 .831 .886 .863 .742 .808

1675 1675.0 3726.4 847.2 .641 .601 .854 .759 .805 .852 .828 .811 .85 .778 .845

1675 4732.0 3726.4 847.2 .641 .601 .851 .776 .841 .879 .83 .888 .87 .795 .785

1675 4951.4 3726.4 847.2 .641 .601 .772 .701 .778 .829 .768 .82 .843 .748 .751

1675 4908.4 3726.4 847.2 .641 .601 .803 .739 .811 .845 .768 .774 .825 .734 .752

1675 4609.6 3726.4 847.2 .641 .601 .834 .735 .791 .886 .837 .855 .854 .749 .785

1675 4949.6 3726.4 847.2 .641 .601 .819 .741 .755 .872 .767 .817 .886 .775 .818

1675 4895.4 3726.4 847.2 .641 .601 .809 .745 .794 .86 .805 .823 .853 .764 .786

1675 4715.8 3726.4 847.2 .641 .601 .82 .771 .785 .826 .772 .852 .861 .77 .778

1675 4950.0 3726.4 847.2 .641 .601 .795 .729 .831 .856 .805 .876 .868 .75 .751

1675 4902.0 3726.4 847.2 .641 .601 .781 .709 .812 .842 .798 .842 .831 .748 .766

1675 4580.4 3726.4 847.2 .641 .601 .738 .715 .758 .839 .746 .772 .854 .745 .787

1675 4946.2 3726.4 847.2 .641 .601 .79 .757 .797 .851 .817 .803 .853 .772 .797

1675 4883.2 3726.4 847.2 .641 .601 .789 .74 .811 .821 .762 .77 .856 .775 .773

1675 11.4 3726.4 847.2 .641 .601 .796 .617 .606 .838 .73 .72 .799 .715 .699

1675 11.4 3726.4 847.2 .641 .601 .69 .643 .63 .832 .731 .72 .79 .742 .686

1675 8.2 3726.4 847.2 .641 .601 .797 .618 .602 .827 .721 .699 .797 .727 .718

1675 11.4 3726.4 847.2 .641 .601 .781 .635 .637 .832 .751 .774 .811 .712 .716

1675 11.4 3726.4 847.2 .641 .601 .773 .628 .615 .815 .7 .722 .844 .717 .7

1675 8.2 3726.4 847.2 .641 .601 .751 .612 .61 .837 .719 .72 .772 .707 .71

1675 11.4 3726.4 847.2 .641 .601 .767 .63 .635 .843 .698 .751 .82 .722 .73

1675 11.4 3726.4 847.2 .641 .601 .78 .622 .617 .803 .725 .736 .812 .734 .714

1675 8.2 3726.4 847.2 .641 .601 .803 .631 .587 .807 .714 .72 .786 .679 .72

1675 11.4 3726.4 847.2 .641 .601 .825 .634 .612 .85 .736 .784 .858 .73 .711

1675 11.4 3726.4 847.2 .641 .601 .761 .645 .6 .849 .761 .717 .778 .723 .713

1675 8.2 3726.4 847.2 .641 .601 .781 .63 .593 .835 .737 .747 .847 .682 .717

1675 11.4 3726.4 847.2 .641 .601 .767 .638 .609 .845 .735 .735 .802 .731 .707

A:S1+S2+L4+G5 1666.6 4834.8 3726.4 847.2 .641 .601 .873 .765 .806 .878 .816 .829 .895 .827 .863 A:S1+S2+L4+D6 1669.6 4685.2 3726.4 847.2 .641 .601 .845 .754 .788 .886 .788 .838 .901 .819 .785 A:S1+S2+L4+DG3 1666.8 3618.0 3726.4 847.2 .641 .601 .803 .743 .769 .81 .769 .803 .834 .772 .774 A:S1+S2+G5+D6 1668.0 3736.4 3726.4 847.2 .641 .601 .883 .798 .823 .856 .794 .823 .893 .818 .877 A:S1+S2+G5+DG3 1667.4 3736.8 3726.4 847.2 .641 .601 .87 .763 .835 .884 .817 .876 .899 .841 .857 A:S1+S2+D6+DG3 1667.0 2502.8 3726.4 847.2 .641 .601 .827 .759 .738 .827 .764 .792 .805 .786 .814

have more general topics and higher in-degree and out-degree ratio (471.9 versus 173.2 on average) in the ACM citation network, compared to the top-10 (0.8 versus 0.5).

5.4 Dependency Relation Prediction
We follow Section 4.3 to explore the expressive power of the following three types of representations: semantic attributed representation (WE), order embedding (OE), and fused semantic order

212

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

Table 7: Performance comparison, where FN denotes job functions and TT denotes paper titles.

Organization Job Hierarchy (n=25K , d =1)

Citation Network (n=43.8K , d =1)

Strategy Train (OE) Train/Test(LR) WE OE OEWE Train (OE) Train/Test(LR) WE OE OEWE

(OE)

| {Et+r } | | {Et-r } | |Et r | |Et est | FN OE OEFN | {Et+r } | | {Et-r } | |Et r | |Et s | TT OE OETT

S1

25K 25K 1.8M 1.8M .6 .783 .78 43.8K 43.8K 1.3M 805.8K .664 .823 .825

S2

25K 25K 1.8M 1.8M .6 .951 .951 43.8K 43.8K 1.3M 805.8K .664 .881 .879

L1

25K 23.8K 1.8M 1.8M .6 .928 .924 43.8K 42K 1.3M 805.8K .664 .858 .85

L2

25K 24.8K 1.8M 1.8M .6 .872 .87 43.8K 43K 1.3M 805.8K .664 .88 .879

L3

25K 24.6K 1.8M 1.8M .6 .867 .865 43.8K 43K 1.3M 805.8K .664 .891 .886

L4

25K 18.7K 1.8M 1.8M .6 .795 .801 43.8K 41K 1.3M 805.8K .664 .854 .849

L5

25K 24.8K 1.8M 1.8M .6 .869 .867 43.8K 43K 1.3M 805.8K .664 .88 .88

L6

25K 24.7K 1.8M 1.8M .6 .856 .849 43.8K 43K 1.3M 805.8K .664 .886 .884

G1

25K 23.8K 1.8M 1.8M .6 .932 .932 43.8K 42K 1.3M 805.8K .664 .857 .85

G2

25K 24.8K 1.8M 1.8M .6 .87 .868 43.8K 43K 1.3M 805.8K .664 .881 .88

G3

25K 24.6K 1.8M 1.8M .6 .867 .866 43.8K 43K 1.3M 805.8K .664 .886 .885

G4

25K 18.4K 1.8M 1.8M .6 .793 .801 43.8K 41K 1.3M 805.8K .664 .85 .842

G5

25K 24.8K 1.8M 1.8M .6 .873 .87 43.8K 43K 1.3M 805.8K .664 .879 .879

G6

25K 24.7K 1.8M 1.8M .6 .85 .863 43.8K 43K 1.3M 805.8K .664 .886 .882

D0

25K 1.1K 1.8M 1.8M .6 .662 .699 43.8K 23K 1.3M 805.8K .664 .684 .696

D1

25K 629 1.8M 1.8M .6 .731 .681 43.8K 14K 1.3M 805.8K .664 .664 .69

D2

25K 1.1K 1.8M 1.8M .6 .658 .667 43.8K 18K 1.3M 805.8K .664 .744 .769

D3

25K 1.1K 1.8M 1.8M .6 .661 .72 43.8K 18K 1.3M 805.8K .664 .666 .698

D4

25K 566.8 1.8M 1.8M .6 .74 .693 43.8K 12K 1.3M 805.8K .664 .663 .75

D5

25K 760.6 1.8M 1.8M .6 .657 .651 43.8K 12K 1.3M 805.8K .664 .781 .784

D6

25K 984.2 1.8M 1.8M .6 .643 .611 43.8K 15K 1.3M 805.8K .664 .663 .737

DG1

25K 629 1.8M 1.8M .6 .696 .683 43.8K 14K 1.3M 805.8K .664 .661 .695

DG2

25K 1K 1.8M 1.8M .6 .635 .663 43.8K 18K 1.3M 805.8K .664 .747 .762

DG3

25K 1.2K 1.8M 1.8M .6 .631 .664 43.8K 18K 1.3M 805.8K .664 .663 .723

DG4

25K 568.6 1.8M 1.8M .6 .653 .673 43.8K 11K 1.3M 805.8K .664 .666 .667

DG5

25K 759 1.8M 1.8M .6 .621 .577 43.8K 12.1K 1.3M 805.8K .664 .77 .782

DG6

25K 989.6 1.8M 1.8M .6 .62 .687 43.8K 15K 1.3M 805.8K .664 .668 .722

embeddings (OEWE). To predict whether an unseen antisymmetric relation (vi ,vj ) holds or not, we train a dependency classifier using logistic regression which takes representations of entity pairs and the label of entity pairs as input. We report the classification accuracy using 5-fold cross validation. Representations Study. Table 6 compares the performance of predictions made by various representations. An observation made from Table 6 is that the prediction accuracy using OE representation is better than using either WE or OEWE. This suggests that OE is effective and reliable, especially when the textual content between a relevant entity pairs is very different. For example, "machine learning" course transitively depends on "data structure", but nonetheless the dependencies do not necessarily reflect in the semantic similarity by course title (TT) or course description (DE). The expressive power of OE is also observed on OrgJob and Citation networks in Table 7, respectively. Dimensionality Study (d). Another observation in Table 6 is that the common sense of "the higher dimensionality the greater performance" does not necessarily hold. OE (d = 3) does give a better performance than OE (d = 1), but OE (d = 3) is still superior to OE (d = 5) in most cases as reported in Table 6. The order embedding method seeks to map each entity in a given hierarchy into a low dimensional vector with a fixed dimension d. Unfortunately, we have no knowledge of the intrinsic dimensionality to describe a particular hierarchical structure. To unravel the true dimensionality, we therefore empirically extend our experiment to multiple configurations. Table 6 suggests that d = 3 should suffice to preserve course dependency hierarchy. Subsampled and Ensemble Study. A key observation throughout the synthetic and real datasets is that S1 consistently outperforms other strategies despite their simplicity and low overheads as reported in Table 6 and 7. In particular, the overall predictive power

can be further optimized with the sample ensemble technique, providing .901 (d=5) accuracy with 4.4% improvement compared to S2 at .863 (d=5) in Table 6. This suggests that the negative samples contributed by S1, S2, L4, and D6 provides diverse and useful signals (4,685.2 unique negative samples on average), resulting in accurately learning order embeddings.
6 CONCLUSION AND FUTURE WORK
In this paper, we integrate the notions of order embedding and semantic proximity to model dependency relations. In practice, negative dependency relations/samples often are missing from the dependency relation graphs.To address this one-class problem, we explore multiple negative sampling strategies based on graph-specific centralities to collect diverse and precise negative information. We learn order embeddings from positive and negative training dependency relations by minimizing the max-margin loss on orderviolation penalties. We conduct extensive experiments, using both synthetic and several real datasets, to demonstrate the usefulness of order embeddings and to gain better understanding of order embedding through the tasks of entity ranking and dependency relation prediction. Our studies show that: (i) the proposed negative sampling strategies enable effective and efficient order embedding learning, (ii) ensembles of diverse negative samples lead to robust and mostly better performance, (iii) while order embedding can give strong features, blended feature representations can contribute to better prediction accuracy when dependent entities share similar semantic. As for the future work, one direction is to extend the work to other order embedding models. Another interesting direction is to learn order embeddings for words to directly infer dependency relations.

213

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

ACKNOWLEDGMENT
This research is supported by the National Research Foundation,
Prime Minister's Office, Singapore under its International Research
Centres in Singapore Funding Initiative. This work is supported
in part by the National Science Foundation under Grant No. IIS-
1717084.
REFERENCES
[1] Fareedah ALSaad, Assma Boughoula, Chase Geigle, Hari Sundaram, and ChengXiang Zhai. 2018. Mining MOOC Lecture Transcripts to Construct Concept Dependency Graphs. In EDM.
[2] Ben Athiwaratkun and Andrew Gordon Wilson. 2018. Hierarchical Density Order Embeddings. In ICLR.
[3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NIPS.
[4] Hongyun Cai, Vincent W Zheng, and Kevin Chang. 2018. A comprehensive survey of graph embedding: problems, techniques and applications. TKDE (2018).
[5] Yetian Chen, José P González-Brenes, and Jin Tian. 2016. Joint Discovery of Skill Prerequisite Graphs and Student Models.. In EDM.
[6] Xin Luna Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Kevin Murphy, Shaohua Sun, and Wei Zhang. 2014. From data fusion to knowledge fusion. Proceedings of the VLDB Endowment 7, 10 (2014), 881­892.
[7] Alexander R Fabbri, Irene Li, Prawat Trairatvorakul, Yijiao He, Wei Tai Ting, Robert Tung, Caitlin Westerfield, and Dragomir R Radev. 2018. TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation. In ACL.
[8] Jonathan Gordon, Linhong Zhu, Aram Galstyan, Prem Natarajan, and Gully Burns. 2016. Modeling concept dependencies in a scientific corpus. In ACL.
[9] Chen Liang, Jianbo Ye, Shuting Wang, Bart Pursel, and C Lee Giles. 2018. Investigating active learning for concept prerequisite learning. EAAI (2018).
[10] Chen Liang, Jianbo Ye, Zhaohui Wu, Bart Pursel, and C Lee Giles. 2017. Recovering Concept Prerequisite Relations from University Course Dependencies.. In AAAI.
[11] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion.. In AAAI.

[12] Hanxiao Liu, Wanli Ma, Yiming Yang, and Jaime Carbonell. 2016. Learning concept graphs from online educational data. JAIR 55 (2016).
[13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.
[14] Jack Minker. 1982. On indefinite databases and the closed world assumption. In International Conference on Automated Deduction.
[15] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2016. A review of relational machine learning for knowledge graphs. Proc. IEEE 104, 1 (2016), 11­33.
[16] Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, and others. 2016. Holographic Embeddings of Knowledge Graphs.. In AAAI.
[17] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A Three-Way Model for Collective Learning on Multi-Relational Data.. In ICML.
[18] Richard Jayadi Oentaryo, Ee-Peng Lim, Xavier Jayaraj Siddarth Ashok, Philips Kokoh Prasetyo, Koon Han Ong, and Zi Quan Lau. 2018. Talent Flow Analytics in Online Professional Network. Data Science and Engineering 3 (2018).
[19] Liangming Pan, Chengjiang Li, Juanzi Li, and Jie Tang. 2017. Prerequisite relation learning for concepts in moocs. In ACL.
[20] Liangming Pan, Xiaochen Wang, Chengjiang Li, Juanzi Li, and Jie Tang. 2017. Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation. In IJCNLP.
[21] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP.
[22] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In SIGKDD.
[23] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding.. In WWW.
[24] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2016. Orderembeddings of images and language. In ICLR.
[25] Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. 2018. Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures. In ACL.
[26] Luke Vilnis and Andrew McCallum. 2015. Word representations via gaussian embedding. In ICLR.
[27] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge Graph Embedding: A Survey of Approaches and Applications. TKDE 29, 12 (2017), 2724­2743.
[28] Hsiang-Fu Yu, Mikhail Bilenko, and Chih-Jen Lin. 2017. Selection of negative samples for one-class matrix factorization. In SDM.

214


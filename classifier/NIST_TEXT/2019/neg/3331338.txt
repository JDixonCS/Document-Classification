Short Research Papers 2A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Local Matrix Approximation based on Graph Random Walk

Xuejiao Yang and Bang Wang*
Huazhong University of Science and Technology (HUST), Wuhan, China yxj603@foxmail.com,wangbang@hust.edu.cn

ABSTRACT
How to decompose a large global matrix into many small local matrices has been recently researched a lot for matrix approximation. However, the distance computation in matrix decomposition is a challenging issue, as no prior knowledge about the most appropriate feature vectors and distance measures are available. In this paper, we propose a novel scheme for local matrix construction without involving distance computation. The basic idea is based on the application of convergence probabilities of graph random walk. At first, a user-item bipartite graph is constructed from the global matrix. After performing random walk on the bipartite graph, we select some user-item pairs as anchors. Then another random walk with restart is applied to construct the local matrix for each anchor. Finally, the global matrix approximation is obtained by averaging the prediction results of local matrices. Our experiments on the four real-world datasets show that the proposed solution outperforms the state-of-the-art schemes in terms of lower prediction errors and higher coverage ratios.
CCS CONCEPTS
· Information systems  Recommender systems.
KEYWORDS
Matrix factorization; local matrix construction; graph random walk; recommendation
ACM Reference Format: Xuejiao Yang and Bang Wang. 2019. Local Matrix Approximation based on Graph Random Walk. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10. 1145/3331184.3331338
*Xuejiao Yang and Bang Wang are with the School of Electronic, Information and Communications, HUST. This work is supported in part by National Natural Science Foundation of China (Grant No: 61771209).
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331338

1 INTRODUCTION
Recommendation systems have been playing an important role in the online social networks and e-commerce websites. Among various recommendation algorithms, the matrix factorization (MF) has gained lots of attentions since its great success in the Netflix contest [2]. For a user-item rating matrix R  R× with  users and  items, the MF technique is to predict the missing values in R by approximating R with another matrix R^ = UVT, where U  R× is a user-factor matrix and V  R× an item-factor matrix, with   min(,  ). After performing a global MF on R, a recommendation list for unrated items can be obtained according to the predicted ratings for each user.
Recently, a global rating matrix has become very large and sparse. Many studies have focused on how to convert the global matrix factorization task into several local matrix approximation tasks [1, 3, 4, 6]. For example, the DFC algorithm [4] randomly selects a subset of rows and columns from the global matrix to form many local matrices. It then performs a local MF on each local matrix and gets the final predictions by averaging the local approximations of the local matrices. Another important direction for decomposing the original rating matrix is based on the idea of anchor selection and neighborhood inclusion, as done in LLORMA [3] and CLLORMA [6]. They first select some of rated user-item pairs as anchors, and then find the neighbor users and items for each anchor, by which a local matrix is constructed.
Distance computation is needed for comparing the similarity in between users and items in the aforementioned algorithms when decomposing R. However, engineering an appropriate feature as a user or item vector representation may become a challenging task. Although using the global MF can obtain user latent features from U and item latent features from V, how to choose an appropriate feature dimensionality  again becomes a new challenge. Furthermore, there exist lots of distance measures, like the Euclidean distance, Pearson correlation, cosine similarity and etc., yet which one is the most appropriate for what kind of datasets is still an open problem. On the one hand, we are motivated from the recent findings for decomposing the original large matrix into many small local matrices. On the other hand, we would like to construct local matrices without involving distance computation.
In this paper, we propose a novel scheme to decompose the original rating matrix into many local matrices, called random walk-based local matrix approximation (RWLMA). We first construct a user-item bipartite graph for the original matrix R. We then select some user-item pairs as anchors according to their convergence probabilities after performing a random walk (RW) on the bipartite graph. Then for each

1037

Short Research Papers 2A: AI, Mining, and others
SIGIR '19, July 21­25, 2019, Paris, France

anchor, we perform a random walk with restart (RWR) with the anchor user (item) as the restarting node to obtain the correlation degrees between this anchor user (item) and other users (items). Each non-anchor user-item pair is included into her top anchors' neighborhood, and for each anchor and her neighborhood, we construct a local matrix. The global matrix approximation is obtained by averaging the approximation results of local matrices. Experiments on four datasets including three public ones show that the proposed RWLMA can outperform the state-of-the-art ones in terms of lower prediction errors and higher coverage ratios.

2 THE PROPOSED METHOD

2.1 Anchor Selection

We propose to use a graph-based random walk to select anchors. Let R  R× denote a user-item rating matrix with  users and  items, where an element   R, if it exists, is a non-negative real value as the rating given

to the th item by the th user. From R, we construct a

bipartite graph of users and items as follows: If  exists, an undirected edge is established between  and  in the

bipartite graph with the edge weight of . Motivated from the PageRank [5], we use a Markov chain to transform the

anchor importance calculation task into a node convergence

probability computation problem. Let P  be the probability transition matrix from items to users, which is obtained by

column-normalizing the rating matrix R. Let P be the
transition matrix from users to items, which is calculated by column-normalizing RT.

We randomly initialize the probability vector of users and items as u(0) and v(0). To obtain the convergence proba-

bilities, the proposed random walk algorithm is to iterative

compute the following equations:

u(+1)

=

(1

-

)

·

P

 v()

+



·

1 

(1)

v(+1)

=

(1

-

)

·

P 

u()

+



·

1 

(2)

where u(+1) and v(+1), respectively, are the user and item

probability vectors in the th iteration. The parameter  

(0, 1) denotes the random visit probability. On the one hand,

using  is equivalent to adding a very small weight for each

node to connect with other nodes, thus guaranteeing the

connectivity of the constructed bipartite graph. On the other

hand, it is reasonable to assume that a user has some chance

to select one item seemingly not within her previous interests.

For example, in Eq. (1), a user node receives (1 - ) prob-

ability of being visited from its connected item nodes, and

 probability of being randomly visited by one of  users.

Similar analysis applies in Eq. (2) for item transition.

The iteration terminates until the pairwise difference in

between two iteration probability vectors is smaller than a

predefined threshold. After the iteration termination, each

node in the graph can obtain its convergence probability,

which can reflect the importance of this node in the network

to some extent. According to the convergence probabilities,

we sort the list of users and the list of items in a decreasing

SIGIR '19, July 21­25, 2019, Paris, France
Xuejiao Yang and Bang Wang

order, respectively. Then we select the top  users and top  items from the sorted lists and randomly pair them to form in total  anchors. Let  denote the set of selected anchors.

2.2 Neighborhood Construction
For each anchor, we next determine its neighbor users and neighbor items so as to construct a corresponding local matrix. To this end, we apply a graph-based random walk with restart to measure the relations between an anchor and other non-anchor user-item pairs as follows. Take anchor (, ) as an example. We first use the anchor user  as a restart node to compute its correlation degree with other user nodes. We use the one-hot vector to initialize the user probability vector as u(0). That is, u(0)() = 1, if  = . Otherwise, u(0)() = 0. We define the user restart vector as r . For user , r () = 1, if  = . Otherwise, r () = 0. Furthermore, we randomly initialize the item probability vector v(0).
To obtain the convergence probabilities, the RWR algorithm is to iteratively compute the following equations:

u(+1) = (1 - ) · P  v() +  · r

(3)

v(+1) = P u()

(4)

where u(+1) and v(+1), respectively, are the probability vectors that user and item nodes are visited in the th iteration. The restart probability  denotes that in each iteration, there is  probability to return to the restart node  and 1 -  probability to walk from items to users. The iteration terminates until the pairwise difference in between the two iteration probability vectors is smaller than a predefined threshold. Let u denote the convergence probability vector of user nodes, which represents the correlation degree between these user nodes and .
Similarly, we perform the random walk with restart for each anchor item  as follows: We use the one-hot vector to initialize the probability vector of items as v(0), and randomly initialize the probability vector of users as u(0). Let r be the item restart vector: If  = , r () = 1; Otherwise, r () = 0. The iteration process is as follows:

v(+1) = (1 - ) · P u() +  · r

(5)

u(+1) = P  v()

(6)

Let v denote the convergence probability vector of item nodes, which represents the correlation degree between these
item nodes and . For each anchor (, )  , we now obtain its conver-
gence probability vectors u and v, by which we construct a user convergence matrix C  R× and an item convergence matrix C  R×, respectively. The th column in C is the user convergence vector u for the anchor (, ), and the th row in C is the user 's convergence probability under different restart anchor node , which can reflect the relationship between the user  and each anchor user. We
define a local matrix scale control parameter (0.5 <  < 1)
for each user and each item. That is, a user can only be

1038

Short Research Papers 2A: AI, Mining, and others
Local Matrix Approximation based on Graph Random Walk

assigned into the user neighborhood of  ×  anchors. The same applies to each item.
For each user , we select the top  ×  anchors in the decreasing order of the element value in the th row of C . In the end, we construct a set of neighboring users  for each anchor . Similarly, the th column in C is the item convergence vector v for the anchor (, ), and the th row in C can reflect the relationship between the item  and each anchor item. We assign each item  into the top  ×  anchors in the decreasing order of the element value in the th row of C and construct a set of neighboring items  for each anchor .
Finally, we construct a local matrix for each anchor  based on the neighbor set  and  as follows: For each neighbor user   , we extract its corresponding th row in the original rating matrix R; And for each neighbor item   , we extract its corresponding th column in R. These extracted rows and columns then construct the local matrix R  R||×|| for the anchor .
We next discuss the coverage of local matrices. Let  and  denote the set of user-item pairs in R and in R, respectively. For each element   , we define I() = 1, if  appears in at least one of s; Otherwise, I() = 0. The coverage of local matrices is defined as

1 

= | |

I().

(7)



As we obtain the global predictions for missing values from those local predictions of local matrices, it is desirable to have complete coverage of the global rating matrix, that is,  = 1. We have the following lemma.

Lemma 2.1. Our local matrices can achieve complete coverage of the global rating matrix R.

Proof. Proof is omitted due to limited space.

2.3 Matrix Approximation

In each local matrix R, we use the SVD algorithm[2] to predict ratings. The loss function in each local matrix is:

arg min ([UV ], - R,)2 + (||U||2 + ||V||2)
(U ,V )

where U and V, respectively, are the user-factor matrix and item-factor matrix of R after matrix factorization. We
train each local matrix by the gradient descent method, and
compute the prediction rating from user  to item  as:

R^ , = [UV ], ( = 1, 2, ..., )

(8)

Finally, we obtain the global rating approximation R^ , by averaging the results R^ , from each local matrix to which
(, ) belongs.

3 EXPERIMENT
Experimental Settings: We conduct our experiments on four datasets: Ciao, Movielens-100K(ML-100K), Movielens1M(ML-1M) and Zhihu Live(Zhihu). Movielens and Ciao are public datasets widely used in the rating prediction problem.

SIGIR '19, July 21­25, 2019, Paris, France

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Dataset Statistics

Ciao MovieLens-100K MovieLens-1M
Zhihu Live

#users 7375 943 6940 4482

#items 106797
1682 3952 3724

#rating 283119 100000 1000209 60285

density 0.04% 6.3% 3.65% 0.01%

Zhihu Live is a real-time Q&A interactive product launched

by Zhihu, a well-known online Q&A community in China.

We craweled Zhihu Live website to build our dataset. In

all the four datasets, the ratings are chosen from {1,...,5}.

Table. 1 summarizes the statistics of the four datasets, where

the density refers to the proportion of rated user-item pairs

among all user-item pairs.

We compare the performance of the following algorithms:

SVD [2]: It is a kind of global matrix factorization method,

which has been widely used for rating prediction.

LLORMA [3]: It splits the global rating matrix into many local

matrices and obtains the global ratings through averaging

the rating results from local matrix factorizations.

CLLORMA [6]: It improves the LLORMA by modifying its

random anchor selection.

RWLMA: It is our proposed local matrix approximation based

on graph random walk.

For the hyper-parameter settings, we set the learning rate

 = 0.01, L2-regularization coefficient  = 0.001, maxi-

mum number of iterations as 50 for the matrix factorization.

The number of anchor points is fixed as 50 according to

LLORMA [3] and CLLORMA [6]. For our scheme, we set the

parameters as follows:  = 0.2,  = 0.5 and  = 0.7. All

the experiment results are averaged through the standard

5-fold cross-validation. To evaluate the performance of rating

prediction, we apply the commonly used metrics: Mean Ab-

solute Error (MAE) and Root Mean Square Error (RMSE).

Obviously, the more accurate the prediction, the lower value

of MAE and RMSE.

We observe from our experiments that for LLORMA and

CLLORMA, some user-item pairs in the test datasets cannot

be included into any of their constructed local matrices. In

this case, we use the SVD to predict the ratings for such

dangling pairs. We define a Coveage (Cvg) performance met-

ric:

   

=

, | |
| |

where



is

the

test

dataset

and

 is the set of testing user-item pairs that can be included

into at least one local matrix. Furthermore, as the size of a

local matrix impacts on the local matrix training efficiency,

we define a metric of Normalized Local Matrix Average Size

(NLMAS)

as

  

=

1 | |

·

,  =1

| |



where



is

the training set and  the set of training user-item pairs

in the local matrix of anchor .

Experiment Results: Table 2 presents the experiment

results for the four datasets. We can first observe that the

SVD algorithm performs the worst in almost all cases. This is

not unexpected as it performs a global matrix factorization

over the original user-item rating matrix, which is very sparse

in all the four datasets. All the other methods are based on

1039

Short Research Papers 2A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Table 2: Performances comparison on four datasets

Dataset Metrics SVD LLORMA CLLORMA RWLMA

Ciao

MAE 0.7768
RMSE 1.0341
Cvg ­ NLMAS ­

0.7636 1.70% 1.0090 2.43% 70.63% 84.34%

0.7631 1.76% 1.0084 2.49% 68.74% 92.66%

0.7528 3.09% 0.9781 5.42% 100% 49.16%

MAE 0.7434
ML-100K RMSE 0.9603
Cvg ­ NLMAS ­

0.7099 4.51% 0.9104 5.20% 99.2% 72.92%

0.7111 4.34% 0.9119 5.04% 98.5% 75.04%

0.7074 4.84% 0.9019 6.08% 100% 49.15%

MAE 0.6740
ML-1M RMSE 0.8723
Cvg ­ NLMAS ­

0.6600 2.08% 0.8469 2.91% 99.4% 51.52%

0.6613 1.88% 0.8487 2.71% 99.5% 51.63%

0.6559 2.69% 0.8391 3.81% 100% 48.82%

Zhihu

MAE 0.5796
RMSE 0.8655
Cvg ­ NLMAS ­

0.5814 -0.31% 0.8632 0.27% 97.7% 89.14%

0.5739 0.98% 0.8489 1.92% 95.3% 95.09%

0.5778 0.31% 0.8327 3.79% 100% 48.92%

local matrix construction and factorization, so that the data sparsity can be alleviated in each smaller local matrix to some extent. In these local methods, we further observe that the proposed RWLMA algorithm can achieve the best prediction performance in almost all cases, with one exception that its MAE is slightly larger than that of CLLORMA in the Zhihu Live dataset. The results suggest the superiority of the proposed anchor selection and neighborhood construction based on the graph random walk.
As for the NLMAS metric, we observe that the proposed RWLMA constructs smaller local matrices, compared with the other local matrix methods LLORMA and CLLORMA, which can help to reduce computation complexity in local matrix factorization. Furthermore, for the Coverage of local matrices, we can observe that the proposed RWLMA achieves 100% coverage in all cases, yet the LLORMA and CLLORMA cannot achieve 100% in many cases. They select neighbors from the perspective of anchors based on the distance between anchors and non-anchor pairs and have not provided a mechanism to ensure complete coverage.
Figs. 1 plots the proposed RWLMA performance against the local matrix scale control parameter . It can be observed that as  decreases, the NLMAS becomes smaller, indicating that local matrices become smaller; Furthermore, the MAE and RMSE experience a slight degradation in three datasets, but even perform slightly better in the Ciao dataset. We argue that this slight degradation might be justified by a higher computation efficiency, as local training and prediction can be performed in smaller size local matrices.

SIGIR '19, July 21­25, 2019, Paris, France
Xuejiao Yang and Bang Wang

(a) Varying  of Ciao

(b) Varying  of Zhihu Live

(c) Varying  of Movielens-100K (d) Varying  of Movielens-1M
Figure 1: Performance for different choices of .
4 CONCLUSION
In this paper, we have proposed the RWLMA scheme to solve the classic matrix approximation problem by constructing many small local matrices. In RWLMA, a bipartite graph for user-item pairs is first established. Based on the graph, we have proposed to use the RW for anchor selection and the RWR for neighborhood construction. We then construct local matrices for each of anchors and compute local predictions in each local matrix. Experiments on four datasets have shown that our proposed scheme can outperform the state-of-the-art schemes in terms of lower approximation errors and higher coverage ratios. We notice that the RWLMA is merely based on the original user-item rating matrix. Our future work shall investigate how to integrate some other system information for local matrix construction and factorization.
REFERENCES
[1] A. Beutel, A. Ahmed, and A. J. Smola, "Accams: Additive coclustering to approximate matrices succinctly," in Proceedings of the 24th International Conference on World Wide Web, ser. WWW '15, no. 11, 2015, pp. 119­129.
[2] Y. Koren, R. Bell, and C. Volinsky, "Matrix factorization techniques for recommender systems," Computer, vol. 42, no. 8, pp. 30­37, 2009.
[3] J. Lee, S. Kim, G. Lebanon, Y. Singer, and S. Bengio, "Llorma: local low-rank matrix approximation," The Journal of Machine Learning Research, vol. 17, no. 1, pp. 442­465, 2016.
[4] L. Mackey, A. Talwalkar, and M. I. Jordan, "Divide-and-conquer matrix factorization," in Advances in Neural Information Processing Systems. Curran Associates, Inc., 2011, pp. 1134­1142.
[5] L. Page, S. Brin, R. Motwani, and T. Winograd, "The pagerank citation ranking: Bringing order to the web," Stanford InfoLab, Technical Report 1999-66, 1999.
[6] M. Zhang, B. Hu, C. Shi, and B. Wang, "Local low-rank matrix approximation with preference selection of anchor points," in Proceedings of the 26th International Conference on World Wide Web Companion, ser. WWW '17 Companion, no. 9, 2017, pp. 1395­1403.

1040


Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Encoding Syntactic Dependency and Topical Information for Social Emotion Classification

Chang Wang, Bang Wang, Wei Xiang and Minghua Xu*
Huazhong University of Science and Technology (HUST), Wuhan, China wang chang,wangbang,xiangwei,xuminghua@hust.edu.cn

ABSTRACT
Social emotion classification is to estimate the distribution of readers' emotion evoked by an article. In this paper, we design a new neural network model by encoding sentence syntactic dependency and document topical information into the document representation. We first use a dependency embedded recursive neural network to learn syntactic features for each sentence, and then use a gated recurrent unit to transform the sentences' vectors into a document vector. We also use a multi-layer perceptron to encode the topical information of a document into a topic vector. Finally, a gate layer is used to compose the document representation from the gated summation of the document vector and the topic vector. Experiment results on two public datasets indicate that our proposed model outperforms the state-of-the-art methods in terms of better average Pearson correlation coefficient and MicroF1 performance.

Figure 1: The framework of our proposed model.

CCS CONCEPTS
· Information systems  Sentiment analysis.
KEYWORDS
Social emotion classification; recursive neural network; dependency embedding; topic model
ACM Reference Format: Chang Wang, Bang Wang, Wei Xiang and Minghua Xu. 2019. Encoding Syntactic Dependency and Topical Information for Social Emotion Classification. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/ 3331184.3331287
*Chang Wang, Bang Wang and Wei Xiang are with the School of Electronic, Information and Communications, HUST; Minghua Xu is with the School of Journalism and Information Communication, HUST. This work is supported in part by National Natural Science Foundation of China (Grant No. 61771209)
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331287

1 INTRODUCTION
Some early methods have been proposed for social emotion classification, including the word-emotion models [2], which discover the direct relations between words and emotions based on the features of individual words. However, those word-emotion models cannot distinguish different emotions of a same word in different contexts. Some topic-emotion models [1, 6] utilize topic models to discover topical information of a document and model the relations between topics and emotions. However, they treat each word separately, yet ignoring some inter-word information like semantics.
Recently, some neural network-based models have been proposed for social emotion classification [4]. These models can automatically learn text features and generate a semantical document representation based on the convolutional neural network (CNN) [4] or recurrent neural network (RNN) [9]. But they have not fully utilized the syntactic feature of a sentence and the topical information of a document.
We argue that the syntactic dependency relations between words in a sentence are important for social emotion classification. Furthermore, we also support the claim in the existing work that the document topical information would help to distinguish the emotion of a same word in different contexts. So we design a new neural network model to include the both in document representation for social emotion classification.
Specifically, our proposed model includes a document encoding component and a topic encoding component. Fig. 1 presents the overall network framework. For document encoding, we design a dependency embedded recursive neural network (DERNN) to encode each syntactic dependency

881

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Figure 2: The left part is an example of dependency tree. The right part is the internal structure of the DERNN unit.
relation in between words of a sentence into the sentence vector in the lower layer, and we use a gated recurrent unit (GRU) to encode a document vector in the upper layer. For topic encoding, we apply a multi-layer perceptron (MLP) to transform the document topical distribution obtained from the Latent Dirichlet Allocation (LDA) model into a topic vector. Afterwards, to let the network adaptively decide the importance of the document vector and the topic vector, we propose to use a gate layer to obtain the final document representation from them yet with a gate controlling mechanism. Finally, we feed the document representation into a softmax layer to generate the predicted social emotion distribution. Experiment results on two public datasets have validated the superiority of our proposed model over the state-of-the-art ones in terms of higher average Pearson correlation coefficient and MicroF1 performance.
2 THE PROPOSED METHOD
1) Document Encoding: Since a document consists of one or more sentences, we first encode the syntactic dependency information in one sentence into the sentence vector in the lower layer network and then compose the document vector from sentence vectors in the upper layer network. We design a dependency embedded recursive neural network (DERNN) to model a sentence. It learns an embedding representation for each dependency relation (called dependency embedding) and encodes the syntactic dependency information into the sentence vector.
Before we input words into the network, we adopt a pretrained word2vec model to transform each word into a word embedding, which is a low dimensional dense vector with real values. To construct the DERNN network, we use a dependency analysis tool for each sentence to obtain its dependency tree, where each node represents a word and each edge represents a dependency relation. The left part in Fig. 2 presents an example of dependency tree. Each parent node is connected with one or more child nodes and the dependency relations between them are marked above the edges. We model each node as a DERNN unit and its internal structure is illustrated in the right part of Fig. 2. We can see that the child hidden states 1 and 2, the parent word

embedding  and the dependency embeddings 1 and 2 are input into the parent DERNN unit whose output is the

hidden state  of the parent node. The modeling process

starts from the bottom leaf nodes till the root node, and we

treat the hidden state of the root node as the sentence vector. Let  denote the th word in a sentence and x  R is
its word embedding, () is the child node set of . The
transition functions of the DERNN are as follows.

h~ =

 h

(1)

( )



q~ =

q

(2)

( )

f = (W()x + U()h + D()q + b())

(3)

i = (W()x + U()h~ + D()q~ + b())

(4)

u = (W()x + U()h~ + D()q~ + b())

(5)



h = (i  u +

f  h )

(6)

( )

where   () in Eq. (3), W(,,), U(,,), D(,,), b(,,) are learnable parameters and h  R is the hidden state of . q  R is the dependency embedding of the depen-
dency relation between the th child node and the parent

node. Because leaf nodes have no child nodes, they share an

additional dependency embedding. All dependency embed-

dings are xavier uniform initialized and updated during the

network training. The input gate i and the forget gate f are both dependent on the dependency embeddings. This

allows the DERNN to forget the child words with unim-

portant dependency relations (like the punctuation relation)

and remember the words with important relations (like the

subject-predicate relation), to compose a sentence vector.
Regarding a document as a sequence of sentences, we use a gated recurrent unit to obtain a document vector from sentence vectors. The GRU transition functions are as follows.

z = (W()s + U()h-1 + b())

(7)

r = (W()s + U()h-1 + b())

(8)

u = (W()s + U()(r  h-1) + b()) (9)

h = (1 - z)  h-1 + z  u

(10)

where s is the sentence vector of the th sentence and h  R is its hidden state, and  =  in our work. The hidden state of the last sentence is viewed as the document vector d.
2) Topic Encoding: Considering that social emotion is often related to the document topic, we propose to encode the document topical information into the document representation based on the LDA model, which regards a document as a mixture over various latent topics. After training a LDA model, we obtain the topic probability distribution of each document, denoted as p = {()} =1, where  is the number of topics and  the topic index. p is treated as the original topic representation and then fed into a multi-layer perceptron followed by a  activation layer to get the topic vector t, formulated as follows:

t = (W()p + b())

(11)

where W  R× ,  is the dimension of the topic vector. Notice that in our work,  = . The MLP layer is used to discover the topic feature in the original topic distribution p.

882

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

3) Gate Layer: After obtaining the document vector d

provided by Google3. Stanford Parser4 is used to construct

and the topic vector t, a straightforward strategy is to sum them to get the final document representation. But we argue that the syntactic dependency and topical information are not always the same important for social emotion classification. So we design a gate layer which has a document gate and a topic gate to combine the two vectors. The transition functions of the gate layer are computed as follows.

dependency trees. Since there are too many kinds of dependency relations in the parsing result of Stanford Parser, to balance the occurrence number of each relation, we map all relations into 9 categories according to universal dependencies v15. Other experiment parameters are setting as follows: For SinaNews and ISEAR, the topic number  in LDA is 30

gd = (W()d + U()t + b())

(12)

and 10,  =  =  are 200 and 100, and the learning rate

gt = (W()d + U()t + b()) v = (gd  d + gt  t)

(13)

is both 0.001, the batch size is both 20.

(14)

Evaluation Metrics: Considering label distributions are

where gd is the document gate, gt is the topic gate and W(,), U(,), b(,) are the learnable parameters. 
represents element-wise multiplication. The gate layer allows
the network to adaptively assign different importance to the
document vector and the topic vector, when composing the final document representation v  R ,  =  = .

very imbalanced in the datasets, we adopt the Micro-averaged F1 score (  1) to reflect the accuracy of the predicted top-ranked emotion label [3]. And the average Pearson correlation coefficient ( ) is used to measure the divergence between the predicted emotion probability distribution and the ground truth distribution [3].

4) Social Emotion Classification: In this part, we first add a linear layer to transform the document representation v into a label vector whose dimension is the number of the emotion labels . Then the label vector is fed into a softmax layer to get the predicted probability vector y^.

y^ =  (W()v + b())

(15)

   1 =  I ,
||

{ 1,
I = 0,

if ^ = , , otherwise,

(17) where  is the actual top-ranked emotion triggered by an article  and ^ is the predicted one.  denotes the
testing set.  between the predicted emotion distribution

where W  R× and b  R are the parameters in the

y^ and the ground truth distribution y:

linear layer. Considering the evaluation objective in social emotion
classification is an emotion probability distribution, other than a single most likely emotion label. So for the network training, we use the Kullback-Leibler divergence between the gold emotion distribution y and the predicted emotion distribution y^ as the loss function:

1




(y^, y) = 

(log() - log(^))

(16)

=1

Finally, we train the whole network with the Adam optimizer.



=




(y^, y) ,

(y^, y) =

(y^, y) . (18)

||

(y^)(y)

where  denotes the Pearson correlation coefficient and  denotes the covariance operation.
Comparison models: We compare our proposed model, Gated DR-G-T (Gated DERNN-GRU-Topic), with the following models. The first group contains the baseline models from the literature, including the SWAT [2], Emotion Topic Model (ETM) [1], Contextual Sentiment Topic Model (CSTM) [6], Social Opinion Mining model (SOM) [5], 1-HNN-BTM [3], and CNN and CNN-SVM [5].

3 EXPERIMENT

The second group is the LSTM models implemented by us.Hierarchical LSTM (H-LSTM) is a hierarchical structure

Datasets: We experiment on two public datasets: SinaNews [5] of two LSTM networks which are used for sentence modeling

and ISEAR [7]. SinaNews is a Chinese dataset which contains

and document modeling, respectively. Child-Sum Tree-LSTM

5,258 hot news collected from the social channel of the news

- LSTM (T-LSTM) uses the Child-Sum Tree-LSTM (a tree-

website (www.sina.com). To be consistent with the baseline

structured LSTM proposed by [8]) to model sentences and

methods [5], we use 3,109 articles as the training set and 2,149

the LSTM to model the document.

articles as the testing set. ISEAR is an English dataset which

The third group is the variants of Gated DR-G-T for mod-

consists of 7,666 samples. Each sample is a paragraph of text

el analysis. The RNN-GRU (R-G) uses a recursive neural

tagged by an emotion label. Like [3], we randomly select 60%

network in the lower layer, which removes dependency em-

of the samples as the training set and the remaining 40% as

beddings compared to DERNN. The GRU is used to model

the testing set.

the document. The DERNN-GRU (DR-G) uses the DERN-

Settings: For the Chinese dataset SinaNews, we train a

N proposed in this work to model sentences and the GRU

100-dimensional word2vec model with the Chinese Wikipedia corpus1. The LTP toolkit2 provided by HIT is used for

to model the document. Compared to Gated DR-G-T, the DERNN-GRU-Topic (DR-G-T) does not use the gate layer.

dependency parsing. There are in total 15 dependency rela-

Experimental Results: Table 1 presents the experiment

tions predefined in the LTP, and we define one dependency

results, where our proposed Gated DR-G-T model outper-

embedding for each relation. For the English dataset ISEAR,

forms all the other models. Specifically, compared to the best

we directly use the 300-dimensional English word2vec model

3 https://code.google.com/archive/p/word2vec/

1 https://dumps.wikimedia.org/

4 https://nlp.stanford.edu/software/lex-parser.shtml

2 https://www.ltp-cloud.com/

5 http://universaldependencies.org/docsv1/u/dep/all.html

883

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Experiment results on the two datasets.

Models
SWAT [2] ETM [1] CSTM [6] 1-HNN-BTM [3] CNN [5] CNN-SVM [5] SOM [5]
H-LSTM T-LSTM
R-G DR-G DR-G-T
Gated DR-G-T

SinaNews

MicroF1 AP

38.97 0.40

54.19 0.49

40.74 0.43

-

-

51.23 -

52.63 -

58.59 0.64

61.69 0.68 62.57 0.68

63.55 0.69 64.48 0.70 64.39 0.70

65.20 0.71

ISEAR

MicroF1 AP

26.29 0.21

48.79 0.35

28.23 0.19

51.21 0.40

-

-

-

-

-

-

59.13 0.56 59.98 0.57

60.04 0.57 59.55 0.56 60.06 0.57

60.44 0.57

Figure 3: Comparison between the proposed Gated DR-G-T, its variants, the best LSTM Model and the best model from the literature. The left vertical axis and the bar stand for   1; The right vertical axis and the line stand for  .
model from the literature (cf., Fig. 3), Gated DR-G-T improves MicroF1 by 6.61% and AP by 0.07 on SinaNews, and improves MicroF1 by 9.23% and AP by 0.17 on ISEAR. We attribute the improvements to the leverage of encoding the syntactic dependency and topical information into document representation. We note that the performance of the two LSTM models performs better than the baseline models from the literature. A possible reason is that they adopt a hierarchical structure and can learn the word-level and sentence-level semantic features respectively. Although the T-LSTM model considers the syntactic dependency structure of a sentence, it ignores the specific dependency relation in between words. On the one hand, the DERNN in our model can encode each specific dependency relation into sentence vectors. On the other hand, our model also values the topical information via LDA-based topic encoding. Furthermore, with the help of the gate layer, the final document representation can well balance the syntactic dependency and topical information for social emotion classification.
Fig. 3 compares the results between the Gated DR-G-T and its variants. We first observe that DR-G performs better

than R-G on SinaNews, where the latter does not concern the specific dependency relations. This indicates that encoding syntactic dependency contributes to the performance improvement. But the result of DR-G is worse than R-G in ISEAR. A possible reason is that there are too many short texts in ISEAR and the number of sentences is not enough to well train the dependency embedding. Another observation is that although DR-G-T uses the topical information, it does not show an obvious advantage over DR-G on the two datasets. Although the dependency feature and topical information enrich the semantics of a document representation, they should be carefully integrated. The straightforward summation might add some noises into the final document representation. Since the proposed Gated DR-G-T applies a gate layer to control their weights in the final document representation, the results reveal that it can perform better than the naive DR-G-T.
4 CONCLUSION
In this paper, we have proposed a new neural network model for social emotion classification. For document encoding, the DERNN encodes syntactic dependency relations into sentence vectors and the GRU transforms sentence vectors into a document vector. For topic encoding, the MLP transforms the document topic distribution into a topic vector. Finally, a gate layer is used to compose the final document representation from the gated summation of document vector and topic vector. Experiments on two public datasets show that compared with the state-of-the-art models, the proposed model can improve the classification performance in terms of higher MicroF1 and AP on two public datasets.
REFERENCES
[1] S. Bao, S. Xu, L. Zhang, R. Yan, Z. Su, D. Han, and Y. Yu, "Mining social emotions from affective text," IEEE transactions on knowledge and data engineering, vol. 24, no. 9, pp. 1658­1670, 2012.
[2] P. Katz, M. Singleton, and R. Wicentowski, "Swat-mp: the semeval2007 systems for task 5 and task 14," in Proceedings of the 4th international workshop on semantic evaluations. Association for Computational Linguistics, 2007, pp. 308­313.
[3] X. Li, Y. Rao, H. Xie, R. Y. K. Lau, J. Yin, and F. L. Wang, "Bootstrapping social emotion classification with semantically rich hybrid neural networks," IEEE Transactions on Affective Computing, vol. 8, no. 4, pp. 428­442, 2017.
[4] X. Li, Y. Rao, H. Xie, X. Liu, T.-L. Wong, and F. L. Wang, "Social emotion classification based on noise-aware training," Data & Knowledge Engineering, 2017.
[5] X. Li, Q. Peng, Z. Sun, L. Chai, and Y. Wang, "Predicting social emotions from readers perspective," IEEE Transactions on Affective Computing, no. 1, pp. 1­1, 2017.
[6] Y. Rao, "Contextual sentiment topic model for adaptive social emotion classification," IEEE Intelligent Systems, no. 1, pp. 41­ 47, 2016.
[7] K. R. Scherer and H. G. Wallbott, "Evidence for universality and cultural variation of differential emotion response patterning." Journal of personality and social psychology, vol. 66, no. 2, p. 310, 1994.
[8] K. S. Tai, R. Socher, and C. D. Manning, "Improved semantic representations from tree-structured long short-term memory networks," arXiv preprint arXiv:1503.00075, 2015.
[9] X. Zhao, C. Wang, Z. Yang, Y. Zhang, and X. Yuan, "Online news emotion prediction with bidirectional lstm," in International Conference on Web-Age Information Management. Springer, 2016, pp. 238­250.

884


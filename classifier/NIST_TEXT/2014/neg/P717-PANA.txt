Click-through-based Cross-view Learning for Image Search

Yingwei Pan 1, Ting Yao , 2,3 Tao Mei 4, Houqiang Li 1, Chong-Wah Ngo , 2,3 Yong Rui 4
1 University of Science and Technology of China, Hefei, China 2 City University of Hong Kong, Kowloon, Hong Kong
3 Shenzhen Research Institute, City University of Hong Kong, Shenzhen, China 4 Microsoft Research, Beijing, China
{panyw, tingyao}.ustc@gmail.com; lihq@ustc.edu.cn;
cscwngo@cityu.edu.hk; {tmei, yongrui}@microsoft.com

ABSTRACT
One of the fundamental problems in image search is to rank image documents according to a given textual query. Existing search engines highly depend on surrounding texts for ranking images, or leverage the query-image pairs annotated by human labelers to train a series of ranking functions. However, there are two major limitations: 1) the surrounding texts are often noisy or too few to accurately describe the image content, and 2) the human annotations are resourcefully expensive and thus cannot be scaled up.
We demonstrate in this paper that the above two fundamental challenges can be mitigated by jointly exploring the cross-view learning and the use of click-through data. The former aims to create a latent subspace with the ability in comparing information from the original incomparable views (i.e., textual and visual views), while the latter explores the largely available and freely accessible click-through data (i.e., "crowdsourced" human intelligence) for understanding query. Specifically, we propose a novel cross-view learning method for image search, named Click-through-based Crossview Learning (CCL), by jointly minimizing the distance between the mappings of query and image in the latent subspace and preserving the inherent structure in each original space. On a large-scale click-based image dataset, CCL achieves the improvement over Support Vector Machinebased method by 4.0% in terms of relevance, while reducing the feature dimension by several orders of magnitude (e.g., from thousands to tens). Moreover, the experiments also demonstrate the superior performance of CCL to several state-of-the-art subspace learning techniques.
 This work was performed when Yingwei Pan and Ting Yao were
visiting Microsoft Research as research interns. The first two authors contributed equally to this work.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6-11, 2014, Gold Coast, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609568.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Retrieval models
General Terms
Algorithm, Experimentation.
Keywords
Image search, cross-view learning, subspace learning, clickthrough data, DNN image representation.
1. INTRODUCTION
Keyword-based image search has received intensive research attention since the early of 1990s [20]. The significance of the topic can be partly reflected from the huge volume of published papers, particularly for addressing the problems of learning the rank or similarity functions. Despite these efforts, the fact that the queries (texts) and search targets (images) are of two different modalities (or views) has resulted in the open problem of "semantic gap." Specifically, a query in the form of textual keywords is not directly comparable with the visual content of images. The commercial search engines to date primarily reply on textual features extracted from the surrounding texts of images. However, the text description might not fully depict the salient aspect of visual content, not to mention that some images actually do not come along with any text description. One feasible solution is learning image rankers from the query-image pairs labeled by human subjects. However, the labeling process is generally time consuming, and in practice difficult to ensure the quality of labels. Furthermore, as the user search intents are not likely to always align with these pre-defined labels, image rankers used to suffer from the poor generalization performance.
Inspired by the success of multi-view embedding [31], this paper studies the cross-view (i.e., text to image views) search problem by learning a common latent subspace that allows direct comparison of text queries and images. Specifically, by mapping to the latent subspace, the relevance or similarity between a textual query and an image can be directly measured between their projections, making the information from the original incomparable cross-view space comparable in the shared subspace. In addition, the dimensionality of the latent subspace is significantly reduced compared with

717

that of any input view, making the memory costs much saved for existing search systems.
Moreover, we consider exploring user click-through data, aiming to bridge the user intention gap for image search. In general, image rankers obtain training data by manually labelling the relevance of query-image pairs. However, it is difficult to fathom user intent based on the queries, especially for those ambiguous queries. For example, given the query "mustang cobra," experts tend to label the images of animal "mustang" and "cobra" as highly relevant. However, empirical evidence suggests that most users wish to retrieve images of a car of brand "mustang cobra." The experts' labels therefore could be erroneous. This will bias the training set and the ranker will be learned sub-optimal. On the other hand, the click-through data provide an alternative to address this problem. In an image search engine, users browse image search results before clicking a specific image. The decision to click is likely dependent on the relevance of an image. Therefore, the click data can serve as a reliable and implicit feedback for image search. We hypothesize that, most of the clicked images are relevant to the given query judged by the real users.
By jointly integrating cross-view learning and click-through data, this paper presents a novel Click-through-based Crossview Learning (CCL) approach to image search, as shown in Figure 1. Specifically, a bipartite graph between the user queries and images is constructed based on the search logs from a commercial image search engine. An edge between a query and an image is established when the users who issued the query clicked the image. Moreover, the textual and visual space is formed by constructing a graph on each view, respectively. The link between every two nodes in each space represents the query or image similarity. The spirit of CCL is to learn a latent subspace in the way of minimizing the distance between the mappings of query and image, while preserving the inherent structure in each original space. After the optimization of subspace learning, the relevance score between a query and an image in the original spaces can be directly computed based on their mappings. For any query, the image search list will be returned by sorting their relevance scores with the query.
In summary, this paper makes the following contributions:
· We study the problem of keyword-based image search by jointly exploring cross-view learning and the use of click-through data. To the best of knowledge, this paper represents one of the first efforts towards this target in the information retrieval research community.
· We propose a novel click-through-based cross-view learning (CCL), which aims to learn a latent subspace by simultaneously minimizing the distance between the mappings of query and image in the latent subspace, and preserving the structure in each original space. By mapping to the subspace, text queries and visual images can be directly compared.
· We evaluate the proposed click-through based image search approach on a large-scale click-based image dataset with over 23 millions of log records, which were sampled from one-year click data of a commercial image search engine.
The remaining sections are organized as follows. Section 2 describes related work on multi-view embedding and the

use of click data, while Section 3 presents our click-throughbased cross-view learning method. Section 4 provides empirical evaluations, followed by the discussions and conclusions in Section 5.
2. RELATED WORK
We briefly group the related work into two categories: multi-view embedding, and search by using click data. The former draws upon research in integrating multiple views to improve learning performance by exploiting either the consensus or the complementary principle, while the latter investigates Web search by mining click-through data.
2.1 Multi-view Embedding
The research in this direction has proceeded along three dimensions: co-training [16][22][33], subspace learning [2][9][25], and multi-kernel learning [5][14][17].
Co-training seeks consensus on two distinct views of the data. Muslea et al. combined active learning with cotraining and proposed robust semi-supervised learning algorithms [22]. Yu et al. developed a Bayesian undirected graphical model for co-training and a novel co-training kernel for Gaussian process classifiers [33]. Kumar et al. advanced co-training for data clustering and designed effective algorithms for multi-view data [16]. The idea of subspace learning is similar to co-training except the consensus is solved by learning a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Canonical correlation analysis (CCA) [9], a classical technique, explored the mapping matrices by maximizing the correlation between the projections in the subspace. Similarly, Partial Least Squares (PLS) also aims to model the relations between two or more sets of data by projecting them into the latent subspace [25]. The difference between CCA and PLS is that CCA utilizes cosine as the similarity function while PLS learns dot product. Later in [2], polynomial semantic indexing (PSI) is performed by learning two low-rank mapping matrices in a learning to rank framework, and then a polynomial model is considered to measure the relevance between query and document.
Different from co-training and subspace learning, multikernel learning exploits different kernels to different views and fuses them either linearly or non-linearly for exploring complementary properties of different views. In [17], a linear (or convex) combination of a set of predefined kernels were learned to identify a good target kernel for the applications. Later in [5], Kernel target alignment was proposed to learn the entries of a kernel matrix by using the outer product of the label vector as the ground-truth. Kloft et al. extended the multi-kernel learning framework to arbitrary lp-norm by adding a regularizer over the mixing coefficients [14].
In summary, our work belongs to subspace learning. Different from these aforementioned subspace learning methods, our approach contributes by studying not only forming the shared latent subspace with the standard objective of subspace learning (i.e., the consensus between views is maximized) but also preserving the inherent structure in each original space.
2.2 Search by Using Click Data
Click-through data has been studied and analyzed widely with different Web mining techniques for improving the efficacy and usability of search engines. The use of the click-

718

W

W

q

v

... ...

Figure 1: Click-through-based image search framework (better viewed in color). (a) Latent subspace learning between textual query and visual image: click-through-based cross-view learning by simultaneously minimizing the distance between the query and image mappings in the latent subspace (weighted by their clicks) and preserving the inherent structure in each original feature space. (b) With the learned mapping matrices Wq and Wv, queries and images are projected into this latent subspace and then the distance in the latent subspace is directly taken as the relevance of query-image.

through data for query clustering was suggested by Befferman and Berger [3], who proposed an agglomerative clustering technique to identify related queries and Web pages. Wen et al. combined query content information and clickthrough information and applied a density-based method to cluster queries [28]. Mei et al. proposed an approach to query suggestion by computing the hitting time on a click graph [19]. Li et al. presented the use of click graphs in improving query intent classifiers [18].
There are also several approaches that have tried to model the representation of queries or documents on the clickthrough bipartite. In [1], the authors introduced another vectorial representation for the queries without considering the content information. Queries were represented as points in a high dimensional space, where each dimension corresponds to a unique URL. The weight assigned to each dimension was equal to the click frequency. Poblete et al. proposed the query-set document model by mining frequent query patterns to represent documents rather than the content information of the documents [24].
In addition, click-through data have also been used to learn the rank function [12]. Joachims et al. observed the relationship between clicked links and the relevance of the target pages by an eye tracking experiment [13]. Wu et al. formalized the learning of similarity as learning of mappings that maximize the similarities of query-documents pairs from the click-through bipartite graph [30]. For image search, click-through data has been found to be very reliable [6][11]. In [6], Craswell et al. built a query-image click graph and performed backward random walks to determine a probability distribution over images conditioned on the given query. In [11], Jain et al. reranked the image search results so as to promote images that are likely to be clicked to the top of the ranked list. Later in [27], an in-depth analysis of several ranking algorithms was performed on Flickr user log data to investigate the importance of many factors,

including internal and external image popularity, the overall attentions, diversity, semantic categories and visual appearance. In [23], Pan et al. employed neighborhood graph search to find the nearest neighbors on an image similarity graph and further aggregated their clicked queries/click counts to get the labels of the new image. In another work by Yao et al. [32], by combining click-through and video document features for deriving a latent subspace, the dot product of the mappings in the latent subspace is taken as the similarity between videos and the similarity is further applied for video tagging tasks.
Most of the above approaches focus on leveraging both the click data and the features only from the textual view. Our work is different that we aim to compute the distance between the textual query and visual features from two different views on the observed query-image pairs and apply the learned distance for image search purpose.
3. CLICK-THROUGH-BASED CROSS-VIEW
LEARNING
The main goal of click-through-based cross-view learning is to construct a latent common subspace with the ability of directly comparing textual query and image content. The training of CCL is performed simultaneously by minimizing the distance between query and image mappings in the latent subspace weighted by their clicks, and preserving the structure relationships between the training examples in the original feature space. In particular, the objective function of CCL is composed of two components, i.e., distance between views in the latent subspace, and the structure preservation in the original space. After we obtain the latent subspace, the relevance between query and image is directly measured by their mappings. The approach overview is shown in Figure 1.
In the following, we will first define the bipartite graph that naturally encodes user actions in the query log, followed

719

by constructing the two learning components of CCL. Then the joint overall objective and its optimization strategy are provided. Finally, the whole algorithm for image search is presented. It is worth noticing that although the two views here are visual (image) and textual (query), our approach is applicable to any other domain.

3.1 Notation
Let G = (V, E) denote a click-through bipartite. V = Q  V is the set of vertices, which consists of a query set Q and an image set V . E is the set of edges between the query and image vertices. The number associated with the edge represents the clicked times in the image search results of the query. Suppose there are n triads {qi, vi, ci}ni=1 generated from the click-through bipartite in total, where ci is the click counts of image vi in response to query qi. Let Q = {q1, q2, . . . , qn}  Rn×dq and V = {v1, v2, . . . , vn}  Rn×dv denote the query and image feature matrix, where qi and vi are the textual and visual feature of query qi and image vi, and dq and dv are the feature dimensionality, respectively. The click matrix C is a diagonal n × n matrix with its diagonal elements as ci. Please note that the query qi and image vi may not be unique in each view as one single query can correspond to multiple clicked images.

3.2 Cross-view Distance
We assume that a low-dimensional common subspace exists for the representation of query and image. The linear mapping function can be derived from this subspace by

f (qi) = qiWq, and f (vi) = viWv,

(1)

where d is the dimensionality of the common subspace, and Wq  Rdq×d and Wv  Rdv×d are the transformation matrices that project the query textual semantics and image content into the common subspace, respectively.
To measure the relations between the textual query and image visual content, one natural way is to measure the distance between their mappings in the latent subspace as

min tr (QWq - VWv)C(QWq - VWv)

Wq ,Wv

(2)

s.t. WqWq = I, WvWv = I

where tr(·) denotes the trace function. The matrices Wq and Wv have orthogonal columns, i.e., WqWq = WvWv = I, where I is an identity matrix. The constrains restrict Wq and Wv to converge to reasonable solutions rather than go to 0 which is meaningless in practice.
Specifically, we view the click number of a query and an image as an indicator of their relevance. As most image search engines display results as thumbnails. The users can see the entire image before clicking on it. As such, barring distracting images and intent changes, users predominantly tend to click on images that are relevant to their query. Therefore, click data can serve as a reliable connection between the queries and images. The underlying assumption is that the higher the click number, the smaller the distance between the query and the image in the latent subspace. To learn this shared latent subspace, we intuitively incorporate the distance as a regularization on the mapping matrices Wq and Wv weighted by the click numbers.

3.3 Structure Preservation
Structure preservation or manifold regularization has been shown effective for semi-supervised learning [21] and multi-

view learning [7]. This regularizer indicates that similar points in the original space should be mapped to the positions closely in the shared latent subspace. The estimation of the underlying structure can be measured by the appropriate pairwise similarity between the training samples. Specifically, it can be given by

n

n

Sqij qiWq - qj Wq 2+

Svij viWv - vj Wv 2,

i,j=1

i,j=1

(3) where Sq  Rn×n and Sv  Rn×n denote the affinity matri-

ces defined on the queries and images, respectively. Under

the structure preservation criterion, it is reasonable to mini-

mize Eq.(3), since it will incur a heavy penalty if two similar

examples are mapped far away.

There are many ways of defining the affinity matrices Sq

and Sv. Inspired by [7], the elements are computed by Gaus-

sian functions in this work, i.e.,



2

 Sitj =

-
e

ti -tj t2

if ti  Nk(tj ) or tj  Nk(ti) ,



0

otherwise

(4)

where t  {q, v} for simplicity, i.e., t can be replaced by any

one of q and v. t is the bandwidth parameters. Nk(ti)
represents the set of k nearest neighbors of ti. By defining the graph Laplacian Lt = Dt - St for t 
{q, v}, where Dt is a diagonal matrix with its elements de-

fined as Dtij = j Stij , Eq.(3) can be rewritten as

tr (QWq)Lq(QWq) + tr (VWv)Lv(VWv) . (5)

By minimizing this term, the similarity between examples in the original space can be preserved in the learned latent subspace. Therefore, we add this regularizer in our framework for optimization.

3.4 Overall Objective
The overall objective function integrates the distance between views in Eq.(2) and structure preservation in Eq.(5). Hence we get the following optimization problem

min tr (QWq - VWv)C(QWq - VWv)
Wq ,Wv
+ tr (QWq)Lq(QWq) + tr (VWv)Lv(VWv) , s.t. WqWq = I, WvWv = I (6)
where  is the tradeoff parameter. The first term is the crossview distance, while the second term represents structure preservation.
For simplicity, we denote L(Wq, Wv) as the objective function in Eq.(6). Thus, the optimization problem can be rewritten as

min L(Wq, Wv), s.t. WqWq = I, WvWv = I.
{Wq ,Wv }
(7) The optimization above is a non-convex problem. Nevertheless, the gradient of the objective function with respect to Wq and Wv can be easily obtained as follows:

Wq L(Wq, Wv) = 2QC(QWq - VWv) + 2QLqQWq Wv L(Wq, Wv) = 2VC(VWv - QWq) + 2VLvVWv

.

(8)

720

3.5 Optimization
To address the difficult non-convex problem in Eq.(7) due to the orthogonal constrains, we use a gradient descent optimization procedure with curvilinear search [29] for a local optimal solution in this work.
In each iteration of the gradient descent procedure, given the current feasible mapping matrices {Wq,Wv} and their corresponding gradients {Gq = Wq L(Wq,Wv), Gv = Wv L(Wq,Wv)}, we define the skew-symmetric matrices Pq and Pv as

Pq = GqWq - WqGq , Pv = GvWv - WvGv . (9)

The new point can be searched as a curvilinear function of a step size  , such that

Fq( )

=

(I

+

 2

Pq

)-1

(I

-

 2

Pq

)Wq

,

Fv( )

=

(I

+

 2

Pv

)-1

(I

-

 2

Pv

)Wv

.

(10)

Then, it is easy to verify that Fq( ) and Fv( ) lead to several characteristics. The matrices Fq( ) and Fv( ) satisfy (Fq( ))Fq( ) = (Fv( ))Fv( ) = I for all   R. The derivatives with respect to  are given as

Fq( )

=

-(I

+

 2

Pq

)-1

Pq

(

Wq

+Fq 2

(

)

)

Fv( )

=

-(I

+

 2

Pv

)-1

Pv

(

Wv

+Fv 2

(

)

)

.

(11)

In particular, we can obtain Fq(0) = -PqWq and Fv(0) = -PvWv. Then, {Fq( ), Fv( )}0 is a descent curve. We use the classical Armijo-Wolfe based monotone curvilinear search algorithm [26] to determine a suitable step  as one satisfying the following conditions:

L(Fq( ), Fv( ))  L(Fq(0), Fv(0))

+1 L (Fq(0), Fv(0)),

(12)

L (Fq( ), Fv( ))  2L (Fq(0), Fv(0)),

where 1 and 2 are two parameters satisfying 0 < 1 < 2 < 1. L (Fq( ), Fv( )) is the derivative of L with respect to  and is calculated by

L (Fq( ), Fv( )) =

-

tr

Rt( )(I

+

 2

-1
Pt)

Pt

Wt + Ft( ) 2

t{q,v}

, (13)

where Rt( ) = Wt L(Fq( ), Fv( )) for t  {q, v}. In particular, we have

L (Fq(0), Fv(0)) = -

tr Gt (GtWt - WtGt )Wt

t{q,v}

=

-

1 2

Pq

2 F

-

1 2

Pv

2 F

. (14)

Please refer to [29] for the theoretical proof details of curvilinear search algorithm.

3.6 CCL Algorithm
After the optimization of Wq and Wv, we can obtain the linear mapping functions defined in Eq.(1). With this, original incomparable textual query and visual image become comparable. Specifically, given a test query-image pair (q^  Rdq , v^  Rdv ), we compute the distance value between

Algorithm 1 Click-through-based Cross-view Learning (CCL)

1: Input: 0 < µ < 1, 0 < 1 < 2 < 1,   0, and initial

Wq and Wq.

2: for iter = 1 to Tmax do

3: compute gradients Gq and Gv via Eq.(8).

4:

if

Gq

2 F

+

Gv

2 F



then

5:

exit.

6: end if

7: compute Pq and Pv by using Eq.(9). 8: compute L (Fq(0), Fv(0)) according to Eq.(14).

9: set  = 1.

10: repeat

11:

 = µ

12:

compute Fq( ) and Fv( ) via Eq.(10).

13:

compute L (Fq( ), Fv( )) via Eq.(13).

14: until Armijo-Wolfe conditions in Eq.(12) are satisfied

15: update the transformation matrices:

Wq = Fq( )

Wv = Fv( )

16: end for

17: Output:

distance function: q^, v^, r(q^, v^) = q^Wq - v^Wv 2.

the pair as

r(q^, v^) = q^Wq - v^Wv 2.

(15)

This value reflects how relevant the query could be used to describe the given image, with lower numbers indicating higher relevance. For any query, sorting by its corresponding values for all its associated images gives the retrieval ranking for these images. The algorithm is given in Algorithm 1.

3.7 Complexity Analysis

The time complexity of CCL mainly depends on the com-

putation of Gq, Gv, Pq, Pv, Fq( ), Fv( ), and L (Fq( ), Fv( )).

Obviously, the computation complexity of Gq and Gv is

O(n2 × dq) and O(n2 × dv), respectively. Pq and Pv take

O(d2q × d) and O(d2v × d).

The

matrix

inverse

(I +

 2

Pq

)-1

and

(I +

 2

Pv

)-1

dom-

inate the computation of Fq( ) and Fv( ) in Eq.(10). By

forming Pq and Pv as the outer product of two low-rank ma-

trices, the inverse computation cost decreases a lot. As de-

fined in Eq.(9), Pq = GqWq - WqGq and Pv = GvWv -

WvGv , Pq and Pv can be equivalently rewritten as Pq = XqYq and Pv = XvYv, where Xq = [Gq, Wq], Yq =

[Wq, -Gq] and Xv = [Gv, Wv], Yv = [Wv, -Gv]. Accord-

ing to Sherman-Morrison-Woodbury formula, i.e.,

(A + XY)-1 = A-1-A-1X(I + YA-1X)-1YA-1,

the

matrix

inverse

(I

+

 2

Pq

)-1

can

be

re-expressed

as

(I

+

 2

Pq

)-1

=

I

-

 2

Xq

(I

+

 2

Yq

Xq

)-1

Yq

.

Furthermore, Fq( ) can be rewritten as

Fq( )

=

Wq

-

 Xq(I

+

 2

Yq

Xq

)-1

Yq

Wq

.

For Fv( ), we can get the corresponding conclusion. S-

ince we typically have d  dq, the cost of inverting (I +

 2

Yq Xq )



R2d×2d

is

much

lower

than

inverting

(I +

 2

Pq )



Rdq ×dq .

The

inverse

of

(I

+

 2

Yq

Xq

)-1

takes O(d3), thus

721

the computation complexity of Fq( ) is O(dqd2) + O(d3). Similarly, Fv( ) is O(dvd2) + O(d3). The computation of L (Fq( ), Fv( )) has a cost of O(n2 × dq) + O(n2 × dv) + O(dqd2) + O(dvd2) + O(d3).
As d  dq, dv  n, the overall complexity of the Algorithm 1 is Tmax × T × O(n2 × max(dq, dv)), where T is the number of searching for appropriate  which satisfies the Armijo-Wolfe conditions and it is usually less than ten in our experiments. Take the training of Wq and Wv on one million {query, image, click} triads with dv = 1, 024 and dq = 10, 000 for example, our algorithm takes about 32 hours on a server with 2.40GHz CPU and 128GB RAM.
3.8 Extensions
Although we only present the distance function between query and image on the learned mapping matrices in the Algorithm 1, the optimization actually can also help learning of query-query and image-image distance. Similar to the distance function between query and image, the distance between query and query, image and image, is computed as (q^, q¯, r(q^, q¯) = q^Wq - q¯Wq 2) and (v^, v¯, r(v^, v¯) =
v^Wv - v¯Wv 2), respectively. Furthermore, the obtained distance can be applied for several IR applications, e.g., query suggestion, query expansion, image clustering, image classification, and so on.
4. EXPERIMENTS
We conducted our experiments on the Clickture dataset [10] and evaluated our approaches for image search.
4.1 Dataset
The dataset, Clickture, is a large-scale click based image dataset [10]. It was collected from one year click-through data of one commercial image search engine. The dataset comprises two parts, i.e., the training and development (dev) sets. The training set consists of 23.1 million {query, image, click} triads, where query is a textual word or phrase, image is a base64 encoded JPEG image thumbnail, and click is an integer which is no less than one. There are 11.7 millions distinct queries and 1.0 million unique images of the training set. Figure 2 shows a few exemplary images with their clicked queries and click counts in the Clickture. For example, users clicked the first image 146 times in the search results when submitting query "obama" in total. It is worth noting that there is no surrounding text or description of images provided in the Clickture.
In the dev dataset, there are 79,926 query, image pairs generated from 1,000 queries, where each image to the corresponding query was manually annotated on a three point ordinal scale: Excellent, Good, and Bad. In the experiments, the training set is used for learning the latent subspace, while the dev set is used for performance evaluation.
4.2 Experimental Settings
Task. We investigate whether our proposed approach can be used to improve image search in this work. Specifically, we use Clickture as "labeled" data for semantic queries and train the ranking model. The task is to estimate the relevance of the image and the query for each test query-image pair, and then for each query, we order the images based on the prediction scores returned by our trained ranking model.
Textual and Visual Features. We take the word in queries as "word features." Words are stemmed and stop

¡ ¢£¢¥ §¤¦

¢& ¢¡$ ¢¨¨©' ©¨¥§ ¢% ¢¥ §

¡ ¢£¢ ¨©¥ 

(%¢$ ¢¨¨©'¥ §

¢%¥ ¦

¢¢ ¡ ¢£¢¥ ¤

¢$& ¢¨¨©'¥ 

¢% ¢¥ §)

¨© ¡ ¢¢ ¡ ¢£¢¥ §! ¡¡# ¢¨¨©'¥ §

¢% ¢ #©$& ¨©¥ §

¨"¡¡ ¡ ¡ ¢£¢ # ¢©$%¥ § "¡(©#¢& ¨¢& ¡¡# ¢$#

¢% ¢ £¡0©¥ §

¨"¡¡ ¡ ¢£¢¥ §

¢¨¨©'¥ §

¢% ¡£ ¢¥ ¤

¨© ¡ ¡ ¢£¢¥ !

¢¡$ ¢¨¨©' ©¨¥ §

( ¡$ 1¢£¥ §! ( ¡$ 1¢£ #¢2©$%¥ § ( ¡$ 1¢£ "¡¥ § ( ¡$ 1¢£ ©£¢%¥  ( ¡$ 1¢£ ¨©¥ 5 ( ¡$ 1¢£ ¨(¢&©$% ¢((¥ §

" #(( 2©¡$©$¥ § ¡¨  2©¡$©$ #((¥ § 2© #((¥ § 2©¡$©$ #((¥ §) 2©¡$©$ #(( ¨¡¥§ 2©¡$©$#((¥ §

"©£¢ ¡¡©¥ 3) "©£¢ ¡¡© ¡ ©#¥  "©£¢ ¡¡© ©£¢%¥ ! "©£¢ ¡¡© ¨©¥ § "©£¢ ¨©¥ ¦¦

Figure 2: Examples in Clickture dataset (upper row: clicked images; lower row: search query with click times on the upper image).

words are removed. With word features, each query is represented by a tf vector in the query space. In our experiments, we use the top 10,000 most frequent words as the word vocabulary. Inspired by the success of deep neural networks (DNN) [4], we use it to generate image representation in this work, which is a 1024-dimensional feature vector. Specifically, similar to [15], the used DNN architecture is denoted as Image - C64 - P - N - C128 - P - N - C192 - C192 - C128 - P - F 4096 - F 1024 - F 1000, which contains five convolutional layers (denoted by C following the number of filters) while the last three are fully-connected layers (denoted by F following the number of neurons); the max-pooling layers (denoted by P ) follow the first, second and fifth convolutional layers; local contrast normalization layers (denoted by N ) follow the first and second max-pooling layers. The weights of DNN are learned on ILSVRC-20101, which is a subset of ImageNet2 dataset with 1.26 million training images from 1,000 categories. For an image, its representation is the neuronal responses of the layer F 1024 by input the image into the learned DNN.
Compared Approaches. We compare the following approaches for performance evaluation:
· N-Gram SVM Modeling (N-Gram SVM). We use all the clicked images of a given query as positive samples and randomly select negative samples from the rest of the training dataset to build a support vector machine (SVM) model for each query, and then use this model to predict the relevance of the query to a new image. In addition, in order to extend the capability of the training data to model queries that are not covered in the dataset, n-gram modeling, which attempts to model each n-gram as a "query," is used. In other words,
1 http://www.image-net.org/challenges/LSVRC/2010/ 2 http://www.image-net.org/

722

Average OVerall Objective Value

100

Dim=40

Dim=80

Dim=120

Dim=160

90

80

70

60

50

40

30

20

10

0 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120
Number of Iteration

Figure 3: The average overall objective value of Eq. (6) for each query-image pair with the increase of the iteration. The changes of the value are given at different dimensionality of the latent subspace.

if a query is not in the training set, but its n-grams appear in some queries of the training set, we can generate the model by linearly fusing the SVM models of these queries. No latent subspace is learned in this baseline. We name this run as N-Gram SVM.
· Canonical Correlation Analysis [8][9] (CCA). A classical and successful approach for mapping visual and textual features into a latent subspace where the correlation between the two views is maximized. This run is named as CCA.
· Partial Least Squares [25][30] (PLS). Similar to CCA, PLS aims to learn linear mapping functions to project two views into a common latent subspace as well. But different from CCA, PLS learns dot product as the similarity function while cosine similarity is used in CCA. Deriving from the ideas in [30], the learning of the mappings is performed by maximizing the similarities of the observed query-image pairs on the click-through data here. We name this run as PLS.
· Polynomial Semantic Indexing [2][32] (PSI). Similar in spirit, PSI first chooses a low dimensional feature representation space for query and image, and then a polynomial model is discriminatively learned for mapping the query-image pair to a relevance score. This run is named as PSI.
· Click-through-based Cross-view Learning (CCL). We designed the run, CCL, for our proposed approach described in Algorithm 1.
Parameter Settings. N-Gram SVM is a baseline without low-dimensional latent subspace learning, thus the relevance score is predicted on the original visual features. For the other four subspace learning methods, the dimensionality of the latent subspace is in the range of {40, 80, 120, 160}. The k nearest neighbors preserved in Eq.(4) is chosen within {100, 500, 1000, 1500, 2000}. The tradeoff parameter  in the overall objective function is set within {0.1, 0.2, ..., 1.0}. We set µ=0.3, 1=0.2, and 2=0.9 in the curvilinear search by using a validation set.
Evaluation Metrics. For the evaluation of image search, we adopted Normalized Discounted Cumulative Gain (N DCG) which takes into account the measure of multi-level relevancy as the performance metric. Given an image ranked list,

0.59

N-Gram SVM (1,024 D) PLS (80 D) PSI (80 D) CCA (80 D) CCL (80 D)

0.57

0.55

0.53

0.51

0.49

0.47

0.45 NDCG@1

NDCG@5 NDCG@10 NDCG@15 NDCG@20 NDCG@25

Figure 4: The NDCG of different approaches for image search. The numbers in the brackets represent the feature dimension used in each approach.

the N DCG score at the depth of d in the ranked list is defined by:

N DCG@d = Zd

d

2rj - 1

j=1 log(1 + j)

(16)

where rj = {Excellent = 3, Good = 2, Bad = 0} is the manually judged relevance for each image with respect to the query. Zd is a normalizer factor to make the score for d Excellent results 1. The final metric is the average of N DCG@d for all queries in the test set.

4.3 Optimization Analysis
As we choose the step  satisfying the Armijo-Wolfe conditions to achieve an approximate minimizer of L(Fq( ), Fv( )) in Algorithm 1 instead of finding the global minimization due to its computationally expense, we depict the average overall objective value of Eq.(6) for one query-image pair versus iterations to illustrate the convergence of the algorithm. As shown in Figure 3, the value does decrease as the iterations increase at all the dimensionality of the latent subspace. Specifically, after 100 iterations, the average objective value between query mapping and image projection is around 10 when the latent subspace dimension is 40. Thus, the experiment verifies that our algorithm can always reach a reasonable local optimum.

4.4 Performance Comparison
Figure 4 shows the NDCG performances on image search of five runs averaged over 1,000 queries in Clickture dev dataset. It is worth noting that the prediction of N-Gram SVM is performed on the original image visual features of 1,024 dimensions and for other four methods, the performances are given by choosing 80 as the dimensionality of the latent subspace.
Overall, our proposed CCL consistently outperforms the other runs across different depths of NDCG. In particular, the NDCG@10 of CCL can achieve 0.5738, which makes the improvement over N-Gram SVM model by 4.0%. More importantly, by learning a low-dimensional latent subspace, the dimension of the mappings of textual query and visual image is reduced by several orders of magnitude. Furthermore, CCL by additionally incorporating structure preservation leads to a performance boost against PLS and CCA. The result basically indicates the advantage of minimizing distance between views in the latent subspace and preserving similarity in the original space simultaneously.

723

DEFGHI PQR

UWP

UPV

SST

SSW

DEFGHI PQR

UWP

UPV

SST

SSW

DEFGHI PQR

UWP

UPV

SST

SSW

DEFGHI PQR

UWP

UPV SST SSW

cde fghidpq rstud

cte qsvwxp dprysu rdtph

cre sfxp trrvx

46 789@AA@BC X6FYY` a6bH` cwe gfp drxh

Figure 5: Examples showing the top 10 image search results by different methods of queries "mustang cobra," "golden anchor cabins," "women bicycle," and "pumpkin faces" (better viewed in color). The relevance scale is provided at the top left corner for each image.

There is a performance gap between CCA and PLS. Though both runs attempt to learn linear mapping functions for forming a subspace, they are different in the way that CCA learns cosine as a similarity function, and PLS learns dot product instead. As indicated by our results, maximizing the correlation between the mappings in the latent subspace can lead to a better performance. Moreover, PSI utilizing click-through data as relative relevance judgements rather than absolute click numbers is superior to PLS, but is still lower than CCL. Another observation is that the performance gain is almost consistent when going deeper into the list. This further confirms the effectiveness of CCL.
Figure 5 shows the top 10 image search results by different approaches for the query "mustang cobra," "golden anchor cabins," "women bicycle," and "pumpkin faces." We can easily see the proposed CCL method gets the most satisfying ranking results. Specifically, compared to other baselines, the top images by CCL are more visually similar to each other, especially of the query "women bicycle" and "pumpkin faces." That is mainly caused by the effect of structure preservation regularization term in the overall objective, which restricts the similar images in the original space to remain close in the low-dimensional latent subspace. Therefore, the ranks of these group of images are likely to be moved up.
4.5 Effect of the Dimensionality of the Latent Subspace
In order to show the relationship between the performance and the dimensionality of the latent subspace, we compared

0.50

PLS

PSI

CCA

CCL

0.49

0.48

NDCG@25

0.47

0.46

0.45 Dim=40

Dim=80

Dim=120

Dim=160

Figure 6: The NDCG@25 performance with different dimensionalities of the latent subspace. We can see that CCL achieves the best performance among the four methods.

the results of the dimension in the range of 40, 80, 120, and 160. As the method N-Gram SVM performs training and prediction by only using the original features rather than learning a latent subspace, it is excluded in this comparison.
The results are shown in Figure 6. Compared to the other three runs, performance improvement is consistently observed at each dimensionality of the latent subspace by our proposed CCL method. Furthermore, CCL achieves the best result at the latent subspace dimensionality of 80, and the results at other dimensionality are pretty close to the best one. This observation basically verifies that CCL has a good

724

NDCG@25 NDCG@25

0.495 0.494 0.493 0.492 0.491 0.490 0.489 0.488

Dim=40

Dim=80

Dim=120

Dim=160

k=100

k=500

k=1,000

k=1,500

k=2,000

Figure 7: The NDCG@25 performance curve at different dimensionalities of the latent subspace with different numbers of nearest neighbors.

property of being affected very slightly with the change of the dimensionality of the latent subspace.
Another important observation is that when the dimensionality of the latent subspace increases, the performances of all the methods are not always improved accordingly. For example, the best performance of CCL happens at the dimensionality of 80 and for the method CCA, the highest NDCG@25 is observed at the dimensionality of 40. This somewhat indicates a general conclusion that the selection of the latent subspace dimensionality is related to the optimized objective considered in learning the subspace.
4.6 Effect of the Number of Nearest Neighbors
The number of nearest neighbors considered in the structure preservation is another parameter in CCL. In the previous experiments, the number was fixed to 2,000. Next, we conducted experiments to evaluate the performance of our CCL method with the number of nearest neighbors in range of {100, 500, 1000, 1500, 2000} at different dimensionality of the latent subspace.
The NDCG@25 with the different number of nearest neighbors are shown in Figure 7. As illustrated in the figure, the optimal k differs at different dimensionality of the latent subspace. However, at each dimensionality of the latent subspace, the performance difference by using different number of nearest neighbors is within 0.0002, which softens the difficulty on choosing the optimal number of nearest neighbors in practice.
4.7 Effect of the Parameter 
A common problem with multiple regularization terms in a joint optimization objective is the need to set the parameters to tradeoff each component. In the previous experiments, the tradeoff  is optimally set in order to examine the performance of CCL on image search irrespective of the parameter influence. We further conducted experiments to test the sensitivity of  towards search performance.
Figure 8 shows the NDCG@25 performance with respect to different values of  at different dimensionality of the latent subspace. Similar to the effect of the number of nearest neighbors, we can see that the performance curve is very smooth when  varies in a range from {0.1, 0.2, ..., 1.0} at each dimension of the latent subspace. Specifically, when the dimension of the latent subspace is 80, the performance

0.495 0.494

Dim=40

Dim=80

Dim=120

Dim=160

0.493

0.492

0.491

0.490

0.489

0.488

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1



Figure 8: The NDCG@25 performance curve at different dimensionalities of the latent subspace with different .

fluctuates within the range of 0.001. Thus, the performance is not sensitive to the change of the tradeoff parameter.
5. DISCUSSION AND CONCLUSION
In this paper, we have investigated the issue of directly learning the multi-view distance between a textual query and an image by leveraging both click data and subspace learning techniques. The click data represent the click relations between queries and images, while the subspace learning aims to learn a latent common subspace between multiple views. We have proposed a novel click-through-based cross-view learning to solve the problem in a principle way. Specifically, we use two different linear mappings to project textual queries and visual images into a latent subspace. The mappings are learned by jointly minimizing the distance of the observed query-image pairs on the click-through bipartite graph and preserving the inherent structure in original single view. Moreover, we make orthogonal assumptions on the mapping matrices. Then the mappings can be obtained efficiently through curvilinear search. We take l2 norm between the projections of query and image in the latent subspace as the distance function to measure the relevance of a pair of (query, image).
Our future works are as follows. First, the two learned mapping matrices can be extended to the learning of queryquery and image-image distances. Next, the learned distances will be further explored for applications such as query expansion, query suggestion, and image clustering, in the learned low-dimensional space. Furthermore, we will investigate the kernel version of our method, making it applicable when kernel matrices instead of features are available.
6. ACKNOWLEDGMENTS
This work was partially supported by the National Natural Science Foundation of China (No. 61390514, No. 61272290), the Fundamental Research Funds for the Central Universities (No. WK2100060011), and the Shenzhen Research Institute, City University of Hong Kong.
7. REFERENCES
[1] R. A. Baeza-Yates and A. Tiberi. Extracting semantic relations from query logs. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining, 2007.

725

[2] B. Bai, J. Weston, D. Grangier, R. Collobert, K. Sadamasa, Y. Qi, C. Cortes, and M. Mohri. Polynomial semantic indexing. In Proceedings of Advances in Neural Information Processing Systems, 2009.
[3] D. Beeferman and A. L. Berger. Agglomerative clustering of a search engine query log. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining, 2000.
[4] C. F. Cadieu, H. Hong, D. Yamins, N. Pinto, N. J. Majaj, and J. J. DiCarlo. The neural representation benchmark and its evaluation on brain and machine. In Proceedings of International Conference on Learning Representations, 2013.
[5] C. Cortes, M. Mohri, and A. Rostamizadeh. Two-stage learning kernel algorithms. In Proceedings of International Conference on Machine Learning, 2010.
[6] N. Craswell and M. Szummer. Random walks on the click graph. In Proceedings of ACM Conference on Research and Development in Information Retrieval, 2007.
[7] Z. Fang and Z. Zhang. Discriminative feature selection for multi-view cross-domain learning. In Proceedings of ACM Conference of Information and Knowledge Management, 2013.
[8] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-view embedding space for modeling internet images, tags, and their semantics. International Journal of Computer Vision, (106):210­233, 2014.
[9] D. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639­2664, 2004.
[10] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, K. Wang, Y. Rui, and J. Li. Clickage: Towards bridging semantic and intent gaps via mining click logs of search engines. Proceedings of ACM International Conference on Multimedia, 2013.
[11] V. Jain and M. Varma. Learning to re-rank: Query-dependent image re-ranking using click data. In Proceedings of International World Wide Web Conference, 2011.
[12] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining, 2002.
[13] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. on Information Systems, 25(2), 2007.
[14] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. Muller, and A. Zien. Evaluating search engines by modeling the relationship between relevance and clicks. In Efficient and accurate lp-norm multiple kernel learning, 2009.
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of Advances in Neural Information Processing Systems, 2012.
[16] A. Kumar, P. Rai, and H. Daume. Co-regularized multi-view spectral clustering. In Proceedings of

Advances in Neural Information Processing Systems, 2011.
[17] G. R. G. Lanckriet, N. Cristianini, P. L. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine Learning Research, 5:27­72, 2004.
[18] X. Li, Y.-Y. Wang, and A. Acero. Learning query intent from regularized click graphs. In Proceedings of ACM Conference on Research and Development in Information Retrieval, 2008.
[19] Q. Mei, D. Zhou, and K. W. Church. Query suggestion using hitting time. In Proceedings of ACM Conference of Information and Knowledge Management, 2008.
[20] T. Mei, Y. Rui, S. Li, and Q. Tian. Multimedia Search Reranking: A Literature Survey. ACM Computing Surveys, 46(3), Sept. 2014.
[21] S. Melacci and M. Belkin. Laplacian support vector machines trained in the primal. Journal of Machine Learning Research, 12:1149­1184, 2011.
[22] I. Muslea, S. Minton, and C. Knoblock. Active learning with multiple views. Journal of Artificial Intelligence Research, 27(1):203­233, 2006.
[23] Y. Pan, T. Yao, K. Yang, H. Li, C.-W. Ngo, J. Wang, and T. Mei. Image search by graph-based label propagation with image representation from dnn. Proceedings of ACM International Conference on Multimedia, 2013.
[24] B. Poblete and R. A. Baeza-Yates. Query-sets: using implicit feedback and query patterns to organize web documents. In Proceedings of International World Wide Web Conference, 2008.
[25] R. Rosipal and N. Kr¨amer. Overview and recent advances in partial least squares. Subspace, Latent Structure and Feature Selection, pages 34­51, 2006.
[26] W. Sun and Y.-X. Yuan. Optimization theory and methods: nonlinear programming, volume 98. springer, 2006.
[27] M. Trevisiol, L. Chiarandini, L. M. Aiello, and A. Jaimes. Image ranking based on user browsing behavior. In Proceedings of ACM Conference on Research and Development in Information Retrieval, 2012.
[28] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user queries of a search engine. In Proceedings of International World Wide Web Conference, 2001.
[29] Z. Wen and W. Yin. A feasible method for optimization with orthogonality constrains. Mathematical Programming, 142:397­434, 2013.
[30] W. Wu, H. Li, and J. Xu. Learning query and document similarities from click-through bipartite graph with metadata. Proceedings of ACM Conference on Web Search and Data Mining, 2013.
[31] C. Xu, D. Tao, and C. Xu. A survey on multi-view learning. CoRR abs/1304.5634, 2013.
[32] T. Yao, T. Mei, C.-W. Ngo, and S. Li. Annotation for free: Video tagging by mining user search behavior. Proceedings of ACM International Conference on Multimedia, 2013.
[33] S. Yu, B. Krishnapuram, R. Rosales, and R. Rao. Bayesian co-training. Journal of Machine Learning Research, pages 2649­2680, 2011.

726


Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Word-Entity Duet Representations for Document Ranking

Chenyan Xiong
Language Technologies Institute Carnegie Mellon University Pi sburgh, PA 15213, USA cx@cs.cmu.edu

Jamie Callan
Language Technologies Institute Carnegie Mellon University Pi sburgh, PA 15213, USA callan@cs.cmu.edu

Tie-Yan Liu
Microso Research Beijing 100080, P.R. China tie-yan.liu@microso .com

ABSTRACT
is paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entitybased representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the a ention mechanism successfully steers the model away from noisy entities, and together they signi cantly outperform both word-based and entity-based learning to rank systems.
KEYWORDS
Word-Entity Duet, Entity-based Search, Explicit Semantics, Text Representation, Document Ranking.
1 INTRODUCTION
Utilizing knowledge bases in text-centric search is a recent breakthrough in information retrieval [5]. e rapid growth of information extraction techniques and community e orts have generated large scale general domain knowledge bases, such as DBpedia and Freebase. ese knowledge bases store rich semantics in semistructured formats and have great potential in improving text understanding and search accuracy.
ere are many possible ways to utilize knowledge bases' semantics in di erent components of a search system. ery representation can be improved by introducing related entities and their texts to expand the query [4, 20]. Document representation can be enriched by adding the annotated entities into the document's vector space model [17, 21, 23]. e ranking model can also be improved by utilizing the entities and their a ributes to build additional connections between query and documents [14, 19]. e
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080768

rich choices of available information and techniques raise a new challenge of how to use all of them together and fully explore the potential of knowledge graphs in search engines.
is work proposes a new framework for utilizing knowledge bases in information retrieval. Instead of centering around words and using the knowledge graph as an additional resource, this work treats entities equally with words, and represents the query and documents using both word-based and entity-based representations.
us the interaction of query and document is no longer a `solo' of their words, but a `duet' of their words and entities. Working together, the word-based and entity-based representations form a four-way interaction: query words to document words (Qw-Dw), query entities to document words (Qe-Dw), query words to document entities (Qw-De), and query entities to document entities (Qe-De). is leads to a general methodology for incorporating knowledge graphs into text-centric search systems.
e rich and novel ranking evidence from the word-entity duet does come with a cost. Because it is created automatically, the entity-based representation also introduces uncertainties. For example, an entity can be mistakenly annotated to a query, and may mislead the search system. is paper develops an a ention-based ranking model, AttR-Duet, that employs a simple a ention mechanism to handle the noise in the entity representation. e matching component of AttR-Duet focuses on ranking with the word-entity duet, while its a ention component focuses on steering the model away from noisy entities. Trained directly from relevance judgments, AttR-Duet learns how to demote noisy entities and how to rank documents with the word-entity duet simultaneously.
e e ectiveness of AttR-Duet is demonstrated on ClueWeb Category B corpora and TREC Web Track queries. On both ClueWeb09B and ClueWeb12-B13 , AttR-Duet outperforms previous wordbased and entity-based ranking systems by at least 14%. We demonstrate that the entities bring additional exact match and so match ranking signals from the knowledge graph; all entity-based rankings perform similar or be er compared to solely word-based rankings. We also nd that, when the automatically-constructed entity representations are not as clean, the a ention mechanism is necessary for the ranking model to utilize the ranking signals from the knowledge graph. Jointly learned, the a ention mechanism is able to demote noisy entities and distill the ranking signals, while without such puri cation, ranking models become vulnerable to noisy entity representations, and the mixed evidence from the knowledge graph may be more of a distraction than an asset.
In the rest of this paper, Section 2 discusses related work; Section 3 presents the word-entity duet framework for utilizing knowledge graphs in ranking; Section 4 is about the a ention-based ranking model; Experimental se ings and evaluations are described in Section 5 and 6; e conclusions and future work are in Section 7.

763

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

2 RELATED WORK
ere is a long history of research on utilizing semantic resources to improve information retrieval. Controlled vocabulary based information retrieval uses a set of expert-created index terms (mostly organized in ontologies) to represent query and documents. e retrieval is then performed by query and document's overlaps in the controlled vocabulary space, and the human knowledge in the controlled vocabulary is included. Controlled vocabulary is almost a necessity in some special domains. For example, in medical search where queries are o en about diseases, treatments, and genes, the search intent may not be covered by the query words, so external information about medical terms is needed. esauruses and lexical resources such as WordNet have also been used to address this issue. Synonyms and related concepts stored in these resources can be added to queries and documents to reduce language varieties and may improve the recall of search results [12].
Recently, large general domain knowledge bases, such as DBpedia [11] and Freebase [1], have emerged. Knowledge bases contain human knowledge about real-world entities, such as descriptions, a ributes, types, and relationships, usually in the form of knowledge graphs. ey share the same spirit with controlled vocabularies but are usually created by community e orts or information extraction systems, thus are o en at a larger scale. ese knowledge bases provide a new opportunity for search engines to be er `understand' queries and documents. Many new techniques have been developed to explore their potential in various components of ad-hoc retrieval.
An intuitive way is to use the texts associated with related entities to form be er query representations. Wikipedia contains well-wri en entity descriptions and can be used as an external and cleaner pseudo relevance feedback corpus to obtain be er expansion terms [24]. e descriptions of related Freebase entities have been utilized to provide be er expansion terms; the related entities are retrieved by entity search [3], or selected from top retrieved documents' annotations [8]. e text elds of related entities can also be used to provide expanded learning to rank features: Entity
ery Feature Expansion (EQFE) expands the query using the texts from related entities' a ributes, and these expanded texts generate rich ranking features [4].
Knowledge bases also provide additional connections between query and documents through related entities. Latent Entity Space (LES) builds an unsupervised retrieval model that ranks documents based on their textual similarities to latent entities' descriptions [14]. EsdRank models the connections between query to entities, and entities to documents using various information retrieval features.
ese connections are utilized by a latent space learning to rank model, which signi cantly improved state-of-the-art learning to rank methods [19].
A recent progress is to build entity-based representations for texts. Bag-of-entities representations built from entity annotations have been used in unsupervised retrieval models, including vector space models [21] and language models [17]. e entity-based so match was studied by Semantics-Enabled Language Model (SELM); it connects query and documents in the entity space using their entities' relatedness calculated from an entity linking system [6].
ese unsupervised entity-based retrieval models perform be er

or can be e ectively combined with word-based retrieval models. Recently, Explicit Semantic Ranking (ESR) performs learning to rank with query and documents' entity representations in scholar search [23]. ESR rst trains entity embeddings using a knowledge graph, and then converts the distances in the embedding space to exact match and so match ranking features, which signi cantly improved the ranking accuracy of semanticscholar.org.
3 WORD-ENTITY DUET FRAMEWORK
is section presents our word-entity duet framework for utilizing knowledge bases in search. Given a query q and a set of candidate documents D = {d1, ..., d |D | }, our framework aims to provide a systematic approach to be er rank D for q, with the help of a knowledge graph (knowledge base) G. In the framework, query and documents are represented by two representations, one wordbased and one entity-based (Section 3.1). e two representations' interactions create the word-entity duet and provide four matching components (Section 3.2).
3.1 Word and Entity Based Representations
Word-based representations of query and document are standard bag-of-words: Qw(w) = tf(w, q), and Dw(w) = tf(w, d). Each dimension in the bag-of-words Qw and Dw corresponds to a word w. Its weight is the word's frequency (tf) in the query or document.
A standard approach is to use multiple elds of a document, for example, title and body. Each document eld is usually represented by a separate bag-of-words, for example, Dwtitle and Dwbody, and the ranking scores from di erent elds are combined by ranking models. In this work, we assume that a document may have multiple
elds. However, to make notation simpler, the eld notation is omi ed in the rest of this paper unless necessary.
Entity-based representations are bag-of-entities constructed from entity annotations [21]: Qe(e) = tf(e, q) and De(e) = tf(e, d), where e is an entity linked to the query or the document. We use automatic entity annotations from an entity linking system to construct the bag-of-entities [21].
An entity linking system nds the entity mentions (surface forms) in a text, and links each surface form to a corresponding entity. For example, the entity `Barack Obama' can be linked to the query `Obama Family Tree'. `Obama' is the surface form.
Entity linking systems usually contain two main steps [7]:
(1) Spo ing: To nd surface forms in the text, for example, to identify the phrase `Obama'.
(2) Disambiguation: To link the most probable entity from the candidates of each surface form, for example, choosing `Barack Obama' from all possible Obama-related entities.
A commonly used information in spo ing is the linked probability (lp), the probability of a surface form being annotated in a training corpus, such as Wikipedia. A higher lp means the surface form is more likely to be linked. For example, `Obama' should have a higher lp than `table'. e disambiguation usually considers two factors.
e rst is commonness (CMNS), the universal probability of the surface form being linked to the entity. e second is the context in the text, which provides additional evidence for disambiguation. A con dence score is usually assigned to each annotated entity by

764

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Ranking features from query words to document words (title and body) (Qw-Dw).

Feature Description BM25 TF-IDF Boolean OR Boolean And Coordinate Match Language Model (Lm) Lm with JM smoothing Lm with Dirichlet smoothing Lm with two-way smoothing Total

Dimension 2 2 2 2 2 2 2 2 2 18

Table 2: Ranking features from query entities (name and description) to document words (title and body) (Qe-Dw).

Feature Description BM25 TF-IDF Boolean Or Boolean And Coordinate Match Lm with Dirichlet Smoothing Total

Dimension 4 4 4 4 4 4 24

the entity linking system, based on spo ing and disambiguation scores.
e bag-of-entities is not the set of surface forms that appear in the text (otherwise it is not much di erent from phrase-based representation). Instead, the entities are associated with rich semantics from the knowledge graph. For example, in Freebase, the information associated with each entity includes (but is not limited to) its name, alias, type, description, and relations with other entities. e entity-based representation makes these semantics available when matching query and documents.
3.2 Matching with the Word-Entity Duet
By adding the entity based representation into the search system, the ranking is no longer a solo match between words, but a wordentity duet that includes four di erent ways a query can interact with a document: query words to document words (Qw-Dw); query entities to document words (Qe-Dw); query words to document entities (Qw-De); and query entities to document entities (Qe-De). Each of them is a matching component and generates unique ranking features to be used in our ranking model.
ery Words to Document Words (Qw-Dw): is interaction has been widely studied in information retrieval. e matches of Qw and Dw generate term-level statistics such as term frequency and inverse document frequency. ese statistics are combined in various ways by standard retrieval models, for example, BM25, language model (Lm), and vector space model. is work applies these standard retrieval models on document title and body elds to extract the ranking features Qw-Dw in Table 1.

Table 3: Ranking features from query words to document entities (name and description) (Qw-De).

Feature Description Top 3 Coordinate Match on Title Entities Top 5 Coordinate Match on Body Entities Top 3 TF-IDF on Title Entities Top 5 TF-IDF on Body Entities Top 3 Lm-Dirichlet on Title Entities Top 5 Lm-Dirichlet on Body Entities Total

Dimension 6 10 6 10 6 10 48

Table 4: Ranking features from query entities to document's title and body entities (Qe-De).

Feature Description

Dimension

Binned translation scores, 1 exact match bin, 5 so match Bins in the range [0, 1).

12

ery Entities to Document Words (Qe-Dw): Knowledge bases contain textual a ributes about entities, such as names and descriptions. ese textual a ributes make it possible to build cross-space interactions between query entities and document words. Specifically, given a query entity e, we use its name and description as pseudo queries, and calculate their retrieval scores on a document's title and body bag-of-words, using standard retrieval models. e retrieval scores from query entities (name or description) to document's words (title or body) are used as ranking features Qe-Dw.
e detailed feature list is in Table 2. ery Words to Document Entities (Qw-De): Intuitively, the
texts from document entities should help the understanding of the document. For example, when reading a Wikipedia article, the description of a linked entity in the article is helpful for a reader who does not have the background knowledge about the entity.
e retrieval scores from the query words to document entities' name and descriptions are used to model this interaction. Di erent from Qe-Dw, in Qw-De, not all document entities are related to the query. To exclude unnecessary information, only the highest retrieval scores from each retrieval model are included as features:
Qw-De  max-k({score(q, e)|e  De}).
score(q, e) is the score of q and document entity e from a retrieval model. max-k() takes the k biggest scores from the set. Applying retrieval models on title and body entities' names and descriptions, the ranking features Qw-De in Table 3 are extracted. We choose a smaller k for title entities as titles are short and rarely have more than three entities.
ery Entities to Document Entities (Qe-De): ere are two ways the interactions in the entity space can be useful. e exact match signal addresses the vocabulary mismatch of surface forms [17, 21]. For example, two di erent surface forms, `Obama' and `US President', are linked to the same entity `Barack Obama' and thus are matched. e so match in the entity space is also useful. For example, a document that frequently mentions `the white house' and `executive order' may be relevant to the query `US President'.

765

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

We choose a recent technique, Explicit Semantic Ranking (ESR), to model the exact match and so match in the entity space [23]. ESR rst calculates an entity translation matrix of the query and document using entity embeddings. en it gathers ranking features from the matrix by histogram pooling. ESR was originally applied in scholar search and its entity embeddings were trained using domain speci c information like author and venue.
In the general domain, there is much research that aims to learn entity embeddings from the knowledge graph [2, 13]. We choose the popular TransE model which is e ective and e cient to be applied on large knowledge graphs [2].
Given the triples (edges) from the knowledge graph (eh, p, et ), including eh and et the head entity and the tail entity, and p the edge type (predicate), TransE learns entity and relationship embeddings (eì and pì) by optimizing the following pairwise loss:

[1 + ||eìh + pì - eìt ||1 - ||eìh + pì - eìt ||1]+,
(eh,p,et )G (eh,p,et )G
where [·]+ is the hinge loss, G is the set of existing edges in the knowledge graph, and G is the randomly sampled negative instances. e loss function ensures that entities in similar graph structures are mapped closely in the embedding space, using the compositional assumption along the edge: eìh + pì = eìt .
e distance between two entity embeddings describes their similarity in the knowledge graph [2]. Using L1 similarity, a translation matrix can be calculated:

T (ei , ej ) = 1 - ||eìi - eìj ||1.

(1)

T is the |Qe| × |De| translation matrix. ei and ej are the entities in the query and the document respectively.
en the histogram pooling technique is used to gather querydocument matching signals from T [9, 23]:

Sì(De) = max T (e, De)
e Qe
Bk (Sì(De)) = log I (stk  Sìj (De) < edk ).
j

Sì(d) is the max-pooled |De| dimensional vector, whose jth dimension is the maximum similarity of the jth document entity to any query entities. Bk () is the kth bin that counts the number of translation scores in its range [stk , edk ).
We use the same six bins as in the ESR paper: [1, 1], [0.8, 1),
[0.6, 0.8), [0.4, 0.6), [0, 2, 0.4), [0, 0, 2). e rst bin is the exact
match bin and is equivalent to the entity frequency model [21].
e other bin scores capture the so match signal between query
and documents at di erent levels. ese bin scores generated the
ranking features Qe-De in Table 4.

3.3 Summary
e word-entity duet incorporates various semantics from the knowledge graph: e textual a ributes of entities are used to model the cross-space interactions (Qe-Dw and Qw-De); the relations in the knowledge graphs are used to model the interactions in the entity space (Qe-De), through the knowledge graph embedding.
e word-based retrieval models are also included (Qw-Dw).

Table 5: Attention features for query entities.

Feature Description Entropy of the Surface Form Is the Most Popular Candidate Entity Margin to the Next Candidate Entity Embedding Similarity with ery Total

Dimension 1 1 1 1 4

Many prior methods are included in the duet framework. For example, the query expansion methods using Wikipedia or Freebase represent the query using related entities, and then use these entities' texts to build additional connections with the document's text [4, 20, 24]; the latent entity space techniques rst nd a set of highly related query entities, and then rank documents using their connections with these entities [14, 19]; the entity based ranking methods model the interactions between query and documents in the entity space using exact match [17, 21] and so match [23].
Each of the four interactions generates a set of ranking signals. A straightforward way is to use them as features in learning to rank models. However, the entity representations may include noise and generate misleading ranking signals, which motivates our AttR-Duet ranking model in the next section.
4 ATTENTION BASED RANKING MODEL
Unlike bag-of-words, entity-based representations are constructed using automatic entity linking systems. It is inevitable that some entities are mistakenly annotated, especially in short queries where there is less context for disambiguation. If an unrelated entity is annotated to the query, it will introduce misleading ranking features; documents that match the unrelated entity might be promoted. Without additional information, ranking models have li le leverage to distinguish the useful signals brought in by correct entities from those by the noisy ones, and their accuracies might be limited.
We address this problem with an a ention based ranking model AttR-Duet. It rst extracts a ention features to describe the quality of query entities. en AttR-Duet builds a simple a ention mechanism using these features to demote noisy entities. e a ention and the matching of query-documents are trained together using back-propagation, enabling the model to learn simultaneously how to weight entities of varying quality and how to rank with the wordentity duet. e a ention features are described in Section 4.1. e details of the ranking model are discussed in Section 4.2.
4.1 Attention Features
Two groups of a ention features are extracted for each query entity to model its annotation ambiguity and its closeness to the query.
Annotation Ambiguity features describe the risk of an entity annotation. ere is a risk that the linker may fail to disambiguate the surface form to the correct entity, especially when the surface form is too ambiguous. For example, `Apple' in a short query can be the fruit or the brand. It is risky to put high a ention on it. ere are three ambiguity features used in AttR-Duet.
e rst feature is the entropy of the surface form. Given a training corpus, for example, Wikipedia, we gather the probability of a surface form being linked to di erent candidate entities, and

766

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Qw-Dw Qw-De 1D-CNN

AttR-Duet: ,

Obama Family

Tree Qe-Dw Qe-De 1D-CNN

Concatenation

`Barack Obama'

`Family Tree'

Matching Feature Matching Model

Dot Product

1D-CNN

Concatenation 1D-CNN

Obama Family Tree
`Barack Obama' `Family Tree'

Attention Model Attention Feature

Figure 1: e Architecture of the Attention based Ranking Model for Word-Entity Duet (AttR-Duet). e le side models the query-document matching in the word-entity duet. e right side models the importances of query entities using attention features. ey together produce the nal ranking score.

calculate the entropy of this probability. e higher the entropy is, the more ambiguous the surface form is, and the less a ention the model should put on the corresponding query entity. e second feature is whether the annotated entity is the most popular candidate of the surface form, i.e. has the highest commonness score (CMNS). e third feature is the di erence between the linked entity's CMNS to the next candidate entity's.
A closeness a ention feature is extracted using the distance between the query entity and the query words in an embedding space. An entity and word joint embedding model are trained on a corpus including the original documents and the documents with surface forms replaced by linked entities. e cosine similarity between the entity embedding to the query embedding (the average of its words' embeddings) is used as the feature. Intuitively, a higher similarity score should lead to more a ention.
e full list of entity a ention features, A (e), is listed in Table 5.

4.2 Model
e architecture of AttR-Duet is illustrated in Figure 1. It produces a ranking function f (q, d) that re-ranks candidate documents D for the query q, with the ranking features in Table 1-4 and a ention features in Table 5. f (q, d) is expected to weight query elements more properly and rank document more accurately.
Model inputs: Suppose the query contains words {w1, ..., wn } and entities {e1, ... , em }, there are four input feature matrices: Rw , Re , Aw , and Ae . Rw and Re are the ranking feature matrices for query words and entities in the document. Aw and Ae are the a ention feature matrices for words and entities. ese matrices' rows are feature vectors previously described:

Rw (wi , ·) = Qw-Dw(wi ) Qw-De(wi )

(2)

Re (ej , ·) = Qe-Dw(ej ) Qe-De(ej )

(3)

Aw (wi , ·) = 1

(4)

Ae (ej , ·) = A (ej ).

(5)

Qw-Dw, Qw-De, Qe-Dw, and Qe-De are the ranking features from the word-entity duet, as described in Section 3. concatenates the two
feature vectors of a query element. A (ej ) is the a ention features for entity ej (Table 5). In this work, we use uniform word a ention (Aw = 1), because the main goal of the a ention mechanism is to handle the uncertainty in the entity representations.

e matching part contains two Convolutional Neural Networks (CNN's). One matches query words to d (Rw ); the other one matches query entities to d (Re ). e convolution is applied on the query element (word/entity) dimension, assuming that the ranking evidence from di erent query words or entities should be treated the same. e simplest setup with one 1d CNN layer, 1 lter, and linear activation function can be considered as a linear model applied `convolutionally' on each word or entity:

Fw (wi ) = Wwm · Rw (wi , ·) + bwm

(6)

Fe (ej ) = Wem · Re (ej , ·) + bem .

(7)

Fw (wi ) and Fe (ej ) are the matching scores from query word wi and query entity ej , respectively. e matching scores from all query words form an n dimensional vector Fw , and those from entities form an m dimensional vector Fe . Wwm,Wem, bwm , and bem are the matching parameters to learn.
e attention part also contains two CNN's. One weights query
words with Aw and the other one weights query entities with Ae . e same convolution idea is applied as the a ention features on
each query word/entity should be treated the same.
e simplest set-ups with one CNN layer are:

w (wi ) = ReLU(Wwa · Aw (wi , :) + bwa )

(8)

e (ej ) = ReLU(Wea · Ae (ej , :) + bea ).

(9)

w (wi ) and e (ej ) are the a ention weights on word wi and entity ej . {Wwa ,Wea, bwa , bea } are the a ention parameters to learn. ReLU activation is used to ensure non-negative a ention weights.
e matching scores can be negative because only the di erences between documents' matching scores ma er.
e nal ranking score combines the matching scores using the a ention scores:

f (q, d) = Fw · w + Fe · e .

(10)

e training is done by optimizing the pairwise hinge loss:

l(q, D) =

[1 - f (q, d) + f (q, d )]+.

(11)

d D+ d D-

D+ and D- are the set of relevant documents and the set of irrel-

evant documents. [·]+ is the hinge loss. e loss function can be optimized using back-propagation in the neural network, and the

matching part and the a ention part are learned simultaneously.

767

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

5 EXPERIMENTAL METHODOLOGY
is section describes the experiment methodology, including dataset, baselines, and the implementation details of our methods.
Dataset: Ranking performances were evaluated on the TREC Web Track ad-hoc task, the standard benchmark for web search. TREC 2009-2012 provided 200 queries for ClueWeb09, and TREC 2013-2014 provided 100 queries for ClueWeb12. e Category B of both corpora (ClueWeb09-B and ClueWeb12-B13) and corresponding TREC relevance judgments were used.
On ClueWeb09-B, the SDM runs provided by EQFE [4] are used as the base retrieval. It is a well-tuned Galago-based implementation and performs be er than Indri's SDM. All their se ings are inherited, including spam ltering using waterloo spam score (with a threshold of 60), INQUERY plus web-speci c stopwords removal, and KStemming. On ClueWeb12-B13, not all queries' rankings are available from prior work, and Indri's SDM performs similarly to language model. For simplicity, the base retrieval on ClueWeb12B13 used is Indri's default language model with KStemming, INQUERY stopword removal, and no spam ltering. All our methods and learning to rank baselines re-ranked the rst 100 documents from the base retrieval.
e ClueWeb web pages were parsed using Boilerpipe1. e `KeepEverythingExtractor' was used to keep as much text from the web page as possible, to minimize the parser's in uence. e documents were parsed to two elds: title and body. All the baselines and methods implemented by ourselves were built upon the same parsed results for fair comparisons.
e Knowledge Graph used in this work is Freebase [1]. e query and document entities were both annotated by TagMe [7]. No
lter was applied on TagMe's results; all annotation were kept. is is the most widely used se ing of entity-based ranking methods on ClueWeb [17, 19, 21].
Baselines included standard word-based baselines: Indri's language model (Lm), sequential dependency model (SDM), and two state-of-the-art learning to rank methods: RankSVM2 [10] and coordinate ascent (Coor-Ascent3) [15]. RankSVM was trained and evaluated using a 10-fold cross-validation on each corpus. Each fold was split to train (80%), develop (10%), and test (10%). e develop part was used to tune the hyper-parameter c of the linear SVM from the set {1e - 05, 0.0001, 0.001, 0.01, 0.03, 0.05, 0.07, 0.1, 0.5, 1}. Coor-Ascent was trained using RankLib's recommended se ings, which worked well in our experiments. ey used the same word based ranking features as in Table 1.
Entity-based ranking baselines were also compared. EQFE [4], EsdRank [19], and BOE-TagMe [21] runs are provided on their authors' websites. e comparisons with EQFE and EsdRank were mainly done on ClueWeb09 as the full ranking results on ClueWeb12 are not publicly available. BOE-TagMe is the best TagMe based runs, which is TagMe-EF on ClueWeb09-B and TagMe-COOR on ClueWeb12-B13 [21]. Explicit Semantic Ranking (ESR) was implemented by ourselves as originally it was only applied on scholar search [23].
1h ps://github.com/kohlschu er/boilerpipe 2h ps://www.cs.cornell.edu/people/tj/svm light/svm rank.html 3h ps://sourceforge.net/p/lemur/wiki/RankLib/

ere are also other unsupervised entity-based systems [14, 17, 20]; it is unfair to compare them with supervised methods.
Evaluation was done by NDCG@20 and ERR@20, the o cial TREC Web Track ad-hoc task evaluation metrics. Statistical signi cances were tested by permutation test with p< 0.05.
Feature Details: All parameters in the unsupervised retrieval model features were kept default. All texts were reduced to lower case, punctuation was discarded, and standard INQUERY stopwords were removed. Document elds included title and body, both parsed by Boilerpipe. Entity textual elds included name and description. When extracting Qw-De features, if a document did not have enough entities (3 in title or 5 in body), the feature values were set to -20.
e TransE embeddings were trained using Fast-TransX library4. e embedding dimension used is 50. When extracting the a ention features in Table 5, the word and entity joint embeddings were obtained by training a skip-gram model on the candidate documents using Google's word2vec [16] with 300 dimensions; the surface form's statistics were calculated from Google's FACC1 annotation [8]. Model Details: AttR-Duet was evaluated using 10-fold cross validation with the same partitions as RankSVM. Deeper neural networks were explored but did not provide much improvement so the simplest CNN se ing was used: 1 layer, 1 lter, linear activation for the ranking part, and ReLU activation for the a ention part. All CNN's weights were L2-regularized. Regularization weights were selected from the set {0, 0.001, 0.01, 0.1} using the develop fold in the cross validation. Training loss was optimized using the Nadam algorithm [18]. Our implementation was based on Keras. To facilitate implementation, input feature matrices of query elements were padded to the maximum length with zeros. Batch training was used, given the small size of training data. Using a common CPU, the training took 4-8 hours to converge on ClueWeb09-B and 2-4 hours on ClueWeb12-B13. e testing is e cient as the neural network is shallow. e document annotations, TransE embeddings, and surface form information can be obtained o line. ery entity linking is e cient given the short query length. If the embedding results and entities' texts are maintained in memory, the feature extraction is of the same complexity as typical learning to rank features.
e rankings, evaluation results, and the data used in our experiments are available online at h p://boston.lti.cs.cmu.edu/appendices/ SIGIR2017 word entity duet/.
6 EVALUATION RESULTS
is section rst evaluates the overall ranking performances of the word-entity duet with a ention based learning to rank. en it analyzes the two parts of AttR-Duet: Matching with the wordentity duet and the a ention mechanism.
6.1 Overall Performance
e overall accuracies of AttR-Duet and baselines are shown in Table 6. Relative performances over RankSVM are shown in percentages. Win/Tie/Loss are the number of queries a method improves, does not change, and hurts, compared with RankSVM on NDCG@20.
4h ps://github.com/thunlp/Fast-TransX

768

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 6: Overall accuracies of AttR-Duet and baselines. (U) and (S) indicate unsupervised or supervised method. (E) indicates that information from entities is used. Relative performances compared with RankSVM are shown in percentages. Win/Tie/Loss are the number of queries a method improves, does not change, or hurts, compared with RankSVM on NDCG@20. Best results in each metric are marked bold. § marks statistically signi cant improvements (p< 0.05) over all baselines.

Method

Lm

(U)

SDM

(U)

RankSVM

(S)

Coor-Ascent (S)

BOE-TagMe (UE)

ESR

(SE)

EQFE5

(SE)

EsdRank5

(SE)

AttR-Duet (SE)

ClueWeb09-B

NDCG@20

ERR@20

0.1757 -33.33% 0.1195 -22.63%

0.2496 -5.26% 0.1387 -10.20%

0.2635

­ 0.1544

­

0.2681 +1.75% 0.1617 +4.72%

0.2294 -12.94% 0.1488 -3.63%

0.2695 +2.30% 0.1607 +4.06%

0.2448 -7.10% 0.1419 -8.10%

0.2644 +0.33% 0.1756 +13.69% 0.3197§ +21.32% 0.2026§ +31.21%

W/T/L 47/28/125 62/38/100
­/­/­ 71/47/82 74/25/101 80/39/81 77/33/90 88/28/84
101/37/62

ClueWeb12-B13

NDCG@20

ERR@20

0.1060 -12.02% 0.0863 -6.67%

0.1083 -10.14% 0.0905 -2.08%

0.1205

­ 0.0924

­

0.1206 +0.08% 0.0947 +2.42%

0.1173 -2.64% 0.0950 +2.83%

0.1166 -3.22% 0.0898 -2.81%

n/a

­ n/a

­

n/a

­ n/a

­

0.1376§ +14.22% 0.1154§ +24.92%

W/T/L 35/22/43 27/25/48
­/­/­ 36/32/32 44/19/37 30/23/47
­/­/­ ­/­/­
45/24/31

Table 7: Ranking accuracy with each group of matching feature from the word-entity duet. Base Retrieval is SDM on ClueWeb09 and Lm on ClueWeb12. LeToR-Qw-Dw uses the query and document's BOW (Table 1). LeToR-Qe-Dw uses the query's BOE and document's BOW (Table 2), LeToR-Qw-De is the query BOW + document BOE (Table 3), and LeToR-Qe-De uses the query and document's BOE (Table 4). LeToR-All uses all groups. Relative performances in percentages, Win/Tie/Loss on NDCG@20, and statistically signi cant improvements () are all compared with Base Retrieval.

Method Base Retrieval LeToR-Qw-Dw LeToR-Qe-Dw LeToR-Qw-De LeToR-Qe-De LeToR-All

ClueWeb09-B

NDCG@20

ERR@20

0.2496 0.2635
0.2729 0.2867 0.2695 0.3099

--
+5.55% +9.33% +14.83% +7.97% +24.13%

0.1387 0.1544 0.1824 0.1651 0.1607 0.1955

--
+11.36% +31.51% +19.07% +15.88% +40.97%

W/T/L ­/­/­ 100/38/62 82/34/84 91/39/70 99/40/61 103/38/59

ClueWeb12-B13

NDCG@20

ERR@20

0.1060

-- 0.0863

--

0.1205 +13.67% 0.0924 +7.14%

0.1110 +4.66% 0.0928 +7.63%

0.1146 +8.09% 0.0880 +1.96%

0.1166 +10.01% 0.0898 +4.13%

0.1205 +13.69% 0.1000 +15.93%

W/T/L ­/­/­ 43/22/35 40/20/40 42/20/38 38/20/42 47/19/34

Best results in each metric are marked Bold. § indicates statistically signi cant improvements over all available baselines.
AttR-Duet outperformed all baselines signi cantly by large margins. On ClueWeb09-B, a widely studied benchmark for web search, AttR-Duet improved RankSVM, a strong learning to rank baseline, by more than 20% at NDCG@20, and more than 30% at ERR@20, showing the advantage of the word-entity duet over bag-of-words. ESR, EQFE and EsdRank, previous state-of-the-art entity-based ranking methods, were outperformed by at least 15%. It is not surprising because the word-entity duet framework was designed to include all of their e ects, as discussed at Section 3.3. ClueWeb12-B13 has been considered a hard dataset due to its noisy corpus and harder queries. e size of its training data is also smaller, which limits the strength of our neural network. However, AttR-Duet still signi cantly outperformed all available baselines by at least 14%. e information from entities is e ective and also di erent with those from words: AttR-Duet in uences more than three-quarters of the queries, and improves the majority of them.
5Ranking results are obtained from the authors' websites. ClueWeb09-B scores are higher than in original papers [4, 19] as we evaluate them using TREC's Category B qrels. e original papers used Category A's qrels although they ranked Category B documents. EQFE and EsdRank's ClueWeb12 results are not available as they were only evaluated on the rst 50 queries of the 100.

6.2 Matching with Word-Entity Duet
In a sophisticated system like AttR-Duet, it is hard to tell the contributions of di erent components. is experiment studies how each of the four-way interactions in the word-entity duet contributes to the ranking performance individually. For each group of the matching features in Table 1- 4, we train a RankSVM individually, which resulted in four ranking models: LeToR-Qw-Dw, LeToR-Qe-Dw, LeToR-Qw-De, and LeToR-Qe-De. LeToR-All which uses all ranking features is also evaluated. In LeToR-Qw-De and LeToR-Qe-De, the score of the base retrieval model is included as a feature, so that there is a feature to indicate the strength of the word-based match for the whole document. All these methods were trained and tested in the same se ing as RankSVM. As a result, LeToR-Qw-Dw is equivalent to the RankSVM baseline, and LeToR-Qe-De is equivalent to the ESR baseline.
eir evaluation results are listed in Table 7. Relative performances (percentages), Win/Tie/Loss, and statistically signi cant improvements () are all compared with Base Retrieval (SDM on ClueWeb09 and Lm on ClueWeb12). All four groups of matching features were able to improve the ranking accuracy of Base Retrieval when used individually as ranking features, demonstrating the usefulness of all matching components in the duet. On ClueWeb09-B, all three entity related components, LeToR-Qe-Dw,

769

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 8: Examples of entities used in Qw-De and Qe-De. e rst half are examples of matched entities in relevant and irrelevant documents, which are used to extract Qw-De features. e second half are examples of entities falls into the exact match bin and the closest so match bins, used to extract Qe-De features.

ery Uss Yorktown Charleston SC Flushing

Examples of Most Similar Entities to the ery

Top Entities in Relevant Documents Top Entities in Irrelevant Documents

`USS Yorktown (CV-10)'

`Charles Cornwallis', `USS Yorktown (CV-5)'

`Roosevelt Avenue', `Flushing, eens'

`Flushing (physiology)', `Flush (cards)'

Examples of Neighbors in Knowledge Graph Embedding

ery

Entities in Exact Match Bin

Entities in So Match Bins

Uss Yorktown Charleston SC `USS Yorktown (CV-10)', `Charleston, SC'

`Empire of Japan', `World War II'

Flushing

`Flushing, eens'

`Brooklyn', `Manha an', `New York'

Relative NDCG@20 Relative NDCG@20

20%

12%

15%

10%

5%

0%

-5%

ClueWeb09

Top 1 Scores

Top 2 Scores

Top 4 Scores

Top 5 Scores

ClueWeb12
Top 3 Scores

9%

6%

3%

0% ClueWeb09

Exact Bins

First 2 Bins

First 4 Bins

First 5 Bins

ClueWeb12
First 3 Bins All Bins

(a) Features from ery Words to Document Entities (Qw-De) (b) Features from ery Entities to Document Entities (Qe-De)

Figure 2: Incremental ranking feature analysis. e y-axis is the relative NDCG@20 improvement over the base retrieval. e x-axis refers to the features from only top k (1-5) entity match scores (2a), and the features from only rst k (1-6) bins in the ESR model (2b), both ordered incrementally from le to right.

LeToR-Qw-De, and LeToR-Qe-De, provided similar or be er performances than the word-based RankSVM. When all features were used together, LeToR-All signi cantly improved RankSVM by 17% and 26% on NDCG@20 and ERR@20, showing that the ranking evidence from di erent parts of the duet can reinforce each other.
On ClueWeb12-B13, entity-based matching was less e ective. LeToR-All's NDCG@20 was the same as RankSVM's, despite additional matching features. e di erence is that the annotation quality of TagMe on ClueWeb12 queries is lower (Table 9) [21]. e noisy entity representation may mislead the ranking model, and prevent the e ective usage of entities. To deal with this uncertainty is the motivation of the a ention based ranking model, which is studied in Section 6.4.
6.3 Matching Feature Analysis
e features from the word space (Qw-Dw) are well understood, and the feature from the query entities to document words (Qe-Dw) have been studied in prior research [4, 14, 19]. is experiment analyzes the features from the two new components (Qw-De and Qe-De).
Qw-De features match the query words with the document entities. For each document, the query words are matched with the textual elds of document entities using retrieval models, and the highest scores are Qw-De features.

We performed an incremental feature analysis of LeToR-Qw-De. Starting with the highest scored entities from each group in Table 3, we incrementally added the next highest ones to the model and evaluated the ranking performance. e results are shown in Figure 2a. e y-axis is the relative NDCG@20 improvements over the base retrieval model. e x-axis is the used features. For example, `Top 3 Scores' uses the top 3 entities' retrieval scores in each row of Table 3.
e highest scores were very useful. Simply combining them with the base retrieval provided nearly 10% gain on ClueWeb09-B and about 7% on ClueWeb12-B13. Adding the following scores was not that stable, perhaps because the corresponding entities were rather noisy, given the simple retrieval models used to match query words with them. Nevertheless, the top 5 scores together further improve the ranking accuracy.
e rst half of Table 8 shows examples of entities with highest matching scores. We found that such `top' entities from relevant documents are frequently related to the query, for example, `Roosevelt Avenue' is an avenue across Flushing, NY. In comparison, entities from irrelevant documents are much noisier. Qw-De features make use of this information and generate useful ranking evidence.
Qe-De features are extracted using the Explicit Semantic Ranking (ESR) method [23]. ESR is built upon the translation model. It operates in the entity space, and extracts the ranking features using histogram pooling. ESR was originally applied on scholar search.

770

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 9: ery annotation accuracy and the gain of attention mechanism. TagMe Accuracy includes the precision and recall of TagMe on ClueWeb queries, evaluated in prior research [21]. Attention Gains are the relative improvements of AttR-Duet compared with LeToR-All. Statistical signi cant gains are marked by .

ClueWeb09 ClueWeb12

TagMe Accuracy Precision Recall
0.581 0.597 0.460 0.555

Attention Gain

NDCG@20 ERR@20

+3.16%

+3.65%

+14.20% +15.45%

Number of Queries Attention Relative Gain
Number of Queries Attention Relative Gain

NDCG ERR

150

8%

6% 100
4% 50
2%

0

0%

1

2

3

(a) ClueWeb09-B

NDCG ERR 50

40

40%

30

20

20%

10

0

0%

1

2

3

(b) ClueWeb12-B13

Figure 3: Attention mechanism's gain on queries that contain di erent number of entities. e x-axis is the number of entities in the queries. e y-axis is the number of queries in each group (histogram), and the gain from attention (plots).

is work introduces ESR into web search and uses TransE model to train general domain embeddings from the knowledge graph.
To study the e ectiveness of ESR in our se ing, we also performed an incremental feature analysis of LeToR-Qe-De. Starting from the rst bin (exact match), the following bins (so matches) were incrementally added into RankSVM, and their rankings were evaluated. e results are shown in Figure 2b. e y-axis is the relative NDCG@20 over the base retrieval model they re-rank. e x-axis is the features used. For example, First 3 Bins refers to using the rst three bins: [1, 1], [0.8, 1), and [0.6, 0.8).
e observation on scholar search [23] holds on ClueWeb: Both exact match and so match with entities are useful. e exact match bin provided a 7% improvement on ClueWeb09-B, while only 2% on ClueWeb12-B13. Similar exact match results were also observed in a prior study [21]. It is another re ection of the entity annotation quality di erences on the two datasets. Adding the later bins almost always improves the ranking accuracy, especially on ClueWeb12.
e second half of Table 8 shows some examples of entities in the exact match bin and the nearest so match bins. e exact match bin includes the query entities and is expected to help. e rst so match bin usually contains related entities. For example, the neighbors of `USS Yorktown (CV-10)' include `World War II' which is when the ship was built. e further bins are mostly background noise because they are too far away. e improvements are mostly from the rst 3 bins.

Table 10: Examples of learned attention. e entities in bold blue draw more attention; those in gray draw less attention.

ery Balding Cure
Nicolas Cage Movies Hawaiian Volcano Observatories Magnesium Rich Foods Kids Earth Day Activities

Entity Attention
`Cure' `Clare Balding'
`Nicolas Cage' `Pokemon (Anime)' `Volcano'; `Observatory' `Hawaiian Narrative'; `Magnesium'; `Food' `First World'
`Earth Day' `Youth Organizations in the USA'

6.4 Attention Mechanism Analysis
e last experiment studies the e ect of the a ention mechanism by comparing AttR-Duet with LeToR-All. If enforcing at a ention weights on all query words and entities, AttR-Duet is equivalent to LeToR-All: e matching features, model function, and loss function are all the same. e a ention part is their only di erence, whose e ect is re ected in this comparison.
e gains from the a ention mechanism are shown in Table 9. To be er understand the a ention mechanism's e ectiveness in demoting noisy query entities, the query annotation's quality evaluated in a prior work [21] is also listed. e percentages in the A ention Gain columns are relative improvements of AttR-Duet compared with LeToR-All.  marks statistical signi cance. Figure 3 breaks down the relative gains to queries with di erent numbers of query entities. e x-axis is the number of query entities. e histograms are the number of queries in each group, marked by the le y-axis.
e plots are the relative gains, marked by the right y-axis. e a ention mechanism is essential to ClueWeb12-B13. With-
out the a ention model, LeToR-All was confused by the noisy query entities and could not provide signi cant improvements over word-based models, as discussed in the last experiment. With the a ention mechanism, AttR-Duet improved LeToR-All by about 15%, outperforming all baselines. On ClueWeb09 where TagMe's accuracy is be er [21], the ranking evidence from the word-entity duet was clean enough for LeToR-All to improve ranking, so the a ention mechanism's e ect was smaller. Also, in general, the a ention mechanism is more e ective when there are more query entities, while if there is only one entity there is not much to tweak.
e motivation for using a ention is to handle the uncertainties in the query entities, a crucial challenge in utilizing knowledge bases in search. ese results demonstrated its ability to do so. We also found many intuitive examples in the learned a ention weights, some listed in Table 10. e bold blue entities on the rst line of each block gain more a ention (> 0.6 a ention score). ose in gray on the second line draw less a ention (< 0.4 score). e a ention mechanism steers the model away from those mistakenly linked query entities, which makes it possible to utilize the correct entities' ranking evidence from a noisy representation.

771

Session 7B: Entities

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

7 CONCLUSIONS AND FUTURE WORK
is work presents a word-entity duet framework for utilizing knowledge bases in document ranking. In this paper, the query and documents are represented by both word-based and entitybased representations. e four-way interactions between the two representation spaces form a word-entity duet that can systematically incorporate various semantics from the knowledge graph. From query words to document words (Qw-Dw), word-based ranking features are included. From query entities to document entities (Qe-De), entity-based exact match and so match evidence from the knowledge graph structure are included. e entities' textual elds are used in the cross-space interactions Qe-Dw, which expands the query, and Qw-De, which enriches the document.
To handle the uncertainty introduced from the automatic-thusnoisy entity representations, a new ranking model AttR-Duet is developed. It employs a simple a ention mechanism to demote the ambiguous or o -topic query entities, and learns simultaneously how to weight entities of varying quality and how to rank documents with the word-entity duet.
Experimental results on the TREC Web Track ad-hoc task demonstrate the e ectiveness of proposed methods. AttR-Duet signi cantly outperformed all word-based and entity-based ranking baselines on both ClueWeb corpora and all evaluation metrics. Further experiments reveal that the strength of the method comes from both the advanced matching evidence from the word-entity duet, and the a ention mechanism that successfully `puri es' them. On ClueWeb09 where the query entities are cleaner, all the entity related matching components from the duet provide similar or be er improvements compared with word-based features. On ClueWeb12 where the query entities are noisier, the a ention mechanism steers the ranking model away from noisy entities and is necessary for stable improvements.
Our method provides a uni ed representation framework to utilize knowledge graphs in information retrieval. As the rst step, this work kept its components as simple as possible. It is easy to imagine further developments in various places. For example, the recent approaches in neural ranking with word embeddings can be incorporated [9]; be er knowledge graph embeddings can be used [13]; be er entity search methods can be applied when extracting word to entity features [3]; the a ention mechanism can be extended to document's entity-based representations. More sophisticated neural ranking models [22] can also be applied with the word-entity duet, especially when more training data are available.
8 ACKNOWLEDGMENTS
is research was supported by National Science Foundation (NSF) grant IIS-1422676, a Google Faculty Research Award, and a fellowship from the Allen Institute for Arti cial Intelligence. We thank Xu Han for helping us train the TransE embedding. Any opinions,
ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.
REFERENCES
[1] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data (SIGMOD 2008). ACM, 1247­1250.

[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NIPS 2013). 2787­ 2795.
[3] Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An empirical study of learning to rank for entity search. In Proceedings of the 39th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval,(SIGIR 2016). ACM, 737­740. [4] Je rey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2014). ACM, 365­374. [5] Laura Dietz, Alexander Kotov, and Edgar Meij. 2017. Utilizing knowledge graphs in text-centric information retrieval. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 815­816. [6] Faezeh Ensan and Ebrahim Bagheri. 2017. Document retrieval model through semantic linking. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 181­190. [7] Paolo Ferragina and Ugo Scaiella. 2010. Fast and accurate annotation of short texts with Wikipedia pages. arXiv preprint arXiv:1006.3498 (2010). [8] Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 201306-26, Format version 1, Correction level 0). (June 2013). [9] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W.Bruce Cro . 2016. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (CIKM 2016). ACM, 55­64. [10] orsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2002). ACM, 133­142. [11] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, So¨ren Auer, and Christian Bizer. 2014. DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic Web Journal (2014). [12] Hang Li and Jun Xu. 2014. Semantic matching in search. Foundations and Trends in Information Retrieval 8 (2014), 89. [13] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference on Arti cial Intelligence (AAAI 2015). 2181­ 2187. [14] Xitong Liu and Hui Fang. 2015. Latent entity space: A novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18, 6 (2015), 473­503. [15] Donald Metzler and W Bruce Cro . 2007. Linear feature-based models for information retrieval. Information Retrieval 10, 3 (2007), 257­274. [16] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed representations of words and phrases and their compositionality. In
Proceedings of the 2 h Advances in Neural Information Processing Systems 2013 (NIPS 2013). 3111­3119. [17] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document retrieval using entity-based language models. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016). ACM, 65­74. [18] Ilya Sutskever, James Martens, George E Dahl, and Geo rey E Hinton. 2013. On the importance of initialization and momentum in deep learning.. In Proceedings of the 29th International Conference on Machine Learning (ICML 2013). 1139­1147. [19] Chenyan Xiong and Jamie Callan. 2015. EsdRank: Connecting query and documents through external semi-structured data. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management (CIKM 2015). ACM, 951­960. [20] Chenyan Xiong and Jamie Callan. 2015. ery expansion with Freebase. In
Proceedings of the h ACM International Conference on the eory of Information Retrieval (ICTIR 2015). ACM, 111­120. [21] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2016. Bag-of-Entities representation for ranking. In Proceedings of the sixth ACM International Conference on the
eory of Information Retrieval (ICTIR 2016). ACM, 181­184. [22] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.
2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of
the 40th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2017). ACM, To Appear. [23] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 25th International Conference on World Wide Web (WWW 2017). ACM, 1271­1279. [24] Yang Xu, Gareth JF Jones, and Bin Wang. 2009. ery dependent pseudorelevance feedback based on Wikipedia. In Proceedings of the 32nd Annual In-
ternational ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2009). ACM, 59­66.

772


Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Relevance-based Word Embedding

Hamed Zamani
Center for Intelligent Information Retrieval College of Information and Computer Sciences
University of Massachuse s Amherst Amherst, MA 01003 zamani@cs.umass.edu
ABSTRACT
Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently a racted much a ention in natural language processing and information retrieval tasks. e embedding vectors are typically learned based on term proximity in a large corpus. is means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks. e primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity. is is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. In this paper, we propose two learning models with di erent objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classi es each term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classi cation. Both query expansion experiments on four TREC collections and query classi cation experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models signi cantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe.
KEYWORDS
Word representation, neural network, embedding vector, query expansion, query classi cation
ACM Reference format: Hamed Zamani and W. Bruce Cro . 2017. Relevance-based Word Embedding. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080831
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080831

W. Bruce Cro
Center for Intelligent Information Retrieval College of Information and Computer Sciences
University of Massachuse s Amherst Amherst, MA 01003 cro @cs.umass.edu
1 INTRODUCTION
Representation learning is a long-standing problem in natural language processing (NLP) and information retrieval (IR). e main motivation is to abstract away from the surface forms of a piece of text, e.g., words, sentences, and documents, in order to alleviate sparsity and learn meaningful similarities, e.g., semantic or syntactic similarities, between two di erent pieces of text. Learning representations for words as the atomic components of a language, also known as word embedding, has recently a racted much a ention in the NLP and IR communities.
A popular model for learning word representation is neural network-based language models. For instance, the word2vec model proposed by Mikolov et al. [24] is an embedding model that learns word vectors via a neural network with a single hidden layer. Continuous bag of words (CBOW) and skip-gram are two implementations of the word2vec model. Another successful trend in learning semantic word representations is employing global matrix factorization over word-word matrices. GloVe [28] is an example of such methods. A theoretical relation has been discovered between embedding models based on neural network and matrix factorization in [21]. ese models have been demonstrated to be e ective in a number of IR tasks, including query expansion [11, 17, 40], query classi cation [23, 41], short text similarity [15], and document model estimation [2, 31].
e aforementioned embedding models are typically trained based on term proximity in a large corpus. For instance, the word2vec model's objective is to predict adjacent word(s) given a word or context, i.e., a context window around the target word. is idea aims to capture semantic and syntactic similarities between terms, since semantically/syntactically similar words o en share similar contexts. However, this objective is not necessarily equivalent to the main objective of many IR tasks. e primary objective in many IR methods is to model the notion of relevance [20, 34, 43]. In this paper, we revisit the underlying assumption of typical word embedding methods, as follows:
e objective is to predict the words observed in the documents relevant to a particular information need.
is objective has been previously considered for developing relevance models [20], a state-of-the-art (pseudo-) relevance feedback approach. Relevance models try to optimize this objective given a set of relevant documents for a given query as the indicator of user's information need. In the absence of relevance information, the top ranked documents retrieved in response to the query are assumed to be relevant. erefore, relevance models, and in general all pseudo-relevance feedback models, use an online se ing to obtain training data: retrieving documents for the query and

505

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

then using the top retrieved documents in order to estimate the relevance distribution. Although relevance models have been proved to be e ective in many IR tasks [19, 20], having a retrieval run for each query to obtain the training data for estimating the relevance distribution is not always practical in real-world search engines. We, in this paper, optimize a similar objective in an o ine se ing, which enables us to predict the relevance distribution without any retrieval runs during the test time. To do so, we consider the top retrieved documents for millions of training queries as a training set and learn embedding vectors for each term in order to predict the words observed in the top retrieved documents for each query. We develop two relevance-based word embedding models. e rst one, the relevance likelihood maximization model (RLM), aims to model the relevance distribution over the vocabulary terms for each query, while the second one, the relevance posterior estimation model (RPE), classi es each term as relevant or non-relevant to each query. We provide e cient learning algorithms to train these models on large amounts of training data. Note that our models are unsupervised and the training data is generated automatically.
To evaluate our models, we performed two sets of extrinsic evaluations. In the rst set, we focus on the query expansion task for ad-hoc retrieval. In this set of experiments, we consider four TREC collections, including two newswire collections (AP and Robust) and two large-scale web collections (GOV2 and ClueWeb09 - Cat. B). Our results suggest that the relevance-based embedding models outperform state-of-the-art word embedding algorithms. e RLM model shows be er performance compared to RPE in the context of query expansion, since the goal is to estimate the probability of each term given a query and this distribution is not directly learned by the RPE model. In the second set of experiments, we focus on the query classi cation task using the KDD Cup 2005 [22] dataset. In this extrinsic evaluation, the relevance-based embedding models again perform be er than the baselines. Interestingly, the query classi cation results demonstrate that the RPE model outperforms the RLM model, for the reason that in this task, unlike the query expansion task, the goal is to compute the similarity between two query vectors, and RPE can learn more accurate embedding vectors with less training data.
2 RELATED WORK
Learning a semantic representation for text has been studied for many years. Latent semantic indexing (LSI) [8] can be considered as early work in this area that tries to map each text to a semantic space using singular value decomposition (SVD), a well-known matrix factorization algorithm. Subsequently, Clinchant and Perronnin [5] proposed Fisher Vector (FV), a document representation framework based on continuous word embeddings, which aggregates a non-linear mapping of word vectors into a document-level representation. However, a number of popular IR models, such as BM25 and language models, o en signi cantly outperform the models that are based on semantic similarities. Recently, extremely e cient word embedding algorithms have been proposed to model semantic similarly between words.
Word embedding, also known as distributed representation of words, refers to a set of machine learning algorithms that learn high-dimensional real-valued dense vector representation w  Rd

for each vocabulary term w, where d denotes the embedding dimensionality. GloVe [28] and word2vec [24] are two well-known word embedding algorithms that learn embedding vectors based on the same idea, but using di erent machine learning techniques.
e idea is that the words that o en appear in similar contexts are similar to each other. To do so, these algorithms try to accurately predict the adjacent word(s) given a word or a context (i.e., a few words appeared in the same context window). Recently, Rekabsaz et al. [30] proposed to exploit global context in word embeddings in order to avoid topic shi ing.
Word embedding representations can be also learned as a set of parameters in an end-to-end neural network model. For instance, Zamani et al. [39] trained a context-aware ranking model in which the embedding vectors of frequent n-grams are learned using click data. More recently, Dehghani et al. [9] trained neural ranking models with weak supervision data (i.e., a set of noisy training data automatically generated by an existing unsupervised model) that learn word representations in an end-to-end ranking scenario.
Word embedding vectors have been successfully employed in several NLP and IR tasks. Kusner et al. [16] proposed word mover's distance (WMD), a function for calculating semantic distance between two documents, which measures the minimum traveling distance from the embedded vectors of individual words in one document to the other one. Zhou et al. [47] introduced an embeddingbased method for question retrieval in the context of community question answering. Vulic´ and Moens [37] proposed a model to learn bilingual word embedding vectors from document-aligned comparable corpora. Zheng and Callan [46] presented a supervised embedding-based technique to re-weight terms in the existing IR models, e.g., BM25. Based on the well-de ned structure of language modeling framework in information retrieval, a number of methods have been introduced to employ word embedding vectors within this framework in order to improve the performance in IR tasks. For instance, Zamani and Cro [40] presented a set of embedding-based query language models using the query expansion and pseudo-relevance feedback techniques that bene t from the word embedding vectors. ery expansion using word embedding has been also studied in [11, 17, 35]. All of these approaches are based on word embeddings learned based on term proximity information. PhraseFinder [14] is an early work using term proximity information for query expansion. Mapping vocabulary terms to HAL space, a low-dimensional space compared to vocabulary size, has been used in [4] for query modeling.
As is widely known in the information retrieval literature [11, 38], there is a big di erence between the unigram distribution of words on sub-topics of a collection and the unigram distribution estimated from the whole collection. Given this phenomenon, Diaz et al. [11] recently proposed to train word embedding vectors on the top retrieved documents for each query. However, this model, called local embedding, is not always practical in real-word applications, since the embedding vectors need to be trained during the query time. Furthermore, the objective function in local embedding is based on term proximity in pseudo-relevant documents.
In this paper, we propose two models for learning word embedding vectors, that are speci cally designed for information retrieval needs. All the aforementioned tasks in this section can potentially bene t from the vectors learned by the proposed models.

506

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

3 RELEVANCE-BASED EMBEDDING
Typical word embedding algorithms, such as word2vec [24] and GloVe [28], learn high-dimensional real-valued embedding vectors based on the proximity of terms in a training corpus, i.e., cooccurrence of terms in the same context window. Although these approaches could be useful for learning the embedding vectors that can capture semantic and syntactic similarities between vocabulary terms and have shown to be useful in many NLP and IR tasks, there is a large gap between their learning objective (i.e., term proximity) and what is needed in many information retrieval tasks. For example, consider the query expansion task and assume that a user submi ed the query "dangerous vehicles". One of the most similar terms to this query based on the typical word embedding algorithms (e.g., word2vec and GloVe) is "safe", and thus it would get a high weight in the expanded query model. e reason is that the words "dangerous" and "safe" o en share similar contexts. However, expanding the query with the word "safe" could lead to poor retrieval performance, since it changes the meaning and the intent of the query.
is example together with many others have motivated us to revisit the objective used in the learning process of word embedding algorithms in order to obtain the word vectors that be er match with the needs in IR tasks. e primary objective in many IR tasks is to model the notion of relevance. Several approaches, such as the relevance models proposed by Lavrenko and Cro [20], have been proposed to model relevance. Given the successes achieved by these models, we propose to learn word embedding vectors based on an objective that ma ers in information retrieval. e objective is to accurately predict the terms that are observed in a set of relevant documents to a particular information need.
In the following subsections, we rst describe our neural network architecture, and then explain how to build a training set for learning relevance-based word embeddings. We further introduce two models, relevance likelihood maximization (RLM) and relevance posterior estimation (RPE), with di erent objectives using the described neural network.

3.1 Neural Network Architecture

We use a simple yet e ective feed-forward neural network with a

single linear hidden layer. e architecture of our neural network

is shown in Figure 1. e input of the model is a sparse query

vector qs with the length of N , where N denotes the total number of vocabulary terms. is vector can be obtained by a projection

function given the vectors corresponding to individual query terms.

In this paper, we simply consider average as the projection function.

Hence, qs

=

1 |q |

w q ew , where ew and |q| denote the one-hot

vector representation of term w and the query length, respectively.

e hidden layer in this network maps the given query sparse vector

to a query embedding vector q, as follows:

q = qs × WQ

(1)

where WQ  RN ×d is a weight matrix for estimating query embedding vectors and d denotes the embedding dimensionality. e
output layer of the network is a fully-connected layer given by:

 (q × Ww + bw )

(2)

query sparse vector

hidden layer

qs

output layer
W1 W2 W3

......... .........

d neurons

WN
N neurons

Figure 1: e relevance-based word embedding architecture. e objective is to learn d-dimensional distributed represen-
tation for words based on the notion of relevance, instead of term proximity. N denotes the total number of vocabulary terms.
where Ww  Rd×N and bw  R1×N are the weight and the bias matrices for estimating the probability of each term.  is the activation function which is discussed in Sections 3.3 and 3.4.
To summarize, our network contains two sets of embedding parameters, WQ and Ww . e former aims to map the query into the "query embedding space", while the la er is used to estimate the weights of individual terms.
3.2 Modeling Relevance for Training
Relevance feedback has been shown to be highly e ective in improving retrieval performance [7, 32]. In relevance feedback, a set of relevant documents to a given query is considered for estimating accurate query models. Since explicit relevance signals for a given query are not always available, pseudo-relevance feedback (PRF) assumes that the top retrieved documents in response to the given query are relevant to the query and uses these documents in order to estimate be er query models. e e ectiveness of PRF in various retrieval scenarios indicates that useful information can be captured from the top retrieved documents [19, 20, 44]. In this paper, we make use of this well-known assumption to train our model. It should be noted that there is a signi cant di erence between PRF and the proposed models: In PRF, the feedback model is estimated from the top retrieved documents of the given query in an online se ing. In other words, PRF retrieves the documents for the initial query and then estimates the feedback model using the top retrieved documents. In this paper, we propose to train the model in an o ine se ing. Moving from the online to the o ine se ing would lead to substantial improvements in e ciency, because an extra retrieval run is not needed in the o ine se ing. To learn a model in an o ine se ing, we consider a xed-length dense vector for each vocabulary term and estimate these vectors based on the information extracted from the top retrieved documents for large numbers of training queries. Note that our models are

507

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

unsupervised. However, if explicit relevance data is available, such as click data, without loss of generality, both the explicit or implicit relevant documents can be considered for training our models. We leave studying the vectors learned based on supervised signals for future work.
To formally describe our training data, letT = {(q1, R1), (q2, R2), · · · , (qm, Rm )} be a training set with m training queries. e ith element of this set is a pair of query qi and the corresponding pseudo-relevance feedback distribution. ese distributions are estimated based on the top k retrieved documents (in our experiments, we set k to 10) for each query. e distributions can be estimated using any PRF model, such as those proposed in [20, 36, 42, 44]. In this paper, we only focus on the relevance model [20], a state-of-the-art PRF model, that estimates the relevance distribution as:

p(w |Ri )  p(w |d )

p(w |d )

(3)

d Fi

w qi

where Fi denotes a set of top retrieved documents for query qi . Note that the probability of terms that do not appear in the top
retrieved documents is equal to zero.

3.3 Relevance Likelihood Maximization Model

In this model, the goal is to learn the relevance distribution R.

Given a set of training data, we aim to nd a set of parameters  R

in order to maximize the likelihood of generating relevance model

probabilities for the whole training set. e likelihood function is

de ned as follows:

m

p (w |qi ;  R )p (w | Ri )

(4)

i=1 w Vi

where p is the relevance distribution that can be obtained given
the learning parameters  R and p(w |Ri ) denotes the relevance model distribution estimated for the ith query in the training set
(see Section 3.2 for more detail). Vi denotes a subset of vocabulary terms that appeared in the top ranked documents retrieved for the
query qi . e reason for iterating over the terms that appeared in this set instead of the whole vocabulary set V is that the probability
p(w |Ri ) is equal to zero for all terms w  V - Vi . In this method, we model the probability distribution p using the
so max function (i.e., the function  in Equation (2)) as follows:1

p(w |q;  R ) =

exp (wT q) w V exp (w T q)

(5)

where w denotes the learned embedding vector for term w and q is the query vector came from the output of the hidden layer in our network (see Section 3.1). According to the so max modeling and the log-likelihood function, we have the following objective:

m
arg max

p(w |Ri ) log exp (wT qi ) - log

exp (w T qi )

R i=1 w Vi

w V

(6)

Computing this objective function and its derivatives would

be computationally expensive (due to the presence of the normal-

ization factor w V exp (w T q) in the objective function). Since all the word embedding vectors as well as the query vector are

1For simplicity, we drop the bias term in these equations.

changed during the optimization process, we cannot simply omit the normalization term as is done in [41] for estimating query embedding vectors based on pre-trained word embedding vectors. To make the computations more tractable, we consider a hierarchical approximation of the so max function, which was introduced by Morin and Bengio [26] in the context of neural network language models and then successfully employed by Mikolov et al. [24] in the word2vec model.
e hierarchical so max approximation uses a binary tree structure to represent the vocabulary terms, where each leaf corresponds to a unique word. ere exists a unique path from the root to each leaf, and this path is used for estimating the probability of the word representing by the leaf. erefore, the complexity of calculating so max probabilities goes down from O (|V |) to O (log(|V |)) which is the height of the tree. is leads to a huge improvement in computational complexity. We refer the reader to [25, 26] for the details of calculating the hierarchical so max approximation.

3.4 Relevance Posterior Estimation Model

As an alternative to maximum likelihood estimation, we can estimate the relevance posterior probability. In the context of pseudorelevance feedback, Zhai and La ery [44] assumed that the language model of the top retrieved documents is estimated based on a mixture model. In other words, it is assumed that there are two language models for the feedback set: the relevance language model2 and a background noisy language model. ey used an expectationmaximization algorithm to estimate the relevance language model. In this model, we make use of this assumption in order to cast the problem of estimating the relevance distribution R as a classi cation task: Given a pair of word w and query q, does w come from the relevance distribution of the query q? Instead of p(w |R), this model estimates p(R = 1|w, q;  R ) where R is a Boolean variable and R = 1 means that the given term-query pair (w, q) comes from the relevance distribution R.  R is a set of parameters that is going to be learned during the training phase.
erefore, the problem is cast as a binary classi cation task that can be modeled by logistic regression (which means the function  in Equation (2) is the sigmoid function):

1

p (R = 1|w, q;  R ) = 1 + e (-wT q)

(7)

where w is the relevance-based word embedding vector for term w.

Similar to the previous model, q is the output of the hidden layer

of the network, representing the query embedding vector.

In order to address this binary classi cation problem, we consider

a cross-entropy loss function. In theory, for each training query,

our model should learn to model relevance for the terms appearing

in the corresponding pseudo-relevant set and non-relevance for all

the other vocabulary terms, which could be impractical, due to the

large number of vocabulary terms. Similar to [24], we propose to

use the noise contrastive estimation (NCE) [12] which hypothesizes

that we can achieve a good model by only di erentiating the data

from noise via a logistic regression model. e main concept in NCE

is similar to those proposed in the divergence from randomness

model [3] and the divergence minimization feedback model [44].

2 e phrase "topical language model" was used in the original work [44]. We call it "relevance language model" to have consistent de nitions in our both models.

508

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Based on the NCE hypothesis, we de ne the following negative cross-entropy objective function for training our model:

arg

max
R

m i =1



+ j =1

Ewj p (w

| Ri

)

log p(R = 1|wj , qi ;  R )

-
+ Ewj pn (w )
j =1

log p(R = 0|wj , qi ;  R ) 

(8)

where pn (w ) denotes a noise distribution and  = (+, -) is a pair of hyper-parameters to control the number of positive and negative instances per query, respectively. We can easily calculate p(R = 0|wj , qi ) = 1 - p(R = 1|wj , qi ). e noise distribution pn (w ) can be estimated using a function of unigram distribution U (w ) in the whole training set. Similar to [24], we use pn (w )  U (w )3/4 which has been empirically shown to work e ectively for negative sampling.
It is notable that although this model learns embedding vectors for both queries and words, it is not obvious how to calculate the probability of each term given a query; because Equation 7 only gives us a classi cation probability and we cannot simply use the Bayes rule here (since, not all probability components are known). is model can perform well when computing the similarity between two terms or two queries, but not a query and a term. However, we can use the model presented in [41] to estimate the query model using the word embedding vectors (not the ones learned for query vectors) and then calculate the similarity between a query and a term.

4 EXPERIMENTS
In this section, we rst describe how we train the relevance-based word embedding models. We further extrinsically evaluate the learned embeddings using two IR tasks: query expansion and query classi cation. Note that the main aim here is to compare the proposed models with the existing word embedding algorithms, not with the state-of-the-art query expansion and query classi cation models.

4.1 Training
In order to train relevance-based word embeddings, we obtained millions of unique queries from the publicly available AOL query logs [27]. is dataset contains a sample of web search queries from real users submi ed to the AOL search engine within a three-month period from March 1, 2006 to May 31, 2006. We only used query strings and no session and click information was obtained from this dataset. We ltered out the navigational queries containing URL substrings, i.e., "h p", "www.", ".com", ".net", ".org", ".edu". All nonalphanumeric characters were removed from all queries. Applying all these constraints leads to over 6 millions unique queries as our training query set. To estimate the relevance model distributions in the training set, we considered top 10 retrieved documents in a target collection in response to each query using the Galago3 implementation of the query likelihood retrieval model [29] with Dirichlet prior smoothing (µ = 1500) [45].

3h p://www.lemurproject.org/galago.php

We implemented and trained our models using TensorFlow4. e networks are trained based on the stochastic gradient descent optimizer using the back-propagation algorithm [33] to compute the gradients. All model hyper-parameters were tuned on the training set (the hyper-parameters with the smallest training loss value were selected). For each model, the learning rate and the batch size were selected from [0.001, 0.01, 0.1, 1] and [64, 128, 256], respectively. For RPE , we also tuned the number of positive and negative instances (i.e., + and -). e value of + was swept between [20, 50, 100, 200] and the parameter - was selected from [5+, 10+, 20+]. As suggested in [40], in all the experiments (unless otherwise stated) the embedding dimensionality was set to 300, for all models including the baselines.

4.2 Evaluation via ery Expansion
In this subsection, we evaluate the embedding models in the context of query expansion for the ad-hoc retrieval task. In the following, we rst describe the retrieval collections used in our experiments. We further explain our experimental setup as well as the evaluation metrics. We nally report and discuss the query expansion results.

4.2.1 Data. We use four standard test collections in our experiments. e rst two collections (AP and Robust) consist of thousands of news articles and are considered as homogeneous collections. AP and Robust were previously used in TREC 1-3 Ad-Hoc Track and TREC 2004 Robust Track, respectively. e second two collections (GOV2 and ClueWeb) are large-scale web collections containing heterogeneous documents. GOV2 consists of the ".gov" domain web pages, crawled in 2004. ClueWeb (i.e., ClueWeb09Category B) is a common web crawl collection that only contains English web pages. GOV2 and ClueWeb were previously used in TREC 2004-2006 Terabyte Track and TREC 2009-2012 Web Track, respectively. e statistics of these collections as well as the corresponding TREC topics are reported in Table 1. We only used the title of topics as queries.

4.2.2 Experimental Setup. We cleaned the ClueWeb collection
by ltering out the spam documents. e spam ltering phase was done using the Waterloo spam scorer5 [6] with the threshold of 60%.

Stopwords were removed from all collections using the standard

INQUERY stopword list and no stemming were performed.

For the purpose of query expansion, we consider the language
modeling framework [29] and estimate a query language model
based on a given set of word embedding vectors. e expanded query language model p(w |q ) is estimated as:

p(w |q ) = pML (w |q) + (1 -  )p(w |q)

(9)

where pML (w |q) denotes maximum likelihood estimation of the original query and  is a free hyper-parameter that controls the weight of original query model in the expanded model. e probability p(w |q) is calculated based on the trained word embedding vectors. In our rst model, this probability can be estimated using Equation (5); while in the second model, we should simply use the Bayes rule given Equation (7) to estimate this probability. However, since we do not have any information about the probability of each

4h p://tensor ow.org/ 5h p://plg.uwaterloo.ca/gvcormac/clueweb09spam/

509

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Collections statistics.

ID AP Robust
GOV2
ClueWeb

collection Associated Press 88-89 TREC Disks 4 & 5 minus Congressional Record
2004 crawl of .gov domains
ClueWeb 09 - Category B

queries (title only) TREC 1-3 Ad-Hoc Track, topics 51-200
TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 2004-2006 Terabyte Track,
topics 701-850 TREC 2009-2012 Web Track
topics 1-200

#docs 165k 528k
25m
50m

avg doc length 287 254
648
1506

#qrels 15,838 17,412
26,917
18,771

Table 2: Evaluating relevance-based word embeddings in the context of query expansion. e superscripts 0/1/2/3/4 denote that the MAP improvements over MLE/word2vec-external/word2vec-target/GloVe-external/GloVe-target are statistically signi cant. e highest value in each row is marked in bold.

Collection AP
Robust GOV2 ClueWeb

Metric
MAP P@20 NDCG@20
MAP P@20 NDCG@20
MAP P@20 NDCG@20
MAP P@20 NDCG@20

MLE
0.2197 0.3503 0.3924
0.2149 0.3319 0.3863
0.2702 0.5132 0.4482
0.1028 0.3025 0.2237

word2vec

external target

0.2399 0.3688 0.4030

0.2420 0.3738 0.4181

0.2218 0.3357 0.3918

0.2215 0.3337 0.3881

0.2740 0.5257 0.4571

0.2723 0.5172 0.4509

0.1033 0.3040 0.2235

0.1033 0.3053 0.2252

GloVe

external target

0.2319 0.3581 0.4025

0.2389 0.3631 0.4098

0.2209 0.3345 0.3918

0.2172 0.3281 0.3844

0.2718 0.5186 0.4539

0.2709 0.5128 0.4485

0.1029 0.3033 0.2244

0.1026 0.3048 0.2244

Rel.-based Embedding

RLM

RPE

0.258001234 0.388601234 0.424201234
0.245001234 0.347601234 0.398201234
0.286701234 0.536701234 0.45760234
0.106601234
0.3073 0.227301

0.254301234 0.3812034 0.422601234
0.237201234 0.3409024 0.39550
0.285501234 0.535801234 0.4557024
0.1031
0.3030
0.2241

term given a query, we use the uniform distribution. For other word embedding models (i.e., word2vec and GloVe), we use the standard method described in [11]. For all the models, we ignore the terms whose embedding vectors are not available.
We retrieve the documents for the expanded query language model using the KL-divergence formula [18] with Dirichlet prior smoothing (µ = 1500) [45]. All the retrieval experiments were carried out using the Galago toolkit [7].
In all the experiments, the parameters  (the linear interpolation coe cient) and m (the number of expansion terms) were set using 2-fold cross-validation over the queries in each collection. We selected the parameter  from {0.1, . . . , 0.9} and the parameter m from {10, 20, ..., 100}.
4.2.3 Evaluation Metrics. To evaluate the e ectiveness of query expansion models, we report three standard evaluation metrics: mean average precision (MAP) of the top ranked 1000 documents, precision of the top 20 retrieved documents (P@20), and normalized discounted cumulative gain [13] calculated for the top 20 retrieved documents (nDCG@20). Statistically signi cant di erences of MAP, P@20, and nDCG@20 values based on the two-tailed paired t-test are computed at a 95% con dence level (i.e., p alue < 0.05).
4.2.4 Results and Discussion. To evaluate our models, we consider the following baselines: (i) the standard maximum likelihood estimation (MLE) of the query model without query expansion, (ii) two sets of embedding vectors (one trained on Google News as a

large external corpus and one trained on the target retrieval collection) learned by the word2vec model6 [24], and (iii) two sets of embedding vectors (one trained on Wikipedia 2004 plus Gigawords 5 as a large external corpus7 and the other on the target retrieval collection) learned by the GloVe model [28].
Table 2 reports the results achieved by the proposed models and the baselines. According to this table, all the query expansion models outperform the MLE baseline in nearly all cases, which indicates the e ectiveness of employing high-dimensional word representations for query expansion. Similar observations have been made in [11, 17, 40, 41]. According to the results, although word2vec performs slightly be er than GloVe, no signi cant di erences can be observed between their performances. According to Table 2, both relevance-based embedding models outperform all the baselines in all the collections, which shows the importance of taking relevance into account for training embedding vectors. ese improvements are o en statistically signi cant compared to all the baselines. e relevance likelihood maximization model (RLM) performs be er than the relevance posterior estimation model (RPE) in all cases and the reason is related to their objective function. RLM learns the relevance distribution for all terms, while RPE learns the classi cation probability of being relevance for vocabulary terms (see Equations (5) and (7)).

6We use the CBOW implementation of the word2vec model.
also performs similarly. 7Available at h p://nlp.stanford.edu/projects/glove/.

e skip-gram model

510

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: Top 10 expansion terms obtained by the word2vec and the relevance-based word embedding models for two sample queries "indian american museum" and "tibet protesters".

query: "indian american museum"

word2vec

Rel.-based Embedding

external

target

RLM

RPE

history

powwows

chumash

heye

art

smithsonian heye

collection

culture

afro

artifacts

chumash

british

mesoamerica smithsonian smithsonian

heritage

smithsonians collection

york

society

native

washington new

states

heye

institution

apa

contemporary hopi

york

native

part

mayas

native

americans

united

cimam

apa

history

To get a sense of what is learned by each of the embedding models8, in Table 3 we report the top 10 expansion terms for two sample queries from the Robust collection. According to this table, the terms added to the query by the word2vec model are syntactically or semantically related to individual query terms, which is expected. For the query "indian american museum" as an example, the terms "history", "art", and "culture" are related to the query term "museum", while the terms "united" and "states" are related to the query term "american". In contrast, looking at the expansion terms obtained by the relevance-based word embeddings, we can see that some relevant terms to the whole query were selected. For instance, "chumash" (a group of native americans)9, "heye" (the national museum of the American Indian in New York), "smithsonian" (the national museum of the American Indian in Washington DC), and "apa" (the American Psychological Association that actively promotes American Indian museums). A similar observation can be made for the other sample query (i.e., "tibet protesters"). For example, the word "independence" is related to the whole query that was only selected by the relevance-based word embedding models, while the terms "protestors", "protests", "protest", and "protesting" that are syntactically similar to the query term "protesters" were considered by the word2vec model. We believe that these di erences are due to the learning objective of the models. Interestingly, the expansion terms added to each query by the two relevance-based models look very similar, but according to Table 2, their performances are quite di erent. e reason is related to the weights given to each term by the two models. e weights given to the expansion terms by RPE are very close to each other because its objective is to just classify each term and all of these terms are classi ed with a high probability as "relevant".
In the next set of experiments, we consider the methods that use the top retrieved documents for query expansion: the relevance model (RM3) [1, 20] as a state-of-the-art pseudo-relevance feedback model, and the local embedding approach recently proposed by Diaz et al. [11] with the general idea of training word embedding models on the top ranked documents retrieved in response to a given query. Similar to [11], we use the word2vec model to train
8For the sake of space, we only report the expanded terms estimated by the word2vec model and the proposed models. 9see h ps://en.wikipedia.org/wiki/Chumash people

query: "tibet protesters"

word2vec

Rel.-based Embedding

external

target

RLM

RPE

demonstrators tibetan

tibetan

tibetan

protestors

lhasa

lama

tibetans

tibetan

demonstrators tibetans

lama

protests

tibetans

lhasa

independence

tibetans

marchers

dalai

lhasa

protest

lhasas

independence dalai

activists

jokhang

protest

open

protesting

demonstrations open

protest

lhasa

dissidents

zone

zone

demonstrations barkhor

followers

jokhang

Table 4: Evaluating relevance-based word embedding in pseudo-relevance feedback scenario. e superscripts 1/2/3 denote that the MAP improvements over RM3/Local Embedding/ERM with Local Embedding are statistically signi cant.
e highest value in each row is marked in bold.

Collection AP
Robust GOV2 ClueWeb

Metric
MAP P@20 NDCG@20
MAP P@20 NDCG@20
MAP P@20 NDCG@20
MAP P@20 NDCG@20

RM3
0.2927 0.4034 0.4368
0.2593 0.3486 0.4011
0.2863 0.5318 0.4503
0.1079 0.3111 0.2309

Local
Emb.
0.2412 0.3742 0.4173
0.2235 0.3366 0.3868
0.2748 0.5271 0.4576
0.1041 0.3062 0.2261

ERM

Local RLM

0.3047 0.4105 0.4411
0.2643 0.3498 0.4080
0.2924 0.5379 0.4584

0.311912 0.423312 0.4495123
0.2761123 0.3605123 0.4173123
0.2986123 0.541712 0.4603123

0.1094 0.112112
0.3145 0.3168 0.2328 0.23602

word embedding vectors on top 1000 documents. e results are reported in Table 4. In this table, ERM refers to the embedding-based relevance model recently proposed by Zamani and Cro [40] in order to make use of semantic similarities estimated based on the word embedding vectors in a pseudo-relevance feedback scenario. According to Table 4, the ERM model that uses the relevance-based word embedding (RLM10) outperforms all the other methods. ese improvements are statistically signi cant in most cases. By comparing the results obtained by local embedding and those reported in Table 2, it can be observed that there are no substantial di erences between the results for local embedding and word2vec. is is similar to what is reported by Diaz et al. [11] when the embedding vectors are trained on the top documents in the target collection, similar to our se ing. Note that the relevance-based model was also trained on the target collection.
10For the sake of space, we only consider RLM which shows be er performance compared to RPE in query expansion.

511

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

MAP
0.06 0.07 0.08 0.09 0.1 0.23 0.24 0.25 0.26

 
 


0.30

0.25 0.20





















0.15 

MAP

 AP Robust GOV2 ClueWeb

5

10

15

20

25

# expansion terms

0.10 0.05 0.00
0.0

 AP Robust GOV2 ClueWeb

0.2

0.4

0.6

0.8

1.0

interpolation coefficient

(a) # expansion terms

(b) interpolation coe cient

Figure 2: Sensitivity of RLM to the number of expansion terms and the interpolation coe cient (), in terms of MAP.

0.29

MAP
0.06 0.08 0.1 0.18 0.2 0.22 0.24 0.26 0.28

0.27







 







 

0.25

MAP

0.09 0.1 0.11

0.07

 AP Robust GOV2 ClueWeb

100

200

300

400

500

embedding dimension

Figure 3: Sensitivity of RLM to the dimension of embedding vectors, in terms of MAP.

An interesting observation from Tables 2 and 4 is that the RLM performance (without using pseudo-relevant documents) in Robust and GOV2 is very close to the RM3 performance, and is slightly be er in the GOV2 collection. Note that RM3 needs two retrieval runs11 and uses top retrieved documents, while RLM only needs one retrieval run. is is an important issue in many real-world applications, since the e ciency constraints do not always allow them to have two retrieval runs per query.
Parameter Sensitivity. In the next set of experiments, we study the sensitivity of RLM as the best performing word embedding model in Table 2 to the expansion parameters. Figure 2a plots the sensitivity of RLM to the number of expansion terms where the parameter  is set to 0.5. According to this gure, in both newswire collections, the method shows its best performance when the queries are expanded with only 10 words. In the GOV2 collection, 15 words are needed for the method to show its best performance.
Figure 2b plots the sensitivity of the methods to the interpolation coe cient  (see Equation 9) where the number of expansion terms is set to 10. According to the curves correspond to AP and Robust, the original query language model needs to be interpolated with the model estimated using relevance-based word embeddings
11Diaz [10] showed that for precision-oriented tasks, the second retrieval run can be restricted to the initial rank list for improving the e ciency of PRF models. However, for recall-oriented metrics, e.g., MAP, the second retrieval helps a lot.

 AP Robust GOV2 ClueWeb

1

2

3

4

5

million queries

Figure 4: e Performance of RLM with respect to di erent amount of training data (training queries), in terms of MAP.
with equal weights (i.e.,  = 0.5). is shows the quality of the estimated distribution via the learned embedding vectors. In the GOV2 collection, a higher weight should be given to the original query model, which indicates that the original query plays a key role in achieving good retrieval performance in this collection.
We also study the performance of RLM as the best performing word embedding model for query expansion with respect to the embedding dimensionality. e results are shown in Figure 3, where the query expansion performance generally improves as we increase the embedding dimensionality. e performances become stable when the dimension is larger than 300. is experiment suggests that 400 dimensions would be enough for the relevance-based embedding model.
Due to the large number of parameters in the neural networks, they can require large amounts of training data to achieve good performance. In the next set of experiments, we study how much training data is needed for training our best model. e results are plo ed in Figure 4. According to this gure, by increasing the number of training queries from one million to four million queries, the performance signi cantly increases, and becomes more stable a er four million queries.

4.3 Evaluation via ery Classi cation
In this subsection, we evaluate the proposed embedding models in the context of query classi cation. In this task, each query is

512

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: Evaluating embedding algorithms via query classication. e superscripts 1/2 denote that the improvements
over word2vec/GloVe are signi cant. e highest value in each column is marked in bold.

Method word2vec GloVe Rel.-based Embedding - RLM Rel.-based Embedding - RPE

Precision
0.3712
0.3643 0.394312 0.396112

F1-measure
0.4008
0.3912 0.426712 0.429412

assigned to a number of labels (categories) which are pre-de ned and a few training queries are available for each label. is is a supervised multi-label classi cation task with li le training data.

4.3.1 Data. We consider the dataset that was introduced in KDD Cup 2005 [22] for the internet user search query categorization task and was previously used in [41] for evaluating query embedding vectors. is dataset contains 800 web queries submi ed by real users randomly collected from the MSN search logs. e queries do not contain "junk" text or non-English terms. e queries were labelled by three human editors. 67 categories were pre-de ned and up to 5 labels were selected for each query by each editor.

4.3.2 Experimental Setup. In our experiments, we performed 5-fold cross-validation over the queries and the reported results are the average of those obtained over the test folds. In all experiments, the spelling errors in queries were corrected in a pre-processing phase, the stopwords were removed from queries (using the INQUERY stopword list), and no stemming was performed.
To classify each query, we consider a very simple kNN-based approach proposed in [41]. We rst compute the probability of each category/label given each query q and then select the top t categories with the highest probabilities. e probability p(Ci |q) is computed as follows:

p(Ci |q) =  (Ci , q)   (Ci , q)

(10)

j  (Cj , q)

where Ci denotes the ith category. Ci is the centroid vector of all query embedding vectors with the label of Ci in the training set. We ignore the query terms whose embedding vectors are not available. e number of labels assigned to each query was tuned on the training set from {1, 2, 3, 4, 5}. In the query classi cation experiments, we trained relevance-based word embedding using Robust as the collection.

4.3.3 Evaluation Metrics. We consider two evaluation metrics that were also used in KDD Cup 2005 [22]: precision and F1measure. Since the labels assigned by the three human editors di er in some cases, all the label sets should be taken into account.
ese metrics are computed in the same way as what is described in [22] for evaluating the KDD Cup 2005 submi ed runs. Statistically signi cant di erences are determined using the two-tailed paired t-test computed at a 95% con dence level (p - alue < 0.05).

4.3.4 Results and Discussion. We compare our models against the word2vec and GloVe methods trained on the external collections that are described in the query expansion experiments. e results are reported in Table 5, where the relevance-based embedding

0.430

F1-measure

0.428

0.426






0.424



0.422


0.420

 RLM RPE

100

200

300

400

500

embedding dimension
Figure 5: Sensitivity of the relevance-based embedding models to the embedding dimensionality, in terms of F1measure.

0.42 0.40

 


F1-measure

0.38 
0.36

0.34

1

 RLM RPE

2

3

4

5

million queries

Figure 6: e Performance of relevance-based embedding models with respect to di erent amount of training data (training queries), in terms of F1-measure.

models signi cantly outperform the baselines in terms of both metrics. An interesting observation here is that contrary to the query expansion experiments, RPE performs be er than RLM in query classi cation. e reason is that in query expansion the weight of each term is considered in order to generate the expanded query language model. erefore, in addition to the order of terms, their weights should be also e ective for improving the retrieval performance with query expansion. In query classi cation, we only assign a few categories to each query, and thus as long as the order of categories is correct, the similarity values between the queries and the categories do not ma er.
In the next set of experiments, we study the performance of our relevance-based word embedding models with respect to the embedding dimensionality. e results are plo ed in Figure 5. According to this gure, the performance is generally improved by increasing the embedding dimensionality, and becomes stable when the dimension is greater than 400. is is similar to our observation in the query expansion experiments. We also study the amount of data needed for training our models in Figure 6. According to this gure, at least 4 million queries are needed in order to learn accurate relevance-based word embeddings. It can be seen from Figure 6 that RLM needs more training data compared to RPE in order to perform well, because by increasing the amount of training data the learning curves of these two models get closer.

513

Session 4C: Queries and Query Analysis

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

5 CONCLUSIONS AND FUTURE WORK
In this paper, we revisited the underlying assumption in typical word embedding models, such as word2vec and GloVe. Instead of learning embedding vectors based on term proximity, we proposed learning embeddings based on the notion of relevance, which is the primary objective in many IR tasks. We developed two neural network-based models for learning relevance-based word embeddings. e rst model, the relevance likelihood maximization model, aims to estimate the probability of each word in a relevance distribution for each query, while the second one, the relevance posterior estimation model, classi es each term as belonging to relevant or non-relevant class for each query. We evaluated our models using two sets of extrinsic evaluation: query expansion and query classi cation. e query expansion experiments using four standard TREC collections, two newswire and two large-scale web collections, suggested that the relevance-based word embedding models outperform state-of-the-art word embedding algorithms. We showed that the expansion terms chosen by our models are related to the whole query, while those chosen by typical word embedding models are related to individual query terms. e query classi cation experiments also validated these ndings and investigated the e ectiveness of our models.
In the future, we intend to evaluate the learned embedding models in other IR tasks, such as query reformulation, query intent prediction, etc. We can also achieve more accurate relevance-based embedding vectors by considering the clicked documents for training query, instead of or in addition to the top retrieved documents.
Acknowledgements. e authors thank Daniel Cohen, Mostafa Dehghani, and Qingyao Ai for their invaluable comments. is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions,
ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.
REFERENCES
[1] Nasreen Abdul-jaleel, James Allan, W. Bruce Cro , Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In TREC '04.
[2] Qingyao Ai, Liu Yang, Jiafeng Guo, and W. Bruce Cro . 2016. Analysis of the Paragraph Vector Model for Information Retrieval. In ICTIR '16. 133­142.
[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357­389.
[4] P. D. Bruza and D. Song. 2002. Inferring ery Models by Computing Information Flow. In CIKM '02. 260­269.
[5] Stephane Clinchant and Florent Perronnin. 2013. Aggregating Continuous Word Embeddings for Information Retrieval. In CVSC@ACL '13. 100­109.
[6] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. E cient and E ective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (2011), 441­465.
[7] Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company.
[8] Sco Deerwester, Susan T. Dumais, George W. Furnas, omas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. 41, 6 (1990), 391­407.
[9] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In SIGIR '17.
[10] Fernando Diaz. 2015. Condensed List Relevance Models. In ICTIR '15. 313­316. [11] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with
Locally-Trained Word Embeddings. In ACL '16. [12] Michael U. Gutmann and Aapo Hyva¨rinen. 2012. Noise-contrastive Estimation of
Unnormalized Statistical Models, with Applications to Natural Image Statistics. J. Mach. Learn. Res. 13, 1 (2012), 307­361. [13] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446.

[14] Yufeng Jing and W. Bruce Cro . 1994. An Association esaurus for Information Retrieval. In RIAO '94. 146­160.
[15] Tom Kenter and Maarten de Rijke. 2015. Short Text Similarity with Word Embeddings. In CIKM '15. 1411­1420.
[16] Ma J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings to Document Distances. In ICML '15. 957­966.
[17] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. ery Expansion Using Word Embeddings. In CIKM '16. 1929­1932.
[18] John La erty and Chengxiang Zhai. 2001. Document Language Models, ery Models, and Risk Minimization for Information Retrieval. In SIGIR '01. 111­119.
[19] Victor Lavrenko, Martin Choque e, and W. Bruce Cro . 2002. Cross-lingual Relevance Models. In SIGIR '02. 175­182.
[20] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR '01. 120­127.
[21] Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. In NIPS '14. 2177­2185.
[22] Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005. KDD CUP-2005 Report: Facing a Great Challenge. SIGKDD Explor. Newsl. 7, 2 (2005), 91­99.
[23] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang. 2015. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classi cation and Information Retrieval. In NAACL '15. 912­921.
[24] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS '13. 3111­3119.
[25] Andriy Mnih and Geo rey E Hinton. 2009. A Scalable Hierarchical Distributed Language Model. In NIPS '09. 1081­1088.
[26] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model. In AISTATS '05. 246­252.
[27] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In InfoScale '06.
[28] Je rey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP '14. 1532­1543.
[29] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In SIGIR '98. 275­281.
[30] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In SIGIR '17.
[31] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In CIKM '16. 711­720.
[32] J. J. Rocchio. 1971. Relevance Feedback in Information Retrieval. In e SMART Retrieval System: Experiments in Automatic Document Processing. 313­323.
[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning representations by back-propagating errors. Nature 323 (Oct. 1986), 533­536.
[34] T. Saracevic. 2016. e Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Morgan & Claypool Publishers.
[35] Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. 2014. Learning Concept Embeddings for ery Expansion by antum Entropy Minimization. In AAAI '14. 1586­1592.
[36] Tao Tao and ChengXiang Zhai. 2006. Regularized Estimation of Mixture Models for Robust Pseudo-relevance Feedback. In SIGIR '06. 162­169.
[37] Ivan Vulic´ and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings. In SIGIR '15. 363­372.
[38] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global Document Analysis. In SIGIR '96. 4­11.
[39] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017. Situational Context for Ranking in Personal Search. In WWW '17. 1531­1540.
[40] Hamed Zamani and W. Bruce Cro . 2016. Embedding-based ery Language Models. In ICTIR '16. 147­156.
[41] Hamed Zamani and W. Bruce Cro . 2016. Estimating Embedding Vectors for eries. In ICTIR '16. 123­132.
[42] Hamed Zamani, Javid Dadashkarimi, Azadeh Shakery, and W. Bruce Cro . 2016. Pseudo-Relevance Feedback Based on Matrix Factorization. In CIKM '16. 1483­ 1492.
[43] ChengXiang Zhai, William W. Cohen, and John La erty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR '03. 10­17.
[44] Chengxiang Zhai and John La erty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01. 403­410.
[45] Chengxiang Zhai and John La erty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (2004), 179­214.
[46] Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. In SIGIR '15. 575­584.
[47] Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for estion Retrieval in Community estion Answering. In ACL '15. 250­259.

514


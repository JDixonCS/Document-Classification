Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Reinforcement Learning to Rank with Markov Decision Process

Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng
CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences zengwei@so ware.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn

ABSTRACT
One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures such as normalized discounted cumulative gain (NDCG). Existing methods usually focus on optimizing a speci c evaluation measure calculated at a xed position, e.g., NDCG calculated at a xed position K. In information retrieval the evaluation measures, including the widely used NDCG and P@K, are usually designed to evaluate the document ranking at all of the ranking positions, which provide much richer information than only measuring the document ranking at a single position. us, it is interesting to ask if we can devise an algorithm that has the ability of leveraging the measures calculated at all of the ranking postilions, for learning a be er ranking model. In this paper, we propose a novel learning to rank model on the basis of Markov decision process (MDP), referred to as MDPRank. In the learning phase of MDPRank, the construction of a document ranking is considered as a sequential decision making, each corresponds to an action of selecting a document for the corresponding position. e policy gradient algorithm of REINFORCE is adopted to train the model parameters. e evaluation measures calculated at every ranking positions are utilized as the immediate rewards to the corresponding actions, which guide the learning algorithm to adjust the model parameters so that the measure is optimized. Experimental results on LETOR benchmark datasets showed that MDPRank can outperform the state-of-the-art baselines.
KEYWORDS
learning to rank; Markov decision process
ACM Reference format: Zeng Wei, Jun Xu , Yanyan Lan, Jiafeng Guo, Xueqi Cheng. 2017. Reinforcement Learning to Rank with Markov Decision Process. In Proceedings of SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080685
1 INTRODUCTION
Learning to rank has been widely used in information retrieval and recommender systems. Among the learning to rank methods,
* Corresponding author: Jun Xu
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080685

directly optimizing the ranking evaluation measures is a representative approach and has been proved to be e ective. Following the idea of directly optimizing evaluation measures, a number of methods have been proposed, including SVMMAP [17] , Adarank [14] , and PermuRank [15] etc. In training, these methods rst construct a loss function on the basis of a prede ned ranking evaluation measure. en, approximations of the loss function or convex upper bounds upon the loss function are constructed and then optimized, leading to di erent directly optimizing learning to rank algorithms. In ranking, each document is assigned a relevance score based on the learned ranking model.
Several learning to rank models that directly optimize evaluation measure have been proposed and applied to variant ranking tasks. Di erent loss functions and optimization techniques are adopted in these methods. For example, the loss function of SVMMAP is constructed on the basis of MAP. e upper bound of the hinge loss is de ned and is optimized with structure SVM. AdaRank, another directly optimizing method, construct its loss function on the basis of any evaluation measure whose values are between 0 and 1. Exponential upper bound is constructed and Boosting is adopted for conduct the optimization.
In general, all the methods that directly optimize evaluation measures perform well in many ranking tasks, especially in terms of the speci c evaluation measure used in the training phase. We also note that some widely used evaluation measures are designed to measure the goodness of a document ranking at all of the ranking positions. For example, the evaluation measure of NDCG can be calculated at all of the ranking positions, each re ects the goodness of the document ranking from the beginning to the corresponding position. Existing methods, however, can only directly optimize the evaluation measure calculated at a prede ned ranking position. For example, the AdaRank algorithm will focus on optimizing NDCG at rank K, if its loss function is con gured based on the evaluation measure NDCG@K. e information carried by the documents a er the rank K are ignored, because they have no contribution to NDCG@K. us, it is natural to ask is it possible to devise a learning to rank algorithm that directly optimizes evaluation measures and has the ability to leverage the measures at all of the ranks?
To answer the above question, we propose a new learning to rank model on the basis of Markov decision process (MDP) [10], called MDPRank. e training phase of MDPRank considers the construction of a document ranking as a process of sequential decision making and learns the the model parameters through maximizing the cumulated rewards to all of the decisions. Speci cally, the ranking of M documents is considered as a sequence of M discrete time steps where each time step corresponds to a ranking position.
e ranking of documents, thus, is formalized as a sequence of M decisions and each action corresponds to selecting one document. At each time step, the agent receives the environment's state and

945

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

chooses an action on the basis of the state. One time step later, as a consequence of the action the system transit to a new state. At each time step, the chosen of the action depends on a policy, which is a function maps from the current state to a probability distribution of selecting each possible action.
Reinforcement learning is employed to train the model parameters. Given a set of labeled queries, at each time step, the agent can receive a numerical action-dependent reward which is de ned upon the evaluation measure calculated at that position. e policy gradient algorithm of REINFORCE [10] is adopted to adjust the model parameters so that expected long-term discounted rewards in terms of the evaluation measure is maximized. us, MDPRank can directly optimize the evaluation measure and leverages the performances at all of the ranks as rewards.
Compared with existing methods that directly optimize IR measure, MDPRank enjoys the following advantages: 1) the ability of utilizing the IR measures calculated at all of the ranking positions as supervision in training; 2) directly optimizes the IR measure on the training data without approximation or upper bounding.
To evaluate the e ectiveness of MDPRank, we conducted experiments on the basis of LETOR benchmark datasets. e experimental results showed that MDPRank can outperform the state-of-the-art learning to rank models including the methods that directly optimize evaluation measures such as SVM-MAP and AdaRank.
2 RELATED WORK
In learning to rank for information retrieval, one of the most straightforward way to learn the ranking model is directly optimizing the measure used for evaluating the ranking performance. A number of methods have been developed in recent years. Some methods try to construct a continuous and di erentiable approximation of the measure-based ranking error. For example, So Rank [11] makes use of the expectation of NDCG over all possible rankings as an approximation of the original evaluation measure NDCG. SmoothRank [2] smooth the evaluation measure by approximating the rank position. Some other methods try to optimize a continuous and di erentiable upper bound of the measure-based ranking error. For example, SVMMAP [17] , SVMNDCG [1] optimize the relaxation of IR evaluation measures of MAP and NDCG, respectively. Structure SVM is adopted for conducting the optimization in both of these two methods. AdaRank [14] directly optimizes the exponential upper bound of the ranking error with a Boosting procedure. [15] summarized the framework of directly optimizing evaluation measure and new algorithms can be derived and analyzed under the framework. Recently, the idea is applied to search result diversi cation. For example, in Xu et al. [16] and Xia et al. [13], structure Perceptron is utilized to directly optimizes the diversity evaluation measures.
In this paper we propose to directly optimize ranking evaluation measure with MDP [10], which has been widely used in variant IR applications. For example, in [6], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [18], the log-based document re-ranking is also modeled as a POMDP to improve the re-ranking performances. MDP is also used for building recommender systems.

For example, [9] designed an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. e multibandit also widely used for ranking [4, 8] and recommendation [5].

3 MDP FORMULATION OF LEARNING TO RANK

In supervised learning se ings, we are given N labeled training

queries

(q(n), X (n), Y (n)) N , where X (n) =
n=1

x(1n), · · · , x(Mnn)

and

Y (n) =

1(n), · · · ,

(n) Mn

are the query-document feature set and

the relevance label 1 set for documents retrieved by query q(n),

respectively; and Mn is the number of the candidate documents {d1, · · · , dMn } retrieved by query q(n).

3.1 Ranking as MDP
e process of document ranking can be formalized as an MDP, in which the construction of a document ranking can be considered as a sequential decision making where each time step corresponds to a ranking position and each action selects a document for the corresponding position. e model, referred to as MDPRank, can be is represented by a tuple S, A, T , R,  composed by states, actions, transition, reward, and policy, which are respectively de ned as follows:
States S is a set of states which describe the environment. In ranking, the agent should know the ranking position as well as the the candidate document set that the agent can choose from. us, at time step t, the state st is de ned as a pair [t, Xt ] where Xt is the remaining documents for ranking.
Actions A is a discrete set of actions that an agent can take. e set of possible actions depends on the state st , denoted as A(st ). At the time step t, at  A(st ) selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at .
Transition T (S, A) is a function T : S × A  S which maps a state st into a new state st+1 in response to the selected action at . Choosing an action at means removing the document xm(at ) out of the candidate set, as shown in the following equation:

st +1 = T ([t, Xt ], at ) = [t + 1, Xt \ {xm(at )}],
Reward R(S, A) is the immediate reward, also known as reinforcement. In ranking, the reward can be considered as an evaluation of the quality of the selected document. It is natural to de ne the reward function on the basis of the IR evaluation measures. In this paper, we de ne the reward received in responds to choosing action at as the promotion of the DCG [3]:

2 m(at ) - 1 t = 0

RDCG(st , at ) =

2 m(at ) -1 log2(t +1)

t>0 ,

(1)

where m(at ) is the relevance label of the selected document dm(at ). Note that the calculation of each reward corresponds the DCG at

each ranking position, which enables the learning of MDPRank to

fully utilize the DCG values calculated at all ranking positions.

Policy  (a|s) : A × S  [0, 1] describes the behaviors of the

agent, which is a probabilistic distribution over the possible actions.

1 e relevance labels are a set of ranks {r1, · · · , r } and there exists a total order between the ranks: r r -1 · · · r1 where ` ' denotes a preference relationship.

946

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Agent

state st = [t, Xt]

reward rt = RDCG(st 1, at 1)
rt+1

action at  (at|st; w)

Environment
st+1

Figure 1: e agent-environment interaction in MDP.

Speci cally, the policy of MDPRank calculates the probabilities of selecting each of the documents for the current ranking position:

 (at |st ; w) =

exp wT xm(at ) a A(st ) exp wT xm(a)

,

(2)

where w  RK is the model parameters whose dimension is same

with the ranking feature.

Construction of a document ranking given a training query can

be formalized as follows. Given a user query q, the set of M retrieved

documents X , and the corresponding human labels Y , the system

state is initialized as s0 = [0, X ]. At each of the time steps t =

0, · · · , M - 1, the agent receives the state st = [t, Xt ], chooses an

action at which selects the document xm(at ) from the document set and places it to the rank t. Moving to the next step t + 1, the

state becomes st+1 = [t + 1, Xt+1]. In the same time, on the basis

of the human labels m(at ) for the selected documents, the agent receives immediate reward rt+1 = R(st , at ). e process is repeated

until all of the M documents are selected. Figure 1 illustrates the

agent-environment the interaction in MDPRank.

In the online ranking/testing phase, there is no reward available

because there exists no labels. e model fully trust the learned

policy  and choose the action with maximal probability at each

time step. us, the online ranking is equivalent to assigning scores f (x; w) = wT x to all of the retrieved documents and sorting the

documents in descending order.

3.2 Learning with policy gradient

MDPRank has parameters w to determine. In this paper, we propose to learn the parameters with the REINFORCE [10, 12], a widely used policy gradient algorithm in reinforcement learning. e goal of the learning algorithm is to maximize the expected long term return from the beginning:

(w) = EZw [G(Z)]

where Z = {xm(a0), xm(a1) · · · , xm(aM-1)} is the ranking list sampled from the MDPRank model and G(Z) is the long-term return

of the sampled ranking list, which is de ned as the discounted sum

of the rewards:

M
G(Z) =  k-1rk .

k =1

Note that the de nition of G is identical to the IR evaluation measure

DCG if  = 1. us, the learning of MDPRank is actually directly

optimizing the evaluation measure.

According to REINFORCE algorithm, the gradient w (w) can be calculated as

w (w) =  t Gt w log w(at |st ; w),

Algorithm 1 MDPRank learning

Input: Labeled training set D = {(q(n), X (n), Y (n))}nN=1, learning rate , discount factor  , and reward function R

Output: w

1: Initialize w  random values

2: repeat

3: w = 0

4: for all (q, X, Y )  D do

5:

(s0, a0, r1, · · · , sM-1, aM-1, rM )  SampleAnEpisode(w, q, X, Y , R)

{Algorithm (2), and M = |X |}

6:

for t = 0 to M - 1 do

7:

Gt 

M -t k =1

 k -1rt +k

{Equation

(3)}

8:

w  w +  t Gt w log  (at |st ; w) {Equation (4)}

9:

end for

10: end for

11: w  w + w

12: until converge

13: return w

Algorithm 2 SampleAnEpisode
Input: Parameters w, q, X , Y , and R Output: An episode
1: Initialize s0  [0, X ], M  |X |, and episode E   2: for t = 0 to M - 1 do 3: Sample an action at  A(st )   (at |st ; w) {Equation (2)} 4: rt +1  R(st , at ){Equation (1), calculation on the basis of Y } 5: Append (st , at , rt +1) at the end of E 6: State transition st +1  t + 1, X \ {xm(at ) } 7: end for 8: return E = (s0, a0, r1, · · · , sM-1, aM-1, rM )

where is further be estimated with Monte-Carlo sampling and shown in Algorithm 1. Algorithm 2 shows the procedure of sampling an episode for Algorithm 1. Speci cally, Algorithm 1 updates the parameters via Monte-Carlo stochastic gradient ascent. At each iteration, an episode (consisting a sequence of M states, actions, and rewards) is sampled according to current policy. en, at each time step t of the sampled episode, the model parameters are adjusted according to the gradients of the parameters w log  (at |st ; w), scaled by the step size , the discount rate  t , and the long-term return of the sampled episode starting from t, denoted as Gt :

M -t

Gt =

 k-1rt +k .

(3)

k =1

e gradient of w at time step t is w log  (at |st ; w), which the direction that most increase the probability of repeating the action
at on future visits to state st , and is de ned as

w log  (at |st ; w)

=

w (at |st ; w)  (at |st ; w)

= xm(at ) -

a At xm(a) exp wT xm(a) a At exp wT xm(a)

.

(4)

Intuitively, the se ing of Gt let the parameters move most in the directions that favor actions that yield the highest return. Note
that if  = 1, G0 is exactly the evaluation measure calculated at the nal rank of the document list, i.e., DCG@M.

947

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4 EXPERIMENTS
4.1 Experimental settings
We conducted experiments to test the performances of MDPRank using two LETOR benchmark datasets [7]: OHSUMED and Million
ery track of TREC2007 (MQ2007). Each dataset consists of queries, corresponding retrieved documents and human judged labels.
e possible relevance labels are relevant, partially relevant, and not relevance. Following the LETOR con guration, we conducted 5-fold cross-validation experiments on these two datasets. e results reported were the average over the ve folds. In all of the experiments, we used LETOR standard features and set  = 1 for making the algorithm to directly optimize DCG. NDCG at position of 1, 3, 5 and 10 were used for evaluation. We compared the proposed MDPRank with several state-of- the-art baselines in LETOR, including pairwise methods of RankSVM, listwise methods of ListNet, and methods that directly optimizing evaluation measures: AdaRank-MAP, AdaRank-NDCG [14], and SVMMAP [17] .
4.2 Experimental results
Table 1 and Table 2 report the performances of MDPRank and all of the baseline methods on OSHUMED and MQ2007, respectively, in terms of NDCG at the positions of 1, 3, 5, and 10. Boldface indicates the highest score among all runs. From the results, we can see that MDPRank outperformed all of the baselines, except on MQ2007 in terms of NDCG@10. We conducted signi cant testing (t-test) on the improvements of MDPRank over the best baseline. e experimental results indicated that the improvements on OHSUMED are signi cant. e results show that MDPRank is e ective algorithm to directly optimize IR evaluation measures.
One advantage of MDPRank is that it has the ability of utilizing evaluation measure calculated at all of the ranking positions (the rewards) as the supervision in training. To test the e ectiveness of this, we modi ed algorithm MDPRank so that the algorithm only utilizes the long term return of the whole episode for training, denoted as MDPRank(ReturnOnly). e modi cation actually controls the Algorithm 1 to execute line 7 and 8 only when t = 0. From the results shown in Table 1 and Table 2, we can see that MDPRank(ReturnOnly) underperformed the original MDPRank, indicating that fully utilizing the evaluation measures calculated at all of the ranking positions is e ective for improving the ranking performances.
5 CONCLUSION
In this paper we have proposed to formalize learning to rank as an MDP and training with policy gradient, referred to as MDPRank. By de ning the MDP rewards on the basis of IR evaluation measure and adopting the REINFORCE algorithm for the optimization, MDPRank actually directly optimizes IR measures with Monte-Carlo stochastic gradient assent. Compared with existing learning to rank algorithms, MDPRank enjoys the advantage of fully utilizing the IR evaluation measures calculated at all of the ranking positions in the training phase. Experimental results based on LETOR benchmarks show that MDPRank cam outperform the state-of-the-art baselines. e experimental results also showed that utilizing the IR evaluation measures calculated at all of the ranking positions do help to improve the performances.

Table 1: Ranking accuracies on OHSUMED dataset.

Method
RankSVM ListNet
AdaRank-MAP AdaRank-NDCG
SVMMAP
MDPRank MDPRank(ReturnOnly)

NDCG@1 0.4958 0.5326 0.5388 0.5330 0.5229
0.5925 0.5363

NDCG@3 0.4207 0.4732 0.4682 0.4790 0.4663
0.4992 0.4885

NDCG@5 0.4164 0.4432 0.4613 0.4673 0.4516
0.4909 0.46949

NDCG@10
0.4140 0.4410 0.4429 0.4496 0.4319
0.4587 0.4591

Table 2: Ranking accuracies on MQ2007 dataset.

Method
RankSVM ListNet
AdaRank-MAP AdaRank-NDCG
SVMMAP
MDPRank MDPRank(ReturnOnly)

NDCG@1 0.4045 0.4002 0.3821 0.3876 0.3853
0.4061 0.4033

NDCG@3 0.4019 0.4091 0.3984 0.4044 0.3899
0.4101 0.4059

NDCG@5 0.4072 0.4170 0.407 0.4102 0.3983
0.4171 0.4113

NDCG@10 0.4383
0.4440 0.4335 0.4369 0.4187 0.4416 0.4350

6 ACKNOWLEDGMENTS
e work was funded by the 973 Program of China under Grant No. 2014CB340401, National Key R&D Program of China under Grant No. 2016QY02D0405, the National Natural Science Foundation of China (NSFC) under Grants No. 61232010, 61472401, 61433014, 61425016, and 61203298, the Key Research Program of the CAS under Grant No. KGZD-EW-T03-2, and the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102.

REFERENCES
[1] Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and Chiru Bha acharyya. 2008. Structured Learning for Non-smooth Ranking Losses. In Proceedings of SIGKDD. 88­96.
[2] Olivier Chapelle and Mingrui Wu. 2010. Gradient descent optimization of smoothed information retrieval metrics. Information Retrieval 13, 3 (2010), 216­235.
[3] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ cher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of SIGIR. 659­666.
[4] Nathan Korda, Balazs Szorenyi, and Shuai Li. 2016. Distributed Clustering of Linear Bandits in Peer to Peer Networks. In Proceedings of ICML, 1301­1309.
[5] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. 2016. Collaborative Filtering Bandits. In Proceedings of SIGIR. 539­548.
[6] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings SIGIR. 587­596.
[7] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2009. LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval. Information Retrieval (2009).
[8] Filip Radlinski, Robert Kleinberg, and orsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of ICML. 784­791.
[9] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. Machine Learning Research 6 (2005), 1265­1295.
[10] Richard S. Su on and Andrew G. Barto. 2016. Reinforcement Learning: An Introduction (2nd ed.). MIT Press.
[11] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. So Rank: Optimizing Non-smooth Rank Metrics. In Proceedings of WSDM. 77­86.
[12] Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 3 (1992), 229­256.
[13] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of SIGIR. 113­122.
[14] Jun Xu and Hang Li. 2007. AdaRank: A Boosting Algorithm for Information Retrieval. In Proceedings SIGIR. 391­398.
[15] Jun Xu, Tie-Yan Liu, Min Lu, Hang Li, and Wei-Ying Ma. 2008. Directly Optimizing Evaluation Measures in Learning to Rank. In Proceedings of SIGIR. 107­114.
[16] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM TIST 8, 3 (2017), 41:1­41:26.
[17] Yisong Yue, omas Finley, Filip Radlinski, and orsten Joachims. 2007. A Support Vector Method for Optimizing Average Precision. In Proceedings of SIGIR. 271­278.
[18] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Content-free Document Re-ranking. In Proceedings of SIGIR. 1139­1142.

948


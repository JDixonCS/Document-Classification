Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

The Probability That Your Hypothesis Is Correct, Credible Intervals, and E ect Sizes for IR Evaluation

Tetsuya Sakai
Waseda University, Tokyo, Japan tetsuyasakai@acm.org

ABSTRACT
Using classical statistical signi cance tests, researchers can only discuss P(D+|H ), the probability of observing the data D at hand or something more extreme, under the assumption that the hypothesis H is true (i.e., the p-value). But what we usually want is P(H |D), the probability that a hypothesis is true, given the data. If we use Bayesian statistics with state-of-the-art Markov Chain Monte Carlo (MCMC) methods for obtaining posterior distributions, this is no longer a problem. at is, instead of the classical p-values and 95% con dence intervals, which are o en misinterpreted respectively as "probability that the hypothesis is (in)correct" and "probability that the true parameter value drops within the interval is 95%," we can easily obtain P(H |D) and credible intervals which represent exactly the above. Moreover, with Bayesian tests, we can easily handle virtually any hypothesis, not just "equality of means," and obtain an Expected A Posteriori (EAP) value of any statistic that we are interested in. We provide simple tools to encourage the IR community to take up paired and unpaired Bayesian tests for comparing two systems. Using a variety of TREC and NTCIR data, we compare P(H |D) with p-values, credible intervals with con dence intervals, and Bayesian EAP e ect sizes with classical ones. Our results show that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di erence in means but also for the e ect size in terms of Glass's .
CCS CONCEPTS
·Information systems  Retrieval e ectiveness; Presentation of retrieval results;
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080766

KEYWORDS
Bayesian hypothesis tests; con dence intervals; credible intervals; e ect sizes; Hamiltonian Monte Carlo; Markov Chain Monte Carlo; p-values; statistical signi cance
1 INTRODUCTION
In March 2016, the American Statistical Association (ASA) published an o cial statement about the limitations of classical significance tests and p-values, in response to their continued misuse and misinterpretations [33]. While ASA's main statement does not contain anything new (e.g., "A p-value, or statistical signi cance, does not measure the size of an e ect or the importance of a result"), the document mentions some alternatives to classical signi cance tests, including Bayesian methods. It goes on to say: "All these [alternative] measures and approaches rely on further assumptions, but they may more directly address the size of an e ect (and its associated uncertainty) or whether the hypothesis is correct." In the IR community, similar warnings against classical signi cance tests have been given by Cartere e [4] and Sakai [22], amongst others.
e above quotations from the ASA statement may be paraphrased as follows. Classical signi cance tests can only give us P(D+|H ), the probability of observing the data D at hand or something more extreme under the assumption that the hypothesis H is true (i.e., the p-value). But what we usually want is P(H |D), the probability that a hypothesis is true, given the data. eoretically, the Bayesian framework proposed in the 18th century [2] can give us exactly this, but it was heavily criticised during the 19th and 20th centuries (See Section 2.1). However, with the recent advent of e ective and e cient sampling algorithms for obtaining posterior distributions known as Markov Chain Monte Carlo (MCMC) methods [15], Bayesian approaches to statistical testing are rapidly gaining popularity among statisticians [30, 31]. us, as alternatives to the classical p-values and 95% con dence intervals which are o en misinterpreted respectively as "probability that the hypothesis is (in)correct" and "probability that the true parameter value drops within the interval is 95%," we can employ Bayesian tests to easily obtain P(H |D) and credible intervals, which represent exactly the above. Moreover, with Bayesian tests, we can easily handle virtually any hypothesis, not just "equality of means," as we shall demonstrate later.
We provide simple tools to encourage the IR community to take up paired and unpaired Bayesian tests for comparing two systems; our tools are based on a state-of-the-art MCMC method called Hamiltonian Monte Carlo and a recently-proposed variant called No-U-Turn Sampler [13]. Using a variety of TREC1 and NTCIR2
1h p://trec.nist.gov/ 2h p://research.nii.ac.jp/ntcir/

25

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

data, we compare P(H |D) with p-values, credible intervals with con dence intervals, and Bayesian EAP e ect sizes with classical ones. Our results show that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di erence in means but also for the e ect size in terms of Glass's .
2 RELATED WORK
2.1 Frequentists Versus Bayesians
It is well known that Ronald A. Fisher heavily and persistently criticised the Bayesian statistics since the 1960s: "the theory of inverse probability [i.e., Bayesian statistics] is founded upon an error, and must be wholly rejected" [9] (p.9). During the 20th century, the de facto standard in statistical analysis was indeed the "frequentist" approach founded upon Fisher's views, and the Bayesian approach was largely neglected.
e Bayesian approach had two weaknesses. e rst, which has not been resolved completely even to this day, is the fact that it relies on prior probabilities which nobody knows and therefore must be set based on researchers' subjective decisions or beliefs. However, given the lack of knowledge about priors, noninformative priors such as those that obey a uniform distribution can always be used, although this too is a subjective decision which may or may not re ect the true nature of the phenomenon under study3. In the present study, we simply follow the standard Bayesian practice of employing uniform distributions for obtaining priors.
e second weakness in the original Bayesian approach was that, despite the theoretical beauty of Bayes' eorem, it was o en di cult to obtain the posterior distributions as this o en involves computationally infeasible integrations. However, this second problem has actually been solved, with the recent advent of e ective and e cient sampling algorithms known as Markov Chain Monte Carlo (MCMC) methods [15]. Because of this, Bayesian statistics is now gaining popularity rapidly: for example, according to Toyoda [30], over one-half of Biometrika4 papers published in 2014 utilised Bayesian statistics. In the present study, we utilise a stateof-the-art MCMC method called Hamiltonian Monte Carlo [17] and a recently-proposed variant called No-U-Turn Sampler [13], which come with easy-to-use implementations.
Meanwhile, the classical signi cance testing approach of the frequentists have also received many criticisms over the past decades (e.g. [12]). Some even consider signi cance tests to be harmful. First, the practice of using the signi cance criterion  instead of the p-value o en leads to dichotomous thinking: "Is the di erence statistically signi cant, or not?" Ziliak and McCloskey [35] hold
3 In 2005, Efron remarked: "What looks uninformative enough o en turns out to subtly force answers in one direction or another" while arguing that a combination of Bayesian and frequentist ideas is needed to handle modern problems [8]. 4 is is the journal in which William S. Gosset (or "Student") published the famous paper on the t -test in 1908 [29]. h p://biomet.oxfordjournals.org/

Fisher responsible for this5. Second, even if the p-value is reported, this is a function not only of the e ect size (the magnitude of the di erence that we are interested in; See Section 3.6) but also the sample size. at is, a small p-value (i.e., a statistically highly signi cant result) may just re ect a large sample size (e.g., number of topics used for computing mean retrieval e ectiveness scores) rather than a large e ect size [22]. Moreover, as was mentioned in Section 1, the outcomes of classical signi cance tests are o en misinterpreted. We believe that it is time for the IR community to start using Bayesian statistics regularly, perhaps along with classical signi cance tests if the community feels reluctant to let the la er go. Since the Bayesian approach is not only highly intuitive and exible but now also computationally feasible, there really is no reason to reject it.
In the eld of psychology, Kruschke [14] argues that the Bayesian approach is superior to the (unpaired) t-test: "Some people may wonder which approach, Bayesian or NHST,6 is more o en correct.
is question has limited applicability because in real research we never know the ground truth; all we have is a sample of data. [. . .] the relevant question is asking which method provides the richest, most informative, and meaningful results for any set of data. e answer is always Bayesian estimation." In the present study, we empirically demonstrate the relationships between paired/unpaired t-tests and the corresponding state-of-the-art Bayesian methods using a variety of real IR evaluation data.
2.2 Classical Signi cance Tests in IR
e limitations of classical signi cance tests have been pointed out in the eld of IR as well. Cartere e argues: "we still believe p-values from paired t-tests provide a decent rough indicator that is useful for many of the purposes they are currently used for. We only argue that p-values and signi cance test results in general should be taken with a very large grain of salt, and in particular have an extremely limited e ect on publication decisions and community-wide decisions about "interesting" research directions." Sakai [22] encourages IR researchers to report not only the p-values but also e ect sizes and con dence intervals. However, Sakai's more recent examination of over 1,000 SIGIR and TOIS papers [23] shows that about 30% of the entire papers lack signi cance testing, while about 65% of the papers with signi cance testing neither report p-values nor test statistics.
While computer-based, distribution-free alternatives to classical signi cance tests, namely, the bootstrap [20] and the randomisation test [27], have been advocated for IR evaluation, they have not been used as widely as the classical tests [23]. Moreover, these tests address the same limited question as the classical tests: what is the p-value?
2.3 Bayesian Inferences for IR
We are already beginning to see Bayesian approaches to IR evaluation. Cartere e [3, 5] have proposed to evaluate IR systems by modelling binary and graded relevance judgments directly instead
5 "Ronald A. Fisher would say, " e potash manures are not statistically signi cant. Disregard them. [. . .] William S. Gosset would say, "[. . .] If you want to know about the potash manures you have to consider their pecuniary value compared to the barley you're trying to make money with" [35]. 6Null Hypothesis Signi cance Testing

26

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

of using evaluation measure scores as the atomic unit. Using three di erent TREC data sets, he compared t-test p-values with the posterior probabilities of his four Bayesian models to argue the advantages of the la er. Cartere e uses JAGS (Just Another Gibbs Sampler), an open-source implementation of BUGS (Bayesian inference Using Gibbs Sampling) and its R interface rjags to conduct MCMC simulations.
Our present work is in a sense less ambitious than that of Cartere e in that we are adhering to using evaluation measure scores as the atomic unit (just like Cartere e's "Model 2"): we would like the IR community to take up the habit of using Bayesian approaches, and to do that, we believe that we need to move cautiously while clarifying how the transition from classical signi cance testing will a ect our research ndings. Cartere e demonstrates that the t-test (i.e., his "Model 1") p-values and his Model 2 posterior probabilities are strinkingly similar; we generalise this in several ways, by using diverse data sets from NTCIR and TREC, and by comparing con dence intervals with credible intervals, and classical sample e ect sizes with their Bayesian counterparts.
Zhang et al. [34] propose to replace the use of classical signi cance tests with Bayesian tests with probabilistic graphical models tailored to a speci c problem in information retrieval, namely, text classi cation. Following Kruschke [14, 15], they propose to make a decision about two text classifers by comparing the High Density Interval (HDI) and the Region of Practical Importance (ROPE) of the performance di erence  . Zhang et al. use Metropolis-Hastings (MH) sampling for their MCMC simulations.
Regarding the implementation of MCMC, we employ the stateof-the-art Hamiltonian Monte Carlo (HMC) [17] and its variant NoU-Turn Sampler (NUTS) [13] using stan and its R interface rstan, which are gaining popularity. According to Ho man and Gelman [13], HMC's features "allow it to converge to high-dimentional
target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling"; NUTS automatically sets a required parameter for HMC, namely the number of steps L for the leap-frog method (See Section 3.2). Also, according to Kruschke [15] (pp.399-400), "HMC can be more e ective than the
various samplers in JAGS and BUGS, especially for large complex models. [. . .] However, Stan is not universally faster or be er (at this stage in its development)."

3 BAYESIAN TESTS 3.1 Bayesian Basics
To discuss Bayesian tests, let us start with the famous Bayes' rule:

f ( |x) =

f (x | )f ( ) f (x)

=

f (x | )f ( )

 +
-

f

(x

| )f

(

)d

,

(1)

where f ( ) is the prior probability distribution of a random variable  (e.g., a population mean), f (x | ) is the likelihood of data x given  , and f ( |x) is the posterior probability distribution of  having observed x. Note that this view is already strikingly di erent from that of classical statistics: in classical signi cance testing, population parameters ( 's) are constants; in Bayesian statistics, they are random variables that form distributions. Since f ( |x) is a probability distribution, f (x) can be viewed as a normalising constant that

ensures:

 +
-

f ( |x)d

=

1 f (x)

 +
-

f (x | )f ( )d

=1.

(2)

Hence, Eq. 1 implies that the property of the posterior distribution
f ( |x) is governed by f (x | )f ( ), i.e., the kernel.
If somehow the posterior distribution f ( |x) has been obtained,
a point estimate of the population parameter  can be obtained as an expected a posteriori (EAP)7:

 ^EAP = E[ |x] =

  f ( |x)d =



f

(x | )f f (x)

(

)

d

.

(3)

Moreover, how the random variable  moves around ^EAP can be quanti ed by the posterior variance (or its square root, posterior
standard deviation):

 V [ |x] = E[( - ^EAP )2|x] = ( - ^EAP )2 f ( |x)d . (4)

We can also obtain an interval estimate of  by removing an /2% area from either side of f ( |x). is is the 100(1 - )% credible interval (or Bayesian con dence interval), which is highly intuitive: the probability that the random variable  lies within the interval is 100(1 - )%. Recall that con dence intervals (CIs) used in classical statistics do not represent this probability: in the classical paradigm,  is a constant, and when (say) 100 CIs are created from 100 di erent samples, 100(1 -)% of them are expected to contain that particular .
As we do not know f ( ), we employ a non-informative prior distribution to avoid subjectivity to the best of our ability. Our choice is to use a uniform distribution, in which case f ( |x) is governed solely by the likelihood f (x | ) in Eq. 1 and therefore the EAP reduces to the maximum likelihood estimate (MLE). at is, in our se ing, the EAP of any parameter  is not directly a ected by our subjective choice of f ( ).

3.2 HMC and NUTS
In the above discussion, we assumed that f ( |x) can be computed as de ned in Eq. 1. However, in practice, it is usually not feasible to do this analytically. at is where MCMC methods, which try to sample  repeatedly according to f ( |x), come into play. MCMC methods construct Markov Chains of parameter values so that the underlying distribution eventually reaches a stationary distribution: a er a burn-in period (B), all of the values in the chain obey the target distribution f ( |x). us, if we collect T values sequentially and throw away the intial B values, the remaining T = T -B values can be regarded as realisations of  that obey f ( |x). Suppose that we managed to obtain T = 100, 000 realisations of  ; then, ^EAP (Eq. 3) can be obtained by simply averaging the T values. Similarly, the posterior variance (Eq. 4) can be obtained by averaging8 the squared di erence from ^EAP . A 95% credible interval for  can be obtained by sorting the T values in ascending order and then taking the 2,500th and 97,500th values as the lower and upper limits. e EAP values of other statistics such as e ect sizes (See Section 3.6) can be computed similarly. Moreover, for virtually any hypothesis H (e.g., "mean 1 is higher than mean 2," "mean 1 is at least 0.2 points
7 Alternatives would be a maximum a posteriori or a posterior median. ese three estimates represent the mean, mode and median of the posterior distribution, respectively. 8 Dividing by T su ces since T is large; there is no need for bias correction.

27

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

higher than mean 2," or "the e ect size is greater than 0.5"), the probability that H is correct can be obtained by just counting the instances in which H holds among the T realisations. It is clear that this approach is more versatile than classical signi cance tests.
Hamiltonian Monte Carlo (HMC) [17] and its variant No U-Turn Sampler (NUTS) [13] are state-of-the-art MCMC methods which we utilise for obtaining realisations of  from f ( |x). For details of HMC, we refer the reader to recent books on this topic [15, 17]. However, it would help for us IR researchers to have a general idea about its basic principles. In physics, Hamiltonian is the sum of potential energy and kinetic energy; given a curved surface in an ideal physical world, any object would move on the surface while keeping the Hamiltonian constant. e path of the moving object is governed by Hamilton's equations of motion, which can be solved by the leap-frog method with parameters  (stepsize) and L (leapfrog steps). To achieve sampling from a posterior distribution (which is our "curved surface"), an intuitive explanation of what HMC does is as follows: put an object somewhere on the surface and give it a push; a er L units of time, let it halt and record its position; give it another push, and so on, until we have recorded T positions.
Brie y, for a set of d parameters  = (1, 2, . . . , d ), the HMC algorithm looks like this:
(1) Set  (1), , L,T , B (where T = T - B); Let t = 1; (2) Generate d independent values p(t ) = (p1t , p2t , . . . , pdt ) from
a standard normal distribution; (3) Obtain candidates  (a), p(a) using the leap-frog method; (4) Let  (t +1) =  (a) (i.e., accept the transition candidate) with
probability min(1, r ); otherwise  (t +1) =  (t ) (i.e., reject the candidate and stay at the current position); (5) End if T = t; otherwise let t = t + 1 and go to Step 2.

e r in Step 4, which governs how o en we can accept new candidates, is given by [30]:

r

=

f ( (a), p(a) |x) f ( (t ), p(t ) |x)

,

(5)

where f (, p|x) is a joint distribution of f ( |x) and an independent standard normal distribution f (p). One strength of HMC is that r is o en close to one, and therefore we can achieve e cient sampling through many successful transitions in Step 4 while preserving the Hamiltonian.
NUTS is a variant of HMC that automatically determines an appropriate value of L [13]; both HMC and NUTS utilise an algorithm called dual averaging to automatically set  [18]; hence, from the viewpoint of the users of HMC and NUTS for Bayesian tests, we do not have to worry about se ing these parameters ourselves.

3.3 R^ and E ective Sample Size
For MCMC algorithms, methods for checking whether the values have converged to a stationary distribution and for measuring the sampling e ciency are available.
R^, a measure for checking convergence, assumes that the MCMC algorithm produces multiple Markov Chains, and compares the variance across the multiple chains with the variance within the chains9. According to Gelman [10], if R^ is less than 1.1 or 1.2, we

9Zhang et al. [34] remark that "it is perfectly right to do a single long sampling run and keep all samples." However, it is very easy to throw away burn-in's that may not yet

can be assured that the chains have reached stationary distributions. MCMC produces Markov Chains and therefore the values in them are correlated to one another. If the chains produce very similar values repeatedly, then that is highly ine cient from the sampling point of view. E ective Sample Size (Ne ) is a measure available in stan for quantifying sampling e ciency, which means "you have obtained a sample of size T , but that is worth a sample of size (approximately) Ne obtained when there is zero correlation within the chain." Hence, we should also check that Ne is a large value. Both of the above measures are computed a er removing the burn-in's from the chains. As our Bayesian test tools that we introduce in Section 3.7 rely on stan and its R interface rstan, the tools output R^ and Ne on the R console. Since all experiments reported in the paper use su ciently large sample sizes, namely T = 100, 000, we do not discuss these indicators henceforth.
3.4 Classical versus Bayesian Tests for Comparing Two Means
As was discussed in Section 3.2, Bayesian tests are versatile. However, the present study focusses on the problems of comparing two means from paired and unpaired data, since these are the most common and basic problems in IR evaluation. While two-sided tests that ask "are the two systems equally e ective or not?" are o en recommended in classical signi cance tests given lack of prior knowledge as to which system might be be er, this is not a very useful question from the Bayesian point of view, as two di erent systems are, by de nition, di erent. What is more practical to consider is the probability that System 1 is be er than System 2, P(S1 > S2|D) (or, alternatively, P(S1 < S2|D) = 1 - P(S1 > S2|D)). Hence we compare Bayesian tests with classical one-sided tests. To be more speci c, given two systems, we let S2 be the less e ective system according to the sample data and consider P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) based on the Bayesian test, while se ing the classical null and alternative hypotheses as H0 : S1 = S2 and H1 : S1 > S2 so that the p-value represents "P(D+|S1 = S2)."
3.5 Statistical Models
For unpaired data, we assume that S1's scores obey N (µ1, 12), while S2's scores obey N (µ2, 22), for both Bayesian and classical tests. Hence, the classical test we employ is the Welch's t-test, which does not assume homoscedasticity (i.e., equal variances). It is known that Student's and Welch's t-tests yield virtually identical p-values when the two sample sizes are equal [16, 24]; the experiments reported in this paper satisifes this condition and therefore our classical test results can be regarded as representing both types of unpaired t -test.
For paired data, the classical test we apply is the paired t-test, which relies on the same normal assumptions as described above and therefore the score di erences obey N (µ1 - µ2, 12 + 22). As for the Bayesian test, we can easily consider a bivariate normal
obey the desired distribution and we prefer to do so. As for the number of chains, since R^ can be computed even for a single chain, by breaking it into multiple chains, we provide sample scripts for handling both multiple and single chains. e Bayesian test results are almost the same either way, but we report those based on multiple chains.

28

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

distribution [31]:

f

(x1, x2 |µ1, µ2, 12, 22, )

=

1 2 12 (1

-

) e-q/2

,

(6)

where

q=

1

1 -

2

[(

x1

- 1

µ1

)2

-

2

(

x1

- 1

µ1

)(

x2

- 2

µ2

)

+

(

x

2

- 2

µ2

)2]

.

(7)

Here,  is the population correlation coe cient for x1's and x2's. us, for paired data, we can discuss hypotheses about the corre-
lation between the two systems just as well as those about means and e ect sizes if we are interested in that aspect.

3.6 E ect Sizes

Sakai [22] stresses the importance of reporting e ect sizes and

con dence intervals in the context of classical signi cance testing as

a small p-value may just re ect a large sample size (See Section 2.2).

When comparing two means, the e ect size is usually given as

the di erence between the two measured in standard deviation

units. We argue that Bayesian test results in IR should also be

accompanied with e ect sizes as well as credible intervals.

While there are several choices of e ect sizes for comparing

two means, we choose to avoid relying on the homoscedasticity

assumption, to be consistent with the statistical models described in

Section 3.5. One of the simplest e ect size in such a case is Glass's

 [19]. at is, if System 1 is taken as the baseline run (or "control

group,") then its standard deviation is probably representative of

an "ordinary world" before the advent of System 2, and therefore:

Gl ass 1

=

µ1 - µ2 1

.

(8)

us, Glass's  measures the absolute di erence in "ordinary" stan-

dard deviation units. Alternatively, if System 2 is taken as the

baseline:

Gl ass 2

=

µ1 - µ2 2

.

(9)

In our experiments, we focus on Eq. 9 since the second system is

the less e ective one (i.e., "baseline") according to the sample data.

For convenience, we will herea er refer to this version of e ect

size simply as "Glass2."

We can easily obtain the EAP and credible intervals for Glass2,

as well as the probability of a hypothesis about the e ect size

being true, in exactly the same way as described in Section 3.2.

For example, the EAP for Glass2 can be obtained by computing

Eq. 9 T times using T realisations of µ1, µ2, 2 and then averaging them. Hence we propose that the IR community report the EAP,

the credible interval and the probablity of hypothesis being true

not only for the raw di erence in means but also for e ect sizes.

Note that this is applicable to both paired and unpaired tests.

3.7 Implementation
Here, we brie y describe our Bayesian test tools for comparing two means. e sample R scripts are based on those developed by Hideki Toyoda [30]10. We have sample scripts for generating both multiple and single chains but discuss only the former here. We have added a few shell scripts for postprocessing the Bayesian
10 Toyoda's original scripts are available from h p://www.asakura.co.jp/G 27 2.php? id=200. e present author is solely responsible for the modi cations and any errors introduced thereby.

realisations, to encourage researchers to use credibile intervals not only for the raw di erence in means but also for e ect sizes. e scripts and sample data are available from our website11. R with the rstan package12 and Rtools13 must be installed rst in order to use these scripts.
Figure 1 shows our sample R script for comparing paired data. It reads a stan le which describes the aforementioned bivariate normal distribution as well as generated quantities (the absolute di erence and Glass's 's), and a system score le wri en in R, and generates ve csv les that correspond to ve Markov chains.
Figure 2 shows an output of our UNIX shell script that summarises the Bayesian test results by reading the csv les. Here, the
rst argument is "1032" because this is the line number in each csv le where the realisations start a er the burn-in; the second argument is the number of realisations contained in the le (excluding the burn-in's); the third argument is the number of chains (i.e., number of csv les); the remaining arguments are the aforementioned Markov chain csv les. is script works for both multiple and single chains, by se ing the arguments appropriately14. e screenshot provides the following information about the paired data from run1 and run2:
· e EAP for the di erence in means is 0.042, and the 95% credibile interval is [0.009, 0.074]. e probability that µ1 - µ2 is greater than the speci ed threshold (which is set to 0 by default within the script) is 99.4%;
· e EAP for Glass2 (with run2 taken as the baseline) is 0.189 (i.e., about 19% of run2's standard deviation), and the 95% credible interval is [0.043, 0.345]. e probability that this e ect size is greater than the speci ed threshold (which is set to 0.2 by default) is 43.3%. Similar results with run1 taken as the baseline are also presented.
· e EAP for the correlation (i.e.,  in Eq. 6) between the scores of run1 and run2 is 0.857, and the 95% credible interval is [0.767, 0.920]. e probability that the correlation is greater than the speci ed threshold (which is set to 0.9 by default) is 12.2%.
Note that thresholds can be altered arbitrarily within the shell script according to researchers' practical needs.
Figures 3 and 4 provide similar screenshots of our scripts for comparing unpaired data.
4 EXPERIMENTS
To encourage IR researchers to transition comfortably from the classical signi cance test paradigm to the Bayesian one for comparison of means, we now report on experiments that compare Bayesian results against classi cal test results using actual IR data. More speci cally, we compare the probability that System X is be er than System Y with p-values; credible intervals with classical con dence intervals, as well as e ect sizes computed based on both approaches. We show that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect
11 h p://www.f.waseda.jp/tetsuya/tools.html 12 h ps://cran.r-project.org/ 13 h ps://cran.r-project.org/bin/windows/Rtools/ 14By typing the command without arguments, suggested sets of arguments are displayed.

29

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: IR data sets and evaluation measures used in this study

Data set name NTCIR7IR4QA NTCIR9INTENT NTCIR12STC TREC03robust TREC11webD TREC15TS

year track/task

language measure

tool

#teams #runs #topics

2008 IR for question answering [25]

Chinese Q-measure

NTCIREVAL

9 40 (20)

97

2011 INTENT [28] (web diversity task)

Chinese D -nDCG@10 NTCIREVAL

7 24 (20)

100

2016 short text conversation [26] (tweet retrieval) Chinese nERR@10

NTCIREVAL

16 44 (20)

100

2003 robust track [32]

English nDCG@1000 NTCIREVAL

16 78 (20)

50

2011 web track diversity task [7]

English  -nDCG@10 ndeval

9 25 (20)

50

2015 temporal summarisation track, task 2 [1]

English H

-

9 22 (20)

21

Figure 1: A sample R script for comparing paired data.

Figure 3: A sample R script for comparing unpaired data.

Figure 2: A shell script for obtaining the EAP, credible intervals, and the probability that the hypothesis is correct (paired data) for the absolute di erence, Glass's , and correlation coe cient (paired data).
sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes.
Table 1 provides a brief summary of the six data sets we used for our experiments. To strengthen the generalisability of our experimental results, we tried to cover diverse IR tasks from both TREC and NTCIR. For each data set (i.e., test collection with its submi ed runs), we chose one particular commonly-used evaluation measure: with the exception of TREC03robust15, we chose from one of the
15 We chose nDCG rather than Average Precision (AP) for TREC03robust as AP cannot handle graded relevance even though the data set comes with graded relevance assessments.

Figure 4: A shell script for obtaining the EAP, credible intervals, and the probability that the hypothesis is correct (paired data) for the absolute di erence, and Glass's  (unpaired data).
o cial measures of that track/task. Also, for each data set, we considered only the top 20 runs for pairwise comparisons as measured by that particular evaluation measure, which gives us 190 run pairs. For TREC11webD, -nDCG was computed using their o cial evaluation script ndeval16; For TREC15TS, the values of H (which is basically like a nugget-based F-measure de ned over a timeline [1]) were obtained from the o cial results of the track; other evaluation measures were computed using NTCIREVAL17, with the exponential gain value se ing (See, for example, [26]).
For each data set, we conducted Bayesian and classical tests for every system pair (where System 1 outperforms System 2 for the sample data), using both paired and unpaired tests. For classical paired and unpaired tests, common sample e ect sizes were obtained by substituting sample means and System 2's sample standard deviations into Eq. 9. For Bayesian paired and unpaired tests, the EAP values of µ1, µ2, 2 were used with Eq. 9, under the two models described in Section 3.5, respectively.
16 h p://trec.nist.gov/data/web/11/ndeval.c 17 h p://research.nii.ac.jp/ntcir/tools/ntcireval-en.html

30

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 5: Paired Bayesian vs classical tests: comparisons with NTCIR data.

4.1 Paired Test Results
Here we compare the paired Bayesian test (based on NUTS) with the classical paired t-test. We compare: (I) the paired Bayesian P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) with the classical one-sided paired p-value (See Section 3.4); (II) the paired 95% credible interval with the classical paired 95% con dence interval; (III) the paired Bayesian EAP Glass2 with the classical Glass2 based on sample statistics. e margin of error for the classical paired 95% con dence interval is given by t(n - 1; 0.05) V /n, where n is the sample size, V is the sample variance of the score di erences, and t(; ) is the two-sided critical t value18.
Figure 5 visualises the results of comparing the Bayesian and classical paradigms for paired data with NTCIR7IR4QA (graphs (A)(D)), NTCIR9INTENT (graphs (E)-(H)), and NTCIR12STC (graphs (I)(L)). Graphs (A), (E) and (I) (i.e., the le most column) compare the Bayesian P(S1 < S2|D) with the p-value; graphs (B), (F), and (J) compare the Bayesian EAP Glass2 with the sample Glass2; graphs (C), (G) and (K) compare the Bayesian credible interval lower limit with the con dence interval lower limit; and graphs (D), (H) and
18 T.INV.2T(P,  ) with Microso Excel.

(L) (i.e., the rightmost column) compare the Bayesian credible interval upper limit with the con dence interval upper limit. Figure 6 provides similar information for TREC03robust, TREC11webD, and TREC15TS.
With the exception of Figure 6(J), i.e., the e ect size results for TREC15TS which we shall discuss in Section 4.3, the results in Figures 5 and 6 are highly consistent across the diverse NTCIR and TREC data sets and the messages are clear:
(1) Graphs (A), (E), and (I) in Figures 5 and 6 show that the Bayesian P(S1 < S2|D) and the classical p-values are very highly correlated, echoing an earlier observation by Cartere e [5] (See Section 2.3). us, while P(S1 < S2|D) is what we usually want, it appears that the one-sided pvalue, which represents P(D+|S1 = S2), can be considered as a reasonable approximation of P(S1 < S2|D).
(2) Graphs (C), (D), (G), (H), (K), (L) in Figures 5 and 6 show that the Bayesian 95% credible intervals and the classical 95% con dence intervals are also very similar, despite the fundamental di erences in what they represent. us, the con dence interval can be considered as an approximation to the credible interval, which is what we really want.

31

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 6: Paired Bayesian vs classical tests: comparisons with TREC data.

(3) Graphs (B) and (F) in Figures 5 and 6 suggest that, while the Bayesian EAP e ect sizes generally align with the sample e ect sizes (Glass2), if the sample e ect size is small (e.g., less than 0.2), that may be an underestimation of the population e ect size. For example, Figure 6(B) indicates an instance (with a baloon) where the sample e ect size is 0.070 even though the Bayesian EAP e ect size, which we believe to be more accurate, is 0.113.
Observations (1) and (2) suggest that, even though the IR community may have relied on p-values and con dence intervals for decades, sometimes with incorrect interpretations, switching to Bayesian approaches would not turn all the experimental results in the literature upside down. As for Observation 3, even though we lack the ground truth for population e ect sizes, we would like to repeat Kruschke's argument [14]: "the relevant question is asking which method provides the richest, most informative, and meaningful results for any set of data [. . .]" (See Section 2.1).
4.2 Unpaired Test Results
Here we compare the unpaired Bayesian test (based on NUTS) with the classical Welch's t-test. We compare: (I) the unpaired Bayesian

P(S1 < S2|D) (i.e., the probability of the less likely hypothesis)
with the classical one-sided unpaired p-value (See Section 3.4);
(II) the unpaired 95% credible interval with the classical unpaired
95% con dence interval; (III) the unpaired Bayesian EAP Glass2
with the classical Glass2 based on sample statistics. e margin of
error for the classical unpaired 95% con dence interval is given by t(; 0.05) V1/n1 + V2/n2, where n1, n2 are the sample sizes, V1, V2 are the sample variances, and the approximated degrees of freedom  is given by [24]:



=

( V1 n1

+

V2 )2/{ (V1/n1)2

n2

n1 - 1

+

(V2/n2)2 } n2 - 1

.

(10)

In fact, since our unpaired test experiments merely regard the paired data from NTCIR and TREC (i.e., two systems evaluated with a common topic set) as unpaired data, n1 = n2 holds in our case.
Figure 7 visualises the results of comparing the Bayesian and classical paradigms for unpaired TREC data, similarly to Figure 6.
e unpaired results with the NTCIR data, which look very much like the corresponding paired test results with the NTCIR data shown in Figure 5, are omi ed in this paper due to lack of space.

32

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 7: Unpaired Bayesian vs classical tests: comparisons with TREC data.

Note that the classical sample e ect sizes used in Graphs (B), (F), and (J) are the same as the ones used in Figure 6. For example, we have observed from Figure 6(B) that while the sample e ect size for a system pair was 0.070, the Bayesian paired model gives us an EAP Glass2 of 0.113; in contrast, Figure 7(B) indicates that the Bayesian unpaired model gives us an EAP Glass2 of 0.161 for the same system pair.
It is clear that Observations 1-3 listed up in Section 4.1 also hold for the unpaired tests as well, which further strengthens our
ndings. However, just like Figure 6(J), Figure 7(J) shows and anomalous result for TREC15TS: we therefore discuss these results separately in the next section.
4.3 On the Anomalous Behaviour of H
e nugget-based H measure, the primary measure from the TREC 2015 Temporal Summarisation track, demonstrates a strange behaviour in Figure 6(J) and Figure 7(J), where the Bayesian EAP e ect sizes and sample e ect sizes are compared. e graphs look quite di erent from Graphs (B) and (F) of Figures 5-7. More specifically, for a small set of system pairs, EAP values are larger than sample e ect sizes; for a few others, sample e ect sizes are larger

than EAPs. For example, for a system pair whose sample e ect size is 11.39, the paired Bayesian EAP is 12.10 (Figure 6(J)), while the unpaired Bayesian EAP is 12.37 (Figure 7(J)), as indicated by baloons. Similarly, for a system pair whose sample e ect size is 16.19, the paired Bayesian EAP is 12.55, while the unpaired Bayesian EAP is 12.76. is is in contrast to the aforementioned Obsevation 3 (Section 4.1) for the other data sets.
While the other evaluation measures used in our experiments have been studied quite extensively (e.g., [6, 20, 21]), we are not aware of any work in the literature that validated H in a similar way, and we believe that an investigation is in order. For example, while the actual nDCG scores from our TREC03robust data range between 0 and 0.9816 (i.e., almost fully covering the theoretical range of [0, 1]), the actual H scores from TREC15TS range between 0 and 0.4021: that is, the actual range width is about 0.4 rather than 1. Moreover, the standard deviations of H for each system are very low compared to the other measures, causing very large e ect sizes (See Eq. 9): hence, note that the axis scales of Figure 6(J) and 7(J) are very di erent from the other e ect size graphs. While studying the properties of a new measure is not the focus of this

33

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

study, these anomalous results with H may deserve a ention from other researchers such as the TREC track coordinators.
5 CONCLUSIONS AND FUTURE WORK
Using diverse data sets from TREC and NTCIR, we compared, under both paired and unpaired data se ings, (I) the Bayesian P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) with the classical one-sided p-value; (II) the 95% credible interval with the classical 95% con dence interval; (III) the Bayesian EAP Glass2 with the classical Glass2 based on sample statistics. Our results showed that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes. Fortunately, our results suggest that Bayesian statistics will not turn all experimental results in the IR literature upside down; however, we hope that these results, as well as our tools, will help IR researchers to take up Bayesian hypothesis testing approaches. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di erence in means but also for Glass's .
e present study focussed on the problem of comparing two systems, and did not address the multiple comparison and familywise error rate problems [11]. If a researcher is interested in the p-value of every system pair, one e ective way to obtain them would be to employ the randomised Tukey HSD test [4, 22], which is completely distribution-free. In future work, we would like to explore Bayesian alternatives to this test and validate them.
ACKNOWLEDGEMENTS
We thank Professor Hideki Toyoda (Waseda University) for le ing us modify his R code and distribute it, and Dr. Ma hew EkstrandAbueg (Google) for providing the TREC temporal summarisation track results.
REFERENCES
[1] Javed Aslam, Fernando Diaz, Ma hew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Tetsuya Sakai. 2016. TREC 2015 Temporal Summarization Track. In Proceedings of TREC 2015.
[2] omas Bayes. 1763. An Essay towards Solving a Problem in the Doctrine of Chances. Philosophical Transactions of the Royal Society of London 53 (1763), 370­418.
[3] Ben Cartere e. 2011. Model-Based Inference About IR Systems. In Proceedings of ICTIR 2011 (LNCS 6931). 101­112.
[4] Ben Cartere e. 2012. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM TOIS 30, 1 (2012).
[5] Ben Cartere e. 2015. Bayesian Inference for Information Retrieval Evaluation. In Proceedings of ACM ICTIR 2015. 31­40.
[6] Charles L.A. Clarke, Nick Craswell, Ian Soboro , and Azin Ashkan. 2011. A Comparative Analysis of Cascade Measures for Novelty and Diversity. In Proceedings

of ACM WSDM 2011. 75­84. [7] Clarles L. A. Clarke, Nick Craswell, Ian Soboro , and Ellen M. Voorhees. 2012.
Overview of the TREC 2011 Web Track. In Proceedings of TREC 2011. [8] Bradley Efron. 2005. Bayesians, Frequentists, and Scientists. J. Amer. Statist.
Assoc. 100, 469 (2005), 1­5. [9] Ronald A. Fisher. 1970. Statistical Methods for Research Workers (14th Edition).
Oliver & Boyd. [10] Andrew Gelman. 1996. Inference and Monitoring Convergence. In Markov Chan
Monte Carlo in Practice, W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.). Chapman & Hall/CRC, Chapter 8. [11] Andrew Gelman, Jennifer Hill, and Masanao Yajima. 2008. Why We (Usually) Don't Have to Worry about Multiple Comparisons. Technical Report. [12] Lisa L. Harlow, Stanley A. Mulaik, and James H. Steiger (Eds.). 2016. What If
ere Were No Signi cance Tests (Classic Edition). Routledge. [13] Ma hew D. Ho man and Andrew Gelman. 2014. e No-U-Turn Sampler:
Adaptively Se ing Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research 15 (2014), 1351­1381. [14] John K. Kruschke. 2013. Bayesian Estimation Supersedes the t test. Journal of Experimental Psychology: General 142, 2 (2013), 573­603. [15] John K. Kruschke. 2015. Doing Bayesian Data Analysis (Second Edition). Elsevier. [16] Yasushi Nagata. 1996. How to Understand Statistical Methods (in Japanese). Nikkagiren. [17] Radford M. Neal. 2011. MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo, Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng (Eds.). 113­162. [18] Yurii Nesterov. 2009. Primal-Dual Subgradient Methods for Convex Problems. Mathematical Programming 120 (2009), 221­259. [19] Matia Okubo and Kensuke Okada. 2012. Psychological Statistics to Tell Your Story: E ect Size, Con dence Interval, and Power. Keiso Shobo. [20] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics based on the Bootstrap. In Proceedings of ACM SIGIR 2006. 525­532. [21] Tetsuya Sakai. 2011. Evaluating Diversi ed Search Results Using Per-Intent Graded Relevance. In Proceedings of ACM SIGIR 2011. 1043­1052. [22] Tetsuya Sakai. 2014. Statistical Reform in Information Retrieval? SIGIR Forum 48, 1 (2014), 3­12. [23] Tetsuya Sakai. 2016. Statistical Signi cance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015. Proceedings of ACM SIGIR 2016 (2016), 5­14. [24] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In Proceedings of ACM SIGIR 2016. 1045­1048. [25] Tetsuya Sakai, Noriko Kando, Chuan-Jie Lin, Teruko Mitamura, Hideki Shima, Donghong Ji, Kuang-Hua Chen, and Eric Nyberg. 2008. Overview of the NTCIR-7 ACLIA IR4QA Task. In Proceedings of NTCIR-7. 77­114. [26] Lifeng Shang, Tetsuya Sakai, Zhengdong Lu, Hang Li, Ryuichiro Higashinaka, and Yusuke Miyao. 2016. Overview of the NTCIR-12 Short Text Conversation Task. In Proceedings of NTCIR-12. 473­484. [27] Mark D. Smucker, James Allan, and Ben Cartere e. 2007. A Comparison of Statistical Signi cance Tests for Information Retrieval Evaluation. In Proceedings of ACM CIKM 2007. 623­632. [28] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. Kato, Yiqun Liu, Miho Sugimoto, Qinglei Wang, and Naoki Orii. 2011. Overview of the NTCIR-9 INTENT Task. In Proceedings of NTCIR-9. 82­105. [29] Student. 1908. e Probable Error of a Mean. Biometrika 6, 1 (1908), 1­25. [30] Hideki Toyoda (Ed.). 2015. Fundamentals of Bayesian statistics: Practical Ge ing Started by Hamiltonian Monte Carlo Method (in Japanese). Asakura Shoten. [31] Hideki Toyoda. 2016. An Introduction to Statistical Data Analysis: Bayesian Statistics for `post p-value era' (in Japanese). Asakuha Shoten. [32] Ellen M. Voorhees. 2004. Overview of the TREC 2003 Robust Retrieval Track. In Proceedings of TREC 2003. [33] Ronald L. Wasserstein and Nicole A. Lazar. 2016. e ASA's Statement on P-values: Context, Process, and Purpose. e American Statistician (2016). [34] Dell Zhang, Jun Wang, Emine Yilmaz, Xiaoling Wang, and Yuxin Zhou. 2016. Bayesian Performance Comparison of Text Classi ers. In Proceedings of ACM SIGIR 2016. 15­24. [35] Stephen T. Ziliak and Deirdre N. McCloskey. 2008. e Cult of Statistical Signi cance: How the Standard Error Costs Us Jobs, Justice, and Lives. e University of Michigan Press.

34


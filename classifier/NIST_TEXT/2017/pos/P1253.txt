Short Resource Papers

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Anserini: Enabling the Use of Lucene for Information Retrieval Research

Peilin Yang, Hui Fang
Department of Electrical and Computer Engineering University of Delaware
{franklyn,hfang}@udel.edu
ABSTRACT
So ware toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrieval models over standard test collections. E orts are generally directed toward be er ranking and less a ention is usually given to scalability and other operational considerations. On the other hand, Lucene has become the de facto platform in industry for building search applications (outside a small number of companies that deploy custom infrastructure). Compared to academic IR toolkits, Lucene can handle heterogeneous web collections at scale, but lacks systematic support for evaluation over standard test collections. is paper introduces Anserini, a new information retrieval toolkit that aims to provide the best of both worlds, to be er align information retrieval practice and research. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial e orts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web-scale collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for many TREC test collections, providing a convenient way to replicate competitive baselines right out of the box. Experiments verify that our system is both e cient and e ective, providing a solid foundation to support future research.
1 INTRODUCTION
Information retrieval researchers have a long history of developing, sharing, and using so ware toolkits to support their work. Over the past several decades, various IR toolkits have been built to aid in the development of new retrieval models, to test hypotheses about information seeking, and to validate new evaluation methodologies. As the eld moves forward, IR toolkits are expected to keep up with emerging requirements such as the ability to handle large web collections and new data formats. e growing complexity of modern so ware ecosystems and the resource constraints most academic research groups operate under make maintaining opensource toolkits a constant struggle.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080721

Jimmy Lin
David R. Cheriton School of Computer Science University of Waterloo jimmylin@uwaterloo.ca
Most IR toolkits developed by academics, such as Indri,1 Galago,2 and Terrier3 were primarily designed to facilitate evaluation over standard test collections from evaluation forums such as TREC, CLEF, NTCIR, etc. In many cases, scalability took a back seat to e orts around improving retrieval models, and thus these systems o en struggle to scale to modern web collection. As an example, the ClueWeb12 collection4 contains 733 million web pages, totaling 5.54 TB compressed (or 27.3 TB uncompressed). e standard practice for working with this collection, as exempli ed by the infrastructure built for the TREC 2014 Session Track [4], is to separately index partitions of the collection and then build a distributed broker architecture that integrates results from each partition. In general, working with web-scale collections using existing academic IR toolkits is time- and resource-intensive, even for basic tasks.
With the exception of a small number of companies (e.g., commercial web search engines), the open-source Lucene system5 and its derivatives such as Solr and Elasticsearch (for convenience, we simply refer to as "Lucene" collectively in this paper) have become the de facto platform for deploying search applications in industry. Examples include LinkedIn, Twi er, Bloomberg, as well as a number of online retailers and many large companies in the nancial services space. Despite its undeniable operational success, a large user base, and a vibrant community of contributors, Lucene is not well suited to information retrieval research. For many reasons, including poor documentation of system internals and a number of unintuitive abstractions, Lucene is not as widely used for research as academic toolkits such as Indri or Terrier.
In this paper, we describe our e orts in developing a new opensource information retrieval toolkit called Anserini that builds on Lucene.6 We aim to bridge the gap described above that separates information retrieval research from the practice of building real-world search applications. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial e orts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for standard TREC test collections, providing a convenient way to replicate competitive baselines "right out of the box", supporting the community's aspirations toward reproducible research [1, 7, 8, 10, 16, 18].
1h p://www.lemurproject.org/indri/ 2h p://www.lemurproject.org/galago.php 3h p://terrier.org/ 4h p://www.lemurproject.org/clueweb12/ 5h ps://lucene.apache.org/ 6h p://anserini.io/

1253

Short Resource Papers

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

We experimentally evaluate the e ciency and e ectiveness of Anserini on a number of standard test collections. In terms of indexing performance, it is able to handle the largest research web collection available today with ease on a single modern server. We observe be er indexing performance compared to Indri, a popular choice among researchers today. In terms of retrieval, we also nd that Anserini is not only faster than Indri, but returns rankings that are comparable in quality. In other words, Anserini is faster and just as good. We present the case that Anserini should be adopted as the toolkit of choice for information retrieval researchers.
2 ANSERINI OVERVIEW
2.1 Motivation
Despite its popularity in industry and broad adoption for operational search deployments, Lucene remains under-utilized in information retrieval research. We begin with some high-level discussions of why we believe this might be so to motivate our e orts in building Anserini.
From the very beginning, Lucene was wri en for "real world" search applications, not with researchers in mind. For the most part, its developers targeted an audience that mostly used search engines as black boxes, as opposed to researchers that required access to ranking internals such as scoring models, mechanisms for postings traversal, etc. Because of the target user population, documentation for Lucene internals has always been quite poor, especially in keeping up with the relatively rapid pace at which the developer community has been releasing improved versions of the so ware. Access to these internals is exactly what information retrieval researchers need for their studies, and therefore poor documentation has been a barrier to entry.
To further compound this issue, the internal APIs in Lucene are not organized in a way that would be intuitive to most IR researchers, with class names that are not indicative of functionality and many levels of indirection. is is not an issue for "black box" users of Lucene, but presents a hurdle for information retrieval researchers who desire access to system internals. As an example, the code to open up a Lucene index and to traverse postings programmatically (without invoking the scoring function) is unnecessarily complex and involves dispatching to several intermediate classes along the way. Some researchers have the impression that Lucene is di cult to use, and indeed there is some truth to this, especially with respect to low-level abstractions.
Another side e ect of Lucene's focus on "black box" search is that it has severely lagged behind in the implementation of modern ranking functions. For the longest time, the default scoring model was an ad hoc variant of tf-idf. Okapi BM25 was not added to Lucene until 2011,7 more than a decade a er it gained widespread adoption in the research community as being more e ective than tf-idf variants. is lag in adopting "research best practices" has contributed to the perception that Lucene is not e ective and illsuited for information retrieval research. However, this perception is no longer accurate today. Lucene comes with implementations of modern baseline retrieval models, and we show that the e ectiveness of Lucene's implementations is at least as good as those o ered by academic IR toolkits (see Section 3).
7h ps://issues.apache.org/jira/browse/LUCENE-2959

Finally, because Lucene is wri en in Java, there is sometimes the perception that it is slow and ine cient, particularly when scaling up to modern web collections. Developers o en point to the managed memory environment of the Java Virtual Machine (JVM) as not being conducive to e cient low-level implementations of search engine internals. We experimentally show that this is de nitely not true (see Section 3). e open-source community has devoted substantial e ort to optimizing the performance of Lucene and taking advantage of today's multi-core processors. It is capable of handling large web collections on a single server with ease.
e goal of Anserini is to align the practice of building search applications with research in information retrieval. Colloquially speaking, our toolkit aims to smooth the "rough edges" around Lucene for the purposes of information retrieval research. It is not our goal to replace or to reimplement Lucene, but rather to facilitate its use for research by presenting as gentle a learning curve as possible to newcomers.
2.2 Main Components
Anserini components fall into two categories: wrappers and extensions. Wrappers provide APIs that leverage core Lucene library components to accomplish speci c tasks. ey are tightly integrated with "core" Lucene and in some cases, represent custom implementations of existing Lucene APIs. Extensions, on the other hand, are components that are distinct from Lucene and more loosely coupled: these may represent our own implementations or connectors to third-party libraries.
Multi-threaded indexing (wrapper). Inverted indexing is one of the most fundamental tasks in information retrieval and the starting point of many research studies. In working with large web collections, it is imperative that indexing operations are e cient and scalable. While academic researchers have a empted to address this issue via MapReduce and related frameworks [5, 11], these solutions impose the burden of requiring clusters and additional so ware infrastructure.
Lucene supports multi-threaded indexing, and as we experimentally show (Section 3), it is able to scale up to large web collections on a single commodity server. e biggest issue, however, is that Lucene itself only provides access to a collection of indexing components that researchers need to assemble together to build an end-to-end indexer. For example, the developer would need to write from scratch custom document processing pipelines, code for managing individual indexing threads, and implementations of load balancing and synchronization procedures.
We address these issues in Anserini by providing abstractions for document collections that an IR researcher would be comfortable with, as well as the implementation of an e cient, high-throughput, multi-threaded indexer that takes advantage of these abstractions. Anserini models collections as comprised of individual segments (for example, the ClueWeb12 collection is comprised of a number of compressed WARC les) and provides implementations for common document formats--for parsing TREC-style XML documents, web pages stored in WARCs, tweets in JSON format, etc. In fact, Anserini ships with the ability to index many TREC collections "right out of the box". is greatly reduces the learning curve for researchers to get started with Lucene.

1254

Short Resource Papers

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Indexing performance of Anserini and Indri on smaller collections using 16 threads on a modest commodity server.

Collection Disk12 Disk45 AQUAINT WT2G WT10G Gov2

docs 742k 528k 1.03m 246k 1.69m 25.2m

terms 219m 175m 318m 182m 752m 17.3b

Anserini (count) time size
00:01:24 199MB 00:01:13 166MB 00:01:53 305MB 00:02:21 143MB 00:04:55 708MB 01:16:32 11GB

Anserini (pos) time size
00:01:44 512MB 00:01:33 423MB 00:02:10 734MB 00:02:55 437MB 00:05:05 2.9GB 02:32:43 38GB

Anserini (doc) time size
00:03:09 2.5GB 00:02:51 2.1GB 00:04:32 3.8GB 00:04:24 2.3GB 00:09:51 12GB 06:52:35 331GB

Indri time size 00:12:28 2.5GB 00:06:55 1.9GB 00:17:36 3.9GB 00:07:25 2.2GB 00:42:51 9.6GB 14:51:12 215GB

Table 2: Indexing performance of Anserini on web collections using 88 threads on a high-end server.

Collection CW09b CW09 CW12b13 CW12

docs 50m 504m 52m 733m

terms 31b 268b 31b 429b

Anserini (count)

time

size

00:42 28GB

07:32 254GB

00:57 29GB

17:01 376GB

Anserini (pos) time size 01:13 75GB 12:18 649GB 01:25 76GB 22:21 1.1TB

Streamlined IR evaluation (extension). Test collections play an important role in information retrieval research, and a substantial amount of research activity in improving ranking models is focused around ad hoc retrieval runs. A research toolkit should make this "inner loop" of IR research as easy as possible. Since Lucene was not originally designed for researchers, support for running experiments on standard test collections is largely missing. Anserini lls this gap by implementing missing features: parsers for di erent query formats, a uni ed driver program for ad hoc experiments that outputs standard trec eval format, etc. For convenience, existing TREC topics and qrels are included directly in our code repository--once again, reducing the learning curve for researchers to get started with Lucene.
ere are two main uses for this feature in Anserini: First, our toolkit provides an easy way for researchers to replicate baselines of standard retrieval models such as BM25 and query likelihood. Armstrong et al. [2] previously identi ed the prevalent problem of weak baselines in experimental IR papers. Lin et al. [10] further observed that authors are o en vague about the baseline parameter se ings and the implementations they use. For example, Mu¨hleisen et al. [13] reported large di erences in e ectiveness across four systems that all purport to implement BM25. Trotman et al. [15] pointed out that BM25 and query likelihood with Dirichlet priors can actually refer to at least half a dozen variants, and in some cases, di erences in e ectiveness are statistically signi cant. ere is substantial community interest in engaging with reproducibilityrelated issues [1, 8], and Anserini contributes to this discussion. Our proposed solution is to have widely-available baselines that are both competitive in e ectiveness and easy to replicate. It is our hope that Anserini can ll this role.
Second, an easy-to-use baseline retrieval component in Anserini provides the starting point for additional ranking extensions. In particular, we advocate a multi-stage ranking architecture [3, 6, 14, 17] so that researchers will not need to directly work with native Lucene scoring APIs. at is, researchers should take advantage of Anserini APIs that generate an initial document ranking and hooks

for feature extraction to build subsequent reranking stages. is, in fact, is the common architecture used in commercial web search engines today to support learning to rank [14].
Relevance feedback (extension). Relevance feedback techniques provide robust solutions to the vocabulary mismatch problem between expressions of user information needs and relevant documents. Anserini provides a reference implementation of the RM3 variant of relevance models [9], built as a reranking module in the multi-stage architecture described above. us, our implementation is useful not only as a baseline for comparing query expansion techniques, but provides an example of how reranking extensions can be implemented in Anserini.
3 EVALUATION
We describe experiments to support three claims about Anserini and the use of Lucene for information retrieval research. First, that Anserini is highly scalable and able to e ciently index large web collections. Second, that Anserini is similarly e cient in searching these collections and ranking documents using standard baseline models. Finally, Anserini is able to achieve scalable indexing and e cient retrieval without compromising ranking e ectiveness.
e indexing performance of Anserini on a number of smaller and older collections is shown in Table 1. ese experiments were conducted on a server with dual AMD Opteron 6128 processors (2.0GHz, 8 cores) with 40GB RAM running CentOS 6.8. is machine can be characterized as an old, modest commodity server. All experiments were run on an otherwise idle machine. With Anserini, we used 16 threads for indexing and we report results from three di erent index con gurations: count indexes where only term frequency information is stored (count), positional indexes that also store term positions (pos), and positional indexes that also store the raw documents and parsed document vectors (doc). For each condition, we report the indexing time in HH:MM:SS (averaged over two trials) as well as the index size. e size of each collection is also shown for reference. As a comparison condition, we indexed the same collections using Indri 5.9 on the same machine.
In Table 2, we report indexing performance for larger web collections on a server with dual Intel Xeon E5-2699 v4 processors (2.2GHz, 22 cores) and 1 TB RAM running Ubuntu 16.04. e table rows indicate di erent collections: CW09b refers to the ClueWeb09 (category B) web crawl, CW09 refers to all English pages in the ClueWeb09 web crawl, CW12b13 refers to the smaller ClueWeb12B13 web crawl, and CW12 refers to the complete ClueWeb12 web crawl. Due to the size of the collections, we only report the count and positional index con gurations. For these experiments, we used

1255

Short Resource Papers

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: Retrieval e ciency for Terabyte 06 e ciency queries on Gov2, using a single thread.

Latency (ms)

Indri

2403

Anserini

382

roughput (qps) 0.42 2.61

Table 4: E ectiveness comparisons between Anserini and Indri on standard TREC test collections.

Collection eries
BM25 (I) BM25 (A) LM (I) LM (A)

Disk12 51-200
0.2040 0.2267 0.2269 0.2232

Disk45 301-450 601-700
0.2478 0.2500 0.2516 0.2465

WT2G 401-450
0.3152 0.3015 0.3116 0.2922

WT10G 451-550
0.1955 0.1981 0.1915 0.2015

Gov2 701-850
0.2970 0.3030 0.2995 0.2951

88 threads on an otherwise idle machine; indexing time is reported in HH:MM (averaged over two trials). On this server, we are able to index all of ClueWeb12, one of the largest collections available to researchers today, in less than a day! As seen from Table 1, even on an older server, the indexing performance of Lucene is impressive. Compared to academic toolkits, Lucene does not appear to have any trouble scaling to large modern web collections.
Our next set of experiments were conducted on the Gov2 collection with Terabyte 06 e ciency queries. We issued all 100,000 queries sequentially against both the Anserini and Indri indexes on the slower AMD Opteron server. Results are shown in Table 3, which reports latency (ms) and throughput (queries per second, or qps). In this experiment, we used only a single query thread, and therefore do not take advantage of Lucene's ability to execute queries in parallel on multiple threads (so in our case, throughput is simply the inverse of latency). We see from these experiments that Lucene is roughly six times faster than Indri.
Finally, we compared the retrieval e ectiveness of Anserini and Indri. For Indri we refer to the RISE work of Yang and Fang [18], as they ne-tuned model parameters to achieve optimal e ectiveness. We considered two baseline ranking models: Okapi BM25 (BM25) and query likelihood with Dirichlet priors (LM). For Anserini, we removed stopwords (the default) and tuned parameters as follows: for BM25, k = 0.9 and b  [0, 1] in increments of 0.1; for LM, µ  [0, 5000] in increments of 500. Results on standard TREC collections and queries are shown in Table 4, where (I) refers to Indri and (A) refers to Anserini. We see that e ectiveness results are comparable between the two systems.
In summary, our experiments show that Anserini is at least as good as Indri in terms of e ectiveness, and much faster in both indexing and retrieval. ese results are consistent with ndings from the recent Open-Source IR Reproducibility Challenge [10]. Together, empirical evidence presents a compelling case for adopting Lucene for information retrieval research.
4 CONCLUSIONS AND FUTURE WORK
Our message to the information retrieval community is that Lucene is e cient and scalable without compromising e ectiveness. Furthermore, Lucene has the bene t of a large user community and

broad adoption in industry. Anserini smooths over the "rough
edges" of using Lucene for information retrieval research by pro-
viding wrappers and extensions that simplify common tasks such
as indexing large research web collections and performing standard
ad hoc retrieval runs. We hope that our toolkit will help to be er
align the research and practice of information retrieval.
Broadly characterized, Anserini provides the foundation for an
IR research toolkit, but currently lacks features that one would
associate with cu ing-edge research. Ongoing work is focused on
addressing this issue, as we are actively exploring retrieval models
based on deep learning [12]. E orts include a empts to replicate
existing neural retrieval models within our framework. Given the
existence of many deep learning toolkits (Torch, TensorFlow, etc.),
it does not make sense to reinvent the wheel. In this spirit, we
have been building connectors between Lucene and the PyTorch
deep learning toolkit. Moving forward, we anticipate substantial
continued interest at the intersection of deep learning and informa-
tion retrieval, and the multi-stage ranking architecture of Anserini
provides a natural integration point for future explorations.
REFERENCES
[1] Jaime Arguello, Ma Crane, Fernando Diaz, Jimmy Lin, and Andrew Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR). SIGIR Forum 49, 2 (2015), 107­116.
[2] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don't Add Up: Ad-Hoc Retrieval Results Since 1998. In CIKM. 601­610.
[3] Nima Asadi and Jimmy Lin. 2013. E ectiveness/E ciency Tradeo s for Candidate Generation in Multi-Stage Retrieval Architectures. In SIGIR. 997­1000.
[4] Ben Cartere e, Evangelos Kanoulas, Mark Hall, and Paul Clough. 2014. Overview of the TREC 2014 Session Track. In TREC.
[5] Marc-Allen Cartright, Samuel Huston, and Henry Feild. 2012. Galago: A Modular Distributed Processing and Retrieval System. In SIGIR 2012 Workshop on Open Source Information Retrieval. 25­31.
[6] Charles L. A. Clarke, J. Shane Culpepper, and Alistair Mo at. 2016. Assessing E ciency--E ectiveness Tradeo s in Multi-stage Retrieval Systems Without Using Relevance Judgments. IRJ 19, 4 (2016), 351­377.
[7] Hui Fang, Hao Wu, Peilin Yang, and ChengXiang Zhai. 2014. VIRLab: A Webbased Virtual Lab for Learning and Studying Information Retrieval Models. In SIGIR. 1249­1250.
[8] Nicola Ferro, Norbert Fuhr, Kalervo Ja¨rvelin, Noriko Kando, Ma hias Lippold, and Justin Zobel. 2016. Increasing Reproducibility in IR: Findings from the Dagstuhl Seminar on "Reproducibility of Data-Oriented Experiments in e-Science". SIGIR Forum 50, 1 (2016), 68­82.
[9] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR. 120­127.
[10] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.
[11] Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan Wang. 2009. Of Ivory and Smurfs: Loxodontan MapReduce Experiments for Web Search. In TREC.
[12] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv:1705.01509.
[13] Hannes Mu¨hleisen, aer Samar, Jimmy Lin, and Arjen de Vries. 2014. Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping. In SIGIR. 863­866.
[14] Jan Pedersen. 2010. ery Understanding at Bing. Invited Talk at SIGIR. [15] Andrew Trotman, An i Puurula, and Blake Burgess. 2014. Improvements to
BM25 and Language Models Examined. In ADCS. 58­65. [16] Ellen M. Voorhees, Shahzad Rajput, and Ian Soboro . 2016. Promoting Repeata-
bility rough Open Runs. In EVIA. 17­20. [17] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A Cascade Ranking Model
for E cient Ranked Retrieval. In SIGIR. 105­114. [18] Peilin Yang and Hui Fang. 2016. A Reproducibility Study of Information Retrieval
Models. In ICITR. 77­86.
Acknowledgments. is research was supported by the Natural Sciences
and Engineering Research Council (NSERC) of Canada and the U.S. National
Science Foundation under IIS-1423002 and CNS-1405688.

1256


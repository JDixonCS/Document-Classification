Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Comparing In Situ and Multidimensional Relevance Judgments

Jiepu Jiang
Center for Intelligent Information Retrieval, University of Massachuse s
Amherst jpjiang@cs.umass.edu

Daqing He
School of Computing and Information, University of Pi sburgh
dah44@pi .edu

James Allan
Center for Intelligent Information Retrieval, University of Massachuse s
Amherst allan@cs.umass.edu

ABSTRACT
To address concerns of TREC-style relevance judgments, we explore two improvements. e rst one seeks to make relevance judgments contextual, collecting in situ feedback of users in an interactive search session and embracing usefulness as the primary judgment criterion. e second one collects multidimensional assessments to complement relevance or usefulness judgments, with four distinct alternative aspects examined in this paper--novelty, understandability, reliability, and e ort.
We evaluate di erent types of judgments by correlating them with six user experience measures collected from a lab user study. Results show that switching from TREC-style relevance criteria to usefulness is fruitful, but in situ judgments do not exhibit clear bene ts over the judgments collected without context. In contrast, combining relevance or usefulness with the four alternative judgments consistently improves the correlation with user experience measures, suggesting future IR systems should adopt multi-aspect search result judgments in development and evaluation.
We further examine implicit feedback techniques for predicting these judgments. We nd that click dwell time, a popular indicator of search result quality, is able to predict some but not all dimensions of the judgments. We enrich the current implicit feedback methods using post-click user interaction in a search session and achieve be er prediction for all six dimensions of judgments.
KEYWORDS
Relevance judgment; search experience; implicit feedback.
1 INTRODUCTION
Test collection-based IR evaluation relies on human assessments of search result quality. e most popular method is the Cran eldstyle relevance judgments [9], such as the approach used in TREC [10], where assessors (usually trained experts) judge a preassigned set of search results one a er another using criteria that focus on topical relevance. is method had achieved great success but also a racted criticism such as focusing solely on topical relevance and ignoring real users' perceptions of the usefulness of results in a particular search context. We examine two directions to improve this status quo.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080840

One direction is to incorporate context into assessments. at is, the value of a search result depends on the scenario and context of accessing the result. Belkin et al. [5] proposed to evaluate interactive search systems by the usefulness of each interaction for accomplishing a search task. We can apply this model to search result judgments--to assess the usefulness of a click (the perceived usefulness of a clicked result). is intrinsically requires us to switch from relevance to usefulness as the primary judgment criteria, and to collect in situ judgments to take into account the particular time and context of accessing a search result.
Two recent e orts [25, 36] examined this direction. Kim et al. [25] collected users' in situ feedback of clicked results a er they had
nished examining the results. However, they restricted the in situ feedback to "thumbs-up" or "thumbs-down". Mao et al. [36] asked users to assess the usefulness of the clicked results a er a search session without considering the particular context. Both studies reported improved correlations with search experience measures comparing to TREC-style relevance judgments by external assessors. However, neither study excluded the in uence of the di erence between searchers and external assessors on relevance judgments.
e other direction is to use a combination of multiple aspects of judgments. Many previous studies tried to complement relevance with seemingly reasonable dimensions, such as novelty [6, 55], understandability [41, 56], credibility [39, 46, 51, 53], readability [42, 49], e ort [20, 50, 54], freshness [11], etc. Multidimensional judgments are also popular approaches used in user-centric evaluation models [19, 27, 52]. However, most previous IR studies had only examined one particular alternative dimension to relevance, and they had not veri ed the value of multidimensional judgments by correlating with user experience measures.
We evaluate and compare these two directions. We collected users' search result judgments from six dimensions (relevance, usefulness, novelty, understandability, reliability, and e ort) in two se ings--an in situ one that happened right a er users had nished examining a clicked search result (called in situ judgments), and a context-independent one collected a er a search session (called post-session judgments). We evaluate the two types of judgments on six dimensions by correlating with six search experience measures collected from a laboratory user study. We also examined implicit feedback methods for predicting these judgments.
We examine the following questions in the rest of this article:
· Do in situ judgments be er correlate with search experience measures than context-independent (post-session) ones? Do multiple dimensions of judgments help relevance/usefulness judgments be er correlate with search experience measures? Which dimensions of judgments should we collect to improve a particular user experience measure? Section 3 seeks answers to these questions.

405

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: A screenshot of the search interface and the in situ judgments interface.

· Can we e ectively predict di erent search result judgments using implicit feedback signals? Section 4 and Section 5 examine techniques for addressing this challenge.
2 USER STUDY
We designed a user study to collect search result judgments. e user study asked participants to work on di erent tasks in an experimental search system. We recorded users' search behavior and collected their in situ and post-session search result judgments.
2.1 Experiment Design
e user study employed a 2×2 within-subject design to balance di erent types of search tasks. e tasks come from the TREC session tracks [7] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classi cation framework [28]. e targeted task product is either factual (to locate facts) or intellectual (to enhance the user's understanding of a topic). e goal of a task is either speci c (clear and fully developed) or amorphous (an ill-de ned or unclear goal that may evolve along with the user's exploration).
We divided participants into groups of four. Participants in the same group worked on the same four tasks (one task for each type) but using a di erent sequence (rotated using a Latin square). We assigned di erent tasks to di erent groups to increase task diversity.
For each task, the participants went through two stages: · Search stage (10 minutes). e participants performed an in-
teractive search session to address the task. ey could submit and reformulate any queries and click on any search results. After clicking on a result's link, the participants switched to the result webpage in a new browser tab. When they had nished examining the result and turned back to the SERP, the participant needed to provide in situ judgments on the clicked results before they could resume the search session. Figure 1 shows the screenshots of the search interface and the in situ judgments. · Judgment stage (about 10 minutes). e participants rated their search experience in the session and nished post-session judgments on each result they visited in the session. Section 2.2 introduces details of the in situ and post-session judgments. As Figure 1 shows, the interface of the experimental system is similar to popular web search engines. e system redirected users' queries to Google and returned ltered Google search results. e system only showed the "10-blue links", vertical search results

(except image verticals), and related queries. Other SERP elements were removed to simplify the user study. e system displayed results in the same way they would appear on Google. e main di erence between our system and Google in SERP design was that our system showed task description on the top of a SERP (to help participants recall task requirements) and we showed related searches on the right side of a SERP.
e participants spent about 100 minutes to nish an experiment. First, they worked on a training task (including all the steps) for 10 minutes. en, they worked on four formal tasks, spending about 20 minutes on each task. We required the participants to take a 5-minute break a er two formal tasks to reduce fatigue.
2.2 Collecting Search Result Judgments
We collected search result judgments in two di erent scenarios: · In situ judgments ­ participants assessed a clicked result when
they had nished examining it and turned back to the SERP. · Post-session judgments ­ the judgments collected a er a search
session (in the judgment stage). e in situ judgments measure the participants' perceptions of
the clicked result at (roughly) the same time and contexts they visit the result. e approach is similar to Kim et al. [25], except that we adopted di erent measures to assess search results. In the search stage, we instructed the participants to examine results as they would normally do when using a search engine in their daily lives. For example, they did not need to fully read a result and they could abandon examining. Particularly, they were instructed that during the in situ judgments, they should not revisit the result for the purpose of answering the judgment questions (and we did not o er a link for revisiting in the in situ judgment interface). is is to ensure that the in situ judgments only measure participants' perceptions of the latest click activity.
e post-session judgments resemble the TREC-style relevance judgments, where the assessors judge results without a particular search context and in a random order--they are asked to judge a set of results one a er another in detail. In our post-session judgments, the assessors are real searchers. We asked them to judge the set of results they visited in the session. We instructed them to examine the results in a be er detail in the post-session judgments. e system also required participants to revisit each clicked result and spend at least 30 seconds to judge a result.

406

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: estions for collecting search result judgments and users' search experience.

Search Result Judgments
Topical Relevance (TRel)
Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia)

estion & Options How relevant is this webpage? · Key (3): this page or site is dedicated to the topic; authoritative and comprehensive; it is worthy of being a top result. · Highly Relevant (2): the content of this page provides substantial information on the topic. · Relevant (1): the content of this page provides some information on the topic, which may be minimal. · Not Relevant or Spam (0). In Situ: How much useful information did you get from this web page? From 1 (none) to 7 (a lot of). Post-session: How much useful information did this web page provide for the task? From 1 (none) to 7 (a lot of). How much new information did you get from this web page? From 1 (none) to 7 (a lot of). How much e ort did you spend on this web page? From 1 (none) to 7 (a lot of). How di cult was it for you to follow the content of this web page? From 1 (very di cult) to 7 (very easy). How trustworthy is the information in this web page? From 1 (not at all trustworthy) to 7 (very trustworthy).

Search Experience Measures Satisfaction (Sat) Frustration (Frus) System Helpfulness (Help) Goal Success (Succ) Session E ort (S.Eff) Di culty (Diff)

estion & Options How satis ed were you with your search experience? From 1 (very unsatis ed) to 7 (very satis ed). How frustrated were you with this task? From 1 (not frustrated) to 7 (very frustrated). How well did the system help you in this task? From 1 (very badly) to 7 (very well). How well did you ful ll the goal of this task? From 1 (very badly) to 7 (very well). How much e ort did this task take? From 1 (minimum) to 7 (a lot of). How di cult was this task? From 1 (very easy) to 7 (very di cult).

We collected users' in situ and post-session judgments of six different measures. Table 1 shows the detailed questions and options.
· TREC relevance (TRel) ­ the de facto standard of relevance judgments due to the popularity of TREC test collections. We collected TRel using the criteria of the latest TREC web track [10]. As Table 1 shows, the criteria focus on topical relevance. We excluded the relevance level Nav (the correct homepage of a navigational query) from the original TREC criteria because our search tasks do not include navigational search.
· Usefulness (Usef) ­ Following Belkin et al.'s model [5] and Mao et al.'s study [36], we collected users' perceptions regarding the usefulness of the clicked results.
· Novelty (Nov) ­ Novelty was o en assessed algorithmically in previous studies based on sub-topic or "nugget" level relevance judgments [8, 40, 43, 55]. In contrast, we collect users' explicit novelty judgments.
· Understandability (Under) ­ the easiness of understanding the content of the result. Recent studies incorporated understandability into search result ranking [41] and evaluation [56].
· Reliability (Relia) ­ the reliability, credibility, and trustworthy of the information presented in the result [39, 46, 51] (here we do not distinguish the three constructs).
· E ort ­ Yilmaz et al. [54] and Verma et al. [50] examined e ort as a dimension of evaluating search result.
e following table summarizes the measures collected in in situ and post-session judgments. We only collected TRel in post-session judgments because the TREC criteria do not consider context. We only collected Nov and Effort during in situ judgments because the participants of a pilot study reported confusions assessing the two measures twice. In the rest of this paper, we will use .i and .p su xes to denote in situ and post-session judgments, respectively. For example, Usef.i denotes users' in situ usefulness judgments.
Except for TRel, we collected judgments using a 7-point Likert scale, because a previous study [48] showed that assessors approximate the optimal level of con dence when using a 7-point scale for relevance judgments. TRel used a di erent scale so that it is

TREC relevance (TRel) Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia)

In Situ (.i)

Post-session (.p)

consistent with the TREC web track (as a representative example of the state-of-the-art relevance judgment methods).
2.3 Search Experience Measures
In the judgment stage, participants rated their search experience in a session. We collected six representative user experience measures used in previous studies of information retrieval and recommender systems--satisfaction (Sat) [17, 21, 26, 35, 36, 45], goal success (Succ) [1, 18], frustration (Frus) [12, 13], task di culty (Diff) [4, 15, 29, 31, 32], the helpfulness of the system (Help) [19] and the total e ort spent (S.Eff) [27]. Table 1 includes the questions.
2.4 Rationale of Experiment Design
e way we balance di erent types of tasks is similar to previous studies [22, 24, 30, 33, 36]. However, we acknowledge that the selected tasks cannot cover all varieties. It is also worth noting that the TREC session track tasks [7] are more complex than regular web search requests such as navigational search.
Our study aims to collect both in situ judgments and user behaviors related to the clicked results. is poses challenges to the experiment design. On the one hand, we hope to collect accurate in situ judgments, which o en requires multi-item measurements [27, 52]. On the other hand, interrupting participants for in situ judgments breaks the ow of search session and can a ect their subsequent search behaviors. To balance between the two purposes, we made a few compromises in experiment design, e.g., we only collected six popular dimensions of judgments, and we simply used one question to measure each dimension.

407

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Spearman's correlation () matrix of di erent judgments for the 727 unique clicks.

In Situ Judgments

Post-session Judgments

Usef.i Novelty Effort Under.i Relia.i TRel Usef.p Under.p

Novelty

0.67

-

-

-

-

-

-

-

In Situ

E ort

0.22

Understandability

0.20

0.24

-

0.14 -0.45

-

-

-

-

-

-

-

Reliability

0.42

0.37

0.05

0.26

-

-

-

-

Topical Relevance

0.63

0.46

0.16

0.14

0.42

-

-

-

Post-session

Usefulness Understandability

0.72 0.20

0.52

0.16

0.18 -0.36

0.18 0.68

0.43 0.83 0.29 0.18

0.24

-

Reliability

0.43

0.38

0.04

0.22

0.82 0.48

0.51

0.31

e reported values are estimated from 1000 bootstrap samples (we used strati ed sampling to balance user and task dependency).

While examining search behaviors, we excluded the time spent on answering in situ judgments from dwell time. On average the participants spent 57.1 seconds on a clicked result and 12.1 seconds to answer the ve in situ judgment questions.
2.5 Collected Data
We recruited 28 participants (16 are female) through iers posted on the campuses of two universities in the United States. We required participants to be English native speakers to exclude the in uence of language uency on relevance judgments [16]. All the participants were undergraduate or graduate students studying di erent elds.
ey were reimbursed $15 per hour. We collected 112 sessions by 28 participants on 28 tasks. Each participant worked on four unique tasks and each task was performed by four unique users. In total, we collected 537 queries (4.8 per session) and 736 clicks (6.6 per session) on 727 unique sessionURL pairs (9 cases of revisiting). We exclude the 9 cases of revisiting from the analysis (about 1% of the data) to simply the analysis.
3 IN SITU VS. POST-SESSION JUDGMENTS
3.1 Correlation of Di erent Judgments
Table 2 reports the correlation of di erent judgments, which are generally consistent with previous studies. For example, relevance and usefulness positively correlate with novelty and reliability [52], understandability negatively correlates with e ort [50], etc. We examined the relationship of the judgments in another article [23].
Note that Mao et al. [36] reported a weak correlation (0.332) of searchers' post-session usefulness judgments and external assessors' relevance judgments. However, Table 2 shows that TRel and Usef.p are strongly correlated ( = 0.83) when both of them are assessed by searchers. is suggests that the low correlation reported by Mao et al. [36] may be mostly due to the disparity between searchers and external assessors, rather than the di erence between using relevance or usefulness as the judgment criteria.
3.2 Correlating with User Experience
We evaluate di erent search result judgments by correlating with (regressing) users' search experience measures in a session. is is based on the assumption that the "quality" of the clicked results in a session can in uence users' search experience in that session--thus, a reasonable search result judgment (assumed to indicate certain "quality"), or a reasonable set of judgments, should also correlate with users' search experience in a session.

3.2.1 Regression Analysis. We use multilevel regression analysis

to examine the relationship between the judgments of the clicked

results and users' search experience in a session. e dependent

variables (DVs) are each of the six search experience measures. e

independent variables (IVs) include the statistics of judgments re-

garding the clicked results in a session (such as the mean, maximum,

and minimum ratings). For TRel, Usef.i, and Usef.p, we include

the mean, maximum, and minimum ratings of the clicked results in

a session as IVs in the regression analysis. For other search result

judgments, we only include the maximum and minimum ratings of

the clicked results as IVs. is is because the mean ratings of the

other measures o en highly correlate with those of TRel and Usef,

causing multicollinearity issues for regression analysis.

For each user experience measure (the DV), we examine six

di erent models that include di erent judgments as IVs.

· Unidimensional & Context-independent ­ Model 1 and 2

only include context-independent search result judgments from

a single dimension--Model 1 includes the statistics of TRel and

Model 2 includes those of Usef.p.

· Unidimensional & In Situ ­ Model 3 includes in situ judg-

ments from a single dimension (the statistics of Usef.i) as IVs.

· Multidimensional & Context-independent ­ Model 4 and

5 extend Model 1 and 2 to include other dimensions of judg-

ments (the statistics of Under.p, Relia.p, Nov, and Effort).

Note that Model 4 and 5 include two in situ judgments (Nov

and Effort) because we did not collect post-session judgments

on these two dimensions (as discussed in § 2.2).

· Multidimensional & In Situ ­ Model 6 extends Model 3 to

include other dimensions of judgments (the statistics of Under.i,

Relia.i, Nov, and Effort).

Contextindependent

In Situ

Unidimensional

1 TRel only 2 Usef.p only

3 Usef.i only

Multidimensional

4 TRel + others 5 Usef.p + others

6 Usef + others

All six models also include the same set of control variables, including: gender (Male or Female), age (four levels; 0 for 18­24, 1 for 25­30, 2 for 31­40, and 3 for Over 40), highest degree obtained or expected (Undergraduate or Graduate), the expertise of using web search engines (SE Expertise) rated using a Likert scale from 1 (very badly) to 5 (very well), task product and goal, user's familiarity with the topic of the task (Topic Familiarity) rated using a Likert scale from 1 (very unfamiliar) to 7 (very familiar), and the number of clicks (# clicks) and queries (# queries) in the session.

408

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: e adjusted R2 of di erent regression models.

Models

Sat Frus Succ S.Eff Help

Base (control only) 0.12 0.06 0.11 0.06 0.11

1 TRel

0.23 0.09 0.18 0.10 0.16

2 Usef.p

0.25 0.15 0.36 0.17 0.18

3 Usef.i

0.29 0.14 0.35 0.16 0.22

4 TRel + others 0.31 0.25 0.33 0.33 0.31

4 vs. 1

**

**

**

**

**

5 Usef.p + others 0.30 0.26 0.42 0.37 0.31

5 vs. 2

**

**

**

**

**

6 Usef.i + others 0.30 0.27 0.34 0.37 0.27

6 vs. 3

**

**

*

* and ** indicate p < 0.05 and p < 0.01 by F-test.

Diff
0.03 0.09 0.18 0.22 0.21
** 0.26
** 0.33
**

We examine multicollinearity between variables using variance
in ation factor (VIF). e IVs of all models satisfy VIF < 4, the com-
monly suggested threshold (4­10) for concerns of multicollinearity issues [37]. Table 3 reports the adjusted R2 of the six models for
regressing the six dimensions of search experience.

3.2.2 TREC Relevance vs. Usefulness. We rst compare TREC relevance criteria (TRel) and post-session usefulness judgments (Usef.p). is is a revisit of Mao et al.'s study [36], which compared searchers' usefulness judgments and external assessors' relevance judgments. Here we collected both judgments from real searchers, removing the in uence caused by the di erence between searchers and external annotators in relevance judgments. e regression analysis suggest that switching from TREC relevance to usefulness is fruitful, consistently enhancing the ability of the regression models to correlate with user experience (by adjusted R2).
Models 1 and 2 include the mean, maximum, and minimum TRel or Usef.p ratings of the clicked results. Model 2 consistently explains the six search experience measures be er than Model 1 (by adjusted R2). We note that usefulness (Usef.p) seems to be particularly be er than TREC relevance (TRel) in terms of correlating with goal success (Succ), with adjusted R2 = 0.36 vs 0.18.
Models 4 and 5 further include other dimensions of judgments as IVs. is helps compare TRel and Usef.p judgments with other search result judgments as controls. Still, we consistently observe that Model 5 explains the six search experience measures be er than or as well as model 4 . ese results verify that usefulness is indeed a be er criteria of relevance judgments than TREC-style relevance (in terms of correlating with users' search experience).

3.2.3 In Situ vs. Context-independent (Post-session) Judgments. We further compare in situ and post-session judgments in both unidimensional and multidimensional se ings. Results suggest in situ usefulness judgments have be er correlations with a few (but not all) user experience measures than post-session usefulness judgments. However, a er combining search result judgments from di erent dimensions, in situ judgments show limited advantages over post-session ones.
Models 3 and 2 include the mean, maximum, and minimum Usef.i or Usef.p ratings of the clicked results as IVs. Results show Model 3 explains satisfaction (Sat), helpfulness (Help), and task di culty (Di ) slightly be er than Model 2 , with about 0.04 di erence in adjusted R2.
We further compare in situ and post-session judgments in a multidimensional se ing, using a combination of Usef.p/Usef.i

and other four judgments as IVs (Models 5 and 6 ). Results show that the post-session multidimensional model ( 5 ) be er correlates with search success (Succ) than the in situ one (adjusted R2 0.42 vs 0.34), but the la er also be er correlates with task di culty (adjusted R2 0.26 vs. 0.21). Overall, no evidence suggests either model is consistently be er than another in terms of correlating with users' search experience measures.
Even though Model 3 (Usef.i only) performs slightly be er than Model 2 (Usef.p only), results suggest limited advantages of in situ judgments over post-session ones in terms of correlating with search experience measures. We suspect a possible reason is that a 10-minute session is not long enough to trigger su cient di erences between in situ and post-session judgments. Although we expect to observe a greater di erence between in situ and post-session judgments in longer sessions, we believe a substantial proportion of web search sessions are no longer than 10 minutes, which may not bene t much from in situ judgments. In addition, it also requires a more complex experiment design to collect in situ judgments.
3.2.4 Unidimensional vs. Multidimensional Judgments. We further compare models using a combination of multiple aspects of judgments (Models 4 , 5 , and 6 ) with those using a single dimension (Models 1 , 2 , and 3 ). Results suggest that it is almost always helpful (enhancing the correlation with most of the six search experience measures signi cantly) to complement either relevance or usefulness with the alternative dimensions.
Models 4 and 5 explain all six dimensions of search experience measures signi cantly be er than Models 1 and 2 , suggesting that multidimensional judgments are almost always helpful for TREC-style relevance judgments (TRel) and post-session usefulness judgments (Usef.p). We also note that in situ usefulness judgments (Usef.i) worked particularly well for correlating with users' satisfaction (Sat) and goal success (Succ), such that combining with more dimensions of judgments adds li le to the model.
Results demonstrate that multidimensional search result judgments are helpful, complementing unidimensional judgments and yielding be er correlation with search experience measures. is also suggests the advantages of multidimensional search result judgments over the in situ one--the former can consistently improve relevance/usefulness to be er correlate with almost all user experience measures, while the la er shows limited advantages.
3.3 Which Dimensions To Judge?
A crucial issue of information retrieval is deciding which criteria to use to rank search results. We come to initial answers by looking into the standardized coe cients () of Model 5 (Table 4) as an example due to its superiority over other models. e standardized coe cient  stands for the magnitude of change in the DV (relative to its standard deviation) caused by one-unit change in the IV (relative to the IV's standard deviation) while other variables being equal. e coe cients of the model indicate how changes in the "quality" of the clicked results will (theoretically) a ect users' search experience in a session. Table 4 suggests that: · To enhance user satisfaction, a search system should present
useful and novel results--both Usef.p (mean) and Nov (max) show signi cant positive e ects on Sat in Model 5 .

409

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 4: Multilevel regression: standardized coe cients () of independent variables for Model 5 ­ Usef.p + others.

Independent

DV: session-level search experience

Variables

Sat Frus Succ S.Eff Help Diff

Gender: Male Age Degree: Graduate SE Expertise

0.10 -0.05 -0.03
0.12

0.19 0.09 0.06 0.00 0.16 0.00 -0.03 -0.02 -0.11 0.00 0.05 0.01 0.13 -0.08 0.23 0.04 0.08 0.01 0.12 -0.00

Product: Factual Goal: Speci c Topic Familiarity

0.02 -0.02 -0.06 -0.09 -0.00 0.02 0.02 -0.07 0.04 0.05 0.07 0.01 0.10 -0.23 0.17 -0.20 0.19 -0.24

# clicks

0.20 -0.12 0.17 -0.07 0.18 -0.01

# queries

-0.36 0.17 -0.25 0.16 -0.35 -0.02

 Usef.p (mean) 0.23 -0.38 0.36 -0.36 0.08 -0.43

 Usef.p (max)  Usef.p (min)

0.16 0.09 0.22 -0.07 0.11 -0.08 0.01 0.19 -0.04 0.19 0.01 0.18

 Nov (max)  Nov (min)

0.24 -0.10 0.18 -0.09 -0.01 -0.20 -0.08 -0.06

0.25 -0.11 0.07 -0.00

 Under.p (max)  Under.p (min)

0.09 -0.27 0.16 -0.08

0.30 -0.15 0.14 -0.26

0.14 -0.22 0.29 -0.27

 Relia.p (max) -0.13 -0.08 0.01 0.05 -0.03 0.06

 Relia.p (min) 0.06 0.01 -0.05 0.08 -0.07 0.04

 Effort (max) -0.12 0.16 0.08 0.28 -0.13 0.02

 Effort (min) Adjusted R2

0.21 0.04 0.12 0.01 0.25 0.02 0.30 0.26 0.42 0.37 0.31 0.26

Light and dark shadings indicate p < 0.05 and 0.01, respectively.

· To reduce user frustration, a search system should o er results that are useful and easy-to-understand--both Usef.p (mean) and Under.p (max) show signi cant negative e ects on Frus.
· To help users successfully reach the goal (Succ), a search system should retrieve useful, novel, and easy-to-understand results-- Usef.p (mean), Nov (max), and Under.p (max) show signi cant positive e ects on Succ.
· To reduce the total e ort of a search session, the system should retrieve easy-to-understand results and avoid those requiring too much e ort--Under.p (min) shows a signi cant negative e ect on S.Eff and Effort (max) shows a positive one.
· To be er help users in a session (enhance the helpfulness of the system), a system should retrieve novel and easy-to-understand results--both Nov (max) and Under.p (max) show signi cant positive e ects on Help.
· To reduce the perceived task di culty, we need to retrieve useful and easy-to-understand results--both Usef.p (mean) and Under.p (min) show signi cant negative e ects on Diff. e coe cients suggest that the mean usefulness of the clicked
results is helpful for explaining all six search experience measures (has statistically signi cant coe cients). In addition, novelty, understandability, and e ort also signi cantly relate to many di erent search experience measures, suggesting they are useful complements to usefulness in search result judgments. In contrast, reliability shows no signi cant e ect on any of the six user experience measures in Model 5 . However, we suspect this is because the top-ranked results returned by Google are mostly reliable ones, which makes reliability a less important judgment measure among the clicked results.

Table 5: Statistics of the absolute di erence of two users' ratings on the same results (||).

Usef.i Effort Nov Relia.i Under.i TRel Usef.p Relia.p Under.p

| | mean (SD) 1.55 (1.45) 1.52 (1.25) 1.60 (1.47) 1.23 (1.21) 1.18 (1.23) 0.63 (0.68) 1.53 (1.54) 1.38 (1.35) 1.08 (1.31)

|| = 0 25.9% 22.9% 26.4% 31.3% 34.8% 48.3% 29.9% 30.8% 38.8%

||  1 58.2% 57.7% 54.2% 67.2% 68.2% 89.1% 60.7% 62.2% 76.6%

||  2 79.1% 76.1% 78.1% 86.1% 88.1% 100.0% 77.6% 81.1% 90.5%

3.4 Variability of Judgments
We further examine the variability of judgments among di erent searchers, because in many practical scenarios we may have to train and evaluate retrieval systems based on relevance judgments made by external assessors. We suspect di erent users may have a greater degree of inconsistencies in their in situ judgments than their post-session ones (due to the contextual nature of the former). However, results do not support this conjecture well.
We examine the absolute di erence of two users' ratings on the same result. Table 5 reports the mean absolute di erence and the distribution. e mean absolute di erence for in situ and postsession usefulness judgments (Usef.i and Usef.p) are very close (1.55 vs. 1.53). e mean absolute di erence of post-session reliability judgments (Relia.p) is slightly higher than that for in situ ones (Relia.i) (1.38 vs. 1.23), but that for post-session understandability judgments (Under.p) is also slightly lower than the in situ ones (Under.i, 1.08 vs. 1.18). Overall, no evidence suggests that either in situ or post-session judgments is more or less consistent than the other across di erent users.
Further, we note that di erent users' reliability and understandability judgments seem more consistent than those for usefulness, e ort, and novelty judgments, regardless of performed in an in situ se ing or a post-session one. is suggests that usefulness, e ort, and novelty judgments may su er from inter-rate consistency by a greater extent, while inter-rate agreement is less likely a concern for understandability and reliability judgments. However, since users judged TRel by a di erent scale, it remains unclear how do the other ve judgments compare with standard TREC relevance judgments in terms of inter-rate consistency.

3.5 Summary
To sum up, this section discloses both opportunities and challenges for future search result judgments. · Opportunity ­ Since a combination of multidimensional judg-
ments explains user experience measures be er than using relevance or usefulness alone, we expect that an appropriate ranking of search results by multiple criteria may potentially yield be er user experience as well. e results in Table 4 also help select ranking criteria according to a targeted user experience measure. · Challenge ­ Extending current judgments from a single dimension to multiple aspects largely increases the cost of judgments.
is is a crucial issue for the scalability of multidimensional judgments. e following sections address this concern by predicting multidimensional judgments using implicit feedback techniques.

410

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 6: Implicit feedback features and their correlation with di erent search result quality measures.

Pearson's r with search result judgments

Click Dwell Time Features

Note

TRel Usef.p Nov Effort Under.p Relia.p

T1 Click dwell time (log).

0.38

0.43 0.41

0.36

0.12

0.34

T2 T3 T4 T5

(t - µ )/ . t is the result's dwell time; µ is average click dwell time;  is the standard deviation of click dwell time. T3-5 are based on personalized versions of µ and  .

all clicks by user by task by length

0.31

0.34 0.30

0.32

0.31

0.36 0.38

0.32

0.31

0.35 0.30

0.32

0.29

0.33 0.29

0.31

0.06 0.09 0.06 0.06

0.24 0.24 0.24 0.24

Follow-up ery Features

Q1

Q2

e number of terms in the next query found in the URL/title/body

Q3 of the result.

URL title body

TRel
-0.04 -0.03 -0.03

Usef.p
0.03 -0.00 -0.03

Nov
-0.01 -0.00
0.10

Effort
-0.02 0.01 0.06

Under.p
-0.02 -0.00
0.03

Relia.p
0.03 -0.02 -0.02

Q4

e percentage of terms in the next query found in the

Q5 URL/title/body of the result.

Q6

URL title body

0.02 0.09 0.02 -0.04

0.01

0.08

0.07 0.10 0.06 -0.00

0.05

0.07

0.17

0.18 0.21

0.04

0.13

0.18

Q7

e number of newly added query terms in the next query refor-

Q8 mulation found in the URL/title/body of the result.

Q9

URL title body

0.03 -0.01 -0.03 -0.06 0.07 0.04 0.00 -0.07 0.07 0.07 0.13 -0.04

0.02

0.02

0.01 -0.00

0.04 -0.02

Q10

e number of removed query terms in the next query reformula-

Q11 tion found in the URL/title/body of the result.

Q12

URL title body

0.01 0.01 -0.00 -0.07 -0.04 -0.12

0.06 0.09 0.06 -0.09

0.03 -0.06

0.08

0.07 0.06

0.01

-0.09

-0.04

Q13

e mean/max/min log likelihood scores between the full content

Q14 of the result and follow-up queries.

Q15

mean max min

0.22 0.23 0.22 -0.03

0.19

0.23

0.22 0.23 0.21 -0.03

0.17

0.18

0.15 0.19 0.19 -0.01

0.17

0.19

Follow-up Click Features

TRel Usef.p Nov Effort Under.p Relia.p

C1

e mean/max/min similarity between the title of the result and

C2 the titles of clicked results in follow-up searches.

C3

mean max min

0.04

0.05 0.12

0.02

0.04

0.01

0.05

0.04 0.09

0.00

0.06

0.00

0.06

0.08 0.10

0.01

-0.01

0.03

C4

e mean/max/min similarity between the snippet of the result

C5 and the snippets of clicked results in follow-up searches.

C6

mean max min

-0.00 -0.04
0.08

0.01 -0.06
0.11

0.01 -0.06
0.10

0.04 -0.05
0.07

0.02

0.03

0.01 -0.04

0.06

0.09

C7

e mean/max/min similarity between the full content of the result

C8 and the full contents of SAT clicks (dwell time > 30s) in follow-up

C9 searches.

C10

e mean/max/min similarity between the title of the result and

C11 the titles of skipped results in follow-up searches.

C12

mean max min mean max min

0.19

0.23 0.09

0.01

-0.02

0.10

0.20 0.20 0.05 -0.00

0.00

0.08

0.12 0.17 0.07 -0.00 -0.01

0.07

0.09

0.11 0.11

0.02

0.05

0.05

0.11 0.09 0.06 -0.01

0.05

0.02

0.09 0.13 0.13 -0.05

0.00

0.05

C13

e mean/max/min similarity between the snippet of the result

C14 and the snippets of skipped results in follow-up searches.

C15

mean max min

0.01

0.03 0.03

0.11

-0.05

0.03

0.02 0.01 -0.01 -0.00

0.02 -0.02

0.10

0.12 0.12

0.13

-0.05

0.11

Light and dark shadings indicate the correlation is signi cant at 0.05 and 0.01 levels, respectively.

4 PREDICTION
is section introduces our techniques for predicting multidimensional judgments of clicked results from search logs. We model the prediction task as a regression problem--the input is features related to a target click, the output is the predicted judgment score of the clicked result. We use gradient boosted regression trees (GBRT) for prediction. Table 6 lists the prediction features. Due to the limited space, we only report results for predicting TRel and the judgments included in Model 5 --Usef.p, Nov, Effort, Under.p, and Relia.p. However, the described approach can also e ectively predict other search result judgments as well.
4.1 Click Dwell Time Features
Click dwell time (T1) is one of the most widely used implicit feedback measure. As Table 6 shows, T1 does not correlate much with understandability, but it still has 0.3­0.4 correlations (signi cant at 0.01 level) with other measures.

T2­T5 measure the deviation of a click's dwell time from the mean dwell time (µ) of a group of clicks (normalized by the standard deviation  ). T2 computes µ and  based on all clicks in the training sets. T3 is based on clicks by the same user. T4 is based on clicks in sessions with the same task type. T5 is based on clicks on documents with similar length (we divide the clicked results into ten bins by length and compute µ and  of a click based on its bin).
4.2 Follow-up ery Features
Follow-up query features are based on the intuition that a clicked result may in uence follow-up query reformulation in a session.
us, we can infer the quality of a click from queries issued a er the clicked result in the same session.
Q1­Q6 match the terms in the immediate follow-up query with the target click. Q7­Q12 match the newly added and removed terms in the immediate follow-up query reformulation with the target click. Q13­Q15 match the target click with all follow-up queries.

411

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Many of the follow-up query features (such as Q6 and Q13­Q15) have signi cant correlations with the search result quality measures, con rming that the intuition is reasonable. We also note that Q6 and Q13­Q15 have stronger correlations with understandability than click dwell time features.
4.3 Follow-up Click Features
Similar to follow-up query features, we may also infer the quality of a target click based on follow-up clicks in a session.
C1­C6 measure the similarity between the target click and followup clicks. C7­C9 measure the similarity with follow-up satisfactory (SAT) clicks. C10­C15 measure the similarity with follow-up skipped results (unclicked results ranked higher than a clicked result). Some features have signi cant correlations with the search result quality measures, suggesting they may be useful predictors.
4.4 Prior-to-click Features (Baseline)
Prior-to-click features include the existing techniques that predict search result quality measures using information available before users clicking on the result. In this paper, they serve as the baseline for the implicit feedback features. We include a full list of prior-toclick features in an online appendix1.
We incorporate di erent prior-to-click features for predicting di erent measures. e shared features for all six measures include the rank of the result by Google search, ad hoc search models (QL, BM25, DFR [3], and SDM [38]), and session search models [14, 47].
e unique features for predicting each measure are: · TRel ­ a subset of LETOR features [34]. · Usef.p ­ a subset of LETOR features [34] and a subset of the
usefulness features by Mao et al. [36] that do not rely on postclick information. · Nov ­ the similarity of the click with previous clicks and higher ranked results in the same SERP (motivated by previous work on novelty-based search result diversi cation [6, 43, 44, 55]). · Effort ­ Yilmaz et al. [54] and Verma et al. [50]. · Under.p ­ Palo i et al. [41, 42]. · Relia.p ­ Olteanu et al. [39] and Wawer et al. [51]. Our prior-to-click features are representatives of the state-of-theart techniques for predicting each dimension of judgments without using implicit feedback. However, we did not include features that we do not have the resource to calculate, which include link structure based features and social media popularity features such as Twi er mention. Note this may reduce the e ectiveness of predicting reliability since the excluded features take about 1/3 of the features by Olteanu et al. [39] and Wawer et al. [51].
5 EVALUATION
5.1 Experiment Settings
We evaluate prediction (regression) by the Pearson's correlation between the predicted values and actual judgments (prediction correlation) and the root mean square error (RMSE) of the predicted values. Note that the RMSE for predicting di erent measures is not comparable-- rst, TREC relevance ranges from 0­3 while others from 1­7; second, their distributions vary a lot. Here we only report
1 h p://ciir.cs.umass.edu/downloads/mdrel/

prediction correlation for its easy interpretability. e results of RMSE is highly consistent with that using prediction correlation.
e dataset for evaluation includes multidimensional judgments on the 727 unique clicked results. We use 10-fold cross validation for evaluation (using eight folds for training, one for validation, and one for testing). We randomly shu e the dataset 10 times and apply 10-fold cross-validation for each random shu ing of the whole dataset--this generates prediction results on 10 × 10 = 100 test folds in total (note that we are not using a 100-fold cross validation). We report the mean and standard deviation (SD) of prediction correlation on the test folds. We note that the prediction correlation reported in this section is di erent from and cannot be compared with the correlation in Table 6, which are computed for the whole dataset without cross validation.
5.2 Click Dwell Time Features
Current techniques for inferring search result quality from logs rely on click dwell time. Results ( 1 in Table 7) suggest the click dwell time features work reasonably well for predicting usefulness, novelty, and e ort, but they have di culties inferring the understandability and reliability of results.
e click dwell time features ( 1 ) are e ective predictors for usefulness, novelty, and e ort. For these three measures, the predicted values have about 0.3­0.4 mean Pearson's correlation with the actual judgments, which is comparable to that for predicting TREC relevance (mean r = 0.35). However, the click dwell time features perform much worse for predicting understandability and reliability. On average the predicted and actual judgments have only 0.10 and 0.22 correlation, suggesting it is necessary to incorporate new implicit feedback signals.
5.3 Follow-up ery and Click Features
We extend click dwell time to include signals from follow-up search activities. Results suggest the new features are helpful.
e follow-up query ( 2 ) and click features ( 3 ) alone have limited prediction capability. However, combining them with the click dwell time features ( 4 ) consistently produces be er prediction than using click dwell time features alone ( 1 ): except for e ort, the prediction correlation for the other ve measures using feature set 4 is signi cantly be er than that for click dwell time features ( 1 ). is indicates that follow-up queries and clicks indeed provide useful implicit feedback that are complementary to click dwell time.
e follow-up query and click features are particularly helpful for predicting reliability. Combining them with the click dwell time features improves the mean correlation of prediction from 0.22 to 0.36. e new features are also helpful for predicting TREC relevance and usefulness as well. is partly con rms our intuition--the quality of a clicked result may in uence follow-up search activities, making it possible to infer the quality of a clicked result based on what happened a erward in the session.
e new features also improved the mean prediction correlation for understandability from 0.10 to 0.20. However, we note the combination of all implicit feedback features still does not work well for predicting understandability (mean r = 0.20). is suggests that, compared with other judgments, it is more challenging to predict understandability based on the implicit feedback information.

412

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 7: e e ectiveness of di erent features for predicting multidimensional search result judgments.

Mean (SD) Pearson's r between true and predicted judgments over the test folds

Features

TRel

Usef.p

Nov

Effort

Under.p

Relia.p

1 Click Dwell Time

0.35 (0.11)

0.40 (0.11)

0.42 (0.11)

0.31 (0.10)

0.10 (0.14)

0.22 (0.13)

2 Follow-up ery

0.19 (0.11)

0.17 (0.14)

0.13 (0.13)

0.12 (0.11)

0.14 (0.12)

0.19 (0.12)

3 Follow-up Click

0.15 (0.12)

0.20 (0.11)

0.11 (0.12)

0.14 (0.11)

0.11 (0.12)

0.17 (0.12)

4 All ( 1 + 2 + 3 )

0.39 (0.09)

0.46 (0.08)

0.45 (0.09)

0.33 (0.11)

0.20 (0.13)

0.36 (0.12)

1 vs. 4

**

**

*

**

**

5 Prior-to-click

0.36 (0.10)

0.29 (0.10)

0.28 (0.11)

0.13 (0.12)

0.20 (0.14)

0.18 (0.13)

4 vs. 5

**

**

**

**

**

6 All+Prior-to-click

0.45 (0.08)

0.49 (0.09)

0.47 (0.09)

0.39 (0.09)

0.26 (0.12)

0.40 (0.11)

5 vs. 6

**

**

**

**

**

**

* and ** indicate the di erence is statistically signi cant at 0.05 and 0.01 levels by two-tail paired t -test.

5.4 Comparing to Prior-to-click Features
An important application of implicit feedback techniques is to infer relevance labels from search logs. Aggregating inferred relevance labels or implicit feedback signals from past search logs may help rank search results in the future [2]. We examine whether or not implicit feedback techniques can serve a similar purpose for multidimensional judgments.
e combination of the implicit feedback features and the priorto-click features ( 6 ) generated signi cantly be er prediction results on all the six judgments than using the prior-to-click features alone ( 5 ). is suggests that the implicit feedback features are indeed helpful and complementary to the prior-to-click features for predicting these judgments. We also note that the improvements in mean prediction correlation can be as large as over 0.2 (such as for predicting reliability and e ort). However, even combining the two sets of features still cannot adequately predict understandability (mean r = 0.26).
6 DISCUSSION AND CONCLUSION
A crucial issue of information retrieval is deciding which criteria to use to rank search results. We compared two seemingly reasonable directions for improving current TREC-style relevance judgments. One direction is to collect in situ search result judgments. e other one is to complement a single dimension of judgments (such as relevance or usefulness) by combining with other aspects. We found that the la er direction seems more e ective and versatile-- using a combination of di erent dimensions of judgments, we can almost always improve correlation with user experience measures.
We envision future search engines should rank results by multiple aspects. We also o ered initial suggestions on which criteria to adopt and when to adopt them. We further examined and improved implicit feedback techniques for predicting multiple judgments, addressing the scalability concern of applying multidimensional judgments in real web search applications.
Our study makes the following contributions: · We evaluated and compared in situ usefulness judgments with
regular relevance/usefulness judgments by searchers. We show that using usefulness as the judgment criteria is fruitful, but in situ judgments do not show clear bene ts over regular ones. · We evaluate multidimensional search result judgments considering four alternative aspects other than relevance/usefulness. We show that multidimensional judgments be er correlate with user

experience measures than using relevance/usefulness judgments alone. We also note that multidimensional judgments is a be er direction for improving TREC-style relevance judgments. · Our study also discloses the connections between di erent user experience measures and various dimensions of search result judgments. is o ers practical suggestions for system design, such as the appropriate dimensions to judge search results for the purpose of improving a particular user experience measure. · We successfully generalize implicit feedback signals to include follow-up searches and clicks in a search session to help click dwell time be er predict multidimensional judgments. To the best of our knowledge, we are also the rst to examine the e ectiveness of implicit feedback approaches for predicting novelty, understandability, reliability, and e ort.
Our work also sheds lights on a few critical areas for exploration in the future:
An important line of future work is to provide more accurate criteria for search result ranking and evaluation. Based on a regression analysis, we have already o ered initial suggestions on what criteria to use and when to use them, as discussed in Section 3.3. We note that, with a su ciently large dataset, one can possibly learn a prediction model for search experience measures by taking multidimensional judgments of results as input. Such a model can further address issues such as what are the proper weights to put on di erent aspects when ranking search results. It may also solve the discrepancy between o ine evaluation measures and user experience measures, and ultimately serve as a be er objective function for training ranking models.
Another important application is to perform multidimensional ranking of search results based on implicit feedback signals and other information. We have already demonstrated that implicit feedback approaches can infer judgments of usefulness, novelty, e ort, and reliability with reasonable accuracy comparing to those for relevance labels. Aggregating such inferred judgments from past search logs may serve as useful features for performing multidimensional search result ranking in the future. However, we also note that our current technique needs to be improved to be er infer understandability of results from search logs.
We do admit certain limitations in our current study. First, our analysis and experiments are solely based on data collected from one laboratory user study, which is limited in both scale and representativeness. We suggest that further studies employ larger

413

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

datasets to verify our ndings. Second, it is worth noting that our way of collecting in situ judgments in uenced users' natural search behaviors. We observed in our log that users spent on average 12.1 seconds to nish the in situ judgments. us some particular user behavior pa erns may vary when applied to another scenario (without interrupting users for in situ judgments). ird, we also note that we only collected search result judgments for the clicked results, while it remains unclear to which extent the ndings can be generalized to the unclicked ones. Last but not least, the collected post-session judgments are more or less in uenced by the search session and the in situ judgments (although we meant to collect context-independent judgments such as to compare with contextual ones). It is also worth noting that our post-session judgments are not fully representative of the existing TREC-style approach.
7 ACKNOWLEDGMENT
is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.
REFERENCES
[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: A game for modeling di erent types of web search success using interaction data. In SIGIR '11, pages 345­354, 2011.
[2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR '06, pages 19­26, 2006.
[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.
[4] J. Arguello. Predicting search task di culty. In ECIR '14, pages 88­99, 2014. [5] N. J. Belkin, M. J. Cole, and J. Liu. A model for evaluation of interactive infor-
mation retrieval. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [6] J. Carbonell and J. Goldstein. e use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR '98, pages 335­336, 1998. [7] B. Cartere e, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: e TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016. [8] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ cher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, pages 659­666, 2008. [9] C. W. Cleverdon. e evaluation of systems used in information retrieval. In Proceedings of the International Conference on Scienti c Information, pages 687­ 698, 1959. [10] K. Collins- ompson, C. Macdonald, P. Benne , F. Diaz, and E. Voorhees. TREC 2014 web track overview. In TREC 2014, 2014. [11] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR '11, pages 95­104, 2011. [12] H. A. Feild and J. Allan. Modeling searcher frustration. In HCIR '09, pages 5­8, 2009. [13] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR '10, pages 34­41, 2010. [14] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13, pages 453­462, 2013. [15] J. Gwizdka. Revisiting search task di culty: Behavioral and individual di erence measures. In ASIS&T '08, 2008. [16] P. Hansen and J. Karlgren. E ects of foreign language and task scenario on relevance assessment. J. Doc., 61(5):623­639, 2005. [17] A. Hassan. A semi-supervised approach to modeling web search satisfaction. In SIGIR '12, pages 275­284, 2012. [18] A. Hassan, R. Jones, and K. L. Klinkner. Beyond DCG: User behavior as a predictor of a successful search. In WSDM '10, pages 221­230, 2010. [19] R. Hu and P. Pu. A study on user perception of personality-based recommender systems. In UMAP '10, pages 291­302, 2010. [20] J. Jiang and J. Allan. Adaptive e ort for search evaluation metrics. In ECIR '16, pages 187­199, 2016.

[21] J. Jiang, A. Hassan Awadallah, X. Shi, and R. W. White. Understanding and predicting graded search satisfaction. In WSDM '15, pages 57­66, 2015.
[22] J. Jiang, D. He, and J. Allan. Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. In SIGIR '14, pages 607­616, 2014.
[23] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017.
[24] D. Kelly, J. Arguello, A. Edwards, and W.-c. Wu. Development and evaluation of search tasks for IIR experiments using a cognitive complexity framework. In ICTIR '15, pages 101­110, 2015.
[25] J. Y. Kim, J. Teevan, and N. Craswell. Explicit in situ user feedback for web search results. In SIGIR '16, pages 829­832, 2016.
[26] J. Kiseleva, E. Crestan, R. Brigo, and R. Di el. Modelling and detecting changes in user satisfaction. In CIKM '14, pages 1449­1458, 2014.
[27] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, H. Soncu, and C. Newell. Explaining the user experience of recommender systems. User Modeling and User-Adapted Interaction, 22(4-5):441­504, 2012.
[28] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, 2008.
[29] C. Liu, J. Liu, and N. J. Belkin. Predicting search task di culty at di erent search stages. In CIKM '14, pages 569­578, 2014.
[30] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task di culty for di erent task types. In ASIS&T '10, 2010.
[31] J. Liu, C. Liu, M. Cole, N. J. Belkin, and X. Zhang. Exploring and predicting search task di culty. In CIKM '12, pages 1313­1322, 2012.
[32] J. Liu, C. Liu, J. Gwizdka, and N. J. Belkin. Can search systems detect users' task di culty?: Some behavioral signals. In SIGIR '10, pages 845­846, 2010.
[33] J. Liu, C. Liu, X. Yuan, and N. J. Belkin. Understanding searchers' perception of task di culty: Relationships with task type. In ASIS&T '11, 2011.
[34] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 workshop on learning to rank for information retrieval, pages 3­10, 2007.
[35] Y. Liu, Y. Chen, J. Tang, J. Sun, M. Zhang, S. Ma, and X. Zhu. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In SIGIR '15, pages 493­502, 2015.
[36] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016.
[37] S. Menard. Applied Logistic Regression Analysis. Sage, 1997. [38] D. Metzler and W. B. Cro . A markov random eld model for term dependencies.
In SIGIR '05, pages 472­479, 2005. [39] A. Olteanu, S. Peshterliev, X. Liu, and K. Aberer. Web credibility: Features
exploration and credibility prediction. In ECIR '13, pages 557­568, 2013. [40] P. Over. e TREC interactive track: An annotated bibliography. Inf. Process.
Manage., 37(3):369­381, 2001. [41] J. Palo i, L. Goeuriot, G. Zuccon, and A. Hanbury. Ranking health web pages
with relevance and understandability. In SIGIR '16, pages 965­968, 2016. [42] J. Palo i, G. Zuccon, and A. Hanbury. e in uence of pre-processing on the
estimation of readability of web documents. In CIKM '15, pages 1763­1766, 2015. [43] D. Ra ei, K. Bharat, and A. Shukla. Diversifying web search results. In WWW
'10, pages 781­790, 2010. [44] R. L. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result
diversi cation. Inf. Retr., 15(5):478­502, 2012. [45] A. Schuth, K. Hofmann, and F. Radlinski. Predicting search satisfaction metrics
with interleaved comparisons. In SIGIR '15, pages 463­472, 2015. [46] J. Schwarz and M. Morris. Augmenting web pages and search results to support
credibility assessment. In CHI '11, pages 1245­1254, 2011. [47] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using
implicit feedback. In SIGIR '05, pages 43­50, 2005. [48] R. Tang, W. M. Shaw, Jr., and J. L. Vevea. Towards the identi cation of the optimal
number of relevance categories. J. Am. Soc. Inf. Sci., 50(3):254­264, 1999. [49] J. van Doorn, D. Odijk, D. M. Roijers, and M. de Rijke. Balancing relevance
criteria through multi-objective optimization. In SIGIR '16, pages 769­772, 2016. [50] M. Verma, E. Yilmaz, and N. Craswell. On obtaining e ort based judgements for
information retrieval. In WSDM '16, pages 277­286, 2016. [51] A. Wawer, R. Nielek, and A. Wierzbicki. Predicting webpage credibility using
linguistic features. In WWW '14 Companion, pages 1135­1140, 2014. [52] Y. Xu and Z. Chen. Relevance judgment: What do information users consider
beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57(7):961­973, 2006. [53] Y. Yamamoto and K. Tanaka. Enhancing credibility judgment of web search
results. In CHI '11, pages 1235­1244, 2011. [54] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and
e ort: An analysis of document utility. In CIKM '14, pages 91­100, 2014. [55] C. X. Zhai, W. W. Cohen, and J. La erty. Beyond independent relevance: Methods
and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10­17, 2003. [56] G. Zuccon. Understandability biased evaluation for information retrieval. In
ECIR '16, pages 280­292, 2016.

414


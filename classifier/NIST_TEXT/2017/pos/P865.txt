Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

An Extended Relevance Model for Session Search

Nir Levine
Technion - Israel Institute of Technology
Haifa, Israel 32000 levin.nir1@gmail.com

Haggai Roitman
IBM Research - Haifa Haifa, Israel 31905 haggai@il.ibm.com

Doron Cohen
IBM Research - Haifa Haifa, Israel 31905 doronc@il.ibm.com

ABSTRACT
e session search task aims at best serving the user's information need given her previous search behavior during the session. We propose an extended relevance model that captures the user's dynamic information need in the session. Our relevance modelling approach is directly driven by the user's query reformulation (change) decisions and the estimate of how much the user's search behavior a ects such decisions. Overall, we demonstrate that, the proposed approach signi cantly boosts session search performance.
1 INTRODUCTION
We propose an extended relevance model for session search. Relevance models aim at identifying terms (words, concepts, etc) that are relevant to a given (user's) information need [5]. Within a session, user's information need, expressed as a sequence of one or more queries [1], may evolve over time. User's search behavior during the session may be utilized as an additional relevance feedback source by the underlying search system [1]. Given user's session history (i.e., previous queries, result impressions and clicks), the goal of the session search task is to best serve the user's newly submi ed query in the session [1].
We derive a relevance model that aims at "tracking" the user's dynamic information need by observing the user's search behavior so far during the session. To this end, the proposed relevance model is driven by the user's query reformulation decisions. Our relevance modelling approach relies on previous studies that suggest that user query change decisions may (at least partially) be explained by the previous user search behavior in the session [4, 9, 12]. We utilize the derived relevance model for re-ranking the search results that are retrieved for the current user information need in the session. Overall, we demonstrate that, our relevance modeling approach can signi cantly boost session search performance compared to many other alternatives that also utilize session data.
2 RELATED WORK
Few previous works have also utilized the session context (i.e., previous queries, retrieved results and clicks) as an implicit feedback
Work was done during a summer internship in IBM Research - Haifa.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080664

source for re ning the user's query [3, 8, 10, 11]. To this end, the query language model was either combined with the language models of previous queries [11] or retrieved (clicked) results [8, 10]. In addition, di erent query score aggregation strategies for session search were explored [3]. Yet, none of these previous works have actually considered the user's query change process itself as a possible implicit feedback source.
Several recent works have studied various query reformulation (change) behaviors during search sessions [4, 9, 12]. Among the various features that were studied, word-level features were found to best explain the changes in user queries during search sessions [4, 9]. A notable feature was found to be the occurrence of query (changed) words in the contents of results that the user previously viewed or clicked [4, 9, 12].
Few previous works have also utilized query change for the session search task (e.g., [6, 12]). Common to such works is the modeling of user queries and their change as states and actions within various Reinforcement Learning inspired query weighting and aggregation schemes [7]; In this work we take a rather more "traditional" approach, inspired by the relevance model framework [5].
3 APPROACH
3.1 Session model
Session search is a multi-step process, where at each step t, the user may submit a new query qt . e search system then retrieves the top-k documents Dq[kt] from a given corpus D that best match the user's query1. e user may then examine the results list; each result usually includes a link to the actual content and is accompanied with a summary snippet. e user may also decide to click on one or more of the results in the list in order to examine their actual content. Let Cqt denote the corresponding set of clicked results in Dq[kt]. In case the user decides to continue and submits a subsequent query, step t ends and a new step t + 1 begins. Let Sn-1 represent the session history (i.e., user queries, retrieved result documents, and clicked results) that was "recorded" prior to the current (latest) submi ed user query qn . On each step 1  t  n - 1, the session history is represented by a tuple St = Qt , Dt , Ct . Qt = (q1, q2, . . . , qt ) is the sequence of queries submi ed by the user up to step t . Dt = (Dq[k1], Dq[k2], . . . , Dq[kt]) is the corresponding sequence of (top-k) retrieved result lists. Ct = (Cq1 , Cq2 , . . . , Cqt ) further represents the corresponding sequence of user clicks.
1With k = 10 in the TREC session track benchmarks [1] that we use later on in our evaluation.

865

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

3.2 Information need dynamics

e session search task is to best answer the current user's query

qn while considering Sn-1 [1]. Let I denote the user's (hidden) information need in the session. e goal of our relevance mod-

elling approach is, therefore, to be er capture the user's informa-

tion need I which may evolve during the session. In order to cap-

ture such dynamics, let It further represent the user's information

need at step t. We now assume that, It depends both on the previ-

ous (dynamic) information need It-1 prior to query qt submission

and the possible change in such need It

def
=

It -1  It ; It

is

assumed be to implied by the change the user has made to her pre-

vious query qt -1 to obtain query qt .

3.3 ery change as relevance feedback
We utilize the user's query reformulation (change) process during the session as an implicit relevance feedback for estimating the change in the user's information need It . As been suggested by previous works [4, 9, 12], user's query changed terms may actually occur in the contents of previously viewed (clicked) search results in St-1. is, therefore, may (partially) explain how the user decided to reformulate her query from qt-1 to qt [4, 9, 12]. Our proposed relevance model aims at exploiting such query changed term occurrences within the contents of previously viewed (clicked) results so as to discover those terms w (over some vocabulary V ) that are the most relevant to the current user's information need In . As a consequence, such terms may be used for query expansion aiming to be er serve the current user's information need In .
Given query qt , compared to the previous query qt-1, there can be three main query change types, namely term retention, addition and removal [4, 9, 12]. User term retention, given by the set of terms that appear in both query qt and qt-1 and denoted qt, usually represent the (general) thematic aspects of the user's information need [4, 9, 12]. Added terms (denoted qt+) are those terms that the user added to query qt-1 to obtain query qt . A user may add new related terms that were encountered in previous results so as to improve the chance of nding relevant content [4]. On the other hand, a user may remove terms from a previous query qt-1 (further denoted qt-) in order to terminate a subtask or trying to improve bad performing queries [4].

3.4 Relevance model derivation
Similar to previous works on relevance models [5], our goal is to discover those terms w ( V ) that are the most relevant to the user's information need In ; To this end, given the user's current query qn and session history Sn-1, let Sn denote our estimate of the relevance (language) model. On each step 1  t  n, such estimation is given by the following rst-order autoregressive model:

p(w

|

St

)

def
=

t p(w | St-1 ) + (1 - t )p(w | Ft ),

(1)

where  Ft now denotes the feedback model which depends on the user's (reformulated) query qt . While St-1 estimates the dynamic information need prior to step t (i.e., It-1),  Ft captures the relative change in such need at step t (i.e., It ).

t further controls the relative importance we assign to model

exploitation (i.e., St-1 ) versus model exploration (i.e.,  Ft ). t parameter is dynamically determined based on the relevance model's

self-clarity at step t [2]. Self-clarity estimates how much the prior

model St-1 already "covers" the feedback model  Ft ; formally:

t

def
=



· exp-DK L (Ft

St -1 ),

(2)

where   [0, 1] and DK L( Ft St-1 ) is the Kullback-Leibler divergence between the two (un-smoothed) language models [13].

Finally, given qn , the current user's query in the session, we de-

rive the relevance model Sn by inductively applying Eq. 1 (with

 S0

def
=

0). We next derive the feedback model  Ft .

3.5 Feedback model derivation

Our estimate of  Ft aims at discovering those terms (in qt , qt -1 or others in V ) that are most relevant to the change in user's dynamic

information need from It-1 to It (i.e., It ). Given queries qt and qt-1, we rst classify their occurring terms w  according to their role in the query change. Let qt further denote the set of terms w  that are classi ed to the same type of query change (i.e., qt  {qt , qt +, qt -}).
Our relevance model now relies on the fact that query changed

terms may also occur within the contents of results that were pre-

viously viewed (or clicked) by the user [4, 9, 12]. erefore, on

each step t, let Ft denote the set of results that are used for (implicit) relevance feedback. We determine the set of results to be

included in Ft as follows. If up to step t < n there is at least one clicked result, then we assign Ft = 1j t Cqj . Otherwise, we rst de ne a pseudo information need Qt. Qt represents a (crude) estimate of the user's (dynamic) information need up to step t and

is obtained by concatenating the text of all observed queries in Qt (with each query having the same importance, following [11]). We

then de ne Ft as the set of top-m results in 1j t Dqj with the

highest query-likelihood given Qt (representing pseudo-clicks). Let

p[µ

](w

|x

)

def
=

t

f

(w,

x

)+µ

t

f

(w , D) |D|

|x |+µ

now denote the Dirichlet smoothed

language model of text x with parameter µ [13]. Inspired by the

RM1 relevance model [5], we estimate  Ft as follows:

def
p(w | Ft ) =

p[0](w |d ) ·

p(d |qt )p(qt ) , (3)

d Ft

qt

where p(qt ) denotes the (prior) likelihood that, while reformulating query qt-1 into qt , the user will choose to either add (i.e., qt +), remove (i.e., qt -) or retain (i.e., qt ) terms. Such likelihood can be pre-estimated [9] (i.e., parameterized); e.g., similarly

to the QCM approach [12]. Yet, for simplicity, in this work we as-

sume that every user query change action has the same odds (i.e.,

p(qt )

=

1 3

).

Please note that, the main di

erence between our

estimate of  Ft and the RM1 model is in the way the later scores documents in Ft . Such score in RM1 is based on a given query

qt [5], with no further distinction between the role that each query

term plays or the fact that some of the terms are actually removed

terms that appeared in the previous query qt-1. Similar to RM1,

we

further

estimate p(d |qt )



d

p(qt |d ) p(qt |d
Ft



)

.

866

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

User added or retained terms are those terms that are preferred
to be included in the feedback documents Ft . On the other hand, removed terms are those terms that should not appear in Ft [4]. In accordance, we de ne:

p(qt

|d

)

def
=

 

p[µ](w |d ),
w qt
1 - w qt- p[0](w |d ),

qt  {qt, qt +} qt = qt - (4)

In order to avoid query dri , on each step t, we further anchor

the feedback model  Ft to the query model qt [5] as follows:

p(w

|

 Ft

)

def
=

(1 - t )p[0](w |qt ) + t p(w | Ft ),

(5)

where t

def
=

 · sim(qt , qn ) is a dynamic query anchoring

parameter,   [0, 1] and sim(qt , qn ) is calculated using the (idf-

boosted) Generalized-Jaccard similarity measure; i.e.:

min (t f (w, qt ), t f (w, qn )) · id f (w)

s i m (qt

,

qn

)

def
=

w qt qn
max (t f (w, qt ), t f (w, qn )) · id f (w)

(6)

w qt qn

According to t de nition, the similar query qt is to the current query qn , the more relevant is the query change in user's information need It (modelled by  Ft ) is assumed to be to the current user's information need In; erefore, less query anchoring e ect is assumed to be needed using query qt .

4 EVALUATION 4.1 Datasets

2011

2012

2013

(train)

(test)

(test)

Sessions

Sessions

76

98

87

eries/session

3.7±1.8

3.0±1.6

5.1±3.6

Topics

Sessions/topic

1.2±0.5

2.0±1.0

2.2±1.0

Judged docs/topic 313±115

372±163

268±117

Collection

Name

ClueWeb09B ClueWeb09B ClueWeb12B

#documents

28,810,564 28,810,564 15,700,650

Table 1: TREC session track benchmarks

Our evaluation is based on the TREC 2011-2013 session tracks [1] (see benchmarks details in Table 1). e Category B subsets of the ClueWeb09 (2011-2012 tracks) and ClueWeb12 (2013 track) collections were used. Each collection has nearly 50M documents. Documents with spam score below 70 were ltered out. Documents were indexed and searched using the Apache Solr2 search engine. Documents and queries were processed using Solr's English text analysis (i.e., tokenization, Poter stemming, stopwords, etc).
2h p://lucene.apache.org/solr/

4.2 Baselines
We compared our proposed relevance modelling approach (hereina er denoted SRM3) with several di erent types of baselines.
is includes state-of-the-art language modeling methods that utilize session context data (i.e., previous queries, viewed or clicked results); namely FixedInt [8] (with  = 0.1,  = 1.0 following [8]) and its Bayesian extension BayesInt [8] (with µ = 0.2,  = 5.0, following [8]) ­ both methods combine the query qn model with the history queries Qn-1 and clicks Cn-1 centroid models; BatchUp [8] (with µ = 2.0,  = 15.0, following [8]) which iteratively interpolates the language model of clicks that occur up to each step t using a batched approach; and the Expectation Maximization (EM) based approach [10] (hereina er denoted LongTEM with q = 0, C = 20 and N C = 1, following [10]), which rst interpolates each query qt model with its corresponding session history model (based on both clicked (C) and non-clicked (NC) results in Dq[kt]); the (locally) interpolated query models are then combined based on their relevant session history using the EM-algorithm [10].
Next, we implemented two versions of the Relevance Model [5]. e rst is the basic RM3 model, denoted RM3(qn ), learned using the last query qn and the top-m retrieved documents as pseudo relevance feedback. e second, denoted RM3(Qn ), uses the pseudo information need Qn (see Section 3.5) instead of qn . We also implemented two query aggregation methods, namely: QA(uniform) which is equivalent to submi ing Qn as the query [11]; the second, denoted QA(decay), further applies an exponential decay approach to prefer recent queries to earlier ones (with decay parameter  = 0.92, following [3, 12]). We further implemented three versions of the ery Change Model (QCM) ­ an MDP-inspired query weighting and aggregation approach [12]. Following [12] recommendation, QCM's parameters were set as follows  = 2.2,  = 1.8,  = 0.07,  = 0.4 and  = 0.92. e three QCM versions are the basic QCM approach [12]; QCM(SAT) which utilizes only "satis ed" clicks (i.e., clicks whose dwell-time is at least 30 seconds [12]); and QCM(DUP) which ignores duplicate session queries [12]. Finally, in order to evaluate the relative e ect of the query-change driven feedback model (i.e.,  Ft ), we implemented a variant of SRM by replacing the query-change driven score of Eq. 3 with the RM1 document score (i.e., p(d |qn )). Let SRM(QC) and SRM(RM1) further denote the query-change and "RM1- avoured" variants of SRM, respectively. It is important to note that, SRM(RM1) still relies on the dynamic relevance model updating formula (see Eq. 1) and the dynamic coe cients t and t ­ both further depend on the session dynamics (captured by St-1 and  Ft ).
4.3 Setup
Our evaluation is equivalent to the TREC 2011-2012 RL4 and TREC 2013 RL2 sub-tasks [1]. To this end, given each session's (last) query qn, we rst retrieved the top-2000 documents with the highest query likelihood (QL) score4 to qn . Documents were then reranked using the various baselines by multiplying their (initial)
3Stands for "Session-Relevance Model". 4For this we used Solr's LMSimilarity with Dirchlet smoothing parameter µ = 2500 which is similar to Indri's default parameter.

867

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

TREC 2012

TREC 2013

Method Initial retrieval FixInt [8] BayesInt [8] BatchUp [8] LongTEM [10] RM3(qn ) [5] RM3(Qn ) QA(uniform) [11] QA(decay) [3] QCM [12] QCM(SAT) [12] QCM(DUP) [12] SRM(RM1) SRM(QC)

nDCG@10 0.249r q 0.333r q 0.334r q 0.320r q 0.332r q 0.311r q 0.305r q 0.301r q 0.303r q 0.329r q 0.298r q 0.299r q 0.348q 0.356r

nDCG 0.256r q 0.296r q 0.297r q 0.288r q 0.295r q 0.284r q 0.284r q 0.282r q 0.284r q 0.262r q 0.281r q 0.281r q
0.300
0.304

nERR@10 0.302r q 0.380r q 0.382r q 0.368r q 0.389r q 0.369r q 0.354r q 0.352r q 0.353r q 0.306r q 0.347r q 0.350r q 0.395q 0.405r

MRR 0.594r q 0.679r q 0.674r q 0.664r q 0.667r q 0.654r q 0.647r q 0.646r q 0.645r q 0.574r q 0.635r q 0.631r q 0.699q 0.716r

nDCG@10 0.113r q 0.165r q 0.171r q 0.181r q 0.167r q 0.134r q 0.153r q 0.160r q 0.163r q 0.158r q 0.158r q 0.160r q 0.188q 0.193r

nDCG 0.105r q 0.132r q 0.131r q
0.134 0.131r q 0.122r q 0.129r q 0.130r q 0.131r q 0.129r q 0.129r q 0.130r q
0.137
0.138

nERR@10 0.140r q 0.209r q 0.208r q 0.233r q 0.205r q 0.161r q 0.203r q 0.204r q 0.207r q 0.201r q 0.202r q 0.208r q 0.240q 0.248r

MRR 0.390r q 0.544r q 0.527r q 0.581r q 0.530r q 0.422r q 0.553r q 0.546r q 0.550r q 0.535r q 0.545r q 0.559r q 0.601q 0.612r

Table 2: Evaluation results. e r and q superscripts denote signi cant di erence with SRM(RM1) and SRM(QC), respectively (p < 0.05).

QL score with the score determined by each method. e document scores of the various language model baselines (i.e., FixInt, BayesInt, BatchUp, LongTEM and the variants of RM3 and SRM ) were further determined using the KL-divergence score [13]; where each baseline's learned model was clipped using a xed cuto of 100 terms [13]. e TREC session track trec eval tool5 was used for measuring retrieval performance. Using this tool, we measured the nDCG@10, nDCG (@2000), nERR@10 and MRR of each baseline. Finally, we tuned the RM3 and SRM's free parameters6 using the TREC 2011 track as a train set. e parameters were optimized so as to maximize MAP. e TREC 2012-2013 tracks were used as the test sets.
4.4 Results
e evaluation results are summarized in Table 2. e rst row reports the quality of the initial retrieval. Overall, compared to the various alternative baselines, the two SRM variants provided signi cantly be er performance; with at least +6.6%, +2.4%, +4.1% and +5.3% be er performance in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. e results clearly demonstrate the dominance of the session-context sensitive language modeling approaches (and the two SRM variants among them) over the other alternatives we evaluated. Furthermore, SRM's consideration of the user's query-change process as an additional relevance feedback source results in a more accurate estimate of the user's information need.
Next, compared to the RM3 variants, it is clear from the results that a dynamic relevance modeling approach that is driven by query-change (such as SRM) is a be er choice for the session search task. Moving from an ad-hoc relevance modelling approach (i.e., one that only focuses on the last query in the session) to a session-context sensitive approach provides signi cant boost in performance; with at least +14%, +7.0%, +9.8% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks.
5h p://trec.nist.gov/data/session/12/session eval main.py 6  {0.1, 0.2, . . . , 0.9},   {0.1, 0.2, . . . , 0.9}, m  {5, 10, . . . , 100}

We further observe that, compared to the baseline methods that
implement various query aggregation and scoring schemes (i.e.,
QA and QCM variants), a query-expansion strategy based on the
user's dynamic information need (such as the one implemented by
SRM variants) provides a much be er alternative; with at least
+18.5%, +6.1%, +15.1% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks.
Finally, comparing the two SRM variants side-by-side, it be-
comes even more clear that, using the query-change as an addi-
tional relevance feedback source is the be er choice; with at least
+2.3%, +1.0%, +2.5% and +1.8% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. Please
recall that, SRM(QC) was trained with a xed and equal-valued
priors p(qt ). Hence, a further improvement may be obtained by be er tuning of these priors.
REFERENCES
[1] Ben Cartere e, Paul Clough, Mark Hall, Evangelos Kanoulas, and Mark Sanderson. Evaluating retrieval over sessions: e trec session track 2011-2014. In Proceedings of SIGIR'2016.
[2] Steve Cronen-Townsend and W. Bruce Cro . antifying query ambiguity. In Proceedings of HLT'2002.
[3] Dongyi Guan and Hui Yang. ery aggregation in session search. In Proceedings of DUBMOD'2014.
[4] Jiepu Jiang and Chaoqun Ni. What a ects word changes in query reformulation during a task-based search session? In Proceedings of CHIIR'2016.
[5] Victor Lavrenko and W. Bruce Cro . Relevance based language models. In Proceedings of SIGIR'2001.
[6] Jiyun Luo, Xuchu Dong, and Hui Yang. Session search by direct policy learning. In Proceedings of ICTIR'2015.
[7] Jiyun Luo, Sicong Zhang, Xuchu Dong, and Hui Yang. Designing states, actions, and rewards for using pomdp in session search. In Proceedings of ECIR'2005, pages 526­537. Springer, 2015.
[8] Xuehua Shen, Bin Tan, and ChengXiang Zhai. Context-sensitive information retrieval using implicit feedback. In Proceedings of SIGIR'2005.
[9] Marc Sloan, Hui Yang, and Jun Wang. A term-based methodology for query reformulation understanding. Inf. Retr., 18(2):145­165, April 2015.
[10] Bin Tan, Xuehua Shen, and ChengXiang Zhai. Mining long-term search history to improve search accuracy. In Proceedings of KDD'2006.
[11] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. Lexical query modeling in session search. In Proceedings of ICTIR'2016.
[12] Hui Yang, Dongyi Guan, and Sicong Zhang. e query change model: Modeling session search as a markov decision process. ACM Trans. Inf. Syst., 33(4):20:1­ 20:33, May 2015.
[13] Chengxiang Zhai and John La erty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), April 2004.

868


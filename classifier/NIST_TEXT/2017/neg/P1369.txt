Doctoral Consortium

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Managing Tail Latencies in Large Scale IR Systems

Joel Mackenzie
RMIT University Melbourne, Australia joel.mackenzie@rmit.edu.au

CCS CONCEPTS
·Information systems Retrieval e ciency; Information retrieval query processing; Search engine architectures and scalability;
KEYWORDS
Tail Latency; E ciency; Scalability
ABSTRACT
With the growing popularity of the world-wide-web and the increasing accessibility of smart devices, data is being generated at a faster rate than ever before. is presents scalability challenges to web-scale search systems ­ how can we e ciently index, store and retrieve such a vast amount of data? A large amount of prior research has a empted to address many facets of this question, with the invention of a range of e cient index storage and retrieval frameworks that are able to e ciently answer most queries. However, the current literature generally focuses on improving the mean or median query processing time in a given system. In the proposed PhD project, we focus on improving the e ciency of high percentile tail latencies in large scale IR systems while minimising end-to-end e ectiveness loss.
Although there is a wealth of prior research involving improving the e ciency of large scale IR systems, the most relevant prior work involves predicting long-running queries and processing them in various ways to avoid large query processing times. Prediction is o en done through pre-trained models based on both static and dynamic features from queries and documents. Many di erent approaches to reducing the processing time of long running queries have been proposed, including parallelising queries that are predicted to run slowly [5, 6], scheduling queries based on their predicted run time [7], and selecting or modifying the query processing algorithm depending on the load of the system [1, 11].
Considering the speci c focus on tail latencies in large-scale IR systems, the proposed research aims to: (i) study what causes large tail latencies to occur in large-scale web search systems, (ii) propose a framework to mitigate tail latencies in multi-stage retrieval systems through the prediction of a vast range of query-speci c e ciency parameters, (iii) experiment with mixed-mode query semantics to provide e cient and e ective querying to reduce tail
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). 978-1-4503-5022-8/17/08 DOI: h p://dx.doi.org/10.1145/3077136.3084152

latencies, and (iv) propose a time-bounded solution for Document-
at-a-Time (D T) query processing which is suitable for current
web search systems.
As a preliminary study, Crane et al. [3] compared some state-of-
the-art query processing strategies across many modern collections.
ey found that although modern D T dynamic pruning strate-
gies are very e cient for ranked disjunctive processing, they have a much larger variance in processing times than Score-at-a-Time (S T) strategies which have a similar e ciency pro le regardless
of query length or the size of the required result set. Further-
more, Mackenzie et al. [8] explored the e ciency trade-o s for
paragraph retrieval in a multi-stage question answering system.
ey found that D T dynamic pruning strategies could e ciently retrieve the top-1,000 candidate paragraphs for very long queries.
Extending on prior work [3, 4, 7], Mackenzie et al. [9] showed
how a range of per-query e ciency se ings can be accurately predicted such that 99.99 percent of queries are serviced in less than 200 ms without noticeable e ectiveness loss. In addition, a
reference list framework [2, 4, 10] was used for training models
such that no relevance judgements or annotations were required.
Future work will focus on improving the candidate generation
stage in large-scale multi-stage retrieval systems. is will include
further exploration of index layouts, traversal strategies [3], and
query rewriting, with the aim of improving early stage e ciency
to reduce the system tail latency, while potentially improving end-
to-end e ectiveness.
REFERENCES
[1] D. Broccolo, C. Macdonald, S. Orlando, I. Ounis, R. Perego, F. Silvestri, and N. Tonello o. 2013. Load-sensitive selective pruning for distributed search. In Proc. CIKM. 379­388.
[2] C. L. A. Clarke, J. S. Culpepper, and A. Mo at. 2016. Assessing e ciency-- e ectiveness tradeo s in multi-stage retrieval systems without using relevance judgments. Information Retrieval 19, 4 (2016), 351­377.
[3] M. Crane, J. S. Culpepper, J. Lin, J. Mackenzie, and A. Trotman. 2017. A comparison of Document-at-a-Time and Score-at-a-Time query evaluation. In Proc. WSDM. 201­210.
[4] J. S. Culpepper, C. L. A. Clarke, and J. Lin. 2016. Dynamic cuto prediction in multi-stage retrieval systems. In Proc. ADCS. 17­24.
[5] S-W. Hwang, S. Kim, Y. He, S. Elnikety, and S. Choi. 2016. Prediction and predictability for search query acceleration. ACM Trans. Web 10, 3 (Aug. 2016), 19:1­19:28.
[6] M. Jeon, S. Kim, S-W. Hwang, Y. He, S. Elnikety, A. L. Cox, and S. Rixner. 2014. Predictive parallelization: taming tail latencies in web search. In Proc. SIGIR. 253­262.
[7] C. Macdonald, N. Tonello o, and I. Ounis. 2012. Learning to predict response times for online query scheduling. In Proc. SIGIR. 621­630.
[8] J. Mackenzie, R-C. Chen, and J. S. Culpepper. 2016. RMIT at the TREC 2016 LiveQA Track. In Proc. TREC-25.
[9] J. Mackenzie, J. S. Culpepper, R. Blanco, M. Crane, C. L. A. Clarke, and J. Lin. 2017. E cient and E ective Tail Latency Minimization in Multi-Stage Retrieval Systems. (2017). arXiv:1704.03970 [cs.IR]
[10] L. Tan and C. L. A. Clarke. 2015. A Family of Rank Similarity Measures Based on Maximized E ectiveness Di erence. TKDE 27, 11 (2015), 2865­2877.
[11] N. Tonello o, C. Macdonald, and I. Ounis. 2013. E cient and e ective retrieval using selective pruning. In Proc. WSDM. 63­72.

1369


Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Teaching the Information Retrieval Process Using a Web-Based Environment and Game Mechanics

omas Wilhelm-Stein
Chemnitz University of Technology 09107, Chemnitz, Germany thomas.wilhelm-stein@cs. tu-chemnitz.de

Stefan Kahl
Chemnitz University of Technology 09107, Chemnitz, Germany
stefan.kahl@cs.tu-chemnitz.de

Maximilian Eibl
Chemnitz University of Technology 09107, Chemnitz, Germany
maximilian.eibl@cs.tu-chemnitz.de

ABSTRACT
Predicting the performance of individual components of information retrieval systems, in particular the complex interactions between those components, is still challenging. erefore, professionals are needed for the implementation and con guration of retrieval systems and retrieval components. Our web-based application, called Xtrieval Web Lab, enables newcomers and learners to gain practical knowledge about the information retrieval process.
ey can arrange a multitude of components of retrieval systems and evaluate them with real world data without utilizing a programming language. Game mechanics guide the learners in their discovery process and motivate them.
CCS CONCEPTS
·Information systems Search engine architectures and scalability; Retrieval models and ranking; Evaluation of retrieval results; ·Applied computing Interactive learning environments;
KEYWORDS
evaluation; information retrieval; so ware-based teaching environment; game mechanics
1 MOTIVATION
Information retrieval systems permeate a multitude of aspects of our digital life. ey are used as an entry point to the Internet or help us organize emails and documents. Beyond that, they are also important in other highly specialized contexts, like patent research or preservation of cultural heritage.
All these use cases require information retrieval systems which are adapted to those tasks and specialized in their behavior. ere is an eminent need for trained professionals, who can perform this kind of adaptations in an e cient way.
An information retrieval system can be represented as a pipeline of diverse components for indexing and searching. Such components can be stemmers, retrieval algorithms, or blind relevance feedback. Based on her practical knowledge, an experienced researcher can decide which components t well given a special use
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3084143

case. She also knows which components work well if combined and which will not. An unacquainted learner needs to acquire this knowledge through lots of trials and errors.
e use of existing information retrieval systems as a starting point enables learners to quickly set up their systems and conduct experiments. For this approach, an easy access to these systems and good documentation are essential. Special courses at universities allow students to get in touch with these systems and also provide rst insights into the underlying components and their interdependencies.
We present a web-based application, which allows learners to conduct experiments without the need to set up a retrieval system beforehand and without the need to deal with the speci cs of an evaluation.
2 EXISTING SYSTEMS
Although some systems, like SMART [20] or Terrier [18], have emerged from research projects at universities, a self-evident use for teaching is rarely the case or undocumented. In contrast, the popular and open-sourced framework Apache Lucene is o en used in practically oriented courses [19]. Lemur, which has its origins in research as well, and its successor Galago are also used for teaching [2, 11].
Courses based on open-source libraries are primarily bene cial for students of computer sciences, as they are more likely to have some knowledge in a programming language and its basic concepts. Without prior knowledge in programming, the information retrieval concepts tend to lose priority and students are forced to learn programming languages rather than the actual concepts and techniques of information retrieval. [8, 16, 17]
As a result, the technical aspects of information retrieval systems are inadvertently at the center of those courses.
2.1 ery-based Systems
Soekia by Jurjevic´, Sto¨cklin and Hartmann [12] has been used in various classes from secondary school to college. Tests showed that Soekia improves the understanding of how search engines work. It presents the information retrieval process on a small scale.
erefore, the learning environment is suitable only for small data collections and the variety of possible se ings are limited. Additionally, tools for the evaluation of retrieval results are missing entirely.
e search queries can only be entered individually and manually. e results are presented in the form of plain HTML pages. INSTRUCT (Interactive System for Teaching Retrieval Using Computational Techniques) was presented by Hendry, Wille and Wood [9, 10] as early as 1986. Its primary goal was to acquaint

1293

Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: Overview of existing teaching so ware for IR and their capabilities
library and information science students with new developments regarding query formulation in the eld of information retrieval systems. e application employed a character-based, menu-driven user interface. e users could choose available features and options from various menus, which were o en described in detail. In addition to the Boolean retrieval, which was very common in this era, queries could be formulated in natural language.
IR Game by Sormunen et al. [22], also known as ery Performance Analyser (QPA), is a web application to evaluate the impact of di erent formulations of search queries, especially in a multilingual context. e IR Game was used for both: research and teaching [21]. e in-depth analysis of the queries and the promotion of an exploratory approach supports the learners in developing conceptual and mental models of the search engine. An important aspect of all tests are instructional exercises that help students handling the IR Game, as the system itself is not self-explanatory.
e IR Game and INSTRUCT focus on the construction of highquality search queries. IR components or algorithms are underrepresented. Although a few characteristics of the index can be altered, the students do not have direct control over the underlying IR mechanics.
2.2 Component-based Systems
IR-Toolbox by E himiadis and Freier [4] is a web-based learning tool with the goal of visualizing the IR process to students. rough practical experimentation without the use of a programming language, students can develop a richer conceptual model and thus a be er understanding of the IR process and the underlying algorithms. Using the IR Toolbox, students are enabled to con gure speci c, predetermined components of the IR process. However, the components can not be arranged freely by learners, resulting in a very restricted scope. e user interface is very minimalistic, which limits the comprehensibility for new users. However, technically savvy students are able to look up speci c implementation details. [3]

SulaIR presented by Ferna´ndez-Lune, Huete, Rodr´iguez-Cano and Rodr´iguez [6] is a Java desktop application, which much like IR Toolbox, represents the basic steps of the information retrieval process. Visualizations support the understanding of each step of the retrieval process. According to Ferna´ndez-Lune et al. [6] the system was tested in an information retrieval course at the University of Holgu´in in Cuba and the results have been positive. However, the nature of the test and the actual results are not available. Although the project is labeled open-source, there is no source code publicly available and the project does seem to be discontinued.
e web-application VIRLab by Fang, Wu, Yang and Zhai [5] is specialized on experiments with IR models. It allows learners to create their own IR models and to compare them with each other.
e IR models are created in a C-based language, which allows the access to document and term statistics. e learners can test their retrieval models with their own queries or prede ned test collections consisting of queries and relevance assessments. For the comparison with other models, metrics and charts are available on experiment and query level. e focus on retrieval models reduces the complexity of such a system by a great margin. us, it is easier for learners to get started with the system. In addition, all experiments are performed server-side. is greatly reduces the learners' e ort to set up their own IR environment. For each test collection, a ranking of the ten best retrieval models is available.
Lo´pez-Garcia and Cacheda [15] presented IR-Components, which is a Java-based framework for developing information retrieval applications. IR-Components contains basic components for each step of the IR process, which can be used for simple experiments. Learners are encouraged to develop their own components in order to improve the results of the base system. To conduct their rst experiments, learners need to know Java as well as the important interfaces and classes of IR-Components. e user interface for search and distributed execution environment are already included.
IR-BASE by Calado, Cardoso-Cachopo and Oliveira [1] has been built to support research and teaching. Much like IR-Components, it provides a framework for the implementation of new information retrieval components. A guideline and a documentation ensure compatibility between those components. At rst, students create a pool of basic components, which serve as the baseline for more sophisticated components. With the next step, students extend the functionality of the whole system by creating more components. Unfortunately, there is no built-in option to evaluate new components and measure their performance. erefore, implementation errors might remain undetected and can compromise the performance of the entire system.
3 OUR SYSTEM
e Xtrieval Web Lab1 is a web-application, which builds upon our Xtrieval framework [14]. It is designed to simplify the experimentation and evaluation process in a way that it can be adapted and executed without prior knowledge in programming languages.
rough the web interface, experiments can be composed and parametrized using pre-fabricated components. A live preview of the processed data assists the students during the process of
ne-tuning the components. A er an experiment is composed by
1h ps://medien.informatik.tu-chemnitz.de/weblab/

1294

Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 2: Screenshot of the user interface with assignment, components pipeline, and preview

Figure 3: Screenshot of the user interface displaying the results as chart and table

the users, the con guration of this very experiment is submi ed to the server which handles the main processing load. Finally, the results are returned to the web-interface. e students can explore these results and gain insight using various visualizations. Furthermore, they can continue to change the parameters of the composed retrieval work ow to in uence the results and observe the e ects of individual changes to the components.
While designing our retrieval web-application we focused on the following design goals:
· Experiments are compiled of components, · Components can be parametrized, · A preview, based on a sampled document of the actual
collection, visualizes how components manipulate data, · Experiments are processed on a web-server, and · Detailed results are displayed to allow the user to explore
and optimize.
In order to maximize exibility for components and parameters and the responsiveness of the entire system, experiments need to be run individually for every user in (near) real-time. e students should not be restricted by limitations of pre-designed work ows. For instance, stop words can be speci ed by the students without relying on any prede ned list, components can be deactivated or re-ordered and individual parameter sets may result in hundreds of di erent result lists. Additionally, experiments need to be processed very fast. Users of web applications are not willing to wait as long as researchers would in their labs; short feedback loops are vital. We optimized our system regarding execution speed. In the IAPR TC-12 Benchmark [7], which was used for ImageCLEF from 2006 to 2008, our system is capable of rendering a preview a er an average of only 400 milliseconds. A complete run of an entire experiment, including the creation of an index, the search and the calculation of evaluation metrics, can be performed within 5 to 20 seconds.
is performance varies depending on the size of the collection, the number of components used and the hardware con guration of the web-server. Overall, our web-application is one of the fastest retrieval systems suitable for consumer hardware.
Although the back-end of our system is implemented in Java due to the use of Apache Lucene and Terrier, the actual components are wri en in JavaScript which enables students to design their

own components without the restrictions of a static desktop UI. Components run in a sandbox which allows e cient access to the collections and the search engine via the Oracle Nashorn JavsScript Engine2. Parallel IO-threads process collections e ciently and utilize threading capabilities of Lucene to a great extend. is results in a major speed-up compared to other retrieval systems.
We use Game mechanics to achieve two main objectives: First, by using a level system in which a user can ascend a er reaching certain milestones, the initial complexity of the system at start is kept low and intricacy increases according to the user's experience.
e complexity of the retrieval system cra ed by the students increases until all components have been unlocked, only skilled users will be presented all available options. Secondly, students should be motivated to use and understand as many components as possible even beyond a potential increase in knowledge. In our conducted studies regarding the motivation for extended learning of retrieval components and processes, students who were presented with achievements and badges for their learning progress showed great desire to reach even higher scores and be er results despite the lack of further assignments. is proves that gami cation of the teaching process for retrieval systems can be very e ective to increase the students ability to understand the underlying concepts.
Besides, by using assignments, learners are familiarized with the user interface and are guided in learning about the available parameters and their in uence on the retrieval results. A mode for unrestricted experimentation allows the use of all available components and serves as playground for free exploration. Achievements do not only inform about certain milestones, but also grant access to further components and motivate continued experimentation. We incorporated leaderboards so users can compare their results with each other and are able to deduct which strategies might be best for the completion of certain tasks.
We designed the Xtrieval Web Lab with non-computer science students in mind as it does not require any knowledge about programming languages. It is aimed at listeners of lectures on the topic of information retrieval and bene ts the learning experience when used in exercises and seminars.
2h p://www.oracle.com/technetwork/articles/java/jf14-nashorn-2126515.html

1295

Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4 SUMMARY
e Xtrieval Web Lab is a learning environment that enables learners to perform real IR experiments on the basis of individually con gurable components. Relying on the Xtrieval Framework, which has already proven its performance and exibility in several evaluation campaigns, it bene ts from established so ware components. Although both systems use similar tools, the Xtrieval Web Lab represents a more qualitative approach. It was designed for manual experimentation, which is supervised and controlled by a tutor. e Xtrieval Framework, on the other hand, represents a more quantitative approach since it was developed primarily for the execution of unsupervised experiments with many di erent con gurations [13].
You can access the Xtrieval Web Lab for free by visiting the following web-site: h ps://medien.informatik.tu-chemnitz.de/weblab/
REFERENCES
[1] Pa´vel Calado, Ana Cardoso-Cachopo, and Arlindo L. Oliveira. 2007. IR-BASE: An Integrated Framework for the Research and Teaching of Information Retrieval Technologies. In Proceedings of the First International Conference on Teaching and Learning of Information Retrieval (TLIR'07). BCS Learning & Development Ltd., Swindon, UK, 2­2. h p://dl.acm.org/citation.cfm?id=2228236.2228238
[2] W Bruce Cro , Donald Metzler, and Trevor Strohmann. 2010. Search engines. Pearson Education.
[3] E himis N. E himiadis, Jamie Callan, and Ray R. Larson. 2007. Approaches to teaching & learning information retrieval. Proceedings of the American Society for Information Science and Technology 44, 1 (2007), 1­3. DOI:h p://dx.doi.org/ 10.1002/meet.1450440136
[4] E himis N. E himiadis and Nathan G. Freier. 2007. IR-Toolbox: An Experiential Learning Tool for Teaching IR. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '07). ACM, New York, NY, USA, 914­914. DOI:h p://dx.doi.org/10.1145/ 1277741.1277982
[5] Hui Fang, Hao Wu, Peilin Yang, and ChengXiang Zhai. 2014. VIRLab: A Webbased Virtual Lab for Learning and Studying Information Retrieval Models. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 1249­1250. DOI:h p://dx.doi.org/10.1145/2600428.2611178
[6] JM Ferna´ndez-Luna, JF Huete, JC Rodr´iguez-Cano, and MC Rodr´iguez. 2012. Teaching and learning information retrieval based on a visual and interactive tool: Sulair. In EDULEARN12 Proceedings. IATED, 6634­6642.
[7] Michael Grubinger, Paul Clough, Henning Mu¨ller, and omas Deselaers. e IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information Systems. In OntoImage 2006 Workshop on Language Resources for Content-based Image Retrieval during LREC 2006 Final Programme.

[8] David G. Hendry. 2007. History Places: A Case Study for Relational Database and Information Retrieval System Design. J. Educ. Resour. Comput. 7, 1, Article 3 (March 2007). DOI:h p://dx.doi.org/10.1145/1227846.1227849
[9] Ian G Hendry, Peter Wille , and Frances E. Wood. 1986. INSTRUCT: a teaching package for experimental methods in information retrieval. Part I. e users view. Program 20, 3 (1986), 245­263. DOI:h p://dx.doi.org/10.1108/eb046940 arXiv:h p://dx.doi.org/10.1108/eb046940
[10] Ian G Hendry, Peter Wille , and Frances E. Wood. 1986. INSTRUCT: a teaching package for experimental methods in information retrieval. Part II. Computational aspects. Program 20, 4 (1986), 382­393. DOI:h p://dx.doi.org/10.1108/ eb046949 arXiv:h p://dx.doi.org/10.1108/eb046949
[11] Seikyung Jung and Joseph Lawrance. 2011. Web Information Retrieval and Filtering Course to Undergraduates Using Open Source Programming. ACM Inroads 2, 3 (Aug. 2011), 47­50. DOI:h p://dx.doi.org/10.1145/2003616.2003634
[12] Diana Jurjevic´, Nando Stcklin, and Werner Hartmann. 2009. Informationskompetenz: ein ema fu¨r den Informatikunterricht. Lecture Notes in Informatics (2009).
[13] Jens Ku¨rsten. 2012. A Generic Approach to Component-Level Evaluation in Information Retrieval. (2012).
[14] Jens Ku¨rsten, omas Wilhelm, and Maximilian Eibl. 2008. Extensible retrieval and evaluation framework: Xtrieval. In LWA 2008: Lernen - Wissen - Adaption, Wu rzburg, October 2008, Workshop Proceedings.
[15] Rafael Lo´pez-Garc´ia and Fidel Cacheda. 2011. A Technical Approach to Information Retrieval Pedagogy. Springer Berlin Heidelberg, Berlin, Heidelberg, 89­105. DOI: h p://dx.doi.org/10.1007/978-3-642-22511-6 7
[16] Frank McCown. 2010. Teaching Web Information Retrieval to Undergraduates. In
Proceedings of the 41st ACM Technical Symposium on Computer Science Education (SIGCSE '10). ACM, New York, NY, USA, 87­91. DOI:h p://dx.doi.org/10.1145/ 1734263.1734294 [17] Masnizah Mohd. 2011. Development of Search Engines using Lucene: An Experience. Procedia - Social and Behavioral Sciences 18 (2011), 282 ­ 286. DOI: h p://dx.doi.org/10.1016/j.sbspro.2011.05.040 [18] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Christina. 2006. Terrier: A High Performance and Scalable Information Retrieval Platform. In In Proceedings of ACM SIGIR'06 Workshop on Open Source Information Retrieval (OSIR 2006). [19] Ian Ruthven, David Elsweiler, and Emma Nicol. 2008. Designing for users: an holistic approach to teaching information retrieval. In Second International Workshop on Teaching and Learning of Information Retrieval (TLIR 2008). [20] Gerard Salton. 1991. e Smart Document Retrieval Project. In Proceedings of the
14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '91). ACM, New York, NY, USA, 356­358. DOI: h p://dx.doi.org/10.1145/122860.122897 [21] Eero Sormunen, Sakari Hokkanen, Pe eri Kangaslampi, Petri Pyy, and Bemmu Sepponen. 2002. ery Performance Analyser -: A Web-based Tool for IR Research and Instruction. In Proceedings of the 25th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR '02). ACM, New York, NY, USA, 450­450. DOI:h p://dx.doi.org/10.1145/564376. 564491 [22] Eero Sormunen, Juha Laaksonen, Heikki Keskustalo, Jaana Keklinen, Harri Laitinen, Ari Pirkola, and Kalervo Jrvelin. 1998. IR Game : a tool for rapid query analysis in cross-language IR experiments. In Singapore: Kent Ridge Digital Labs. 22­32.

1296


Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Online Learning to Rank for Cross-Language Information Retrieval

Razieh Rahimi
Department of Computer Science Georgetown University Washington D.C.
razieh.rahimi@georgetown.edu
ABSTRACT
Online learning to rank for information retrieval has shown great promise in optimization of Web search results based on user interactions. However, online learning to rank has been used only in the monolingual se ing where queries and documents are in the same language. In this work, we present the rst empirical study of optimizing a model for Cross-Language Information Retrieval (CLIR) based on implicit feedback inferred from user interactions. We show that ranking models for CLIR with acceptable performance can be learned in an online se ing, although ranking features are noisy because of the language mismatch.
CCS CONCEPTS
·Information systems Users and interactive retrieval; Learning to rank; Multilingual and cross-lingual retrieval;
KEYWORDS
Online learning; Learning to rank; Cross-language information retrieval
1 INTRODUCTION
Leveraging user interactions to optimize Web search results has a racted considerable a ention in recent years. is is because o ine evaluation of retrieval models based on the Cran eld paradigm does not necessarily generalize to actual users and other time periods [18], while online optimization based on user interactions allows learning of personalized ranking function [3].
An online learning to rank algorithm for information retrieval integrates user feedback in optimizing the parameters of a ranking function. In contrast, the common approach in learning to rank algorithms is to optimize parameters of a retrieval model based on manually annotated training data, thus these algorithms perform o ine learning. Online systems for IR examine a new se ing of retrieval parameters at each iteration and update the parameters based on user feedback on the provided ranking. e goal of an online system for IR is to maximize cumulative performance of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080710

Azadeh Shakery
School of Electrical and Computer Engineering College of Engineering, University of Tehran School of Computer Science, Institute for Research in
Fundamental Sciences (IPM), Tehran, Iran shakery@ut.ac.ir
result lists presented to the user in the learning process, referred to as online performance. is objective function is to ensure that users do not experience low-quality results during maximizing the performance of a ranking function. On the other hand, nal performance refers to the performance of the learned ranking function on test data in both online and o ine cases.
Although user interactions are exploited to optimize systems for monolingual information retrieval in several studies (e.g., [9, 11, 16, 20, 21, 26]), to the best of our knowledge, these interactions have not been speci cally used to optimize models for CLIR. In this paper, we investigate online optimization of systems for CLIR.
Implicit feedback inferred from user interactions with a retrieval system is inherently noisy, which makes online optimization of ranking functions challenging [9]. In addition to the noisy nature of such feedback, features of a cross-lingual ranking function are noisy. is happens because some statistics in ranking features such as term frequency and document frequency of a term, cannot be computed directly in CLIR, and are estimated using translation models. e noisier nature of systems for CLIR makes learning from user interactions more challenging. In this work, we study the suitability of online learning of a ranking function for CLIR.
In this paper, we address three research questions: (1) How does the nal performance of an online learning to rank algorithm compare to that of an o ine learning to rank algorithm for CLIR?
is comparison demonstrates how a learned ranking model for CLIR based on user interactions compare to that based on explicit manual judgments. (2) How does the nal performance of an online learning to rank algorithm for CLIR compare to that for monolingual IR? (3) How does the online performance of an online learning to rank algorithm for CLIR compare to that for monolingual IR? e second and third comparisons speci cally reveal the impact of noisier ranking features in CLIR on the performance of an online learning to rank algorithm.
We demonstrate that, although the cross-lingual environment is noisier than the monolingual one, online learning to rank algorithms can be successfully adopted to learn personalized ranking functions.
2 RELATED WORK
e purpose of our study is to examine online learning to rank for CLIR, which is not yet explored to the best of our knowledge. However, there are several studies related to our work, which form two groups: (1) work on using (o ine) learning to rank algorithms for CLIR, and (2) work on online learning to rank for monolingual information retrieval.

1033

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Learning to rank algorithms have been widely used for ad hoc IR and many IR applications [13]. For multilingual information retrieval, as a sub-task of IR, there are a number of methods based on learning to rank algorithms [7, 23, 24]. ese methods mainly focus on learning to merge result lists retrieved separately for each language in multilingual IR. However, this step is not required in CLIR where all documents are in the same language, but di erent from the query language. Gao et al. [6] learn a ranking function based on bilingual features for monolingual IR. In another line, Azarbonyad et al. [1] de ne cross-lingual features to learn a ranking function for CLIR, however parameters are learned in an o ine se ing.
Online learning to rank for IR. Learning to rank approaches for IR in an online se ing try to optimize search results based on user interactions. e challenging point is that retrieval systems cannot estimate the utility value for a provided ranking from user interactions. Ordinal feedback for some document pairs [12] or lists of documents [17] can only be inferred. e la er feedback can be obtained using interleaved comparison methods including balanced interleave [17], and multileave comparison [21]. In particular, interleaving methods facilitate comparison of utility values for result lists of two or more rankers. Using such methods, online optimization of ranking functions is modeled as a dueling bandits problem in [26]. Using interleaved comparison methods allows listwise learning of ranking functions in an online se ing. In another line, Hofmann et al. [11] investigate online learning to rank based on pairwise document preferences. ey nally improve the performance of online learning to rank by balancing exploration and exploitation for both listwise and pairwise learning. In another study, Hofmann et al. [9] investigate how the learning speed can be increased by reusing historical interaction data of users.
Online learning of parameter values is also adopted to optimize the base BM25 ranker using user interactions [20]. In addition to optimization of parameters with continuous values, interleaved comparison methods are used for online evaluation of a nite set of rankers, which is formulated as k-armed dueling bandit problems in [25]. However, all these methods are examined for monolingual IR.

Algorithm 1 DBGD algorithm [26]

Require:  ,  , w1 1: for query qt (t = 1 . . .T ) do

2: 3: 4: 5:

wCSifaotmwmtpplwwaerituenn+wsittthuvaeetnncdtowrtut uniformly.

6:

wt +1  wt + ut

7: else

8:

wt +1  wt

9: return wT +1

3 ONLINE LEARNING TO RANK FOR IR
In this section, we describe the problem of online optimization of a ranking function. Online learning to rank for information retrieval is modeled as a reinforcement learning problem in which

Table 1: Dataset properties.

Year

Data collection

Document

ery Experiment

language language

name

French 2002:En-Fr

Los Angeles Times 1994 English

Italian

2002:En-It

2002

Spanish 2002:En-Es

Le Monde 1994 French SDA 94

French

English 2002:Fr-En

La Stampa 1994 Italian SDA 94

Italian

English 2002:It-En

2003

Los Angeles Times 1994 Glasgow Herald 1995

English

French Spanish

2003:En-Fr 2003:En-Es

the retrieval system repeatedly interacts with the user to learn an approximately optimal ranking function by maximizing the cumulative reward. e cumulative reward is calculated over an in nite horizon of time steps using [11]:

C =   t -1rt ,

(1)

t =1

where rt is the reward received at time t, and is weighted by the discount rate   [0, 1) to place more emphasis on immediate rewards. Reward rt is computed using a retrieval performance measure such as average precision or Normalized Discounted Cumulative Gain (NDCG).
Online optimization of retrieval functions parameterized by a weight vector w, is formulated as dueling bandits problem for continuous parameter space [26]. e Dueling Bandit Gradient Descent (DBGD) algorithm to learn the weight vector in this problem is shown in Algorithm 1. e algorithm works as follows. At each timestep t, the system receives query qt and provides a ranked list to the user. is ranked list is generated by interleaving methods, which compare two retrieval functions by providing the user with a combination of their respective rankings. One ranking is generated by using the current best estimate of the weight vector, maintained in wt . e other exploratory ranking is produced by perturbation of wt along a random direction ut . e user interacts with the result list, and click information is used to determine the winner. If the exploratory list wins the comparison, the current best weight vector is updated by moving along ut . is process repeats continuously.
Herein, the goal is online learning of a weight vector for linear combination of ranking features in CLIR. In particular, the ranking model f for CLIR is a linear function of the form

f (x) = wTx,

(2)

where x denotes the ranking features for CLIR and w is a weight vector. e goal is online learning of w based on users' implicit feedback using Algorithm 1. Line 4 of this Algorithm compares two ranking models. For this step, we employ probabilistic interleave method [10].

4 EXPERIMENTS AND RESULTS
Datasets. Evaluations are carried out against test collections from ad-hoc cross-language track in CLEF-2002 and CLEF-2003 campaigns. We use English, French, and Italian collections with query sets in multiple languages for the experiments reported here, which represent di erent language pairs and di erent translation directions. Test collections and their languages as well as query languages are shown in Table 1. In addition, the query set in the

1034

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Learning features for CLIR.

Feature 1
2 3
4
5
6 7 8 9 10 11

Description

qi qi

q q

log(1 log(1

+ +

CLTF(qi , d))

CLTF(qi d

,

d

)

)

qi q log(CLIDF(qi ))

qi q qi q

log(1 log(1

+ +

C CCLLTTFF((qqii
d

, ,

C) d)

) 

CLIDF(qi

))

qi q

log(1

+

CLTF(qi,d ) d



C CLTF(qi

,

C

)

)

PSQ score

LMIR with DIR smoothing

LMIR with JM smoothing

LMIR with ABS smoothing

d

Category Q-D
Q-D Q
Q
Q-D
Q-D Q-D Q-D Q-D Q-D
D

language of each test collection is used to provide monolingual baseline for CLIR performance. We index the TEXT and TITLE
elds of documents in test collections for retrieval. Preprocessing. Diacritic characters are mapped to the corre-
sponding unmarked characters. Stopwords are removed. Next, we use Snowball stemmers for all languages.
Translation Models. We build a word-to-word translation model for each language pair using the Europarl Corpus [22]. Statistical translation models (IBM model 1) are obtained using the GIZA++ toolkit. Before word alignment, the aforementioned preprocessing steps are done on both sides of each parallel corpus. Obtained translation probabilities are then linearly normalized by selecting the top 3 translations for each word.
Learning features. For feature extraction, we rst sample some documents for each query. We use the BM25 and Probabilistic Structured ery (PSQ) [5] models to rank all documents with respect to each query in monolingual and cross-lingual se ings, respectively. A er ranking, the top 1,000 documents for each query are selected for feature extraction. We extract 11 features for each query-document pair, which are shown in Table 2 for the crosslingual se ing similar to [1]. In this se ing, frequency of query term qi in document d as well as document frequency of qi are estimated using translation models as follows [5]:

cltf(qi , d) =

p(qi , w) × tf(w, d),

(3)

w d

cldf(qi ) =

p(qi , w) × df(w),

(4)

w d

where d represents the vocabulary set of the document language, and p(qi , w) shows the translation probability of word w to qi in

the respective translation model. Inverse document frequency of

a

term

is

computed

as

clidf (w )

=

log

N cldf

+1 (w

)

,

where

N

is

the

total

number of documents in the document collection. e PSQ model

for CLIR uses the estimates in Eqs. 3 and 4 in the BM25 model to rank documents. Parameters of the BM25 model are set as k1 = 1.2, k3 = 7, and b = 0.75 [5, 15]. Features 8, 9, and 10 are calculated by integration of translation models in the query language model [14], and smoothing parameters of these features are set as µ = 2, 000,  = 0.1, and  = 0.7, respectively [15]. For monolingual query-

document pairs, we extract the same set of features, where cltf and

0.65 0.6
0.55 0.5
0.45 0.4
0.35 0.3 0

Perfect-CLIR Perfect-Mono Navigational-CLIR Navigational-Mono Informational-CLIR Informational-Mono

200

400

600

800 1000

Figure 1: Final performance (NDCG@10) over iterations for 2002:En-Fr dataset and all click models.

clidf are respectively replaced by tf and idf, and feature 7 is the BM25 score. Finally, we perform query-based normalization for each feature.
Simulation of user clicks. User clicks are generated based on the dependent click model [8] which generalizes the cascade model to multiple clicks. We instantiate this click model based on click and stop probabilities similar to the instantiations used in [11]. ese instantiations simulate three levels of increasing noise in user's feedback. e perfect click model provides reliable feedback, and is used to obtain an upper bound on the performance. e other two models, navigational and informational, are realistic user models. To the best of our knowledge, there is no study to speci cally model user clicks in cross-lingual search sessions. However, using click models for monolingual IR seems reasonable, since instantiations are based on the purpose of the search, which is independent of the query language, and result lists in CLIR contain documents in one language, similar to monolingual IR.
Experimental setup. Online learning experiments are done using the lerot framework [19]. We split each query set into 5 parts to perform 5-fold cross validation. e discount factor in Eq. 1 is set as  = 0.995, and all experiments are run for 1000 iterations [11]. We repeat all experiments 25 times and average results over folds and repetitions.
Results and Discussion. We rst report the nal performance of the learned ranking function for CLIR a er 1,000 iterations for each dataset in Table 3. To gain more insights into the obtained
nal performance of the online learning to rank algorithm (the DBGD algorithm) for CLIR, we provide two performance in Table 3; (1) performance of ListNet [2], one of the representative listwise algorithms for o ine learning to rank, since the DBGD algorithm performs listwise learning [9]. e ListNet algorithm uses manual relevance judgments in learning, while the DBGD algorithm learns from relative comparisons of two ranking functions based on user clicks. e results of ListNet, therefore, determine the level of nal performance that can be expected from online learning to rank algorithms. For our experiments, we use the RankLib [4] implementation of the ListNet algorithm with the default parameters.
e results in Table 3 show that online learning to rank can be successfully adopted for the CLIR se ing, since online optimization outperforms the o ine learning. However, the improvements are not statistically signi cant. (2) nal performance of online learning

1035

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: Final performance of the online learning to rank algorithm (perfect click model) in comparison with the supervised ranking algorithm in terms of NDCG@10.

Online L2R for CLIR ListNet for CLIR Online L2R for Monolingual IR

2002:En-Es 0.377 0.321

2002:En-Fr 0.366 0.309 0.475

2002:En-It 0.377 0.300

2002:Fr-En 0.472 0.406 0.471

2002:It-En 0.330 0.280 0.439

2003:En-Es 2003:En-Fr

0.423

0.476

0.390

0.407

0.478

Table 4: Online performance in terms of cumulative NDCG over 1000 iterations.

Click model Perfect
Navigational
Informational

IR model Cross-lingual Monolingual Cross-lingual Monolingual Cross-lingual Monolingual

2002:En-Es 58.753
53.935
38.471

2002:En-Fr 58.817 78.322 51.938 72.447 37.618 55.685

2002:En-It 58.685
51.754
38.760

2002:Fr-En 76.628 79.562 68.411 72.385 53.207 58.847

2002:It-En 54.531 74.492 49.956 69.955 37.841 55.755

2003:En-Es 2003:En-Fr

66.167

74.059

77.232

59.741

65.927

71.180

41.768

47.707

57.790

to rank for the monolingual se ing of test collections. One metric to evaluate the performance of CLIR is the percentage compared to the performance of monolingual IR [14]. erefore, the results of the DBGD algorithm for the monolingual se ing of test collections are also reported in Table 3, which show that online learning to rank for the CLIR cases achieves reasonable percentage of that for the monolingual ones, and even performs equivalently for 2002:Fr-En dataset (higher performance of CLIR than monolingual IR for some cases is also reported in [14]). Figure 1 shows the learning curves for 2002:En-Fr dataset, nal performance of the learned ranking function at each iteration, for di erent click models in both monolingual and cross-lingual se ings. In both se ings, the noisier the click model, the lower the nal performance. e learning curve for each click model in the cross-lingual se ing has almost the same trend as the one in the monolingual se ing.
Table 4 reports the online performance using Eq. 1 obtained by di erent click models for each dataset. e results in the table include the online performance for the CLIR se ing in comparison with that for the monolingual se ing, which show that the online performance in the CLIR se ing achieves acceptable percentage of that in the monolingual se ing. e results thus demonstrate that users would not experience low-quality results.
5 CONCLUSION AND FUTURE WORK
In this paper, we studied the optimization of retrieval functions for CLIR based on users' implicit feedback. We demonstrated that although the cross-lingual environment is noisier than monolingual one, the online learning to rank algorithm DBGD can be successfully adopted for learning of personalized ranking models. ere are several possible directions for future work. A promising line is to reuse historical data to accelerate the learning speed in the cross-lingual se ing. We also would like to investigate how user interactions with monolingual search results can be integrated in the learning process of models for CLIR.
6 ACKNOWLEDGMENTS
is research was in part supported by a grant from the Institute for Research in Fundamental Sciences (No. CS1396-4-51). Any opinions, ndings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily re ect those of the sponsor.

REFERENCES
[1] Hosein Azarbonyad, Azadeh Shakery, and Heshaam Faili. 2012. Using Learning to Rank Approach for Parallel Corpora Based Cross Language Information Retrieval. In ECAI. 79­84.
[2] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In ICML. 129­136.
[3] Yiwei Chen and Katja Hofmann. 2015. Online Learning to Rank: Absolute vs. Relative. In WWW. ACM, 19­20. DOI:h p://dx.doi.org/10.1145/2740908.2742718
[4] Van Dang. 2016. h ps://people.cs.umass.edu/vdang/ranklib.html. (2016). [5] Kareem Darwish and Douglas W. Oard. 2003. Probabilistic structured query
methods. In SIGIR. ACM, 338­344. DOI:h p://dx.doi.org/10.1145/860435.860497 [6] Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 2009. Exploiting Bilingual
Information to Improve Web Search. In ACL '09. 1075­1083. [7] Wei Gao, Cheng Niu, Ming Zhou, and Kam-Fai Wong. 2009. Joint Ranking for
Multilingual Web Search. In ECIR. 114­125. [8] Fan Guo, Chao Liu, and Yi Min Wang. 2009. E cient Multiple-click Models in
Web Search. In WSDM. 124­131. DOI:h p://dx.doi.org/10.1145/1498759.1498818 [9] Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. 2013.
Reusing Historical Interaction Data for Faster Online Learning to Rank for IR. In WSDM. 183­192. DOI:h p://dx.doi.org/10.1145/2433396.2433419 [10] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2011. A Probabilistic Method for Inferring Preferences from Clicks. In CIKM. 10. [11] Katja Hofmann, Shimon Whiteson, and Maarten Rijke. 2013. Balancing Exploration and Exploitation in Listwise and Pairwise Online Learning to Rank for Information Retrieval. Inf. Retr. 16, 1 (Feb. 2013), 63­90. [12] orsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In KDD. 133­142. DOI:h p://dx.doi.org/10.1145/775047.775067 [13] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Found. Trends Inf. Retr. 3, 3 (March 2009), 225­331. DOI:h p://dx.doi.org/10.1561/1500000016 [14] Jian Yun Nie. 2010. Cross-Language Information Retrieval. Morgan & Claypool. [15] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval. Inf. Retr. 13, 4 (2010). [16] Filip Radlinski and orsten Joachims. 2005. ery Chains: Learning to Rank from Implicit Feedback. In KDD. ACM, 239­248. [17] Filip Radlinski, Madhu Kurup, and orsten Joachims. 2008. How Does Clickthrough Data Re ect Retrieval ality?. In CIKM. 10. [18] Mark Sanderson. 2010. Test Collection Based Evaluation of Information Retrieval Systems. Found. Trends Inf. Retr. 4, 4 (2010). [19] Anne Schuth, Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Lerot: An Online Learning to Rank Framework. In LivingLab. 4. [20] Anne Schuth, Floor Sietsma, Shimon Whiteson, and Maarten de Rijke. 2014. Optimizing Base Rankers Using Clicks: A Case Study using BM25. In ECIR. [21] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten de Rijke. 2014. Multileaved Comparisons for Fast Online Evaluation. In CIKM. [22] Jrg Tiedemann. 2012. Parallel Data, Tools and Interfaces in OPUS. In LREC. [23] Ming-Feng Tsai, Hsin-Hsi Chen, and Yu-Ting Wang. 2011. Learning a Merge Model for Multilingual Information Retrieval. Inf. Process. Manage. 47, 5 (Sept. 2011), 635­646. [24] Nicolas Usunier, Massih-Reza Amini, and Cyril Gou e. 2011. Multiview Semisupervised Learning for Ranking Multilingual Documents. In ECML PKDD. Springer-Verlag, 443­458. [25] Yisong Yue, Josef Broder, Robert Kleinberg, and orsten Joachims. 2009. e k-armed dueling bandits problem. In COLT. [26] Yisong Yue and orsten Joachims. 2009. Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem. In ICML. 8.

1036


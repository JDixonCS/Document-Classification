Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Detecting Positive Medical History Mentions

Bing Bai
Richard Jackson
ABSTRACT
In medical practice, knowing the medical history of a patient is crucial for diagnosis and treatment suggestion. However, such information is o en recorded in unstructured notes from doctors, potentially mixed with the medical history of family members and mentions of disorders for other reasons (e.g. as potential sidee ects). In this work we designed a scheme to automatically extract the medical history of patients from a large healthcare database. More speci cally, we rst extracted medical conditions mentioned using a rule-based system and a medical gaze eer, then we classi-
ed whether such mentions re ected the patient's history or not. We designed our method to be simple and with li le human intervention. Our results are very encouraging, supporting the potential for e cient and e ective deployment in clinical practice.
KEYWORDS
electrical health record, medical history, string kernels, skip-gram
1 INTRODUCTION
Electronic Medical Records (EMRs) are becoming the standard repository for healthcare information. Large volumes of invaluable information in EMRs can potentially help medical scientists, doctors and patients improve the quality of care. However, these large volumes can also make it very hard to extract information of interest, and manual record screening is tedious, tiring and error-prone.
ere is therefore a pressing need for be er analytical systems. A major challenge in EMR mining is that a signi cant amount of data is in unstructured natural text. Analysis of such data heavily depends on the unsolved problem of natural language understanding. Compounding this, medical text frequently contains incomplete sentences, jargon, idiosyncratic measurements and abbreviations that make it hard to use general purpose natural language process (NLP) modules such as sentence spli er and syntactic and semantic parsing. ere is also development of specialist end-to-end systems for EMR text analysis (e.g. [17]), usually with a combination of rules
bbai@nec-labs.com, NEC Labs America, INC. pierre.francois@gmail.com, Google INC. is work was conducted when in NEC Labs America, INC. richard.r.jackson@kcl.ac.uk, King's College London. §robert.stewart@kcl.ac.uk, King's College London.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080717

Pierre-Francois Laquerre
Robert Stewart§
made by domain experts and statistical learning models trained on labeled data. However, both methods tend to be di cult to apply to datasets that are di erent from the training data.
One of the most important types of information is the medical history. E ectively extracting information such as previous diagnoses and treatment would greatly reduce a doctors' time in reading EMRs, and much work have been devoted to the extraction of disorder mentions from doctors' notes [12, 16].
In this paper, we focus on identifying patients' disorders from unstructured text. We not only extracted the disorder mentions, but also made distinctions between instances applied to patients and others. In EMR, a disorder can be mentioned for various reasons: 1) a disorder the patient has; 2) a disorder that the patient does not have; 3) a disorder a ecting someone other than the patient (e.g. a family member); 4) other mentions of disorders in the EMR (e.g. as a potential side e ect). Integrating this distinction in a disorder extraction system would represent an important step forward in re ning the extracted disorder information, and help doctors or researchers more e ectively characterize a patient's previous health experiences.
Our system included a simple rule-based module and a machine learning-based module. We used string kernels with support vector machines (SVM) [19] on raw note text which required li le human intervention (e.g. se ing up gaze eers for disorders of interest). In a study on a large mental healthcare EMR database, we obtained good accuracy scores for most of the disorders we tested, and the application performed well on unseen datasets.
2 RELATED WORK
ere are a number of conferences or workshops focusing on EMR analysis. For example, CLEF eHealth [12], SemEval clinical tasks[16] hold annual events, trying to integrate research to solve certain problems in EMR mining. Common topics like medical name entity recognition have been extensively studied. Medical record data has historically been di cult to access because of privacy issues. is is changing since the release of large de-identi ed datasets like MIMIC1 and i2b2 2. e availability of large datasets will certainly encourage more research and development on EMR mining.
Besides the specialized EMR topics, there is also a long history of end-to-end systems for EMR analytics. Information extraction systems such as cTAKES [17], MedLee [8], MetaMap [1] can speed up notes reviewing by extract important information from unforma ed notes. ese systems usually take advantage of the medical terminology database UMLS [4]. ere are also more ambitious goals for higher level machine intelligence. For instance, IBM has devoted signi cant resources in developing the medical version of
1h ps://mimic.physionet.org/ 2h ps://www.i2b2.org/NLP/DataSets/Main.php

1049

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

question answering system Watson [7], which claimed fame in the 2013 Jeopardy contest.
3 METHOD
3.1 Problem Setting
We de ne in this paper that a "disorder mention" (or simply "mention") is an instance of a disorder name occurred in a medical record. We are trying to identify medical disorder mentions whose subject is the patient. As we indicated in the introduction, there are quite di erent reasons a disorder is mentioned in notes. In this work, we only consider two cases: 1) Positive: e patient had or is having the disorder, 2) Negative: all other cases. is is a simple but e ective measure to identify the disorder mention of interest.
Our method has two steps. In the rst step, we use o -the-shelf information extraction engines GATE [5] with customized gazeteers to mark possible mentions (for example, keywords "asthma", "asthmatic", and other descriptive phrases can be used in the gazeteer of the disorder asthma). In the second step, we classify whether the marked mentions are positive. In this paper, we assume the rst step works well, and focus on building machine learning models for the second step.
3.2 Classi cation
3.2.1 SVM with Kernel Functions. We do not discuss Support Vector Machines (SVM) in details, due to page limit and abundance of literature (e.g. [18]). In short, SVM is a classi cation method that proved very exible and powerful by customizing kernel functions K (xi ,xj ) for data samples xi and xj . A kernel function can be viewed as the similarity between two data samples. e classi cation of a new data sample is based on its similarity (kernel function value) with learned landmark data samples (support vectors). Knowledge about a speci c problem can be integrated into a kernel function to allow SVM to nd rather sophisticated class boundaries. Being able to leverage kernel functions is an important reason that SVM is one of the most popular machine learning methods.
3.2.2 Feature Extraction. We then build features for each mention. Most information needed to classify the mention is typically around the mention. In our experiments, we found that taking a xed-length window worked be er than taking a sentence or a paragraph. e window includes 20 words before and a er the mention, unless it reaches the beginning or the end of the document. Formally, we de ne the window to be
W = [wb ,wb+1,wb+2, ...,wm , ...,we-2,we-1,we ],
where wi is the i-th word in the document, b = max(m - 20, 0), and e = min(m + 20, N ), where m is the index of the mention and N is the length of the document.
Bag of Words (BOW). e most straightforward feature scheme is "bag-of-word" (BOW): each word in window W is a separate feature. e information on the order of words is discarded, thus making it a "bag". A simple yet powerful variation of BOW is the n-gram model. at is: we take each sequence of n words in W to be a single feature. ere have been studies [2] that show common

choices of n = 2 (bigram), or n = 3 (trigram) give best results in document classi cation.
String Kernels. We explore string kernels that are widely used in sequence analysis [15]. String kernels usually implement heuristics on how two sequences should be similar. Sparse Spatial String Kernels (SSSK) [13] got us the best results we have (to be shown in the experiment section). In such kernels, the kernel function for two sequences X and Y can be de ned as:

K (t,k,d ) (X ,Y )

=

ai

k ,0di <d

CX CY

(a1,d1, ...,at -1,dt -1,at (a1,d1, ...,at -1,dt -1,at

)· )

(1)

where ai are k-grams, separated by di < d words in the sequence, and CX and CY are counts of such units in X and Y respectively. To give an example, suppose t = 2, k = 1, and d = 2, we have
two sequence X = "ABC" and Y = "ADC". We can see the count CX ("A", 1, "C") = 1 and CY ("A", 1, "C") = 1, thus K (1,1,2) (X ,Y ) = 1  1 = 1.
We also explore a new variation of SSSK, in which the distance
requirements are relaxed as follows:

Kr(t,k,d ) (X ,Y )

=

ai k ,0di ,di <d

CX CY

(a1,d1, ...,at -1,dt -1,at )· (a1,d1, ...,at -1,dt -1,at )

(2)

In other words, in Eq.1, K (1,1,2) ("ABC", "AC") = 0, but in its relaxed version Eq.2, Kr(1,1,2) ("ABC", "AC") = 1. Intuitively, this adaptation will enable the model to match phrases like "her mother had ...",

and "her mother earlier had ...".

Interestingly, when using SVM, the above SSSK kernel method

is equivalent to "skip-grams" [10] features with a linear kernel,

when k = 1. Similar to n-grams, a "skip-gram" is a sequence of

n words. e di erence from n-gram is that skip-grams allow

intervals between words: a 2-skip-gram for the sequence " e cat

sat on the mat" can be "sat mat", when 2-gram can only allow

consecutive words like "sat on" or "the mat".

4 EXPERIMENTS
4.1 Dataset and experiment setup
We used a medical record database for patients with mental disorders. All records were de-identi ed, with the name entities replaced by special tokens like "YYYYY" or "ZZZZZ". We extracted 500 to 1000 mentions on each of the following disorders: asthma, arthritis, stroke, diabetes, and myocardial infarction, and manually labeled them with either "Positive" or "Negative".
As shown in Table 1, the distribution of labels varies between di erent disorders. For asthma, most mentions are positive. On the other hand, most mentions of strokes are about family history rather than about the patient. Overall the dataset slightly favors positive cases (56% being positive). However, Myocardial Infarction is dominated by negative cases. We looked at these cases, and found that one of the acronyms of Myocardial Infarction is "MI" was also used as an acronym of "Mental Illness", which caused many wrong instances be extracted by the GATE module, and in turn be labeled as "negative" by human labelers. is problem needs to be addressed in future work.

1050

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Statistics on experimental datasets

Asthma Arthritis Diabetes Stroke Myocardial Infarction
All

Total 964 745 981 1090 575 4355

Positive 769 631 509 441 120 2470

Negative 195 114 472 649 455 1885

Table 2: Testing accuracy. Trained on all disorders (80% of each disorder merged), and tested on the rest 20% of each disorder

Asthma Arthritis Diabetes
Stroke Myocardial Infarction
Average

1-gram 0.878 0.858 0.829 0.834 0.771 0.834

2-gram 0.887 0.873 0.824 0.864 0.830 0.860

3-gram 0.895 0.874 0.851 0.870 0.824 0.863

SSSK 0.901 0.877 0.845 0.869 0.821 0.863

To extract features for mention classi cation, we take a surrounding window according to section 3.2.2. en we build the n-gram features or skip-gram features for equivalent SSSK from this window. e mention in the middle is replaced with a constant token "DISO", forcing the learned model focus on the context rather than the name of the disorder, thus making it more generalizable to untrained disorders. For n-gram models, higher n-grams is a superset of lower n-grams. For examples, 3-gram features include all 2-grams and 1-grams. For SSSK/skip-gram features, we set t = 3, k = 1, and d = 2.
All our results are tested on liblinear [6], with model hyperparameters selected using grid search, with di erent model types and cost coe cients selected using grid search,
4.2 Training/testing on the same disorders
We rst study the performance on the same disorders that were present in the training set. We split cases of each disorder into 80% training and 20% testing.
e corresponding accuracies are in Table 2. Note the average are micro average, meaning we rst calculate the score of each disorder, then take average of those scores.
We can see that in this case, the SSSK ties the performance of 3-gram.
4.3 Training/testing on di erent disorders
We now turn to the more important problem of generalization to disorders that were not in the training set. A er all, it is impractical to collect training data for each medical disorder. So in practice, we will certainly want mentions of new disorders to be classi ed.
In Table 3, we listed the results of disorder-wise cross validation. at is, we train on 4 disorders, and test the remaining disorder. e rst column are the disorders tested. We can see the model works

Table 3: Testing accuracy. Trained on 4 other disorders, and tested on the listed disorder.

Asthma Arthritis Diabetes
Stroke Myocardial Infarction
Average

1-gram 0.771 0.846 0.715 0.757 0.593 0.743

2-gram 0.806 0.847 0.705 0.783 0.634 0.770

3-gram 0.801 0.820 0.739 0.790 0.708 0.772

SSSK 0.807 0.816 0.745 0.782 0.708 0.773

Table 4: Testing accuracy. Trained on di erent number of disorders, and tested on the rest.

Number of Train Disorders 1 2 3 4

1-gram 0.597 0.683 0.724 0.743

2-gram 0.603 0.705 0.750 0.770

3-gram 0.605 0.707 0.752 0.772

SSSK 0.603 0.701 0.748 0.773

reasonably well, with the SSSK achieving the best score, although the di erence is minor.
In Table 4, we show that, as we add more disorders to the training set, the performance on unseen disorders goes up.
4.4 Analysis of learned models
Besides the quantitative results, here we also analyze the models and try to interpret the results. We list most positive and most negative n-gram/skip-gram features in Table 5. For each set of features, on the le are the features with largest learned weights, and on the right the smallest. "DISO" is the masked disorder mention, as indicated in 4.1. Note in the column SSSK, there could be up to d words between the presented words (see Eq. 2).
We can get quite intuitive features even using 1-gram features. For example, words "mother", "father" are strong indicator that this might be the condition of family members. Also, "died" is most likely not about the patient, since these are the ongoing record of the patient.
If we look at the 3-gram features, we can see many useful features that were missing in the 1-gram case. "her/his DISO" are likely to be positive because the mention of family history would probably not going to details that using words of "his/her", also it is clear that the mention is not a negation of the patient having that condition. "or DISO" is among top negative features because it is o en in a family history list: "... nobody in family had diabetes, or DISO ...". e word "history" are seen here in both positive feature "medical history", and negative feature "family history", which were not caught in the 1-gram features sets. Also note that there is no actual 3-grams in the top 20 3-gram features since the list 1-gram or 2-gram features turned out to be more signi cant for the model.
e SSSK features looks similar to the 3-gram results. However we can still notice features that were missing in the la er. For example: "medical history DISO" can be in slightly di erent forms in records: "medical history : DISO", or "medical history: asthma, DISO". Since 3-gram features require consecutive words, they will

1051

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: e most positive and most negative features learned with di erent feature schemes.

1-gram

Positive Negative

medical mother

inhalers able since

died or father

prior type although

feels epilepsy
who

controlled allergies control

family believes
dad

history physical previous

no denied brother

le which sleep

stroke maternal possibly

she harm diagnosed

sta check insulin

childhood risk

3-gram

Positive

Negative

her DISO

died

his DISO since
inhalers

mother or
father

able medical history
although

who or DISO a DISO

medical prior
prior to

: integer DISO unit
brother

diagnosed type le

DISO or believes her mother

DISO control controlled allergies

feels that this
sta

history control
),

DISO / stroke family history of

previous

denied

SSSK
Positive
her DISO his DISO
since inhalers medical history medical although
able her DISO .
), since DISO . medical . medical history medical history DISO
control controlled following DISO
prior type diagnosed

Negative
a DISO mother
died or a DISO father : integer or DISO , or DISO or who her mother brother DISO or feels stroke sta a DISO . believes /

miss the matching of these type of exible forms of features. us, although in this dataset the SSSK models didn't manage to get signi cantly be er results than 3-grams, we still think it has potential to outperform n-grams in other datasets.
4.5 Additional experiments
We also tried 1) "RBF kernels", "polynomial kernels" as alternative to the linear kernels and SSSK, 2) domain adaptation as in [3], 3) Conditional Random Field (CRF) [14] (sequence labeling "positive mention" or otherwise), 4) deep learning methods as in [2, 11], 5) extra features with negation detection NegEx [9], and 6) EHR analysis toolkit cTAKES [17]. We did not include the results for their unsatisfying results. We think these methods, though with good potentials, are not necessarily as powerful for a small training set as the proposed methods.
5 CONCLUSION
We presented a system that extracts positive disorder mentions from unstructured text. e method we used is simple, e ective, interpretable and has good capability to generalize. We believe it can be a useful component in a EMR analysis system.
REFERENCES
[1] Alan R Aronson and Franois-Michel Lang. 2010. An overview of MetaMap: historical perspectiveand recent advances. JAMIA 17 (2010), 229­236.
[2] Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shokoufandeh. 2011. Sentiment Classi cation Based on Supervised Latent n-gram Analysis. In CIKM.
[3] John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classi cation. In ACL.
[4] Olivier Bodenreider. 2004. e Uni ed Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Research 32 (2004).

[5] H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva. 2013. Ge ing More Out of Biomedical Documents with GATE's Full Lifecycle Open Source Text Analytics. PLoS Comput Biol 9, 2 (2013).
[6] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classi cation. Journal of Machine Learning Research 9 (2008).
[7] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building Watson: An Overview of the DeepQA Project. AI Magazine (2010).
[8] C. Friedman. 2000. A broad-coverage natural language processing system. Proceedings of the AMIA symposium (2000).
[9] Stephan Gindl. 2006. Negation Detection in Automated Medical Applications. Technical Report. Vienna University of Technology.
[10] David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. A CLoser Look at Skipgram Modelling. In LREC.
[11] Rie Johnson and Tong Zhang. 2015. Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding. In NIPS.
[12] Liadh Kelly, Lorraine Goeuriot, Hanna Suominen, Tobias Schreck, Gondy Leroy, Danielle L. Mowery, Sumithra Velupillai, Wendy W. Chapman, and David Martinez. 2014. Overview of the ShARe/CLEF eHealth Evaluation Lab 2014. Lecture Notes in Computer Science 8685 (2014).
[13] Pavel P. Kuksa and Vladimir Pavlovic. 2010. Spatial Representation for E cient Sequence Classi cation. In International Conference on Pa ern Recognition (ICPR).
[14] John La erty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.
[15] Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text Classi cation using String Kernels. Journal of Machine Learning Research 2 (2002).
[16] Sameer Pradhan, Noemie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova. 2014. SemEval-2014 Task 7: Analysis of Clinical Text. In 8th International Workshop on Semantic Evaluation (SemEval 2014).
[17] Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-Schuler, , and Christopher G Chute1. 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. J Am Med Inform Assoc 17, 5 (2010).
[18] Bernhard Scholkopf and Alexander J. Smola. 2001. Learning with Kernels. MIT Press.
[19] Vladimir Vapnik. 1998. Statistical Learning eory.

1052


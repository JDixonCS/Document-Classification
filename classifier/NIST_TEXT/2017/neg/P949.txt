Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

A Comparative Live Evaluation of Multileaving Methods on a Commercial cQA Search

Tomohiro Manabe
Yahoo Japan Corporation Chiyoda-ku, Tokyo, Japan 102-8282
tomanabe@yahoo-corp.jp

Akiomi Nishida
Yahoo Japan Corporation Chiyoda-ku, Tokyo, Japan 102-8282
anishida@yahoo-corp.jp

Makoto P. Kato
Graduate School of Informatics, Kyoto University
Sakyo-ku, Kyoto, Japan 606-8501 kato@dl.kuis.kyoto-u.ac.jp

Takehiro Yamamoto
Graduate School of Informatics, Kyoto University
Sakyo-ku, Kyoto, Japan 606-8501 tyamamot@dl.kuis.kyoto-u.ac.jp
ABSTRACT
We present one of the world's rst a empts to examine the feasibility of multileaving evaluation of document rankings on a large scale commercial community estion Answering (cQA) service. As a natural enhancement of interleaving evaluation, multileaving merges more than two input rankings into one and measures the search user satisfaction of each input ranking on the basis of user clicks on the multileaved ranking. We evaluated the adequateness of two major multileaving methods, team dra multileaving (TDM) and optimized multileaving (OM), proposing their practical implementation for live services. Our experimental results demonstrated that multileaving methods could precisely evaluate the e ectiveness of ve rankings with di erent quality by using clicks from real users. Moreover, we concluded that OM is more e cient than TDM by observing that most of the evaluation results with OM converged a er showing multileaved rankings around 40,000 times and an in-depth analysis of their characteristics.
CCS CONCEPTS
·Information systems Evaluation of retrieval results; Retrieval e ectiveness; Information retrieval;
KEYWORDS
QA search and retrieval; Interleaving and multileaving; Evaluation of document ranking
1 INTRODUCTION
Evaluating search result rankings on the basis of implicit feedback from a large number of real users is a fascinating way to improve the e ectiveness of search services. For this purpose, A/B testing has been widely conducted in search services. However, A/B testing has
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080687

Sumio Fujita
Yahoo Japan Corporation Chiyoda-ku, Tokyo, Japan 102-8282
sufujita@yahoo-corp.jp
a crucial shortcoming: as di erent rankings are examined by di erent users with di erent search intents, A/B testing requires a large number of page views to reach a conclusion on the e ectiveness of subjects. Alternative ways include interleaving or multileaving. Interleaving methods combine two search result rankings [5, 6], whereas multileaving methods combine more than two rankings [9]. e quality of each input ranking is measured by user clicks observed on the combined ranking. Since di erent rankings are compared by the same user in interleaving or multileaving, we can evaluate multiple rankings more e ciently than A/B testing [1].
Currently, there are four well-known interleaving methods, three of which have multileaving counterparts: balanced interleaving (BI) [4], team dra interleaving/multileaving (TDI/TDM) [6, 9], optimized interleaving/multileaving (OI/OM) [5, 9], and probabilistic interleaving/multileaving (PI/PM) [2, 8]. ey are promising as they can evaluate multiple rankings at a time. However, they have been studied mainly on the basis of simulation where user clicks were arti cially generated by simple models. us, the evaluation accuracy and e ciency of multileaving methods with real users need to be understood in order to apply them to production environments in which more complicated models are expected.
is paper describes the rst a empt to examine their performance on one of the world's largest commercial cQA search services. We introduce practical implementations of TDM and OM and answer the following research questions in a real service:
RQ 1. How precisely can we evaluate the e ectiveness of rankings by using multileaving methods in a live test of a commercial service?
RQ 2. How fast do the results converge? How stable are they?
RQ 3. Which multileaving method is more e ective and e cient?
Our experimental results demonstrated that multileaving methods could precisely evaluate the e ectiveness of ve rankings by using clicks from real users. Moreover, we concluded that OM is more e cient than TDM, with an observation that most of the evaluation results with OM converged a er showing the combined rankings around 40,000 times and an in-depth analysis of their characteristics. Note that we did not compare PM in our experiments due to its property: PM can generate a combined ranking that has

949

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Algorithm 1: Team Dra Multileaving (TDM)

Require : Input rankings I, number of output rankings m,

and number of items in each output ranking l

1 O  {} and for k = 1, . . . , m, j = 1, . . . , n do Tk, j  {};

2 for k = 1, . . . , m do

3 for i = 1, . . . , l do

4

Select j randomly s.t. |Tk, j | is minimized;

5

r = 1;

6

while Ij,r  Ok do r  r + 1;

7

if r  |Ij | then

8

Ok,i = Ij,r and Tk, j  Tk, j  {Ij,r };

9

end

10 end

11 O  O  {Ok }; 12 end

13 return O;

Algorithm 2: Optimized Multileaving (OM)

Require : Input rankings I, number of output rankings m, and number of items in each output ranking l

1 O  {};

2 for k = 1, . . . , m do

3 for i = 1, . . . , l do

4

Select j randomly;

5

r = 1;

6

while Ij,r  Ok do r  r + 1;

7

if r  |Ij | then

8

Ok,i = Ij,r ;

9

end

10 end

11 O  O  {Ok }; 12 end

13 return O;

signi cantly worse quality than any of the input rankings, which
is not a preferable feature for a live test of an operating service.
e remainder of the paper is organized as follows. Section 2
explains the details of TDM and OM. Section 3 describes the se ings
of our live test, and presents the experimental results. Finally,
Section 4 concludes the paper by summarizing our contributions.
2 MULTILEAVING METHODS
A multileaving method takes a set of rankings I = {I1, I2, . . . , In } and returns a set of combined rankings O = {O1, O2, . . . , Om }, where each combined ranking Ok consists of l items. e i-th items of an input ranking Ij and an output ranking Ok are denoted by Ij,i and Ok,i , respectively. When a user issues a query, we return an output ranking Ok with probability pk and observe user clicks on Ok . If Ok,i is clicked by the user, we give a credit  (Ok,i , Ij ) to each input ranking Ij . Each multileaving method consists of a way to construct O from I, probability pk for each output ranking Ok , and a credit function  . e original multileaving methods decide which input ranking is be er for each input ranking pair every time it
presents an output ranking, whereas we opt to accumulate the cred-
its through all the presentations and to measure the e ectiveness
of each input ranking on the basis of the sum of the credits, mainly
because this approach must provide more informative evaluation
results. We next explain the details of TDM and OM.
2.1 Team Dra Multileaving
TDM [9] outputs a combined ranking by randomly selecting an
input ranking, choosing an item from the input ranking that has not
been added to the output ranking yet, and appending the item to
the output ranking. An input ranking obtains a certain credit if an
item a ributed to the input ranking is clicked by a user. Although
the original TDM generates an output ranking every time a query is issued, we generate m output rankings o ine and randomly choose one of them online to reduce the query latency.
As shown in Algorithm 1, TDM generates a new output ranking Ok of length l by randomly choosing an input ranking Ij that has appended the least number of items into Ok (Line 4) and appending

to Ok the item at the highest rank that is not in Ok yet (Lines 5­9). e appended item is also added to the team (Tk, j ) of Ij (Line 8).
Note that Ij,n+1 here is de ned as null for convenience.
e credit of TDM is given to an input ranking corresponding
to a team that holds a clicked item and is de ned as:  (Ok,i , Ij ) = 1 if Ok,i  Tk, j ; otherwise, 0. e presentation probability pk is the same for all the output rankings, i.e. pk = 1/m.

2.2 Optimized Multileaving
OM [9] is a multileaving method that generates output rankings by Algorithm 2, which is almost the same as Algorithm 1 except for Lines 1, 4, and 8. Furthermore, OM computes the presentation probability pk that maximizes the sensitivity of the output rankings while ensuring no bias. e sensitivity of an output ranking is the power to discriminate e ectiveness di erences between input rankings when the output ranking is being presented to users. Intuitively, the sensitivity is high if random clicks on an output ranking give a similar amount of credits to each input ranking. High sensitivity is desirable as it leads to fast convergence of evaluation results. Bias of output rankings measures the di erence between the expected credits of input rankings for random clicks. If the bias is high, a certain input ranking can be considered be er than the others even if only random clicks are given. us, multileaving methods should reduce the bias as much as possible.
e sensitivity can be maximized by minimizing insensitivity, de ned as the variance of credits given by an output ranking Ok through rank-dependent random clicks [9]:

k2 =

n j =1

l i =1

f

(i) (Ok,i ,

Ij)

- µk

2
,

(1)

where f (i) is the probability with which a user clicks on the i-

th item. We follow the original work [9] and use f (i) = 1/i

and  (Ok,i , Ij ) = 1/i if Ok,i  Ij ; otherwise, 1/(|Ij | + 1). e

mean credit µk of the output ranking Ok is computed as: µk =

(1/n)

n j =1

l i =1

f (i) (Ok,i , Ij ).

Since

each

output

ranking Ok

is

presented to users with probability pk , OM should minimize the

expected insensitivity: E[k2] =

m k =1

pk

k2

.

950

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

e bias of output rankings O is de ned as the di erence between

expected credits of input rankings. e expected credit of an input
ranking Ij given rank-independent random clicks on top-1, top-2, · · · , and top-r items is de ned as follows:

E[ (Ij , r )] =

m k =1

pk

r i =1



(Ok,

i

,

Ij

).

(2)

If the expected credits of input rankings are di erent, evaluation

results obtained by multileaving is biased. us, the original version

of OM imposes a constraint that the expected credits of all the input rankings must be the same, i.e. (r , cr , j) E[ (Ij , r )] = cr .
According to the paper by Schuth et al. [9] and their publicly available implementation1, their version of OM tries rst to satisfy

the constraint for le ing the bias be zero, and then to maximize

the sensitivity given that the bias constraint is satis ed. However,

we found that the bias constraint cannot be satis ed for more than 90% of the cases in our experiment, i.e. we could not nd any

solution that satis ed the bias constraint. Hence, we propose using

a more practical implementation of OM that minimizes a linear

combination of the sum of biases and the insensitivity as follows:

minpk 

l r

=1

r

+

m k =1

pk

k2

(3)

subject to

m k =1

pk

=

1,

0

 pk



1

(k

=

1, . . . , m),

and

r , j, j , -r  E[ (Ij , r )] - E[ (Ij , r )]  r ,

where  is a hyper-parameter that controls the balance between

the bias and insensitivity, and r is the maximum di erence of the expected credits in any input rankings pairs. If r is close to zero,

the expected credits of input rankings are close, and accordingly, the

bias becomes small. Also, our implementation is publicly available2.

In summary, we developed an evaluation method on the basis

of accumulated credits, o ine sampling of output rankings for

TDM, and a solvable optimization problem for OM for enabling

multileaving methods to be practically used.

3 EXPERIMENTS
To answer our research questions, we conducted a live test of Yahoo! Chiebukuro, a community-based commercial QA search service in Japan, which claimed 714 million page views in October 2014.

3.1 Evaluation Methodologies
Similar to a web search, the cQA search service receives keyword queries and returns ranked questions in decreasing order of their query relevance scores. From its search query logs, we pooled queries that were issued ve or more times on two or more days in Oct. 2016, from which we randomly chose 1,062 queries for the live test. As for questions to be ranked, we selected ten or more questions for each query from the current production search results, which account for 19,637 questions in total. We set 10 to l, which is the length of output rankings, since a er this position in the ranking, the click probability of items steeply decreases, which would make the evaluation procedure ine cient.
We adopted ve methods to rank the questions for each query: (1) Descending order of the number of answer postings (pop), (2) Descending order of human-annotated topical relevance between the query and question (rel), (3) BM25 score of the question and its best
1h ps://github.com/djoerd/mirex 2h ps://github.com/mpkato/interleaving

Table 1: Comparison of nDCG@10 scores of input rankings. Statistical signi cance (p < 0.05) by t-test are marked by .

Method
1. pop 2. rel 3. qa 4. cat 5. irrel

nDCG @10 0.605 0.601 0.576 0.569 0.541

Di erence from 1. pop 2. rel 3. qa 4. cat

-0.004 -0.029 -0.036 -0.063

-0.025 -0.032 -0.060

-0.007 -0.035

-0.028

Table 2: Di erence between the sum of credits. Statistical signi cance (p < 0.05) by t-test are marked by .

2. rel 3. qa 4. cat 5. irrel

(a) Team Dra (TDM)
1. pop 2. rel 3. qa 4. cat 1185  1187  2 1697  512  510  1649  464  462  -48

(b) Optimized (OM)
1. pop 2. rel 3. qa 4. cat 977 1360 382 1854 876 494 1767 789 407 -87

answer (qa), (4) BM25 score of category labels of the question (cat), and (5) ascending order of the topical relevance (irrel). Note that

tied documents are ranked in descending order of their timestamps.

As the prior work did, we needed another evaluation methodol-

ogy for measuring the accuracy of evaluation results obtained by the

multileaving methods. We evaluated the rankings by nDCG@10 [3],

where DCG@10 =

10 i =1

(2

i

-

1)/log(1

+ i),

and

the

grade

of

the

i-th document is given by i  {0, 1, 2, 3}. In our experiments, we

estimated i on the basis of a clickthrough log of the same service,

rather than relevance judgments from assessors, as we aimed to

evaluate rankings on the basis of real user preferences. We used

a simple position based click model to estimate the grade of each

question [7]. Note that we estimate the grade on the basis of a

clickthrough log between Nov. 2014 and Nov. 2016, much longer

than the live test period. Table 1 shows the evaluation results of
the ve input rankings. We used  to mark the pairs for which the di erences were found statistically signi cant (p < 0.05) by paired t-test. e results suggest that users prefer questions that have

more answers and are more topically relevant to the queries.

Finally, we generated multileaved rankings from the ve input

rankings for each query using both TDM and OM methods in the

same ratio each, and examined them in the live test, where we set
100 to m. erefore, the number of examined rankings for each query accounts for 200 in total. e hyper-parameter  was set to 1

because we had no knowledge about a reasonable value. We carried

out the live test from Nov. 25 2016 to Jan. 29 2017. We presented

multileaved rankings for a particular portion of search requests and

received 210,389 page views of multileaved search results.

3.2 Results and Discussions
Table 2 shows the di erence between the sum of credits obtained by the methods of each column and row. "" denotes the statistically signi cant di erences between credits given to two input rankings, found by paired t-test (p < 0.05). Both methods correctly evaluated the e ectiveness of all the pairs, except for cat and irrel. Statistically signi cant di erences were indicated for nine pairs with OM, but
only for eight pairs with TDM. Note that it may not be a failure of

951

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Team Draft Multileaving (TDM)
1.0

0.8

p-value

0.6

0.4

0.2

0.0

Optimized Multileaving (OM)

1.0

pop-rel

pop-qa

0.8

pop-cat

pop-irrel

rel-qa

0.6

rel-cat

rel-irrel

0.4

qa-cat

qa-irrel

cat-irrel

0.2

p-value

0.0 0

10

20

30

40

50

60

Days passed

Figure 1: Time series of p-values. Each line corresponds to a pair of two input ranking methods. Horizontal axis shows number of days passed from beginning of live test.

multileaving to indicate statistically signi cant di erences which

are not in Table 1 since their numbers of samples (paired nDCG@10

scores or credits) are di erent. Figure 1 shows the time series of p-values obtained by each multileaving method for all the pairs of

input rankings. As shown in the gure, OM indicated statistically signi cant di erences (p < 0.05) of eight out of ten pairs a er one

week from the beginning (bo om). e evaluation result of OM between rel and qa converged (p < 0.05) on the 13th day, or with

around 40,000 page views. In contrast, TDM indicated di erences

of only six out of ten pairs (top) a er a week and showed unstable
evaluation results for rel-qa and cat-irrel throughout the live test. In a simulation study [9], Schuth et al. observed that OM converged

slightly faster than TDM, whereas TDM achieved more accurate

evaluation results than OM. Unlike their simulation, our live test

showed that the evaluation results of OM converged much faster

than TDM while achieving the same evaluation results as TDM.

We try to explain these results on the basis of their bias and

insensitivity. Figure 2 shows a sca er plot of actual bias and in-

sensitivity of TDM and OM. Each dot represents a query and the

values are normalized considering the distributions of their credits.

Since the scale of the credits is di erent for the two methods, we

used (1/l)

l r

=1

(1

-

minj

E[

(Ij , r )]/maxj

E[

(Ij

, r )]) as the bias.

We divided the insensitivity k2 by the square of the average credit

µk2 , since the insensitivity is proportional to it. Comparing two

sca er plots, we can see that both the bias and the insensitivity

Insensitivity

Team Draft Multileaving (TDM)
6

5

4

3

2

1

0

-1

-0.2

0.0

0.2

0.4

0.6

0.8

1.0

1.2

Bias

Optimized Multileaving (OM)

-0.2

0.0

0.2

0.4

0.6

0.8

1.0

1.2

Bias

Figure 2: Scatter plot of bias-insensitivity tradeo . Horizontal axis shows bias, and vertical axis shows insensitivity, where each dot corresponds to a query.

of expected credits are smaller with OM than TDM thanks to OM directly minimizing both the bias and the insensitivity.

4 CONCLUSION
We compared two promising multileaving methods, team dra multileaving (TDM) and optimized multileaving (OM), on a large scale commercial community estion Answering (cQA) search service. We introduced some practical implementations of TDM and OM to apply them to production environments. We compared the methods for answering our three research questions (RQs). As an answer to RQ1, our experimental results demonstrated that both multileaving methods could precisely evaluate the e ectiveness of ve rankings of di erent quality by using clicks from real users. Finally, as answers to RQ2 and RQ3, we conclude that OM performs more e ciently than TDM on the basis of the observation that most OM results converged a er having presented the combined rankings around 40,000 times and the analyses of bias and sensitivity.

ACKNOWLEDGMENTS
is work was supported by JSPS KAKENHI Grant Numbers 26700009 and 16K16156.

REFERENCES

[1] Olivier Chapelle, orsten Joachims, Filip Radlinski, and Yisong Yue. 2012. Large-

scale Validation and Analysis of Interleaved Search Evaluation. ACM TOIS 30, 1

(2012), 6.

[2] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2011. A Probabilistic

Method for Inferring Preferences from Clicks. In CIKM. 249­258.

[3] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated Gain-based Evaluation

of IR Techniques. ACM TOIS 20, 4 (2002), 422­446. [4] orsten Joachims. 2002. Unbiased Evaluation of Retrieval
through Data. Technical Report. SIGIR MF/IR.

ality Using Click-

[5] Filip Radlinski and Nick Craswell. 2013. Optimized Interleaving for Online

Retrieval Evaluation. In WSDM. 245­254.

[6] Filip Radlinski, Madhu Kurup, and orsten Joachims. 2008. How Does Click-

through Data Re ect Retrieval ality?. In CIKM. 43­52.

[7] Ma hew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting

Clicks: Estimating the Click-through Rate for New Ads. In WWW. 521­530.

[8] Anne Schuth, Robert-Jan Bruintjes, Fritjof Buu¨ ner, Joost van Doorn, Carla

Groenland, Harrie Oosterhuis, Cong-Nguyen Tran, Bas Veeling, Jos van der Velde,

Roger Wechsler, David Woudenberg, and Maarten de Rijke. 2015. Probabilistic

Multileave for Online Retrieval Evaluation. In SIGIR. 955­958.

[9] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten

de Rijke. 2014. Multileaved Comparisons for Fast Online Evaluation. In CIKM.

71­80.

952


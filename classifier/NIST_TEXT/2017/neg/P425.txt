Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Evaluating Web Search with a Bejeweled Player Model

Fan Zhang
DCST, Tsinghua University Beijing, China
frankyzf94@gmail.com

Yiqun Liu
DCST, Tsinghua University Beijing, China
yiqunliu@tsinghua.edu.cn

Xin Li
DCST, Tsinghua University Beijing, China x-l08@163.com

Min Zhang
DCST, Tsinghua University Beijing, China
z-m@tsinghua.edu.cn

Yinghui Xu
Alibaba Group Hangzhou, China renji.xyh@taobao.com

Shaoping Ma
DCST, Tsinghua University Beijing, China
msp@tsinghua.edu.cn

ABSTRACT
e design of a Web search evaluation metric is closely related with how the user's interaction process is modeled. Each behavioral model results in a di erent metric used to evaluate search performance. In these models and the user behavior assumptions behind them, when a user ends a search session is one of the prime concerns because it is highly related to both bene t and cost estimation. Existing metric design usually adopts some simpli ed criteria to decide the stopping time point: (1) upper limit for bene t (e.g. RR, AP); (2) upper limit for cost (e.g. Precision@N, DCG@N). However, in many practical search sessions (e.g. exploratory search), the stopping criterion is more complex than the simpli ed case. Analyzing bene t and cost of actual users' search sessions, we
nd that the stopping criteria vary with search tasks and are usually combination e ects of both bene t and cost factors. Inspired by a popular computer game named Bejeweled, we propose a Bejeweled Player Model (BPM) to simulate users' search interaction processes and evaluate their search performances. In the BPM, a user stops when he/she either has found su cient useful information or has no more patience to continue. Given this assumption, a new evaluation framework based on upper limits (either xed or changeable as search proceeds) for both bene t and cost is proposed. We show how to derive a new metric from the framework and demonstrate that it can be adopted to revise traditional metrics like Discounted Cumulative Gain (DCG), Expected Reciprocal Rank (ERR) and Average Precision (AP). To show e ectiveness of the proposed framework, we compare it with a number of existing metrics in terms of correlation between user satisfaction and the metrics based on a dataset that collects users' explicit satisfaction feedbacks and assessors' relevance judgements. Experiment results show that the framework is be er correlated with user satisfaction feedbacks.
Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080841

CCS CONCEPTS
·Information systems  Evaluation of retrieval results;
KEYWORDS
Bene t and Cost; Evaluation Metrics; User Model
1 INTRODUCTION
System-oriented tests and user-oriented studies are currently two complementary approaches to Web search evaluation. A systemoriented test, which is known as Cran eld approach [15], typically develops a set of relevance judgments to compare the quality of ranked lists returned by di erent systems in response to a xed set of queries. On the contrary, a user-oriented study makes use of actual user behaviors during interactive retrieval sessions to measure e ectiveness of systems. For instance, A/B testing and interleaving method [24] are widely used by commercial search engines.
One of the main advantages of user-oriented studies is that they are designed to re ect users' opinions in the realistic search process. However, they are more costly and harder to reproduce than system-oriented tests. On the other hand, although a systemoriented test is inexpensive and reproducible, it cannot capture search activities of users. To tackle this challenge, most Web search evaluation metrics have been built on top of di erent user models. In these models, when user ends a search session is one of the prime concerns because it is highly related to both bene t and cost estimation.
Bene t, also referred to as gain or utility in some researches, has been discussed and introduced in a variety of ways. For example, utility is deemed to be associated with relevance [17], in the sense that one can receive bene t or gain from relevant documents. In terms of relevance, what constitutes it is subject to much interpretation [28]. Cost is considered as temporal e orts or cognitive e orts in processing, reading and understanding documents in many related works [2, 34, 38, 39]. In this paper, we use the terms bene t and cost, although many other equivalent terms are also used in existing researches.
Regarding bene t and cost, underlying user models of existing metrics usually adopt some simpli ed criteria to decide the stopping time point. For instance, user model of Reciprocal Rank (RR) assumes that a user will stop once he/she nds a perfect document.
at is, when users stop only depends on when they get the bene t that they expected. We refer to this kind of stopping criterion as

425

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

upper limit for bene t. In contrast, Precision@N measures the percentage of relevant documents in top-N results, which means that users will scan a ranked list from top to bo om and stop at the N-th document. is kind of stopping criterion is called upper limit for cost. e user models of other metrics like Discounted Cumulative Gain (DCG) [23], Expected Reciprocal Rank (ERR) [12] and Average Precision (AP) are more complex, since user variety and stopping probability distribution at di erent ranked results are considered. Nevertheless, the stopping criteria of these user models still focus on only one aspect of upper limits for bene t or cost, as we show in Section 3.2. However, in many practical search sessions (e.g. exploratory search), the stopping criterion should be usually combination e ects of both bene t and cost factors.
Figure 1 shows two search sessions collected from an experimental user study in [27]. Figure 1(a) shows a session where the user was seeking information on ice-breaking games. In this session, the user issued four queries and clicked four results. e usefulness feedbacks provided by the user indicated that the user judged all the clicked results to be "highly useful". e user was also "highly satis ed" with the session according to the satisfaction feedback.
erefore, we suppose that the user ended this session because he/she has received enough bene ts from clicked results. It was upper limit for bene t that a ected the stopping criterion. On the contrary, in Figure 1(b), we see a session where the user was exploring di erent aspects of a topic "Fixed Gear Bicycle". In this session, the user issued seven queries and clicked eight results, while only one result is thought to be "fairly useful". e last result he clicked was "useless" and then he ended the session in spite that he was "somewhat satis ed". So we assume that the user stopped with no more patience. It seems to be the upper limit for cost that stopped the user.
Comparing the sessions, we nd that the stopping criterion for a search session may be either upper limit for bene t or cost in di erence circumstances. To take this a bit complex criterion into consideration, inspired by a popular computer game named Bejeweled, we propose a Bejeweled Player Model (BPM) to simulate users' search interaction processes and evaluate their search performances. In Action Mode on Bejeweled1, the game starts with the timer bar at the bo om half full, which will start to decrease every second. e player must match gems to add more seconds, with bigger moves ge ing more time. e player will advance to the next level when the bar is full. However, if the bar completely empties, the player lose the game. When no more moves can be made, the game reshu es the gems. Overall, the stopping criterion of the game may be the bar is either empty (i.e. Game Over) or full (i.e. Level Up). Similar to the game, frustration and satisfaction are two nal states of Web search. Many related works [11, 19, 21] have worked on predicting these two states and tell the di erence between them. However, to the best of our knowledge, there is no research that incorporates the di erence between frustration and satisfaction into the design of evaluation framework. Inspired by the game, we assume that frustration o en means that users have invested too much cost and run out their patience (i.e. the bar is empty) while satisfaction is usually due to the ful llment of their bene ts (i.e. the bar is full), so we propose the BPM to describe the stopping criterion for Web search. To be emphasized, unlike the
1h p://bejeweled.wikia.com/wiki/Action mode

Q1

Q2

Q3 Q4

END(5)

C1(4) C2(4)

C3(4)

C4(4)

(a) An example session of upper limit for bene t

Q1 Q2

Q3

Q4

Q5

Q6 Q7

END(3)

C1(1)

C2(1) C3(3)

C4(2) C5(2)

C6(1) C7(2)

C8(1)

(b) An example session of upper limit for cost

Figure 1: Sessions showing di erent stopping criteria. Q: issuing a query; C: clicking a result. e number in brackets a er a click is its usefulness level (4 point scale where 4 means the most useful) and that a er the END is the session's satisfaction level (5 point scale where 5 means the most satis ed).

game where costs and scores are both represented as time in the same bar, we consider users' bene ts and costs separately in the BPM. at is to say, bene ts and costs are accumulated in two bars and there is an upper limit for each bar. e stopping criterion for a session is either the bene t bar is full or the cost bar is full.
As described above, in the BPM, a user stops when he/she either has found su cient useful information or has no more patience to continue. Given this assumption, we propose a new evaluation framework based on upper limits (either xed or evolving as search session proceeds) for both bene t and cost. To apply this framework to Web search evaluation, we show how to derive metrics from it. As mentioned previously, the stopping criteria of user models behind some metrics such as DCG, ERR and AP focus on only one aspect of upper limits for bene t or cost. erefore, we demonstrate that these metrics can be derived from the framework considering onesided case of the stopping criterion. Finally, to show e ectiveness of the proposed framework, we compare it with a number of existing metrics in terms of the correlation between user satisfaction and the metrics.
In summary, we make the following contributions in our work:
· We introduce a Bejeweled Player Model to simulate users' search interaction processes and explain the stopping criterion for search sessions.
· Based on the BPM, we propose a new uni ed framework for Web search evaluation and demonstrate that some existing metrics can be derived from the framework considering one-sided case of the stopping criterion.
· Based on a dataset that collect users' explicit satisfaction feedbacks and assessors' relevance judgements, we show e ectiveness of our proposed framework by comparing it with a number of existing metrics in terms of correlation between user satisfaction and the metrics.
e remainder of this paper is organized as follows. In Section 2, we introduce our proposed evaluation framework based on the BPM. Section 3 shows how to instantiate a metric from the framework and how it can be adapted to existing metrics. en we show e ectiveness of the framework by comparing it with existing metrics in terms of correlation between user satisfaction and the metrics in

426

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

User State (Benefit, Cost, E_Benefit, T_Cost)

Next User State

Interaction

Benefit  E_Benefit or Cost  T_Cost?
Yes Stop

Next No Interaction
Bejeweled Player Model

Metric = Function(Benefit, Cost)

Figure 2: Evaluation framework based on the BPM

Section 4. We review related researches in Section 5 and conclude in Section 6.
2 EVALUATION FRAMEWORK
Figure 2 shows our proposed evaluation framework based on the Bejeweled Player Model (BPM). In a session described by the BPM, a user will interact with the system to satisfy her information need. At each round of interaction, the user will pay some costs and obtain some bene ts simultaneously. As a result, she changes the user state with bene t and cost. Bene ts and costs are accumulated with interactions and we use Bene t and Cost to denote them. e BPM supposes that the bene ts that the user expects to obtain and the costs that she is willing to pay are limited, which are denoted as Expected Bene t (E Bene t) and Tolerated Cost (T Cost). Based on upper limits for both bene t and cost, a user stops when either she has found su cient useful information (reach upper limit for bene t, i.e. Bene f it  E Bene f it) or she has no more patience to continue searching (reach upper limit for cost, i.e. Cost  T Cost). To be emphasized, E Bene t and T Cost may change with interactions, so we incorporate them into the user state with Bene t and Cost as well.
Note that in the BPM introduced above, we mainly focus on upper limits for both bene t and cost that determine when the user stops, rather than speci c interactions. We believe that the BPM is an intrinsic user model which simulates users' search interaction processes and explains the stopping criterion for search sessions. We assume that user satisfaction can be represented by bene t and cost factors. erefore, based on the BPM, we propose an evaluation framework by combining it with a metric function of Bene t and Cost at the end of the session. e function is de ned to evaluate users' search performances. Given the framework, interactions, Bene t and Cost, E Bene t and T Cost, as well as metric function are important components for evaluation. erefore, before we apply this conceptual framework to Web search evaluation, we should talk more about these components.
2.1 Interactions
In our proposed framework, user behaviors are represented as a sequence of interactions. ese interactions are associated with user models. For example, the interactions are examining results one-byone for user models behind most rank-based retrieval metrics such

as DCG [23], ERR [12] and AP. While for click model-based information retrieval metrics [14, 37], the interactions would be examining snippets or click results. Sakai and Dou [33] proposed U-measure to evaluate Web search based on the concept trailtexts, where the interactions would be handling trailtexts of course. ough we do not focus on how to de ne interactions in this paper, we believe that interactions that are closer to user behaviors lead to more e ective metrics.
2.2 Bene t and Cost
As suggested by Azzopardi et al. [5], bene ts and costs are associated with interactions. ey provide a summary of di erent bene ts and costs for various interactions. For most traditional metrics that take examining results as interactions, the bene t of interaction is considered to be associated with relevance. Binary and graded relevance are two most common ways to model it.
As for the cost, most metrics assume that the cost of processing each document is the same. Recently, cost has been considered from a variety of angles. For instance, Smucker and Clarke [34] use the time spent by the user as a proxy for cost and propose Time Biased Gain (TBG). In addition, the cost involved in processing a document in terms of readability and understandability has been explicitly included in other measures (e.g., [2, 38, 39]).
In this paper, we will use bene f itk and costk to denote the bene t and the cost of the k-th interaction, while Bene t and Cost denote the bene ts and costs accumulated on interactions. Unlike metrics that accumulate bene ts or costs with a discount function for di erent interactions (e.g., DCG [23], TBG [34], U-measure [33] for bene t and ERR [12] for cost), we accumulate them with no discounts. Inspired by Cartere e [10] and Mo at et al. [29], we argue that the discount function is the result of user variety and stopping probability distribution at di erent ranked results. is will be discussed further in Section 3.2.
2.3 E Bene t and T Cost
In this paper, we focus on upper limits for bene t and cost that determine when the user stops. E Bene t denotes the bene t that the user expects to obtain (upper limit for bene t), which should be in the same unit with Bene t. For example, Mo at et al. [29] use a parameter T to denote the target number of relevant documents the user wishes to identify. T Cost denotes the cost that the user is willing to pay (upper limit for cost), which should be in the same unit with Cost. For example, in TBG [34], T Cost can be expressed as the time that the user is willing to use for search.
Motivated by [29], we argue that E Bene t and T Cost should be user-sperci c and task-sperci c. at is, for di erent users or di erent tasks, the values of E Bene t and T Cost are di erent. For instance, Broder [9] groups Web queries into informational, navigational and transactional categories. Given that informational queries o en require more information than navigational or transactional queries to satisfy the information need, we assume that the value of E Bene t will be larger for informational queries. On the other hand, for users who have more patience to search, the value of T Cost will be larger as well, just like the persistence parameter p associated with RBP [30]. In [6], Bailey et al. reveal that searchers display substantial individual variation in the numbers of documents and queries they anticipate needing to issue, and there

427

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

are underlying signi cant di erences in these numbers in line with increasing task complexity levels. erefore, we can consider different E Bene f it and T Cost for di erent users and di erent tasks to design more sensitive evaluation metrics.
For further thinking, we suggest that E Bene t and T Cost should also be dynamic. Fuhr [20] proposes the Interactive Probability Ranking Principle (iPRP), an extension to the well known Probability Ranking Principle [32]. When developing the iPRP, one of the main requirements is allowing for the information need to change through the course of interaction. Motivated by this point, we assume that E Bene t and T Cost will also involve with interactions, thus incorporated into the user state with Bene t and Cost.
It is also important to note that interactions are a ected by user states. In [4], Azzopardi et al. examine three theories of Information Seeking and Retrieval. ey enumerate a list of hypotheses about search behavior, and show that these theories make similar predictions. eir work indicates that searchers will change their search behavior based on their context. Nevertheless, in this paper, we focus on independent interactions that users scan ranked results one-by-one from top to bo om.
2.4 Metric Function
is function is de ned to instantiate a metric from the framework and measure user satisfaction when the user stops. In previous metrics, some focus on Bene t (e.g. DCG), some focus on Cost (e.g. ERR) while others focus on A era e U tilit which means Bene t devided by Cost (e.g. AP). erefore, we assume that the metric should be a function of Bene t and Cost. In this paper, however, we do not discuss the form what the metric function should be. We just compare three choices mentioned above, i.e. Bene t, reciprocal Cost, and Bene t devided by Cost.
3 METRICS
In Section 2, we proposed an evaluation framework based on the BPM. Now we will show how to instantiate a metric from the framework and how it can be adapted to existing metrics.
3.1 Metric Based on e BPM
For simplicity, in this paper we take interactions as scaning down ranked results one-by-one before the user stops. is simple interaction process is usually regarded as a cascade assumption [18] and accepted by many existing user behavior models. Given the k-th interaction (i.e. examining the k-th result), its bene t and cost will be bene f itk and costk . In [10], Cartere e compares di erent utility accumulation models that describe how a user accumulates utility in the course of browsing. For simplicity, here we assume Bene t or Cost accumulated with interactions to be the sum of bene f itk or costk . en in the k-th user state a er the k-th interaction, Bene t and Cost can be represented as follow:

k

k

Bene f itk = bene f iti , Costk = costi

(1)

i =1

i =1

As mentioned in Section 2.3, E Bene t and T Cost will change with interactions, thus represented as:

k

E Bene f itk = E Bene f it0 + E Bene f iti

(2)

i =1

k

T Costk = T Cost0 + T Costi

(3)

i =1

where E Bene f it0 and T Cost0 are the initial values when the user starts searching. e increments, E Bene f iti and T Costi , may depend on all the interactions and user states up to the i-th user

state.

Considering the probability that the user stops at rank k:

P (k ) = P ( k ) - P ( k + 1)

(4)

where P ( k ) denotes the probability that the stopping rank is not less than k. In our proposed framework, we assume that a user stops only when Bene f it  E Bene f it or Cost  T Cost. So P ( k ) can be represented as follow:

P (Bene f iti < E Bene f iti , Costi < T Costi : i = 1, ..., k - 1) (5)

en we can get the metric:



M = Function(Bene f itk , Costk )  P (k )

(6)

k =1

Note that di erent users may stop at di erent ranks. erefore, considering user variety, the metric is represented as the expectation of the metric function. For a system-oriented test, given de nitions of bene f iti , costi , E Bene f iti , T Costi and the metric function, we can derive a speci c metric from the framework.

3.2 Existing Metrics

Equation 6 shows how to instantiate a metric from our proposed framework based on the BPM. Given that the stopping criteria of user models behind some metrics such as DCG, ERR and AP focus on only one aspect of upper limits for bene t or cost, we assume that the framework can be applied to derive these metrics when considering their underlying user models. erefore, in this part, we take DCG as an example to show how it can be derived from the framework.
Note that DCG is based on an assumption that the user scans down the ranked list one-by-one and the cost of processing each document is the same, so we de ne costi as one unit for them, thus Costk equals to k units. As described in [23], for DCG, bene f iti is de ned as 2r eli - 1, where reli is the relevance level of documenti . Since DCG assumes that the probability that the lower-ranked documents are examined is smaller, which leads to a discount function 1/lo 2 (i + 1), we explain it with di erent values of T Cost for di erent users. To be speci c, the proportion of users willing to examining at least i results is 1/lo 2 (i + 1). erefore, the probability that T Costk equals to i is:

P (T Costk = i) = 1/lo 2 (i + 1) - 1/lo 2 (i + 2)

(7)

Note that T Costk is independent of k, which means that T Cost is static for each user and will not change with interactions. However,
for DCG, bene t is not limited, which means E Bene t is in nite.

428

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

erefore, the probability that the stopping rank is not less than k can be wri en as:

P ( k ) =P (Costj < T Costj : j = 1, ..., k - 1)



= P (j < i : j = 1, ..., k - 1) · P (T Costj = i)

i =1

(8)


=
i =k

1

1

-

lo 2 (i + 1) lo 2 (i + 2)

=

lo

1 2 (k + 1)

According to the Equation 4, the probability that the user stops at rank k equals to 1/lo 2 (k + 1) - 1/lo 2 (k + 2).
Since DCG focuses on cumulative gain, we de ne the metric function as:

k
Function(Bene f itk , Costk ) = Bene f itk = bene f iti (9)
i =1

en the form of DCG is:


DCG = Function(Bene f itk , Costk )  P (k )
k =1



k





= P (k ) bene f iti = bene f itk P (i)

k =1

i =1

k =1

i =k

(10)


=
k =1

2r elk - 1

 i =k

1

1

-

lo 2 (i + 1) lo 2 (i + 2)

=

 k =1

2r elk lo 2 (k

- +

1 1)

Considering a truncate depth K, it reduced to the form that intro-

duced in [23]. Note that the transition at line 2 in Equation 10 shows

two ways to compute the expectation.

k i =1

bene

f

iti

is the bene

t

cumulated from rank 1 to rank k and P (k ) is the probability that

the user stops at rank k. erefore, the le side of the transition

should be denoted as expected cumulated bene t. On the other hand,

bene f itk is the bene t at rank k, and

 i =k

P (i)

is

equivalent

to

P ( k ). It is the probability that the stopping rank is not less than

k, which also means the probability that the k-th result will be

examined. Consequently, the right side of the transition should be

denoted as cumulated expected bene t.

Similar to DCG, most existing metrics involve summing over

the product of a discount function of ranks and a bene t function

mapping relevance assessments to numeric utility values, i.e.

K

M = bene f it (relk ) · discount (k )

(11)

k =1

Based on the example described above, we can clearly see that the discount (k ) can be regarded as the probability that the k-th document is examined when a user scans a ranked list from top to bo om. Note that the k-th document being examined means that the user does not stop before rank k. So the discount function is the result of user variety and stopping probability distribution at di erent ranked results, as we mentioned in Section 2.2.
Here we should state that besides DCG, most other metrics including ERR, AP, RBP, TBG, U-measure etc. can also be derived from the framework given di erent de nitions of bene f iti , costi ,

E Bene f itk , T Costk and the metric function. Due to space limitations, we will not discuss all of them in detail. Table 1 shows the de nitions for them. For simplicity, here we take interactions as examining results before the user stops, thus the bene t and cost are calculated based on rank. Although some metrics such as TBG and U-measure focus on time or trailtext rather than rank, we think the simpli ed cases can explain underlying user models of these metrics from the perspective of bene t and cost to some extent. However, we nd that these metrics focus on only one aspect of upper limits for bene t or cost. at is, either E Bene t or T Cost is in nite. On the other hand, these metrics do not consider the situation where E Bene t and T Cost change with interactions. In this paper, we mainly focus on these two points to instantiate metrics from the framework to show its e ectiveness.

3.3 Upper Limits for Both Bene t and Cost
In Section 3.2, we show that many existing metrics can be derived from our proposed framework based on the BPM while these metrics focus on only one aspect of upper limits for bene t or cost. However, according to the examples in Figure 1, we nd that the stopping criteria vary with search tasks and are usually combinational e ects of both bene t and cost factors, which indicates that upper limits for both bene t and cost exist simultaneously. In this section, at rst, we consider a simple case where upper limits for both bene t and cost are independent of users and their interactions. In other words, E Bene f itk and T Costk are static values, thus denoted as E B and T C in this section. erefore, Equation 5 can be wri en as:

P ( k ) = P (Bene f iti < E B, Costi < T C : i = 1, ..., k - 1) (12)

Following a number of metrics like DCG, we adopt a graded bene f iti associated with relevance level of documenti and a costi de ned as one unit, which means that:

bene f iti = 2r eli - 1, costi = 1

(13)

Given E B and T C are static, we de ne them as follows:

E B = B  (2r elmax - 1), T C = C  1

(14)

where B and C are positive parameters for Expected Bene t and Tolerated Cost and relmax is the maximum relevance level (e.g. relmax = 3 if a 4 point scale is used). Approximately, B can be regarded as the number of highly relevant documents that a user
expects to nd, while C is the number of documents that the user is willing to examine. ough we do not know the optimal values
of B and C , we suppose that they would be di erent for di erent tasks and compare di erent values of B and C . As we mentioned before, a metric function should be de ned to instantiate a metric
for evaluation. In this paper, the metrics de ned above are called
Static BPM Metrics and denoted as SBPMf (B , C ), where B and C are parameters, while f is the metric function. Referring to the existing metrics, we adopt three forms of metric function, which
are Bene t (B), reciprocal Cost (1/C) and Average Bene t (B/C, i.e.
Bene t devided by Cost), and compare e ectiveness of them.
Given the values of B and C , and the form of metric function, we can calculate the value of a Static BPM Metric for a ranked
list where relevance judgements of top-N results are provided.
Algorithm 1 shows the calculation process for Static BPM Metrics.

429

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: De nitions of di erent components for di erent metrics. See more details in related researches [12, 30, 33, 34] . Note that here we consider TBG and U-measure as o line metrics given their associated parameters.

benef iti

costi

E Benef it

T Cost

P (k )

metric f unction

ERR

P (bene f iti

= 1)

=

2r eli -1 2r elmax

1

1



AP

r eli

1 P (E Bene f it = j ) = 1/m, j  [1, 2, ..., m] 

k -1 j =1

1

-

2r elj -1 2r elmax

2r elk -1 2r elmax

1 m

I

(r

e

lk

)

1/C o s tk Bene f itk /Costk

benef iti costi E Benef it

T Cost

P (k )

metric f unction

DCG RBP TBG U-measure

2r eli - 1 r eli
bT BG (i ) l (si )

1 1

 

P (T

Cost = j) P (T Cost

= =

lo
j)

1 2 (j+1)
= (1 -

p-)pl oj-121,(j

+2) , j

j [1,

 [1, 2, ..., 2, ..., )

)

lo

1 2 (k +1)
(1 -

- lo p )p k

1 -21 (k

+2)

cT BG (i )



|si |



P (T

Cost

 t)

=

F (t )

=

1

-

e -t

ln2 h

,

t



[0, )

P (T Cost = j ) = 1/L, j  [1, 2, ..., L]

F(

k i =1

costi

)

-

F

(

k -1 i =1

c

os

ti

)

min{pos (sk ), L}-min{pos (sk -1 ), L}

L

Note: bT BG (i ) = I (r eli )P (C = 1 |R = 1)P (S = 1 |R = 1), cT BG (i ) = TS + TD (Li )P (C = 1 |R = r eli )

Benef itk Benef itk  (1 - p)
Benef itk Benef itk

Algorithm 1 Calculation process for Static BPM Metrics.
Require: B , C , f (Bene f it, Cost ); relmax , reli : i  [1, N ]. Ensure: SBPMf (B , C )
1: E B  B  (2r elmax - 1), T C  C  1; B  0, C  0, i  1
2: while B < E B and C < T C and i < N do 3: i  i + 1 4: B  B + (2r eli - 1), C  C + 1 5: end while 6: SBPMf (B , C ) = f (B, C )
Regarding Static BPM Metrics, we examine the following research questions based on a test collection (described in Section 4.1) containing user's explicit satisfaction feedbacks and external assessor's relevance judgments:
RQ1 Given proper upper limits for bene t and cost and forms of metric function, will Static BPM Metrics have a be er correlation with user satisfaction feedbacks than existing metrics?
RQ2 Are there di erences in optimal upper limits for benet and cost between di erent taxonomies of queries (e.g.
informational queries and navigational queries)? RQ3 Are there di erences in optimal forms of metric function
between di erent taxonomies of queries?
3.4 Dynamic E Bene t and T Cost
As we mentioned before, upper limits for bene t and cost may evolve with the search interaction processes. erefore, in this section, we will consider a situation where E Bene f itk and T Costk are dynamic values depending on interactions. ey are described by Equation 2 and Equation 3.
Regarding E Bene f itk , we assume that when a user nds a relevant document, she may expect to nd more relevant documents because she becomes more interested in this query topic and wants to learn more. On the contrary, if a user nds an irrelevant document, the number of relevant documents she expects to nd may decrease. Given this assumption, we de ne a simple linear function for the increment E Bene f iti in Equation 2 as follows:
E Bene f iti = hB  (bene f iti - bene f itmedian ) (15)

where hB is a sensitivity parameter for E Bene f it increment (hB >

0). e larger hB is, the more likely it is that a user will stop a er

nding an irrelevant document. As for bene f itmedian , it can be

wri en as:

bene f itmedian = 2r elmedian - 1

(16)

where relmedian is the median relevance level (e.g. relmedian = 1.5 if a 4 point scale is used).

On the other hand, in terms of T Costk , our hypothesis is that when a user nds a relevant document, she may be willing to

examine more documents since the user is more con dent to nd

useful information in the remaining documents. In contrast, if a

user nds an irrelevant document, the number of documents she

is willing to examine may decrease. Given this assumption, we

also de ne a simple linear function for the increment T Costi in Equation 3 as follows:

T Costi = hC  (bene f iti /bene f itmedian - 1) (17)
where hC is a sensitivity parameter for T Cost increment (hC > 0). Similar to hB , the larger hC is, the more likely it is that a user will stop a er nding an irrelevant document.
In addition, the initial values of upper limits for bene t and cost are de ned in the same way as E B and T C:

E B0 = B  (2r elmax - 1), T C0 = C  1

(18)

erefore, compared with Static BPM Metrics, the values of Dynamic BPM Metrics, which are denoted as DBPMf (hB , hC , B , C ), based on dynamic upper limits for bene t and cost de ned above can be calculated by Algorithm 2.
Regarding Dynamic BPM Metrics, we want to answer the following research question:
RQ4 Do our hypotheses about dynamic upper limits for bene t and cost hold? In other words, given proper values of hB and hC , will Dynamic BPM Metrics have a be er correlation with user satisfaction feedbacks than Static BPM Metrics?

4 EXPERIMENTS
Kelly [25] stated that satisfaction can be understood as the ful llment of a speci ed desire or goal. Satisfaction is used to re ect users' actual feelings about the system, thus becoming an important criterion in the user-centric evaluation for Web search engines [1, 22]. To show e ectiveness of our proposed framework and answer the

430

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Algorithm 2 A calculation process for Dynamic BPM Metrics.
Require: hB , hC ; B , C , f (Bene f it, Cost ); relmedian , relmax , reli : i  [1, N ].
Ensure: DBPMf (hB , hC , B , C ) 1: E B  B  (2r elmax - 1), T C  C  1; B  0, C  0, i  1 2: while B < E B and C < T C and i < N do 3: i  i + 1 4: B  B + (2r eli - 1), C  C + 1 5: E B  E B + hB  [(2r eli - 1) - (2r elmedian - 1)] T C  T C + hC  [(2r eli - 1)/(2r elmedian - 1) - 1] 6: end while
7: DBPMf (hB , hC , B , C ) = f (B, C )

Table 2: Statistics of the test collection

#tasks #SERPs #participants #sessions

65

300

98

2685

research questions, we compare Static BPM Metrics and Dynamic BPM Metrics with a number of existing metrics in terms of correlation between user satisfaction and the metrics based on a test collection containing users' explicit satisfaction feedbacks and assessors' relevance judgements. We will brie y introduce the test collection in Section 4.1 and the results of our experiments will be shown and discussed in Section 4.2.
4.1 Test Collection
In our experiment, the test collection is based on experimental user studies conducted in our previous works [13, 26]. In the user studies, each participant was asked to complete 30 search tasks within about an hour. For each task, a er understanding corresponding information need, the participant would be guided to pre-designed search result pages (SERPs) where the query and search results are
xed. e participant was asked to examine the results provided on the SERP and end the search session either if the information need was satis ed or he/she was disappointed with the results. Each time they ended a search session, they were required to provide a ve point scaled satisfaction feedback to the session where 5 means the most satisfactory and 1 means the least. en they would be guided to continue to the next search task. In addition, we invited three professional assessors from a commercial search engine company to label four point scaled relevance scores for all query-result pairs in the experiment. e KAPPA coe cient of their annotation is about 0.7, which can be characterized as a substantial agreement. ere are 65 tasks in total, which contains 27 informational queries and 38 navigational queries. e speci c statistics of the test collection are shown in Table 2.
4.2 Results
Following Chen et al. [13], considering that satisfaction feedback may be quite subjective, we regularize the satisfaction scores by each participant to Z-scores. For each session, given relevance judgements, we can compute the value of di erent metrics. In this paper, we use DCG@10, RBP-0.8 (0.8 is the value of the persistence

Table 3: Pearson's Correlations between Satisfaction Feedbacks and Existing Metrics.

DCG@10 RBP-0.8
AP ERR

informational queries
0.493 0.490 0.400 0.393

navigational queries
0.321 0.323 0.326 0.313

parameter), AP and ERR as representatives of existing metrics. Since we assume that e ectiveness of metrics based on the BPM may be a ected by di erent taxonomies of queries, we compare di erent groups of queries. One group contains 27 informational queries in the test collection, while the other group contains 38 navigational queries.
4.2.1 Static BPM Metrics. In order to examine RQ1, RQ2 and RQ3, rst we compute Pearson's correlations between satisfaction feedbacks and existing metrics. e correlations are shown in Table 3. For informational queries, we can see that DCG@10 and RBP-0.8 have be er correlations with satisfaction than AP and ERR. However, for navigational queries, all these metrics have similar correlations. erefore, we use DCG@10 as a baseline to be compared with metrics based on the BPM in the following experiments. en we compute Pearson's correlations between satisfaction feedbacks and Static BPM Metrics. Given that we should tune parameters B and C for Static BPM Metrics, to avoid over ing and unfair comparison with the baseline, we randomly divide the test collection into two halves. e rst half is the training set which containing 620 sessions of informational queries and 723 sessions of navigational queries for tuning parameters, while the other half is the test set which containing 620 sessions of informational queries and 722 sessions of navigational queries for comparing di erent metrics.
First we try di erent values of parameters B and C and di erent metric functions for Static BPM Metrics on the training set. e results are shown in Table 4. In order to compare two correlation coe cients (rs), we construct a t-statistic to test the signi cance of the di erence between dependent r's [16]. Note that as long as B > C = k, a er examining k documents, Costk is equal to T Costk while Bene f itk is smaller than E Bene f itk . en users will always stop at rank k, which is the same as the case where B = C = k. erefore, we omit all the results for the cases where B > C . In addition, if the form of metric function is reciprocal Cost (i.e. 1/Cost) and B = C = k, the metrics will have the same values (i.e. 1/k) for all sessions. en the correlations between satisfaction feedbacks and metrics will make no sense, thus omi ed as well.
From the results, we can determine the parameters and the metric function. For informational queries, if we de ne metric function as Bene t, when B = 5 and C = 8 or 9, we can get the best correlation (0.520 or 0.521), which is signi cantly larger than DCG@10 (0.482) on the training set. For navigational queries, if we de ne metric function as reciprocal Cost, when B = 1 and C = 5, we can get the best correlation (0.365), which is signi cantly larger than DCG@10 (0.312) on the training set.

431

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 4: Pearson's Correlation between Satisfaction Feedbacks and Static BPM Metrics for informational and navigitional
queries. e rst column is the form of metric function. Numbers in the second column are values of B , while numbers in the second row are values of C . * indicates the di erence of correlation between the Static BPM Metric and DCG@10 is signi cant at p < 0.05 when the Static BPM Metric has a better correlation with Satisfaction.

f (B, C ) Benef it
1 Cost
Benef it Cost

informational queries

navigational queries

1

2

3

4

5

6

7

8

9

10

1

2

3

4

5

6

7

8

9

10

1 0.345 0.338 0.279 0.242 0.170 0.029 -0.033 -0.068 -0.102 -0.122 0.335 0.248 0.166 0.088 -0.019 -0.100 -0.179 -0.198 -0.204 -0.225

2-

0.385 0.398 0.413 0.419 0.374 0.312 0.283

0.214

0.100 -

0.309 0.329 0.296 0.257

0.211 0.086 0.049 0.006 -0.032

3-

-

0.409 0.437 0.468 0.490 0.469 0.481

0.450

0.416 -

-

0.321 0.324 0.319

0.289 0.235 0.188 0.157 0.102

4-

-

-

0.433 0.462 0.490 0.501 0.521

0.510

0.502 -

-

-

0.308 0.322

0.309 0.277 0.254 0.228 0.204

5-

-

-

-

0.454 0.477 0.488 0.520* 0.521* 0.523 -

-

-

-

0.312

0.305 0.284 0.264 0.253 0.238

6-

-

-

-

-

0.469 0.475 0.497

0.502

0.502 -

-

-

-

-

0.299 0.279 0.266 0.258 0.247

7-

-

-

-

-

-

0.475 0.484

0.491

0.488 -

-

-

-

-

-

0.277 0.266 0.258 0.245

8-

-

-

-

-

-

-

0.483

0.478

0.477 -

-

-

-

-

-

-

0.264 0.256 0.246

9-

-

-

-

-

-

-

-

0.475

0.468 -

-

-

-

-

-

-

-

0.254 0.244

10 -

-

-

-

-

-

-

-

-

0.465 -

-

-

-

-

-

-

-

-

0.242

1-

0.313 0.332 0.343 0.353 0.360 0.365 0.365

0.366

0.366 -

0.351 0.355 0.362 0.365* 0.364 0.363 0.363 0.363 0.363

2-

-

0.305 0.329 0.358 0.379 0.393 0.403

0.410

0.415 -

-

0.227 0.250 0.273

0.285 0.293 0.295 0.293 0.293

3-

-

-

0.303 0.326 0.350 0.369 0.385

0.400

0.410 -

-

-

0.182 0.213

0.234 0.251 0.263 0.270 0.275

4-

-

-

-

0.288 0.318 0.332 0.349

0.370

0.386 -

-

-

-

0.134

0.163 0.190 0.206 0.221 0.231

5-

-

-

-

-

0.252 0.293 0.319

0.337

0.355 -

-

-

-

-

0.106 0.119 0.169 0.191 0.207

6-

-

-

-

-

-

0.218 0.257

0.299

0.326 -

-

-

-

-

-

0.102 0.117 0.143 0.164

7-

-

-

-

-

-

-

0.180

0.198

0.222 -

-

-

-

-

-

-

0.061 0.100 0.120

8-

-

-

-

-

-

-

-

0.162

0.158 -

-

-

-

-

-

-

-

0.061 0.100

9-

-

-

-

-

-

-

-

-

0.162 -

-

-

-

-

-

-

-

-

0.061

10 -

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

1 0.345 0.363 0.370 0.378 0.391 0.393 0.392 0.392

0.392

0.391 0.335 0.339 0.353 0.354 0.355

0.355 0.353 0.353 0.353 0.353

2-

0.385 0.397 0.419 0.445 0.463 0.463 0.466

0.463

0.458 -

0.309 0.319 0.315 0.316

0.312 0.299 0.297 0.300 0.299

3-

-

0.409 0.432 0.450 0.465 0.463 0.471

0.466

0.460 -

-

0.321 0.313 0.313

0.302 0.286 0.281 0.280 0.280

4-

-

-

0.433 0.449 0.462 0.459 0.469

0.468

0.465 -

-

-

0.308 0.306

0.292 0.271 0.261 0.257 0.255

5-

-

-

-

0.454 0.465 0.465 0.475

0.473

0.468 -

-

-

-

0.312

0.296 0.272 0.259 0.254 0.252

6-

-

-

-

-

0.469 0.469 0.476

0.473

0.468 -

-

-

-

-

0.299 0.277 0.262 0.253 0.244

7-

-

-

-

-

-

0.475 0.483

0.475

0.464 -

-

-

-

-

-

0.277 0.264 0.254 0.240

8-

-

-

-

-

-

-

0.483

0.475

0.463 -

-

-

-

-

-

-

0.264 0.254 0.242

9-

-

-

-

-

-

-

-

0.475

0.465 -

-

-

-

-

-

-

-

0.254 0.242

10 -

-

-

-

-

-

-

-

-

0.465 -

-

-

-

-

-

-

-

-

0.242

To answer RQ1, we examine correlations with satisfaction on the test set. For informational queries, we set B = 5 and C = 8 and choose Bene t as metric function. e Pearson's correlation between Static BPM Metric and satisfaction is 0.553, which is signi cantly larger than DCG@10 (0.503, p < 0.01). For navigational queries, we set B = 1 and C = 5 and choose reciprocal Cost as metric function. e Pearson's correlation is 0.298, which is smaller than DCG@10 (0.329). Based on these results, we can infer that for informational queries, Static BPM Metrics can have a be er correlation with user satisfaction feedbacks than existing metrics given proper upper limits for bene t and cost and forms of metric function. However, for navigational queries, Static BPM Metrics has poor performance.
In terms of B , C and forms of metric function, we can see that there are di erences between informational queries and navigational queries. For informational queries, Bene t is a good form of metric function, while reciprocal Cost is be er for navigational queries. We suppose that it is because users are usually willing to pay more costs to get more information for informational queries than navigational queries, thus focusing more on Bene t for informational queries and Cost for navigational queries. In fact, for informational queries, 4-6 are proper values of B while 8-10 are proper values of C , which indicates that users may examine the whole result list to nd a number of relevant documents for informational queries. However, for navigational queries, 1 is the best value of B . It suggests that users want to nd just an exactly relevant document for navigational queries, which is consistent with the de nition of navigational queries in [9]. Note that values of C have li le e ect on correlations between satisfaction and metrics. Our explanation is that the result lists for navigational queries are usually not bad. As a result, users o en stop at lower ranks, which are smaller than upper limit for Cost.

Now we can answer RQ2 and RQ3. Based on our results, there are di erences in optimal upper limits and forms of metric function between informational queries and navigational queries. Users are usually willing to pay more costs and expect to obtain more bene ts for informational queries than navigational queries. In other words, users searching for informational queries may have higher upper limits for bene t and cost. On the other hand, since users are willing to pay a lot of costs and expect to obtain a great many bene ts for informational queries, they will focus on bene t rather than cost. However, for navigational queries, what users want to nd is xed, so they will focus more on their costs. ese di erences can guide us to choose proper metric functions and upper limits for di erent queries when we use metrics based on the BPM.
4.2.2 Dynamic BPM Metrics. Regarding RQ4, we compare effectiveness of Dynamic BPM Metrics with Static BPM Metrics. Specifically, we compute Pearson's correlations between satisfaction and Dynamic BPM Metrics. Here we focus on informational queries because our results shown in Section 4.2.1 suggest that Static BPM Metrics have a signi cantly be er correlation with user satisfaction than existing metrics especially for informational queries. Inspired by the results, we de ne metric function as Bene t. Note that here we mainly focus on whether dynamic upper limit for bene t or cost a ects e ectiveness of BPM Metrics, rather than how it works.
erefore, we xed one of them as zero and try di erent values of the other one from 0 to 1 with a step of 0.1. Based on our results, Table 5 shows two suboptimal cases for correlations between satisfaction and Dynamic BPM Metrics. Further analysis of parameters are not discussed and leaved for future work.
First, we examine e ectiveness of dynamic upper limit for bene t based on the case where hB = 0.2 and hC = 0. From the results, we can see that when B = 3 and C = 9, we can get the best correlation (0.552), which is signi cantly larger than the optimal

432

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: Pearson's Correlation between Satisfaction Feedbacks and Dynamic BPM Metrics for informational queries. e rst
column is the form of metric function. Numbers in the second column are values of B , while numbers in the second row are values of C . * (or **) indicates the di erence of correlation between the Dynamic BPM Metric and the Static BPM Metric (B = 5, C = 8) is signi cant at p < 0.05 (or p < 0.01) when the Dynamic BPM Metric has a better correlation with Satisfaction.

f (B, C )

hB = 0.2, hC = 0

hB = 0, hC = 0.3

1

2

3

4

5

6

7

8

9

10

1

2

3

4

5

6

7

8

9

10

1 0.349 0.393 0.384 0.390 0.420 0.400 0.379 0.376

0.372

0.372 0.367 0.330 0.275 0.243 0.160 0.090 0.027 0.012 -0.021 -0.053

2-

0.393 0.414 0.441 0.482 0.512 0.502 0.523

0.502

0.476 -

0.409 0.399 0.391 0.358 0.368 0.341 0.333 0.326 0.246

3-

-

0.414 0.444 0.477 0.521 0.529 0.550** 0.552** 0.543 -

-

0.423 0.434 0.454 0.480 0.477 0.492 0.481 0.453

4-

-

-

0.444 0.469 0.495 0.509 0.542* 0.539

0.541 -

-

-

0.446 0.484 0.498 0.507 0.525 0.526 0.516

Benef it

5 6

-

-

-

-

0.469 -

0.485 0.485

0.492 0.488

0.519 0.497

0.522 0.508

0.518 0.507 -

-

-

-

0.489 -

0.502 0.489

0.522 0.513

0.543 0.532

0.548 0.535

0.540 0.527

7-

-

-

-

-

-

0.488 0.497

0.490

0.485 -

-

-

-

-

-

0.505 0.522 0.523 0.514

8-

-

-

-

-

-

-

0.497

0.490

0.479 -

-

-

-

-

-

-

0.511 0.511 0.502

9-

-

-

-

-

-

-

-

0.490

0.479 -

-

-

-

-

-

-

-

0.501 0.492

10 -

-

-

-

-

-

-

-

-

0.479 -

-

-

-

-

-

-

-

-

0.487

Static BPM Metric (0.537) where B = 5 and C = 8. is may indicate that dynamic upper limit for bene t is e ective to describe some aspects that Static BPM Metrics do not consider. When a user start search, she wants to get some useful information. If the user
nds a relevant document, she may expect to nd more relevant documents. However, if the user nd an irrelevant document, the bene ts she expects may decrease. We explain it with change of task di culty perceived by the user. en, we examine e ectiveness of dynamic upper limit for cost based on the case where hB = 0 and hC = 0.3. ere are no signi cant di erences of correlations between Dynamic BPM Metrics and the optimal Static BPM Metric, which suggests that the costs users are willing to pay may not be a ected by the relevances of documents they have examined before. We think it is due to the fact that in the experimental user studies, queries and SERPs are xed and not allowed to change. Participants are usually willing to examine all the results if needed, thus have relatively static upper limits for cost. Based on the results, our answer for RQ4 is that our hypotheses about dynamic upper limits for bene t and cost partially hold. Considering dynamic upper limit for bene t, e ectiveness of Dynamic BPM Metrics can be improved. However, participants in experimental user studies usually have static upper limits for cost. We may have to leave further analysis in practical user behavior data set for future work.
5 RELATED WORK
In order to evaluate user satisfaction of Web search, many evaluation metrics are designed with di erent user models. In these models, when a user ends a search session is one of the prime concerns because it is highly related to both bene t and cost estimation.
e simple model of RBP [30] assumes that users progress from one result in the ranked list to the next with persistence p and end their examination with probability 1 - p. e cascade model proposed by Craswell [18] assumes that a user views search results from top to bo om and has a certain probability of being satis ed at each position. Once the user is satis ed with a document, he/she terminates the search. Based on this model, ERR [12] de nes the probability that a user is satis ed with a document to be related with relevance of the document. Considering realistic user behavior, some works [14, 37] combine evaluation metrics with click models, and estimate the probability of leaving a search session given the relevance of the clicked document from click logs. In [29], Mo at et al. explore the link between user models and metrics and use a function CM (i) to describe the conditional probability that users proceed to depth i +1 once they have reached depth i in the ranking.

ey analyze di erent forms of CM (i) for di erent user models. However, these models lack insights into factors which a ect when users stop search sessions.
Some theories of search and search behavior are proposed to describe how users interact with search systems. A well known conceptual model of information seeking is the Berry Picker model proposed by Bates [7], which draws an analogy between a searcher and a forager. Based on this model, Information Foraging eory (IFT) [31] predicts how long a forager should stay in a patch before moving on to the next patch. IFT assumes that foragers wish to maximize their gain per unit of time. More recently, Fuhr [20] extendes the PRP [32] to consider a series of interactions in the interactive Probability Ranking Principle (iPRP), which accounts for the different costs and bene ts associated with particular choices when ranking documents. In [8], Birchler and Butler explain how Stigler's theory [35] can be applied to search in order to predict when a user should stop examining results in a ranked list. However, they did not conduct any empirical study to verify whether the theory was consistent with users' actual behavior. en Azzopardi suggests that Production eory [36] could be used to model the search process instead and proposes Search Economic eory (SET) [3] to model ad-hoc topic retrieval. In [4], Azzopardi et al. examine three theories and show that the models are complementary to each other but operate at di erent levels. Given these theories, it is possible to explain why users behave the way they do. However, these theories are not applied to developing Web search evaluation.
6 CONCLUSION AND FUTURE WORK
In summary, in this work, we introduce a Bejeweled Player Model to simulate users' search interaction processes and explain the complex stopping criterion for search sessions. In the BPM, we suppose that a user stops when he/she either has found su cient useful information or has no more patience to continue. Given this assumption, we propose a new evaluation framework based on upper limits for both bene t and cost. en we show how to instantiate a metric from the framework and demonstrate that some existing metrics can be derived from the framework considering one-sided case of the stopping criterion. To show e ectiveness of our proposed framework, we compare Static BPM Metrics and Dynamic BPM Metrics with a number of existing metrics in terms of correlation between user satisfaction and the metrics based on a test collection containing users' explicit satisfaction feedbacks and assessors' relevance judgements. e results show that, given proper upper limits for bene t and cost and forms of metric function, Static

433

Session 4A: Evaluation 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

BPM Metrics and Dynamic BPM Metrics have a be er correlation with user satisfaction feedbacks than existing metrics, especially for informational queries. In addition, there are di erences in optimal upper limits and forms of metric function between informational queries and navigational queries. We also compare e ectiveness of Static BPM Metrics and Dynamic BPM Metrics, and we nd that considering dynamic upper limit for bene t may further improve e ectiveness of BPM Metrics.
Our work has a few limitations: (1) We make some simpli ed assumptions for Static BPM Metrics and Dynamic BPM Metrics. In the future work, we plan to explore more complex situations for them. For instance, we can consider upper limits as latent variables and estimate them with large scale user logs. (2) Our test collection is based on an experimental user study where users examine results on xed SERPs. In fact, there is a natural upper limit for cost (i.e. 10 results for each SERP), which constrains e ectiveness of upper limit for cost in our framework. We would like to use realistic user logs in the future. (3) We only measure correlation with user satisfaction to show e ectiveness of the framework, but the usual comparison in IR is to see how well we can determine the quality of one retrieval system versus another. We will apply the framework to this comparison to see the performance of the metrics.
ACKNOWLEDGMENTS
is work is supported by Natural Science Foundation of China (Grant No. 61622208, 61532011, 61672311) and National Key Basic Research Program (2015CB358700).
REFERENCES
[1] Azzah Al-Maskari, Mark Sanderson, and Paul Clough. 2007. e relationship between IR e ectiveness measures and user satisfaction. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 773­774.
[2] Paavo Arvola, Jaana Keka¨la¨inen, and Marko Junkkari. 2010. Expected reading e ort in focused retrieval evaluation. Information Retrieval 13, 5 (2010), 460­484.
[3] Leif Azzopardi. 2011. e economics in interactive information retrieval. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 15­24.
[4] Leif Azzopardi and Guido Zuccon. 2015. An analysis of theories of search and search behavior. In Proceedings of the 2015 International Conference on e eory of Information Retrieval. ACM, 81­90.
[5] Leif Azzopardi and Guido Zuccon. 2016. An analysis of the cost and bene t of search interactions. In Proceedings of the 2016 ACM on International Conference on the eory of Information Retrieval. ACM, 59­68.
[6] Peter Bailey, Alistair Mo at, Falk Scholer, and Paul omas. 2015. User variability and IR system evaluation. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 625­634.
[7] Marcia J Bates. 1989. e design of browsing and berrypicking techniques for the online search interface. Online review 13, 5 (1989), 407­424.
[8] Urs Birchler and Monika Bu¨tler. 2007. Information economics. Routledge. [9] Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, Vol. 36.
ACM, 3­10. [10] Ben Cartere e. 2011. System e ectiveness, user models, and user utility: a
conceptual framework for investigation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 903­912. [11] Irina Ceaparu, Jonathan Lazar, Katie Bessiere, John Robinson, and Ben Shneiderman. 2004. Determining causes and severity of end-user frustration. International journal of human-computer interaction 17, 3 (2004), 333­356. [12] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management. ACM, 621­630. [13] Ye Chen, Yiqun Liu, Ke Zhou, Meng Wang, Min Zhang, and Shaoping Ma. 2015. Does vertical bring more satisfaction?: Predicting search satisfaction in a heterogeneous environment. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 1581­1590.

[14] Aleksandr Chuklin, Pavel Serdyukov, and Maarten De Rijke. 2013. Click modelbased information retrieval metrics. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, 493­502.
[15] Cyril W Cleverdon, Jack Mills, and Michael Keen. 1966. Factors determining the performance of indexing systems. (1966).
[16] Jacob Cohen, Patricia Cohen, Stephen G West, and Leona S Aiken. 2013. Applied multiple regression/correlation analysis for the behavioral sciences. Routledge.
[17] William S Cooper. 1973. On selecting a measure of retrieval e ectiveness. Journal of the American Society for Information Science 24, 2 (1973), 87­100.
[18] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 87­94.
[19] Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais, and omas White. 2005. Evaluating implicit measures to improve web search. ACM Transactions on Information Systems (TOIS) 23, 2 (2005), 147­168.
[20] Norbert Fuhr. 2008. A probability ranking principle for interactive information retrieval. Information Retrieval 11, 3 (2008), 251­265.
[21] Ahmed Hassan, Ryen W White, Susan T Dumais, and Yi-Min Wang. 2014. Struggling or exploring?: disambiguating long search sessions. In Proceedings of the 7th ACM international conference on Web search and data mining. ACM, 53­62.
[22] Sco B Hu man and Michael Hochster. 2007. How well does result relevance predict session satisfaction?. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 567­574.
[23] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422­446.
[24] orsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 133­142.
[25] Diane Kelly and others. 2009. Methods for evaluating interactive information retrieval systems with users. Foundations and Trends® in Information Retrieval 3, 1­2 (2009), 1­224.
[26] Yiqun Liu, Ye Chen, Jinhui Tang, Jiashen Sun, Min Zhang, Shaoping Ma, and Xuan Zhu. 2015. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 493­502.
[27] Jiaxin Mao, Yiqun Liu, Ke Zhou, Jian-Yun Nie, Jingtao Song, Min Zhang, Shaoping Ma, Jiashen Sun, and Hengliang Luo. 2016. When does Relevance Mean Usefulness and User Satisfaction in Web Search?. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 463­472.
[28] Stefano Mizzaro. 1997. Relevance: e whole history. JASIS 48, 9 (1997), 810­832. [29] Alistair Mo at, Paul omas, and Falk Scholer. 2013. Users versus models:
What observation tells us about e ectiveness metrics. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management. ACM, 659­668. [30] Alistair Mo at and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Transactions on Information Systems (TOIS) 27, 1 (2008), 2. [31] Peter Pirolli and Stuart Card. 1999. Information foraging. Psychological review 106, 4 (1999), 643. [32] Stephen E Robertson. 1977. e probability ranking principle in IR. Journal of documentation 33, 4 (1977), 294­304. [33] Tetsuya Sakai and Zhicheng Dou. 2013. Summaries, ranked retrieval and sessions: A uni ed framework for information access evaluation. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, 473­482. [34] Mark D Smucker and Charles LA Clarke. 2012. Time-based calibration of e ectiveness measures. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. ACM, 95­104. [35] George J Stigler. 1961. e economics of information. Journal of political economy 69, 3 (1961), 213­225. [36] Hal R Varian and Jack Repcheck. 2010. Intermediate microeconomics: a modern approach. Vol. 6. WW Norton & Company New York. [37] Emine Yilmaz, Milad Shokouhi, Nick Craswell, and Stephen Robertson. 2010. Expected browsing utility for web search evaluation. In Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, 1561­1564. [38] Yinglong Zhang, Jin Zhang, Ma hew Lease, and Jacek Gwizdka. 2014. Multidimensional relevance modeling via psychometrics and crowdsourcing. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 435­444. [39] Guido Zuccon. 2016. Understandability biased evaluation for information retrieval. In European Conference on Information Retrieval. Springer, 280­292.

434


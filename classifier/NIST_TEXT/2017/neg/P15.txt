Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Meta-evaluation of Online and O line Web Search Evaluation Metrics

Ye Chen, Ke Zhou, Yiqun Liu, Min Zhang, Shaoping Ma Tsinghua National Laboratory for Information Science and Technology, Department of Computer
Science & Technology, Tsinghua University, Beijing, China University of No ingham, No ingham, U.K. yiqunliu@tsinghua.edu.cn

ABSTRACT
As in most information retrieval (IR) studies, evaluation plays an essential part in Web search research. Both o ine and online evaluation metrics are adopted in measuring the performance of search engines. O ine metrics are usually based on relevance judgments of query-document pairs from assessors while online metrics exploit the user behavior data, such as clicks, collected from search engines to compare search algorithms. Although both types of IR evaluation metrics have achieved success, to what extent can they predict user satisfaction still remains under-investigated. To shed light on this research question, we meta-evaluate a series of existing online and o ine metrics to study how well they infer actual search user satisfaction in di erent search scenarios. We nd that both types of evaluation metrics signi cantly correlate with user satisfaction while they re ect satisfaction from di erent perspectives for di erent search tasks. O ine metrics be er align with user satisfaction in homogeneous search (i.e. ten blue links) whereas online metrics outperform when vertical results are federated. Finally, we also propose to incorporate mouse hover information into existing online evaluation metrics, and empirically show that they be er align with search user satisfaction than click-based online metrics.
KEYWORDS
Search satisfaction, evaluation metrics, online evaluation
1 INTRODUCTION
Search engine evaluation is important in both academic and industrial IR research. e goal of IR researchers is to bulid search engine systems which can satisfy users' information needs. Both o ine and online metrics have been adopted to measure how well the system serves real users. O ine metrics mainly originated from
Corresponding author is work was supported by Natural Science Foundation of China (Grant No. 61622208, 61532011, 61672311), Tsinghua University Initiative Scienti c Research Program(2014Z21032), National Key Basic Research Program (2015CB358700).
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: h p://dx.doi.org/10.1145/3077136.3080804

Cran eld approach [12] and are based on editorial judgments of the relevance of search results. Typical o ine metrics include Average Precision (AP), Normalized Discounted Cumulative Gain (NDCG) and Rank-Biased Precision (RBP) [35]. ese metrics are widely used to measure the quality of ranking algorithms [44] and are of great value in guiding search algorithm designing. However, although they may provide easily interpretable outcomes, o ine search evaluation has encountered two major problems. e rst one lies in that editorial judgments are o en less credible when measuring actual user experience. Recent studies show that assessors' judgments may signi cantly di er from users' assessments [31]. e second problem is that the evaluation results based on o ine metrics can be biased because they are usually generated with a small and incomplete dataset [13].
Rather than relying on o ine metrics with relevance judgments, a popular contrasting approach is to use online metrics based on the simple fact that the interactions between users and search engines re ect the actual users' experiences in a natural usage environment. Such metrics are calculated based on practical users' behavior logs and can give us straightforward descriptions on how users interact with search engines. In addition, it is o en cheap and fast to collect such data in modern search engines, making it particularly easy to scale up online evaluation. Typical online metrics include clickbased metrics such as CTR (click through rate), UCTR [11] (binary value representing click) and PLC [9] (number of clicks divided by the position of the lowest click) as well as dwell time-based metrics such as query dwell time, average of click dwell time [21] and so on. Although easily scalable and arguably more truthful indication of operational IR e ectiveness, online metrics can su er from various biases present in typical search logs. Online behavior of users can be a ected by many factors, with position bias being the most widely recognized e ect, which requires de-biasing when inferring search success. In addition, online metrics may not be as reusable as o ine metrics [42].
Both online and o ine metrics have been widely used to measure search performance in the past years. Nonetheless, they are usually poorly correlated [11] because they measure IR systems from di erent perspectives. Which measures be er re ect the ultimate actual user satisfaction remains an open research question.
erefore, in this work we investigate the relationship between o ine/online metrics and actual user satisfaction, aiming to establish a thorough understanding of the e ectiveness of those metrics in various search scenarios. We meta-evaluate a series of existing evaluation metrics based on two public datasets with more than two

15

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

thousand search sessions. ery-document relevance assessments, users' interaction behaviors and their explicit satisfaction feedback are all included in the datasets, which makes us able to compute most of the widely-used online and o ine metrics. With more than thirty metrics, we calculate Pearson correlation and conduct concordance test [40] to study how well each metric infers actual search user satisfaction. We found that while online and o ine metrics measure users' search experience from di erent perspectives, they generally both signi cantly correlate with actual user satisfaction.
To enable thorough meta-evaluation with di erent information needs, we categorize the search tasks according to two existing taxonomies, with respect to search goal types [5] and cognitive level [3]. We nd that top-weighted o ine metrics correlate extremely well with user satisfaction in navigational search while online metrics perform comparatively be er in informational and transactional search tasks. Inspired by previous studies on federated search [10, 32], we also compare the performance of evaluation metrics in both homogeneous and heterogeneous search environment as users search behaviors as well as satisfaction perception may be a ected by vertical results. We nd that online metrics perform be er than o ine metrics in heterogeneous search environment.
is is probably because o ine metrics mainly rely on relevance assessments while the interaction-based online metrics may be more sensitive to the e ect of vertical results. In addition, inspired by research on users' ne-grained interaction behaviors such as satis ed clicks [51] and mouse hovers [16], we also investigate the di erences between online metrics calculated based on di erent interaction behavior signals (clicks, satis ed clicks, hovers). e results show that online metrics can be er estimate user satisfaction when mouse hover information is incorporated.
Our contributions in this paper are three-folds: (1) We present a thorough meta-evaluation of online/o ine metrics from the perspective of their relationship with user satisfaction for various types of information needs. e results provide insights for both evaluation metrics study and user satisfaction understanding. (2) We investigate the di erences and applicabilities of di erent evaluation metrics in both homogeneous and heterogeneous search environment. We demonstrate that o ine metrics work be er in homogeneous search while online metrics outperform in heterogeneous search environment. (3) We propose to incorporate mouse hover information into existing online evaluation metrics and empirically demonstrate that they correlate be er with user satisfaction.
2 RELATED WORK
Of particular interest to our research is the extensive body of work on (i) meta evaluation of IR metrics, and (ii) search satisfaction.
2.1 Meta Evaluation of IR Metrics
As evaluation serves as an important part in IR-related research, the meta-evaluation of evaluation metrics has also been widely studied in recent years and di erent criteria have been adopted to compare di erent evaluation metrics [33].
One widely-used method is to use "discriminative power" to measure evaluation metrics. Early in 2000, Buckley and Voorhees proposed to use error rate, which is the likely error of concluding "System A is be er than system B", to compare between di erent

metrics [6]. ey also adopted "fuzziness value" to examine "the power of a measure to discriminate among systems". is idea was further formalized to be "discriminative power" by Sakai in 2006 [39]. He pointed out that mildly top-weighted metrics, such as AP, NDCG and RBP(0.95) usually have higher discrimination ratios than those strongly-weighted metrics, such as Prec@5 and RBP(0.5) [41]. In 2010, Yilmaz and Robertson compared AP and NDCG based on their statistical ability to predict outcomes on held-out data [50]. Another criteria to compare metrics is to calculate the correlations between the system orderings generated by di erent metrics. Correlation coe cients such as Kendall's  and Spearman's  are widely used [42]. However, a weakness is that such coe cients introduce the same penalty for the discords at di erent ranking positions, whereas the top positions are usually more important in IR systems. To shed light on these considerations, Yilmaz et al. [49] introduced an AP-based correlation coe cient called ap to achieve a top-weighted emphasis. Webber et al. [48] also proposed Rank Biased Overlap (RBO) to operate over inde nite and non-conjoint rankings. Other evaluation criteria including evaluation based on judgment cost [7, 8, 35, 49], coverage [38], inversions, interpretation and volatility matrix were also proposed in the past few years [33].
While A/B tests [27] and interleaving [23] as well as their variants [30, 43] are also widely-used approaches for search system evaluation [37], we choose user satisfaction as the ground truth for evaluating di erent evaluation metrics because satisfaction re ects users' search experience directly. Since the goal of IR systems is to satisfy users' information needs, it is important to investigate to what extent can existing evaluation metrics measure practical users' search experience. In this paper, we make a deep analysis of how di erent evaluation metrics correlate with user satisfaction. Because satisfaction has been regarded as the gold standard in search performance evaluation, we try to nd metrics which can surrogate satisfaction, and introduce metrics which can be er estimate user experience.
2.2 Search Satisfaction
Search satisfaction has become one of the major concerns in search evaluation studies. e concept of satisfaction was rst proposed by Su et al. [45] and was de ned as "the ful llment of a speci ed desire or goal" by Kelly [25]. To evaluate a search system, satisfaction can be considered as regarding not only to the whole search experience but also to some speci c aspects [46], such as the precision or completeness of search results, response time and so on. Since satisfaction is important for both search engine evaluation and optimization, a number of research studies have tried to quantify user satisfaction in both desktop search [20, 47] and mobile search [26, 28], and in both homogeneous [31] and heterogeneous search environment [10]. In recent years, a number of works (e.g, [21, 22]) have started using the bene t-cost framework to analyze the satisfaction judgement process of users. In this framework, both the bene t factors (document relevance) and cost factors (the e ort users spend on examining search engine result pages (SERPs) and landing pages) are used to estimate satisfaction.
Although satisfaction can be regarded as the gold standard in search performance evaluation, as mentioned, it is not easy to be collected. is makes it urgent to nd a reliable and reusable metric to

16

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

estimate user satisfaction. However, as indicated by recent studies, relevance-based evaluation metrics, such as MAP and nDCG, may not be perfectly correlated with users' search experience [2, 20]. Recently, Mao et al. [34] further studied the relationship between relevance, usefulness and satisfaction and also suggested that traditional system-centric evaluation metrics are not well aligned with user satisfaction.
Di erent from these existing works, we study the relationships between user satisfaction and both o ine and online evaluation metrics. Our work is complementary to those research on satisfaction understanding and prediction, but also provides additional insights from the evaluation measure perspective. We investigate how the existing metrics perform in di erent search scenarios and compare the applicabilities of di erent metrics in homogeneous and heterogeneous search environment. Meanwhile, we also investigate how mouse hover information can be incorporated into online metrics to obtain be er alignment with user satisfaction.
e major contribution of our work lies in that we comprehensively meta-evaluate over thirty most popular IR metrics for many search tasks. e obtained insights can be used for guiding search evaluation practitioners.
3 DATASET AND METHODOLOGY
In this section, we describe the datasets as well as the meta-evaluation methods used throughout this paper. We also perform an analysis of user satisfaction distribution according to the characteristics of the datasets.

3.1 Overview

Our study aims to meta-evaluate di erent metrics based on two datasets which we have made publicly available1. ese two datasets
contain more than 2400 search sessions collected under 56 search
search tasks in total. e detailed statistics are shown in Table 1.

Table 1: Characteristics of Datasets

Dataset #1 Dataset #2

# queries
26 30

# di erent rankings per query
3 610

# users
40 58

# sessions
1038 1397

ese two datasets are generated under the same experimental process which is shown in Figure 1. Each participant completed a series of no more than 30 tasks in the datasets and they were required to perform two warm-up search tasks rst to get familiar with the search process. Before each task, the participant was shown the search query and task explanations (see the card on the top right corner of Figure 1 as an example) rst to avoid ambiguity. A er that, the participant would be guided to search result page where the query is not allowed to change. Each participant was asked to examine the 10 xed search results provided by the system and end the search session either if the search goal was completed or he/she was disappointed with the results. e provided search result lists were pre-crawled from commercial search engines and we made sure that the participant is able to complete the search goal as long as he/she examines all the provided search results. Each time the participant completed a search session, he/she was required to
1 e link of the datasets and detailed data descriptions is h ps://1drv.ms/f/s!AqRbaaorUiT1avvHQdagVv9uOPM

label a satisfaction score to re ect his/her search experience. en they would be guided to continue to the next search task.
Figure 1: Data Collection Procedure Note that no query reformulation are allowed and the number of search results are xed to 10 for the consistency of result sets across users. Such data collection se ings are similar to previous studies such as [36]. Meanwhile, we adopt SERP-level satisfaction rather than session-level satisfaction in this paper because most o ine metrics are designed to measure the quality of only one search result page. While there may be ways to adjust or merge the metrics to measure session-level result quality, the metric adaptation may cause other uncontrollable e ects and is out of the scope of this paper.
e search results in Dataset #1 are all organic search results and the ones in Dataset #2 are mainly federated search results. e vertical results included in Dataset #2 contain various types, including image, encyclopedia, news and download. e combination of Dataset #1 and Dataset #2 is consistent with real-life se ings because not all SERPs provided by commercial search engines contain vertical results. Furthermore, such composition of our datasets also provides the advantage for us to evaluate how metrics perform di erently in homogeneous and heterogeneous search environment (see section 4.4).
In these two datasets, each task is accompanied by several different search result pages provided by several ranking mechanisms with a same pre-speci ed query, which is to ensure that all users saw the same page under the same ranking mechanism. is makes it possible for us to meta-evaluate the performance of di erent evaluation metrics. Both datasets contain the following information for each search session: (1) ery and corresponding task descriptions. (2) Information of ranked search results as shown on SERPs. (3) 4scaled relevance assessments of all search results. (4) 5-scaled user satisfaction annotations. (5) Users' interaction behaviors during the search process, including click-through, mouse hover and dwell time information.
With the rich information provided by the datasets, we can compute most widely-used o ine / online metrics and hence metaevaluate the relationship between these metrics and users' percieved satisfaction scores.
3.2 Search Task Taxonomy
To further evaluate the performance of di erent evaluation Metrics in di erent search scenarios, we classify the search sessions into di erent categories based on the query and corresponding task descriptions provided in the dataset. We organize the search sessions

17

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Examples of Search eries and Corresponding Taxonomies

ery Meizu o cial website
Stramaccioni interview of Lee Sedol yesterday once more
dunk video

Task Description
nd the o cial website of Meizu nd a biographical sketch of Stramaccioni nd the interview of Lee Sedol a er his match against AlphaGo nd the online audition of "yesterday once more" sung by carpenters nd online videos about dunking

Search Goal Navigational Informational Informational Transactional Transactional

Cognitive Level Remember Remember Understand Remember Understand

into the following two most widely-used task taxonomies in this paper:
· Search Goal [5]: is taxonomy classi es search tasks from the perspective of search goals. e search queries are classi ed into navigational queries, informational queries and transactional queries.
· Cognitive Level [3]: is taxonomy is proposed by Anderson and Krathwohl, which identi es six cognitive process dimensions: remember, understand, apply, analyze, evaluate and create.
Table 2 shows an example set of the search tasks and their corresponding taxonomies while Table 3 presents the numbers of search queries / sessions within di erent types of search tasks. Note that we only include a subset of all the task types described in the task taxonomies due to the composition of our dataset. With respect to the cognitive level taxonomy, we only classify search sessions as either "remember" or "understand" since it is di cult and unrealistic to classify the xed 10 result based search sessions into the other four types of search tasks. e data size of the navigational search tasks is comparatively small. Although the meta-evaluation results may be potentially less reliable for this navigational search task, we believe this can still provide useful preliminary insights while the more thorough analysis with more data points are le for future work.

Table 3: Distribution of eries and Sessions of Di erent Types of Search Tasks

eries Sessions
eries Sessions

Navigational 10 400
Remember 31 1325

Informational 23 1047

Transactional 23 988

Understand 25 1110

3.3 Analysis of User Satisfaction

In this section, we try to compare the satisfaction distribution

across di erent search task taxonomies. Inspired by [25], which

says satisfaction judgement may be quite subjective and di erent

users may have di erent opinions, we regularize the satisfaction

scores labelled by each user into Z-scores according to equation (1),

where sati is one particular satisfaction score given by one user and

A (Sat ) is the average of all satisfaction scores he/she labelled.

V ar (Sat ) in equation (1) refers to the variance of the satisfaction

scores of this user.

Z -scorei

=

sati - A (Sat ) V ar (Sat )

(1)

Figure 2 shows the distribution of quintiles in increasing Z-scores based on di erent search task taxonomies. Di erent colors show satisfaction scores from search sessions originated from di erent

(a) Taxonomy I
(b) Taxonomy II
Figure 2: Satisfaction Distribution Based on Di erent Search Task Taxonomies with Di erent intiles task categories. We can see that in all types of search tasks, the general trend is that users tend to give a high satisfaction score in most cases, which indicates that commercial search engines generally provide promising results for these non-long-tailed search tasks.
With respect to the rst taxonomy based on query intent, we can see that users tend to be the least satis ed if they are searching with informational queries because the percentage of sessions with lower Z-scores (less than 60%) is comparatively higher in the informational case (35.4%) than in navigational (29.0%) and transactional (18.9%) cases. Also, the percentage of sessions of the highest 20 percent of Z-scores is extremely low (1.3%) for informational search tasks compared with the other two types of search tasks. is is reasonable because in most navigational and transactional search tasks, users intended to nd a speci c website or information resource, which can o en be satis ed with only one search result. While in the case of informational search tasks, users o en have to read quite a number of search results to get a comprehensive understanding of the information need, which may be more di cult and time consuming.

18

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Total = 0; Correct1 = 0; Correct2 = 0; foreach pair of search sessions (s1, s2) do
Total + +; M = M (s1) - M (s2);  M = M (s1) - M (s2); if ( M × M )> 0 then // M and M positively agree
Correct1 + +; if ( M × M )< 0 then // M and M negatively agree
Correct2 + +; if ( M = 0 and  M = 0) then // M and M agree
Correct1 + +; Correct2 + +; Correct = Max (Correct1, Correct2) Concordance (M, M) = Correct/Total; Algorithm 1: Computing the concordance of metrics M and golden standard metric M (user satisfaction feedback) based on preference agreement.
From the perspective of the second task taxonomy based on task cognitive level, we can see that users give lower satisfaction scores in search tasks which belong to "understand" categories.
is is in line with our expectation because search tasks identi ed as "understand" category are considered to be more di cult than those identi ed to be "remember" tasks. In general, from the results shown in Figure 2, we can see that users perceive di erent levels of satisfaction in di erent search scenarios, which inspires us to study the relationship between di erent evaluation metrics and satisfaction across di erent search task taxonomies.
3.4 Meta-Evaluation Methods
With satisfaction widely regarded as the gold standard of usercentric evaluation metrics, we analyze which metrics can be er re ect user satisfaction based on the datasets described in this section. We do not consider session-based SAT in our work, rather we assume one SERP page interaction, which consists of most of the search sessions. With respect to the meta-evaluation methodology, we use both pearson correlation coe cient [4] and concordance test to compare di erent evaluation metrics. e idea of using concordance test is inspired by [40] and the method is described in Algorithm 1. We use s1 and s2 to denote di erent systems of the same task in our dataset and M (si ) to denote the averaged value of metric M computed on all sessions under si . e gold standard metric M is user satisfaction. Our algorithm di ers from the algorithm used in [40] in that we take the possibility of both positive and negative correlation into consideration. Furthermore, we use strict > and < instead of  and  in the concordance test [40] because loose restrictions cannot work properly for two-valued metrics such as UCTR.
e number of data points for computing pearson correlation and data pairs for concordance test in di erent search scenarios are shown in Table 4. We should note that the size of data points/pairs of navigational search and homogeneous search (due to the limited ranking mechanisms of Dataset #1) is comparatively small, which may lead to the insigni cant results in these two taxonomies.

Table 4: Numbers of data points / pairs for meta-evaluation

All
Navigational Informational Transactional
Remember Understand
Homogeneous Heterogeneous

# data points for Pearson Correlation
291
30 130 131
152 139
78 213

# data pairs for Concordance Test
744
30 348 366
378 366
78 666

We believe our results (shown in Sec. 4) are still informative and evaluation on a larger dataset can be carried out in the future.

4 EXPERIMENTAL RESULTS
In this section, we meta-evaluate the performance of di erent evaluation metrics in di erent search scenarios based on the algorithm described in section 3.4 to obtain thorough insights into how different metrics perform according to di erent information needs. We rst evaluate the performance of o ine metrics and online metrics in section 4.1 and 4.2, respectively. In section 4.3, we take a deep insight into how some main o ine/online metrics infer user satisfaction in di erent search scenarios. Finally, we incorporate mouse hover information into some existing online metrics and demonstrate its e ectiveness in section 4.4.

4.1 Comparison Across O line Metrics

Table 5: Comparison of Pearson Correlations / Concordance between Satisfaction and O line Metrics (* indicates t-test statistical signi cance at p < 0.01 level)

CG DCG@3 DCG@5 DCG@10
AP RBP(0.1) RBP(0.5) RBP(0.8) RBP(0.95)
ERR

Pearson Correlation 0.354* 0.356* 0.411* 0.421* 0.396* 0.389* 0.438*
0.445* 0.384* 0.433*

Concordance 45.8% 61.6%* 65.7%* 65.3%* 60.2%* 66.7%* 66.5%* 65.7%* 63.4%*
66.8%*

Based on the search result relevance assessments provided by the datasets, we compute some widely-used traditional o ine metrics and investigate how they align with user satisfaction, including cumulative gain (CG), discounted cumulative gain (DCG), average precision (AP) and rank-biased precision (RBP). For DCG, we compare the performance of the metric calculated at di erent ranking lengths to investigate the e ect of evaluation depth. For RBP, we compare the metric performance when the persistence parameter p is set as 0.1, 0.5, 0.8 and 0.95, which is suggested to be appropriate for impatient, neutral, patient and extremely patient users [35], respectively. We also include expected reciprocal rank (ERR), which is based on the "cascade" user model and is suggested to be er correlates with click-based metrics compared to DCG and other editorial metrics [9]. e pearson correlation coe cients and concordance

19

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

test results between these metrics and user satisfaction are shown in Table 5. e best pearson correlation and concordance results are bolded.
We can see that ERR achieves the highest concordance with user satisfaction based on the results in Table 5, which is in line with the ndings in [9]. is may be because the "cascade" user model utilized in ERR be er models user behavior that capture satisfaction. RBP(0.8) achieves the highest pearson correlation among all the metrics, which may indicate that a patient (but not extremely patient) rank-biased user model can best describe the characteristic of the tested user group. From the perspective of DCG, we can see that DCG@10 correlates user satisfaction slightly be er than DCG@3 and DCG@5, which probably indicates that metrics calculated based on a longer ranking length can capture more information and may have a be er estimation of user satisfaction. Among all these o ine metrics, CG has the lowest pearson correlation and concordance with user satisfaction. is is due to the fact that DCG, AP as well as ERR are all top-weighted metrics while CG is not. Relevant results placed at top positions may be much more important than those placed at bo om. Futhermore, we can note that the poor performance of CG is especially remarkable in the case of navigational search scenario (shown in Table 7) where one top-ranked relevant result is usually su cient to complete the search goal.
Overall, we can observe that many o ine metrics (DCG, RBP and ERR) have signi cant and moderate correlations (0.4 to 0.6 [14]) with user satisfaction.
4.2 Comparison Across Online Metrics

Table 6: Comparison of Pearson Correlations / Concordance between Satisfaction and Online Metrics (* indicates t-test statistical signi cance at p < 0.01 level)

Pearson Correlation Concordance

UCTR QCTR PCTR@3 PCTR@5 PCTR@10 MaxRR MinRR MeanRR PLC MaxScroll

-0.069 -0.330* 0.043 -0.092 -0.226* 0.095 0.330* 0.266* 0.222*
-0.519*

24.2%* 57.9%* 50.3%* 44.5%* 33.5%* 50.1%*
61.2%* 59.5%* 58.1%* 60.9%*

SumClickDwell AvgClickDwell
eryDwellTime TimeToFirstClick TimeToLastClick DsatClickCount DsatClickRatio

-0.417* -0.109 -0.559* -0.432* -0.504* -0.170* -0.130*

58.9%* 50.9%* 62.6%* 65.6%* 67.3%* 56.0%* 52.3%*

While o ine metrics are especially valuable when evaluating a system in prior to its deployment [13, 34], online metrics have been widely adopted for modern search engines because such metrics are calculated based on the interactions between practical users and systems. Inspired by previous research on metrics meta-evaluation [9, 11, 15, 19], we compare the evaluation performance of some most widely-used online metrics, including:

· Mouse-based (clicks or scroll) metrics ­ UCTR - Binary variable indicating whether there was a click or not in the session (the opposite of abandonment). ­ QCTR - Number of clicks in a session. ­ PCTR - Page click-through rate as de ned in [51]. We use all clicks rather than satis ed clicks to calculate PCTR in Table 6, which is di erent from [51] where only satis ed clicks are used. Comparisons between metrics based on other user interaction signals (e.g. satis ed clicks) are further discussed in section 4.3. ­ MaxRR, MinRR, MeanRR - Respectively maximum, minimum and mean reciprocal ranks of the clicks. Zero if no clicks. ­ PLC - Number of clicks divided by the position of the lowest click. ­ MaxScroll - Maximum of scroll distance.
· Dwelltime-based metrics ­ SumClickDwell, AvgClickDwell - Respectively sum and average of click dwell time in a query. ­ eryDwellTime - ery dwell time. ­ TimeToFirstClick, TimeToLastClick - Time delta between the start of search session and the rst click and last click in the session, respectively. ­ DsatClickCount, DsatClickRatio - Previous studies divide clicks into satis ed clicks and dissatis ed clicks based on various dwell time thresholds [17, 21]. We tested di erent thresholds and choose to de ne clicks with a dwell time <15s as dissatis ed clicks because it performs the best on our dataset. Previous work [21] also analyzed the threshold of 15s to di erentiate satis ed clicks. We calculate the number and ratio of dissatis ed clicks, respectively.
e online metrics we discuss in this section are mostly based on mouse (click and scroll) behaviors and dwell time information, which can be easily computed based on users' behavior logs. ere are also several studies tried to utilize users' behavior information to quantify or predict user satisfaction (e.g. [1, 18]). We do not include these methods in our work because they are more complicated prediction models, rather than the simple and easy-to-interpret evaluation metrics of our interests. Meanwhile, we do not include the session-based metrics discussed in [17, 24] because there are no query reformulations included in the datasets.
e correlations / concordances between the online evaluation metrics and user satisfaction are shown in Table 6, whereas the highest correlation based on each interaction signals are shown in bolded terms. e results in reveal a number of interesting ndings:
(1) In contrast to the positive correlation between o ine metrics and user satisfaction, online metrics generally correlates with user satisfaction negatively. is is reasonable because the o ine metrics measure the quality of search result page based on relevance assessments and users usually tend to feel more satis ed if the search results are of high quality [34]. On the contrary, the interactions signals which online metrics adopted usually re ects search e ort and high search e ort can reduce user satisfaction [10, 21]. MaxScroll also correlates with satisfaction negatively because a longer scroll distance may also indicate more search e ort.

20

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

MaxRR, MinRR and MeanRR compute the reciprocal ranks of the clicked results and hence correlate with satisfaction positively. It is in line with the ndings in previous studies that PLC correlates with satisfaction positively as PLC is regarded as approximately the precision of examined results [9].
(2) e metrics based on click behaviors in general correlates more weakly with user satisfaction, compared with dwelltime-based metrics. is may be because a clicked result does not always necessarily mean a high quality document hence the click-based metrics may fail. Meanwhile, previous studies [37, 43] pointed out that approximately one order of magnitude more online samples are required to match corresponding o ine metrics' reliability, which may also explain the reason of the comparatively poor performance of click-based metrics. In contrast, some metrics based on scroll (MaxScroll) and dwelltime information (SumClickDwell, eryDwellTime and TimeToLastClick ) have stronger (moderate) negative correlation with user satisfaction, which means scrolls and dwelltime information are quite important behavior signals to infer user satisfaction.
(3) Among all these online metrics, TimeToLastClick has the best concordance with user satisfaction. e last click in a search session is usually considered as satis ed click [51] and therefore TimeToLastClick measures the time "wasted" before the user nd a satisfactory document, which may account for the good performance of TimeToLastClick. We can also note that the concordance between TimeToLastClick and user satisfaction is even be er than the o ine metrics, which implies that online metrics can be as available as o ine metrics even without o ine relevance judgments.
eryDwellTime has the strongest (moderate) pearson correlation but relatively low concordance with user satisfaction. is may be because query dwell time has the largest value range among all these metrics, which may take an advantage during the computing process of pearson correlation.
(4) From the perspective of ranking length, we can observe that PCTR@10 correlates user satisfaction be er than PCTR@3 and PCTR@5, which is consistent with the ndings of DCG and further indicates that metrics calculated based on a longer ranking length can be er estimate user satisfaction. We can also note that PCTR has very low concordance with user satisfaction compared with other metrics. is is because there are quite a number of SERPs with the same PCTR metric values in our dataset while there are minor changes within the satisfaction judgements. erefore, this can result in discordance according to Algorithm 1.
(5) It is in line with our expectation that there is almost no correlation between user satisfaction and simple metrics such as UCTR. UCTR also has a very poor concordance with satisfaction, which is because it is a two-valued metric while we require strict < or > in Algorithm 1.
In general, we can observe that several online metrics (MaxScroll, eryDwellTime,TimetoFirstClick and TimeToLastClick) maintain signi cant and moderate correlations (0.4 to 0.6) with satisfaction.
4.3 Online Metrics v.s. O line Metrics
Based on the ndings in section 4.1 and 4.2, we select out some well-behaved metrics and investigate their correlations with user satisfaction in di erent task taxonomies described in section 3.2 to make a more detailed comparison. e results are shown in Table 7.

e highest correlation achieved by o ine metrics/online metrics in each task taxonomy are in bolded terms.
e results in Table 7 show that generally online metrics have as good correlation and concordance as o ine metrics, which further veri es the value of using online metrics in guiding search engine development because they achieve comparatively good performance without external relevance assessments. In most task scenarios, RBP and ERR perform the best among all o ine metrics while TimeToLastClick along with eryDwellTime perform the best among all online metrics, which is consistent with the ndings in section 4.1 and 4.2.
From the search goal-based task taxonomy, we should note that top-weighted o ine metrics correlate quite well with user satisfaction in navigational search tasks, especially for RBP(0.1) which models the search behavior of impatient users (strong pearson correlation, 0.6 to 0.8 [14]). While the concordance test results may be discrete and not so reliable due to the limited number of data pairs, the pearson correlation coe cients are extremely signi cant.
is is because in navigational search scenario, where the user is usually required to reach a particular site [5], high quality results placed at the top positions are especially important. In contrast, online metrics perform slightly worse in navigational tasks than in informational and transactional tasks. is is probably because the search e ort required in navigational search is usually less than that required in informational and transactional search, in which case the online metrics can hardly capture the di erences of user's satisfaction perception.
We do not observe too much di erence of the performance of di erent metrics from the perspective of cognitive level based taxonomy. e dwelltime based online metrics perform slightly be er in "understand" search scenario but the di erence is not remarkable. Such ndings may suggest that users do not behave signi cantly di erent in such two types of search tasks. An analysis in tasks with more deep cognitive levels such as "analyze" and "create" [3] can be carried out in the future.
We also observe the poor concordance of PCTR. is is because the test is conducted within the same task across di erent SERPs generated by di erent ranking mechanisms. e results on di erent SERPs are the same in most cases while the rankings are di erent.
erefore, there might exist many identical metric PCTR values for the two SERPs of many data pairs while users' perceived satisfaction is di erent. According to Algorithm 1, such case will be regarded as discordance, which is the reason for the poor concordance of PCTR. While most of the results in Table 7 are informative, we must admit the comparatively small-scaled dataset in Navigational search scenarios is an inevitable limitation. A small number of data pairs for concordance test may result in discrete and unavailable results. We may need a larger dataset for more robust results in the future.
4.4 Metric Evaluation in Homogeneous /
Heterogeneous Search
With data collected from both homogeneous search environment (Dataset #1) and heterogeneous search environment (Dataset #2), we investigate how di erent metrics perform in di erent search environments as shown in Table 8. e highest correlation achieved by di erent metrics in di erent search scenarios are bolded.

21

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 7: Comparison of Pearson Correlations / Concordance between Satisfaction and O line and Online Metrics in Di erent Search Scenarios(* indicates t-test statistical signi cance at p < 0.01 level)

Search Goal

Cognitive Level

Navigational Informational Transactional

Remember

Understand

CG DCG@10 RBP(0.1) RBP(0.8)
ERR

0.335 / 53.3% 0.543* / 76.7%* 0.653* / 80.0%* 0.566* / 80.0%* 0.625* / 80.0%*

0.405* / 47.1% 0.454* / 64.1%* 0.400* / 66.4%* 0.475* / 65.8%* 0.451* / 66.7%*

0.354* / 44.0%* 0.403* / 65.6%* 0.314* / 65.8%* 0.419* / 64.5%* 0.348* / 65.8%*

0.414* / 44.7%* 0.475* / 67.5%* 0.407* / 68.0%* 0.492* / 66.7%* 0.432* / 68.0%*

0.389* / 47.0%* 0.437* / 63.1%* 0.371* / 65.3%* 0.454* / 64.8%* 0.430* / 65.6%*

QCTR

-0.187 / 53.3% -0.345* / 58.0%* -0.290* / 58.2%* -0.272* / 58.2%* -0.367* / 57.7%*

PCTR@10

-0.135 / 0.0%* -0.358* / 33.9%* 0.018 / 35.8%* -0.005 / 33.6%* -0.392* / 33.3%*

MinRR

0.454* / 73.3%* 0.323* / 61.5%* 0.281* / 59.8%* 0.318* / 59.8%* 0.344* / 62.6%*

PLC

0.408* / 70.0%* 0.195* / 57.2%* 0.181* / 57.9%* 0.242* / 57.7%* 0.213* / 58.5%*

MaxScroll

-0.477* / 73.3%* -0.577* / 59.2%* -0.465* / 61.5%* -0.471* / 63.2%* -0.547* / 58.5%*

eryDwellTime -0.392* / 50.0% -0.600* / 62.1%* -0.479* / 64.2%* -0.485* / 64.6%* -0.572* / 60.7%*

TimeToLastClick -0.351 / 50.0% -0.525* / 68.1%* -0.428* / 68.0%* -0.445* / 67.5%* -0.551 / 67.2%*

DsatClickCount -0.020 / 53.3% -0.187 / 55.7%* -0.145 / 56.6%* -0.134 / 56.1%* -0.223* / 56.0%*

Table 8: Comparison of Pearson Correlations / Concordance between Satisfaction and O line and Online Metrics in Homoge-

neous and Heterogeneous Search Environment(* indicates t-test statistical signi cance at p < 0.01 level)

O ine Metrics

CG DCG@10 RBP(0.1) RBP(0.8)
ERR

homogeneous search

Pearson Correlation Concordance

0.483*

55.1%

0.535*

70.5%*

0.433*

71.8%*

0.535*

71.8%*

0.462*

71.8%*

heterogeneous search

Pearson Correlation Concordance

0.321*

44.7%

0.392*

64.7%*

0.383*

66.1%*

0.418*

65.0%*

0.429*

66.2%*

Online Metrics (Mouse-based)

QCTR PCTR@10
MinRR PLC
MaxScroll

-0.137 -0.062 0.417* 0.366*
-0.510*

55.1% 2.6%* 64.1%* 61.5%* 64.1%*

-0.401* -0.284* 0.325* 0.176*
-0.540*

58.3%* 37.4%* 60.8%* 57.7%* 60.5%*

Online Metrics (Dwelltime-based)

eryDwellTime TimeToLastClick DsatClickCount

-0.224* -0.235* -0.034

57.7% 50.0% 53.8%

-0.637* -0.570* -0.209*

65.0%* 69.4%* 56.3%*

Di erent metrics are categorized into three groups and it is obvious from Table 8 that di erent groups of metrics reveal di erent characteristics in di erent search environments. O ine metrics be er align with user satisfaction in homogeneous search environment while in heterogeneous search tasks, online metrics, especially the dwelltime-based metrics perform much be er. is is reasonable because most existing o ine metrics do not take the e ect of vertical results into consideration during the evaluation process. While both organic and vertical search results can provide relevant information, vertical results are presented in di erent styles and can help satisfy users' information need from various dimensions [10]. O ine metrics are solely based on relevance assessments and do not consider the e ect of vertical results, which may account for their comparatively poor performance in heterogeneous search environment. Online metrics are mainly based on users' interaction behaviors, which may also be sensitive to the existence of vertical results. erefore, the online metrics may be more e ective in the heterogeneous search environment. Dwelltime-based metrics achieves a pearson correlation of around -0.6 (strong) in heterogeneous search while the best o ine metrics is only moderately correlated (around 0.4). In addition, note that since high quality vertical results can provide users su cient information to complete the search goal without a need to click (called "good abandonment" in

previous work [29]), the number of clicks in heterogeneous search may not be su cient and hence may lead to a drop of performance of click-based online metrics. In our datasets, there are on average 2.83 clicks in a homogeneous search session while only 1.81 clicks in a heterogeneous one. is may be the reason that click-based online metrics do not perform so well in heterogeneous search.
4.5 Click and Hover based Online Metrics
Previous research [10, 16] suggested that ne-grained user interaction information can help be er model user behaviors and estimate user satisfaction. With the rich interaction informational provided by the datasets, we further investigate the performance of online metrics calculated based on the following four types of user interaction signals:
All click-based: is group of evaluation metrics are calculated based on all clicks in a search session, which is the same as the metrics used in the previous sections.
Satis ed click-based: Previous studies pointed out that sometimes clicks do not imply the high relevance of a result document in web search. is is because although the result snippet may appear to be relevant and a ractive, the landing page may be of low quality. For example, a click is de ned as a satis ed click if the user spent 30 seconds or more reading the clicked document or if it was the last click in the search session [51]. Satis ed click is widely

22

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

regarded as the signal of a relevant document. For this group of metrics, we use satis ed clicks to replace the all clicks used in "all click-based" metrics.
Hover-based: Hovers are also regarded to be important behavior signal because there have been various types of results which do not require a click to provide users with necessary information [10, 29]. We use hovers to replace the all clicks used in "all click-based" metrics to achieve the "hover-based" metrics.
Click and Hover-based: We combine the click and hover information in this group of evaluation metrics. If a search results is either clicked or hovered on, then this result will be regarded as "clicked" as in the calculation process of "all click-based" metrics. In this way, we get the "click and hover-based" metrics.
Figure 3: Percentage of all search sessions with clicks/hovers at di erent result positions
We use su x " ac", " sc", " h" and " ch" to represent the "all click-based", "satis ed click-based", "hover-based" and "click and hover-based" metrics, respectively. We choose MinRR and PLC as examples because they correlate with user satisfaction be er than other click-based metrics based on the ndings in previous sections.
e correlations between user satisfaction and metrics computed based on di erent information signals are shown in Table 9. e best correlation / concordance achieved by each metric in di erent search scenarios are bolded.
We can see from the results that in almost all search scenarios, both of these two click-based metrics can be er estimate user satisfaction when hover information is incorporated. is is probably because in today's search engine, various types of results such as instant answers, verticals and even result snippets contain su cient information to satisfy the users, which sometimes makes clicks unnecessary. In such case, hovers can help capture more information than clicks. It is not surprising that the performance of MinRR h and MinRR ch are the same because in most cases the clicked results are usually a subset of hovered results. Furthermore, we can see that the hover information is especially e ective in heterogeneous search according to the performance of MinRR. When hover information is incorporated, the pearson correlation coe cient improves by 0.169(from 0.325 to 0.494) in heterogeneous search and only 0.025(from 0.417 to 0.442) in homogeneous search. e improvement of concordance test result is also larger in heterogeneous search environment. is is reasonable because sometimes users can accomplish their search tasks by interacting with the vertical results on heterogeneous SERPs. An example of the distribution differences between hovers and clicks are shown in Figure 3. For each

rank position of search results, we compute the percentage of sessions with clicks and hovers respectively. It is apparent that there are more hovers than clicks in all positions, which may further help con rm that clicks solely may not be su cient to capture users' interaction information. ere appears to be at least 27% sessions with hovers in all positions while only the rst two positions have a probability of more than 27% to be clicked. In this way, hovers may contain much more valuable information than clicks. From the perspective of MinRR and PLC, a metric which combines click and hover information may be the most reliable because neither click nor hover information alone (PLC h) can achieve the best correlation with satisfaction.
5 CONCLUSIONS
Search engine evaluation is essential in both academic and industrial IR research. Both o ine and online evaluation metrics are adopted to measure the performance of search engines. While search satisfaction is widely regarded as the gold standard in search performance evaluation, the relationship between di erent evaluation metrics and satisfaction remains under-investigated.
In this work, we meta-evaluate the performance of di erent o ine/online evaluation metrics based on two datasets. We investigate how di erent metrics align with user satisfaction in di erent search scenarios using both pearson correlation and concordance test. We nd that di erent types of evaluation metrics estimate user satisfaction from di erent perspectives. O ine metrics work be er in homogeneous search environment while online metrics are more consistent with user satisfaction in heterogeneous search environment. We further compare the e ectiveness of metrics calculated based on di erent user interaction signals. We propose to incorporate hover information into traditional click-based online metrics because they can help be er estimate user satisfaction.
ere are still some limitations of our work which we would like to list as our future work directions. Due to the nature of the utilized laboratory-based datasets, compared to the commercial search engine se ings, online metrics are calculated based on relatively small-scale search sessions (from which our conclusions are drawn). Meanwhile, our datasets are based on a xed search result page and no ability to reformulate the query, which may also a ect users' interaction behaviors. While metrics calculated based on multi-query sessions can be developed, it will be another challenging research question. Our current aim is to meta-evaluate all the metrics on a single search page level, and we leave the session-level or even task-level evaluation as future work. Finally, all the participants within the datasets are undergraduate students. We think this may help reduce potential distractions and make the collected data more consistent. However, such speci c age distribution may also cause potential bias. e study of large-scale online/o ine metrics comparison, online metric sensitivity and more evaluation measures (such as interleaving) are le for future work.
REFERENCES
[1] Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene Agichtein. 2011. Find it if you can: a game for modeling di erent types of web search success using interaction data. In SIGIR'11. ACM, 345­354.
[2] Azzah Al-Maskari, Mark Sanderson, and Paul Clough. 2007. e relationship between IR e ectiveness measures and user satisfaction. In SIGIR'07. ACM, 773­ 774.
[3] Lorin W Anderson, David R Krathwohl, and Benjamin Samuel Bloom. 2001. A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of

23

Session 1A: Evaluation 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 9: Comparison of Pearson Correlations / Concordance between Satisfaction and Online Metrics Based on Di erent User Interaction Signals (* indicates t-test statistical signi cance at p < 0.01 level)

Search Goal

Cognitive Level

Search Environment

Navigational Informational Transactional

Remember

Understand Homogeneous Heterogeneous

MinRR ac MinRR sc MinRR h MinRR ch

0.454* / 73.3%* 0.442* / 76.7%* 0.464* / 70.0% 0.464* / 70.0%

0.323* / 61.5%* 0.322* / 62.6%* 0.474* / 61.8%* 0.474* / 61.8%*

0.281* / 59.8%* 0.209* / 58.2%* 0.440* / 66.7%* 0.440* / 66.7%*

0.318* / 59.8%* 0.254* / 58.7%* 0.462* / 67.5%* 0.462* / 67.5%*

0.344* / 62.6%* 0.339* / 63.4%* 0.486* / 61.5%* 0.486* / 61.5%*

0.417*/64.1%* 0.385*/67.9%* 0.442*/65.4%* 0.442*/65.4%*

0.325*/60.8%* 0.278*/60.2%* 0.494*/64.4%* 0.494*/64.4%*

PLC ac PLC sc PLC h PLC ch

0.408* / 70.0%* 0.416* / 73.3%* 0.302 / 56.7% 0.444 / 80.0%*

0.195* / 57.2%* 0.198* / 56.9%* 0.113 / 48.0% 0.262* / 59.2%*

0.181* / 57.9%* 0.119* / 57.1%* 0.176 / 46.4% 0.302* / 61.7%*

0.242* / 57.7%* 0.190* / 57.7%* 0.198 / 47.6% 0.347* / 63.8%*

0.213* / 58.5%* 0.215* / 58.5%* 0.108 / 47.5% 0.271* / 58.7%*

0.366*/61.5% 0.360*/67.9%* 0.179/51.3% 0.429*/69.2%*

0.176*/57.7%* 0.139/57.7%* 0.142/47/1% 0.265*/60.4%*

educational objectives. Allyn & Bacon. [4] Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson
correlation coe cient. In Noise reduction in speech processing. Springer, 1­4. [5] Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, Vol. 36.
ACM, 3­10. [6] Chris Buckley and Ellen M Voorhees. 2000. Evaluating evaluation measure
stability. In SIGIR'00. ACM, 33­40. [7] Stefan Bu¨ cher, Charles LA Clarke, Peter CK Yeung, and Ian Soboro . 2007.
Reliable information retrieval evaluation with incomplete and biased judgements. In SIGIR'07. ACM, 63­70. [8] Ben Cartere e, Evangelos Kanoulas, and Emine Yilmaz. 2010. Low cost evaluation in information retrieval. In SIGIR'10. ACM, 903­903. [9] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected reciprocal rank for graded relevance. In CIKM'09. ACM, 621­630. [10] Ye Chen, Yiqun Liu, Ke Zhou, Meng Wang, Min Zhang, and Shaoping Ma. 2015. Does Vertical Bring more Satisfaction?: Predicting Search Satisfaction in a Heterogeneous Environment. In CIKM'15. ACM, 1581­1590. [11] Aleksandr Chuklin, Pavel Serdyukov, and Maarten De Rijke. 2013. Click modelbased information retrieval metrics. In SIGIR'13. ACM, 493­502. [12] Cyril Cleverdon, Jack Mills, and Michael Keen. 1966. FACTORS DETERMINING THE PERFORMANCE OF INDEXING SYSTEMS VOLUME 1. DESIGN. (1966). [13] Alex Deng and Xiaolin Shi. 2016. Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned. In SIKDD'16. ACM. [14] Geo rey Evans, Anthony Heath, and Mansur Lalljee. 1996. Measuring le -right and libertarian-authoritarian values in the British electorate. British Journal of Sociology (1996), 93­112. [15] Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais, and omas White. 2005. Evaluating implicit measures to improve web search. TOIS'05 23, 2 (2005), 147­168. [16] Qi Guo, Dmitry Lagun, and Eugene Agichtein. 2012. Predicting web search success with ne-grained interaction data. In CIKM'12. ACM, 2050­2054. [17] Ahmed Hassan, Xiaolin Shi, Nick Craswell, and Bill Ramsey. 2013. Beyond clicks: query reformulation as a predictor of search satisfaction. In CIKM'13. ACM, 2019­2028. [18] Ahmed Hassan, Yang Song, and Li-wei He. 2011. A task level metric for measuring web search satisfaction and its application on improving relevance estimation. In CIKM'11. ACM, 125­134. [19] Katja Hofmann, Lihong Li, Filip Radlinski, and others. 2016. Online evaluation for information retrieval. Foundations and Trends® in Information Retrieval 10, 1 (2016), 1­117. [20] Sco B Hu man and Michael Hochster. 2007. How well does result relevance predict session satisfaction?. In SIGIR'07. ACM, 567­574. [21] Jiepu Jiang, Ahmed Hassan Awadallah, Xiaolin Shi, and Ryen W White. 2015. Understanding and predicting graded search satisfaction. In WSDM'15. ACM, 57­66. [22] Jiepu Jiang, Daqing He, and James Allan. 2014. Searching, browsing, and clicking in a search session: changes in user behavior by task and over time. In SIGIR'14. ACM, 607­616. [23] orsten Joachims. 2002. Optimizing search engines using clickthrough data. In SIGKDD'02. ACM, 133­142. [24] Evangelos Kanoulas, Ben Cartere e, Paul D Clough, and Mark Sanderson. 2011. Evaluating multi-query sessions. In SIGIR'11. ACM, 1053­1062. [25] Diane Kelly. 2009. Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Information Retrieval 3, 1 2 (2009), 1­224. [26] Julia Kiseleva, Kyle Williams, Jiepu Jiang, Ahmed Hassan Awadallah, Imed Zitouni, Aidan C Crook, and Tasos Anastasakos. 2016. Predicting User Satisfaction with Intelligent Assistants. In SIGIR'16. 495­505. [27] Ron Kohavi, Roger Longbotham, Dan Sommer eld, and Randal M Henne. 2009. Controlled experiments on the web: survey and practical guide. Data mining

and knowledge discovery 18, 1 (2009), 140­181. [28] Dmitry Lagun, Chih-Hung Hsieh, Dale Webster, and Vidhya Navalpakkam. 2014.
Towards be er measurement of a ention and satisfaction in mobile search. In SIGIR'14. ACM, 113­122. [29] Jane Li, Sco Hu man, and Akihito Tokuda. 2009. Good abandonment in mobile and PC internet search. In SIGIR'09. ACM, 43­50. [30] Lihong Li, Jin Young Kim, and Imed Zitouni. 2015. Toward predicting the outcome of an A/B experiment for search relevance. In WSDM'15. ACM, 37­46. [31] Yiqun Liu, Ye Chen, Jinhui Tang, Jiashen Sun, Min Zhang, Shaoping Ma, and Xuan Zhu. 2015. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In SIGIR'15. ACM, 493­502. [32] Zeyang Liu, Yiqun Liu, Ke Zhou, Min Zhang, and Shaoping Ma. 2015. In uence of vertical result in web search examination. In SIGIR'15. ACM, 193­202. [33] Xiaolu Lu, Alistair Mo at, and J Shane Culpepper. 2016. e e ect of pooling and evaluation depth on IR metrics. Information Retrieval Journal 19, 4 (2016), 416­445. [34] Jiaxin Mao, Yiqun Liu, Ke Zhou, Jian-Yun Nie, Jingtao Song, Min Zhang, Shaoping Ma, Jiashen Sun, and Hengliang Luo. 2016. When does Relevance Mean Usefulness and User Satisfaction in Web Search?. In SIGIR'16. ACM. [35] Alistair Mo at and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. TOIS'08 27, 1 (2008), 2. [36] Vidhya Navalpakkam, LaDawn Jentzsch, Rory Sayres, Sujith Ravi, Amr Ahmed, and Alex Smola. 2013. Measurement and modeling of eye-mouse behavior in the presence of nonlinear page layouts. In WWW'13. ACM, 953­964. [37] Filip Radlinski and Nick Craswell. 2010. Comparing the sensitivity of information retrieval metrics. In SIGIR'10. ACM, 667­674. [38] Sri Devi Ravana and Alistair Mo at. 2010. Score estimation, incomplete judgments, and signi cance testing in IR evaluation. In AIRS'10. Springer, 97­109. [39] Tetsuya Sakai. 2006. Evaluating evaluation metrics based on the bootstrap. In SIGIR'06. ACM, 525­532. [40] Tetsuya Sakai. 2013. How intuitive are diversi ed search metrics? Concordance test results for the diversity U-measures. In AIRS'13. Springer, 13­24. [41] Tetsuya Sakai and Noriko Kando. 2008. On information retrieval metrics designed for evaluation with incomplete relevance assessments. Information Retrieval 11, 5 (2008), 447­470. [42] Mark Sanderson. 2010. Test collection based evaluation of information retrieval systems. Now Publishers Inc. [43] Anne Schuth, Katja Hofmann, and Filip Radlinski. 2015. Predicting search satisfaction metrics with interleaved comparisons. In SIGIR'15. ACM, 463­472. [44] Hinrich Schu¨tze. 2008. Introduction to Information Retrieval. In Proceedings of the international communication of association for computing machinery conference. [45] Louise T Su. 1992. Evaluation measures for interactive information retrieval. Information Processing & Management 28, 4 (1992), 503­516. [46] Louise T Su. 2003. A comprehensive and systematic model of user evaluation of Web search engines: II. An evaluation by undergraduates. Journal of the American Society for Information Science and Technology 54, 13 (2003), 1193­1223. [47] Hongning Wang, Yang Song, Ming-Wei Chang, Xiaodong He, Ahmed Hassan, and Ryen W White. 2014. Modeling action-level satisfaction for search task satisfaction prediction. In SIGIR'14. ACM, 123­132. [48] William Webber, Alistair Mo at, and Justin Zobel. 2010. A similarity measure for inde nite rankings. TOIS'10 28, 4 (2010), 20. [49] Emine Yilmaz, Evangelos Kanoulas, and Javed A Aslam. 2008. A simple and e cient sampling method for estimating AP and NDCG. In SIGIR'08. ACM, 603­610. [50] Emine Yilmaz and Stephen Robertson. 2010. On the choice of e ectiveness measures for learning to rank. Information Retrieval 13, 3 (2010), 271­290. [51] Masrour Zoghi, Toma´s Tunys, Lihong Li, Damien Jose, Junyan Chen, Chun Ming Chin, and Maarten de Rijke. 2016. Click-based Hot Fixes for Underperforming Torso eries. SIGIR.

24


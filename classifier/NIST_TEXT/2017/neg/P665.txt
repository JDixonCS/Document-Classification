Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

The Utility and Privacy E ects of a Click

Rachid Guerraoui
EPFL rachid.guerraoui@ep .ch

Anne-Marie Kermarrec
Inria Anne-Marie.kermarrec@inria.fr

Mahsa Taziki
EPFL mahsa.taziki@ep .ch

ABSTRACT
Recommenders are becoming one of the main ways to navigate the Internet. They recommend appropriate items to users based on their clicks, i.e., likes, ratings, purchases, etc. These clicks are key to providing relevant recommendations and, in this sense, have a signi cant utility. Since clicks re ect the preferences of users, they also raise privacy concerns. At rst glance, there seems to be an inherent trade-o between the utility and privacy e ects of a click. Nevertheless, a closer look reveals that the situation is more subtle: some clicks do improve utility without compromising privacy, whereas others decrease utility while hampering privacy.
In this paper, for the rst time, we propose a way to quantify the exact utility and privacy e ects of each user click. More speci cally, we show how to compute the privacy e ect (disclosure risk) of a click using an information-theoretic approach, as well as its utility, using a commonality-based approach. We determine precisely when utility and privacy are antagonist and when they are not. To illustrate our metrics, we apply them to recommendation traces from Movielens and Jester datasets. We show, for instance, that, considering the Movielens dataset, 5.94% of the clicks improve the recommender utility without loss of privacy, whereas 16.43% of the clicks induce a high privacy risk without any utility gain.
An appealing application of our metrics is what we call a click-advisor, a visual user-aware clicking platform that helps users decide whether it is actually worth clicking on an item or not (after evaluating its potential utility and privacy e ects using our techniques). Using a game-theoretic approach, we evaluate several user clicking strategies. We highlight in particular what we de ne as a smart strategy, leading to a Nash equilibrium, where every user reaches the maximum possible privacy while preserving the average overall recommender utility for all users (with respect to the case where user clicks are based solely on their genuine preferences, i.e., without consulting the click-advisor).
1 INTRODUCTION
The growth of data available online makes it di cult for individuals to extract information relevant to their interests. Recommenders do the job for them: they build pro les [35] representing user interests, and recommend items to users based on those pro les [23], typically using collaborative ltering (CF) schemes [38].
The pro les of users are derived from their clicks, e.g., their
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080783

ratings in terms of likes (or dislikes) [35], their purchases, the pages they spend time on, etc. On the one hand, the clicks have an important e ect on the recommender utility for a user. On the other hand, clicks may also disclose private information about users [34].1 At rst glance, a user might face a dilemma: To click or not to click? The click could improve the utility of a recommender for that speci c user, yet might also disclose private information. But is there really always a trade-o between utility and privacy?
Consider as a rst illustration, the case of Alice, a user who clicks on "Game of thrones". By doing so, Alice improves utility by helping the recommender nd similar users to her. Assuming indeed that a large number of users have watched "Game of thrones", Alice's click makes her also less distinguishable among those users (than before she clicked). This click improves her privacy. There is no trade-o in this case between utility and privacy. If Alice instead had clicked on an esoteric movie, she would have revealed a lot. A curious user (an attacker), knowing through the item pro les that only one user liked that esoteric movie, could eventually deduce the entire pro le of Alice. (The curious user could, through a KNN attack, create fake pro les containing the esoteric movie and would be recommended the entire pro le of Alice.). The motivation of this work is to determine exactly when a click induces a trade-o and when it does not, by precisely quantifying the e ects of every click on both utility and privacy.
In this paper, we compute the e ect of a click on utility by introducing the notion of commonality of a user pro le, i.e., representing through a number how close the taste of a user is to that of other users (which helps a recommender suggest relevant items that are likely to match the user's preferences). This notion captures precision, the classical well-known measure of the quality of recommenders [32]. Whereas the idea of precision has been so far considered as an empirical measure of the utility of a recommender for all the users, we compute commonality, theoretically, and for every individual user (in a user-centric manner).2 The di erence between the commonality of a user pro le, before and after the click, is what we de ne as the utility of the click.
We compute the privacy e ect of a click through the concept of disclosure degree of a user pro le, using an information-theoretic approach. The disclosure degree corresponds to the amount of information stored in a user pro le, also known as entropy [15]. Roughly speaking, the larger the amount of information in a user pro le, the higher the disclosure degree of the user pro le. If the disclosure degree of a user pro le is low, then the user is not easily distinguishable from others. We capture the disclosure risk of a click, and hence its privacy e ect, as the di erence between the
1This is without even considering the recommender itself as a threat, but only other curious users who could deduce other pro les through what is recommended to them. 2It is important to note at this point that two user pro les might be very di erent, and yet might have the same commonality, basically meaning that they could be recommended the same number of relevant items but not necessarily the same items.

665

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

i1 i2 i3 i4

u1  A 

u2

 f

u3

 B

u4 

C

Table 1: Clicks of Users

disclosure degree of a user pro le before and after the click. Inter-
estingly, we prove that after user u's click on item i, the disclosure degree of u is never increased by the clicks of other users on i.3

To illustrate our notions of utility and privacy e ects of the clicks

beyond Alice's example above, consider the example depicted in

Table 1, involving 4 items and 4 users. (We will use this example

throughout the paper.) A bold style (resp. an outlined style) depicts

a like (resp. dislike) click on an item in the corresponding (user)

row and (item) column, respectively. The check-marks and cross-

marks represent the clicks already performed by the users and the

capital letters represent the following three clicks that could be

performed by the users:
A denotes a click by u1 to dislike i2. A improves the recommender utility for u1 as A indicates that the preference of u1 is close to that of u3. Hence, the recommender can suggest relevant items to u1 after A (e.g., i4). However, A compromises the privacy for u1 in the sense that u1 becomes more distinguishable among other users after A. Indeed, the group of users who dislike i2 after A (u1 and u3) is smaller than the group of users who did not click on i2 before A (u1, u2 and u4); A induces a utility-privacy trade-o .
B denotes a click to like i3 by u3. B improves the privacy for u3 in the sense that after this click, by knowing the preference of u3 for i3, u3 becomes indistinguishable from a group of other users who liked i3 (this group consists of two users out of the total of three users, other than u3). Moreover, B helps the recommender
nd relevant items to u3 because i3 is similar to i4 and i1, and user u3 has not clicked on i1 yet; the recommender can now propose i1 to u3. Hence, B improves both utility and privacy for u3.
C is a click by u4 to dislike i3 that neither improves privacy for u4 nor helps the recommender suggest relevant items to u4; before C, the recommender could gure out that the preference of u4 is close to u1 (neither to u2 nor u3). However, C indicates that the preference of u4 toward i3 is also opposite to u1. Hence, C does not improve the recommender utility for u4; worst, C compromises the privacy for u4.
Clearly, this example contradicts the traditional belief [29] of

an inherent trade-o between utility and privacy, meaning that a

user necessarily improves recommendation utility at the expense

of compromising privacy (or vice versa). There are clicks that im-

prove utility without decreasing privacy and, at the other extreme,

there are clicks that hamper privacy without improving utility.
An interesting application of our work is what we call the clickadvisor, a virtual platform enabling users to decide whether or not

to actually click on an item. The process behind a click-advisor is

3 Three remarks are in order here. First, our notion of disclosure degree is a privacy measure of a user pro le, unlike k-anonymity [39] and di erential privacy [12], which are privacy measures of a dataset and an algorithm, respectively. In Section 6, we show the extend to which our notion of disclosure degree is correlated to di erential privacy and k-anonymity. Second, (just like commonality) the disclosure degree of a user pro le depends on other pro les. The fact that Bob is the only one to click on his esoteric movie is what make his disclosure degree high. Third, a low disclosure degree conveys a protection against possible attacks of curious users, but not against a recommender, which is trusted.

as follows: a user (a) pre-clicks on an item, (b) gets a quick feedback (in constant time, i.e. O (1), as shwon in the paper) on the utility and privacy e ects of that pre-click and then (c) decides to con rm, cancel, or even change the pre-click.
We use a game-theoretic approach to explore several user clicking strategies. For example, a user may follow a careful strategy to con rm a pre-click i the pre-click does not hamper privacy without improving utility (and to cancel the pre-click otherwise). We highlight in particular a Nash equilibrium strategy, which we call smart. If all users follow the smart strategy, they minimize their disclosure degrees (maximize their privacy) while ensuring that the expected value of the overall recommender utility for all users remains the same as the case without consulting the click-advisor.
We illustrate our notions of utility and disclosure risk of a click as well as the e ects of various clicking strategies through experiments on real datasets from Movielens [2] and Jester [19]. We show that our notion of commonality of a user pro le indeed conveys the classical concept of precision of a recommender [32], restricted to a user pro le. We also show for instance that, according to Movielens, 5.23% of the clicks improve utility without loss of privacy­at the other extreme, 16.48% of clicks induce a high privacy risk without a utility gain. Finally, we also show that the smart clicking strategy does not impact utility (while maximizing privacy).
The rest of the paper is organized as follows. In Section 2 and Section 3, we de ne and show how to compute the e ects of a click on utility and privacy, respectively. Section 4 discusses the relation between utility and privacy, and introduces our notion of a clickadvisor. Section 5 analyzes several clicking strategies based on a game-theoretic approach. We report on our measurements with datasets from Movielens and Jester in Section 6. Section 7 discusses related work and Section 8 concludes the paper with remarks about future work. For space limitations, we defer some algorithms and discussions about the click-advisor as well as the proofs of our theorems and lemmas to a companion technical report [1].

2 UTILITY 2.1 Recommender Model

We consider a general model of a CF recommender scheme [23].
The set of user clicks is modeled as a matrix, denoted by E. The pro le of user u, denoted by U, corresponds to a row of E. Each column of E is related to an item. For the sake of presentation

simplicity, but without loss of generality, we model each click as a

like/dislike

4
action.

The

result

of

user

u

clicking

on

item

i

is

either

a "like" or a "dislike", represented by 1 and -1 in the corresponding

cell of E, e (u, i), respectively. Also, we mark 0 in e (u, i) if user u

has not clicked on item i.

We denote by M the size of the set of items and by N the number of users. A user pro le, U  {-1, 0, 1}M , is a vector of size M, in

which the cell corresponding to item i is e (u, i):

1

e

(u,

i

)

=

 

0

u has clicked on i and likes it; u has not clicked on i;

 -1 u has clicked on i and dislikes it. 

For each item i, we denote by NLike (i) and NDislike (i) the number of users who like and dislike i, respectively. The item pro le

4 This can model binary as well as non-binary types of rating: a low rating as a dislike, and a high rating as a like.

666

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Items Users E u U i I I M N NLike (i ) NDislike (i ) NN otClicked (i ) Clicked (u)
U

Notations The set of all items in the system The set of all users in the system A recommender dataset A user A user pro le An item An item pro le The set of all item pro les The number of items in the system The number of users in the system The number of users who like item i The number of users who dislike item i The number of users who have not clicked on item i The set of items clicked by user u A random created user pro le based on the information in item pro les
Table 2: Notation Table

Item Pro le

Item NLike NDislike NN otClicked Popularity Preferability

i1

2

0

2

0.5

0.5

i2

0

1

3

0.25

-0.25

i3

2

0

2

0.5

0.5

i4

2

1

1

0.75

0.25

Table 3: Pro les, Popularity and Preferability of Items in

Table 1

of i, denoted by I, contains NLike (i), NDislike (i) as well as the number of users who have not clicked on i yet, NN otClicked (i).5
Although user pro les are private, item pro les are typically public6

(e.g., in IMDb and Movielens). Table 2 summarizes the notations

we use in the paper.

2.2 Commonality of a User Pro le

To de ne our notion of commonality of a user pro le (or simply, of a user), we rst go through the concepts of popularity and preferability of items.

De nition 1. (Popularity)

popularity (i )

=

NLike (i )

+ ND N

is

l

ike

(i

)

.

De nition 2. (Preferability, i.e., average of preferences of users

toward

an

item) preferability (i )

=

NLike (i )

- NDi N

sl

i

k

e

(i

)

.

The item pro les as well as popularity and preferability of items

in Table 1 are represented in Table 3.

De nition 3. (Mainstream preference) The mainstream preference of users for item i, denoted by m(i), is computed as popularity(i) · preferability(i). For all items managed by a recom-

mender, the mainstream preference of users is stored in a vector of
size M in which the corresponding cell to item i is m(i). We denote the mainstream vector by m  [-1, 1]M .
We now introduce the concept of commonality of user u, (with respect to other users) denoted by commonality(u). Roughly speaking, we capture by a number, how close the user pro le U is to

other users pro les in the system in general.

5 In other words, an item pro

le,

I, includes the average rating of i

as well as the

number of users who like/dislike i.

6Item pro les (the average rating, the percentages of users who have used/purchased

an item, etc) play an important role in convincing a user to actually purchase an item.

i1 i2 i3 i4

Commonality

u1 



0.5 × 0.5 + 0 + 0.5 × 0.5 + 0 = 0.5

u2

  0 + 0 + 0.5 × 0.5 + 0.75 × 0.25 = 0.4375

u3



 0 - 0.25 × (-0.25) + 0 + 0.75 × 0.25 = 0.25

u4 

 0.5 × 0.5 + 0 + 0 - 0.75 × 0.25 = 0.0625

Table 4: Commonalities of Users in Table 1

De nition 4. (Commonality) commonality(u) = popularit (i) · preferability(i) · e (u, i).
i I
Remark 1. commonality(u) = U · mT .
Remark 2. Basically, the commonality of a user represents how close the direction of the vector U is to the direction of the mainstream vector, m, by relatively computing the cosine of the angle between vector U and vector m.
The commonalities of the users in Table 1 are computed in
Table 4 using Table 3 and De nition 4. As all the users clicked on
exactly two items in Table 1, Table 4 shows that commonality is not
proportional to the number of user clicks. Instead, commonality
is a weighted function of user clicks based on how well the clicks
help the recommender connect users (or items) to suggest them new items. For instance, the commonality of u1 is higher than the commonality of u4, therefore the recommender should provide more accurate recommendations to u1 than to u4. In Section 6, we emprically show that the commonality captures the quality of not only SVD-based recommenders but also KNN-based ones. 7

2.3 Utility of a Click
The utility of a click by user u is the di erence between commonalit (u) before and after that click. To distinguish both cases, we use the prime notation for the latter: commonalit (u) denotes u's commonality after the click.
De nition 5. (Utility of a click by user u) commonalit (u) = commonalit (u) - commonalit (u).

3 PRIVACY

We de ne the notion of disclosure degree of a user pro le (or simply,

of a user) based on the classical concept of entropy in information

theory [15]. Basically, the disclosure degree of user u, which we

denote by u , corresponds to the amount of information that item

pro

les

contain

about

u

8
.

Remember

that

our

goal

here

is

to

protect

users from other (curious) users. We assume that the recommender,

which stores all the user and item pro les, is trusted.

3.1 Disclosure Degree of a User Pro le

We address the situation where an intruder (i.e., a curious user), given public item pro les (I ), tries to disclose information about user pro les with a disclosure probabilistic model. The considered
privacy disclosure conveys the intruder's ability to uniquely iden-
tify a user using this probabilistic model [3]. One could interpret the disclosure degree of u as the number of bits of information that the intruder has gained in order to uniquely identify u, given I .
The disclosure probabilistic model determines the probabilities
that users have clicked on items; these probabilities are assigned by the intruder to a district random user pro le, U . Let U have the

7 The commonality of a user pro le can be further speci ed for a CF recommender based on a factorization method with latent factors. 8 We exclude users without any click from our study as those have no privacy concern.

667

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

User Commonality

Disclosure Degree

u1

0.5

- log(0.50 × 0.75 × 0.50 × 0.25) = 1.329

u2

0.4375

- log(0.50 × 0.75 × 0.50 × 0.50) = 1.028

u3

0.25

- log(0.50 × 0.25 × 0.50 × 0.50) = 1.505

u4

0.0625

- log(0.50 × 0.75 × 0.50 × 0.25) = 1.329

Table 5: Disclosure Degrees of Users in Table 1

conditional disclosure probability function Q (U ) = Pr (U = U |I ), in which U represents each possible user pro le in the system.

For a given disclosure probability distribution, the concept of dis-

closure degree provides a measure of the information (entropy in
information theory) about user pro le U stored in I . In this case, we denote by u = H (U ) the entropy of the user pro le given the
public item pro les, what we call the disclosure degree.
Naturally, the lower Q (U ) = Pr (U = U |I ), the more unique user pro le U is (the higher u ). When the interests of user u are determined by click on an item by u, most of the time, this click decreases Pr (U = U |I ) and compromises the privacy for u. In other

words, after a click, the user usually provides more information

in the system and makes it hard for the recommender to hide this

information (we show however this is not always the case).

To compute the disclosure degree of a user pro le, we compute

the probability of a click on an item i in U as follows:

Ei

=

 

1 0

 -1

with

probability

NLike (i ) N

(U likes i);

with

probability

1-

NLike (i )+NDislike (i ) N

(U does not click on i);

with

probability

NDislike (i ) N

(U dislikes i).



The following remark highlights the distribution of our disclosure probabilistic model for each possible user pro le given I .

Remark 3. For any possible user pro le U:

Q (U ) = Pr (U = U |I ) =

Pr (Ei = e (u, i)).

i Items

We use the above disclosure model to de ne u , the disclosure degree of user u:

De nition 6. (Disclosure degree) u = - log(Pr (U = U |I )).
Note that u captures the amount of information about a user
pro le stored in the public item pro les. In Section 6, we simulate

a KNN attack [6] to illustrate the protection level of di erent users

with di erent disclosure degrees. We show how accurately an

intruder can disclose information about users with di erent disclo-

sure degrees. The following remark highlights how to precisely
compute u . Later, we use Remark 4 to compute the privacy e ect of a click by user u.
Remark 4. u = - i Items log(Pr (Ei = e (u, i))).
The disclosure degrees of the users in Table 1 are computed

in Table 5 using De nition 6. The commonality and disclosure degree9 of each user are shown in the corresponding columns in Table 5. Despite the fact that u4 has a higher disclosure degree than u2, the recommender can provide better recommendations to u2 than to u4 because the commonality of u2 is higher than that of u4.
This shows that a higher disclosure degree does not necessarily

9 A user with high disclosure degree has a low level of privacy.

commonalit (u) > 0 commonalit (u) < 0

u > 0 u < 0

Trade-o Safe

Dangerous/Deleterious Trade-o

Table 6: Conditions for Each Click Zone lead to a better utility. Actually, not only does the click of u4 on
i4 compromise u4's privacy but neither does it help her get good

recommendations.

3.2 Disclosure Risk of a Click
We x a user u for whom we compute the e ect of a click on privacy. We denote the di erence of the disclosure degree of user u before and after the click by u which can be expressed as the disclosure risk of the click.
De nition 7. (Disclosure risk of a click by user u) u = u - u .
Note that a click with a positive disclosure risk compromises the
privacy for a user. The larger the absolute value of the disclosure
risk of a click, the greater the privacy e ect of that click. The
following remarks can then be easily derived:
Remark 5. Whenever u clicks on i, the disclosure degree of u does not increase over time by clicks of other users on i
Remark 5 basically says that the disclosure risk of a click on i by u is actually an upper bound of all changes in the disclosure degree of u corresponding to item i after the time of the click.

4 CLICK-ADVISOR

In traditional recommenders, when a user clicks on an item, the
information about that click is directly propagated to the system.
We propose to leverage our utility and privacy metrics to build a user-aware clicking tool: the click-advisor. After getting a new recommendation, a user pre-clicks on the item and previews the utility and privacy e ects of the pre-click through the click-advisor.
Based on the click-advisor feedback, the user nalizes the decision
on whether to click or not.
We categorize the clicks into four di erent zones depending on the
10
sign of utility and privacy e ects of the click as well as its reverse :
· Safe zone: improves both utility and privacy. · Trade-o zone: induces a utility-privacy trade-o . · Dangerous zone: compromises both utility and privacy
but the reverse of the click improves privacy.11 · Deleterious zone: Both the click and its reverse compro-
mise utility as well as privacy.

In Section 6, we show that, besides trade-o clicks, safe, deleterious and dangerous clicks indeed exist in real-world datasets. Table 6 describes the corresponding zones for all possible cases for the disclosure risk and utility of a click. Note that there are two cases in which the clicks induce a trade-o : clicks with positive utility and positive disclosure risk, as well as clicks with negative utility and negative disclosure risk. To distinguish deleterious and dangerous zones for a click, we compute the disclosure risk of the reverse of the click,   (u). If   (u) > 0, the click is in a deleterious zone, otherwise, it is in a dangerous zone.
Table 7 shows the utility e ect (commonalit ) and privacy effect ( ) of each of the following clicks. The zone of a click is

10 The reverse of a like click is a dislike click and vice versa.
11 Note that the reverse of the click always compromises the utility for the user as it
is not based on the real preferences of that user.

668

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

i1 i2 i3 i4 Click commonalit



 

u1   

D1
2

u2

   L2
2

u3

   L3
3

u4 

  D4
3

0.25 0
0.5625 -0.1875

0.176 0.477 -0.176 0.301

0.477 0.176 0.301 -0.176

Table 7: The E ects of Clicks

Figure 1: Click-advisor

determined by the sign of the utility and disclosure risk of that

click as described in Table 6:

D1
2

(the

dislike

click

on

i2

by

u1):

has

a

positive

utility

and

a

positive disclosure risk. Hence, D1 is in the trade-o zone.
2

L2
2

(the

like

click

on

i2

by

u2):

has

a

negative

e

ect on the rec-

ommender utility for u2.

Moreover, not only L2
2

has a positive

disclosure risk but the reverse of L2 also does. Hence, changing L2

2

2

does

not

improve

the

privacy

for

u2;

L2
2

is

in

a

deleterious

zone.

L3
3

(the

like

click

on

i3

by

u3):

has

a

positive

utility

and

a

negative

disclosure

risk.

L3
3

thus

improves

both

utility

and

privacy

for

u3

and is thus a safe click.

D4
3

(the

dislike

click

on

i3

by

u4):

compromises

both

utility

and

privacy

for

u4;

D4
3

is

an

unsafe

click

for

u4.

However,

considering

the reverse of D4, even though changing D4 decreases the recom-

3

3

mender

utility

for

u4,

the

reverse

of

D4
3

improves

the

privacy

for

u4

( ),

as

shown

in

Table

7.

That

makes

D4
3

a

dangerous

click

so

u4 may prefer to change this click.

The following theorem determines the exact signs of u ,   (u) and commonalit (u).

Theorem 1. When user u clicks on item i , the sign of the changes in commonalit (u) and u are as follows: s n(u ) = s n(2 - 3popularit (i) - e (u, i ) · pre f erabilit (i)),
s n(commonalit (u)) = s n(e (u, i ) · pre f erabilit (i)), s n(  (u)) = s n(2 - 3popularit (i) + e (u, i ) ·pre f erabilit (i)).
Figure 1 represents the zones in the click-advisor in general. At

any point in time the click-advisor of a user represents the zones

as well as the points corresponding to all pre-clicks of the user. A

pre-click is presented as a point in one of the zones based on the

privacy and utility e ects of a click and its reverse using Theorem 1

(which we prove in our technical report [1]) and Table 6. The lo-

cation of a pre-click is de ned by the corresponding coordinates

based on whether the pre-click is a like or dislike. For example, to locate the corresponding point to a click on item i in order to like it, we use the liking coordinates.

Furthermore, the place of a pre-click in the click-advisor repre-

sents the amount of disclosure risk and utility of that pre-click and can be computed in O (1) as shown in Algorithm 1. At the time

Algorithm 1 : Computations of Commonality and Disclosure Degree in O (M ) and Computation of the Utility and Privacy E ects and Updates after a Click in O (1)

1: procedure G C

2: commonalit  0.

3:  [u]  0.

4: for i : Items do

5:

if u likes i then

6:

commonalit  commonalit + m[i].

7:

 [u]   [u] - log(Pr (Ei = 1)).

8:

else

9:

if u dislikes i then

10:

commonalit  commonalit - m[i].

11:

 [u]   [u] - log(Pr (Ei = -1)).

12:

else

13:

 [u]   [u] - log(Pr (Ei = 0)).

return (commonalit ,  [u]);

14: procedure U

C

15: m[i]  m[i] + m[i].

16: r  {-1, 0, 1} : Pr (Ei = r )  Pr (Ei = r ) + Pr (Ei = r ).

of a click on item i, Algorithm 1 only computes and updates m[i], Pr (Ei = 1), Pr (Ei = 0) and Pr (Ei = -1) in a constant time to be
employed by the click-advisor. Also, Algorithm 1 computes the

commonality (using Remark 1) and the disclosure degree (using

Remark 4) of a user pro le, linearly in the number of items.

The click-advisor solely uses the public item information to

locate the pre-clicks of a users. Hence, the click-advisor can be

implemented in the user-end to inform users about the utility and

privacy e ects of their pre-clicks without introducing any new

privacy risks for the users. It is important to note here that the

click-adviosr is not a privacy-preserving platform, but rather the

visually informative one. More discussion about the click-advisor

is available in our companion technical report [1]. For example,

we show that safe pre-clicks never end up being in a dangerous or

deleterious zone in the future because of the clicks of other users.

5 THE CLICKING GAME
To analyze the behavior of the users, we model the act of clicking as a game, which we call the clicking game. The players of this game are the users of the system (providing recommendations as well as a click-advisor) who want to maximize a reward function, described in the following, for each click.
5.1 Reward Function
The reward function is denoted by  : Users  R. Without loss of generality, we assume that u  Users,  (u) = 0 at the beginning. A click by user u with utility and privacy e ects  = commonalit (u) and  = -u modi es  (u) as follows:
 (u)   (u) + f (,  ),
in which function f : R2  R has the following properties: 1. f is an increasing function over . 2. f has the single crossing property over max(,  ).
Property 1 of f implies that the higher the recommender utility for a user, the bigger the reward function for that user. The single crossing property (de ned in [4]) merely de nes the sign of f . More precisely, Property 2 means that the sign of max(,  ) is the

669

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

same as the sign of f (,  ). Property 2 of f implies the following naturally desired sub-properties for  (u):
2a. If user u cancels a click,  (u) remains the same. 2b. Compromising utility and privacy by a click of user u leads
to decreasing  (u) (because of a negative f (,  )). Upon getting a new recommendation, a user pre-clicks on the item and consults the click-advisor for the e ects of that pre-click. Then, the user chooses the desired action on the pre-click in order to maximize the reward function through the clicking game.
5.2 User Actions
Consider N users, playing the clicking game. If a pre-click of a user is in a safe or trade-o zone, then either the privacy or the utility improves for that user after con rming that pre-click. However, con rming a pre-click in either a dangerous or a deleterious zone compromises both utility and privacy. The user may then decide to change or cancel a deleterious or dangerous pre-click. Basically, we can consider three possible actions for the user:
· Con rm: When the click follows the initial preference of the user (i.e., the user con rms the pre-click), the utility and disclosure risk of the click are the same as what we computed in Theorems 1 and 2, respectively.
· Change (for better privacy): The user decides to change the pre-clicks (i.e., clicking to dislike an item instead of clicking to like it) to improve privacy. In this case, the disclosure risk of the click is the same as what is computed in Theorem 2. However, the utility of the click would change from what we measured in Theorem 1 because this click is not the real preference of the user, but actually the opposite to it. So, the utility of the click is the opposite of
12
the utility computed in Theorem 1. · Cancel: In this case, the user decides not to click on an
item to preserve privacy. By canceling a pre-click on the item, a user preserves privacy (which is the opposite of what is calculated based on Theorem 2 assuming the preclick was con rmed) at the expense of missing a potential utility. With the same approach, the lost utility is what the user would get with the assumption (of con rming the pre-click) in Theorem 1.
5.3 User Strategies
After being recommended a new item, a user considers one of the above actions to gain the maximum possible reward over time. There are various possible user clicking strategies. We highlight few of them here.
· Basic: A user always con rms the pre-click based on her preference. In this strategy, the click-advisor is ignored.
· Careful: The user only cancels a pre-click in a dangerous or deleterious zone. Otherwise, the user con rms it.
· Smart: The user always con rms a pre-click in a safe or trade-o zone, cancels the deleterious pre-click and reverses the pre-click in a dangerous zone.
The following theorem says that the smart strategy leads to a Nash equilibrium in the clicking game.
12 Note that the utility of a recommender for users depends on the real preferences
of users. Hence, the action of changing a pre-click by a user always decreases the recommender utility for that user.

Theorem 2. All users playing smart is a Nash equilibrium, i.e., playing the smart strategy maximizes the reward function  for a user u, if every user other than u plays smart.
Lemma 1. If all users play smart, for item i: (i) If preferability(i) < 0, all users clicking on i disliked it. (ii) If preferability(i) > 0, all users clicking on i liked it.

The proofs of Theorem 2 and Lemma 1 are given in our technical

report [1]. From Lemma 1, we can conclude that playing the smart

strategy by all users guarantees the maximum possible privacy

for all users. However, it can hamper the recommender utility for users. We consider the sum of commonality for all users as an

overall recommender utility. In the following, we compare the

e ect of the smart strategy and the basic strategy (the strategy

of clicking based on the real preferences of users) on the overall

recommender utility for users. By playing the smart strategy, the

users may change some of their clicks. If all users play the smart

strategy, the changes of the clicks depends on the initial clicks on

items. In that case, we compute the average of the overall recommender utility for users for all possible initial clicks on items (in

order to compare with the overall recommender utility for users

playing the basic strategy in Theorem 3). The set of all possible
initial clicks on all items is denoted by C. For user u who is playing the smart strategy and a possible c  C, commonalit Smar t (c ) (u) represents the commonality of the real pro le of u when all users play the smart strategy and the initial clicks on items is c.
The following theorem says that the expected value of the av-

erage of the commonalities of the users in the case where they

all play the smart strategy is the same as the case where all users

play the basic strategy (users click on items based on their real

preferences). Theorem 3.

commonalit Basic (u) = E(

commonalit Smar t (c ) (u)).

u Users

u Users

In other words, Theorem 3 (which we prove in our technical

report [1]) says that the overall average of the recommender utility

for all users playing the smart strategy is the same as the case where

users click on items based on their real preferences. However,

with regard to privacy, the smart strategy provides better privacy

compared to the basic strategy.

6 EXPERIMENTAL EVALUATION

We evaluate the utility and privacy metrics, as well as their application to build a click-advisor, on two real-world datasets: the Movielens 100K [2] and Jester [19] datasets. Movielens (ML) consists of 100,000 ratings given by 943 users over 1682 movies. Jester involves the clicks of 24,983 users on 100 items [10]. (As Jester does not provide timestamps, we perform the time-based evaluations for Movielens only).
6.1 Commonality as a Utility Measure

We rst show here empirically that the notion of commonality of a user pro le indeed expresses the quality of a recommender for that user. We more speci cally highlight the positive linear correlation between commonality and the classical notion of precision [32].
We measure the precision of a recommender for each user as follows: we divide the dataset into a training set and a test set. We put each rating from the original dataset into the test set with

670

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

100

60

100

Precision(%)

Precision(%)

Precision(%)

40

50

50

20

Precision(%)

0

0

0

1

2

-0.5 0 0.5 1 1.5 2

Commonality

Commonality

0

0

1

2

Commonality

(a) Matrix Reconstruction as in [36]

(b) SVD-based Neighborhood as in [24]

(c) K Nearest Neighbors as in [38]

Figure 2: The Relation between Commonality and Precision in Movielens for Di erent Recommenders

100

50

0

-10

0

10

Commonality

Precision(%)

60 400
40 200 20

0

0

-10

0

10

Commonality

Precision(%)

64 300 62

200 60

100 58

0

56

-10

600

400

200

0

0

10

Commonality

(a) Matrix Reconstruction as in [36]

(b) SVD-based Neighborhood as in [24]

(c) K Nearest Neighbors as in [38]

Figure 3: The Relation between Commonality and Precision in Jester for Di erent Recommenders

probability 20% and hide all the ratings in the test set from the training set. For a user u, we put Nu = 0.2 · |Clicked (u)| and determine the top-Nu recommendations for u based on state-of-the-art recommender algorithms, namely a KNN (K Nearest Neighbors)
algorithm [38], a matrix reconstruction approach [36] and a SVD-
based recommender leveraging neighborhood [24].
Precision is measured in terms of standard classi cation accu-
racy metrics (CAM) [32]. More precisely, we evaluate how well a recommender can predict the context of the test set of a user u using each of the mentioned algorithms on the training set. We compare the top-Nu recommendations for user u with the hidden part of user pro le u (test set). We compute Precision(u) as the classi cation accuracy metric used on top-Nu recommenders computed in [10]. Precision(u) is then the ratio of the number of relevant recommended items to the total number of recommended items to user u. Actually, Precision(u) computes the recommendation quality for u.
We report here on our results for both Movielens and Jester. As
conveyed in Figures 2a and 2b, the commonality and precision fol-
low a linear fashion in Movielens for a SVD-based recommenders
using matrix reconstruction and neighborhood approaches. Math-
ematically evaluated, the correlation between commonality and precision is 0.6557 for SVD-based recommender using matrix reconstruction and the correlation increases to 0.6838 for a SVD-based recommender leveraging neighborhood. For Jester, because of
the massive number of users, the direct correlation between com-
monality and precision in Jester for SVD-based recommenders is
presented using pseudocolor plots in Figures 3a and 3b. Hence, the
color of each block represents the number of users in it. (A darker
color of a block means more number of users in that block.) For
KNN recommender, Figures 2c and 3c depict a direct proportional-
ity (more precisely, linear for both Movielens and Jester) between

the commonality of a user pro le and the precision of KNN recommender for the corresponding user. This basically means that users with high commonality get high precision for their recommendations in contrast to users with low commonality.
6.2 Disclosure Degree as a Privacy Measure
In this section, we use the Movielens and Jester datasets to show the extend to which our notion of disclosure degree (de ned for each user pro le) captures other well-known privacy concepts such as di erential privacy [12, 31] (de ned for algorithms) and k-anonymity [39, 8] (de ned for datasets).
6.2.1 Di erential Privacy. This guarantees that the presence or absence of a record in a dataset will not signi cantly a ect the nal output of the algorithm [12]. In the following, we study the disclosure degree as a privacy parameter of user pro les, and determine the relation between di erential privacy and disclosure degree. We employ the approach proposed in [31] to provide di erential privacy to recommenders by adding di erent levels of Laplacian noise to the Movielens dataset. Each noisy dataset corresponds to a level of di erential privacy. Figure 4 represents the average of the disclosure degrees of user pro les in each of these noisy datasets. We observe that the users in a dataset with a high level of noise (i.e., a low ) have smaller disclosure degrees in average compared to users in a dataset with a low level of noise. Intuitively, the presence (or absence) of a rare click (i.e., with high disclosure risk) highly a ects the output of the recommender compared to the presence (or absence) of a regular expected click.
6.2.2 K-anonymity. F. Casino et. al. applied the concept of k-anonymity to collaborative ltering [8]. Figure 5 represents the average of disclosure degrees of user pro les in Movielens as well as Jester with di erent levels of k-anonymity. As depicted in Figure 5, increasing k (i.e., stronger anonymity for users) results in

671

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Avg(u)

600

100

Avg(u)

400 80

200

60

0

0.5

1

0

0.2

0.4





Avg(u)

(a) Movielens

(b) Jester

Figure 4: Disclosure Degree and Di erential Privacy

600
100 400

Avg(u)

80 200

0

60

0

100

200

0

20

40

k

k

(a) Movielens

(b) Jester

Figure 5: Disclosure Degree and K-anonymity

decreasing the average of disclosure degrees of user pro les.13 Our

concept of disclosure degree of a user pro le captures how unique

a user pro le is in the system. Intuitively, the higher the disclosure

degree of a user pro le, the more distinguishable the user is among

others (the higher the risk of identifying the corresponding user

among all the users in the system).

6.2.3 KNN A ack. We consider here the KNN attack described in [6]. An attacker creates K fake pro les with some auxiliary information about a target user uT and requests for recommendations. In the simple case of a KNN recommender, the K nearest neighbors of a fake pro le are uT and K - 1 other fake pro les. When a fake pro le gets a new item as a recommendation, this likely comes from the items which are already clicked by uT . The reason is that the only items clicked by the K nearest neighbors of a fake pro le which are not included in that fake pro le are the items in the pro le of uT . Upon a recommendation for any of the fake pro les, all the fake pro les click on the recommended item
in order to add the new recommended item to their pro les and be updated to get further information about uT . The attack is performed through several iterations. In each iteration, the attacker accumulates new information about uT . To gure out the relation between the disclosure degree of a user pro le and the accuracy
of the information an attacker can disclose about that user pro le,
we trigger this KNN attack for the users with di erent disclosure
degrees in Movielens and Jester.
For this simulation, we consider the Movielens as well as the
Jester dataset. We consider an attacker simulating a KNN attack
to disclose information about a given target user. We denote the disclosure degree of the target user by uT . We illustrate the accuracy and the amount of extracted information (%EI ) by the attacker about di erent target users with di erent disclosure degrees.
For our simulations, we put K = 10. Table 8 shows how accurate information an attacker can disclose by simulating the attack for
users with di erent disclosure degrees after 10 iterations. Table 8

13 As

k

anonymity

is

a

privacy

parameter

for

a

dataset,

not

for

a

user

pro

le,

to

compare k-anonymity and disclosure degree, we use the average disclosure degrees

of all user pro les as an overall privacy measure for users.

Dataset uT

Auxiliary Information

uT

%EI Accuracy

MLV 3 [1, 2, 3] 235.09 7.33 97.14

MLV 86 [1, 2, 3] 191.66 13.00 80.00

MLV 200 [1, 2, 3] 118.07 15.33 65.71

Jester 7 [0, 3, 6, 7, 5] 118.57 8.24 98.00

Jester 62 [1, 3, 4, 5, 0] 93.21 11.63 85.71

Jester 303 [5, 6, 7, 9, 10] 70.23 20.91 65.71

Table 8: Result of a KNN Attack for Jester and Movielens after 5 Iterations

Figure 6: An Screen-shot of the Implemented Click-advisor shows that the higher the disclosure degree of a user, the more accurate the information disclosed by the attacker about that user.
We show that our notion of disclosure degree is compatible with state-of-the-art privacy metrics (i.e., k-anonymity and di erential privacy). To illustrate the disclosure degree further, we show that a user with low disclosure degree is more resistant to a KNN attack, than another user with high disclosure degree. It is important to notice though that our concept of disclosure degree is independent of any particular attack. Also, remember that an attacker here has only access to the output of a recommender, not an (anonymized) version of the dataset. Therefore, de-anonymization attacks cannot be applied in this case.
6.3 Click Zones
We compute the utility and privacy e ects as well as the zones of the clicks in Movielens, as discussed in Sections 2 and 3 and 4. (Jester does not provide timestamps.) We observe that most of the clicks (77.63%) induce a trade-o between utility and privacy. Yet, there are clicks which do not induce a utility-privacy trade-o : others are safe (5.94%), dangerous (2.00%) and deleterious (14.43%).
6.4 Click-advisor
We implemented the click-advisor for a SVD-based neighborhood recommender (as in [24]) using the Movielens dataset. Figure 6 represents a screen-shot of our click-advisor. At the beginning, three recommended items are provided to a user. The genuine pre-clicks (like or dislike) of a user on the recommended items are shown in Figure 6a. In Figure 6b, the zone of each pre-click (shown as grey circles) corresponds to the utility and privacy e ects of that

672

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Con rm Change Cancel Ask Me

Safe

100 % 0%

0%

0%

Trade-o 96.97 % 0 %

0 % 3.03 %

Dangerous 3.03 % 39.4 % 30.30 % 6.07 %

Deleterious 3.03 % 0 % 93.94 % 3.03 %

Table 9: The Preferred Default Action in the Click-advisor by the Participants in the Survey

Strategy

Smart Careful Basic

Precision(%)

27.02 24.63 28.87

Accuracy of a KNN Attack (%) 55.67 60.53 82.12

Table 10: Recommender Precision and KNN Attack Accuracy for Di erent User Strategies

pre-click on the user pro le. Consulting the click-advisor, the user would decide an action for each pre-click.
The link to our click-advisor as well as a survey to gather the opinions of users on the click-advisor were distributed. More than 95% of respondents agreed that the click-advisor increases their con dence in clicking and their trust to click on more items. Also, we asked about the preferred default strategy in the survey. Table 9 represents the percentage of the respondents who prefer a default action (corresponding to a column) in the case of each types of a click (corresponding to a row). As shown in Table 9, the users chose the presented strategies as their default strategy as follows: Basic (3.03%), Careful (30.30%), and Smart (39.4%). The rest of respondents (6.06%) preferred to be asked as default for some types of clicks. Knowing that the Smart strategy is a Nash equilibrium, we can advertise the Smart strategy as the default strategy. However, the users may adopt their actions as they prefer.
6.5 Clicking Strategies
Section 5 discusses di erent user strategies for the clicking game. We consider here the basic, careful, and smart strategies and apply each of them to the Movielens dataset. Based on the timestamps in Movielens14, we consider a click as a pre-click and place the pre-click of the user in the click-advisor instantaneously. Based on the position of the pre-click and the user strategy, we apply an action to the pre-click (con rm, cancel or change). Applying the same strategy to all the users, we create a new updated dataset and compute the precision of the recommender for this updated dataset. Table 10 shows the precision of the recommender applying each of the strategies to all the users. As predicted by Theorem 5, the precision is almost the same for the basic and smart strategies.
We also apply the KNN attack to the updated datasets for each user strategies. We consider the KNN attack with auxiliary information as what described in Section 6.2.3 for Movielens. For this experiment, we average the accuracy of the KNN attack over all the cases in which every single user is the target user. Table 10 shows that the users become more resistant to the KNN attack as they choose the Smart strategy over the Careful and Basic ones. 15

7 RELATED WORK
CF recommenders. Although it has been shown that CF recommenders perform well, they have some limitations for sparse datasets [38]. Sarwar, et. al. applied Singular Value Decomposition (SVD) to reduce the dimensionality of sparse datasets of

14 The absence of timestamps makes this simulation impossible in Jester.
15 A higher accuracy of a KNN attack means the users are less resistant to that attack.

collaborative ltering recommenders [36]. Recently, many collaborative ltering recommenders used matrix factorization methods to cope with the sparsity of data, items correlations and dynamic recommendation domains [26, 24, 25, 28]. Our utility and privacy measures are compatible with matrix factorization-based CF recommenders as well as neighborhood-based ones.
Privacy in IR and recommenders. Privacy risks of user posts in online communities were studied in [5] for textual contents, unlike this paper which studies the privacy risks of a single click. Moreover, several papers proposed privacy-preserving methods in the context of recommenders [33, 30, 22]. Perturbative methods were proposed in [33], where users submit perturbed ratings to the recommender. However, even showing perturbed interest in a certain item may also disclose the preferences of users. For example, the perturbed rating of a user on a comedy movie still highlights the interest of the user in watching comedy movies. Furthermore, some works [17, 21] indicate that the use of randomized data distortion techniques might not able to preserve privacy. Regarding the use of cryptographic techniques, Canny proposed a method that enables a group of users to calculate a public aggregate of their pro les without revealing them on an individual basis [41, 7]. The major downside of this method is, however, the assumption of an acceptable number of users is online and willing to participate in the protocol. None of these privacy preserving approaches measured the privacy e ect of a single click in a recommender (which we do in this paper).
Di erential privacy. The notion of di erential privacy was introduced by Dwork in the context of databases [12] and later adapted to recommender algorithms [31, 20, 16]. In contrast, our concept of disclosure degree studies the privacy for a speci c user regardless of the recommender algorithm. While in di erential privacy the recommendations are considered the only observation of an intruder, we take into account the public item pro les as another source of available information for the intruder (unlike di erential privacy, we seek to assign a privacy parameter to each user pro le in the system, regardless of the recommender algorithm).
A noise addition technique was described in [31] to apply di erential privacy to recommenders. In Section 6, we analyze the relation between (a) the di erent levels of additional noise to dataset as a di erential privacy approach in [31] and (b) the disclosure degree of users in the obfuscated datasets with di erent . We empirically show that the higher the level of noise in an obfuscated dataset (higher di erential privacy), the lower the disclosure degrees of users in average.
K-anonymity. The concept of k-anonymity as a privacy protection method, was rst formulated in [39], and recently applied to collaborative ltering recommenders in [8]. In Section 6, we compare the disclosure degree of users in di erent obfuscated datasets for di erent k, and we show that the average disclosure degree of users in a dataset actually captures the level of anonymity of that recommender dataset.
Degree of anonymity. Chaum introduced the notion of anonymity set in order to model the security of Dining Cryptographers' networks in [9]. Serjantov and Danezis raised some issues about anonymity sets [37]. For example, anonymity sets do not take into account the risk of inferring some sensitive attributes of

673

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

the users in a set. To address these issues, a general measure to quantify the degree of anonymity for message passing was proposed in [37, 11]. Both papers independently proposed the use of entropy as the basis for formally measuring anonymity. The anonymity degree provided by the system quanti es the amount of information the system is leaking. We apply the idea of computing the amount of information stored in a user pro le as a privacy parameter (disclosure degree) in the context of recommender datasets.
Disclosure risk. In the literature, disclosure risk measures have been classi ed as measures for record re-identi cation or con dential value disclosure [27, 14]. The latter focuses on measuring the risk of compromising a con dential value of a particular individual while the former focuses on measuring the risk of inferring the identity of an individual. In both cases, the disclosure risk measures may be applied to the database as a whole, or to individual records.
Alfalayleh and Brankovic measured the disclosure risk based on entropy [3]. However, if we use the same approach as in [3] to compute the disclosure risk of a user pro le, we would get the same result for all users. Indeed, that approach provides a general metric for all users while our measure of disclosure degree is a speci ed metric for a user pro le.
Recommender utility. The utility of recommenders has been measured di erently in [18, 10, 13]. These measures, empirically evaluate the global quality of recommenders. In contrast, our commonality measure predicts the quality of recommenders for a user based on the user pro le and an actual click. In Section 6, we show that commonality and precision indeed have a positive correlation.
Privacy-utility relation. The utility-privacy trade-o in databases was studied by Sankar et. al. in [29]. Their utility parameter is not applicable to recommenders for it computes the rate-distortion [40] of the database, which is related to the input of the recommender, not to the quality of its output. As we show in this paper, in the case of clicks in recommenders, there is not always a trade-o between utility and privacy.
8 CONCLUDING REMARKS
This paper is the rst to precisely quantify the e ects of a user click both in terms of utility and privacy in the context of recommenders. We show that all clicks do not have the same e ect, neither in terms of privacy nor in terms of utility. In contrast to a common belief, we also show that there is not always a trade-o between utility and privacy. We introduce the idea of a click-advisor, which makes users aware of the e ects of their clicks. Consulting the click-advisor, users can formulate their own informed clicks. Considering a reward function for the users in a clicking game, we highlight in particular a smart clicking strategy that leads to a Nash equilibrium. We prove that if all users follow this strategy, their privacy improves without compromising the average overall recommender utility (compared to the case where users do not use the click-advisor).
Our utility and privacy metrics could be further extended to latent factors of users and items in a SVD-based recommender. It would also be interesting to incorporate a recommender that optimizes the utility and privacy for all users solving a linear optimization problem for their commonalities and disclosure degrees. Acknowledgements. This work has been supported in part by the European ERC Grant 339539 - AOC.

REFERENCES
[1] The utility and privacy e ects of a click (technical report), 2017.
http://go.ep .ch/clickadvisor-technical-report.
[2] M. 100K. Movielens dataset, 2003.
[3] M. Alfalayleh and L. Brankovic. Quantifying privacy: A novel entropy-based measure of disclosure risk. In IWOCA, 2015.
[4] S. Athey. Monotone comparative statics under uncertainty. QJE, 2002. [5] J. A. Biega, K. P. Gummadi, I. Mele, D. Milchevski, C. Tryfonopoulos, and
G. Weikum. R-susceptibility: An ir-centric approach to assessing privacy risks for users in online communities. In SIGIR, 2016. [6] J. A. Cal, A. Kilzer, A. Narayanan, E. W. Felten, and V. Shmatikov. You might also like: Privacy risks of collaborative ltering. In S&P, 2011. [7] J. Canny. Collaborative ltering with privacy. In S&P, 2002. [8] F. Casinoa, J. Domingo-Ferrerb, C. Patsakisc, D. Puigb, and A. Solanasa. A kanonymous approach to privacy preserving collaborative ltering. JCSS, 2015. [9] D. Chaum. The dining cryptographers problem: Unconditional sender and recipient untraceability. JCRYPTOL, 1988. [10] P. Cremonesi, Y. Koren, and R. Turrin. Performance of recommender algorithms on top-n recommendation tasks. RecSys, 2010. [11] C. Diaz, S. Seys, J. Claessens, and B. Preneel. Towards measuring anonymity. In PET, 2003. [12] C. Dwork. Di erential privacy. In ICALP, 2006. [13] M. Ge, C. Delgado-Battenfeld, and D. Jannach. Beyond accuracy: evaluating recommender systems by coverage and serendipity. RecSys, 2010. [14] D. L. George T. Duncan. Disclosure-limited data dissemination. JASA, 1986. [15] R. M. Gray. Entropy and information theory. Springer, 2011. [16] R. Guerraoui, A. Kermarrec, R. Patra, and M. Taziki. D2p: distance-based di erential privacy in recommenders. VLDB, 2015. [17] Z. Huang, W. Du, and B. Chen. Deriving private information from randomized data. In SIGMOD, 2005. [18] M. Z. Islam, P. Barnaghi, and L. Brankovic. Measuring data quality: Predictive accuracy vs. similarity of decision trees. In ICCIT, 2003. [19] Jester. Online joke recommender system, 2001.
[20] Z. Jorgensen, T. Yu, and G. Cormode. Conservative or liberal? personalized di erential privacy. In ICDE, 2015.
[21] H. Kargupta, S. Datta, Q. Wang, and K. Sivakumar. On the privacy preserving properties of random data perturbation techniques. In ICDM, 2003.
[22] S. Katzenbeisser and M. Petkovic. Privacy-preserving recommendation systems for consumer healthcare services. ARES, 2008.
[23] J. A. Konstan and J. Riedl. Recommender systems: from algorithms to user experience. UMUAI, 2012.
[24] Y. Koren. Factorization meets the neighborhood: A multifaceted collaborative ltering model. In KDD, 2008.
[25] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 2009.
[26] D. kumar Bokde, S. Girase, and D. Mukhopadhyay. Role of matrix factorization model in collaborative ltering algorithm: A survey. IJAFRC, 2015.
[27] D. Lambert. Measures of disclosure risk and harm. JOS, 1993. [28] S. Li, A. Karatzoglou, and C. Gentile. Collaborative ltering bandits. In SIGIR,
2016.
[29] L.Sankar, S. Rajagopalan, and H. Poor. Utility-privacy tradeo s in databases: An information-theoretic approach. IFS, 2013.
[30] Y. Luo, J. Le, and H. Chen. A privacy-preserving book recommendation model based on multi-agent. WCSE, 2009.
[31] F. McSherry and I. Mironov. Di erentially private recommender systems: Building privacy into the net ix prize contenders. In KDD, 2009.
[32] Y. K. P. Cremonesi and R. Turrin. Performance of recommender algorithms on top-n recommendation tasks. RecSys, 2010.
[33] H. Polat and W. Du. Privacy-preserving collaborative ltering using randomized perturbation techniques. ICDM, 2003.
[34] N. Ramakrishnan, B. J. Keller, B. J. Mirza, A. Y. Grama, and G. Karypis. Privacy risks in recommender systems. Internet Computing, 2001.
[35] A. M. Rashid, I. Albert, D. Cosley, S. K. Lam, S. M. McNee, J. A. Konstan, and
J. Riedl. Getting to know you: learning new user preferences in recommender systems. In IUI, pages 127­134, 2002. [36] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. T. Riedl. Application of dimensionality reduction in recommender system ­ a case study. In ACM WEBKDD Workshop, 2000. [37] A. Serjantov and G. Danezis. Towards an information theoretic metric for anonymity. In PET, 2002. [38] X. Su and T. M. Khoshgoftaar. A survey of collaborative ltering techniques. AAI, 2009. [39] L. Sweeney. K-anonymity: a model for protecting privacy. IJUFKS, 2002. [40] E. Tuncel, P. Koulgi, and K. Rose. Rate-distortion approach to databases: storage and content-based retrieval. IT, 2004. [41] J. Canny. Collaborative ltering with privacy via factor analysis. In SIGIR, 2002.

674


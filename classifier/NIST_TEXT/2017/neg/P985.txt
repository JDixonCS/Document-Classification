Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Top-N Recommendation with High-Dimensional Side Information via Locality Preserving Projection

Yifan Chen
National University of Defense Technology
Changsha, China yfchen@nudt.edu.cn

Xiang Zhao
National Univ. of Defense Tech. Collaborative Innovation Center of
Geospatial Technology Changsha, China
xiangzhao@nudt.edu.cn

Maarten de Rijke
University of Amsterdam Amsterdam, e Netherlands
derijke@uva.nl

ABSTRACT
In this paper, we leverage high-dimensional side information to enhance top-N recommendations. To reduce the impact of the curse of high dimensionality, we incorporate a dimensionality reduction method, Locality Preserving Projection (LPP), into the recommendation model. A joint learning model is proposed to achieve the task of dimensionality reduction and recommendation simultaneously and iteratively. Speci cally, item similarities generated by the recommendation model are used as the weights of the adjacency graph for LPP while the projections are used to bias the learning of item similarity. Employing LPP for recommendation not only preserves locality but also improves item similarity. Our experimental results illustrate that the proposed method is superior over state-of-the-art methods.
1 INTRODUCTION
Top-N recommendation has been widely adopted to recommend ranked lists of items so as to help users identify the items that best
t their personal tastes. Over the last decades, various e orts have been dedicated to provide top-N recommendations. Among them, the item-based scheme stands out for its solid performance. Representative methods include item-based k-nearest-neighbor, sparse linear methods (SLIM) [5], and so forth, which have been shown to outperform user-based scheme.
e recommendation accuracy of such item-based neighborhood methods relies largely on the item similarities computed or learned. Speci cally, item similarities are usually made available based on user feedback (both explicit and implicit), e.g., purchases, ratings, reviews, clicks, and check-ins. Lately, there has been an increase in the amount of additional information associated with items, referred to as side information [6]. Typical examples include descriptions of movies in movie recommendation, resumes of applicants in job matching, content of emails in spam detection, reviews of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080697

items in online shopping, and so forth. Side information has generated the interest of many researchers and has led to the development of hybrid algorithms to enhance the performance of recommendations by taking advantage of such information.
Side information comes with a high dimensionality. For example, side information can be the text descriptions of items; when regarding each unique term in the corpus as one dimension, it is indisputably high-dimensional. Moreover, side information can also be in the form of images or videos where the dimensionality is evidently much higher. Nonetheless, existing methods overlook this fact when utilizing side information, and hence, they are facing problems of e ciency and accuracy due to the curse of high dimensionality. We address the issue in this paper, and investigate how to leverage side information to boost the recommendation performance while limiting the impact from high dimensionality.
While side information is high-dimensional and sparse, it is reasonable to expect a low dimensionality of intrinsic features, and this suggests that we should incorporate dimensionality reduction for this task. Among the many available dimensionality reduction methods, Locality Preserving Projection (LPP) [3] has been shown to produce a low-dimensional space that well preserves locality. As recommendation quality largely depends on item similarity, LPP is a natural candidate in this se ing.
To summarize, we propose a top-N recommendation method to harness high-dimensional side information. By introducing a projection matrix, high-dimensional side information is reduced into a low-dimensional space. We present a joint learning model to simultaneously perform LPP and learn item similarity. We then conceive an alternative iterative optimization method to solve the model. Our experimental evaluation shows that the proposed method enjoys a performance gain of up to 21.2% on Hit Rate and 36.8% on Average Reciprocal Hit Rate over state-of-the-art methods.
2 RELATED WORK
We are aware of several recent methods that leverage side information for top-N recommendation. On top of SLIM [5], SSLIM [6] utilizes a regularized optimization process to learn a sparse coe cient matrix. UFSM [1] combines item similarity model with factor models. Recently, Zhao et al. [9] have proposed a predictive collaborative ltering approach to utilize side information.
We also summarize recent methods using side information for rating prediction. AFM [2] maps side information to latent item factors by learning the map function. LCE [8] proposes a local collective factorization method. Lu et al. [4] propose an interactive

985

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

model for matrix completion. Distinct from them, we integrate dimensionality reduction into top-N recommendation.
As to dimensionality reduction, this topic has been investigated extensively, for sparse feedback via various methods [7], including principal component analysis, singular value decomposition, nonnegative matrix factorization and so on. However, high-dimensional side information has rarely been addressed in the se ing, and this paper tries to ll in the gap.

3 THE PROPOSED APPROACH
3.1 Notation
We rst introduce the notations used throughout the paper. Let U and I be the sets of all users and all items, respectively, each of size m and n. e user feedback (both explicit and implicit) shows the items that the users have purchased, viewed or rated, which is denoted by a matrix R of size m × n. We treat feedback as binary, that is, if user u provided feedback for item i, then the (u, i)-entry of R (denoted by rui ) is 1, otherwise it is 0. e item similarity matrix is represented by S  Rn×n, where each value of entry si j is within [0, 1]. e feature matrix (side information associated with items) is denoted by F  Rn×d , where d indicates the dimensionality of side information. e projection matrix is denoted by W  Rd×k , which is used to map d-dimensional side information into a k-dimensional space where k  d.

3.2 Model description
is section describes the proposed model. We start with introducing the Baseline method without performing dimensionality reduction, then summarize LPP, and explain how to incorporate it in a recommender system. Finally, the proposed method is formed.

Recommendation with side information. Typically, top-N recommender systems perform matrix completion for R, the core of which

is to learn item similarity, which is directly relevant to recommen-

dation. Side information is utilized to enhance the learning of item

similarity. While various forms of incorporating side information exist, we incorporate a regularization term on S along with feature matrix F and form the model as the following problem:

1 2

R - RS

2 F

+

 2

n i, j

f i - f j 22si j

 +2

S

2 F

,

(1)

such that sTj 1 = 1, j = 1, · · · , n; 0  si j  1, i, j = 1, · · · , n; and

sjj = 0, j = 1, · · · , n. sj is the j-th column vector of S, represent-

ing how similar item j is to other items. e constraint sTj 1 = 1

is incorporated to avoid the case when the learned S is close to 0

especially when R is very sparse.

e

term

1 2

R-RS

2 F

in

the

objec-

tive function tries to reconstruct the feedback matrix by learning

the coe cient matrix S, which was rst introduced by SLIM [5] for

top-N recommendation. As suggested there, the 2-norm is used

to regularize S. While 1-norm is also suggested to encourage sparsity, it is omi ed as it turns out to be constant here (due to sTj 1 = 1).  is a user-speci ed parameter to balance the two sources of infor-

mation. We further justify the regularization to S by F in detail.

Given the feature matrix F , f i represents the feature vector for item i. A natural way to measure the item distance in terms of

features is to compute the Euclidean distance between them, i.e.,

f i - f j 2. Although the item similarity is unknown, it is reasonable to assume that closer items (in terms of feature distance) are
likely to have higher similarities, and thus, item similarity between item i and j can be regularized as f i - f j 2si j .

Locality preserving projection. LPP is a linear approximation of the nonlinear Laplacian Eigenmap. e algorithmic procedure starts with constructing the adjacency graph from feature matrix F . e item similarity matrix S learned from (1) can be used for this task.
en, we need to solve the generalized eigenvector problem:

FT LFw =  FT DFw,

(2)

where D is a diagonal matrix, of which the i-th diagonal entry

equals

n j

si j +sji 2

;

L

is

a

Laplacian

matrix

for S,

i.e.,

L

=

D

-

S +ST 2

.

e projection matrixW is formed asW = (w1, w2, . . . , wk ), where

eigenvector wi corresponds to eigenvalue i , which is in an ascend-

ing order as 1  · · ·  d . e linear combination FW denotes

the projection of side information in a low-dimensional space.

The proposed model. Pu ing (1) and (2) together forms our proposed optimization problem as follows:

min 1 SW 2
,

R - RS

2 F

+

 2

n i, j

W T f i - W T f j 22si j

 +2

S

2 F

,

(3)

such that W TW = I ; sTj 1 = 1, j = 1, . . . , n; 0  si j  1,

i, j = 1, . . . , n; and sjj = 0, j = 1, · · · , n. Rather than impose the

constraint W T FT DFW = I on W according to LPP, we directly

assume W TW = I to learn a distinctive feature space. Besides,

we regularize si j by

WT fi -WT fj

2 2

instead

of

fi - fj

2 2

for

two reasons. First, the model is formulated as a joint learning op-

timization problem so as to achieve dimensionality reduction and

top-N recommendation simultaneously. We will show later in Sec-

tion 3.3 that optimizing W is under the framework of LPP. Second,

the training of the item similarity matrix S is enhanced in the pro-

jected low-dimensional feature space. We argue that incorporating

LPP is able to not only preserve locality but also improve item sim-

ilarity, which is explained below. Denote the projection matrix asW = (pT1 , . . . , pTd )T , where pi is
a k-dimensional row vector, representing the embedding of feature

i. ough projection, each feature is represented by k distinctive

aspects. We contend that the "synonyms" (di erent but semanti-

cally similar features) will have closer embeddings through LPP un-

der the assumption that the synonyms are likely to appear in items

with high similarities. erefore, items containing synonyms will

get closer in the projected space, which can further guide the learn-

ing of similarity towards more similar.
Once the solution (W , S) are obtained, we can recover the item-user recommendation score matrix R~ by se ing R~ = RS.

We then rank the scores for unrated items of each user in a non-

increasing order and recommend the rst N items.

3.3 Solution
e optimization problem de ned above is non-convex in terms of S, W together. us, it is unrealistic to expect an algorithm to
nd the global minimum. In what follows, we derive an alternative iterative algorithm to solve the problem.

986

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Fix W update S. We rst de ne the Lagrange function:

L(s

j

,

j

,

j

,

j

)

=

1 2

r j - Rsj

2 2

+

 2

qTj

s

j

+

 j sTj

1+

 2

sTj

s

j

+ Tj s j

+

j sjj ,

(4)

where j , j , j (j = 1, . . . , n) are the lagrangian multipliers, qi j =

WT fi

-WT fj

2 2

and

1

is

the

vector

with

all

elements

equal

1.

e partial derivation of L w.r.t s j is

L s j

= RT Rsj

- RT r j

+

 2 qj

+ j 1 + sj

+ j

+ jej,

(5)

where e j is the vector with only the j-th element equal 1 and others 0. A closed-form solution could be derived as follows:

 RT R + I si j = 0,

-1

RT r j

-

 2

qj

-

j 1

,
i+

if i j if i = j,

(6)

where RT R + I is positive de nite if  > 0 and j = sTj RT r j -

sTj RT Rs j

-

 2

sTj

qj

-

sTj sj ;

[·]i +

is

the

operator

to

take

the

i -th

element of the vector if it is not less than 0, otherwise 0.

Fix S update W . To update W , we rst introduce the following equation, which is based on the theory of spectral analysis:

1 2 i,j

W T f i - W T f j 22si j = Tr W T FT LFW .

(7)

Hence, the problem is equivalent to solving

min Tr W T FT LFW .

(8)

W T W =I

Applying the Karush-Kuhn-Tucker (KKT) rst-order optimality con-

ditions, we derive

FT LFW = W ,

(9)

and the solution is formed by the k eigenvectors of FT LF corre-

sponding to the k smallest eigenvalues. Note that W is updated

under the framework of LPP.

4 EXPERIMENT
4.1 Setup
To evaluate the performance of our method on the task of top-N recommendation with side information, we perform experiments on di erent real-world datasets, respectively, CUL1, Enron2 and Yahoo.3 CUL is an online service that allows researchers to add scienti c articles to their libraries. For each user, the articles added in his or her library are considered as preferred articles, from which titles and abstracts are collected and used as side information. Enron1 and Enron2 represent the two largest mailbox extracted from Enron Email. e data is composed of email messages released during investigation of the Federal Energy Regulatory Commission against the Enron Corporation. By regarding the email content as side information, we predict the most likely recipients of new messages. Yahoo contains a small sample of the Yahoo! Movies community's preferences for various movies, rated on a scale from A+
1CiteULike: h p://www.citeulike.org/ 2Enron Mail Box: h ps://www.cs.cmu.edu/enron/ 3Yahoo! Movies: h ps://webscope.sandbox.yahoo.com/

Dataset
Enron1 Enron2 Yahoo CLU

Table 1: Statistics of datasets.

#users #items #feeds density #features

663 953 7,594 9,537

1,773 5,366 8,641 8,222

1,588 3,401 19,434 29,352

0.14% 0.07% 0.03% 0.04%

25,133 32,063 7,823 6,860

to F, binarized to 0 or 1. e dataset also contains a large amount of side information about many movies. e statistics of the datasets are summarized in Table 1.
To comprehensively understand the e ectiveness of the methods, we adopt 5-time Leave-One-Out Cross Validation. e evaluation of the model is conducted by comparing the recommendation list of each user with the item of that user in the test set. e recommendation quality is measured using the Hit Rate (HR) and the Average Reciprocal Hit Rank (ARHR).4 We evaluate the performance of our proposed method on top-N recommendation.
In this set of experiments, we refer to our method as Prism (Projection regularized item similarity model). To evaluate its performance, Prism is rst compared with SLIM to demonstrate the need to utilize side information when feedback is sparse. e performance of CoSim (a pure content-based method [1]) is evaluated to show the quality of side information. To appreciate the e ectiveness of dimensionality reduction, the performance of Baseline, formulated in Equation (1), is also evaluated. We also compare Prism with state-of-the-art top-N recommendation methods with side information, including SSLIM [6], UFSM [1] and the method proposed in [9] (referred to as PCF). Parameters of all methods are carefully tuned through grid search.
4.2 Results and analysis
We vary the size of recommendation list, and nd that Prism always achieves the best results. Table 2 shows the result of comparisons over four datasets with top-10 items recommended. By looking at the results achieved by SLIM and CoSim, we can characterize the datasets. Overall speaking, SLIM performs inferiorly to CoSim on both Enron1 and Enron2, whereas the order is reversed on Yahoo and CUL. is shows that while all datasets are sparse with respect to user feedback information, the side information of Enron is of high quality and more relevant for recommendation. As the Enron datasets are of higher dimensionality, a signi cant performance gain is expected with Prism on Enron1 and Enron2. To verify this, we scrutinize the results of Prism and Baseline, and
nd that the improvement of Prism over Baseline is much more evident on Enron1 and Enron2 than that on Yahoo and CUL. ese results demonstrate the e ectiveness of incorporating LPP for recommendation involving high-dimensional side information.
As for the comparison with other methods, Prism achieves the best results over all tested datasets, especially on Enron2, which has the highest dimensionality of side information. e recommendation accuracy of Prism on this dataset enjoys a performance gain up to 21.2% on HR and 36.8% on ARHR over state-of-the-art methods. is demonstrates the e ectiveness of Prism. On the Yahoo
4For each user, we recommend N items, where N = 5, 10, 15, 20. Due to space limitations, we only present the result with N = 10.

987

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Method
CoSim SLIM SSLIM1 SSLIM2 UFSMrms e UFSMb p r PCF Baseline Prism
Method
CoSim SLIM SSLIM1 SSLIM2 UFSMrms e UFSMb p r PCF Baseline Prism

Table 2: Comparison of top-N recommendation algorithms.

Enron1 Parameters --  = 0.6,  = 0.2  = 0.9,  = 0.1,  = 0.2  =  = 0.1,  = 0.2 l = 1. = 0.1, µ1 = 0.01, µ2 = 10-5 l = 1. = 10-5, µ1 = 0.01, µ2 = 10-4  = 0.5,  = 10.0,  = 500  = 1.0,  = 0.3 k = 100,  = 2.0,  = 0.2
Yahoo Parameters --  = 0.9,  = 0.5  = 0.7,  = 0.5,  = 0.1  =  = 0.1,  = 0.5 l = 6,  = 0.1 = µ1 = 0.1, µ2 = 10-4 l = 5,  = µ1 = µ2 = 1-5  = 1.0,  = 10,  = 2000  = 1.0,  = 0.5 k = 300,  = 0.9,  = 0.1

HR10 0.1408 0.0865 0.2032 0.0853 0.1485 0.1416 0.2013 0.0966 0.2153
HR10 0.0241 0.0558 0.0543 0.0485 0.0408 0.0400 0.0556 0.0618 0.0672

ARHR10 0.0992 0.0347 0.0966 0.0327 0.1059 0.1040 0.1011 0.0435 0.1091
ARHR10 0.0106 0.0181 0.0193 0.0181 0.0195 0.0192 0.0208 0.0230 0.0232

Enron2 Parameters --  = 0.1,  = 0.5  = 0.8,  = 0.1,  = 0.5  = 0.1,  = 0.1,  = 0.5 l = 4,  = 0.1, µ1 = 0.1, µ2 = 10-5 l = 3,  = 10-5, µ1 = 0.01, µ2 = 0.01  = 0.2,  = 5,  = 1000  = 0.9,  = 0.5 k = 100,  = 0.3,  = 0.4
CUL Parameters --  = 1.0,  = 0.5  = 0.9,  = 1.0,  = 0.5  = 0.1,  = 0.1,  = 0.5 l = 5,  = 10-5, µ1 = 10-4, µ2 = 10-5 l = 5,  = 10-5, µ1 = 0.01, µ2 = 10-5  = 0.8,  = 5,  = 2000  = 1.0,  = 0.1 k = 200,  = 0.3,  = 0.1

HR10 0.1460 0.1569 0.2204 0.1547 0.1693 0.1511 0.2318 0.1679 0.2810
HR10 0.1238 0.1961 0.1916 0.2223 0.1821 0.1942 0.2167 0.2118 0.2247

ARHR10 0.1196 0.0668 0.1119 0.0733 0.1273 0.1142 0.1201 0.0850 0.1742
ARHR10 0.0559 0.0758 0.0733 0.0873 0.0705 0.0803 0.0834 0.0864 0.0936

dataset SSLIM and UFSM actually degrade the accuracy compared with SLIM. While PCF increases it, the increment is limited. is should be a ributed to the poor quality of side information. By contrast, Prism improves it, exhibiting the robustness of Prism; that is, even on the dataset where side information is of limited correlation to recommendation, the preferable result could be expected.
is robustness is also displayed on CUL, which takes good user feedback but poor side information. It seems that CUL is more suitable to the methods that loosely couple with side information like SSLIM2. In this case, Prism is still able to achieve quite competitive performance, as the relevance of side information is improved through dimensionality reduction and  is tuned small to emphasize more on feedback information. e performance on Enron1 is not that distinctive. As the side information is of high quality, the methods that tightly couple with side information stand out (SSLIM1 and PCF). On the other hand, as the feature dimensionality is lower than that on Enron2, dimensionality reduction is not equally e ective.
5 CONCLUSION
In this paper, we showed the problems encountered when utilizing high-dimensional side information to enhance the performance of recommendation, which had not been well investigated by existing literature. We proposed a novel method to address the challenge, namely Projection regularized item similarity model­Prism.
e method integrates LPP and top-n recommendation into a joint learning algorithm. Under the novel framework, LPP not only resolved the issue brought by high dimensionality, but also improved the relevance of item similarity. We conducted extensive experiments and the results demonstrated the superiority of Prism.
Acknowledgment. is work was partially supported by NSFC No. 61402494, 61402498, 71690233, NSF Hunan No. 2015JJ4009, Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research

Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/20072013) under grant agreement nr 312827 (VOX-Pol), the Microso
Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scienti c Research (NWO) under project nrs 612.001.116, HOR-11-10, CI-14-25, 652.002.001,
612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
REFERENCES
[1] A. Elbadrawy and G. Karypis. User-speci c feature-based similarity models for top-n recommendation of new items. ACM Trans. Intel. Syst. Tech., 6(3):33, 2015.
[2] Z. Gantner, L. Drumond, C. Freudenthaler, S. Rendle, and L. Schmidt- ieme. Learning a ribute-to-feature mappings for cold-start recommendations. In 10th IEEE International Conference on Data Mining, ICDM 2010, Sydney, Australia, pages 176­185, 2010.
[3] X. He and P. Niyogi. Locality preserving projections. In Advances in Neural Information Processing Systems 16: Annual Conference on Neural Information Processing Systems 2003, NIPS 2003, Vancouver and Whistler, British Columbia, Canada, pages 153­160, 2003.
[4] J. Lu, G. Liang, J. Sun, and J. Bi. A sparse interactive model for matrix completion with side information. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, NIPS 2016, Barcelona, Spain, pages 4071­4079, 2016.
[5] X. Ning and G. Karypis. SLIM: sparse linear methods for top-n recommender systems. In 11th IEEE International Conference on Data Mining, ICDM 2011, Vancouver, BC, Canada, pages 497­506, 2011.
[6] X. Ning and G. Karypis. Sparse linear methods with side information for topn recommendations. In Sixth ACM Conference on Recommender Systems, RecSys 2012, Dublin, Ireland, September 9-13, 2012, pages 155­162, 2012.
[7] F. Ricci, L. Rokach, and B. Shapira, editors. Recommender Systems Handbook. Springer, 2015.
[8] M. Saveski and A. Mantrach. Item cold-start recommendations: learning local collective embeddings. In Eighth ACM Conference on Recommender Systems, RecSys 2014, Foster City, Silicon Valley, CA, USA, pages 89­96, 2014.
[9] F. Zhao, M. Xiao, and Y. Guo. Predictive collaborative ltering with side information. In Twenty-Fi h International Joint Conference on Arti cial Intelligence, IJCAI 2016, New York, NY, USA, pages 2385­2391, 2016.

988


Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

On the Power Laws of Language: Word Frequency Distributions

Flavio Chieriche i
Sapienza University Rome, Italy
avio@di.uniroma1.it

Ravi Kumar
Google Mountain View, CA, USA
ravi.k53@gmail.com

Bo Pang
Google Mountain View, CA, USA
bopang42@gmail.com

ABSTRACT
About eight decades ago, Zipf postulated that the word frequency distribution of languages is a power law, i.e., it is a straight line on a log-log plot. Over the years, this phenomenon has been documented and studied extensively. For many corpora, however, the empirical distribution barely resembles a power law: when plo ed on a loglog scale, the distribution is concave and appears to be composed of two di erently sloped straight lines joined by a smooth curve. A simple generative model is proposed to capture this phenomenon.
e word frequency distributions produced by this model are shown to match the observations both analytically and empirically.
1 INTRODUCTION
e distribution of word frequencies is a fundamental phenotype of a language. Word frequency distributions have been studied by statisticians and linguists since the statistics of word usage yield valuable insights into the language, its construction, and its evolution. ese distributions have been long-studied outside of statistics and linguistics as well. In information retrieval, word frequency distributions (and sometimes the ranks of word frequency) are directly used by many algorithms for many tasks, e.g., weighting the signi cance of documents and query terms [2, 36], text classi cation [6, 26], topic distillation [7, 13, 38], latent semantic analysis [24, 25], and so on. e word frequency distribution plays a central role in determining the size of inverted indices [14, 30], the compression ratio of natural texts [11, 12].
In his pioneering work, Zipf postulated that the frequency of any word in the language is inversely proportional to a power of its rank [44, 45]. On a log-log plot, with the x-axis representing the rank, and the -axis representing the frequency, the distribution would thus appear as a straight line with a negative slope. Subsequent studies have con rmed similar phenomena on di erent corpora and genre. Even though the actual parameters can depend on the corpus, the power-law phenomenon itself was shown to be pervasive and robust. ere have been many a empts to explain
Supported in part by the ERC Starting Grant DMAP 680153, by a Google Focused Research Award, and by the SIR Grant RBSI14Q743.

and re ne Zipf's law [10, 15, 19, 20, 28, 29, 31, 34, 40, 41]. Additionally, Zipf's law has been considered in the context of document retrieval by IR systems [1­3, 8, 9].
In large-scale empirical studies, however, the rank-vs-frequency distributions do not appear as straight lines on a log-log plot. Instead, they exhibit a bend that makes the curve look concave; we call the rank value at which the bend occurs as the knee. Interestingly, the bend is consistent with Zipf's original plot: the maximum rank in his plots is close to 103, whereas the knee is usually observed at a rank that is an order of magnitude higher. It is likely that the lack of computing power and automated tools made it infeasible for Zipf to move to a rank signi cantly larger than 103. is concavity-in-the-tail phenomenon has been noted empirically [23].
In this work we focus on the concavity phenomenon of the word frequency distribution. We postulate that the concavity arises from a seamless fusion of two power laws around the knee; this fusion is the byproduct of a natural corpus generative model that we introduce. To validate our model, we examine a variety of corpora, ranging from novels to news articles, and t the functional form that comes out of our process to their word frequency distributions.
e t is surprisingly accurate, at the head, the tail, and the knee of the distributions.
Informally stated, our model works in two stages. In the rst stage, a vocabulary for the corpus is generated by choosing the words from a power law distribution on the language. In the second stage, the corpus is generated by sampling the vocabulary words according to the same or another power law distribution. We show that this two-stage process gives rise to a distribution that is made up of two fused power laws. We validate this model by showing that the distortion between the distributions produced by our model and the empirical distributions is quite small. We also argue that a double Pareto distribution, which is a natural candidate to explain two fused power laws, would not be able to produce such a small distortion. e use of a two-stage process is convenient for modeling corpora obtained from di erent topics (e.g., sports, politics), where the rst stage selects the topic vocabulary. Latent factor models [24, 25] also use a two-level process for text generation; however, each word in the text is determined by a mixture of topics rather than a single topic.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080821

We then turn our a ention to the distribution of k-grams, which has also been studied [17, 21]. It has been observed for some English and Chinese corpora that the distribution becomes a er as k increases [22, 23]. However, to the best of our knowledge, no work
has tried to explain this phenomenon. We prove analytically that the k-gram distributions become a er as k increases, under the simple assumption that the head of the word frequency distribution
is a power law.

385

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

2 RELATED WORK
Power laws, also known as Pareto distributions or Zipf's laws, have been observed in a broad range of se ings, i.e., city populations, sizes of earthquakes, number of citations received by papers, sales of books, number of hits on webpages, etc. [33]. According to Mitzenmacher [32], " e rst known a ribution of the power law distribution of word frequencies appears to be due to Estoup [18], although generally the idea (and its elucidation) are a ributed to Zipf [44­46]."
e power law, as stated by Zipf [46] ( = Kx- ), appears as a straight line on a log-log plot. Mandelbrot [29] extended this form to = K (B + x )- to obtain a more accurate t for high-frequency words. On the other hand, Simon [41] developed a stochastic process based on the work of Yule [43]. Sichel [40] studied empirical t of word frequencies with compound Poisson distribution. Baayen [5] compared di erent statistical models proposed in previous work for word frequency distribution. ese early papers used smallscale datasets (by today's standards) and therefore do not necessarily provide a good t to the tail of large datasets. Ha et al. [23] noted that on large-scale datasets, the word frequency distribution clearly has a concavity when plo ed on the log-log scale. at is, the curve bends away from the single straight line predicted by Zipf's law. is phenomenon was more pronounced when they looked at the distribution of Chinese characters. Ha et al. [23] did not a empt to explain the form of the curves. e double Pareto distribution, which approximates the concavity with two straight lines, has also been considered for approximating word frequency distributions [14]; the t achieved by our model is signi cantly more accurate than the one achievable by double Pareto distributions (see Section 5.3). Baayen [4] studied similarity relations between words and word frequency distribution. He also noted that function words straighten out the head of the distribution and complex words straighten the tail. Samuelsson [37] related Zipf's law to Turing's local re-estimation formula and van Leijenhorst and van der Weide [42] related Zipf's law to Heap's law.
ere has been some work on studying the distribution of kgrams as well. Ha et al. [22, 23] studied k-grams in English and Chinese and observed that the distribution became a er as k increased. Character k-grams and k-tuple distributions were also studied in [17] and [21]. None of these works a empted to explain these phenomena; in our work, we analytically establish the form of the k-gram distributions and show these become a er as k increases.
Various generative models have been proposed for producing power laws. Zipf [46] hypothesized that the power law is the result of the "principle of least e ort"; this was re-examined later by Mandelbrot [29], Ferrer i Cancho and Sole [20], and Ferrer i Cancho et al. [19], who developed arguments for deriving power law distributions based on information-theoretic considerations. In another line of research, people have argued that preferential a achment can lead to power laws. e general argument can be traced back to Yule [43], and a generalization was proposed by Simon [41]. Perc [34] proposes a preferential a achment process for the evolution of a language, and uses it to derive the Zipf's law. Power laws can also be obtained through the "monkeys typing randomly" (or "not-so-randomly") processes [15, 28, 31]. None of

these works a empted to explain the concavity in the tail of the word frequency distribution. Mitzenmacher [32] provides a good survey on the topic of power laws.
3 EMPIRICAL ANALYSIS
We rst study if the concavity in word frequency distribution is pervasive and robust, i.e., does it exist over a broad range of datasets and does it exist even when we restrict the data to a speci c genre or topic? A plausible hypothesis for observing the concavity could be that for a collection of text restricted to a given genre or a given topic, the distribution would be straighter; and mixing such distributions leads to the concavity. To test this hypothesis, we constructed di erent datasets that can be split by di erent criteria. What we observe is that the concavity exists for each sub-sample.
3.1 Datasets
We conducted our empirical studies over the following four datasets. e rst is Gutenberg, which is a mixed-genre, multi-topic, multi-
lingual, and multi-author corpus of electronic books that are in the public domain from the Gutenberg project. We use the average of the birth and death years of the author as the approximation for the publication year of the book. We took the subset of 16,797 books that were wri en in English and has a publication year between the 17th century and the 20th century, and grouped them into four disjoint time periods (by century). e vocabulary sizes range from 100K words to over 800K words, and corpus sizes from 10 million to over 500 million tokens. In addition, we can also sample this dataset by authors.
e second dataset is News, which is a large-scale collection of news articles on two topics, namely, sports and politics. e third dataset is ANC (American National Corpus), which is a collection of American English, with wri en texts of di erent genres and transcripts of spoken data produced post 1990. e fourth dataset is Europarl, which is a multilingual collection of European Parliament proceedings [27]. It includes semantically equivalent content in 21 European languages. e size of the text in each language ranges from 10 million to 50 million tokens. is allows us to examine the word frequency distribution in multiple languages without having to worry whether di erences were due to di erences in topics or genres.
All datasets went through the same preprocessing, where all punctuation marks were removed, and all remaining tokens lowercased. Table 1 shows the main statistics of each of these four datasets.

vocabulary size corpus size

dataset

(# types) (# tokens)

Gutenberg 19th News (politics) ANC (wri en)

400,876 256,758 115,806

185M 31M 8.6M

Europarl

87,554

56M

Table 1: Details of the four datasets

386

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: Word frequency distribution in the Gutenberg datasets: books in di erent time period (centuries), as well as a random subset of authors, and the subset of authors whose names begin with "J".
Figure 2: Word frequency distribution in news articles: politics vs sports
3.2 Word frequency distribution
First, we plot the empirical observations of word frequency distributions in di erent datasets on a log-log scale. We observe a clear concave shape over a broad range of corpora (Figures 1­3). Figure 1 shows word frequency distributions for di erent time periods in the Gutenberg dataset. As we can see, while the time periods (and vocabulary sizes) di er greatly, all curves closely resemble each other. In fact, the curve for AuthorJ (authors whose names begin with "J") largely follows the same shape. In subsequent plots, we include the AuthorJ curve as a reference point.

Figure 3: Word frequency distribution in 21 European languages. English is shown in blue solid line, other languages shown in dotted lines

Figure 2 shows word frequency distributions for two di erent topics (politics vs sports) in News. We observe a similar concave shape for both of them as AuthorJ. Figure 3 shows that the concave shape exists in a broad range of (21 European) languages. Furthermore, one could have hypothesized that smaller vocabulary leads to a straighter line (given previous studies with smaller datasets that focused on the straight-line Zipf distribution); but note the curve for English exhibits a more concave shape than that for AuthorJ, even though it is a smaller corpus and arguable over a more focused range of topics with less variations in styles.

3.3 k-gram frequency distribution
Figure 7 plots the empirical distribution of k-grams for k = 1 up to 5. Given the space constraints, we include only the plot for the Europarl data (where the unigram frequency distribution exhibits the highest degree of concavity). As observed in [22, 23] for some English and Chinese corpora, the lines get a er as k grows.

4 MODEL

We de ne a simple and natural stochastic process for generating a

corpus in a language. e process takes place in two stages. In the

rst stage, a founding text for the topic is wri en by choosing words

from the language; the set of distinct words used in the founding

text will form the vocabulary of the topic. In the second stage, the

corpus itself is generated using the words in the vocabulary.

Let the parameters ,   (0, 1),  > 0, and a positive integer n,

be given. Here, 

is

the

exponent

of

the

power

law

P

( |U

) |

de

ned

over the universe of words U ; n will be length of the founding text;

 determines the position of knee (which will be located around

n ). e parameter  is not necessary, but lends more exibility to

our model as we will see below.
1- 
We set N = n 1- = |U |, i.e., N is the number of words in the

language. e distribution on the language will be the power law

387

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

corpus







n

Gutenberg 19th 0.618 0.795 1.034 400,876

News (politics) 0.569 0.725 0.898 256,758

ANC (wri en) 0.595 0.866 0.996 115,806

Europarl 0.691 0.800 0.986 87,554

Table 2: Fitted parameters for various corpora.

P

( N

)

.

As

we

will

see

later,

this

choice

of

N

guarantees

that

the

knee

will be positioned close to the rank n .

(i) In the rst stage, we choose n words independently from U

according

to

the

power

law

P

( N

)

to

produce

the

founding

text

of

the topic. e vocabulary V of the topic will be the set of words

that appeared at least once in the founding text. As we will see

below, with high probability we will have |V |  n.

(ii) In the second stage, we use a second power law PN( ) over

the language, possibly but not necessarily di

erent

from

P

( N

)
.

A

corpus for the topic is generated by choosing words independently

from PN( ) restricted to the vocabulary (i.e., support) V .

e use of a two-stage process is convenient to model corpora

belonging to di erent topics (e.g., sports, politics): the rst stage

e ectively determines the vocabulary of the topic. e assumption of choosing words from V according to the original power law PN( ) has been made before; see, for example [40].
In our model, the parameter  gives additional exibility, since one is not forced to use the same power law exponent  to choose words from the vocabulary. If we insist on model parsimony, the  parameter can be removed by choosing  as a function of .

Figure 5 shows the best t of our model (formalized in the next

Section) to four empirical corpora. Note that the t from our model

traces the empirical distribution quite accurately (we discuss this

in Section 5.4). We include the parameters for each t in Table 2.

5 THEORETICAL ANALYSIS
We prove in this section that, while having a very small number of parameters, our model is able to generate curves that match the empirically observed curves. We then study the distribution of k-grams and prove that the distribution gets a er as a function of k, which matches the empirical observation as well.
All the proofs missing from this section can be found in the Appendix.

5.1 Preliminaries

We begin with some basic notation. Let the upper incomplete

Gamma function be given by (a, b) =

 b

xa-1e-x dx;

let

 (a )

=

(a, 0). e function (a, b) is well-de ned for every real a if b > 0

and is well-de ned for every a > 0 if b = 0. For each integer k  1,

we have (k ) = (k - 1)!.

Let  ( ) =

 i =1

i

-

be

the

value

of

the

Riemann

Zeta

function

at

 > 1. For  > 0, and an Suppose that  > 1, and

integer N let P (i)

 1, let = i- /

N ( ) =

N i =1

i -

.

( ) for each integer

i  1. en, P ( ) is the power law distribution with exponent  ,

de ned on the universe U = {1, 2, . . .} = N.

Also, let [N ] = {1, 2, . . . , N }. For  > 0, if we let U = [N ], we

have that the truncated power law distribution on U is given by

PN( ) (i) = i- /N ( ), for i  U .

We

rst

obtain

some

bounds

on

N

(

)

and

P

( N

)

(i ).

L

5.1.

For each 0

<



<

1, it holds that N ( )

=

N 1- 1-

±

O (1) and PN( ) (i) = 1 ± O N  -1

i -

1- N 1-

.

5.2 Word frequency distribution

In this section we analyze the word frequency distribution produced
by the generative model. We proceed to study the probability R(i) that the ith word, 1  i  N , appears in the vocabulary V . Since V was constructed using n independent samples, we have

R(i) = 1 - (1 - PN( ) (i))n .

By N =

1+O

n-

1-  1-

1- 
· n 1- , and by Lemma 5.1, we have

P

( N

)

(i

)

=

1- i n1- 

-O

i- n2  -2

.

at is,

PN( ) (i ) = 1 - O n  -1

1- . i n1- 

Since 0 < ,  < 1 the multiplier is no worse than 1 - o(1).

Our analysis begins by showing that R(i) -- that is, the probability

that the ith term of the language appears in the vocabulary -- can

be expressed by a simple exponential term.

L

5.2. R(i) = (1 ± o(1))

1

-

e - (1-

)

n  i

.

Lemma 5.2 can be shown by approximating PN( ) (i) as in Lemma 5.1, since the error term in Lemma 5.1 is small enough to prove the

statement of Lemma 5.2.

We then use the new expression of R(i) to compute the expected

number of terms with rank at most k that appears in the vocabulary.

Speci cally, let Uk  U be the set of words that have rank at most

k in PN( ). We focus on the number of words in Uk that make it to

the vocabulary V . I.e., we focus on the random variable |V  Uk |.

Observe that E[|V  Uk |] =

k i =1

R (i ).

Lemma

5.3

shows

that

this

expectation is very well approximated by the function D (k ) (we

use this notation as a shorthand for D,n (k )):

D (k ) = k - (1 -  )1/ n 

1 - , (1 - )

n



.





k

L

5.3. E[|V  Uk |] = (1 ± o(1))D (k ).

e above lemma can be shown by integrating the expression of R(i) that we obtained in Lemma 5.2, and by controlling the error
term. e next step of the proof is showing that D (k ) behaves like a
simple polynomial in the ranges k < o(n ) and k >  (n ), i.e., for all k far enough from the knee. is will be key for proving that
the head and the tail of the nal distribution will be power laws.

L

5.4. If k < o n , then D (k ) = (1±o(1))k. If k >  n ,

then D (k ) = (1 ± o(1))n  k1- .

388

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

e proof of Lemma 5.4 is relatively simple. We just have to use

the approximations of the R(i)'s given by Lemma 5.2, i.e., R(i) =

1 - o(1) if i < o

n

,

and

R (i )

=

(1

±

o(1))n 

1- i

if i > 

n

.

en, the linearity of expectation, and Lemma 5.2, directly entail

the claim.

Negative dependence can be used to prove the next Lemma,
which simply states that with high probability for each k  [N ], the random variable |V  Uk | will be quite close to its expectation E[|V  Uk |]; by Lemma 5.3, that random variable will then be very close to the function D (k ) itself.

L

5.5. With probability 1 - o(1), we will have that for each

k  [N ],

|V  Uk | = (1 ± o(1)) · D (k ).

Lemma 5.5 allows us to get an expression for the frequency curve of the corpus. For k  1, the frequency curve can be expressed parametrically as
x (k ) = D (k ), and (k ) = W · k- ,
where W is the normalization factor. In other words, the abscissa x ( ) that one has to associate to a given ordinate is equal to x ( ) = D W 1/ .

Finally, we can state our main result about the word frequency distribution of the generative model. It follows as a corollary from Lemma 5.4 and Lemma 5.5.

T

5.6. With probability 1 - o(1), we will have:

(i) |V | = (1 ± o(1))n, and

(ii) for each rank 1  k  |V |, the probability associated to the word

of rank k in V will be proportional to

(1 ± o(1)) · k- ,

(1 ± o(1)) ·

k n 

-

 1-

,

if k = o n , if k =  n .

eorem 5.6 states the main properties of the word frequency distribution: the model produces a vocabulary of size close to n, the head of the vocabulary frequency curve follows a power law with exponent - , and the tail follows a power law with exponent - /(1- ). Moreover, our parametric de nition of the curve gives a precise description of how the two power laws merge in one another.

is is important for us since we want to precisely t the curve to the

datasets we have. Figure 4 shows the word frequency distribution

produced by our model. Observe that our model produces two power laws that are joined around the knee at n , as expected.

5.3 Relation with double Pareto

One might wonder why we did not use a simple double Pareto curve

(as in [14]) to model the distributions. e main reason is that the ratio of the probability at the rank i n of the double Pareto curve

(with the correct power laws, and the correct knee) and of our curve

at the same i is quite large. We will show in this section that (i)

the multiplicative distortion is at least

e e -1



= (1.5819 . . .)

for

  0, and (ii) the distortion diverges exponentially to  as 

approaches 1. Moreover, we numerically obtain that at  = 0.6

(close to empirical numbers; see Table 2), the distortion becomes (3.0270 . . .) . Later in Section 5.4, we empirically analyze the dis-

tortion in two of our corpora.

Figure 4: e result of an execution of the stochastic process

1- 

with 

=

1, 
2

=

2,
3

=

9 ,n
10

=

106

and N

=

n

1-

= 108. In this

execution, the vocabulary V ended up with a cardinality of

|V | = 984328, i.e., so many distinct words were randomly se-

lected in the rst stage of the process. e curve represents

the probability distribution of the vocabulary. e head of

the curve follows a power law with exponent - = -0.9

and the tail of the curve follows a power law with exponent

-

 1-

= -1.8.

Observe that the two power laws cross at an

abscissa value close to n = 104.

We now show how this distortion can be computed, and obtain
its limiting values at  = 0, 1. First, for a given , let k be the minimum integer such that D (k )  n . Observe that k  n .
e probability of the n th word in our model's dictionary will then be (1 ± o(1)) · W · k- for some normalizing factor W .
Consider the double Pareto curve having the same head and tail
power laws of our curve, and the same knee n . e value of this double Pareto curve at the n th word will be (1 ± o(1)) ·W · n- .

erefore, the distortion of the two curves at the n th word (i.e.,

at the knee) for a given , as n tends to in nity, is at least (d ) ,

where

d

=

lim inf
n

k n

.

We show in Lemma 5.7 that lim 0+ d

=

e e -1

Moreover, we

will show in Lemma 5.8 that, as  converges to 1, d diverges at

least as fast as c1/(1- ) for some constant c > 1.

We state the two Lemmas, and brie y comment on how they

can be proved.

L

5.7.

For 0 < 

<

1 2

,

we

have

k =

e

e -

1

±

O

(

ln

1/ )

· n .

A heuristic proof of the above statement would argue that, if

 = 0, then all the terms are equally likely to be chosen, i.e., they

have probability N -1, with N

1- 
= n 1-

= n. In other words, the

389

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

vocabulary is constructed by throwing n balls (the words in the

founding text), into N = n bins (the words of the language). By

the Cherno bound, any given set of t 1 bins will be hit by

approximately t balls with high probability. Moreover, by classic

balls-in-bins arguments, the

e e -1

· n

balls that hit the

rst

e e -1

· n

bins will be distributed across approximately n distinct bins with

high probability (essentially because of the Poissonian approxima-

tion

of

the

binomial

distribution).

Hence, k



e e -1

· n.

e above reasoning can be made formal, so that it can be applied

to small  > 0.

Finally, we show that in the opposite regime,  = 1 - , d

diverges to in nity at an exponential rate.

L 5.8.  < 1, we have

ere exists a constant c

>

1 such that,

for all

1 2

<

k

1
 c 1-

· n .

e above statement can be proven by partitioning the set of

1
words of index up to c 1-

· n

into buckets in such a way that

words in a given bucket have roughly the same probability of being

selected in the vocabulary. e bucketing makes it easy to compute

the expected number of words per bucket that end up in the vocab-

ulary. Finally, adding up these expected numbers gives the above

lower bound.

5.4 Fitting

e parameters of the ing were obtained by minimizing the

Kullback-Leibler divergence DKL (P ||E) of our model's frequency distribution P from the empirical distribution E. I.e., given E, we

searched for the P that minimizes DKL (P ||E) =

i

P

(i )

·

ln

P (i ) E (i )

.

More precisely, if the empirical distribution E was over n distinct

words then, given a triple of parameters (, ,  ), we computed a

candidate distribution P = Pn,, , by le ing, for each i = 1, . . . , n, P (i) be proportional to ki- with ki equal to the solution of i = D,n (ki ).

We used a brute-force approach to guess the optimal ing

parameters , ,  . e results are reported in Table 2. e empirical

curves, and their ings, are shown in Figure 5. To show how much

be er our curve's ts are (with respect to the double Pareto ts),

we plot in Figure 6 the ratios between the probabilities given by our

curve and the actual distribution, and those given by double Pareto

and the actual distribution, for the Gutenberg and News corpora.

6 K-GRAM FREQUENCY DISTRIBUTION
Let P ( ) be the power law distribution with exponent  > 1 over an in nite language U = N. Given an integer k  1, let P (,k ) be the probability distribution on k-tuples u1, . . . , uk , where u1, . . . , uk  U are chosen independently from P ( ). We now show analytically that (i) the distribution of P (,k ) will be close to the original power law P ( ) and (ii) the curves corresponding to k-grams will become
a er as k increases, when plo ed on a log-log scale; this phenom-
enon can be observed empirically in Figure 7.

T

6.1. If we sort the k-tuples decreasingly by their proba-

bilities in P (,k ), then the probability of the r th k-tuple will be equal

Figure 5: e empirical and the ( tted) theoretical curves of four corpora.

390

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 7: k-grams frequency distribution for the Europarl dataset (English).

Figure 6: A log-log plot of the ratios between our tted curve (resp., the Double Pareto curve) and the empirical curve, at word ranks 500 to 50000 (that is, around the knee). e vertical dashed line represents the position of the Double Pareto knee. Our curve is a very good multiplicative approximation of the empirical curve (the ratios induced by our curve are quite close to 1), and generally a much better approximation than Double Pareto; the maximum ratios, or distortions (see Section 5.3), incurred by Double Pareto happen around the knees: the ratios, there, are close to 2.8 in the Gutenberg corpus, and to 2.3 in the News corpus.

to

(1 ± or (1)) ·  ( )-k · (k )- ·

lnk-1 r r


.

P . Our goal is to compute the position (or rank) ri1, ...,ik of the product P ( ) (i1) · · · P ( ) (ik ) in the ordered multiset
P ( ) (j1) · · · P ( ) (jk ) | j1, . . . , jk  Z+ .
In other words, we aim to compute the number of tuples j1, . . . , jk such that P ( ) (i1) · · · P ( ) (ik ) < P ( ) (j1) · · · P ( ) (jk ). Rewriting this condition using the fact p(i)  i- and le ing n = i1 · · · ik , we get

r = rn = j1, . . . , jk | j1 · · · jk < n

=

n lnk-1 n (k )

+ O (n lnk-2 n),

Figure 8: Computed k-gram distribution,  = 1.5.

which follows from [35, 39]. Inverting this, we obtain

n

=

(1

+

or

(1))(k )

r lnk -1

r

.

e proof is concluded by recalling that the probability of the tuple i1, . . . , ik is
P ( ) (i1) · · · P ( ) (ik ) =  ( )-k n- .

Observe that our eorem gives sharp bounds on the probability of the r th k-gram, as r diverges. Figure 8 shows the k-gram distribution computed with a synthetic power law distribution with  = 1.5; Figure 9 shows the curves predicted by eorem 6.1. We can see that the empirical curves agree asymptotically with the
theoretical estimates.

391

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 9: k-grams distribution using eorem 6.1,  = 1.5.
7 CONCLUSIONS
In this paper we took a closer look at the word frequency distribu-
tion. We observed a knee, leading to a concavity, in the empirical
distributions of many di erent kinds of corpora, and proposed a
natural text generation model to explain the knee and the concavity.
We then analytically showed that our model produces distributions
nearly identical to the empirically observed ones. We also analyzed the k-gram distribution that one obtains by picking words independently from a power law distribution. We proved that the k-gram distribution becomes a er as k increases; this phenomenon had only been empirically observed in the literature but never analyzed.
Our generative model opens up many interesting questions: can
the distributions it produces be used in applications such as text
compression, translation, and information retrieval?
REFERENCES
[1] IJsbrand Jan Aalbersberg. 1991. Posting compression in dynamic retrieval environments. In SIGIR. 72­81.
[2] IJsbrand Jan Aalbersberg. 1994. A document retrieval model based on term frequency ranks. In SIGIR. 163­172.
[3] Leif Azzopardi. 2009. ery Side Evaluation: An empirical analysis of e ectiveness and e ort. In SIGIR. 556­563.
[4] Harald Baayen. 1991. A stochastic process for word frequency distributions. In ACL. 271­278.
[5] Harald Baayen. 1992. Statistical models for word frequency distributions: A linguistic evaluation. Computers and the Humanities 26, 5 (1992), 347­363.
[6] L. Douglas Baker and Andrew Kachites McCallum. 1998. Distributional clustering of words for text classi cation. In SIGIR. 96­103.
[7] Krishna Bharat and Monika R. Henzinger. 1998. Improved algorithms for topic distillation in a hyperlinked environment. In SIGIR. 104­111.
[8] David C Blair. 1990. Language and Representation in Information Retrieval. Elsevier Science Publishers.
[9] David C. Blair. 2002. e challenge of commercial document retrieval, Part I: Major issues, and a framework based on search exhaustivity, determinacy of representation and document collection size. Information Processing & Management 38, 2 (2002), 273­291.
[10] Andrew D Booth. 1967. A "Law" of occurrences for words of low frequency. Information and Control 10, 4 (1967), 386­393.
[11] Nieves R. Brisaboa, Antonio Farin~ a, Susana Ladra, and Gonzalo Navarro. 2008. Reorganizing compressed text. In SIGIR. 139­146.
[12] Nieves R. Brisaboa, Antonio Farin~ a, Gonzalo Navarro, and Jose´ R. Parama´. 2007. Lightweight Natural Language Text Compression. Information Retrieval 10, 1 (2007), 1­33.

[13] Soumen Chakrabarti, Mukul Joshi, and Vivek Tawde. 2001. Enhanced topic distillation using text, markup tags, and hyperlinks. In SIGIR. 208­216.
[14] Flavio Chieriche i, Ravi Kumar, and Prabhakar Raghavan. 2009. Compressed web indexes. In WWW. 451­460.
[15] Brian Conrad and Michael Mitzenmacher. 2004. Power laws for monkeys typing randomly: The case of unequal probabilities. IEEE Transactions on Information eory 50, 7 (2004), 1403­1414.
[16] Devda Dubhashi and Alessandro Panconesi. 2009. Concentration of Measure for the Analysis of Randomized Algorithms. Cambridge University Press.
[17] Leo Egghe. 2004. e distribution of N -grams. Scientometrics 47, 2 (2004), 237­252.
[18] Jean-Baptiste Estoup. 1916. Gammes Ste´nographiques. Institut Stenographique de France.
[19] Ramon Ferrer i Cancho, Oliver Riordan, and Be´la Bolloba´s. 2005. e consequences of Zipf's law for syntax and symbolic reference. Proceedings of the Royal Society B: Biological Sciences 272, 1562 (2005), 561­565.
[20] Ramon Ferrer i Cancho and Ricard V. Sole. 2003. Least e ort and the origins of scaling in human language. PNAS 100, 3 (2003), 788­791.
[21] Xiaocong Gan, Dahui Wang, and Zhangang Han. 2009. N -tuple Zipf analysis and modeling for language, computer program and DNA. Technical Report 0908.0500v1. arXiv.
[22] Le Q Ha, P Hanna, D W Stewart, and F J Smith. 2006. Reduced n-gram models for English and Chinese corpora. In COLING-ACL. 309­315.
[23] Le an Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J. Smith. 2002. Extension of Zipf's law to words and phrases. In COLING. 1­6.
[24] omas Hofmann. 1999. Probabilistic Latent Semantic Analysis. In UAI. 289­296. [25] omas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In SIGIR. 50­57. [26] orsten Joachims. 2001. A statistical learning model of text classi cation for
support vector machines. In SIGIR. 128­136. [27] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine transla-
tion. MT summit. (2005).
[28] Andreas Krause and Andreas Zollmann. 2002. Not so randomly typing monkeys­
Rank-frequency behavior of natural and arti cial languages. Algorithms for
Information Networks­Project Report. (2002).
[29] Benoit Mandelbrot. 1953. An informational theory of the statistical structure of language. In Communication eory, W. Jackson (Ed.). Bu erworths, London, 486­502.
[30] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu¨tze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York.
[31] George A Miller. 1957. Some e ects of intermi ent silence. e American Journal of Psychology 70, 2 (1957), 311­314.
[32] Michael Mitzenmacher. 2004. A brief history of generative models for power law and lognormal distributions. Internet Mathematics 1, 2 (2004), 226­251.
[33] Mark EJ Newman. 2005. Power laws, Pareto distributions and Zipf's law. Contemporary Physics 46, 5 (2005), 323­351.
[34] Matja Perc. 2012. Evolution of the most common English words and phrases over the centuries. Journal of e Royal Society Interface 9, 77 (2012), 3323­33238.
[35] Adolf Piltz. 1881. U¨ber das Gesetz, nach welchem die mi lere Darstellbarkeit der natu¨rlichen Zahlen als Produkte einer gegebenen Anzahl Faktoren mit der Gro¨sse der Zahlen wa¨chst. Ph.D. Dissertation. University of Berlin.
[36] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing & Management 24, 5 (1988), 513­523.
[37] Christer Samuelsson. 1996. Relating Turing's Formula and Zipf's Law. In Proceedings of the 4th Workshop on Very Large Corpora. 70­78.
[38] Issei Sato and Hiroshi Nakagawa. 2010. Topic Models with power-law using Pitman­Yor process. In KDD. 673­682.
[39] Atle Selberg. 1954. Note on a paper by L. G. Sathe. J. Indian Math. Soc., N. Ser. 18 (1954), 83­87.
[40] Herbert S Sichel. 1975. On a distribution law for word frequencies. J. Amer. Statist. Assoc. 70, 351a (1975), 542­547.
[41] Herbert A Simon. 1955. On a class of skew distribution functions. Biometrika 42, 3/4 (1955), 425­440.
[42] D.C. van Leijenhorst and .P. van der Weide. 2005. A formal derivation of Heaps' law. Information Sciences 170 (2005), 263­272.
[43] G Udny Yule. 1925. A mathematical theory of evolution, based on the conclusions of Dr. JC Willis, FRS. Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character 213 (1925), 21­87.
[44] George K. Zipf. 1932. Selective Studies and the Principle of Relative Frequency in Language. Harvard University Press.
[45] George K. Zipf. 1935. e Psycho-Biology of Language: An Introduction to Dynamic Philology. Houghton Mi in Company.
[46] George K. Zipf. 1949. Human Behavior and the Principle of Least E ort. AddisonWesley Press.

392

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

APPENDIX

Proof of Lemma 5.1

First we upper bound N ( ) with one (the rst term of its sum) plus the area under x- in the interval [1, N ],

N ( )  1 +

N 1

x -

dx

=

N 1- -  . 1-

Analogously, a lower bound is given by the area under (x + 1)-

in the interval [0, N ],

N ( ) 

N
(x
0

+ 1)-

dx



N

1-
1-

- 

1

.

If



<

1,

using

the

expression

for

P

( N

)

(i

),

we

get

i -

N

1-
1-

 -





PN( ) (i )



i -

1- N 1-

 -

1

.

Proof of Lemma 5.2

If i

<

o(n ),

then

observe

that,

by

R (i )

=

1-

(1

-

P

( N

)

(i

)

)n

and

P

( N

) (i)

=

1-O

n  -1

1- i  n1-



,

we

have

R(i) = 1 - o(1).

(1)

e right-hand expression in our claim simpli es to: (1 ± o(1)) 1 - e- (1) = 1 ± o(1).

e claim is thus proved. Next assume that i is a positive integer such that i >  (n2-1/ ). Observe that, by ,  < 1, this case includes all the i's that are not

part of the previous case. For 0 < a < 1 , and b > 0, it holds that
2

e-ab  (1 - a)b  e-ab-2a2b .

Recall that 1 - R(i) = (1 - PN( ) (i))n . en, we bound the following quantities, using (5.2):

(1) nPN( ) (i) = (1 -  )n  i- + O

n  i

n



-1

. Observe that

the error term is o(1) for each i in our range.

(2)

2n

P

( N

)

(i

)

2
=

nP

( N

)

(i

)

P

( N

)

(i

)

=O

n  i

n



-1

,

since PN( ) (i) = O n -1 for each i  1.

It follows that

1 - R (i ) = e-(1- )n  i- ±O

n  i

n

 -1

=

1±O

n  i

n



-1

e-(1- )n  i- .

Moreover, since 0  e-(1- )n  i-  1, we have

R (i ) = 1 - e-(1- )n  i- ± O

n  i

n



-1

Observe that, if i  n , then we have 1 - e-(1- )n  i-  1 -

e-1+ = (1), while O

n  i

n



-1

 O n  -1 .

at is, R(i) =

(1 ± o(1)) 1 - e-(1- )n  i- .

On the other hand, if i  n , then if we let x be the exponent of the exponential term, we have 0  x  1. For this range of

x's, it holds e-x  1 - x -- equivalently, 1 - e-x  x . erefore,

2

2

1 - e-(1- )n  i-  1 (1 -  )n  i- . erefore, even for i  n ,

2

we have

R (i ) = (1 ± o(1)) 1 - e-(1- )n  i- .

Proof of Lemma 5.3
For each integer k  1 and for each non-decreasing and nonnegative function f (x ) admi ing a nite integral in [0, k + 1], we have

k

k

k +1

FL = f (x )dx  f (i) 

f (x )dx = FU .

0

i =1

1

Suppose that 0  f (x )  1. en,

k +1

1

FU - FL =

f (x )dx - f (x )dx  1,

k

0

and hence

k i =1

f

(i )

=

FL

+

for some 



[0, 1].

Observe that for all q > 0 and   (0, 1), the function f (x ) =

e-qx- satis es the above conditions. We also have

f

(x )dx

=

1 q1/  

-

1 

,

qx

-

+ c,

where c

is

a

constant.

By

choosing q

=

(1- )n 

,

we

get e-(1-

)

n  i

=

f (i). Now,

FL =

k

e

-(1-

)

n  i

- =

i =1







lim 

 0+

 

(1 -  )n  

1/


1 -

,

(1

-  )n  x

k
     

=

 

 

x

=

n

(1

-  )1/ 



1 - , (1 - ) ·

n k



,

since by de nition limx (a, x ) = 0. us, we have

k

1

-

e

-(1-

)

n  i

= D (k ) -  .

(2)

i =1

Now, consider the rst o(n ) terms of the LHS sum in (2). e exponent of e in each of them is  (1), and therefore each of them has value 1 - o(1). It follows that the LHS of (2) has value  (1). Since 0    1, we have

k
(1 ± o(1))

1

-

e

-(1-

)

n  i

= D (k ).

i =1

e claim then follows from Lemma 5.2 and the linearity of expectation.

Proof of Lemma 5.4
e rst part is implied directly by (1) and Lemma 5.3. Hence, let k =  n . Let us de ne = n k to be the ceiling of the geometric mean of n and k. Observe that  n < < o(k ).

393

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Since R(i)  1, we have

Proof of Lemma 5.7

k

k

R(i)  E[|V  Uk |]  +

R (i ) .

i= +1

i= +1

By Lemma 5.2, we have that R(i)

=

(1 ± o(1)) · n 

1- i

whenever

i > , since >  n . en, we can write:

k

k

R(i) = (1 ± o(1))(1 -  )n 

i -

i= +1

i= +1

= (1 ± o(1))(1 -  )n  k ( ) -  ( )

= (1 ± o(1))n  k1- ,

where the last step follows from Lemma 5.1. e value of the sum is  n  k1- =  n , since k >  n . erefore,

E[|V  Uk |] = (1 ± o(1)) · n  k1- .

Suppose that i = x · n . en, R(i ) = (1 ± o(1)) 1 - e-(1- )x - .

Observe that, if x



 , then x-



 -

= e ln

1 

= 1+O



ln

1 

.

Moreover, if x  e then, x-  e- = 1 - O ( ).

By the monotonicity of x- we thus obtain that x- = 1 ±

O



ln

1 

for all x  [, e].

For all n  i  en , we then have

R(i) = (1 ± O ( ln 1/ )) · 1 - e-1 .

For i < n , we have R(i)  1.

erefore, for

k i =1

R (i )

to

be

at

least n , we need

k n



1 1 - e-1

- O ( ln 1/ ).

Moreover, for the inequality to hold, it su ces to have

Lemma 5.3 completes the proof.

Proof of Lemma 5.5

Observe that, by Lemma 5.3, it is su cient to prove that, with

probability 1 -o(1), it will happen that, for each k  [N ], |V Uk | =

(1 ± o(1))E [|V  Uk |].

Let us de

ne X

=

n

log-

1 

n.

We will use two arguments for

proving the claim: one that holds if k < o(X ), and one that holds if

k >

X 1-

, for any constant 0 <  <


.

4

First, consider k < o(X ). Recall that we have PN( ) (i) = 

1 n

n i



.

erefore,

for

i

<

o(X ),

we

have

P

( N

)

(i

)

>



log n n

. For the same

i's, therefore, we have

R(i)  1 -

1-

log n n

n
 1 - n- (1) .

By the union bound, the probability that at least one of the terms
of rank i < o(X ) in U does not end up in V is n- (1). e claim is then proved for each k < o(X ).
Now consider k >  X 1- . Let Yi, j be the indicator random variable of the event "the jth term of the founding text happened to be the ith term of the language". en, for each j, the variables Y1, j , Y2, j , . . . , YN, j are negatively associated (see Chapter 3 of [16]). Moreover, by closure under product, the variables Yi, j , for each i  [N ], j  [n] are as a whole negatively associated. Finally,

since max is a monotone non-decreasing function the variables

Yi = maxj=1, ...,n Yi, j , i  [N ], are also negatively associated --

the Cherno bound can then be applied to their sum, that is, to

k i =1

Yi

=

|V

 Uk |.

us, for each 0 <  < 1,

Pr [ V  Uk - E [ V  Uk ]  E [ V  Uk ]]

2e

-

2 3

E

[

|V

Uk

|]

.

Since k >  X 1- , we have E[|V  Uk |] >  n-2 . If we

choose



=

n-

1 2

 +2 ,

we

get:

Pr [ V  Uk = (1 ± 2 ) · E [ V  Uk ]]  e-(n2 ) . By the union bound, the claim is then proved for each k >  X 1- .

k n



1 1 - e-1 + O ( ln 1/ ) .

Proof of Lemma 5.8

Let us de ne  = 1 - . Recall that E[|V  Uk |] =

k i =1

R (i ).

Let

1 = t0 < t1 < . . . < tr = k be integers and, for 0  j  r - 1, let pj

be any real number such that pj  R(tj ). en, by the monotonicity

of the R(i)'s, we have pj  R(i) for 1  i  tj . erefore,

r tj

r tj

E[|V  Uk |] =

R(i) 

R (t j -1 )

j=1 i =tj-1

j=1 i =tj-1

r

 tj · pj-1 .

j =1

We set t0 = 1 and, for j  1, let tj =

j -2
2

· n

.

We let r

be

unspeci ed for now. Also, let p0 = 1, and, for j  1,

pj

=

1

-

e - (1-

)n



t

- j

= 1 - e- 22-j

O

 2-j

.

We

have

that

1 

-1

=

1 1-

-1



O ( ).

For j

=

1, we have

tjpj-1 = t1 

n 2

. Moreover, for j  2,

tj · pj-1  O

n

2j

(

1 

-1)

= n 2O (j ) .

As j  O (1/ ), the la er is at most O (n ). In fact, there exists a constant b > 0 such that if we let r = b/ , we have

E[|V

 Uk

|]



 

n





2





     

+

r
O
j =2

n

< 3 · n . 4

Lemma 5.3 shows that D (k ) = (1 ± o(1))E[|V  Uk |]. With our choice of r , k equals

k = tr =

r -2
2

· n

 c1/ · n ,

for some constant c > 1. erefore, D c1/ · n  (1 ± o(1)) 3n . By the la er, and by
4
the monotonicity in k of E[|V  Uk |], we have that k  c1/ · n .

.

394


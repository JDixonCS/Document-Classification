Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Non-negative Matrix Factorization Meets Word Embedding

Melissa Ailem
LIPADE - Paris Descartes University melissa.ailem@parisdescartes.fr

Aghiles Salah
LIPADE - Paris Descartes University aghiles.salah@parisdescartes.fr

Mohamed Nadif
LIPADE - Paris Descartes University mohamed.nadif@parisdescartes.fr

ABSTRACT
Document clustering is central in modern information retrieval applications. Among existing models, non-negative-matrix factorization (NMF) approaches have proven e ective for this task. However, NMF approaches, like other models in this context, exhibit a major drawback, namely they use the bag-of-word representation and, thus, do not account for the sequential order in which words occur in documents. is is an important issue since it may result in a signi cant loss of semantics. In this paper, we aim to address the above issue and propose a new model which successfully integrates a word embedding model, word2vec, into an NMF framework so as to leverage the semantic relationships between words. Empirical results, on several real-world datasets, demonstrate the bene ts of our model in terms of text document clustering as well as document/word embedding.
CCS CONCEPTS
·Computing methodologies  Unsupervised learning; Cluster analysis; Non-negative matrix factorization;
KEYWORDS
Non-negative Matrix Factorization, Word Embedding, Text Document Clustering.
1 INTRODUCTION
Text data clustering and embedding are of great interest in modern text mining and information retrieval (IR) applications, for several practical reasons such as: automatic summarization and organization of documents, e cient browsing and navigation of huge text corpora, speed up search engines, etc. Among existing approaches, Non-negative Matrix Factorization (NMF) [7, 14] has proven very e ective for both text document clustering and embedding. In this context, NMF models decompose the document-word matrix into two non-negative document and word factor matrices containing, respectively, the low dimensional representations--embeddings--of documents and words [14]. Even though clustering is not the primary objective of NMF, it turns out that, the document factor matrix encodes a latent structure of the original data that is well suited to cluster text documents. Since the works of [7, 14], there has been a lot of work to develop new variants of NMF in the direction of clustering, most of which focused on text data [1, 5, 10, 15]. Another
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080727

stream of works has focused on establishing theoretical connections between NMF and k-means clustering [3, 5]. All these e orts highlighted the potential of NMF for text document clustering. Despite its success for handling text data [10], NMF, like other models in this context, still exhibits some shortcomings, namely it uses the "bag-of-words" or vector space representation and, therefore, ignores the sequential order in which words occur in documents.
is is an important issue, because it may result in a signi cant loss of information, especially the semantic relationships between words. Hence, words with a common meaning--synonyms--or more generally words that are part of a common context are not guaranteed to be mapped in the same direction in the latent space.
In order to overcome the above issue, we propose a novel NMF model that explicitly accounts for the sequential order in which words occur in documents so as to capture the semantic relationships between words. e research question lies in how to do so appropriately. In this paper we propose a solution which draws inspiration from the recent success of neural word embedding models, such as word2vec [11], which have proven e ective in learning continuous representations of words--embeddings--that successfully capture meaningful syntactic and semantic regularities between words [12]. More precisely, we rely on the distributional hypothesis [6], which states that words in similar contexts have similar meanings. Given a corpus of unannotated text documents, each of which represented as a sequence of words, the contexts for word wj are the words wj surrounding it in an L-sized window in any document of the corpus. Other de nitions of contexts are possible [8]. us, following the distributional hypothesis, we assume that frequently co-occurring words in a L-word window in any article are likely to have a common meaning. Based on the above assumptions, we propose a novel NMF model, Semantic-NMF, which jointly decomposes the document-word and word-context co-occurrence matrices with shared word factors. e decomposition of the word-context matrix has been shown to be equivalent to a state-of-the-art neural word embedding method, namely the skip-gram model [8].
For inference, we propose a scalable alternating optimization procedure based on a set of multiplicative update rules, which are analogous to those of the original NMF model [7, 14]. Empirical results on several real-world datasets provide strong support for the e ectiveness of the proposed model in terms of both clustering and embedding.
Notation. Matrices are denoted with boldface uppercase letters and vectors with boldface lowercase le ers. e Frobenius norm is denoted by . and the Hadamard multiplication by .
e document-word matrix is represented by a matrix X = (xij )  Rn+×d , its ith row represents the weighted term frequency vector of document i  I, i.e., xi = (xi1, . . . , xid ) where denotes the transpose. e word and context vocabularies are identical and denoted by J . Without loss of generality, we de ne the context for word j  J to be the words surrounding it in a L-word window

1081

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

in any document of the corpus. e word-context matrix is represented by C = (cj j )  Rd+×d , where row j  J corresponds to word wj , column j  J denotes context word wj , and each entry
cj j denotes the number of times word pair (wj , wj ) occurred in a L-word window in any document of the corpus.

2 RELATED BACKGROUND

NMF on a document-term matrix X is to nd the document and

word

factor

matrices

Z

=

(zik )



n×
R+

and W

=

(wjk )



d×
R+

,

respectively, such that X  ZWT . e inference of the factor ma-

trices is usually done by optimizing a cost function that quanti es

the quality of the approximation. e most widely used one is the

square of the

Frobenius norm:

1 2

X - ZWT

2. In the context of

clustering, the document factor matrix Z is usually treated as a

so cluster membership matrix, where zik denotes the degree to which document i belongs to cluster k. A partition of the set of

documents can then be obtained by assigning each document to

the most likely cluster. Notice that, in order to make the solution to

the minimization of the NMF's cost function unique, Z is usually

normalized so as to have unit-length column vectors.

Neural word embedding models seek continuous representations

of words, in such a way that words with a common meaning have

roughly similar embeddings in the latent space. e skip-gram

model [11] tries to achieve this objective by maximizing the dot-

product between the vectors of frequently occurring word-context

pairs. Recently, Levy and Goldberg [8] showed that the skip-gram

model with negative sampling (SGNS) [11] is implicitly factoriz-

ing a word-context matrix, whose cells are the pointwise mutual

information (PMI) shi ed by log(N ), where N is the number of

negative samples in SGNS. e PMI is widely used to quantify the

association between pairs of words. Formally, the PMI between

word wj and its context word wj is given as follows

PMI(wj , wj

)

=

log

p(wj , wj ) . p(wj )p(wj )

(1)

Given the word-context matrix C de ned above, we can estimate

the PMI empirically as follows PMI(wj , wj

)

=

log

cj j ×c.. cj . ×c.j

where

c.. = j j cj j , cj . = j cj j and c.j = j cj j . Since it is intractable

to work directly with the shi ed PMI matrix, which is dense and

high-dimensional, Levy and Goldberg [8] proposed to approximate

it with the sparse Shi ed Positive PMI matrix (SPPMI), M = (mj j )  Rd+×d , de ned as follows

mj j = max{PMI(wj , wj ) - log(N ), 0}.

(2)

Levy and Goldberg [8] proposed, thus, to obtain the word embeddings by performing Singular Value Decomposition (SVD) on M. Nevertheless, they pointed out the importance to investigate other factorization techniques. Given that the above SPPMI matrix is positive and symmetric, we perform a symmetric NMF on M.

3 JOINT NMF AND WORD EMBEDDING
In order to explicitly leverage the semantic relationships among words, we propose to jointly decompose the document-word and word-context co-occurrence matrices with a shared word factor matrix. is gives rise to our model, Semantic-NMF, whose objective

function, to be minimized, is given by

F (Z, W, S)

=

1 2

||X

-

ZW

||2

+

1 2

||M

-

WSW

||2,

(3)

NMF

word embedding

where S  Rk+×k is a symmetric extra factor which plays a key role in the approximation of M. Indeed, it has been found that the above form of symmetric NMF provides be er approximations than its basic variant where S is the identity matrix [9]. Note that, both NMF and word embedding infer low dimensional representations of words. In the former, word factors capture how words are used in documents, while in the la er word embeddings capture the word co-occurrence information. By sharing W between NMF and word embedding in (3), Semantic-NMF seeks to leverage both the above information, simultaneously. Note that, objective function (3) can be interpreted as regularizing NMF's objective function with word embedding. To infer the factor matrices we rewrite (3) as follows

F (Z, W, S) =

1 2

Tr

XX

- 2XWZ

+ ZW

WZ

+

1 2

Tr

MM

- 2MWSW

+ WSW

WSW

.

(4)

In the following, we derive a set of multiplicative update rules in order to minimize F under the constrains of positivity of Z, W and S. Let   Rn×k ,   Rd×k and   Rk×k be the Lagrange multipliers for the above-mentioned constraints, the Lagrange function L(Z, W, S,  , ,  ) = L is given by

L = F (Z, W, S) + Tr ( Z ) + Tr (W ) + Tr ( S ).

e derivative of L with respect to Z, W and S are

L Z

= -XW + ZW

W+

(5a)

L W

=

-X

Z - 2MWS + W

Z

Z + 2SW

WS

+

(5b)

L S

= -W

MW + W

WSW

W +.

(5c)

Making use of the Kuhn-Tucker conditions  Z = 0,  W = 0

and  S = 0, we obtain the following stationary equations:

-(XW) Z + (ZW W) Z = 0

(6a)

-(X Z + 2MWS) W + W Z Z + 2SW WS W = 0 (6b)

-(W MW) S + (W WSW W) S = 0.

(6c)

Based on the above equations we derive the following update rules

Z  Z XW

(7a)

ZW W

WW

X Z + 2MWS W(2SW WS + Z Z)

(7b)

SS

W MW .

(7c)

W WSW W

ese update rules are analogous to those of NMF [7]. e di erence

is in how we update the word factors. In equation (7b), the update

of W depends on two sources of data (i) the document-word matrix

and (ii) the SPPMI co-occurence matrix.

Equation (7a) is similar to that of NMF [7], and equation (7c)

is similar to that of three factor NMF [4]. erefore, based on

eorem 1 in [7] and eorem 7 in [4] the objective function of

1082

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Semantic-NMF is non-increasing under these update rules, for a xed W. Fixing Z and S, it is complicated to demonstrate that (7b)
monotonically decreases (3), the full proof has not been done, yet. Nevertheless, the update rule (7b) is correct because at convergence it satis es the KKT xed point condition in equation (6b). Furthermore, we illustrate empirically, in the experimental section, that the objective function (3) is non-increasing under the above three update rules, and the iterations converge to a locally optimal solution.
Computational Complexity. Below, we shall analyse the computational complexity of semantic-NMF.
Proposition 1. Let nzX and nzM denote respectively the number of non-zero entries in X and M, and let t be the number of iterations.
e computational complexity of semantic-NMF is O(t · · (nzX + nzM ) + t · 2 · (n + d)).
Proof. e computational bo leneck of Semantic-NMF is with the multiplicative update formulas (7a), (7b) and (7c). Equations (7a) and (7c) are similar to those of NMF and symmetric 3-factor NMF, and their respective complexities are O(nzX · + (n + d) · 2) and O(nzM · + d · 2). e number of operations in (7b), including multiplications, additions and divisions, is (2nzX + 2nzM + d + (8d + 2n + 4 + 2)). e complexity of (7b) is thereby given in O( · (nzX + nzM ) + (n + d) · 2). erefore, the total computational complexity of semantic-NMF is O(t · ·(nzX +nzM )+t · 2 ·(n +d)).
Proposition 1 shows that the complexity of Semantic-NMF scales linearly with the number of non-zero entries in the documentword and SPPMI matrix. In practice X and M are very sparse, i.e., nzX n × d and nzM d × d. Furthermore, since multiplicative update rules (7a), (7a) and (7c) involve only basic matrix operations, it is possible to take bene ts from distributed computing. ereby, semantic-NMF can easily scale to large datasets.

4 EXPERIMENTAL STUDY
Our primary purpose is to study the e ects of neural word embedding on NMF. To this end, we benchmark the proposed model, Semantic-NMF (SeNMF), against comparable NMF models. Datasets. We use three popular datasets, described in Table 1, namely CLASSIC41, REUTERS2 the four largest classes of the Reuters corpus and 20-newsgroups NG202. We use the TF-IDF data representation and normalize each document to have unit L2 norm. For each dataset we build the word-context matrix M based on the original corpus, using a 10-word window.

Table 1: Description of Datasets

Datasets
CLASSIC4 REUTERS NG20

#Documents 7095 6387 18846

#Words 5896 16921 26214

Characteristics

#Clusters nzX (%)

4

0.59

4

0.25

20

0.59

Balance 0.323 0.080 0.628

nzM (%) 2.41 0.47 1.80

Competing methods. SeNMF corresponds to the original (NMF) [14] with an extra word embedding term. Hence, we can study the e ects of word embedding on NMF most e ectively by comparing
1h p://www.dataminingresearch.com/ 2h p://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html

SeNMF to NMF. We also consider the orthogonal (ONMF) [15] variant, due to its strong relation to clustering. Settings. We perform 50 runs for all methods, using 50 di erent starting points obtained from the spherical k-means algorithm [2]. We set to the true number of clusters and N to 2 which is a good trade-o between keeping much information and increasing the sparsity of the PPMI matrix [8]. Evaluation metrics. We retain two widely used measures to assess the quality of clustering: the Normalized Mutual Information (NMI) and the Adjusted Rand Index (ARI).
Results and discussion. Figure depicts the values of F (Z, W, S) as a function of the number of iterations. e successive iterations of update rules (7a), (7b) and (7a) monotonically decrease SeNMF's objective function and converge to a locally optimal solution. In table 2, we report the average performances of the di erent models in terms of all metrics, over all datasets. Between brackets, we report the result corresponding to the trial with the highest/lowest criterion. From this table, the proposed model SeNMF markedly outperforms the other competing methods in terms of all metrics. Without the word embedding component, SeNMF reduces to the original NMF model. erefore, we can a ribute the improvement achieved by SeNMF over NMF to the word embedding due to the factorization of the SPPMI matrix.

F(Z,W ,S) F(Z,W ,S) F(Z,W ,S)

4.18e+06 4.16e+06 4.14e+06 4.12e+06 4.10e+06 4.08e+060

20

40

60

80

It e r a t io n s

(a) CLASSIC4

1.09e+07

1.09e+07

1.09e+07

1.09e+07

1.08e+07

1.08e+07

1.08e+07

100

1.08e+070

20

40

60

80

It e r a t io n s

(b) REUTERS

1.96e+07

1.94e+07

1.92e+07

1.90e+07

1.88e+07

100

0

Figure 1: Convergence curves.

20

40

60

80

100

It e r a t io n s

(c) NG20

Table 2: NMI and ARI results over di erent datasets. Increase indicates the di erence between the performances of SeNMF and NMF.

REUTERS NG20 CLASSIC4

Data per. SKmeans
NMI 0.60±0.02 (0.59)
ARI 0.47±0.01 (0.46)
NMI 0.48±0.02 (0.52)
ARI 0.30±0.03 (0.33)
NMI 0.40±0.05 (0.38)
ARI 0.21±0.10 (0.18)

NMF
0.53±0.006 (0.53)
0.45±0.003 (0.45)
0.42±0.02 (0.44)
0.23±0.02 (0.26)
0.35±0.0004 (0.35)
0.13±0.001 (0.13)

ONMF
0.50±0.005 (0.50)
0.42±0.004 (0.42)
0.41±0.01 (0.42)
0.20±0.01 (0.20)
0.36±0.004 (0.36)
0.15±0.02 (0.15)

SeNMF 0.74±0.004
(0.74) 0.69±0.02
(0.73) 0.52±0.02
(0.53) 0.33±0.01
(0.34) 0.61±0.02
(0.62) 0.57±0.02
(0.58)

increase (%) 21% 28% 9% 8% 27% 45%

word/document embeddings. Figures 2a, 2b and 2c show the distribution of the cosine similarities between pairs of top words3 of the same class computed using the word vectors inferred by NMF and SeNMF. Because the cosine similarity is likely to be high between low dimensional vectors (e.g. = 4), we vary from the real number of clusters to 300 for each dataset. e top words of each class have more similar embeddings under SeNMF than NMF.
is con rms that SeNMF does a be er job than NMF in capturing
3Based on the document-word matrix, we select the top thirty words of each true class.

1083

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Cosine sim ila rit y Cosine sim ila rit y Cosine sim ila rit y Cosine sim ila rit y Cosine sim ila rit y Cosine sim ila rit y

1.2

1.2

1.2

1.2

SeNM F

NMF

SeNM F

NMF

SeNM F

NMF

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

1.2

SeNM F

NMF

1.0

0.8

1.2

SeNM F

NMF

1.0

0.8

SeNM F NMF

0.6

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.2

0.0

44

100 100 200 200 300 300
g
(a) CLASSIC4

0.0

44

100 100 200 200 300 300
g
(b) REUTERS

0.0

20 20

100 100 200 200 300 300
g
(c) NG20

0.0

44

100 100 200 200 300 300
g
(d) CLASSIC4

0.0

44

100 100 200 200 300 300
g
(e) REUTERS

0.0

20 20

100 100 200 200 300 300
g
(f) NG20

Figure 2: (a), (b), (c): Distribution of cosine similarities between the top 30 words characterizing each document class, computed using the words' embeddings obtained by NMF and SeNMF.
(d), (e), (f): Distribution of cosine similarities between pairs of documents belonging to the same class, computed using the documents' embeddings obtained by NMF and SeNMF.

Table 3: Top 20 words inferred on CLASSIC4 dataset. corpus.

SeNMF

treatment liver distanc veloc languag execut depart industri

patient rat

angl aerodynam program compil research medic

blood

tissu vehicl wing

scheme

bit institut social

cell

serum

li

propel arithmet symbol univers servic

anim metabol motion nose

code memori school librarian

diseas renal

jet

vortic implement processor scienc profession

plasma lung plane superson algorithm fortran educ societi

tumor lesion radiu

drag

ibm

store public scientist

protein infect

axi

diamet

algol string librari academ

acid hormon pitch chord

input sequenc book nation

PMI : 3.62

PMI : 2.40

PMI : 2.35

PMI : 2.01

NMF

matrix integr commun user

solut transfer languag system

squar determin system inform method pressur comput process

t

invers search public

equat

heat techniqu discuss

function random research retriev

ow

layer program gener

gener normal index librari number bodi problem data

algorithm exponenti scientif docum distribut shock machin time

linear evalu univers book

surfac plate method design

number complex studi servic boundari wing oper compil

order permut scienc catalog e ect laminar automat paper

polynomi gamma literatur journal theori mach code structur

PMI : 2.43

PMI : 1.83

PMI : 1.75

PMI : 1.56

semantics, by making the representations of words which are about the same topic (class) closer to each other in the latent space.
Similarly, gures 2d, 2e and 2f show the distribution of the cosine similarities between pairs of documents belonging to the same class. We observe that documents from the same class (topic) tend to have more similar embeddings under SeNMF than NMF. is provides empirical evidence that accounting for the semantic relationships among words yields document factors that encode the clustering structure even be er. Cluster interpretability. Herein, we compare SeNMF with NMF in terms of cluster interpretability. To human subjects, interpretability is closely related to coherence [13], i.e., how much the top words of each cluster are "associated" with each other. For each cluster k, we select its top 30 words based on the kth column of W. We use the PMI, which is highly correlated with human judgments [13], to measure the degree of association between top word pairs of clusters obtained with each method. Following Newman et al. [13], we use the whole English WIKIPEDIA corpus, that consists of approximately 4 millions of documents and 2 billions of words. Hence, p(wj ) is the probability that word wj occurs in WIKIPEDIA, and p(wj , wj ) is the probability that words wj and wj co-occur in a 5-word window in any WIKIPEDIA document. Table 3 illustrates the top words inferred by SeNMF and NMF on the ClASSIC4 dataset; for each cluster we average the PMI s among its top words. e average PMI's of each method over all datasets, presented in the format (dataset, SeNMF, NMF), are as follows: (CLASSIC4, 2.59, 1.89), (REUTERS, 1.7, 1.21), (NG20, 2.64, 1.49). From these results, it is clear that SeNMF does a be er job than NMF in capturing semantics and inferring more interpretable clusters.

5 CONCLUSION
Inspired by the recent success of neural word embedding models, we propose to jointly perform NMF on the document-word and SPPMI word-context matrices, with shared word factors. is gives rise to a new NMF model which explicitly follows the distributional hypothesis. Empirical results on real-world datasets show that the proposed model, Semantic-NMF, does a be er job than original NMF in capturing semantics. More interestingly, in doing so, Semantic-NMF implicitly brings the embeddings of documents which are about the same topic closer to each other, which results in document factors that are even be er for clustering, as illustrated in our experiments.
Our ndings open up good opportunities for future research. For instance, a possible improvement is to consider regularization schemes already applied to the original NMF. On the other hand, the idea of Semantic-NMF could be extended to other models including the di erent variants of NMF. Another interesting line of future work is to investigate SPPMI matrices estimated using huge external corpora such as WIKIPEDIA and GOOGLE.
Acknowledgment. is work has been funded by AAP Sorbonne Paris Cite´.
REFERENCES
[1] Deng Cai, Xiaofei He, Jiawei Han, and omas S Huang. 2011. Graph regularized nonnegative matrix factorization for data representation. IEEE TPAMI 33, 8 (2011), 1548­1560.
[2] Inderjit S. Dhillon and Dharmendra S. Modha. 2001. Concept Decompositions for Large Sparse Text Data Using Clustering. Mach. Learn. 42, 1-2 (2001), 143­175.
[3] Chris Ding, Xiaofeng He, and Horst D Simon. 2005. On the equivalence of nonnegative matrix factorization and spectral clustering. In SIAM SDM. 606­ 610.
[4] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative matrix t-factorizations for clustering. In SIGKDD. ACM, 126­135.
[5] Chris HQ Ding, Tao Li, and Michael I Jordan. 2010. Convex and semi-nonnegative matrix factorizations. IEEE TPAMI 32, 1 (2010), 45­55.
[6] Zellig S Harris. 1954. Distributional structure. Word 10, 2-3 (1954), 146­162. [7] Daniel D Lee and H Sebastian Seung. 2001. Algorithms for non-negative matrix
factorization. In NIPS. 556­562. [8] Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix
factorization. In NIPS. 2177­2185. [9] Tao Li and Chris Ding. 2006. e relationships among various nonnegative
matrix factorization methods for clustering. In ICDM'06. IEEE, 362­371. [10] Tao Li and Chris HQ Ding. 2013. Nonnegative Matrix Factorizations for Cluster-
ing: A Survey. (2013). [11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013.
Distributed representations of words and phrases and their compositionality. In NIPS. 3111­3119. [12] Tomas Mikolov, Wen-tau Yih, and Geo rey Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations.. In Hlt-naacl, Vol. 13. 746­751. [13] David Newman, Sarvnaz Karimi, and Lawrence Cavedon. 2009. External evaluation of topic models. In in Australasian Doc. Comp. Symp. [14] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on nonnegative matrix factorization. In SIGIR. 267­273. [15] Jiho Yoo and Seungjin Choi. 2008. Orthogonal nonnegative matrix factorization: Multiplicative updates on Stiefel manifolds. In IDEAL. 140­147.

1084


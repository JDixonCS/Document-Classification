Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

On Including the User Dynamic in Learning to Rank

Nicola Ferro
University of Padua, Padua, Italy ferro@dei.unipd.it

Maria Maistro
University of Padua, Padua, Italy maistro@dei.unipd.it

ABSTRACT

Ranking query results e ectively by considering user past behaviour

and preferences is a primary concern for IR researchers both in

academia and industry. In this context, LtR is widely believed to be

the most e ective solution to design ranking models that account

for user-interaction features that have proved to remarkably im-

pact on IR e ectiveness. In this paper, we explore the possibility

of integrating the user dynamic directly into the LtR algorithms.

Speci cally, we model with Markov chains the behaviour of users

in scanning a ranked result list and we modify L

M ,a

state-of-the-art LtR algorithm, to exploit a new discount loss func-

tion calibrated on the proposed Markovian model of user dynamic.

We evaluate the performance of the proposed approach on publicly

available LtR datasets, nding that the improvements measured

over the standard algorithm are statistically signi cant.

CCS CONCEPTS
·Information systems Learning to rank; sis; Retrieval e ectiveness;

ery log analy-

KEYWORDS

L

M ; learning to rank; user dynamic

1 INTRODUCTION

Information Retrieval (IR) systems are nowadays challenged with

increasingly complex search tasks where information about how

users interact with IR systems play a central role to adapt them to

user needs and interests [13]. A lot of IR research focused on

improving e ectiveness, by exploiting information about user-

system interactions recorded in the query logs of Web search

engines. e number of clicks on a given query-result pair, the

click-through rate, and the dwell time, are examples of actionable

information to improve various aspects of IR systems. In the con-

text of Learning to Rank (LtR), user actions recorded in query logs

are used to extract several important features [1]. As an empir-

ical evidence of the importance of user interaction features, we

trained a L

M [4, 19] model on the MSLR-WEB10K LtR

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080714

Claudio Lucchese
ISTI­CNR, Pisa, Italy claudio.lucchese@isti.cnr.it

Ra aele Perego
ISTI­CNR, Pisa, Italy ra aele.perego@isti.cnr.it

dataset (h ps://www.microso .com/en-us/research/project/mslr/)

with and without user-interaction features: the nDCG measured

on the test set without such features drops from 0.4636 to 0.4410.

In this paper, we explore the embedding of user interaction dy-

namics into L

M , a state-of-the-art LtR algorithm. Instead

of proposing new features modeling this particular aspect of user

behavior and then training the LtR model on this extended set of

features, we adopt a complementary approach. We model the user

dynamic in scanning a ranked result list with Markov chains trained

on query log data and we modify the L

M loss function

to embed this trained Markov chain.

To the best of our knowledge, the integration of the user dynamic

in a LtR algorithm is novel and has not been addressed yet. Our

approach di ers in fact from on-line LtR and reinforcement learning.

e authors of [11] exploit click logs to infer preferences between

rankers in order to make on-line LtR faster. Lerot [16] proposes an

on-line LtR algorithm which uses clicks as feedback for interleaving

methods. We instead propose an o -line LtR algorithm where the

user dynamic is directly embedded in the ranking function.

2 METHODOLOGY
As shown in [20], e ectiveness is o en measured as the inner product of a relevance vector J and a discounting vector D. e elements Ji account for the bene t of ranking an high-quality document at the i-th position of the Search Engine Result Page (SERP), while D denotes such contribution for low-ranked documents. For instance, according to Discounted Cumulated Gain (DCG) the i-th element of J is de ned as Ji = 2li - 1, where li is the relevance label of the i-th ranked document, and Di = log(i +1). e underlying assumption is that low-ranked documents receive less a ention by the user and therefore they contribute less to the user-perceived quality of the SERP. De ning a proper quality metric is crucial both for evaluating retrieval systems and for learning e ective ranking models as such metrics are used to drive the training process.
Most metrics assume the user analyzes a SERP from top to bottom, and therefore de ne a decreasing discount vector, hoever some user studies suggest that the probability of observing a result depends on the quality of the documents ranked higher: if the user
nds a relevant document at position i it is less likely that he will inspects the document at position i + 1 [21]. However, the user behavior is more complex as he/she can move forward and backward, can jump from one document to any other and visit already visited documents, as suggested by [10].
Our work stems from the simple observation that user behavior in visiting a SERP di ers depending on query type and the number

1041

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Probability Probability Discount Probability Discount

0.25 0.2
0.15

1 Relevant Document 0 Relevant Documents 3 Relevant Documents 5 Relevant Documents 7 Relevant Documents 9 Relevant Documents

0.1

0.05

0 2

4

6

8

10

Rank Positions

(a) Di erent ery Types

0.25 0.2
0.15 0.1
0.05

1

Stationary Distribution

User Dynamic

nDCG Discount

(i) = i-1+ i+

0.8

= 0.2601

= 0.0112

= -0.0378

0.6

0.4

0 2

4

6

8

Rank Positions

(b) Navigational eries

0.2 10

0.25 0.2
0.15 0.1
0.05

1

Stationary Distribution

User Dynamic

nDCG Discount

(i) = i-1+ i+

0.8

= 0.0848

= 0.0045

= 0.0502

0.6

0.4

0 2

4

6

8

Rank Positions

(c) Informational eries

0.2 10

Figure 1: Stationary distributions for queries retrieving di erent number of relevant documents (a), and stationary distribution with its tted curve and DCG discount for navigational (b), and informational queries (c).

of relevant results. For example, it is likely that on a SERP with a

single highly relevant result in the rst position the user assumes a

navigational behavior, while a SERP with several relevant results

may likely correspond to an informational query, where a more

complex SERP visiting behavior can be observed [2]. Since at train-

ing time a list-wise LtR algorithm such as L

M is aware of

the number and distribution of relevance labels associated with the

training samples for each query, we suppose that it can pro t from

the knowledge of the user dynamic associated with the speci c kind

of query. In the following we discuss our model of user dynamic

and the methodology followed to integrate it into L

M.

2.1 Modeling User Dynamic
We model the user dynamic with a Markovian process [14] where the user scans the ranked documents in the SERP according to possibly complex paths. Let us denote by X1, X2, . . . the sequence of random variables representing the rank positions in R = {1, 2, . . . , R} visited by the user, where Xn = j means that the nth document visited by the user is at rank j. Moreover, we assume that the probability to move from the document at rank i to the document at rank j depends on the document at rank i only and is independent of all the previously visited documents. Finally, we denote by P the transition matrix whose entries represent the transition probabilities P = (pij : i, j  R), where pij = P[Xn+1 = j |Xn = i]. e sequence of random variables (Xn )n>0 de nes a discrete-time homogeneous Markov chain. Under the assumption of irreducibility and aperiodicity, P admits a unique stationary distribution  = P, which is the limit of the n-step transition probabilities pi(nj )  j as n   for all i, j [14]. When extending this analysis to a long-term query log, we can consider the behavior recorded for each user as a di erent observation of the same stochastic process, and the resulting stationary distribution can be considered as an aggregated representation of user dynamics. In addition, since we observe that the behavior of users change depending on the number of relevant documents in the SERP, we can classify queries on the basis of the number of relevant documents returned and estimate di erent transition matrices P^ for di erent classes of queries. Speci cally, we rst aggregate the dynamics of di erent users on the basis of

the typology of query, then we adopt the maximum likelihood estimator approach [18] on the aggregated data:

(1) for each i  R let i be the number of times that the users

visited the document at rank i given the query;

(2) if i = 0, then p^ij = 0 for all j i and p^ii = 1;

(3) if i > 0, let ij be the number of transitions from docu-

ment at rank i to document at rank j, then p^ij =

ij .
i

Figure 1(a) plots the stationary distributions obtained from the

Yandex query log detailed in Section 3.1. When considering queries

with just one relevant retrieved document, i.e. the red line with

circle markers in Figure 1(a), the user dynamic exhibits a spike

with respect to the rst rank position, while for queries without

any relevant documents or with more than one relevant document,

i.e. the blue lines, the probability tends to be distributed more

uniformly, meaning that the user is exploring the whole SERP.

We focus on these two distinct macroscopic behaviors, and, for

the sake of simplicity, we call navigational the queries where users

concentrated on just the rst item, and we consider all the other

queries as informational since users tend to visit more documents.

On the basis of the above experimental observation, we claim

that the user dynamic can be described as a mixture of the navi-

gational and informational behavior. e navigational component

is

represented

by

the

inverse

of

the

rank

position i,

1 i

,

while

the

informational component is linear with respect to the rank position

i. erefore, we model the user dynamic as

 (i) = i-1 + i + 

where the parameters ,  and  are calibrated in order to t the estimated stationary distributions computed on the Yandex dataset.
Figures 1(b) and 1(c) show the stationary distributions together with the ed curves for the navigational and informational cases, respectively. In Figure 1(b) the stationary distribution is the same reported in the red line of Figure 1(a), while to compute the stationary distribution reported in Figure 1(c) we aggregate all the user dynamics corresponding to the other queries, i.e. queries without relevant documents or with more than one relevant document.
e user dynamic de ned above can actually be considered as a discounting vector to be exploited in any given quality metric.

1042

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Di erently from other approaches, the user dynamic is de ned on the basis of two di erent query classes which exhibit a di erent user behavior. Figures 1(b) and 1(c) show how di erent is the derived user dynamic w.r.t. the DCG discounting component. Below we discuss how  can be exploited in a state-of-the-art LtR algorithm.

2.2 Integrating User Dynamic into LtR

A LtR algorithm exploits a ground-truth set of training examples in

order to learn a document scoring function  [12]. Such training set

is composed of a collection of queries Q, where each query q  Q is

associated with a set of assessed documents D = {d0, d1, . . .}. Each document di is labeled by a relevance judgment li according to its relevance to the query q. ese labels induce a partial ordering over

the assessed documents, thus de ning an ideal ranking which the

LtR algorithm aims at approximating. Each query-document pair

(q, di ) is represented by a vector of features x, able to describe the query (e.g., its length), the document (e.g., the in-link count) and

their relationship (e.g., the number of query terms in the document).

Since IR measures are not di erentiable, their optimization is

very challenging. To address this issue, the state-of-the-art solution

is the L

R gradient approximation [5], which is based on

the idea of measuring the cost variation a er swapping any two

documents in a given result list. As discussed in [9], this approach

can be applied to several IR measures and it is capable of accurately

discovering local optima.

L

R can be summarized as follows. di and dj are two

candidate documents for the same query q, with relevance labels li

and lj respectively, si and sj are the currently predicted document

scores. e lambda gradient of any given IR quality function Q is:

i j = Qi j  si - sj

= sgn( i -

j)

Qi j

·

1 1 + esi -sj

where, the sign is determined by the document labels only, the rst

factor Q is the quality variation when swapping scores si and

sj , and the second factor is the derivative of the RankNet cost [3],

which minimizes the number of disordered pairs. When li  lj ,

the quality Q increases with the score of document di . e larger

the quality variation Q, the higher the document di should be

scored. Note that the RankNet multiplier fades Q if documents are

scored correctly, i.e. si  sj , and boosts Q otherwise. e lambda

gradient for a document di is computed by marginalizing over all

possible pairs in the result list: i = j ij . L

R uses

Normalized Discounted Cumulated Gain (nDCG) as Q and so Q is

the variation in nDCG caused by the swap of two documents.

We enhance the existing L

M algorithm by replacing

the above Q with a new quality measure which integrates the

proposed user dynamic  . is new measure is called Normalized

Markov Cumulated Gain (nMCG) and it is de ned as follows:

nMCG@k =

i k 2li - 1 ·  c (i ) h k,sorted by lh 2lh - 1 ·  c (h)

where li is the relevance label of the i-th ranked document and c (i)

is the user dynamic function at rank i relative to the query class c,

either navigational or informational. Basically, nMCG can be seen

as an extension of nDCG where the discount function is de ned

by the user dynamic and depends on the query class. Moreover,

since c depends on the query class, i.e. depends on the query q, we are optimizing two di erent variants of the same quality measure nMCG across the training dataset. Finally, nMCGij can be computed e ciently as follows:

nMCGi j =

- 2li - 2lj (c (i ) - c (j)) .
h k,sorted by lh 2lh - 1 ·  c (h)

Hereina er, we use nMCG-MART to refer to the described variant

of L

M aimed at maximizing nMCG.

Note that the query class is known at training time, and therefore the algorithm can optimize the proper user dynamic c . Nor

the document relevance, neither the query class information are

available at test time, therefore the algorithm should, at the same

time, classify queries and rank documents according to the di erent class-based dynamics c .

3 EXPERIMENTS
3.1 Experimental Setup
We remark that there is no publicly available dataset providing user session data, document relevance and query-document pairs features at the same time. erefore, we have to use two di erent datasets: the rst for the user dynamic derivation and the second for the LtR analysis.
We calibrate the proposed user model on the basis of the click log dataset provided by Yandex [17] (h p://imat-relpred.yandex.ru/en/).
e dataset is composed of 340,796,067 records with 30,717,251 unique queries, retrieving 10 URLs each. We used the training set, which consists of 5191 assessed queries with binary judgments, corresponding to 30,741,907 records. Notice that 9% of the sessions corresponds to navigational queries while the remaining 91% corresponds to informational ones.
e accuracy of the proposed algorithm is evaluated on three public LtR datasets, MSLR-WEB30K and MSLR-WEB10K, provided by Microso [15] and Istella provided by Tiscali Istella Web search engine [8]. Dataset MSLR-WEB30K encompasses 31,531 queries from the Microso Bing search engine for a total of 3,771,125 querydocument pairs represented by 136 features. e dataset is provided as a 5-fold split. e MSLR-WEB10K dataset contains 10,000 queries samples at random from the previous. Dataset Istella provides 33,018 queries for a total of 3,408,630 query-document pairs represented by 220 features. e dataset is provided as a 60/20/20 train/validation/test split.
Both the Microso and Istella datasets use integer relevance labels in the range [0, 4]. In order to classify queries as navigational or informational we adopt the following criterion. A query is considered as navigational if it contains only one result with relevance label  3. Approximatively 15% of the queries in the Microso datasets are classi ed according to this heuristic as navigational queries, which is quite similar to the value measured on the Yandex dataset. e Istella dataset instead contains a smaller set of navigational queries, covering about 3% of the dataset.

3.2 Experimental Results
We compare the e ectiveness of state-of-the-art LtR algorithm MART and nMCG-MART both in terms of nDCG@10 and nMCG@10

1043

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: nDCG@10 and nMCG@10 across test datasets for di erent model sizes (results are averaged across the 5 folds for Microso datasets.). Statistically signi cant di erences at p = 0.05 and at p = 0.01 w.r.t. -MART marked resp. with  and .

Algorithm -MART nMCG-MART
Algorithm -MART nMCG-MART

MSLR-WEB30K

100

500

Full

0.4564 0.4759 0.4793 0.4598 0.4778 0.4808

0.4684 0.4878 0.4914 0.4718 0.4898 0.4933

MSLR-WEB10K

100 500

Full

nDCG@10
0.4479 0.4637 0.4634 0.4499 0.4646 0.4648

nMCG@10
0.4609 0.4767 0.4768 0.4626 0.4782 0.4790

100
0.7031 0.7070
0.7551 0.7595

Istella-S 500
0.7451 0.7466
0.7970 0.8000

Full
0.7536 0.7549
0.8059 0.8090

metrics. Recall that, at training time, -MART optimizes nDCG

while nMCG-MART exploits the proposed nMCG metric. e al-

gorithms' hyper-parameters were set a er parameter sweeping,

similarly to [6], to a learning rate of 0.05, maximum number of

leaves of 64, and a maximum number of trees of 1500. e actual

number of trees is tuned on the validation set. We also evaluate

smaller models with 100 and 500 trees. In Table 1 we report the

e ectiveness scores of the proposed algorithm computed in terms

of nDCG@10 and nMCG@10.

We rst observe that nMCG-MART is more e ective in opti-

mizing nMCG in every dataset and with every model size. is

was expected as the proposed algorithm is the only one aimed at

optimizing the proposed nMCG. At the same time, this con rms

the soundness of the integration of nMCG into L

M.

An interesting result is that nMCG-MART always provides higher

nDCG@10 than L

M . Recall that even relative improve-

ments in nDCG below 1% are signi cant in terms of user satis-

faction [7]. According to randomization test, the improvement is

statistically signi cant at p = 0.01 on the larger MSLR-WEB30K

dataset and on the other datasets limited to the small models with

100 trees. e proposed nMCG seems to provide more stable results,

as optimizing nMCG also helps in optimizing nDCG. We believe

that nMCG@k is somehow a simpler function to maximize: for

informational queries it mainly discriminates between documents

inside and outside the top-k results, and for navigational queries

an additional boost is given if the relevant document is ranked rst.

is possibly drives the learning algorithm along a smoother cost

function. e bene t is larger at the initial training iterations as

suggested by the statistically signi cant improvements on small

models with 100 trees, where di erence is at p = 0.01 on every

dataset. Larger models reach a plateau of e ectiveness where it

is anyway di cult to improve further. ese hypotheses needs a

detailed investigation as part of our future work.

We conclude that the proposed nMCG may provide a be er

modeling of the user behavior and perceived quality of a SERP, and

that it may also provide high quality rankings according to other

quality metrics of interest.

4 CONCLUSION AND FUTURE WORK

In this paper we presented a way to describe user dynamic through

a model based on Markov chains and we integrated this dynamic

in L

M by de ning a new quality measure called nMCG.

Moreover, since nMCG depends on the query type, the proposed

algorithm optimizes two di erent versions of the same quality

measure. Experiments conducted on publicly available datasets
showed that the proposed algorithm improves over the state-of-
the-art with respect to both nDCG and nMCG.
As future work we aim at analyzing the properties of nMCG as
well as the correlations with other evaluation measures. Moreover,
we will conduct a user study in order to investigate whether the
metric correlates with the quality of a ranking perceived by a user.
Acknowledgments. is work was partially supported by the EC H2020
Program INFRAIA-1-2014-2015 SoBigData: Social Mining & Big Data Ecosys-
tem (654024), and SID16 Ferro, PRAT 2016: Improving Information Retrieval
E ectiveness via Markovian User Models.
REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais. Improving Web Search Ranking by Incorporating User Behavior Information. In SIGIR, pages 19­26, ACM, 2006.
[2] A. Broder. A Taxonomy of Web Search. SIGIR Forum, 36(2):3­10, 2002. [3] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and
G. Hullender. Learning to Rank Using Gradient Descent. In ICML, pages 89­96, ACM, 2005. [4] C. J. C. Burges. From RankNet to LambdaRank to LambdaMART: An Overview. Technical Report, 2010. [5] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to Rank with Nonsmooth Cost Functions. In NIPS, Vol. 6, pages 193­200, 2006. [6] G. Capannini, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego and N. Tonello o.
ality versus E ciency in Document Scoring with Learning-to-Rank Models. In IPM, 52(6):1161­1177, 2016. [7] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. Large-scale Validation and Analysis of Interleaved Search Evaluation. In TOIS, 30(1):6:1­6:41, 2012. [8] D. Dato, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonello o, and R. Venturini. Fast Ranking with Additive Ensembles of Oblivious and NonOblivious Regression Trees. In TOIS, 35(2):15:1­15:31, 2016. [9] P. Donmez, K. M. Svore, and C. J. C. Burges. On the Local Optimality of LambdaRank. In SIGIR, pages 460­467, ACM, 2009. [10] M. Ferrante, N. Ferro, and M. Maistro. Injecting User Models and Time into Precision via Markov Chains. In SIGIR, pages 597­606, ACM, 2014. [11] K. Hofmann, A. Schuth, S. Whiteson, and M. de Rijke. Reusing Historical Interaction Data for Faster Online Learning to Rank for IR. In WSDM, pages 183­192, ACM, 2013. [12] T. Liu. Learning to Rank for Information Retrieval. IN FnTIR, 3(3):225­331, 2009. [13] C. Lucchese, S. Orlando, R. Perego, F. Silvestri and G. Tolomei. Discovering tasks from search engine query logs. In TOIS, 31(3):14:1­14:43, 2013. [14] J. R. Norris. Markov Chains. Cambridge University Press, 1998. [15] T. Qin, and T. Liu. Introducing LETOR 4.0 Datasets. In CoRR, 2013. [16] A. Schuth, K. Hofmann, S. Whiteson, and M. de Rijke. Lerot: An Online Learning to Rank Framework. In LivingLab, pages 23­26, ACM, 2013. [17] P. Serdyukov, N. Craswell, G. Dupret. WSCD2012: Workshop on Web Search Click Data 2012. In WSDM, pages 771­772, ACM, 2012. [18] I. Teodorescu. Maximum Likelihood Estimation for Markov Chains. In arXiv preprint arXiv:0905.4131, 2009. [19] Q. Wu, Burges, C. J. C. Christopher K. M. Svore, J. Gao. Adapting boosting for information retrieval measures. In Information Retrieval, 13(3):254­270, 2010. [20] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson. Expected Browsing Utility for Web Search Evaluation. In CIKM, pages 1561­1565, ACM, 2010. [21] Y. Zhang, L. A. F. Park, and A. Mo at. Click-based Evidence for Decaying Weight Distributions in Search E ectiveness Metrics. In Information Retrieval, 13(1):46­69, 2010.

1044


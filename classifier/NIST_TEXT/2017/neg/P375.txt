Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Jointly Learning Word Embeddings and Latent Topics

Bei Shi
Department of Systems Engineering and Engineering Management e Chinese University of Hong Kong Hong Kong bshi@se.cuhk.edu.hk

Wai Lam
Department of Systems Engineering and Engineering Management e Chinese University of Hong Kong Hong Kong wlam@se.cuhk.edu.hk

Shoaib Jameel
School of Computer Science and Informatics
Cardi University Cardi , UK
JameelS1@cardi .ac.uk

Steven Schockaert
School of Computer Science and Informatics
Cardi University Cardi , UK
SchockaertS1@cardi .ac.uk

Kwun Ping Lai
Department of Systems Engineering and Engineering Management e Chinese University of Hong Kong Hong Kong kplai@se.cuhk.edu.hk

ABSTRACT
Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation pa erns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. ese two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a uni ed manner. STE naturally obtains topic-speci c word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-speci c word embeddings and coherent latent topics in an e ective and e cient way.
CCS CONCEPTS
·Information systems  Document representation; Document topic models; ·Computing methodologies  Topic modeling;
KEYWORDS
word embedding; topic model; document modeling
e work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) and the Microso Research Asia Urban Informatics Grant FY14-RES-Sponsor057. Steven Schockaert and Shoaib Jameel are supported by ERC Starting Grant 637277. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080806

ACM Reference format: Bei Shi, Wai Lam, Shoaib Jameel, Steven Schockaert, and Kwun Ping Lai. 2017. Jointly Learning Word Embeddings and Latent Topics. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080806
1 INTRODUCTION
Word embeddings, also known as distributed word representations, are a popular way of representing words in Natural Language Processing (NLP) and Information Retrieval (IR) applications [7, 22, 27, 35, 39]. Essentially, the idea is to represent each word as a vector in a low-dimensional space, in a way which re ects the semantic, and sometimes also syntactic, relationships between the words. One natural requirement is that the vectors of similar words are themselves also similar (e.g. in terms of cosine similarity or Euclidean distance). In addition, in some models, several kinds of linear regularities are observed. For example, in Skip-gram [21], one of the most commonly used word embedding models, analogous word pairs tend to form parallelograms in the space, a notable example being vec("man") - vec("king")  vec("woman") - vec("queen"). Most word embedding models rely on statistics about how o en each word occurs within a local context window of another word, either implicitly [22] or explicitly [27, 39].
Topic models, such as Latent Dirichlet Allocation (LDA) [5], assign a discrete topic to each word occurrence in a corpus. ese topics can be seen as groups of semantically related words. In this sense, like word embeddings, topic models can be viewed as models for capturing the meaning of the words in a corpus. However, there are several key di erences between word embeddings and topic models, which make them complementary to each other. First, word embeddings are continuous representations, whereas topic assignments are discrete. Second, word embeddings are learned from local context windows, whereas topic models take a more global view, in the sense that the topic which is assigned to a given word occurrence (in the case of LDA) equally depends on all the other words that appear in the same document. Several researchers have already exploited this complementary representation between word embeddings and topic models.

375

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

On the one hand, topic models can be used to improve word embeddings, by addressing the problem of polysemous words. Standard word embedding models essentially ignore ambiguity, meaning that the representation of a word such as "apple" is essentially a weighted average of a vector that would intuitively represent the fruit and a vector that would intuitively represent the company. A natural solution, studied in Liu et al. [19], is to learn di erent word embeddings for each word-topic combination. In particular, they propose a model called Topical Word Embeddings (TWE), which
rst employs the standard LDA model to obtain word-topic assignments. Regarding each topic as a pseudo-word, they then learn embeddings for both words and topics. Finally, a given word-topic combination is represented as the concatenation of the word vector and the topic vector.
On the other hand, word embeddings can also be used to improve topic models. For example, Nguyen et al. [26] suggest to model topics as mixtures of the usual Dirichlet multinomial model and a word embedding component. It is shown that the top words associated with the resulting topics are semantically more coherent. Word embeddings can also be used to help with identifying topics for short texts or small collections. For example, Li et al. [16] propose a model which can promote semantically related words, identi ed by the word embedding, by using the generalized Polya urn model during the sampling process for a given topic. In this way, the external knowledge about semantic relatedness that is captured by the word embedding is exploited to alleviate sparsity problems.
While combining word embeddings and topic models is clearly bene cial, existing approaches merely apply a pipeline approach, where either a standard word embedding is used to improve a topic model, or a standard topic model is used to learn be er word embeddings. Such two-step approaches cannot capture the mutual reinforcement between the two types of models. For example, knowing that "apple" occurs in two topics can help us to learn be er word embeddings, which can in turn help us to learn be er topic assignments, etc. e research question which we address in this paper is whether a uni ed framework, in which topic assignments and word embeddings are jointly learned, can yield be er results than the existing two-step approaches. e uni ed framework we propose, named STE, can learn di erent topic-speci c word embeddings, and thus addresses the problem of polysemy, while at the same time generating the term distributions of topics and topic distributions of documents. Our hypothesis is that this will lead both to more meaningful embeddings and more coherent topics, compared to the current state-of-the-art.
From a technical point of view, there are two challenges that need to be addressed. e rst challenge concerns the representation of topics with embedding vectors, and the mechanism by which words are generated from such topics. Clearly, the commonly used multinomial distribution is inappropriate in our se ing. e second challenge is to obtain the embedding vectors e ciently. Because of the huge amount of parameters, the traditional Skipgram model exploits the Hierarchical So max Tree [23] or Negative Sampling method to maximize the likelihood. When latent topics are considered, however, this alone does not lead to a su ciently e cient method.

To address the rst challenge, we use a generating function that can predict surrounding words, given a target word and its topic. e probability that a given word is generated is based on the inner product of a topic-speci c embedding of that word and a topic-speci c embedding of the target word. is generating function also allows us to identify the top-ranked words for each topic, which is important for the interpretability of the model. To address the second challenge, we design a scalable EM-Negative Sampling method. is inference method iterates over every skipgram (i.e. each local context window), each time sampling the corresponding negative instances. In the E-step, we evaluate the posterior topic distribution for each skip-gram. In the M-step, we update the topic-speci c embeddings and the topic distribution of the documents. We consider two variants of our model, which make di erent assumptions on the consistency of topics among the word pairs in a skip-gram.
We compare our model with existing hybrid models and perform extensive experiments on the quality of the word embeddings and latent topics. We also evaluate our performance on the downstream application of document classi cation. e experimental results demonstrate that our model can generate be er word embeddings and more coherent topics than the state-of-the-art models.
2 RELATED WORK
In traditional vector space models, individual words are encoded using the so-called one-hot representation, i.e. a high-dimensional vector with all zeroes except in one component, corresponding to that word [1]. Such representations su er from the curse of dimensionality, as there are as many components in these vectors as there are words in the vocabulary. Another important drawback is that semantic relatedness of words cannot be modelled using such representations. To address these shortcomings, Rumelhart et al. [32] propose to use distributed word representation instead, i.e., word embeddings. Several techniques for generating such representations have been investigated. For example, Bengio et al. [3, 4] propose a neural network architecture for this purpose. Later, Mikolov et al. [21] propose two methods that are considerably more e cient, namely Skip-gram and CBOW. is has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embeddings. Word embedding models have been applied to many tasks, such as named entity recognition [38], word sense disambiguation [8, 13], parsing [31], and information retrieval [29].
Basic word embedding methods perform poorly for polysemous words such as "apple" and "bank", as the vectors for such words intuitively correspond to a weighted average of the vectors that would normally be associated with each of the individual senses. Several approaches have been proposed to address this limitation, by learning multiple vectors for each word, one corresponding to each sense [28, 30, 33]. For example, Huang et al. [12] exploit global properties such as term frequency and document frequency to learn multiple embeddings via neural networks. Tian et al. [37] introduce a latent variable to denote the distribution of multiple prototypes for each word in a probabilistic manner. Neelakantan et al. [24] propose a non-parametric way to evaluate the number of senses

376

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

for each word. Bartunov et al. [2] also propose a non-parametric Bayesian method to learn the required number of representations.
Note that our model is di erent from these models. First, the aforementioned models consider the prototype vectors for each word in isolation, intuitively by clustering the local contexts of each word. is means that these models are limited in how they can model the correlations between the multiple senses of di erent words. Second, these models do not capture the correlations between the prototypes of words and topics of documents. While there are some word embedding models that do consider topics, to the best of our knowledge no approaches have been studied that exploit the mutual reinforcement between latent topics and word embeddings. For example, Liu et al. [19] concatenate pre-trained topic vectors with the word vectors to represent word prototypes. Building on this idea, Liu et al. [18] combine topic vectors and word vectors via a neural network.
In traditional topic models, such as LDA [5] and PLSA [11], a document is represented as a multinomial distribution of topics, and the topic assignment of a word only depends on that multinomial distribution. In the Bi-gram Topic Model [40, 41], the topic of a given word additionally depends on the topic of the preceding word. Our model is related to this bi-gram model, in the sense that the objective functions of both models are based on a similar idea.
Nguyen et al. [26] propose a topic model named LFTM, which generates vectors from a pre-trained word embedding, instead of words. In this way, the model can bene t from the semantic relationships between words to generate be er topics. e Gaussian LDA model from Das et al. [9], similarly associates with each topic a Gaussian in the word embedding, from which individual word vectors are sampled. Li et al. [16] propose a model which can promote semantically related words (given a word embedding) within any given topic. Note that the above models all rely on a pre-trained word embedding. Li et al [17] propose a model which learns an embedding link function to connect the word vectors and topics. However, their model mainly focuses on the distributed representation of each topic, instead of words, and generates topics as an abstract vector, thus losing the interpretability of topics.
3 MODEL DESCRIPTION
In this section, we present the details of our model, which we call Skip-gram Topical word Embedding (STE).
3.1 Representing Topics and Embeddings
Each word w is associated with an input matrix Uw and an output matrix Vw , both of which have dimension K ×s, with K the number of topics and s the number of dimensions in the word embedding.
e fact that Uw and Vw are matrices, rather than vectors, re ects our modelling assumption that a word w may have a di erent representation under each topic.
As in standard topic models, a document will correspond to a probability distribution over topics. In contrast to standard topics models, however, topics in our case are more than probability distributions over words. In particular, for a document d and some central word wt under consideration, the probability of predicting a surrounding word wt+j depends on the topic of the word wt . For example, suppose that the central word is "apple"; if its topic relates to technology, words such as "technology" might be predicted with

high probability, whereas if its topic relates to fruit, words such as
"juice" might instead be predicted. In particular, we assume that the
probability of predicting the word wt+j given the word wt under the topic z is evaluated as follows:

p (wt +j |wt , d ) = p (wt +j |wt , z )p (z |d )

(1)

z

where the summation is over the set of all K topics, p(.|d ) is the

topic distribution of the document d, and we assume that j is within

the window size.

We consider two variants, which di er in how the probability

p(wt+j |wt , z) is evaluated. In the rst variant, called STE-Same, we assume that for each skip-gram wt +j , wt , the words wt +j and wt belong to the same topic z:

p (wt +j |wt , z) =

exp(Vwt+j,z · Uwt ,z ) w  exp(Vw ,z · Uwt ,z )

(2)

where  is the vocabulary of the whole corpus. Computing the value

p(wt+j |wt , z) based on Eq. 2 is not feasible in practice, given that the computational cost is proportional to the size of . However,

similar as for the standard Skip-gram model, we can rely on negative

sampling to address this (see Section 3.2).

In the second variant, called STE-Di , we assume that for each

skip-gram wt +j , wt , the topic assignment zt +j of word wt +j is independent of the topic assignment zt of word wt . We then have:

KK

p (wt +j |wt , d ) =

p (wt +j |wt , zt , zt +j )p (zt , zt +j |d )

zt =1 zt +j =1

(3)

KK

=

p (wt +j |wt , zt , zt +j )p (zt |d )p (zt +j |d )

zt =1 zt +j =1

e probability that the word wt+j is generated, given the central word wt and the topic assignments zt+j and zt is then evaluated as follows:

p (wt +j |wt , zt , zt +j ) =

exp(Vwt+j,zt+j · Uwt ,zt ) w  exp(Vw ,zt+j · Uwt ,zt )

(4)

Clearly, both variants have complementary advantages and drawbacks. e STE-Same model will lead to more coherent topics, but it will not allow us to measure the similarity between words across di erent topics. e STE-Di model, on the other hand, does allow us to evaluate such inter-topic similarities, but the resulting topics may be less coherent. In practice, we could of course also consider intermediate approaches, where zt+j = zj is assumed to hold with a high probability, rather than being imposed as a hard constraint.

3.2 Algorithm Design
We need an inference method that can learn, given a corpus, the values of the model parameters, i.e. the word embeddings Uw,z and Vw,z corresponding to each topic z, as well as the topic distribution p(z|d ) for each document d. Our inference framework combines the Expectation-Maximization (EM) method with the negative sampling scheme. It is summarized for the STE-Same variant in Algorithm 1. e inference method for STE-Di is analogous. In each iteration of this algorithm, we update the word embeddings and then evaluate the topic distribution p(z|d ) of each document. To update the word embeddings, we iterate over each skip-gram, sample several negative instances and then compute the posterior

377

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Algorithm 1 EM negative sampling for STE-Same

1: Initialize U , V , p(z|d )

2: for out iter = 1 to Max Out iter do

3: for each document d in D do

4:

for each skip-gram wt +j , wt in d do

5:

Sample negative instances from the distribution P.

6:

Update p(wt +j |wt , z), p(zk |d, wt +j , wt ) by Eq. 9

and Eq. 6 respectively.

7:

for in iter = 1 to Max In iter do

8:

Update U , V using the gradient decent method

with Eq. 10 and Eq. 11

9:

end for

10:

end for

11:

Update p(z|d ) using Eq. 8

12: end for

13: end for

topic distribution for the skip-gram. en we use the EM algorithm

to optimize the log-likelihood of the skip-grams in the document.

In the E-step, we use the Bayes rule to evaluate the posterior topic

distribution and derive the objective function. In the M-step, we

maximize the objective function with the gradient descent method

and update the corresponding embeddings Uw and Vw . e overall training objective measures how well we can predict

surrounding words, taking into account the topic distributions of

the documents. For each document d, given a sequence of words

w1, w2, · · · , wTd , the log-likelihood Ld is de ned as follows.

Td

Ld =

log p (wt +j |wt , d )

(5)

t =1 -c j c
j0

where c is the size of the training windows. e overall log-likelihood
is then given by L = d Ld . In the E-step, the topic distribution for each skip-gram in d can
be evaluated using the Bayes rule as:

p (zk |d, wt , wt +j ) =

p (wt +j |zk , wt )p (zk |d ) z p(wt +j |z, wj )p(z|d )

(6)

In the M-step, given the posterior topic distribution Eq. 6, the goal is to maximize the following Q function:

Td
Q=

p (z |d, wt , wt +j )lo (p (z |d )p (wt +j |z, wt ))

d t =1 -c j c z

j0
(7)

=

n (d, wt , wt +j ) p (z |d, wt , wt +j )

d {wt ,wt +j }Pd

z

[log(z |d ) + log(p (wt +j |z, wt ))]

where Pd is the set of the skip-grams in d. n(d, wt , wt+j ) denotes the number of the skip-gram wt , wt+j in d. Using the Lagrange
multiplier, we can obtain the update rule of p(z|d ), satisfying the

normalization constrains that z p(z|d ) = 1 for each document d:

p(z|d ) =

{wt ,wt+j }Pd n(d, wt , wt +j )p (z |d, wt , wt +j ) {wt ,wt+j } Pd n(d, wt , wt +j )

(8)

As already mentioned, it is not feasible to directly optimize Uw,z and Vw,z due to the term w  exp(Vw,z · Uw,z ). Inspired by the

negative sampling scheme, we therefore estimate the probability of predicting the context word p(wt+j |wt , z) as follows:

log p (wt +j |wt , z )  log  (Vwt+j ,z · Uwt ,z )

n

(9)

+ Ewi P [log  (-Vwi ,z · Uwt ,z )]

i =1

where  (x ) = 1/(1 + exp(-x )) and wi is a negative instance which is sampled from the distribution P (.). Mikolov et al. [21] have

investigated many choices for P (w ) and found that the best P (w ) is

equal to the unigram distribution Unigram(w ) raised to the 3/4rd

power. We exploit the same se ing of P (w ) in [21]. Evaluating

log p(wt+j |wt , z) for each term in the overall objective function, we obtain the following gradients: erefore, the gradients of the

objective function with respect to U and V can be formulated as

follows:

L Uwt,z = -(w wt -  (Vw ,z · Uwt,z )) · Vw ,z · P (z|d, wt , w ) (10)

L Vw ,z = -(w wt -  (Vw ,z · Uwt,z )) · Uwt,z · P (z|d, wt , w ) (11)

where

w

wt

=

1,
 0,

if w is a word in the context window of wt otherwise

(12)



e di erence between the updated rules of U and V and those in

the original Skip-gram model is that we maximize P (zk |d, wt , wt+j )· log P (wt +j |wt , z) instead of log P (wt+j |wt ) for each skip-gram. is

is in accordance with the fact that our model uses the topic distri-

bution to predict context words.

Td

LBT M =

log p (wt +1 |wt , d )

(13)

d t =1

Comparing the original likelihood of Bi-gram Topic Model (BTM) [40]

in Eq. 13 with ours, we can see the connection between our STE

model and BTM. Speci cally the objective functions in Eq. 5 and

Eq. 13 share similar form. Both of them are related to the product

of conditional probabilities which predict the next word given the

preceding word no ma er skip-gram or bi-gram. Such connection

provides an insight for our model indicating that it is capable of dis-

covering good topics and identifying high-quality word embedding

vectors jointly.

3.3 Topic Generation
One important aspect of topic models is their interpretability, where the semantic meaning of a topic can be naturally perceived by examining the top ranked words. In standard topic models, these top-ranked words are simply those that maximize P (w |z) for the multinomial distribution associated with the considered topic z. In our model, on the other hand, we can evaluate the probability of p(wt +j |z, wt ) for each skip-gram wt , wt +j . erefore, we represent each topic as the ranked list of bi-grams. Each bi-gram is sorted using Eq. 9 and the top-ranked bi-grams are selected from the ranking list. e original time complexity of calculating p(wt+1|z, wt ) is ||2 × K, where || is the size of the vocabulary, i.e., around 105. To make it more e cient, we rst collect all the bi-grams in the corpus and evaluate the corresponding probability p(wt+1|z, wt ).
en the time complexity is reduced to be linear to the number of

378

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

bi-grams. Note that in Eq. 9, we do not need to consider the part related to the sampled negative instances for each bi-gram, i.e., the summation expression, as we can assume it to be constant.
3.4 Folding-in for New Documents
Given a new document d , our algorithm can infer the topic distribution of d . Given U and V learned from the training process, we x the values of U and V , and then only update p(z|d ) using Algorithm 1.
For each word w in d , the posterior topic distribution of w, p(z|w, d ) can also be inferred. We consider that the topic distribution of w is related to not only its context words but also the topic distribution of d . erefore, using the Bayes rule, we have:
log p(z|w, cw , d )  log p(z|d ) + log p(cw |z, w, d ) (14)
where cw is the set of the context words of w. e likelihood term log p(cw |z, w, d ) can be de ned as the sum of log p(wt+j |wt , z), where wt+j belongs to the context words. e probability p(wt+j |wt , z) can be computed in Eq. 9. e term p(z|d ) is the corresponding prior probability.
4 EXPERIMENTS
In this section, we present a detailed analysis of the performance of our method. We rst present a qualitative analysis of the learned topic-speci c embeddings. We then focus on evaluating the quality of the word embedding on a standard word similarity benchmark. Subsequently, we evaluate the quality of the identi ed topics, focusing in particular on their coherence. Finally, as an example of a downstream task, we analyze the performance of our model in a document classi cation task.
4.1 alitative Analysis
To illustrate the interactions between word embeddings and latent topics, we visualize the results of our STE-Same and STE-Di models in Figures 1a and 1b respectively. For this gure, and in the following experiments, we have used the Wikipedia dump from April 2010 [34], which has previously been used for other word embedding models [12]. We have chosen the number of topics K = 10.
e number of outer iterations and inner iterations are both set to 15. e dimension of the embedding vectors was chosen as 400, in accordance with [19]. For each skip-gram, we set the window size to 10 and sample 8 negative instances following [37]. To generate the visualization in Figure 1, we have used the t-SNE algorithm [20], applied to the vectors of the 500 most frequent words.
In Figure 1, each node denotes a topic-speci c word vector. To illustrate how polysemy is handled, we show labels for the word "party", as an example of a polysemous word, and for the word "government", as an example of a monosemous word. e labels show both the word and topic index, separated by "#". In Figure 1a, we can observe that our STE-Same model divides the whole space into K disjoint subspaces, with each subspace representing a topic. Within each subspace the words with similar meanings are close. Note that the similarity of the words "government" and "party" depend on the considered sense for the la er word. Accordingly, we see that "government" and "party" are close to each other in some subspaces, but far apart in others. For example, in the subspace of

Topic 0, the position of the word "party" is far from the position of the word "government", which suggests that the meaning of "party" under Topic 0 is not related to a political party. In contrast, for Topics 4, 6 and 8, the vectors for "party" and "government" are similar, suggesting that "party" in these spaces is regarded as a political organization.
On the other hand, Figure 1b illustrates how STE-Di generates a more universal space in which word embeddings from di erent topics co-exist in this shared space. Words from di erent topics with a similar meaning are represented using similar vectors. In particular, for monosemous words such as "government" the word vectors are approximately the same. For the word "party", on the other hand, we see three clearly distinct representations, only one of which (party#2) is close to the vectors for "government". Moreover, we found that "party#3" represents the semantic sense of community because it is close to the word "organization" and the word "group".
e representations of the word "party" from the other topics are approximately the same. ey are close to the representations of the word "summer" and the word "shout". It indicates that the word "party" represents the meaning about the concept of human activity.
From the comparison between Figure 1a and Figure 1b, the STESame model and the STE-Di model can be regarded as two di erent paradigms derived from the treatment of topic consistency in a skipgram. e advantage of the STE-Di model over the STE-Same model is that the STE-Di model can support be er the evaluation of the similarity of words from di erent topics. For example, the senses of "party" under Topics 4 and 6 are very close to "government" in the STE-Same model. However, when we evaluate the similarity between "party#4" and "government#6", we nd that the distance cannot re ect very well the word similarity. Nevertheless, this STESame model can still achieve comparable performance with existing models in the quantitative word embedding evaluation experiment as presented in the next subsection. On the other hand, our STEDi model can handle very well the evaluation of the similarity of words from di erent topics. e reason is that it represents every sense in a more universal shared space without gaps between di erent topics.

Table 1: e most similar words identi ed by the original Skip-gram model and our STE-Di model.

Model Skip-gram STE-Di Skip-gram STE-Di Skip-gram STE-Di

Words apple apple#1 apple#2 java java#1 java#2 cell cell#1 cell#2

Similar Words macintosh, ios, juice peach, orange, juice macintosh, ipod, windows sumatra, html, somalia sumatra, somalia, sudan
html, lisp, jde phones, viral, biology phones, technology, scanner viral, tumor, embryonic

Table 1 shows the nearest neighbours of some polysemous words, according to the Skip-gram model and our STE-Di model (using cosine similarity in both cases). We observe that these nearest neighbours for Skip-gram mix di erent senses of the given words, which is expected since Skip-gram does not address polysemy. For

379

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

(a) STE-Same
(b) STE-Di Figure 1: Visualization of the word embeddings learned using STE-Same and STE-Di with 10 topics. e polysemous word "party" and the monosemous word "government" are highlighted for the comparison.
380

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

example, the nearest neighbours of "apple" are given as "macintosh", "ios", and "juice", indicating the "company" and "fruit" interpretations of the word "apple" are mixed. In contrast, our STE-Di model can distinguish di erent prototypes of polysemous words via the latent topics. For example, the most similar words of "apple" under Topic 1 are "peach", "orange" and "juice", which clearly corresponds to the fruit interpretation. Under Topic 2, they are "macintosh", "ipod" and "windows", clearly referring to the company interpretation.

4.2 Word Embedding Evaluation
e most common approach for evaluating word embeddings is to assess how well the similarity scores they produce correlate with human judgments of similarity. Although there are several word similarity benchmark datasets, most do not provide any context information for the words, and are therefore not appropriate for evaluating models of similarity for polysemous words. Huang et al. [12] prepared a data set, named Stanford's Contextual Word Similarity (SCWS) data set, which includes 2003 word pairs together with their context sentences. e ground truth similarity score with the range [0, 10] was labeled by humans, according to the semantic meaning of the words in the given contexts. We adopt this benchmark data set for evaluating the quality of our word embeddings.
We compare our results with the following baselines and stateof-the-art methods, reporting the previously published results from their papers.
TFIDF We consider two variants, TFIDF and Pruned TFIDF. e TFIDF method represents each word as a vector, captur-
ing the context words with which it co-occurs in a 10-word window. Each context word is presented by the one-hot representation and weighted via TF-IDF learned from the training set. e Pruned TFIDF method proposed by [28] improves the original TFIDF model by pruning the context words with low TF-IDF scores. Word embedding ese baselines include the C&W method proposed by [7] and the Skip-gram model [21]. Note that since neither of these methods considers polysemy, word similarity is evaluated without regarding the context sentences. Topic models e rst model, named LDA-S, represents each word w in a document d as the posterior topic distribution, namely, p(z|w ) where p(z|w )  p(w |z)p(z|d ). e second model, named LDA-C, additionally considers the posterior topic distribution of the surrounding context words, as follows:

p(z|w, c)  p(w |z)p(z|d ) p(w |z)

(15)

w c

where c is the set of the context words of w. Multiple prototype models ese methods represent each
word sense as a xed length vector. One representative work, proposed by Huang et al. [12], exploits global properties of the corpus such as term frequency to learn multiple embeddings via neural networks. Tian et al. [37] introduce a latent variable to denote the distribution of multiple prototypes for each word in a probabilistic manner. Liu

et al. [19] propose a model called TWE, which concatenates the pre-trained topics with the word embeddings for representing each prototype.
We present the related parameter se ings as reported in the previous papers [12, 19, 37]. For Pruned TFIDF, top 200 words with the highest scores are preserved. For the model in [37], each word is assumed with 10 prototypes. Following [19], the number of topics of LDA-S, LDA-C and TWE is 400. Note that the size of each embedding vector in TWE is 800 [19], which consists of 400dimension word embedding and 400-dimension topic embedding.
e parameter se ing of our STE model is the same as described in Section 4.1.
For all the di erent representations, the word similarity is evaluated using cosine similarity. However, for the multiple prototype based methods, as well as for our STE model, two di erent variants are considered:
AvgSimC Given a word w and its associated context words cw , we can infer the posterior topic distribution p(z|w, cw , d ) according to Eq. 14. en the averaged similarity between two words (wi , wj ) over the assignments of topics is computed as:

AvgSimC(wi , wj ) =

p (zi |wi , cwi )p (zj |wj , cwj )

zi zj

× cos(Uwi,zi , Uwj,zj )

where U (wi , zi ) is the embedding vector of wi under the topic zi and cos(·) is the cosine similarity. MaxSimC In this case, we instead evaluate the similarity
between the most probable vectors of each word. It is
de ned as:

MaxSimC(wi , wj ) = Sim(Uwi,zi , Uwj,zj )
where z = arg maxz (p(z|w, c)).
Following previous work, we use the Spearman correlation coefcient as the evaluation metric. e results are shown in Table 2. e STE model performs comparably to the state-of-the-art TWE model, and outperforms the baseline methods. TWE also exploits both topics and embeddings. However, the vectors in the TWE model have twice as many dimensions as those in our model, since each word is represented as the concatenation of a word vector and a topic vector. e STE-Di variant outperforms STE-Same, for both the A SimC and MaxSimC ranking measures. While STESame can generate more coherent topics as indicated in Section 4.3, this comes at the price of slightly less accurate word vectors, which is not unexpected.
It is interesting to note that the original Skip-gram model can still achieve satisfactory performance. We observe that the words in the SCWS data set are mostly monosemous words. Particularly, among 2003 word pairs in this data set, there are 241 word pairs containing the same word within a word pair. One may expect that such identical words have di erent senses leading to a low ground truth similarity score. However, only 50 of them have the ground truth similarity score less than 5.0. It indicates that the proportion of the challenging polysemous words in this data set is quite small.

381

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Spearman correlation  × 100 for the SCWS data set.

Model C&W Skip-gram TFIDF Pruned TFIDF LDA-S LDA-C Tian Tian Huang Huang TWE TWE STE-Same STE-Same STE-Di STE-Di

Similarity Metrics Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity A SimC M ax S imC A SimC A SimC A SimC M ax S imC A SimC M ax S imC A SimC M ax S imC

 × 100 57.0 65.7 26.3 62.5 56.9 50.4 65.4 63.6 65.3 58.6 68.1 67.3 66.7 65.5 68.0 67.7

4.3 Topic Coherence
In our model, topics can be interpreted by looking at the top-ranked bi-grams according to Eq. 9. To evaluate how coherent these topics are, we have applied our model to the training set of the 20Newsgroups corpus. e 20Newsgroups corpus1 is a collection of 19,997 newsgroup documents. e documents are sorted by date and split into training set (60%) and test set (40%). e data is organized, almost evenly, into 20 di erent newsgroups, each corresponding to a di erent topic. Some of the newsgroups are very closely related to each other, however. e categories of this corpus, partitioned according to subject ma er. are shown in Table 3. As text preprocessing, we have removed punctuations and stop words, and all words were lowercased.

Table 3: e categories of the 20Newsgroups corpus.

comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x

rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey

sci.crypt sci.electronics sci.med sci.space

talk.politics.misc

misc.forsale

talk.politics.guns alt.atheism talk.politics.mideast soc.religion.christian

talk.religion.misc

To evaluate the generated topics, we use a common topic coherence metric which measures the relatedness between the top-ranked words [6, 36]. e intuition is that topics where the top-ranked words are all closely semantically related are easy to interpret, and in this sense semantically coherent. Following Lau et al. [14, 25], we use the pointwise mutual information (PMI) score as our topic coherence metric. PMI has been found to strongly correlate with human annotations of topic coherence. For a topic z, given the top-ranked T words, namely, w1, w2, · · · , wT , the PMI score of the
1h p://qwone.com/ jason/20Newsgroups/

Table 4: Topic coherence evaluation using the PMI metric with di erent numbers of top words.

BTM STE-Same STE-Di

T =5 0.014 0.180 0.015

T = 10 0.036 0.110 0.067

T = 15 0.041 0.107 0.058

T = 20 0.048 0.102 0.054

topic z can be calculated as follows:

PMI-Score (z )

=

1 i

j T

log

P (wi , wj ) P (wi )P (wj

)

(16)

where P (wi , wj ) represents the probability that words wi and wj co-occur and P (wi ) = w P (wi , w ). We compute the average PMI score of word pairs in each topic using Eq. 16. e average score of all the topics is computed. e higher the value, the be er is the coherence. Newman et al. [25] observed that it is important to use another data set to evaluate the PMI based measure. erefore, we use a 10-word sliding window in Wikipedia [34] to estimate the probabilities P (wi , wj ).
We compare our STE model with the Bi-gram Topic Model (BTM) [40] which predicts the next word given the topic assignments as well as the current word. Note that our STE model and LDA are not directly comparable because LDA can only output unigrams as topics.
All models are trained on the training set of the 20newsgroups corpus. e number of topics was set to 20 for all models, which is the same as the number of categories in the 20Newsgroups corpus. For our model, 400-dimensional word vectors were used. To extract the top-ranked words, we rst learn the topic-speci c word embeddings Uw and Vw and then use Eq. 9. Only bi-grams with frequency greater than 5 are considered. For the BTM model, we use the default se ing of hyper-parameters provided by the package 2, i.e.,  = 50.0,  = 0.01, and  = 0.01.
e average PMI scores with di erent numbers of top-ranked words are shown in Table 4. We can see that our STE model generally improves the coherence of the learned topics. Compared with BTM, our STE model incorporates the semantic relationship between words which is learned from word embeddings to improve the quality of topics. Compared with the variant STE-Di , the STE-Same model can produce more coherent topics as presented in Figure 1.
Table 5 shows the top 10 bi-grams of the topics identi ed by the STE-Same model. We can observe that many topics are corresponding to categories from Table 3. For example, the top-ranked bi-grams of Topic 1, such as the terminology phrase "remote sensing" and the name of the company, "mcdonnell douglas", indicate that Topic 1 is related to the category "sci.space". Similarly, most of the top-ranked bi-grams of Topic 5 are medical terms, such as "mucus membrane" and "amino acids". e top-ranked bi-grams of Topic 6 are the names of some famous baseball and hockey players, such as "brind amour"3 and "garry galley"4, indicating that Topic 6 is associated to the categories "rec.sport.baseball" and "rec.sport.hockey". Among others, we also observe a connection between Topic 4 and

2h p://mallet.cs.umass.edu/topics.php 3h ps://en.wikipedia.org/wiki/Rod Brind'Amour 4h ps://en.wikipedia.org/wiki/Garry Galley

382

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

"talk.politics", Topic 7 and "comp.sys.ibm.pc.hardware", as well as Topic 8 and "soc.religion.christian". ese correspondences further illustrate that our model can identify latent topics e ectively.
Another interesting observation is that most of top-ranked bigrams are person names and domain-speci c terms. is arises because we rank the bi-grams according to the probability p(wi+1|wi , z), which indicates that the words wi+1 and wi should have strong connections under the topic z. e name of famous people and domain-speci c terms can satisfy this requirement.

4.4 Document Classi cation
We also analyze the suitability of our model for representing documents. To this end, we consider the task of document classi cation, using again the 20Newsgroup corpus. We have used the original splits into training (60%) and testing (40%) data. For our STE model, as before, we set the number of topics as 20 and the dimensionality of the vector space as 400. To apply our model to document representation, we rst infer the posterior topic distribution of each word in the test set using Eq. 14, and then represent each document as the average of the word vectors in the document, weighted by the TF-IDF score of each word. In particular, each word vector is given by:

K

Vecw = T F I DFw × p(z|w, c)Uw,z

(17)

z =1

where T F I DFw is the TF-IDF score of w in the document. To perform document classi cation, we use a linear SVM classi er using the package from [10].
We compare our approach with several other methods for representing documents, including bag-of-words, vector-space embeddings and latent topics. BOW is the standard bag-of-words model, which represents each document by weighting terms using the TFIDF score. As a form of feature selection, we only consider the 50,000 most frequent words in the bag-of-words representation. e embedding-based methods include the word embedding method Skip-gram, a document embedding method called the Paragraph Vector model (PV) [15], and the TWE model which also considers topics. For the Skip-gram model, we set the number of dimension to 400. We represent each document as the average of word vectors weighted by TFIDF scores. e PV model proposed by [15] represents each document directly as a vector. We use the doc2vec implementation 5 for PV. For TWE, we report the experimental results published in [19]. e topic based methods include LDA, LFTM [26], and GPU-DMM [16]. ese models represent each document via the posterior topic distributions. For LFTM, we reported the experimental result published in [26]. Only F-measure of LFTM is provided. e number of topics for LFTM is 80, as that value was reported to lead to the best performance in [26]. e GPUDMM model promotes semantically related words learned from pre-trained word embeddings by using the generalized Polya urn model. We use the default parameter se ing in the GPU-DMM model. We use a number of standard evaluation metrics for classi cation tasks, namely accuracy, precision, recall and F1 measure.

5h p://github.com/ccri/gensim

e results of the document classi cation task are presented in Table 6, showing that our STE-Di model achieves the best results. Good results are also obtained for the TWE model, which suggests that the quality of document representations can clearly bene t from combining topics models with word embeddings. Nevertheless, compared to TWE, our model can take advantage of the interaction between topics and word embeddings to improve both, and is thus able to outperform the TWE model. e performance of GPU-DMM is not as good as expected. One reason is that this model is proposed for handling short texts [16]. Unfortunately, the documents in 20Newsgroups are not short texts.
5 CONCLUSIONS
We have proposed a model that jointly learns word embeddings and latent topics. Compared to standard word embedding models, an important advantage of incorporating topics is that polysemous words can be modeled in a more principled manner. Compared to standard topic models, using word embeddings can achieve superiority because more coherent topics can be obtained. While some previous works have already considered combinations of word embedding and topic models, these works have relied on a two-step approach, where either a standard word embedding was used as input to an improved topic model, or a standard topic model was used as input to an improved word embedding. In contrast, we jointly learn both word embeddings and latent topics, allowing our model to be er exploit the mutual reinforcement between them. We have conducted a wide range of experiments, which demonstrate the advantages of our approach.
REFERENCES
[1] Ricardo Baeza-Yates, Berthier Ribeiro-Neto, and others. 1999. Modern information retrieval. Vol. 463.
[2] Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, and Dmitry Vetrov. 2015. Breaking Sticks and Ambiguities with Adaptive Skip-gram. arXiv preprint arXiv:1502.07257 (2015).
[3] Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and Trends® in Machine Learning 2, 1 (2009), 1­127.
[4] Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research 3 (2003), 1137­1155.
[5] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research 3 (2003), 993­1022.
[6] Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of NIPS. 288­296.
[7] Ronan Collobert and Jason Weston. 2008. A uni ed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML. 160­167.
[8] Ronan Collobert, Jason Weston, Le´on Bo ou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12 (2011), 2493­2537.
[9] Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015. Gaussian lda for topic models with word embeddings. In Proceedings of ACL. 795­804.
[10] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classi cation. Journal of Machine Learning Research 9 (2008), 1871­1874.
[11] omas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR. 50­57.
[12] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL. 873­882.
[13] Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Embeddings for word sense disambiguation: An evaluation study. In Proceedings of ACL. 897­907.
[14] Jey Han Lau, David Newman, and Timothy Baldwin. 2014. Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model ality.. In

383

Session 3C: Document Representation and Content Analysis 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: Top 10 bi-grams of the generated topics from the 20Newsgroups data set.

Topic 1 mcdonnell douglas
remote sensing southern hemisphere northern hemisphere
ozone layer cosmic rays orthodox physicists black holes ames dryden sounding rockets circular orbit
Topic 5 mucus membrane
amino acids kidney stones
anti fungal candida albicans
oxalic acid kirlian photography
kidney stone glide jubilee penev venezia vaginal tract

Topic 2 graphlib inria inria graphlib rainer klute burkhard neidecker renderman pixar pixar renderman spine dinks re ections shadows marching cubes liquid fueled gravity assist
Topic 6 spring training
bre hull cape breton garry galley brind amour bobby clarke cheap shot wade boggs wayne komets tommy soderstrom chris chelios

Topic 3 carpal tunnel blaine gardner vernor vinge thermal recalibration syndrome carpal tunnel syndrome mudder disciples lance hartmann hartmann lance blgardne javelin evans sutherland
Topic 7 setjmp longjmp xtaddtimeout xtaddworkproc xtaddinput xtaddtimeout bname pname
babak sehari infoname uuname uuname infoname physicist dewey richardd hoskyns arithmetic coding mcdonnell douglas

Topic 4 deir yassin o oman empire bedouin negev negev bedouin ermeni mezalimi ishtar easter eighth graders prime minister nagorno karabakh lieutenant colonel democracy corps
Topic 8 saint aloysius empty tomb respiratory papillomatosis zaurak kamsarakan biblical contradictions recurrent respiratory ohanus appressian archbishop lefebvre joseph smith serdar argic rodney king

Table 6: Document classi cation results.

Model BOW Skip-Gram TWE
PV LDA LFTM GPU-DMM STE-Same STE-Di

Accuracy 79.7 75.4 81.5 75.4 72.2 48.0 80.4
82.9

Precision 79.5 75.1 81.2 74.9 70.8 56.8 80.3
82.5

Recall 79.0 74.3 80.6 74.3 70.7 46.9 80.4
82.3

F-measure 79.0 74.2 80.6 74.3 70.0 76.8 47.3 80.2
82.5

Proceedings of EACL. 530­539. [15] oc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences
and Documents.. In Proceedings of ICML. 1188­1196. [16] Chenliang Li, Haoran Wang, Zhiqian Zhang, Aixin Sun, and Zongyang Ma. 2016.
Topic Modeling for Short Texts with Auxiliary Word Embeddings. In Proceedings of SIGIR. 165­174. [17] Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan Miao. 2016. Generative topic embedding: a continuous representation of documents. In Proceedings of ACL. 666­675. [18] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015. Learning context-sensitive word embeddings with neural tensor skip-gram model. In Proceedings of IJCAI. 1284­1290. [19] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical Word Embeddings.. In Proceedings of AAAI. 2418­2424. [20] Laurens van der Maaten and Geo rey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research 9 (2008), 2579­2605. [21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems. 3111­3119. [22] Tomas Mikolov, Wen-tau Yih, and Geo rey Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations.. In Proceedings of NAACL. 746­751. [23] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model. Proceedings of AISTATS (2005), 246. [24] Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2015. E cient non-parametric estimation of multiple embeddings per word in vector space. In Proceedings of EMNLP. 1059­1069.

[25] David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Proceedings of NAACL. 100­108.
[26] Dat oc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. 2015. Improving Topic Models with Latent Feature Word Representations. TACL 3 (2015), 299­313.
[27] Je rey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of EMNLP. 1532­1543.
[28] Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proceedings of NAACL. 109­117.
[29] Navid Rekabsaz. 2016. Enhancing Information Retrieval with Adapted Word Embedding. In Proceedings of SIGIR. 1169­1169.
[30] Yafeng Ren, Yue Zhang, Meishan Zhang, and Donghong Ji. 2016. Improving Twi er Sentiment Classi cation Using Topic-Enriched Multi-Prototype Word Embeddings.. In Proceedings of AAAI. 3038­3044.
[31] Michael Roth and Mirella Lapata. 2016. Neural semantic role labeling with dependency path embeddings. (2016), 1192­1202.
[32] David E Rumelhart, Geo rey E Hinton, and Ronald J Williams. 1988. Learning representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1.
[33] Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015. A Word Embedding Approach to Predicting the Compositionality of Multiword Expressions.. In Proceedings of NAACL. 977­983.
[34] Cyrus Shaoul. 2010. e westbury lab wikipedia corpus. Edmonton, AB: University of Alberta (2010).
[35] Richard Socher, Cli C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of ICML. 129­136.
[36] Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Bu ler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of EMNLP. 952­961.
[37] Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A Probabilistic Model for Learning Multi-Prototype Word Embeddings.. In Proceedings of COLING. 151­160.
[38] Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL. 384­394.
[39] Peter D Turney, Patrick Pantel, and others. 2010. From frequency to meaning: Vector space models of semantics. Journal of Arti cial Intelligence Research 37, 1 (2010), 141­188.
[40] Hanna M Wallach. 2006. Topic modeling: beyond bag-of-words. In Proceedings of ICML. 977­984.
[41] Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of ICDM. 697­702.

384


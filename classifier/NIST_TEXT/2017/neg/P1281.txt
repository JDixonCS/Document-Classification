Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

RankEval: An Evaluation and Analysis Framework for Learning-to-Rank Solutions

Claudio Lucchese
ISTI-CNR Via G. Moruzzi, 1
Pisa, Italy claudio.lucchese@isti.cnr.it

Cristina Ioana Muntean
ISTI-CNR Via G. Moruzzi, 1
Pisa, Italy cristina.muntean@isti.cnr.it

Franco Maria Nardini
ISTI-CNR Via G. Moruzzi, 1
Pisa, Italy francomaria.nardini@isti.cnr.it

Ra aele Perego
ISTI-CNR Via G. Moruzzi, 1
Pisa, Italy ra aele.perego@isti.cnr.it

Salvatore Trani
ISTI-CNR Via G. Moruzzi, 1
Pisa, Italy salvatore.trani@isti.cnr.it

ABSTRACT
In this demo paper we propose RankEval, an open-source tool for the analysis and evaluation of Learning-to-Rank (LtR) models based on ensembles of regression trees. Gradient Boosted Regression Trees (GBRT) is a exible statistical learning technique for classi cation and regression at the state of the art for training e ective LtR solutions. Indeed, the success of GBRT fostered the development of several open-source LtR libraries targeting e ciency of the learning phase and e ectiveness of the resulting models. However, these libraries o er only very limited help for the tuning and evaluation of the trained models. In addition, the implementations provided for even the most traditional IR evaluation metrics di er from library to library, thus making the objective evaluation and comparison between trained models a di cult task. RankEval addresses these issues by providing a common ground for LtR libraries that o ers useful and interoperable tools for a comprehensive comparison and in-depth analysis of ranking models.
KEYWORDS
Learning to Rank, evaluation, analysis.
ACM Reference format: Claudio Lucchese, Cristina Ioana Muntean, Franco Maria Nardini, Ra aele Perego, and Salvatore Trani. 2017. RankEval: An Evaluation and Analysis Framework for Learning-to-Rank Solutions. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3084140
1 INTRODUCTION
In this demo paper we propose RankEval, an extensible tool for the analysis and evaluation of LtR ensemble-based models. LtR techniques leverage e cient machine learning algorithms and large amounts of training data to build high-quality ranking models. A
Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or a liate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3084140

training dataset consists in a collection of query-document pairs where each example is annotated with a relevance label. A LtR algorithm aims at learning a document scoring function that approximates the ideal ranking induced by the training relevance labels.
Today, models exploiting forests of regression trees, e.g., Gradient Boosted Regression Trees (GBRTs) [4], are considered as the most e ective LtR approaches [5]. eir success is also witnessed by the results of the Kaggle 2015 competitions, where GBRTs were adopted by the majority of the winning solutions, and by the KDD Cup 2015 where all of the top-10 teams used GBRT-based solutions [2]. Indeed, the success of GBRTs fostered the development of several open-source implementations. Among them we cite LightGBM [6], XGBoost [2], QuickRank [1], RankLib [3], and scikit-learn [8]. However, all these libraries o er very limited help for the tuning and evaluation of the trained models. However, their implementation of most common IR evaluation metrics differ signi cantly. For example, when considering NDCG, a query with no relevant results is assigned a score equal to -1 in LightGBM, equal to 0 in QuickRank and RankLib, and equal to 0.5 in the Yahoo Learning to Rank Challenge. Today is thus not trivial to evaluate and compare models learnt with di erent tools since a common and open-source framework for comprehensively evaluating LtR solutions is still missing.
RankEval tries to ll this gap by proposing a comprehensive framework for tuning, evaluation and comparison of GBRT LtR models. Like trec eval [7] in the TREC framework, it aims at becoming a new de facto standard for evaluation among IR researchers and practitioners. While the popular LtR libraries mentioned above focus on the training phase, and provide very li le support to the analysis of the produced model, RankEval provides a consistent evaluation framework, o ering the possibility of comparing di erent models trained with di erent libraries, and, even more, helping users in understanding the model and tuning its e ectiveness.
RankEval supports an in-depth analysis of a given LtR model. Regarding e ectiveness analysis, RankEval allows to compare different models under di erent IR metrics (MAP, DCG, NDCG, etc.), to identify low-performing queries in the test set, to measure quality on varying sizes of the model, and to evaluate the contribution of each feature and each tree to the model accuracy. RankEval allows

1281

Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

to conduct a structural analysis reporting statistics about shape, depth and balancing of trees in the forest. Given multiple models, it supports statistical validation with standard randomisation tests. Finally, it allows to translate a model into the di erent formats adopted by the most popular tools.
RankEval is implemented as a lightweight Python library. It can be used within a Jupyter notebook1 in which case the user can take advantage of inline plots and interactive visualisation features. e proposed functionalities may take advantage of a parallel processing infrastructure when available.
We believe that RankEval can bring signi cant value to the scienti c community as well as to industrial engineers. On the one hand, RankEval allows a fair comparison and analysis among stateof-the-art LtR methods. On the other hand, "this high value, low cost" tool can speed up the process of model tuning by supporting the feature analysis and engineering phase. Ultimately, RankEval is designed with the goal of speeding up the process of investigating strengths and weaknesses of given LtR models, and to support the user in achieving an optimal model ne-tuning.
2 RELATED WORK
Several tools propose implementations of LtR algorithms and o er some basic evaluation facilities. In the following we discuss existing projects and what they o er in terms of evaluation support.
A very popular tool is scikit-learn [8], a general machine learning library in Python which implements several algorithms from simple regressors to random forests classi ers, gradient boosting and so on. For each of these algorithms the library implements evaluation metrics (accuracy, average precision, F1, precision, recall, mean squared/absolute error) for quantifying the quality of predictions, but does not go into the depth of analysing the generated models. Moreover, the framework does not provide dedicated LtR functionalities, both in terms of algorithms or metrics. Despite this lack of support, scikit-learn is successfully used for adressing ranking problems with regression-based algorithms.
RankLib [3] is a Java LtR library currently implementing eight popular algorithms, as well as several retrieval evaluation metrics. Regarding model evaluation, RankLib provides basic support by computing win/loss statistics and relative p-value.
QuickRank [1] is a C++ library o ering a variety of LtR algorithms. Di erently from other tools, it introduces post-learning optimisations pipelined with the LtR algorithms.
XGBoost [2] provides "scalable, portable and distributed gradient boosting (GBM, GBRT, GBDT)" for di erent tasks: regression, classi cation, ranking. It supports multiple languages (C++, Java, Python R and Julia) and it is extensively used by machine learning practitioners: it recently resulted to be the most popular GBRT solution adopted by Kaggle contributors.
LightGBM [6] is a gradient boosting framework developed by Microso . It is designed to be distributed and e cient and it was shown to outperform existing boosting framework on both e ciency and accuracy, with signi cant lower memory consumption.
All the tools mentioned above are mostly oriented to the e ciency of the training phase rather than to the thorough analysis of learned models and their e ectiveness. To the best of our knowledge
1h p://jupyter.org/

Figure 1: Code snippet measuring model performance.
RankEval is the rst tool focusing on providing a common framework for evaluation and analysis of machine-learned tree-based models. It provides a common ground between all pre-existing tools and o ers services which support the interpretation of di erently generated models in a uni ed environment, allowing an easy, comprehensive comparison and in-depth analysis.
3 RANKEVAL
In the following, we summarise RankEval functionalities by grouping them along ve dimensions, namely: e ectiveness analysis, feature analysis, structural analysis, topological analysis, interoperability among GBRT libraries. ese functionalities can be applied to several models at the same time, so to have a direct comparison of the analysis performed. Below, we provide a detailed explanation along with concrete examples of use.
3.1 E ectiveness Analysis
is class of functionalities focuses on assessing the performance of the models in terms of accuracy.
Model performance. RankEval provides a general and uni ed framework for measuring model performance in terms of one or more of the following metrics: DCG, NDCG, MAP, ERR, MRR, pfound, RBP, precision, recall, Spearman's Rho, Kendall's tau. is functionality is important because di erent libraries available today do this evaluation by implementing di erent versions of the same metric. Consider for instance list-wise metrics, the way ties or queries without relevant results are dealt with can yield inconsistent results. RankEval provides a detailed documentation of the metrics implemented and the possibility for the user to easily control and customise each of them with preferred se ings. Figure 1 reports a snippet of code showing how easy, user-friendly and customisable is the evaluation process o ered by RankEval.
Tree-wise performance. RankEval provides the possibility to evaluate di erent e ectiveness metrics incrementally by varying the number of trees in the model, i.e., top-k trees. Moreover, it allows to measure the contribution of each single tree to the global model accuracy independently of their order in the forest (see Fig. 2). e rst functionality can be fruitfully employed to nd the "optimal" model size, e.g., by evaluating the over ing e ect on a validation set. By evaluating the impact of each tree to the

1282

Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 2: Tree-Wise model comparison (le ) and average contribution of each tree to the global score (right).

total global accuracy, the second functionality allows to reduce the size of the forest by identifying subsets of trees to prune, as their contribution to the model is limited.
ery-wise performance. RankEval allows to compute the cumulative distribution of a given performance metric, e.g., fraction of queries with a NDCG score smaller than any given threshold  , over a speci c set of queries (see Figure 3(le )).
ery class performance. RankEval complements the feature above with a comprehensive analysis of the e ectiveness of a given model by providing a breakdown of the performance over query class. Whenever query class information is given, e.g., navigational, informational, transactional, number of terms composing the query, etc., RankEval provides a complete analysis of the model e ectiveness over such classes. is analysis is extremely important especially in a production environment, as it allows to be er calibrate the ranking infrastructure w.r.t a speci c context. Document graded-relevance performance. For each document relevance class RankEval allows the evaluation of the cumulative predicted score distribution. As an example, in Figure 3 (right), for each relevance label available in the data, RankEval renders one curve detailing, for each possible document score, the fraction of documents with a smaller predicted score. e bigger the distance amongst curves the larger the model's discriminative power. Rank confusion matrix. RankEval allows for a novel rank-oriented confusion matrix by reporting for any given relevance label li , the number of documents with a predicted score smaller than documents with label lj (see Figure 4). When li > lj , it corresponds to the number of mis-ranked document pairs. is can be considered as a breakdown over the relevance labels of the ranking e ectiveness of the model.
Figure 3: ery-wise performance (le ) and Document graded-relevance performance analyses (right).

Figure 4: Rank confusion matrix.
3.2 Feature analysis
e feature analysis conducted by RankEval is aimed at investigating how features are used in a given model and their impact on accuracy. Similar functionalities are also present in scikit-learn for tasks di erent from ranking, and feature importance is evaluated by a few LtR so ware. RankEval contributes in this speci c topic by enabling the analysis on all the available ranking metrics, instead of doing it on the root mean squared error (RMSE) only.
Feature importance. RankEval allows an evaluation of the contribution of each feature to the global error reduction. is analysis can be computed on the whole forest or incrementally, by taking into account each tree of the forest. Moreover, this error reduction breakdown can be provided in combination with speci c metrics of interest, e.g., (NDCG, MAP, etc.). By doing so, RankEval enables the possibility to identify a subset of high-quality features, and to validate their contribution across di erent metrics and datasets. Eventually, RankEval provides several user-friendly and customizable ways of reporting this information to allow an easy interpretation of results.
Feature use statistics. Additional insights on the use of features can be provided by reporting statistics about how and where features appears in the forest. RankEval allows for a quick assessment of several indicators like, e.g., fraction of nodes of the forest using a given feature, their average depth in the ensemble, the distribution of thresholds associated with the feature, etc. ese indicators have an important role in engineering a speci c LtR model by allowing the researcher/IR practitioner to be er understand the role and the use of each feature in the forest. As an example, when this analysis shows that a feature is used frequently and always in the top nodes of the trees is it more likely to impact signi cantly on the score of the ranked documents.
3.3 Statistical Analysis
Statistical Signi cance. Statistical signi cance tests are of paramount importance in IR, helping to assess when and how a given solution provides statistically be er performance w.r.t a competitor. RankEval allows the user-friendly computation and visualization of statistical signi cance tests on two given LtR models. Several tests are implemented, including the Randomisation test as recommended in [9].
Bias vs. variance decomposition. To further investigate strengths and weaknesses of a speci c LtR algorithm, RankEval provides bias

1283

Demonstration Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

vs. variance analysis. To this end, RankEval automatically produces several bootstrap samples (without replacement) of the training data on which the algorithm under analysis is tested. e set of learnt models are used to analyse the bias vs. variance and error decomposition of the algorithm as a function of a speci c target metric given by the user.
3.4 Topological analysis
is analysis focuses on the topological characteristics of a given LtR model. e statistics provided by RankEval include min/max/avg depth of trees in the model, most frequent shapes of the trees, i.e., balanced trees or chain-like structures. is is another important feature of RankEval allowing to identify over ing behaviours, usually associated with deep chains, and to support the researcher/IR practitioner in ne-tuning the hyper-parameters of the LtR algorithm such as number of trees, maximum depth, maximum number of leaves, etc.
3.5 Interoperability among GBRT libraries
e aim of RankEval is to provide a uni ed, user-friendly and comprehensive framework for in-depth analysis of LtR models. To enable this, RankEval must be able to analyse models trained with di erent libraries and saved in di erent formats. RankEval can thus read and write ranking models in any of the formats adopted by the ve most popular GBRT implementations, namely XGBoost, LightGBM, scikit-learn, QuickRank, and RankLib. Additionally, it provides format conversion facilities that allow users to build a model with one tool and apply it through a di erent so ware.
3.6 How to use RankEval
RankEval is an open-source project2 wri en in Python to ensure exibility, extensibility, and e ciency. e core of the library ex-
ploits numpy, scipy and scikit-learn. ere are two ways in which RankEval can be used: i) as a
Python module (o ering also a command line interface) and ii) as a Python Jupyter notebook. e functionalities presented above are implemented in submodules. e results of each test are presented in the form of report tables and plots, which can be either saved into les or observed inline, if running the module in a Python Jupyter notebook.
4 RANKEVAL DEMO AT SIGIR 2017
We propose RankEval as a demonstration at ACM SIGIR 2017, consisting of a light and exible solution for LtR model evaluation. As we argue in this section, it is intended as a product itself more than a simple evaluation module for scienti c/research purposes.
e target audience for such a product is varied. It can be used by research scientists in the LtR eld, but also so ware engineers and product managers who look for the best practical solution for analysing and solving their machine-learned ranking problems.
To illustrate RankEval's applicability, suppose we have the following three scenarios: i) an engineer is looking for the best hyperparameter combination for training a target model, ii) a research scientist wants to obtain a fair evaluation between his model and a model derived with one competitor solution, and iii) a product manager wants to experiment the contribution and e ectiveness of
2Interested users can nd it at: h p://rankeval.isti.cnr.it.

new features to decide whether to add them or not in the production search environment.
In i) RankEval helps the engineer to evaluate the model by using the topological analysis o ering insights about the number of leaves, tree depth, and the tree-wise performance of the forest, which plots performance improvements by increasing the number of trees. In addition, for e ciency reasons he may decide to use only 50% of the trees as the statistical analysis submodule con rms that a smaller forest achieves statistically similar results in terms of accuracy w.r.t. the original one.
In ii) RankEval allows to import and process models through the interoperability among GBRT so ware o ering the possibility to unify the models to a single, comparable format by using the conversion facilites. Not only can the researcher evaluate model performance individually, but also compare them by plo ing the performance metrics of the two models in the same view and by exploiting the statistical analysis.
In iii) RankEval enables the manager to understand the contribution of the features to a model. Using the feature analysis submodule, she is able to compute feature importance and usage statistics in several ways. By loading distinct models, she can observe if the features have the same impact and how they may di er.
e demonstration of RankEval at ACM SIGIR 2017 will allow the three target users above to directly interact with its features. We will provide two PCs with a ready-to-use RankEval installation and we will invite people to experience and test RankEval on their own ranking models, so as to understand the potential bene ts of the analysis that RankEval provides.
Participants will also have the possibility to test RankEval on a repository of several ready-to-use ranking models built on publicly available LtR datasets by means of ve di erent libraries: LightGBM, XGBoost, QuickRank, RankLib, and scikit-learn. is will allow them to directly compare and assess the di erent results such libraries produce.
Acknowledgments. is work was supported by the EC H2020 INFRAIA-1-2014-2015 SoBigData (654024).
REFERENCES
[1] Gabriele Capannini, Domenico Dato, Claudio Lucchese, Monica Mori, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego, and Nicola Tonello o. 2015. ickRank: a C++ Suite of Learning to Rank Algorithms. In 6th Italian Information Retrieval Workshop.
[2] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). ACM, New York, NY, USA, 785­794.
[3] Van Dang. 2011. RankLib. Online. (2011). h p://www.cs.umass.edu/ [4] Jerome H. Friedman. 2000. Greedy Function Approximation: A Gradient Boosting
Machine. Annals of Statistics 29 (2000), 1189­1232. [5] Andrey Gulin, Igor Kuralenok, and Dmitry Pavlov. 2011. Winning e Transfer
Learning Track of Yahoo!'s Learning To Rank Challenge with YetiRank.. In Yahoo! Learning to Rank Challenge. 63­76. [6] Microso . 2016. LightGBM. Online. (2016). h ps://github.com/Microso / LightGBM [7] NIST. 2017. trec eval. Online. (2017). h p://trec.nist.gov/trec eval/ [8] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. irion, O. Grisel, M. Blondel, P. Pre enhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825­2830. [9] Mark D. Smucker, James Allan, and Ben Cartere e. 2007. A Comparison of Statistical Signi cance Tests for Information Retrieval Evaluation. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management (CIKM '07). ACM, New York, NY, USA, 623­632.

1284


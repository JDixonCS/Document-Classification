Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Learning to Rank Using Localized Geometric Mean Metrics

Yuxin Su
Department of Computer Science and Engineering
The Chinese University of Hong Kong Shatin, N.T., Hong Kong yxsu@cse.cuhk.edu.hk

Irwin King
Department of Computer Science and Engineering
The Chinese University of Hong Kong Shatin, N.T., Hong Kong king@cse.cuhk.edu.hk

Michael Lyu
Department of Computer Science and Engineering
The Chinese University of Hong Kong Shatin, N.T., Hong Kong lyu@cse.cuhk.edu.hk

ABSTRACT
Many learning-to-rank (LtR) algorithms focus on query-independent model, in which query and document do not lie in the same feature space, and the rankers rely on the feature ensemble about querydocument pair instead of the similarity between query instance and documents. However, existing algorithms do not consider local structures in query-document feature space, and are fragile to irrelevant noise features. In this paper, we propose a novel Riemannian metric learning algorithm to capture the local structures and develop a robust LtR algorithm. First, we design a concept called ideal candidate document to introduce metric learning algorithm to query-independent model. Previous metric learning algorithms aiming to nd an optimal metric space are only suitable for querydependent model, in which query instance and documents belong to the same feature space and the similarity is directly computed from the metric space. Then we extend the new and extremely fast global Geometric Mean Metric Learning (GMML) algorithm to develop a localized GMML, namely L-GMML. Based on the combination of local learned metrics, we employ the popular Normalized Discounted Cumulative Gain (NDCG) scorer and Weighted Approximate Rank Pairwise (WARP) loss to optimize the ideal candidate document for each query candidate set. Finally, we can quickly evaluate all candidates via the similarity between the ideal candidate document and other candidates. By leveraging the ability of metric learning algorithms to describe the complex structural information, our approach gives us a principled and e cient way to perform LtR tasks. The experiments on real-world datasets demonstrate that our proposed L-GMML algorithm outperforms the state-of-the-art metric learning to rank methods and the stylish query-independent LtR algorithms regarding accuracy and computational e ciency.
KEYWORDS
Learning to Rank, Distance Metric Learning, Local Metric Learning
ACM Reference format: Yuxin Su, Irwin King, and Michael Lyu. 2017. Learning to Rank Using Localized Geometric Mean Metrics. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: http://dx.doi.org/10.1145/3077136.3080828
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080828

1 INTRODUCTION
In many information retrieval systems, especially Web search, users expect to obtain the most relevant documents according to users' query phrase or document. This task is technically formulated as a ranking problem. Most of the Web search engines exploit this ranking task based on learning-to-rank (LtR) techniques [21]. In the LtR framework, a machine learning algorithm is typically employed to derive a ranking model about document collection from a training subset of documents with labels or partial order. After the supervised or semi-supervised learning procedures, the ranking model is expected to retrieval top-k (ordered) relevant documents from the candidate collection when a query is given.
In practice, search engines develop the LtR model in two stages: (i) candidate retrieval and (ii) candidate re-ranking [22]. In the
rst stage, search engine retrieves from the inverted document repository a su ciently large set of relevant candidate documents Dq matching a user's query. It is used to avoid applying the ranking model to all documents possibly matching a user's query. This stage usually requires that the size of candidate set is much larger than the number of the relevant URLs to be included in the returned page. Based on the candidate document set Dq obtained in the
rst stage, Web search engines reformulate the documents with features extracted from the query-document pair and hide query features, then employ the LtR model without the dependency of query instance to score and re-rank the document collection Dq . Finally, search engines return the top-k documents to the user.
In Web search engine, the time-budget of this two-stage framework is usually limited. Therefore, strongly motivated by the time budget consideration, the current two most e cient and the stateof-the-art methods are based on the additive ensemble of regression trees, namely Gradient-Boosted Regression Tree (GBRT) [11] and -MART [4]. These two kinds of methods are capable of meeting the time requirement with acceptable accuracy even when thousands of regression tree are evaluated for each document. However, one of the drawbacks of this line of methods is that when the input samples contain an enormous amount of non-informative features, many methods fail to identify the most relevant features. Therefore, researchers are still trying to devise techniques and strategies to nd a better way of combining features extracted from query-document pairs through discriminative training to accelerate the training process for document ranking without losing in quality [12, 35].
Another perspective to the ranking problem is to seek the best similarity measurement and develop the corresponding e cient algorithm. These approach aims to optimize the accuracy in the
rst stage to nd candidate documents or even directly return the

45

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

top-k documents with an order. Concerning accuracy, the similaritybased models for a ranking problem can be classi ed into three categories from the formulation of the loss function: pointwise, pairwise and listwise loss functions [3]. Practically, the pairwise loss function tends to be more e cient for training and have been widely adopted by large Web search engines [3].
The pairwise similarity motivates that how to apply the classical metric learning or similar learning methods to the ranking problem [7]. The metric learning algorithms aim to nd a better distance metric than Euclidean metric to measure the pairwise similarity. The advantage of such metric-learning-to-rank [24] framework has two folds: (1) the metrics often preserves the nearest neighborhood information, which is the perfect structure to conduct ranking; (2) a proper metric containing the structural information of the document collections in the document space is useful for reducing the over- tting and improving the robustness to noise features [16]. Therefore, the metric-learning-to-rank methods [18, 19, 25] typically enjoy higher accuracy. Nevertheless, unfortunately, the disadvantage of metric-learning-to-rank also has two folds: (1) many metric learning algorithms [8, 32] are degraded by its extremely high computational expense; (2) the similarity measurement is not suitable for LtR because we can not estimate the similarity between query and documents with features extracted from other domain knowledge.
In this paper, we focus on improving the ranking accuracy at the second stage in the search engine and attempt to provide a new query-independent model for LtR task. Di erent from the existing research on how to combine features extracted from other domains, we try to learn an optimal representation of these features via metric learning algorithm. To adopt query-dependent metric learning framework to a query-independent model, we propose a concept called ideal candidate document, which represents a perfect match for a given query. With the help of this concept, we can quickly evaluate all candidate documents and sort them by calculating the distance based on the optimal metric space between the ideal candidate document and other documents. Same with the querydependent model, the shorter distance leads to a higher relevance to the query.
Since features from di erent domains generate local structure on the whole feature space, in order to preserve more local information and avoid over tting, we develop a novel local metric learning framework for ranking with high e ciency and accuracy. Our localized metric learning algorithm extends from the state-of-theart global metric learning algorithm called Geometric Mean Metric Learning (GMML) [36], and we apply Weighted Approximate Rank Pairwise (WARP) loss to optimize the metric space around the ideal document from the combination of several anchor documents.
We summarize our main contributions as follows:
· To the best of our knowledge, we are the rst to extend geometric mean metric learning algorithm to a local metric learning approach in order to capture the local structures for LtR problem.
· We propose a novel ideal candidate document concept to transform metric-learning-to-rank framework from querydependent model to query-independent model, which brings

wider applications for metric learning and also improves the accuracy of classical LtR task. · We conduct extensive experiments to reveal that our method outperforms the state-of-the-art query-dependent metriclearning-to-rank algorithms and query-independent LtR methods both in the accuracy and the computational complexity.

2 PRELIMINARIES AND RELATED WORK
Since our approach employs local metric learning algorithm to conduct the LtR task, two sets of previous work relate to our work:

2.1 Learning to Rank
In the information retrieval setting, a search engine maintains a collection of candidate examples D. Given a query q, the search engine returns the top ranked subset of documents p  Rd  Dq  D from the collection with order, ranked by a speci c ranking model fq (p).
According to the formulation of the loss function, the LtR methods are categories into three folds: (1) pointwise loss approach, (2) pairwise loss approach and (3) listwise loss approach.
For pointwise loss function, Li et al. [17] cast the ranking problem to a multi-class classi cation problem. The training process relies on enough labeled information, which is not always easy to satisfy. Pairwise loss approach such as RankNet [2], RankBoost [10] focus on the relative order, which is capable of being adapted to classi cation problem. In the listwise loss approach, a relevance label l related with the query for ground truth is usually bound to the document p. Cao et al. [5] rst propose to nd the optimal permutation to minimize the listwise loss function. McFee [25] proposes a similar objective, but the di erent solution from the metric learning methods.
The majority of LtR methods follows listwise loss function. Currently, the most popular methods [4, 9, 11] come from the combination of an ensemble of trees like random forest and the boosting-like methods [10]. Based on multiple decision trees, this kind of methods gains an accepted level of accuracy.

2.2 Metric Learning
The (squared) Mahalanobis distance, an extension of Euclidean distance, measures the distance between two points lie on the special linear space. It is de ned as

dM (p1, p2) = (p1 - p2)T M (p1 - p2) ,

(1)

where p1, p2  Rd are input examples, M is a symmetric and

positive semi-de nite d × d matrix. When M = I, the Mahalanobis

distance is equivalent to the Euclidean distance.

There are plenty of algorithms aiming at learning such metric

by solving a semide nite or a quadratic program [29, 32, 34]. Al-

most all the metric learning algorithms try to constrain the similar

data points and to scatter those dissimilar data points. Early work

like [34] formulates this problem as an optimization problem on

the second-order cone, which is costly solvable. Davis et al. [8],

Weinberger et al. [32] and Shen et al. [29] formulate di erent kinds

of optimization problems, namely ITML, LMNN, BoostMetric re-

spectively. However, the common issue that their solutions are

46

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

computationally expensive. Very recently, Zadeh et al. [36] propose a new objective function and give the closed-form solution from the geometric domain. It is the most promising global metric learning method because of the computational speed several orders of magnitude faster than the widely used ITML and LMNN methods.
There are two di erent roadmaps to conduct the LtR tasks from the metric learning perspective: (1), McFee [25] and Lim et al. [18, 19] learn global metric with Positive Semi-De nite (PSD) constraint on the metric parameter. They belong to the application of the standard metric learning algorithm. (2), Chechik et al. [7] and Liu et al. [20] remove PSD constraint and employ the bilinear model to measure the similarity between two data points. Usually, without PSD constraints, the bilinear model easily leads to over- tting. However, PSD constraint brings a tremendous amount of computation.
In most cases, global metric learning relies on a learned PSD matrix, which is not only computational expensive in high-dimensional case but not reasonable for retrieval ranking problem. In the LtR framework, the local similarity is far more important than the dissimilar information because we aim at ranking the relevant documents around a user's query. Therefore, several important local metric learning approaches are related to our work. Wang et al. [31] parameterize the weight function of each data point. The approach enhances the model complexity but brings the extra computation. Hauberg et al. [13] provide the theoretical analysis about the optimal weight function. However, the calculation of the geodesics is extremely expensive.
3 OUR PROPOSED METHODOLOGY
In the LtR problem, a ranked list of the relevant documents is returned for a speci c user's query. In this situation, we can assume without losing generality that all relevant documents should be closer to an unreal document than other irrelevant documents. This unreal document should be related to the query. Therefore, although the query instance is not accessible in the document feature space, we can still construct this unreal candidate document to represent the query in the feature space of the document. In our paper, this unreal but perfect-matching document is named as the ideal candidate document.
Usually, the indexed documents are assumed to be static, and the set of queries considered as input testing data is dynamic. This assumption allows us to transform the training documents to an another static representation, and model ideal candidate documents for each query to a dynamic combination of static documents.
In our paper, we assume the documents including candidate documents and ideal documents lie on the surface of a Riemannian manifold. Then, we attempt to build the similarity measurement between documents on the geodesic lines in the Riemannian manifold. Very often, a single linear metric M can not describe the whole surface of Riemannian manifold adequately. It reveals the inability of a single metric to model the complexity of the LtR problem. Furthermore, the discriminative features vary between di erent neighborhoods on the surface of the manifold. To address this limitation, researchers try to learn a set of local metrics representing the various regions of the surface. In most cases, local metric learning

algorithms will generate a local metric for each learning example [26]. The whole parameters of these kinds of the algorithm are prohibitively huge when the number of examples becomes large.
In our approach, we follow [31] to learn a local metric for a part of the feature space of documents, in which case the number of learned metrics m can be considerably smaller than n, the size of the examples collection.
Suppose we have learned m local metrics {M1, . . . , Mm } and the associated anchor points p1, . . . pr , . . . pm . The choice of anchor points and the computation of local metrics are described in Subsection 3.1. Then the similarity model f (q, p) between two documents pi and pj is extended from Eq. (1) as follows:

f (pi , pj ) = dM(pi ) (pi , pj )

(2)

m

M(pi ) =

wr (pi )Mr ,

(3)

r =1

where wr (pi )  0 is the weight of document pi for local metric Mr . The PSD constraints of M(p) is automatically satis ed if all local metric Mr are PSD matrices. These formulation includes m anchor documents and pi should be close to these anchor documents [27]. It is clear that the ideal candidate document should be close to several
high relevant documents. Therefore, we can employ these high
relevant documents as anchor documents to construct the local metric space around the ideal candidate document.
With the above assumption and observation, the task of infor-
mation retrieval precedes in the following steps:

(1) Given a candidate collection Dq for query q , we employ high/low relevant documents to compute a M and nd a anchor point pr to maximize the ranking scorer under the metric M by computing (pi - pr ) M (pi - pr ) .
(2) After sampling m candidate collection to nd m anchor documents and m associated metrics, we can construct the ideal candidate document based on a combination of m
anchor documents.
(3) We can build an evaluation function to measure the similarity between candidate document and ideal candidate document, then, sort these documents via the similarity to ideal candidate document.

3.1 Computation of Basis Metrics
Before constructing the local metrics in Eq. (3), we need to learn m local metrics. With the assumption that each local metric Mr represents a part of feature space, we can employ the classical single metric learning algorithm associated with a subset of the triplets from a part of examples space.
In this paper, we extend the state-of-the-art global metric learning algorithm GMML [36] into local metric forms. The extension consists of two parts:
(1) The local basis metric associated with the triplets set Dr is computed by the original GMML.
(2) The smooth weighting function wr (p) is computed from Eq. (11).
Given a subset of the triplets Tr = pi , pj , pk such that pi is more similar to pj than to pk , we can extract the similarity set Sr and

47

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Training Data

Sampling

Query 1 Doc Doc Doc ... Query 2 Doc Doc Doc ...

Anchor Doc

Relevant Set Irrelevant Set

Anchor Doc

Relevant Set Irrelevant Set

...

Query 3 Doc Doc Doc ...

Query 4 . .

Doc Doc Doc ...

Training

Mahalanobis

...

Metrics

Test Query Doc

Doc

Doc ...

Evaluation

Ranked List Doc

Doc

Doc ...

Weighted Local Metric Model

NDCG Scorer

Figure 1: General framework of proposed L-GMML for ranking. Di erent gray levels in query test represent the relevant level of the document

the dissimilarity set Dr by following the instruction in Section 3.4. Then we construct two corresponding matrices:

Sr =

pi - pj pi - pj

(4)

(pi,pj ) Sr

Dr =

(pi - pk ) (pi - pk )

(5)

(pi,pk ) Dr

Then, the basic optimization formulation of local metric Mr is de ned as follows:

min
Mr 0

h(Mr ) := tr (Mr Sr ) + tr Mr-1Dr

(6)

Equation (6) implies that GMML will return a single local metric

Mr that minimize the sum of distances over all the similar pairs Sr

and maximize the distance over all the dissimilar pairs Dr .

The closed-form solution of Eq. (6) is obtained by

h(Mr ) = Sr - Mr-1Dr M-r 1

(7)

Taking h(Mr ) = 0, we obtain:

Mr Sr Mr = Dr

(8)

Equation (8) is a Riccati equation whose unique solution is [36]

Mr = Sr-1/2 Sr1/2Dr Sr1/2 1/2 Sr-1/2

(9)

In experiments, Mr is e ciently computed from Cholesky-Schur

method [14].

3.2 Smoothing Weight Functions
Lots of researchers try to provide the insights of their local metric learning approaches [13, 31] by modeling their methods from the perspective of Riemannian metric. An important property about the Riemannian metric is that a Riemannian metric M (p) on a manifold M is a smoothly varying inner product xi , xj p = xiT M(p)xj in the tangent space Tp M of each point p  M. From Lemma 1 in [13],

when the weight function wr (p) is smooth with p, Eq. (3) will be a well-studied Riemannian metric. Therefore, any well-designed
local metric methods should provide a smooth weight function. Another important issue is that the weight function wr (p) should
re ect the tness of the local metric Mr . Suppose (p, pr )  Sr , it indicates that Mr is the best local metric to measure the similarity between pr and other examples, which means that Eq. (8) should be robust against the additive similar pair (p, pr ). Therefore, the weight function wr (p) should be in the opposite to Mr (p -pr )(p -pr )T Mr .
Take the limit as an example, if Mr (p - pr )(p - pr )T Mr = 0, then,

Mr (Sr + (p - pr )) (Sr + (p - pr ))T Mr = Dr

(10)

The solution of Eq. (10) is the same with Eq. (9), which indicates that Mr (p - pr )(p - pr )T Mr is a proper measurement whether the Mr is the optimal local metric for the document p.
By taking consideration about the above observation, we propose

the smoothing weight functions [1] as:



wr (p) = exp

- 2

p - pr Mr

,

(11)

where,

·

2 Mr

is the L2 norm with the metric Mr .

p - pr

2 Mr

= tr

Mr (p - pr )(p - pr )T Mr

(12)

From Eq. (12) and Eq. (10), we can easily know p - pr Mr is a proper measurement about the similarity between query p and the

anchor point pr associated with the local metric Mr .

Therefore, our evaluation function is formulated as:

m
fq (p, q ) = - q(r ) · exp - p - pr Mr · p - pr Mr , (13)
r =1
where, q  Rm , q(r ) = exp q(r )/2 is the key parameter we
need to learn in order to nd a better manifold structure. Higher fq (p, q ) means that p is closer to the ideal candidate document. In

48

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

the next subsection, we will introduce our exploration to optimize  for a speci c ranking problem.
3.3 Update of 
In the above subsection, we formulate a general local metric framework in Eq. (13) to represent the manifold structure. From the theoretical analysis in [27], the whole space of  keeps the learned manifold smooth. Therefore, we de ne our loss function under the popular Weighted Approximate Rank Pairwise (WARP) framework [33] and optimize the associated objective function to obtain an optimal solution for ranking task.
The WARP loss for a given set of candidate document Dq with query ID q is de ned as:

L (q) = 1

L

Dq+ p  Dq+

q p+

,

(14)

where q (p+) is the number of violators in Dq for positive p+, de ned as:

q (p+) =

I fq p-, q - fq p+, q

(15)

p-  Dq-

To obtain better NDCG score, L (·) is de ned as:

L(k )

=

k i =1

log2

1 (i

+

1)

(16)

In order to optimize q , we follows the methods in [18, 33] to approximate L q (p+) by a continuous formulation with hinge
loss:

L Vq,p+
p- Vq,p+

 - fq p+, q + fq p-, q + , (17) Vq, p +

where for a given q, p+,  is the hinge loss margin. Vq,p+ is the set of violators with hinge loss:

Vq,p+ = p-  Xq- | fq (p+, q )

(18)

In order to obtain an unbiased estimation of the loss function in Eq. (17), we can randomly sample q, p+  Dq and nd an violator
p- such that  + fq p-, q > fq p+, q . In this situation, the tuple of q, p+, p- has the following contribution to Eq. (17):

l q, p+, p- = L Vq,p+  - fq p+, q + fq p-, q (19)
From the WARP framework, Vq,p+ can be approximated by Dq- /Nq , where Nq is the number of less relevant documents p- drawn with replacement from Dq- until a violator is found. Finally, the stochastic gradient descent for the parameter  can be easily conducted at iteration t as:

q (t + 1)

=q (t ) - µ l

q, p+, p- q (t )

,

(20)

=q (t ) - µL

 

Dq-

 



 

Nq

 

·

 



fq

(p-,

q

(t

)

)



-

 fq

p+, q (t )

 

q (t )

q (t )



  

,



 



 

(21)

where

fq (p, q ) q

=

fq p, q(r ) q(r )

. To avoid over- tting,
r =1...m

we project q(r ) to zero when Eq. (21) leads to negative value. We

take derivation from Eq. (13) to obtain:

 fq p, q(r )

q(r )

= exp - p - pr Mr · p - pr Mr

(22)

Overall, our proposed algorithm is illustrated in Figure 1 and

summarized in Algorithm 2.

3.4 Sampling Strategy
Our approach will not iterate all triplets for D introduced in Section
3.1, because learning the global ranking model from all triplets
is an NP-hard problem [23]. Hence, we choose to stochastically
sample the triplets pi , pj , pk from candidate documents. pi and pj representing similar documents are sampled from high relevant documents, then pk is sampled from the less relevant documents. In our implementation, we only sample pk from the documents with zero relevant label.
For sampling procedure in Section 3.3, i and j are independent for two queries i and j. Therefore, the update can be computed in a
highly parallel way.

ALGORITHM 1: Geometric Mean Metric Learning (GMML) [36]
Input: D+ : positive set of documents, D- : negative set of documents,  : regularization parameter
Output: M  Sd+ : Mahalanobis metric; S = I + pi pj,pi D+,pj D+ pi - pj pi - pj ;
D = I + pi D+,pj D- pi - pj pi - pj ; M = S-1/2 S1/2DS1/2 1/2 S-1/2

4 EVALUATION
In this section, we discuss the implementation of our approach for the LtR problem and display extensive experiments evaluating our methodology in comparison to the state-of-the-art (R-MLR, GBRT, and -MART). Our design on experiments tackle the following questions:
· Do we develop a correct localized extension to the global GMML? To answer this question, we generate varied scale synthetic datasets to evaluate the performance gain against global metric learning algorithm when a di erent number of local metrics invoke in our L-GMML approach to prove the correctness.

49

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

ALGORITHM 2: L-GMML to Rank

Input: Candidate set for c queries D1, D2, . . . , Dq, . . . , Dc , m : number of local metrics, T : number of iteration, µ : step size,  :

hinge loss margin

Output: (p1, M1) , (p2, M2) . . . , (pm, Mm ) : set of local metrics and associated anchor points, p  Rd , M  Sd+ ,   Rc×m : weights for local metrics to model the ideal candidate documents for each

queries

for q  [1, c] do Extract Dq+ and Dq- from Dq ;

end

for i  [1, m] do

Sample Di+ and Di- from Dq q[1,c];

Mi = GMML Di+, Di- ;

for p  p(i )

Di+ do Sort Di

in

ascending

order

by

computing

p -d

2 Mi

d  Dq ;

end

Find the anchor point pr with maximum NDCG score of p(ir) ; end

for t = 1 to T do

Sample a tuple (q, p+, p-) from Dq q[1,c] such that

 + fq p+, q (t ) > fq p-, q (t ) ;

Nq  the number of less relevant documents drawn with replacement from Dq- until p- is found;

q (t + 1) =

q (t ) - µ L

Dq- Nq

·

fq (p-, q (t )) q (t )

-

fq (p+, q (t ))
q (t )

;
+

end

· Is our assumption on the existence of local structures reasonable? If reasonable, does our solution enjoy high computational e ciency and the good scalability for scaled datasets? We make comparisons with the state-of-the-art metric learning algorithms for ranking in the query-dependent model on scaled datasets. We attempt to demonstrate the improvements of our approach over other metric learning algorithms.
· Does our LtR algorithm have any amazing properties? We conduct experiments on real-world large-scale datasets to illustrate the enormous improvement of our approach on accuracy compared with the dominant ranking methods like GBRT and -MART in the query-independent framework.
4.1 Experiments Setting
In our experiments, we have implemented our local GMML (LGMML) algorithm in Julia1, the source code is released at Github2. To make a fair comparison against the state-of-the-art ranking methods, we also implement R-MLR, GBRT and -MART in Julia. We take RankLib3, an open-source implementation of the GBRT
1 http://julialang.org/ 2 https://github.com/yxsu/LtR.jl 3 http://sourceforge.net/p/lemur/wiki/RankLib/

Table 1: Di erent kinds of song representation

# of features # of songs

Audio Lyrics-128 Lyrics-256

1,024 128 256

5,419 2000 2000

and -MART algorithms and MLR4 as references to implement these algorithms in Julia.
Our program is executed on an Ubuntu 14.04 LTS server with 12 Intel Xeon E5-2620 cores and 128GB main memory. All baseline methods and our method are performed in a parallel way to fully utilize the computational resources. Our R-MLR implementation is based on the parallel MLR-ADMM [19]. GBRT and -MART come from RankLib.
The statistical tests in the following experiments are computed over the values for Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG) [15] at the top k retrieved documents denoted as NDCG@k. These two metrics are the most important and frequently used in information retrieval community to evaluate a given permutation of a ranked list using binary and multi-relevance order.
4.2 Datasets
For all real-world datasets, we split each of them into two components: 1), the training set is used to learn ranking models; 2), the test set is purely used to evaluate the performance of the learned ranking models.
All the datasets we use are freely available online for scienti c purpose. Such datasets can be divided into two groups:
4.2.1 ery-dependent Dataset. We employ CAL10K [30] to make fair comparisons between our L-GMML and R-MLR. Because, in the original paper, R-MLR performs well on the CAL10K dataset. Following the experiments in [19], we use a subset of the CAL10K dataset, which is provided as ten 40/30/30 splits of a collection of 5419 songs.
4.2.2 ery-Independent Datasets. In this subsection, we employ two popular real-world large-scale datasets: Yahoo! and MSN to evaluate the competitive performance of proposed L-GMML against the state-of-the-art query-independent LtR methods.
Yahoo! datasets come from Yahoo! Learning to Rank Challenge [6]. The datasets consist of feature vectors extracted from query-url pairs along with relevance judgment labels.
In our experiments, we also employ the two set of MSN learning to rank5 datasets: MSLR-10K and MSLS-30K, both of which consists of 136 features extracted from query-url pairs. The MSN datasets provide relevance judgment labels ranging from 0 (irrelevant) to 4 (perfect match). In experiments, each MSN dataset is partitioned into ve subsets for ve-fold cross validation.
The complete statistical information about these datasets are listed at Table 2.
4 https://github.com/bmcfee/mlr 5 https://www.microsoft.com/en-us/research/project/mslr/

50

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Characteristics of publicly available large-scale datasets for learning to rank

Name

# of Queries

# of Doc.

Rel. Levels # of Features Year

Train Vali. Test Train Vali. Test

Yahoo! Set I 19,944 2,994 6,983 473,134 71,083 165,660

5

Yahoo! Set II 1,266 1,266 3,798 34,815 34,881 103,174

5

MSLR-WEB10K 6,000 2,000 2,000 723,412 235,259 235,259

5

MSLR-WEB30K 31,531 6,306 6,306 3,771k 6,306 753k

5

519

2010

596

2010

136

2010

136

2010

4.3 Evaluation of the Proposed Approach
In our L-GMML model, the most important hyper-parameter is the number of local metrics, which has signi cant in uence on the overall model performance. We will evaluate the correction of our localized extension method from synthetic datasets, and reveal the impact of the metric numbers.
4.3.1 Global GMML vs Local GMML. In this subsection, we attempt to employ multi-class classi cation problem to verify the correction of the local metric learning algorithm. Because multiclass synthetic datasets certainly contain local structures around the center of each class. If the accuracy gain is observed, we can also address the objective that local metric learning approach is designed to extend the global metric learning method's ability of modeling complex data manifold.
We employ the normal distribution to generate synthetic datasets with multiple centers and 95% con dence interval. The datasets with {10,50,100} classes are denoted as Synthetic-10, Synthetic-50, Synthetic-100 respectively. In these synthetic datasets, we assign the index of class to the relevant label of the corresponding data point.
We report the performance gain of the proposed local GMML against the global GMML in Figure 2. We can easily nd the fact that when the number of local metrics is approximate to the number of the real centers in Gaussian synthetic data distribution, the relative accuracy gain of local metrics is maximized. This observation meets the objective of local metric learning approach.
4.3.2 The Number of Local Metrics. In this subsection, we will evaluate the signi cance of the number of local metrics, which is typically the most important parameter in the eld of local metric learning. A large number of local metrics will enhance the algorithm's ability to model the complex manifold structure. However the computational complexity increases linearly with the number of local metrics. In experiments, we need to carefully tune the number of local metrics to make the balance between model's ability and computational complexity.
Figure 3 displays the impact of the number of local metrics on all datasets used in our paper. For all datasets, localized method can compete with the corresponding global method with a single metric. This fact proves that our localized extension is reasonable. Another obvious observation is that the optimal number of local metrics varies dramatically among di erent datasets, since it is decided by the complexity of the manifold structure sealed in the data space.
4.3.3 Scalability. In our experiments, the synthetic datasets is primarily invoked to evaluate the scalability of our approach.

Due to the limited scalability of real-world datasets, we synthesize datasets with the feature dimensionality scaled from 10 to 1000. In this experiment, we x the number of local metrics as 10 since we only concern about the computational complexity on di erent scaled dimensions instead of the optimal number of local metrics. Figure 4 illustrates the training time of our L-GMML on these datasets.
Compared with other local metric learning methods, the less training time come from two-fold issues: (1) the GMML in Algorithm 1 is very fast. (2) The update of weighting function in our approach is relatively simple and straightforward. It does not involve the huge computational resources to nd the optimal form.
4.4 Comparison with R-MLR
The Robust Metric-Learning-to-Rank (R-MLR) [19] is the most competitive metric learning method for ranking. It retrieves relevant examples in response to a query instance. To make direct comparisons, we need to modify our approach by assigning all anchor points to the query instance. Because our approach is originally designed for the query-independent framework.
In this set of experiments, we evaluate our approach on the music similarity task, because the R-MLR method is veri ed to be successful in music similarity task compared with other metriclearning-to-rank methods such as MLR [25], L1-MLR [28]. For each song pi , a relevant set Di+  Dtrain is de ned as the subset of songs in the training set performed by the top 10 most similar artists to the performer of pi , where the similarity between artists is measured by the number of shared users in a sample of collaborative
lter data [24]. This top-10 thresholding results in the relevant sets in this data being asymmetric and non-transitive. Therefore, the traditional pairwise metric learning methods do not work in this situation. However, our approach based on the sampling on the relevant set is not necessary to obey the symmetric and transitive properties.
The experiments are conducted on two di erent kinds of song representation: audio and lyrics, whose details are listed in Table 1. We use recommended candidate hyper-parameters in the original paper to tune R-MLR on validation set and select the best parameter to evaluate the performance of the model.
Since the scalability of the original R-MLR is limited, the experiments of R-MLR employ the latent features compressed by PCA. Our approach has no such problem and is suitable to conduct the training process on the original 1,024 features.
Figure 5 illustrates the performance of three metric learning algorithms. We x the number of local metrics in our L-GMML as 1 to obtain the global GMML algorithm. The motivation of making

51

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

MAP

Synthetic-2

Synthetic-10

Synthetic-50

Synthetic-100

0.42

L-GMML

GMML

0.5

0.45

0.34

0.41

0.4

0.39

2

4

MAP

0.45
0.4 0

L-GMML

GMML

0.4

MAP

MAP

0.32

L-GMML

GMML

0.3

0.28

0.35

5

10

15

0

20

40

60

0

L-GMML GMML

50

100

# of metrics

# of metrics

# of metrics

# of metrics

Figure 2: Comparisons between global GMML and local GMML on synthetic datasets. The performance is measured by MAP

MAP

Audio 0.68 0.66 0.64 0.62

0

50 100

# of metrics Yahoo Set I

0.7 0.6 0.5 0.4
0

200 400 # of metrics

NDCG@10

MAP

Audio with PCA
0.7 0.68 0.66 0.64
0 20 40 60 80 # of metrics Yahoo Set II

0.7 0.6 0.5 0.4
0

200 400 # of metrics

NDCG@10

MAP

Lyrics128

0.64 0.62 0.6 0.58
0

20 40 60
# of metrics MSLR-10K

0.5 0.4 0.3
0 200 400 # of metrics

NDCG@10

MAP

Lyrics256

0.6

0.55

0

50

# of metrics MSLR-30K

0.5 0.4 0.3 0.2
0

200 400 # of metrics

NDCG@10

Figure 3: The variation of performance caused by di erent number of local metrics

104

103

Training time (s)

102

101

100
10-1 0

R-MLR L-GMML
200 400 600 800 1,000 # of dimension

Figure 4: Training time of L-GMML on di erent scaled synthetic datasets

Table 3: Comparison on the training time of R-MLR and LGMML. The number of local metrics in L-GMML is xed as 50

Time (s)

R-MLR L-GMML

Audio

N/A

38

Audio with PCA 607

4.7

Lyrics-128

302

2.6

Lyrics-256

1241

7.8

such comparison is that we attempt to demonstrate the di erent in uence of the new GMML algorithm and our proposed L-GMML algorithm on the performance improvements.
Therefore, we can draw the conclusion from the experiments in this subsection that the proposed approach outperforms other metric learning algorithms for the ranking problem regarding accuracy and computational e ciency.

52

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

MLR

0.8

GMML

L-GMML

0.7

0.6

0.5

Audio

Lyrics-128

Lyrics-256

Figure 5: Music similarity performance of each algorithm on the three feature representation Audio, Lyrics-128 and Lyrics-256. Performance was measured by MAP and averaged across 10 folds.

4.5 Comparisons on Large-scale Real-world Datasets
We attempt to nd amazing features of our method in the comparisons with two state-of-the-art ranking methods, Gradient-Boosted Regression Trees (GBRT) and -MART on Yahoo! Set I&II, MSLR10K, and MSLR-30K. Because they have been proved to be the most e ective in the Yahoo! learning to rank challenge and become the dominant methods in the LtR eld.
For these four datasets, the feature domain varies dramatically. To avoid for challenging the oating point precision in complex mathematical computation, we preprocess these four datasets by normalizing each feature dimension with 2-norm. For the stochastic sampling procedure in Algorithm 2, to nd the optimal model, we try di erent initial weight values (1) ranging from 0.1 to 10, the hinge loss margin  ranging from 0.01 to 1.
The training time of GBRT and -MART is sensitive to the number of trees in both of the models. The number of local metrics also determines the training time of L-GMML. When we plan to make comparisons on the accuracy and training time of three methods, we x the number of trees of GBRT and -MART as 5000 and the number of local metrics as 500. The motivation of these choices is that the performance of these two methods become stable on the four datasets. The comprehensive comparisons on a di erent measurement of the above three methods are illustrated in Table 4. From the table, we can draw a conclusion that our approach enjoys a huge advantage in accuracy compared with the state-of-the-art ranking methods.
Currently, the only disadvantage of our approach lies in scoring time. Table 5 displays the comparisons about the time of scoring documents. Our algorithm heavily relies on the scoring for each document in di erent stages, which is less e cient than GBRT and -MART. On the other hand, our approach is simple in structure, and GMML in the rst stage is also e cient. Therefore, our

method still has an advantage in computationally e ciency. The time-consuming comparison in Table 4 can prove this statement.
5 CONCLUSION
In this paper, we focus on improving the accuracy of LtR methods by utilizing the local structure of documents and degrading irrelevant features. We rstly developed a localized GMML algorithm for the query-independent ranking framework. Speci cally, we proposed a concept called ideal candidate document to adopt metric learning for ranking algorithm from a query-dependent model to widely used query-independent model. In our approach, a well de ned smooth weighting function is optimized by reducing the popular WARP loss, which is de ned for the candidate document set of a given query. Then we can e ciently score document by calculating the distance between candidate documents and a nonexistent ideal candidate document from an optimized metric space. The experiments prove that our approach outperforms both of the state-of-the-art querydependent algorithms and query-independent algorithms.
6 ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper. The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14234416 of the General Research Fund), and 2015 Microsoft Research Asia Collaborative Research Program (Project No. FY16- RES-THEME005).
REFERENCES
[1] D. Broomhead, D. S. and Lowe. 1988. Multivariable Functional Interpolation and Adaptive Networks. Complex Systems 2 (1988), 321­ 355.
[2] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning - ICML '05. 89­96. DOI: http://dx.doi.org/10.1145/1102351.1102363
[3] Christopher J C Burges. 2010. From RankNet to LambdaRank to LambdaMART : An Overview. Technical Report (2010).
[4] Christopher J C Burges, Krysta M Svore, Paul N. Bennett, Andrzej Pastusiak, and Qiang Wu. 2011. Learning to Rank Using an Ensemble of Lambda-Gradient Models. Journal of Machine Learning Research (JMLR): Workshop and Conference Proceedings 14 (2011), 25­35.
[5] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank. In Proceedings of the 24th international conference on Machine learning ICML '07. ACM Press, New York, New York, USA, 129­136. DOI:http://dx.doi. org/10.1145/1273496.1273513
[6] Olivier Chapelle. 2011. Yahoo ! Learning to Rank Challenge Overview. JMLR: Workshop and Conference Proceedings 14 (2011), 1­24.
[7] Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. 2010. Large Scale Online Learning of Image Similarity Through Ranking. The Journal of Machine Learning Research 11 (3 2010), 1109­1135.
[8] Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. 2007. Information-theoretic metric learning. In Proceedings of the 24th international conference on Machine learning - ICML '07. ACM Press, New York, New York, USA, 209­216. DOI:http://dx.doi.org/10.1145/1273496.1273523
[9] Clebson C.A. de Sá, Marcos A. Gonçalves, Daniel X. Sousa, and Thiago Salles. 2016. Generalized BROOF-L2R. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16. ACM Press, New York, New York, USA, 95­104. DOI:http://dx.doi.org/10. 1145/2911451.2911540
[10] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An E cient Boosting Algorithm for Combining Preferences. Journal of Machine Learning Research 4 (2003), 933­969. DOI:http://dx.doi.org/10.1162/jmlr.2003.4.6.933
[11] Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics 29, 5 (2001), 1189­1232. DOI:http://dx.doi.org/ DOI10.1214/aos/1013203451 arXiv:arXiv:1011.1669v3

53

Session 1B: Retrieval Models and Ranking 1

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 4: Performance of GBRT, -MART and proposed L-GMML on large-scale real-world datasets. Results of MSLR-WEB10K and MSLR-WEB30K are averaged from the 5 folds in the datasets.

GBRT

-MART

L-GMML

Dataset

Test Set

Time (min.)

Test Set

Time (min.)

Test Set

Time (min.)

NDCG@5

0.6529

41.2

0.6567

46.5

0.6698

28.1

Yahoo! Set I NDCG@10

0.6824

43.3

0.7060

48.0

0.6715

28.9

NDCG@20

0.6912

41.5

0.7091

46.9

0.6934

28.8

NDCG@5

0.6731

37.6

0.6791

43.1

0.7096

26.5

Yahoo! Set II NDCG@10

0.6817

36.8

0.7062

43.3

0.7264

26.6

NDCG@20

0.6954

37.4

0.7087

43.8

0.7219

26.4

NDCG@5 0.4019 ± 0.0083 MSLR-WEB10K NDCG@10 0.4342 ± 0.0219
NDCG@20 0.4512 ± 0.0279

49.4 ± 5.2 48.3 ± 2.1 48.8 ± 3.8

0.4417 ± 0.0131 0.4513 ± 0.0196 0.4634 ± 0.0257

58.3 ± 2.8 57.6 ± 3.8 57.1 ± 5.2

0.4771 ± 0.0951 0.5390 ± 0.0812 0.551 ± 0.0728

19.7 ± 2.1 19 ± 3.1 19 ± 2.8

NDCG@5 0.409 ± 0.0312 MSLR-WEB30K NDCG@10 0.4146 ± 0.0327
NDCG@20 0.421 ± 0.361

167 ± 28.6 177 ± 30.1 167 ± 27.3

0.3812 ± 0.0297 0.409 ± 0.0232 0.4112 ± 0.0240

182 ± 19.8 183 ± 17.9 181 ± 10.7

0.4837 ± 0.0715 0.4976 ± 0.0619 0.5038 ± 0.0718

71.7 ± 2.7 71.9 ± 3.9 72.5 ± 5.3

Table 5: Per-document scoring time of GBRT, -MART and L-GMML on Yahoo! and MSLR datasets. The scoring time is united in ms

GBRT -MART L-GMML

Yahoo! Set I 276

302

421

Yahoo! Set II 218

286

421

MSLR-WEB10K 73

92

158

[12] Yasser Ganjisa ar, Rich Caruana, and Cristina Videira Lopes. 2011. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information - SIGIR '11. ACM Press, New York, New York, USA, 85. DOI: http://dx.doi.org/10.1145/2009916.2009932
[13] SÃÿren Hauberg, Oren Freifeld, and Michael J. Black. 2012. A Geometric take on Metric Learning. In Advances in Neural Information Processing Systems. 2024­ 2032.
[14] Bruno Iannazzo. 2016. The geometric mean of two matrices from a computational viewpoint. Numerical Linear Algebra with Applications 23, 2 (3 2016), 208­229. DOI:http://dx.doi.org/10.1002/nla.2022
[15] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422­446. DOI:http://dx.doi. org/10.1145/582415.582418
[16] Brian Kulis. 2013. Metric Learning: A Survey. Foundations and Trends® in Machine Learning 5, 4 (2013), 287­364. DOI:http://dx.doi.org/10.1561/2200000019
[17] Ping Li, Qiang Wu, and Christopher J. Burges. 2008. McRank: Learning to Rank Using Multiple Classi cation and Gradient Boosting. In Advances in Neural Information Processing Systems. 897­904.
[18] Daryl Lim and Gert Lanckriet. 2014. E cient Learning of Mahalanobis Metrics for Ranking. In Proceedings of The 31st International Conference on Machine Learning. 1980­1988.
[19] Daryl K H Lim, Brian Mcfee, and Gert Lanckriet. 2013. Robust Structural Metric Learning. Proceeding of the 30th International Conference on Machine Learning (ICML '13) 28 (2013), 615­623.
[20] Kuan Liu, AurÃlien Bellet, and Fei Sha. 2015. Similarity Learning for HighDimensional Sparse Data. Aistats 38 (2015), 1­14.
[21] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Foundations and Trends® in Information Retrieval 3, 3 (6 2009), 225­331. DOI:http://dx.doi.org/10. 1561/1500000016
[22] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego,
and Nicola Tonellotto. 2015. Speeding up Document Ranking with Rank-based Features. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15. ACM Press, New

York, New York, USA, 895­898. DOI:http://dx.doi.org/10.1145/2766462.2767776 [23] Brian McFee. 2012. More like this: machine learning approaches to music similarity.
Ph.D. Dissertation.
[24] Brian McFee, Luke Barrington, and Gert Lanckriet. 2012. Learning content similarity for music recommendation. IEEE Transactions on Audio, Speech and Language Processing 20, 8 (2012), 2207­2218. DOI:http://dx.doi.org/10.1109/TASL. 2012.2199109
[25] Brian McFee and Gert R Lanckriet. 2010. Metric learning to rank. Proceedings of the 27th International Conference on Machine Learning (ICML-10) (2010), 775­782.
[26] Yung-Kyun Noh, Byoung-Tak Zhang, and Daniel D Lee. 2010. Generative local metric learning for nearest neighbor classi cation. Advances in Neural Information Processing Systems 18 (2010), 417­424.
[27] Deva Ramanan and Simon Baker. 2011. Local Distance Functions A Taxonomy, New Algorithms, and an Evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 4 (2011), 794­806.
[28] Rómer Rosales and Glenn Fung. 2006. Learning sparse metrics via linear programming. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '06. ACM Press, New York, New York, USA, 367. DOI:http://dx.doi.org/10.1145/1150402.1150444
[29] Chunhua Shen, Junae Kim, Lei Wang, and Anton van den Hengel. 2012. Positive Semide nite Metric Learning Using Boosting-like Algorithms. Journal of Machine Learning Research 13 (2012), 1007­1036. http://dl.acm.org/citation.cfm?id= 2343679
[30] Derek Tingle, Youngmoo E. Kim, and Douglas Turnbull. 2010. Exploring automatic music annotation with "acoustically-objective" tags. In Proceedings of the international conference on Multimedia information retrieval - MIR '10. ACM Press, New York, New York, USA, 55. DOI:http://dx.doi.org/10.1145/1743384.1743400
[31] Jun Wang, Alexandros Kalousis, and Adam Woznica. 2012. Parametric Local Metric Learning for Nearest Neighbor Classi cation. In Advances in Neural Information Processing Systems. 1601­1609.
[32] Kilian Q Weinberger and Lawrence K Saul. 2009. Distance Metric Learning for Large Margin Nearest Neighbor Classi cation. Journal of Machine Learning Research 10 (2009), 207­244. DOI:http://dx.doi.org/10.1126/science.277.5323.215
[33] Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: Learning to rank with joint word-image embeddings. Machine Learning 81, 1 (2010), 21­35. DOI:http://dx.doi.org/10.1007/s10994-010-5198-3
[34] Eric P. Xing, Michael I. Jordan, Stuart J. Russell, and Andrew Y. Ng. 2002. Dis-
tance Metric Learning with Application to Clustering with Side-Information. In Advances in Neural Information Processing Systems. 521­528. [35] Zhixiang Eddie Xu, Kilian Q Weinberger, Olivier Chapelle, St Louis, and
Olivier Chapelle Cc. 2012. The Greedy Miser: Learning under Test-time Budgets. In Proceedings of the 29th International Conference on Machine Learning (ICML-12). 1175­1182.
[36] Pourya Habib Zadeh, Reshad Hosseini, and Suvrit Sra. 2016. Geometric mean metric learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML '16). 2464­2471.

54


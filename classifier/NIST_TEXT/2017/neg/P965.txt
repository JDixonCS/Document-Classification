Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Translation of Natural Language ery Into Keyword ery Using a RNN Encoder-Decoder

Hyun-Je Song
Naver Search Naver Corporation Seongnam 13561, Korea hyunje.song@navercorp.com

A-Yeong Kim
School of Comp. Sci. & Eng. Kyungpook National University
Daegu 41566, Korea aykim@sejong.knu.ac.kr

Seong-Bae Park
School of Comp. Sci. & Eng. Kyungpook National University
Daegu 41566, Korea sbpark@sejong.knu.ac.kr

ABSTRACT
e number of natural language queries submi ed to search engines is increasing as search environments get diversi ed. However, legacy search engines are still optimized for short keyword queries.
us, the use of natural language queries at legacy search engines degrades the retrieval performance of the engines. is paper proposes a novel method to translate a natural language query into a keyword query relevant to the natural language query for retrieving be er search results without change of the engines. e proposed method formulates the translation as a generation task. at is, the method generates a keyword query from a natural language query by preserving the semantics of the natural language query. A recurrent neural network encoder-decoder architecture is adopted as a generator of keyword queries from natural language queries. In addition, an a ention mechanism is also used to cope with long natural language queries.
CCS CONCEPTS
·Information systems  ery reformulation; ·Computing methodologies Neural networks;
KEYWORDS
ery reformulation; RNN encoder-decoder; Translation natural language query into keyword query; Neural machine translation
1 INTRODUCTION
Keyword query is a common input form to most traditional web search engines. When a query composed of a single or a few keywords is submi ed to a search engine, the engine provides its best result relevant to the query. With the help of the huge number of studies on search engines and factual information derived from knowledge graphs [21] or community QAs [13], most search engines provide their users with high quality results. For instance, Figure 1a shows the search result of a query `Obama age' by Google.
e result is accurate as well as concise. As virtual assistants such as Google Allo or Microso Cortana get available, natural language queries become widely used. e
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080691

(a)

(b)

Figure 1: Examples on di erent search results according to query forms. (a) is the result of `Obama age', while (b) is that of "I would be happy if you can tell me how old Obama is."

natural language queries, however, su er from so-called "lexical chasm" [18] between keyword queries and natural language queries, since each query type has its own style and vocabulary. erefore, the direct use of natural language queries at traditional web search engines degrades the retrieval performance of the engines. Figure 1b shows such an example which is the search result of a natural language query "I would be happy if you can tell me how old Obama is." Although this query is semantically similar to the previous query at Figure 1a, its result is much di erent from Figure 1a and is not satisfactory at all. Some studies adopted natural language processing (NLP) techniques such as dependency parsing or relation extraction to solve this problem [5], but these NLP techniques are specialized in question manipulation and are not adequate for keyword queries.
is paper proposes a simple method to translate a natural language query into its corresponding keyword query for retrieving be er search results. e proposed method formulates the query translation as a generation task. at is, a keyword query is generated from a natural language query by preserving the semantics of the natural language query as much as possible. For this, this paper adopts a recurrent neural network (RNN) encoder-decoder [3, 19] which is widely used in several NLP tasks such as machine translation [19] and text summarization [4]. ese NLP tasks are similar to ours in that they need to preserve the semantics between their input and output, and the output is generated from the input.
In the proposed RNN encoder-decoder architecture, an encoder encodes a natural language query into a vector representation, and a decoder generates a keyword query relevant to the natural language query from the vector representation. Note that the length of natural language queries is in general quite long. us, an a ention mechanism [2] is adopted in the encoder-decoder architecture to

965

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

cope with the long natural language queries, since it can focus on some speci c parts of an input sequence. As a result, the proposed method comes to generate a relevant keyword query, even if an input natural language query is quite long.
e advantages of the proposed method are twofold. One is that the proposed method requires just a set of pairs of a natural language query and its corresponding keyword query, since encoder-decoder architectures are trainable only with input-output pairs. erefore, it does not require any other resources dependent on a search engine like click-through graphs. e other is that legacy web search engines can be used to retrieve search results of natural language queries, since the proposed method outputs a keyword query adequate to the engines.
2 RELATED WORK
ery reformulation for search engines has been an important task of information retrieval to provide search engine users with be er search results. us, there have been a number of approaches to query reformulation. e approaches include query expansion by pseudo-relevance feedback [22] and by knowledge base [16], and query rewriting with correlation information derived from query logs [11] or click-through graphs [1]. e major problem of these approaches is that they require some resources dependent on a speci c search engine.
Several previous studies adopted machine translation techniques as an extension of query rewriting [6, 18]. ese studies regard a user query as a sentence in a source language of machine translation and a retrieved web document as a sentence in a target language, respectively. en, they translate a user query into an extended query suitable for retrieving web documents. Recently, He et al. [8] adopted a recurrent neural network encoder-decoder that is the state-of-the-art architecture in machine translation. However, since these studies focused on query expansion, they just tried to preserve the whole semantics of a user query while the query contains many words that cannot be keywords.
ere have been also several studies that adopt the translation of natural languages into keywords for sub-query identi cation [12]. In order to identify sub-queries, Huston and Cro [10] removed a few unimportant words, and Wang and Zhai [20] substituted the words in a query with their similar words. Park and Cro [17] and Maxwell and Cro [15] calculated the importance of the words in a query, and then chose relevant words from the query according to the importance. All these studies formulated translation as a selection or a substitution of words. us, it is di cult to produce concise expressions relevant to the original query by the methods. On the other hand, the proposed method considers translation as a generation of a query. As a result, the translated keyword query by the proposed method contain some expressions unseen at the original query, but relevant to the query.
3 TRANSLATION OF NATURAL LANGUAGE
QUERY INTO KEYWORD QUERY
When a natural language query x = {x1, ...xn } is given by a user where xi is the i-th word of the query, the goal of this paper is to translate x into a keyword query y = { 1, ..., m } such that y has the same meaning as x. Since this translation can be explained with the

conditional probability p(y|x) from the probabilistic perspective, a

recurrent neural network (RNN) encoder-decoder is used to model

the conditional probability. e RNN encoder-decoder is a neural

network that is designed to model directly the conditional probabil-

ity of translating an input sequence x to an output sequence y, and

consists of two RNNs. e rst RNN (encoder) encodes the input

sequence to its continuous representation. It reads x sequentially,

and then encodes it into a sequence of hidden states h represented

as h = (h1, h2, ..., hn ), where ht  Rdh is a hidden state at time t

and dh is the number of dimensions. Following the study of Graves and Schmidhuber [7], each hidden state hi in the encoder is repre-

sented again by two RNNs: a forward RNN and a backward RNN.

e forward RNN reads x in forward direction, while the backward

RNN reads it in reverse direction. en, the hidden state ht at time

t is actually a concatenation of the hidden states from the forward

- -

- -

and the backward RNNs. - --- ---

at is, ht = [h-t ; ht ],-w--her-e--ht and ht are

de ned as ht = fe--n-c (ht -1,e-x-(-xt )) and ht = fenc (ht +1, ex (xt )), re-

spectively. Here, fenc and fenc are non-linear activation functions,

and ex (xt ) is an embedding vector of xt .

e second RNN (decoder) de nes the probability of a keyword

query y by decomposing the probability into ordered conditional

probabilities.

m
p(y) = p( i |y<i, x),
i =1

where y<i = { 1, ..., i-1}. at is, the decoder generates a keyword query by predicting a word at a time. In predicting a word, the decoder uses its hidden state and the previously generated words.
us, the conditional probability p( i |y<i , x) is modeled as

p ( i |y<i , x) = (e ( ~t -1), zt ),

(1)

where is a non-linear function that outputs the probability of i , ~t-1 is the word generated at time t - 1, and e ( t ) is an embedding vector of t . In this equation, zt  Rdh is a hidden state of the decoder at time t which is updated by

zt = fdec (zt -1, e ( ~t -1)).

(2)

Here, fdec is again a non-linear activation function. e initial hidden state of the decoder, z0, is initialized based on the last hidden state of the encoder. at is, z0 = hn .
e semantics between input and output sequences is, in general, preserved well in the RNN encode-decoder [3]. However, in query translation, the semantics of long natural language queries is not kept completely in the output keyword queries. is is because a long natural language query contains many words which have nothing to do with or are weakly related with the theme of the query. If both relevant and weakly-relevant words are equally considered in translation, the encoded representation can be biased negatively by the weakly-relevant words. As a result, the decoder generates inaccurate keywords due to the biased representation. erefore, the translation should not consider only the whole semantics of natural language queries, but also focus more on some relevant words of the queries.
An a ention mechanism [2] is adopted to our encoder-decoder architecture as a solution to the problem. Since the decoder with the a ention mechanism can focus selectively on some speci c words

966

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

of x, it predicts a word at a time re ecting the speci c words. In order to re ect the mechanism into our encoder-decoder, Equation (1) should be rewri en as

p ( i |y<i , x) = (e ( ~t -1), zt , ct ),

where ct is a context vector that captures relevant source-side information to help predicting i . is context vector also in uences the update of hidden states in the decoder. us, Equation (2) is rewri en as
zt = fdec (zt -1, e ( ~t -1), ct ).
To obtain ct at time t, the relevance score of each hidden representation of the encoder should be rst calculated which is de ned as
t,i = fscor e (hi , zt -1, e ( ~t -1))
for all i = 1, ..., n. fscor e is a neural network that measures how relevant the i-th hidden representation in the natural language query is in deciding the t-th keyword query. e local a ention of Luong et al. [14] among various approaches is used to implement this neural network, since it focuses only on a small subset of the words. A er normalizing each relevance score by

t,i =

exp(t,i )

n k =1

exp(t,k

)

,

the context vector ct is computed as a weighted sum of the hidden representations with their normalized scores. at is,

n
ct = t,iht .
i =1

e a ention-based RNN encoder-decoder is trained by maximiz-

ing the conditional log-likelihood over a dataset D =

(x(j), y(j) )

N j =1

de ned as

L( ) =

1 N

Nm
log p
j=1 i=1

i=

(j i

)

|y(<ji)

,

x(j

)

,

where

(j) i

is

the

i-th

ground-truth

keyword

of

the

j-th

training

instance. e parameters  are optimized by the backpropagation

with stochastic gradient descent.

4 EXPERIMENTS
4.1 Experimental Settings
e search result by each query type is used as a criterion to evaluate the e ectiveness of the proposed query translation. A er a user submits natural language queries, the relevance of all results returned by search engines is judged manually. Google and Naver1 are adopted as search engines that return the search results from queries.
Since there is no dataset publicly available to train the encoderdecoder for our task, we prepare the dataset D rst. As the rst step for preparing D, keyword queries are sampled from the Naver query log, and then the top-1 title in a CQA site2 per a keyword query is regarded as a natural language query of the keyword query. Table 1 shows the simple statistics of this dataset. Note that the
1h p://m.naver.com 2h p://kin.naver.com

Table 1: Simple statistics on dataset for training RNN encoder-decoder

Information
# of natural language and keyword query pairs Average # of tokens in a natural language query
Average # of tokens in a keyword query # of unique vocabularies in natural language queries
# of unique vocabularies in keyword queries

Value
828,826 7.94 2.57
151,657 108,838

Table 2: Simple statistics on evaluation dataset

Information
# of users Average # of queries per a user Average # of tokens in a query

Value
8 33.38 7.89

Table 3: Statistics on translated queries

Average # of tokens Ratio of queries with nonexistent tokens

Nouns 3.40 0%

RNN 2.56 7.5%

Proposed Method 2.45 9.0%

average number of tokens in natural language queries is 7.94 which is around three times larger than that of keyword queries.
Another set of user queries is needed to evaluate the search results by the proposed method. We collect these queries from eight users who are independent from our work. e only restriction to preparing the set is that the queries should be wri en in a natural language. As a result, the set includes various questions, asks, and recommendations. Table 2 summarizes this evaluation set.
e proposed method is compared with three baselines. One is to use original natural language queries without any transformation into keywords (NL), and another is to use only nouns within a natural language query (Nouns). e third is to use the RNN encoder-decoder without an a ention mechanism (RNN). Long Short Term Memory (LSTM) [9] is used as all non-linear activation
--- --- functions (fenc , fenc , and fdec ).
4.2 Experimental Results
Before comparing the proposed method with baselines, we show the statistics on translated queries by each method in Table 3. e average number of tokens in a natural language query is around eight, but all translation methods reduce this number to around three. One thing to note here is that the methods based on the RNN encoder-decoder generate the tokens which do not exist in the natural language queries. For instance, a natural language query "computer is really slow" is translated to `computer lag' by the RNN encoder-decoder, where the token `lag' is nonexistent in the natural language query. is implies that such methods are able to generate new tokens that represent the theme of a natural language query.

967

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 2: NDCG@5 scores over two search engines

Table 4: Performances of pair comparisons

NL - Nouns NL - RNN NL - Proposed Method

Google 0.66 ± 0.38 0.49 ± 0.59

1.08 ± 0.57

Naver 0.41 ± 0.37 0.52 ± 0.46

1.04 ± 0.46

First of all, we evaluate the baselines and the proposed method by measuring their retrieval results. For this evaluation, all methods (Nouns, RNN, and the proposed method) transform the natural language queries made by eight users into keyword queries. en, the search results by Google and Naver for the four query types including NL query are anonymized, and then evaluated by the users.
e normalized discount cumulative gain at ranks 5 (NDCG@5) is used as an evaluation metric. e scale of NDCG is set from 0 to 3 by the relevance to query intent. Figure 2 shows average NDCG@5 scores over the two search engines. e performances of query translation methods are be er than natural language query itself in both search engines, which indicates that the translated keyword queries are useful for legacy search engines because the engines are optimized for keyword queries. e proposed method achieves 0.80 score in Google and 0.72 score in Naver, which are up to 0.13 higher than those of the noun-only query. In addition, its performance is also higher than that of the RNN encoder-decoder without a ention. is implies that keyword queries by the proposed method focus more on the theme of natural language queries through the a ention mechanism.
We evaluate also the performance of the proposed method by comparing the search result of NL and results of keyword queries translated by each translation method. As an evaluation metric, 7-point Likert scale from -3 to 3 is used, where negative values indicate the result by a translated query is worse than that of NL query and positive values imply be er result over NL query. e comparison result is given in Table 4. All the values in this table are positive, which implies that the search results of transformed keyword queries are more satisfactory to users than that of natural language queries. Especially, the proposed method achieves the best performance. us, the keywords translated by the proposed method are the best among the models compared. ese two experiments prove that the proposed method is e ective for translating a natural language query into a keyword query.

5 CONCLUSION
is paper has proposed a method to translate a natural language query into a keyword query. e proposed method regards the translation as a generation and uses a RNN encoder-decoder architecture as a generation model. e proposed method also adopts an a ention mechanism to the RNN encoder-decoder architecture in order to concentrate on some words related with the theme of the natural language query. As a result, the proposed method does not re ect only the whole semantics of the natural language query, but also focuses more on the words relevant to the theme. e experiments have shown that the proposed model returns the search results with high NDCG score in legacy search engines.
ACKNOWLEDGEMENT
is work was partly supported by IITP grant funded by the Korea government (2013-0-00109, WiseKB: Big data based self-evolving knowledge base and reasoning platform).
REFERENCES
[1] I. Antonellis, H. G. Molina, and C. C. Chang. 2008. Simrank++: ery Rewriting rough Link Analysis of the Click Graph. Proceedings of VLDB 1, 1 (2008),
408­421. [2] D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of ICLR. [3] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
and Y. Bengio. 2014. Learning Phrase Representations using RNN Encoder­ Decoder for Statistical Machine Translation. In Proceedings of EMNLP. 1724­ 1734. [4] S. Chopra, M. Auli, and A. M. Rush. 2016. Abstractive Sentence Summarization with A entive Recurrent Neural Networks. In Proceedings of NAACL. 93­98. [5] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A Kalyanpur, A. Lally, J W. Murdock, E. Nyberg, and J. Prager. 2010. Building Watson: An overview of the DeepQA project. AI magazine 31, 3 (2010), 59­79. [6] J. Gao and J.-Y. Nie. 2012. Towards Concept-Based Translation Models Using Search Logs for ery Expansion. In Proceedings of CIKM. 1:1­1:10. [7] A. Graves and J. Schmidhuber. 2005. Framewise phoneme classi cation with bidirectional LSTM and other neural network architectures. Neural Networks 18, 5 (2005), 602­610. [8] Y. He, J. Tang, H. Ouyang, C. Kang, D. Yin, and Y. Chang. 2016. Learning to Rewrite eries. In Proceedings of CIKM. 1443­1452. [9] S. Hochreiter and J. Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735­1780. [10] S. Huston and W. B. Cro . 2010. Evaluating Verbose ery Processing Techniques. In Proceedings of SIGIR. 291­298. [11] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proceedings of WWW. 387­396. [12] G. Kumaran and V. R. Carvalho. 2009. Reducing Long eries Using ery
ality Predictors. In Proceedings of SIGIR. 564­571. [13] B. Li and I. King. 2010. Routing estions to Appropriate Answerers in Commu-
nity estion Answering Services. In Proceedings of CIKM. 1585­1588. [14] T. Luong, H. Pham, and C. D. Manning. 2015. E ective Approaches to A ention-
based Neural Machine Translation. In Proceedings of EMNLP. 1412­1421. [15] K. T. Maxwell and W. B. Cro . 2013. Compact ery Term Selection Using
Topically Related Text. In Proceedings of SIGIR. 583­592. [16] A. Otegi, X. Arregi, O. Ansa, and E. Agirre. 2015. Using knowledge-based
relatedness for information retrieval. Knowledge and Information Systems 44, 3 (2015), 689­718. [17] J. H. Park and W. B. Cro . 2010. ery Term Ranking Based on Dependency Parsing of Verbose eries. In Proceedings of SIGIR. 829­830. [18] S. Riezler and Y. Liu. 2010. ery Rewriting Using Monolingual Statistical Machine Translation. Computational Linguistics 36, 3 (2010), 569­582. [19] I. Sutskever, O. Vinyals, and Q. V Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of NIPS. 3104­3112. [20] X. Wang and C. Zhai. 2008. Mining Term Association Pa erns from Search Logs for E ective ery Reformulation. In Proceedings of CIKM. 479­488. [21] G. Weikum and M. eobald. 2010. From Information to Knowledge: Harvesting Entities and Relationships from Web Sources. In Proceedings of SIGMOD/PODS. 65­76. [22] J. Xu and W. B. Cro . 1996. ery Expansion Using Local and Global Document Analysis. In Proceedings of SIGIR. 4­11.

968


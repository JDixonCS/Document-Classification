Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

But Is It Statistically Significant?
Statistical Significance in IR Research, 1995­2014

Ben Cartere e
University of Delaware carteret@udel.edu

ABSTRACT
We analyze 5,792 IR conference papers published over 20 years to investigate how researchers have used and are using statistical signi cance testing in their experiments.
CCS CONCEPTS
·Information systems Evaluation of retrieval results; Presentation of retrieval results;
1 INTRODUCTION
Experimentation has long been an important aspect of research in information retrieval, going all the way back to the 1963 tests of indexing devices by Cleverdon and Mills [3]. It became clear early on that two di erent retrieval systems could di er in performance by only a few points of precision (or average precision, or, later, nDCG), and that in order to say whether a di erence was likely to be "real" or not, we would have to turn to statistical hypothesis testing [11, 12]. Early work published on this problem largely took a theoretical perspective on which statistical tests were most appropriate to apply in IR and how to interpret their results [4, 9, 14]; more recent work has investigated from an empirical perspective in the context of community-wide competitions like those run at TREC, CLEF, and NTCIR [1, 10, 15].
In this paper we look back over 20 years of research to investigate the adoption of statistical signi cance testing by the IR community. We look at which tests are most widely used, how reporting of test results has changed over time, how the use of tests di ers by conference and by paper type, why papers sometimes do not report test results, and distributions of p-values in papers. We also present a novel method for simulating the distribution of p-values we should expect; that our method works well suggests there is still much important work to be done in search.
2 SIGNIFICANCE TESTING IN IR
e most common scenario for signi cance testing in IR is comparing the e ectiveness of two automatic retrieval systems over a sample of topics or queries. ree di erent tests have been suggested for
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080738

venue SIGIR ECIR CIKM
total

years 1995­2014 2005­2014 2005­2014
1995­2014

papers 2,413 759 2,620
5,792

stats 1,159 short, 1,254 long 346 short, 413 long 1,015 short, 1,605 long; 780 IR, 755 KM, 383 DB, 702 not labeled 2,520 short, 3,272 long 232,720 unique terms

Table 1: IR research paper corpora used in this work. Each

paper is labeled "short" or "long". Some CIKM papers are

additionally labeled with the track they were submitted to

(IR, KM, or DB).

this: the sign test (also called the binomial test), Wilcoxon's signedrank test, and Student's t-test [10]. More recently, researchers have recommended distribution-free tests like Fisher's exact test (which, when implemented using random sampling, is sometimes called the randomization or permutation test) [10] and tests based on bootstrap sampling [6, 10]. ANOVA (analysis of variance) has been used for this purpose [15] as well, and it is also commonly used in user studies.
Other tests that have been used in IR include the  2 test, McNemar's test, and various proportion tests, all of which can be applied to 2 × 2 contingency tables and are thus useful for classi cation tasks; distribution tests such as Kolmogorov-Smirnov and Anderson's test of normality; correlation tests; and multiple comparisons adjustments like the Bonferroni correction or Tukey's HSD. ese are all rare compared to the six mentioned above, however.
3 SIGNIFICANCE REPORTING IN IR
Similar to Sakai in his investigation of power in the IR literature [8], we obtained all research papers (full papers, short papers, and poster papers) for all SIGIR conferences from 1995­2014, all ECIR conferences from 2005­2014, and all CIKM conferences from 20052014. is is a total of 5,792 papers; some additional information about them is presented in Table 1.
We indexed the papers using indri1 with the Krovetz stemmer and no stopword list2. We queried the index using terms and phrases related to six tests commonly used in IR as well as more general phrases referring to statistical signi cance; the queries and number of papers matching them are shown in Table 2. Fisher's exact test seems to have no standard name, which made it more di cult to search for. In addition, the bootstrap procedure is sometimes used for purposes other than testing, which means some of
1h p://www.lemurproject.org/indri/ 2We also veri ed that indri's PDF parser was able to successfully parse papers. e papers it had trouble parsing constitute less than 1% of all papers.

1125

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

test

indri query

matches unique

Student's t-test

#ow1(t test)

843

732

Wilcoxon signed-rank #syn(wilcoxon wilcoxons)

bootstrap

#combine(bootstrap #ow2(bootstrap test))

372

301

144

102

ANOVA sign test

anova #ow1(sign test)

132

78

108

61

Fisher's exact test

#combine(#ow1(exact test) #ow1(randomization test) #ow1(permutation test) #ow2( sher test))

58

36

#combine(#ow1(statistical signi cance) #ow1(statistically signi cant))

1323

Table 2: Indri queries to nd papers that used common statistical signi cance tests in an index of 5,791 papers published at

SIGIR, ECIR, and CIKM. e bottom row gives the number of papers that matched a general query for statistical signi cance.

Note that some papers matched one of the individual test queries but not this general query, so it is not equal to the sum of

the number of matches. e last column gives the number of papers that matched that query and none of the others (apart

from the last one).

SIGIR ECIR CIKM

SIGIR long papers ECIR long papers CIKM long papers

percent reporting significance 0.0 0.2 0.4 0.6 0.8 1.0

percent reporting significance 0.0 0.2 0.4 0.6 0.8 1.0

1995

2000

2005 year

2010

Figure 1: Proportion of all papers published at SIGIR, ECIR, and CIKM reporting signi cance.

1995

2000

SIGIR short papers ECIR short papers CIKM short papers

2005 year

2010

percent reporting significance 0.0 0.2 0.4 0.6 0.8 1.0

the papers matching that query do not actually use the bootstrap test. e nal line of the table shows the number of papers matching two phrases that o en indicate signi cance. While one of these phrases appears in most papers that mention a test, they do not appear in all of them.
Based on Table 2, it seems that most researchers in IR are using the t-test, with Wilcoxon's signed-rank test a distant second. But the biggest takeaway from this table is that, unless we have missed some major indicator of signi cance3, it seems that fewer than 25% of published papers have reported signi cance testing.
However, Figure 1 shows that the proportion of papers reporting signi cance has trended upward over time, driven primarily by a steady increase from 2001­2010 leading to around 50% of papers reporting signi cance at recent conferences. SIGIR and ECIR are roughly similar, but CIKM is far below the two of them. is is most likely due to di erent standards in CIKM's three tracks; we explore this below.
Figure 2 shows the proportion of papers reporting signi cance broken out by paper length (short/poster versus long/full). As expected, short papers tend to report signi cance less o en than full papers. For SIGIR, about twice as many long papers report signi cance as short papers. For ECIR and CIKM, about 50% more long papers report signi cance than short papers.
3Other phrases, such as "signi cant di erence", are common but o en used without any accompanying statement of test used. Authors use the word "signi cant" to mean "large" or "substantial" rather than "statistically signi cant".

1995

2000

2005 year

2010

Figure 2: Proportion of full (top) and short (bottom) papers reporting signi cance at SIGIR, ECIR, and CIKM. Note that SIGIR did not start explicitly calling for short papers until 1999.

CIKM has traditionally had papers submi ed to one of three research tracks: IR, databases (DB), or knowledge discovery/management (KM). us our CIKM corpus provides an opportunity to compare IR to these elds. For all but three years (2005, 2006, and 2011) we were able to label nearly every paper by track; the proportion of papers from each track reporting signi cance is shown in Figure 3. We note that IR has many more papers reporting signi cance than either DB or KM. is is likely not a perfectly fair comparison between elds, as those elds may use di erent tests than the ones we use in IR (also, CIKM is rated higher as an IR conference than either a DB or KM conference4), but since it includes vague phrases
4Based on rankings at h p://webdocs.cs.ualberta.ca/zaiane/htmldocs/ConfRanking. html

1126

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

percent reporting significance 0.0 0.2 0.4 0.6 0.8 1.0
density

CIKM IR track CIKM KM track CIKM DB track

2006

2008

2010 year

2012

2014

Figure 3: Proportion of CIKM papers reporting signi cance by track. Breaks in the lines re ect years for which track information was not readily available (CIKM 2005, 2006, and 2011).

like "statistically signi cant" that appear in most papers reporting test results, it is suggestive.
3.1 Why not test signi cance?
We investigated a sample of papers that did not match any of our queries in Table 2 to determine whether signi cance could have been tested, and if it couldn't have, why. We sampled around 30 papers uniformly at random from ve 4-year spans (1995­1998, 1999­2002, 2003­2006, 2007­2010, and 2011­2014). Based on this sample, we found that most papers that did not report signi cance actually could have (80% of our 150 papers); that is, experiments were performed and systems compared, but no signi cance testing was reported.
e next most common reasons for not reporting signi cance were that the paper was entirely theoretical or that the paper was more of a proposal or proof of concept (this was most common in short papers). However these two only made up 14% of papers in our sample, and were more common in the earlier timespans--it has actually become more common for papers not reporting test results to be experimental even as experimental papers are more likely to include test results. e remaining 5% were position papers, qualitative studies, or di cult to assess.
Over time, the proportion of papers sampled that could have tested signi cance rose (from 73% to 95%), while the proportion of theoretical papers and positions papers fell. As conferences have required more and more empirical papers, there is less reason to not test signi cance. However, these di erences are not statistically signi cant themselves.
3.2 p-values
Another aspect of signi cance testing is the p-value produced by the test. Most papers do not report exact p-values, instead reporting the threshold at which signi cance was tested (e.g. p < 0.05) and whether a result was signi cant or not. But among papers we found that did report exact p-values, we transcribed them into a spreadsheet for analysis.
We found 30 papers that reported a total of 466 exact p-values. e top histogram in Figure 4 shows the distribution of these 466 p-values. Note that this includes p-values from all tests and experimental se ings; most are the results of tests we listed above, but

100 50 0
100 50 0
100 50 0 0.000

actual p-values in IR papers

simulated p-values

TREC 2012 Web track p-values

0.025

0.050 p-value

0.075

0.100

Figure 4: Distributions of p-values reported in published IR papers (top); generated by simulation (middle); and from paired t-tests between all TREC 2012 Web track runs (bottom).

some are from other tests and some are from ed linear models and other statistical procedures. e distribution is skewed towards zero, with a li le over 50% of p-values less than 0.05.
It is worth asking what the distribution "should" look like, i.e. what the expected distribution of p-values is in publications in a
eld in which there are still discoveries to be made. For this, we will use the idea of e ect size [2, 7, 13]. E ect size refers to the degree of di erence between two treatments (retrieval algorithms) in a population, independent of the number of samples in a particular experiment. It is used to estimate the probability that an e ect will be found signi cant given a certain sample size.
Cohen's d is a measure of e ect size speci c to the t-test. In the behavioral sciences, d < 0.1 is considered a small e ect, scienti cally not very interesting; 0.1  d < 0.3 a medium e ect; and 0.3  d < 0.5 a large e ect. Small e ects occur when the di erence in means is small compared to the variance; they can be signi cant with a very large sample (n in the thousands, for instance) even if they are not scienti cally interesting. Large e ects occur when the di erence in means is large compared to the variance; large e ects will usually be signi cant even in small samples (n  10). Medium e ects typically require on the order of a few dozen samples to be found signi cant; test collections in IR consisting of 50­100 topics are ideal for medium e ects.
Cohen's d is computed as the mean of the di erence in e ectiveness between two systems divided by the standard deviation in e ectiveness di erences. For example, if evaluating by average precision (AP), we would compute d as:

d

=

MAP1 - MAP2 std.de .(AP1 - AP2)

It is closely related to the t-statistic used in Student's t-test:

t=

MAP1 - MAP2 

std .de 

.(AP1 - AP2)/

n

=d n

where n is the sample size.

1127

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Let us assume the following:
(1) published papers on average present results of 10 t-tests; (2) a paper must have at least one p-value < 0.05 in order to
be published; (3) the "true" e ect sizes being studied are medium-large, i.e.
Cohen's d  0.3.
Using these assumptions, we can simulate the p-values that should appear in published papers: wesimply sample 10 t statistics from a t distribution centered at 0.3 n (for some value of n; we used n = 50 since that is the size of a typical TREC test collection), then look up those values in a reference t distribution centered at zero to get 10 p-values. If at least one of these is less than 0.05 we keep all 10, otherwise we discard the sample.
e middle histogram in Figure 4 shows the result--p-values are skewed towards zero about as much as they are in actual published papers, though we nd more p-values just above 0 and fewer between 0.025 and 0.05. Di erences between the two are most likely due to the fact that the histogram of actual values con ates many di erent tests and experimental se ings and designs, while the simulated values assumes a t-test applied to a standard within-subjects design with 50 topics.
e similarity between the two distributions is suggestive that there are many medium-large e ects being reported in IR, and hence that there may be many more to be found. is can only be good for the eld: it means there are potentially many more important results waiting to be discovered.
As another validation, we compare to the p-value distribution in TREC 2012 Web track results; this is shown in the bo om histogram in Figure 4. is distribution is similar to the other two as well. We nd nothing in our investigation that would cause us to reject the hypothesis that published p-values are di erent than expected under some reasonable assumptions5.
3.3 Negative results
Many elds of science have been criticized for not publishing "negative" results, giving a biased record of what is known and in particular leading to the so-called "publication bias" problem in which small e ects can be published as signi cant just because one group found a signi cant e ect by chance, while ve others failed to: only one group gets a publication, while the overall weight of evidence, if available, would suggest that the e ect is not real.
While it is somewhat unclear what is meant by a negative result, we posit that most IR papers, while wri en as a description of a positive result, contain negative results as well. is is because most papers report on comparisons between methods that had negative e ect or were not statistically signi cant. e p-values we analyzed above provide one piece of evidence: if a li le over 50% were less than 0.05, then just under 50% were greater than 0.05, and therefore wouldn't qualify as signi cant by usual standards.
We selected some papers at random to expand this analysis to those that do not report exact p-values, but just an indicator of signi cance (an asterisk, for example). In these papers we found
5We note that the distributions are statistically signi cantly di erent by the Kolmogorov-Smirnov distribution test. But that can easily be explained by the mismatch between our assumptions and the actuality of taking p-values from experiments with many di erent sample sizes, types of tests, experimental conditions, etc.

that about 60% of comparisons were signi cant while 40% were not. Based on our scanning of papers for this analysis, we would argue that the larger problem is that most papers do not have su cient analysis of negative results that can lead to generalizable conclusions.
4 CONCLUSIONS AND FUTURE WORK
We have analyzed 5,792 papers published in IR conferences over 20 years and shown that reporting of statistical signi cance has improved over time, but is still lacking. Nevertheless, based on comparisons across CIKM research tracks, it seems to be substantially be er in IR than in other elds. In addition, we have argued that p-values in IR papers are distributed more or less as expected, suggesting that researchers are probably "honest" in their computation of tests. is is interesting, as a similar analysis of research in the biomedical sciences suggested that papers were reporting more p-values just below 0.05 than should be expected by any reasonable model [5]. Overall, it seems that reporting of results in IR is rather good, even if more analysis could be done.
We intend to expand our investigation to include IR journals; it may be the case that conference publications lagged behind journal publications in reporting signi cance in the 90's. In addition, we would like to compare to other "allied" elds such as NLP and machine learning in addition to databases and knowledge discovery/data mining.
Acknowledgements is work was supported in part by the National Science Foundation (NSF) under grant number IIS-1350799. Any opinions, ndings and conclusions or recommendations expressed in this material are the authors' and do not necessarily re ect those of the sponsor.
REFERENCES
[1] Leonid Boytsov, Anna Belova, and Peter Westfall. 2013. Deciding on an Adjustment for Multiplicity in IR Experiments. In Proc. SIGIR.
[2] Ben Cartere e and Mark D. Smucker. 2007. Hypothesis Testing with Incomplete Relevance Judgments. In Proceedings of CIKM. 643­652.
[3] Cyril W. Cleverdon and J. Mills. 1963. e Testing of Index Language Devices. Morgan Kaufmann Publishers, 98­110.
[4] David A. Hull. 1993. Using Statistical Testing in the Evaluation of Retrieval Experiments. In Proceedings of SIGIR. 329­338.
[5] E. J. Masicampo and Daniel R. Lalande. 2012. A peculiar prevalence of p-values just below .05. e arterly Journal of Experimental Psychology 65, 11 (2012).
[6] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics Based on the Bootstrap. In Proceedings of SIGIR. 525­532.
[7] Tetsuya Sakai. 2014. Designing Test Collections for Comparing Many Systems. In Proc. CIKM.
[8] Tetsuya Sakai. 2016. Statistical Signi cance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006­2015. In Proc. SIGIR.
[9] Jacques Savoy. 1997. Statistical Inference in Retrieval E ectiveness Evaluation. Information Processing and Management 33, 4 (1997), 495­512.
[10] Mark Smucker, James Allan, and Ben Cartere e. 2007. A Comparison of Statistical Signi cance Tests for Information Retrieval Evaluation. In Proceedings of CIKM. 623­632.
[11] Jean Tague. 1981. e Pragmatics of Information Retrieval Evaluation. Bu ersworth, 59­102.
[12] C. J. van Rijsbergen. 1979. Information Retrieval. Bu erworths, London, UK. [13] William Webber, Alistair Mo at, and Justin Zobel. 2008. Statistical Power in
Retrieval Experimentation. In Proceedings of the nth ACM International Conference on Information and Knowledge Management. [14] W. J. Wilbur. 1994. Non-parametric signi cance tests of retrieval performance comparisons. Journal of Information Sciences 20, 4 (1994), 270­284. [15] Justin Zobel. 1998. How Reliable are the Results of Large-Scale Information Retrieval Experiments?. In Proceedings of SIGIR. 307­314.

1128


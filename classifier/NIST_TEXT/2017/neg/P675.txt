Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Privacy through Solidarity: A User-Utility-Preserving Framework to Counter Profiling

Asia J. Biega
Max Planck Institute for Informatics Saarland Informatics Campus jbiega@mpi-inf.mpg.de

Rishiraj Saha Roy
Max Planck Institute for Informatics Saarland Informatics Campus rishiraj@mpi-inf.mpg.de

Gerhard Weikum
Max Planck Institute for Informatics Saarland Informatics Campus weikum@mpi-inf.mpg.de

ABSTRACT
Online service providers gather vast amounts of data to build user pro les. Such pro les improve service quality through personalization, but may also intrude on user privacy and incur discrimination risks. In this work, we propose a framework which leverages solidarity in a large community to scramble user interaction histories. While this is bene cial for anti-pro ling, the potential downside is that individual user utility, in terms of the quality of search results or recommendations, may severely degrade. To reconcile privacy and user utility and control their trade-o , we develop quantitative models for these dimensions and e ective strategies for assigning user interactions to Mediator Accounts. We demonstrate the viability of our framework by experiments in two di erent application areas (search and recommender systems), using two large datasets.
KEYWORDS
Anti-Pro ling, Privacy, User Utility, Personalization, Mediator Accounts, Pro le Scrambling, Search Engines, Recommender Systems
1 INTRODUCTION
Motivation: Users are pro led and targeted in virtually every aspect of their digital lives: when searching, browsing, shopping, or posting on social media. e gathered information is used by service providers to personalize search results, customize ads, provide di erential pricing, and more [19, 42]. Since such practices can greatly intrude on an individual's privacy, the goal of our research is to devise a mechanism to counter such extensive pro ling.
A careful user can largely preserve her privacy by taking measures like anonymizing communication or using online services only in a non-linkable manner (for instance, by changing accounts or pseudonyms on a regular basis). However, this comes at the cost of greatly reducing utility, both for the service providers and the user. On the one hand, the service provider will miss out on learning from the same user's long-term behavior, which may result in less e ective systems. is issue of system-level utility has been studied in the past research on privacy [20, 23]. On the other hand, the individual user will experience degraded service quality, such
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080830

as poor search results, as the service provider would not understand the user's interests and intentions. is notion of user-level utility has not been extensively explored in prior work. Our paper formalizes the trade-o between a user's pro ling privacy and her individual utility.
State of the art and its limitations: Research in privacy has primarily addressed the disclosure of critical properties in data publishing [5, 7, 13]. Common techniques include coarsening the data so that di erent users become indistinguishable (e.g., kanonymity [41], l-diversity [26], and t-closeness [25]), or perturbing the answers of an algorithm so that the absence or presence of any record does not signi cantly in uence the output ­ the principle of di erential privacy [11]. ese methods consider notions of utility that re ect a system-level error in an analytical task, such as classi cation. In contrast, our goal is to prevent detailed pro ling and targeting while keeping the individual user utility as high as possible, for example, in terms of the quality of personalized search results or product recommendations.
For privacy-preserving search, many approaches have been proposed based on query obfuscation [14, 33]. In these solutions, queries are generalized to hide their actual intentions, or additional dummy queries are generated to prevent accurate pro ling. Both techniques come at the cost of largely reducing user utility. Similar obfuscationbased techniques have been explored for recommenders [17, 27]. However, none of the prior work addressed the trade-o between privacy and user utility in a quantitative manner. A few methods [8, 33] have considered an entire user community as a means for query obfuscation. is idea is related to our approach in this paper ­ we generalize it and make it applicable in the context of anti-pro ling.
Approach and contribution: Our approach to reconcile privacy and user utility builds on the following observation: service providers o en do not need a complete and accurate user pro le to return personalized results. us, in accordance with the needto-know principle, we assign user requests to Mediator Accounts (MA) mimicking real users, such that (i) individual user pro les are scrambled across MAs to counter pro ling, while (ii) coherent fragments of a user's pro le are kept intact in the MAs to keep user utility high. We call this paradigm privacy through solidarity. Speci cally, MAs are constructed by split-merge assignment strategies: spli ing the interaction history of a user and merging pieces of di erent users together. Mediator Accounts are meant as an intermediate layer between users and the service provider, so that the provider only sees MAs instead of the real users.
Ideas along these lines have been around in the prior literature [15, 34­36, 44], but the formalization of the privacy - userutility trade-o has never been worked out. In particular, to make

675

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: Overview of the MA framework.
this idea viable, one needs to devise quantitative measures for the e ects of Mediator Accounts on privacy and utility. In addition, a strategy is needed for assigning user requests to such accounts.
e simplest approach of uniform randomization would be ideal for privacy but could prove disastrous for user utility. is paper addresses these challenges within a framework of Mediator Accounts. Our ideas are general enough to be applied to search engines, recommender systems, and other online services where personalization is based on the user interaction history. Our salient contributions are:
· a model with measures for quantifying the trade-o between pro ling privacy and user utility;
· the Mediator Accounts framework together with strategies for assigning user interactions to MAs;
· comprehensive experiments with two large datasets: search logs derived from the StackExchange Q&A community, and Amazon product ratings.
2 FRAMEWORK OVERVIEW
2.1 Architecture
e architecture of the Mediator Accounts framework is shown in Fig. 1. It consists of three parties: users, a service provider (SP), and a Mediator Accounts proxy (MA-proxy). A user pro le consists of a set of objects, such as queries, product ratings or other forms of user interactions with the SP. Instead of issuing objects directly to the SP, users pass them on to the MA-proxy together with some context information. e goal of the MA-proxy is to redistribute the incoming objects on to mediator pro les mimicking real users.
e MA-proxy assigns each incoming object to a Mediator Account o ering the right context for the current object and user, and issues the object to the SP from the chosen MA. Upon receiving a response (for example, a result page or a product recommendation) from the SP, the MA-proxy passes it back to the user. When an interaction is over, the MA-proxy discards all linking information about the original user and the object and remembers only the association between the mediator account and the object. As a result, the original user pro les are scrambled across multiple MAs, and each MA consists of data from multiple users.

2.2 Incentives of participating parties
Users. e goal of a user participating in an MA system is to be able to get high-quality personalized results, while not le ing any online provider (neither SPs nor the MA-proxy) keep her interaction history and link it to her as an individual. e MA-proxy has the user interaction history scrambled across multiple accounts, and no links between the objects and the real users are stored.
Users of anonymous services that do not o er topical personalization, such as the DuckDuckGo, Startpage or Qwant search engines, may be open to trading o some privacy for enhanced results through the MA framework. Non-pro ling service providers. e incentive of a non-pro ling service provider would be to enhance personalization in the results, without compromising on the non-pro ling principle. Pro ling service providers. A big question is whether pro ling service providers would allow a third-party like an MA-proxy to mediate between them and the users. While examples of such thirdparties already exist (the Startpage search engine uses Google as a source of search results), we believe that (i) an MA-proxy being able to group objects into realistic pro les that yield similar analytics results for the SP, and (ii) an MA-proxy being able to a ract privacywary users who would not otherwise use the pro ling SP, would be viable incentives for an SP not to block an MA service. MA-proxy. An MA-proxy could be set up by individuals, or cooperatives of non-pro ling SPs (to provide personalization without accumulating real user pro les), or by non-governmental organizations that promote online privacy. e Electronic Frontier Foundation is such an organization ­ a non-pro t organization that has built privacy-preserving solutions like Privacy Badger.
2.3 Trusted and adversarial parties
MA-proxy. Users opting for an MA service would need to trust that it scrambles their pro les across mediator accounts, and discards the original pro les as well as any identifying information once an interaction (a single request or a session) is complete. A standard approach to gain such trust would be to make the MA solution open-source, enabling the code to be ve ed by the community. A real implementation of an MA framework would have to take into account secure end-to-end communication channels between users and SPs via the MA-proxy. ese issues may be resolved using encryption and security techniques (e.g., secure browser, onion routing, etc.), and are outside the scope of this paper. Provider. e service provider is not exactly distrusted, but there have been cases where user-related information has been leaked or passed on beyond the original intentions ­ by sabotage, acquisition by other companies, or enforcement by government agencies. By detaching users from pro les and limiting their accuracy, the potential damage is bounded.
Other risks might result from service providers displaying privacysensitive personalized ads, such as ads related to pregnancy or health issues, especially when observed by others on a user's screen.
e architecture would allow an MA-proxy to support ltering ads and adjusting them to users' topical interest. Such a con guration has indeed been found to be a preferable ad-serving setup in a user study [1]. Ad ltering, however, is orthogonal to this research.

676

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

ird parties. Pro ling companies that operate outside the userprovider connections are considered untrusted. e same holds for agglomerates of providers that aggregate and exchange user data. A conceivable a ack could be to guess a user's a ribute (e.g., whether she is pregnant) by combining (i) observations on the MAs and (ii) observations on a set of accounts in a social network, using statistical inference methods. e MA framework aims to keep such risks low by breaking observable associations between MAs and real users, and limiting the pro ling accuracy of the split-merge superpositions of di erent users that cannot be easily disentangled.
3 ASSIGNMENT MODEL
e core of the MA framework is an algorithm for assigning user objects to Mediator Accounts. To guide it on the privacy-utility trade-o s and to assess the quality of the output, we need measures for quantifying the e ect of an assignment on privacy and user utility. is section presents such measures, and the algorithm for object assignment based on the split-merge concept.
3.1 Concepts and notation
We use the following notations:
· A set U of users u1 . . . up .
· A set O of objects o1 . . . os issued by users; the objects are treated as unique even if they represent the same content. For instance, a query folk music issued by user ui is treated as an object distinct from the same query issued by uj . Analogously, a product rating for a book (Folk Music History, 3.0) by ui is distinct from the rating by uj for the same book, irrespective of the rating value.
· A set M of mediator accounts m1 . . . mt to which objects are assigned by the MA-proxy.
We reserve the symbols i, j,k for subscripts of users, objects, and MAs. If user ui issues object oj , we write oj  ui . Similarly, if oj is assigned to MA mk , we write oj  mk . Assignments. An assignment of objects on to MAs can be denoted as an s × t matrix A of 0-1 values, where Aij = 1 means that oi is assigned to mj . If we think of the Cartesian product O × M as a bipartite graph, then the assignment can be conceptualized as a subgraph S  O × M where each node of type O has exactly one edge with one of the M nodes.
3.2 Objective
In a real application, an MA-proxy has to assign objects to accounts in an online manner, one object at a time as input arrives. In this paper, we focus on analyzing the model and assignments in an o ine se ing, although the algorithm we devise can be applied in both o ine and online scenarios. e o ine case is useful for two reasons. First, it is a foundation for understanding the underlying privacy-utility trade-o s. Second, performing o ine assignment on a set of initial user pro les can address the cold-start problem that a new MA-proxy would face. Using the notation from Sec. 3.1, the MA o ine assignment problem can be de ned as follows:
· Given a set of objects O belonging to a set of users U , and the set of mediator accounts M, compute an assignment matrix A that

optimizes a desired objective function for the privacy-utility trade-o .
e MA online assignment problem is:
· Given an assignment A of past user objects to MAs and a newly arriving object o of user u, nd the best MA to which o should be assigned with regard to a desired goal for the privacy-utility trade-o .

3.3 Measuring privacy gain

An ideal situation from the perspective of privacy is when the

objects from a user pro le are spread across MAs uniformly at

random ­ this minimizes the object-level similarity of any MA to

the original pro le. We thus measure privacy as the entropy of the

user distribution over MAs, formalizing these notions as follows.

Entropy. We introduce for each user ui an MA-per-user vector mui  (N0)t with one counter ( 0) per MA, wri en as mui =

xi1 . . . xij . . . xit where xij is the number of objects by user ui in

account mj (such that

t j =1

xi

j

=

|ui |).

We can cast this into an MA-

per-user probability distribution i = i1 . . . ij . . . it by se ing

ij = xij /|ui | followed by smoothing (e.g., Laplace smoothing) so

that ij > 0 for each j and

t j =1

i

j

=

1.

e degree of ui 's pro le fragmentation can be captured by the

entropy of the distribution i . We can de ne the MA-per-user

entropy as a measure of privacy gain (gain over having each user

exhibit her full individual pro le):

t

pri ac - ain(ui ) = Hi = - ij log ij

(1)

j =1

is quanti es the spread of the user's objects across accounts. e higher the entropy value, the higher the gain in pro ling privacy. Pro le overlap. If a use-case requires a more user-interpretable measure of privacy, an alternative is to minimize the maximum pro le overlap. For a user ui , this measure can be expressed as:

Oi

=

mtax
j =1

|{o



ui  mj }| |ui |

(2)

is measure of overlap can directly tell a user how much "error" could be made by an adversary, who assumes one of the MAs is the user's pro le. e optimum for this measure, as with entropy, is achieved when the objects are uniformly spread across accounts.
us, in the following, we use entropy as our privacy measure, and leave maximum pro le overlap as a design alternative.

3.4 Measuring user utility loss
User utility loss measures to what extent an object ok of user ui is placed out of context by mapping it to account mj . We de ne a real-valued function sim(·, ·) to measure the coherence of user and MA pro les: sim(oi ,oj )  [0, 1] is a symmetric measure of the relatedness between objects represented by oi and oj . In practice, di erent notions of relatedness can be used, based on object properties or usage. In se ings where labels for topics or categories are available, we can set sim(oi ,oj ) = 1 if oi and oj are issued by the same user and have the same topic/category label, and 0 otherwise. Generally, we assume that sim measures are normalized with values between 0 and 1.

677

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

e objects of user ui form a context, typically with high pairwise relatedness among the objects. When considering sets of objects as
a whole (rather than time-ordered sequences of object posts), we
can measure the normalized context coherence of an object ok in the pro le of user ui by:

coh(ok ,ui ) =

ol ui,k l sim(ok ,ol ) |ui | - 1

(3)

When ok is placed in MA mj , we analogously de ne:

coh(ok ,mj ) =

ol mj,k l sim(ok ,ol ) |mj | - 1

(4)

e utility loss of ui in a given MA assignment is then measured as an average coherence loss over all user objects:

utilit -loss (ui ) =

ok ui [coh(ok ,ui ) - coh(ok ,mj )] |ui |

(5)

where mj is the account containing ok in the given assignment. e normalization helps to account for varying sizes of user

pro les. As a result, coherence values are always between 0 and

1, and utility loss is normalized to take values between -1 and 1.

Note that our utility measure assumes that the context coherence

can increase if an object is assigned to an MA with more similar

objects. Coherence increase will result in negative utility loss.

3.5 Assignment algorithms
e role of an assignment algorithm is to scramble user objects across accounts so as to satisfy a desired privacy-utility tradeo or optimize a corresponding objective function. In this paper, we experiment with a number of assignment algorithms and study their output quality.

3.5.1 Optimal assignment (O line). e trade-o can be expressed as a joint non-linear optimization problem as follows:

max min [ · pri ac ain(u) - (1 -  ) · utilit loss (u)] (6)
Au
Alternatively, one could optimize one of the two measures with a constraint on the other. Solving this problem exactly, however, is computationally expensive. If we use the less complex overlap privacy measure, we could cast the problem into a adratic Integer Program. However, this would have millions (|M | · |O |) of variables; so it would remain intractable in practice. We thus do not pursue this direction in this paper and instead consider a number of heuristics. e following are also suitable for the online case.

3.5.2 Profiling-tradeo assignment. We aim to approximate the

combined objective function as follows. Let o be an object we want

to assign to one of the accounts mj . If we want to optimize for privacy (i.e., entropy), we should choose an MA at random from a

uniform distribution over MAs:

Ppr i

(mj |o)

=

1 |M |

(7)

If we want to optimize for utility, we could choose an MA that o ers the best coherence:

Pu t i l

(mj

|o)

=

1,
 0,

if mj = mmax otherwise

(8)



where mmax = arg maxmk coh(o,mk ).

Let  be a parameter that controls the trade-o between privacy and utility. We sample an MA according to the distribution:

P (mj |o) =  · Ppr i (mj |o) + (1 -  ) · Putil (mj |o)

(9)

In the o ine case, we may choose an arbitrary order of objects to feed into this assignment heuristic. In the online case, we process objects ordered by the timestamps in which they are issued to the MA-proxy. It is also worth noting that in an online se ing users could choose di erent  for each object, deciding that some should be assigned randomly, and some with the best possible context.

3.5.3 Random assignment. In this assignment, objects are assigned to accounts uniformly at random. is is a special case of the Pro ling-tradeo algorithm with  = 1. is assignment maximizes privacy.

3.5.4 Coherent assignment. Personalization is usually based on semantically coherent parts of user pro les. If we retain such coherent fragments of a pro le within the accounts, individual utility should be preserved be er than in a completely random assignment. e mode in which we assign an object to the account that o ers the best coherence is a special case of the Pro lingtradeo algorithm, in which we set  = 0. We refer to this method as Coherent. is assignment explicitly aims for the best utility only, yet some privacy is gained as chunks of user pro les get assigned to MAs randomly.

4 MA IN SEARCH SYSTEMS
By analyzing query-and-click logs, search engines can customize results to individual users. Such user pro ling, however, may reveal a detailed picture of a person's life, posing potential privacy risks. At the same time, personalization of a single query is o en based on only a subset of a user's history. us, as a rst use case, we apply the MA framework in a search engine se ing, scrambling the query histories of di erent users across accounts.

4.1 Framework elements
In the search scenario, the elements of the framework described in Sec. 3 are instantiated as follows. e objects are keyword queries, and user pro les consist of sets (or sequences) of queries, possibly with timestamps. Accounts contain re-assigned queries of di erent users. Object similarity can be understood as topical similarity between queries, with topics being either explicit such as categories or classi er labels, or latent, based on embeddings. As a query is characterized by a set (or weight vector) of topics, the similarity can be computed, for instance, using (weighted) Jaccard overlap or vector cosine. e service provider in this se ing is a search engine, which, upon receiving a query from a given user pro le, returns a ranked list of documents personalized for that user. User utility is measured by the quality of the result list.

4.2 Service provider model
e ability of the MA framework to preserve utility while splitting user pro les across accounts depends on a retrieval model for ranking query answers. We use the language-model-based retrieval technique [10], as described below.

678

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Let o  u be a query of user u consisting of a number of words w  o, and D be the document collection. e model retrieves the results in two steps. First, it fetches a set of top-k documents Do  D, each document d  D being scored by the query-likelihood model with Dirichlet smoothing (parameter µD [10]):

score (o,d ) = log P (o|d ) = log
w o

t fw,d + µD · P (w |D) |Vd | + µD

(10)

where t fw,d is the count of w in d, P (w |D) is the probability that w occurs in D, and |Vd | is the count of all words in d. For every user u, we compute a personalization score as the log-probability of the
document d being generated from the user language model using
Dirichlet smoothing with parameter µU , where U is the set of all users (or equivalently, the collection of their search histories):

score (d,u) = log P (d |u) = log
w d

t fw,u + µU · P (w |d ) |Vu | + µU

(11)

where t fw,u is the count of w in the search history of u, P (w |d ) is the probability that w occurs in d, and |Vu | is the count of all words in the search history of u.
In the second step, documents d  Do are re-ranked using a linear combination of the two scores:

scoreu (o,d ) =  · score (o,d ) + (1 -  ) · score (d,u) (12)

In practice,  would be set to a low value to put more importance on personalization.
When we use the MA framework, the computations are similar. e notion of a user is simply replaced by an account m. e personalization stage is adjusted as follows: we compute score (d,m) using P (d |m), which in turn is computed using t fw,m , µM and |Vm | with Eq. 11. De nitions of these quantities are analogous to their user counterparts.

5 MA IN RECOMMENDER SYSTEMS
Recommendation platforms like online shops, movie review forums, and music streaming sites, aggregate user interaction histories over time. As in the search se ing, there is a direct correlation between the accuracy and completeness of user pro les and the quality of the obtained recommendations. us, as a second use case, we apply the MA framework to recommender systems.

5.1 Framework elements
Users of a recommendation platform rate di erent items like movies, books, and hotels. ese item-rating pairs are the objects in the pro le of a user. An item belongs to a set of categories (e.g., movie genres) from a taxonomy de ned by the service provider. e object similarity function can be de ned by topical similarity using such categories or tags. Additionally or alternatively, similarity can consider the ownership of objects, that is, whether two ratings are by the same user or di erent ones.
User utility here refers to the quality of recommendations. Rating predictions for items unseen by a user are made based on the past ratings of that user, as well as ratings of similar items by similar users. us, scrambling a user's ratings across accounts, if not done in a principled fashion, can potentially destroy user-item preference pa erns, and hence degrade the quality of rating prediction.

5.2 Service provider model

A widely deployed rating prediction algorithm is collaborative l-
tering (CF) [22], which has been used by providers like Net ix,
Amazon and Google [49]. In this model, the values of user-item
ratings are stored as a matrix, where rows represent users, and
columns represent items. CF maps both users and items into a low
dimensional (latent) space using matrix factorization, such that
user-item interactions (ratings) are modeled as inner products in that space, r^(ui ,ok ) = qok T pui , where pui  Rf and qok  Rf are vector representations of the user ui and item ok in the latent space with dimensionality f . CF avoids over ing by adding a regular-
ization term to the objective function of the matrix factorization.
To learn the low-dimensional vectors (pui and qok ), the system minimizes the regularized squared error on the set of known ratings
(Eq. 13) [22]:

min
q,p

(r (ui ,ok ) - qok T pui )2 + (||qok ||2 + ||pui ||2) (13)

(ui ,ok ) R

where R is the set of (ui ,ok ) pairs for which r (ui ,ok ) is known (the training set of gold ratings), and  is the regularization parameter.

e trained model can then be used to predict ratings for unseen

(user, item) pairs using the expression for r^(ui ,ok ). In the MA framework, the service provider no longer predicts the

ratings for users, but for mediator accounts. Speci cally, it learns

a CF model from the MA-ratings data (triples of MA-id, item-id,

rating), and then makes predictions for the unseen (mj ,ok ) pairs. As the ratings of an individual user are spread across di erent

MAs, we need a method for propagating the predicted ratings from

the MAs back to the individual users. Assume that we need to

predict the rating r^(ui ,ok ) when the MAs are present. Under a given assignment, ui 's items are split across MAs {mj  M } with a distribution i (Sec. 3.3), where the fraction of ui 's mass in some mj is given by ij . We assume that the MA-proxy has access to the predicted ratings r^(mj ,ok ) from the service provider. We compute the propagated rating r^m (ui ,ok ) as the weighted sum of r^(mj ,ok ):

|M |

r^m (ui ,ok ) = i jr^(mj ,ok )

(14)

j =1

6 EXPERIMENTS ON SEARCH
6.1 Experimental setup
6.1.1 Dataset. For lack of publicly available query logs with user pro les, we created a query log and a document collection using the data from the Stack Exchange Q&A community (dump as of 1306-2016). We excluded the large so ware subforums from outside the Stack Exchange web domain (such as StackOver ow), as they would dominate and drastically reduce the topical diversity. e
nal dataset consists of ca. 6M posts of type ` estion' or `Answer' in 142 diverse subforums (e.g., Astronomy, Security, Christianity, Politics, Parenting, and Travel). Document collection. We use all posts of type `Answer' as our collection. e resulting corpus contains 3.9M documents. User query histories. We construct a query log from posts of type ` estion', as these re ect users' information needs. Each question is cast into a keyword query selecting the top-l question

679

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

words with the highest TF-IDF scores, where l is a random integer between 1 and 5. We consider only users with at least 150 questions, which yields a total of 975 users and 253K queries. Each query is assigned a topical label, used for object similarity. We set this label to the subforum where the original question was posted.

6.1.2 Service provider. For reproducible experiments, we base our search engine model on the open-source IR system Indri [40]. Indri ranks query answers based on state-of-the-art statistical language models with Dirichlet smoothing [10]. We use Indri to retrieve the top-100 results for every query from the entire corpus, and implement user-personalized re-ranking ourselves (see Sec. 4.2). We compute per-user language models from the original questions to tackle sparsity. e Dirichlet smoothing parameter is set to the average document length (56 words), and  is set to 0.1.

6.1.3 Empirical measures.

Privacy Gain. e model entropy re ects how scrambled the user

pro les are. Yet from the perspective of a pro ling adversary it

is rather the distribution over semantic topics that ma ers. Em-

pirically, a proper way to measure privacy then is to compare the

original topic distribution per user against the topic distributions

of the MAs. e minimum KL-divergence between pairs of these

distributions signi es the privacy level:

emp -pri

-

ain (ui

)

=

min
mj M

DK L (Pui

Qmj )

(15)

where Pui and Qmj refer to the user and MA pro le distributions

over topics with add-one Laplace smoothing. We use subforums as

explicit labels for topics.

Utility Loss. Rankings of documents d for a query are derived

from scoreu (o,d ) and scorem (o,d ) (Eq. 12), respectively, where the former refers to the query being issued by user u and the la er to

the query being issued by the mediator account m (see Sec. 4.2).

We quantify the empirical utility loss as the divergence between

the two rankings. We compute two measures: the loss in Kendall's

Tau over the top-100 document rankings: 1 - KT au@100 (as the

personalization step considers the top-100 documents), and the loss

in Jaccard similarity coe cient over the rst 20 ranking positions:

1 - accard@20 (as end-users typically care only about a short

pre x of ranked results). For each user, we average these scores

over all queries.

6.1.4 Assignment methods. Object similarity. We set sim(oi ,oj ) = 1 if both oi and oj belong to the same user and to the same topic, and 0 otherwise. During the assignment, this measure helps to keep related parts of a user pro le together. Assignment algorithms. We run the Pro ling-Tradeo algorithm varying  between 0 and 1 with a 0.1 increment, and se ing the number of MAs to be the number of users (975). With the chosen object similarity, the special case of  = 0, i.e. the Coherent assignment, results in spli ing user pro les into subforum chunks and assigning each chunk to a randomly chosen account.

6.2 Results and insights
Aggregate trends. Table 1 presents the results on the model measures and empirical measures for di erent values of the assignment trade-o parameter, macro-averaged over users. Recall that  = 0.0

Table 1: Search results with trade-o parameter  for the model (M) and empirical (E) measures.


Original
0.0 (Coh) 0.2 0.4 0.6 0.8
1.0 (Rand)

M-Priv-Gain (Entropy)
0.000
1.180 2.208 3.130 3.975 4.731 5.287

M-Util-Loss (Coherence Loss)
0.000
0.178 0.293 0.389 0.463 0.515 0.535

E-Priv-Gain (Min. KL-div.)
0.000
0.320 0.319 0.346 0.389 0.494 0.863

E-Util-Loss (1 - KTau@100)
0.000
0.170 0.203 0.228 0.246 0.260 0.266

Figure 2: Model measures per user (search).
Figure 3: Empirical measures per user (search). and  = 1.0 correspond to the special cases of Coherent and Random assignments, respectively. ese results need to be contrasted with the baseline, denoted Original in the table, where each original user forms exactly one account (i.e., no scrambling at all). Compared to the baseline, all numbers are statistically signi cant by

680

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 4: E ect of pro le size and diversity (search).
paired t-tests with p < 0.01. For empirical utility loss, we report Kendall's Tau; the results for the Jaccard coe cient are similar.
e results show that the Pro ling-Tradeo assignments improve privacy over the Original baseline (the topical KL-div. between original users and MAs is increased) while keeping the utility loss low. is is largely true regardless of the exact choice of . So the MA framework provides a fairly robust solution to reconciling privacy and utility, supporting the observation that high-quality topical personalization does not require complete user pro les.
With the  increasing, assignments become more random, so the privacy increases and utility is reduced (but with a low gradient). In this regard, the empirical measures re ect the expected behavior according to the model measures well.
Results per user. Figs. 2 and 3 show privacy and utility values of each user, for the model and empirical measures, respectively. Di erent colors represent di erent assignments, and each dot represents a user, with measures averaged over the user's queries. We have several observations:
· Higher privacy gain is correlated with higher utility loss. e Original assignment maps each user to the origin (0 utility loss, but also 0 privacy gain). No assignment reaches the bo omright area of the chart ­ which would be an ideal.
· Varying  not only tunes the privacy-utility tradeo at the community aggregate level, but also a ects the variance over individual user scores. is suggests that we should further explore choosing  on an individual per-user basis (which is easily feasible in our framework, but is not studied in this paper).
· Even the Random assignment ( = 1.0) keeps utility reasonably high. is is due to the fact that random MAs ­ sampled from queries in the community ­ end up being averaged rather than random pro les.
· Some users achieve high privacy gains without losing hardly any utility, and vice versa. We investigate this further below.

E ect of pro le size and diversity. We analyze how di erent user pro le characteristics a ect the assignment results. Figure 4 presents the empirical trade-o s for the Coherent (top row) and Random (bo om row) assignments, where each dot is a user and the dot color represents (i) the logarithm of the number of queries in the user pro le (le column), or (ii) the diversity of the pro le measured by the entropy of the distribution of queries across topics (right column). We make the following observations:
· Users with more queries (darker dots) in the Coherent assignment clearly gain privacy at the cost of losing utility, whereas for the smaller pro les (lighter dots), the trade-o is not as pronounced. In the Random assignment this trade-o is less pronounced irrespective of the size of the pro le.
· In the right column, one can see the lighter dots (pro les with li le diversity) moving from the bo om-le for the Coherent assignment (li le privacy gain, li le utility loss) to the top-right for the Random assignment (higher privacy gain, higher utility loss). is suggests that our framework does not o er much help to the users with uniform and focused interests. is is an inherent limitation, regardless of which privacy protection is chosen. Such homogeneous users cannot hide their speci c interests, unless they give up on personalization utility.
· Our split-merge assignments o er good results for users with high diversity. As suggested by the darker dots, the Coherent assignment leads to a lower utility loss and higher privacy gain for users with diverse pro les, when compared to the Random assignment. is is because such users have more independent and internally coherent chunks that can be split without a ecting utility. is class of users is exactly where the right balance of utility and privacy ma ers most, and where we can indeed reconcile the two dimensions to a fair degree.
7 EXPERIMENTS ON RECOMMENDERS
7.1 Experimental setup
7.1.1 Dataset. We use the Amazon product rating data collected by Mukherjee et al. [30] which contains user-item ratings, along with review text and metadata. We extract (user, item, rating) triples from this data with associated timestamps, and restrict product type to music, one of the most frequent product types. We identify active users who rated between 50 and 1000 items. Within music there are 22 categories, as de ned by Amazon: rock, jazz, country, etc. An item is originally associated with 2.12 categories on average, but we assign it uniquely to one category by selecting the most frequent category. e nal data had 1, 719 users, 72, 464 items and 197, 215 ratings (ratings are between 1 and 5).
7.1.2 Service provider. We used a parallelized implementation of a collaborative ltering model based on matrix factorization: ALS-WR [22, 49]. is is available in Apache Spark (h ps://goo.gl/ X33DN9, Accessed 23 Jan 2017), and widely used. e model has three parameters ­ number of latent features f , the regularization parameter , and the number of iterations ni to run. f was set to 22, which is the number of categories in the data.  and ni were learnt to be 0.3 and 10 respectively, using grid search on a separate development set of 60K ratings.

681

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Recommender results with parameter  for the model (M) and empirical (E) measures.


Original
0.0 (Coh) 0.2 0.4 0.6 0.8
1.0 (Rand)

M-Priv-Gain (Entropy)
0.000
0.826 1.778 2.573 3.339 4.009 4.495

M-Util-Loss (Coh. Loss)
0.000
0.094 0.086 0.106 0.122 0.133 0.137

E-Priv-Gain (Min. KL-Div.)
0.000
0.073 0.064 0.075 0.093 0.130 0.262

E-Util-Loss (MSE)
0.000
0.501 0.576 0.623 0.664 0.689 0.690

7.1.3 Empirical measures. Privacy Gain. We use the same measure of per-user empirical privacy as in the search scenario, which is the KL-divergence between the topical distributions representing the user pro le and the "closest" MA pro le (Eq. 15). e distributions are computed over the 22 Amazon Music categories (with add-one Laplace smoothing). Utility Loss. Empirical utility for user ui is measured in terms of mean squared error (MSE). MSE is computed for Rutei st , where the system needs to generate recommendations for each of 40 given items, for each user ui . ese test cases are not part of the training data. e error is computed between predictions made by the user model and the model propagated through the MAs, i.e., r^(ui ,ok ) and r^m (ui ,ok ) (Sec. 5.2). We thus have in Eq. 16:

emp-util-loss (ui ) =

|Rut ei st =1

|
(r^

(ui ,ok )

-

r^

m (ui ,ok ))2

|Rut ei st |

(16)

e set of 40 unseen items for a user to make predictions on is picked at random from the set of 72, 464 items. Our complete test set thus contained 68, 760 (= 1, 719 × 40) (user, item) pairs.
Under the MA model, each user-item rating prediction requires multiple MA-item predictions. For the assignment methods presented, we had to make up to 7.5M predictions in total from the MA model, for the 68, 760 test ratings. However, with the parallelized Spark implementation, this took only a few minutes on a single machine with 8 GB RAM.

7.1.4 Assignment methods. We create 1, 719 MA pro les, equal to the number of users. e object similarity is set to 1 if two objects belong to the same user and the same category, and 0 otherwise.
e tradeo parameter  was varied from 0.0 through 1.0 in steps of 0.1. Rating timestamps were used to determine the order of object assignments.

7.2 Results and insights
Aggregate trends. Table 2 shows how the model and empirical measures of privacy and utility vary with tradeo parameter . Each value is an average over all 1719 users.  = 0.0 and 1.0 correspond to Coherent and Random assignments respectively. Original again denotes the baseline where each original user forms her own account. Note that higher values for the privacy measures of entropy and KL-divergence indicate be er privacy gain, and lower values for the utility measures of coherence loss and MSE imply be er user utility loss.

Figure 5: Model measures per user (recommenders).
Figure 6: Empirical measures per user (recommenders).
e major insight here is the following. For the Coherent assignment, we observe good empirical utility (a very low MSE of 0.501), while providing a substantial improvement in empirical privacy (0.073 from 0). us, this assignment is a good candidate for practical deployment. Generally, the Pro ling-Tradeo assignment works well for all , demonstrating the robustness of our approach.
e trends in empirical privacy and utility mimic those of the model measures. As  is increased from 0.0 to 1.0, there is gradual improvement in empirical privacy and a monotonic degradation in user utility, both with low gradients. Results per user. Figs. 5 and 6 visualize privacy versus utility for model and empirical measures, where each point denotes an individual user. Di erent colors represent di erent tradeo se ings, as shown in the legend. ere are two notable observations:
· Users form di erent clusters in the privacy-utility space as  is varied. e clusters are so er (more overlapping) in the empirical case than in the model. is shows that analytically

682

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 7: E ect of pro le size and diversity (recommenders).
predicting these tradeo statistics is not easy; hence our experimental approach.
· Fig. 6 indicates that, for the Coherent assignment, a fraction of users have both high privacy gain and low utility loss (the brown dots in the lower right area of the chart), while most users exhibit the expected privacy-utility tradeo . is suggests further optimizations by tuning assignment parameters for each individual user separately.
E ect of pro le size and diversity. As in the search se ing, we report on the variation of the empirical measures with respect to changes in user pro le size (number of items rated) and diversity (entropy of the distribution of item categories in the user pro le) in Fig. 7. e two plots on the le refer to size and the ones on the right to diversity. e upper and lower plots correspond to the Coherent and Random assignments, respectively. We observe:
· Users with diverse interests do well on both dimensions (more dark dots towards lower right), and thus bene t the most from our framework. is represents a signi cant fraction of all users, as seen by the numerous dark dots in the diversity graphs.
· A higher pro le size yields lower utility loss, for the Coherent assignment. So while a bigger pro le typically entails be er prediction accuracy (our measure of empirical utility), we can see that this is true only if related parts of the user pro le are preserved in the MAs. is comes at the cost of somewhat reduced privacy gains. However, the Coherent assignment does fairly well in both dimensions. For Random assignment, pro le size does not have a notable e ect on privacy, and utility is o en completely lost.
· Overall, users with larger pro les and varied interests have the best chance of preserving utility while having substantial privacy gains under the Coherent assignment. us, we can derive guidelines for tuning  (Table 2):  may be kept high in the beginning for objects of all users, and it may gradually be decreased when users show diversity in their interests as their pro les grow bigger (cf. Sec. 3.5.2).

8 RELATED WORK
Grouping for privacy: e idea of masking the traces of individual users by combining them into groups has been around since the Crowds proposal by [35]. However, this early work solely focused on anonymity of web-server requests. [31] devised an abstract framework for group privacy over obfuscated databases, but did not address utility. For search engines speci cally, [21] proposed a notion of query bundles as an implicit grouping of users, but focused on countering de-anonymization in the presence of so-called vanity queries. e short paper [50] sketches a preliminary approach where semantically similar queries by di erent users are grouped for enhancing privacy. Aggregation of users' website-speci c privacy preferences through a centralized server [47], can also be perceived as a type of privacy through solidarity. e principle of solidarity has moreover been explored through a game-theoretic framework over recommender systems [18].
Tracking and pro ling: A good body of work investigates to what extent and how users are tracked by third parties in web browsers [24, 29, 47], or through mobile apps [28]. ese are primarily empirical studies with an emphasis on identifying the tracking mechanisms. e interactions with service providers, where users log in and leave extensive traces, have been largely disregarded. In contrast, our framework helps counter both tracking and individual pro ling by detaching users from online accounts.
To reduce the scale of pro ling, a model called stochastic privacy has been proposed to selectively sample user pro les for use by personalizing algorithms [39]. To counter pro ling by search engines in particular, [45] has proposed to issue queries anonymously, but provide the engine with a coarse topical pro le for answer quality. On the tracking front, the Non-Tracking Web Analytics system reconciles users' need of privacy and online providers' need of accurate analytics [3]. Although these various works address the privacy-utility trade-o , no explicit control mechanism has been proposed for user utility.
Privacy-preserving IR: e intersection of privacy and IR has received some a ention in the past years [46]. One of the key problems studied in the eld is that of post-hoc log sanitization for data publishing [9, 16, 48]. Online sanitization, on the other hand, aims at proactively perturbing and blurring user pro les. Techniques along these lines typically include query broadening or dummy query generation (e.g., [4, 33, 37, 43]). It has also been proposed to perturb user pro les by making users swap queries and execute them on behalf of each other [34]. Very few of these prior works consider the adverse impact that obfuscation has on utility, and the usual focus is on the utility of single query results. To the best of our knowledge, none of them focuses on personalization utility or o ers quantitative measures for the trade-o .
Another privacy concept studied in IR is that of exposure. Recently, the notions of R-Susceptibility and topical sensitivity have been proposed to quantify user exposure in sensitive contexts within a given community [6].
Privacy-preserving data mining: ere is a vast body of literature on preserving privacy in mining data for rules and pa erns and learning classi ers and clustering models [2, 12]. In this context, utility is measured from the provider's perspective, typically an error measure of the mining task at hand (e.g., classi cation error)

683

Session 6A: Personalization and Privacy

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

[5]. In the speci c context of recommender systems, rating prediction accuracy [27, 32] and category aggregates [38] are typically used as proxies for utility. Techniques for user pro le perturbation have also been studied for utility-preserving di erentially-private recommeders [17].
9 CONCLUSIONS
We presented Mediator Accounts (MAs): a framework to counter user pro ling while preserving individual user utility as much as possible. e framework enables decoupling users from accounts, making direct targeting impossible, and pro le reconstruction or deanonymization much harder. At the same time, users are still able to bene t from personalization by service providers. e versatility of the framework has been demonstrated in two di erent application scenarios. While our model allows for exible trade-o s between privacy and utility, a key question in our two empirical studies has been to understand how well the MAs can preserve the utility in terms of high-quality search results and recommendations. e experiments show that the split-merge approach with Coherent assignment improves the privacy, while incurring li le user utility loss. ese bene ts are most pronounced for users with larger pro les (i.e., more activity) and higher diversity of interests.
Open issues for future work include practical deployment, handling of other personalization features, and exploring the options for tuning assignments and framework parameters to the speci c needs of individual users. On top of that, analyzing the threedimensional trade-o between user privacy, user utility and the traditional service provider utility could help ensure that the resulting mediator pro les are a useful source for user analytics, making an MA proxy a tolerable component of the online landscape.
Finally, we would hope that the MA proposal stirs up the investigation of how the need-to-know principle could be implemented in case of personalized online services.
Acknowledgements. We would like to thank Subhabrata Mukherjee from MPII for useful discussions at various stages of this work.
REFERENCES
[1] L. Agarwal, N. Shrivastava, S. Jaiswal, and S. Panjwani. Do Not Embarrass: Re-examining User Concerns for Online Tracking and Advertising. In SOUPS '13.
[2] C. Aggarwal. Data mining: e textbook. Springer, 2015. [3] I. E. Akkus, R. Chen, M. Hardt, P. Francis, and J. Gehrke. Non-tracking web
analytics. In CCS '12. [4] E. Balsa, C. Troncoso, and C. Diaz. Ob-pws: Obfuscation-based private web
search. In S&P '12. [5] E. Bertino, D. Lin, and W. Jiang. A survey of quanti cation of privacy preserving
data mining algorithms. In Privacy-preserving data mining '08. [6] J. A. Biega, K. P. Gummadi, I. Mele, D. Milchevski, C. Tryfonopoulos, and
G. Weikum. R-Susceptibility: An IR-centric approach to assessing privacy risks for users in online communities. In SIGIR '16. [7] B.-C. Chen, D. Kifer, K. LeFevre, and A. Machanavajjhala. Privacy preserving data publishing. Foundations and Trends in Databases '09. [8] G. Chen, H. Bai, L. Shou, K. Chen, and Y. Gao. UPS: e cient privacy protection in personalized web search. In SIGIR '11. [9] A. Cooper. A survey of query log privacy-enhancing techniques from a policy perspective. TWeb '08. [10] W. B. Cro , D. Metzler, and T. Strohmann. Search engines. Pearson Education, Inc., 2010. [11] C. Dwork. Di erential Privacy: A Survey of Results. In TAMC '08. [12] L. Fan, L. Bonomi, L. Xiong, and V. Sunderam. Monitoring Web browsing behavior with di erential privacy. In WWW '14.

[13] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu. Privacy-preserving Data Publishing: A Survey of Recent Developments. ACM Computing Surveys '10.
[14] A. Gervais, R. Shokri, A. Singla, S. Capkun, and V. Lenders. antifying websearch privacy. In CCS '14.
[15] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and R. Tamassia. Privacypreserving group data access via stateless oblivious RAM simulation. In SODA '12.
[16] M. Go¨tz, A. Machanavajjhala, G. Wang, X. Xiao, and J. Gehrke. Publishing search logs: A comparative study of privacy guarantees. TKDE '14.
[17] R. Guerraoui, A.-M. Kermarrec, R. Patra, and M. Taziki. D2P: distance-based di erential privacy in recommenders. VLDB '15.
[18] M. Halkidi and I. Koutsopoulos. A game theoretic framework for data privacy preservation in recommender systems. In ECML PKDD'11.
[19] A. Hannak, P. Sapiezynski, A. Molavi Kakhki, B. Krishnamurthy, D. Lazer, A. Mislove, and C. Wilson. Measuring personalization of web search. In WWW '13.
[20] X. He, A. Machanavajjhala, and B. Ding. Blow sh privacy: Tuning privacy-utility trade-o s using policies. In SIGMOD '14.
[21] R. Jones, R. Kumar, B. Pang, and A. Tomkins. Vanity fair: Privacy in querylog bundles. In CIKM '08.
[22] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. IEEE Computer '09.
[23] A. Krause and E. Horvitz. A utility-theoretic approach to privacy in online services. JAIR '10.
[24] A. Lerner, A. K. Simpson, T. Kohno, and F. Roesner. Internet jones and the raiders of the lost trackers: An archaeological study of web tracking from 1996 to 2016. In USENIX Security '16.
[25] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity. ICDE '07.
[26] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam. l-diversity: Privacy beyond k-anonymity. TKDD '07.
[27] F. McSherry and I. Mironov. Di erentially private recommender systems: Building privacy into the Net ix prize contenders. In KDD '09.
[28] W. Meng, R. Ding, S. P. Chung, S. Han, and W. Lee. e Price of Free: Privacy Leakage in Personalized Mobile In-App Ads. In NDSS '16.
[29] W. Meng, B. Lee, X. Xing, and W. Lee. TrackMeOrNot: Enabling Flexible Control on Web Tracking. In WWW '16.
[30] A. Mukherjee, B. Liu, and N. Glance. Spo ing fake reviewer groups in consumer reviews. In WWW '12.
[31] A. Narayanan and V. Shmatikov. Obfuscated databases and group privacy. In CCS '05.
[32] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, N. Ta , and D. Boneh. Privacypreserving matrix factorization. In CCS '13.
[33] S. T. Peddinti and N. Saxena. Web search query privacy: Evaluating query obfuscation and anonymizing networks. JCS '14.
[34] D. Rebollo-Monedero, J. Forne, and J. Domingo-Ferrer. ery pro le obfuscation by means of optimal query exchange between users. TDSC '12.
[35] M. K. Reiter and A. D. Rubin. Crowds: Anonymity for Web transactions. TISSEC '98.
[36] N. Santos, A. Mislove, M. Dischinger, and K. Gummadi. Anonymity in the personalized web. In NSDI Posters '08, 2008.
[37] X. Shen, B. Tan, and C. Zhai. Privacy protection in personalized search. In SIGIR Forum '07.
[38] Y. Shen and H. Jin. Epicrec: Towards practical di erentially private framework for personalized recommendation. In CCS '16.
[39] A. Singla, E. Horvitz, E. Kamar, and R. White. Stochastic privacy. In AAAI '14. [40] T. Strohman, D. Metzler, H. Turtle, and W. B. Cro . Indri: A language model-
based search engine for complex queries. In International Conference on Intelligent Analysis '05. [41] L. Sweeney. k-anonymity: A model for protecting privacy. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems '02. [42] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing search via automated analysis of interests and activities. In SIGIR '05. [43] P. Wang and C. V. Ravishankar. On masking topical intent in keyword search. In ICDE '14. [44] Y. Xu, K. Wang, G. Yang, and A. W. Fu. Online anonymity for personalized web services. In CIKM '09. [45] Y. Xu, K. Wang, B. Zhang, and Z. Chen. Privacy-enhancing personalized web search. In WWW '07. [46] H. Yang, I. Soboro , L. Xiong, C. L. Clarke, and S. L. Gar nkel. Privacy-Preserving IR 2016: Di erential Privacy, Search, and Social Media. In SIGIR '16. [47] Z. Yu, S. Macbeth, K. Modi, and J. M. Pujol. Tracking the trackers. In WWW '16. [48] S. Zhang, G. H. Yang, and L. Singh. Anonymizing query logs by di erential privacy. In SIGIR'16. [49] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale parallel collaborative
ltering for the Net ix prize. In AAIM '08. [50] Y. Zhu, L. Xiong, and C. Verdery. Anonymizing user pro les for personalized
web search. In WWW '10.

684


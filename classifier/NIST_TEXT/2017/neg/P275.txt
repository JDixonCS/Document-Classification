Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Investigating Examination Behavior of Image Search Users

Xiaohui Xie
DCST, Tsinghua University xiexh thu@163.com

Yiqun Liu
DCST, Tsinghua University yiqunliu@tsinghua.edu.cn

Xiaochuan Wang
Sogou Incorporation wxc@sogou-inc.com

Meng Wang
Hefei Institute of Technology eric.mengwang@gmail.com

Zhijing Wu
DCST, Tsinghua University wzjingzai@163.com

Yingying Wu
Department of Mathematics, e University of Texas at Austin
ywu@math.utexas.edu

Min Zhang
DCST, Tsinghua University z-m@tsinghua.edu.cn

Shaoping Ma
DCST, Tsinghua University msp@tsinghua.edu.cn

ABSTRACT
Image search engines show results di erently from general Web search engines in three key ways: (1) most Web-based image search engines adopt the two-dimensional result placement instead of the linear result list; (2) image searches show snapshots instead of snippets (query-dependent abstracts of landing pages) on search engine result pages (SERPs); and (3) pagination is usually not (explicitly) supported on image search SERPs, and users can view results without having to click on the "next page" bu on. Compared with the extensive study of user behavior in general Web search scenarios, there exists no thorough investigation how the di erent interaction mechanism of image search engines a ects users' examination behavior. To shed light on this research question, we conducted an eye-tracking study to investigate users' examination behavior in image searches. We focus on the impacts of factors in examination including position, visual saliency, edge density, the existence of textual information, and human faces in result images. ree interesting ndings indicate users' behavior biases: (1) instead of the traditional "Golden Triangle" phenomena in the user examination pa erns of general Web search, we observe a middle-position bias, (2) besides the position factor, the content of image results (e.g., visual saliency) a ects examination behavior, and (3) some popular behavior assumptions in general Web search (e.g., examination hypothesis) do not hold in image search scenarios. We predict users' examination behavior with di erent impact factors. Results show that combining position and visual content features can improve prediction in image searches.
CCS CONCEPTS
·Information systems  Image search; Multimedia and multimodal retrieval;
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080799

KEYWORDS
Image search; Eye-tracking; Examination behavior;
1 INTRODUCTION
Multimedia content (e.g., images, video) has been increasingly incorporated into search engine result pages (SERPs) to increase both user experience and engagement. However, the way to show results in image search engines di ers greatly from that of general Web search engines. Take the SERP in Figure 1 for example; in image searches, results are placed in a two-dimensional panel rather than a sequential result list. Instead of the query-dependent abstract of the landing page, the image snapshot is shown together with some meta information of the image (sometimes the meta information is only available with a hover behavior on the result), highlighted in Figure 1 with a red rectangle. Further, since results are joined, users can view results by scrolling up and down instead of clicking on the "next page" bu on.
User behavior data has been successfully adopted to improve general Web searches in result ranking [1, 36], query suggestion [7, 35], query auto completion [16, 17], etc. We therefore believe that understanding user interaction behavior in these multimedia search scenarios will also provide valuable insight into the optimization of their performances.
ere exist a number of studies on user behavior log analysis of image search engines [2, 11, 20, 24, 30]. Click-through behaviors, query reformulation pa erns, and session characteristics are investigated in [23, 24]. A comparison with general Web search behaviors was also performed in [2, 11]. Some researchers [15, 20, 22] focus on extracting implicit feedback signals from image search user behavior to improve result ranking performance. However, compared with the research on general Web searches, li le a ention has been paid to the examination behavior of image search users.
Examination is one of the prime concerns in existing search behavior studies because it is closely related with a user's a ention distribution mechanism. With a be er understanding of examination behavior, we can make be er use of existing click-through signals, designing be er evaluation metrics and helping users achieve their information needs more e ectively. Although much research exists on search examination behavior, most e orts are focused on the SERP layout containing a single column of search results

275

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: An example SERP in image search engine ( e red box shows the meta information of the image while cursor hovering on the corresponding position)
[9, 19] or with some sidebar components such as knowledge graph [3, 34]. Considering that image SERPs usually present results in a two-dimensional placement style, whether the examination bias and hypothesis remain applicable becomes an open question.
As a preliminary a empt to model image search examination behavior, we perform a lab-based user study with the help of eyetracking devices. We collect click-through, eye movement, mouse movement, and other behavior data during the search processes.
e relevance scores of image results are also annotated to reveal its relationship with examination and click-through behaviors. We focus on the factors (behavior biases) that have impacts on user examination behavior and how to predict users' a ention distribution using these factors. Speci cally, this study addresses the following research questions:
· RQ 1: How do positions of results a ect user examination behaviors in image searches: are there also vertical position biases as in general Web searches? How do the horizontal positions of results a ect user examination?
· RQ 2: How do the content of image search results a ect user examination behaviors? Do content factors such as saliency, edge density, the existence of human faces, or textual information a ect user examination during image searches?
· RQ 3: What are the relations between relevance, examination, and click in image searches? Does the examination hypothesis [8] still hold or there are new pa erns?
· RQ 4: Can we predict examination behavior in image search? How can this prediction help us be er understand users' examination behaviors?
e paper is organized as follows. Section 2 reviews related work. Section 3 introduces our user-study se ings. Sections 4-6 present
ndings from the user study. We report the experimental results of examination behavior prediction in Section 7. Finally, Section 8 discusses conclusions and future work.
2 RELATED WORK
We brie y summarize the related work in two categories: user behavior in image search and examination behavior of Web users.
e former concentrates on analyzing large scale image search logs, and the la er employs eye-tracking devices to investigate user examination behavior in di erent se ings.

2.1 User behavior in Image Search
With the high volume of tra c on Web search engines, the analysis of query logs becomes one of the most common approaches to understand user behavior. Previous works in image search also characterized the general user behaviors based on search logs [2, 11, 20, 24, 30]. Many features like query reformulation pa erns, session length, and the number of viewed result pages are measured. Compared to general Web (text) search, image search leads to shorter queries, tends to be more exploratory, and requires more interactions. Park et al. [23] analyzed a large-scale query log from Yahoo Image Search to investigate user behavior toward di erent query types and identi ed important behavioral di erences across them.
Interactive behaviors with image search result pages contain abundant implicit user feedback. Previous studies on multimedia search [14, 15, 22] explored user click-through data to bridge user intention gaps for image searches. O'Hare et al. [20] proposed a number of implicit relevance feedback features based on additional interactions including hover-through rate, converted-hover rate, and referral page click-through to improve image search resultranking performance.
Most of the above works focused on mouse-based interactive behavior and tried to improve result ranking performance with corresponding features. Although Wang et al. [32] showed that understanding user examination behavior in Web searches provides powerful insight into user behavior modeling, li le a ention has been paid to examination behavior in multimedia searches.
2.2 Examination Behavior of Web Users
Understanding examination behavior is important for advertising, interaction design, and result ranking in Web search researches. Compared to techniques that rely on the explicit user actions (e.g., mouse clicks), eye-tracking yields more detailed momentby-moment observations about how users interact with search information [13]. Cutrell et al. [9] used eye-tracking to explore the e ects of changing in the presentation of search results. Pan et al. [21] and Buscher et al. [5] explored the determinants of ocular behavior on a single web page and tried to predict salient regions on Web pages. Tatler and Vincent [29] discovered that understanding eye-moving biases triggers gazing decisions in complex scenes. Underwood et al. [31] investigated how eye movements are a ected by visual saliency and the semantic incongruence when inspecting pictures. e above works showed the e ectiveness of eye-tracking in understanding examination behavior on images in di erent settings. In this study, we carry out an eye-tracking experiment to investigate the user examination behavior in image searches.
Click models also are used to model users' click and examination behavior in Web searches. Previous works on click models and search user's examination behavior pa erns [8, 10, 32] were based on general search result pages with one-dimensional result lists. e major variables that most click models consider include examination, click, and relevance. ey follow the examination hypothesis [8] that a clicked document should satisfy two independent conditions: it is examined and it is relevant. e cascade model [8] assumes that while a user examines the results from top to bottom sequentially, she/he immediately decides whether to click on a result. e partially observable Markov Model (POM) [33] treats

276

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 2: Data Collection Procedure

Table 1: Examples of Search eries and Corresponding Taxonomies

ery Type [23] Speci c Generic
Abstract

# ery 8
7 (1 for Warm-up Training)
6

ery Example (Task Description) Doraemon (Find an image of Doraemon for your Wechat pro le picture)
New York (Find images about New York City's beautiful scenery) pleasant surprise (Find some images that express pleasant surprise to make
a WeChat expression)

the user examination events as a partially observable stochastic process. Wang et al. [32] investigated the user's non-sequential examination behavior in ordinary search result pages and proposed a click model named PSCM (Partially Sequential Click Model) that captures this behavior. To the best of our knowledge, no existing click models are employed to model user examination behavior in image searches. rough our investigation in user examination behavior and its relation to clicks and relevance in two-dimensional result pages we may help design be er click models that describes the interaction processes of image search users more precisely.
3 USER BEHAVIOR DATASET
3.1 Data Collection procedure
To conduct the tasks of our experiment , we sampled 21 intermediate frequency queries from the query log of a popular image commercial search engine1. Since search behaviors may be a ected by di erent types of queries [23], we tried to design tasks to cover di erent query categories. We applied the Shatford-Panofsky approach [26] for image classi cation, like previous work in image search analysis has performed in [23]. In the sampled query set, eight queries are "Speci c" (e.g., "Doraemon"), seven queries are "Generic" (e.g., "New York"), and six queries are "Abstract" (e.g., "pleasant surprise"). We also provided a task description for each query to make sure that users know exactly what results need to be looked for on the image
1 e data overs the period of June in the year of 2016, which we will release a er the review process

SERPs. Table 1 shows a sample set of search queries and their corresponding descriptions. We crawled these queries' result pages from the commercial search engine and displayed them as SERPs in our experiment. e SERPs in the experiment contain 20 rows of image results to ensure a reasonable user study duration.
To investigate user's examination behavior during the search process, we carry out a laboratory study. e process is shown in Figure 2. Our user study consists of two stages. In the rst stage, the participants perform several image search tasks, and meanwhile, their eye movement behaviors are collected. In the second stage, the participants are asked to annotate the relevance of each examined image search result. e dataset involves 46 undergraduate students majored in science, engineering and arts. All of them were frequent users of Web search engines. Because of calibration problems with the eye-tracking devices, not all participants' eye movement data were available, and 40 of them (female=16, male=24) were nally taken into account. Among the 40 participants, 20 performed the rst stage of the experiment while the others took part in both stages, including recording eye-tracking data and labeling relevance.
At the beginning of the rst stage, participants are given 21 search tasks, which include 1 warm-up task to familiarize them with the procedure and other 20 formal tasks. For each search task, participants were presented with an initial search query and a short description about the task. A er reading the instructions, users click "start" bu on and a retrieved SERP containing 20 rows of pictures for this query is returned. Participants are instructed

277

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Relevance Labeling Dataset

#Images

#Images with same relevance judgement (66%)

#Relevant Images

#Irrelevant Images

1522

585

414

to examine the SERPs as they normally do. Like practical search scenario, they could scroll to move the page up and down, use the mouse to see hover text, and click a thumbnail to view and download the full-size image in the preview page. In the warm-up task, participants can browse and click the search results, and they can adjust the sensitivity of the mouse to the most appropriate level as well. We do not give any further instruction on which results to click on or when to end until participants are familiar with the experimental search engine.
e second stage is about relevance labeling. For each task, participants are given a relevance labeling page containing 20 images sampled from the images that they examined in the rst stage in SERP. Users are asked to label each image with one of three levels (1-not relevant; 2-fairly relevant; 3-very relevant), consistent with previous works on query-image pairs relevance labeling[14, 37]. We deem the images to be relevant if their labels are score 2 or score 3. To further make sure the quality of the relevance judgment, we remove all images which participants have di erent opinions in their relevance and only retain the ones that di erent participants have the same opinions. e remaining annotated image set contains 999 images, in which 585 are relevant and 414 are irrelevant. Details about our relevance labeling dataset are shown in Table 2.
3.2 Data Collection System
In our study, we inject customized JavaScript into search result pages to log mouse activities on search pages when users perform search tasks. Considering that head-free eye trackers allow the collected interactions to be more natural and realistic, a Tobii X2-30 eye tracker is used to capture participants' eye movements. e search system is deployed on a 17-inch LCD monitor whose resolution is 1366 × 768. Google Chrome browser is used to display results of search system. To identify users' examination behavior, we detect xations using built-in algorithms and all default parameters from Tobii Studio. All the collected data including the relevance judgments will be open to public a er the double-blind review process.
4 POSITION BIAS AND EXAMINATION
With the user behavior data set described in Section 3, we look into the examination behavior of image search users and try to answer the four research questions in Section 1. Firstly, we focus on the in uence of the position factor (RQ1).
4.1 Two-dimensional Placement of Results
As stated in previous sections, image search results are placed in a two-dimension manner. In the image search engine from which we collected SERPs, each result page contains 5 rows (row height = 200 px) and there are 3 to 7 images in each row (4 to 6 images in most cases). e users can scroll across di erent pages without having to click a "next page" bu on as in general Web search. Since we retain only the top 20 rows of results for each query, the SERP we

collected each contains 4 "pages" of results (although the pages are only conceptional and there is only a page number shown on the SERP which is usually omi ed by users).
Considering the fact that each row may contain di erent numbers of results (for example, the rst row in the SERP from Figure 1 contains 6 images while the third contains 7), we decide to use absolute position instead of the border of images to segment the SERPs. Speci cally, for each SERP, we equally divided it into 20 rows and 5 columns and use the median values of xation duration/ rst arrival time in each grid to show users' examination pa erns. Median is used instead of mean values here to reduce e ects of outliers.
4.2 First arrival time
First arrival time of an image is the rst time that a user gazes at the image. It is usually used to show user's examination sequence facing a given Web page. Figure 3(a) shows the heat map of the rst arrival time of the images in the rst result page, where the -axis denotes the row number from 1 to 5. In Figure 3(b), we plot another heat map which represents the rst arrival time of the images in page 1 to 4, with -axis being the page number. e x-axis in Figure 3 indicates the column number as noted in Section 4.1. From Figure 3(a), we can see that the minimum rst arrival time of the rst row is at the position (1, 2), which is located around the middle position of the rst row and highlighted by a black rectangular frame in the
gure. From both Figures 3(a) and 3(b), we can see that users generally
follow a top-down pa ern in the vertical direction of their examination sequences since the rst row (in 3(a)) and the rst page (in 3(b)) are both rstly xated. However, in both of the rst two rows in 3(a), the le most positions are not examined rstly. is is an interesting nding considering that the top-le position is usually regarded as the rstly-viewed position on a Web page [13]. In most current ranking strategies of image search engines, the most relevant image is usually placed at the top-le position (1, 1). However, according to our results, perhaps (1, 2) is a be er choice since users pay a ention rstly to this position (1, 2).
4.3 Examination duration
Examination duration of an image is de ned as the dwell time during which a user examines the image. Dwell time has been regarded as an important implicit feedback for improving result ranking in search engines [21]. We calculate the examination duration for each image and plot users' examination duration distribution in Figure 4.
Similar with Figure 3, we also notice a middle position bias in the horizontal direction in both Figures 4(a) and 4(b). In 4(a), the position (1, 3) is xated the most (as highlighted by a black rectangular frame), which is located around the middle position of the rst row with a median xation duration of 1.35 seconds. e le -most position (1,1) of the rst row is xated longer than the right-most position (1,5), but the duration is still shorter than the middle position (1,3). In 4(b), we also nd that the middle position of the rst page is xated the most. In each page, it seems that the middle positions always draw more a ention than the le most or the right-most positions. is leads to a "T-shape" xation distribution instead of the "F-shape" one (or "Golden Triangle") as described in most existing general Web search studies [13].

278

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

 

  
  

   
    
 
           
       
 
(a)   
  

        
          


   

            
    
      
     
Duration (a)
  
    

     
 

            

(b)
Figure 3: First arrival time distribution(s) in rst page view (a) and global view (b)

(b)
Figure 4: Examination duration distribution (s) in rst page view (a) and global view(b)

In the vertical direction, we can see from Figures 4(a) and 4(b) that the rst row and rst page a ract more user a ention than the other positions. Generally, the xation duration decreases with the row number or page number.
4.4 Statistical Modeling of Middle-position Bias
In this section, we statistically verify the observed middle-position bias in Figures 3 and 4 by analyzing the relation between the location of an image and gaze behaviors using a linear mixed model.
e mixed model is a statistical model introduced by Bryk and Raudenbush [25]. e model contains both xed e ects and random e ects, and they are particularly apt for multiple observations that are gathered over time on a set of people. e experiment was conducted with a set of 40 participants performing 20 search tasks, so we chose a linear mixed model, which is suitable for this data set size.
More speci cally, we consider the e ect of placing an image in the center columns (Columns 2, 3, and 4) on the rst arrival time of the gaze. e dependent variable is " rst arrival time," and the predictor variable is whether an image is placed in the center columns or on the sides (Columns 1 and 5). e random intercepts for both user and task are signi cant with p < 2.2×10-16

each. erefore, we chose a linear mixed model with the user and task controlled as random e ects. Testing the rst arrival time with both the user and task controlled as random e ects, the di erence between the center columns and the sides is signi cant with p < 2.2 × 10-16. Hence the statistical model suggests that users tend to look rst at pictures placed in the center columns and then at ones on the sides.
Similarly, we consider the e ect on the xation duration of gaze. e dependent variable is " xation duration," and the predictor variable is whether an image is placed in the center columns or on the sides. e random intercepts for both user and task are signi cant with p < 2.2 × 10-16 each. Hence, we chose a linear mixed model with the user and task controlled as random e ects as well. Testing the duration with both the user and task controlled as random e ects, the di erence is signi cant with p < 2.2 × 10-16. erefore, the statistical model shows that the xation duration tends to be longer for pictures placed in the center columns than for those on the sides. In conclusion, eye gaze behaviors are related to the location of an image, and placing an image in the middle columns has a signi cant impact on both the rst arrival time and the xation

279

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: Eye shi probability from images at Le column (a), middle column (b) and right column (c) to other images. Number 1 to 10 mean the distance (d) from image i to image j which de nes according to the Equation 1. We only reserve the probability larger than 0.0001

Down Lower-right
Right Upper-right
Up

1 0.1137 0.0769 0.2187 0.0428 0.0696

2 0.0085 0.0693 0.1302 0.0458 0.0060

3 0.0006 0.0348 0.0685 0.0214 0.0015

4 0.0002 0.0101 0.0205 0.0093 0.0010

(a)
5 0.0001 0.0019 0.0043 0.0026 0.0003

6 0.0 0.0 0.0006 0.0005 0.0003

7 0.0 0.0001 0.0 0.0002 0.0002

8 0.0001
0.0 0.0 0.0001 0.0

9 0.0 0.0002 0.0 0.0002 0.0003

10 0.0 0.0 0.0 0.0001 0.0001

>10 0.0044 0.0334 0.0001 0.0006 0.0001

Upper-le Le
Lower-le Down
Lower-right Right
Upper-right Up

1 0.0289 0.1918 0.0567 0.0902 0.0518 0.2143 0.0296 0.0541

2 0.0150 0.0497 0.0303 0.0128 0.0266 0.0570 0.0167 0.0086

3 0.0043 0.0080 0.0074 0.0019 0.0060 0.0095 0.0049 0.0024

4 0.0012 0.0013 0.0014 0.0004 0.0012 0.0013 0.0013 0.0010

(b)
5 0.0004 0.0001 0.0002 0.0002 0.0001 0.0001 0.0003 0.0007

6 0.0004
0.0 0.0001
0.0 0.0001
0.0 0.0004 0.0004

7 0.0002
0.0 0.0 0.0001 0.0 0.0 0.0002 0.0003

8 0.0001
0.0 0.0 0.0 0.0 0.0 0.0001 0.0003

9 0.0001
0.0 0.0001
0.0 0.0 0.0 0.0001 0.0002

10 0.0002
0.0 0.0 0.0 0.0 0.0 0.0001 0.0001

>10 0.0004
0.0 0.0019 0.0017 0.0021 0.0001 0.0003 0.0003

Upper-le Le
Lower-le Down Up

(c)

1

2

3

4

5

6

7

8 9 10 >10

0.0329 0.0337 0.0193 0.0066 0.0022 0.0008 0.0007 0.0001 0.0 0.0001 0.0009

0.1966 0.1024 0.0662 0.0194 0.0030 0.0007 0.0 0.0 0.0 0.0 0.0

0.0978 0.1110 0.0641 0.0182 0.0052 0.0003 0.0001 0.0001 0.0 0.0 0.0039

0.1139 0.0161 0.0016 0.0002 0.0001 0.0001 0.0 0.0 0.0 0.0001 0.0018

0.0660 0.0082 0.0023 0.0004 0.0005 0.0009 0.0 0.0 0.0 0.0 0.0001

duration. ese results agree with the observed middle-position bias in Section 4.2 and 4.3.

4.5 Transition Probabilities of Eye Fixation
Di erent from Web search results, the display of image search results is two-dimensional, so the transitions of xation positions are in eight directions (upper le , up, upper right, right, lower right, down, lower le and le ). To look into the eye movement pa erns between di erent image search results, we show the transition probabilities for results in di erent directions and distances in Table 3. We also compare the di erences of results in di erent positions (le -most column, right-most column and middle-column). Here, we de ne the distance (d) from image i to image j as:

d = max (abs[Xj - Xi ], abs[Yj - Yi ])

(1)

Where Xi and Xj are row numbers of image i and image i while Yi and Yj are column numbers. e experimental data suggests the following ndings.

· Most (44%) outgoing transitions from images located at the le column goes to the right while 23% goes lower right, 13% goes down, 12% goes upper right, and 8% goes up.
· Most (28%) outgoing transitions from images in the middle column goes to the right while 25% goes le , 11% goes

down, 10% goes lower le , 10% goes lower right, 7% goes up, 5% goes upper right, and 4% goes upper le . · Most (39%) outgoing transitions from images in the right column goes to the le , 30% goes lower le , 13% goes down, 10% goes upper le , and 8% goes up. · In every direction, the transition probability decreases monotonically as the distance from the original image result increases.
From the transition probability, we infer that no ma er where users' xations are, eyes tend to move horizontally rather than vertically or diagonally. Results show a tendency toward a "near by principle" in which users prefer short-distance saccades than long-distance ones. ese ndings accord with existing researches in the mechanisms of eye movements [29].
5 IMAGE CONTENT AND EXAMINATION
Besides the position factor, we also want to investigate the in uence of result content in examination. In many existing researchers, content of images have been shown to have impacts in people's a ention allocation mechanisms. erefore, we also want to look into the correlations between image content features and users' examination behavior including xation duration and number of durations.

280

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 4: Image content features

Feature Name Saliency Edge Face
Text

Feature Description Sum, mean and max of the given element.
Density of the given element. e number of human faces, the ratio of face
areas. e number of texts, the ratio of text areas. (a Chinese character or an English word
contributes for one text)

Table 5: Correlation between image content feature and examination behavior. *(or **) indicates the di erence is signi cant at p < 0.05 (p < 0.01)

(coe cient, p-value)
Sum Saliency Mean Saliency Max Saliency Edge Density
Face num e ratio of face areas
Text num e ratio of text areas

Examination duration 0.4842, ** 0.3442, ** 0.2380,**
0.1394,0.0532 0.0441,0.6731 0.0324,0.7559 0.1815,0.1889 -0.0237,0.8648

Examination times
0.5015, ** 0.3589, ** 0.2583,** 0.1375,0.0566 0.0461,0.6589 -0.0554,0.5956 0.0014,0.9919 -0.1801,0.1926

Image content features have been thoroughly surveyed in existing works. In [18], visual saliency features are demonstrated to signi cantly improve the success of examination prediction. Lowdimensional features like edge density are also found to be maximally informative features [4]. High-dimensional features like face and textual information are also widely used in image retrieval [27]. In this paper, we study four static image content features elaborated in Table 4. e GBVS algorithm [12] is applied to generate the visual saliency map. We use an edge detection algorithm introduced by Canny [6] to determine the boundaries of items (e.g., buildings) in the images. One of the authors manually labeled whether an image contains human face or textual information and use the labeling results as the features as well.
Given a list of image, we can extract static features from each image based on the method above. Further, each image's xation duration and number of xations are also collected by the eyetracking devices. We use the Spearman's correlation coe cient which is a nonparametric measure of rank correlation used to assess the relation between image content features and users' examination behavior. e results are shown in Table 5 which shows that saliency has the highest correlation with xation behavior in the sum, mean, and max of a given element, followed by the edge density feature. However, face features and the textual features are not closely related to the examination behavior. e reason why the high-dimensional features of faces and text have less in uence on users' xation may be that the number of faces and amount of text contained in image search results are limited, and sometimes they are auxiliary. For example, most texts in keyword-based image search results are watermarks rather than meaningful information.

Figure 5: Examination duration boxplots for results with different relevance labeling scores
6 RELEVANCE, CLICK AND EXAMINATION
In the rst stage of our experiment, we obtained participants' examination data and mouse behavior information including "click," "move," and "scroll" recorded by our built-in designed JavaScript. We conducted relevance labeling in the second stage. us, we obtained a feature space for selected images including examination duration, revisit behavior, click, and relevance score, which enabled us to investigate the relations between examination, click behavior, and relevance in our task. In this paper, We use pairwise T-test to verify the signi cance of di erences between the di erent features and report the p-value.
6.1 Examination duration and Relevance
For each image in a task, we used a tuple with three elements (row, column, and duration) to describe it in an examination. An image has two or more tuples when there exist more than one
xations on it. We calculate the sum of examination duration for each image in the result panel if the image has two or more tuples. Based on the second stage of our experiment, we obtained a list of images with relevance scores. We draw a boxplot to illustrate the relation between examination duration and relevance in Figure 5, and perform a t-test between two of three image sets with di erent scores. Figure 5 shows that the mean examination duration of images with score 3 or score 2 is longer than images with score 1 (p-value < 0.01) while the di erence between image set with score 2 and image set with score 3 is not signi cant. e above results show that when an image has a longer examination duration, it is more likely to be relevant, which may be labeled as "fairly relevant" or "very relevant". us, users' examination duration can be considered as a sign of image relevance, which implies that longer examination duration has a strong correlation with relevance.

281

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan



     


     

   
   

   
  

    
   














(a)

     

 
   
 



  
  
 

     













(b)

Figure 6: Click through rate distribution in rst page view (a) and global view (b)

6.2 Revisit and Relevance
A revisit behavior in a task means that a user goes back to the previously examined image a er examining another image. Here, we calculate the average scores of two groups of images (revisited and not revisited) e average score of revisited images in our task is 2.60 and higher than that of un-revisited ones (2.49). With paired two-tailed t-test, revisit behavior is signi cantly di erent between "relevant" and "irrelevant" (p-value < 0.05 for score 1 and score 2, and p-value < 0.01 for score 1 and score 3). is nding shares the same perspective in [36]. Xu et al. [36] shows that 50.2% of the revisited items in general web search have a high relevance.
6.3 Click and Relevance
Previous studies in image search [22, 28] indicate that image search click-through data is considerably more accurate in general than document-based search click-through data and can be used to boost the performance of an image search re-ranking system [15]. erefore, based on the built-in JavaScript in our experimental platform, we investigate users' click behavior. Corresponding results are illustrated in Figure 6. Figure 6(a) shows the click-through rate for the rst page. In contrast to the rst arrival time and examination

duration distribution, there is no noticeable middle-position bias on the rst page. Furthermore, the four result pages overviewed in Figure 6(b) indicate that more clicks are observed on the rst page and the le column of the second page, which is di erent from the middle position bias in examination behaviors shown in Figure 3(b) and Figure 4(b). Please be noted that if one of the images whose center positions are within the grid is clicked, then the grid receive one click.
e di erences between click behavior and examination behavior ( rst arrival time, examination duration, and examination distribution) motivated us to further investigate the relation among click, examination, and relevance.
In this section, we address the two questions in RQ3: (1) Does examination hypothesis still hold in image search? (2) What is the relation between click, examination, and relevance? or will an image be clicked if it is examined and relevant with search target in image searches?
Examination hypothesis [8] assumes that a document being clicked (Ci = 1) accords with two conditions which are independent from each other: it is examined (Ei = 1) and it is relevant (Ri = 1). It is widely applied in Web search related studies such as click model constructions [10, 32]. Following this assumption, the probability of a document being clicked is determined as follows:

P (Ci = 1) = P (Ei = 1)P (Ri = 1)

(2)

According to the examination hypothesis, if an image in image

searches is examined and relevant, then it will be clicked. We deem

the images to be relevant if they obtain score 2 or score 3 and regard

the images that have examination duration longer than 200ms as

examined. If an image satis es the examination hypothesis, it needs

to satisfy any one of the following conditions: (1) it is relevant,

examined and clicked; (2) it is not relevant and not clicked; (3) it is

not examined and not clicked. Based on the experimental data, for

each participant, we calculate the proportion of images that satisfy

examination hypothesis. Result shows that 53% of images satisfy

examination hypothesis while other 47% do not. us, in image

search engine where results are shown in a completely di erent

way with general Web search engines, examination hypothesis may

not still hold.

To further investigate the relations among click, examination,

and relevance, and to determine why the examination hypothesis

is not applicable in image search, we calculated a confusion ma-

trix that shows the relation between click (C) and relevance score,

shown in Table 6. Because few images are not examined (6.7%), we

only illustrate the results of examined images. We see that irrelevant

images receive almost none click, which is consistent with the result

in a general search. However, the probability of one relevant result

being clicked is also very low ((4.08+4.84)%/(6.94+60.88)%=13.15%),

which is not true according to Equation 2. is phenomenon may

be caused by the fact that there are many similar images in the

result panels in both the visual sense and content (for example,

4 out of 6 images in the rst row of Figure 1 looks quite similar

with each other). Sometimes there will be many relevant results.

In contrast to general Web search results, image search results are

self-contained, so that, users do not need to click the document as in

a Web search to view the landing page in general. Instead, they can

observe several images before deciding which ones to download

or to click to see the larger version. erefore, there is no need to

282

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 6: Relation between click and relevance score given images are examined

E=1 Score=1 Score=2 Score=3

C=1 0.21% 4.08% 4.84%

C=0 26.72% 6.94% 60.88%

click each relevant image (many of them look quite similar with each other) to collect the required information and examination hypothesis doesn't hold any more here.
7 EXAMINATION BEHAVIOR PREDICTION
rough the eye-tracking study introduced in Sections 4-6, we obtained several interesting ndings on user behavior biases including a middle-position bias in the user examination process and correlation biases between visual features and user examination behavior. As presented in Section 6, examination behavior can be an implicit feedback for content relevance. erefore, predicting users' examination behavior based on static features that can be obtained o ine is meaningful for search page optimization and UI evaluation.
7.1 Features and Model
As Table 5 illustrates, saliency and edge features play important roles in a ecting the users' examination behavior. We extracted the visual saliency and edge density of images in our task into the feature space, which also contains image position (row, column) used to incorporate position bias into our prediction model. Because the rst page receives more user examination as shown in Figure 4(b), the images in the rst page are selected to construct our dataset which contains around 7,000 triples (participants,tasks and images). We set the input of the prediction model to be a combination of di erent features, and we compared the root mean square error (RMSE) of predictions by di erent feature combinations in Section 7.2. e prediction model outputs the probability of an image being examined, while the actual examination was obtained from the eye-tracking device as ground truth.
In this study, we compare di erent combinations of position and visual features. Our baseline feature group contains only the position feature (PF). Position bias is modeled in most existing click modeling e orts, so we use it as a reference to track how much improvement can be achieved by our method. Feature group SF involves only saliency features, and we use an index to mark which measurement (1-Sum, 2-Mean, 3-Max) it belongs. In feature group EF we only consider the edge density of images.
Given an image with a combination of features, our task is to predict its examination probability, which can be treated as a regression problem. In this paper, we apply a gradient boosting regression tree (GBRT) for the examination prediction tasks as in [18]. e prediction results of di erent combinations are compared with a 10-fold cross validation, and we report the average performance on the test folds.

Table 7: Examination prediction performance of di erent features groups

PF
SF1 PF+SF1 PF+SF2 PF+SF3 PF+EF
PF+SF+EF

RMSE 0.1200 0.1138 0.1131 0.1126 0.1124 0.1117 0.1098

p-value -
0.0904 0.0279 0.0296 0.0070 0.0332 0.0388

7.2 Prediction results and Discussions
We report RMSE for comparison, and a two-tailed t-test was performed to detect signi cant changes in the RMSE of prediction with position feature. Table 7 illustrates the corresponding results.
As Table 7 shows, adding visual saliency or edge information to a position-based feature (PF) can improve the RMSE of prediction. We nd that PF+SF1, PF+SF2, PF+SF3, and PF+EF signi cantly (with paired two-tailed t-test) outperform PF with p-value < 0.05 while PF+SF3 has p-value < 0.01. Further, when we combine all features PF+SF+EF, the prediction model outperforms other combinations with p-value < 0.05. us, we can infer that combining position and visual features boosts the prediction performance in image searches. What is noteworthy is that as the features used in our prediction model are all static and can be calculated o ine, they can be quite valuable for situations lacking users' behavior information.
8 CONCLUSIONS AND FUTURE WORK
In this paper, we carry out a lab-based user study with the help of eye-tracking devices in image search and obtain three interesting
ndings. (1) Based on rst arrival time and examination duration analysis, We observe a middle-position bias of user's examination behavior, and we also apply a linear mixed model to justify the middle-position bias is signi cant statistically. Furthermore, we
nd that users are "lazy" in image search, they prefer small amplitude and short distance transitions between images. (2) Besides the position factor, visual features including saliency and edge density are demonstrated to have stronger correlation with user's examination behavior than high-dimensional features like the existences of human faces and textual features. (3) We investigate the relation between click, examination and relevance and nd that examination duration is an useful implicit feedback for relevance of query-image pairs in image search. Also, as image results are self-contained, users do not need to click a document as in a Web search to view the landing page which results in the failure of examination hypothesis in image search scenarios.
Besides user study, we also perform an examination prediction experiment. Results show that combining position and visual features which can be calculated o ine improves the prediction performance in image searches.
e research outputs of this paper are meaningful to inform the future image searches. For example, as there exists a middleposition bias of user's examination behavior, placing the most relevant results in the middle of each row rather than the le most positions may improve the users' satisfaction.

283

Session 3A: Search Interaction 2

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Our study makes a rst step towards user's examination behavior analysis in image search. Interesting directions for future work include interaction behavior analysis result in preview pages (which is similar with but not completely the same as landing pages in general Web search). Moreover, we also plan to investigate user behavior in image searches on mobile devices and perform a comparison with PC based image searches.
ACKNOWLEDGMENTS
is work was supported by Natural Science Foundation of China (Grant No. 61622208, 61532011, 61672311), Tsinghua University Initiative Scienti c Research Program(2014Z21032), National Key Basic Research Program (2015CB358700). Wu was supported in part by NIH grant 5U54CA193313.
REFERENCES
[1] Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving web search ranking by incorporating user behavior information. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 19­26.
[2] Paul Andre´, Edward Cutrell, Desney S Tan, and Greg Smith. 2009. Designing novel image search interfaces by understanding unique characteristics and usage. In IFIP Conference on Human-Computer Interaction. Springer, 340­353.
[3] Ioannis Arapakis and Luis A Leiva. 2016. Predicting user engagement with direct displays using mouse cursor information. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 599­608.
[4] Roland J Baddeley and Benjamin W Tatler. 2006. High frequency edges (but not contrast) predict where we xate: A Bayesian system identi cation analysis. Vision research 46, 18 (2006), 2824­2833.
[5] Georg Buscher, Edward Cutrell, and Meredith Ringel Morris. 2009. What do you see when you're sur ng?: using eye tracking to predict salient regions of web pages. In Proceedings of the SIGCHI conference on human factors in computing systems. ACM, 21­30.
[6] John Canny. 1986. A computational approach to edge detection. IEEE Transactions on pa ern analysis and machine intelligence 6 (1986), 679­698.
[7] Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li. 2008. Context-aware query suggestion by mining click-through and session data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 875­883.
[8] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 87­94.
[9] Edward Cutrell and Zhiwei Guan. 2007. What are you looking for?: an eyetracking study of information usage in web search. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 407­416.
[10] Georges E Dupret and Benjamin Piwowarski. 2008. A user browsing model to predict search engine click data from past observations.. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 331­338.
[11] Abby Goodrum and Amanda Spink. 1999. Visual Information Seeking: A Study of Image eries on the World Wide Web.. In Proceedings of the ASIS Annual Meeting, Vol. 36. ERIC, 665­74.
[12] Jonathan Harel, Christof Koch, and Pietro Perona. 2006. Graph-based visual saliency. In Advances in neural information processing systems. 545­552.
[13] Gord Hotchkiss, Steve Alston, and Greg Edwards. 2005. Eye tracking study. Research white paper, Enquiro Search Solutions Inc (2005).
[14] Xian-Sheng Hua, Linjun Yang, Jingdong Wang, Jing Wang, Ming Ye, Kuansan Wang, Yong Rui, and Jin Li. 2013. Clickage: towards bridging semantic and intent gaps via mining click logs of search engines. In Proceedings of the 21st ACM international conference on Multimedia. ACM, 243­252.
[15] Vidit Jain and Manik Varma. 2011. Learning to re-rank: query-dependent image re-ranking using click data. In Proceedings of the 20th international conference on World wide web. ACM, 277­286.
[16] Jyun-Yu Jiang, Yen-Yu Ke, Pao-Yu Chien, and Pu-Jen Cheng. 2014. Learning user reformulation behavior for query auto-completion. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 445­454.
[17] Yanen Li, Anlei Dong, Hongning Wang, Hongbo Deng, Yi Chang, and ChengXiang Zhai. 2014. A two-dimensional click model for query auto-completion.

In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 455­464. [18] Yiqun Liu, Zeyang Liu, Ke Zhou, Meng Wang, Huanbo Luan, Chao Wang, Min Zhang, and Shaoping Ma. 2016. Predicting Search User Examination with Visual Saliency. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 619­628. [19] Yiqun Liu, Chao Wang, Ke Zhou, Jianyun Nie, Min Zhang, and Shaoping Ma. 2014. From skimming to reading: A two-stage examination model for web search. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 849­858. [20] Neil O'Hare, Paloma de Juan, Rossano Schifanella, Yunlong He, Dawei Yin, and Yi Chang. 2016. Leveraging user interaction signals for web image search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 559­568. [21] Bing Pan, Helene A Hembrooke, Geri K Gay, Laura A Granka, Ma hew K Feusner, and Jill K Newman. 2004. e determinants of web page viewing behavior: an eye-tracking study. In Proceedings of the 2004 symposium on Eye tracking research & applications. ACM, 147­154. [22] Yingwei Pan, Ting Yao, Tao Mei, Houqiang Li, Chong-Wah Ngo, and Yong Rui. 2014. Click-through-based cross-view learning for image search. In Proceedings
of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 717­726. [23] Jaimie Y Park, Neil O'Hare, Rossano Schifanella, Alejandro Jaimes, and Chin-Wan Chung. 2015. A large-scale study of user image search behavior on the web. In
Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 985­994. [24] Hsiao-Tieh Pu. 2005. A comparative analysis of web image and textual queries. Online Information Review 29, 5 (2005), 457­467. [25] Stephen W Raudenbush and Anthony S Bryk. 2002. Hierarchical linear models: Applications and data analysis methods. Vol. 1. [26] Sara Shatford. 1986. Analyzing the subject of a picture: a theoretical approach. Cataloging & classi cation quarterly 6, 3 (1986), 39­62. [27] Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Transactions on pa ern analysis and machine intelligence 22, 12 (2000), 1349­1380. [28] Gavin Smith and Helen Ashman. 2009. Evaluating implicit judgements from image search interactions. (2009). [29] Benjamin W Tatler and Benjamin T Vincent. 2009. e prominence of behavioural biases in eye guidance. Visual Cognition 17, 6-7 (2009), 1029­1054. [30] Dian Tjondronegoro, Amanda Spink, and Bernard J Jansen. 2009. A study and comparison of multimedia Web searching: 1997­2006. Journal of the American Society for Information Science and Technology 60, 9 (2009), 1756­1768. [31] Geo rey Underwood and Tom Foulsham. 2006. Visual saliency and semantic incongruency in uence eye movements when inspecting pictures. e arterly journal of experimental psychology 59, 11 (2006), 1931­1949. [32] Chao Wang, Yiqun Liu, Meng Wang, Ke Zhou, Jian-yun Nie, and Shaoping Ma. 2015. Incorporating non-sequential behavior into click models. In Proceedings
of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 283­292. [33] Kuansan Wang, Nikolas Gloy, and Xiaolong Li. 2010. Inferring search behaviors using partially observable Markov (POM) model. In Proceedings of the third ACM international conference on Web search and data mining. ACM, 211­220. [34] Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, and Qiaozhu Mei. 2016. Beyond ranking: Optimizing whole-page presentation. In
Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, 103­112. [35] Wei Wu, Hang Li, and Jun Xu. 2013. Learning query and document similarities from click-through bipartite graph with metadata. In Proceedings of the sixth ACM international conference on Web search and data mining. ACM, 687­696. [36] Danqing Xu, Yiqun Liu, Min Zhang, Shaoping Ma, and Liyun Ru. 2012. Incorporating revisiting behaviors into click models. In Proceedings of the h ACM international conference on Web search and data mining. ACM, 303­312. [37] Yongdong Zhang, Xiaopeng Yang, and Tao Mei. 2014. Image search reranking with query-dependent click-based relevance feedback. IEEE transactions on image processing 23, 10 (2014), 4448­4459.

284


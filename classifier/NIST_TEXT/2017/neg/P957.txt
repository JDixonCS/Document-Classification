Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

AutoSVD++: An E icient Hybrid Collaborative Filtering Model via Contractive Auto-encoders

Shuai Zhang
University of New South Wales Sydney, NSW 2052, Australia shuai.zhang@student.unsw.edu.au

Lina Yao
University of New South Wales Sydney, NSW 2052, Australia
lina.yao@unsw.edu.au

Xiwei Xu
Data61, CSIRO Sydney, NSW 2015, Australia
Xiwei.Xu@data61.csiro.au

ABSTRACT
Collaborative ltering (CF) has been successfully used to provide users with personalized products and services. However, dealing with the increasing sparseness of user-item matrix still remains a challenge. To tackle such issue, hybrid CF such as combining with content based ltering and leveraging side information of users and items has been extensively studied to enhance performance. However, most of these approaches depend on hand-cra ed feature engineering, which is usually noise-prone and biased by di erent feature extraction and selection schemes. In this paper, we propose a new hybrid model by generalizing contractive auto-encoder paradigm into matrix factorization framework with good scalability and computational e ciency, which jointly models content information as representations of e ectiveness and compactness, and leverage implicit user feedback to make accurate recommendations. Extensive experiments conducted over three large-scale real datasets indicate the proposed approach outperforms the compared methods for item recommendation.
CCS CONCEPTS
·Information systems Recommender systems;
KEYWORDS
collaborative ltering; deep learning; contractive auto-encoders
1 INTRODUCTION
With the increasing amounts of online information, recommender systems have been playing more indispensable role in helping people overcome information overload, and boosting sales for ecommerce companies. Among di erent recommendation strategies, Collaborative Filtering (CF) has been extensively studied due to its e ectiveness and e ciency in the past decades. CF learns user's preferences from usage pa erns such as user-item historical interactions to make recommendations. However, it still has limitation in dealing with sparse user-item matrix. Hence, hybrid methods have been gaining much a ention to tackle such problem by combining content-based and CF-based methods [7].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080689

However, most of these approaches are either relying handcra ed advanced feature engineering, or unable to capture the nontriviality and non-linearity hidden in interactions between content information and user-item matrix very well. Recent advances in deep learning have demonstrated its state-of-the-art performance in revolutionizing recommender systems [4], it has demonstrated the capability of learning more complex abstractions as e ective and compact representations in the higher layers, and capture the complex relationships within data. Plenty of research works have been explored on introducing deep learning into recommender systems to boost the performance [2, 9, 10, 12]. For example, Salakhutdinov et al. [9] applies the restricted Boltzmann Machines (RBM) to model dyadic relationships of collaborative ltering models. Li et al. [6] designs a model that combines marginalized denoising stacked auto-encoders with probabilistic matrix factorization.
Although these methods integrate both deep learning and CF techniques, most of them do not thoroughly make use of side information (e.g., implicit feedback), which has been proved to be e ective in real-world recommender system [3, 7]. In this paper, we propose a hybrid CF model to overcome such aforementioned shortcoming, AutoSVD++, based on contractive auto-encoder paradigm in conjunction with SVD++ to enhance recommendation performance. Compared with previous work in this direction, our contributions of this paper are summarized as follows:
· Our model naturally leverages CF and auto-encoder framework in a tightly coupled manner with high scalability. e proposed e cient AutoSVD++ algorithm can signi cantly improve the computation e ciency by grouping users that shares the same implicit feedback together;
· By integrating the Contractive Auto-encoder, our model can catch the non-trivial and non-linear characteristics from item content information, and e ectively learn the semantic representations within a low-dimensional embedding space;
· Our model e ectively makes use of implicit feedback to further improve the accuracy. e experiments demonstrate empirically that our model outperforms the compared methods for item recommendation.
2 PRELIMINARIES
Before we dive into the details of our models, we rstly discuss the preliminaries as follows.
2.1 Problem De nition
Given user u = [1, ..., N ] and item i = [1, ..., M], the rating rui  R  RN ×M is provided by user u to item i indicating user's preferences on items, where most entries are missing. Let ru^i denote the

957

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

predicted value of rui , the set of known ratings is represented as K = {(u, i)|rui is known}. e goal is to predict the ratings of a set
of items the user might give but has not interacted yet.

2.2 Latent Factor Models

2.2.1 Biased SVD. Biased SVD [5] is a latent factor model, unlike conventional matrix factorization model, it is improved by

introducing user and item bias terms:

ru^i = µ + bu + bi + ViT Uu

(1)

where µ is the global average rating, bu indicates the observed deviations of user u, bi is the bias term for item i, Uu  Rk and Vi  Rk represent the latent preference of user u and latent property of item i respectively, k is the dimensionality of latent factor.

2.2.2 SVD++. SVD++ [5] is a variant of biased SVD. It extends the biased SVD model by incorporating implicit information. Generally, implicit feedback such as browsing activity and purchasing history, can help indicate user's preference, particular when explicit feedback is not available. Prediction is done by the following rule:

ru^ i

=

µ

+ bu

+ bi

+ ViT (Uu

+

|

N

(u

)|-

1 2

j)

(2)

j N (u)

where j  Rf is the implicit factor vector. e set N (u) contains the items for which u provided implicit feedback, N (u) can be replaced by R(u) which contains all the items rated by user u [7], as
implicit feedback is not always available. e essence here is that

users implicitly tells their preference by giving ratings, regardless of

how they rate items. Incorporating this kind of implicit information

has been proved to enhance accuracy [5]. is model is exible to be

integrated various kinds of available implicit feedback in practice.

2.3 Contractive Auto-encoders
Contractive Auto-encoders (CAE) [8] is an e ective unsupervised learning algorithm for generating useful feature representations.
e learned representations from CAE are robust towards small perturbations around the training points. It achieves that by using the Jacobian norm as regularization:

cae ( ) =

(L(x, (f (x))) + 

f (x)

2 F

)

(3)

x Dn

where x  Rdx is the input, Dn is the training set, L is the recon-

struction error, the parameters  = W ,W , bh, b , (f (x)) is the reconstruction of x, where:

(f (x)) = s (W sf (W x + bh ) + b )

(4)

sf is a nonlinear activation function, s is the decoder's activation function, bh  Rdh and b  Rdx are bias vectors, W  Rdh ×dx and W  Rdh ×dx are weight matrixes, same as [8], we de ne
W = W . e network can be trained by stochastic gradient descent

algorithm.

3 PROPOSED METHODOLOGY
In this section, we introduce our proposed two hybrid models, namely AutoSVD and AutoSVD++, respectively.

Figure 1: Illustration of AutoSVD (remove the implicit feedback) and AutoSVD++.

3.1 AutoSVD

Suppose we have a set of items, each item has many properties or side information, the feature vector of which can be very highdimensional or even redundant. Traditional latent factor model like SVD is hard to extract non-trivial and non-linear feature representations [12]. Instead, we propose to utilize CAE to extract compact and e ective feature representations:

cae(Ci ) = sf (W · Ci + bh )

(5)

where Ci  Rdc represents the original feature vector, cae(Ci )  Rk denotes the low-dimensional feature representation. In order to

integrate the CAE into our model, the proposed hybrid model is

formulated as follows:

ru^i = µ + bu + bi + ( · cae(Ci ) + i )T Uu

(6)

Similar to [11], we divide item latent vector Vi into two parts, one is the feature vector cae(Ci ) extracted from item-based content information, the other part i  Rk (i = 1...n) denotes the latent item-based o set vector.  is a hyper-parameter to normalize cae(Ci ) . We can also decompose the user latent vector in a similar
way. However, in many real-world systems, user's pro les could

be incomplete or unavailable due to privacy concern. erefore, it

is more sensible to only include items side information.

3.2 AutoSVD++

While the combination of SVD and contractive auto-encoders is capable of interpreting e ective and non-linear feature representations, it is still unable to produce satisfying recommendations with sparse user-item matrix. We further propose a hybrid model atop contractive auto-encoders and SVD++ , which takes the implicit feedback into consideration for dealing with sparsity problem. In many practical situations, recommendation systems should be centered on implicit feedback [3]. Same as AutoSVD, we decompose the item latent vectors into two parts. AutoSVD++ is formulated as the following equation:

ru^ i

=

µ +bu

+bi

+ ( ·cae(Ci ) +i )T (Uu

+

|N

(u

)

|

-

1 2

j ) (7)

j N (u)

Figure 1 illustrates the structure of AutoSVD and AutoSVD++.

958

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

3.3 Optimization
We learn the parameters by minimizing the regularized squared error loss on the training set:

min

(rui - ru^i )2 +  · fr e

(8)

b, ,U ,  (u,i)K

where fr e is the regularization terms to prevent over ing. e fr e for AutoSVD++ is as follows:

fr e = bu2 + bi2 + i 2 + Uu 2 +

2

j

(9)

j N (u)

e regularization for AutoSVD is identical to AutoSVD++ with the implicit factor j removed.
In this paper, we adopt a sequential optimization approach. We rst obtain the high-level feature representations from CAE, and then integrated them into the AutoSVD and AutoSVD++ model. An alternative optimization approach, which optimizes CAE and AutoSVD (AutoSVD++) simultaneously, could also apply [6]. However, the later approach need to recompute all the item content feature vectors when a new item comes, while in the sequential situation, item feature representations only need to be computed once and stored for reuse.
e model parameters are learned by stochastic gradient descent (SGD). First, SGD computes the prediction error:

eui d=ef rui - ru^i

(10)

then modify the parameters by moving in the opposite direction of the gradient. We loop over all known ratings in K. Update rules for AutoSVD are as follows:

bu  bu + 1(eui - 1 · bu )

(11)

bi  bi + 1(eui - 1 · bi )

(12)

i  i + 2(eui · Uu - 2 · i )

(13)

Uu  Uu + 2(eui · ( · cae(Ci ) + i ) - 2 · Uu ) (14)

Update rules for AutoSVD++ are:

i



i

+ 2(eui

·

(Uu

+

|N

(u

)

|

-

1 2

·

j ) - 2 · i ) (15)

j N (u)

j  N (u) :

j

j

+ 2(eui

·

|

N

(u

)|-

1 2

·

(

· cae(Ci )

+ i )

- 2

·

j)

(16)

Where 1 and 2 are the learning rates, 1 and 2 are the regularisation weights. Update rule for Uu of AutoSVD++ is identical to equation (14).

Although AutoSVD++ can easily incorporate implicit informa-

tion, it's very costly when updating the parameter . To accelerate

the training process, similar to [13], we devise an e cient training

algorithm, which can signi cantly decrease the computation time

of AutoSVD++ while preserving good performance. e algorithm

for AutoSVD++ is shown in Algorithm 1.

4 EXPERIMENTS
In this section, extensive experiments are conducted on three realworld datasets to demonstrate the e ectiveness of our proposed models.

Algorithm 1 E cient training algorithm for AutoSVD++

1: procedure U

P

2: for all user u do

3:

pim



|N

(u

)

|

-

1 2

·

j N (u) j

4:

pold  pim

5:

for all training samples of user u do

6:

upadate other parameters

7:

pim  pim + 2(eui · ( · cae(Ci ) + i ) - 2 · pim )

8:

end for

9:

for all i in items rated by u do

10:

i

i

+

|N

(u)|-

1 2

· (pim

- pold )

11:

end for

12: end for

13: end procedure

Table 1: Datasets Statistics

dataset

#items #users #ratings density(%)

MovieLens 100k 1682 943 100000 MovieLens 1M 3706 6040 1000209 MovieTweetings 27851 48852 603401

6.30 4.46 0.049

4.1 Experimental Setup
4.1.1 Dataset Description. We evaluate the performance of our AutoSVD and AutoSVD++ models on the three public accessible datasets. MovieLens1 is a movie rating dataset that has been widely used on evaluating CF algorithms, we use the two stable benchmark datasets, Movielens-100k and Movielens-1M. MovieTweetings[1] is also a new movie rating dataset, however, it is collected from social media, like twi er. It consists of realistic and up-to-date data, and incorporates ratings from twi er users for the most recent and popular movies. Unlike the former two datasets, the ratings scale of MovieTweetings is 1-10, and it is extremely sparse. e content information for Movielens-100K consists of genres, years, countries, languages, which are crawled from the IMDB website2. For Movielens-1M and Movietweetings, we use genres and years as the content information. e detailed statistics of the three datasets are summarized in Table 1.

4.1.2 Evaluation Metrics. We employ the widely used Root Mean Squared Error (RMSE) as the evaluation metric for measuring the prediction accuracy. It is de ned as

1

RMSE = |T |

(ru^i - rui )2

(17)

(u, i ) T

where |T | is the number of ratings in the testing dataset, ru^i denotes the predicted ratings for T , and rui is the ground truth.

4.2 Evaluation Results
4.2.1 Overall Comparison. Except three baseline methods including NMF, PMF and BiasedSVD, four very recent models closely relevant to our work are included in our comparison.

1h ps://grouplens.org/datasets/movielens 2h p://www.imdb.com

959

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Average RMSE for Movielens-100k and Movielens1M from compared models with di erent training data percentages.

Methods

ML-100K 90% 50%

Methods

ML-1M 90% 50%

NMF PMF NNMF(3HL) mSDA-CF Biased SVD SVD++ AutoSVD AutoSVD++

0.958 0.952 0.907
* 0.911 0.913 0.901 0.904

0.997 0.977
* 0.931 0.936 0.938 0.925 0.926

NMF PMF U-AutoRec RBM-CF Biased SVD SVD++ AutoSVD AutoSVD++

0.915 0.883 0.874 0.854 0.876 0.855 0.864 0.848

0.927 0.890 0.911 0.901 0.889 0.884 0.877 0.875

· RBM-CF [9], RBM-CF is a generative, probabilistic collaborative ltering model based on restricted Boltzmann machines.
· NNMF (3HL) [2], this model combines a three-layer feedforward neural network with the traditional matrix factorization.
· mSDA-CF [6] , mSDA-CF is a model that combines PMF with marginalized denoising stacked auto-encoders.
· U-AutoRec [10], U-AutoRec is novel CF model based on the autoencoder paradigm. Same as [10], we set the number of hidden units to 500.
We use the following hyper-parameter con guration for AutoSVD in this experiment, 1 = 2 = 0.01, 1 = 2 = 0.1,  = 0.1 . For AutoSVD++, we set 1 = 2 = 0.007, 1 = 0.005, 2 = 0.015, and  = 0.1. For all the comprison models, we set the dimension of latent factors k = 10 if applicable. We execute each experiment for
ve times, and take the average RMSE as the result. According to the evaluation results in Table 2 and Figure 2(a), our
proposed model AutoSVD and AutoSVD++ consistently achieve be er performance than the baseline and compared recent methods. On the ML-100K dataset, AutoSVD performs slightly be er than AutoSVD++, while on the other two datasets, AutoSVD++ outperforms other approaches.
4.2.2 Scalability. Figure 2(b) shows CPU time comparison in log scale. Compared with traditional SVD++ and Original AutoSVD++, our e cient training algorithm achieves a signi cant reduction in time complexity. Generally, the optimized AutoSVD++ performs R¯ times be er than original AutoSVD++, where R¯ denotes the average number of items rated by users[13]. Meanwhile, compared with biased SVD model, the incorporated items Cae(Ci ) and o set i does not drag down the training e ciency. is result shows our proposed models are easy to be scaled up over larger datasets without harming the performance and computational cost.
5 CONCLUSIONS AND FUTURE WORK
In this paper, we present two e cient hybrid CF models, namely AutoSVD and AutoSVD++. ey are able to learn item content representations through CAE, and AutoSVD++ further incorporates the implicit feedback. We devise an e cient algorithm for training

RMSE
log(t[ms])

1.5 1.49 1.48 1.47 1.46 1.45 1.44 1.43 1.42 1.41
1.4

Biased SVD SVD++ AutoSVD AutoSVD++
90%

Training Size

50%

(a)

12

11

10

Biased SVD

SVD++

9

AutoSVD Original AutoSVD++

Efficient AutoSVD++ 8

7

6

5

10

20

30

40

50

60

70

80

90

training size (%)

(b)

Figure 2: (a) Average RMSE Comparson on MovieTweetings Dataset (the lower the better). (b) Comparison of training time of one epoch on Movielens-100K (the lower the better).
AutoSVD++, which signi cantly speeds up the training process. We
conduct a comprehensive set of experiments on three real-world
datasets. e results show that our proposed models perform be er
than the compared recent works.
ere are several extensions to our model that we are currently
pursuing .
· First, we will leverage the abundant item content informa-
tion such as textual, visual information and obtain richer
feature representations through stacked Contractive Auto-
encoders; · Second, we can further improve the proposed model by
incorporating temporal dynamics and social network in-
formation.
REFERENCES
[1] Simon Dooms, Toon De Pessemier, and Luc Martens. 2013. MovieTweetings: a Movie Rating Dataset Collected From Twi er. In Workshop on Crowdsourcing and Human Computation for Recommender Systems, CrowdRec at RecSys 2013.
[2] Gintare Karolina Dziugaite and Daniel M Roy. 2015. Neural network matrix factorization. arXiv preprint arXiv:1511.06443 (2015).
[3] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative ltering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on. Ieee, 263­272.
[4] Alexandros Karatzoglou, Bala´zs Hidasi, Domonkos Tikk, Oren Sar-Shalom, Haggai Roitman, and Bracha Shapira. 2016. RecSys' 16 Workshop on Deep Learning for Recommender Systems (DLRS). In Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 415­416.
[5] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative ltering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 426­434.
[6] Sheng Li, Jaya Kawale, and Yun Fu. 2015. Deep collaborative ltering via marginalized denoising auto-encoder. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 811­820.
[7] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor. 2010. Recommender Systems Handbook. Springer-Verlag New York, Inc., NY, USA.
[8] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. 2011. Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th international conference on machine learning (ICML-11). 833­840.
[9] Ruslan Salakhutdinov, Andriy Mnih, and Geo rey Hinton. 2007. Restricted Boltzmann machines for collaborative ltering. In Proceedings of the 24th international conference on Machine learning. ACM, 791­798.
[10] Suvash Sedhain, Aditya Krishna Menon, Sco Sanner, and Lexing Xie. 2015. Autorec: Autoencoders meet collaborative ltering. In Proceedings of the 24th International Conference on World Wide Web. ACM, 111­112.
[11] Chong Wang and David M Blei. 2011. Collaborative topic modeling for recommending scienti c articles. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 448­456.
[12] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning for recommender systems. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1235­1244.
[13] Diyi Yang, Tianqi Chen, Weinan Zhang, Qiuxia Lu, and Yong Yu. 2012. Local implicit feedback mining for music recommendation. In Proceedings of the sixth ACM conference on Recommender systems. ACM, 91­98.

960


Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Characterizing and Predicting Enterprise Email Reply Behavior

Liu Yang1 Susan T. Dumais2 Paul N. Benne 2 Ahmed Hassan Awadallah2
1 Center for Intelligent Information Retrieval, University of Massachuse s Amherst, Amherst, MA, USA 2 Microso Research, Redmond, WA, USA
lyang@cs.umass.edu,{sdumais,pauben,hassanam}@microso .com

ABSTRACT
Email is still among the most popular online activities. People spend a signi cant amount of time sending, reading and responding to email in order to communicate with others, manage tasks and archive personal information. Most previous research on email is based on either relatively small data samples from user surveys and interviews, or on consumer email accounts such as those from Yahoo! Mail or Gmail. Much less has been published on how people interact with enterprise email even though it contains less automatically generated commercial email and involves more organizational behavior than is evident in personal accounts. In this paper, we extend previous work on predicting email reply behavior by looking at enterprise se ings and considering more than dyadic communications. We characterize the in uence of various factors such as email content and metadata, historical interaction features and temporal features on email reply behavior. We also develop models to predict whether a recipient will reply to an email and how long it will take to do so. Experiments with the publicly-available Avocado email collection show that our methods outperform all baselines with large gains. We also analyze the importance of different features on reply behavior predictions. Our ndings provide new insights about how people interact with enterprise email and have implications for the design of the next generation of email clients.
KEYWORDS
Email reply behavior; information overload; user behavior modeling
1 INTRODUCTION
Email remains one of the most popular online activities. Major email services such as Gmail, Outlook, and Yahoo! Mail have millions of monthly active users, many of whom perform frequent interactions like reading, replying to, or organizing emails. A recent survey shows that reading and answering emails takes up to 28% of enterprise workers' time, which is more than searching and gathering information (19%), communication and collaboration internally (14%), and second only to role speci c tasks (39%) [6]. Understanding and characterizing email reply behaviors can
Work primarily done during Liu Yang's internship at Microso Research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080782

To: Alice; Bob; Philip CC: James From: Jack
Subject: Meeting

Friday 11:51 PM High Importance

Hi Alice, Bob and Philip:

Could we have a meeting tomorrow to discuss a possible paper collaboration? In particular, I'd like to discuss a SIGIR'17 submission on email reply behavior prediction. See the a ached
le for some promising results.

ank you!

Jack SIGIR17 Experimental Results.pptx (304K) Predicted Reply Probability: 67% (Likely to receive a response) Predicted Reply Time Latency: 245 minutes (High)
Figure 1: A motivational email example with predicted reply probability and reply time latency.

improve communication and productivity by providing insights for the design of the next generation of email tools.
By modeling user reply behaviors like reply rate and reply time, we can integrate machine intelligence into email systems to provide value for both email recipients and senders. For email recipients, reply predictions could help lter emails that need replies or fast replies, which can help reduce email overload [9]. For email senders, the reply behaviors could be predicted in advance during email composition. More generally, be er reply strategies could lead to improved communication e ciency. Figure 1 shows a motivating email example with predicted reply probability and reply time latency shown in the bo om panel. Speci c features could also be highlighted. For example, identifying a request in the email (Could we have a meeting...) could improve automated triage for the recipient by highlighting that a reply is needed; or alerting the sender that a reply is likely to take longer if it is sent late at night or over the weekend could improve communication e ciency.
Previous work investigated strategies that people use to organize, reply to, or delete email messages [9, 10, 22, 29]. However, those studies are based on relatively small surveys or interviews. Some recent research proposes frameworks for studying user actions on emails with large scale data [11, 20]. Both of these studies are based on consumer emails from Yahoo! Mail. Enterprise email has received li le a ention compared to consumer email even though several studies have shown that enterprise email usage is not the same as consumer email usage. For example, [25] reports that enterprise users send and receive twice as much emails as consumer users and [15] shows that consumer email is now dominated by machine-generated messages sent from business and social networking sites.
Perhaps the closest prior research to our work is the study on email reply behavior in Yahoo! Mail by Kooti et al. [20]. However, they focus on personal email and only consider a subset of email exchanges, speci cally those from dyadic (one-to-one) email conversations for pairs of users who have exchanged more than 5 prior

235

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

emails in consumer email. Focusing only on dyadic email conversations is limiting, especially in the context of enterprise emails. In the enterprise email data that we study, 52.99% of emails are non-dyadic emails, that is, they are sent to more than one recipient other than the sender. us it is important and more realistic to study the more general se ing of modeling email reply behavior including both dyadic emails and emails sent to a group of people, without any threshold on previous interactions.
In this paper, we address this gap by characterizing and predicting reply behaviors in enterprise emails, where we consider both dyadic conversations and group discussions. We use the publiclyavailable Avocado research email collection,1 which consists of emails and a achments taken from a defunct information technology company referred to as "Avocado". ere are 938, 035 emails from 279 accounts in this email collection.
We analyze and characterize various factors a ecting email replies including: temporal features (e.g. time of day and day of week), historical interaction features (e.g., previous interactions between sender and recipients), properties of the content features (e.g. length of subject and email body), predictions based on textual content features (e.g., sentiment, contains a request), address features (e.g., recipients), and metadata features (e.g., a achments). We
nd several interesting pa erns connecting these factors to reply behavior. For example, emails with requests or commitments get more but slower replies while longer emails tend to get fewer and slower replies. Based on this analysis, we used a variety of features to build models to predict whether an email will receive a reply and the corresponding reply time. We show that our proposed model outperforms all baselines with large gains. We also perform feature importance analysis to understand the role di erent features play in predicting user email reply behavior.
Our contributions can be summarized as follows:
(1) We introduce and formalize the task of reply behavior prediction in enterprise emails involving both one-to-one (dyadic) and one-to-many communication. Unlike previous work either on small user surveys and interviews [9, 10, 22, 29] or only for dyadic email conversations in consumer emails [20], our work is the rst to model reply behavior in a more general se ing including emails sent to groups of people as well as individuals for enterprise email.
(2) We analyze and characterize various factors a ecting email replies. Compared to previous work, we study many novel factors including email textual content, request / commitment in emails, address features like internal/external emails, and number of email recipients.
(3) We extract 10 di erent classes of features and build models to predict email reply behaviors. We perform thorough experimental analysis with the publicly-available Avocado email collection and show that the proposed methods outperform all baselines with large gains. We also analyze the importance of each class of features in predicting email reply behavior.
1h ps://catalog.ldc.upenn.edu/LDC2015T03

2 RELATED WORK
Our work is related to several research areas, including modeling actions on email, email overload, email acts and intent analysis, email classi cation and mining.
Modeling Actions on Email. Our work is related to previous research on user behavior and email action modeling [10, 11, 20, 23, 24, 28]. Dabbish et al. [10] examined people's ratings of message importance and the actions they took on speci c email messages with a survey of 121 people at a university. While this work provides insights on email usage and user actions on email messages, how well the results generalize to other user groups is not clear. DiCastro et al. [11] studied four common user actions on email (read, reply, delete, delete-withoutread) using an opt-in sample of more than 100k users of the Yahoo! Mail service. ey proposed and evaluated a machine learning framework for predicting these four actions. Kooti et al. [20] also used Yahoo! Mail data to quantitatively characterize the reply behavior for pairs of users. ey investigated the e ects of increasing email overload on user behaviors and performed experiments on predicting reply time, reply length and whether the reply ends a conversion. Our work is inspired by the la er two studies but it di ers in several important ways. ese two studies looked at behavior in Yahoo! Mail, a consumer email collection, whereas we studied interaction in a enterprise se ing using the Avocado collection. What's more, the study by Kooti et al. [20] only considers dyadic emails from a subset of people who had at least ve previous email interactions. Our work considers a more general se ing where we consider both dyadic emails and emails sent to a group of users. We allow cases where there is no previous interactions between the sender and receivers, which makes our prediction task a more challenging (and realistic) one. Our experimental data is also publicly available in the recently-released Avocado collection from LDC, whereas prior research used proprietary internal data. Last but not least, we analyzed several novel features including properties of the content like email subject and body length, predictions of whether the email contained a request or commitment, and address features like internal vs. external emails and showed that they are useful for building models to predict user email reply behavior.
Email Overload. Several research e orts have examined the email overload problem and proposed solutions to reduce it [2, 9, 12, 22, 29]. In their pioneering work, Whi aker and Sidner [29] explored how people manage their email and found that email was not only used for communication, but also for task management and maintaining a personal archive. A decade later, Fisher et al. [12] revisited the email overload problem by examining a sample of mailboxes from a high-tech company. ey showed that some aspects of email dramatically changed, such as the size of archive and number of folders, but others, like the average inbox size remained more or less the same. Several researchers have proposed solutions to mitigate the email overload problem [1, 2, 13, 18]. Aberdeen et al. [1] proposed a per-user machine learning model to predict email "importance" and rank email by how likely the user is to act on that mail; this forms the the Priority Inbox feature of Gmail. Our work shares similar motivations for handling the email overload problem by modeling and predicting user reply behaviors on emails. We focused on email reply behaviors, speci cally identifying emails

236

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

(a) e number of sent emails in each month.

(b) e number of active users in each month.

Figure 2: Temporal analysis of the number of sent emails and active users in each month of the Avocado email collection.

which receive replies and the reply latency. eir work looked at consumer emails, Gmail, whereas we focus on the enterprise email collection, Avocado.
Email Acts and Intent Analysis. Previous research studied email acts and email intent analysis [3, 7, 21, 26, 27]. Cohen et al. [7] proposed machine learning methods to classify emails according to an ontology of verbs and nouns, which describe the "email speech act" intended by the email sender. Follow-up work by Carvalho and Cohen [4] described a new text classi cation algorithm based on a dependency-network-based collective classi cation method and showed signi cant improvements over a bag-of-words baseline classi er. In a recent study using the Avocado collection, Sappelli et al. [26] studied email intent and tasks, and proposed a taxonomy of tasks in emails. ey also studied predicting the number of tasks in emails. Our work extends previous research on email acts and intent by extracting requests and commitments from emails and using them as features for predicting user reply behavior.
Email Classi cation and Mining. We formalized the user email reply behavior prediction task as a classi cation task. ere are many previous papers on email classi cation and mining [5, 14­ 16, 19]. Klimt and Yang [19] introduced the Enron corpus as a data set and used it to explore automated classi cation of email messages into folders. Graus et al. [14] studied the task of recipient recommendation. ey also used enterprise email collections, but their collection was proprietary and their focus was on recipient recommendation, rather than reply behavior modeling.
To summarize, the research described in this paper extends previous research on email interaction in several ways. Using a recently released public enterprise email collection, we formalize the task of prediction whether an email will be responded to and how long it will take to do so. In contrast to earlier work on reply prediction, we study enterprise (vs. consumer) email behavior, consider both one-to-one (dyadic) and one-to-many emails, and develop several new types of features to characterize the email content and intent and study their importance in reply prediction.
3 DATA SET & TASK DEFINITION
Our data set is the Avocado research email collection from the Linguistic Data Consortium. is collection contains corporate emails from a defunct information technology company referred to as "Avocado". e collection contains the full content of emails,

various meta information as well as a achments, folders, calendar entries, and contact details from Outlook mailboxes for 279 company employees.
e full collection contains 938, 035 emails and 325, 506 a achments. Since our goal is to study email reply behavior, we need to preprocess the data to gure out the reply relationships between emails. For emails that are replies, the email meta information includes a "reply to" relationship eld, containing the ID of the email that this email is replying to. We generate the email thread structure by parsing the "reply to" relationship elds in email meta data. Speci cally, we rst use the "reply to" relationship elds to collect sets of messages that are in the same thread. en emails in the same thread are ordered by the sent time to generate their thread positions. We removed all the duplicated quoted "original messages" in the email text les to identify the unique body for each message. From the "reply to" relationship elds and associated email sent time, we generate the ground truth for reply behavior including whether the email received a reply, and reply latency.
We rst perform a temporal analysis of sent emails and active users in each month. Figure 2a and Figure 2b show the number of sent emails and the number of active people in this collection respectively. Emails are aggregated by the sent time into di erent months. Each person is represented by an interval based on the sent time of his/her rst and last email. en the number of active people in each month is the number of overlapping intervals during the month. We see that the number of active people increases to a peak in February 2001 and then decreases to under 50 a er February 2002, as the company started laying o employees. e number of emails sent has a similar pa ern over months. To ensure that our analysis and experimental results are built upon emails when the company was under normal operations, we select the emails from June 1st 2000 to June 1st 2001.
We further perform a few additional data cleaning and ltering steps. We focus on the reply behaviors toward the rst message in each thread. Because replies towards the rst messages and follow-up replies may have di erent properties and we want to focus on one phenomenon in this study. We lter out email threads where the rst reply comes from the sender himself or herself and emails where the only recipient is the sender. A er these ltering

237

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

steps, we have 351, 532 emails le , which become the basis of our data analysis.2
e email behavior we study includes the reply action and reply time. We formally de ne the task as following. Given an email message m that user ui sent to a user set {uj } at time t, we study: (1) Reply Action: whether any one of users in {uj } will reply to message m; (2) Reply Time: how quickly users in {uj } reply, which is the di erence between the rst response time and the start time t. If there are multiple replies from users in {uj }, we consider only the time latency corresponding to the rst reply.
us our se ing includes both "one-to-one" and "one-to-many" emails, which is more general than [20] which only included dyadic emails between pairs of users with at least ve previous emails.
(a) Reply rate vs. time of day. (b) Reply time vs. time of day.
(c) Reply rate vs. day of week. (d) Reply time vs. day of week. Figure 3: e e ects of temporal features of the original sent emails on user email reply behavior. e median reply time denoted by Median RT is in minutes.
4 CHARACTERIZING REPLY BEHAVIOR
We characterize multiple factors a ecting email reply action and reply time. For reply action, we compute the reply rate which is the percentage of emails that received replies. 3 In Section 5 we show that these factors enable us to learn models to predict user reply behavior.
4.1 Temporal Factors
We rst study the impact of temporal factors on user replies. Figure 3a and Figure 3b show the reply rate and median reply time to emails sent at di erent times of the day. We partition the time of a day into Night (0 to 6), Morning (6 to 12), A ernoon (12 to 18) and Evening (18 to 24). en we aggregate emails in each time range and compute the reply rate and median reply time. e unit of reply time is in minutes. We see that emails sent in morning and
2 e email IDs of the ltered subset can be downloaded from h ps://sites.google.com/ site/lyangwww/code- data. 3For partitions of the data we compute the reply rate for each partition separately. For example, the reply rate to emails sent at night is the percentage of emails sent at night that receive a reply regardless of whether the reply is sent at night or day.

a ernoon receive more and faster replies. For reply rate, emails sent in the a ernoon have the highest reply rate (7.77%) and emails sent at night have the lowest reply rate (3.63%). For reply time, the median reply time for morning and a ernoon is less than 1 hour, whereas reply time for mails sent in evening and night is more than 7 hours. is makes sense since users are more active during the day than night time.
Figure 3c and Figure 3d show the reply rate and reply time to emails sent on di erent days of a week. We see that emails sent on weekdays receive more and faster replies. e reply rate for emails sent on weekdays is around 7%, but that rate drops to 4% on weekends. For the median reply time, we see that emails sent on weekdays receive replies in less than 1 hour. However, emails sent on the weekend have 13 (for Sunday) or 30 (for Saturday) times longer reply latency. is is consistent with the fact that most people don't check or reply to emails as regularly on the weekend. Most emails sent over the weekend are replied to a er Sunday.
4.2 Email Subject and Body
Next we study the e ects of properties of the email content on user reply behavior. We start with the length of email subjects and email bodies. We remove punctuations and maintain stop words when counting the number of words. Figure 4a and Figure 4b show the e ects of email subject length on user reply behavior. e reply rate decreases from 11.47% to 1.10% as the length of email subject increases from 1 to 29, which is an interesting phenomenon. Because we have access to the full content of emails in Avocado, we were able to examine emails that have long and short subjects to identify. We found that many long subjects include announcements in the subjects but no text in the body or some machine generated bug reports. Such email subjects are similar to "[Bug 10001] Changed - Loop length not assigned correctly for xml le when extracted to a
le and tested, works OK when tested from database.". 4 Users don't need to reply to such emails, leading to lower reply rate.
In examining short subjects, we computed the reply rates for all unique subjects. Checking those that occur 20 times or more, we summarize those that have the highest reply rate. A large number of these are simply company names ­ these o en indicate current contracts, customers, or sales leads that are being pursued. Other phrases such as "expense report", "sales training", "meeting", "lunch", and "alerts" indicate common activities that require approval, coordination, or action. Finally, a set of other phrase types such as "hello", "hi", "hey", etc. indicate recipient-sender familiarity (note that spam e-mail lters were in place before collection). We also
nd that reply time is in uenced by subject length. e main trend is that the reply time decreases as the subject length increases.
We also analyze the impact of the email body length on reply behavior. Figure 4c and Figure 4d show the e ects of email body length on reply behavior. We see that the reply rate initially increases from 4.65% to 9.64% words as the email length increases from 1 to 40 words and then decreases to 2.82% as the email body length increases to 500 words. us people are not likely to reply to emails with too few or too many words. e median reply time increases as the length of the email body increases. is may be
4We paraphrase this sentence due to data sensitivity.

238

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

(a) Reply rate vs. subject length. (b) Reply time vs. subject length.

Table 2: e e ects of email addresses and attachments on user reply behavior. e median reply time denoted by Median RT is in minutes.

Factor Email Feature Reply Rate Median RT

Email Addresses Internal External 7.76% 2.26% 65.52 134.67

A achments

HasA ach NoA ach

8.08%

6.38%

80.37

61.23

(c) Reply rate vs. body length. (d) Reply time vs. body length.
Figure 4: e e ects of email subject length and email body length on reply rate and median reply time. e median reply time denoted by Median RT is in minutes.

Table 1: e e ects of requests and commitments in emails on user reply behavior. e median reply time denoted by Median RT is in minutes.

Factor Email Feature Reply Rate Median RT

Requests HasReq 14.81% 81.13

NoReq 7.45% 54.51

Commitments HasCom NoCom 8.32% 8.78% 86.71 60.60

because people need to spend more time reading and digesting the content of longer emails, thus increasing reply time.
4.3 Requests and Commitments in Emails
Previous work [3, 4, 7, 21] has studied "speech acts" in emails. Carvalho and Cohen [4] developed algorithms to classify email messages as to whether or not they contain certain "email acts", such as a request or a commitment. We classify emails into those with requests (e.g. "Send me the report.") or commitments (e.g. "I'll complete and send you the report.") and those without requests or commitments using an internally developed classi er inspired by previous work in this area [8].
Table 1 shows the e ects of whether an email contains a request or commitment on reply rate and reply time. Emails that contain requests are almost twice as likely to receive replies as those that do not, 14.81% vs. 7.45% respectively. is is reasonable since intuitively people are more likely to reply to emails if the sender makes a request. In contrast, there is very li le di erences between the reply rate toward emails with and without commitments.
e median reply time toward emails with requests or commitments is longer, which may seem a bit counter-intuitive. However, this may be because such emails are associated with tasks, and therefore, people may need to do some work like searching information, reading or writing before they can reply to such a mail.

4.4 Internal / External Email Addresses
Next we investigate the impact of internal or external emails on reply rate and reply time. To do this, we adopt a heuristic method to classify the email addresses. For each receiver address, we check whether there is a "@" in it. If a receiver address contains "@" and does not contain "@avocadoit.com", this address is assigned an "external" label. If there is no "@" or only "@avocadoit.com" in a receiver address, we classify it as an internal address. Email addresses with "@avocadoit.com" are de nitely internal addresses. However, a limitation of this method is that if email addresses with external domains are in the sender's contacts, they will be treated as internal addresses. In this sense, the internal email addresses in our analysis are those of Avocado employees or people who have frequent communications with Avocado employees and are stored in the contact books. Emails sent to at least one external address are labeled as external, otherwise they are treated as internal emails.
Table 2 shows the comparison of the reply rate/ reply time for internal emails and external emails. People are 3.4 times more likely to reply to internal emails (7.76%) than external emails (2.26%). In addition median reply time to internal emails is 2.1 times faster than to external emails, 66 vs. 135 minutes respectively. In our context, internal emails come from the colleagues or people in the sender's contacts, and are thus more likely to be quickly replied to.
4.5 Email Attachments
We further analyze the e ects of email a achments on user reply behaviors. Table 2 shows that reply rates are higher for emails with a achments (8.08%) than emails without a achments (6.38%). is may be because the types of emails that contain a achments are more likely to require replies. e median reply time for emails with a achments is 23.81% longer than that for emails with no a achments. is result is consistent with the fact that people need to open the a achments and spend more time reading or replying to emails with a achments.
4.6 Number of Email Recipients
Unlike [20] who consider only dyadic emails, we also include emails sent to multiple recipients in our analysis. Figure 5 shows the comparisons of reply rate and reply time for emails with di erent number of recipients. Most emails are sent to 1 to 8 addresses. Emails sent to 3 to 5 addresses have the highest reply rates (approximately 10%). As the number of email recipients continues to increase, the reply rate begins to decrease. us more email recipients does not always mean a higher reply rate. ere are at least two reasons for this: 1) some emails sent to many addresses are general announcements or reports which do not require replies; 2) when an email is

239

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: Reply time prediction class distribution.

Class 1:  25min 2: 25-245min 3:  245min

Training Data 5,143 32.80% 5,150 32.84% 5,389 34.36%

Testing Data 3,299 38.53% 2,820 32.93% 2,444 28.54%

(a) Reply rate vs. the number of (b) Reply time vs. the number of

email recipients.

email recipients.

Figure 5: e e ects of the number of email recipients on reply rate and median reply time. e median reply time denoted by Median RT is in minutes.

Table 4: Reply action prediction class distribution.

Class 0: No Reply 1: Has Reply

Training Data 224,605 93.47% 15,682 6.53%

Testing Data 102,682 92.30% 8,563 7.70%

sent to many recipients, it is more likely that no one will reply to it since people may think the other co-recipients will reply to it.
Median reply time increases from 44 minutes to 95 minutes as the number of email recipients increases from 1 to 8. us emails sent to more recipients get slower replies. When an email has many recipients, people may wait a while to see if others reply before they choose to do so, or it may be that emails that are sent to many people require some work to be accomplished before the reply.
5 PREDICTING REPLY BEHAVIOR
e results presented in Section 4 show that email reply behaviors are in uenced by various factors such as time, email content, email addressees, email recipients and a achments. In this section, we use these insights to guide the development of features to train a supervised learning model to predict email reply behavior including reply time and reply action.
5.1 Data Overview and Methodology
Given the Avocado email collection described in Section 3, we split the data into training / testing partitions using the sent time of emails. Speci cally, we use emails in 9 months from June 1st 2000 to February 28th 2001 for training and use emails in 3 months from March 1st 2001 to June 1st 2001 for testing. Because email is a temporally ordered collection, we used a split by time (rather than randomly) to ensure that we do not use future information to predict past reply behavior.
We formalize the reply action prediction task as a binary classication problem and the reply time prediction task as a multi-class classi cation problem. We follow the notations and task de nitions presented in Section 3. Given an email message m that user ui sent to a user set {uj } at time t, reply action prediction is to predict whether any user in {uj } will reply to message m. us the classi ed instances are emails with binary labels. For reply time prediction, we do not necessarily need to get the exact reply time latency. We follow a similar se ing as previous related work [20], where we consider three classes of reply times: immediate replies that happen within 25 minutes (32.80% of the training data), quick replies that happen between 25 minutes and 245 minutes (32.84% of the training data), and slow replies that take longer than 245 minutes (34.36% of the training data). e ground truth labels can be directly extracted from the training/testing data based on the actual reply time and actions on emails. e statistics of the experimental

data and the distribution of labels for reply time/action prediction task are shown in Table 3 and Table 4.
5.2 Learning Models and Evaluation Metrics
We experiment with a variety of machine learning algorithms including Logistic Regression, Neural Networks, RandomForest, AdaBoost [31] and Bagging algorithms. Since the focus of our work is on feature analysis, we only report the experimental results with a basic Logistic Regression (LR) model and the best model, AdaBoost. We used the sklearn5 package for the implementation of LR and AdaBoost. We did multiple runs of hyper-parameter tuning by grid search to nd the best se ing for each model with cross validation on the training data.6
Since reply action prediction is highly imbalanced (see Table 4) and ranking quality is of importance for tasks like triage, we report Area Under the ROC Curve (AUC), which is a ranking metric insensitive to class imbalance, as the metric for reply action prediction. For reply time prediction, we report precision, recall, F-1 and accuracy (i.e., the percentage of correctly classi ed emails). e precision, recall, and F-1 are computed as weighted macro averages over all three classes.
5.3 Features
Our analysis in Section 4 identi es factors that impact email reply behaviors. Building on these observations we develop 10 classes of features that can be used to build models for predicting both reply time and reply action. e summary and description of the extracted features is provided in Table 5. In total, we extract 65 features and Bag-of-Words features in 10 feature groups. We normalize all features to be in the range of [0, 1] using min-max feature normalization.
Address Features (Address): ese features include features derived from the email addresses such as whether the email is internal or external and the number of recipients.
5h p://scikit-learn.org 6For reply time prediction with LR, we set C = 1, the maximum number of iterations as 500, tolerance for stopping criteria as 0.0001, penalization as l1 norm. For reply action prediction with LR, we set C = 1, the maximum number of iterations as 100, tolerance for stopping criteria as 0.0001, penalization as l1 norm. For reply time prediction with AdaBoost, we set the learning rate as 0.1, the maximum number of estimators as 800, boosting algorithm as S AM M E .R. For the reply action prediction with AdaBoost, we set the learning rate as 1, the maximum number of estimators as 50, boosting algorithm as S AM M E .R.

240

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: e features extracted for predicting user email reply behaviors. e 10 feature groups Address, BOW, CPred, CProp, HistIndiv, HistPair, Meta, MetaAdded, Temporal, User stand for "Address Features", "Bag-of-Words", "Content Predictions", "Content Properties", "Historical Interaction-Individual", "Historical Interaction-Pairwise", "Metadata Properties", "Metadata Properties-Sender Added", "Temporal Features" and "User Features" respectively. Note that for the computation of features in User, HistIndiv and HistPair, we respect the temporal aspects of the data and only use the historical information before the sent time of the email instance.

Feature
IsInternalExternal NumOfRecipients BagOf Words SentimentWords CommitmentScore RequestScore EmailSubLen EmailBodyLen HistReplyRateGlobalUI HistReplyNumGlobalUI HistRecEmailNumSTGlobalUI HistRecEmailNumCCGlobalUI HistSentEmailNumGlobalUI HistReplyTimeMeanGlobalUI HistReplyTimeMedianGlobalUI HistGlobalUJ HistReplyNumLocal HistReplyTimeMeanLocal HistReplyTimeMedianLocal HasA achment NumOfA achment IsImportant IsPriority IsSensitivity TimeOfDay DayOf Week WeekDayEnd UserDepartment UserJobTitle

Group
Address Address BOW CPred CPred CPred CProp CProp HistIndiv HistIndiv HistIndiv HistIndiv HistIndiv HistIndiv HistIndiv HistIndiv HistPair HistPair HistPair Meta Meta MetaAdded MetaAdded MetaAdded Temporal Temporal Temporal User User

Description
1 binary feature indicating whether the email is internal or external e number of recipients of the email
e bag-of-words features indicating the TF-IDF weights of terms in the email body text
2 integer features indicating the number of positive/negative sentiment words in the email body text e commitment score of the email from an internal binary classi er e request score of the email from an internal binary classi er
e length of the subject of the email e length of the body of the email
e historical reply rate of the sender ui towards all the other users e historical reply count of the sender ui towards all the other users e historical number of received emails of the sender ui as in sent to address from all the other users e historical number of received emails of the sender ui as in CC address from all the other users e historical number of sent emails of sender ui to all the other users e historical mean reply time of sender ui to all the other users e historical median reply time of sender ui to all the other users 21 features indicating similar mean/min/max historical behavior statistics of recipients {uj } towards the other users 3 features indicating the historical mean, min, max reply count of the recipient {uj } to sender ui 3 features indicating the historical mean, min, max of the mean reply time of the recipient {uj } towards sender ui 3 features indicating the historical mean, min, max of the median reply time of the recipient {uj } towards sender ui 1 binary feature indicating whether the email has a achments 1 integer feature indicating the number of a achment of the email
1 binary feature indicating the importance of the email, which is a tag speci ed by ui 1 binary feature indicating the priority of the email, which is a tag speci ed by ui 1 binary feature indicating the sensitivity of the email, which is a tag speci ed by ui 4 binary features indicating the time of the day (0-6, 6-12, 12-18, 18-24) 7 binary features indicating the day of week (Sun, Mon, ... , Sat) 2 binary feature indicating whether the day is a weekday or a weekend
1 feature indicating the department of the email sender ui . 1 feature indicating the job title of the email sender ui .

Bag-of-Words (BOW): ese features include the TF-IDF weights of non-stop terms in the email body text. e vocabulary size of our experimental data set is 554061.
Content Predictions (CPred): ese features include some predictions like positive / negative sentiment words, commitment / request scores from email textual content. We count the number of positive sentiment words and negative sentiment words in email body text using a sentiment lexicon from a previous research [17]. We also include the commitment and request score of emails from an internal classier to infer the likelihood of whether an email contains a commitment or a request.
Content Properties (CProp): ese features are content properties including the length of email subjects and email body text.
Historical Interaction-Individual (HistIndiv): ese features characterize the historical email interactions related to each sender ui and recipient in {uj } aggregated across all interactions. is feature group has two subgroups: global interaction features for the sender ui and global interaction features for recipients {uj }.
e global interaction features for the sender ui contain a set of features to capture the historical interactions between ui with all the other users such as reply rate, reply count, number of received emails, number of sent emails, mean/median reply time, etc. For the global interaction features for recipients {uj }, since there could be multiple recipients, we compute the mean/min/max of those

statistics to capture the historical interactions between {uj } and all other users.
Historical Interaction-Pairwise (HistPair): ese features characterize the local (pairwise) interactions between the sender ui and the recipients {uj }, which are statistics like number of replied emails, mean/median reply time from the historical interactions between the sender ui and the recipients {uj }. Note that for the computation of "HistIndiv" and "HistPair" features, we compute per day updated user pro les and only use the information before the email instance.
Metadata Properties (Meta): is feature group contains features derived from email a achments including whether the email has a achments and number of email a achments.
Metadata Properties-Sender Added (MetaAdded): ese features include tags speci ed by the sender ui to indicate the importance, priority or sensitivity of the sent email. In our data, less than 3% of emails have such tags. But they can still provide some clues to infer user reply behavior once they are set by the sender.
Temporal Features (Temporal): ese features are generated based on the sent time of emails to capture the temporal factors on user email reply behaviors.
User Features (User): ese features include the department and job title of the person.

241

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 6: Summary of the prediction results for user email reply time and reply action. e Precision, Recall and F1 scores are weighted averages by supports over all classes.
e best performance is highlighted in boldface. Both LR and AdaBoost show signi cant improvements over all baseline methods with p < 0.01 measured by micro sign test [30].

Method
Random Majority Vote Previous Reply LR AdaBoost

Action
AUC
.5024 .5000 .5858 .7036 .7208

Prec
.3262 .0815 .3717 .3952 .4561

Time
Rec F1
.3253 .3244 .2854 .1267 .3742 .3613 .4098 .3791 .4591 .4476

Accuracy
.3257 .2854 .3633 .4098 .4591

5.4 Baselines
We compare our method against three baselines as follows: Random. is method randomly generates a predicted class
from the label distribution in the training data. Majority Vote. is method always predicts the largest class,
which is class 0 (no reply) for reply action prediction and class 3 (> 245min) for reply time prediction.
Previous Reply. is method generates predictions according to the previous reply behaviors of the recipients {uj } towards the sender ui before the sent time t of email m. For reply action, it predicts 0 (no reply) if there is no previous reply behavior from {uj } to ui . For reply time, it predicts the majority class if there is no previous reply behavior from{uj } to ui . If there are previous reply behaviors, it computes the median time of previous reply time as the predicted result. Note that this baseline is similar to the "Last Reply" baseline used in [20]. 7
5.5 Experimental Results and Analysis
We now analyze the results of our proposed method compared with various baseline methods. Table 6 shows the prediction results for reply action and reply time. Figure 6 shows the ROC curves for all methods for reply action prediction. e baseline "Majority Vote", while accurate since 92.30% of emails are negative, achieves zero true positive rate (recall) and predicts no positive instances. Likewise "Random" falls nearly on the x = dashed line in red which indicates expected random performance (empirical variance leads to a slight bit of luck). As shown in Table 6, both LR and AdaBoost outperform all three baselines by AUC. e best model AdaBoost achieves large improvements of 44.16% comparing with "Majority Vote" and 23.05% comparing with "Previous Reply". AdaBoost achieves slightly be er performance than LR. Examining the ROC curves, the most competitive baseline is "Previous Reply", which is still under the ROC curves of LR and AdaBoost. e AUC scores show that our methods outperform all baseline methods with a large margin.
For reply time prediction, both LR and AdaBoost models with the proposed features outperform all baseline methods with large
7 For the proposed method in [20], we can not reproduce their method since they don't disclose the details of the 83 features in their model and they also don't release the code and data due the proprietary nature of their work.

gains. e di erences are statistically signi cantly with p < 0.01 measured by a micro sign test [30]. e best method based on AdaBoost achieves large improvements of 23.89%, 26.36% for F-1 and accuracy comparing with "Previous Reply" and 253.18%, 60.85% for F-1, accuracy comparing with "Majority Vote". Comparing the two learning models, AdaBoost has be er performance than LR in terms of both F-1 and accuracy. is shows the advantages of AdaBoost that can feed the relative "hardness" of each training sample into the tree growing algorithm such that later trees tend to focus on harder-to-classify instances.
5.6 Feature Importance Analysis
Feature Group Analysis. We further perform analyses to understand the relative importance of di erent feature groups in predicting reply time. We consider two approaches: (1) Remove one feature group. We observe the change in performance when we remove any one of the 10 feature groups. (2) Only one feature group. We observe the change in performance when we classify emails only using one feature group. Table 7 and Table 8 show the results of these analyses for reply action prediction and reply time prediction using the AdaBoost model, which is the best method in our previous experiments.
Table 7 shows the performance when we use only one feature group. e classes are ordered by AUC scores on action prediction. e most important features are highlighted using a triangle. For reply action prediction, "HistIndiv" and "HistPair" show the best performance compared to other feature groups. Using only "HistIndiv" features results in 0.6924 for the AUC score, which is close to the performance with all feature groups. ese results suggest that historical interactions are important features for reply action prediction. "CPred" features (i.e., algorithmic predictions of request, commitments and sentiment) are also important although somewhat less so than the historical interaction features. However, for reply time prediction we see a di erent story. "Temporal" features are the most important features for predicting reply time as highlighted in Table 7. Using only "Temporal" features results in good latency prediction accuracy of 0.4261, which is only slightly worse than the result from combining all feature groups. "HistIndiv" features which result in accuracy above 0.40 are also helpful in predicting latency. For reply actions, historical interaction features are the most important in indicating whether people will reply to the email eventually no ma er the time latency. Given they will reply to an email, people seem to strongly prefer to reply during o ce hours on workdays, which explains why "Temporal" factors are so important for reply time prediction.
Another way of looking at the importance of features is to remove one class and look at the decrease in performance. Table 8) shows the results of removing one feature group. Performance decrease the most when we remove "HistIndiv" features for reply action prediction and "Temporal" features for reply time prediction.
ese results are consistent with the results when we only use one feature group. We also found some features are not very useful for reply behaviour prediction. For instance, when we remove "Meta" features which are derived from email a achments, both F-1 and accuracy increase slightly for reply time prediction. is suggests that there is still space for feature selection to further improve the

242

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 7: Comparison of performance on predicting reply time and reply action when we only use one feature group.
e learning model used is AdaBoost. e best performance is highlighted in boldface. indicates strong performance when only use one feature group. e feature settings are sorted by the AUC scores.

Feature Set
HistIndiv HistPair CPred User Address Temporal CProp MetaAdded Meta BOW AllFeat

Action
AUC
.6924 .6382 .5954 .5944 .5912 .5401 .5346 .5291 .5247 .5106 .7208

Prec
.3891 .3721 .3748 .3729 .3790 .4436 .3415 .2524 .2398 .3744 .4561

Time

Rec F-1

.4104 .3890 .3784 .3847 .3641 .4261 .3785 .3877 .3670 .3976

.3642 .3463 .3352 .3575 .3038 .4264 .3060 .2672 .2866 .3391

.4591 .4476

Accuracy
.4104 .3890 .3784 .3847 .3641 .4261 .3785 .3877 .3670 .3976 .4591

Table 8: Comparison of performance on predicting reply time and reply action when we remove one feature group.
e learning model used is AdaBoost. indicates large drops in performance when remove one feature group. e feature settings are sorted by the AUC scores.

Feature Set
­HistIndiv ­CProp ­Address ­MetaAdded ­HistPair ­Meta ­Temporal ­CPred ­BOW ­User AllFeat

Action
AUC
.6620 .7112 .7187 .7191 .7198 .7216 .7218 .7229 .7237 .7256 .7208

Prec
.4453 .4472 .4550 .4549 .4544 .4573 .3841 .4540 .4539 .4473 .4561

Time

Rec F-1

.4481 .4543 .4599 .4579 .4572 .4604 .4056 .4611 .4573 .4482 .4591

.4409 .4413 .4476 .4449 .4432 .4515 .3800 .4457 .4503 .4431 .4476

Accuracy
.4481 .4543 .4599 .4579 .4572 .4604 .4056 .4611 .4573 .4482 .4591

performance of reply behavior prediction. We leave the study of feature selection to future work.
Importance of Individual Features. AdaBoost [31] provides a mechanism for reporting the relative importance of each feature. By analyzing the relative importance, we gain insights into the importance of individual features for di erent email reply prediction tasks. Table 9 shows the most important features for predicting reply time and reply action with the relative feature importance learned by AdaBoost. e most important features for reply action prediction are historical interaction features including "HistReplyCountRecipientMax", "HistSentEmailCountSender", "HistReceiveEmailSTRecipientMin", content properties like the length of email subjects and

Figure 6: e ROC curves of di erent methods for the reply action prediction task.
email bodies, and address features like "NumOfReceivers", "IsInternalExternal" etc. On the other hand, temporal features like "TimeOfDay1Morning", "IsWeekDay", "DayOfWeek1Sunday", "TimeOfDay4Night" are among the most important features for reply time prediction. ese interesting di erences are also consistent with the results in the feature group analysis. Some features including historical interaction features and content properties like the length of email bodies are important for both reply action prediction and reply time prediction.
6 CONCLUSIONS AND FUTURE WORK
In this paper, we introduce and formalize the task of reply behavior prediction in a corporate email se ing, using the publicly available Avocado collection. We characterize various factors a ecting email reply behavior, showing that temporal features (time of day and day of week), content properties (such as the length of email subjects and email bodies) and prior interactions between the sender and recipients are related to reply behavior. We use these insights to extract 10 classes of features groups and build models to predict whether an email will be responded to and how long it will take to do so. We show that the proposed methods outperform all baselines with large gains. We further show that temporal, textual content properties, and historical interaction features are especially important in predicting reply behavior.
Our research represents an initial e ort to understand email actions in a corporate se ing. We examined email reply behavior in detail in one technology company, but is unclear how representative the company is. It is important to see how our ndings generalize to di erent industry sectors and di erent demographic backgrounds of employees. Future work will consider more available email collections and more features that could be signals for user reply behavior prediction. Of special interest is the use of richer content features such as lexical, syntactic and semantic features. We shared the IDs of the emails that we consider in our research so that others can extend our research on the Avocado collection.

243

Session 2C: Other Applications and Specialized Domains

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 9: e most important features for predicting reply time and reply action with relative feature importances in AdaBoost.

Rank
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15

Reply Action Prediction

Feature Name

Group

EmailSubjectLen HistReplyCountRecipientMax HistSentEmailCountSender HistReceiveEmailSTRecipientMin NumOfReceivers EmailBodyLen HistLocalMeanRTMin IsInternalExternal UserJobTitleSender HistLocalReplyCountMin NumoOfA achment HistReceiveEmailCCSender HistRTMeanSender HistReplyCountRecipientAvg HistLocalReplyCountMax

Cprop HistIndiv HistIndiv HistIndiv Address Cprop HistPair Address User HistPair Meta HistIndiv HistIndiv HistIndiv HistPair

Importance
1.0000 0.5714 0.4286 0.4286 0.4286 0.4286 0.2857 0.2857 0.2857 0.2857 0.2857 0.1429 0.1429 0.1429 0.1429

Reply Time Prediction

Feature Name

Group

TimeOfDay1Morning IsWeekDay DayOf Week1Sunday EmailBodyLen TimeOfDay4Night HistRTMedianRecipientAvg HistRTMedianRecipientMin HistLocalReplyCountMax HistReceiveEmailSTSender IsPriority IsWeekEnd HistLocalMedianRTMin HistReceiveEmailCCSender IsSensitivity HistRTMeanRecipientAvg

Temporal Temporal Temporal Cprop Temporal HistIndiv HistIndiv HistPair HistIndiv MetaAdded Temporal HistPair HistIndiv MetaAdded HistIndiv

Importance
1.0000 0.8083 0.4946 0.3260 0.3052 0.1690 0.1563 0.1125 0.1101 0.0951 0.0909 0.0907 0.0858 0.0759 0.0674

7 ACKNOWLEDGMENTS
is work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-1419693. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.
REFERENCES
[1] Douglas Aberdeen, Ondrej Pacovsky, and Andrew Slater. e Learning behind Gmail Priority Inbox. In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds.
[2] Victoria Bello i, Nicolas Ducheneaut, Mark Howard, and Ian Smith. Taking Email to Task: e Design and Evaluation of a Task Management Centered Email Tool. In CHI '03.
[3] Paul N. Benne and Jaime Carbonell. Detecting Action-items in E-mail. In SIGIR '05.
[4] Vitor R. Carvalho and William W. Cohen. On the Collective Classi cation of Email "Speech Acts". In SIGIR '05.
[5] Marta E. Cecchinato, Abigail Sellen, Milad Shokouhi, and Gavin Smyth. Finding Email in a Multi-Account, Multi-Device World. In CHI'16.
[6] Michael Chui, James Manyika, Jacques Bughin, Richard Dobbs, Charles Roxburgh, Hugo Sarrazin, Geo rey Sands, and Magdalena Westergren. 2012. e social economy: Unlocking value and productivity through social technologies. (2012). A report by McKinsey Global Institute.
[7] William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. Learning to Classify Email into Speech Acts. In EMNLP '04.
[8] Simon Corston-Oliver, Eric Ringger, Michael Gamon, and Richard Campbell. Integration of Email and Task Lists. In First Conference on Email and Anti-Spam.
[9] Laura A. Dabbish and Robert E. Kraut. Email Overload at Work: An Analysis of Factors Associated with Email Strain. In CSCW '06.
[10] Laura A. Dabbish, Robert E. Kraut, Susan Fussell, and Sara Kiesler. Understanding Email Use: Predicting Action on a Message. In CHI '05.
[11] Dotan Di Castro, Zohar Karnin, Liane Lewin-Eytan, and Yoelle Maarek. You've Got Mail, and Here is What You Could Do With It!: Analyzing and Predicting Actions on Email Messages. In WSDM '16.
[12] Danyel Fisher, A. J. Brush, Eric Gleave, and Marc A. Smith. Revisiting Whi aker & Sidner's "Email Overload" Ten Years Later. In CSCW '06.
[13] Michael Freed, Jaime G. Carbonell, Geo rey J. Gordon, Jordan Hayes, Brad A. Myers, Daniel P. Siewiorek, Stephen F. Smith, Aaron Steinfeld, and Anthony Tomasic. RADAR: A Personal Assistant that Learns to Reduce Email Overload. In AAAI '08.

[14] David Graus, David van Dijk, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke. Recipient Recommendation in Enterprises Using Communication Graphs and Email Content. In SIGIR '14.
[15] Mihajlo Grbovic, Guy Halawi, Zohar Karnin, and Yoelle Maarek. How Many Folders Do You Really Need?: Classifying Email into a Handful of Categories. In CIKM '14.
[16] Ido Guy, Michal Jacovi, Noga Meshulam, Inbal Ronen, and Elad Shahar. Public vs. Private: Comparing Public Social Network Information with Email. In CSCW '08.
[17] Minqing Hu and Bing Liu. Mining and Summarizing Customer Reviews. In KDD '04.
[18] Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos, Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter Young, and Vivek Ramavajjala. Smart Reply: Automated Response Suggestion for Email. In KDD '16.
[19] Bryan Klimt and Yiming Yang. e Enron Corpus: A New Dataset for Email Classi cation Research. In ECML'04.
[20] Farshad Kooti, Luca Maria Aiello, Mihajlo Grbovic, Kristina Lerman, and Amin Mantrach. Evolution of Conversations in the Age of Email Overload. In WWW '15.
[21] Andrew Lampert, Robert Dale, and Cecile Paris. Detecting Emails Containing Requests for Action. In HLT '10.
[22] Carman Neustaedter, A. J. Bernheim Brush, and Marc A. Smith. Beyond "From" and "Received": Exploring the Dynamics of Email Triage. In CHI EA '05.
[23] Byung-Won On, Ee-Peng Lim, Jing Jiang, Amruta Purandare, and Loo-Nin Teow. Mining Interaction Behaviors for Email Reply Order Prediction. In ASONAM '10.
[24] Ashequl Qadir, Michael Gamon, Patrick Pantel, and Ahmed Hassan Awadallah. Activity Modeling in Email. In NAACL-HLT '16.
[25] S. Radicati. 2014. Email statistics report, 2014-2018. (2014). [26] Maya Sappelli, Gabriella Pasi, Suzan Verberne, Maaike de Boer, and Wessel Kraaij.
2016. Assessing E-mail intent and tasks in E-mail messages. Inf. Sci. 358-359 (2016), 1­17. [27] Michael Gamon Richard Campbell Simon H. Corston-Oliver, Eric Ringger. Taskfocused Summarization of Email. In ACL'04. [28] Joshua R. Tyler and John C. Tang. When Can I Expect an Email Response? A Study of Rhythms in Email Usage. In ECSCW'03. [29] Steve Whi aker and Candace Sidner. Email Overload: Exploring Personal Information Management of Email. In CHI '96. [30] Yiming Yang and Xin Liu. A Re-examination of Text Categorization Methods. In SIGIR '99. [31] Ji Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. 2009. Multi-class AdaBoost. Statistics and Its Interface (2009).

244


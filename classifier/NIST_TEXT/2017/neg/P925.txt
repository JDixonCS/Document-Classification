Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

E ective Music Feature NCP: Enhancing Cover Song Recognition with Music Transcription

Yao Cheng
Institute of Computer Science and Technology Peking University, Beijing, China chengyao@pku.edu.cn
Deshun Yang
Institute of Computer Science and Technology Peking University, Beijing, China yangdeshun@pku.edu.cn
ABSTRACT
Chroma is a widespread feature for cover song recognition, as it is robust against non-tonal components and independent of timbre and speci c instruments. However, Chroma is derived from spectrogram, thus it provides a coarse approximation representation of musical score. In this paper, we proposed a similar but more e ective feature Note Class Pro le (NCP) derived with music transcription techniques. NCP is a multi-dimensional time serie, each column of which denotes the energy distribution of 12 note classes. Experimental results on benchmark datasets demonstrated its superior performance over existing music features. In addition, NCP feature can be enhanced further with the development of music transcription techniques. e source code can be found in github1.
CCS CONCEPTS
·Information systems Similarity measures; Sentiment analysis;
KEYWORDS
Cover Song Recognition;Dynamic Programming
1 INTRODUCTION
Cover song recognition, which is also called music version identi cation, was extensively studied in recent years. Partly because its potential commercial values such as music copyright protection and management. Another reason is that nding the transformation of music piece that retains its essential identity helps us develop intelligent audio algorithms that recognize common pa erns among musical excerpts.
Music versions are usually performed with their own characteristics in reality, to adapt to di erent singer or live atmosphere or speci c instruments, sometimes just for musical aesthetics. e
1h ps://github.com/gmcather/NCP-exp
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: h p://dx.doi.org/10.1145/3077136.3080680

Xiaoou Chen
Institute of Computer Science and Technology Peking University, Beijing, China chenxiaoou@pku.edu.cn
Xiaoshuo Xu
Institute of Computer Science and Technology Peking University, Beijing, China xuxiaoshuo@pku.edu.cn
variations among music versions are the big obstacles for cover song recognition research. What we need to do is to nd an e ective feature or sequence matching method that satisfy key invariance, tempo invariance and structure invariance.
Up to now, Chroma has been widely used in cover song recognition, as it is robust against non-tonal components and independent of timbre and speci c instruments. A erwards, various variants of Chroma have been come up with in succession. In the past years, Ellis[4] enhanced Chroma to make it synchronized with beats detected by a dynamic programming method. e enhanced chroma, also called Beat-Chroma, is insensitive to tempo variance. Harmonic Pitch Class (HPCP) [5] was rstly proposed by Emilia, and exhibited a be er performance in Serra's work[13]. Chroma Energy distribution Normalized Statistics (CENS) [11] was another widely used variant of Chroma. An overview of these features can be found in [7].
Moreover, part of researchers held that Chord Sequence can be regarded as a good representation in cover song recognition. Attempts can be found in [1, 8, 10]. Lee[10] extracted Chord Sequence from Chroma using Hidden Markov Model, and measured similarity between songs by Dynamic Time Warping[13]. Bello[1] extended Lee's method[10] with a BLAST string alignment method widely used in bioinformatics. Furthermore, Maksim[8] derived Chord Pro le by folding Chord Sequence into a 24-dimensional feature, so that Chord Pro le can be directly applied in large scale cover song recognition owing to its xed low dimension. However, none of these proposed features exhibited a promising future.
When the size of database is not very large, for instance hundreds of songs, performing Qm ax [13] seems like a state of the art method. However, when the number of songs reaches up to thousands or even tens of thousands, the sequence matching methods didn't work at all because of its high computational cost. To adapt to large scale cover song recognition, most existing methods took measures to speed up the computational e ciency at the expense of accuracy. Some approaches a empted to design a xed low dimensional feature, obtained by dimension-reduction algorithms or just derived from Chroma. A er the low dimensional features extracted, a lot of information retrieval algorithms can be exploited to solve the original problem. Typical recent research works included cognition-inspired descriptors[15], 2DFM[2, 6], Chrod Pro le[8], MPLPLC[3], combining features[12], music shapelets[14] and so on. Unfortunately, even the state of the art methods for large scale

925

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

cover song recognition are still far away from business application. Considering the facts mentioned above, we mainly concentrated on small scale cover song recognition.
MIDI, short for Musical Instrument Digital Interface, is a technical standard that allows a wide variety of electronic musical instruments to connect and communicate with one another. It provided a symbolical representation form for music. Assuming that we had the MIDI representations of each song, the di culty of cover song recognition research would be greatly decreased, as symbolic features such as melody or chord can be more easily extracted from MIDI. However, MIDI les of songs are not easy to obtain in reality, as composers are not willing to publish them for copyright protection.
Inspired by the above facts, we a empted to substitute ConvertedMIDI (converted from audio by music transcription techniques) for Edited-MIDI (the original MIDI wri en by composer), and explored whether experimental results can be improved further over previous methods. In this work, we derived NCP feature from Converted-MIDI, and demonstrated its superior performance over Beat-Chroma and CENS. Besides we also explored why NCP showed a be er result.
2 NCP FEATURE EXTRACTION
In the following subsections, we elaborated the procedures of NCP feature extraction.
2.1 Wav2MIDI
It's easier to extract extract more precise pitch information from original MIDI les than audio les, however, original MIDI les are usually hard to being accessible for the copyright reason. Alternatively, we collected MIDI les converted from audio pieces with music transcription tools. Henrique [9] summarized the widespread music transcription benchmark tools, to name a few, WIDI2, Ako Composer3, Waon4 and Sound2MIDI5. In our work, WIDI was used as the main MIDI conversion tool, as it is one of great commercial music transcription tools, and Waon was used for contrast experiment.
2.2 MIDI2Events
MIDI, short for Musical Instrument Digital Interface, is a technical standard that describes how to perform a song, and widely used for electronic music. To parse a MIDI le, we resorted to an open source tool called Pre y-MIDI6. Before devling into the parsing process, let's clarify some basic relevant concepts.
· Track: A MIDI le consists of multiple tracks, each track stores the performance by a speci c instrument.
· Event: Each track contains a series of events, each event consists of four a ributes: start time, end time, pitch and velocity, and usually represents a short action on a speci c instrument.
· Pitch: Each pitch ranges from 0 to 127.
2h p://www.widiso .com/english/products.html 3h p://ako .com/download.html 4h ps://sourceforge.net/projects/waon/ 5h p://sound2midi.so ware.informer.com/ 6h ps://github.com/cra el/pre y-midi

· Velocity: Velocity represents how strong the note or key is pressed.
· Start Time and End Time: Start Time and End Time describes when the event started and ended.

Oboe Finger-style bass


Event Track

Piano

Guitar

Piano
Figure 1: e Edited-MIDI shown by GarageBand

Piano



Piano

Piano

Piano

Piano
Figure 2: e Converted-MIDI shown by GarageBand
With Pre y-MIDI, we can easily separate di erent tracks from MIDI, extract events from tracks. We made a distinction between Edited-MIDI and Converted-MIDI. Edited-MIDI, also refers to original MIDI, is composed by musicians. However, Converted-MIDI is converted from audio le with music transcription tool, therefore some useful information might lose during conversion procedure. In reality, Even though Converted-MIDI sounds not as euphonious as Edited-MIDI, its main melody can be clearly perceived. e samples of Converted-MIDI and Edited-MIDI on github7 were presented for listening. Both of them refer to the same Chinese Pop Music. Two synchronized music pieces were cut down from samples mentioned above, and presented in Figure 1 and Figure 2 respectively. By observing, Edited-MIDI distinguished the instruments in each track well, yet Converted-MIDI was not capable of recognizing intruments in tracks, and marked them by Piano. Moreover, di erent music transcription tools varied in the quality of conversion as well.
7h ps://github.com/gmcather/NCP-exp

926

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

2.3 Events2NCP
Inspired by the extraction process of Chroma, we devised the following computation procedures to extract NCP feature, shown in Algorithm 1. Note that all the events should be taken as the input of Algorithm, regardless of which of track they come from.

Algorithm 1 Algorithm for computing NCP feature

Input: An array of start time of Events, E ent St ar t An array of end time of Events, E ent End An array of pitch of Events, E ent P itch An array of velocity of Events, E entV elocit e number of Events, N

Output: An matrix represents NCP feature, F e number of rows of NCP feature, T
1: // compute earliest start time and latest end time 2: Ear l iestT ime, Lat estT ime = 0, + 3: for each i  [0, N ) do 4: update Ear l iestT ime with E ent St ar t [i] 5: update Lat estT ime with E ent End [i]

6: end for

7:

8: // initialize matrix F, T

9: T = (Lat estT ime - Ear l iestT ime)/0.1

10: FT 12 = 0T 12

11:

12: // accumulate events into F

13: for each i  [0, N ) do

14: p = E ent P it ch[i]%12

15:

= E entV elocit [i]

16: for each j  [0, T ) do

17:

t = EarliestT ime + j  0.1

18:

if t  E ent St ar t [i] and t <E ent End [i] then

19:

Fj,p +=

20:

end if

21: end for

22: end for

23:

24: // normalize F by row

25: for each i  [0, T ) do

26: nd the maximum value in i-th row, r

27: if r 0 then

28:

for each j  [0, 12) do

29:

Fi, j /= r

30:

end for

31: end if

32: end for

Algorithm 1 described how to derive NCP feature F from Events. To begin with, the earliest start time and the latest end time are precomputed for later use. Next, each event was split into pieces by 0.1s time unit. en, each split event was accumulated to NCP feature F . At last, each column of F was normalized to accommodate to the variance of loudness. Note that we took 0.1s as the least time unit for the reason that a musical note usually lasts for 0.1 second when performing music.

3 EXPERIMENT AND ANALYSIS
3.1 Datasets and Metric
Two datasets were used in our experiments as following: · Covers808: A public widespread cover song dataset built by LabROSA9, a collection of 80 songs, each performed by 2 artists. · Covers3810: A private dataset built by ourselves, a collection of 38 Chinese pop songs, each performed by 34 artists, 132 songs in all. Each of 38 songs has its corresponding Edited-MIDI for the following veri cation experiment.
Metric: We adopted Mean Average Precision (MAP)11 to measure the relevance of retrieved songs.
3.2 e performance of NCP
Two types of NCP features, which are NCP-WIDI from WIDI tool and NCP-Waon from Waon tool respectively, were presented in experiments, in comparison to widespread features Beat-Chroma[4] and CENS[11]. Qm ax [13] was conducted to measure the similarity between songs. In experiments, we carefully adjusted signi cant parameters, for example the length of embedding window m in Qm ax , to make sure each features achieved their best performances.
e experimental results on Covers80 and Covers38 datasets were presented in Table 1.
Table 1: Comparison among Beat-Chroma, CENS and NCP

MAP
Covers80 Covers38

Beat-Chroma
0.554 0.656

CENS
0.596 0.781

NCP-Waon
0.613 0.799

NCP-WIDI
0.645 0.828

Both NCP-WIDI and NCP-Waon outperformed Beat-Chroma and CENS on Covers80 and Covers38, this evidence indicated NCP feature is more suitable for cover song recognition at least for Qm ax algorithm. Moreover, e fact that NCP-Waon performed not as well as NCP-WIDI, not only presented the di erences of music transcription tools in cover song recognition, but also reminded us that the performance can be improved further with the development of music transcription techniques.
3.3 Explore the evidence
Chroma is computed by a DFT or constant-Q transform method, where di erent frequency bands are divided into their corresponding Chroma bins. While NCP was directly derived from MIDI, taking music transcription techniques into account, so it provided a more accurate music symbolical representation over Chroma.
erefore we infered that it is the di erences of features mentioned above that caused their preformance di erences, and veri ed our guess in the following experiments.
For simplicity, we clari ed some concepts in advance.
· Fedited: NCP feature derived from an Edited-MIDI.
8h p://labrosa.ee.columbia.edu/projects/coversongs/covers80/ 9h p://labrosa.ee.columbia.edu 10h ps://github.com/gmcather/NCP-exp/cover38 11h p://en.wikipedia.org/wiki/Information retrieval#Mean average precision

927

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

· Fwidi: NCP feature derived from a song of mp3 or wav form by WIDI.
· Fchroma: Chroma feature derived from a song of mp3 or wav form.
e more the similarity between music feature and Fedited , the closer the distance between music feature and authentic music
symbolical feature representation. To verify Fwidi is closer to the authentic music symbolical representation than Fchroma , we collected N (=132) songs from Covers38, each contains its corresponding Edited-MIDI and audio le. Here we adopted Qm ax to measure the similarity between di erent features. e similarity Simw between Fedited and Fwidi , or the similarity Simc between Fedited and Fchroma can be computed with the following formulations.

Simw = Qm ax (Fedit ed , Fwidi )

(1)

Simc = Qm ax (Fedit ed , Fchr oma )

(2)

Figure 3: Simw and Simc for 132 songs
We computed both Simw and Simc for all 132 songs, sorted all pairs of (Simw , Simc ) in an ascending order by Simw , and showed the outcome in Figure 3. Simw is larger than Simc among almost all songs, and only a few songs exhibited an opposite result. In addition, Simw is approximately twice as large as Simc on average. From all these evidences, we found NCP is more suitable for cover song recognition over Chroma.
4 CONCLUSION AND FUTURE WORK
In this paper, we proposed a novel music feature NCP by exploiting existing music transcription techniques, and demonstrated its superior performance over current popular features on small scale datasets. Moreover, we also explored and veri ed the important factors that contribute to the great performance. We conjectured the performance of NCP can be improved further with the development of music transcription techniques.
However, NCP cannot be directly applied to large scale cover song recognition due to its varied and long sequence. In the future

work, to acclimate NCP to large scale cover song recognition, we
will a empt to derive a xed and low dimensional feature that still
preserved distinctive musical information from NCP. For instance,
2DFM might be a great a empt to perform the transformation.
ACKNOWLEDGMENTS
is work was supported by the Natural Science Foundation of
China under Grant No.61370116.
REFERENCES
[1] Juan Pablo Bello. 2007. Audio-Based Cover Song Retrieval Using Approximate Chord Sequences: Testing Shi s, Gaps, Swaps and Beats. In International Society for Music Information Retrieval Conference.
[2] ierry Bertin-Mahieux and Daniel PW Ellis. 2012. Large-Scale Cover Song Recognition Using the 2D Fourier Transform Magnitude. In International Society for Music Information Retrieval Conference.
[3] Ning Chen, J Stephen Downie, Haidong Xiao, Yu Zhu, and Jie Zhu. 2015. Modied Perceptual Linear Prediction Li ered Cepstrum (MPLPLC) Model for Pop
Cover Song Recognition. In International Society for Music Information Retrieval Conference. [4] Daniel PW Ellis and Graham E Poliner. 2007. Identifying Cover Songs with Chroma Features and Dynamic Programming Beat Tracking. In IEEE International Conference on Acoustics, Speech and Signal Processing. [5] Emilia Go´mez. 2006. Tonal Description of Polyphonic Audio for Music Content Processing. In INFORMS Journal on Computing. [6] Eric J Humphrey, Oriol Nieto, and Juan Pablo Bello. 2013. Data Driven and Discriminative Projections for Large-Scale Cover Song Identi cation. In International Society for Music Information Retrieval Conference. [7] Nanzhu Jiang, Peter Grosche, Verena Konz, and Meinard Mu¨ller. 2011. Analyzing Chroma Feature Types for Automated Chord Recognition. In Audio Engineering Society Conference: 42nd International Conference: Semantic Audio. Audio Engineering Society. [8] Maksim Khadkevich and Maurizio Omologo. 2013. Large-Scale Cover Song Identi cation Using Chord Pro les.. In International Society for Music Information Retrieval Conference. [9] Henrique BS Lea~o, Germano F Guimara~es, Geber L Ramalho, Se´rgio V Cavalcante, and others. 2003. Benchmarking Wave-to-MIDI Transcription Tools. In University of Sa~o Paulo. [10] Kyogu Lee. 2006. Identifying Cover Songs from Audio Using Harmonic Representation. In MIREX task on Audio Cover Song Identi cation. [11] Meinard Mu¨ller, Frank Kurth, and Michael Clausen. 2005. Audio Matching via Chroma-Based Statistical Features. In International Society for Music Information Retrieval Conference. [12] Julien Osmalsky, Jean-Jacques Embrechts, Peter Foster, and Simon Dixon. 2015. Combining Features for Cover Song Identi cation. In International Society for Music Information Retrieval Conference. [13] Joan Serra. 2011. Identi cation of Versions of the Same Musical Composition by Processing Audio Descriptions. In Department of Information and Communication Technologies. [14] Diego Furtado Silva, Vin´icius Moura~o Alves de Souza, Gustavo Enrique de Almeida Prado Alves Batista, and others. 2015. Music Shapelets for Fast Cover Song Regognition. In International Society for Music Information Retrieval Conference. [15] JMH van Balen, Dimitrios Bountouridis, Frans Wiering, Remco C Veltkamp, and others. 2014. Cognition-inspired Descriptors for Scalable Cover Song Retrieval. In International Society for Music Information Retrieval Conference.

928


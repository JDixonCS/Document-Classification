Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Personalized Response Generation via Domain adaptation

Min Yang
Tencent AI Lab min.yang1129@gmail.com

Zhou Zhao
Zhejiang University zhaozhou@zju.edu.cn

Wei Zhao
Tencent weizhao@tencent.com

Xiaojun Chen
Shenzhen University xjchen@szu.edu.cn

Jia Zhu

Lianqiang Zhou

South China Normal University Tencent AI Lab

jzhu@m.scnu.edu.cn

tomcatzhou@tencent.com

Zigang Cao
IIE, Chinese Academy of Sciences caozigang@iie.ac.cn

ABSTRACT
In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-speci c information). Second, we ne tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for di erent users.
CCS CONCEPTS
· Natural language processing  Response generation;
KEYWORDS
Response generation, Domain adaptation, Reinforcement learning
1 INTRODUCTION
Conversational system (also called dialogue system) has become increasingly important in a large variety of applications, such as e-commerce, technical support services, entertaining chatbots, and information retrieval dialogue systems. Generally, we have two di erent kinds of conversational systems: goal-oriented and nongoal-oriented (open domain) conversational system. In this paper, we focus on the non-goal-oriented conversational system. Instead of multiple rounds of conversation, we only consider one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) and the latter a response. Hopefully, we expect our work of one round conversation to help understand the intricate mechanism behind the natural language conversation.
Zigang Cao is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080706

Inspired by recent success of recurrent neural network (RNN) in statistical machine translation, most non-goal-oriented conversational systems employ sequence-to-sequence (seq2seq) framework to generate responses [6, 7]. In general, the seq2seq model rstly uses an encoder to summarize the post as a vector representation, and then it feeds this representation into a decoder to generate the response. Despite the remarkable progress of existing conversational systems, generating personalized responses still remains a challenge.
Some recent studies [2, 13­15] show that the user-speci c information (e.g., identity, age, gender, personal information) is valuable, since it is directly related to the content and the style of user's responses, which may further impact the chatting process and the user experience. Arguably, personalization plays an important role in improving conversational system and making it closer to the user's actual information requirement. However, it is di cult to train a personalized conversational system because the data collected from each individual is often insu cient. In particular, if a user has only a few conversations, the generated responses often have uency or even grammatical problems. One way to solve this problem is to consider a large collection of general training data as a source domain and the personalized data as the target domain, and perform transfer learning from the source domain to the target domain. Namely, we can learn common conversation knowledge from the source domain and then adapts this knowledge to the target user.
To address the above challenge, in this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). Firstly, we pre-train the response generation model with an attention LSTM encoder decoder architecture on a large scale general training data (without user-speci c information). Secondly, we ne-tune the model on a small size of personalized data (with user-speci c information) to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses.
The most related work is [14], which uses one single seq2seq model on both the source domain and the target domain. Our approach di ers from [14] in several aspects. First, we generate personalized responses with a dual learning mechanism, which extract promising reward signals for reinforcement learning. Second, in order to generate personalized responses, we formulate the userspeci c information as a vector representation, and then we feed it directly into the LSTM decoder as an additional input. Third, we

1021

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

propose three new rewards rewards (i.e., reconstruction reward and language model reward) to characterize good conversations.

2 RELATED WORK
Inspired by recent success of RNNs in statistical machine translation, there are a number of studies attempting to extend the neural language model to the eld of dialogue modeling. [7] and [6] made use of sequence-to-sequence (seq2seq) model to learn response in short conversations.
There has been increasing interest in applying deep reinforcement learning to dialogue generation [3, 5]. For example, [3] proposed a model which learned a policy by optimizing the long-term reward from ongoing dialogue simulations using policy gradient methods [8], rather than the MLE objective de ned in standard seq2seq models. On the other hand, dual learning [9] has been proposed to improve the neural machine translation.
Recently, several studies have been proposed to capture personal characteristics and handle the information consistency of a user [2, 5, 11, 12, 14]. [2] integrated the speaker embedding and word embedding into a sequence to sequence learning framework. [14] extended the traditional encoder decoder approach to personalized response generation by introducing a domain adaptation scheme, namely initialization-then-adaptation. [5] proposed a transfer learning framework based on POMDP to learn a personalized dialogue system.

3 PERSONALIZED RESPONSE GENERATION
In this section, we elaborate our personalized response generation system.
We use Ds to denote the collection of general post-response pairs data (source domain data) that do not contain user speci c information, and use Dt to denote the collection of personalized post-response pairs data (target domain data) that contain user speci c information. We use p and r to represent the post and the response, respectively.
Suppose we have two agents (post agent A and response agent B) that can generate responses based on posts and generate posts based on responses, respectively. These two agents are pre-trained on the post-response pairs and response-post pairs in general dataset Ds . Our goal is to ne-tune the post agent and the response agent on the personalized dataset Dt . Speci cally, we expect to learn common conversation knowledge from the source domain and then adapt this knowledge to the target user.

3.1 Model Pre-training
LSTM encoder decoder model is originally described in [7], which rst uses an encoder to summarizes the input as a vector represen-
tation, then it feeds this representation into a decoder to generate the output.

3.1.1 LSTM Encoder. The LSTM encoder converts the input

sequence x = (w1, . . . , wTx ) into a sequence of hidden states h =

{h1, . . . , hTx }:

ht = f (xt , ht -1)

(1)

where ht  Rn is a hidden state of LSTM encoder at time t, and we use LSTM as f .

For long sequences, the last state of the LSTM encoder may not re ect important information seen at the beginning of the sequence. Thus, we adopt the bidirectional LSTM, which runs two chains: one forward through the input and another backward, i.e. reversing the tokens in the input sentence. In step t, we summarize the information in the forward and backward LSTM h-idde-n states by taking the concatenation of the two LSTMs ht = [ht , ht ].

3.1.2 LSTM Decoder. Following [1], we use the attention signals to determine which part of the hidden representation h should be emphasized during the generation process. The LSTM decoder is essentially a LSTM language model except conditional on the context input c = {c1, . . . , cT }. The generation probability of the i-th word is calculated by

P ( i | 1, . . . , i-1, x) = ( i-1, si , ci )

(2)

where is a LSTM decoder, and st is the hidden state of LSTM decoder at time t, computed by

si = f ( i-1, si-1, ci )

(3)

The context vector ci is then computed as a weighted sum of

these annotations ht :

Tx

ci = i,tht

(4)

t =1

The weight i,t of each annotation ht is computed by

i,t =

exp(ei,t )

Tx k =1

exp(ei,

k

)

;

ei,

t

=  (si-1, ht )

(5)

where  is a feed-forward neural network, which maps a vector
to a real valued score. This attention mechanism i,t models the alignment between the input sequence at position t and the output
at position i.
We use the post-response pairs and response-post pairs in Ds to pre-train the post agent (denote its parameter as A) and the response agent (denote its parameter as B ) respectively, though the attention LSTM encoder decoder model. The response agent is

trained using the same model as that of the post agent, with posts

and responses interchanged.

3.2 Model adaptation
After obtaining the post agent and the response agent that learn common conversation knowledge from general dataset Ds , in this section, we aim to exploit user-speci c information in personalized dataset Dt to improve the performance of our personalized conversational system. Inspired by the success of dual learning in [9], we leverage the duality of the post agent (primal) and the response agent (dual). The primal and dual tasks can form a closed loop, and generate informative feedback signals to train the conversational system, even with a small number of personalized data.
Starting form a post p in Dt , we rst generate a middle response srmid with the post agent, and then further generate the post back with the response agent. By evaluating this two-hop generation results, we will get a sense about the quality of the post agent and the response agent, and be able to improve them accordingly. This process can be iterated for many rounds until both agents converge, via policy gradient. We denote the post agent and the response agent as P (srmid |p; A) and P (p|srmid ; B ), respectively.

1022

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

3.2.1 Personalized LSTM decoder. To generate personalized responses, in model adaptation step, we explicitly calculate the user vector u for each user based on the user-speci c information (e.g., identity, age, gender, personal information), and directly feed it into the LSTM decoder as an additional input. We describe the process of building user vector representation below.
User Vector Representation . In response generation, user-speci c information (e.g., age, gender, job, education) play an important role in generating personalized responses [10]. We represent each user as a vector representation which encodes user-speci c information. In this project, we mainly employ the following user attributes: User identity, Age (we divide age into ve evenly spaced classes), Gender (female, male or None), Education, Location. We convert these user speci c information into a user vector using a one-hot representation, denoted as u. Since the one-hot representation of the user pro le su ers from data sparsity, we further transform the user pro le vector to a user embedding ue which is low-dimensional and dense by using the matrix-vector product: ue = W eu. The matrix W e  Rde ×|u | is a parameter to be learned, and the size of each column de is a hyperparameter. The user pro le vector is directly incorporated into the decoder as an additional input.
3.2.2 Dual learning with policy gradient . In this section, we propose a policy gradient [8] reinforcement learning algorithm to optimize long-term rewards of the post agent and the response agent. We rst introduce the state, action, policy and reward of the reinforcement learning architecture below.
3.2.3 State. In time t, a state s is denoted by the input sequence of the post agent or the response agent.
3.2.4 Policy. We employ the stochastic policy gradient method to approximate a stochastic policy directly using an independent function approximator with its own parameters. In this paper, the policies for the post agent and the response agent are P (srmid |p; A) and P (p|srmid ; B ), whose inputs are the representations of the states, whose outputs are action selection probabilities.
3.2.5 Action. An action a in timestep k is the sequence to generate (srmid or p) by the policy (P (srmid |p; A) or P (p|srmid ; B )).
3.2.6 Reward. The policy gradient algorithm is a type of reinforcement learning method, which relies upon optimizing parametrized policy with respect to the expected return (long-term cumulative reward) by gradient descent. We assume that our model receives a reward r at each iteration. We discuss the major factors that contribute to the reward for post agent A below, and denote the total reward as rA. Similarly, we can get the reward rB for response agent B in the dual learning procedure.
Reconstruction reward. For dual learning, given the generated middle response srmid by P (srmid |p; A), we use the log probability of post p recovered from srmid by P (p|srmid ; B ) as the reward of the reconstruction. Mathematically, we de ne the reconstruction reward as rA (r ec ) = log P (p|srmid ; B ).
Language model reward. To generate grammatical responses, we have an immediate language model reward r (LM ), indicating how

natural the output response is in the language. The language model reward for the response r is:

rA(LM ) = LM (srmid )

(6)

We train the language model with LSTM on Wikipedia data. Then

the language model is xed and the log likelihood of a received

message is used to reward the output utterance.

Finally, we simply adopt a linear combination of the reconstruction reward r (r ec ) and language model reward r (LM ) as the total

reward for post agent A:

rA = 1r (r ec ) + 2r (LM )

(7)

where 1 + 2 = 1, and we set 1 = 0.5 and 2 = 0.5.

3.2.7 Updating model parameters. We can explore the stateaction space and learn the policies P (srmid |p; A) and P (p|srmid ; B ) that leads to the optimal expected reward E[r ]. According to the pol-
icy gradient theorem [8], we compute the gradient of the expected reward E[r ] with respect to parameters A and B :

A E[r ] = E[rAA log P (srmid |p; A)]

(8)

B E[r ] = E[1B log P (p|srmid ; B )]

(9)

The reader can refer to [8] for details of policy gradient theorem.

We update the parameters using stochastic gradient descent.

4 EXPERIMENTS
4.1 Datasets description
In the experiments, we rst pre-train the attention LSTM encoder decoder using a large scale general training data (without user speci c information), and then ne-tunes the model on the small size of personalized training data (with user speci c information). The detailed properties of the general dataset and the personalized dataset are described as follow. Sina Weibo conversation data without user information (Sinasource): We use the Sina Weibo dataset introduced by [6] as our general training data. On Sina Weibo 1, a user can post short messages (referred to as posts in this paper) visible to the public or a group of users following her/him, and other users make comments on the published posts ( referred to as responses). This dataset is a collection of 4,435,959 post-response pairs (with 219,905 posts) crawled from Sina Weibo. Sina Weibo conversation data with user information (Sinatarget): We write a crawler to grab tweets from Sina Weibo as our personalized dataset. Following the strategy used in [6], we rst crawl a large number of post-response pairs, and then clean the raw data. Finally, we obtain 51,921 post-response pairs (with 6028 posts). We randomly choose 1000 post-response pairs (with 178 posts) as our test set, and treat the remaining as training set. For each involved user, we crawled his/her pro le information such as user id, province ID user located, city ID user located, user description, gender.
For both dataset, we use Stanford Chinese word segmenter 2 to split the posts and responses into sequences of words. To reduce data sparsity, we construct two separate vocabularies for posts and
1 http://www.weibo.com 2 http://nlp.stanford.edu/software/segmenter.shtml

1023

Short Research Paper

SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan

responses by using 20,000 most frequent words on each side. All the remaining words are replaced by a special token "UNK".
4.2 Baseline Methods
In the experiments, we evaluate and compare our model with several baseline methods: Seq2Seq model [7], Neural Responding Machine (NRM) [6], Speaker-Addressee model (SA) [2], Deep Reinforcement Learning (RL) [3], Personalized Response Model (PRM) [14].
4.3 Implementation details
In this paper, the word embeddings are initialized using 300-dimensional word2vec [4] trained on Chinese Wikipedia Dumps, and these word embeddings are ne-tuned during model training. We initialized the recurrent parameter matrices as orthogonal matrices, and all other parameters from a Gaussian random distribution with mean zero and standard deviation 0.01. We set the encoder and decoder hidden state space sizes as 300. We conduct mini-batch training with Adadelta optimization method.
4.4 Experiment results
In this section, we compare our model with baseline methods from quantitative and qualitative perspectives.
4.4.1 antitative evaluation. Following the same evaluation as in [2], we rstly compare our model with baseline methods in terms of BLEU score, perplexity and human evaluation.

ID

Conversation 1

Post 

Conversation 2 ,?

My life may be fake I will quit smoking,do you believe?

Resp 1





Resp 2

I am sleeping 

Wait and see 

Resp 3

I feel faint 

It's good ?

Resp 4

It is too abstruse 

How could be? ,?

let's follow each other

Resp 5



Really, are you sure? ,

Tired and very happy

Haha, have con dence

Table 2: Some responses generated for ve di erent users to

two real case questions.

questions in Table 2. We randomly select 5 users from the test dataset, and integrate their user vector representations into our personalized LSTM decoder. The model tends to generate speci c responses for di erent people in response to the factual questions.

5 CONCLUSION AND FUTURE WORK
In this paper, we proposed a novel personalized response generation model via domain adaptation. We rst pre-trained the response generation model with an attention LSTM encoder decoder architecture on a large scale general training data, and then ne-tuned the model on a small size of personalized data with a dual learning mechanism. In the future, we plan to devote our e ort to adapt our model to task-oriented dialogue systems.

Evaluation Seq2Seq NRM SA RL PRM PRG-DM

BLEU

1.03% 1.12% 1.15% 1.23% 1.19% 1.32%

Perplexity 40.1 38.5 37.7 35.3 36.1 34.2

Human 0.892 0.932 1.096 1.152 1.158 1.192

Table 1: Quantitative evaluation results.

BLEU scores has been shown to correlate well with human judgment on the response generation task. Table 1 (second line) shows the BLEU scores. Our PRG-DM model achieves BLEU value of 1.32% on the test data, which is 0.09% higher than the best BLEU value of baseline methods(i.e., RL).
Perplexity is a measurement of how well a probability model predicts a sample text. The lower the perplexity, the better the model. We calculate the average perplexity (log-likelihood) of the responses. We summarize the perplexity results in Table 1 (third line). PRG-DM substantially outperforms other models.
We also adopt human annotation to compare the performance of di erent models. We generate the response for each post in the test data. Three researchers who work on natural language processing are invited to do human evaluation. Three levels are assigned to a response with scores from 0 to 2, where 2 denotes suitable response, 1 denotes neutral response, and 0 denotes unsuitable response. The experimental results based on human annotation are summarized in Table 1 (fourth line). PRG-DM outperforms other methods. For example, the average score of PRG-DM is 0.034 higher than the best results of baselines (i.e., PRM) on the test data.
4.4.2 alitative evaluation. To evaluate the proposed model qualitatively, we show the generated responses for two real case

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Preprint arXiv:1409.0473 (2014).
[2] Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. Preprint arXiv:1603.06155 (2016).
[3] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation. Preprint arXiv:1606.01541 (2016).
[4] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. 3111­3119.
[5] Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, and Qiang Yang. 2016. Personalizing a Dialogue System with Transfer Learning. Preprint arXiv:1610.02891 (2016).
[6] Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural Responding Machine for Short-Text Conversation. Computer Science (2015).
[7] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS. 3104­3112.
[8] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229­256.
[9] Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual Learning for Machine Translation. Preprint arXiv:1611.00179 (2016).
[10] Min Yang, Jincheng Mei, Fei Xu, Wenting Tu, and Ziyu Lu. 2016. Discovering Author Interest Evolution in Topic Modeling. In SIGIR. 801­804.
[11] Min Yang, Wenting Tu, Wenpeng Yin, and Ziyu Lu. 2015. Deep Markov Neural Network for Sequential Data Classi cation. In ACL, Vol. 2. 32­37.
[12] Min Yang, Fei Xu, and Kam-Pui Chow. 2016. Interest Pro ling for Security Monitoring and Forensic Investigation. In ACISP. 457­464.
[13] Min Yang, Dingju Zhu, Yong Tang, and Jingxuan Wang. 2017. Authorship Attribution with Topic Drift Model. AAAI (2017).
[14] Weinan Zhang, Ting Liu, Yifa Wang, and Qingfu Zhu. 2017. Neural Personalized Response Generation as Domain Adaptation. Preprint arXiv:1701.02073 (2017).
[15] Zhou Zhao, Lijun Zhang, Xiaofei He, and Wilfred Ng. 2015. Expert nding for question answering via graph regularized matrix completion. TKDE (2015).

1024


Discovering Author Interest Evolution in Topic Modeling

Min Yang
University of Hong Kong Pokfulam Road, Hong Kong
myang@cs.hku.hk

Jincheng Mei
University of Alberta
Edmonton, AB, Canada
ljmei2@ualberta.ca


Fei Xu
Chinese Academy of Sciences Haidian District, Beijing, China
xufei@iie.ac.cn

Wenting Tu
University of Hong Kong Pokfulam Road, Hong Kong
wttu@cs.hku.hk

Ziyu Lu
University of Hong Kong Pokfulam Road, Hong Kong
zylu@cs.hku.hk

ABSTRACT
Discovering the author's interest over time from documents has important applications in recommendation systems, authorship identification and opinion extraction. In this paper, we propose an interest drift model (IDM), which monitors the evolution of author interests in time-stamped documents. The model further uses the discovered author interest information to help finding better topics. Unlike traditional topic models, our model is sensitive to the ordering of words, thus it extracts more information from the semantic meaning of the context. The experiment results show that the IDM model learns better topics than state-of-theart topic models.
1. INTRODUCTION
Author interests play a crucial role in a variety of natural language processing (NLP) application. Motivated by the demand of understanding people's preferences and behavioral pattern, a flurry of researches devote to discovering author interests. The common approach is to build a generative model that represents the author's interests as a latent variable (see Section 2). Most of these models (LDA-style models) make the bag-of-word assumption, which states that the meaning of the document is fully characterized by the unigram statistics of words. The bag-of-word assumption neglects the ordering of words, sentences and paragraphs, though these factors are important in defining the semantics meaning of the context.
We propose a Interest Drift Model (IDM) for modeling the dynamic evolution of individual author's interest. The IDM learns the representation of words, sentences and document as vectors. It also learns the interests of authors as vectors. Since the interests of authors may change over time, each author's interests is represented by a sequence of vectors,
Fei Xu is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914723

where the speed of the interest drifting is controlled by the similarity of consecutive vectors. The IDM model also captures the fact that co-authors usually share common interests, by making their interest vectors positively correlated. The parameters of the model are learnt through a prediction task: given vectors associated with the context, the goal is to predict each word in the document. This scheme has been proven effective in discovering topics from documents [7, 6, 11].
Our model is conceptually related to the work of [11] that incorporates the ordering of words and the semantic meaning of sentences into topic modeling. However, [11] does not consider the authorship and temporal information of the text. In contrast, the goal of this paper is to address the challenges of modeling author interest evolution. Our model has three advantages over the traditional LDA model. First, given the document's authors, the document's topic is determined by the authors' interests, which may change over time. Second, our model provides a natural way to measure the similarity of interest, which helps controlling the speed of interest drift, as well as measuring the similarity between authors. Third, our model takes the ordering of words, sentences and paragraphs into account, so that the topics reflect the semantics of the context.
2. RELATED WORK
The Author-Topic model [8] is the first generative model that simultaneously models the content of documents and the interests of authors. There are also some variants of Author-Topic models such as [5]. These models are devoted to discovering static latent topics and author's interests, without considering temporal information.
To characterize topics and their evolution over time, [2] propose a dynamic topic model (DTM) which jointly models word co-occurrence and time. [9] propose a non-Markov continuous-time model (ToT). These models capture the evolution of topics, but they completely ignore the authorship information.
There are recent works taking both timestamp and authorship information into account, including the TemporalAuthor-Topic (TAT) model [4], the Author-topic over time (AToT) model [10]. Nevertheless, these models do not characterize the drift of the individual author's interests.
All of the aforementioned topic models employ the bagof-words assumption, which is rarely true in practice. [11]

801

propose a generative topic model that represent each topic as a cluster of multi-dimensional vectors. Nevertheless, this model doesn't use temporal information or models the interest of authors.

3. INTEREST DRIFT MODEL
In this section, we describe the IDM model as a generative model. Then we illustrate the inference algorithm for estimating the model parameters.

3.1 Model description
We assume that there are W different words in the vocabulary and there are D documents in corpus. In addition, these documents belong to T topics, where T is a hyperparameter specified by the user. For a specific document d, suppose that it has m authors a1,..., am. We use an interest vector vec(ai, d)  Rp to represent the interests of author ai when he/she writes document d. Two authors have similar interests if the distance between their interest vectors is small. Our goal is to learn these interest vectors from the content of the corpus. It is clear that the interests of co-authors should be correlated. The interests of the same author in writing different documents should consistent.

3.1.1 Author interests
We now specify the generative model for vectors vec(ai, d)  Rp (i = 1, . . . , m), conditioning on the interest vectors of all earlier documents. This defines a valid model, since interest vectors can be generated sequentially from the earliest document to the latest document. Let t be the time that document d was written. Let di be the last document that author ai has written before she writes the current document, and let the timestamps for di be ti. We have ti t for all i = 1, ...., m. We define a joint distribution on the vectors vec(a, d) := (vec(a1, d), ..., vec(am, d)). It is a multivariate normal distribution on Rmp taking the form N (µd, d). Firstly, we specify the mean µd. Let

µd := (vec(a1,d1), ...., vec(am,dm)),

(1)

where vec(ai; di) is the interest vector for author ai when she wrote document di. This definition implies that the new interests of authors have connections to the history.

Second, we define the covariance matrix d. Note that d is an mp × mp matrix, so that it can be partitioned into m2 sub-matrices, each with dimension p×p. Let ij  Rp×p be the sub-matrix of d on the i-th row and j-th column. If i = j, then the sub-matrix characterize the covariance of

the author i's interest. Let

ii

:=

( (t

-

ti ))2

I,

(2)

where (x) is an increasing function of x. It means that as time passing, the covariance matrix entries get bigger, indicating that the interest of author i is less likely to concentrate on its mean ­ the interest vector when she wrote the earlier document d. More concretely, we adapt a linear function (x) =  + x with hyper-parameters  > 0 and  > 0. The values of  and  are chosen by cross-validation.
If i = j, then the sub-matrix ij characterizes the correlation between the interest of the author i and the author j. Let

ij := (t - ti)(t - tj )I,

(3)

where   [0, 1] is another hyper-parameter measuring the correlation of the interest of co-authors. if  = 0, then
vec(a1, d),...,vec(am, d) are mutually independent, meaning

that there is no correlation between co-authors. If  = 1, then the drift vectors vec(ai, d) - vec(ai, di) are in the same direction for all co-authors, meaning that the interest drift
is perfectly correlated. In summary, the density function of
vector vec(a, d) is defined by

p(vec(a, d)

=

v)



( exp

-

(v

-

µd)T d-1(v

-

µd) )

(4)

2

The vector µd and the matrix d follow the definition above.

3.1.2 Words, sentences and documents

The words, sentences and documents are represented by

vectors so that their semantic representations are naturally

connected with that of the author interests. For each word

w  {1, . . . , W }, there is a p-dimensional vector representation vec(w)  Rp. Each document with index d  {1, . . . , D} has a vector representation vec(d)  Rp. There are S sen-

tences in the corpus, each sentence indexed by s  {1, . . . , S}.

The sentence with index s is associated with a vector representation vec(s)  Rp.

We assume that all these vectors are generated from a

multi-variate normal distribution conditioning on the topic.

Given the a topic k, the conditional distribution of vectors are defined by vec(w)  N (µwk ord, wk ord), vec(s)  N (µsken, sken) and vec(d)  N (µdkoc, dkoc) where µk and k represent the mean and the covariance matrix. We assume

thaTkt=1thek

topic k is = 1. Thus,

chosen with probability k such that the vectors are generated from a Gaus-

sian mixture model with unknown mixture weights and un-

known Gaussian parameters. We use  to collectively rep-

resent these parameters.

3.1.3 Word tokens

Given the vectors representing author interests, words, sentences and documents, we describe how the word tokens are generated. Let V be the collection of all latent vectors:

V := {vec(w)}  {vec(d)}  {vec(s)}  {vec(a, d)} (5)

For the i-th word in the sentence, we represent its word realization by wi, which is generated by

p (wi = w|d, a, s, wi-n, . . . , wi-1)

n

 exp(awauthor + awdoc + awsen +

awt )

(6)

t=1

where awauthor, awdoc, awsen and awt are linear transformations of the vectors of the author interest, the document, the sentence and the previous n words. These linear transformations are defined by

awauthor = uwauthor, vec(a, d)

(7)

awdoc = uwdoc, vec(d)

(8)

awsen = uwsen, vec(s)

(9)

awt = uwt , vec(wi-t)

(10)

where uwauthor, uwdoc, uwsen, uwt  Rp are transformation coefficient. These coefficients are unknown parameters of the

model. They are shared across all word slots in the corpus.

We use U to represent this collection of these coefficients.

Finally, we summarize the set of unknown parameters to

be learned: the set V containing the vector of author inter-

ests, words, sentences and documents, the set U containing

the unknown linear transformation coefficients, and the set

 containing parameters that defines the Gaussian mixture

model. We will see that given arbitrary two of these sets,

the remaining set of parameters can be solved by efficient

algorithms.

802

3.2 Topic Inference
Given the model parameters U ,  and the vectors V , we can infer the posterior probability distribution of topics. In particular, for a word w with vector representation vec(w), the posterior distribution of its topic, namely q(z(w)), is easy to derive given the generative model. For any topic z  1, 2, . . . , T , we have

q(z(w) = z) = Tk=z1Nk(Nvec(v(wec)(|wµz)|,µkz,)k) .

(11)

The topics of sentences, documents and author interest can be inferred in the same way. Similarly, we can use the same formula for inferring the author interest vectors' associated topics, i.e.

q(z(a, d) = z) = Tk=z1Nk(Nvec(v(aec, (da),|µdz)|,µkz,)k) .

(12)

The term q(z(a, d) = z) reflects the ratio of the author a's interests in topic z when she writes document d. In the experiments, we demonstrate that such a scheme yields reasonable representation for the author's interests.

3.3 Estimating model parameters
We estimate the model parameters and latent vectors , U and  by maximizing the likelihood of the generative model. The parameter estimation consists of two stages. In Stage I, we maximize the likelihood of the model with respect to . Since  characterizes a Gaussian mixture model, this procedure can be implemented by the Expectation Maximization (EM) algorithm. In Stage II, we maximize the model likelihood with respect to U and  , this procedure can be implemented by stochastic gradient descent. We alternatively execute Stage I and Stage II until the parameters converge.

3.3.1 Stage I: Estimating 
In this stage, the latent vector of words, sentences and documents are fixed. We estimate the parameters of the Gaussian mixture model  = {k, µk, k}. This is a classical statistical estimation problem which can be solved by running the EM algorithm. The reader can refer to the book [1] for the implementation details.

3.3.2 Stage II: estimating U and 
When  is known and fixed, we estimate the model parameters U and the latent vectors  by maximizing the loglikelihood of the generative model. In particular, we iteratively sample a location in the corpus, and consider the log-likelihood of the observed word at this location. Let the word realization at location i be represented by w. The log-likelihood of this location is equal to

 m Ji(U,  ) = log(p( |)) + awauthor + awdoc + awsen + awt + b
t=1

-

log

(

exp(awauthor

+

awdoc

+

awsen

+

 m awt

+

) b) ,

w

t=1

where log(p( |)) is the log-prior of parameters  . The quantities awdoc, awsen, awa and awt are defined in equations (7)(10). Using stochastic gradient descent, we update

vec(wi)



vec(wi)

+



Ji(U,  ) vec(wi)

(13)

uwt i



uwt i

+





Ji(U,  uwt i

)

(14)

Data Set NIPS Enron

LDA 2820 1208

AT 2766 1134

DTM 2685 1095

GMNTM 2490 1049

AToT 2580 1066

IDM 2442 1016

Table 1: Comparison of test perplexity per word

for the word vector vec(wi) and the associated weight uwt i . Similarly, we update vec(s), vec(d), vec(a, d) and uwdoc, uwsen, uwauthor using the same gradient step. Once the gradient update is complete, we randomly sample another location to continue the update. The procedure terminates when there are sufficient number of updates performed, so that both U and  converge to stationary values.
4. EXPERIMENTS
In this section, we test the IDM model on two publicly available datasets. We compare our model with state-ofthe-art topic models with both quantitative and qualitative evaluations.
4.1 Datasets
We use the NIPS paper data and the Enron emails data in our experiment. Data preprocessing is executed before training the models. We remove non-alphabet characters, numbers, pronoun, punctuation and stop words from the text. Then, stemming is applied to reduce the vocabulary size and settle the issue of data spareness. The detailed properties of the datasets are described as follow. NIPS papers (NIPS): This dataset is a collection of papers from the NIPS conference between 1987 and 19991. After data preprocessing, the data set contains 1,740 research papers with 2,037 authors. Each timestamp is determined by the year of the proceedings. Following the preprocessing in [8, 12], the 1,740 papers are further divided into a training set of 1,557 papers and a test set of 183 papers of which 102 are single-authored papers. Enron emails (Enron): This data was originally published by the FERC during its investigations2. We further clean the corpus by removing computer generated files from each user and remove "quoted original messages" in replies. Finally, there are 75,686 messages in our Enron data set. We further divide the data into a training set of 65,686 messages and a test set of 10,000 messages. Each author has at least one document in the training set.
4.2 Baseline methods
In the experiments, the proposed IDM model is compared with several baseline methods, including Latent Dirichlet Allocation (LDA) model[3], Author-Topic (AT) model [8], Dynamic Topic model (DTM) [2], Gaussian Mixture Neural Topic Model (GMNTM) [11] and Author-Topic over Time (AToT) model [10]. We use the same settings for all hyper parameters as in their original papers.
4.3 Implementation details
In our model, the learning rate  is set to 0.05 and gradually reduced to 0.0001. For each word, the previous six words in the same sentence are included in the context. For easy comparison with other models, the word vector size is set to p = 100. Increasing the word vector size further
1Available at http://www.cs.nyu.edu/~roweis/data.html 2http://www.cs.cmu.edu/~enron/

803

Time 1995 1996 1997 1998 1999

Neural networks Bengio Y, Sejnowski T, LeCun Y, Giles C, Ruppin E
Giles C, Horn D, Bengio Y, LeCun Y, Giles C Bengio Y, Tresp V, Niranjan M, LeCun Y, Giles C LeCun Y, Spence C, Niranjan M, Bengio Y, Tresp V Bengio Y, Tresp V, Bengio S, Doucet A, LeCun Y

Table 2: Top 5 most likely authors

Time 1995 1996 1997 1998 1999

M. I. Jordan Mixtures model, HMM, Probabilistic model Belief networks, Graphical models, Mixture model
Belief networks, Graphical models, EM EM, Mixture model, Unsupervised learning Bayesian learning, Graphical models, Probabilistic model

Table 3: Top 5 most likely topics

improves the quality of the topics. To perform comparable experiments with restricted vocabulary, words outside of the vocabulary is replaced by a special token and doesn't count into the word perplexity calculation.

4.4 Quantitative evaluation

To evaluate the predictive power of the proposed model,

we compare test-set perplexity of our model with that of the

baselines on both NIPS and Enron data sets. Following the

same evaluation as in [8], for the NIPS corpus we calculate

the perplexity for the 102 single-authored test document.

Whereas, for the Enron corpus, we calculate the perplexity

for the 1000 held-out documents that are sampled from the

test sets. After running the algorithm that estimates the

vector representation of words, sentences, documents and

author interests, the averaged test perplexity is defined as

(

)

1

exp

- Ntest

w

log p(wd|ad, Dtrain)

,

where Ntest are the total number of words in the held-out test documents and p(wd|ad, Dtrain) is the probability assigned to the words wd in the test document, conditioned on the known authors ad of the test document and training data Dtrain.
Table 1 shows the results of the perplexity comparison on both data sets, with T = 100 topics. The IDM model is clearly better than the other models, as illustrated by its relatively low perplexity score on both data sets. This is because that the IDM model take the authorship information and the timestamps information into consideration. In addition, it also use the ordering of the words.

4.5 Author interest evolution analysis
An interesting feature of our model is the capability of discovering those authors who have drifting interests. This interest-tracking feature is crucial in recommender systems and in assisting investigators finding criminal activities such as insider threats. Due to the limited space, in this experiment, we test our model on the NIPS dataset between 19951999. We take Neural Networks topic as an example. For the selected topic, we choose the top 5 most likely authors according to equation (12). The results are illustrated in Table 2. By examining the author's homepage, our results are consistent with the truth. For example, Bengio was interested in neural network over the whole period, while Sejnowski shown great interest in neural network only in the early phase.

We can also extract individual author's interests over time. In this paper, we report M. I. Jordan, who has published a sufficient number of paper on the NIPS conference. We show the top 3 most likely topics of the corresponding author (by equation (12)). The research interest evolution for the selected authors between 1995 and 1999 are reported in Table 3. As shown by Table 3, the most likely topics of the author are varying during the experimental period. For instance, Jordan's research interest is mainly focused on "expert networks" and "dynamical model" in the early years, then it expanded to Reinforcement learning and Graphical models. This can be verified from his homepage.
5. CONCLUSION
In this paper, we have proposed a interest drift model (IDM) for topic modeling. The IDM model achieves significantly lower perplexity compared to the state-of-the-art methods, indicating that our model finds high-quality topics.
6. REFERENCES
[1] C. M. Bishop. Pattern recognition and machine learning, volume 1. springer New York, 2006.
[2] D. M. Blei and J. D. Lafferty. Dynamic topic models. In Proceedings of the 23rd ICML, pages 113­120. ACM, 2006.
[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993­1022, 2003.
[4] A. Daud. Using time topic modeling for semantics-based dynamic research interest finding. Knowledge-Based Systems, 26:154­163, 2012.
[5] N. Kawamae. Author interest topic model. In Proceedings of the 33rd international ACM SIGIR, pages 887­888. ACM, 2010.
[6] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053, 2014.
[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111­3119, 2013.
[8] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The author-topic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 487­494. AUAI Press, 2004.
[9] X. Wang and A. McCallum. Topics over time: a non-markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD, pages 424­433. ACM, 2006.
[10] S. Xu, Q. Shi, X. Qiao, L. Zhu, H. Jung, S. Lee, and S.-P. Choi. Author-topic over time (atot) a dynamic users interest model. In Mobile, Ubiquitous, and Intelligent Computing, pages 239­245. Springer, 2014.
[11] M. Yang, T. Cui, and W. Tu. Ordering-sensitive and semantic-aware topic modeling. In Proceedings of the 28th AAAI, 2015.
[12] M. Yang, D. Zhu, and K.-P. Chow. A topic model for building fine-grained domain-specific emotion lexicon. In ACL (2), pages 421­426, 2014.

804


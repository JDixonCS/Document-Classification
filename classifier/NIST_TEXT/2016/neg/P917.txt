Not All Links Are Created Equal: An Adaptive Embedding Approach for Social Personalized Ranking
Qing Zhang, Houfeng Wang Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
{zqicl,wanghf}@pku.edu.cn

ABSTRACT
With a large amount of complex network data available, most existing recommendation models consider exploiting rich user social relations for better interest targeting. In these approaches, the underlying assumption is that similar users in social networks would prefer similar items. However, in practical scenarios, social link may not be formed by common interest. For example, one general collected social network might be used for various specific recommendation scenarios. The problem of noisy social relations without interest relevance will arise to hurt the performance. Moreover, the sparsity problem of social network makes it much more challenging, due to the two-fold problem needed to be solved simultaneously, for effectively incorporating social information to benefit recommendation. To address this challenge, we propose an adaptive embedding approach to solve the both jointly for better recommendation in real world setting. Experiments conducted on real world datasets show that our approach outperforms current methods.
1. INTRODUCTION
With a large amount of complex network data available, how to effectively incorporate rich social information for recommendation is an important research topic [1]. Collaborative Filtering (CF) with implicit feedback [7] which is also called personalized ranking [7] is the consideration in this paper, since in most real applications, the collected data of user behaviors, e.g., assigning tags, purchasing products and watching videos, are usually implicit [5] without explicit feedback. To incorporate rich social information, existing approaches utilize social connections in different ways, but all of those hold the same underlying assumption [1] that given social networks, similar users would prefer similar items. In this paper, we argue that this assumption is too strong, and even not reasonable in many real applications, because social link may not be formed by common interest. For example, one general collected social network might be used for various specific recommendation scenarios. To investigate the issue, we first show the following two problems observed from the widely used real-world datasets.
Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914740

(a) Correlation: LastFM (b) Correlation: Delicious

(c) Sparsity: LastFM

(d) Sparsity: Delicious

Figure 1: Subfigure(a,b): The distribution of interest correlation for links. Subfigure(c,d): The distribution of link sparsity.

Problem 1: Noisy Social Link. From Figure (1.a,1.b), we can see that the interest correlation, calculated by Pearson's correlation coefficient, is not high enough to support the assumption in the conventional social regularization based approaches, that the similar user in social networks leads to similar interest. In fact, this is the fundamental reason why social regularization based approach may lead to inconsistent results for different datasets, due to the noisy social patterns, violating the model assumption.
Problem 2: Sparse Social Link. From Figure (1.c,1.d), we can see that the sparsity of the social relation is quite low. A large number of users have only a small fraction of social connections in the entire network. Thus, directly using the social information is hard to achieve the best performance, and even may hurt the performance, due to the underlying noisy links in real applications.
Intuitively, to solve the problem 1, we would like to eliminate the noisy social links, but this will cause the sparse social link problem. Likewise, to solve the problem 2, we would like to create more links, which may also incorporate extra noisy social links causing the problem 1, even though the underlying noisy patterns that may have existed in the sparse links as shown in Figure 1, are not considered. Thus, we are in a dilemma, while only considering one specific problem of the two. In fact, the two tasks are mutually beneficial. If we can incorporate interest information to control social link selection, the noisy link problem could be alleviated, and if we can utilize more clean and higher order social relations, the sparsity problem could be alleviated for benefiting interest targeting. Thus, joint considering the two problems is an intuitive solution.
However, in previous widely adopted Matrix Factorization (MF) framework, the matrix factorizing process is a global process with

917

Figure 2: Model Framework. Joint learning interest and social embeddings with adaptive interest propagation.

low-rank constraint. It is hard to incorporate local interest information to control the global factorization process. To solve this challenge, inspired by the recent revealed equivalency [2] between word embedding (word2vec) [3] and matrix factorization, we propose a novel solution from an embedding perspective instead of the original matrix factorization view, as illustrated in Fig.(2), which allows a more flexible way to model user interest with complex social interactions.

2. A UNIFIED EMBEDDING APPROACH
The goal of the joint embedding is to learn user interest embedding eiu, user social embedding esu and item embedding e i, by maximizing a posterior of AUC ranking based criterion [7] for interest embedding, and maximizing a criterion for social embedding,

L = ln p(| >u, G)  ln(p(>u, G|) · p()), = ln( p(>u |) · p(G|) · p()) (1)

Interest Embedding Social Embedding P rior

where  the model parameters, G the user social information, and >u the ranking-based user interest information.

2.1 Interest Embedding
BPR-opt Criterion. For personalized ranking, a widely employed optimization criterion is Bayesian Personalized Ranking criterion denoted as BPR-opt as shown in Eq.(2). It has a strong theoretical underpinning as a surrogate objective, for optimizing the ranking-based AUC loss [7].

BP R - opt := ln p(| >u) = ln p(>u |)p(),

= ln

(xuij )p()

(2)

(u,i,j)

where  is a sigmoid function (x)

=

1 1+exp(-x)

,

p()

is

a

normal

distribution N (0, ) with zero mean and a diagonal covariance

matrix  = (1/)I, I identity matrix, xuij suggests that user u

is assumed to prefer item i over item j. In the following, we show

how to formulate the ranking score xuij as a function of the model

parameters . In Eq.(2), we define the ranking score as

xuij

=

eue

T i

- eue

T j

(3)

where eu is the user u embedding; e i and e j are the item i and item j embeddings. All these embeddings are the model parameters
which can be learned by our model. Interest Embedding. We model item i embedding e i in Eq.(3)
as a vector representation with real values. We model user u embedding eu in Eq.(3) as a linear combination of eiu and esu, s  [0, 1],

eu = (1 - s)eiu + sesu

(4)

where eiu is the user i interest-specific embedding, and esu is the corresponding social-specific embedding as a structured bias, which will be discussed in the next section.

2.2 Social Embedding
The goal of social embedding is to maximize the likelihood of a specific sequence of vertexes appearing in a user neighbour context, with the fundamental assumption that similar context leads to similar representation. To achieve the goal, formally, given a sequence of vertexes V n = (v0, v1, · · ·, vn), where v  V the vertex set in a social network, in general, we would like to maximize the Eq.(5) over all the training corpus with the window size w,

logP (esi |esi-w, ..., esi+w \ esi )

(5)

i

where \esi excluding esi . For our p(G|), we present a novel so-

Figure 3: A toy example of social network.

lution as a variant of Deep-walk (Perozzi et al. 2014) [6] and LINE (Tang et al. 2015) [9], the state-of-the-art network embedding models, to incorporate both first-order and second-order proximities simultaneously in a unified framework with interest propagation.
Incorporating First-order Proximity. Many existing graph embedding algorithms, have the objective to preserve the first-order proximity [9] directly. However, in real world, the links observed are only a small proportion, and even with much noise. To solve the problem, we do not directly enforce the pairwise constraint on the two connected users. Instead, we add a virtual common node, representing the neighbourhood relation, for each user and its adjacent neighbours. The effectiveness of this modeling approach with the similar idea has been proved in NLP community [8].

DEFINITION 1 (LOCAL: FIRST-ORDER PROXIMITY). The first-order proximity in a network is the local pairwise proximity between two vertices, such as nodes 4,6 in Fig.(3). For each pair of vertices linked by an edge (u, v), the weight on that edge, wuv, indicates the first-order proximity between u and v. If no edge is observed between u and v, their first-order proximity is 0.

The objective function of the first-order modeling is the log likelihood of all users,

N

l1 =

log(P (esui |esAn ))

(6)

n=1 uiAn

where An denote the

denotes the neighbour set of user user i embedding in context An

n as , and

user esAn

context; denotes

esui the

user context embedding esAn . We use a softmax function to define

the probability, esU denotes the entire set of user social embeddings,

P (esui |esAn ) =

exp(esui
esuesU \An+ui

· esAn ) exp(esu

·

esAn ) .

(7)

Incorporating Second-order Proximity. Following previous study [6], we assume nodes occurring in similar contexts tend to have similar meanings. This assumption leads to a more robust

918

way of measuring the similarity between two users, compared with the direct pair-wise link assumption, which may cause overfitting due to the link sparsity and noise problems.
DEFINITION 2 (GLOBAL: SECOND-ORDER PROXIMITY). The second-order proximity between two vertices (u, v) in a network is the similarity between their neighborhood network structures, such as nodes 5,6 in Fig.(3). Let pu = (wu,1, ..., wu,|V |) denote the first-order proximity of u with all other vertices, then the second-order proximity between u and v is determined by the similarity between pu and pv. If no vertex is linked from/to both u and v, the second-order proximity between u and v is 0.

The objective function of the second-order modeling is as follows,

N

l2 = log(P (esui |esci ))

(8)

i=1

where P (esui |esci ) is similarly defined by using a softmax function as in the first-order. We define the user 2-order context embedding,

esci = f (csui1 A(ui), ..., csuikA(ui) ),

(9)

where f () can be sum, average, concatenate or max pooling of context user vectors csuik in set A(ui). In this paper, we use weighted average on user social neighbour set A(ui) (SCHEME-1), which is
different from that of [3] and [6]. The weights here are learnt from
data automatically for adaptive interest propagation. We define the weight p(uikA(ui)|ui) for esuik in Eq.(10), a random-walk like transition probability for each user to propagate interest,

1 p(uikA(ui)|ui) = Z ·

eik eTi , eik ei

(10)

where eu, ej are defined in Eq.(4); p(uikA(ui)|ui) is calculated by the percentage of weights, i.e., the interest embedding based cosine similarity by this edge against the total edge weight Z of a user.
Note that in the above objective, the interest strength is naturally
incorporated for different users by Eq.(10) adaptively. Moreover, we also introduce an alternative scheme to generate
esci (SCHEME-2), which is similar to DeepWalk. The difference is that we adaptively generate the higher order user specific context set A (ui) by an interest-aware random walk process [6] using Eq.(10). Then we use an average f () for those generated vectors in A (ui), instead of using the social neighbour set A(ui) for Eq.(9).
Combination: First-order and Second-order Proximities. To
embed the networks by preserving both the first-order and second-
order proximities with interest propagation, we linearly combine the two objectives for joint learning as ln(p(G|)) in Eq.(1),

l = l1 + (1 - )l2

(11)

where   [0, 1] is the strength weight; l1 and l2 are the first-order objective and second-order objective respectively. The social regularization parameter l can be added ll for controlling the overall social network strength, as in the traditional MF approaches.

2.3 Parameter Learning

We employ the widely used Stochastic Gradient Descent (SGD) algorithm to optimize the objective function in Eq.(1). The main process of SGD is to randomly select a pair of positive and negative feedbacks, and iteratively update model parameters. Specifically, for each training instance (user,positive-item,negative-item), we calculate the derivative and update the corresponding parameters by walking along the ascending gradient direction,

t+1 = t +  L .

(12)



where the model parameters  are user interest and user social embeddings eiu, esu; item embedding e i and user context embeddings esAn , csuik . In the above process, to optimize Eq.(11), we adopt the negative sampling technique (Mikolov et al., 2013) [3] for efficient
learning since it is generally intractable for direct optimization.

3. RELATED WORK
Social recommendation methods focus on using the social network to improve the recommendation performance. Most existing methods are based on Matrix Factorization (MF) with regularization approaches. The social information is typically used as two ways. The first way is to directly use the first-order proximity to construct Laplacian matrix for regularizing the latent user. The limitation is that it only works well if the social is dense and noiseless. To overcome the limitation, recent approaches [1] employ the twice matrix factorization approach, i.e., not only factorizing the original rating matrix but also the social adjacent matrix, and then employ the latent social representation as regularization for users. This approach can be seen as a way of utilizing the second-order proximity due to the equivalency between the word2vec objective [3] and matrix factorization as shown in [2]. While the sparse social link problem may be solved, the noisy link problem is seldom addressed in previous work successfully. The most similar work to this paper is [10], which proposed a model with joint friendship and interest propagation in social network. However, it mainly focus on modeling the explicit feedback using regression-based modeling approach, and did not consider social representation modeling with ranking-based optimization criterion. Different from the above approaches, this work can incorporate both first-order and second-order proximities simultaneously in a unified embedding framework jointly with local interest propagation.

4. EXPERIMENTS

4.1 Experiment Setup

Dataset and Evaluation Metrics. We evaluate our method on two public real-world datasets1, LastFM and Delicious as shown in

Table 1. We use three popular metrics [5], Recall@K (R@K), ND-

CG@K (Normalized Discounted Cumulative Gain@K) and Area

Under the Curve (AUC), to measure the recommendation quali-

ty of our proposed approach in comparison to baseline methods.

k

2reli -1

Recall@k

=

#relevant in T opK #total relevant

,

N DCG@k

=

, i=1 log2(1+i)
IDCG

where reli denotes the relevant degree which is binary in our task,

i.e., 1 denotes it is relevant, 0 denotes it is not relevant and ID-

CG is the optimal score computed using the same form in numer-

ator but with optimal ranking result known in advance. AU C =

1 |U |

1 u |E(u)|

(i,j)E(u) (xui > xuj ), where U user set; the

evaluation pairs per user u are E(u) := {(u, i)  Stest  (u, j) /

(Stest  Strain)}, S denotes observed Set.

Dataset #User #Item Sparsity #Ratings #Avg.R

LastFM 1892 18745 0.28% 92834

49

Delicious 1867 69223 0.08% 104799 56

Table 1: Datasets Statistics. R denotes the #rated per user. The statistics of social relations can be found in Fig.(1).

Settings. We conducted experiments on one validation set and one test set, by randomly drawing new train/test (90%/10%) splits of the observed data, for users with more than 20 items. Our
1Data available at http://grouplens.org/datasets/hetrec-2011/

919

Dataset LastFM s = 0.2,  = 0.7
Delicious s = 0.5,  = 0.4

Metric
R@30 NDCG@30
R@100 NDCG@100
AUC
R@30 NDCG@30
R@100 NDCG@100
AUC

WRMF
0.2221 0.1312 0.4414 0.2008 0.9772
0.1533 0.1031 0.2215 0.1212 0.6813

BPR-MF
0.2383 0.1472 0.4622 0.2119 0.9848
0.1622 0.1119 0.2303 0.1329 0.6962

GBPR
0.2418 0.1518 0.4791 0.2292 0.9866
0.1815 0.1342 0.2441 0.1519 0.7091

GBPR-1
0.2497 0.1515 0.4742 0.2286 0.9855
0.1785 0.1304 0.2432 0.1441 0.7041

MR-BPR
0.2501 0.1597 0.5036 0.2353 0.9886
0.1933 0.1419 0.2520 0.1604 0.7101

EBPR-U
0.2545 0.1613 0.5247 0.2416 0.9902
0.2047 0.1531 0.2597 0.1701 0.7106

EBPR
0.2636 0.1694 0.5428 0.2494 0.9924
0.2280 0.1814 0.2789 0.1974 0.7143

Table 2: Model Comparison. Our best results are achieved while adopting the Scheme-2 on LastFM dataset, and the Scheme-1 on Delicious dataset respectively, for modeling esci as shown in Eq.(9). For simplicity, we do not consider item bias fairly for all methods.

datasets are more challenging than [5], since we have a much larger number of item candidates for each user while predicting. The hyperparameters for baseline methods are optimized via grid search following the original works in the validation set, and afterwards are kept constant in the test set. For the optimization, we iteratively update the parameters until the likelihood does not increase by 0.01 or the maximum iteration limit is reached, i.e., 250 on LastFM; 400 on Delicious. In each iteration, we randomly select instances 2|U | for training. The latent dimension is fixed as 100. For our method, the parameters are randomly initialized within [0, 1]. The learning rate  for SGD is set to 0.01. The optimal parameters  in p() for user embedding eu and item embedding ei are 0.1, 0.1 on LastFM, and 0.01, 0.1 on Delicious respectively. The best social strength parameters l are 1 on LastFM, and 0.001 on Delicious respectively.
Baselines. We compare the following popular and the state-ofthe-art social regularization based models for personalized ranking. WRMF [4] is a weighted matrix factorization method, which uses a point-wise strategy for one-class recommendation. BPR-MF [7] is a well known BPR method with pairwise assumption for item ranking. GBPR [5] is a state-of-the-art BPR method which relaxes BPR's assumption to a group pairwise preference assumption. Here we also introduce additional first-order social connection to construct groups, denoted as GBPR-1. MR-BPR [1] is a state-ofthe-art social regularization based BPR method, as a second-order regularization approach which combines BPR-MF with social matrix factorization. EBPR is our proposed method. In addition, we also test a variant, EBPR-U. EBPR-U denotes using a uniform distribution instead of the weighting in Eq.(10).
4.2 Result and Analysis
From Table 2, we can see that the proposed method consistently performs batter than other baselines in different datasets. It shows the increased recommendation performance in the presence of link sparsity and noise, getting improvements of Recall, NDCG and AUC, with strong baselines. It could be explained that the adaptive interest propagation for social link modeling is important for achieving the good results in real applications. In addition, we show that an appropriate linear combination in Eq.(4) with s can benefit the performance for different datasets while using different second order proximities. In fact, different social networks may have different properties. Empirically, in the process of finding the optimal  in Eq.(11), to balance the strength of different order proximities, we found that our model is more relaying on the firstorder proximity on LastFM, and slightly more relaying the secondorder proximity on Delicious, but adding the complementary one is also a necessary which can improve the performance. In contrast, purely using one specific of the two did not achieve the best results, which verifies our joint proximity modeling is a necessity.

5. CONCLUSION
In this paper, we studied the social personalized ranking problem. To relax the conventional assumption, in practical scenarios we argue that not all links are created equal, and we consider to do automatic social link selection jointly with learning user interest representation for social recommendation. The challenge is that solving the noisy social link problem solely may aggravate the sparsity problem and vice versa. To tackle this challenge, we developed a unified embedding framework to present a new viewpoint of social network regularization, which provides an avenue to allow a more flexible way of incorporating user social information with complex interactions. Experiment results demonstrate the superiority of the proposed method for real world applications.
6. ACKNOWLEDGMENTS
Our work is supported by National High Technology Research and Development Program of China(863 Program)(No.2015AA015 402), National Natural Science Foundation of China(No.61370117) and Major National Social Science Fund of China(No.12&ZD227).
7. REFERENCES
[1] A. Krohn-Grimberghe, L. Drumond, C. Freudenthaler, and L. Schmidt-Thieme. Multi-relational matrix factorization using bayesian personalized ranking for social network data. In Proceedings of WSDM, pages 173­182, 2012.
[2] Y. Li, L. Xu, F. Tian, L. Jiang, X. Zhong, and E. Chen. Word embedding revisited: A new representation learning and explicit matrix factorization perspective. In Proceedings of IJCAI, pages 3650­3656, 2015.
[3] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.
[4] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose, M. Scholz, and Q. Yang. One-class collaborative filtering. In Proceedings of ICDM, pages 502­511, 2008.
[5] W. Pan and L. Chen. GBPR: group preference based bayesian personalized ranking for one-class collaborative filtering. In Proceedings of IJCAI, 2013.
[6] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: online learning of social representations. In Proceedings of KDD, pages 701­710, 2014.
[7] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. BPR: bayesian personalized ranking from implicit feedback. In Proceedings of UAI, pages 452­461, 2009.
[8] F. Sun, J. Guo, Y. Lan, J. Xu, and X. Cheng. Learning word representations by jointly modeling syntagmatic and paradigmatic relations. In Proceedings of ACL, pages 136­145, 2015.
[9] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: large-scale information network embedding. In Proceedings of WWW, pages 1067­1077, 2015.
[10] S. Yang, B. Long, A. J. Smola, N. Sadagopan, Z. Zheng, and H. Zha. Like like alike: joint friendship and interest propagation in social networks. In Proceedings of WWW, pages 537­546, 2011.

920


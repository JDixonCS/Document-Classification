A Comparison of Cache Blocking Methods for Fast Execution of Ensemble-based Score Computation
Xin Jin, Tao Yang, Xun Tang
University of California at Santa Barbara, CA 93106, USA
{xin_jin, tyang, xtang}@cs.ucsb.edu

ABSTRACT
Machine-learned classification and ranking techniques often use ensembles to aggregate partial scores of feature vectors for high accuracy and the runtime score computation can become expensive when employing a large number of ensembles. The previous work has shown the judicious use of memory hierarchy in a modern CPU architecture which can effectively shorten the time of score computation. However, different traversal methods and blocking parameter settings can exhibit different cache and cost behavior depending on data and architectural characteristics. It is very timeconsuming to conduct exhaustive search for performance comparison and optimum selection. This paper provides an analytic comparison of cache blocking methods on their data access performance with an approximation and proposes a fast guided sampling scheme to select a traversal method and blocking parameters for effective use of memory hierarchy. The evaluation studies with three datasets show that within a reasonable amount of time, the proposed scheme can identify a highly competitive solution that significantly accelerates score calculation.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Ensemble methods; query processing; cache locality
1. INTRODUCTION
Ensemble-based machine learning techniques have been proven to be effective for dealing data-intensive applications with complex features and document ranking is a representative application benefiting from use of the large number of ensembles. For example, in the Yahoo! learning-to-rank challenge [7], all winners have used some forms of gradient boosted regression trees, e.g. [8]. The total number of trees
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17 - 21, 2016, Pisa, Italy c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911520

reported for ranking can be upto 3,000 to 20,000 [10, 5, 11], or even 300,000 or more using bagging method [13]. Ranking for large ensembles is expensive. As reported in [14], it takes more than 6 seconds to rank the top-2000 results for a query processing a 8,051-tree ensemble and 519 features per document on an AMD 3.1 GHz core. If such an algorithm is used to compute scores for a large number of vectors in applications such as classification, the total job is also very time consuming.
The previous work addressed the speedup of runtime execution for ensemble-based ranking in several aspects including tree trimming [3] for a tradeoff of ranking accuracy and performance, earlier exit [6], and loop unrolling [4], and ensemble restructuring for a tree-based model [12]. Memory access can be 100x slower than L1 cache and un-orchestrated slow memory access incurs significant cost, dominating the entire computation. The work shown in [14, 12] proposes a cache-conscious blocking method for better cache locality. However, there are other block methods to select and it is an open problem how to identify the best cache blocking method and parameter settings given different data and architecture characteristics. Experimentally determining this choice can be extremely time-consuming and the comparative result may not be valid any more with a change of underlying feature vector structure or architecture. This paper provides an analysis of multiple blocking methods with different data traversal orders, which provides better insight on program execution performance and leads a fast approximation to select the optimized structure.
Here, we consider the fast computation of ensemble-based scoring that aggregates and derives final scores for n feature vectors using m ensembles. For testing and comparing performance in ranking q sampled queries, the time cost for searching through all combinations can be as high as O(m2  n2  q). The main contribution of this work is to develop an analytic framework to compare memory access performance of data traversal under multi-level caches to find the fastest program execution with effective use of memory hierarchy. Our scheme results in a much smaller complexity with O(m  n  q). Our experiments with three datasets corroborate the effectiveness of search cost reduction while the guided approximation identifies a highly competitive blocking choice. We also demonstrate the use of this scheme with QuickScorer [12] and for batched query processing.
The rest of the paper is organized as follow. Next, we describe the background information and related work. Section 3 discusses the design considerations. Section 4 gives a comparative analysis on different blocking methods. Section

629

5 presents evaluation results. Finally, Section 6 concludes the paper.
2. BACKGROUND AND RELATED WORK
Given n feature vectors and an ensemble model that contains m scorers, these vectors and scorers fit in memory. The ensemble computation calculates a score for each feature vector and each scorer contributes a subscore to the overall score for a vector. For example, for ranking a document set with an additive regression tree model [8, 5], each document is represented as a feature vector and each tree can be stored in a compact array-based format [4]. Following the notation in [6], Algorithm 1 shows the DS method with the two-loop standard execution order. At each out loop iteration i, all scorers are used to gather subscores for a vector before moving to another vector. The dominating cost is slow memory accesses when scorers read feature vector values and update partial values.
Algorithm 1: DS standard method for score calculation.
for i = 1 to n do for j = 1 to m do Update score for vector i with scorer j.

Tang et. al [14] proposed a 2D cache blocking structure called SDSD as depicted in Algorithm 2 which partitions the program in Algorithm 1 into four nested loops. The inner two loops process d feature vectors with s trees. To simplify the presentation, we assume n/d and m/s are integers. By fitting the inner block in fast cache, this method can be much faster than DS. There are other possible cache blocking methods with different data traversal orders and it is an unanswered question on how to choose among them. Also, in [14] there is no cost analysis on how to set a proper parameter for the size of blocking in terms of s and d values. While choices of their values can be restricted to fit in the fast cache, they can still be fairly large. For example, s and d can still reach upto 3,276 and 11,440 respectively in some of our experiments shown in Section 5. Assume m and n are smaller than these upper bound numbers, s ranges from 1 to m and d ranges from 1 to n and there are m  n combinations to compare as they all fit in different levels of cache. Since running each test query takes O(n  m  q), the total cost is O(m2  n2  q). For instance, given n = 10, 000, m = 3, 000, q = 1000, the total time takes over 1,141 years with one core, assuming it takes 40 nanoseconds to compute a partial score for a vector with a scorer. If we sample each of s and d values with step gap 100, the total one-core time is over 41 days without knowing if such sampling finds a solution competitive to the optimum. While running such a sampling can be fully parallelized, we still need a faster scheme with well-guided approximation.

Algorithm 2: 2D blocking with SDSD structure.

for j = 0 to for i = 0

m
s
to

-
n d

1 do - 1 do

for jj = 1 to s do

for ii = 1 to d do

Update score for vector i × d + ii with

scorer j × s + jj.

There are other performance speedup techniques proposed in the previous work to speedup fast ranking score computation, which can be summarized into two categories. The first category is to achieve a tradeoff between ranking efficiency and accuracy. In [6], an early exit optimization was developed to reduce scoring time while retaining a good ranking accuracy. In [16, 17], ranking is optimized to seek the tradeoff between efficiency and effectiveness. Asadi et.al [3] considered the fact that compact, shallow, and balanced trees yield faster computation and generated such trees with trimming technique. The second category is to improve efficiency given a fixed model. The work in [4] proposed an architecture-conscious solution called VPred that converts control dependence of code to data dependence and employs loop unrolling with vectorization. Lucchese et.al proposed the QuickScorer (QS) algorithm [12] which traverses multiple trees in an interleaved manner and accelerates with bit-wise operations. They propose a block-wise variant of QS (called BWQS) by partitioning trees into blocks and applying QS to each block of trees. Given different dataset characteristics, it is an open problem how to find the optimal partitioning. Also there are other ways to arrange blocking and our work is complementary and can be used to compare different options.
3. DESIGN CONSIDERATION AND COST MODEL
There are six ways of loop blocking depending on the order of data traversal: DSD, SDS, DSDS, DSSD, SDDS, and SDSD. Following the naming in [15], symbol D here stands for a loop control over feature vectors and S stands for a loop control over scorers. For example, DSDS means that feature vector traversal is controlled by the outermost and the third outermost loops while scorer traversal is controlled by the second and the innermost loops. The inner two loops access d vectors and s scorers.
Figure 1 illustrates the execution and data traversal order of these methods. Figure 1(a) shows that DSD initially visits one scorer and d vectors. Then it visits another scorer and the same d vectors. Figure 1(b) depicts that SDS initially visits one vector and s scorers. Then it visits another vector and the same s scorers. Figure 1(c) illustrates DSDS which visits scorers and vectors block by block and row by row. Figure 1(f) illustrates SDSD which visits scorers and vectors block by block and column by column.
Our objective is to compare these blocking methods and find a value for s and d to minimize the time cost of score computation under a constraint 1  s  m, 1  d  n. We have the following considerations.
· For DSD, when the inner most loop uses d = 1, it becomes a special case DS which is the same as the traditional loop structure DS shown in Algorithm 1. For SDS, when the inner loop uses s = 1, it becomes a special case SD.
· The traversal order of DSSD during execution is the same as that of DSD as illustrated in Figure 1(a) and (d). Thus DSD can represent both during our analysis. Similarly, the traversal order of SDDS is the same as that of SDS as shown in Figure 1(b) and (e). As a result of the above argument, the six types of control are reduced to four.

630

(a) DSD

(b) SDS

(c) DSDS

(d) DSSD

(e) SDDS

(f) SDSD

Figure 1: Data traversal order of cache blocking methods during execution

Figure 2: Data access flow of CPU with memory hierarchy.
The following parameters are used in assessing the average memory access cost of processing n feature vectors with m scorers. We assume that CPU has three levels of caches: L1, L2, and L3 and the three level setting is popular in the currently available processors from Intel and AMD. Let 1 be the read or write cost of accessing L1 and cost for accessing other cache is 1 multiplied by a constant ratio. Namely c21 is the cost of accessing L2, c31 is the cost of accessing L3, and c41 is the cost of accessing memory.
Our analysis separates the cost for accessing feature vectors and scorers. Without losing the generality, let Ai be the total amount of data access to feature vectors at cache level i + 1 while Ai be the total amount of accesses to scorers at cache level i+1 and  is the average frequency ratio between access of feature vectors and scorers during computation.
The total data access cost is the summation of the cost of accessing each level of memory hierarchy:
Cost = A01(1 + Dc2 + DDc3 + DDDc4)
+ (A01(1 + S c2 + S S c3 + S S S c4)).
where S, S, S, D, D, and D are the miss rates of L1, L2 and L3 to access scorers and feature vectors respectively. Data accesses flow from CPU to memory for feature vectors is illustrated in Figure 2. A1 = A0D is the total number of feature data access to L2 due to their misses to L1; A2 = A0DD is the total number of feature data access to

L3. A3 = A0DDD is the total number of data access to memory.
Then the time cost divided by A01 is defined as
C ost T = 1A0 = TS + TD
where TD = 1 + Dc2 + DDc3 + DDDc4 and TS = 1 + S c2 + S S c3 + S S S c4.
Since A0 and 1 are constants, in the rest of the analysis, we focus on computing the above data access cost ratio T .
Notice that once data is brought from memory hierarchy, the arithmetic computing cost of all four methods is the same. Thus we just need to analyze and compare the data access cost ratio T for the four traversal methods. In practice, data access cost often weights more than arithmetic cost.
Due to the restriction on the paper length, we first present the analysis of cache performance for DSD and then list the result of DSDS, SDSD, and SDS as the case subdivision and cost derivation process are similar. Finally we describe an approximate scheme to select the best structure by taking advantages of the derived data access cost ratio for the four methods.
4. COST ANALYSIS AND COMPARISON
4.1 Time cost for DSD
4.1.1 Cases under consideration
Algorithm 3: The program structure of DSD method. for all vector blocks do for i in all scorers do for j in a vector block do Update score for vector j with scorer i.

631

Algorithm 3 lists the program control structure of DSD and a vector block contains d vectors. Once a scorer si is loaded to cache, it will be used by d vectors in the inner most loop. Then the next scorer si+1 will go through the same d vectors. If we choose d properly such that d vectors fit in cache, we do not need to load them from memory for each scorer. Figure 3 illustrates how the cost of score computation could change when value d increases from 1 to n. The impact of d value on the cost is segmented with respect to the size of L1, L2, and L3. When d is small, the d vectors can fit in L1 cache, and there is an advantage of reusing these d vectors within L1 cache. Thus d should be as large as possible. When d value becomes too big, the benefit of leveraging L1 cache decreases because d vectors may not fit in L1 any more and therefore the access cost can increase with larger d value. We can reason similarly when d vectors fit or do not fit in L2 and L3 caches.

Otherwise, old vectors will be kicked out from L1 cache by the scorer and the vector block could not stay in L1 cache. Thus there are only three cases for the d vectors as depicted in the second root branch of Figure 4: the vector block fits in L2 cache, L3 cache or memory.
· Scenario 3: L2 < Ssize  L3. When a scorer size is inbetween L2 and L3 cache size, there are two cases for the d vectors: the vector block fits in L3 cache or memory.
· Scenario 4: Ssize > L3. When a scorer cannot fit in L3 cache, the d feature vectors will not be able to fit in L3 cache also and they can only fit in memory.

Figure 3: Performance under different values of d.

We will clarify the tradeoff of increasing d value when we derive a more concrete analysis. Let F size be the average data size of each feature vector. Without introducing more symbols, we also let L1, L2 and L3 represent the size of L1 cache, L2 cache and L3 cache respectively in a formula expression. To assess the impact of increasing d values, we divide the increasing range into four parts as illustrated in Figure 3.

·

d

vectors

fits

in

L1

cache.

Namely

d

L1 F size

· d vectors do not fit in L1 cache, but fit in L2 cache.

L1 F size

<d



L2 F size

· d vectors do not fit in L2 cache, but fit in L3 cache.

L2 F size

<d



L3 F size

·

d vectors exceed L3 cache and but fit in memory.

L3 F size

<

d  n.

The cache access behavior of inner most loop in Algorithm 3 is affected by the average size of each scorer. For example, a larger scorer footprint leaves little space for L1 to host feature vectors. Figure 4 illustrates that we need to consider the following four scenarios and for each scenario, we need to further consider the four d range cases discussed above. Let Ssize represent the average data size of each scorer and the four scenarios corresponding to the root branches in Figure 4 are defined as follows.

· Scenario 1: Ssize  L1. When a scorer can fit in L1 cache, there are four cases for the d vectors: the vector block fits in L1 cache, L2 cache, L3 cache or memory.

· Scenario 2: L1 < Ssize  L2. When a scorer size is between L1 and L2 cache sizes, the vector block can only fit in a higher level of cache (say L2 or L3 cache).

Figure 4: Range cases of d considered under different scenarios for DSD.

To simplify the analysis, we assume that m and n are sufficiently large so that m scorers do not fit in L3 cache, and also n feature vectors do not fit in L3 cache.
4.1.2 DSD under Scenario 1
Under Scenario 1, we first compute TS as follows. Since each scorer is loaded once for the inner loop most and will reused d times for computing the subscores for d vectors in the inner most loop. Then the L1 cache miss ratio S  1/d. If there is an L1 cache miss for a scorer, L2 cache miss and L3 cache miss can occur with a high chance because the unseen new scorer has not been used ever and thus it is fetched from memory. Thus S  1 and S  1.

TS



1+

c2 d

+

1 d (c3

+

c4)



1

+

c4 d

We shall estimate TD under 4 different ranges of d values following Figure 3. We call these 4 range cases under DSD as DSDi where 1  i  4. The total cost ratio of accessing scorers for DSD is

TDSDi

=

C ost 1A0

=

TS

+

TD





+

c4 d

+

TD

where

TD = 1 + c2i + ii(c3 + c4i)

(1)

and i, i and i are cache miss rates for accessing feature vectors under range case i. Note that in differentiating these miss rate of different cases, we use script "i" instead of "D, i" in order to simplify the presentation. Table 1 summarizes the cost of DSD for Scenario 1 when each scorer fits in L1 cache on average.
Range Case DSD1: d vectors fit in L1. Once a scorer is loaded to L1, the inner most loop load d feature vectors to L1 and these vectors stay and will be available in L1 when a new scorer is fetched to L1. Given m scorers, each feature

632

vector in L1 is accessed m times and there is one 1 miss initially and the rest of m - 1 accesses will hit L1. Thus the L1 cache miss ratio with respect to feature vectors is 1  1/m. If there is an L1 cache miss for a feature vector, there must be an L2 cache miss and L3 cache miss. Thus, 1  1 and 1  1. Plugging into Equation 1, we get the total cost ratio:

TD



1

+

c2 m

+

1 m

·

(c3

+

c4).

Range case DSD2: d vectors fits in L2. Once a scorer is loaded to L1, the inner most loop can load a feature vector and keep it at least at L2 when a new scorer is loaded. Given there are m scorers, A2/A0  1/m. Namely 22 = A2/A0  1/m. Since L2 can hold d vectors needed for inner most loop, A2  A3. Thus 2 = A3/A2  1.

1 TD  1 + 2 · c2 + m · (c3 + c4). Range case DSD3: d vectors fit in L3. Once a scorer is loaded to L1, the inner most loop can load a feature vector and keep it at least at L3 when a new scorer is loaded. Given there are m scorers, A3/A0  1/m. Namely 333 = A3/A0  1/m.

1 TD  1 + 3 · c2 + 33 · c3 + m · c4. Range case DSD4: d vectors donot not fit in L3. In this case, A0  A1  A2  A3. In this case, actually we put n documents in the inner loop. It's obvious to see that L1, L2 and L3's cache miss ratio are all 1 because comparing to the memory size, even L3 cache size is too small. 4  1, 4  1 and 4  1.
TD  1 + c2 + c3 + c4.

Cases
DSD1 DSD2 DSD3 DSD4

d vectors fit in
L1 L2 L3 memory

TDSDi = Ts + TD 



+



c4 d1

+1+

c2 +c3 +c4 m



++d+c43+dcdc42441+++11+3+c2c22+c+2+c33c3+3cm +3cc4+4

c4 m

Table 1: Cost of DSD when 1 scorer fits in L1.

4.1.3 Other Scenarios of DSD

For Scenario 2 when a scorer fits in L2 on average, there

are only 3 range cases: DSD2, DSD3, and DSD4. L1 miss

rate

in

TD

becomes

1

and

TS

adds

c2

as

TS



1 + c2

+

c4 d

.

For Scenario 3 where a scorer fits in L3, there are only 2

possible cases to consider: DSD3 and DSD4. L1 and L2

miss

rates

in

TD

become

1

and

TS

adds

c3

as

TS



1+c3

+

c4 d

.

For Scenario 4 where a scorer fits memory only, there is

one case to consider: DSD4. Its TD does not change while

TS  1 + c4.

4.2 Cost Comparison of the Four Methods

The data access cost for SDS, DSDS, and SDSD is listed in Appendix A. There is a total of 28 range cases considered in these four methods: DSDi, SDSi, DSDiSj, and SDSiDj where 1  i, j  4. The cost results of these 28 cases can be used in the following two aspects.

· Identify the approximated optimum selection from 28 cases instead of exhaustive search of all combinations. The selection algorithm goes through 28 cases and runs the average time performance sampling through a benchmark of q queries.

From Tables 1, Tables 8, Tables 9, and Tables 10, in-

creasing d or s under its range limit decreases the time

cost. Thus d and s should be chosen to be as large as

possible. On the other hand, scorers and vectors share

each level of cache and we set a constraint that vectors

and scorers accessed in the inner most loop of DSD and

SDS or in the two inner most loops of DSDS and SDSD

fit in the corresponding cache. For example, as a mid-

point approximation, we let d vectors occupy upto half

of each cache level and s scorers occupy upto half of

each cache level. Namely, for 1  i  3, d and s are

chosen

for

each

range

case

as:

di

=

0.5Li F size

,

si

=

0.5Li Ssize

.

As all n vectors and m scorers fit in memory, d4 = n

and s4 = m.

· Narrow the search scope when characteristics of a dataset

or targeted machine architecture is given because many

of 28 cases may be eliminated. That facilitates the

reduction of search space and allows more sampling

points at each range case selected as long as time com-

plexity permits. For example, the above midpoint

sampling for each range case after elimination can be

expanded as follows. Since each scorer may only ac-

cess a fraction of a feature vector and a fraction of

the scorer data structure for computation, we choose

sampling points as di =

0.5Li µF size

,

si

=

0.5Li µSsize

.

Coeffi-

cient µ represents the average data usage of a vector

or a scorer during computation and for example, we

sample more points with µ as 1, 0.75, 0.5, and 0.25.

To illustrate the second point above, we show that the following proposition is true and can narrow the search scope from 28 to 4 range cases.

Proposition 1. When each feature vector fits in L1 and

each

score

fits

in

L1

on

average,

and

c4 d2

1,

c4 s2

1, the

candidates with the lowest access cost are among range cases

DSD2, DSD2S1, SDS2D2, and SDS2D1.

A proof is listed in Appendix B. Yahoo!, MS, and MQ

datasets discussed in Section 5 fall into the condition of this

proposition when each regression tree used is not too big

(e.g. containing upto 50 leaves). The range of d and s

values for these datasets is listed in Table 3 and Table 2

of Section 5. When a regression tree contains 150 leaves,

ratio c4/s2 is getting close to 1, cases SDS3D1 and SDS3D2

can be competitive as a best candidate. Thus with such a

condition, we can search for 6 cases instead of 28 cases.

In summary, a guided sampling scheme conducts the fol-

lowing steps. 1) Identify data and architecture parameters.

When possible, apply Proposition 1 or its variation to elim-

inate some of 28 range cases from the cost analysis of DSD,

SDS, DSDS, and SDSD. 2) For each of selected range cases,

choose blocking factor di and si under a constraint that vec-

tors and scorers accessed in the inner most loop of DSD and

SDS or in the two inner most loops of DSDS and SDSD fit in

the corresponding level of cache. One approach is to choose

di

=

0.5Li µF size

si

=

0.5Li µSsize

with

a

number

of

sampled

µ

values.

3) Run and collect the average query response time with m

633

scorers and n vectors from each sampled case. Select the case and parameter setting with the lowest response time. The total complexity of this scheme with q test queries is O(m  n  q).
4.3 Discussions
Integration with the QuickScorer method. When the ensemble computation uses the original computing algorithm for gradient boosted regression trees (e.g. [8, 5]), the main data structure of each scorer is a tree. To use the BWQS algorithm [12], we treat each scorer as the application of QS on a block of trees. The following parameters are involved: the size of a scorer changes when different partitioning is adopted while the number of inner-loop scorers (s) and inner-loop vectors (d) can vary too for different blocking methods. Thus we add a partitioning search loop on the top of the aforementioned comparison and sampling scheme to select the best partitioning.
Batched query processing. When the ensemble score computation is used for query processing where n is small, d value of the inner most loop limited by n can be insufficient to explore the cache locality and the effectiveness of blocking degrades. When batch processing is allowed, we can boost the cache utilization by processing feature vectors from multiple queries in fast cache, which essentially raises n values. One application of such batched processing is to conduct an offline experiment to assess the ranking performance of an algorithm in answering a large number of queries and there is no need to output ranking results immediately.
For an online ranking application, the ranking results need to be produced promptly. While reaching a high throughput, batching a large number of queries can increase the average waiting time of batched queries and affect the response time. With this constraint in mind, we set a limit on the largest waiting time allowed in choosing a batch size for a higher throughout with a modest increase of response time.
5. EVALUATIONS
5.1 Settings
This section provides an experimental comparison of different cache blocking methods and validates the effectiveness of the selected method with unoptimized ones. The evaluation tasks are listed as follows: (1) Illustrate the fast comparison of the 28 range cases for using DSD, SDS, SDSD and DSDS with guided sampling. (2) Integrate our cache blocking selection algorithm with the QS algorithm [12] for tree-based ranking. (3) Assess the batched query processing in improving the throughput when n is small.
We implement the blocking methods using C compiled with GCC optimization flag -O3. Experiments are conducted on a Linux CentOS 6.6 server with 8 cores of 3.1GHz AMD Bulldozer FX8120 and 16GB memory. FX8120 has 16KB of L1. We set L2  1M B as 2MB L2 cache is shared by two cores. Its 8MB L3 cache is shared by 8 cores and since L3 hosts tree data useful for multiple queries, we set L3  2M B. The cache line is of size 64 bytes. For AMD Bulldozer, c2 is around 7.3, c3 is around 25.1, and c4 is around 80.9. We have also conducted experiments in a 24core Intel Xeon E5-2680v3 2.5 GHz server with L1 = 32KB, L2 = 256KB,, and L3  2.5M B per core. The Intel results are similar and thus we mainly report the AMD numbers.

The following learning-to-rank datasets are used as evaluation benchmarks. (1) Yahoo! dataset [7] with 700 features per document feature vector. (2) MSLR-30K dataset [2] with 136 features per document vector. (3) MQ2007 dataset [1] with 46 features per document vector. Table 2 shows the range of d values when fitting d vectors in different cache levels for these 3 datasets.

d vectors fit in
L1 L2 L3 Memory

Yahoo!
d5 d  373 d  747 dn

MS
d  30 d  1928 d  3856
dn

MQ
d  89 d  5720 d  11440
dn

Table 2: The vector counts for fitting in differnt cache levels.

We use LambdaMART [5] for ranking with additive tree ensembles and derive tree ensembles using the open-source jforests [9] package. To assess score computation in presence of a large number of trees, we have also used a bagging method [13] to combine multiple ensembles and each ensemble contains additive boosting trees. Because the size of a scorer affects the cache performance and parameter choices, we generate the size of each tree with several settings: 10 leaves per tree, 50 leaves per tree, and 150 leaves per tree. Table 3 shows the range of s values when fitting s scorers in different cache levels under three choices of the regression tree size. The  value is about 1 because the basic access operation of a scorer is to fetch 1 tree node and then a document feature. Each of them fits in one cache line. The default total number of trees used is about 20,000 for Yahoo! dataset, 10,000 for MS, and 4,000 for MQ. We also use other numbers of trees in our experiments. When using the QS method [12], each scorer is a meta tree merged from multiple trees and  value is around 4 because the basic access operation of a scorer fetches elements from 4 data structures and then a document feature.

s scorers fit in
L1 L2 L3 Memory

10 leaves
s  25 s  1638 s  3276
sm

50 leaves
s5 s  327 s  655 sm

150 leaves
s=1 s  109 s  218 sm

Table 3: The tree counts for fitting different cache levels.

The above data sets contain 23 to 120 documents per query with labeled relevancy judgment. In practice, a search system with a large dataset ranks thousands or tens of thousands of top results after a preliminary selection. To evaluate the score computation in such a setting, we synthetically generate more matched document vectors for each query. In this process, we generate relatively more vectors that bear similarity to those with low labeled relevance scores, because a large percentage of matched results per query are less relevant in practice. The number of vectors per query including synthetically generated vectors varies from 3,000 to 10,000 for Yahoo! dataset, from 2,000 to 6,000 for MS, and from 1,000 to 4,000 for MQ.
Metrics. We mainly report the average time of computing a subscore for each vector under one tree. With n matched vectors scored using an m-tree model, this scoring

634

Time(ns) L3 miss ratio(%)
Time(ns) L3 miss ratio(%)

time multiplied by n and m is the scoring time per query. The throughput is the number of feature vectors scored per second. The number reported here is measured in a multicore environment where each query is executed in a single core.

5.2 A comparison of cache blocking methods
Table 4 shows the score computing time of a vector per tree in nanoseconds under different cache blocking cases for Yahoo!, MS and MQ datasets. "Y! 10" means Yahoo! dataset and each regression tree has 10 leaves. Row 2 is the scoring time of DS without cache blocking. The cost of all 28 cases under 4 cache blocking methods using guided sampling are listed, starting from Row 3. Under Proposition 1, our scheme searches the optimum only from four cases DSD2, DSD2S1, SDS2D1, and SDS2D2. The corresponding entries in this table are marked in a gray color. For Yahoo! dataset with 150 leaves per tree, as we discussed in Section 4.2, extra two cases SDS3D1, and SDS3D2 are also compared and thus marked in a gray color. For each column from column 2, entry marked ` ' indicates the smallest value is found and this entry is considered to be highly competitive. Our comparison scheme selects DSD2 as the best range case with d = 373 for Yahoo! dataset under all three tree size settings, d = 1928 for MS 50 leaves case and d = 5720 for MQ 10 leaves case. It selects SDS2D2 with d = 1928 and s = 1638 for MS 10 leaves case.

220 200 180 160 140 120 100
80 60 40 20
1

90

Time

L3 miss

80

70

60

50

40

30

20

10

0 10 100 1000 10000100000

d value

(a) Yahoo! 50 leaves

400

80

Time

350 L3 miss

70

300

60

50 250
40 200
30

150

20

100

10

50 1

0 10 100 1000 10000100000

d value

(b) Yahoo! 150 leaves

Figure 5: Time cost and cache miss of DSD as d varies.

The running cost of the above guided sampling in CPU hours with one core is shown the second row of Table 5 and can be completed within about 10 hours using a 8core server. We have also conducted exhaustive search with greatly-increased sampling points to obtain an estimated optimum solution. The best cases identified in the estimated optimum are listed in the third row of Table 5 and exactly match what has been selected by our guided sampling scheme. The fourth row of the Table 5 shows the sample error which is the cost difference ratio between the optimum solution and the solution approximated by our scheme. The difference is within 2.2%. The fifth and sixth rows are the best cases and difference ratio obtained on the Intel machine. The error is within 2.4% while all best cases of the estimated optimum match those of the approximated solution. The above result shows that our guided sampling can find a highly competitive blocking solution within reasonable hours using a modest server and such a solution can result in upto 6.57x response time reduction compared to DS without cache blocking.
Impact of blocking size on time cost and cache miss rate. In Section 4, we have used Figure 3 to illustrate the correlation between data access time cost and blocking

DS DSD1 DSD2 DSD3 DSD4
DSD1S1 DSD2S1 DSD3S1 DSD4S1 DSD2S2 DSD3S2 DSD4S2 DSD3S3 DSD4S3 DSD4S4
SDS1D1 SDS2D1 SDS3D1 SDS4D1 SDS2D2 SDS3D2 SDS4D2 SDS3D3 SDS4D3 SDS4D4
SDS1 SDS2 SDS3 SDS4

Y! 10
59.83 33.04 14.09 25.51 54.06
41.56 21.55 25.71 40.13 29.77 30.49 29.73 39.58 46.13 59.81
60.67 23.75 27.08 31.58 14.5 15.71 15.55 20.76 26.28 54
42.96 30.91 46.11 59.83

Y! 50
214.29 99.00 39.11 71.45 126.32
125.44 50.41 77.75 120.84 72.42 72.79 72.74 79.73 105.91 217.14
149.17 58.28 72.31 98.72 39.75 42.68 45.46 59.03 73.81 131.58
113.73 72.22 107.19 214.29

Y! 150
374.56 193.95 78.11 143.55 241.47
237.35 84.87 141.07 235.36 123.3 124.16 123.07 131.6 157.47 375.24
267.69 102.93 130.48 192.1 82.26 88.05 88.45 120.15 143.4 250.17
240.89 122.22 156.67 374.56

MS 10
59.67 17.34 14.37 25.95 42.37
24.46 17.31 18.1 19.93 30.38 31.25 31.35 40.73 46.2 59.09
23.67 16.28 16.85 17.47 13.21 15.09 15.07 21.55 26.28 42.46
21.39 31.65 46.56 59.67

MS 50
206.98 47.79 31.49 49.72 77.75
63 38.61 40.38 52.93 67.33 67.71 68.45 76.2 100.58 210.08
62.61 40.53 43.22 48.65 32.23 37.30 34.52 50.02 52.29 81.7
50.83 67.28 100.75 206.98

MQ 10
34.89 9.82 8.52 13.84 20.13
13.67 11.4 11.6 11.83 19.5 19.88 19.85 22.39 28.16 34.95
10.99 9.52 9.6 9.83 8.68 10.12 10.03 13.04 13.93 20.13
12.62 20.27 28.78 34.89

Table 4: Scoring time of one vector per tree in nanoseconds for different cache blocking range cases.

size in deriving the cost for DSD. Figure 5 shows the experimental result to validate. This figure shows time cost curve of DSD and L3 miss rate measured using Linux tool perf when d value varies for Yahoo! dataset with 50 leaves (a) and 150 leaves (b) per tree. When d is too small, cache is not fully utilized and the cost of TS for DSD derived in Section 4.1.2 is large. When d is too big which falls into case DSD3 or DSD4, the cost curve matches the analysis in Appendix A that TDSD4 > TDSD3 > TDSD2 .
There is also a correlation between the overall time cost and L3 cache miss when d varies. For small d, the coefficient for L3 cost c3 in TS is big. For large d, there is more L3 cache miss, making TD bigger as shown in Section 4.
Impact of m and n values on time cost. Figure 6 shows the time cost per tree per document when m changes from 2,000 to 20,000 for all datasets. In this experiment, we generate extra trees for MS and MQ datasets. It shows that with sufficiently large value of m, the cache behavior does not change much and the processing cost is about the same for different m values.

Comp. time Best AMD Error AMD Best Intel Error Intel

Y! 10 10.67h DSD2 0.21% DSD2 1.4%

Y! 50 27.09h DSD2 0.15% SDS2D2 2.4%

Y! 150 81.86h DSD2 0.10% SDS2D2 0.64%

MS 10 2.719h SDS2D2 0.61% DSD2 0.18%

MS 50 6.349h DSD2 2.2% SDS2D2 0.52%

MQ 10 0.424h DSD2 0.47% DSD2 0.14%

Table 5: CPU hours for comparison, sampling errors, and best cases.

Figure 7 shows the time cost per tree per vector when n changes from 1 to 100,000. When n is smaller than 100, the performance drops significantly and cache is not fully utilized. When n is larger than 1000, the cost becomes stable

635

Time(ns)

80 70 60 50 40 30 20 10
0 0

Yahoo! 50 leaves Yahoo! 150 leaves
MS 50 leaves MQ 10 leaves
5000 10000 15000 20000 25000 m value

Figure 6: Scoring time per vector per tree when m changes.

Time(ns)

400 350 300 250 200 150 100
50 0 1

Yahoo! 50 leaves Yahoo! 150 leaves
MS 50 leaves MQ 10 leaves
10 100 1k n value

10k 100k

Figure 7: Scoring time per vector per tree when n changes.

and there is no much reduction. We will discuss the experiment results when batched query processing is allowed shortly.
5.3 Selective cache blocking for QuickScorer
We integrate our scheme with the BWQS algorithm [12] as follows. Step 1: Given m trees and let  be the number of trees that will be merged to use the QS method. The number of scorers is m = m/ . Step 2: Given m scorers and n vectors, use our scheme to find the best blocking method and parameters. Step 3: Repeat Step 1 and Step 2 for a different sampling choice of  . Step 4: The  with the smallest time cost yields the best overall performance.
Figure 8 shows the BWQS results under the different number of scorers m where m is fixed as 20,000 and m varies from 1 to 20,000. Notice that when m is small, each scorer contains many trees and does not fit in L1 or even L2. The

Time(ns)

350 300 250 200 150 100
50 0 1

Yahoo! 64 leaves Yahoo! 32 leaves Yahoo! 16 leaves
MS 64 leaves

10

100

1k

No. of scorers

5k 20k

Figure 8: Scoring time of a vector per tree when varying the number of BWQS scorers.

Time(ns) Yahoo! 10 Yahoo! 50 Yahoo! 150
MS 50 MQ 10

Tree scorers 14.09 39.11 78.11 31.49 8.52

BWQS scorers 11.24 52.88
2869.29 44.64 7.39

Table 6: Use of the comparison and selection scheme with BWQS scorers and with the original regression tree scorers.

best m found for Y!64, Y!32, Y!16 and MS64 are 20, 10, 4

and 200, respectively. For Y!64, when m = 20, case DSD3

with d = 75 reaches the best performance with 62.89ns scor-

ing time. If d is chosen as 1, the scoring time would be

89.33ns. Here the constraint to derive d value is explained

as follow. Let F be the number of features in each document

vector and L be the number of leaves in each tree. Following

[12], the size of d document vectors is 4dF bytes and the QS

data structure is composed of 6 parts: the result bit vectors

with

size

d

·

L 8

,

the

thresholds

with

size

4

L,

the

offsets

with

size 4F , the tree ids with size 4 L, the bitvectors with size



·L2 8

,

and leaves with size 4 L.

The total size of d vectors

and

QS

data

structure

is

4F

(d

+

1)

+

(d

·

L 8

+

(12

+

L 8

)L)

·

,

which needs to fit in L3 cache because one BWQS scorer

may not fit in L2. For Yahoo! dataset with F = 700,

when L = 64 and  = 1000, we can derive d = 75.4 with

L3  2M B.

Table 6 shows the scoring time per vector per tree when

applying cache blocking with our comparison and selection

scheme to the original tree scorer and to the BWQS scorer.

This comparison shows that when the number of leaves per

tree is small(10), BWQS performs better. When the number

of leaves per tree increases to 150, BWQS becomes fairly

slow. Our explanation is listed as follows. The core QS

scheme has a complexity sensitive to the number of tree

nodes detected as "false" nodes because bit-wise operations

need to be conducted for all such nodes. When the number

of "false" nodes is large and linear to L, the overall time

cost grows at linearly to increasing of L for small L. When

L > 64, the bit operation has to be carried by multiple 64-bit

instructions and there is additional overhead for managing

this complexity. For a large tree with many false nodes,

QuickScorer can become very expensive. On the other other

hand, the original regression tree algorithm has a complexity

logarithmically proportional to L. It should be mentioned

that the scoring time per vector per tree reported here seems

to be slower than what was reported in [12] for L = 64. That

can be caused by a difference in dataset characteristics, our

code implementation, and test platform. We will investigate

this issue in the future work.

5.4 Batched Query Processing
We illustrate the benefit of batched query processing when n is small. Table 7 shows the throughput under different batch sizes when ranking only 10 document vectors (n = 10). The throughput is defined as the number of queries processed per second. The last row shows the throughput when DS without cache blocking is used. When batch size is 10, for MS 50, the average processing time is reduced from 79.79ns to 42.93ns. When the batch size becomes much bigger, the benefit is not significant any more while there is an increase of waiting time. Thus a modest batch size is sufficient in

636

Batch size 10k 1k 100 10 1
DS

Y! 50 125.79 125.60 125.00 121.54 73.53
23.87

Y! 150 60.78 60.64 60.37 56.03 31.40
13.12

MS 50 310.27 304.88 308.45 232.94 125.33
38.89

MQ 10 2880.2 2805.8 2673.8 2460.6 1505.1
565.6

Table 7: Throughput under different batch size when n = 10.

this case to reach upto 1.86x throughput performance improvement.
6. CONCLUSIONS
The main contribution of this paper is a fast comparison and selection scheme to find an optimized cache blocking method with guided sampling. Our analysis estimates the data access cost of different methods approximately, which provides a foundation to select sampling points in comparing different methods and in narrowing search space.
The evaluation studies with 3 datasets show that different blocking methods and parameter values can exhibit different cache and cost behavior and our guided sampling can identify a highly competitive solution among DSD, SDS, DSDS, and SDSD methods in a reasonable amount of hours using a modest multi-core server. The difference between the selected solution and the estimated optimum is within 2.4% and the response time of this solution can be 6.57x faster than DS without cache blocking. The analytic cost analysis shows that the search space for datasets such as Yahoo!, MS, and MQ can be greatly narrowed by taking advantages of data and architectural characteristics. When the number of feature vectors per query is small, cache utilization is affected and if allowed, batched query processing can bring upto 1.86x performance improvement. The evaluation demonstrates that our scheme can be used to find the optimized partitioning for QuickScorer.
Acknowledgments. We thank the anonymous referees for their thorough comments. This work is supported in part by NSF IIS-1528041 and IIS-1118106. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.
APPENDIX
A. COST ANALYSIS FOR SDS, DSDS, AND SDSD
Similar to the analysis of DSD in Section 4, this appendix section estimates the data access cost of SDS, DSDS, and SDSD. We skip the details on how the results are derived and summarize them in the following tables. We use a similar naming of range cases for these 3 methods: SDSi, DSDiSj, and SDSiDj where 1  i, j  4. Notation Di in each range case name means that d vectors fit in cache level i, but not i - 1. Level i = 4 refers to memory. Notation Si means that s scorers fit in cache level i, but not i - 1.
Table 8 lists the access cost ratio of SDS for the scenario when a feature vector fits in L1. For the scenario when one vector fits in L2 only, there are 3 range cases to consider:

SDS2, SDS3, and SDS4. Formula TS is the same while TD

adds c2 as:

TD

=

1 + c2 +

c4 s

.

For the scenario when a

vector fits in L3 only, there are only 2 cases to consider:

SDS3, and SDS4. Formula TS is the same while TD adds

c3

as

TD

=

1 + c3

+

c4 s

.

For

the

scenario

when

a

vector

fits

in memory only: there is one case to consider: SDS4. TS is

the same while TD changes as TD = 1 + c4.

Cases
SDS1 SDS2 SDS3 SDS4

s scorers fit in
L1 L2 L3 memory

TSDSi = TD + TS 

1+ 1+ 1+

c4 sc14 sc42 s3

+ + +

+



c4 n

+ c2

+ c3

1+

c4 s4

+

+ c4

Table 8: Cost of SDS when 1 feature vector fits in L1.

Table 9 lists the data access cost ratio of DSDS for the scenario when a feature vector fits in L1. For the scenario when one vector fits in L2, there are only 6 range cases to consider: DSD2S2, DSD3S2, DSD4S2, DSD3S3, DSD4S3 and DSD4S4. Formula TS does not change while TD adds an extra c2 term. For the scenario when one vector fits in L3, there are only 3 cases to consider: DSD3S3, DSD4S3 and DSD4S4. Formula TS does not change while TD adds an extra c3 term. For the scenario when one vector fits in memory only, there is only one case to consider: DSD4S4. Formula TS does not change while TD adds an extra c4 term.

Cases
DSD1S1 DSD2S1 DSD3S1 DSD4S1 DSD2S2 DSD3S2 DSD4S2 DSD3S3 DSD4S3 DSD4S4

d vectors fit in L1 L2 L3
memory
L2 L3 memory
L3 memory
memory

s scorers fit in L1 L1 L1 L1
L2 L2 L2
L3 L3
memory

TDSDiSj = TD + TS 

1+ 1+

c4 m c2 s1

+ +

 

+ +

 

c4 dc41 d2

1+

c3 s1

+



+



c4 d3

1+

c4 s1

+



+



c4 d4

1+

c2 s2

+  + c2

+



c4 d2

1+

c3 s2

+  + c2

+



c4 d3

1+

c4 s2

+

+ 2

+



c4 d4

1+ 1+

c3 sc43 s3

+  + c3 +  + c3

+ +

 

c4 dc43 d4

1+

c4 s4

+

+ c4

Table 9: Cost of DSDS when 1 feature vector fits in L1.

Table 10 lists the data access time ratio for SDSD when a feature vector fits in L1. Symbol i,j denotes L1 cache miss rate in the corresponding case SDSiDj; Symbol i,j is the corresponding L2 miss rate.
For the scenario 2 when one scorer fits in L2, there are 6 range cases to consider: SDS2D2, SDS3D2, SDS4D2, SDS3D3, SDS4D3 and SDS4D4.  in TD becomes 1 and TS adds an extra c2 term. For the scenario when one scorer fits in L3, there are 3 cases to consider: SDS3D3, SDS4D3 and SDS4D4.  and  in TD becomes 1 and TS adds an extra c3 term. For the scenario when one scorer does not fit L3, there is only one case to consider: SDS4D4. Its TD does not change while TS adds an extra c4 term.
B. PROOF FOR PROPOSITION 1
For each of DSD, SDS, DSDS, and SDSD methods, we eliminate its range cases that do not qualify for the best candidate as follows.

637

Cases SDS1D1 SDS2D1 SDS3D1 SDS4D1
SDS2D2 SDS3D2 SDS4D2
SDS3D3 SDS4D3
SDS4D4

TSDSiDj = TS + TD 



+



c4 n

  

+ + +

  

c2 dc31 dc41 d1

+1+ +1+ +1+ +1+

c4 sc14 sc42 s3 c4 s4

  

+ + +

  

c2 dc32 dc42 d2

+ 1 + 2,2c2 + 1 + 3,2c2 + 1 + 4,2c2

+ + +

c4 sc42 s3 c4 s4

 

+ +

 

c3 dc43 d3

+ 1 + 3,3c2 + 1 + 4,3c2

+ 3,33,3c3 + 4,34,3c3

+ +

c4 sc43 s4



+



c4 d4

+1

+

c4

Table 10: Cost of SDSD when one scover fits in L1.

· For DSD cases listed in Table 1, case DSD4 is ex-
cluded from the best case list as term c4 in DSD4
cost dominates the weight. Thus TDSD4 > TDSD3 , TDSD4 > TDSD2 , and TDSD4 > TDSD1 . We also drop DSD1 because TDSD1 > TSDS2D1 .

Now
c4 d3

we compare DSD2 and 1, and these two terms

DSD3.

Since

c4 d2

1,

can be dropped approx-

imately from the cost expressions. Note that 33 =

1 m3



1 m

,

and

c3

is much larger than c2.

Also 2

< 3

since the inner most loop of DSD2 accesses less vec-

tors. This makes TDSD3 > TDSD2 .

· Now we compare SDS cases listed in Table 8. Since

c4 s2

1,

this

leads

to

c4 s3

1,

and

c4 s4

1,

and

c4 m

1.

Therefore TSDS4 > TSDS3 > TSDS2 . Thus we drop

cases SDS3 or SDS4.

We drop case SDS2 because TSDS2 > TDSD2S2 . We

also drop case SDS1 because TSDS1 > TDSD2S1 given

c4 d2

1.

·

For DSDS, since

c4 d2

1,

TDS D2 S1



1

+



+

c2 s1

.

Then

TDSD2S1 < TDSD3S1 and TDSD2S1 < TDSD4S1 . Thus

we drop cases DSD2S1 and DSD4S1.

Since

c4 s2

1,

c2 s2

1. Then TDSD2S2  1 +  +

c2. Then TDSD2S2 is smaller than any of TDSD3S2 ,

TDSD4S2 , TDSD3S3 , TDSD4S3 , and TDSD4S4 . We drop

all cases in DSDS except DSD1S1, DSD2S1, and DSD2S2.

Since TDSD2S2

>

1

+



+



c2 d1

 TSDS2D1

given

c4 s2

1, we can further drop case DSD2S2. Then TDSD1S1 >

TDSD2S1 and we can further drop case DSD1S1.

· For SDSD, we drop case SDS1D1 because TSDS1S1 

1++

c4 s1

> TDSD2S1

given

SDS3D1 because TSDS3S1 

c4
d2
1+



+

1. ·

We drop Case

c3 d1

> TSDS2S1 .

We drop Case SDS4D1 because TSDS4S1  1 +  +  ·

c4 d1

> TSDS2S1 .

Note that TSDS2S2  1 +  + 2,2c2

and

c2 d2

<

c4 d2

1. Then we drop

because SDS4D4

c4 s2

1

because

TSDS2D2 < TSDS4D4 . Also, TSDS2D2 is smaller than

any of TSDS3D2 , TSDS4D2 , TSDS3D3 , and TSDS4D3 .

That is because 2,2 < 3,2, 4,2, 3,3, 4,3 due to the

fact that s scorers fits in the smaller cache in Case

SDS2D2 than other cases compared here. Thus only

SDS2D2 and SDS2D1 qualify for the best candidates.

C. REFERENCES
[1] Lector 4.0 datasets. http://research.microsoft.com/enus/um/beijing/projects/letor/letor4dataset.aspx.
[2] Microsoft learning to rank datasets. http://research.microsoft.com/en-us/projects/mslr/.
[3] Nima Asadi and Jimmy Lin. Training Efficient Tree-Based Models for Document Ranking. In ECIR, pages 146­157, 2013.
[4] Nima Asadi, Jimmy Lin, and Arjen P De Vries. Runtime Optimizations for Tree-Based Machine Learning Models. IEEE TKDE, pages 1­13, 2013.
[5] Christopher J. C. Burges, Krysta Marie Svore, Paul N. Bennett, Andrzej Pastusiak, and Qiang Wu. Learning to rank using an ensemble of lambda-gradient models. In J. of Machine Learning Research, pages 25­35, 2011.
[6] B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. Early exit optimizations for additive machine learned ranking systems. WSDM '10, pages 411­420, 2010.
[7] Olivier Chapelle and Yi Chang. Yahoo! Learning to Rank Challenge Overview. J. of Machine Learning Research, pages 1­24, 2011.
[8] Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2000.
[9] Yasser Ganjisaffar, Rich Caruana, and Cristina Lopes. Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models. In SIGIR, pages 85­94, 2011.
[10] Pierre Geurts and Gilles Louppe. Learning to rank with extremely randomized trees. J. of Machine Learning Research, 14:49­61, 2011.
[11] Andrey Gulin, Igor Kuralenok, and Dmitry Pavlov. Winning the transfer learning track of yahoo!'s learning to rank challenge with yetirank. J. of Machine Learning Research, 14:63­76, 2011.
[12] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, and Rossano Venturini. Quickscorer: A fast algorithm to rank documents with additive ensembles of regression trees. In SIGIR, pages 73­82, 2015.
[13] Dmitry Yurievich Pavlov, Alexey Gorodilov, and Cliff A. Brunk. Bagboo: a scalable hybrid bagging-the-boosting model. In CIKM, pages 1897­1900, 2010.
[14] Xun Tang, Xin Jin, and Tao Yang. Cache-conscious runtime optimization for ranking ensembles. SIGIR '14, pages 1123­1126, 2014.
[15] Jiancong Tong, Gang Wang, and Xiaoguang Liu. Latency-aware strategy for static list caching in flash-based web search engines. In CIKM, pages 1209­1212, 2013.
[16] Lidan Wang, Jimmy Lin, and Donald Metzler. Learning to efficiently rank. SIGIR '10, pages 138­145, 2010.
[17] Lidan Wang, Jimmy Lin, and Donald Metzler. A cascade ranking model for efficient ranked retrieval. SIGIR '11, pages 105­114, 2011.

638


Distributional Random Oversampling for Imbalanced Text Classification

Alejandro Moreo, Andrea Esuli
Istituto di Scienza e Tecnologie dell'Informazione Consiglio Nazionale delle Ricerche 56124 Pisa, Italy E-mail: {firstname.lastname}@isti.cnr.it

Fabrizio Sebastiani
Qatar Computing Research Institute Hamad bin Khalifa University Doha, Qatar E-mail: fsebastiani@qf.org.qa

ABSTRACT
The accuracy of many classification algorithms is known to suffer when the data are imbalanced (i.e., when the distribution of the examples across the classes is severely skewed). Many applications of binary text classification are of this type, with the positive examples of the class of interest far outnumbered by the negative examples. Oversampling (i.e., generating synthetic training examples of the minority class) is an often used strategy to counter this problem. We present a new oversampling method specifically designed for classifying data (such as text) for which the distributional hypothesis holds, according to which the meaning of a feature is somehow determined by its distribution in large corpora of data. Our Distributional Random Oversampling method generates new random minority-class synthetic documents by exploiting the distributional properties of the terms in the collection. We discuss results we have obtained on the Reuters-21578, OHSUMED-S, and RCV1-v2 datasets.
1. INTRODUCTION
Many applications of binary text classification exhibit severe data imbalance, i.e., are characterized by sets of data in which the examples of one class are far outnumbered by the examples of the other. Such cases are especially frequent in information retrieval and related tasks, where the binary distinction to be captured is between a class of interest and "the rest", i.e., between the (typically few) documents relevant to a certain concept (e.g., as expressed by a query) and the (typically many) documents unrelated to it. This phenomenon is exacerbated in applications of multi-label multiclass (MLMC) text classification, i.e., applications where, given a set C = {c1, ..., c|C|} of classes, each document may be labelled by several classes at the same time1. In these applications the average prevalence (i.e., relative frequency) of a class is low, since C typically exhibits a power-law be-
1MLMC classification is typically solved by training |C| independent binary classifiers, one for each class of interest.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914722

haviour, with few classes having high prevalence and very many classes having low or very low prevalence.
Severe imbalance is known to degrade the performance of a number of inductive learning algorithms, such as decision trees, neural networks, or support vector machines [8]. The main approaches previously proposed for solving this problem may be grouped into the following classes: [7, 9]: (i) data-level approaches, which perform a random resampling of the dataset in order to rebalance class prevalences (ii) algorithmic approaches, which focus on adapting traditional classification methods to scenarios where data are imbalanced; and (iii) cost-sensitive learning approaches, that combine the data-level and algorithmic approaches by imposing a higher cost on the misclassification of examples from the minority class. We here focus on approaches of type (i), most of which rely on oversampling the minority class (i.e., adding new minority-class training examples, typically duplicates or quasi-duplicates of the existing ones) and/or undersampling the majority class (i.e., removing some majority-class examples from the training set), with the goal of rebalancing the class distribution in the training set.
We propose a novel method based on oversampling the minority class, and specifically designed to deal with types of data (such as text) where the distributional hypothesis (according to which the meaning of a feature is somehow determined by its distribution in large corpora of data ­ see [6]) may be assumed to hold. Our method, dubbed Distributional Random Oversampling (DRO), consists of extending the standard vector representation (based on the bagof-words model) with random latent dimensions based on distributional properties of the observed features. We assign to each document a discrete probabilistic function that operates in a latent space and is queried as many times as desired in order to oversample a given document (i.e., to produce distributionally similar versions of it). Since this generative function is based on the distributional hypothesis, the expectation is that the variability introduced in the newly generated examples reflects semantic properties of the terms that occur in the document being oversampled. We present the results of experiments conducted on popular text classification benchmarks such as Reuters-21578, OHSUMED-S, and RCV1-v2.
Our method is presented in Section 2; Section 3 discusses our empirical results, while Section 4 concludes.
2. THE LATENT SPACE OVERSAMPLING FRAMEWORK
We assume a binary classification context, with classes C = {c, c}. Let T r = {d1, . . . , d|T r|} be a set of training

805

documents and F = {t1, . . . , t|F |} its vocabulary. We use

W|T r|×|F | to denote the document-term matrix, where wij 

R a

wiseitghhetiwngeigfuhntcotifotne.rmBytj#diindRoc|Fum| wenetddeinoatse

computed by the vectorial

representation of document di.

We present a general framework for oversampling, that we

dub Latent Space Oversampling (LSO); our Distributional

Random Oversampling method will be a specific instantia-

tion of it. In LSO we oversample minority-class documents

by extending the original feature space F with an additional

latent ument where

s#ddpiiacweilLRl .|bFe|Eaiescxhpthrneeessw(efidxsyeadns)th#ooebtkisce=revxe[ad#dmip;pa#vlerkto]iknftoRhre|Fao|+dri|oLgci|--,

nal feature space (i.e., a copy of the i-th row of W ), and #v k  R|L| is the variable part in the latent space L, which

is generated by some stochastic function.

The vector expansion involves a two-step process for each

document di, i.e., (i) the estimation of model parameters i

for di via a parameter estimation criterion (W, di), such

that i di; and

 (ii)

(W, di) is calculated the generation of the

only once for variable part

each example #v k  G(i),

obtained by means of a generation function G. This func-

tion is called several times for each minority-class example

until the desired level of balance is reached, and exactly once

for each majority-class example, since we neither oversample

nor undersample majority-class examples. The oversampled

matrix is then re-weighted (e.g., in order to bring to bear

updated idf values, and in order to perform correct length

normalization) before training the classifier. Each test docu-

ment dt is also expanded to the enlarged vector space before

being fed to the classifier; the only difference with the expan-

sion process we carry out for training documents is that any

global knowledge involved in the estimation of parameters

t comes from the training data. Different oversampling strategies could thus be defined by

considering different parameter estimation criteria  and

different generation functions G. In the following sections

we first illustrate one possible such strategy, based on proba-

bilistic topic models (Section 2.1); we then present our DRO

method based on the distributional hypothesis (Section 2.2).

2.1 Latent Dirichlet Oversampling
One possible instantiation of the LSO framework is what we will here call Latent Dirichlet Oversampling (LDO). LDO relies on Latent Dirichlet Allocation (LDA ­ [1]), a probabilistic topic model that assumes, in order to define the model parameters and the generative function, that each (observed) document in a collection is generated by a mixture of (unobserved) topics. As the weight wij we here take the raw number of occurrences of term tj in document di.
As the parameter estimation criterion LDO we may choose any Bayesian inference method (such as Variational Bayes or Gibbs Sampling). The document-specific model parameters are i = [i; ], where i is the topic distribution of di and  is the per-topic word distribution obtained from T r.
We will choose a generation function GLDO that returns a vectorial representation of a bag of n words, each of which is drawn by first choosing a topic zk  M ultinomial(i), and then choosing a term tj  M ultinomial(zk ). We set n = length(di) (i.e., to the total number of word occurrences in di) so that the synthetic bag of words will allocate the same number of term occurrences as the original document (thus preserving sparsity in the new space). Note that, in this case, the latent space is mirroring the original feature space, with

a dedicated latent dimension for each term in the vocabulary, i.e., |L| = |F |. LDO assumes each minority-class document to be governed by similar topic distributions, causing the variable part of oversampled documents to exhibit topically similar patterns.

2.2 Distributional Random Oversampling

We propose Distributional Random Oversampling (DRO), a

different instantiation of LSO. DRO is based on the hypoth-

esis that related documents (such as, e.g., the minority-class

documents) may be expected to contain semantically similar

words, and relies on a direct application of the distributional

hypothesis, by virtue embedded in column

of which the

#t

T j

 R|T r|

meaning of matrix

of feature tj W . Unlike

is in

LDO, we here take weight wij to be generated by a real-

valued weighting function such as, e.g., tfidf or BM25.

As the parameter estimation criterion we take a function DRO that returns i = (pi1, . . . , pi|T r|), where pik will be used as parameters of a multinomial distribution for document di. Parameter pik is computed as

#t

T j

[k]

·

wij

·

s(tj )

pik

=

tj di |T r|

(1)

#t

T j

[k]

·

wij

·

s(tj )

k=1 tj di

i.e., the

by (i) summing together the k-th components

(length-normalized)

feature

vectors

#t

T j

(i.e.,

the

#t

T j

[k]

of

columns

of the W matrix) corresponding to all unique terms tj  di,

weighted by (a) their relative importance with respect to

the document (the wij component) and by (b) their relative

importance with respect to the classification task (the s(tj)

component)2, and (ii) normalizing to satisfy

|T r| k=1

pik

=

1.

We will choose a generation function GDRO that returns

a vectorial representation of a bag of n (latent) words, each

of which is drawn from lk  Multinomial (i). Note that

in this case |L| = |T r|. Similarly to the case of LDO we

set n = length(di), so that sparsity is preserved in the en-

larged feature space. In contrast to LDO, the multinomial

distribution of DRO is deterministically obtained from the

training collection, thus avoiding the need for computation-

ally expensive statistical inference methods.

Each test document is also expanded to the enlarged vec-

ttohrats,pianceEbqeufaotrieonbe1i,ng#t

fed to the

T j

­

which

classifier. In encodes the

this case note distributional

knowledge ­ and s(tj) are the supervised components, i.e.,

they are obtained from T r. Instead, wij is computed partly

from the document itself (e.g., the tf component) and partly

from the training set (e.g., the idf component).

In sum, the rationale of our method is to generate syn-

thetic minority-class vectors where the part corresponding

to the latent space is the result of a generative process that

brings to bear the distributional properties of the words con-

tained in the document being oversampled.

3. EXPERIMENTS
As the datasets for our experiments we use Reuters-21578, OHSUMED-S, and RCV1-v2. All these collections are multilabel, i.e., each document may be labelled by zero, one, or
2In this paper we compute s as the mutual information between the feature and C = {c, c}.

806

several classes at the same time, which gives rise to |C| binary classification problems, with C the set of classes in the dataset. For Reuters-215783 we use the standard ("ModApt´e") split, which identifies 9,603 training documents and 3,299 test documents. We restrict our attention to the 115 classes with at least one positive training example. OHSUMED-S [4] consists instead of 12,358 training and 3,652 test MEDLINE textual records from 1987 to 1991, classified according to 97 MeSH index terms. RCV1-v24 comprises 804,414 news stories generated by Reuters from Aug 20, 1996, to Aug 19, 1997. In our experiments we use the entire training set, containing all 23,149 news stories written in Aug 1996; for the test set we pick the 60,074 news stories written during Sep 1996. We restrict our attention to the 101 classes with at least one positive training example.
As the evaluation measures we use microaveraged F1 (F1µ) and macroaveraged F1 (F1M ).
We compare the performance of LDO5 and DRO with the following baselines: (i) Random Oversampling (RO), a method that performs oversampling by simply duplicating random minority-class examples; (ii) Synthetic Minority Oversampling Technique (SMOTE ­ [2]), a method that generates new synthetic minority-class examples as convex linear combinations of the document di being sampled and a document randomly picked among the k minority-class nearest neighbours of di (typically using k = 5); (iii) BorderlineSMOTE (BSMOTE ­ [5]), a more recent version of SMOTE that only oversamples those borderline minority-class examples that would be misclassified as negatives by a k-NN classifier; (iv) DECOM [3], a probabilistic topic model that assumes all documents belonging to the same class to follow the same topic distribution that, once determined, is used to oversample minority-class examples following the LDA generation procedure6; (v) a bag-of-words model (BoW) where no oversampling is performed. For LDA-based methods we follow the related literature and set the number of topics to 30; in order to favour convergence, i.e., to allow the system to find a stationary point for the distribution parameters  that maximize the posterior probability of the corpus, we set the number of iterations to 3,000 and perform 10 passes over the corpus in each iteration.
As the learner of our experiments we adopt linear-kernel SVMs (in the popular SVM-light implementation7); in all our experiments we use the default SVM-light parameters. All methods are fed with the same preprocessed version of the datasets where, for each distinct binary decision problem, the top 10% most informative words have been selected, using mutual information as the selection function and tfidf as the weighting function. We perform oversampling of the minority class until a desired prevalence  for the minorityclass is reached; we let  range on {0.05, 0.10, 0.15, 0.20}. We do not consider undersampling in this paper, i.e., all negative examples are picked exactly once. The results we present are all averages across 5 random trials we have run for each setting. For each dataset we partition the classes
3Available from http://bit.ly/1F8AFcO 4Available from http://1.usa.gov/1mp7RGr 5For LDO we used the Gensim implementation of LDA (see http://bit.ly/1Rl7pFV) which also allows estimating the document-topic distribution of test examples. 6For this method, as suggested in [3], we used the MATLAB implementation of Gibbs sampling available at http://bit. ly/1Rl7DNl 7http://svmlight.joachims.org/

Dataset

Training Test Features Classes HP LP VLP

Reuters-21578

9,603 3,299 23,563

115 3 50 62

Ohsumed-S

12,358 3,652 26,382

97 9 60 28

RCV1-v2

23,149 60,074 37,211

101 16 73 12

Table 1: Details on the 3 datasets used.

DRO

LDO

DECOM

BSMOTE

SMOTE

RO

BoW

Prev.



F1M

.05 .907 .907 .907 .907 .907 .907 .907

HP

.10 .907 .907 .911 .904 .912 .909 .15 .907 .910 .911 .902 .911 .908

.897 .905

.20 .907 .909 .911 .899 .911 .911 .899

.05 .633 .700 .754 .678 .650 .706 .761

LP

.10 .633 .682 .718 .678 .639 .15 .633 .662 .684 .678 .629

.690 .766 .679 .759

.20 .633 .648 .654 .678 .629 .664 .764

.05 .426 .485 .478 .426 .441 .484 .568

VLP

.10 .15

.426 .426

.456 .473

.416 .395

.426 .426

.418 .398

.482 .476

.568 .567

.20 .426 .473 .387 .426 .398 .474 .570

.05 .954 .954 .954 .954 .954 .954 .954

HP

.10 .954 .952 .953 .952 .954 .953 .15 .954 .953 .953 .951 .953 .952

.950 .951

.20 .954 .953 .953 .950 .955 .952 .947

.05 .767 .788 .809 .782 .773 .790 .810

LP

.10 .767 .778 .784 .783 .762 .15 .767 .770 .756 .783 .750

.786 .812 .777 .807

.20 .767 .764 .731 .782 .738 .774 .805

.05 .132 .319 .428 .212 .315 .310 .509

VLP

.10 .15

.132 .132

.272 .269

.357 .302

.212 .212

.280 .250

.308 .289

.515 .519

.20 .132 .269 .277 .212 .240 .287 .507

Table 2: Results on Reuters-21578.

F1µ

into (i) HighPrevalence (HP), the classes with a prevalence higher than 0.050; (ii) LowPrevalence (LP), the classes with a prevalence in the range [0.015, 0.050]; and (iii) VeryLowPrevalence (VLP), the classes with a prevalence smaller than 0.015. The reason for partitioning the classes according to prevalence is to allow the results to provide insights as to which classes benefit from oversampling and which do not.
Table 1 shows some details of the document collections used in the experiments. Tables 2 to 4 report the results of our experiments in terms of F1M and F1µ, for Reuters21578, OHSUMED-S, and RCV1-v2, respectively. Results are reported at different levels  of oversampling; we use boldface to highlight the best performing method, while symbol "" indicates that the method outperforms all others in a statistically significant sense8. Note that, for each block of 4 rows identifying a certain set of classes (HP, LP, VLP), the results for BoW are always the same; this is obvious since there is no oversampling in BoW, which thus does not depend on the value of . Note also that, in all three tables, the first row of the HP results for  = 0.05 always contains identical values, since the HP classes have a prevalence  0.05.
Overall, the results of these experiments indicate that DRO is superior to the other six methods presented (including LDO). In the low-prevalence groups (LP and VLP) DRO is superior in most cases, across the different datasets and the different degrees  of oversampling, and especially so in terms of F1M ; when DRO is not superior, the differences in performance with the top-performing method are
8Two-tailed t-test on paired examples at 0.05 confidence level.

807

DRO

LDO

DECOM

BSMOTE

SMOTE

RO

BoW

Prev.

DRO

DECOM LDO

BSMOTE

SMOTE

RO

BoW

Prev.





F1M

.05 .753 .753 .753 .753 .753 .753 .753

HP

.10 .753 .758 .756 .15 .753 .764 .767

.754 .763

.755 .757 .760 .765

.752 .753

.20 .753 .769 .771 .767 .763 .769 .756

.05 .479 .557 .603 .571 .538 .569 .588

LP

.10 .479 .552 .578 .15 .479 .526 .550

.570 .569

.532 .565 .588 .525 .555 .578

.20 .479 .514 .524 .568 .523 .542 .576

.05 .354 .385 .455 .458 .354 .396 .451

VLP

.10 .15

.354 .354

.372 .363

.433 .440

.448 .448

.352 .378 .330 .373

.469 .455

.20 .354 .364 .427 .448 .314 .376 .476

.05 .801 .801 .801 .801 .801 .801 .801

HP

.10 .801 .803 .803 .15 .801 .804 .806

.802 .805

.802 .803 .804 .804

.798 .795

.20 .801 .805 .807 .806 .805 .805 .795

.05 .616 .662 .672 .666 .647 .668 .647

LP

.10 .616 .657 .654 .669 .644 .665 .15 .616 .645 .625 .666 .640 .658

.640 .626

.20 .616 .642 .595 .666 .633 .652 .620

.05 .282 .299 .518 .437 .365 .313 .552

VLP

.10 .15

.282 .282

.241 .198

.484 .446

.416 .416

.328 .262 .570 .311 .243 .553

.20 .282 .200 .415 .416 .291 .251 .553

Table 3: Results on OHSUMED-S.

F1µ

fairly small. This superiority is more pronounced for the VLP classes, where DRO obtained 23 out of 24 best results, almost always with very large margins. In the HP classes, instead, our results do not reveal any clear winner, since the best results are haphazardly distributed among all of the baselines. Moreover, the best system is not substantially better to BoW in the vast majority of cases, which makes the idea of oversampling such classes questionable.
In sum, the results seem to indicate that the smaller the prevalence of the minority class is, the higher is the gain that can be obtained due to the use of DRO. This is an appealing feature for an oversampling method. We attribute this behaviour to DRO's distributional nature, which enables the information of the entire collection to contribute in the generation of each synthetic example (whereas RO and SMOTE-based methods are limited to local information provided by one or two examples, respectively). This could be advantageous for ill-defined classes (as those belonging to LP and VLP). It may instead introduce noise, or even some redundancy, for well-defined ones (i.e., those in HP); this suggests that the best policy may be that of applying DRO to low- or very-low prevalence classes only, while leaving high-prevalence classes untouched.
4. CONCLUSIONS
We have presented a new oversampling method for imbalanced text classification, based on the idea of assigning a probabilistic generative function to each minority-class document in the training set, a function that can be iteratively queried until the desired level of balance is reached. This probabilistic function is built upon distributional representations of the words contained in the document being oversampled, which allows the model to introduce some random variability in the new examples while preserving the underlying semantic properties motivated by the distributional hypothesis.

F1M

.05 .843 .843 .843 .843 .843 .843 .843

HP

.10 .843 .848 .848 .15 .843 .848 .848

.846 .848

.845 .845

.847 .847

.839 .838

.20 .843 .846 .845 .848 .844 .846 .838

.05 .489 .600 .616 .573 .577 .613 .617

LP

.10 .489 .602 .603 .15 .489 .597 .584

.582 .583

.587 .591

.619 .617

.631 .632

.20 .489 .594 .563 .584 .593 .614 .629

.05 .048 .249 .257 .148 .263 .269 .295

VLP

.10 .048 .15 .048

.237 .228

.210 .186

.148 .148

.271 .265

.261 .252

.294 .297

.20 .048 .220 .172 .148 .267 .245 .295

.05 .877 .877 .877 .877 .877 .877 .877

HP

.10 .877 .878 .878 .15 .877 .877 .877

.878 .878

.877 .877

.877 .876

.873 .871

.20 .877 .876 .875 .878 .876 .875 .869

.05 .638 .676 .674 .666 .663 .677 .664

LP

.10 .638 .685 .672 .15 .638 .680 .656

.678 .680

.673 .691 .675 .688

.683 .680

.20 .638 .676 .637 .680 .675 .683 .674

.05 .106 .408 .408 .268 .426 .446 .489

VLP

.10 .106 .15 .106

.391 .379

.343 .303

.268 .268

.431 .429

.437 .424

.489 .497

.20 .106 .367 .283 .268 .430 .415 .493

Table 4: Results on RCV1-v2.

F1µ

5. REFERENCES
[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993­1022, 2003.
[2] Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16(1):321­357, 2002.
[3] Enhong Chen, Yanggang Lin, Hui Xiong, Qiming Luo, and Haiping Ma. Exploiting probabilistic topic models to improve text categorization under class imbalance. Information Processing & Management, 47(2):202­214, 2011.
[4] Andrea Esuli and Fabrizio Sebastiani. Improving text classification accuracy by training label cleaning. ACM Transactions on Information Systems, 31(4):Article 19, 2013.
[5] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning. In Proceedings of the 1st International Conference on Intelligent Computing (ICIC 2005), pages 878­887, Hefei, CN, 2005.
[6] Zellig S. Harris. Distributional structure. Word, 10(23):146­162, 1954.
[7] Haibo He and Edwardo A. Garcia. Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 21(9):1263­1284, 2009.
[8] Nathalie Japkowicz and Shaju Stephen. The class imbalance problem: A systematic study. Intelligent Data Analysis, 6(5):429­449, 2002.
[9] Yanmin Sun, Andrew K. Wong, and Mohamed S. Kamel. Classification of imbalanced data: A review. International Journal of Pattern Recognition and Artificial Intelligence, 23(4):687­719, 2009.

808


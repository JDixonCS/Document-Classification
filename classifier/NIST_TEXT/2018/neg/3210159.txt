Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

K-plet Recurrent Neural Networks for Sequential Recommendation

Xiang Lin
University of Chinese Academy of Sciences linxiangblcu@gmail.com

Shuzi Niu
Institute of Software, Chinese Academy of Sciences shuzi@iscas.ac.cn

Yiqiao Wang
University of Chinese Academy of Sciences wangyiqiao15@mails.ucas.ac.cn
ABSTRACT
Recurrent Neural Networks have been successful in learning meaningful representations from sequence data, such as text and speech. However, recurrent neural networks attempt to model only the overall structure of each sequence independently, which is unsuitable for recommendations. In recommendation system, an optimal model should not only capture the global structure, but also the localized relationships. This poses a great challenge in the application of recurrent neural networks to the sequence prediction problem. To tackle this challenge, we incorporate the neighbor sequences into recurrent neural networks to help detect local relationships. Thus we propose a K-plet Recurrent Neural Network (Kr Network for short) to accommodate multiple sequences jointly, and then introduce two ways to model their interactions between sequences. Experimental results on benchmark datasets show that our proposed architecture Kr Network outperforms state-of-the-art baseline methods in terms of generalization, short-term and long term prediction accuracy.
CCS CONCEPTS
· Computer systems organization  Embedded systems; Redundancy; Robotics; · Networks  Network reliability;
KEYWORDS
Sequence prediction; Similarity Regularization; K-plet Network; Sequential Recommendation
ACM Reference Format: Xiang Lin, Shuzi Niu, Yiqiao Wang, and Yucheng Li. 2018. K-plet Recurrent Neural Networks for Sequential Recommendation. In SIGIR '18: 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8-12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210159
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210159

Yucheng Li
Institute of Software, Chinese Academy of Sciences yucheng@iscas.ac.cn
1 INTRODUCTION
Recurrent Neural Networks are supposed to be able to capture sequential pattern successfully from sequence data, such as speech and text [10, 17, 21]. Recurrent networks retain a state that can represent information from an arbitrarily long context window [10]. So it can capture time dependencies in each sequence globally.
Sequential recommendation is solved by recurrent neural network as follows: At each time step, it takes one item of the user's sequence in a sequential order. Then, it encodes this item's information in the context for the next step. Finally, it utilizes the current item information and the context information at last step to predict the next item. Through this process, recurrent neural networks only learn the global information from each sequence alone. However, neither the global method (latent factor model) or the local approach (neighborhood approach) performs the best [9]. This poses a great challenge in the application of recurrent neural network to sequential recommendation.
To tackle this challenge, we introduce neighborhood model into recurrent neural networks to capture both the local and global information at the same time. Therefore we propose a novel network structure, k-plet recurrent neural network (inspired by triplet network [7]), to accommodate the query sequence and its k neighbor sequences jointly. Kr network (short for k-plet recurrent neural network) is comprised of k + 1 instances of the same recurrent neural networks.
There are various ways to model the interaction between the query sequence and its neighbor sequences in the Kr network. The first and intuitive way is to introduce the interaction into the loss function, denoted as KrNN-L. Specifically, we model the similarity between the query sequence and its neighbors, and employ this similarity as the regularization of the original loss function, e.g. cross entropy loss. We optimize this similarity regularized loss to learn the network parameters. A deeper interaction is built in the prediction function of the query sequence, and we denote this method as KrNN-P. In light of k-NN classification, we predict the next item distribution of the query sequence as the weighted sum of probabilistic outputs of the query sequence and its neighbors. Weights are estimated as the similarity between the query sequence and its neighbors, such as the Euclidean distance. Experimental results on benchmark datasets MovieLens-1M and Amazon Movies show that our proposed Kr Network outperforms state-of-the-art baselines but needs more time for training compared with traditional RNNs.
Our main contribution lies in the following two aspects:

1057

Short Research Papers I
SIGIR'18, July 8­12, 2018, Ann Arbor, MI, USA
(1) We propose a K-plet recurrent neural network architecture, referred to as Kr Networks, to capture both the local and global information in the sequences for better recommendation.
(2) We introduce two distinguished ways to model the interaction between the query sequence and its neighbors in Kr Network.
2 BACKGROUND
In this section, we first review some related work on recommendation algorithms. Then we formalize the sequence prediction problem in sequential recommendation with recurrent neural networks. Finally, we introduce some related network structures in deep metric learning.
2.1 Recommendation Algorithms
Recommendation algorithms aim at matching the user interests with the item set. According to different views on users' historical behavior, recommendation algorithms mainly falls into two categories: general and sequential recommendation.
General Recommendation. Users' historical behaviors, such as rating and consuming an item, are represented as a set by ignoring the temporal factor. Matrix factorization techniques are popular and effective methods to capture the general interests of users. Among these, Bayesian Personalized Rank (BPRMF [13]) is famous for its pairwise learning strategy to solve the factorization problem [12].
Sequential Recommendation. Different from general recommendation, the sequential pattern in the user's rating or buying behavior is of the main concern in the sequential recommendation. Existing sequential recommendation algorithms mainly include the Markov Chain and Recurrent neural network based methods. Markov Chain models were first introduced into web page recommendation [22]. In most Markov Chain models, sequence data is transformed into transition graph. Factorized Personalized Markov Chain [14] is such a method that is combined with matrix factorization in order to handle sparsity. To improve the performance further, a Markov Decision Process based recommendation method was proposed [15]. Hierarchical Representation Model combined the last action information with the general user interest to model user representations for next basket recommendation [18]. For these Markov Models, it is difficult to model the long-range dependence. Thus recurrent neural network based models appear to overcome this problem, which will be discussed in the following section.
2.2 Recurrent Neural Networks for Sequential Recommendation
In light of the success of RNN in sequence modeling, many RNN based approaches reduce the recommendation task to the sequence prediction problem. Suppose there are m users denoted as U = {u1, . . . , um } and n items A = {a1, . . . , an }. For each user u, the (tmpsou1uetrn,ci.dhs.aa.rtse,ieosptunroe),r,steshrtunaettreiendgaAarsbe. eSAuh[utsas0u,vutaib]lols=yreqim(ssutuoef0orn,er.cm.eth.ao,alsfintuzue)os.denFreaousritfnearmeosxmestqibuntaiemssntukecesefttoarSremuectpoacmt=h0-

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Xiang Lin, Shuzi Niu, Yiqiao Wang, and Yucheng Li

time stamp, i.e. |stu | > 1; for sequential recommendation in Movie-
Lens and Amazon Product dataset, there is usually one item per time stamp such as |stu | = 1. The history for all users is S = {Sui }im=1.
Vanilla RNN was first used to predict the next tracks users

wanted to play in the blog [1]. Then, variants of RNNs were com-

prehensively studied for sequential recommendation task [2]. GRU

based RNN was used for session-based recommendation with two

types of ranking loss functions: BPR [12] and their first devised

TOP1 [5]. DREAM [19] utilized pooling to summarize the basket of

embedded items and then fed into vanilla RNN to solve next bas-

ket recommendation. All these studies consider RNN as a function

f (·) of a user sequence, for example LSTM [6] as Eq.( 1), where  = {Wi ,Wf ,WC ,Wo, bi , bf , bC , bo }.

itu =  (Wi · [hut -1, xtu ] + bi

ftu =  (Wf · [hut -1, xtu ] + bf ) Ctu = ftu  Ct -1 + itu  tanh(WC · [ht -1, xtu ] + bC )

(1)

hut =  (Wo · [hut -1, xtu ] + bo )  tanh(Ctu )

CRNN [8] learned a RNN fu for each user u, shared Wi and Wo among all users. Thus CRNN is prone to overfit for its over-

parameterization. Whatever, RNN and its variants are only good

at capturing the overall structure in the sequence data. Thus their

performances are not meant to be optimal.

3 K-PLET RECURRENT NEURAL NETWORK

Here we introduce K-plet Recurrent Neural Network, i.e. Kr Net-

work. Kr Network is composed of k +1 instances of recurrent neural

bnoetrwhooorkdss:eoqnueenfocresthNe qqtu.eWryhseenquweenuceseSKqrt

, the other for the neighnetwork to predict what

will be the next item at time step t + 1 with the training instance

Sqt  Nqt , the following two questions are of the most concerned:

· How to define the sequence similarity to determine the neigh-

bor sequences in Kr Network?

· How to model the interaction between the query sequence

and its neighbors in the sequence prediction task?

We first briefly describe the framework of K-plet Recurrent Neural Networks (Kr Network for short, a.k.a KrNN), then two different sequence interaction modeling methods are proposed, referred to as KrNN-L and KrNN-P separately.

3.1 Framework
Given the query sequence, we determine its neighbor sequences by k nearest neighbor search based on the sequence similarity. Then we feed one RNN with the query sequence, and use the other k RNNs for its neighbors. Parameters are shared among k + 1 RNNs. Finally, we model the RNN interaction at the loss layer or the output layer, denoted as KrNN-L and KrNN-P repectively. Thus Kr Network is built.

3.2 KrNN-L
It is well known that two instances with similar inputs are supposed to have similar labels. For the sequence prediction task, we reverse this assumption, and suppose two context sequences to be similar if their next item predictions are the same. Given the query sequence Sqt , we extract its similar sequence as S[imax (0,ti -l ),ti ] from each

1058

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

K-plet Recurrent Neural Networks for Sequential Recommendation

SIGIR'18, July 8­12, 2018, Ann Arbor, MI, USA

user sequence Si if stq+1 = stii +1 and t  ti . l is the context window, which are set as 200 in our experiments. We use k-NN search to find k neighbor sequences of Sqt , denoted as Nqt .
We introduce a special kind of graph regularization [16, 20] at the loss layer to add local information to the recurrent neural network, namely KrNN-L. The underlying assumption is that two users' context sequences with the same next item share similar latent representations. Following this assumption, the local consistency respecting the item similarity can be preserved with the addition of Lsr . Thus we incorporate the sequence similarity into the loss function, and obtain the similarity regularized objective function as Eq.( 2), where  is the parameter controls the trade-off between item prediction error and similarity regularization.

L = Ler r or +Lsr = Ler r or +

h(Sqt )-h(S[imax (0,ti -l ),ti ])22

i Nqt

(2)

This similarity regularized loss function is optimized through Ada-

grad [3].h(.) is the output of hidden layer. In our experiments, the

prediction error Lerror is Cross Entropy loss.

3.3 KrNN-P

Different from the sequence similarity definition used in KrNN-

L, in

KrNN-P employs the neighbor the prediction function of the

sequences Nqt query sequence

to play a Sqt . So

role here

we utilize another sequence similarity based on Euclidean dis-

tance between Sqt and S[imax (0,ti -l ),ti ] (t  ti ) denoted as q,i =

v (Sqt

)-v

1 ( S[imax

(0, ti

-l

), ti

])

+C

,

where

v (S

)

is

the

item

frequency

vec-

tor of sequence S and C is a constant. Based on the sequence sim-

ilarity q,i , we use k-NN search to find k neighbor sequences of Sqt , denoted as Nqt .
In light of k-NN classification, we propose a weighted sum of

the network outputs of the query sequence and its neighbors as the

final output as follows:

F (Sqt , Nqt ) = f (Sqt )+

q,i f (S[imax (0,ti -l ),ti ])

S[imax (0, ti -l ), ti ]  Nqt

(3)

where f (·) is the output of each recurrent neural network and F (·) is the final output of the query sequence. We refer to this method

as KrNN-P. In the experiments, we optimize Cross Entropy loss

through Adagrad.

4 EXPERIMENTAL RESULTS
To explore the performance of our Kr Network, we conduct comprehensive experiments on two benchmark datasets for sequential recommendation.

4.1 Experimental Setting
Datasets. We use two benchmark recommendation datasets: Movielens 1M and Amazon Product Data [4] with category Movies and TV, denoted as amazonmovies in our experiments. (1) Movielens 1M: This dataset is a subset of the Movielens dataset with 6, 040 users, 3, 706 movies and 1, 000, 209 ratings. (2) Amazonmovies: In the category of Movies and TV in Amazon Product Data, we

remove the users with the number of ratings less than 50. Finally there are 3, 241 users and 24, 355 items, and 451, 804 ratings for our experiments.
Baselines. As we introduce in background, we use three kinds of baselines:
(1) General recommendation methods: POP, UKNN, and BPRMF [13]. (2) Markov Chain based sequential recommendation methods:
MC [2] and FPMC [14]. (3) RNN based sequential recommendation methods: RNN [2]
and manifold regularized output of the recurrent neural network denoted as MrRNN[11]. Our Kr Network: KrNN-L and KrNN-P. Evaluation Metric. We employ three kinds of metrics: (1)Short term prediction: sps. A short term prediction aims to predict items which will be bought or rated at next time [2]. (2)Long term prediction: precision, recall, ndcg and F1-score. (3)Generalization: user_coverage. We use the fraction of users who received at least one correct recommendation. All those metrics are cut off at 10 (@10) in the experiments. Evaluation Protocol.For each user in each dataset, we transform his or her historical behavior into a rating or consuming sequence according to time stamps. Each dataset is split into train, validation, and test set by a proportion 8:1:1 according to the number of users. We use 10-fold cross validation to tune the parameter and all the performances reported in our quantitative analysis section is averaged on test sets of these 10 folds. Parameter Tuning. We choose the best parameter setting over the 10-fold validation set. (1) BPRMF and FPMC: the latent dimension is set 32 for both, the learning rate is 0.05 and 0.2 respectively, the sampling bias is 200 for both,  is 1 and 0.01 respectively. (2) UKNN: the neighborhood size is 80. (3)RNN, MrRNN, KrNN-P and KrNN-L: one LSTM layer with 50 hidden units, the learning rate is set as 0.1 and we all use the adagrad for optimization. The sample size is 7 and the trade-off parameter is 0.5.
4.2 Quantitative Analysis
All the performance differences between Kr Network and baselines are statistically significant with p-value < 0.01 for paired t-test.
Short-term Prediction As shown in Table 1, KrNN-L and KrNNP outperform other recommendation algorithms in short term prediction. For example, sps@10 of KrNN-L is 34% higher than the best general recommendation UKNN on MovieLens 1M and 1.7 times higher on Amazonmovies. Compared with MC on MovieLens and Amazonmovies, the sps@10 of KrNN-P improvement is 53.5% and 39.9%. For KrNN-L, sps@10 is 5% and 97% better than RNN on MovieLens and Amazonmovies seperately. Compared with MrRNN, KrNN-L improves sps@10 by 23% on Amazonmovies.
Long-term Prediction As shown in Table 1, RNN based method outperform others. KrNN-L model achieve better performance than others recommendation algorithms on both datasets. On the Amazonmovies, the ndcg of KrNN-L is 38% higher than best general methods UKNN. Compared with MC methods on the Movielens 1M, KrNN's recall@10 is 50% higher and the precision@10 is 44.8% higher. Compared with RNN model, KrNN-L achieves better performance on both the four metrics especially on Amazonmovies.

1059

Short Research Papers I SIGIR'18, July 8­12, 2018, Ann Arbor, MI, USA

Table 1: Performance Comparison on Movielens 1M and Amazonmovies
(a) Movielens 1M

Methods s@10 r@10 p@10 F1 n@10 u@10

POP UKNN BPRMF MC FPMC RNN MrRNN KrNN-P KrNN-L

5.02 21.80 1.40 19.1 15.4 27.88 28.62 29.32 29.29

3.91 14.34 1.05 5.14 4.68 7.56 7.52 7.43 7.70

23.60 19.42 6.99 22.30 21.20 30.78 30.78 30.37 32.28

6.71 16.50 1.83 8.35 7.67 12.14 12.09 11.94 12.43

24.50 20.10 7.90 23.90 22.28 32.29 32.17 31.98 33.92

70.77 82.25 42.30 77.94 76.57 87.66 86.50 87.71 88.45

(b) Amazonmovies

Methods s@10 r@10 p@10 F1 n@10 u@10

POP

0.65 0.78 4.98 1.35 5.08 31.38

UKNN 1.70 1.09 4.62 1.76 4.86 42.73

BPRMF 0.38 0.31 1.99 0.53 2.22 16.30

MC

2.63 1.42 7.74 2.39 7.91 39.39

FPMC 2.20 0.83 4.76 1.41 5.04 28.10

RNN

2.28 1.09 6.49 1.86 6.77 33.99

MrRNN 3.65 1.10 6.47 1.88 6.90 36.54

KrNN-P 3.68 1.20 6.71 2.03 6.97 35.64

KrNN-L 4.50 1.40 8.28 2.40 8.75 40.62

Table 2: Efficiency Comparison on MovieLens

Methods K=1 K=3 K=5 K=7 K=15
KrNN-L 658.00 815.95 967.96 1186.22 1810.80 KrNN-P 1253.55 1360.75 1423.07 1456.00 1551.21
Generalization As shown in Table 1, for generalization performance, KrNN-P and KrNN-L achieves better performance than others on movielens 1M, KrNN-L achieves nearly the same generalization performance as the user_coverage@10 is 5% lower than UKNN, but it is 7% higher than UKNN on Amazonmovies. Compared with other baselines, KrNN's advantage is clear, KrNN-L's user_coverage@10 is 13.5% higher than MC and 11% higher than MrRNN on the MovieLens 1M.

4.3 Efficiency Analysis
Here we explore the sample size effect on the efficiency of KrNN-L, KrNN-P and compare the training time (seconds) per epoch between them on MovieLens 1M. The time of orignal RNN is 613.58s.
Through Table 2, we can see the sample size effect on the efficiency of KrNN is large. But k needs not to be large. We've proved that only k = 1 is enough to achieve better performance in the two datasets in our experiments. All the training time reported here is based on one Nvidia K40m GPU.

5 CONCLUSION
In this paper, We propose Kr Networks to capture both the local and global information in the sequences for better recommendation, which accommodate the query sequence and its k neighbor

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Xiang Lin, Shuzi Niu, Yiqiao Wang, and Yucheng Li

sequences jointly. We introduce two distinguished ways to model the interaction between the query sequence and its neighbors. Experimental results on benchmark datasets show that our proposed methods outperform others. In future, we will explore to make K-NN search more efficient for GPU.

6 ACKNOWLEDGMENTS
The research was supported by National Natural Science Foundation of China under Grant No.61602451.

REFERENCES

[1] Erik Bernhardsson. 2014.

Recurrent Neural Networks for

Collaborative Filtering.

https://erikbern.com/2014/06/28/

recurrent- neural- networks- for- collaborative- filtering.html.

[2] Robin Devooght and Hugues Bersini. 2016. Collaborative Filtering with Recurrent

Neural Networks. CoRR abs/1608.07400 (2016).

[3] John C Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Meth-

ods for Online Learning and Stochastic Optimization. Journal of Machine Learning

Research 12 (2011), 2121­2159.

[4] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Visual Evo-

lution of Fashion Trends with One-Class Collaborative Filtering. In Proceedings

of the 25th International Conference on World Wide Web (WWW '16). Interna-

tional World Wide Web Conferences Steering Committee, Republic and Canton

of Geneva, Switzerland, 507­517. https://doi.org/10.1145/2872427.2883037

[5] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.

2015. Session-based Recommendations with Recurrent Neural Networks. CoRR

abs/1511.06939 (2015).

[6] Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long Short-Term Memory.

Neural Computation 9, 8 (1997), 1735­1780.

[7] Elad Hoffer and Nir Ailon. 2014. Deep metric learning using Triplet network.

(2014), 84­92.

[8] Young Jun Ko, Lucas Maystre, and Matthias Grossglauser. 2016. Collaborative

Recurrent Neural Networks for Dynamic Recommender Systems. In Journal of

Machine Learning Research: Workshop and Conference Proceedings, Vol. 63.

[9] Yehuda Koren. 2008. Factorization Meets the Neighborhood: A Multifaceted

Collaborative Filtering Model. In Proceedings of KDD 2008. 426­434.

[10] Zachary Chase Lipton. 2015. A Critical Review of Recurrent Neural Networks for

Sequence Learning. CoRR abs/1506.00019 (2015). http://arxiv.org/abs/1506.00019

[11] Shuzi Niu and Rongzhi Zhang. 2017. Collaborative Sequence Prediction for

Sequential Recommender. In Proceedings of the 2017 ACM CIKM (CIKM '17).

ACM, New York, NY, USA, 2239­2242.

[12] Steffen Rendle and Christoph Freudenthaler. 2014. Improving Pairwise Learning

for Item Recommendation from Implicit Feedback. In Proceedings of WSDM 2014.

[13] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidtthieme.

2009. BPR: Bayesian personalized ranking from implicit feedback. (2009), 452­

461.

[14] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factoriz-

ing Personalized Markov Chains for Next-basket Recommendation. In Proceedings

of WWW2010. 811­820.

[15] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based

Recommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265­1295.

[16] Alexander J Smola and Risi Kondor. 2003. Kernels and Regularization on Graphs.

computational learning theory (2003), 144­158.

[17] Ilya Sutskever, Oriol Vinyals, and Q Le. 2014. Sequence to sequence learning

with neural networks. (2014), 3104­3112.

[18] Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi

Cheng. 2015. Learning Hierarchical Representation Model for NextBasket Rec-

ommendation. In Proceedings of SIGIR 2015. 403­412.

[19] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A Dynamic

Recurrent Model for Next Basket Recommendation. In Proceedings of SIGIR 2016.

729­732.

[20] Xiao Yu, Xiang Ren, Quanquan Gu, Yizhou Sun, and Jiawei Han. [n. d.]. Collabora-

tive Filtering with Entity Similarity Regularization in Heterogeneous Information

Networks.

[21] Matthew D Zeiler, Marcaurelio Ranzato, Rajat Monga, Mark Z Mao, Ke Yang, Q

Le, Patrick Nguyen, A Senior, Vincent Vanhoucke, Jeffrey Dean, et al. 2013. On

rectified linear units for speech processing. (2013), 3517­3521.

[22] Andrew Zimdars, David Maxwell Chickering, and Christopher Meek. 2001. Using

Temporal Data for Making Recommendations. In Proceedings of UAI 2001.

1060


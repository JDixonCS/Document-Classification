Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Automated Comparative Table Generation for Facilitating Human Intervention in Multi-Entity Resolution

Jiacheng Huang
State Key Laboratory for Novel Software Technology Nanjing University, China jchuang.nju@gmail.com

Wei Hu
State Key Laboratory for Novel Software Technology Nanjing University, China whu@nju.edu.cn

Haoxuan Li
State Key Laboratory for Novel Software Technology Nanjing University, China hxli.nju@gmail.com
ABSTRACT
Entity resolution (ER), the process of identifying entities that refer to the same real-world object, has long been studied in the knowledge graph (KG) community, among many others. Humans, as a valuable source of background knowledge, are increasingly getting involved in this loop by crowdsourcing and active learning, where presenting condensed and easily-compared information is vital to help human intervene in an ER task. However, current methods for single entity or pairwise summarization cannot well support humans to observe and compare multiple entities simultaneously, which impairs the efficiency and accuracy of human intervention. In this paper, we propose an automated approach to select a few important properties and values for a set of entities, and assemble them by a comparative table. We formulate several optimization problems for generating an optimal comparative table according to intuitive goodness measures and various constraints. Our experiments on real-world datasets, comparison with related work and user study demonstrate the superior efficiency, precision and user satisfaction of our approach in multi-entity resolution (MER).
CCS CONCEPTS
· Information systems  Entity resolution; Data extraction and integration;
KEYWORDS
Entity resolution; knowledge graph; comparative table; multi-entity summarization; holistic property matching
ACM Reference Format: Jiacheng Huang, Wei Hu, Haoxuan Li, and Yuzhong Qu. 2018. Automated Comparative Table Generation for Facilitating Human Intervention in MultiEntity Resolution. In SIGIR '18: The 41st International ACM SIGIR Conference
Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210021

Yuzhong Qu
State Key Laboratory for Novel Software Technology Nanjing University, China yzqu@nju.edu.cn

e1 [dbp:Lil_Eazy-E] ­ rdf:type : Person, MusicalArtist ­ rdfs:label : Lil Eazy-E ­ owl:sameAs : fb:m.01wf_p_ ­ birthDate : 1984-4-23 ­ birthPlace : Compton ­ gender : male

e2 [fb:m.01wf_p_] ­ alias : Eric Wright, Eazy-E ­ date_of_birth : 1963-9-7 ­ gender : male ­ genre : gangsta rap, hip hop ­ name : Eazy-E ­ place_of_birth : Compton

e3 [wd:Q36804] ­ rdfs:label : Eazy-E ­ altLabel : Eric Lynn Wright ­ date_of_birth : 1963-9-7 ­ desc : Gangsta rapper, producer ­ genre : gangsta rap ­ instance_of : human

e1 [dbp: ­ rdf:typ ­ rdfs:la ­ owl:sa ­ birthD ­ birthPl ­ gender ­ genre

­ genre : Gangsta rap, Hip hop ­ profession : rapper, producer ­ occupation : musician, rapper

...

­ givenName : Eric Darnell Wright ­ type : person, music.artist

­ place_of_birth : Compton

(146 property-values in total) (391 property-values in total) (141 property-values in total)

Figure 1: A motivating example. For simplicity, entities are

givenName

birthDate

denoted abliays e1, e2,dea3te._oPf_rboirtph erties and values are separated by

colons, aaltnLadbelmultidpatel_eof_vbairtlhues are divided by comme1a[dsb.p:NLila_Emazye-E-]

spe1acEersic

Darnell Wright 1984-4-23
are omitted except

"rdf:",

"rdfs:"

and

"owl­:"rd.f:type

:

Person,

MusicalArtist

e2 Eric Wright, Eazy-E 1963-9-7

­ genre : Gangsta rap, Hip hop

one3

Eric Lynn Wright

1963-9-7

Research and Development

in

Information

Retrieval,

July

8­­1g2iv,e2nN01am8,e

A: Enrinc Darnell

Wright

Arbor, MI, USA.gAiveCnNMa,mNe ew 32gr0o9up9?78.3210021 alias

York,

NrdYf:,tyUpeSA,
type

10

­ rdfs:label : Lil Eazy-E
pdaabgteier_toshfD._habtitertthps://do­i.boirrthgP/l1ac0e.1: 1C4om5/pton

altLabel

instance_of

date_of_birth

­ gender : male

1  e1 Eric Darnell Wright Person, MusicalArtist 1984-4-23

1 INTRODUCTION 2  e2 Eric Wright, Eazy-E person, music.artist

1963-9-7

2
Along

we3 ithEtrihc LeynrnaWpriidghtgrowth

human

1963-9-7

of knowledge graphs

(KGs),

billions

e2 [fb: ­ type ­ genre ­ alias
­ place ­ gend

of entities have been created from diverse sources. Entity resolution

(ER) aims to identify different entities referring to the same real-

world object. While a significant number of automated approaches

have been proposed to address the ER problem, they are still being

challenged by large-scale, heterogeneously-described entities. For

example, Figure 1 depicts an ER task with three real entities in DB-

pedia, Freebase and Wikidata, respectively. Each entity is described

by hundreds of properties and values, and some of them are very

similar, e.g., types, names and genders, which make automated ER

approaches difficult to decide whether all the three entities refer to

the same person or not.

Recent studies have demonstrated great benefits of involving humans in the ER loop. For instance, crowdsourcing, where human

workers are recruited to solve micro-tasks like verifying the cor-

rectness of entity matches, offers a feasible solution to large-scale

ER [8, 25, 32, 34, 38]. Also in some cases, the human workers are

actively engaged to improve the algorithms in automated ER ap-

proaches with active learning [21, 27]. Although all these works request a human to judge which entities in a single ER task refer

to the same, little effort has been made on how to present the criti-

cal information (e.g., important properties and values) to help her

complete the task more efficiently and accurately [4, 32].

585

1963-9-7
rap, hip hop
: Compton pper, producer music.artist values in total)

­ altLabel : Eric Lynn Wright ­ date_of_birth e: 119[6d3b-p9:-L7il_Eazy-E] ­ desc : Gangsta­rarpdpfe:try, ppero:dPuecresron, MusicalArtist ­ genre : gangst­a rradpfs:label : Lil Eazy-E
­ instSanecse_soifo:nh­u5omwBaln::saEmnetAist:iefbs:m.01wf_p_
­ occupation : m­ubsiirctihaDn,artaep:p1e9r84-4-23 ­ place_of_birth­:bCirothmPplatocne : Compton
(141 property­-vgaelunedserin: mtoatalel) ­ genre : Gangsta rap, Hip hop
e1­ g[divbepn:LNial_mEea:zyE-rEic]Darnell Wright ­ rdf:(t1y4p6ep:rPoepresrotny-,vMaluuseiscainlAtorttiaslt)

­eo2wl[:fsbam:meA.0s1:wfbf_:mp..._] ­ genre : ganeg3sta[wradp:Q...36804]

­ birthDate : 1984-4-23 ...

­ alias : Eric Wright, Eazy-E ­ rdfs:label : Eazy-E

­­bidrtahtPel_aocef_:bCirotmhp:to1n963-9e-37[wd:Q36­80a4l]tLabel : Eric Lynn Wright

­ gender : male ­­gegnerned:eGra:nmgsatalerap ...

­ rdfs:label :­Edaaztye-_Eof_birth : 1963-9-7 ­ altLabel : Eric Lynn ...

...­

genre

:

gangsta

rap,

hip hop ­ desc : Gangsta ­ date_of_birth : 1963-9-7

rapper,

producer

­ name : Eazy-E ...

­ genre : gangsta rap

­ place_of_birth : Compton ­ instance_of : human

e1 [dbp:Lil_Eazy-E] ­ rdf:type : Person ...

e2 [fb:m.01wf_p_] ­ alias : Eric Wright ...

­ rdfs:label : Lil Eazy-E ­ date_of_birth : 1963-9-7

­ owl:sameAs : fb:m... ­ genre : gangsta rap ...
­ birthDaSteI:G19I8R4-'41-283 , Ju...ly 8-12, 2018, Ann Arbor, MI, USA

­ birthPlace : Compton ­ gender : male ­ genre : Gangsta rap ...

e3 [wd:Q36804] ­ rdfs:label : Eazy-E ­ altLabel : Eric Lynn ...

­ profession : rapper, producer ­ occupation : musician, rapper

...

­ date_of_birth : 1963-9-7

e­2 ty[fpbe:m: p.0e1rswofn_,pm_]usic.artist

­ place_of_birth : Compton

Given

an

MER

task,

we

want

to
...

generate

an

optimal

comparative

­(1ty,2p5e3: ppreorpsoernt,ym-vuasluice.saritnisttotal) (141 property-values in tottaal)ble for this task. To achieve this, we deal with several challenges:

of alArtist .artist

­ genre : Gangsta rap, Hip hop ­ genre : gangsta rap, hip hop

Firstly, different entities often use various properties to convey the

­ givenNamgeiv:eaEnlirNaicsaDmaernell Wrigdhatbteir_toh­fD_aabltiieratsh: Eric Wright, Eazy-E ­ rdfs:label : aLltiLl Eabaezly-E date_of_birth

e1

[dbp:Lil_Eazy-E]
same

meaning,ea2 n[fdb:mp.r0o1wpfe_rp_ty]

heterogeneity

causes

the

difficulty

in

­ rdf:type : Person, MusicalArtist ­ type : person, music.artist
comparison and the sparseness of the table. We use several similar-

­ be1irthPElaricceD: aCronmellpWtonright 1984­-4p-2la3ce_of_birth : Compton  match

­ genre : Gangsta rap, Hip hop ­ genre : gangsta rap, hip hop
ity measures to find matched property pairs, and derive property

birthDate

­ ge2endeErr:icmWalreight, Eazy-E 1963­-9g-e7nder : male

 nonmatch

­ givenName : Eric Darnell Wright ­ alias : Eric Wright, Eazy-E
cliques by maximizing the overall match probability estimate under

ddaattee__ooff__bbiirrtthhFiguree23 :

REeriscuLylntnoWfriaghtpairw196i3s-e9-7approach

for

e1, e2

in

Figure­
­

r1dfs:label

:

Lil
a

Eazy-E
matching

birthPlace : Compton

cardinality constraint.
­ place_of_birth : Compton

Secondly,
 match

we

study

what

factors

1984-4-23

givenName

rdf:type

birthDate

contribute to the goodness of a property clique and a value, and

­ gender : male

­ gender : male

 nonmatch

1963-9-7

alias

type

date_of_birth

design several scoring functions according to the intuitions related

1963-9-7

group?

altLabel

instance_of

date_of_birth

to how much information they provide and how important they are

1

e1 Eric Darnell Wright Person, MusicalArtist 1984-4-23

to humans. The challenge is how to measure the commonalities and

2

e2 Eric Wright, Eazy-E person, music.artist

1963-9-7

3

e3 Eric Lynn Wright

human

1963-9-7

Figure 3: A comparative table for e1, e2, e3 in Figure 1

differences among multiple entities. Thirdly, the goal of generating a comparative table is to help humans attain a quick comparison of the entities, and thus the comparative table must provide adequate

Current approaches. Given an ER task to a human, there are two kinds of often-used approaches to present entities and their properties and values for the task. The first kind uses pairwise presentation, which compares two entities at a time and aligns similar
properties between them [4, 35]. The other kind displays multiple entities in a form of list [15, 19] (or grid, in particular for images [32]), just like what is typically seen from a Web search engine.
Because the lengthy and heterogeneous entity descriptions, such as hundreds of property-values in entities like e1, e2, e3 in Figure 1, would overload a human with too much superfluous information, the two kinds of approaches both use pairwise or single entity summarization to help the human focus on a small portion of important properties and values to reduce her workload [4, 16, 17, 19, 31].
Arguably, each of the two kinds of approaches has its strengths
and weaknesses. The pairwise one allows humans to focus on two
specific entity summaries at each time, but is challenging to scale
due to the large number of entity pairs to be judged. For example,
given a task containing 20 entities, a human may need to provide answers for up to 20 = 190 entity pairs without using transitivity.
2
On the other hand, the list-based needs humans to scroll among the
entities and remember and compare their summaries in mind. More
importantly, extra identity evidences observed from transitivity
and grouping would be unseen if a set of entities is broken down
to pairs or a list. Figure 2 shows the result of a pairwise approach [4] for entities e1, e2 in Figure 1. Without seeing the other entity pairs, a human is very likely to decide that e1, e2 refer to the same artist (which is wrong in this case!). So, it is natural to ask if there
is any other solution to help human intervene in ER? Our approach. In this paper, we propose a table-based solution
that presents an ER task in a condensed, rather simple, and well-
structured fashion. Specifically, given an ER task containing a set of entities to be resolved, called a multi-entity resolution (MER) task in this paper, we aim to automatically generate a special type of table, called comparative table, which respectively arranges the entities and properties in the task as the row and column headers of a table
and their corresponding values in the cells. In fact, comparative
table is considered suitable to present symbolic information and
has long been used in decision-makings [33]. For instance, Figure 3 depicts a comparative table for e1, e2, e3 in Figure 1. With the help of this table, a human would be confident of saying that e2, e3 refer to the same person, but e1 does not.

information in a limited display space. Given the constraints on entity coverage and table cell size, we formulate the optimization problem of generating a comparative table with the best goodness and propose efficient algorithms.
The main contributions of this paper are summarized as follows: · We optimize the discovery of matched property cliques with the highest overall match probability estimate among those satisfying a global 1:1 matching constraint. (Section 3) · We propose scoring functions to measure the goodness of property cliques and values for a set of entities according to several intuitions. (Section 4) · We formulate the problem of optimal comparative table generation with the entity coverage constraint. We propose an efficient algorithm to obtain approximate solutions and prove its approximation ratio. (Section 5) · We conducted extensive experiments, comparison to stateof-the-art methods and user study to verify the accuracy of matched properties, effectiveness of goodness measures and user satisfaction of comparative tables for MER. (Section 6)
2 APPROACH OVERVIEW
An entity in the KG is denoted by a URI and described by a set of properties and values, where a value can be another entity, a literal or a blank node. We treat blank nodes as null, and null value is not comparable or countable. We say that a property instantiates an entity, if it is described by that property with any non-null value.
Definition 2.1 (Human intervention in MER). Let R be an MER task containing a set of entities E = {ei }iN=1. A human intervenes in resolving E by dividing it into a set of entity groups G = {Gj }jL=1, such that all entities in a group refer to the same real-world object, and no two entities in different groups denote the same object.
To facilitate human intervention in MER, our approach leverages comparative table, which is defined as follows:
Definition 2.2 (Comparative table). Given an MER task R containing a set of entities E, let P, V be the sets of properties and values of E, respectively. A comparative table for R, denoted by CT , is a triple CT = (E, Q, W), where Q  P and W  V. E, Q respectively form the row and column headers of CT and W fills the cells.
To generate a comparative table for an MER task, our approach employs three processing steps. Figure 4 illustrates the workflow.

586

Output: comparative table

Comparative table generation Property clique comparabilities

Session 5B: Entities

{rdfs:label, name} {givenName, alias, altLabel} {rdf:type, type, instance_of}
...

0.2 {givenName, alias, altLabel} 0.4 {rdf:type, type, instance_of} 0.5 {birthDate, date_of_birth}
...

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Multiple entities

e1 [dbp:Lil_Eazy-E] ­ rdf:type : Person ... ­ rdfs:label : Lil Eazy-E ­ owl:sameAs : fb:m... ­ birthPlace : Compton ­ desc : CEO NWA... ­ gender : male ­ genre : Gangsta rap ...

e2 [fb:m.01wf_p_] ­ alias : Eric Wright ... ­ date_of_birth : 1963-9-7 ­ genre : gangsta rap ...
e3 [wd:Q36804] ­ rdfs:label : Eazy-E ­ altLabel : Eric Lynn ... ­ date_of_birth : 1963-9-7

Holistic property matching Similarity calculation
Prop. clique derivation

Property cliques Goodness measurement

Goodness scores Comparative table generation Comparative table

Discriminability

Prop. clique selection

Diversity

Abundance

Value selection

Semantics
Figure 4: Workflow of our approach

divide into groups

1. Holistic property matching aims to deal with the heterogeneity of properties. Our approach firstly leverages three similarity measures on properties' labels, local names and values for finding matched property pairs, e.g., e2.alias  e3.altLabel, which are later used to derive matched property sets, called property cliques, among multiple entities, e.g., {e1.givenName, e2.alias, e3.altLabel}. Our approach follows the widely-used duplicate-free assumption [26, 36] to constrain the global 1:1 matching, and develops an optimization algorithm to efficiently derive the optimal property cliques among those satisfying this constraint.
2. Goodness measurement obtains the goodness of property cliques by combining four factors: (1) discriminability measures how well a property clique reveals the commonalities and differences among multiple entities; (2) abundance assesses how adequate of information the property clique provides; (3) semantics gives extra scores to the ones particularly useful for ER, e.g., owl:sameAs; and (4) diversity evaluates the redundancy between different property cliques. The goodness of values is measured using similar intuitions.
3. Comparative table generation assembles entities, property cliques and values to construct a comparative table. In addition to greedily pick up several property cliques having the best goodness, our approach imposes a new entity coverage constraint to make sure that each entity is instantiated by a least number of properties. Due to the NP-hardness of optimal property clique selection under this constraint, our approach develops an approximate algorithm to efficiently find the solution. Also, value selection is optimized under a table cell size constraint.
3 HOLISTIC PROPERTY MATCHING
Given two properties, we compute three different kinds of similarities between them:
· Label similarity, denoted by SL, is obtained by comparing the labels of properties after normalization. We also extend labels with synonyms in WordNet.
· Local name similarity, denoted by SN , is measured by comparing the local names of properties' URIs.
· Value similarity, SV , is computed by comparing all the values of properties and retaining the highest score.
We employ various similarity measures for strings, numerics, dates and entities. For strings, we normalize them (via lowercasing, tokenization, stemming, etc.) and use the Soft TF-IDF measure [6] for similarity calculation, as it repeatedly achieved the best for ontology matching (OM) [2]. For numerics and dates, we use the maximum percentage differences in absolute values and days, respectively [5]. For entities as values, we compute the above string similarity

Algorithm 1: Holistic property matching

Input: Set of matched property pairs J
Output: Set of property cliques C 1 C  ; J  J; 2 Sort J in descending order according to match probability estimates;

3 repeat 4 Pop pa  pb  J holding the highest match probability estimate;

5 Find cliques Ci, Cj  C containing pa, pb , respectively;

6 if Ci =  and Cj =  then

7

Create a new clique {pa, pb } and add it in C;

8 else if Ci has any property from the same namespace as pb or Cj

has any property from the same namespace as pa then

9

Drop pa  pb , due to violate the global 1:1 constraint;

10 else

11

Merge Ci, Cj into a larger clique Ck ;

12

Remove Ci, Cj from C and add Ck in C;

13 until J = ; 14 return C;

between their labels. Furthermore, when two datasets have been partially aligned (e.g., owl:sameAs relations exist, or a simple entity matching approach is conducted ahead of time), the aligned entities are treated as the uniquely-selected representative of them. Besides, a parallel technique is developed for efficiently processing large sets of property-values. We apply the conventional logistic regression [14] on SL, SN , SV to compute the match probability estimates of property pairs. Note that our approach is adaptive to other similarity measures and combination methods (see Section 6.2).
Next, we derive matched property sets, called property cliques, from the previously-estimated matched property pairs. The properties in each clique are all matched with each other. Following the widely-used duplicate-free assumption [26, 36], we restrict that each property under one namespace can match at most one property under another namespace. This global 1:1 matching constraint serves for improving the accuracy of property cliques and the clearness of column headers in comparative tables. In other words, we do not want a clique to contain many loosely-related properties. However, the derivation of property cliques under the global 1:1 matching constraint is not a trivial process, because a property is often involved in more than one matched property pairs, while simply choosing the pair with the highest match probability estimate may lead to conflicts. See the example below.
Example 3.1. Assume that one first compares properties of e2, e3 in Figure 1. As a result, e2.alias would form two property matches with e3.label and e3.altLabel, due to they have common words in values, such as "Eric", "Wright", "Eazy-E". It is also reasonable to assume that e2.alias and e3.label have a higher match probability estimate, due to they have one identical value "Eazy-E". But after

587

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

comparing properties between e1, e3 and between e1, e2, one would end with a conflict involving e2.alias  e3.label, e1.givenName  e3.altLabel, e1.givenName  e2.alias. On the contrary, if e1, e2, e3
can be treated as a whole, one can obtain two consistent cliques:
{e1.label, e2.name, e3.label}, {e1.givenName, e2.alias, e3.altLabel}.

We refer to the process of deriving property cliques as holistic property matching, which takes as input the set of matched property pairs and returns a set of property cliques. Let I, J be the numbers of namespaces and matched property pairs from the pre-
vious logistic regression based classifier, respectively. For any two different namespaces ns , nt , the numbers of properties under ns , nt are denoted by Ms , Mt , and the ath, bth properties under ns , nt are denoted by pas , pbt , respectively. We optimize holistic property matching by maximizing the overall match probability estimate
among all matched property pairs, such that the global 1:1 matching
constraint is satisfied:

I I Ms Mt

max

 (pas , pbt ) · 1(pas  pbt ),

(1)

s=1 t =1 a=1 b=1

st

Mt
s.t. 1(pas  pbt )  1, s, t  [1, I ], s t, a  [1, Ms ], (2)
b =1

wpfrrohomepreetrhtye(tppraasaii,rnp. ebTtd)hleoobigntiasdtiiinccasftutohnrecfutminoacnttictohhnap,t1rcol(abpsaassbifiileiptsybtp)aes,s,etpqibtmuaaalstsea1omiffapptaassch,, ppebbttd are chosen to form a clique, and 0 otherwise.
Optimal holistic property matching (I  3) is NP-hard, because the NP-hard 3-dimensional assignment problem [7] can be reduced to the simplest case (I = 3). Inspired by the simple and efficient algorithm for bipartite max-weight matching, we design a greedy
algorithm for holistic property matching (Algorithm 1). Due to
the 1:1 constraint, each property exists in exactly one property cliques and every property clique contains I properties at most. Thus, we use an array to record which property clique contains
it for each property. The time complexity of sorting (Line 2) in is O (J log J ). The selection step (Lines 3­13) must examine O (J ) matched property pairs and each time takes O (|Ci | + |Cj |) = O (I ) time to check if adding this pair would violate the 1:1 constraint. So, the time complexity of selection is O (I J ). In practice, matched property pairs are few and the match probability estimates are
relatively well-behaved, so Algorithm 1 runs fast.

4 GOODNESS MEASUREMENT
For ER, humans often require "good" properties and values to help them identify entity matches and non-matches. Different from pairbased ER, an MER task consists of both matched and non-matched entities, and thus a goodness measure should simultaneously consider the commonalities and differences among multiple entities. To this end, we propose goodness measures considering four aspects of a set of entities: discriminability, abundance, semantics and diversity. Furthermore, we compute the goodness based on the information within one MER task instead of the whole dataset, which eliminates the interference from irrelevant entities and accelerates the computation process.

0.7 1

1 = 0.5 2 = 0.3 3 = 0.2

discriminability
goodness

0

1.0

0.1

0

0

1.0

proportion of distinct values

proportion of

proportion of

0.1

1.0

distinct values

entities

Figure 5: A simulation of property clique goodness measure-

ment. The left part shows a discriminability curve only, and

the right shows its combination with abundance and seman-

tics. Diversity is omitted due to it relates to others. A smaller

score (deeper color) indicates a better goodness.

First, we define the scoring functions for property cliques, where

a lower score indicates a better goodness. Discriminability. To aid a human to divide the entities in an

MER task into different groups, a property clique that holds com-

pletely different or exactly identical values for all the entities may

not be considered informative. For example in Figure 1, all the three entities have the same value for {birthPlace, place_of_birth}, and

thus it is difficult to distinguish them based on this property clique.

However, a property clique whose values have relative variety or

across a categorical scale may be more useful. To formalize, let E

be a set of entities. For a property clique Ci , we define a scoring function for its discriminability on E, denoted by discr (Ci ), inspired

by the marginal cost curve known in economics [28]:

discr (Ci ) =

|

e E norm(val (e, Ci ))| - X  , e E |norm(val (e, Ci ))|

(3)

where val (e, Ci ) extracts all the values of properties in Ci instantiating e. norm() normalizes the heterogeneous representations of

equivalent value from different sources by reusing the method in the value similarity measure in Section 3. X is our expectation of

how many distinct values a property clique should best have for MER. The farther the proportion of distinct values away from X,

the less discriminative the property clique is. We employ a power function with exponent  to amplify the discriminability margin.

The left part of Figure 5 exemplifies a discriminability curve with X = 0.25,  = 1.5.
Abundance. A property clique whose values are largely unre-

ported may be less convincing than a clique whose values are all

known. In other words, more values provide more evidences for ER
and increase the confidence of human intervention. Let E be a set of entities. For a property clique Ci , we define a scoring function for its abundance on E, denoted by abund (Ci ), by counting the proportion of entities that Ci has values for them:

1

abund (Ci )

=

1

-

log

|E|

log 1
e E

val (e, Ci )

,

(4)

where 1 val (e, Ci )  equals 1 if any property in Ci has a value for e, and 0 otherwise. Clearly, we have 1  e E 1 val (e, Ci )   |E|. The log function is used to smooth the score.
Semantics. A special kind of properties in the KG is useful for equivalence reasoning., e.g., owl:sameAs and (inverse) functional

588

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

properties (abbr. IFPs and FPs). For example, the semantics of an IFP foaf:mbox claims that different entities can be inferred identical by having the same email address. We assign a boolean score to this special kind. Given a property clique Ci , due to Ci may contain several properties and some of them belong to this special kind
while the others do not, the scoring function for the semantics of Ci , denoted by sem(Ci ), is defined by averaging the scores among all the properties in Ci :

1

sem(Ci )

=

|Ci |

sem(pa ),
pa Ci

(5)

sem(pa )

=

0 

pa is owl:sameAs, IFP or FP .

(6)

1 otherwise



Diversity. To increase the diversity and reduce the overlap

among different property cliques, we refine the maximal marginal

relevance (MMR) scheme [1] to penalize redundant property cliques.

Every time a property clique is to be selected, we compare it with

those property cliques that have already been selected and decrease

its diversity if it is largely repeating the redundant information. Let
Ci be a property clique to be selected and D be the set of already selected property cliques. The diversity of Ci given D, denoted by div (Ci | D), is defined as follows:

div (Ci

|

D)

=

max
Cj D

div

(Ci

|

Cj

),

(7)

max SV (vx , vy )

div (Ci

| Cj )

=

e Si Sj

vx val (e,Ci ) vy val (e,Cj )
|Si  Sj |

,

(8)

where Si denotes the set of entities instantiated by properties in Ci , i.e., Si = {e  E | val (e, Ci ) }. Sj is defined similarly. SV (vx , vy ) computes the value similarity of vx , vy . Let us revisit our example in Figure 1. Assume that {e1.givenName, e2.alias, e3.altLabel} has been selected, the diversity of {e1.label, e2.name, e3.label} would
be weakened due to its redundancy.
Combination. We adopt a two-phase strategy to combine the
discriminability, abundance, semantics and diversity of a property

clique. In the first phase, we combine the discriminability, abun-

dance and semantics, because they reflect the static goodness of a

property clique. In the second phase, we refine the static goodness

with diversity. We rank the static goodness of property cliques in

ascending order and adjust the goodness of a property clique by

considering the cliques ranked in front of it. In both phases, we use
the weighted sum [14] to combine different scores. Let C be a set of ranked property cliques. For a property clique Ci  C, its overall goodness, denoted by ood (Ci ), is computed as follows:

ood (Ci

)

=



comb (Ci

)

+

(1

-



)

div (Ci

|

{Cj

}ij

-1 =1

),

(9)

comb (Ci ) = 1 discr (Ci ) + 2 abund (Ci ) + 3 sem(Ci ), (10)

where k  0 (k = 1, 2, 3) are weighting factors and

3 k

=1

k

=

1. 

is a weighting factor in [0, 1]. The right part of Figure 5 exemplifies

a combination of discriminability, abundance and semantics.
Let N , M be the numbers of entities and property cliques, respectively. In each round, we only need to compare O (M ) property

cliques with the new one selected in the previous round, since we

have cached the "old" results, and a comparison costs O (N ) time to compute diversity. In total, the time complexity is O (M2N ).
Next, we measure the goodness of values. Given an entity, the
number of values of a property w.r.t. the entity can be very large as well. For example, a film star may be starred in many films, or an entity has many types. For the goodness measurement, we prefer
the values with longer length (implying more discriminative and abundant) and reuse MMR to penalize redundant ones. Let L be the table cell size constraint. The goodness of a value vi w.r.t. a set of selected values W, denoted by vood (vi ), is computed as follows:

vood (vi ) = max

len(vi ) L

-



max
vj W

SV

(vi

,

v

j

),

0

,

(11)

where len(vi ) counts the number of characters in vi and SV (vi , vj ) measures the value similarity as aforementioned.  is a weighting
factor in [0, 1]. A larger score indicates a better value goodness.

5 COMPARATIVE TABLE GENERATION
The purpose of constructing a comparative table is to help humans
attain a quick comparison of multiple entities, thus the comparative
table must fit into a limited display space (e.g., the maximal number
of table columns). Based on the goodness of property cliques, the
total score is minimized when we greedily select the set of property
cliques with lowest scores (i.e., best goodness). Therefore, the first method that we use to select property cliques is a greedy method. Given the maximal number B of property cliques in a comparative table, we simply select top-B property cliques with best goodness. Values are selected greedily as well.
However, the greedy method has a shortcoming. It cannot guar-
antee that each entity is at least described by several properties. In
an extreme case, if an entity is not shown in any property, then
humans cannot provide judgment on it due to lack of information.
Based on this intuition, we impose an entity coverage constraint
to comparative tables and accordingly formulate the optimization
problem of selecting property cliques with the best goodness among
those satisfying the entity coverage constraint.
Definition 5.1 (Optimal property clique selection with the entity coverage constraint). Let E be the set of entities and C be the set of property cliques, where each clique Ci  C instantiates a subset Si  E and has a goodness score. Given an entity least cover times T , selecting optimal property cliques for E is to select a subset D  C, such that (1) for each ej  E, there are at least T distinct property cliques that instantiate it, and (2) the total goodness score of D is minimized.

Optimal property clique selection with the entity coverage con-
straint is NP-hard, because there exists a reduction from the NPhard set cover problem [12] to it. Thus, we design an approximation algorithm shown in Algorithm 2. In each iteration, it selects the
property clique having the best goodness effectiveness (Line 4),
which is defined as a tradeoff between the goodness of a property
clique and the size of uninstantiated entities to be instantiated by that property clique. Let N , M denote the numbers of entities and property cliques, respectively. In each round, the most costly step is to examine O (M ) property cliques with each examination involving O (N ) time to intersect two entity sets. For N entities, the total time

589

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

complexity is O (MN 2). We prove the approximation ratio of the algorithm below.
Theorem 5.2. Algorithm 2 achieves approximation ratio H (N ) for optimal property clique selection with the entity coverage constraint.1

Proof. Let us sort the entities in E in the order in which they are

fully instantiated for T times by Algorithm 2, denoted by e1, e2, . . . ,

eN . Let OPT be the goodness of an optimal solution.

Suppose that entity ej is fully instantiated by property cliques

{Cjk }Tk=1, let tiated before

Ejk Cjk

be is

the set of entities that has not picked, and Djk be the set of

been fully instanproperty cliques

which the optimal solution uses to fully instantiate Ejk . Before

property clique Cjk is selected, ej , ej+1, . . . , eN are not fully instan-

tiated, thus

|Ejk |



N

-j

+ 1. Because

ood (Cjk ) |Sjk Ejk |

is the minimum,

we have

price (ej )

T
=
k =1

ood (Cjk ) |Sjk  Ejk |



T k =1

Cik Djk ood (Cik ) Cik Djk |Sik  Ejk |

T

OPT

OPT





.

(12)

k=1 T |Ejk | N - j + 1

As the goodness of each property clique is evenly distributed

among the new fully-instantiated entities, we have

ood (D) =

N j =1

price

(ej

)



N OPT
j=1 N -j+1 = H (N ),

(13)

OPT

OPT

OPT

which proves the approximation ratio of Algorithm 2.



Then, given the row and column headers (i.e., entities and se-
lected property cliques, respectively) of a comparative table, we
fill the cells of the comparative table with values. We model the
value selection based on the classic 0/1 knapsack problem and use a table cell size constraint L to limit the occupied space of each cell. Let V denote maxe E,Ci C{|val (e, Ci )|}, the dynamic programming algorithm costs O (V L) time to select values in each table cell. In practice, table cells whose values need to be selected are few (see
Table 4), so value selection runs fast.
Finally, we generate a comparative table for an MER task.

6 EXPERIMENTS AND RESULTS
We implemented our approach, called CTab, on a personal workstation with an Intel Xeon E3 3.3GHz CPU and 64GB RAM, using Virtuoso 7 to store the data and Tomcat 8 to run the Web interfaces. In this section, we report our experimental results. The datasets, source code, gold standards and experimental results are available online2.

6.1 Multi-Entity Resolution Tasks
To the best of our knowledge, there is no benchmark for MER in the KG area. We chose 10 popular domains (i.e., upper-level classes) in terms of entity numbers in DBpedia and sampled 25 testing entities per domain as seeds to build our own dataset. For each seed entity, we searched its Wikipedia disambiguation page and identified 2­4 distinct entities. Then, we inferred their matches with Freebase,
1H (N ) is the N -th harmonic number. 2 http://ws.nju.edu.cn/ctab

Algorithm 2: Property clique selection with entity coverage

Input: Property clique set C, entity set E, least cover times T

Output: Property clique subset D  C 1 D  ; C  C;

2 foreach ea  E do xa  T ;

3 while E  do

4

Select Cj

 C

such that

ood (Cj ) | ESj |

is minimized;

5 Add Cj in D and remove it from C;

6 foreach ea  E  Sj do

7

xa  xa - 1;

8

if xa = 0 then Remove ea from E;

9 return D;

Table 1: Statistical data of MER tasks

Domains Props. Values Domains

Props. Values

Person

894 53,204 Natural place

326 12,937

City

499 82,934 Edu. institution 288 26,287

Movie

457 46,458 Written work

326 18,034

Company 416 17,677 Sport team

226 19,756

Building

200 8,578 Musical work

160 12,977

Each task consists of 10 entities from DBpedia, Freebase, Wikidata and YAGO.

Wikidata and YAGO via owl:sameAs. Finally, we randomly selected 10 entities to constitute an MER task, as we want to let each task have a moderate difficulty for humans. In total, we created 250 MER tasks involving 2,500 entities and referring to 804 distinct objects. The statistical data are detailed in Table 1.

6.2 Experiment on Holistic Property Matching
Quality of matched property pairs. We tried our best to reuse existing property matches "officially" created by the dataset owners as our gold standards. As a result, 156 reference property matches in Freebase­Wikidata3 and 67 in DBpedia (ontology)­Wikidata4 were found related. We used them as seed properties and complemented their remaining matches with the other datasets. For non-matched property pairs, we got them from the pairs having one of label, local name and value similarities larger than 0.9. In total, 484 property matches and 1,397 non-matches were labeled by three graduate students in our group using majority voting. The level of agreement measured by Fleiss's kappa [18] is 0.82, indicating a sufficient agreement among the judges.
In addition to the logistic regression (LR) that we used in CTab, we also tested three other combination methods in scikit-learn5: linear regression (LinReg), CART decision tree (DecTree) and SVM. Furthermore, we employed two off-the-shelf OM tools: Falcon [20] and LogMap [23], for comparison. Their default parameters were used. We chose the conventional precision, recall and F1-score as our metrics and carried out 10-fold cross-validation to calculate the means of these metrics.
As shown in Figure 6, CTab (LR) obtained the best F1-score, and the other three combination methods also got comparable results, indicating the effectiveness of our similarity measures. For example, CTab found dbo:almaMater  yago:graduatedFrom based on the

3 https://www.wikidata.org/wiki/Wikidata:WikiProject_Freebase/Mapping 4 http://downloads.dbpedia.org/2016- 10/dbpedia_2016- 10.owl
5
http://scikit- learn.org

590

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

CTab (LR) LinReg DecTree SVM

CTab (LR) Falcon LogMap

0.868 0.824 0.893 0.877 0.727 0.73 0.669 0.706 0.791 0.773 0.763 0.782 0.868
0.983 0.97 0.727 0.233 0.066 0.791 0.377 0.124

1

0.5

0

Precision

Recall

F1-score

Precision Recall F1-score

Figure 6: Results of matched property pairs

CTab K-medoids DBSCAN APCluster

0.789 0.558
0.65 0.43
0.869 0.64
0.741 0.55
0.787 0.548
0.648 0.422

1

0.8

0.6

0.4

0.2

NMI

Purity

V-measure

Figure 7: Results of derived property cliques

involved entities and WordNet sense. In total, CTab (LR) found 4,477 property matches. The two OM tools performed not well, due to: (1) they are not particularly developed for property matching; and (2) the complete ontologies of some datasets are unavailable currently, thus the tools could not fully leverage their matching algorithms, e.g., the match repair module in LogMap. In fact, property matching is naturally harder than class matching [3], even for human experts.
Quality of derived property cliques. Based on the reference property matches in the first experiment, we computed the connected components and derived 135 reference property cliques, excluding isolated cliques containing one property only. We compared our holistic property matching with three well-known clustering methods: K-medoids, DBSCAN and APCluster ("AP" means affinity propagation)4. The distances used in these clustering methods were the reciprocals of the match probability estimates of property pairs from CTab (LR). We employed the widely-used metrics for clustering assessment: NMI (normalized mutual information), purity and V-measure (the harmonic mean of homogeneity and completeness [29]). We tested various parameters and used the ones achieving the best NMI scores. For K-medoids, n_clusters = 135. For DBSCAN, min_samples = 1, eps = 0.3. For APCluster, dampin = 0.95.
As shown in Figure 7, CTab achieved the best results according to all the metrics. In total, CTab derived 343 non-singleton property cliques. We observed that: (1) our holistic matching algorithm based on the match probability estimates was effective; (2) the clustering methods were not suitable for holistic property matching, due to matching requires higher accuracy; and (3) DBSCAN was radical to generate many single-node clusters, while APCluster was conservative to form several very big clusters.
Running time. The most expensive step in CTab was similarity calculation, which in average took 27 seconds per task. Other steps, including property clique derivation, goodness measurement and comparative table generation, were done in less than one second.

6.3 Experiment on Property Clique Selection
We developed an online system to score the non-singleton property cliques in each MER task. Highly-useful, fairly-useful, marginallyuseful and useless property cliques in a task were graded 3, 2, 1 and 0, respectively. 24 humans in the KG and database fields, who have experience in similar gold standard construction, were invited and each task was scored by three judges. The average rank correlation

between the judges, measured by intraclass correlation coefficient [30], is 0.67, indicating a good agreement among them. Following the strategy in [22], when the three judges gave different grades to the same property clique, we arbitrated the property clique and assigned a more appropriate grade if the largest difference is 1 point; otherwise we dropped this clique.
We made a comparison between CTab and two state-of-the-art systems: FACES [16] and C3D+P [4], each representing a kind of automatic solution for presenting ER tasks. Here, we only compared their property selection components. We would compare the original full systems in the next subsection.
· FACES [16] is an entity summarization approach using incremental hierarchical conceptually clustering to select important property-value pairs (called features). We employed it to provide a summary for each entity in an MER task and displayed all the entities in a list.
· C3D+P [4] generates several pairs of aligned property-values as a summary for two entities, by considering the commonality, conflict and informativeness. We refined it for MER by allowing humans to select any two entities to compare.
We extended FACES and C3D+P to generate property cliques for multiple entities. For FACES, the reference property cliques were used to group selected properties and ranked in decreasing order based on the summation of the score for each property in a clique. For C3D+P, property cliques were built from the aligned property pairs based on connected components and ranked by summing up the scores of the aligned property pairs in a clique. For CTab, we uniformly used the parameters in the rest of our experiments: X = 0.3,  = 1.5, 1 = 0.45, 2 = 0.25, 3 = 0.3,  = 0.84,  = 0.1,T = 6 and L = 100. We would show the analysis of parameter sensitivity shortly. We also developed an entropy-based scoring function as our baseline, which measures the value distribution of each property clique and uses entropy as its goodness.
First, we evaluated the property clique ranking quality by directly using reference property cliques as input. We used precision at rank B (B = 1, 5, 10) and normalized discounted cumulative gain at rank 5 (nDCG@5) in IR [22], as our metrics. Second, we assessed the quality of property clique selection and goodness measurement together. The evaluation metric is somehow complex, as the property cliques found by an approach were not the same as the references, and their rankings were also different. We employed the Hausdorff version of the Kendall tau distance [10], denoted by KH aus (), which treats property clique rankings as partial rankings of properties (i.e., the properties with the same grade and in the same clique are tied):

KH aus (D, Z) = max

max min K (µ, ), max min K (µ, ) , (14)

µ D  Z

 Z µ D

where µ is a refined full ranking of the property cliques D found by an approach (written as µ  D), and  is similarly defined for the references Z. K () denotes the original Kendall tau distance [24].

Useless property cliques were removed to avoid noises.

As shown in Table 2, the results of CTab are most consistent to the

rankings given by the judges, no matter using the reference property

cliques or its own goodness measurement. For FACES, its ranking

method is based on the idea of TF-IDF, which often ranked a few par-
ticular properties with high scores, e.g., fb:notable_for. C3D+P discovered similar properties (e.g., foaf:name and foaf:surname) rather

591

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Results of selected property cliques

Using reference property cliques P@1 P@5 P@10 nDCG@5

KH aus

FACES

0.176 0.310 0.290 0.239

0.753

C3D+P

0.040 0.347 0.511 0.154

0.647

CTab (entropy) 0.180 0.178 0.184 0.092

0.811

CTab (greedy) 0.632 0.660 0.615

CTab

0.756 0.754 0.643

0.684 0.798

0.647 0.615

The best results are marked in bold. The same to the following.

Table 3: Ablation study of goodness measurement

KH aus

Discr. Abund. Sem. w/o Div. Goodness

CTab (greedy) 0.678 0.686 0.673 0.655

0.647

CTab

0.675 0.633 0.815 0.618

0.615

0.85
0.8
0.75
0.7
0.65
0.6 0 0.2 0.4 0.6 0.8 1.0 0

0.623 0.621 0.619

Best KHaus is achieved at
=0.84.

0.617

0.615
1.0 0.8 0.6 0.4
(1­) for diversity

0.72 0.76
0.8 0.84 0.88 0.92 0.96
1

(a) Weights of discriminability (1), abundance (2) (b) Weight of diversity (1-

and semantics (3)

)

Figure 8: Analysis of parameter sensitivity

than equivalent ones, which led to some incorrect or conflicting
property cliques. For the entropy-based alternative, it preferred to pick the cliques with a large collection of values, e.g., {dbo:starring, wd:cast_member}. The greedy alternative performed slightly worse than CTab, which indicated the importance of entity coverage. Note
that the average running time per task for C3D+P was more than 6
minutes and much slower than the others, because it generated 45
pairwise summaries per task and some entities have many features, e.g., fb:m.0k3p (Amsterdam) has 192 properties and 3,330 features.
Next, we conducted an ablation study on goodness measurement.
From Table 3, we observed that, for CTab and its greedy alternative,
combining the discriminability, abundance and semantics achieved
better results than only using a single measure, which were fur-
ther slightly improved with diversity. Although this improvement
seems not very large, we argue that it is in fact important to the
comparative tables, especially when the number of property cliques
is small. Besides, even for discriminability, it was still better than
entropy, showing that entropy is not a suitable measure for MER.
Also, we carried out an analysis of parameter sensitivity for the
weighting factors in goodness combination (see Eqs. (9, 10)). Under  = 1.0, we tested weights 1, 2, 3 from 0.0 to 1.0 with step 0.05, and found the best result at 1 = 0.45, 2 = 0.25. Then, we used the above weights and tested  from 0.7 to 1.0 with step 0.04, and found the best result at  = 0.84. As shown in Figure 8, we observed that our parameters were relatively stable in a range of parameters.
Based on the reference property clique rankings, we observed
values in each table cell (i.e., values of a property w.r.t. an entity) exceeding our size constraint (L = 100). Table 4 shows that the percentage of table cells whose values need to be selected is 5.3% in

Table 4: Statistical analysis of value selection

Scores

Table cell % of cells need to be selected

Highly-useful Fairly-useful Marginally-useful Total

5,181 7,430 8,189 20,800

2.8% 1.4% 10.3% 5.3%

total, where the majority comes from the marginally-useful properties. We leave the validation of selected values to future work.

6.4 Experiment on Human Intervention in MER
We recruited real humans to use and compare full CTab, FACES and C3D+P for MER. We carried out two independent groups of experiments showing top-5 and top-10 properties and values to the participants. In the top-5 (top-10) experiment, the number of features for each entity in FACES was 5 (10), the maximum number of features for each pair of entities in C3D+P was 10 (20), and the entity least cover times T for a comparative table in CTab was 3 (6). If this least cover times could not be satisfied within 5 (10) property cliques, we only kept the top-5 (top-10) for a fair comparison.
We recruited 60 graduate students in computer science by posting ads and paid each student 15USD for participating in the experiments. Before the experiments, all the students confirmed that they had no difficulty in English reading and well understood the goal of the experiments. 30 participants conducted the top-5 experiment, and the other 30 participants conducted the top-10 experiment.
During the experiments, a participant was instructed to assign a unique group ID to the entities in an MER task referring to the same real-world object, by observing the systems but not to use other external resources like accessing the full information of the entities. We offered a tutorial to the participant ahead of time and gave her a warm-up task as a training exercise to help her familiarize each system. Then, the participant conducted 10 randomly-selected MER tasks for each system, and all the 30 tasks were orthogonal to eliminate the learning effect caused by repeating on the same task. For each system, we guaranteed that the number of tasks in each domain was the same, e.g., a participant conducted one task about "movie" for each system. The three systems were arranged in a balanced order to all the participants, and thus each system was rotated to different places equally. To further reduce potential physiological and psychological factors, the participant was given a one hour break before changing to another system. We recorded the resolution results and completion time for each participant.
Once the participant completed each task, she was asked to complete a survey about task difficulty and domain familiarity, using a five-point Likert item from 1 point to 5. We then calculated the average scores and significance in statistics (one-way ANOVA and LSD post-hoc) for all the systems. Table 5 shows that the task difficulty is not significantly different in statistics among the three systems, which excluded this factor to affect the participants' performance and ensured the fairness of the following experiments. The participants were relatively familiar with "movie" and "written work", while the other domains have no difference.
Human completion time. For each system, the participants' completion time per task is listed in Table 6, indicating that they spent much less time on CTab than the other two systems. ANOVA found that the differences are significant in statistics on both top-5

592

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

and top-10 experiments. The reasons are that, at each time, CTab

allowed the participants to observe 10 entities, while C3D+P only

compared two entities and FACES needed the participants to scroll

among the entities. It was surprising that the participants spent

more time on C3D+P in the top-5 experiment. The participants told

that they had to view more entity pairs before making a decision,

due to lack of consistency of features among different entity pairs.
Human precision. We assessed the participants' precision. Assume that, given a task, x pairs of entities are judged correctly by a

participant (by breaking the entities in each entity group down to

pairs) and the gold standard contains y pairs, her precision on this

task

is

x y

.

Table

6

shows

that

the

participants

achieved

a

better

av-

erage precision on CTab given top-5 property cliques, and ANOVA

showed the significance in statistics. However, for the top-10 ex-

periment, the average precision of the participants all increased,

and the difference between the three systems became insignificant.

Human scoring and comments. We designed an effectiveness

assessment questionnaire to score the three systems from four dif-

ferent aspects, using a five-point Likert item from 1 for "totally

disagree" to 5 for "totally agree". As depicted in Table 7, CTab pre-
sented more adequate (Q1), comparative information (Q3) and was easier to use (Q4) than FACES and C3D+P, and ANOVA demon-

strated the significance in statistics. Also, because FACES did not

well consider the diversity among entities, the scores of C3D+P and CTab on Q2 are better than FACES.

We collected the participants' comments. For FACES, most par-

ticipants complained it not specifically designed for ER. The lack of

property matching was a major weakness. For C3D+P, most com-

plaints were about the increased number of mouse clicks to com-

pare all pairs of entities. For the entities in each task, C3D+P would

change feature pairs with the change of entity pairs, which really

confused the participants. Furthermore, the number of properties

for each entity was sometimes skewed due to its knapsack-based

selection method (e.g., selecting 3 features for one entity and 7 for

the other). For CTab, if the least cover times was not satisfied, a few

entities would have inadequate properties describing them (e.g., 1.6% entities had less than three properties in the top-5 experiment).

Also, several property matches were observed not exactly identical (e.g., fb:genre and rdfs:type).

7 RELATED WORK
Entity resolution (ER), a.k.a. entity linkage or matching, has attracted much attention in the KG community, particularly driven by the Linking Open Data (LOD) project. While recent studies have shown great benefits of using crowdsourcing [8, 25, 32, 34, 35, 38] and active learning [21, 27] for ER, little effort has been made to assist human intervention in a single ER task [4]. Existing ER systems, e.g., SameAs.org [15] and ObjectCoref [19], simply list entities in order and show a few property-values (e.g., labels). They bring a human heavy burdens to participate in the MER tasks, because she has to scroll up and down and compares all entities mentally. To present entity pairs, C3D+P [4] extracted pairwise summaries with a quadratic knapsack algorithm on matched property-values. However, ER often involves entities from several sources [32] and pairwise comparison cannot scale. Moreover, its pairwise comparison may

Table 5: Scores of task difficulty and domain familiarity

FACES C3D+P

CTab p-value

Task difficulty

1.41 (0.38) 1.38 (0.26) 1.39 (0.32) 77.2%

Domain familiarity movie, written work > the others 0.05%

The numbers in brackets are standard deviations. The significance levels of oneway ANOVA and LSD post-hoc were set to 5%. The same to the following.

Table 6: Results of human completion time and precision

Top-5 FACES (L) C3D+P (P) CTab (T) p-value Post-hoc

Time (s) 153 (51.8) 208 (86.8) 96 (27.1) 0.01% P < L < T

Prec.

0.63 (0.16) 0.69 (0.12) 0.77 (0.14) 0.07% L, P < T

Top-10 FACES (L) C3D+P (P) CTab (T) p-value Post-hoc

Time (s) 175 (68.5) 180 (53.7) 131 (53.1) 1.13% L, P < T

Prec.

0.79 (0.12) 0.77 (0.12) 0.80 (0.09) 69.8%

mislead humans on seemingly similar entities, because some implicit evidences like transitivity and grouping may be lost during breaking a set of entities down to pairs and switching between different pairs (recall Figure 2 for example).
ER has also been extensively studied in other domains [5, 9, 14]. Whang et al. [35] implemented various presentations for image pair resolution, and analyzed the factors that can improve their effectiveness for crowdsourcing. CrowdER [34] compared pairwise and cluster-based ER interfaces, and designed a hybrid method to generate the minimum number of crowdsourcing tasks. Waldo [32] proposed a hybrid approach to combine pairwise and multi-item interfaces for image resolution, and optimized the trade-off between cost and accuracy of the two interfaces based on task difficulty. But, the multi-item interfaces developed by [32, 34] are very basic, which did not consider how to extract important information of multiple entities and organize it in a proper presentation.
Ontology matching (OM) exploits class and property matches between different ontologies. CogZ [11] provided cognitive support and visualization for semi-automatic OM. Fu et al. [13] compared two common interfaces in OM tools, referred to as indented tree and node-link graph, which both displayed two ontologies side by side and used lines or a list to show matches. Generally, ontologies have richer hierarchies and axioms, while entities are described by flat property-values; OM mainly focuses on two ontologies, while ER may involve multiple entities. So, it is difficult to directly reuse the OM presentation approaches for ER.
Entity summarization aims to extract a small snippet of an entity as its summary for quick access of the entity-related information. Although the summaries generated by FACES [16] and LinkSum [31] may be useful for a single entity understanding, as demonstrated in our experiments, it would be more effective if we use comparative summaries specifically designed for ER. C3D+P [4] generated pairwise summaries. REMES [17] created summaries for multiple entities. Different from our work, its goal is to summarize several different but context-dependent entities, e.g., Apple Computer and Steve Jobs in the same piece of news. Yan et al. [37] built preview tables for an entity graph. However, they only focused on summarizing a schema structure.
To the best of our knowledge, no previous methods have generated comparative tables for MER. We believe that our approach can be integrated with current crowdsourcing platforms, e.g., Amazon MTurk and CrowdFlower, to publish MER tasks.

593

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 7: Results of human scoring for effectiveness assessment

Questions [from 1: "totally disagree" to 5: "totally agree"]

FACES (L) C3D+P (P) CTab (T)

Q 1. The system provided adequate information of entities. Q 2. The system provided unsuperfluous information of entities. Q 3. The system helped me easily compare entities of interest. Q 4. I found the system easy to use.

3.11 (1.25) 2.67 (1.02) 2.43 (1.17) 3.00 (1.11)

3.17 (1.06) 3.30 (0.95) 3.37 (1.07) 3.13 (1.22)

3.70 (1.03) 3.23 (1.30) 4.00 (1.17) 3.70 (1.15)

p -value 0.76%
4.46% < 0.01% 2.28%

Post-hoc
L, P < T L < T, P L<P<T L, P < T

8 CONCLUSION
In this paper, we proposed an automated approach to generating
comparative tables that can facilitate human intervention in MER.
In summary, we presented a holistic property matching method
that combined three kinds of similarity measures and optimized
the discovery of property cliques under constraint. We designed
various goodness functions to score the discriminability, abundance,
semantics and diversity of property cliques. We formulated the op-
timal comparative table generation problem and designed efficient
algorithms. By comparing with existing methods and conducting
user study, our experimental results showed the superior efficiency,
precision and user satisfaction of our approach in MER. In future
work, we plan to study the feasibility of combining comparative
tables with other presentation enhancements as a hybrid approach.
We also want to extend our work to other areas such as knowledge
base summarization.
ACKNOWLEDGMENTS
This work is supported by the National Natural Science Foundation
of China (No. 61772264), and the Collaborative Innovation Center
of Novel Software Technology and Industrialization. Part of this
work was done during Wei Hu's visit to University of Toronto.
REFERENCES
[1] Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR. ACM, Melbourne, Australia, 335­336.
[2] Michelle Cheatham and Pascal Hitzler. 2013. String similarity metrics for ontology alignment. In ISWC, Part II, Vol. LNCS 8219. Springer, Sydney, Australia, 294­309.
[3] Michelle Cheatham and Pascal Hitzler. 2014. The properties of property alignment. In ISWC Workshop on Ontology Matching. CEUR-WS, Trentino, Italy, 13­24.
[4] Gong Cheng, Danyun Xu, and Yuzhong Qu. 2015. C3D+P: A summarization method for interactive entity resolution. Journal of Web Semantics 35 (2015), 203­213.
[5] Peter Christen. 2012. Data matching: Concepts and techniques for record linkage, entity resolution, and duplicate detection. Springer, Berlin, Germany.
[6] William W. Cohen, Pradeep Ravikumar, and Stephen E. Fienberg. 2003. A comparison of string distance metrics for name-matching tasks. In IIWEB. AAAI Press, Acapulco, Mexico, 73­78.
[7] Yves Crama and Frits C.R. Spieksma. 1992. Approximation algorithms for threedimensional assignment problems with triangle inequalities. European Journal of Operational Research 60, 3 (1992), 273­279.
[8] Gianluca Demartini, Djellel Eddine Difallah, and Philippe Cudrè-Mauroux. 2012. ZenCrowd: Leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking. In WWW. ACM, Lyon, France, 469­478.
[9] Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and Vassilios S. Verykios. 2007. Duplicate record detection: A survey. IEEE Transactions on Knowledge and Data Engineering 19, 1 (2007), 1­16.
[10] Ronald Fagin, Ravi Kumar, Mohammad Mahdian, D. Sivakumar, and Erik Vee. 2004. Comparing and aggregating rankings with Ties. In PODS. ACM, Paris, France, 47­58.
[11] Sean M. Falconer and Margaret-Anne Storey. 2007. A cognitive support framework for ontology mapping. In ISWC/ASWC, Vol. LNCS 4825. Springer, Busan, Korea, 114­127.
[12] Uriel Feige. 1998. A threshold of ln n for approximating set cover. J. ACM 45, 4 (1998), 634­652.
[13] Bo Fu, Natalya F. Noy, and Margaret-Anne Storey. 2013. Indented tree or graph? A usability study of ontology visualization techniques in the context of class

mapping evaluation. In ISWC, Part I, Vol. LNCS 8218. Springer, Sydney, Australia, 117­134.
[14] Lise Getoor and Ashwin Machanavajjhala. 2012. Entity resolution: Tutorial. In PVLDB, Vol. 5. VLDB, Istanbul, Turkey, 2018­2019.
[15] Hugh Glaser, Afraz Jaffri, and Ian C. Millard. 2009. Managing co-reference on the semantic web. In WWW Workshop on Linked Data on the Web. CEUR-WS, Madrid, Spain, 6.
[16] Kalpa Gunaratna, Krishnaparasad Thirunarayan, and Amit Sheth. 2015. FACES:
Diversity-aware entity summarization using incremental hierarchical conceptual clustering. In AAAI. AAAI Press, Austin, TX, USA, 116­122. [17] Kalpa Gunaratna, Amir Hossein Yazdavar, Krishnaparasad Thirunarayan, Amit
Sheth, and Gong Cheng. 2017. Relatedness-based multi-entity summarization. In IJCAI. IJCAI Organization, Melbourne, Australia, to appear. [18] Harry Halpin, Daniel M. Herzig, Peter Mika, Roi Blanco, Jeffrey Pound, Henry S.
Thompson, and Thanh Tran Duc. 2010. Evaluating ad-hoc object retrieval. In ISWC Workshop on Evaluating Semantic Technologies. CEUR-WS, Shanghai, China, 9.
[19] Wei Hu and Cunxin Jia. 2015. A bootstrapping approach to entity linkage on the semantic web. Journal of Web Semantics 34 (2015), 1­12.
[20] Wei Hu and Yuzhong Qu. 2008. Falcon-AO: A practical ontology matching system. Journal of Web Semantics 6 (2008), 237­239.
[21] Robert Isele and Christian Bizer. 2013. Active learning of expressive linkage rules using genetic programming. Journal of Web Semantics 23 (2013), 2­15.
[22] Kalervo Järvelin and Jaana Kekäläinen. 2000. IR evaluation methods for retrieving highly relevant documents. In SIGIR. ACM, Athens, Greece, 41­48.
[23] Ernesto Jiménez-Ruiz and Bernardo Cuenca Grau. 2011. LogMap: Logic-based and scalable ontology matching. In ISWC, Vol. LNCS 7031. Springer, Bonn, Germany, 273­288.
[24] Maurice George Kendall. 1938. A new measure of rank correlation. Biometrika 30, 1/2 (1938), 81­93.
[25] Fenglong Ma, Yaliang Li, Qi Li, Minghui Qiu, Jing Gao, Shi Zhi, Lu Su, Bo Zhao,
Heng Ji, and Jiawei Han. 2015. FaitCrowd: Fine grained truth discovery for crowdsourced data aggregation. In KDD. ACM, Sydney, Australia, 745­754. [26] Imen Megdiche, Olivier Teste, and Cassia Trojahn. 2016. An extensible linear approach for holistic ontology matching. In ISWC, Part I, Vol. LNCS 9981. Springer, Kobe, Japan, 393­410.
[27] Axel-Cyrille Ngonga Ngomo, Klaus Lyko, and Victor Christen. 2013. COALA ­ Correlation-aware active learning of link specifications. In ESWC, Vol. LNCS 7882. Springer, Montpellier, France, 442­456.
[28] Arthur O'Sullivan and Steven M. Sheffrin. 2003. Economics: Principles in action. Pearson Prentice Hall, Upper Saddle River, NJ, USA.
[29] Andrew Rosenberg and Julia Hirschberg. 2007. V-Measure: A conditional entropybased external cluster evaluation measure. In EMNLP-CoNLL. ACL, Prague, Czech Republic, 410­420.
[30] Patrick E. Shrout and Joseph L. Fleiss. 1979. Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin 86, 2 (1979), 420­428.
[31] Andreas Thalhammer, Nelia Lasierra, and Achim Rettinger. 2016. LinkSUM: Using link analysis to summarize entity data. In ICWE, Vol. LNCS 9671. Springer, Lugano, Switzerland, 244­261.
[32] Vasilis Verroios, Hector Garcia-Molina, and Yannis Papakonstantinou. 2017. Waldo: An adaptive human interface for crowd entity resolution. In SIGMOD. ACM, Chicago, IL, USA, 1133­1148.
[33] Iris Vessey. 1991. Cognitive fit: A theory-based analysis of the graphs versus tables literature. Decision Sciences 22, 2 (1991), 219­240.
[34] Jiannan Wang, Tim Kraska, Michael J. Franklin, and Jianhua Feng. 2012. CrowdER: Crowdsoucing entity resolution. In PVLDB, Vol. 5. VLDB, Istanbul, Turkey, 1483­ 1494.
[35] Steven Euijong Whang, Julian McAuley, and Hector Garcia-Molina. 2012. Compare me maybe: Crowd entity resolution. Technical Report. Stanford University.
[36] Chuncheng Xiang, Baobao Chang, and Zhifang Sui. 2015. An ontology matching approach based on affinity-preserving random walks. In IJCAI. IJCAI Organization, Buenos Aires, Argentina, 1471­1477.
[37] Ning Yan, Sona Hasani, Abolfazl Asudeh, and Chengkai Li. 2016. Generating preview tables for entity graphs. In SIGMOD. ACM, San Francisco, CA, USA, 1797­1811.
[38] Yudian Zheng, Guoliang Li, and Reynold Cheng. 2017. DOCS: A domain-aware crowdsourcing system using knowledge bases. In VLDB. VLDB Endowment, Munich, Germany, 361­372.

594


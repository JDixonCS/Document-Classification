Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

On Link Prediction in Knowledge Bases: Max-K Criterion and Prediction Protocols

Jiajie Mei
BDBC and SKLSDE Beihang University jiajie.mei@buaa.edu.cn

Richong Zhang
BDBC and SKLSDE Beihang University zhangrc@act.buaa.edu.cn

Yongyi Mao
School of Electrical Engineering and Computer Science University of Ottawa ymao@uottawa.ca
ABSTRACT
Building knowledge base embedding models for link prediction has achieved great success. We however argue that the conventional top-k criterion used for evaluating the model performance is inappropriate. This paper introduces a new criterion, referred to as max-k. Through theoretical analysis and experimental study, we show that the top-k criterion is fundamentally inferior to max-k. We also introduce two prediction protocols for the max-k criterion. These protocols are strongly justified theoretically. Various insights concerning the max-k criterion and the two protocols are obtained through extensive experiments.
CCS CONCEPTS
· Computing methodologies  Reasoning about belief and knowledge; Statistical relational learning; · Information systems  Top-k retrieval in databases;
KEYWORDS
Knowledge Base Embedding, Link Prediction, Evaluation Metric
ACM Reference Format: Jiajie Mei, Richong Zhang, Yongyi Mao, and Ting Deng. 2018. On Link Prediction in Knowledge Bases: Max-K Criterion and Prediction Protocols. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210029
1 INTRODUCTION
The emerging of large-scale knowledge bases (KBs) such as Freebase [2], Yago [16] and DBpedia [1] has stimulated the development of novel information-retrieval applications with great commercial or societal impact. Usually a KB is presented as a collection of triples (h, r , t), where h and t are entities and r is a relation. For example, a knowledge triple could be (Paris, IsCapitalOf,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210029

Ting Deng
BDBC and SKLSDE Beihang University dengting@act.buaa.edu.cn
France). Although the existing KBs contain enormous amount of such information, it is well known that significant amount of factual knowledge is in fact missing. For example, not all facts about Paris, about France, or about which city is the capital of which country are collected in the KB. This has motivated intense research on knowledge base completion, where the objective is to "fill in" the missing information based on the current content of a KB.
Most commonly the KB completion problem is posed as link prediction. Briefly explained, in a link prediction task (h, r , ?) for a given entity h and a given relation r , the objective is to determine which entity (or entities) t that can form a factual triple (h, r, t).
The current dominating methodology for KB link prediction is via learning a KB embedding model [3, 5, 10­12, 15, 18, 20], in which the entities and relations of the KB are represented as quantities in some Euclidean space. Such a methodology, fundamentally based on distributed representations [8], has not only proved to be effective for KB link prediction, but also helped to improve our understanding and engineering of knowledge representation.
Despite the success of various KB embedding models, we argue that the evaluation criterion used in link prediction is inappropriate. Specifically, when answering the link prediction task (h, r , ?), implicitly or explicitly the top-k criterion is used. That is, for a given value of k, the predictive algorithm outputs k answers based on ranking certain score computed for each entity. We argue that this criterion is problematic when for different (h, r, ?) tasks, the number of correct answers, or "answer multiplicity", varies significantly. In this case it is impossible to select a global cutoff that compromises well across such a wide spectrum of answer multiplicities.
This paper presents a new evaluation criterion, which we refer to as max-k. In this criterion, the predictive algorithm is asked to give at most k answers for a prescribed k value. In other words, the predictive algorithm is free to use a strategy or protocol to output any number of answers, no greater than k. We adapt the classical precision, recall and F1 metrics to this setting to evaluate the answers provided by the predictive algorithm. We show that when the predictive algorithm is an oracle knowing the correct answers, then max-k is at least as good as top-k under these metrics.
We then present two protocols applying universally to all prediction models. The first is a sampling protocol which draws k entities from the predictive distribution generated by the model, using distinct drawn entities as the answers. We show that if the model's predictive distribution reflects the oracle's knowledge of the correct

755

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

answers, the sampling protocol is optimal in an average sense. The second is a deterministic greedy protocol, developed and theoretically justified via an asymptotic analysis of the sampling protocol.
Using these protocols, we carry out extensive experiments investigating the performance of some representative KB embedding models over several popular datasets. We highlight the following experimental findings. First, the precision, recall and F1 metrics under the max-k criterion behave differently from the conventional metrics used for the top-k criterion. We believe that these max-k metrics provide a more balanced measure for model performances. Additionally, we may use the sampling and greedy protocols on the existing models and compare their performance against the oracle max-k limits. This allows us to assess the performance gap between these models and the oracle limits thereby assessing the performance of the current art relative to the theoretical limits. Finally, a comparison of the top-k and max-k oracle limits suggests that the top-k criterion is fundamentally and significantly inferior.
2 LINK PREDICTION AND TOP-K
2.1 The Link Prediction Problem
Consider a knowledge base (KB) that contains N entities. We will, for simplicity, identify the set of entities as the set of positive integers [N ] := {1, 2, . . . , N }. Let R denote the set of relations contained in the KB. Then the KB can be expressed as a collection of triples (h, r , t)  [N ] × R × [N ], where h and t are referred to as the head and tail of the triple. Each triple here represents a factual knowledge item. The KB considered for link prediction is then specified by the set of all triples in the KB, which we denote by G.
In practice, the set G of triples is far from being complete since there are additional factual triples missing from G. Let G0 denote the complete collection of all factual triples involving the entities in [N ] and the relations in R. The goal of a link prediction task (h, r, ?), based on G, is to predict the set of all triples (h, r , t)  G0 \ G.
Here the link prediction problem is formulated as predicting the tails. The problem may also be formulated as predicting the heads. Although in our experiments we will consider both, we restrict our discussion to tail prediction, to ease the presentation.
In essence, every link-prediction algorithm develops a score function  : [N ] × R × [N ]  R on the space of all possible triples (factual or not) based on the information contained in G. For every given triple (h, r, t), (h, r , t) attempts to measure the likelihood that the triple (h, r, t) is factual. Without loss of generality, we will assume that a higher score indicates a higher (estimated) likelihood.
The score function  is built right upon the representation of knowledge (triples). A large portion of link-prediction models try to represent entities and relations as embeddings in some low dimensional space. An embedding model essentially constructs such a score function , which depends on h, r , t only through their embeddings. The differences between various KB embedding models mainly lie in the parametrization of function . In a separate section, we will give a brief overview of some of these models.
2.2 Top-K Criterion and Conventional Metrics
In all link-prediction models presented to date, the top-k criterion is implicitly or explicitly assumed for selecting the final answers. Under this criterion, for a given link prediction task (h, r, ?), the

-score, i.e., (h, r, t ), for each candidate entity t is computed and ranked; the candidate entities giving the top-k highest scores are declared as the answers. Here, k is a prescribed cutoff rank.
As the top-k criterion is completely based on the ranking of candidate answers, conventional rank-based performance metrics are usually used to evaluate the prediction models [4].
The definitions of these metrics depend on the availability of another set of factual triples G  G0 \ G. Note that a standard practice to obtain G is to reserve a subset of the KB triples as G and use the remainder as G. In standard terminologies, G is regarded as the testing data, and the data in G is used for training and possibly validation as well. Now we review these metrics.
The raw rank RNK(h, r, t) of a triple (h, r, t)  G is the position of (h, r, t) in the list of {(h, r, t ) : t  [N ]} sorted in descending order of -score. For any given triple (h, r, t)  G , denote C(h, r , t) := {(h, r, t )  G  G : t t }. The filtered rank fRNK(h, r , t) is the position of (h, r, t) in the list of {(h, r, t ) : t  [N ]} \ C(h, r , t) sorted in descending order of -score.
For a given positive integer k, the top-k hit HIT@k and the filtered top-k hit fHIT@k are the fraction of triples (h, r, t) in G for which RNK(h, r, t)  k and that for which fRNK(h, r , t)  k, re-
spectively. The mean rank RNK and the filtered mean rank fRNK are respectively the average of RNK(h, r, t) and that of fRNK(h, r , t) over all triples in G . The mean reciprocal rank MRR and the filtered mean reciprocal rank fMRR are respectively the average of RNK(h, r , t)-1 and that of fRNK(h, r , t)-1 over all triples in G .
Among these metrics, the filtered metrics are meant to characterize the predictability of the algorithm. For the prediction algorithms that are based on machine learning models, these metrics essentially measure the generalizability of the models. The raw metrics, on the other hand, characterize the aggregated performance both in terms of generalizability and in terms of memorization capability (namely, how well the model "remembers" the training examples).

3 PREVIOUS EMBEDDING MODELS

We now give a concise review of KB embedding models for link

prediction, where we focus on the models used in our experiments.

TransE [4] is one of the pioneering models for KB embedding,

generalizing an earlier model known as Unstructured Model [3],

and is extended to a sequence of later models, such as TransH [19]

and TransR [11]. Briefly, for each triple (h, r, t) in the KB, TransE

assumes that the embeddings h and r for the head and tail entities

and the embedding r for the relation satisfy h + r  t. The score

function (h, r, t) is defined as the L2 norm of h + r - t.

ComplEx [18] embeds entities and relations in complex space.

For each r , it assumes a score matrix Xr whose sign matrix is

partially observed. The entries correspond to factual and nonfactual

triples have the sign "1" and "-1", respectively. The score function

(h, r, t) = Re

K i =1

ri

hi

t¯i

models the entries of Xr (Re(·) is the

real part of a complex number and t¯i is the conjugate of ti ).

Analogy [12] imposes analogical properties on embeddings.

Each r is represented as a normal matrix Wr and for any r , r  R, Wr and Wr are constrained to statisfy commutativity property. The score function is defined as (h, r , t) = h Wr t.
ProjE [14] projects the embeddings WE of the entities onto

the vector f (h  r), calculating the scores of all candidate triples

756

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

collectively. Here, f is a non-linearity and hr = Dh h+Dr r+bc (Dh , Dr and bc are parameters). The score vector d(h, r) is defined as l WE f (h  r) + bp , where l is the sigmoid or soft-max function
and bp is a bias term. (h, r , t) is the entry in d(h, r) with index t. To date, ComplEx, Analogy and ProjE are among the state-of-
the-art KB embedding models. Other competitive models, including
PTransE [10] and ConvE [6] are not introduced due to page limit.

4 MAX-K CRITERION AND METRICS
4.1 From Top-K To Max-K
We now argue that the top-K criterion has severe limitations for practical link prediction tasks.
To ease notation, we may often re-write the pair (h, r ) as x and the triple (h, r , t) as (x, y). That is, x  X := [N ] × R, and y  [N ]. For each x  X, let Y0(x) be the set of entities y forming a factual triple (x, y) in G0. Similarly, let Y(x) (and resp. Y (x)) be the set of entities y forming a factual triple (x, y) in G (and resp. in G ). Obviously, Y(x)  Y (x)  Y0(x). We call the size of Y0(x), i.e., |Y0(x)|, the answer multiplicity of the link prediction task (x, ?).
For practical KBs, it can be reasoned that the answer multiplicity varies significantly across different link prediction tasks. Note that although the distribution of answer multiplicities in the ground truth G0 can not be directly observed, extrapolating from the observed data G allows us to estimate this distribution1. The statistics of several popular datasets given in Table 1 demonstrate the high variability of the answer multiplicities2.
Table 1: Statistics of Answer Multiplicity {|Y(x)| : (x, y)  G}

Dataset

min max mean stddev sum

FB15K [4]

1 3,963 3.68 24.11 1,066,284

FB15K-237 [17] 1 3,962 3.73 24.22 579,300

WN18 [4]

1 486 1.66 5.17 292,884

WN18RR [7] 1 486 1.69 4.73 179,738

YAGO3-10 [13] 1 61,329 5.52 101.35 2,168,080

The high variability of the answer multiplicities challenges the choice of a cutoff rank in the top-k criterion. If a large cutoff is selected, significantly many false positive answers will be produced for the low-multiplicity prediction tasks. If a small cutoff is selected, the answers for the high-multiplicity tasks will suffer from a poor recall. Mostly, the cutoff rank k is chosen customarily as 10 (thereby giving rise to the HIT@10 and fHIT@10 metrics). However, for example, in the testing data of FB15K, we observe that there exist approximately 21% of the link prediction tasks having answer multiplicities higher than 10. Besides nearly 39% of the link prediction tasks possess answer multiplicities lower than 3. This, in practice, would make the cutoff rank 10 a poor choice for both precision and recall. In fact, it could be shown that any particular global choice of the cutoff rank would face a similar dilemma, using which one either suffers from a poor recall, or from a poor precision, or from both. This is because for such high variability in answer multiplicities, simultaneously achieving good precision and recall necessarily

1If we assume that observed data G is obtained by randomly sampling the groundtruth data G0, then it can be shown the answer multiplicity distribution in G0 has the
same shape as the distribution of | Y(x ) | but has magnified mean and variance. 2In Table 1,"stddev" stands for standard deviation.

requires the cutoff rank value to be individualized for each link prediction task.
In general, one expects that such a problem exists in wide classes of query-answering and multi-label classification problems. From a practical point view, the querier (i.e, the person who demands the answers), not knowing the answers, may have in mind the maximum number of answers that he would get. For example, in question like "which books are in the Harry Potter Series?", he may expect up to 10 answers (rather than, for example, 50); in tasks like "which countries are in Europe?", he may expect up to 80 answers (rather than 10, for example). That is, the querier may specify a value k as the maximum number of answers demanded, and the answering/prediction engine generates k answers according to the -score of the answers, where k is maximally k. Such a criterion we call the max-k criterion.
Under the max-k criterion, we note that it is desirable for the answering engine to automatically select the reliable answers that provide a good balance between precision and recall. However, having no ground-truth answers available, using what protocol to generate answers based on -scores becomes a central problem.

4.2 Max-K Metrics

Under the max-k criterion, natural metrics can be constructed to
evaluate the performance of a link-prediction model. Let K := {x : (x, y)  G }, i.e., K is the set of all "keys" extracted
from the testing triples in G . That is, each key x  K defines a single link prediction task (x, ?). Recall that the rank-based metrics for the top-k criterion evaluate the prediction performance for each testing triple. Under the max-k criterion, it is more natural to evaluate the performance for each task (x, ?).
Let S(x; k) be the set of distinct entities in [N ], under the max-k criterion, returned by a prediction algorithm for task (x, ?). The
conventional notions of precision, recall and F1 directly apply. For a task (x, ?), the raw precision P@k is the fraction of entities in
S(x; k) that are contained in Y(x)  Y (x), and the filtered precision fP@k is the fraction of entities in S(x; k) that are contained in Y (x). The raw recall R@k is the fraction of entities in Y(x)  Y (x) that are contained in S(x; k), and the filtered recall fR@k is the fraction of entities in Y (x) that are contained in S(x; k).
Correspondingly, the raw F1 F1@k and filtered F1 fF1@k for task (x, ?) are respectively defined as

F1@k

:=

2 · P@k · R@k P@k + R@k

fF1@k

:=

2 · fP@k · fR@k fP@k + fR@k

Over the entire set of prediction tasks specified by K, each of these measures can then be averaged. Among these metrics, it is well-known that the F1 metrics can be viewed as an overall score integrating both precision and recall. Note that all these measures are equally applicable to the top-k criterion, in which case S(x; k) is simply taken as the set of the highest ranked k answers.
To compare the max-k and top-k criteria, imagine an oracle who knows the correct answers Y(x)Y (x) and needs to select answers for the max-k and top-k criteria respectively. For max-k, he would select precisely all answers in Y(x)  Y (x) if |Y(x)  Y (x)|  k

757

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

and select k answers in Y(x)Y (x) if |Y(x)Y (x)| > k. For top-k, he would select k answers in Y(x)Y (x) if |Y(x)Y (x)|  k, and if |Y(x)  Y (x)| < k, he would select all answers in Y(x)  Y (x) together with k - |Y(x)  Y (x)| additional answers in order to make up k answers. We then have the following results.
Lemma 1. The raw precision, recall and F1 performances of the oracle under the top-k and max-k criteria for task (x, ?) are as follows.

P@ktop

=

min

|Y (x )

 k

Y

(x)| ,

1

R@ktop

=

min

|Y (x )

k 

Y

(x)| ,

1

F1@ktop

=

min

|Y (x )



2k Y

(x )|

+

k

,

2|Y(x)  |Y(x)  Y

Y (x)| (x)| + k

P@km ax = 1

R@km ax

=

min

|Y (x )

k 

Y

(x)| ,

1

F1@km ax

=

min

|Y (x )



2k Y

(x )|

+

k

,

1

Note that the filtered metrics can be obtained similarly. From these results, we see that for the same k, the oracle's precision, recall and F1 performances for max-k are all at least as good as those for top-k. This should justify the use of max-k criterion.

5 PREDICTION PROTOCOLS FOR MAX-K
Given the -score of each entity, we now consider the problem of designing appropriate protocols for selecting up to k answers.
First note that simply implementing the top-k criterion can be regarded as a trivial protocol that also satisfies the max-k criterion. We call this protocol the TopK protocol, which is defined below.
TopK Protocol For each prediction task (x, ?), return k answers having the highest -scores to form the answer set S(x; k).
Introducing this protocol merely serves to evaluate the top-k approach under the above defined max-k metrics. This enables us to compare top-k and max-k in the same framework in experiments.
Now we turn to designing protocols that arrive at a good compromise between precision and recall, or to maximize the F1 metrics if possible. Such an optimization problem appears difficult both analytically and numerically. Nevertheless we will present two protocols. The first is a sampling protocol, motivated by an intuitive understanding and supported by a theoretical justification. The second is a deterministic greedy protocol, developed based on an asymptotic analysis of the sampling protocol. These protocols are capable of automatically dealing with diverse answer multiplicities and generating a list of answers of varying lengths. Not only theoretically justified, these protocols are shown in a later section to result in significantly improved F1 values over the TopK protocol.

5.1 Sampling Protocol
We first consider the case in which the -score function (x, y) is a conditional distribution pY |X (y|x), indicating the probability that an entity y is a correct answer for task (x, ?).

0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

10

20

30

40

50

0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

10

20

30

40

50

Figure 1: Two example answer predicting distributions.

To fix ideas, consider the following toy example. Suppose that N = 50 and the querier specifies k = 20 in the max-k criterion. Consider the following two cases.
· Suppose that the obtained -score is the predictive distribution in Figure 1 (left). In this case, the entities 1 to 10 have dominating probabilities. Then we desire that these 10 entities are selected as the answers.
· Suppose that the obtained -score is the predictive distribution in Figure 1 (right). In this case, none of probability values dominate but entities 1 to 40 each have significantly larger probability than the remaining entities. Then we desire that the selected answers are 20 entities from the first 40 entities.
Desiring such a behaviour, we propose the following sampling protocol for max-k criterion.
Sampling Protocol For each prediction task (x, ?), draw k answers i.i.d. from the predictive distribution pY |X (·|x) given by the model, and use the distinct drawn entities to form the answer set S(x; k).
Obviously how well such a protocol performs not only depends on the protocol per se, but also depends on the predictive distribution used. To assess the quality of this protocol, we consider an oracle setting in which the effect of the predictive distribution's incorrectness is removed from our analysis. Let pY |X (·|x) be the oracle distribution, namely, it puts equal non-zero probability on each entity in the correct answers Y(x)Y (x) and puts zero probability on other entities. Then the following result can be proved.
Theorem 1. Using the oracle distribution pY |X (·|x) as the predictive distribution in the sampling protocol, the expected precision, recall, and F1 values all achieve those given in Lemma 1 for the max-k criterion.

Similar results concerning the filtered metrics may also be obtained. At this point, we conclude that the sampling protocol for the max-k criterion is at least optimal in an average sense.

5.1.1 From Score Functions to Probability Distributions. Many link-prediction models are not probabilistic. That is, their -score functions do not have probabilistic interpretations. Hence, the sampling protocol does not apply directly to these models. We now present a simple transformation, which converts the -score function to the desired predictive distribution pY |X as follows.

pY |X (y|x) :=

exp (x, y) exp (x, y

),

y [N ]

758

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

where  is a positive parameter one may tune to shape the distribution. Noting that this transformation merely exploits the widely used soft-max function, we call it the soft-max transformation. It is well-known that this transformation is monotonic in y. Namely, for the same x, it preserves the rank of the -score for each entity y.
Although there can be other ways to generate probability values from scores, the soft-max transformation appears the most natural and universal. Indeed in most models that output a probability (mass) distribution, this transformation has been used.
Having this transformation ready, from here on, we will restrict our discussion to the setting in which the predictive model outputs a distribution pY |X (·|x) for each task (x, ?).

5.2 Asymptotic Analysis of Sampling Protocol
We now show an asymptotic property of the sampling protocol. This property will be the foundation to establish the next protocol.
In the max-k criterion, we will now consider the setting where k is asymptotically large. In this setting, we will investigate how many answers will be generated by the sampling protocol and what these answers are, using a given predictive distribution pY |X (·|x).

Theorem 2. Suppose that pY |X (·|x) is such that pY |X (1|x) 
pY |X (2|x)  . . .  pY |X (N |x). Let k be the integer such that pY |X (k |x) 
1/k and pY |X (k + 1|x) < 1/k. If no such k exists, let k = 0. Then the size of the answer set S(x; k) obtained by sampling pY |X (·|x) satisfies

|S(x; k)| -p k 1 - k pY |X (i |x) + k, as k - ,

(1)

i =1

p
where - denotes convergence in probability, and the summation term is taken as 0 when k = 0.

The proof of the theorem is somewhat technical, but since it
contains additional insight, we sketch its key ideas.
Proof Sketch: By the Weak Law of Large Number (WLLN), in the large k limit, with probability approaching 1 that the sequence of k entities drawn from pY |X (·|x) contains pY |X (1|x)k "1"s, pY |X (2|x)k "2"s, . . . , pY |X (N |x)k "N "s. That is, as long as

pY |X (i |x)k  1,

(2)

the sequence contains entity i with probability approaching 1. The
largest i satisfying Equation 2 can be shown to be k precisely. Consider the case in which no i  [N ] satisfies Equation (2), i.e.,
k = 0. In this case, again by WLLN, the probability that the sequence contains two repeated entities approaches 0. Thus, the probability that sequence contains precisely k distinct entities approaches 1,
p
namely, |S(x; k)| - k and the theorem is proved for this case. Now suppose k > 0. Then entities 1, 2, . . . , k are contained in
the drawn sequence with probability approaching 1. Thus S(x; k) includes these entities almost surely. Remove entities 1, 2, . . . , k from this sequence. The remaining sequence is shortened and has
k
length arbitrarily close to k 1 - pY |X (i |x) . By an argument
i =1
similar to that used in the first case, with vanishing probability that the shortened sequence contains two repeated entities. Thus

k
the shortened sequence contains k 1 - pY |X (i |x) distinct en-
i =1
tities with probability approaching 1. As such, with probability approaching 1, S(x; k) contains precisely entities 1, 2, . . . , k and
k
k 1 - pY |X (i |x) other distinct entities. Hence the theorem is
i =1
also proved for this case.

5.3 Greedy Protocol
The proof in Theorem 2 motivates another prediction protocol. We will assume that the condition pY |X (1|x)  pY |X (2|x) 
. . .  pY |X (N |x) is satisfied. Note that making this condition satisfied only requires sorting the probability values pY |X (y|x) for all y  [N ] and re-label the entities according their sorted order. The greedy protocol is then given as follows.
Greedy Protocol Find the value of k in Theorem 2; compute q as the nearest int-
k
eger to k 1 - pY |X (i |x) ; return S(x; k) as {1, 2, . . . , k + q}.
i =1
From the proof of Theorem 2, for asymptotically large k, the greedy and sampling protocol are equivalent. Since the sampling protocol is optimal in an average sense, similar optimality statement can be made about the greedy protocol for large k. With small or modest k, the greedy protocol has the advantage of providing deterministic results where the answers provided by the sampling protocol are subject to uncertainty and statistical irregularities.
Besides, the greedy protocol in the max-k setting may be viewed as implementing a "top-k" criterion with a self-determined k value, k + q. The protocol is guaranteed to select k + q entities with the highest probabilities. Though the sampling protocol implicitly aims the same, its probabilistic nature gives no guarantee in this regard.

6 EXPERIMENT
We perform extensive experiments to study the max-k criterion and the proposed protocols for various link prediction models and over a range of datasets. Due to limited space, we only select a small fraction of our results to illustrate the key findings.

Table 2: Statistics of Datasets

Dataset FB15K FB15K-237 WN18 WN18RR YAGO3-10

N 14,951 14,541 40,943 40,943 123,182

|R| 1,345 237
18 11 37

#train 483,142 272,115 141,442 86,835 1,079,040

#valid 50,000 17,535 5000 3,034 5,000

#test 59,071 20,466 5000 3,134 5,000

6.1 Experiment Setup
6.1.1 Datasets. The datasets used in our experiments include FB15K [4], WN18 [4], FB15K-237 [17], WN18RR [7], and YAGO310 [13]. These datasets are popular in KB link-prediction research. The FB15K dataset is filtered from Freebase, which contains general facts. The WN18 dataset is a subset of WordNet, containing the lexical relational data between synsets. Recently, it has been observed that these two datasets are populated with reciprocal

759

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Selected  value for each model on each dataset

Model FB15K WN18 FB15K-237 WN18RR YAGO3-10

TransE 10 10

10

20

5

ComplEx 2

2

5

5

2

Analogy 2

2

5

5

2

triples3 [9], providing optimistic artifacts for link prediction. The FB15K-237 [17] and WN18RR [7] datasets are subsets of FB15K and WN18 respectively. They are obtained by further filtering FB15K and WN18 and removing reciprocal triples. YAGO3-10 is filtered from YAGO3, which extends YAGO using multilingual information in Wikipedia. The statistics of these datasets are listed in Table 2.
6.1.2 Models. The prediction models used in the experiments are TransE [4], ComplEx [18], Analogy [12], and ProjE [14]. Among these models, only projE is a probabilistic model, which outputs a predictive distribution for each link prediction task. For each model, we use the code released in the public domain. Specifically, we use the code released in [10] for TransE, the code released in [12] for ComplEx and Analogy, and the code released in [14] for ProjE4. Default hyper-parameter settings of the code are used.

6.2 Results
6.2.1 Converting Scores to Distributions. Since the scoring functions in TransE, ComplEx and Analogy are not distributions, to evaluate their behaviour under the proposed protocols for max-k, we apply the soft-max transformation to the score functions. Tuning the scaling factor parameter  in the soft-max transformation allows the resulting distribution to be shaped with different contrast and is expected to impact the performance of the prediction algorithm.
Using the sampling protocol over FB15K-237 as an example, Figure 2 shows the filtered precision, recall, and F1 performances for these four models with -settings in {0.2, 0.5, 1, 2, 5, 10, 20, 50}. Note that under the sampling protocol, precision is independent of the k value for max-k, equal to the sum of the predictive probability values of the correct answers. Thus precision is only plotted against changing  (top row). As expected, increasing  monotonically increases precision. This is because larger  leads to higher contrast in the resulting distribution, shaping the distribution farther away from the uniform distribution. Such higher contrast shrinks the set S(x; k) of selected answers to only contain highly confident ones, thereby increasing precision. The recall performance however exhibits a complex behaviour with respect to  (middle row). At very low , the resulting distribution is nearly uniform; sampling from such a distribution puts little emphasis on the correct answers, thereby resulting in very low recall. As  increases, the distribution is gradually shaped to give good contrast, enabling it to distinguish the correct answers from the wrong ones. This makes the output answers include more correct answers and results in higher recall. But as  keeps increasing and moves away from this regime, the contrast in the distribution becomes too high. And the probabilities
3A pair of triples (h, r, t ) and (t, r , h) are referred to as a reciprocal pair, if r and r indicate the same relation but with object and subject swapped. For example, (John, isParentOf, Mary) and (Mary, isChildOf, John) are a reciprocal pair. 4An error in the code of ProjE was noted by its author after the publication of the paper, and we have fixed the error.

of only very few (or even one) entities are very high, and all other probability values are suppressed, including those for other correct answers. As such, recall drops. This recall-vs- behaviour, first rising and then dropping, also depends on the value k as well as the correctness of the predictive distribution, i.e., the model. Note that the order of the colour-coded recall curves for TransE is quite different from that for ComplEx and that for Analogy.
The combined effect of precision and recall then also gives a nonmonotonic behaviour of F1 performance with respect to  (bottom row). It is interesting to note that for each of the examined models and for each choice of  value, the F1 curve starts to flatten after k = 10. This can be explained by the fact that F1 is the harmonic mean of precision and recall: since precision stays constant with respect to k, the increase of recall necessarily saturates the F1 value.
6.2.2 Selection of . The remainder of the results are reported for fixed choices of . For each dataset and each model that is nonprobabilistic, we pick the  value in the examined choices given above5 that maximizes fF1@10 in the above experiments. Though such a choice of  may seem arbitrary, it reflects our interest in a balanced measure of performance (hence the F1 measure rather than precision or recall measure) and the predictability of the models (hence a filtered metric rather than a raw metric). Besides, picking 10 as the value of k for optimizing fF1 is justified by the bottom-row plots of Figure 2, where the fF1 curves start to flatten at around k = 10, and further increasing k only increases fF1 marginally. This is also supported by the conventional top-k metrics, where k = 10 is the most popular (e.g. in HIT@10).
The selected  values are given in Table 3. It can be seen that the "best choices" of  values for ComplEx and Analogy are the same. This correlates with the model similarity between ComplEx and Analogy, and is also supported by the similar behaviours of ComplEx and Analogy in Figure 2.
6.2.3 Comparing Models Under Top-K Metrics and Max-K Metrics . The four models are compared under the conventional metrics (top-k hit, mean rank, and mean reciprocal rank) and the max-k metrics (precision, recall and F1). Under the max-k criterion, the models are evaluated under both the sampling protocol and the greedy protocol. Due to page limit, only the filtered metrics are reported (Table 4), with the winning values shown in bold.
It can be seen that the max-k metrics under the sampling protocol behave in a trend very similar to those under the greedy protocol. This is expected since the greedy protocol is derived as a limiting case of the sampling protocol, under certain asymptotic assumption. However, the conventional metrics behave quite differently from the max-k metrics. Among the three examined conventional metrics, it appears that only the fMRR metric follows a trend consistent with the precision metrics under max-k criterion. But there isn't sufficient data to support a strong correlation between the two.
We believe that much of the distinction between the top-k and the max-k metrics (for example, between fHIT@10 and fR@10) lies in the difference between the definitions of the max-k and top-k criteria: top-k demands strictly k answers, whereas max-k allows any number of answers, up to k. As we will show in later
5From precision results presented in Figure 2, one can see that the choices of  values mark the important transitions of the performance curves.

760

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

fP

fR@k

fF1@k

0.12

TransE

0.12

ComplEx

0.12

Analogy

0.10

0.10

0.10

0.08

0.08

0.08

0.06

fP

0.06

fP

0.06

0.04

0.04

0.04

0.02

0.02

0.02

0.000

10

20

30

40

50

0.000

10

20

30

40

50

0.000

10

20

30

40

50

0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.000
0.12

= 0.2 = 0.5 =1 =2 =5 = 10 = 20 = 50

TransE

20

40

60

k

TransE

80

100

fR@k

0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.000
0.12

= 0.2 = 0.5 =1 =2 =5 = 10 = 20 = 50

ComplEx

20

40

60

k

ComplEx

80

100

fR@k

0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.000
0.12

= 0.2 = 0.5 =1 =2 =5 = 10 = 20 = 50

Analogy

20

40

60

k

Analogy

80

100

0.10

0.10

0.10

0.08 0.06 0.04 0.02 0.00
0

= 0.2 = 0.5 =1 =2 =5 = 10 = 20 = 50

20

40

60

80

100

k

fF1@k

0.08 0.06 0.04 0.02 0.00
0

= 0.2 = 0.5 =1 =2 =5 = 10 = 20 = 50

20

40

60

80

100

k

fF1@k

0.08 0.06 0.04 0.02 0.00
0

= 0.2 = 0.5 =1 =2 =5 = 10 = 20 = 50

20

40

60

80

100

k

Figure 2: Impact of  on model performance (FB15K-237, sampling protocol)

Table 4: Comparing models under conventional and Max-K metrics

Dataset FB15K WN18 FB15K-237 WN18RR YAGO3-10

Models
TransE ComplEx Analogy
ProjE TransE ComplEx Analogy ProjE TransE ComplEx Analogy ProjE TransE ComplEx Analogy ProjE TransE ComplEx Analogy ProjE

Conventional Metrics
fHIT@10 fRNK fMRR 73.46% 74 0.456 83.46% 119 0.716 83.89% 114 0.723 75.35% 78 0.588 94.55% 479 0.584 94.50% 737 0.942 94.57% 717 0.942 94.64% 298 0.820 40.50% 327 0.219 36.03% 525 0.206 36.90% 521 0.211 42.46% 231 0.249 43.44% 5805 0.191 41.48% 8358 0.390 41.29% 8252 0.391 47.69% 3675 0.367 30.32% 1904 0.151 43.65% 2920 0.266 43.42% 2506 0.257 64.20% 1027 0.470

Max-K Metrics (Sampling)
fP fR@10 fF1@10 17.11% 26.81% 0.187 24.88% 37.68% 0.272 25.23% 38.01% 0.275 19.27% 46.95% 0.243 19.56% 36.78% 0.226 61.56% 83.49% 0.660 61.42% 83.53% 0.659 40.37% 70.01% 0.468 8.74% 15.16% 0.098 11.51% 13.35% 0.118 11.43% 13.54% 0.118 11.34% 24.45% 0.135 1.62% 2.27% 0.018 22.40% 33.73% 0.247 22.45% 34.08% 0.249 14.67% 33.51% 0.183 2.00% 9.87% 0.030 1.60% 8.76% 0.025 1.66% 8.83% 0.025 6.23% 22.38% 0.083

Max-K Metrics (Greedy)
fP fR@10 fF1@10 16.53% 28.75% 0.195 24.14% 39.68% 0.278 24.51% 40.01% 0.281 16.95% 63.30% 0.239 22.32% 40.08% 0.273 61.90% 87.77% 0.676 61.78% 87.89% 0.675 31.71% 76.96% 0.417 8.31% 17.02% 0.102 11.18% 13.70% 0.118 11.20% 13.84% 0.119 10.27% 31.20% 0.136 1.53% 2.43% 0.018 22.23% 36.02% 0.257 22.36% 36.32% 0.259 13.34% 40.81% 0.190 2.36% 14.79% 0.040 1.93% 10.08% 0.031 2.12% 11.45% 0.034 6.99% 31.96% 0.102

761

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

P@k

P@k

TransE 1.0

TransE 1.0

TransE 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

R@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

ComplEx 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

ComplEx 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

ComplEx 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

R@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

Analogy 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

Analogy 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

Analogy 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

R@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

ProjE 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

ProjE 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

ProjE 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

R@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

Figure 3: Comparing various protocols and theoretical limits on FB15K-237

P@k

P@k

experiments, they induce very different behaviour for the same predictive distribution. Besides, note that the conventional top-k metrics are triple-based, namely, the basic performance measure evaluates how likely a given triple (x, y) is factual, and such a measure is then averaged over all testing triples. By contrast the max-k metrics are key-based or task-based, namely, the basic performance measures the quality of the answers provided for a prediction task (x, ?), then averaged over all tasks. This should also cause some dataset-dependent difference.
Nonetheless, these results suggest that a model that performs well under the top-k metrics does not necessarily perform well under the max-k metrics. Take ProjE for example. Though it appears as the best model on YAGO3-10 under both top-k metrics and max-k metrics, these metrics give contradicting indications on WN18.

6.2.4 Comparing Sampling, Greedy, TopK Protocols and Theoret-
ical Limits. We now study the performance of various models under the max-k criterion. Specifically, we evaluate the performance of
a given model under the sampling, greedy and TopK protocols.
Additionally, we evaluate two notions of theoretical limit. One notion is the oracle's max-k performance, which includes the precision, recall, and F1 of the oracle under the max-k criterion, i.e., P@km ax, R@km ax and F1@km ax given in Lemma 1; these quantities are labeled by MaxK_Oracle in the plots. The other notion is the oracle's top-k performance, which includes the precision, recall, and F1 of the oracle under the top-k criterion, i.e., P@ktop, R@ktop and F1@ktop given in Lemma 1; these quantities are labeled by TopK_Oracle in the plots. Only raw metrics are reported.
Figure 3 contains the performance of TransE, ComplEx, Anal-
ogy and ProjE under the two protocols over FB15K-237, together
with the MaxK_Oracle and TopK_Oracle limits. Overall, for each

762

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

F1@k

F1@k

On FB15K, Performance of ComplEx 1.0

On FB15K, Performance of Analogy 1.0

On FB15K, Performance of ProjE 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

F1@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

On WN18, Performance of ComplEx 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

On WN18, Performance of Analogy 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

On WN18, Performance of ProjE 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

F1@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

On WN18RR, Performance of ComplEx 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

On WN18RR, Performance of Analogy 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.00

20

40

60

80

100

k

On WN18RR, Performance of ProjE 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

F1@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

On YAGO3-10, Performance of ComplEx 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

On YAGO3-10, Performance of Analogy 1.0

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

On YAGO3-10, Performance of ProjE 1.0

0.8

0.8

0.8

0.6

0.6

0.6

F1@k

F1@k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

0.4

Sampling

Greedy

0.2

TopK

TopK Oracle

MaxK Oracle

0.0

0

20

40

60

80

100

k

Figure 4: Comparing F1@k performance and theoretical limits on other datasets

F1@k

F1@k

model, the performances of the sampling protocol and those of the greedy protocol are close to each other. In some cases (e.g., for Analogy and ComplEx), their difference is negligible, confirming that the greedy protocol is a reasonable approximation of the sampling protocol. In other cases, the differences between the two protocols are more visible (as for ProjE and TransE), we believe that this is due to a compound effect of the correct answer's distribution and the model's predictive distribution; when the two distributions do not match well enough and/or when the contrast of the model's predictive distribution isn't significant, such a phenomenon will occur. To verify this, we compute two quantities for each model. The first is the total variational distance (TVD) between the model's predictive distribution and the uniform distribution over the set of all entities, averaged over all keys. The second is the total probability mass (TPM) of the correct answers under the model's predictive

distribution, again averaged over all keys. The TVD quantity can be seen as a measure of the predictive distribution's contrastiveness, and the TPM quantity can be regarded as a measure of the matching between the model predictive distribution and the correct answer's distribution. The results (see table below) confirm our conjecture.
TransE ComplEx Analogy ProjE TVD 0.9957 0.9993 0.9992 0.9561 TMP 0.5971 0.7608 0.7596 0.5137 Comparing the F1 curves of MaxK_Oracle and TopK_Oracle in the plots in Figure 3 reveals that the max-k limit is significantly higher than the top-k limit. With increasing k, the TopK_Oracle curve decreases after a small value of k, whereas the MaxK_Oracle curve continuously increases. This is due to the great advantage of the max-k protocol in precision, which dominates its slight disadvantage in recall. If one believes in the F1 score as a balanced

763

Session 6C: Knowledge Bases/Graphs

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

measure of performance, this should convincingly indicate that the top-k criterion is fundamentally inferior to the max-k criterion.
This is further comfirmed by comparing the F1 score of the sampling and greedy protocols against that of the TopK_Oracle limit. The F1 performances of all four models under these two protocols exceed TopK_Oracle, the theoretical upper limit that all possible models can ever achieve under the top-k criterion.
Besides, when applying the TopK protocol on the model's predictive distribution, we see that its F1 score is only slightly lower than the TopK_Oracle F1 curve and the gap between the two curves diminishes with k. That is, with the top-k criterion, there is little need to further improve the model, particularly at large k.
6.2.5 Comparing Models and Datasets Under The Max-K Criterion. Using a similar approach as that in Section 6.2.4, we now compare three state-of-the-art models, ComplEx, Analogy and ProjE, and assess the datasets with respect to these models.
The plots in Figure 4 are in the same form as those in Figure 3, except that we here only include the F1 metric. From the figure, one can conclude that under the F1 metric, ComplEx and Analog behave nearly identically. On FB15K, WN18 and WN18RR, they both outperform ProjE by a visible margin. On YAGO3-10, the three models have very similar performance. For each of the examined dataset in Figure 4, when comparing the model performance against the MaxK_Oracle limit, it is interesting to observe that on WN18, there is only very small gap between the MaxK_Oracle curve and the curves of these state-of-the-art models. That is, on this dataset, the performance of the current state of the art has already approached the ultimate limit. As such, we believe that on the WN18 dataset, further improvement upon these models will be very difficult, hence anticipating little progress in the coming years.
However, on FB15K, YAGO3-10 and particularly on WN18RR, the performance of the three models is still far away from the MaxK_Oracle limit. This suggests that there is still significant room for developing innovative models to further improve the performance on these datasets. One should not expect a model to achieve the MaxK_Oracle limit. To what extent a model can approach the MaxK_Oracle performance is in fact governed by the learnability bound (intrinsically dictated by the structure and size of the data), living somewhere below the MaxK_Oracle curve.
7 CONCLUDING REMARKS
Although the context of this work is link prediction in KBs, the max-k criterion and the protocols introduced in this paper in general apply widely to information retrieval, multi-label classification and many related areas. This criterion is practically motivated and at the same time theoretically sound. The proposed sampling and greedy protocols also have a theoretical foundation and a universal applicability. We anticipate that the max-k criterion and the proposed protocols find many applications beyond link prediction in KBs. Return to this context, we suggest that the conventional metrics used for link prediction be discarded, and replaced by the precision, recall and F1 metrics introduced herein.
ACKNOWLEDGMENTS
This work is supported partly by China 973 program (No.2014CB340 305, 2015CB358700), by the National Natural Science Foundation

of China (No. 61772059, 61602023, 61421003). This paper is also
supported by the State Key Laboratory of Software Development
Environment of China and Beijing Advanced Innovation Center
for Big Data and Brain Computing.
REFERENCES
[1] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. Springer.
[2] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. ACM, 1247­1250.
[3] Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic matching energy function for learning with multi-relational data. Machine Learning 94, 2 (2014), 233­259.
[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems. 2787­2795.
[5] Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embeddings of knowledge bases. In Conference on Artificial Intelligence.
[6] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2017. Convolutional 2D Knowledge Graph Embeddings. CoRR abs/1707.01476 (2017). arXiv:1707.01476 http://arxiv.org/abs/1707.01476
[7] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2017. Convolutional 2D Knowledge Graph Embeddings. CoRR abs/1707.01476 (2017). arXiv:1707.01476 http://arxiv.org/abs/1707.01476
[8] G. E. Hinton, J. L. McClelland, and D. E. Rumelhart. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1. MIT Press, Cambridge, MA, USA, Chapter Distributed Representations, 77­109. http://dl. acm.org/citation.cfm?id=104279.104287
[9] Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. 2017. Knowledge Base Completion: Baselines Strike Back. In Proceedings of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL 2017, Vancouver, Canada, August 3, 2017. 69­74. https://aclanthology.info/papers/W17- 2609/w17- 2609
[10] Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015. Modeling relation paths for representation learning of knowledge bases. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015 (2015).
[11] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of AAAI.
[12] Hanxiao Liu, Yuexin Wu, and Yiming Yang. 2017. Analogical Inference for Multirelational Embeddings. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research), Doina Precup and Yee Whye Teh (Eds.), Vol. 70. PMLR, International Convention Centre, Sydney, Australia, 2168­2178.
[13] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. 2015. YAGO3: A Knowledge Base from Multilingual Wikipedias. In CIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings. http://cidrdb.org/cidr2015/Papers/CIDR15_Paper1.pdf
[14] Baoxu Shi and Tim Weninger. 2017. ProjE: Embedding Projection for Knowledge Graph Completion.
[15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems. 926­934.
[16] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web. ACM, 697­706.
[17] Kristina Toutanova and Danqi Chen. 2015. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. 57­66.
[18] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016. 2071­2080. http://jmlr.org/proceedings/papers/ v48/trouillon16.html
[19] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence. Citeseer, 1112­1119.
[20] Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. 2016. On the Representation and Embedding of Knowledge Bases beyond Binary Relations. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016. 1300­1307. http: //www.ijcai.org/Abstract/16/188

764


Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Reducing Variance in Gradient Bandit Algorithm using Antithetic Variates Method

Sihao Yu, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng
1 University of Chinese Academy of Sciences, Beijing, China 2 CAS Key Lab of Network Data Science and Technology,
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China yusihao@software.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn

ABSTRACT
Policy gradient, which makes use of Monte Carlo method to get an unbiased estimation of the parameter gradients, has been widely used in reinforcement learning. One key issue in policy gradient is reducing the variance of the estimation. From the viewpoint of statistics, policy gradient with baseline, a successful variance reduction method for policy gradient, directly applies the control variates method, a traditional variance reduction technique used in Monte Carlo, to policy gradient. One problem with control variates method is that the quality of estimation heavily depends on the choice of the control variates. To address the issue and inspired by the antithetic variates method for variance reduction, we propose to combine the antithetic variates method with traditional policy gradient for the multi-armed bandit problem. Furthermore, we achieve a new policy gradient algorithm called Antithetic-Arm Bandit (AAB). In AAB, the gradient is estimated through coordinate ascent where at each iteration gradient of the target arm is estimated through: 1) constructing a sequence of arms which is approximately monotonic in terms of estimated gradients, 2) sampling a pair of antithetic arms over the sequence, and 3) re-estimating the target gradient based on the sampled pair. Theoretical analysis proved that AAB achieved an unbiased and variance reduced estimation. Experimental results based on a multi-armed bandit task showed that AAB can achieve state-of-the-art performances.
KEYWORDS
Policy gradient; Antithetic variates; Coordinate gradient
ACM Reference Format: Sihao Yu, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng. 2018. Reducing Variance in Gradient Bandit Algorithm using Antithetic Variates Method. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210068
 Corresponding author: Jun Xu
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210068

1 INTRODUCTION
Reinforcement learning, including the Multi-Armed Bandit(MAB) [7] and Markov Decision Process(MDP) [5], have been successfully used in variant machine learning applications recently. Among the algorithms that solve the reinforcement learning problems, policy gradient [13] has shown its advantages in e ectiveness in highdimensional/continuous action spaces, fast convergence rate, and handling stochastic policies etc. Roughly speaking, policy gradient relies upon optimizing parametrized policies (a distribution over the agent actions) with respect to the expected return (long-term cumulative reward) by gradient ascent.
To calculate the parameter gradients at each optimization iteration, policy gradient algorithms such as REINFORCE [12, 13] usually adopt the Monte Carlo method [9] to estimate the expectation of the gradient. The gradient estimated by the Monte Carlo method is unbiased but usually has large variance, which hurts the e ciency and e ectiveness of the traditional policy gradient algorithm. How to reduce the variance of the estimated gradient becomes a key issue in policy gradient algorithms.
A number of research has been conducted to reduce the gradient variance in policy gradient. For example, Policy gradient with baseline [3] is commonly used in real reinforcement learning tasks. In the method, a baseline variate, which is designed as the averaged rewards of the history steps, is rst designed. Then, the real reward of the action minus the baseline is used as the reward for the gradient estimation and parameter updating. It also shows that variance of the newly estimated gradients is reduced while its expectation is identical to that of the traditional policy gradient. More methods on variance reduction for policy gradient please referred to [1, 8, 11]
From the viewpoint of statistics, the policy gradient with baseline is a direct application of control variates method [2], a variance reduction approach in Monte Carlo Method, to improve the traditional policy gradient. The baselines are implementations of the control variates in the reinforcement learning environment. In general, designing reliable control variates is critical for the success of control variates method. Inappropriate setting of the control variates may result in the raising of variance and hurt the estimation. When applied to reinforcement learning, though the policy gradient with baseline heuristically constructs the baselines, which is far away from the ideal control variates, it is di cult to achieve its optimal e ect.
To get rid of this problem, antithetic variates method [4] is proposed. Every time antithetic variates method draws a pair of antithetic samples for the estimation. Since one antithetic sample in the pair is easily derived from another, the auxiliary functions (e.g., the control variates) is not a mandatory anymore.

885

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Based on the observation, we propose a novel policy gradient method which uses antithetic variates to improve policy gradient for MAB. The proposed method, referred to as Antithetic-Arm Bandit (AAB), estimates the parameter gradients through sampling a pair of antithetic arms at each time. To achieve this, AAB adopts the coordinate ascent framework for the optimization where each coordinate corresponds an arm. At each iteration, the arms are sorted according to their estimated gradients. After that, a pair of antithetic arms are sampled on the basis of the sorted arms. The gradient of the target arm (determined with another sampling) is then re-estimated and updated.
Theoretical analysis showed that the gradients calculated with AAB was an unbiased estimation and the variance of the estimation was e ectively reduced with high con dence.
Experiments were conducted to show the e ectiveness of the proposed AAB. The experimental results based on an MAB task showed that AAB outperformed the baseline of traditional policy gradient and achieved comparable performances with the policy gradient with baseline.

2 BACKGROUND: VARIANCE REDUCTION IN POLICY GRADIENT
This section introduces the formulation of variance reduction methods in the policy gradient for the multi-armed bandit problem.

2.1 Gradient bandit algorithm

Suppose we are facing repeatedly with a choice among k di erent

actions. After each choice, we receive a numerical reward chosen

from a stationary probability distribution that depends on the se-

lected action. Each action has an expected reward, called value. The

objective is to maximize the expectation of total reward over some

time periods. There are two targets in the game, that is, nding

the best action which has the largest value and maximizing the

accumulative reward in limited time periods.

Policy gradient aims to learn a numerical preference Ht (a) for each action a = {1, 2, · · · k }, to calculate the policy t (a) on time step t. Denote the action selected on time t as At , the corresponding reward as Rt , and the value of selecting an action a as q(a) = E[Rt |At = a]. The policy t (a distribution over the actions) is de ned as the softmax over the preferences Ht :

t (a) = Pr (At = a) =

k a

exp{Ht (a)} =1 exp{Ht (a

. )}

(1)

The policy in Equation (1) is used to play the bandit game, and

the expected reward at time step t is E[Rt ] = a t (a)q(a). In

principle,

the

gradient

of

the

expected

reward

E[Rt ] Ht (a)

is

used

to

update the preferences of the actions. However, it is di cult to

calculate E[Rt ] and its partial gradient to preference because q(a)

is unknown. Monte-Carlo method is used to estimate the gradient.

The basic idea is the system gets a sample q(a) as a reward when

action a has been issued. Thus, the gradient is estimated as:

E[Rt ] Ht (a)

=

b

t (b)q(b)(1a=b - t (a))

n

(2)

= E sample

1

n

q(xi )(1a=xi
i =1

- t (a))

= E []

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Sihao Yu, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng

where n



1, the sam=ple samples xi



t , and 

=

1 n

n i =1

q(x

i

)(1a=xi

-

t (a)). The equation above shows that  is an unbiased estimation

for the gradient of Rt (i.e., E[] =

E[Rt ] Ht (a)

).

Thus,

one

e

ective way

to the accuracy of the estimation is to reduce the variance V[].

2.2 Control variates method for policy gradient

Policy gradient with baseline is an e ective method to reduce the

variance of . In the k-armed bandit problem, it calculates the gradi-

ent

of

Ht (a)

as

1 n

ni=1[q(xi ) · (1a=xi - t (a)) - R · (1a=xi - t (a))],

where R is the averaged rewards of the history samplings.

From the viewpoint of statistics, policy gradient with baseline is

an application of control variates in policy gradient. The original

control variates method can be described as follows: considering the

estimation  of an integral of function f (x), Monte Carlo method

1
directly uses n

ni=1(i ), where i  U (0, 1) and n  1, as an

estimation of  , which is an obviously unbiased estimation.

The control variates method, on the other hand, estimates the

integral with:

1

1

1

 = f (x)dx = [f (x) - c (x)]dx + c (x)dx

0

0

1 = [f (x) - c (x)]dx + C,

0
(3)

0

where c and C = c  1 (x)dx are constants. De ne a new ran-
0
dom variable  = f ( ) - c ( ) + C, where   U (0, 1) is a random number uniformly distributed over the interval (0, 1). It is sim-
ple to calculate that E[ ] = E[f ( )], and V[ ]  V[f ( )] when Corr [f ( ), ( )]  0, thus,  could be a better unbiased estimation

with reduced variance for the integral.
In policy gradient with baseline, R ·(1a=xi -t (a)) corresponds to c (x), R·E[(1a=xi -t (a))] = 0 corresponds to C, and Corr [R(1a=xi - t (a)), q(xi )(1a=xi - t (a))]  0. Thus, policy gradient with base-
line can be considered as an application of control variates method

to policy gradient.

2.3 Antithetic Variates Method

In statistics, antithetic variates method is another approach to reduc-

ing the variance of Monte Carlo method. Still consider the problem

of estimating  =  1 f (x)dx. Antithetic variates method adopts the
0

strati ed sampling strategy [10]. The sampler chooses a xed set of

numbers 0 = 0 < 1 < · · · < n = 1 and de nes a new estimation 
n

 = (j - j-1)f [j-1 + (j - j-1)j ],

(4)

j =1

where the j  U (0, 1)(j = 1, · · · , n). It can be shown that  is an

unbiased

estimation

for



and

V( )



V(

1 n

n i =1

f

(i )).

Speci

-

cally, antithetic variates method constructs antithetic pair among

the variables f [j-1 + (j - j-1)j ] (i.e.,Co [1, 2] < 0 where j = f [j-1 + (j -j-1)j ], j = 1, 2, . . . , n) to get the smaller V( ).

3 OUR APPROACH: ANTITHETIC-ARM BANDIT
This section proposes Antithetic-Arm Bandit (AAB), a novel variance reduction policy gradient algorithm for multi-armed bandit, on the basis of antithetic variates method.

886

Short Research Papers I Reducing Variance in Gradient Bandit Algorithm using Antithetic Variates Method

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Algorithm 1 Antithetic-Armed Bandit (AAB)

Input: Action (arm) set A = {1, · · · , k }, number of iterations T,

strati ed parameter m, learning rate 

1: Ht  0

2: for t = 1 to T do

3:

t 

exp Ht (a) a exp Ht (a )

k
{Equation (1)}
a=1

4: Sample an arm d  A according to t

0

a =d

5:

a  A, p(a) 

t (a) (1-t (d ))

otherwise

6: a  A, I(a) Index of action a after descent sorting arms

according to p

7:



m i =1

t

(I(i

))+

1 2

t

(I(m

+1)){Heuristics

for

calculating

}

8: Sampling   U (0, 1)

9: 1   , 2  1 - (1 -  )

10: a1   (I,p,1), a2   (I,p,2){Algorithm 2}

11: d  q(d)(1 - t (d)) 12: 1  q(a1)(-t (a1)); 2  q(a2)(-t (a2))

13: d  t (d) d + (1 - t (d))( 1 + (1 -  ) 2) {Equation (5)} 14: Ht (d)  Ht (d) + a

15: end for

3.1 Antithetic-Arm Bandit

Suppose an k-armed bandit problem where the set of actions are A = {1, 2, · · · , k } and each action a  A represents the a-th arm of the

bandit. AAB updates the gradient of di erent arms with coordinate

ascent. Given an arm a, the estimation of gradient proposed by

Monte Carlo method for a is

n

1



=

n

q(xi )(1a=xi
i =1

-

t (a))

where xi  t , t is the current policy at time step t, and n is the

number of samplings.

Denote the random sampled arm according to current policy t
as ft ( ) where   U (0, 1). Also denote the function for calculating the gradient as G. Thus, after taking an action xi = ft ( ), the gradient of the d-th arm can be calculated as Gd (xi ) = 1d=xi - t (d). At each iteration, AAB samples three arms: the rst sampling
chooses a target action d  t and makes d to be the arm to update;

the second and the third sample two antithetic arms for calculating

the gradients for d. Speci cally, given three sampled actions, the

estimation of gradient for arm d can be calculated as:

d = t (d)Gd (d) + (1 - t (d))Gd ft  ( ) (5)
+ (1 - t (d))(1 -  )Gd ft  (1 - (1 -  ) )

where the random variable   U (0, 1),  is the permutation func-

tion that sorts the arms so that the second and the third sampled

arms are antithetic, and   (0, 1) is the parameter which is heuristi-

cally set as

m i =1

t

(I

(i))+

1 2

t

(I(m

+1))

where

m

(named

strati

ed

parameter) represents the position of stratifying.

Algorithm 1 shows the AAB process and Algorithm 2 shows the

function for sorting the arms. At time step t, given the current policy t , an action d is sampled
which corresponds the arm to update. Then the algorithm sort the actions (arms) with  . After that, a pair of antithetic actions are

Algorithm 2 Sort function 

Input: Sorted index I, solved policy p, random variable 

Output: action a

1: for a = 1 to length of p do

2: if  - p(I(a)) < 0 then

3:

return I(a)

4: end if

5: end for

sampled from A \ {d }, and the two antithetic random variables 1 and 2 are constructed. It can be shown that 1 and 2 have high probability to be antithetic, because the arms were sorted. Finally the gradient for the chosen arm d is calculated and the policy is updated. The iteration is repeated until converge.
Intuitively, AAB constructs a monotonic compound function Gd ft  which makes Co [ 1, 2] < 0 because Co [1, 2] < 0. In the next section, we show that AAB makes an unbiased estimation
of the gradient and reduces the variance of the estimated gradients.

3.2 Theoretical analysis

AAB makes an unbiased estimation of the gradient, as shown in the
following Theorem 3.1 because Gd ft ( ) is an unbiased estimation obviously when   U (0, 1).

T

3.1. t = 1, 2, · · · , and d  A = {1, 2, . . . , k}, the

expectation of d in Equation (5) satis es E[d ] = E[Gd ft ( )], where   U (0, 1).

P . Denote pij as the probability of choosing the actions i and j at the same time after action d being chosen. And we set
pij = 0 if i = d, j = d or i > j. We have

kk

E[d ] =t (d)Gd (d) + (1 - t (d))

[pi jGd (i) + (1 -  )pi jGd (j)]

i=1 j=1

kk

k

=t (d)Gd (d) + (1 - t (d)) ( pij + (1 -  )pji )Gd (i)

i=1 j=1

j =1

=t (d)Gd (d) +

t (i)Gd (i)

i A\{d }

=

t (i)Gd (i) = E[Gd ft ( )]

i {A}

AAB uses strati ed sampling as the traditional antithetic variates method do. Given   (0, 1), strati ed sampling has lower variance than the primal Monte Carlo method. In coordinate ascent, the strati ed sampling estimates the gradient of the arm d as:
d = t (d)Gd (d) + (1 - t (d))Gd ft (t (d) + 1) (6)
+ (1 - t (d))(1 -  )Gd ft (t (d) +  + (1 -  )2) where 1, 2  U (0, 1) and Co [1, 2] = 0.
Moreover, it can be shown that the estimation of AAB has a
smaller variance than strati ed sampling, as shown in Theorem 3.2.

T

3.2. t = 1, 2, · · · , and d  A = {1, 2, . . . , k}, the

variance of d in Equation (5) and d in Equation (6) satis es V[d ]  V[d ]

887

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Sihao Yu, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng

% Optimal action Variance

80 70 60 50 40 30 20 10
0

100000

200000 300000 Steps

AAB PGB AABwithBase PGBwithBase
400000 500000

Figure 1: Performance curves of di erent methods.

0.005 0.004 0.003 0.002 0.001 0.000
0

PGB AAB

100000

200000 300000 Steps

400000

500000

Figure 2: The variance of PGB and AAB.

P . Let 1 = Gd ft  ( ), 2 = (1- )Gd ft  (1-(1- ) ), 3 = Gd ft (t (d) + 1), and 4 = (1 -  )Gd ft (t (d) +  + (1 -  )2) where  , 1, 2  U (0, 1).
V[d ] = (1 - t (d))2(V[1] + V[2] + Co [1, 2]) V[d ] = (1 - t (d))2(V[3] + V[4] + Co [3, 4])  1 and 3 are I.I.D., and 2 and 4 are I.I.D.
 V[1] = V[3], V[2] = V[4]
 Co [1, 2]  0, Co [3, 4] = 0
 V[d ]  V[d ]

4 EXPERIMENTS

We conducted experiments to test the proposed AAB algorithm.

Following the practices in [6], "one real competitor" in [6] was used as our experiments. As for the reward in the k-armed bandit, the

reward distributions were set to Bernoulli and the expected rewards

of actions were set as p1

=

0.5, p2

=

0.5 -

1 10k

and pi

=

0.4, i

=

3, . . . , k. The number of arms k was set to k = 20. The parameter

m for calculating  in AAB was set as m = 4.

Figure (1) shows the performance curves of di erent methods

in terms of the ratio of choosing the optimal action. From the results, we can see that our approaches (AAB and AABwithBase1)

performed better than the baseline method of policy gradient (PGB),

and have similar performance to the method of policy gradient with

baseline (PGBwithBase). Note that PGB and PGBwithBase only

update the rst of 3 sampled arms at each iteration. From the results

we can see that 1) AAB can e ectively reduce the variance, making

it outperform PGB; 2) both AAB and policy gradient with baseline

can e ectively reduce the gradient variance, leading to similar

performances; and 3) combining AAB with policy gradient with

baseline (AABwithBase) can marginally improve the performances.

We also tested the variance curves of AAB and PGB, as shown in

Figure (2). The variances of the estimated gradient by AAB are in

general smaller than that of by PGB in all of the iterations, showing

the e ectiveness of AAB in reducing the gradient variance.

5 CONCLUSION
In this paper, we propose a novel variance reduction method for policy gradient in multi-armed bandit problem, called Antithetic-Arm Bandit (AAB). Compared with existing method of policy gradient
1Note that AAB can be combined with the policy gradient with baseline for further reducing the variance.

with baseline which can be viewed as a control variates method
in statistics, AAB resorts to the antithetic variants method for the
task. Algorithms were proposed to conduct the estimation and the
theoretical analysis showed that the gradients estimated by AAB
are unbiased and the variance is smaller than that of by the con-
ventional Monte Carlo methods. Experimental results also showed
that AAB can achieve the state-of-the-art performances.
6 ACKNOWLEDGMENTS
This work was funded by the National Key R&D Program of China
under Grants No. 2016QY02D0405, the 973 Program of China under
Grant No. 2014CB340401, the National Natural Science Foundation
of China (NSFC) under Grants No. 61773362, 61425016, 61472401,
61722211, and 20180290, and the Youth Innovation Promotion As-
sociation CAS under Grants No. 20144310, and 2016102.
REFERENCES
[1] Jonathan Baxter and Peter L Bartlett. 2001. In nite-horizon policy-gradient estimation. Journal of Arti cial Intelligence Research 15, 1 (2001), 319­350.
[2] E.C.FIELLER and H.O.HARTLEY. 1954. SAMPLING WITH CONTROL VARIABLES. Biometrika 41, 3/4 (1954), 494­501.
[3] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. 2004. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research 5, Nov (2004), 1471­1530.
[4] J. M. Hammersley and K. W. Morton. 1956. A new monte carlo technique: Antithetic variates. Mathematical Proceedings of the Cambridge Philosophical Society 52, 3 (1956), 449­475.
[5] Ronald A Howard. 1960. Dynamic programming and markov processes. (1960). [6] Zohar Karnin, Tomer Koren, and Oren Somekh. 2013. Almost optimal exploration
in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13). 1238­1246. [7] Michael N Katehakis and Arthur F Veinott Jr. 1987. The multi-armed bandit problem: decomposition and computation. Mathematics of Operations Research 12, 2 (1987), 262­268. [8] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In Advances in neural information processing systems. 1008­1014. [9] Nicholas Metropolis and Stanislaw Ulam. 1949. The monte carlo method. Journal of the American statistical association 44, 247 (1949), 335­341. [10] Jerzy Neyman. 1934. On the Two Di erent Aspects of the Representative Method: The Method of Strati ed Sampling and the Method of Purposive Selection. Journal of the Royal Statistical Society 97, 4 (1934), 558­625. [11] R. S Sutton. 1999. Policy Gradient Methods for Reinforcement Learning with Function Approximation. Submitted to Advances in Neural Information Processing Systems 12 (1999), 1057­1063. [12] R. J. Williams. 1988. Towards a theory of reinforcement-learning connectionist systems. Issues in Education 4, 1 (1988), 1­94. [13] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229­256.

888


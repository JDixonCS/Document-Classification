Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

SAAN: A Sentiment-Aware Attention Network for Sentiment Analysis

Zeyang Lei
Graduate School at Shenzhen, Tsinghua University
leizy16@mails.tsinghua.edu.cn

Yujiu Yang
Graduate School at Shenzhen, Tsinghua University
yang.yujiu@sz.tsinghua.edu.cn

Min Yang
Shenzhen Institutes of Advanced Technology,
Chinese Academy of Sciences min.yang1129@gmail.com

ABSTRACT
Analyzing public opinions towards products, services and social events is an important but challenging task. Despite the remarkable successes of deep neural networks in sentiment analysis, these approaches do not make full use of the prior sentiment knowledge (e.g., sentiment lexicon, negation words, intensity words). In this paper, we propose a SentimentAware Attention Network (SAAN) to boost the performance of sentiment analysis, which adopts a three-step strategy to learn the sentiment-specific sentence representation. First, we employ a word-level mutual attention mechanism to model word-level correlation. Next, a phrase-level convolutional attention is designed to obtain phrase-level correlation. Finally, a sentence-level multi-head attention mechanism is proposed to capture various sentimental information from different subspaces. The experiments on Movie Review (MR) and Stanford Sentiment Treebank (SST) show that our model consistently outperform the previous methods for sentiment analysis.
KEYWORDS
Sentiment knowledge; mutual attention; multi-source attention
ACM Reference Format: Zeyang Lei, Yujiu Yang, and Min Yang. 2018. SAAN: A SentimentAware Attention Network for Sentiment Analysis. In SIGIR '18: 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8-12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3209978.3210128
1 INTRODUCTION
Sentiment classification of natural language texts is an active research field, which attempts to automatically determine the subjective statement of a given text. It is recently a widely used way in the repertoire of social media analysis carried
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210128

out by companies, marketers, and political analysts. Sentiment classification is typically treated as a text classification problem. Support Vector Machine (SVM) [7], Convolutional Neural Network (CNN) [4], Long Short-Term Memory (LSTM) [2], and Tree-LSTM [11] are representative supervised machine learning methods for sentiment classification.
Although significant progresses have been achieved by deep neural networks, most approaches (e.g., CNN, LSTM) do not make full use of domain-specific sentiment linguistic knowledge (i.e., sentiment lexicon, negation words, intensity words). Recently there have been few studies attempting to alleviate the problem [5, 8, 9], but the implementation of the networks is insufficient and classification performance is unsatisfactory.
In this study, all the aforementioned limitations are considered and alleviated to some extend. Our method SAAN, a Sentiment-Aware Attention Network, incorporates many kinds of sentiment resources as a whole into convolutional neural networks to improve the final sentiment classification performance. Specifically, our model utilizes sentiment resources including sentiment words, negation words, intensity words as a whole attention source to attend to the context, which enhances the performance of convolutional neural networks. Meanwhile, to learn better sentiment-specific sentence semantics, we adopt a three-step strategy including the word-level correlation modeling, the phrase-level correlation modeling and the sentence-level semantic modeling to exploit the semantic compositionality of the sentence and capture more comprehensive information of the sentence. Concretely, inspired by [5], we first employ a mutual attention mechanism to capture the word-level correlation between context words and sentiment words within the sentence. The mutual attention mechanism helps highlight the sentimentrelevant context words and provides guidance to learn the representation of sentiment resource words by leveraging the contextual global information interactively. Second, a dynamic convolutional attention network is adopted to extract the important and distinguishable n-gram features that form the phrase-level correlation between context words and sentiment words. Finally, motivated by [13], we propose a sentence-level multi-head attention mechanism to capture different sentimental information from different subspaces, which helps to learn more comprehensive sentiment-specific sentence representation.
The main contributions of this study can be summarized as follows.

1197

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

 We incorporate many kinds of sentiment linguistic knowledge as a whole into convolutional neural networks to learning the sentiment-specific sentence representation.
 Motivated by multi-head attention, we design a multihead attentional convolution neural network to model the sentence semantics from different semantic subspaces and capture more comprehensive sentimentspecific information.

2 THE PROPOSED MODEL
SAAN is composed of three key components: the word-level correlation modeling (§2.1), the phrase-level correlation modeling (§2.2) and sentence-level semantic modeling (§2.3). The overall framework of SAAN is shown in Figure 1.

2.1 Word-Level Correlation Modeling

Mathematically, we first assume that the embedding of a sentence  containing  context words is =[1, 2, ... , ..., ] and the embedding of corresponding sentiment resource words is  = [1, 2, ...,  , ..., ], where each  and  denotes a specific word embedding. To model the word-level correla-
tion between the context words and the sentiment resource
words, we adopt the dot product between them to define the
correlation matrix  as follow:



=

()

·





×
R

(1)

where each element , refers to the correlation between the -th word  in the context and -th word  in the sentiment
resource words.

Next, we average the values of each row of , resulting in

a column vector of size . This column vector is input into a

softmax layer to produce an attention vector for the context:



=



(

 =1

[:,]



)

Similarly,

we

can

get

a

row

vector

by averaging the values of each column of  and apply it as

the input of a softmax layer to produce an attention vector

for

the

sentiment

resource

words:



=



(

 =1

[,:]



).

Then we take the context representation matrix  and the

attention weight vector  as inputs and can obtain the overall

correlation

vector

v

for

the

context:

v

=


=1

x .

Simi-

larly, The overall correlation vector v can be obtained for

the

sentiment

resource

words

as

follow:

v

=


=1

x .v

and v denote the overall relevancy metric for the context

words and the sentiment resource words, respectively. Finally, given the context embedding matrix  and the
correlation vector v, the sentiment-enhanced context representation matrix   R× can be computed as follows:

 = ~( + en  v )

(2)

{ 

>0

~() = (() - 1)   0,  > 0 (3)

where en = [1, 1, .., 1] denotes an -dimensional all-ones vector, en  v = [v; v; ...; v] denotes the Kronecker product between en and v , and ~ is the semantic mapping
function . Here, we choose exponential linear unit (ELU) [1] as ~ since it can reduce bias shift effect. In the same

way, we can obtain the contextual enhanced sentiment re-

sources

representation

matrix





×
R

as

follow:

 = ~( + em  v )

(4)

With the above transformation process, we can model the correlation between the context and the sentiment resources, which is helpful for the following phrase-level modeling.

2.2 Phrase-Level Correlation Modeling
Given  and , multi-gram convolution is performed to extract different local semantic features. The convolution operation involve a filter    R×2, which is applied to  continuous words to extract a n-gram phrase. With different window size  (i.e. -gram), we have the combined multi-gram feature maps   and   for context words and sentiment words, respectively, which are computed as:

  = 1  2 · · ·  

(5)

  = 1  2 · · ·  

(6)

 =( *   + )  R(-+1)×

(7)

 = ( *   + )  R(-+1)×,  = 1, 2, ...,  (8)

where (·) is a nonlinear activation function,  is a bias matrix, * represents the convolution operator, k means the number of filters,  is the concatenation operator,  and  are the convolution results of the context and the sentiment resource words with window size , respectively.
We then design a novel dynamic attention mechanism to choose the important and distinguishable phrases from the learned n-gram chunks by the convolution operation. Mathematically, we formulate the chunk-based attention mechanism below:

 =    {( )  ( ( ) )}

(9)

¯

=


=1

[:, ]

(10)



where  is element-wise multiplication.

2.3 Sentence-level Semantic Modeling

Inspired by [13], we design a sentence-level multi-head attention mechanism to capture various sentimental information from different sentiment resources, which helps form more complete sentiment-specific sentence representation. To be specific, the multi-head attention mechanism is implemented by using multiply sentiment resources as attention sources to attend to the context in the form of attention weight matrix. The detailed formulation is shown as follows:

 =  (~ (  +   ¯))

(11)

()



=


=1

()

(12)

o~ =  ( )

(13)

where  is the attention matrix with each row denoting a
hop of sentiment resources attention,   is an operation that will flatten matrix into vector form, , ~ are projection

1198

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Context
Sentiment Resources

...

...

Word-level Correlation Modeling

Col-wise average

correlation matrix

Row-wise average

Phrase-level Correlation Modeling

Sentence-level Semantics Modeling

Multi-gram Convolution

Chunk-based attention

Softmax Classifier

...

Figure 1: The Overall Framework of SAAN Model

parameters to be learned and o~ refers to the final enhanced sentiment-specific sentence representation.

2.4 Sentence Classifier

We feed the final sentence representation o~ to a softmax layer to predict the category label distribution:

^

=

( o~ + )


=1

(

o~

+



)

(14)

where ^ is prediction probabilities, C is the number of class-

es,  and  are weight matrix and bias to be learned, respectively. Given a corpus with N training samples (, ), the parameters of the networks are trained to minimize the

following loss function.



(^,

)

=

-





 (^ )

+

 (

2)

=1 =1



(15)

each sample is annotated as very negative, negative, neutral, positive, or very positive. The sentiment lexicon used in this paper is the combination of two popular sentiment lexicons from [8] and [3]. For negation and intensity words, we collect them manually since the number is small.
3.2 Experiment Setups
300-dimensional Glove3 vectors are utilized as the initializations of the word embeddings in SAAN. The tokens that did not appear in the pre-trained word embeddings are initialized from the uniform distribution [-0.25,0.25]. We set the initializations of bias vectors to zero, and all weight matrices are randomly sampled from orthogonal matrices. The window sizes of the multi-gram CNN are set to 1, 2, 3, 4, respectively. SAAN is trained by using the RMSprop optimization algorithm with mini-batch. Other hyperparameters include: 2 regularization 10-5, batch size 60.

where  is the ground truth label, ^ is the prediction probabilities,  denotes each parameter to be regularized,  is parameter set,  is the coefficient for 2 regularization. Here, the first term of the loss function are the cross-entropy function of the predicted and true distributions and the second term is 2 regularization .
3 EXPERIMENTS
3.1 Datasets and Sentiment Resources
We evaluate our model on Movie Review (MR)1 and Stanford Sentiment Treebank (SST) datasets2. The detailed properties of the datasets are described below. MR consists of 5,331 positive and 5,331 negative samples with each sample annotated as positive and negative, and SST contains 8545 training samples, 1101 validation samples, 2210 test samples, where

3.3 Experiment Results
In the experiments, we adopt classification accuracy as the evaluation metric, which is widely used in recent sentiment classification studies. Table 1 shows the performance comparison of SAAN with the baseline methods. From Table 1, we observe that SAAN substantially outperforms the compared methods by a noticeable margin on both MR and SST datasets. It demonstrates that our proposed model is effective to improve the performance of sentence-level sentiment classification. Concretely, LR-LSTM and LR-Bi-LSTM stably outperform RNTN, LSTM, BiLSTM, and CNN on both datasets. The advantage of LR-LSTM and LR-Bi-LSTM is that they employ linguistic role of sentiment, negation and intensity words into deep neural networks. SAAN further improves the performance of sentiment analysis by designing

1 http://www.cs.cornell.edu/people/pabo/movie- review- data/ 2 https://nlp.stanford.edu/sentiment/

3 http://nlp.stanford.edu/pro jects/glove

1199

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

attention mechanisms to integrate the knowledge into deep neural network.

Methods

MR SST(sent.-level)

RNTN [10] LSTM [2] BiLSTM Tree-LSTM [11] CNN [4] NSCL [12] LR-LSTM [8] LR-Bi-LSTM [8] Self-attention [6] SAAN (our model)

75.9% 77.4% 79.3% 80.7% 81.5% 82.9% 81.5% 82.1% 82.5%* 84.3%

45.7% 45.6% 46.5% 48.1% 48.0% 47.1% 48.3% 48.6% 48.7%* 49.7%

Table 1: Evaluation results. The best result on each dataset is in bold. Results with * are obtained by our own implementation.

4 MODEL ANALYSIS
Effect of Sentiment Resources Attention. In order to further reveal the effect of sentiment resources attention, we compare SAAN model and the CNN model without sentiment resources attention. The experimental results are shown in Figure 2. From Figure 2, we can observe that our proposed SAAN model significantly outperforms the CNN model without sentiment resources attention. This verifies the effectiveness of the sentiment resources attention.
Effect of Each Component of SAAN. To investigate the effect of each component of the SAAN model, we also perform the ablation test of SAAN in terms of discarding word-level mutual attention module (denoted as SAAN-I), phrase-level dynamic semantic attention module (denoted as SAAN-II), sentence-level multi-source attention module (denoted as SAAN-III). The results are reported in Figure 2. Generally, all three factors contribute, and the word-level mutual attention module contributes most. This is within our expectation since word-level mutual attention module, establishing the correlation relation between the context and the sentiment resources, is the basis of the latter sentence semantic modeling. The phrase-level dynamic semantic attention module also makes great contributions to sentence-level sentiment classification. We believe this is because the dynamic semantic attention module can select out the sentimentrelevant phrase chunks to compose the sentiment-specific sentence representation.
5 CONCLUSION AND FUTURE WORK
In this paper, we propose a Sentiment-Aware Attention Network (SAAN) to boost the performance of convolutional neural networks, which incorporates many kinds of the sentiment linguistic knowledge as a whole into the deep neural network. Experiments on the MR and SST datasets show the effectiveness of SAAN. As for future work, instead of using the sentiment resource words as a whole attention source to model the sentiment-specific sentence semantics,

Figure 2: Ablation test results we will consider using sentiment words, negation words and intensity words separately as attention source to model the sentiment-specific sentence representation.
ACKNOWLEDGMENTS
We would like to thank all anonymous reviewers for their valuable comments. The corresponding author is Yujiu Yang. This work was supported in part by the Research Fund for the development of strategic emerging industries by ShenZhen city (No.JCYJ20160301151844537 and No. JCYJ20170412170118573).
REFERENCES
[1] Sepp Hochreiter Djork-Arne Clevert, Thomas Unterthiner. 2016. fast and accurate deep network learning by exponential linear units (ELUS). In ICLR 2016.
[2] Felix A Gers, Ju¨rgen Schmidhuber, and Fred Cummins. 1999. Learning to forget: Continual prediction with LSTM. (1999).
[3] Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In SIGKDD 2004.
[4] Yoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP 2014.
[5] Zeyang Lei, Yujiu Yang, and Yi Liu. 2018. LAAN: A LinguisticAware Attention Network for Sentiment Analysis. In The 2018 Web Conference Companion.
[6] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In ICLR 2017.
[7] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In ACL 2002.
[8] Qiao Qian, Minlie Huang, Jinhao Lei, and Xiaoyan Zhu. 2017. Linguistically Regularized LSTM for Sentiment Classification. In ACL 2017.
[9] Bonggun Shin, Timothy Lee, and Jinho D. Choi. 2017. Lexicon integrated cnn models with attention for sentiment analysis.. In In Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis.
[10] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP 2013.
[11] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. In ACL 2015.
[12] Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016. ContextSensitive Lexicon Features for Neural Sentiment Analysis.. In EMNLP 2016.
[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS 2017.

1200


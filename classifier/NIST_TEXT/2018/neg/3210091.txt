Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Who is the Mr. Right for Your Brand? Г Discovering Brand Key Assets via Multi-modal Asset-aware Projection

Yang Liu
Department of Computer Science Hong Kong Baptist University Hong Kong SAR, China
IRACE, HKBU, Shenzhen, China csygliu@comp.hkbu.edu.hk

Tobey H. Ko
Department of Industrial and Manufacturing
Systems Engineering The University of Hong Kong
Hong Kong SAR, China tobeyko@hku.hk

Zhonglei Gu
Department of Computer Science Hong Kong Baptist University Hong Kong SAR, China cszlgu@comp.hkbu.edu.hk

ABSTRACT
Following the rising prominence of online social networks, we observe an emerging trend for brands to adopt influencer marketing, embracing key opinion leaders (KOLs) to reach potential customers (PCs) online. Owing to the growing strategic importance of these brand key assets, this paper presents a novel feature extraction method named Multi-modal Assetaware Projection (M2A2P) to learn a discriminative subspace from the high-dimensional multi-modal social media data for effective brand key asset discovery. By formulating a new asset-aware discriminative information preserving criterion, M2A2P differentiates with the existing multi-model feature extraction algorithms in two pivotal aspects: 1) We consider brand's highly imbalanced class interest steering towards the KOLs and PCs over the irrelevant users; 2) We consider a common observation that a user is not exclusive to a single class (e.g. a KOL can also be a PC). Experiments on a real-world apparel brand key asset dataset validate the effectiveness of the proposed method.
KEYWORDS
Key Opinion Leader, Potential Customer, Brand Key Asset Discovery, Multi-modal Asset-aware Projection
ACM Reference Format: Yang Liu, Tobey H. Ko, and Zhonglei Gu. 2018. Who is the Mr. Right for Your Brand? Г Discovering Brand Key Assets via Multi-modal Asset-aware Projection. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8Г12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3209978.3210091
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8Г12, 2018, Ann Arbor, MI, USA Е 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210091

1 INTRODUCTION
More and more brands today embrace influencer marketing to deliver brand messages over social networks. As it is impossible to reach every consumer in the market, brands are targeting the key asset - potential customers (PCs) whose taste match what the brand delivers - to maximize their profitability. The brand's major collaborators in influencer marketing are the key opinion leaders (KOLs), who are another brand key asset that bridges the brand with its PCs. Given that the effectiveness of the influencer marketing campaign relies on selecting the right key assets, brands are not hesitant in investing on high caliber specialists capable of accurately identifying the key assets from an ocean of other irrelevant users (IUs). Despite its relatively high accuracy, one major drawback of manual identification is its scalability. As the number of users grows, the time and money required to conduct manual identification is likely to explode. The eminent need for more cost effective methods for brand key asset discovery thus prompts for the proposal of computational methods for PC and KOL discovery [5, 9].
The brand key asset discovery task is exceptionally challenging due to the complexity (i.e. high-dimensionality) and the diversity (i.e. multiple modalities) of the social media data. To address the challenges, we propose a novel method dubbed Multi-modal Asset-aware Projection (M2A2P) to learn compact and discriminative features from high-dimensional multimodal social media data. M2A2P differentiates from the state-of-the-art multi-modal/multi-view feature extraction algorithms [3, 4, 11] by considering two pivotal characteristics of the brand key asset discovery task: 1) As brands are generally more interested in KOLs and PCs than IUs, the imbalanced interests on different classes are modeled; 2) By observing that a brand's KOL can simultaneously be a PC, signaling a correlation between these two groups of users, the non-exclusivity of brand key assets is explored. As shown in Figure 1, the goal of M2A2P is to learn  transformation matrices W1 (for Modality 1), . . . , W (for Modality  ) to project the high-dimensional multi-modal data to a common subspace, where the data within the KOL/PC class are expected to be close to each other while the data from different classes are expected to be faraway from each other. Since the brand key assets, i.e., KOLs and PCs, correlate with each other in some extent, M2A2P tolerates the overlapping between KOLs and PCs in the learned subspace.

1113

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

where (и) denotes the trace operation, and

 = 1{l = l } О l/||l||, l /||l || О  ,

(3)

 = 1{l = l } О (1 - l/||l||, l /||l ||) О  , (4)

where 1{и} is the indicator function which equals one if the argument holds and zero otherwise, и, и denotes the inner prod-

uct operation, l/||l|| is the normalized label vector of the

-th

sample1,

and



=


=1

(-||x -x ||2 /2 )/

indicates the closeness of two samples in the original space,

where



is

empirically

set

by



=


=1

||x

- x ||2/

with x being the -th nearest neighbor of x.

In Eq. (3), we have  = 0 if two samples have different

labels, or both of them are from the class of IUs. This indicates

that Eq. (1) attempts to minimize the asset-aware intra-

Figure 1: Illustration of the idea of Multi-modal Asset-aware Projection (M2A2P).

class scatter for only KOLs and PCs. By doing so, we focus on bringing together the data samples within the classes of more interests. Meanwhile, Eq. (2) maximizes the asset-aware

inter-class scatter for different classes. The more distinct two

2 MULTI-MODAL ASSET-AWARE PROJECTION

label vectors from each other, the more effort we put on to differentiate them in the learned subspace. By using the inner product to measure the similarity between label vectors, the

Let  be the set of data samples with  modalities:  = {(x11, ..., x1 ), ..., (x1, ..., x )}, where x  R ( =
1, ..., ,  = 1, ...,  ) denotes the feature representation

correlation between different labels can be well captured [6]. Moreover, if two samples from different classes are close to each other in the original space, it is relatively hard to classify

of the -th data sample (i.e., the -th user) in the -th

them. Therefore, we put more effort on differentiating them in

modality,  denotes the number of data samples in the set, the learned subspace, which is achieved by incorporating the

 denotes the number of modalities, and  denotes the original dimension of the -th modality. To represent the data in  modalities, we define X1 = [x11, ..., x1], ..., X = [x1 , ..., x ]. We further define a label set  with two labels: {,  }. The label vector associated with the -th user is represented by a 2-dimensional binary vector l, where l = [1, 1] if the -th user is both a KOL and a PC for the brand; l = [1, 0] if a KOL but not a PC; l = [0, 1] if a PC but not a KOL; and l = [0, 0] if an IU.
Given the above training dataset, M2A2P aims to learn  transformation matrices W  RО ( = 1, ...,  ), which are capable of projecting the multi-modal data to a common subspace  = R, where both the asset-aware discrimination and the modality consistency of the dataset could be preserved.
2.1 Asset-aware Discrimination Preserving

closeness indicator  in . Eqs. (1)-(2) could be rewritten

as:

W = arg min (W QW),

(5)

W

W = arg max (W QW),

(6)

W

where W = [W1 , W2 , и и и , W  ] and Q =

 X1L11X1

иии

X1L1 X 

  

...

...

...

, in which  = , , and


X L1X1 и и и X L  X

L (,  = 1, ...,  ) is the (, )-th block of the matrix

L11 и и и L1  

L,

i.e.,

L

=

 



...

...

...

. Here L is a Lapla-


L1 и и и L 

cian matrix defined as L = Diag - Adj, where

Adj = 1О  Ind (Ind = A, B; A = [ ]О and

B = [ ]О) is an  О  matrix, with 1О de-

noting the  О  matrix with all entries being 1 and

To take the imbalanced interests on different classes and the key asset correlation into consideration, we propose the following objective functions for asset-aware discrimination

the symbol  denoting the Kronecker product operation

on two matrices, Diag is a diagonal matrix defined as

()

=


=1

(



 )

(

=

1, ...,  ).

preserving:

2.2 Modality Consistency Preserving


(  min 







(W 

x

-W

x

)(W 

x

-W

x



)

) ,

In multi-modal learning, various modalities actually characterize the same data items. As a result, the consistency among

,=1 ,=1

(1)

multiple modalities should be taken into consideration.

Assume that the representation of the -th modality can


(  max 






(W 

x

-W

x

)(W 

x

-W

x

)

) ,

be transformed from that of the -th modality by a  О matrix T, i.e., X = TX (,  = 1, ...,  ). Then W and

,=1 ,=1

(2)

1We define l/||l|| = [0, 0] if l = [0, 0] .

1114

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

W should follow the same relationship, i.e., W = TW. According to the Representer Theorem [8], the columns of W and W can be represented by the linear combination of {x1, ..., x} and {x1, ..., x}, respectively, i.e.,

W = X, W = X.

(7)

Then we have X = W = TW = TX = X, which indicates  = , or at least  and  are similar (i.e., the columns of ( - ) are in the nullspace of X and that of X). To preserve the modality consistency, we propose the following objective function:



min  || - ||2 .

(8)

,=1

Based on Eq. (7), we rewrite Eq. (8) as follows:

W = arg min (W CW),

(9)

W

( - 1)C1 C1 и и и

-C1 C 

where

C

=

 



...

...

...

, in which


-C C1

и и и ( - 1)C C

C = (XX)X ( = 1, и и и ,  ) and (и) denotes the

Moore-Penrose pseudoinverse.

2.3 Multi-modal Asset-aware Projection

To preserve both the asset-aware discrimination and the modality consistency, we integrate the aforementioned three objectives in Eqs. (5), (6), and (9) to form a unified objective function of the proposed M2A2P:

W=

arg max
W

( 

W

W Q W (Q + C)W

) ,

W W=I

(10)

where   [0, +) is the trade-off parameter to balance the weight of the asset-aware discrimination and that of the modality consistency. The constraint W W = I is introduced to remove the scaling factor, where I denotes the -dimensional identity matrix. The optimal W that maximizes the objective function in Eq. (10) is composed of the normalized eigenvectors corresponding to the  largest eigenvalues of the following eigen-decomposition problem:

Qw = (Q + C)w.

(11)

3 EXPERIMENTS
In this section, we validate the effectiveness of M2A2P on a real-world apparel brand key asset dataset2, which is composed of 8, 426 users, including 1, 451 KOLs, 3, 284 PCs, and 4, 320 IUs, where 629 users are labeled as both KOL and PC. For each user, the original feature dimension is 1, 914, consisting of 10 modalities with 4 types of information: i) User profile: 2 modalities of features extracted from the user's profile image Г a 128-D color histogram feature vector and a 256-D LBP feature vector; ii) User-generated content: 6 modalities of features popped from the user's latest 3 published media: a 128-D color histogram feature vector and
2The dataset is available at: https://goo.gl/ZZ3T9a (feature set) and https://goo.gl/D2M1EB (label set).

Figure 2: Performance comparison of different feature extraction methods on the apparel brand key asset dataset.
a 256-D LBP feature vector for each published media; iii) Textual description: 1 modality of a 373-D TF-IDF feature vector summarizing the text of user's full name, bio, and captions of user's 20 latest published media; and iv) Statistical information: 1 modality of a 5-D feature vector encapsulating statistics of user's published media, followers, followings, as well as an aggregated number of comments and likes in the user's 20 latest published media.
3.1 Quantitative Evaluation
We compare M2A2P with PCA [2], LDA [1], multi-label LDA (MLDA) [10], multi-emotion similarity preserving embedding (ME-SPE) [6], multi-view discriminant analysis (MvDA) [4], and multi-view feature learning (MVFL) [11]. We also include the original 1, 914-dimensional data as the baseline. For PCA, LDA, MLDA, and ME-SPE, we first concatenate the original features of 10 modalities into a 1, 914-D feature vector, and then map it to a low-dimensional subspace. For MvDA, MVFL, and the proposed M2A2P, we first map the original features of 10 modalities to a common subspace, and then concatenate the reduced representations of these 10 modalities into a single feature vector. For LDA, MvDA, and MVFL, we transform the labels from [1, 1] , [1, 0] , [0, 1] , and [0, 0] to 3, 2, 1, and 0, respectively.
We use two groups of criteria for performance evaluation. i) The label-based metrics: Macro/Micro average precision and F1 score [12]. The -nearest neighbor (-NN) classifier is used for classification after feature extraction. ii) The example-based metrics: average precision, Hamming loss, oneerror, and ranking loss [12]. The multi-label -NN classifier is used for classification. We set  = 10 in our experiments. We utilize 10% of samples from the dataset for training, and the rest 90% for testing. We use 10-fold cross validation strategy to select the parameters for all methods. The experiment is repeated 10 times on randomly selected training/test sets to obtain the average as the final result. Figure 2 shows the results of all the algorithms. By jointly learning the structure of multiple modalities under the criteria of assetaware discrimination preserving and modality consistency preserving, M2A2P achieves the best performance.
We then examine the individual contribution of Assetaware discrimination preserving (referred to as A) and modality Consistency preserving (referred to as C) in M2A2P by adjusting the values of the parameter  (= 0, 1, 108). The performance of M2A2P with different s is shown in Table 1.

1115

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: The performance (average precision and ranking loss) of M2A2P with different settings of  (= 0, 1, 108).

Aver. Precision Ranking Loss

 = 0 (A)
0.875 0.111

 = 1 (A, C)
0.892 0.095

 = 108 (C)
0.880 0.107

The dominant components with different s are listed in the bracket of the corresponding cell. We can observe that the performance of M2A2P is comparable to some existing feature extraction methods even if we only consider a single component, which validates the rationality of preserving the asset-aware discrimination and modality consistency when learning the subspace from high-dimensional multi-modal social media data for brand key asset discovery. By preserving both components simultaneously, the performance of M2A2P is further improved, which shows the superiority of jointly optimizing the asset-aware discrimination and modality consistency in a unified framework.
3.2 Qualitative Analysis
We qualitatively analyze the effectiveness of the proposed method with several typical users in our dataset. Figure 3 presents the multi-modal information of three typical users, including (from left to right) their statistical information, profile images, latest published media, and text highlights. Here users 1-3 are annotated as KOL, IU, and KOL+PC, respectively. The identification results of different feature extraction methods are shown in the last column of Figure 3 using different colors (with and О indicating the correct and incorrect identification, respectively).
User 1 is a typical KOL for the brand. The massive volume of followers and media likes together with the high quality media of herself showcasing different styles of fashion-forward apparel make her a simple identification task for all the algorithms. User 2 can easily be identified as irrelevant user in the manual process due to his low relevance to the brand, but was wrongly identified by half of the algorithms. One possible reason might be related to the high occurrence of the term "travel" in the user's text highlight, which coincides with that of many of the brand's KOLs. M2A2P's correct identification of user 3 over all other algorithms shows the robustness of our method. At a glance, user 3 does not engage in much interaction with its followers despite a high follower count, which lead all the other algorithms misidentify the user. Yet, with her text highlight shown high relevance with the apparel brand, M2A2P was able to uncover the hidden correlation between different user-generated contents, and correctly identify her in the experiment.
4 CONCLUSION
In this paper, we presented Multi-modal Asset-aware Projection (M2A2P) to learn a discriminative subspace from the high-dimensional multi-modal social media data for brand key asset discovery. By capturing both the asset-aware discrimination and the modality consistency, M2A2P achieved

Figure 3: Three typical examples from the dataset and the identification results of different feature extraction methods on these examples.
good performance on a real-world apparel brand key asset dataset. For the future work, we are particularly interested in incorporating the domain knowledge from different brands for delivering brand-specific key asset discovery.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science Foundation of China under Grant 61503317, the General Research Fund from the Research Grant Council of Hong Kong under Project RGC/HKBU12202417, and the Science and Technology Research and Development Fund of Shenzhen with Project Code JCYJ20170307161544087.
REFERENCES
[1] R. A. Fisher. 1936. The Use of Multiple Measurements in Taxonomic Problems. Ann. Eugen. 7 (1936), 179Г188.
[2] H. Hotelling. 1933. Analysis of a complex of statistical variables into principal components. J. Educ. Psychol. 24, 6 (1933), 417Г 441.
[3] F. Huang, Y. Cheng, C. Jin, Y. Zhang, and T. Zhang. 2017. Deep Multimodal Embedding Model for Fine-grained Sketchbased Image Retrieval. In Proc. 40th SIGIR. 929Г932.
[4] M. Kan, S. Shan, H. Zhang, S. Lao, and X. Chen. 2016. MultiView Discriminant Analysis. IEEE Trans. Pattern Anal. Mach. Intell. 38, 1 (2016), 188Г194.
[5] Y. Liu, Z. Gu, T. H. Ko, and J. Liu. 2017. Brand Key Asset Discovery via Cluster-wise Biased Discriminant Projection. In Proc. IEEE/WIC/ACM Int. Conf. Web Intell. 284Г290.
[6] Y. Liu, Y. Liu, Y. Zhao, and K.A. Hua. 2015. What Strikes the Strings of Your Heart? Г Feature Mining for Music Emotion Analysis. IEEE Trans. Affect. Comput. 6, 3 (2015), 247Г260.
[7] Y. Lu, K. Jerath, and P. V. Singh. 2013. The Emergence of Opinion Leaders in a Networked Online Community: A Dyadic Model with Time Dynamics and a Heuristic for Fast Estimation. Management Science 59, 8 (2013), 1783Г1799.
[8] B. Schеolkopf and A. J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA.
[9] H. Sharara, L. Getoor, and M. Norton. 2011. Active Surveying: A Probabilistic Approach for Identifying Key Opinion Leaders. In Proc. 22nd IJCAI. 1485Г1490.
[10] H. Wang, C. Ding, and H. Huang. 2010. Multi-label Linear Discriminant Analysis. In Proc. 11th ECCV. 126Г139.
[11] J. Xu, J. Han, and F. Nie. 2017. Multi-view Feature Learning with Discriminative Regularization. In Proc. 26th IJCAI. 3161Г3167.
[12] M.-L. Zhang and Z.-H. Zhou. 2014. A Review on Multi-Label Learning Algorithms. IEEE Trans. Knowl. Data Eng. 26, 8 (2014), 1819Г1837.

1116


Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Deep Domain Adaptation Based on Multi-layer Joint Kernelized Distance

Sitong Mao, Xiao Shen, Fu-lai Chung
Department of Computing, Hong Kong Polytechnic University Hong Kong, China
sitong.mao@connect.polyu.hk,xiao.shen@connect.polyu.hk,cskchung@comp.polyu.edu.hk

ABSTRACT
Domain adaptation refers to the learning scenario where a model learned from the source data is applied on the target data which have the same categories but different distributions. In information retrieval, there exist application scenarios like cross domain recommendation characterized similarly. In this paper, by utilizing deep features extracted from the deep networks, we proposed to compute the multi-layer joint kernelized mean distance between the kth target data predicted as the ith category and all the source data of the jth category dikj . Then, target data Tm that are most likely to belong to the ith category can be found by calculating the relative distance diki / j dikj . By iteratively adding Tm to the training data, the finetuned deep model can adapt on the target data progressively. Our results demonstrate that the proposed method can achieve a better performance compared to a number of state-of-the-art methods.
CCS CONCEPTS
· Computing methodologies  Object recognition; Transfer learning;
ACM Reference Format: Sitong Mao, Xiao Shen, Fu-lai Chung. 2018. Deep Domain Adaptation Based on Multi-layer Joint Kernelized Distance. In Proceedings of SIGIR '18. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210155
1 INTRODUCTION
Machine learning has become an essential part of many tasks recently, including information retrieval as in cross domain recommendation [1] and cross network influence maximization [2]. However, methods that work well rely on the assumption that the training set and the test set are drawn from the same feature space and the same distribution, which does not always happen in realistic settings. Hence, to obtain favorable performance on a target dataset whose feature space or distribution is different from the source data, one may need to recollect labeled training data manually and then retrain the models on it. However, it is prohibitively expensive or even impossible to recollect the training data with label information. Thus, it is important to develop methods that can borrow prior knowledge to compensate for the unavailable or
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210155

insufficient labels of the target data. Domain adaptation is one of the fields that are associated with this problem.
Domain adaptation addresses the case that adapts a model trained on source data to target data from different but related domains. The importance of domain adaptation has been explored in a series of applications, such as natural language processing, computational biology, computer vision and information retrieval. Under the setting of domain adaptation, despite consisting of the same categories, the source data and the target data are typically distributed differently which is referred to as domain shift. For instance, in the scenario of visual domain adaptation, the distribution can be substantially affected by angle transformation, illumination, or occlusion. Therefore, many approaches to domain adaptation focus on bridging different domains by reducing their distribution discrepancy.
To combat the performance degradation on target domain arising from domain shift, previous works have explored approaches in various directions. The common approaches include feature augmentation, feature transformation, and domain re-sampling. Although these methods have made prominent progress, their shallow architectures prevent them from achieving more desirable performance. Recently, deep neural networks have been proved having strong ability of learning transferable features, and this depicts the potential of empowering domain adaptation with deep learning.
Features in deep neural networks eventually transit from general to specific as the layer goes higher(deeper), which shows that parameters from lower-layers are general enough to suit both source and target tasks, while the transferability gap between tasks grows in higher(deeper) layers. Therefore, the network pre-trained on source dataset is not likely to be discriminative enough when it is applied to the target dataset directly. Inspired by such characteristic of deep neural networks, some hierarchical approaches have been proposed to boost the generalization performance by reducing the domain discrepancy in higher layers [3] [4].
However, deep neural networks are hard to train when regularizer is integrated into the loss function, which play a role of adversarial training, and there are too many parameters needed to be finetuned very carefully. The key idea in this paper is to add samples that are most likely to be correctly predicted from each category to the source domain for more effective transfer. Such process to control the transfer is carried out in an iterative finetuning manner. Furthermore, the process of selecting target data is conducted for each category simultaneously, which makes the proposed framework efficient and accurate.
In the next section, we review some existing works related to domain adaptation. After that, our proposed method is introduced. We then evaluate our approach and show the comparative results before concluding the paper.

1049

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

2 RELATED WORK
In standard domain adaptation scenario, labels of target domain are usually assumed unavailable during training. Daumé III [5] proposed a feature augmentation-based method that maps features to an augmented space. Another direction is to learn a transformation under which source and target distributions can be represented closer [6]. One additional type of approach is making labeled source instances that are most similar to data in the target domain carry more weights [7]. Despite the appreciable improvement made by these methods, they are still limited by the shallow architecture which cannot effectively learn representative features and suppress domain-specific variability.
Deep neural networks have recently gained much attention in many applications for its distinctive power in learning more robust features that are invariant to the differences between tasks. However, the distribution discrepancy between domains cannot be effectively minimized in the higher layers. This indeed inspires exploration of domain adaptation approaches based on deep neural networks. Most previous works mainly consider the distribution changes of the features P(X ), namely marginal distribution. Long et al. [3] propose a Deep Adaptation Network (DAN) which incorporates multi-kernel maximum mean discrepancy (MK-MMD) of the highest few layers as a regularizer in the CNN loss function. In this approach, the MK-MMD is computed without considering the labels. However, it is not clear that under what conditions, the approximately same marginal distributions (PS (T (X ))  PT (T (X ))) can imply similar conditional distributions (PS (Y |T (X ))  PT (Y |T (X ))).
To address the shortcoming of merely relying on correcting the domain shifts in marginal distributions, Gong et al. [8] aim to find conditional transferable components that are invariant across different domains. In [4], Long et al. recently propose a "joint distribution discrepancy" (JDD) metric, by bounding which together with the source loss, the conditional distribution discrepancy can be reduced. To compute JDD, the distribution over the class labels produced by the pre-trained CNN model is used. However, as aforementioned, the transferability decreases with CNN layer going higher, and this may limit the predictive accuracy achieved and affect the transfer learning performance.
In fact, the model to be proposed in the next section attempts to alleviate the mis-prediction problem of target labels arising from domain shifts. Quite different from previous approaches, we make use of joint multi-layer kernelized distance to identify the target samples that are most likely to be correctly predicted and utilize them to carry out transfer learning. In this way, the domain distribution discrepancy can be reduced for more effective domain adaptation. Such a process is proposed to carry out in rounds so that a better control of transfer can be achieved. In the proposed model, selecting correctly predicted target data is conducted for each category at the same time, by solving which a more accurate and efficient performance than processing multiple categories together can be obtained.
3 METHODOLOGY
In the standard settings of unsupervised domain adaptation, each category has labeled source dataset S = {X S, Y S } and un-labeled target data T = {X T }. Here X S = {xis }, i = 1, 2, . . . , ns , where ns

is 1,

the number 2, . . . , ns , yis

of 

samples in {1, 2, . . . , c

the source }, where c

domain, and Y is the number

S = {yis }, i = of categories.

The number of examples in T is denoted as nt .

In the following, the "multi-layer joint distance" is firstly demon-

strated. Then, the method of selecting data that are most likely

to be predicted correctly and the iterative finetuning process are

described.

3.1 Multi-layer Joint Distance

Given the source and target deep features xism , xjtm extracted

from different layers  = 1, . . . , 2 of the pre-trained deep neural

network. Here m = 1, 2, . . . , c denotes the ground truth label of the

source data and the predicted label of the target data. xism denotes the deep feature of the ith source data of the mth category extracted

from

layer

,

and

x

t jm

denotes

the

deep

feature

of

the

jth

target

data of the mth category extracted by layer . Many previous works

measure the discrepancy between two distributions by reflecting

the deep features in the Reproducing Kernel Hilbert Space Hk , where the inner product is defined as < (x), (x ) >= k(x, x ) and

k(·, ·) is the kernel function. In the proposed method, instead of

calculating the distance between all the target data and the source

data, we consider each target data as an individual distribution.

Then,

the

distance

between

a

target

data

x

t jm

and

a

source

category

m = 1, . . . , c is defined as:

dk2

(x

t jm

,

xms 

)

=



E[2=1





(x

t jm

)]

-

E[2=1





(xms 

)]

2 Hk

.

(1)

Here, 2=1(x ) = 1 (x 1 )· · ·2 (x 2 ) is the joint embedding feature maps of different layers. And the inner product in this joint

embedding Reproducing Hilbert Space is defined as

2

< 2=1 (x  ), 2=1 (x  ) >=

k(x , x ).

(2)

=1

Then

the

distance

between

x

t jm

and

source

category

m

is

2

dk2

(x

t jm

,

xms 

)

=

k



(x

t jm

,

x

t jm

)+

=1

1 (nms  )2

nms  i,k =1

2 =1

k  (xism , xks m )-

(3)

2 nms 

nms  i =1

2

k



(x

t jm

,

xism 

).

=1

3.2 Overall Adaptation Process

3.2.1 Selecting Data for Adaptation. Given target data predicted

to be the mth category xjtm , j = 1, 2, . . . , nmt ,  = 1, . . . , 2, the

distance between each target data xjtm and the m source category

dk2

(x

t jm

,

xms 

)

can

be

computed

by

the

method

described

previously.

Then,

the

relative

distance

between

x

t jm

is

defined

as

R(x

t jm

)

=

c m=1

dk2 (xjtm , xms )

dk2

(x

t jm

,

xms 

)

.

(4)

1050

Short Research Papers I Deep Domain Adaptation Based on Multi-layer Joint Kernelized Distance

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Intuitively,

the

correctly

predicted

target

data

x

t jm

will

be

close

to

xms and far from xms , m m. So the smaller R(xjtm ) is, the more

likely

x

t jm

belongs

to

the

mth

category.

Then

in

each

category,

we

select top k target data that are most likely to be correctly predicted

to add to the training set. Here, k could be selected according to the

amount of data in the test set. For larger test set, we could choose

larger k.

3.2.2 Iterative Implementation. The proposed method is an iter-
ative design with the involved deep neural network (CNN in this
paper) fine-tuned with updated training data. At start, a pre-trained network model M0 is fine-tuned by the original labeled samples from the source domain. The trained network is then used to classify
the unlabeled samples of the target domain. The label information together with the deep neural network extracted features xs and xt  are fed to the multi-layer joint kernelized distance module to
compute the correctly predicted ranking for each individual cat-
egory (label). Based on the ranking information, a portion of the
target samples are deemed as correctly labeled data, which can then
be added to the source samples to form the updated labeled training
set. At the same time, such auxiliary data are removed from the
unlabeled target sample set to prepare for the next round of model
training (fine-tuning) and correctly predicted target data selection. This process will be repeated in rounds until there is no remain-
ing target data for adding to the training dataset, or enough target data have been incorporated. The integrated procedure has been described by Algorithm 1.

Algorithm 1 Deep Domain Adaptation Based on MJKD
Input: 1: Source data S = {X S, Y S } 2: Source labels Y S = {ys }, s = 1, 2, . . . , c 3: Target data T = {X T } 4: Given pre-trained model M
Procedure: 5: Get the initial model M0 by fine-tuning from M on S; 6: for p = 0; T  and not enough integrated data; p + + do 7: Apply Mp on T ; 8: Find correctly predicted target data {Nip } by using the RMJKD; 9: S = S {Nip }; T = T - {Nip }; 10: Update the network: Mp+1 = Finetune(Mp , S); 11: end for
Output: 12: The classification of target samples.

4 EXPERIMENTS
In this section, we evaluate the proposed approach by employing the convolutional neural networks (CNNs) for classification and compare the results to some state-of-the-art methods in a wellknown domain adaptation dataset. More results in the context of information retrieval will be reported in the online archive. To investigate from which layer to extract the features for adaptation, we consider features from different layers.

4.1 Experimental Setup
Office-31 dataset. The Office-31 dataset [6] contains images originated from three domains: Amazon, Webcam, and DSLR. These three domains consist of same set of categories, i.e., 31 categories. Amazon(A) contains images downloaded from online merchants1. These images are product shots at medium resolution typically taken in an environment with studio lighting conditions without redundant background. DSLR(D) consists of images that were captured with a digital SLR camera in realistic environments with natural lighting conditions. The images have high resolution and low noise. Webcam(W) consists of images from a webcam. These images are of low resolution, show significant noise, as well as white balance artifacts. These three domains in Office-31 database represents several interesting visual domain shifts. Hence, we can evaluate the proposed approach on 6 transfer tasks: W  D, W  A, D  W , D  A, A  W , and A  D.
4.1.1 Network Architecture. In the experiment discussed here, we implement our method with AlexNet [9]. The AlexNet contains eight layers with weights. The first five layers are convolutional layers and the last three layers are fully-connected. This architecture requires constant size of inputs, so all the images are rescaled to 227 × 227 × 3-dimensional before being fed to it. The number of neurons in the fully-connected layer fc6 and fc7 are all 4096, and in fc8 it is equal to the number of categories in the dataset.
4.1.2 Fine-tuning. The initial model M0 is fine-tuned on the source domain from AlexNet pre-trained on ImageNet utilizing the framework Caffe [10]. In the standard fine-tuning procedure, the first three convolutional layers are frozen. The learning rate for conv4 - f c7 is set to a small number to slightly tune the parameters initialized from the pre-trained model. The learning rate for f c8 can be set larger, usually 10 times that of conv4 - f c7. We used the stochastic gradient descent (SGD) update strategy with a momentum of 0.9. The learning rate is initialized from the range 0.0001 to 0.0003, and will be reduced by multiplying a factor  at the order of 10-3 gradually. To make the proposed approach directly compare with previous methods, we follow an unsupervised domain adaptation protocol, in which all labeled source domain samples are used for training the initial network, while labels of all target examples are not provided [3]. To make sure that our approach is feasible to implement, the parameters such as the initial learning rates and the batch size are constant in each round.
4.2 Results
We compare our results with six existing methods: TCA [11], GFK [12], LapCNN [13], DDC [14], DAN [3], and JAN, JAN-A [4]. The results obtained for the Office-31 dataset are summarized in Table 1. Obviously, the proposed method achieves better accuracy than the previous state-of-the-art methods in each transfer task as highlighted in bold. The second best results are underlined. We can observe that JAN which jointly reduces domain discrepancy by considering conditional distribution performs better than DAN which only relies on correcting marginal distribution shifts. The proposed model which minimizes the prediction error of the target
1 www.amazon.com

1051

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Comparison of accuracy on Office-31 dataset under conventional unsupervised domain adaptation settings.

Method
TCA GFK AlexNet LapCNN DDC DAN JAN JAN-A
MJKD

WD
95.2 95.0 99.0 99.1 98.5 99.0 99.5 99.6
99.8

WA
50.9 48.1 49.8 48.2 52.2 53.1 55.0 56.3
56.4

DW
93.2 95.6 95.1 94.7 95.0 96.0 96.6 96.6
97.0

DA
51.6 52.4 51.1 51.6 52.1 54.0 58.3 57.5
59.3

AW
61.0 60.4 71.6 60.4 61.8 68.5 74.9 75.2
77.0

AD
60.8 60.6 63.8 63.1 64.4 67.0 71.8 72.8
73.7

labels can achieve distinctive performance, though there are mislabeled target data added to the training set in each round.

(a) Initial source(A) features

(b) Initial target(D) features

networks to adapt the training set from the source domain to the target domain. Different from previous works, the proposed method makes use of multi-layer joint distance to control the transfer in an iterative manner. Such an approach can enhance the accuracy of predicting the target labels and thereby effectively avoid potentially ineffective or negative transfer of mis-predicted samples. Our results show that the proposed method can outperform the state-of-the-art methods adopted in our experiments.
ACKNOWLEDGEMENT
This work was supported by UGC GRF Project No. PolyU 152039/14E.

(c) Final source(A) features

(d) Final target(D) features

Figure 1: Top 2 images are t-SNE embeddings of deep features extracted by the initial model for task AD. Bottom 2 images are t-SNE embeddings of features extracted the model learned by the proposed approach.

4.3 Qualitative Analysis
To illustrate how the proposed approach makes the network more discriminative on target domain by reducing the discrepancy, we plot the t-SNE embeddings of deep features extracted from the initial model and the model obtained from the last iteration. Let us take the task AD as an example. The top 2 plots in Figure 1 show the source and target deep features extracted by the initial network while the bottom 2 plots show the features extracted by the final model learned by the proposed approach. We plot the features from ten categories for each domain. Each class is marked with a number as shown in the figures. We can observe that the distribution discrepancy between source features and target features are reduced.
5 CONCLUSION
In this paper, we present a new domain adaptation approach which can be applied to different IR tasks by employing deep neural

REFERENCES
[1] Aleksandr Farseev, Ivan Samborskii, Andrey Filchenkov, and Tat-Seng Chua. Cross-domain recommendation via clustering on multi-layer graphs. In SIGIR, pages 195­204. ACM, 2017.
[2] Xiao Shen, Fu-lai Chung, and Sitong Mao. Leveraging cross-network information for graph sparsification in influence maximization. SIGIR '17, pages 801­804. ACM.
[3] Mingsheng Long and Jianmin Wang. Learning transferable features with deep adaptation networks. CoRR, abs/1502.02791, 1:2, 2015.
[4] Mingsheng Long, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint adaptation networks. ICML, 2017.
[5] Hal Daumé III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.
[6] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. Computer Vision­ECCV 2010, pages 213­226, 2010.
[7] Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In ICML, pages 222­230, 2013.
[8] Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Schölkopf. Domain adaptation with conditional transferable components. In ICML, pages 2839­2848, 2016.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097­1105, 2012.
[10] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM International Conference on Multimedia, pages 675­678. ACM, 2014.
[11] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199­210, 2011.
[12] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In CVPR, pages 2066­2073. IEEE, 2012.
[13] Jason Weston, Frédéric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade, pages 639­655. Springer, 2012.
[14] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.

1052


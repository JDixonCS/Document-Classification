Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

New Embedded Representations and Evaluation Protocols for Inferring Transitive Relations

Sandeep Subramanian
IIT Bombay
ABSTRACT
Beyond word embeddings, continuous representations of knowledge graph (KG) components, such as entities, types and relations, are widely used for entity mention disambiguation, relation inference and deep question answering. Great strides have been made in modeling general, asymmetric or antisymmetric KG relations using Gaussian, holographic, and complex embeddings. None of these directly enforce transitivity inherent in the is-instance-of and is-subtype-of relations. A recent proposal, called order embedding (OE), demands that the vector representing a subtype elementwise dominates the vector representing a supertype. However, the manner in which such constraints are asserted and evaluated have some limitations. In this short research note, we make three contributions specific to representing and inferring transitive relations. First, we propose and justify a significant improvement to the OE loss objective. Second, we propose a new representation of types as hyperrectangular regions, that generalize and improve on OE. Third, we show that some current protocols to evaluate transitive relation inference can be misleading, and offer a sound alternative. Rather than use black-box deep learning modules off-the-shelf, we develop our training networks using elementary geometric considerations.
ACM Reference Format: Sandeep Subramanian and Soumen Chakrabarti. 2018. New Embedded Representations and Evaluation Protocols for Inferring Transitive Relations. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210150
1 INTRODUCTION
Contemporary information extraction from text, relation inference in knowledge graphs (KGs), and question answering (QA) are informed by continuous representations of words, entities, types and relations. Faced with the query "Name scientists who played the violin," and having collected candidate response entities, a QA system will generally want to verify if a candidate is a scientist. Testing if e  t or t1  t2, where e is an entity and t, t1, t2 are types, is therefore a critical requirement. Unlike Albert Einstein, lesser-known candidates may not be registered in knowledge graphs, and we may need to assign a confidence score of belongingness to a target type.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210150

Soumen Chakrabarti
IIT Bombay
A common recipe for inferring general relations between entities is to fit suitable vectors to each of them, and to train a network to input query vectors and predict presence or absence of the probed relationship. A key question has been whether types merit a special representation, different from the generic devices that represent KG relations, because of their special properties. Two types may be disjoint, overlapping, or one may contain the other. Containment is transitive.
Compared to the vast array of entity-relation representations available [2, 10, 12, 16, 19], few proposals exist [5, 17, 18] for representing types to satisfy their specific requirements. Of these, only order embedding (OE) by Vendrov et al. [17] directly enforces transitivity by modeling it as elementwise vector dominance.
We make three contributions. First, we present a significant improvement to the OE loss objective. Second, we generalize OE to rectangle embeddings for types: types and entities are represented by (hyper-)rectangles and points respectively. Ideally, type rectangles contain subtype rectangles and entity instance points. Rather than invoke established neural gadgets as black boxes, we introduce constraints and loss functions in a transparent manner, suited to the geometric constraints induced by the task at hand. Third, we remove a limitation in the training and evaluation protocol of Vendrov et al. [17], and propose a sound alternative. Experiments using synsets from the WordNet noun hierarchy (same as Vendrov et al. [17]) show the benefits of our new formulations. Our code will be available1.
2 RELATED WORK
Words and entities2 are usually embedded as points or rays from the origin [7, 13, 21]. It is well appreciated that relations need more sophisticated representation [2, 10, 12, 16, 19], but types seem to have fallen by the wayside, in relative terms. Vilnis and McCallum [18] pioneered a Gaussian density representation for words, to model hypernymy via the asymmetric KL divergence as an inference gadget. Items x, y are represented by Gaussian densities x , y (with suitable mean and covariance parameters). If x  y we want low KL(x y ). Normalized densities with unit mass seem inappropriate for types with diverse population sizes. Athiwaratkun and Wilson [1] have used a thresholded divergence d (x, y) = max{0, KL(x y ) -  }. However, modeling asymmetry does not, in itself, enforce transitivity. Neither is anti-symmetry modeled. Jameel and Schockaert [5] proposed using subspaces to represent types. They do not address type hierarchies or transitive containment. Recently, Nickel and Kiela [11] introduced an elegant hyperbolic geometry to represent types, but moving away from Euclidean space can complicate the use of such embeddings in
1 https://gitlab.com/soumen.chakrabarti/rectangle 2Also see wiki2vec https://github.com/idio/wiki2vec

1037

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

downstream applications, in conjunction with conventional word
embeddings. Vendrov et al. [17] proposed a simpler mechanism: embed each type t to vector ut  RD , and, if t1  t2, then require u t1  u t2 , where  is elementwise. I.e., u t1 must dominate u t2 . OE was found better at modeling hypernymy than Gaussian embed-
dings. In OE, types are open cones with infinite volume, which
complicates representing various intersections.

3  OE: OE WITH IMPROVED LOSS OBJECTIVE

In what follows, we use the partial order x  y to unify e  t and

t1  t2 for notational simplicity. If x  y, OE required u x  uy . OE defines (x, y) =  max{0,uy -u x }22, which is 0 iff uy  u x . Given labeled positive instances x  y and negative instances x  y, the overall loss is the sum of two parts:

L+ = +(x, y) = (x, y)

(1)

x y

x y

L- = -(x, y) = max{0,  - (x, y)},

(2)

x y

x y

where  is a tuned additive margin. The intuition is that when

x  y, we want (x, y)  . There are two limitations to the above loss definitions. First,  · · · 22 is too sensitive to outliers. This is readily remedied by redefining (x, y) using L1 norm, as

D

 max{0,uy - u x }1 = [uy,d - ux,d ]+,

(3)

d =1
where [·]+ = max{0, ·} is the hinge/ReLU operator. But the semantics of - are wrong: we are needlessly encouraging all dimensions to violate dominance, whereas violation in just one dimension would

have been enough.

Specifically, for x  y, loss should be zero if ux,d < uy,d for any d  [1, D]. Accordingly, we redefine

-(x, y) = d m[1i,nD][ux,d - uy,d ]+,

(4)

so that the loss is zero if dominance fails in at least one dimension.

To balance this L form in case of positive instances, we redefine

+(x, y) = d m[1a,xD][uy,d - ux,d ]+,

(5)

so that the loss is zero only if dominance holds in all dimensions.

The unbounded hinge losses above mean a few outliers can hijack the aggregate losses L+ and L-. Moreover, the absence of a SVM-like geometric margin (as distinct from the loss margin  above) also complicates separating  and  cases confidently. Our final design introduces a nonlinearity (sigmoid function) to normalize per-instance losses, additive margin  and a standard stiffness hyperparameter  .

+(x, y) =   d m[1a,xD][uy,d +  - ux,d ]+ - 1/2

(6)

-(x, y) =   d m[1i,nD][ux,d +  - uy,d ]+ - 1/2.

(7)

(Obviously the `-1/2' terms are immaterial for optimization, but

bring the loss expression to zero when there are no constraint

violations.)

4 RECTANGLE EMBEDDINGS

Despite its novelty and elegance, OE has some conceptual limitations. A type t with embedding ut is the infinite axis-aligned open convex cone {p : p  ut } with its apex at ut . Thus, types cannot "turn off" dimensions, all pairs of types intersect (although the intersection may be unpopulated), and all types have the same infinite measure, irrespective of their training population sizes.

We propose to represent each type by a hyper-rectangle (hereafter, just `rectangle'), a natural generalization of OE cones. A rectangle is convex, bounded and can have collapsed dimensions (i.e., with zero width). Obviously, rectangles can be positioned to be disjoint, and their sizes can give some indication of the number of known instances of corresponding types. Containment of one rectangle in another is transitive by construction, just like OE. Entities remain represented as points (or infinitesimal rectangles for uniform notation).

Each type or entity x is represented by a base vector u x , as well as a nonnegative width vector v x  R+D , so that in dimension d, the rectangle has extent [ux,d , ux,d + vx,d ]. Informally, the rectangle representing x is bounded by "lower left corner"u x and "upper right corner" u x + v x . For entities, v x  0. For types, v x are regularized with a L2 penalty. The rectangles are allowed to float around freely,
so u x are not regularized.

If x  y or x  y, the rectangle representing x must be con-

tained in the rectangle representing y. Let the violation in the dth

dimension be

+(x, y; d) = max{[uy,d - ux,d ]+,

(8)

[ux,d + vx,d - uy,d - vy,d ]+} Then the loss expression for positive instances is

L+ =

max [+(x, y; d)]+.

(9)

x y d [1, D]

This ensures that the loss is proportional to the largest violating

margin and that the loss is zero if the rectangle of x is contained in

the rectangle of y. Analogously, we define

-(x, y; d) = min{[ux,d - uy,d ]+,

(10)

[uy,d + vy,d - ux,d - vx,d ]+}

and L- =

min [-(x, y; d)]+.

(11)

x y d [1, D]

As in  OE, we can add margin, stiffness, and nonlinearity to rect-

angles, and get

+(x, y; d) = max{[uy,d +  - ux,d ]+,

(12)

[ux,d + vx,d +  - uy,d - vy,d ]+}

-(x, y; d) = min{[ux,d +  - uy,d ]+,

(13)

[uy,d + vy,d +  - ux,d - vx,d ]+}

L+ = +(x, y) =   max +(x, y; d)

(14)

x y

x y

d [1, D]

L- = -(x, y) =   min -(x, y; d) . (15)

x y

x y

d [1, D]

5 TRAINING AND EVALUATION PROTOCOLS
Because the training and evaluation instances are tuple samples from a single (partially observed) partial order, great care is needed

1038

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

in designing the training, development and testing folds. To use unambiguous short subscripts, we call them learn, dev and eval folds, each with positive and negative instances L+, L-, D+, D-, E+, E-. Let T be the raw set of tuples (x  t or t1  t2). The transitive closure (TC) of T , denoted clo(T ), includes all tuples implied by T via transitivity.
5.1 OE protocol
Vendrov et al. [17] followed this protocol:
(1) Compute clo(T ). (2) Sample positive eval fold E+  clo(T ). (3) Sample positive learn fold L+  clo(T )\E+. (4) Sample positive dev fold D+  clo(T ) \ (E+  L+). (5) Generate negative eval, learning and dev folds, E-, L- & D-
(see below). (6) Return L+, L-, D+, D-, E+, E-. A negative tuple is generated by taking a positive tuple (x, y) and perturbing either of them randomly to (x, y) or (x , y), where x , y are sampled uniformly at random. In OE negative folds were the same size as positive folds.
The WordNet [8] hypernymy data set used by Vendrov et al. [17] has |T | = 82115 and | clo(T )| = 838073. E+ and D+, sampled from clo(T ), had only 4000 tuples each. All remaining tuples were in the learn fold. Vendrov et al. [17] freely admit that "the majority of test set edges can be inferred simply by applying transitivity, giving [them] a strong baseline." They reported that the TC baseline gave a 0/1 accuracy of 88.2%, Gaussian embeddings [18] was at 86.6%, and OE at 90.6%.

1.0

0.8

Test fold F1

0.6

0.4

0.2
0.0 0

Order embedding (OE) Transitive closure

200000 400000 |Train fold|

600000

Figure 1: A large fraction of test instances can be inferred by simply computing the transitive closure of the training fold in the OE protocol.

Instead of 0/1 accuracy, Figure 1 shows the more robust F1 score on test instances achieved by transitive closure and OE, as the size of training data is varied. Vendrov et al. [17] reported accuracy near the right end of the scale, where OE has little to offer beyond TC. In fact, OE does show significant lift beyond TC when training data is scarce. As we shall see, even with ample training data,  OE and rectangle embeddings improve on OE.

5.2 Sanitized OE protocol
Clearly, evaluation results must be reported separately for instances that cannot be trivially inferred via TC, where the algorithm needs discover a suitable geometry from the combinatorial structure of clo(T ) beyond mere reachability. To this end, we propose the following sanitized protocol.
(1) Sample positive learn fold L+  clo(T ). (2) Negative learn fold L- of size |L+| is generated by repeating
as needed: (a) Sample (u, v)  L+ uniformly. (b) Perturb one of u or v to get (u, v ). (c) If (u, v )  clo(T ), discard. (3) Sample positive dev fold D+  (clo(T ) \ clo(L+)). (4) Discard (u, v) from D+ if u or v not found in L+  L- (ex-
plained below). (5) Sample positive eval fold E+  (clo(T ) \ (clo(L+)  clo(D+))). (6) Discard elements from E+ using the same protocol used to
discard elements from D+. (7) Generate negative dev and eval folds, D- and E-, using the
same protocol used to generate L- from L+. An entity or type never encountered in the learn fold cannot be embedded meaningfully (unless corpus information is harnessed, see Section 7), so it is pointless to include in dev or eval folds instances that contain such entities or types. Such sampled instances are discarded. To fill folds up to desired sizes, we repeatedly sample pairs until we can retain enough instances.
6 EXPERIMENTS
Data set: We prepare our data set similar to Vendrov et al. [17]. WordNet [8] gives 82115 (hypernym, hyponym) pairs which we use as directed edges to construct our KG. The WordNet noun hierarchy is prepared by experts, and is also at the heart of other type systems [9, 15] used in KG completion and information extraction. We augment the KG by computing its transitive closure, which increases the edge count to 838073. Then we use the two protocols in Section 5 to create training, dev and test folds. The sanitized protocol produces 679241 positive and 679241 negative training instances, 4393 positive and 4393 negative dev instances, and 4316 positive and 4316 negative test instances. These sizes are close to those of Vendrov et al. [17].
Code and hyperparameter details: OE and our enhancements,  OE and rectangle embeddings, were coded in Tensorflow with Adam's optimizer. Hyperparameters, such as batch size (500), initial learning rate (0.1), margin  and stiffness  , were tuned using the dev fold. Optimization was stopped if the loss on the dev fold did not improve more than 0.1% for 20 consecutive epochs. All types and entities were embedded to D = 50 dimensions.
Results: Vendrov et al. [17] reported only microaveraged 0/1 accuracy (`Acc'). Here we also report average precision (AP), recall (R), precision (P) and F1 score, thus covering both ranking and set-retrieval objectives. AP and R-P curves are obtained by ordering test instances by the raw score given to them by OE,  OE, and rectangle embeddings. Table 1 compares the three systems after using the two sampling protocols to generate folds.

1039

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

OE protocol

OE

 OE Rect

Acc 0.922 0.921 0.926

AP 1

1

1

P 0.994 0.915 0.973

R 0.850 0.929 0.877

F1 0.916 0.922 0.923

Sanitized OE protocol OE  OE Rect 0.574 0.742 0.767 0.977 0.969 0.986 0.987 0.925 0.983 0.151 0.527 0.544 0.262 0.671 0.700

Table 1: Performance of OE,  OE, and rectangle embeddings under the OE protocol and the sanitized protocol, on the WordNet hypernymy relation.

It is immediately visible that absolute performance numbers are very high under the original OE protocol, for reasons made clear earlier. As soon as the OE protocol is replaced by the sanitized protocol, no system is given any credit for computing transitive closure. The 0/1 accuracy of OE drops from 0.922 to 0.574. F1 score drops even more drastically from 0.916 to 0.262. In contrast,  OE and rectangle embeddings fare better overall, with rectangle embeddings improving beyond  OE.

1.0

0.9

0.8

Precision

0.7

OE

0.6

sigmaOE

Rect

0.5

0.0

0.2

0.4

0.6

0.8

1.0

Recall

Figure 2: Recall-precision profiles on WordNet.

Whereas  OE and rectangle embeddings improve on OE at the task of set retrieval, their ranking abilities are slightly different. Figure 2 shows that  OE is inferior at ranking to both OE and rectangle embeddings. Rectangle embeddings have the best precision profile at low recall. Modifying our code to use ranking-oriented loss functions [3] may address ranking applications better.

7 CONCLUDING REMARKS
Here we have addressed the problem of completing e  t and t1  t2 relations starting from an incomplete KG, but without corpus support. For out-of-vocabulary (not seen during training) entities, mention contexts in a corpus are vital typing clues [6, 14, 20]. We plan to integrate context (word) embeddings with order and rectangle embeddings. It would be of interest to see how our refined loss objectives and testing protocols compare with other corpus-based methods [4, 22].
Acknowledgment: Thanks to Aditya Kusupati and Anand Dhoot for helpful discussions, and nVidia for a GPU grant.

REFERENCES
[1] Ben Athiwaratkun and Andrew Gordon Wilson. 2018. On Modeling Hierarchical Data via Probabilistic Order Embeddings. In ICLR. https://openreview.net/foru m?id=HJCXZQbAZ
[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NIPS Conference. 2787­2795. http://papers.nips.cc/paper/5071-translati ng- embeddings- f or- modeling- multi- relational- data.pdf
[3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In ICML. 129­136. http://www.machinelearning.org/proceedings/icml2007/papers/139.pdf
[4] Haw-Shiuan Chang, ZiYun Wang, Luke Vilnis, and Andrew McCallum. 2017. Unsupervised Hypernym Detection by Distributional Inclusion Vector Embedding. arXiv preprint arXiv:1710.00880 (2017). https://arxiv.org/pdf/1710.00880.pdf
[5] Shoaib Jameel and Steven Schockaert. 2016. Entity embeddings with conceptual subspaces as a basis for plausible reasoning. arXiv preprint arXiv:1602.05765 (2016).
[6] Xiao Ling and Daniel S Weld. 2012. Fine-Grained Entity Recognition.. In AAAI. http://xiaoling.github.io/pubs/ling- aaai12.pdf
[7] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS Conference. 3111­3119. https://goo.gl/x3DTzS
[8] G Miller, R Beckwith, C FellBaum, D Gross, K Miller, and R Tengi. 1993. Five papers on WordNet. Princeton University. ftp://ftp.cogsci.princeton.edu/pu b/wordnet/5papers.pdf
[9] Shikhar Murty, Patrick Verga, Luke Vilnis, and Andrew McCallum. 2017. Finer Grained Entity Typing with TypeNet. arXiv preprint arXiv:1711.05795 (2017). https://arxiv.org/pdf /1711.05795.pdf
[10] Maximillian Nickel. 2013. Tensor factorization for relational learning. Ph.D. Dissertation. Ludwig­Maximilians­Universität, München. https://edoc.ub.un i- muenchen.de/16056/1/NickelM aximilian.pdf
[11] Maximillian Nickel and Douwe Kiela. 2017. Poincaré embeddings for learning hierarchical representations. In NIPS Conference. 6341­6350. http://papers.nips.cc/paper/7213- poincare- embeddings- f or- learning- hie rarchical- representations.pdf
[12] Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. 2016. Holographic Embeddings of Knowledge Graphs. In AAAI Conference. 1955­1961. https: //arxiv.org/abs/1510.04935
[13] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global Vectors for Word Representation.. In EMNLP Conference, Vol. 14. 1532­ 1543. http://www.emnlp2014.org/papers/pdf/EMNLP2014162.pdf
[14] Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2016. An Attentive Neural Architecture for Fine-grained Entity Type Classification. arXiv preprint arXiv:1604.05525 (2016). https://arxiv.org/pdf/1604.05525.pdf
[15] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia. In WWW Conference. ACM Press, 697­706. http://www2007.org/papers/paper391.pdf
[16] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. 2016. Complex embeddings for simple link prediction. In ICML. 2071­ 2080. http://arxiv.org/abs/1606.06357
[17] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2015. Orderembeddings of images and language. arXiv preprint arXiv:1511.06361 (2015). https://arxiv.org/pdf /1511.06361
[18] Luke Vilnis and Andrew McCallum. 2014. Word representations via gaussian embedding. arXiv preprint arXiv:1412.6623 (2014).
[19] Qizhe Xie, Xuezhe Ma, Zihang Dai, and Eduard Hovy. 2017. An Interpretable Knowledge Transfer Model for Knowledge Base Completion. arXiv preprint arXiv:1704.05908 (2017). https://arxiv.org/pdf/1704.05908.pdf
[20] Yadollah Yaghoobzadeh and Hinrich Schütze. 2015. Corpus-level Fine-grained Entity Typing Using Contextual Information. In EMNLP Conference. 715­725. http://aclweb.org/anthology/D15- 1083
[21] Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2017. Learning Distributed Representations of Texts and Entities from Knowledge Base. arXiv preprint arXiv:1705.02494 (2017). https://github.com/studio-ousia/ntee
[22] Josuke Yamane, Tomoya Takatani, Hitoshi Yamada, Makoto Miwa, and Yutaka Sasaki. 2016. Distributional Hypernym Generation by Jointly Learning Clusters and Projections. In COLING. 1871­1879. http://www.aclweb.org/antholog y/C16- 1176

1040


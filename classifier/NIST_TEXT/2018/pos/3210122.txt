Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A Study of Per-Topic Variance on System Comparison

Meng Yang
School of Computer Science and Technology
Tianjin University mirandayang1213@163.com

Peng Zhang 
School of Computer Science and Technology
Tianjin University pzhang@tju.edu.cn

Dawei Song
School of Computer Science and Technology
Beijing Institute of Technology dawei.song2010@gmail.com

ABSTRACT
Under the notion that the document collection is a sample from a population, the observed per-topic metric (e.g., AP) value varies with di erent samples, leading to the per-topic variance. The results of the system comparison, such as comparing the ranking of systems according to the summary metric (e.g., MAP) or testing whether there is signi cant di erence between two systems, are a ected by the variability of per-topic metric values. In this paper, we study the e ect of per-topic variance on the system comparison. To measure such e ects, we employ two ranking-based methods, i.e., Error Rate (ER) and Kendall Rank Correlation Coe cient (KRCC), as well as two signi cance test based methods, namely Achieved Signi cance Level (ASL) and Estimated Di erence (ED). We conduct empirical comparison of TREC participated systems on Robust and Adhoc track, which shows that the e ect of per-topic variance on the ranking of systems is not obvious, while the signi cance test based comparisons are susceptible to the per-topic variance.

However, Cormack and Lynam proposed [1] that the document collection is a sample from a population of a large document collection. For di erent document collection samples, the per-topic metric values can be di erent. On the basis of this notion, the pertopic variance is generated. The per-topic variance may have e ect on system comparison. For example, the ranking of systems or the result of signi cance test between systems varies with di erent samples of the document collection population. We employ two kinds of methods to measure this e ect. The Error Rate (ER) and Kendall Rank Correlation Coe cient (KRCC) are utilized to analyze the consistency of the rankings of systems. On the other hand, Achieved Signi cance Level (ASL) and Estimated Di erence (ED) are used to compute the signi cance test results that re ects the statistical di erence between two systems. In the premise that document collection is a sample, this work seeks to explore the reliability of the evaluation results on the aspects of the ranking performance and the signi cance test.

KEYWORDS
Evaluation; Per-Topic Variance; System Comparison
ACM Reference Format: Meng Yang, Peng Zhang, and Dawei Song. 2018. A Study of Per-Topic Variance on System Comparison. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210122
1 INTRODUCTION
In traditional Cran eld-Style experiment, the evaluation is based on the xed document collection, a set of topics and the relevance judgment. Given a system-topic pair, the value of a per-topic evaluation metric (e.g., Average Precision, AP) is xed. When comparing two systems, the values of summary metric concerning all topics (e.g., Mean Average Precision, MAP) are used to rank systems. The signi cance test is conducted to judge whether there is signi cant di erence between two systems based on all the per-topic metric values.
Corressonding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210122

2 RELATED WORK
Under the condition of Cran eld-Style experiment, the variance of a system usually originates from the di erent topics or query variations. Zhang et al. proposed the bias-variance evaluation to study the variance of ranking performance across topics for the query language modeling approaches [13, 14] and the TREC participated systems [12].
Under the notion of document collection being a sample from a population, the variance can be generated from the di erent document collection samples. Cormack and Lynam tested the testcollection variability on AP and MAP [1]. Robertson studied the reverse relationship between recall and precision [4] and re-examined the common metrics in IR from the view of this notion [3]. In addition, Robertson et al. explored the per-topic noise caused by di erent document collections [6] and presented some insights to model score distribution without real scores [5].
However, the focus of this study is the e ect of per-topic variance on performance comparison among systems. The methods we used are inspired by the prior work. Voorhees [10] de ned ER to represent the probability of one experiment that leads to the wrong conclusion. KRCC is used to compare the rankings of systems using di erent metrics [2, 8]. Sakai employed ASL to compare metrics' sensitivity and ED to estimate the performance di erence for guaranteeing ASL <  (signi cance level) [7]. The application of these existing methods was on the Cran eld-Style experiment where document collection is xed. In this paper, however, these methods are not applied into the Cran eld-Style setup. Instead, we modify these methods to study the e ect of per-topic variance when the document collection is a sample.

1181

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

3 SYSTEM COMPARISON BASED ON PER-TOPIC VARIANCE
The implementation of document collection being a sample is through the method of simulation, such as bootstrap (BST), Kernel Density Estimation (KDE) and etc. In Section 3.1, these two models are brie y reviewed. The methods of comparing retrieval systems are introduced in Section 3.2.

3.1 Simulation Models
Given a system-topic pair, suppose that there are 1000 retrieved documents with 1000 scores correspondingly, and a number (denoted as r ) of them are relevant. Following [6], using Poisson distribution with the mean r , we take a sample rs from it. Speci cally, rs simulated scores are taken from the distribution of relevant scores and 1000 - rs simulated scores are from the score distribution of non-relevant ones. These 1000 simulated scores are sorted with a descending order. Then, a binary IR metric can be applied since each score is labeled as relevant or non-relevant.
The BST takes samples from the relevant scores and non-relevant scores with replacement, respectively. The KDE is a non-parametric method to estimate the probability density function. There are different kernels for KDE and the Gaussian kernel is used in our experiment. The relevant and non-relevant probability density functions are derived by tting relevant scores and non-relevant scores to KDE model, respectively. Then, simulated relevant and non-relevant scores are sampled from the relevant and non-relevant probability density functions, independently. Comparisons of these models are:
Non-parametric Distribution These two models assume no distributions for raw relevant and non-relevant scores. Thus, these models have a wider range of adaptability for each system-topic pair simulation.
Use of Real Data or Not The BST employs raw scores directly while the KDE samples scores from a tted score distribution, which indicates the simulated scores according to the BST model may be closer to the raw scores compared with the simulated scores according to the KDE model.

3.2 Methods of Comparing Systems

For two systems, X and Y, let x =(x1, x2, ..., xn ) and y =( 1, 2, ...,

n ) represent the raw metric values on n topics in a topic set Q. In

previous studies, such as [7, 9, 11], the topic in Q is a sample while

the document collection is xed, leading to the xed metric value

for each topic. In this paper, we study the per-topic variance, so that

each topic is xed while the document collection is a sample, where

the per-topic metric values are di erent for di erent samples. For a

simulation model, we take samples of relevant and non-relevant

scores from the corresponding score distributions and the simulated

metric values can be computed, denoted by xi =(x1i , x2i , .., xni ) and

yi =( 1i ,

2i , ..,

i n

),

where

xi

and

yi

represent

the ith

sample.

The methods of comparing systems under the notion of doc-

ument collection being a sample are based on the following two

ideas:

· For each topic, the raw metric value and the simulated metric value can be computed. Systems can be compared using these

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA
Meng Yang, Peng Zhang, and Dawei Song
per-topic raw metric values and simulated metric values, respectively, deriving raw comparison result and simulated comparison result. · The consistency between raw comparison result and simulated comparison result can be measured by methods described below. If two results are consistent, the variance is considered to have less e ect.
Due to the randomness of samples, simulated comparison results may be di erent. Therefore, the comparison can be repeated multiple times.
3.2.1 Error Rate. The assumption of computing ER is that the simulated comparison result about deciding the ranking between two systems should be the same as the counterpart of the raw data, otherwise it is a wrong conclusion. Generally, the larger the simulated summary metric values' di erence is, the smaller the probability of deriving a wrong conclusion will be. Let M(x) and M(y) denote the raw summary metric value for system X and Y, respectively, and the raw performance di erence is denoted as d. M(xi ) and M(yi ) represent the simulated summary metric value for ith sample of system X and Y and the corresponding simulated performance di erence is denoted as di . The performance di erence can be divided into several bins, which is similar with [10]. The algorithm for calculating the ER is shown in Algorithm 1.
ALGORITHM 1: Algorithm for computing error rate.
set Count = 0 and ErrorCount = 0 for each bin; for each system pair (X,Y) do
d = M (x) - M (y); for i = 1 to B do
take the ith sample from the simulation model, respectively, xi and yi ;
compute M (xi ) and M (yi ); di = M (xi ) - M (yi ); Count = Count + 1 for bin corresponding di ; if d  di < 0 then
ErrorCount = ErrorCount + 1 for bin corresponding di ; end end end for each bin do ER = ErrorCount / Count; end
3.2.2 Kendall Rank Correlation Coe icient. KRCC is used to test the similarity of the orderings of two groups of data when the two groups are ranked by each of the quantities. The larger coe cient re ects the more similar rankings of two groups of data. Given a speci c metric, the KRCC between the raw and simulated system ranking is calculated. Suppose that there are m systems, the raw and simulated summary metric values for these systems are denoted as S = (M(x1), M(x2), ..., M(xn )) and Si = (M(xi1), M(xi2), ..., M(xin )) where Si represents the ith sample. The algorithm is shown in Algorithm 2.

1182

Short Research Papers II A Study of Per-Topic Variance on System Comparison

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

ALGORITHM 2: Algorithm for computing KRCC.
compute S; set k[1...B] = 0 ; for i = 1 to B do
take the ith sample from the simulation model for each system; compute Si ; k[i] = kendall(S,Si ); end KRCC = mean(k);

3.2.3 Achieved Significance Level. The previous two methods

are based on the relative ranking of systems. Now, we describe a

signi cance test based method, namely Achieved Signi cance Level

(ASL). Whether there is signi cant di erence between systems can

be re ected by signi cance test. Let z =(z1, z2, ..., zn ) where zi = xi

- i . Under the the null hypothesis, the paired t test statistic is:

t(z) =

z¯
¯

(1)

n

where z¯ is the mean of z and ¯ is the standard deviation of z. Algorithm 3 shows the algorithm for computing ASL. The smaller the

ALGORITHM 3: Algorithm for computing achieved signi cance level.

Count = 0;

for i = 1 to B do

take the ith sample from the simulation model;

t(zi)

=

z¯i

/

( i

/

 n

);

if |t (zi) | > |t12- /2(n - 1) | then

Count = Count + 1;

end

end

ASL = Count/B;

ASL is, the stronger the evidence for that null hypothesis is false is, which means there is signi cant di erence between two systems with a larger probability.
3.2.4 Estimated Di erence. The summary metric values' di erence does not guarantee that there is signi cant di erence between two systems. In order to derive the con dent di erence for guaranteeing the signi cant di erence, the ED was utilized in [7]. The modi ed algorithm is shown in Algorithm 4.
4 EXPERIMENT
4.1 Experimental Set-Up
The data used for experiment are Robust track 2004 and Adhoc track 1999. There are 249 topics and 110 runs for Robust track 2004 and 50 topics and 129 runs for Adhoc track 1999. We use Average Precision (AP) as the per-topic metric. The summary metric is Mean Average Precision (MAP).
4.2 Results
4.2.1 Variability of Estimated Metric Values. The simulated metric values for each system-topic pair on di erent samples are di erent. The per-topic error bar is plotted against the raw metric value

Simulated AP Simulated AP

ALGORITHM 4: Algorithm for estimating the performance di erence at a given signi cance level  .
DIFF = Ø;
for each system pair (X,Y) do sample B samples from the simulation models; compute t(zi ) for i = 1 to B; sort |t (zi ) | for i = 1 to B; if |t (zi ) | is the B largest value then DIFF = DIFF  zi ;
end
end ED = max(di  DIFF)

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Raw AP

Raw AP

(a) BST

(b) KDE

Figure 1: The error bar of BST and KDE for a speci c run of Robust track 2004.

for a speci c run of Robust track 2004 in Figure 1. The standard deviations of per-topic metric values for two models are similar. The per-topic variance is small because of the similarity among samples. The simulated value is close to the raw value when the raw value is small and vice versa. The same pattern exists for other runs of two data sets. Thus, these two models basically simulate the raw values well and have a wider range of adaptability. In comparison with the KDE model, the per-topic metric value of BST simulation is closer to the observed value, which indicates that the BST model simulates the raw data better than KDE model.
4.2.2 Error Rate. We set B = 10000 in Algorithm 1 and the number of bins is 21. In the rst bin, 0 < di < 0.01; In the second bin, 0.01  di < 0.02;..., and di  0.2 in the last bin. The results of ER for each model on two data sets are shown in Figure 2. For Robust track 2004, the ER of the BST is higher than that of KDE in the rst seven bins while the ER of these two models are close to 0 for the last bins. The ER is decreasing when the bin is increasing, which indicates that the smaller the di erence between two sample means is, the larger the probability of deriving the wrong conclusion is. Although the ER on Adhoc 1999 is larger than that on Robust track 2004, the trends of these two ER lines are similar. This further indicates that the smaller the di erence between two simulated summary metric values is, the larger the probability of deriving the wrong conclusion will be.
4.2.3 Kendall Rank Correlation Coe icient. We set B = 1000 in Algorithm 2. The KRCC between the systems' simulated ranking

1183

Error Rate Error Rate

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Meng Yang, Peng Zhang, and Dawei Song

0.3 0.25
0.2 0.15
0.1 0.05
0 0

5

10

15

Bins

(a) Robust 2004

KDE BST

20

25

0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
0 0

KDE BST

5

10

15

20

25

Bins

(b) Adhoc 1999

Figure 2: Results of error rate on Robust 2004 and Adhoc 1999.

Table 1: The KRCC, Per and ED for BST and KDE on Robust 2004 and Adhoc 1999.

KRCC PCT(ACL < 0.05) ED

Robust 2004 BST KDE 0.9176 0.8088 0.1041 0.0896 0.3955 0.3273

Adhoc 1999 BST KDE 0.8509 0.8121 0.1783 0.1756 0.4135 0.4111

overall distribution. That is the reason why that PCT(ACL < 0.05) is not close to the raw result and changes obviously on the e ect of per-topic variance.
5 CONCLUSION
Motivated by the recent development of the per-topic variance by considering the document collection is a sample from a population, we propose to study the e ect of the per-topic variance on comparing systems. Four comparison methods are employed. Two ranking-based methods are Error Rate (ER) and Kendall Rank Correlation Coe cient (KRCC), and two signi cance test based methods are Achieved Signi cance Level (ASL) and Estimated Di erence (ED). We discover that the ranking of systems changes a little with the variability of per-topic metric value and the results concerning the signi cance test are obviously a ected by the per-topic variance. In the future, we will study more simulation models and comparison methods and explore the theory behind these discoveries.
6 ACKNOWLEDGMENTS
This work is supported in part by the state key development program of China (grant No. 2017YFE0111900), Natural Science Foundation of China (grant No. U1636203, 61772363), and the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 721321.

and the raw ranking is listed in the rst row of Table 1. We can see that the coe cient value of BST is larger than that of KDE. All the values are larger than 0.8, which represents that the simulated ranking of systems is similar to the raw ranking.
4.2.4 Achieved Significance Level and Estimated Di erence. Algorithm 3 re ects that whether there is signi cant di erence between two systems. The smaller ASL indicates that the larger probability of null hypothesis false. We assume that if ASL < 0.05 there is signi cant di erence between two systems. For Robust track 2004, there are 110*109*0.5 system comparisons and each system pair comparison is repeated B = 10000 times. The percent of ACL < 0.05 (denoted as PCT(ACL < 0.05)) of system comparisons displays in the second row of Table 1. The power of discriminating di erent systems of two models from high to low is BST and KDE. The gures signify about 10 percent system is signi cantly di erent. The percent value on raw systems' comparison is 0.8148, which is much larger than the counterpart of simulation models. The third row shows the ED for guaranteeing the signi cant di erence between two systems, which is pretty large. Similar results are observed for Adhoc 1999.
4.2.5 Summary. According to the low ER and high KRCC, the variability of ranking of systems is small if the per-topic value
uctuates within a certain range or the per-topic variance is small. The reason is that the ranking is based on the summary metric MAP. When some per-topic metric values become larger and some become smaller, the overall change of MAP is not obvious. The signi cance test makes a hypothesis about the overall distribution in advance and then judges whether the sample is from the overall distribution. The variability of per-topic metric values for all topics causes that the sample distribution may not be consistent with the

REFERENCES
[1] Gordon V Cormack and Thomas R Lynam. 2006. Statistical precision of information retrieval evaluation. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 533­540.
[2] Jaana KekÃlÃinen. 2005. Binary and graded relevance in IR evaluationsâComparison of the e ects on ranking of IR systems. Information Processing & Management 41, 5 (2005), 1019­1033.
[3] Stephen Robertson. 2007. On document populations and measures of IR e ectiveness. In Proceedings of ICTIR 2007. 9­22.
[4] Stephen Robertson. 2007. On score distributions and relevance. Advances in Information Retrieval (2007), 40­51.
[5] Stephen Robertson, Evangelos Kanoulas, and Emine Yilmaz. 2013. Modelling score distributions without actual scores. In Proceedings of the 2013 Conference on the Theory of Information Retrieval. ACM, 20.
[6] Stephen E Robertson and Evangelos Kanoulas. 2012. On per-topic variance in IR evaluation. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. ACM, 891­900.
[7] Tetsuya Sakai. 2006. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 525­532.
[8] Tetsuya Sakai. 2007. On the reliability of information retrieval metrics based on graded relevance. Information Processing & Management 43, 2 (2007), 531­548.
[9] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 1045­1048.
[10] Ellen M Voorhees and Chris Buckley. 2002. The e ect of topic set size on retrieval experiment error. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 316­323.
[11] William Webber, Alistair Mo at, and Justin Zobel. 2008. Score standardization for inter-collection comparison of retrieval systems. (2008), 51­58.
[12] Peng Zhang, Linxue Hao, Dawei Song, Jun Wang, Yuexian Hou, and Bin Hu. 2014. Generalized Bias-Variance Evaluation of TREC Participated Systems. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management (CIKM '14). 1911­1914.
[13] Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. 2013. Bias-variance Decomposition of Ir Evaluation. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '13). 1021­1024.
[14] Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. 2014. Bias-variance Analysis in Estimating True Query Model for Information Retrieval. Inf. Process. Manage. 50, 1 (Jan. 2014), 199­217.

1184


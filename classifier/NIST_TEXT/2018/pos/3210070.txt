Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

ery Performance Prediction using Passage Information

Haggai Roitman
IBM Research AI Haifa, Israel
haggai@il.ibm.com

ABSTRACT

2 RELATED WORK

We focus on the post-retrieval query performance prediction (QPP) task. Speci cally, we make a new use of passage information for this task. Using such information we derive a new mean score calibration predictor that provides a more accurate prediction. Using an empirical evaluation over several common TREC benchmarks, we show that, QPP methods that only make use of document-level features are mostly suited for short query prediction tasks; while such methods perform signi cantly worse in verbose query prediction settings. We further demonstrate that, QPP methods that utilize passage-information are much better suited for verbose settings. Moreover, our proposed predictor, which utilizes both documentlevel and passage-level features provides a more accurate and consistent prediction for both types of queries. Finally, we show a connection between our predictor and a recently proposed supervised QPP method, which results in an enhanced prediction.
1 INTRODUCTION
We focus on the post-retrieval query performance prediction (QPP) task [4]. Given a query, a corpus and a retrieval method that evaluates the query, our goal is to predict the query's performance based on its retrieved result list [4].
Motivated by previous works on document retrieval using passage information [1­3], we propose to use such information for the post-retrieval QPP task as well. To this end, we extend Kurland et al.'s probabilistic QPP framework [10] and show how passage information may be utilized for an enhanced prediction.
Using an evaluation with several TREC corpora, we rst demonstrate that, existing state-of-the-art document-level post retrieval QPP methods, are mostly suited for prediction tasks that involve short (and probably more ambiguous) queries; whereas such methods are less suited for prediction tasks that involve verbose (long and probably more informative) queries. We next demonstrate that, our proposed QPP method which makes use of passage information provides a more robust prediction, regardless of query type. We further set a direct connection with Roitman et al.'s mean retrieval score estimation framework [12]. Moreover, by integrating our proposed passage-information QPP signal as an additional calibration feature within Roitman et al.'s framework [12], we are able to achieve the overall best QPP accuracy.

The query performance prediction task has been extensively studied, where two main approaches have been proposed, either preretrieval or post-retrieval prediction [4]. Yet, most of previous QPP research has focused on ad-hoc retrieval prediction tasks that involved short (keyword-based) queries [4]. In this work, we further study QPP in cases with verbose (long) queries [8].
Passage information has been shown to assist in ad-hoc document retrieval [2, 3] and verbose query evaluation tasks [1]. Motivated by these previous works, in this work, we rely on passage information as a strong evidence source for query performance. A couple of previous works [6, 9] have predicted the outcome of passage retrieval for question answering tasks. Yet, as we shall later show, predictors that were suggested for the passage retrieval QPP task are less suited for the document retrieval QPP task.
Our work extends Kurland et al.'s [10] probabilistic QPP framework with passage information. Finally, Roitman et al. [12] have recently proposed an extension to [10], where the authors derived a generic calibrated (discriminative) mean retrieval score estimator for post-retrieval QPP tasks. Using their proposed predictor (dubbed WPM2), the authors achieved the best reported QPP accuracy [12]. In this work, we further show a connection between our proposed passage-based QPP method and Roitman et al.'s [12] framework, where we utilize it to provide overall better prediction.
3 FRAMEWORK
3.1 Preliminaries
Let q denote a query and let C denote a corpus on which the query is evaluated. For a given text x (e.g., a document d  C or a passage
 d), let sq (x) denote the (retrieval) score assigned to x given q. In this work, we estimate each query q's performance using a post-retrieval approach [4]. Accordingly, let D denote the top-k documents in C with the highest retrieval score sq (d) as determined by the underlying retrieval method. The post-retrieval QPP task is to estimate p(D|q, r ) ­ the likelihood that D contains relevant information to query q [4].

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210070

3.2 QPP using passage information
Our goal is to predict a given query q's performance as accurate as possible. To this end, we propose to utilize passage information extracted from documents in D as an additional source for QPP. Our main hypothesis is that, relevant passage information obtained in D may provide valuable evidence whether a given retrieval was (overall) e ective or not.

893

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Our prediction framework is built on top of Kurland et al.'s [10]
probabilistic QPP framework. According to [10], p(D|q, r ) may be estimated1 as follows:

p^d

oc

(D

|q,

r

)

def
=

p(D|d, r )p(d |q, r ).

(1)

d D

p(d |q, r ) denotes document d's likelihood of being a relevant re-

sponse to query q. p(D|d, r ) denotes the likelihood that such rele-

vant response will be further included in D, which we assume in

this work to be uniformly distributed. Applying this assumption,

our QPP estimator can be de ned as follows:

p^ps

(D

|q,

r

)

def
=

p(d |q, r ).

(2)

d D

We next show how p^ps (D|q, r ) can be estimated using passage-

information. As a rst step, we note that:

p(d

|q,

r

)

def
=

p(q|d, r )p(r |d)p(d) p(r |q)p(q) .

(3)

The term p(r |d) denotes the likelihood that document d contains relevant information regardless of any speci c query. Using a MaxPs estimation approach [2], we now estimate this term as follows:

p^(r

|d )

def
=

max sq (

)p(r |

)p(

|d ).

(4)

d

sq ( ) is the query score assigned to passage ( d). p(r | ) represents the likelihood that passage contains relevant information.

We estimate this term as a combination of two sub-terms as fol-

lows:

p^(r |

)

def
=

H(

) · posBias(

).

def
H( ) = -

p(w | ) log p(w | ) is the entropy of passage 's

w

unsmoothed language model ­ preferring more diverse passages.

posBias(

)

def
=

1+

1 log(2+

.s) , where

.s denotes the start position

(in character o sets) of passage within its containing document.

Hence, posBias( ) prefers passages that are located as earlier as

possible within their containing documents.

p( |d) in Eq. 4 further captures the relationship between passage

and its containing document d, estimated as the Bhattacharyya

similarity between their unsmoothed language models:

p^(

|d )

def
=

p(w | )p(w |d).

w

We

next

assume

that,

p(q)

is

uniformly

distributed;

p (d )

def
=

1 |D |

is uniformly distributed over D;

and

sq

(d

)

def
=

p(q|d, r ).

Applying these assumptions back into Eq. 3 and using our deriva-

tion of p^(r |d) according to Eq. 4, we obtain our new estimator ac-

cording to Eq. 2 as follows:

p^ps

(D

|q,

r

)

def
=

1 |D|

max sq ( )p^(r | )p^( |d)

d

sq (d) ·

p(r |q)

. (5)

d D

1See Eq. 3 in [10].

Finally, we note that, similarly to many post-retrieval predic-

tors [4], p(r |q) is a query-sensitive normalization term, estimated

in

this

work

according

to

q's

length:

p^(r

|q)

def
=

|q |.

3.3 Connection with the WPM method

We conclude this section by showing that our proposed passageenhanced QPP method shares direct connection with the recently proposed mean retrieval score estimation framework of Roitman et al. [12]. According to [12], many previous post-retrieval predictors (e.g., Clarity [5], WIG [15], etc) share the following general form:

p^(D

|q,

r

)

def
=

1 |D|

d D

sq (d)

·

r , F

(d ),

where r, F (d)

def
=

fj (d) j is a Weighted Product Model

j

discriminative calibrator; with fj (d) is some retrieval feature and

j  0 denotes its relative importance [12].

According to Eq. 5, our predictor is essentially a calibrated mean

retrieval estimator [12]; Our predictor utilizes two calibration fea-

tures, namely

f1 (d )

= p^(r |d) (see Eq. 4) and

f2 (d )

=

1 p^(r |q)

;

both

features are assigned with equal weights of 1 = 2 = 1. Using this

connection in mind, we make two important observations. First,

by calibrating the two feature weights i ; i  {1, 2}, we might improve our own predictor's performance [12]. Second, we can utilize

f1(d) as a new passage-based calibration feature within Roitman et al.'s [12] QPP framework. As we shall shortly demonstrate, such

an extension indeed signi cantly boosts prediction performance;

even in cases where the prediction quality was already relatively

high.

4 EVALUATION 4.1 Datasets

Corpus #documents

Queries

Disks

TREC5

524,929

251-300

2&4

WSJ

173,252

151-200

1-2

AP

242,918

51-150

1-3

ROBUST 528,155 301-450, 601-700 4&5-{CR}

WT10g 1,692,096

451-550

WT10g

GOV2 25,205,179

701-850

GOV2

Table 1: Summary of TREC benchmarks.

The details of the TREC corpora and queries that we used for the evaluation are summarized in Table 1. These benchmarks were used by many previous QPP works [4, 12]. We evaluated two main types of queries, namely: short (keyword) queries and verbose queries. To this end, for short queries evaluation, we used the titles of TREC topics. For verbose queries evaluation, we further used the TREC topic descriptions, following [1, 8]. We used the Apache Lucene2 open source search library for indexing and searching documents. Documents and queries were processed using Lucene's English text analysis (i.e., tokenization, Porter stemming, stopwords, etc.).
2http://lucene.apache.org

894

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

As the underlying retrieval method we used Lucene's Dirichletsmoothed query-likelihood implementation, with its Dirichlet parameter xed to µ = 1000, following [10, 12].
4.2 Baselines
We compared our proposed QPP method (hereinafter referred to as PI3) with several di erent baseline QPP methods, as follows.
As a rst line of baselines, we compared with Clarity [5], WIG [15] and NQC [13], which are commonly used document-level postretrieval QPP methods [4]. The Clarity [5] method estimates query performance proportionally to the divergence between the relevance model [11] induced from D and the background model induced from C. The WIG method [15] estimates query performance according to the di erence between the average retrieval score in D and that of C. The NQC [13] method estimates query performance according to standard deviation of the retrieval scores of documents in D, further normalized by the corpus score sq (C).
As alternative QPP methods that also utilize passage information, we closely followed [6, 9] and implemented the passage-level counterparts of the three former baselines; denoted Clarity(psg) [6, 9], WIG(psg) [9], NQC(psg) [9], respectively.
As a very strong document-level baseline, we further compared with the WPM2 method [12]. WPM2 utilizes 10 di erent document retrieval score calibration features4, whose weights need to be learned [12].
Following the rst observation we made in Section 3.3, we also implemented a calibrated version of our PI method (denoted C-PI). Finally, following the second observation we made in Section 3.3, we further extended the WPM2 method with our new passagelevel calibration feature (denoted WPM2+PI).

4.3 Setup

We evaluated the various methods using two di erent query set-

tings: once using the short (title) queries and once using the ver-

bose (description) queries. On each setting, we predicted the perfor-

mance of each query based on its top-1000 retrieved documents [4].

Following a common practice [4], we assessed prediction over queries

quality according to the Pearson's- correlation between the pre-

dictor's values and the actual average precision (AP@1000) values

calculated using TREC's relevance judgments.

In order to realize our predictor in Eq. 5, according to Eq. 4, for

each document d( D), we need to obtain the passage  d with

the highest likelihood (score) p^(r |d). To this end, given document

d( D), we rst extracted candidate passages from it using a xed

L = 500 characters-windowing approach [14]; and then scored the

candidates according to Eq. 4. We used Okapi-BM25 as our choice

of sq ( ), with k1 = 0.8 and b = 0.3, following [7]. Most of the methods that we evaluated (and among them the PI

and WPM2 variants) required to tune some free parameters. Com-

mon

to

all

methods

is

the

free

parameter k

def
=

|D|, which is the

number of top scored documents (out of a total of 1000 retrieved

documents) to be used for the prediction. To this end, for each

method we selected k  {5, 10, 20, 50, 100, 150, 200, 500, 1000}.

3PI stands for "Passage Information". 4The full list of features is described in Section 5 of [12].

TREC5 WSJ AP WT10g Robust GOV2

Clarity WIG NQC

.490bp .347p .483bp

.607p .677b .718b

.596p .526bp .554bp

.380bp .434bp .486p

.477p .411bp .575b

.407p .535b .432bp

Clarity(psg) WIG(psg) NQC(psg)

.204bp .344p .292bp

.629p .576bp .497b

.622p .397bp .304bp

.289bp .494bp .488p

.477p .435bp .401bp

.395p .468b .220bp

PI C-PI

.567pc .613c

.677 .718c

.684pc .694c

.518pc .532c

.577pc .585c

.465c .501c

WPM2 WPM2+PI

.738w .787w

.725w .738w .743w .764w

.540w .557w

.640w .647w

.655w .661w

Table 2: Results of prediction over short queries. The super-

script b denotes a statistically signi cant di erence between

one of the rst document-level baselines and its passage-

level counterpart. The superscript p denotes a signi cant dif-

ference between PI and the six rst baselines. The subscript

c denotes a signi cant di erence between PI and C-PI. The

subscript w denotes a signi cant di erence between WPM2

and WPM2+PI.

To implement the three passage-level alternatives (i.e., Clarity(psg),WIG(psg)

and NQC(psg)), we rst used the same window-based passage ex-

traction approach [14] and ranked candidate passages extracted

from the various documents in D according to their Okapi-BM25

score with k1 = 0.8 and b = 0.3 [7]. We then used the top-m scored

passages over D for prediction [6, 9], with m  {5, 10, 20, 50, 100, 150, 200}.

For Clarity and Clarity(psg), following [5, 6, 9], we further clipped

the induced relevance model at the top-n terms cuto , with n 

{5, 10, 20, 50, 100, 150, 200}.

To learn the calibration feature weights of C-PI, WPM2 and

WPM2+PI, following [12], we used a Coordinate Ascent approach.

Similar to [12], we selected the feature weights {j }hj=1 in the grid [0, 5]h with a step size of 0.1 within each dimension, with h 

{2, 10, 11} such di erent features implemented within the C-PI,

WPM2 and WPM2+PI methods, respectively. Further following [12],

feature

values

were

smoothed

fj (d; )

def
=

max(fj (d), ), where

 = 10-10 is a hyperparameter.

Following [12, 13], we trained and tested all methods using a

holdout (2-fold cross validation) approach. Accordingly, on each

benchmark, we generated 30 random splits of the query set; each

split had two folds. We used the rst fold as the (query) train set.

We kept the second fold untouched for testing. We recorded the

average prediction quality over the 30 splits. Finally, we measured

statistical signi cant di erences of prediction quality using a two-

tailed paired t-test with (Bonferroni corrected) p < 0.05 computed

over all 30 splits.

4.4 Prediction over short queries
The results of our rst evaluation setting with short queries are summarized in Table 2. First, we observe that, the three rst documentlevel QPP baselines (i.e., Clarity WIG and NQC) and their passagelevel counterparts (i.e., Clarity(psg) WIG(psg) and NQC(psg)) exhibit a mixed relative performance. This serves as a rst evidence that passage-information is an important QPP signal.

895

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

TREC5 WSJ AP WT10g Robust GOV2

Clarity WIG NQC

.254p .011bp .271bp

-.005bp .368bp
.642

.174bp .510p .528bp

.381bp .377p .397bp

.288bp .278bp .461bp

.281bp .303bp .360p

Clarity(psg) WIG(psg) NQC(psg)

.256p .292bp .338bp

.188bp .588bp
.640

.198bp .526p .545bp

.267bp .377p .458bp

.468bp .446bp .499bp

.446bp .343bp .361p

PI C-PI

.512p .528

.655pc .678c

.637pc .689c

.464pc .476c

.559pc .624c

.460pc .482c

WPM2

.732

WPM2+PI .740

.650 .621w .406w .587w .509w .660 .729w .487w .602w .521w

Table 3: Results of prediction over verbose (long) queries.

The superscripts and subscripts notations are identical to

those de ned in Table 2.

verbose query performance prediction settings. Moreover, such a strategy guarantees a more robust QPP, which is less sensitive to query type. Finally, again, we can observe that, by further calibrating PI (i.e., C-PI), even better prediction quality can be achieved. Moreover, the contribution of PI's passage-level calibration feature p^(r |d) to WPM2+PI is even more notable in this setting.
5 CONCLUSIONS
The conclusions of this work are two-fold. First, this work clearly demonstrates that existing post-retrieval QPP methods that only focus on document-level features are not well suited to the prediction of verbose queries performance. Utilizing passage-information for such QPP sub-task is clearly important. As we further demonstrated, a mixed strategy that considers both types of features, such as the one employed by the PI variants, may result in an enhanced prediction, which is less sensitive to query type.

Next, in most cases, our newly proposed method, PI, had a better prediction. We note that, while the former baselines either uti-

REFERENCES
[1] Michael Bendersky and W. Bruce Croft. Modeling higher-order term dependencies in information retrieval using query hypergraphs. In Proceedings of the 35th

lize only document-level or only passage-level features, PI basi-

International ACM SIGIR Conference on Research and Development in Information

cally utilizes both feature types. This, therefore, supports again the importance of passage-level QPP signals for the document-level

Retrieval, SIGIR '12, pages 941­950, New York, NY, USA, 2012. ACM. [2] Michael Bendersky and Oren Kurland. Utilizing passage-based language models
for document retrieval. In Proceedings of the IR Research, 30th European Confer-

QPP task. Furthermore, by calibrating the two "features" of PI ac-

ence on Advances in Information Retrieval, ECIR'08, pages 162­174, Berlin, Hei-

cording to our rst observation in Section 3.3, we obtained an enhanced performance of C-PI over PI.

delberg, 2008. Springer-Verlag. [3] James P. Callan. Passage-level evidence in document retrieval. In Proceedings
of the 17th Annual International ACM SIGIR Conference on Research and Devel-

Overall, WPM2 was the best document-level only QPP method. Yet, further following the second observation we made in Section 3.3,

opment in Information Retrieval, SIGIR '94, pages 302­310, New York, NY, USA, 1994. Springer-Verlag New York, Inc. [4] David Carmel and Oren Kurland. Query performance prediction for ir. In Pro-

we can observe that, by solely adding the single passage-level fea-

ceedings of the 35th International ACM SIGIR Conference on Research and Develop-

ture of p^(r |d) (see again Eq. 4) to WPM2, which resulted in the WPM2+PI extension, has obtained a signi cant boost in predic-

ment in Information Retrieval, SIGIR '12, pages 1196­1197, New York, NY, USA, 2012. ACM. [5] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query per-

tion quality. This is yet another strong testimony on the impor-

formance. In Proceedings of the 25th Annual International ACM SIGIR Conference

tance of passage-level QPP signals for the document-level QPP task.

on Research and Development in Information Retrieval, SIGIR '02, pages 299­306, New York, NY, USA, 2002. ACM. [6] Steve Cronen-Townsend, Yun Zhou, and W Bruce Croft. Precision prediction

based on ranked list coherence. Information Retrieval, 9(6):723­755, 2006.

4.5 Prediction over verbose queries

[7] Mathias Géry and Christine Largeron. Bm25t: A bm25 extension for focused information retrieval. Knowl. Inf. Syst., 32(1):217­241, July 2012.

The results of our second evaluation setting with verbose (long) queries are summarized in Table 3. First, comparing these results

[8] Manish Gupta and Michael Bendersky. Information retrieval with verbose queries. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '15, pages 1121­1124,

with the results in Table 2, it becomes clear that those baseline methods that only utilize document-level features, perform signif-

New York, NY, USA, 2015. ACM. [9] Eyal Krikon, David Carmel, and Oren Kurland. Predicting the performance of
passage retrieval for question answering. In Proceedings of the 21st ACM Interna-

icantly worse in this setting compared to their performance over

tional Conference on Information and Knowledge Management, CIKM '12, pages

short queries. Moreover, those QPP methods that utilize passagelevel information (i.e., Clarity(psg), WIG(psg), NQC(psg), PI, C-

2451­2454, New York, NY, USA, 2012. ACM. [10] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri
Rom. Back to the roots: A probabilistic framework for query-performance pre-

PI and WPM2+PI) provide signi cantly better prediction. This

diction. In Proceedings of the 21st ACM International Conference on Information

demonstrates that, utilizing passage-information for QPP becomes even more eminent in verbose query settings. Verbose queries are

and Knowledge Management, CIKM '12, pages 823­832, New York, NY, USA, 2012. ACM. [11] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In

usually more informative than short queries [8]; yet, existing document-

Proceedings of SIGIR '01.

level QPP methods are not well-designed to predict their quality.

[12] Haggai Roitman, Shai Erera, Oren Sar-Shalom, and Bar Weiner. Enhanced mean retrieval score estimation for query performance prediction. In Proceedings of

Verbose queries tend to express more focused information needs [8];

the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR

hence, passage-information may provide a better "proxy" to such needs satisfaction within retrieved documents [1].

'17, pages 35­42, New York, NY, USA, 2017. ACM. [13] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits.
Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst.,

Further notable is that, compared to the six rst baseline meth-

30(2):11:1­11:35, May 2012.

ods, PI provided signi cantly better prediction quality; and even exceeded in some of the cases that of WPM2 ­ a very strong

[14] Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of SIGIR '03.

document-level QPP baseline. This serves as another strong evidence that, a mixed document-level and passage-level prediction

[15] Yun Zhou and W. Bruce Croft. Query performance prediction in web search environments. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '07, pages

strategy, such as the one employed by PI, is a better choice for

543­550, New York, NY, USA, 2007. ACM.

896


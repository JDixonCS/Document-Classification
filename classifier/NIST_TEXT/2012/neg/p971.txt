Improving Tweet Stream Classification by Detecting Changes in Word Probability

Kyosuke Nishida, Takahide Hoshide
NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan
{nishida.kyosuke, hoshide.takahide} @lab.ntt.co.jp
ABSTRACT
We propose a classification model of tweet streams in Twitter, which are representative of document streams whose statistical properties will change over time. Our model solves several problems that hinder the classification of tweets; in particular, the problem that the probabilities of word occurrence change at different rates for different words. Our model switches between two probability estimates based on full and recent data for each word when detecting changes in word probability. This switching enables our model to achieve both accurate learning of stationary words and quick response to bursty words. We then explain how to implement our model by using a word suffix array, which is a full-text search index. Using the word suffix array allows our model to handle the temporal attributes of word n-grams effectively. Experiments on three tweet data sets demonstrate that our model offers statistically significant higher topicclassification accuracy than conventional temporally-aware classification models.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications-- Data mining; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Twitter, Document classification, Data streams, Concept drift, Control charts, Suffix arrays
1. INTRODUCTION
Twitter, a micro-blogging service, has emerged as a new information infrastructure. Over 300 million status messages, called tweets, are posted per day by users all over the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

Ko Fujimura
Otsuma Women's University Tokyo, Japan
fujimura@otsuma.ac.jp
world. The demand for classifying tweets in order to organize the massive volume of tweets has become more urgent.
Classifying document streams is a challenging problem because their statistical properties will change over time. This characteristic is called concept drift, and it is one of the hottest topics in data mining [2, 13]. There are multiple types of changes, and the effectiveness of strategies for building classifiers depends on the types of changes.
For classifying tweet streams, the trade-off between quick response to bursty words and accurate learning of stationary words is a significant problem. Conventional methods conduct temporal selection or weighting on documents; so, they cannot handle the different time-scale changes expected for each word. We propose a probabilistic classification model, P-Switch, as an extension of the multinomial naive Bayes classifier. In order to resolve the trade-off, our model switches between the two probability estimates based on full and recent data for each word, by monitoring changes in the probability of word occurrence. It implements monitoring by using a control chart [20], which is a statistical process control tool.
We then describe an implementation method that uses a word suffix array [7]. Our classification model can be formalized as searching the positions of word n-grams of a new tweet, in the training documents that are concatenated chronologically. We, therefore, regard the construction of full-text search indexes as a learning procedure. This approach can effectively handle word n-grams, which are highly important information given the 140 character limit of tweets.
Experiments on topic classification were conducted on three real-world tweet data sets that have different characteristics in terms of topics, languages, and changes. Results show that our model offers statistically significant improvements over the standard multinomial naive Bayes classifier and conventional document weighting and selection methods in terms of accuracy and macro F1 measures.
Contributions: (i) analysis of the changes in tweet streams, (ii) proposal of a probabilistic classification model that adapts to and detects changes at word-level, (iii) an implementation approach that uses word suffix arrays for effectively learning temporal factors in word n-grams, and (iv) significantly improved topic classification accuracy for tweet streams.
Outline: §2 describes the problem definitions on document stream classification. §3 presents the results of our analysis and preliminary experiments on tweet streams. §4 explains our proposed model. §5 describes the implementation method. §6 presents the results of the experiments. §7

971

describes related work and §8 summarizes the contributions of this study.
2. CLASSIFYING DOCUMENT STREAMS
We address previous work on document stream classification and explain the scope of this study.
2.1 Problem Definitions
Problem definitions for document stream classification can be roughly classified into three types depending on how the training and test examples are presented:
[P1] Test-Then-Train (Incremental) [6, 35, 11, 38]
At each time step t = 1, 2, . . ., a classifier is given a new document dt, and outputs a class prediction, c^t, for dt. It then updates itself based on the true class label, ct.
[P2] Test-Then-Train (Chunk) [17, 16, 15, 30, 18, 12]
At each t = 1, 2, . . ., a classifier is given a new document set, Dt = {dt,1, dt,2, . . .}, and outputs a class prediction, c^t,j, for each dt,j  Dt. It then updates itself based on the true class label set, Ct = {ct,1, ct,2, . . .}.
[P3] Train-Then-Test [8]
At each t = 1, 2, . . ., a classifier is given a new training document set, Dt = {(dt,1, ct,1), . . .}, and a new test document set, Dt = {dt,1, dt,2, . . .}. It updates itself based on Dt and then outputs c^t,j for each dt,j  Dt.
Temporally-aware classification methods for batch learning have also been proposed [21, 5, 28, 29]: a classifier is given Dt and Dt for all t = 1, 2, . . . . , T . It outputs a class prediction, c^t,j , for each dt,j  Dt, based on (t)Dt.
2.2 Concept Drift
Concept drift means that the statistical properties of data change over time [36, 34], and it creates the most difficult situation in document stream classification. There are three major types of concept drift: [C1] sudden shift, [C2] gradual drift, and [C3] recurring themes.
One example of concept drift is found in the spam mail filtering problem because mail is characterized by skewed and changing class distributions, users' dynamic and growing interests, recurring and periodic themes (e.g., Christmasrelated spam), and intelligent and adaptive adversaries.
2.3 Learning Strategies
Previous studies on handling concept drift in document streams employ instance selection, instance weighting, ensemble, and feature selection methods.
Instance selection methods select and adapt to previous training examples [16, 15, 11, 38]. Sliding window is a typical approach that is used to select the most recent previous examples. Klinkenberg et al. proposed a method for automatically deciding the size of window and demonstrated high classification performance in the topic classification of news articles [15].
Instance weighting methods assume that newer instances are more essential for classifying the current instance [15, 18, 5, 11, 28, 29]. Lebanon et al. proposed a naive Bayes classifier with a weighting scheme for handling temporal document streams [18].

These instance selection and weighting methods will adapt well to C2, but often fail to handle C1, because learning with old data worsens the classification of most recent data when sudden changes occur. There is a trade-off between response quickness, needed for C1, and learning accuracy, needed for C2. For example, using a short window improves response quickness but fails to offer good learning accuracy.
Ensemble methods use a set of classifiers adapted to the current concept [8, 35, 30, 12]. New models are added, and old or poor models are removed, in a fixed period of time or when a decrease in classification accuracy is observed. Ensemble methods are effective for C2 and C3; e.g., Katkis et al. proposed a general framework for tracking recurring contexts using ensemble classifiers and improved email filtering accuracy [12].
Feature selection methods dynamically select the most valuable feature (word) set in order to respond to concept drift, which is characterized by changes to the underlying feature space [6, 35, 11]. Feature selection methods are frequently combined with other methods; e.g., Wenerstrom et al. [35] proposed a feature-adaptive ensemble approach, which uses multiple classifiers with different features.
2.4 Scope of this Study
This study handles the incremental test-then-train problem definition P1, the most basic problem definition, and tackles the trade-off between responding to sudden shift (C1) and gradual drift (C2). We propose a classification model, a new weighting method at the word level, for tweet streams. While conventional instance weighting methods use temporal weighting functions on documents, our model is able to respond to changes in each word. Extending our model to suit ensemble and dynamic feature selection methods in order to handle recurring themes (C3) is our future work.
3. TWEET STREAMS
This section provides the results of analysis and preliminary experiments on the topic classification of tweets, where topics are defined by hashtags.
3.1 Data set
We collected three tweet data sets as shown in Table 1: tweets on 12 Nippon (Japanese) professional baseball teams (NPB), tweets on 7 Japanese television networks (TV), and tweets on 30 major league baseball teams (MLB), by searching hashtags via the Twitter Streaming API (statuses/filter), from 13 September 2011 to 26 September 2011.
These data sets consist of tweets that include a single hashtag; that is, a tweet belongs to a single class. Classes are hashtags on baseball teams for the NPB and MLB data sets, and hashtags on television networks for the TV data set. Retweets (including the tweets containing the string "RT" or "QT"), replies, mentions are not included in these data sets. In the problem definition P1, document d means a tweet text without any hashtags, and class label c means a hashtag. For the NPB and TV data sets, we selected noun, verb, and adjective words by using MeCab 0.98 with IPA dictionary1 for Japanese word segmentation and part of speech labeling. For the MLB data set, we removed stop words and treated punctuation and other non-alphabetic characters as separate tokens.
1http://mecab.sourceforge.net

972

Table 1: Tweet data set. We collected tweets that included a single hashtag from 13 to 26 September 2011

by using Twitter Streaming API. Classes are hashtags on "baseball teams" for NPB and MLB data sets, and

hashtags on "television networks" for TV data set. Each hashtag corresponds to a class label.

Data # Tweets # Classes Language Track Keywords (Hashtags)

NPB 314,210

12

Japanese #dragons, #hanshin, #giants, #swallows, #carp, #baystars, #sbhawks, #seibulions, #chibalotte,

#lovefighters, #orix buffaloes, #rakuteneagles

TV 249,080

7

Japanese #nhk, #etv, #ntv, #tvasahi, #tbs, #tvtokyo, #fujitv

MLB 200,521

30

English #angels, #astros, #athletics, #bluejays, #braves, #brewers, #cubs, #dbacks, #dodgers, #indi-

ans, #mariners, #marlins, #mets, #nats, #orioles, #padres, #phillies, #pirates, #rangers, #rays,

#reds, #redsox, #rockies, #royals, #sfgiants, #stlcards, #tigers, #twins, #whitesox, #yankees

1.0 0.8 0.6 0.4 0.2 0.0 35000

(a) Cosine Similarity

NPB TV MLB

25000

15000

5000 (b) Number of Tweets 0 13 14 15 16 17 18 19 20 21 22 23 24 25 26 September 2011

Figure 1: (a) Cosine similarities between class distributions on 13 September 2011 and all other days. (b) Total number of tweets for each day.

100%

90%

80%

tvtokyo

70%

tvasahi

60%

fujitv

50%

tbs

40%

ntv

30%

etv

20%

nhk

10%

0% 0 2 4 6 8 10 12 14 16 18 20 22 24 [h] Monday, 19 September 2011

Figure 2: Class distribution (ten minute periods) for TV data set, Monday, 19 September 2011.

3.2 Analysis Results
We analyzed tweet streams in terms of the changes in class distribution and word probability, the occurrence of new words, and the effectiveness of considering word n-grams.
3.2.1 Changes in Class Distribution
Fig. 1 shows (a) the cosine similarities between class distributions on 13 September 2011 and all other days and (b) the total number of tweets for each day. We can see that the class distribution on the NPB and MLB data sets fluctuate more than that on the TV data set. The TV data set also fluctuates dramatically as shown in Fig. 2, which shows the class distribution over ten minute periods.
In summary, the class distributions in tweet streams fluctuate constantly and dramatically at micro and macro time

100 80

rakuteneagles orix_buffaloes

lovefighters

chibalotte

(a) Hourly Number of Tweets containing "home-run"

60

seibulions baystars

sbhawks carp

chibalotte

40

swallows

giants

hanshin

dragons

20

0

0 2 4 6 8 10 12 14 16 18 20 22

Sunday, September 18, 2011

500

(b) Daily Number of Tweets

400

baystars

containing "yokohama"

300

200

100

0 13 14 15 16 17 18 19 20 21 22 23 24 25 26 September 2011

Figure 3: (a) Hourly number of tweets containing "home-run" and (b) Daily number of tweets containing "yokohama" from NPB data set.

scales. Classification models have to respond to these abrupt and gradual changes in class distribution.
3.2.2 Changes in Word Probability
Fig. 3 shows (a) the hourly number of tweets containing "home-run" and (b) the daily number of tweets containing "yokohama" from the NPB data set. "Home-run" is a bursty word since its relevant classes change rapidly. For example, a player of the team #chibalotte hit a home-run at 4PM and the word probability for "home-run" increased suddenly (Fig. 3a). On the other hand, "yokohama" is a relatively stationary word. The most relevant class #baystars, referring to the baseball team that has its home ground in Yokohama, does not change (Fig. 3b).
In summary, the statistical properties of tweet topics change at different rates for different words. Conventional instance weighting and selection methods cannot handle these differences.
3.2.3 Occurrence of New Words
In order to analyze the occurrence and change of effective words for classifying tweets, we calculated the chi-squared statistics 2c (see details in [39]) of all words for each class c in each day, and got the daily top 100 words in terms of 2c for each class.
Fig. 4 shows (a) the cosine similarities between the daily top 100 words on 13 September 2011 and all other days and (b) the cumulative number of distinct daily top 100 words. The results of Fig. 4 are averaged over classes.
We can see from Fig. 4b that effective words for classifying

973

1.0 (a) Cosine Similarity
0.8
0.6
0.4

NPB TV MLB

0.2

0.0 1000 800

(b) Cum. Num. of Distinct Words

600

400

200

0 13 14 15 16 17 18 19 20 21 22 23 24 25 26 September 2011

Figure 4: (a) Cosine similarities between daily top 100 words on 13 September 2011 and all other days. (b) Cumulative number of distinct daily top 100 words. These results are averaged over classes.

4500 4000 3500 3000 2500 2000 1500 1000 500
0 100% 80% 60% 40% 20%
0%

(a) Number of Kinds of Word n-grams

Notable n-gram 38.8%
40.8%

53.0% 55.0%

Unigram Bigram Trigram
26.2% 23.2%

46.8% 47.0%

60.6% 47.1%

68.3% 65.0%

(b) Distribution on Top 200 Word n-grams

NPB

TV

MLB

Figure 5: (a) Number of kinds of word n-grams that
occur more than five times in each day. (b) Distribution of top 200 word n-grams in terms of maxc{2c}. These results are averaged over days. Chi-squared statistics maxc{2c} of a notable n-gram is greater than that of the first word of the notable n-gram.

tweets emerge continuously. In particular, the words on the TV data set changes dramatically. Moreover, the daily top 100 words in the TV data set change periodically because of the influence of the day of the week, as shown in Fig. 4a.
In summary, classification models have to handle emerging words as well as recurring and periodic themes effectively.
3.2.4 Effects of Word n-grams
The word context of the tweet--which is highly important information given the 140 character limit--is lost in the bag-of-words model, which assumes that word order is unimportant.
Accordingly, we investigated whether word n-grams were useful for classifying tweets or not. We first calculated the chi-squared statistics maxc{2c} of all word unigrams, bigrams, and trigrams that occurred more than five times in each day. We then found some notable word n-grams, where the maxc{2c} value of a notable word n-gram is greater than that of the first word of the word n-gram (Fig. 5a). For example, from the MLB data set, "white sox" is more suitable

0.56 0.54 0.52 0.50 0.48 0.46 0.55 0.53 0.51 0.49 0.47 0.45 0.45 0.41 0.37 0.33 0.29 0.25
10000

(a) Macro average F1 on NPB

(b) Macro average F1 on TV

(c) Macro average F1 on MLB

50000

100000 Kernel Width

Selection Weighting
Selection Weighting
Selection Weighting 150000

Figure 6: Macro F1 vs. kernel width of instance selection and weighting methods with multinomial naive Bayes classifier (Eqs. (1)­(5))

for identifying the class #whitesox than "white". Moreover, Fig. 5b shows that the distribution on top 200 word n-grams in terms of maxc{2c} contains word bigrams and trigrams (37.6%, 52.0%, 28.1% for the NPB, TV, and MLB data sets).
In summary, considering word sequences can be effective for classifying tweets because tweets are short and limited to 140 characters.

3.3 Preliminary Experiments

We applied a temporally-aware multinomial naive Bayes

classifier, which was proposed by Salles et al. [29], and inde-

pendently, by Lebanon et al. [18], to problem definition P1.

The temporally-aware multinomial naive Bayes classifier

is given by:

Y

c^t  arg max{ pK(c|t) c

pK(wi|c, t) },

(1)

pK pK

Pt (c|t) = 
(wi|c, t) =

wiW (dt)
P =P1tKtP ==h11t(PKt=-1whK(tdh)-[([ctK-)h=(t)c-f]]c,,)(fwc,i)(w)

,

(2) (3)

where [[·]] is 1 if the predicate is true and 0 otherwise, W (dt) is the set of selected words in document dt, and fc, (w) is the frequency of word w in d . Kh(t -  ) is a kernel function such that:

Kh(t -  ) = [[t -  < h]],

(4)

Kh(t -  ) = (1 - (t -  )/h) · [[t -  < h]],

(5)

where h is the width of the kernel. Using Eq. (4) means adopting an instance selection method and using Eq. (5) means adopting an instance weighting method. The proposal of Salles et al. [29] learns a kernel function with a training data set; however, its learning cost for P1 is extremely high. Accordingly, we used the above kernel functions in this preliminary experiment. We note that Lebanon et al. reported that Eq. (5) was the best kernel function in their experiments [18].

974

We evaluated the macro F1 measure of the instance selection and weighting methods while changing the value of h (Fig. 6). Feature selection was conducted by using the chisquared statistics 2c based on documents {d | t - h <  < t}: that is, W (dt) = {w| maxc{2c(w)} > 30.0}.
For the TV data set, decreasing kernel width h improved classification accuracy. This data set exhibited significant and frequent changes in class and word probabilities and new and important words emerged constantly (see Figs. 2 and 4); therefore, learning from newer tweets was important. The instance weighting method achieved better classification accuracy than the instance selection method in which all data within the kernel are equivalent. We note that macro F1 is not a monotonic function of kernel width because of the existence of recurrent themes, as shown in Fig. 6b.
For the NPB and MLB data sets, decreasing the kernel width worsened classification accuracy. The instance selection and weighting methods cannot learn stationary words fully if the kernel width is small. These data sets contained sudden and significant shifts in some words at micro time scales, as shown in Fig. 3a. Responding to such shifts is important for achieving further improvements in classification accuracy.
3.4 Data Characteristics Considered
From the results of data analysis and preliminary experiments, an ideal classification model for handling concept drift in tweet streams should be able to: [A1] respond to changes in class distribution, [A2] respond to changes in word probability, [A3] handle the emergence of new words effectively, [A4] resolve the trade-off between quick response to bursty words and accurate learning of stationary words, [A5] consider word n-grams, and [A6] treat recurring themes effectively. We propose a method that realizes the first five abilities, A1 to A5. A6 lies outside the scope of this study.

4. PROPOSED MODEL
We propose a classification model of tweet streams in which statistical properties will change over time. Our model, called P-Switch, is based on the analysis results shown in §3.

4.1 Outline

At each time step t = 1, 2, . . ., our proposed model out-

puts a class prediction, c^t, for a new given document, dt, as

follows:

Y

c^t  arg max{ p(c|t) c

p(wi|c, wii--n1+1, t) }, (6)

wiW (dt)

where wi is the i-th word in dt, wii--n1+1 is the word sequence from the (i-n+1)-th to (i-1)-th word in dt, and W (dt) is the set of selected words in dt. Our model then updates itself based on the true class label ct.
Our model, Eq. (6), is a temporally-aware version of the multinomial naive Bayes classifier. The following sections describe three essential components: class distribution, word probability, and word n-gram probability.

4.2 Class Distribution
To acquire ability A1, our model incrementally updates the class distribution, p(c|t), based on ct:

p(c|t) = (1 - ) p(c|t-1) + [[ct = c]],

(7)

where [[·]] is 1 if the predicate is true and 0 otherwise. Eq. (7) is the class distribution estimated using an expo-
nentially weighted moving average (EWMA), where  is a smoothing parameter that represents the degree of weighting decrease (0 <  < 1). The weighting for each older data point decreases exponentially, it never reaches zero.

4.3 Word Probability
To acquire abilities A2, A3, and A4, our model estimates the temporally-aware word probability for each word.
First, our model chronologically concatenates documents given until t for each class c, Dc,t.
A standard multinomial naive Bayes classifier estimates the word probability of wi in class c with the maximum likelihood (ML) method,

pML(wi|c, t)

=

P fc(wi)

,

wDc,t fc(w)

(8)

where fc(w) is the frequency of w in Dc,t. Additionally, our model estimates the word probability
with the EWMA method,

X

pEWMA(wi|c, t) =

(1 - )|Dc,t|-j ,

(9)

j Jc (wi )

where Jc(wi) is the position of word wi in Dc,t, Jc(wi) = {j | Dc,t[j] = wi, 1  j  |Dc,t|}, and  is a smoothing parameter of EWMA (0 <  < 1). Eq. (9) is a closed-form

expression of EWMA.

Our model monitors the two probability estimates for each

word wi in class c, and switches between them as follows:

(

p(wi|c, t) =

pEWMA pML

if pEWMA > pML + L c,t , otherwise

p

p

where c,t = pML(1 - pML) /(2 - ).

(10) (11)

Note that Eqs. (10) and (11) mean monitoring the statistical process of wi occurring in Dc,t with a Bernoulli EWMA control chart[31]. EWMA charts are generally used for detecting small shifts in the process mean [20]. The value of L in Eq. (10) determines the upper control limit of the chart, pML + Lc,t. As long as pEWMA is within the control limit, the word occurrence process is assumed to be currently stable (i.e., our model uses pML).
Fig. 7 clarifies that Eq. (9) preserves the probabilistic interpretation. We can see that the EWMA value, pEWMA, tracks the changes in true probability. The EWMA method is able to respond to sudden shifts quickly. On the other hand, the ML method is able to estimate the probability for stationary words more accurately. The ML method, however, is apt to underestimate the probabilities of bursty words. This switching of the two probability estimates for each word enables our model to resolve the trade-off between quick response to bursty words and the accurate learning of stationary words.

4.4 Word n-gram Probability
To acquire ability A5, our model extends the bag-of-words model to the n-gram language model [26]. It uses the absolute discounting method [22, 4] as the smoothing method

975

0.25 Estimated Probability
0.20

0.15 0.10 0.05

True Prob. ML EWMA

0

0

10000

20000

30000

40000

Number of Word Observations

Figure 7: Simulation of estimating probabilities of word occurrence with ML (Eq. (8)) and EWMA (Eq. (9),  = 0.002) methods.

used in estimating the word n-gram probability:

p(wi|c, wii--n1+1, t)

=

max{fc(wii-n+1) - fc(wii--n1 +1)

,

0}

+



· rc(wii--n1 +1 fc(wii--n1 +1)

)

p(wi|c,

wii--n1 +2

,

t),

(12)

where rc(wii--n1+1) is the number of distinct words following word sequence wii--n1+1. The lowest order model in this linear interpolation is p(wi|c, t); therefore, Eq. (12) retains the

temporal effects.

The smoothing of Eq. (8) is also calculated according to

the absolute discounting method:

pML(wi|c,

t)

=

8 >>< >>:

P wDc,t fc(w)
P fc(wi) wDc,t fc(w)

if fc(wi) = 0
, otherwise

(13)

and Eq. (13) is used instead of Eq. (8).

5. IMPLEMENTATION
We can implement our model by constructing full text search indexes (e.g., suffix arrays [19]) for each Dc,t. This study uses the word suffix array (WSA) [7]. WSA enables our model to efficiently get Jc(wii-n+1), the positions of wii-n+1 in Dc,t, for each word n-gram wii-n+1 in each class c.
5.1 Summary of WSA
Let T [1, . . . , l] be a text of length l over a constant-sized alphabet , tokenized into k words by word-delimiters, and let I be the set of alphabet positions at which new words start. The word suffix array A[1, . . . , k] is a permutation of I such that T [A[i-1], . . . , l] < T [A[i], . . . , l] for all 1 < i  k; i.e., the A array represents the lexicographic order of all suffixes, as shown in Fig. 8 (see details in [7]). The set of word positions, B, is obtained as a by-product of constructing WSA. Our model uses B to find Jc(wii-n+1) in Dc,t.
WSA can be constructed in O(l) time and O(k) space when using the linear-time construction algorithms for suffix arrays (e.g., [23]). It can search wii-n+1 (alphabet length of m) quickly with a binary search in O(m log k + fc(wii-n+1)), as in suffix arrays.

T = "ab#a#aa#a#ab#baa#aab#a#aa#baa#" i A B Word-aligned Suffix 1 4 2 a#aa#a#ab#baa#aab#a#aa#baa# 2 22 8 a#aa#baa# 3 9 4 a#ab#baa#aab#a#aa#baa# 4 6 3 aa#a#ab#baa#aab#a#aa#baa# 5 24 9 aa#baa# 6 18 7 aab#a#aa#baa# 7 1 1 ab#a#aa#a#ab#baa#aab#a#aa#baa# 8 11 5 ab#baa#aab#a#aa#baa# 9 27 10 baa# 10 14 6 baa#aab#a#aa#baa#
Figure 8: Word suffix array. `#' is a word boundary. Columns A and B are the set of alphabet and word positions at which word-aligned suffixes start.
Algorithm 1 Learning word n-gram probability with word suffix arrays.
Require: {dt, ct}, document stream 1: for each (dt, ct) do 2: Dct,t  Dct,t-1 + dt // append dt after Dct,t-1 3: construct WSAct from Dct,t 4: end for
5.2 Learning and Classification Procedures
This section shows the learning and classification procedures for word n-gram probability (Eqs. (8)­(13)).
Learning constructs WSAs for each class. Algorithm 1 shows the learning algorithm for a document stream, {dt, ct}. The sequential update of WSA is not supported, so, it reconstructs the WSA of class ct when given a new tweet (dt, ct). (lines 2­3)
For reducing the learning time, it is effective to divide tweets into periods and reconstruct the newest WSA on the most recent tweets, while retaining old WSAs. We note that it is able to obtain Jc(wii-n+1) from the divided WSAs by storing the offset position of each WSA and adding it to the positions of wii-n+1 found by each WSA.
Classification needs the fc(wji ), Jc(wji ), and rc(wji ) values (0  i - j < n) to calculate the word n-gram probability for each word n-gram, wii-n+1, in class c. Algorithm 2 shows the pseudo-code that acquires the above values from a WSA.
Our model uses three heuristics for speed enhancement: (1) caching search results (lines 6­11, 13), (2) caching the initial search interval for the first alphabets [19] (line 3), and (3) reducing the number of character comparisons by remembering the number of matching characters [19] (line 9). Caching the search results of (n-1)-grams enables our model to narrow the initial interval when searching n-grams. The worst case of our approach is O(m log k + fc(wii-n+1)) time; however, Ferragina et al. reported in [7] that the O(m log k+ fc(wii-n+1)) algorithm, which includes heuristics 2 and 3, is faster than the O(m+log k +fc(wii-n+1)) [19] and O(m||+ fc(wii-n+1)) [1] algorithms.
5.3 Implementation Details
We used the original implementation of the WSA construction algorithm2 by Fischer [7] and the sais library3,
2http://ab.inf.uni-tuebingen.de/people/fischer/wordSA.tgz 3http://sites.google.com/site/yuta256/sais

976

Algorithm 2 Acquiring classification parameters for class c.

Require: d = "w1#w2# · · · #wz#" = w1z, new document Require: R(a), search range of WSA for each alphabet a

Require: W (d), set of selected words in d

1: for i = 1 to z do

2: next if wi  W (d)

3: R  R(wi[1])

4: for j = i to i + n - 1 do

5:

s  wji // word (i - j + 1)-gram.

6: if is_cached?(s) = true then

7:

fc(s), Jc(s), rc(s), R(s)  get_cache(s)

8: else

9:

fc(s), Jc(s), rc(s), R(s)  search_by_WSA(s, R)

10:

set_cache(s, fc(s), Jc(s), rc(s), R(s) )

11: end if

12:

break if fc(s) < 0

13:

R  R(s) // narrow next search range.

14: end for

15: end for 16: return {fc(wji ), Jc(wji ), rc(wji )|1  i  z, 0  i-j < n}

0.3 Construction Time [sec]
0.25

0.2

0.15

0.1

0.05

0

0

5

10

15

20

25

30

Number of Alphabets (x 105 )

Figure 9: Construction time of word suffix array on tweets of class #dragons on NPB data set. Total 53,000 tweets, 399,825 words, 2,701,985 alphabets. CPU: Xeon X5680 3.33GHz, Memory: 192GB.

which implements Nong et al.'s induced sorting algorithm [23], as the suffix array construction algorithm used by WSA. We also used Kasai et al.'s algorithm [10] for calculating the longest common prefixes. The character encoding of tweets was UTF-8, and the number of alphabets was 256; i.e., a three-byte Japanese character consisted of three alphabets. Fig. 9 shows the results of constructing WSA on the tweets in class #dragons for the NPB data set: the construction time of WSA was linear to the number of alphabets.
6. EXPERIMENTS
In order to evaluate the impact that our proposed model has on the incremental test-then-train problem definition P1 (see §2), we first evaluated our model components and then compared our model against three conventional versions of multinomial naive Bayes classifiers (standard, instance weighting, and instance selection), by using the three tweet data sets shown in Table 1.
The classifiers used the chi-squared statistics feature se-

lection method [39]: that is, W (dt) = {w| maxc{2c(w)} > 30.0}, and conducted the smoothing of Eq. (13):  = 0.9.

6.1 Effects of Individual Contributions
Our model, P-Switch, is also a temporally-aware version of multinomial naive Bayes. The following sections demonstrate the effects of our model's components.

6.1.1 Class Distribution

In order to evaluate the effects by introducing the temporally-

aware class distribution described in Eq. (7), we evaluated

the Proposal1 model:

Y

c^t  arg max{ p(c|t) c

pML(wi|c, t) },

wiW (dt)

(14)

which extends the standard multinomial naive Bayes classi-

fier, MNB:

Y

c^t  arg max{ p(c) c

pML(wi|c, t) },

wiW (dt)

(15)

while changing smoothing parameter  from 0.001 to 0.5. We can see from Fig. 10 that the Proposal1 model, where
 = 0.01, performed statistically significantly better than MNB (McNemar's test; p < .001). We note that the multinomial naive Bayes classifier that assumes a uniform class distribution performed poorly.

6.1.2 Word Probability

In order to evaluate the effects of switching between the

two probability estimates described in Eq. (10), we evaluated

the Proposal2 model:

Y

c^t  arg max{ p(c|t) c

p(wi|c, t) },

wiW (dt)

(16)

which extends the Proposal1 model, while changing smoothing parameter  from 0.0001 to 0.01. Other parameter settings were:  = 0.01 and L = 0.5 for all data sets.
Fig. 11 shows that the Proposal2 model, when  = 0.002, performed statistically significantly better than the Proposal1 model for all data sets (McNemar's test; p < .001). The switching of the two probability estimates: pEWMA, which is based on recent data, and pML, which is based on data that cover long-term period, enables the Proposal2 model to resolve the trade-off between quick response to bursty words and the accurate learning of stationary words. Moreover, treating class distribution and word probability at different time scales ( and ), unlike instance selection and weighting methods, is effective in classifying tweet streams.
We also note that the best value for  is the same for all three data sets, which have different change characteristics as shown in Figs. 1 and 4.

6.1.3 Word n-gram Probability
In order to evaluate the effects of introducing the temporallyaware word n-gram probability described in Eq. (12), we evaluated our P-switch model defined in Eq. (6), while changing the value of n from 1 (corresponds to the Proposal2 model) to 5. Other parameters settings were:  = 0.01,  = 0.002, L = 0.5 for all data sets.
We can see from Fig. 12 that our model performed best when n = 2, and there were statistical differences (McNemar's test; p < .001) between n = 1 and n = 2 on NPB and

977

Table 2: Overall accuracy and macro F1 results. Bold-faced results were statistically significantly better than other results (McNemar's test; p < .001).

NPB

TV

MLB

Methods Accuracy [%] MacroF1 [%]

Accuracy [%] MacroF1 [%]

Accuracy [%] MacroF1 [%]

MNB

55.87

53.81

50.61

50.71

48.29

43.92

MNB-W 50.57 (-5.396) 47.24 (-6.573) 54.77 (+4.160) 53.96 (+3.256) 33.82 (-14.47) 26.85 (-17.06)

MNB-S 51.57 (-4.298) 48.02 (-5.785) 53.26 (+2.649) 52.53 (+1.822) 38.29 (-10.00) 31.60 (-12.32)

Proposal1 60.52 (+4.652) 56.61 (+2.805) 62.82 (+12.21) 62.11 (+11.40) 56.05 (+7.751) 47.03 (+3.119)

Proposal2 65.87 (+9.998) 61.46 (+7.654) 70.77 (+20.16) 70.22 (+19.52) 60.26 (+11.97) 50.59 (+6.671)

P-Switch 66.90 (+11.04) 62.88 (+9.072) 70.80 (+20.19) 70.31 (+19.60) 60.86 (+12.56) 51.40 (+7.480)

0.65 Macro average F1
0.60 NPB TV MLB
0.55

0.50

0.45

0.40

MNB: Eq.(15)

Proposal1: Eq.(14)

0.35

Uniform ML 0.001

0.01

0.1

1

Smoothing Value for Class Prior

Figure 10: Effects of introducing temporally-aware class distribution, p(c|t). "Uniform" means the multinomial naive Bayes classifier that assumes a uniform class distribution.

0.75 Macro average F1

0.70

NPB

TV

0.65

MLB

0.60

0.55

0.50

Proposal1 0.45

Proposal2: Eq.(16)

ML

0.0001

0.001

0.01

0.1

Smoothing Value for Word Probability

Figure 11: Effects of introducing temporally-aware word probability, p(wi|c, t).  = 0.01 and L = 0.5.

MLB data sets. Larger values of n worsened classification accuracy because the zero frequency problem is serious in higher-order n-gram models [37].
6.2 Overall Results
§6.1 demonstrated that all of our individual contributions are effective in handling concept drift in tweet streams.
We next compared the whole P-Switch model against conventional methods for handling concept drift: a temporallyaware multinomial naive Bayes classifier with the instance weighting method (MNB-W; Eqs. (1) and (5)), and with

0.630

0.625

0.620

0.615

0.610

0.704

0.703

0.702

n.s.

0.701

0.700

0.517

0.514

0.511

0.508 0.505 Proposal2
1

(a) Macro average F1 on NPB

(b) Macro average F1 on TV

(c) Macro average F1 on MLB

P-Switch: Eq.(6)

2

3

4

5

Value of N for Word N-gram

Figure 12: Effects of introducing temporally-aware word n-gram probability, p(wi|c, wii--n1+1, t).  = 0.01,
 = 0.002, and L = 0.5.

the instance selection method (MNB-S; Eqs. (1) and (4)) (see details in §3.3 and [29, 18]). Parameter settings were:  = 0.01,  = 0.002, L = 0.5, and n = 2. The kernel width of MNB-W and MNB-S was h = 10,000, which was the best value for the TV data set. Note that the best value of h for the NPB and MLB data sets was  (corresponds to MNB) as shown in Fig. 6.
Fig. 13 plots macro F1 of the classifiers vs. the training number of tweets (time steps). Table 2 is the overall accuracy and macro F1 of the classifiers after learning all data.
Although the NPB and MLB data sets contain various changes, MNB performed better than MNB-W and MNBS; that is, learning stationary words with sufficient data is more important than responding to bursty words for these data sets. Our model resolved the trade-off between quick response and accurate learning by switching between the two probability estimates for each word. It offered a statistically significant improvement over MNB, up to 11.04% and 12.56% in terms of overall accuracy, and up to 9.072% and 7.480% in terms of macro F1, for the NPB and MLB data sets, respectively (McNemar's test; p < .001).
Considering temporal effects enabled MNB-W and MNBS to perform better than MNB on the TV data set; however, increasing the number of training tweets is not effective for achieving their higher classification accuracy, as shown in Fig. 13b. This is because using the small kernel width yields rapid dropping of old data. MNB, which learns all data, performed worst because the TV data set contained sudden and significant shifts as shown in Fig. 2. Our PSwitch model was able to respond to both the sudden shift and gradual drift; therefore, it achieved statistically signifi-

978

0.65

0.6 0.55

P-Switch MNB-W MNB-S MNB

0.5

0.45
0.4 0
0.75

100000

(a) Cum. Macro average F1 on NPB

200000

300000

0.7 0.65
0.6

P-Switch MNB-W MNB-S MNB

0.55

0.5
0.45 0
0.55 0.5
0.45 0.4
0.35 0.3
0.25 0.2 0

50000

(b) Cum. Macro average F1 on TV

100000

150000

200000 250000

P-Switch MNB-W MNB-S MNB

(c) Cum. Macro average F1 on MLB

50000

100000

150000

Number of Tweets (=Time Step)

200000

Figure 13: Number of tweets vs. cumulative macro F1 on (a) NPB, (b) TV, and (c) MLB data sets.  = 0.01,  = 0.002, L = 0.5, n = 2, and h = 10000.

cant improvements over MNB, up to 20.19% and 19.60% in terms of overall accuracy and macro F1, respectively (McNemar's test; p < .001).
7. RELATED WORK
As related work on classification of tweets, Sriram et al. devised a naive Bayes classifier for identifying tweet types such as news, events, opinions, deals, and private messages based on author information and tweet texts [32]. Sakaki et al. devised a support vector machine for finding tweets related to an event (e.g., earthquake) [27]. Kinsella et al. uses metadata from hyperlinked objects to improve topic classification [14]. The above studies, however, employ batch learning and do not consider concept drift. While sentiment classification methods have been proposed most recently [9, 3, 25], they also do not consider temporal effects.

Some studies that use suffix arrays (SAs) for document classification have been proposed. Teo and Vishwanathan proposed fast and space efficient string kernels based on SAs and used the kernel with the support vector machine [33]. Okanohara and Tsujii proposed a logistic regression model with all-effective substrings, which are found with SAs [24]. These studies target the finding of effective feature spaces for batch learning; our model, on the other hand, uses lazy learning for streaming documents--generalization beyond the training data is delayed until a new document is given.
Salles et al. proposed temporally-aware versions of Rocchio and k-nearest neighbors classifiers that conduct temporal weighting on documents [29], in addition to the multinomial naive Bayes classifier described in Eq. (1). Their experiments showed that there were no significant differences among the classifiers. We find that responding to changes at the word level is more important than model selection for classifying document streams.
8. CONCLUSIONS
We proposed a classification model of tweet streams, which are representative of document streams whose statistical properties change over time.
We analyzed tweet streams by examining the changes in class distribution and word probability, the occurrence of new words and recurring themes, and the effectiveness of considering word n-grams. In particular, we demonstrated that tweet streams change significantly at the word level-- the word occurrence probability changes at different rates for different words.
Our model solves this problem by detecting changes in word occurrence probability for each word. It switches between two probability estimates, the maximum likelihood and exponentially weighted moving average estimates, by using a control chart and is very effective in achieving both accurate learning of stationary words and quick response to bursty words. Also, it handles the changes in class distributions and extends the bag-of-words model to cover the n-gram language model. It preserves the probabilistic interpretation of the multinomial naive Bayes classifier.
We then presented an implementation method that uses a word suffix array. This approach enables our model to effectively handle word n-grams, which are highly important information given the 140 character limit of tweets. This new idea, searching the positions of words in documents concatenated chronologically, allows consideration of the temporal effects on changing word streams.
Experiments on topic classification were conducted using three real-world tweet data sets: tweets about Japanese baseball teams, television networks in Japan, and major league baseball teams. Results demonstrated that our model performed statistically significantly better than conventional instance weighting and selection models. In particular, our model was superior to the multinomial naive Bayes classifier with an instance weighting method by up to 16.33% and 16.35% in terms of accuracy and macro F1, for the television network data set.
9. REFERENCES
[1] M. I. Abouelhoda, S. Kurtz, and E. Ohlebusch. Replacing suffix trees with enhanced suffix arrays. J. Discrete Algorithms, 2(1):53­86, 2004.

979

[2] A. Bifet, J. Gama, M. Pechenizkiy, and I. Zliobaite. Handling concept drift: Importance, challenges and solutions. In PAKDD Tutorial, 2011.
[3] S. Brody and N. Diakopoulos. Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs. In EMNLP, pages 562­570, 2011.
[4] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4):359­393, 1999.
[5] L. C. da Rocha, F. Mour~ao, A. M. Pereira, M. A. Gon¸calves, and W. Meira Jr. Exploiting temporal contexts in text classification. In CIKM, pages 243­252, 2008.
[6] S. J. Delany, P. Cunningham, and B. Smyth. ECUE: A spam filter that uses machine learning to track concept drift. In ECAI, page 627, 2006.
[7] P. Ferragina and J. Fischer. Suffix arrays on words. In CPM, pages 328­339, 2007.
[8] G. Forman. Tackling concept drift by temporal inductive transfer. In SIGIR, pages 252­259, 2006.
[9] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao. Target-dependent twitter sentiment classification. In ACL, pages 151­160, 2011.
[10] T. Kasai, G. Lee, H. Arimura, S. Arikawa, and K. Park. Linear-time longest-common-prefix computation in suffix arrays and its applications. In CPM, pages 181­192, 2001.
[11] I. Katakis, G. Tsoumakas, E. Banos, N. Bassiliades, and I. P. Vlahavas. An adaptive personalized news dissemination system. J. Intell. Inf. Syst., 32(2):191­212, 2009.
[12] I. Katakis, G. Tsoumakas, and I. Vlahavas. Tracking recurring contexts using ensemble classifiers: an application to email filtering. Knowl. Inf. Syst., 22:371­391, 2010.
[13] L. Khan, M. Pechenizkiy, and I. Zliobaite, editors. 2nd International Workshop on Handling Concept Drift in Adaptive Information Systems, 2011.
[14] S. Kinsella, A. Passant, and J. G. Breslin. Topic classification in social media using metadata from hyperlinked objects. In ECIR, pages 201­206, 2011.
[15] R. Klinkenberg. Learning drifting concepts: example selection vs. example weighting. Intell. Data Anal., 8:281­300, 2004.
[16] R. Klinkenberg and T. Joachims. Detecting concept drift with support vector machines. In ICML, pages 487­494, 2000.
[17] R. Klinkenberg and I. Renz. Adaptive information filtering: Learning in the presence of concept drifts. In ICML/AAAI-98, pages 33­40, 1998.
[18] G. Lebanon and Y. Zhao. Local likelihood modeling of temporal text streams. In ICML, pages 552­559, 2008.
[19] U. Manber and E. W. Myers. Suffix arrays: A new method for on-line string searches. SIAM J. Comput., 22(5):935­948, 1993.
[20] D. C. Montgomery. Introduction to statistical quality control. John Wiley & Sons, 6th edition, 2008.
[21] F. Mour~ao, L. C. da Rocha, R. B. Arau´jo, T. Couto, M. A. Gon¸calves, and W. M. Jr. Understanding

temporal aspects in document classification. In WSDM, pages 159­170, 2008.
[22] H. Ney, U. Essen, and R. Kneser. On structuring probabilistic dependences in stochastic language modeling. Computer Speech & Language, 8:1­38, 1994.
[23] G. Nong, S. Zhang, and W. H. Chan. Linear suffix array construction by almost pure induced-sorting. In DCC, pages 193­202, 2009.
[24] D. Okanohara and J. ichi Tsujii. Text categorization with all substring features. In SDM, pages 838­846, 2009.
[25] A. Pak and P. Paroubek. Twitter as a corpus for sentiment analysis and opinion mining. In LREC, 2010.
[26] F. Peng, D. Schuurmans, and S. Wang. Augmenting naive bayes classifiers with statistical language models. Inf. Retr., 7(3-4):317­345, 2004.
[27] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes twitter users: real-time event detection by social sensors. In WWW, pages 851­860, 2010.
[28] T. Salles, L. C. da Rocha, F. Mour~ao, G. L. Pappa, L. Cunha, M. A. Gon¸calves, and W. Meira Jr. Automatic document classification temporally robust. JIDM, 1(2):199­212, 2010.
[29] T. Salles, L. C. da Rocha, G. L. Pappa, F. Mour~ao, W. Meira Jr., and M. A. Gon¸calves. Temporally-aware algorithms for document classification. In SIGIR, pages 307­314, 2010.
[30] M. Scholz and R. Klinkenberg. Boosting classifiers for drifting concepts. Intell. Data Anal., 11(1):3­28, 2007.
[31] S. E. Somerville, D. C. Montgomery, and G. C. Runger. Filtering and smoothing methods for mixed particle count distributions. Int. Journal of Prod. Res., 40(13):2991­3013, 2002.
[32] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, and M. Demirbas. Short text classification in twitter to improve information filtering. In SIGIR, pages 841­842, 2010.
[33] C. H. Teo and S. V. N. Vishwanathan. Fast and space efficient string kernels using suffix arrays. In ICML, pages 929­936, 2006.
[34] A. Tsymbal. The problem of concept drift: definitions and related work. Technical Report TCD-CS-2004-15, Trinity College Dublin, 2004.
[35] B. Wenerstrom and C. Giraud-Carrier. Temporal data mining in dynamic feature spaces. In ICDM, pages 1141­1145, 2006.
[36] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69­101, 1996.
[37] I. H. Witten and T. C. Bell. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Trans. Inf. Theory, 37(4):1085­1094, 1991.
[38] E. S. Xioufis, M. Spiliopoulou, G. Tsoumakas, and I. P. Vlahavas. Dealing with concept drift and class imbalance in multi-label stream classification. In IJCAI, pages 1583­1588, 2011.
[39] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML, pages 412­420, 1997.

980


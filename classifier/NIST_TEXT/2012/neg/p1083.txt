Genre Classification for Million Song Dataset Using Confidence-Based Classifiers Combination

Yajie Hu
Department of Computer Science University of Miami
Coral Gables, Florida, USA
yajie.hu@umail.miami.edu
ABSTRACT
We proposed a method to classify songs in the Million Song Dataset according to song genre. Since songs have several data types, we trained sub-classifiers by different types of data. These sub-classifiers are combined using both classifier authority and classification confidence for a particular instance. In the experiments, the combined classifier surpasses all of these sub-classifiers and the SVM classifier using concatenated vectors from all data types. Finally, the genre labels for the Million Song Dataset are provided.
Categories and Subject Descriptors
I.5.2 [Design Methodology]: Classifier design and evaluation; H.5.5 [Sound and Music Computing]: Methodologies and techniques
General Terms
Algorithms, Experimentation
Keywords
Classifier Combination, Song Genre Classification
1. INTRODUCTION
In music information retrieval, many methods see song genre as important metadata for retrieving songs. As the largest currently available dataset, the Million Song Dataset (MSD) is a collection of audio features and metadata for a million contemporary popular music tracks. However, none of records have any genre labels. The goal of this paper is to automatically classify songs in the MSD according to genre.
Some papers have discussed the importance of multiple data sources in genre classification and proposed methods to use them. Most of these methods [3] concatenated features from different data sources into a vector to represent the song. However, for a huge scale dataset, it is impossible that every instance will have valid data in all data sources. It is inevitable for the classification results to be demoted due to data missing influence in the concatenated vector.
If we imagine classifiers as experts in voting, the accuracy of each classifier represents the authority of the expert. Because the types of input data are different, the views of experts are not same. Therefore, the confidences to make
Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Mitsunori Ogihara
Department of Computer Science University of Miami
Coral Gables, Florida, USA
ogihara@cs.miami.edu

a correct decision regarding a particular item are also different. Hence, the voting result of an instance is related to both the authority of the classifier and the confidence of the classifier to classify the particular instance.
In this paper, we extract features from audio, artist terms, lyrics and social tags to represent songs and train sub-classifiers. The trained sub-classifiers are combined to classify song genre. The songs with missing data in certain data types are classified by the remaining data without any negative influence by the missing data.

2. METHOD
We apply each data source to train a sub-classifier and we assume that classifier set C contains n sub-classifiers, namely, C = {c1, c2, . . . , cn}. Furthermore, we assume that songs are distributed into m genres, G = {g1, g2, . . . , gm}. The voting result is shown in Equation 1.





 |C|



G

(Ik)

=

arg

max gj

 i=1

[Auth

(ci)

·

Conf

(ci ,

gj ,

Ik )]

(1)

Auth (ci) denotes the authority of the classifier ci from 0.0 to 1.0. Auth (ci) is estimated by the accuracy of the classification in the validation test.
Conf (ci, gj, Ik) is the confidence of the classifier ci to classify the instance Ik to genre gj. For Na¨ive Bayes, the posterior probability is seen as the confidence for a class. Neural Net has normalized real value output from -1.0 to 1.0. A positive value means the confidence to assign the instance to a positive label. We employ the method proposed by Lee [2] to estimate the confidence for logistic regression. The margin from the instance location to the classification hyperplane is considered to be the confidence of the SVM classifier. The confidences of classifiers are normalized into [0.0, 1.0]. The confidence for invalid data is set to 0.0, in order to avoid negative effect caused by invalid data.

3. EXPERIMENT
In our experiment, we applied MSD, MusiXmatch and Last.fm tag datasets to extract features, as shown in Table 1. The records in these data sources are matched via trackID.
AllMusic.com provides genre taxonomy, which consists of 10 major genres with sample songs. 1,138 songs are collected from AllMusic.com and they have valid records in MSD as the ground truth. The distribution of the songs according to genre is shown in Figure 1.

1083

Bules 52 38 1 1 24 1 30 13 6 3

Country 36 75 0 1 17 0 62 15 2 2

Electronic 0 1 2 1 1 0 9 2 3 0

International 2 0 0 4 2 0 4 1 1 0

Jazz 21 18 0 2 56 3 14 9 1 0

Latin 1 0 0 0 5 0 5 0 1 0

Pop/Rock 20 43 6 3 14 4 252 34 11 4

R&B 17 21 1 1 6 1 46 16 6 6

Rap 7 1 0 0 2 0 13 3 22 0

Reggae 3 3 0 1 1 0 9 3 3 7

Bules CountryElectronIincternatiJoanzazl

Latin

Pop/RocRk&B

Rap

Reggae

(a) Neural Net by audio features

Bules 140 2 0 1 5 4 6 6 3 2

Country 6 185 0 1 0 1 14 2 0 1

Electronic 0 1 13 0 1 0 4 0 0 0

nternational 0 0 1 8 2 1 2 0 0 0

Jazz 2 0 0 3 110 2 6 1 0 0

Latin 0 1 0 0 4 5 2 0 0 0

Pop/Rock 6 16 1 11 7 77 253 12 6 2

R&B 3 0 0 0 2 10 6 92 8 0

Rap 0 0 0 0 1 1 5 0 41 0

Reggae 0 0 0 0 1 4 3 1 0 21

Bules CountryElectronIincternatiJoanzazl

Latin

Pop/RocRk&B

Rap

Reggae

(b) Neural Net by artist terms

Bules 36 20 10 16 16 13 17 13 15 13

Country 17 35 19 12 17 18 48 16 17 11

Electronic 0 3 2 0 0 2 5 3 2 2

International 0 1 0 3 2 4 2 0 1 1

Jazz 11 11 14 12 15 11 17 9 12 12

Latin 0 1 1 0 2 3 2 1 0 2

Pop/Rock 46 46 27 25 25 30 90 42 30 30

R&B 14 9 9 3 7 11 29 17 8 14

Rap 1 7 0 3 4 6 12 2 9 4

Reggae 3 5 1 1 4 6 5 3 1 1

Bules CountryElectronIincternatiJoanzazl

Latin

Pop/RocRk&B

Rap

Reggae

(c) Logistic Regression by lyrics

Bules 47 12 11 16 16 9 23 13 9 13

Country 7 103 12 6 13 6 32 17 6 8

Electronic 0 1 1 1 3 0 7 1 4 1

International 1 2 0 5 0 0 3 1 2 0

Jazz 10 9 9 5 56 5 6 7 10 7

Latin 0 1 0 0 3 4 1 1 1 1

Pop/Rock 10 9 11 15 9 10 308 7 4 8

R&B 11 11 8 4 16 10 31 13 9 8

Rap 4 3 3 5 5 4 5 5 10 4

Reggae 1 2 5 1 0 5 5 0 5 6

Bules CountryElectronIincternatiJoanzazl

Latin Pop/RocRk&B

Rap

Reggae

(d) Na¨ive Bayes by social tags

Figure 2: Confusion matrixes of four sub-classifiers

Name MSD
MuisXmatch Last.fm tags

Table 1: Data sources Extracted information Number of records

Audio features,

1,000,000

artist terms

Lyrics features

237,662

Social tags

505,216

Bules 50 17 8 11 13 11 29 16 7 7

Country 18 92 6 10 13 8 35 11 12 5

Electronic 3 0 3 1 1 2 6 2 0 1

International 1 2 1 7 1 0 1 0 1 0

Jazz 17 7 5 6 57 6 9 5 4 8

Latin 5 0 0 2 0 1 2 2 0 0

Pop/Rock 13 13 16 11 14 13 265 14 21 11

R&B 17 8 4 7 9 7 38 17 7 7

Rap 5 6 5 2 2 4 5 2 12 5

Reggae 1 3 4 1 1 2 7 1 4 6

Bules CountryElectronIincternatiJoanzazl

Latin

Pop/RocRk&B

Rap

Reggae

(a) SVM by long vector

Bules 145 5 0 0 4 0 5 5 3 2

Country 5 183 0 0 0 0 20 1 0 1

Electronic 0 1 11 0 1 0 6 0 0 0

International 1 0 1 8 1 0 3 0 0 0

Jazz 3 2 0 3 109 0 5 2 0 0

Latin 0 1 0 0 5 4 2 0 0 0

Pop/Rock 5 16 1 0 3 1 348 9 6 2

R&B 3 1 1 0 4 0 23 84 5 0

Rap 0 0 0 0 0 0 6 0 42 0

Reggae 0 0 0 0 1 0 7 1 3 18

Bules CountryElectronIincternatiJoanzazl

Latin

Pop/RocRk&B

Rap

Reggae

(b) Combined classifier

Figure 3: Confusion matrixes by all data

Figure 1: Genre Samples in AllMusic.com
In order to improve classification performance, we convert genre classification into a series of binary classifications. Thus, the classification result of a song is a vector of confidence to classify the song into a genre. The predicted genre is the one whose confidence is highest.
We extract features from different data sources and trained individual classifiers by each type of features using Na¨ive Bayes, Rule Induction, LDA, Neural Net, Logistic Regression and SVM, respectively. The classifiers' performances are evaluated by 5-folder cross validation. The best performance classifiers in different types of features are shown in Figure 2.
The four sub-classifiers are combined based on classification authority and confidence. The resultant combined classifier is significantly better than sub-classifiers and the SVM classifier using concatenated vectors from four data sources as shown in Figure 3 and Table 2. The result is encouraging regarding to the result of genre classification task in MIREX [1]. Furthermore, we apply the combined classifier to classify all of the songs in the MSD and the result is available at http://web.cs.miami.edu/home/yajiehu/resource/genre.

Table 2: Experiment result comparison

Data

Method

Accuracy

Audio

Neural Net

42.70%

Artist terms

Neural Net

76.27%

Lyrics

Logistic Regression 18.54%

Social Tags

Na¨ive Bayes

48.59%

All data

SVM

44.82%

All data Combined classifiers 83.66%

4. CONCLUSION
Based on classifier authority and classification confidence, the combined classifier integrates sub-classifiers, which are good at classification of certain data sources. The combined classifier performs with higher accuracy than sub-classifiers and the SVM classifier using concatenated vectors.
5. REFERENCES
[1] http://www.music-ir.org/mirex/wiki/2009. [2] C.-H. Lee. Learning to combine discriminative
classifiers: confidence based. In Proceedings of the 16th ACM SIGKDD, KDD '10, pages 743­752, New York, USA, 2010. [3] C. McKay, J. A. Burgoyne, J. Hockman, J. B. L. Smith, G. Vigliensoni, and I. Fujinaga. Evaluating the genre classification performance of lyrical features relative to audio, symbolic and cultural features. In Proceedings of the 11th ISMIR, ISMIR '10, pages 213­718, Utrecht, Netherlands, 2010.

1084


SimFusion+: Extending SimFusion Towards Efficient Estimation on Large and Dynamic Networks

Weiren Yu, Xuemin Lin , Wenjie Zhang, Ying Zhang, Jiajin Le

The University of New South Wales, Australia

East China Normal University, China

NICTA, Australia

Donghua University, China

{weirenyu, lxue, zhangw, yingz}@cse.unsw.edu.au lejiajin@dhu.edu.cn

ABSTRACT
SimFusion has become a captivating measure of similarity between objects in a web graph. It is iteratively distilled from the notion that "the similarity between two objects is reinforced by the similarity of their related objects". The existing SimFusion model usually exploits the Unified Relationship Matrix (URM) to represent latent relationships among heterogeneous data, and adopts an iterative paradigm for SimFusion computation. However, due to the row normalization of URM, the traditional SimFusion model may produce the trivial solution; worse still, the iterative computation of SimFusion may not ensure the global convergence of the solution. This paper studies the revision of this model, providing a full treatment from complexity to algorithms. (1) We propose SimFusion+ based on a notion of the Unified Adjacency Matrix (UAM), a modification of the URM, to prevent the trivial solution and the divergence issue of SimFusion. (2) We show that for any vertex-pair, SimFusion+ can be performed in O(1) time and O(n) space with an O(km)-time precomputation done only once, as opposed to the O(kn3) time and O(n2) space of its traditional counterpart, where n, m, and k denote the number of vertices, edges, and iterations respectively. (3) We also devise an incremental algorithm for further improving the computation of SimFusion+ when networks are dynamically updated, with performance guarantees for similarity estimation. We experimentally verify that these algorithms scale well, and the revised notion of SimFusion is able to converge to a non-trivial solution, and allows us to identify more sensible structure information in large real-world networks.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Storage and Retrieval; G.2.2 [Graph Theory]: Discrete Mathematics
Keywords
Similarity Computation, SimFusion, Web Ranking Algorithm
1. INTRODUCTION
The conundrum of measuring similarity between objects based on hyperlinks in a graph has fueled a growing interest in the fields of information retrieval. This problem is also known as "link-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

G1

D2

P2

P3

D1 P1

D3

P4

P5

lim SimFusion(k) =

lim SimFusion+(k) =

k

  k



D1 1 .219 .219 .219 .219

1 .290 .194 .139 .100

D2 D3



.219 .219 .219

1 .219 .219

.219 1
.219

.219 .219
1

.219 .219 .219



.219 .219 .219 .219 1



.290 .194 .139

1 .304 .217

.304 1
.145

.217 .145
1

.156 .105 .075



.100 .156 .105 .075 1

D1

D2

D3

D1

D2

D3

Figure 1: Trivial SimFusion on Heterogeneous Domain

based analysis" or "structural similarity search", and it has been extensively studied by different communities with a proliferation of emerging applications. Examples include collaborative filtering, hyper-text classification, graph clustering and proximity query processing. Recently, while the scale of the Web has dramatically increased our need to produce large graphs, the study of efficiently computing object similarity on such large graphs becomes a desideratum. In practice, an effective similarity measure should not only correlate well with human intuition but also scale well for large amounts of data.
Among the existing metrics, SimFusion [1] can be regarded as one of the attractive ones on account of the following reasons. (i) Similar to PageRank [2] and SimRank [3], SimFusion is based on hyperlinks and follows the reinforcement assumption that "the similarity between objects is reinforced by the similarity of their related objects", which is fairly intuitive and conforms to our basic understandings. (ii) Unlike other measures (e.g., PageRank and SimRank) that explore the linkage patterns merely from a single data space [2­4], SimFusion has the extra benefits of incorporating both inter- and intra-relationships from multiple data spaces in a unified manner to measure the similarity of heterogeneous data objects. (iii) SimFusion offers more intuitive and flexible ways of assigning weighting factors to each data space that reflects their relative importance, as opposed to the PageRank and SimRank measures that need to determine a damping factor. (iv) SimFusion provides a general-purpose framework for measuring structural similarity in a recursive fashion; other well-known measures, such as CoCitation [5] and Coupling [6] are just special cases of SimFusion.
However, existing work on SimFusion has the following problems. Firstly, although the basic intuition behind the SimFusion model is appealing, it seems inappropriate to use the Unified Relationship Matrix (URM) to represent the relationships of heterogeneous objects. The main problem is that, according to the definition of URM L in [1], the sum of each row of L is always equal to 1. Since the product of L and the matrix 1 whose entries are all ones is equal to the matrix 1 of all ones, there always exists a trivial solution S = 1 to the original SimFusion formula S = L · S · LT [1], as illustrated in Example 1. The same phenomena of yielding such a trivial solution may occur in our experimental results in Section 6. To address this issue, we shall revise the original SimFusion model.

EXAMPLE 1 (TRIVIAL SOLUTION). Figure 1 depicts a graph G1 partly extracted from Cornell CS Department. Each vertex Pi

365

G2 P2 P1 D1 P3

P4 P6
P5

lim SimFusion(2k)

lim SimFusion(2k+1)

lim SimFusion+(k)

 k

 k

  k



.46 0 .46 0 0 .46

.38 0 .38 0 0 .38 1 0 .25 0 0 .25

.40006

.38 0 .38 .38

0 .46 0 0

.38 0 .38 .38

.38 0 .38 .38

.40006

=

.30008

.46 0 .46 .46

0 .38 0 0

.46 0 .46 .46

.46 0 .46 .46

.30008

.20005

1 0 .25 .18

0 1 0 0

.25 0 1 .25

.18 0 .25 1

.10008

.46 0 .46 0 0 .46

.38 0 .38 0 0 .38 .25 0 .18 0 0 1

Figure 2: Divergent SimFusion on Homogeneous Domain

denotes a web page, and each edge a hyperlink. There are three categories: D1 = {P1} (student), D2 = {P2, P3} (staff), and D3 = {P4, P5} (faculty). We want to retrieve the top-3 similar pairs of web pages in G1. However, the naive SimFusion fails to
correctly find them. We observe that the SimFusion solution is a
(trivial) matrix whose entries are all the same. In fact, vertices in G1 do not have the identical neighbor structures. Hence, the trivial
solution is non-semantic in real communities. Secondly, it is rather expensive to compute SimFusion similar-
ities. The existing approach for SimFusion computation deploys a fixed-point iteration: S(k+1) = L · S(k) · LT , which requires O(kn3) time and O(n2) space [1]. This impedes the scalability of SimFusion on large graphs. Worse still, the iterative computation of SimFusion do not always converge. The convergence of the SimFusion iterations heavily depends on the choice of the initial guess S(0), as shown in Example 2.
EXAMPLE 2 (DIVERGENCE SIMFUSION). Consider the disease transmission graph G2, where each vertex is an organism Pi which can carry the disease, and an edge represents one organism
spreading it to another. One wants to find the three most similar organisms to P2 in G2. However, the iterative computation of Sim-
Fusion does not work properly. We observe the following: (i) When S(0) is set to an n×n identity matrix (according to [1]) the "even" and "odd" subsequences of {S(k)} are convergent
respectively, but they do not converge to the same limit, which makes the full sequence {S(k)} divergent. (ii) Choosing S(0) = 1n (i.e., an n × n matrix of all 1s) instead, we observe that the full SimFusion sequence {S(k)} is always convergent to 1n regardless of the graph structure.
This suggests that the original SimFusion iterations may be divergent or converge to a trivial solution, not to mention its scalability. This highlights the need to find a feasible way to guarantee the convergence of the SimFusion iterations, but it is hard to devise an efficient algorithm for the revised SimFusion computation.
Thirdly, it is a big challenge to incrementally compute SimFusion on dynamic graphs. The traditional method [1] has to recompute the similarity from scratch when edges in a graph change over time, which is not adaptive to many evolving networks. Fortunately, we have an observation that the size of the areas affected by the updates is typically small in practice. To this end, we propose an incremental algorithm that fully utilizes these affected areas to compute SimFusion on dynamic graphs.
Contributions. This paper proposes SimFusion+, a revised notion of SimFusion, to provide a full treatment of SimFusion for the convergence issues and to improve its computational efficiency. In summary, we make the following contributions. 1. We formalize the problem of SimFusion+ computation (Section
2). The notion of SimFusion+ revises the divergence and nonsemantic convergence worries of the traditional model [1]. 2. We present optimization techniques for improving the computation of SimFusion+ to O(1) time and O(n) space for every pair of vertices, plus an O(km)-time precomputation run only once (Section 3). 3. We devise an efficient algorithm to compute the SimFusion+ similarity with better accuracy guarantees (Section 4). An error estimate is also given for the SimFusion+ approximation. 4. We devise an incremental algorithm for further optimizing the SimFusion+ computation when edges in networks are dynamically updated (Section 5). We show that the update cost of the

incremental algorithm retains O(n) time and O(n) space for handling a sequence of  edge insertions or deletions. 5. We experimentally verify the effectiveness and scalability of the algorithms, using real and synthetic data (Section 6). The results show that SimFusion+ can govern the convergence towards a meaningful solution, and our algorithms achieve high accuracy and significantly outperform the baseline algorithms.

Related Work. The link-based similarity has become increas-

ingly popular since the famous result of Google PageRank [2] on

ranking web pages. Since then, there has been a surge of papers

focusing on web link analysis. In particular, a growing interest has

been witnessed in the SimFusion model over the past decade [1, 7]

as it provides a useful measure of similarity that supports different

kinds of intra- and inter-node relations from multiple data spaces.

The iterative computation of SimFusion was proposed in [1] with

several problems left open there. In comparison, this work extend-

s [1] by (i) addressing the divergent and trivial solution of the orig-

inal SimFusion, (ii) optimizing the time and space complexity of

similarity computation, and (iii) supporting incremental update on

evolving graphs, none of which was considered in [1].

It is worth mentioning that Jeh and Widom have proposed a sim-

ilar structural measure called SimRank [3], predicated, as SimFu-

sion is, on the idea that vertices are similar if they have similar

neighbor structures. The essential difference between the two mod-

els is the notion of the convergence principle. SimFusion ensures

the existence of the stationary distribution and ergodicity conver-

gence to this distribution, whereas SimRank hinges on a damping

factor 0 < c < 1 to govern the convergence.

Optimization techniques have been devised for SimRank com-

putation (e.g., [8­11]). The best-known SimRank algorithm yields

O(k

min{nm,

n3 log n

})

time

[8].

The performance gain is mainly

achieved by a partial sum function for amortization; as for SimFu-

sion, the conventional matrix multiplication in its iterative formula misled its complexity, which was previously considered O(kn3) time and O(n2) space. The idea of the dominant eigenvector in

this work significantly improves its computation to O(km) time

and O(kn) space, which is more efficient than SimRank [8].

There has also been work on link-based similarity computation.

A unified framework of link-based analysis was addressed in [7],

which extends PageRank and HITS by (i) considering both inter-

and intra-type relationships, and (ii) bringing order to data objects

in different data spaces. It differs from this work in that the fo-

cus is on finding attribute values of a single object, rather than on

improving the complexities for similarity estimation. Extension-

s of similarity reinforcement assumption were studied in [12], by

spreading multiple relationship similarities over interrelated data

objects to enhance their mutual reinforcement effects. Neverthe-

less, neither of these deduces rigorous mathematical formulae, and

the rationales behind the integration approaches are different from

this work. Recently, a closed-form solution to P-Rank (Penetrating-

Rank) formula was addressed in [13]. Cai et al. [13] showed that

when the damping factor c = 1 and weighting factor  = 0, P-

Rank can be reduced to SimFusion. However, this reasoning is

based on the flawed assumption that the diagonal entries diag(S)

of the P-Rank similarity matrix were not considered. We argue that

P-Rank is defined recursively, and hence, the omission of diag(S)

has an impact on the similarity of a vertex with itself, and recursive-

ly, it has an impact on the similarity of different pairs of vertices.

2. SIMFUSION ESTIMATION REVISED

In this section, we first revisit the definition of data space and data relation. We then introduce the notion of the Unified Adjacency Matrix (UAM) to revise the SimFusion model.

2.1 Data Space and Data Relation
Graphs studies here are digraphs having no multiple edges.

366

Data Space. A data space is the finite set of all data objects

where S is called the Unified Similarity Matrix (USM) whose

(vertices) with the same data type, denoted by D = {o1, o2, · · · }.

(i, j)-entry represents the similarity score between object i and j.

|D| denotes the number of data objects in D. Two nonempty data

The uniqueness and existence of the SimFusion+ solution S to

spaces D and D are said to be disjoint if D  D = .

Eq.(1) can be established by the power iteration [14, pp.381]. A

Throughout the paper, we shall use the following notations. (i)

detailed proof will be shown in Proposition 1 (Section 3).

The entire space D in a network is the union of N disjoint da-

The revised notion of SimFusion utilizes UAM (rather than

ta spaces D1, · · · , DN such that D =

N i=1

Di

and Di

 Dj

=

 (i = j). (ii) The total size |D| of the entire space, denoted by

n, is the sum of the number ni of the data objects in each data space

Di, i.e., n =

N i=1

ni

with

ni

=

|Di|

(i = 1, · · · , N ).

Intuitively, for heterogeneous networks, the distinct spaces Di

form a partition of D into classes. For homogenous networks, the

partition of D is itself.

Data Relation. A data relation on D is defined as R  D × D,

where (o, o )  R is a connection (a directed edge) from object

o to o . Data objects in the same data space are related via intra-

type relations Ri,i  Di × Di. Data objects between distinct data

spaces are related via inter-type relations Ri,j  Di×Dj (i = j).

Intuitively, the intra-type relation carries connected information

URM) to represent data relations because UAM can effectively avoid divergent and trivial similarity solutions while well preserving the intuitive reinforcement assumption of the original model [1]. We observe that the root cause of the flawed solution to the original SimFusion is the "row normalization" of URM. Thus, by using UAM, we have an opportunity to postpone the operation of "row normalization" in a delayed fashion. To this end, we utilize the matrix 2-norm A · S · AT 2 to squeeze similarity scores in S into [0, 1]. The obtained similarity results in USM can not only prevent the divergence issue and the trivial solution but effectively capture the reliability of the similarity evidence between data objects. For instance, the SimFusion+ USMs in Examples 1 and 2 are nontrivial and intuitively explainable.

in each data space (e.g., co-citation between web pages); the intertype relation represents interlinked information between different

3. COMPUTING SIMILARITY VIA DOMI-

data spaces (e.g., making user requests). As an example, in Figure 1 there are three data spaces : D = {P1}  {P2, P3}  {P4, P5}, where (P2, P2), (P2, P3), (P3, P2), (P4, P5), (P5, P4) are intratype relations; (P1, P2), (P2, P1), (P3, P1), (P1, P3), (P1, P4), (P4,

P1)

to

NANT EIGENVECTOR
A conventional approach for finding the SimFusion+ solution Eq.(1) is to employ the following fixed-point iteration: 1

S

are inter-type relations.
2.2 Unified Adjacency Matrix

S(k+1) =

A · S(k) · AT A · S(k) · AT

.
2

(2)

Let us now introduce the unified adjacency matrix (UAM). Consider a graph G = (D, R) with data space D and data relation R.

However, as the matrix multiplication may contain O(n3) operations, it requires O(kn3) time and O(n2) space to compute Eq.(2)

Unified Adjacency Matrix (UAM). The matrix A = A~ + 1/n2

of size n × n is said to be a unified adjacency matrix of the relation

R whenever

 1,1A1,1

A~

=



2,1 A2,1 ...

1,2A1,2 · · · 1,N A1,N 

2,2 A2,2 ...

··· ...

2,N A2,N ...

 ,

N,1AN,1 N,2AN,2 · · · N,N AN,N

for k iterations, which may be quite expensive. In this section, we study the optimization techniques to improve
the computation of SimFusion+. Our key observation is that SimFusion+ computation can be converted into finding the dominant eigenvector of the UAM A. The idea is to calculate the dominant eigenvector of A once, offline, for the preprocessing, and then it can be effectively memorized to compute similarity at query time.
We first revisit the definition of the dominant eigenvector.

where (i) 1 is the n × n matrix of all ones; (ii) Ai,j is the sub-

matrix of A whose (o, o )-entry equals 1 if there is an edge from

data object

o to o ,

i.e.,

 (o, o )



R,

1 nj

if data object

o has no

neighbors in Dj, or 0 otherwise; and (iii) i,j is called the weight-

ing factor between data space Di and Dj with 0  i,j  1 and

N j=1

i,j

=

1

(i

=

1, · · ·

, N ).

Intuitively, Ai,j represents the intra- (i = j) or inter- (i = j)

relation from data space Di to Dj. i,j reflects the relative impor-

tance between data spaces Di and Dj.

DEFINITION 1 ( [14, P.379]). The dominant eigenvector of the X is an eigenvector, denoted by max(X), corresponding to the eigenvalue  of the largest absolute value of X such that

X · max(X) =  · max(X) with max(X) 2 = 1.

The dominant eigenvector of the UAM can be utilized for speeding up SimFusion+ computation based on the following proposition.

PROPOSITION 1. Let A be the UAM of network G = (D, R). The SimFusion+ matrix S can be computed as

[S]i,j = [max(A)]i × [max(A)]j ,

(3)

EXAMPLE 3. In Figure 1, the relative importance between data
space Di and Dj is denoted by a weighting matrix  = (i,j )3×3. Then, the UAM A of G1 can be derived from A = A~ + 1/n2, where A~ is computed from  as follows.

where [ ]i,j denotes the (i, j)-entry of a matrix, and [ ]i denotes the i-th entry of a vector.
PROOF. We shall use the knowledge of Kronecker product () and vec operator (see [15, p.139] for a detailed description).



=

D1 D2

D11
2
1
6

D2
1 6 7 12

D3
1 3 1 4

 

D3

115 3 4 12



A~ =

1



2 1 6
1

1 1 1 1

30

1 6

11

7 11 12 1 0

11 1 22 4 11
22

1 3

10

11 1 22 4 11
22

5 01 12 1 0

 1 1 1



=



2
1 6 1 6
1 3

6
7 12 7 12
1 8

6 7 12
0
1 8

1 3
1 8 1 8
0



0

1 8
1 8
5 12



0

1 8

1 8

5 12

0

SimFusion+ Model. In light of the UAM A, we next propose the revised model of SimFusion, termed SimFusion+, as follows:

S=

A · S · AT A · S · AT

,
2

(1)

(i) We first prove that vec(S) = max(A  A).

Taking vec( ) on both sides of Eq.(2) and applying Kronecker property vec(BCDT ) = (D  B) · vec(C) [15, p.147] yield

vec(S(k+1)) =

(A  A) · vec(S(k)) (A  A) · vec(S(k))

2

.

(4)

1Note that the uniqueness of the SimFusion+ solution guarantees

that S is insensitive to the initial guess S(0). For convenience, we

choose S(0) = 1 of all 1s, which can be interpreted as "initially, no

other vertex-pair is presumably more similar than itself".

367

Let x(k) vec(S(k)) and M A  A. Then Eq.(4) having the form x(k+1) = Mx(k)/ Mx(k) 2 fits the power iteration paradigm [14, pp.381], which follows that the sequence {x(k)} converges to the dominant eigenvector of M. This in turn implies

vec(S) lim vec(S(k)) = max(A  A).
k
One caveat is that the convergence of vec(S(k)) is ensured by the positivity of AA [16, p.508] 2. This is true because A is positive and the self-Kronecker product of two positive matrices preserves positivity.
(ii) We next show that max(A  A) = max(A)  max(A).
Since A · max(A) =  · max(A), it follows that
(A  A) · (max(A)  max(A)) = (Amax(A))  (Amax(A)) = ( · max(A))  ( · max(A)) = 2 · (max(A)  max(A)).
This implies that the dominant eigenvector of A A is actually the self-Kronecker product of the dominant eigenvector of A. Hence,

vec(S) = max(A  A) = max(A)  max(A).
It can be noticed that the (i, j)-entry of the matrix S (i.e., the ((i-1)×n+j)-th entry of the vector vec(S)) is exactly the product of the i-th and j-th entries of max(A). Thus, Eq.(3) holds.

Proposition 1 provides the efficient technique for accelerating SimFusion+ computation. The central point in optimizing S computation is that only matrix-vector multiplication is used for computing max(A). Once calculated, the vector max(A) is memorized and thus will not be recomputed when subsequently required, as opposed to the naive matrix-matrix multiplication in Eq.(2).

EXAMPLE 4. Consider the graph G1 in Fig.1 with its UAM A already computed in Example 3. The dominant eigenvector of A is

max(A) = [.431 .673 .451 .322 .232]T .

Then using Eq.(3) for computing S yields

.186 .290 .194 .139

S = ...112399940

.453 .304 .217

.304 .203 .145

.217 .145 .104

.100 .156 .105 .075

.100 ...011705556 . .054

Note that max(A) is calculated only once for the preprocessing and can be used for computing any entry of S at query time, e.g.,

[S]1,2 = [max(A)]1 × [max(A)]2 = .431 × .673 = .290. [S]1,3 = [max(A)]1 × [max(A)]3 = .431 × .451 = .194.

Regarding computational complexity, our approach only needs O(km) preprocessing time and O(n) space to compute max(A) by using the following power iteration [14, pp.381]:

(0) = e,

(k+1) =

A(k) = A(k) 2

A~ (k) + (k)e A~ (k) + (k)e

2

,

(5)

where e (1, · · · , 1)T  Rn and (k)

1 n2

n i=1

[(k)]i.

3

The

existence and uniqueness of the dominant eigenvector max(A) is

2According to the Perron-Frobenius theorem [14, p.383], the positivity of A  A ensures that there exists a unique dominant eigenvector of AA associated with its eigenvalue being strictly greater
in magnitude than its other eigenvalues.
3The correctness of Eq.(5) can be proved as follows:

A(k)

=

(A~ +

1 n2

eeT

)(k)

=

A~ (k)+(k)e with (k)

=

1 n2

eT

(k)

.

guaranteed by the combination of Perron-Frobenius theorem [14, p.383] and the positivity of A. Thus, by applying the power method, the sequence {(k)} converges to max(A). Then, with max(A) being memorized, only O(1) time is required at query stage for computing each entry of S via Eq.(3). Indeed, due to S symmetry, only n(n + 1)/2 entries [S]i,j (i  j) need to be computed. In contrast to the O(kn3) time and O(n2) space of the conventional iterations, our approach is a significant improvement achieved by max(A) computation.
Our method of memorizing max(A) can extra accelerate SimFusion+ computation when only a small portion of similarity values of S need to be computed. Specifically, for certain applications like K-nearest neighbor (KNN) queries, given a vertex i as a query, one needs to retrieve the top-K ( n) most similar vertices in a graph by computing the i-th row of S. Before Proposition 1 is introduced, computing the similarity of only one vertex-pair still requires O(kn3) time. In contrast, using the memorized max(A), we only need O(1) time for computing a single entry of S at query time. In fact, for KNN queries, after max(A) is memorized with its entries sorted in an descending order for the preprocessing, it only takes constant time to retrieve the top-K results at query stage.
Proposition 1 also gives an interesting characterization of the SimFusion+ matrix.
COROLLARY 1. The SimFusion+ matrix S is a rank 1 matrix.
PROOF. Applying Eq.(3) to S, we obtain that for any two rows of S,   = [max(A)]x/[max(A)]y s.t. [S]x, =  × [S]y,. Hence, the rank of S is 1.

4. ESTIMATING SIMFUSION+ WITH BET-
TER ACCURACY
After the dominant eigenvector max(A) has been suggested to speed up SimFusion+ computation, the algorithm presented in this section can guarantee more accurate similarity results.
The main idea of our approach is to leverage an orthogonal subspace for "upper-triangularizing" the UAM A (n × n dimension) into a small matrix Tk (k × k dimension) with k n. Due to Tk small size and almost "upper-triangularity", computing the dominant eigenvector max(Tk) is far less costly than straightforwardly computing max(A). We show that the choice of k provides a user-controlled accuracy over the similarity scores. The underlying rationale is that the dominant eigenvector of a matrix can be well-preserved by an orthogonal transformation.
We first use the technique of the Arnoldi decomposition [17] to build an order-k orthogonal subspace for the UAM A.

LEMMA 1 ( [17, PP.25-33]). Let A be an n × n matrix. Then, for every k = 1, 2, · · · , we have the following results.
(a) There exists a unique k × k almost triangular matrix Tk s.t.

VkT AVk = Tk,

(6)

where Vk = [v1 v2 · · · vk] is an n × k matrix consisting of k orthonormal column-vectors vi  Rn (i = 1, · · · , k).
(b) The difference between AVk and VkTk is a zero matrix except the last column. Precisely, there exist a small scalar k and an orthonormal vector vk+1  Rn such that

AVk - VkTk = kvk+1eTk ,

(7)

where ek = (0, · · · , 0, 1)T  Rk is a unit vector.

As depicted in Fig.3, by using the upper-triangularization process 4, the matrix A  Rn×n can be transformed into the small almost triangular Tk  Rk×k by the n × k orthonormal ma-
4From the computational viewpoint, Vk, Tk, vk and k in Eq.(7) can be obtained by an algorithm in our later developments.

368

almost upper

n

k

triangular

k

n

k

k VkT · A n · Vk n = Tk k

·k n  Vk n
max (Tk )

Rn×n  Rk×k

max(A)

Figure 3: Upper Triangular Process of UAM
trix Vk for every iteration. As k increases, Tk+1 can be iteratively obtained by bordering the matrix Tk at the last iteration (i.e., Tk+1 = [ Tk ]), and Vk+1 by augmenting the matrix Vk at the last iteration with the vector vk+1 (i.e., Vk+1 = [Vk vk+1]). When k = rank(A), it follows that k = 0.

EXAMPLE 5. Consider the network G1 in Fig.1 and its UAM A = A~ + 1/52 in Example 3. For k = 3,  V3 = [v1 v2 v3] 

R5×3 mapping A  R5×5 into T3  R3×3 s.t. T3 = V3T AV3,

where

1

A~

=



2 1
6 1
6 1

3

1
6 7
12 7
12 1
8

1 6 7 12
0
1 8

1 3 1 8 1 8
0

0
1 8 1 8 5

  ,

T3

=

1.08 .298 0  .298 .190 .359  ,
0 .359 -.083

V3

=

 .447 .125 -.089 



.447 .447 .447

.750 -.125 -.125

.044 .710 -.696

.

12

0

1 8

1 8

5 12

0

.447 -.625 .032

 3 = .231, v4 = [-.881 .328 .137 .280 .135]T s.t. Eq.(7) holds. (see Example 7 for a detailed iterative process)

In light of Lemma 1, we next provide an error estimate for Sim-
Fusion+ similarity when using max(Tk) to compute max(A). Error Estimation. We define a k-approximation similarity ma-
trix S^k over a low-order parameter k:

[S^k]i,j = [Vk · max(Tk)]i × [Vk · max(Tk)]j , (8)

whTeoredViffkeraenndtiaTtekS^ckanfrboemoSbt,awineesdhfarlol mrefLeremtomSaa1s.exact similarity. The following estimate for the approximate similarity S^k with
respect to the exact S can be established.

PROPOSITION 2. For every k = 1, 2, · · · , the following estimate holds:

S^k - S 2  k,

(9)

where

k = 2 × |k × [max(Tk)]k|,

(10)

and k is a small scalar given in Eq.(7); [max(Tk)]k is the k-th entry of the dominant eigenvector of Tk.

(Please refer to the Appendix for a detailed proof.) The parameter k is intended as a user control over the difference between the approximate and the exact similarity matrices, and hence k is generally chosen by a user. Provided that k is selected to satisfy Eq.(10), Proposition 2 states that the gap between the approximate and the exact similarity scores does not exceed k.

EXAMPLE 6. Consider the network G1 in Fig.1 and the matrix T3, V3, 3 given in Example 5. For k = 3, we have

max(T3) = [.945 .316 .089]T .

Therefore, V3 · max(T3) = [.454 .663 .447 .321 .228]T . Then applying Eq.(8) and the exact S in Example 4 yields

S^3 = {Using Eq.(8)} =





.206 .301 .203 .146 .103

...123400631

.440 .296 .213

.296 .199 .143

.213 .143 .102

...011705321

.103 .151 .102 .073 .051

S^3 - S = {Using S in Example 4} =





2.02 1.09 .83 .67 .34

.01

×

1...860379

-1.33 -.75 -.41

-.75 -.40 -.21

-.41 -.21 -.09

---...531107

.34 -.51 -.30 -.17 -.20

Algorithm 1: SimFusion+ (G, , (u, v))

Input : Network G = (D, R), accuracy , vertex pair (u, v).
Output: Similarity score s(u, v). 1 compute the matrix A~  Rn×n of the UAM A in G ;

2

initialize e



(1, 1, · · ·

, 1)T



Rn, and v1



1 n

e

;

3 foreach iteration k = 1, 2, · · · do

4

initialize

the

auxiliary

vector

w



A~ vk

+

1 n2

(eT

vk )e

;

5

for i = 1, 2, · · · , k - 1 do

6

compute the almost upper triangular matrix Tk  Rk×k :

[Tk]i,k-1  vkT · w ;

7

orthogonalize w s.t. wspan{v1, · · · , vk} :

w  w - [Tk]i,k-1 · vk-1 ;

8

compute the residual scalar k  w 2 ;

9

find the dominant eigenvector max(Tk) ;

10

estimate the error k  2 × |k × [max(Tk)]k|;

11

if k  then exit for ;

12

compute the residual vector vk+1 :

w  w/k and vk+1  w ;

13

free k, max(Tk) ;

14 free w, Tk, vk+1, k ; 15 compute the approximate dominant eigenvector ^max(A)
^max(A)  [v1|v2| · · · |vk] · max(Tk) ; 16 free v1, · · · , vk, max(Tk) ; 17 compute the approximate similarity score of (u, v)
s^(u, v)  [^max(A)]u × [^max(A)]v ; 18 return s^(u, v) ;

We note that the gap between S and S^k for k = 3 is actually S^3 - S 2 = .0257,
which is smaller than k (using Eq.(10) with 3 = .231)

k = 2 × |.231 × .089| = .0411.

Notice that if k = rank(A) ( n), 5 then k = 0 and the kapproximation similarity matrix S^k becomes the conventional exact USM S. From this perspective, the k-approximation similarity
can be regarded as a generalization for the conventional similarity.
One of the possible ways of choosing an appropriate low order k for achieving the desired accuracy is to calculate the estimation
error k from Eq.(10) in an a-posteriori fashion after each iteration k = 1, 2, · · · . 6 The iterative process stops once k  . Due to k decreasing monotonicity, such k is the minimum low order s.t. S^k - S 2  . More concretely, the residual k in Eq.(7) (Lemma 1) approaches 0 as k is increased to n, which implies

k = 2 × |k| × |[max(Tk)]k|  2 × |k|.
Hence, the condition k  (with k being obtained from Eq.(10)) can be used as a stopping criterion for determining the minimum low order k needed for the desired accuracy .
Capitalizing on Eq.(8) and Proposition 2, below we provide an algorithm for SimFusion+ computation with accuracy guarantee.
Algorithm. SimFusion+ is shown in Algorithm 1. It takes as input a network G = (D, R), a desired accuracy , and a vertex pair (u, v); it returns the approximate similarity s^(u, v) such that |s^(u, v) - s(u, v)|  with s(u, v) being the exact value.
Before illustrating the algorithm, we first present the notations it uses. (a) [Tk]i,j is the (i, j)-entry of the matrix Tk, and [max(Tk)]i is the i-th entry of the eigenvector max(Tk). (b)

5When smaller

k is than

sreatntok(aArg)m), inkk={kk=+1

0} =

(which is practically · · · = n = 0.

much

6As increased by 1 per iteration, the low order parameter k equals

the iteration number.

369

#-line
4 6 7 8 9 10 12

time
O(m) O(n) O(n) O(n) O(k) O(1) O(n)

memory
O(n) O(n) O(n) O(n) O(k) O(k) O(n)

operation
sparse matrix-vector multiplication vector dot product vector addition and scalar multiplication computing the 2-norm of a vector using the power iteration getting the vector component scaling the vector

Table 1: Running Time & Memory Space Required per Iteration for Algorithm SimFusion+ in Lines 4-12
span{v1, · · · , vk} is the set of all linear combinations of vectors v1, · · · , vk. (c) ^max(A) denotes the approximation of max(A).
The algorithm SimFusion+ works as follows. It first computes A and initializes v1 (lines 1-2). Using A, it then computes Tk (lines 4-6), k (lines 7-8) and vk+1 (line 12) by orthonormalizing the vector Avk with respect to v1, · · · , vk for every iteration; SimFusion+ also calculates max(Tk) (line 9), and utilizes k and max(Tk) to estimate the error k (line 10). The process (lines 3-13) iterates until k  , i.e., the minimum low order k is found s.t. k meets the desired accuracy (line 11). For such k, the matrix-vector product [v1|v2| · · · |vk] · max(Tk) is used to approximate the dominant eigenvector of A, and is memorized to compute ^max(A) (line 15). The product of the u-th and v-th entries of ^max(A) is collected in s^(u, v), which is returned as the estimated similarity between vertex u and v (lines 17-18).
EXAMPLE 7. We show how SimFusion+ estimates the similarity in G1 of Example 1. Given the desired accuracy = 0.05, SimFusion+ first initializes the UAM A (in Example 3). It then iteratively computes Tk, vk+1, k, max(Tk) and k as follows:

k

Tk

vk+1

k max(Tk)

k

0

-

1

[1.08]

[.447 .447 .447 .447 .447]T

-

[.125 .750 -.125 -.125 -.625]T .298

-

-

[1]

.596

2

1.08 .298 .298 .190

[-.089 .044 .710 -.697 .032]T .359





1.08 .298 0

3

 .298 .190 .359 

T
[-.881 .329 .137 .280 .135 .231]

.231

0 .359 -.083

.957 .208
.290 
.945 .316 .041
.090

The iteration terminates at k = 3 because the estimation error 3 = .041  (= .05). SimFusion+ then memorizes ^max(A) =
[v1|v2|v3] · max(T3) and returns the similarity s^(u, v), i.e., the (u, v) entry of S^3, as shown in Example 6.

We next analyze the time and space complexity of SimFusion+ .
Running Time. The algorithm consists of two phases: preprocessing (lines 1-16), and on-line query (lines 17-18).
(i) For the preprocessing, (a) it takes O(m) time to compute A~ (line 1) and O(n) time to initialize v1 (line 2). (b) The total time of the for loop is analyzed in Table 1 (line 3-13), which is bounded by O(m + 4n + k + 1) for each iteration. (c) It takes O(kn) time to compute ^max(A) (line 15). Hence, the total time in this phase is O(m + k(m + 4n + k + 1) + kn), which is bounded by O(km).
(ii) The on-line query phase (lines 17-18) can be done in constant time for each query by virtue of ^max(A) memorization.
Combining (i) and (ii), the query time of SimFusion+ is in O(1), plus an O(km)-time precomputation.
Memory Space. (i) In the precomputation, (a) initializing A~ and
v1 takes O(n) space (lines 1-2). (b) For each iteration k, the space complexity is analyzed in Table 1, which is bounded by O(n) (line 3-13). (c) As the for loop terminates, only v1, · · · , vk and max(Tk) are kept in memory, yielding O(kn + k) space; the other intermediate results can be freed (line 14). (d) Computing ^max(A) takes O(k) space (line 15). Once computed, ^max(A) is

memorized, yielding O(n) space; v1, · · · , vk and max(Tk) are not used subsequently and thus can be freed (line 16).
(ii) For the on-line query (lines 17-18), s^(u, v) can be computed in O(n) space with ^max(A) memorized.
Taking (i) and (ii) together, the total space is bounded by O(kn).

5. INCREMENTAL SIMFUSION+
For certain applications like social networks, graphs are frequently modified [9]. It is too costly to recalculate similarities every time when edges in the graphs are updated. This motivates us to study the following incremental SimFusion+ estimating problem.
Given a network G, the eigen-information in G, and a list G¯ of updates (edge deletions and insertions) to G, it is to compute the new USM S in G . Here G is the updated G, denoted by G + G¯.
The idea is to maximally reuse the eigen-information in G when computing S . The observation is that G¯ is often small in practice; hence, S (= S + S¯) is slightly different from S. It is far less costly to find the change S¯ to the old S than to recalculate the new S from scratch. The main result in this section is the following.

THEOREM 1. The incremental SimFusion+ estimating problem is solvable in O(n) time and O(n) space for every vertex pair, where  is the number of edges affected by the update G¯.

As we shall see later,  captures the size of areas in a graph G that is affected by updates G¯; hence  is much smaller than n when G¯ is small. That is, the incremental SimFusion+ can be performed more efficiently than computing similarities in G . This suggests that we compute the eigenvector of A in G once, and then incrementally compute SimFusion+ when G is updated.
To prove Theorem 1, we first introduce a notion of incremental UAM. We then devise an incremental algorithm for handling batch edge updates with the desired bound.

5.1 Incremental Unified Adjacency Matrix
Consider an old network G = (D, R) and a new G = (D, R ). Incremental UAM. The matrix A¯ is said to be the incremental UAM of the update G¯ (= G - G, i.e., a list of edge insertions and deletions) iff A¯ = A - A, where A and A are the UAMs of the old network G and the new G , respectively. Intuitively, the nonzero entries of A¯ can identify the edges in G that is affected by updates G¯. Typically, A¯ is a sparse matrix when  is small. Indeed, the number of nonzero entries in A¯ is bounded by O(n), which represents the costs that are inherent to the incremental problem itself, i.e., the amount of work absolutely necessary to be performed for the problem. Using A¯ , we next provide a strategy for incrementally computing SimFusion+ similarity.

PROPOSITION 3. Given a network G and an update G¯ to G, let A be the UAM of G, and A¯ the incremental UAM of G¯. Then the new USM S of the new network G (= G + G¯) can be computed as

[S ]i,j = [ ]i · [ ]j with [ ]i = [1]i +

n p=2

cp

×

[p]i

cp

=

pT · p -1

and  = A¯ · 1.

(11)

where p is the eigenvector of A corresponding to the eigenvalue p with p 2 = 1, and 1 is the dominant eigenvector of A.

(Please refer to the Appendix for a detailed proof.) The main idea in incrementally computing S is to reuse A¯ and the eigen-pair (p, p) of the original A. From the computational perspective, memorization techniques can be applied to Eq.(11) for an extra speed-up in computing [ ]i. Once  is computed, it can be memorized for computing c2, · · · , cn. When c2, · · · , cn are calculated, they can be memorized for computing [ ]i and [ ]j.

370

Algorithm 2: IncSimFusion+ (G, A, (p, p), G¯, (u, v))

Input : Network G = (D, R), the old UAM A of G, eigen-pairs (p, p) of A, the update G¯ to G, query (u, v).

Output: New similarity score s (u, v). 1 compute the incremental UAM A¯ for the update G¯ :

A¯  UpdateA (G, A, G¯) ;

2 3

initialize compute

a 

 

[A¯1·]u,1

;

b



[1]v

;

4 free A¯ , 1;

5 for p  2, · · · , n do

6

compute t  pT · , cp  t/(p - 1) ;

7

compute a  a + cp × [p]u, b  b + cp × [p]v ;

8

free p, p ;

9 free , t ; 10 compute s (u, v)  a × b ; 11 return s (u, v) ;

5.2 An Incremental Algorithm for SimFusion+
We next prove Theorem 1 by providing an incremental algorithm, referred to as IncSimFusion+, for handling  edge updates.
Algorithm. The algorithm accepts as input a network G, the UAM A of G, the eigen-pairs (p, p) of A, an update G¯ (a list of edge insertions and deletions) to G, and a vertex pair (u, v).
It works as follows. (a) IncSimFusion+ first computes the incremental UAM A¯ for the update G¯ by using procedure UpdateA (line 1). UpdateA incrementally finds all the changes to the old UAM A in the presence of a list of edge updates to G. (b) For the given vertex pair (u, v), IncSimFusion+ initializes a and b based on the dominant eigenvector 1 of A (line 2) ; it computes  once and memorizes  for computing c2, · · · , cn (line 3). (c) Once computed, c2, · · · , cn are memorized for calculating the u-th and v-th entries of the dominant eigenvector  of the new UAM,which is collected in a and b, respectively (lines 4-9). IncSimFusion+ returns a × b as the similarity s^(u, v) (lines 10-11).
Edge Update. The procedure UpdateA is used for incrementally updating the UAM A by virtue of G¯. An update G¯ is represented as a sequence of 2-tuples (D × D, op) that records every single action of the edge update, in which D × D is a set of  edges to be inserted or deleted, and op is either "+" (edge insertion) or "-" (edge deletion). For instance, after the edge (P3, P5) is added and (P1, P2) is removed from G1 in Fig.1, the update G¯ is denoted by
G¯ = {(P3, P5, +), (P1, P2, -)}.
UpdateA identifies the incremental A¯ in two phases. (i) It first finds the affected nodes and the data spaces for each edge update in G¯ using a breadth-first search. (ii) It then updates the corresponding entries of A¯ based on the following. We abuse the notation ND(u) to denote all the neighbors of object u in the data space D, i.e.,

ND(u) = {v  D| (u, v)  R}.

Based on ni = |Di

the partition of the entire data |, the incremental UAM A¯ can

space D =

N i=1

Di

with

be accordingly partitioned

into N 2 submatrices A¯ i,j.

· For each edge insertion (u, v, +)  G¯ with u  Di and

v  Dj , (i) we set all entries of [A¯ i,j]u,

to

- i,j
nj

except

for their v-th entries to 0 if NDj (u) = ; (ii) we set all

entries of [A¯ j,i] if NDi (v) = ;

,v to (iii)

- j,i
ni
we set

except for their u-th entries to [A¯ i,j ]u,v = i,j otherwise.

0

· For each edge deletion (u, v, -)  G¯ with u  Di and

v  Dj , (i) we set all entries of [A¯ i,j]u,

to

i,j nj

except

for their v-th entries to 0 if |NDj (u)| = 1; (ii) we set all

entries of |NDi (v)|

[A¯ j,i = 1;

]

(i,vii)towenjsi,iete[xA¯cei,pjt]ufo,vr

their u-th entries to 0 = -i,j otherwise.

if

Complexity. The algorithm IncSimFusion+ is in O(n) time and O(n) space for handling  edge updates in G¯. (i) The procedure UpdateA can be bounded by O(n) time and O(n) space (line 1). For each edge update in G¯, it is in at most O(n) time and O(n) intermediate space to update the corresponding entries of A¯ . (ii) Computing  requires an O()-time and O(n)-space sparse matrix-vector multiplication A¯ · 1 (line 3). (iii) For every cp, it takes O() time and O(n) space to calculate pT ·  (line 6) since  is a sparse vector with only O() nonzeros; and c2, · · · , cp are memorized for computing [ ]i, which requires O(n) time and O(n) space in total. (iv) Computing a, b and s (u, v) needs con-
stant time and space (lines 7 and 10). Thus, combining (i)-(iv) ,
the total complexity is bounded by O(n) time and O(n) space.

EXAMPLE 8. We show how IncSimFusion+ works. Consider the graph G1 in Fig.1 with its UAM A in Example 3. Suppose two edges (P1, P2) and (P2, P1) are removed from G1. The new USM S is updated as follows.
First, UpdateA is invoked for precomputing the incremental A¯ and the eigen-pairs (p, p) of A in an off-line fashion:

A¯

=

 0



-

1 6

0

0

-

1 6

0

0

0

0 0 0 0

0 0 0 0

 0

0 0 0



0 0 000

p p

p

1 1.184

[.431 .673 .451 .322 .232]T

2 .503 [.708 -.522 -.242 .388 .132]T

3 -.480 [-.256 -.020 .095 .716 -.641]T

4 -.366 [-.021 -.507 .853 -.119 .017]T

5 .242 [.497 .127 .037 -.467 -.719]T

cp - .062 -.018 -.025 .069

IncSimFusion+ next computes  from A¯ and 1 (line 3):  = A¯ · 1 = [-.112 -.072 0 0 0]T .

Then, cp can be derived from the memorized  and (p, p), e.g.,

c2 = 2T · /(2 - 1) = -.0419/(.503 - 1.184) = .062, c3 = 3T · /(3 - 1) = .030/(-.480 - 1.184) = -.018.

Once computed, c2, · · · , c5 are memorized for calculating [ ] :

 = 1 +

5 p=2

cp

×

p

=

[.327

.703

.485

.326

.266]T

.

Hence, applying [ ] to the new USM S (line 10) yields

.107 .230 .159 .107 .087

S = ...211350097

.494 .341 .230

.341 .235 .158

.230 .158 .107

...110828797 .

.087 .187 .129 .087 .071

6. EXPERIMENTAL EVALUATION
In this section, a comprehensive empirical study of the proposed similarity estimating methods is presented.
6.1 Experimental Setting
Datasets. We used three real-life datasets and a synthetic dataset.
(1) MSN Data. 7 The MSN search log data were taken from "Microsoft Live Labs: Accelerating Search in Academic Research". This dataset was also used in the prior work [1]. It contains about 15M user queries from the United States in May 2006 and the corresponding clickthrough URLs. The dataset was formatted by showing each query, the URLs of the associated web pages, and the number of clickthroughs by query, as depicted below.
7http://research.microsoft.com/ur/us/fundingopps/RFPs/Search_2006_RFP.aspx

371

Query

URL

Clicks URL

Clicks

Shopping shopping.yahoo.com 2,375 www.ebay.com 1,859

The 15K most common queries in the search log were chosen, and the hyperlinks from the contents of the top 32K popular web pages were parsed. We built a network G = (D, R), which consists of a web page space Dw and a query space Dq.
(2) DBLP Data. 8 This dataset was derived from a snapshot of the computer science bibliography (from 2001 to 2010). We selected the research papers published in the following conference proceedings: "SIGIR", "KDD", "VLDB", "ICDE", "SIGMOD" and "WWW". Choosing a time step of two years, we built 5 DBLP web graphs Gi (i = 1, · · · , 5) with the sizes listed below:

G1: 01-02 G2: 01-04 G3: 01-06 G4: 01-08 G5: 01-10

|D|

1,838

3,723

5,772

9,567

12,276

|R|

7,103

14,419

29,054

45,310

64,208

For each graph Gi = (Di, Ri), two data spaces were used: paper space Dpi and author space Dai .
(3) WEBKB Data. 9 This dataset collects web pages from the
computer science departments of four universities: Cornell (CO),
Texas (TE), Washington (WA) and Wisconsin (WI). It was also
used in the previous work [13] for link-based similarity estimation. For each university, a network GUi = (Di, Ri) was built, in which (a) the web pages in Di were classified into 7 categories (data spaces): student, faculty, staff, department, course, project and others, and (b) the UAM of Ri represented the hyperlink adjacency matrix. The sizes of these networks are as follows:

U1: CO U2: TE U3: WA U4: WI

|D| 867

827

1,263 1,205

|R| 1,496 1,428 2,969 1,805

(4) Synthetic Data. The data were produced by the C++ boost

graph generator, with 2 parameters: the number of vertices and

the number of edges. Varying the graph parameters, we used this

dataset to represent homogenous networks for an in-depth analysis.

Compared Algorithms. The following algorithms were imple-

mented in C++: (1) SimFusion+ and IncSimFusion+ ; (2) SF,

a SimFusion algorithm via matrix iteration [1]; (3) CSF, a variant

of SF, which leverages PageRank stationary distribution [13]; (4)

SR, a SimRank algorithm via partial sums function [8]; (5) PR, a

Penetrating-Rank algorithm encoding both in- and out-links [4].

Evaluation Metrics. For evaluating the performance of the

algorithms, we used Normalized Discounted Cumulative Gain

(NDCG) metrics [13]. The NDCG at a rank position p is defined as

NDCGp

=

1 IDCGp

p i=1

, 2ranki -1
log2 (1+i)

where

ranki

is

the

graded

rele-

vance of the similarity result at rank position i, and IDCGp is the

normalization factor to guarantee that NDCG of a perfect ranking

at position p equals 1.

Twelve IT experts were hired to judge the similarity of the five

algorithms. The final judgment was rendered by a majority vote.

All experiments were run on a machine with a Pentium(R) Dual-

Core (2.00GHz) CPU and 4GB RAM, using Windows Vista. The

algorithms were implemented in Visual C++. Each experiment was

repeated over 5 times, and the average is reported here.

6.2 Experimental Results

6.2.1 Accuracy
We first evaluated the accuracy of SimFusion+ vs. SF, CSF, SR and PR in estimating the similarity, using real-life data.
We randomly chose 50 queries and 40 pages from the MSN query log, and compared the average NDCG10 (and NDCG30) of the five
8http://www.informatik.uni-trier.de/~ley/db/ 9http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/

SF+

SF

CSF

SR

PR

0.8

0.8

AVG NDCG30

AVG NDCG10

0.6

0.6

0.4

0.4

0.2

0.2

0 Query

Webpage

0 Query

Webpage

Figure 4: Comparing SimFusion+ with other ranking algorithms for the average NDCG10 and NDCG30 on MSN data

SF+

SF

CSF

SR

PR

1

1

0.8

0.8

NDCG10

NDCG10

0.6

0.6

0.4

0.4

0.2

0.2

01 3 5 7 9 11 13 15 Query

0 1 3 5 7 9 11 13 15 Webpage

Figure 5: The detailed query-by-query and page-by-page comparisons for NDCG10 on MSN data

algorithms. The results are shown in Figure 4, in which the x-axis categorizes the objects according to query and web page. We find the following. (i) In most cases, SF seems hardly to get sensible
similarities because with the increasing number of iterations, all the similarities of SF will asymptotically approach the same val-
ue. This verifies the convergence issue of the original model [1]. (ii) When SF did not fail, SimFusion+ always gave more accurate
estimation on average than the other algorithms. For instance, for the top 10 queries, the average NDCG10 of SimFusion+ (0.79) is 10x better than SF (0.07), 39% better than CSF (0.57), 58% better than SR (0.50), and 15% better than PR (0.69), whereas for the top 30 web pages, the average NDCG30 of SimFusion+ (0.64) is 12x better than SF (0.05), 45% better than CSF (0.44), 33% better than SR (0.48), and 21% better than PR (0.53). This is be-
cause substituting UAM for URM effectively avoids divergent or trivial solutions, thus improving the quality and reliability of SimFusion+ similarity, as expected.
To further verify the accuracy, we randomly selected another 15
queries and 15 web pages from MSN data. In Figure 5, the query-
by-query and page-by-page comparisons are shown for NDCG10
of the five algorithms. We observe that (i) for 12 out of 15 queries, SimFusion+ achieved highest accuracy of the five algorithms;for 13 out of 15 web pages, SimFusion+ outperformed the other algorithms in its accuracy, and was slightly less accurate than PR for only 2 pages. (ii) For all the queries and web pages, SimFusion+ showed the best accuracy performance on average, PR the second, and SF the worst. This is because SimFusion+ uses UAM
to encode the intra- and inter-relations in a comprehensive way,
thus making the results unbiased. We also evaluated the performance of SimFusion+ on D-
BLP and WEBKB datasets. In DBLP experiments, 20 authors were randomly chosen from G1:01-02, G3:01-06 and G5:01-10 data, respectively. We compared the similarity of the top 10 authors in Gi (i = 1, 3, 5) estimated by the five algorithms. The results of the average NDCG10 are depicted in Figure 6. It can be seen that SimFusion+ again achieved better accuracy on DBLP data. For instance, SimFusion+ (0.88) on G3:01-06 was 13x better than SF (0.06), 95% better than CSF (0.45), 26% better than SR (0.7), and 19% better than PR (0.74). In WEBKB experiments, we com-
puted NDCG within 10 web pages for each object in each universi-
ty data (CO,TE,WI,WA) and evaluated the average scores. Figure

372

SF+

SF

CSF

SR

PR

1

1

AVG NDCG10

AVG NDCG10

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0 01-02 01-06 01-10 DBLP Dataset

0 CO TX WI WA WebKB Dataset

Figure 6: Comparing SimFusion+ with other ranking algo-

rithms for the average NDCG10 on DBLP and WEBKB data

200K

300

SF+

SF+

150K

SF CSF

SR

100K

PR

SF

200

CSF

SR

PR

50K

100

Memory (MB)

CPU Time (sec)

001-02 01-04 01-06 01-08 01-10

001-02 01-04 01-06 01-08 01-10

Figure 7: Comparing the CPU time and memory of the ranking algorithms on DBLP data
6 shows that SimFusion+ outperformed the other 4 algorithms on CO, WI and WA data, except that PR (0.8) did 6% better than SimFusion+ (0.75) on TX data. This tells that SimFusion+ accuracy performance is consistently stable on different experimental datasets.
6.2.2 CPU Time & Memory Space
We then evaluated the running time and memory space efficiency of SimFusion+ , SF, CSF, SR and PR using real datasets.
Figure 7 shows the CPU time and memory consumption for the five algorithms on DBLP. The total time and memory for each algorithm showed an increasing tendency with the growing size of DBLP. It can be noted that the time for SimFusion+ was at least one order of magnitude faster than CSF and SR on average, and more than 20x faster than PR and SF, whereas the space for SimFusion+ and SR increased linearly with the size of DBLP, in contrast with the quadratic increase in memory for SF, CSF, PR, as expected. This drastic speedup and decrease in RAM is due to the memorization of max(Tk) for computing USM, thus saving much time and space for repetitive matrix multiplications.
To further evaluate the efficiency, we compare the time and memory of the five ranking algorithms on WEBKB. In Figure 8, the results indicate that SimFusion+ took about 10x less time than SF and PR, and 6x less time than CSF and SR on average. The memory space for SimFusion+ was also efficient and scaled well with the size of WEBKB. It can be seen that SF also took small memory space (approx. 1M) when the data size was small (e.g., CO and TX). However, when the data size increased (e.g., WI and WA), SF was less useful since large memory storage (about 2.5M) was required to keep the intermediate result of the k-th iterative USM. In all the cases, SimFusion+ performed the best.
On large datasets, the effect of SimFusion+ is even more pronounced. Figure 9 reports the average time and memory of the five algorithms on MSN data, in which the y-axis is log-scale. We chose 50 queries and 40 web pages from Dq and Dw, respectively. For each oq  Dq (resp. ow  Dw), we estimated the similarity between oq (resp. ow) and other object o  Dq  Dw. We see that SimFusion+ was highly efficient on large datasets (nearly 2 orders of magnitude than SF and PR, and 1 order of magnitude than CSF in both time and space). This validates that the performance of SimFusion+ is fairly stable among different datasets.
6.2.3 Incremental Performance
We next evaluated the incremental performance of IncSimFu-

Memory (MB)

CPU Time (sec)

300

SF+

SF

200

CSF SR

PR

100

6

SF+

SF

4

CSF SR

PR

2

0 CO TX WI WA

0 CO TX WI WA

Figure 8: Comparing the CPU time and memory of the ranking algorithms on WEBKB data

Memory (MB)

CPU Time (sec)

SF+

SF

CSF

SR

PR

105

103

104

102

103

101

102 Query

Webpage

100 Query

Webpage

Figure 9: Comparing the CPU time and memory of the ranking algorithms for the given query and web page on MSN data

sion+ on real datasets. Given a sequence of edge updates ( edge insertions and deletions in G¯), we compared the running time of IncSimFusion+ with that of SimFusion+ ; the latter had to recalculate the UAM when edges were updated, and the computational cost was counted. In these experiments, the eigen information of the old UAM can be preconditioned in an off-line fashion and shared by all the updated graphs for incremental computation, and hence their costs were not counted at the query stage. Due to space limitations, below we only reported the results on MSN dataset.
Varying  (the number of the edges to be updated) from 600 to 3000, we first evaluated the running time of IncSimFusion+ and SimFusion+ over MSN data, respectively, for estimating all the similarities between the query objects in Dq. Figure 10 shows that IncSimFusion+ outperformed SimFusion+ when  < 2800, but SimFusion+ performed better for larger , as expected. This is because the small value of  often preserves the sparseness of the incremental UAM, and hence reduces the computational cost of  when the USM was incrementally updated.
To further validate the performance of IncSimFusion+ , we estimated the similarity among the web page in Dw and tested its CPU time on MSN data. In Figure 10, the result shows that IncSimFusion+ was highly efficient for the small number of edge updates. When  > 7700, SimFusion+ did better than IncSimFusion+ . This tells that increasing the number of updated edges induces more nonzeros in A¯ , thus increasing the difficulty of incremental computation. We also noticed that the SimFusion+ time was less sensitive to the small number of updated edges, whereas the IncSimFusion+ time was linearly increased with , as expected. This is because once the edges are changed, SimFusion+ has to recompute all the similarities from scratch. In contrast, IncSimFusion+ only computes the similarities from the affected area of edge updates. In light of this, IncSimFusion+ scales well with .
6.2.4 Effect of
We used 9 web graphs with the number of vertices increased from 600K to 1.4M. We varied from 0.01 to 0.0001 and ran SimFusion+ on each graph. The results are reported in Figure 11. It can be seen that the computational time and memory consumption for SimFusion+ was sensitive to . The smaller the is, the larger amounts of the CPU time and memory space are, as expected. These confirmed our observation in Section 4, where we envisage that the small choice of imposes more iterations on computing Tk and vk, and hence increases the estimation costs.

373

CPU Time (sec)

CPU Time (sec)

4K

IncSF

3K

SF+

|D| = 47, 000

2K

|R| = 127, 600

|Dq| = 15, 000

1K

15K 10K
5K

IncSF SF+
|D| = 47, 000 |R| = 127, 600 |Dw| = 32, 000

0 600 1200 1800 2400 3000

0 800 2400 4000 5600 7200 8800

#-edges Updated

#-edges Updated

Figure 10: Comparing the CPU time of IncSimFusion+ with that of SimFusion+ on MSN data

CPU Time (sec)

Memory Space (MB)

200K 150K

= 0.0001 = 0.001 = 0.01

1,000 750

= 0.0001 = 0.001 = 0.01

100K

500

50K

250

0 0.6M 0.8M 1M 1.2M 1.4M

0 0.6M 0.8M 1M 1.2M 1.4M

Figure 11: Effect of for SimFusion+ on synthetic data

7. CONCLUSIONS
We present SimFusion+, a revision of SimFusion, for preventing the trivial solution and the divergence issue of the SimFusion model. We propose efficient techniques to improve the time and space complexity of SimFusion+ computation with accuracy guarantees. We also devise an incremental algorithm to compute SimFusion+ similarity on dynamic graphs when edges are frequently updated. The empirical results on both real and synthetic datasets show that our methods achieve high performance and result quality.
We are currently studying the vertex-updating methods for incrementally computing SimFusion+. We are also to extend our techniques to parallel SimFusion+ computing on GPU.

8. REFERENCES
[1] W. Xi, E. A. Fox, W. Fan, B. Zhang, Z. Chen, J. Yan, and D. Zhuang, "SimFusion: Measuring similarity using unified relationship matrix," in SIGIR, pp. 130­137, 2005.
[2] L. Page, S. Brin, R. Motwani, and T. Winograd, "The PageRank citation ranking: Bringing order to the web," technical report, Stanford InfoLab, November 1999.
[3] G. Jeh and J. Widom, "SimRank: A measure of structural-context similarity," in KDD, pp. 538­543, 2002.
[4] P. Zhao, J. Han, and Y. Sun, "P-Rank: A comprehensive structural similarity measure over information networks," in CIKM, pp. 553­562, 2009.
[5] H. Small, "Co-citation in the scientific literature: A new measure of the relationship between two documents," J. Am. Soc. Inf. Sci., vol. 24, no. 4, pp. 265­269, 1973.
[6] B. Jarneving, "Bibliographic coupling and its application to research-front and other core documents," J. Informetrics, vol. 1, no. 4, pp. 287­307, 2007.
[7] W. Xi, B. Zhang, Z. Chen, Y. Lu, S. Yan, W. Ma, and E. A. Fox, "Link Fusion: A unified link analysis framework for multi-type interrelated data objects," in WWW, pp. 319­327, 2004.
[8] D. Lizorkin, P. Velikhov, M. N. Grinev, and D. Turdakov, "Accuracy estimate and optimization techniques for SimRank computation," VLDB J., vol. 19, no. 1, pp. 45­66, 2010.
[9] C. Li, J. Han, G. He, X. Jin, Y. Sun, Y. Yu, and T. Wu, "Fast computation of SimRank for static and dynamic information networks," in EDBT, pp. 465­476, 2010.
[10] G. He, H. Feng, C. Li, and H. Chen, "Parallel SimRank computation on large graphs with iterative aggregation," in KDD, pp. 543­552, 2010.
[11] W. Yu, W. Zhang, X. Lin, Q. Zhang, and J. Le, "A space and time efficient algorithm for SimRank computation," World Wide Web, vol. 15, no. 3, pp. 327­353, 2012.
[12] G. Xue, H. Zeng, Z. Chen, Y. Yu, W. Ma, W. Xi, and E. A. Fox, "MRSSA: An iterative algorithm for similarity spreading over interrelated objects," in CIKM, pp. 240­241, 2004.
[13] Y. Cai, M. Zhang, C. H. Q. Ding, and S. Chakravarthy, "Closed form solution of similarity algorithms," in SIGIR, pp. 709­710, 2010.
[14] G. Williams, Linear Algebra with Applications. Jones and Bartlett Publishers, 2007.
[15] A. J. Laub, Matrix Analysis for Scientists and Engineers. SIAM: Society for Industrial and Applied Mathematics, 2004.
[16] R. A. Horn and C. R. Johnson, Matrix Analysis. Cambridge University Press, February 1990.
[17] Y. Saad, Iterative Methods for Sparse Linear Systems, Second Edition. Society for Industrial and Applied Mathematics, 2 ed., April 2003.

Appendix: Proofs

Proof of Proposition 2. PROOF. Let (x) = Ax - (x) · x be a vector function of x 
Rn, with  (x) being a real function of x. To simplify notations, we shall denote by k the dominant eigenvalue of Tk, and
k = max(Tk), k = Vk · max(Tk),  = max(A).
Using Tkk = kk and Eq.(7) in Lemma 1, we have
 (k) = VkTkk + kvk+1eTk k - kVkk = kvk+1(eTk k) = k[k]kvk+1,
where [k]k denotes the k-th entry of k. Hence,
k -  2   (k) 2 = k[k]k vk+1 2 = k[k]k . Since vec(Sk) = k  k and vec(S) =   , we have
Sk - S 2 = vec(Sk) - vec(S) 2 = k  k -    2 = k  (k - ) + (k - )   2  k 2 · k -  2 + k -  2 ·  2

1

1

= 2 × k -  2  2 × |k × [k]k|,

which completes the proof.

Proof of Proposition 3.

PROOF. For the new graph G , let  be the dominant eigenvector of A with its eigenvalue  , and ¯1 =  - 1, ¯1 =  - 1, A¯ = A - A. Then, A  =   can be rewritten as

(A + A¯ )(1 + ¯1) = (1 + ¯)(1 + ¯1).

Expanding the above equation, eliminating A¯ ¯1 and ¯1¯1 (the high-order infinitesimals of ¯1), and using A1 = 11, we have

A¯1 +  = 1¯1 + ¯11 with  = A¯ 1.

(12)

Since 1, · · · , n of A constitute a basis for Rn, there exist scalars c1, · · · , cn s.t. ¯1 = c11 + · · · + cnn. Substituting this into Eq.(12) and pre-multiplying both sides by jT produce

n

n

jT ciii+jT  = 1jT cii+¯1jT 1. (j = 2, · · · , n)

i=1

i=1

Due to i orthonormality (i = 1, · · · , n), we have

cj j jT j +jT  = cj 1jT j + ¯1jT 1 .

=cj j

=cj 1

=0

Hence, cj = (jT )/(j - 1) with  = A¯ 1 (j = 2, · · · , n). To determine c1, we use the identity (1 + ¯1)T (1 + ¯1) = 1.
Expanding the left-hand side, eliminating the high-order infinitesimal ¯1T ¯1, and replacing ¯1 with c11 + · · · + cnn, we have

n

n

1T 1 + 1T

cii +

ciiT 1 = 1.

=1

i=1

i=1

=c1

=c1

Due to 1, · · · , n orthonormality, it follows that c1 = 0. Thus,

n
vec(S ) =    with  = 1 + ¯1 = 1 + cp · p,
p=2

cp

=

pT  p - 1

,

and



=

A¯ 1.

374


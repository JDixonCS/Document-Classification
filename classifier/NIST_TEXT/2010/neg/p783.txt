Text Document Clustering with Metric Learning

Jinlong Wang, Shunyao Wu
School of Computer Engineering Qingdao Technological University
Qingdao, 266033, China
{wangjinlong, shunyaowu}@gmail.com

Huy Quan Vu, Gang Li
School of Information Technology Deakin University
Victoria 3125, Australia
{hqv, gang.li}@deakin.edu.au

ABSTRACT
One reason for semi-supervised clustering fail to deliver satisfactory performance in document clustering is that the transformed optimization problem could have many candidate solutions, but existing methods provide no mechanism to select a suitable one from all those candidates. This paper alleviates this problem by posing the same task as a soft-constrained optimization problem, and introduces the salient degree measure as an information guide to control the searching of an optimal solution. Experimental results show the effectiveness of the proposed method in the improvement of the performance, especially when the amount of priori domain knowledge is limited.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering
General Terms
Algorithm, Experimentation
Keywords
Document Clustering, Metric Learning
1. INTRODUCTION
As one of the most fundamental data mining tasks, clustering is a subjective process in nature: different users may want different clusterings when exploring the same data set. However, specifying an appropriate similarity measure in advance is usually difficult for general users. Recently, semisupervised clustering which can utilize priori pair-wise constraints has attracted a lot of research interest [5].
For the topic of document clustering, there have been some pioneer work in applying semi-supervised clustering to increase clustering quality [3]. However, their performance is still not as good as expected, especially when the number of priori constraints is limited [2, 3]. One of the possible reasons is that: existing work transforms the clustering into an optimization problem, with priori constraints as hard constraints. When the number of constraints is inadequate, there could be many candidate solutions to this optimization problem, and existing methods fail to provide a mechanism to select a suitable one from these possible solutions.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Moreover, when the priori knowledge contains some inconsistent constraints, existing methods might even not be able to generate a feasible result because no solution can satisfy all constraints.
In order to alleviate this problem so that the document clustering can work more effectively with inadequate priori constraints, we propose a novel soft-constraint algorithm for document clustering: instead of satisfying ALL constraints, the method aims to satisfy constraints as much as possible. The proportion of satisfied constraints is adopted as a heuristic to inform the search of the optimal solution. Experimental results show the effectiveness of the proposed method, especially when the number of priori constraints are limited.

2. CLUSTERING DOCUMENT WITH MET-

RIC LEARNING

Let    stands for the data space which contains 

data points {x1,    , xn}, x = [1,    , ] . Let w = [1,    , ] , and  are the weights for attributes ( =

1, ..., ). For document clustering, we use the weighted 

similarity and set


=1

  

and

w(x, xw

y)=

1-

x,yw xw yw

,

= x, xw. With

where x, yw = a must-link pair-

wise constraints set  and a cannot-link pair-wise constraints

set , the document clustering problem can be transformed

into an optimization problem with the objective as [5]:



w

w(x, y)

(x,y)

 subject to (x,y) w(x, y)   and w  0.
Considering the fact that any  similarity is within the range [0, 1], set (x,y) w(x, y)   ( stands

for the number of elements in the cannot-link set ) in the

algorithm which enforces that every pair in the cannot-link

set  is exactly 1, with an aim to make sure distances of

instances in the cannot-link set  as large as possible.

During the search for an optimal set of weights, we in-

troduce a new measure, the salient degree, to evaluate how

the current clustering result respects the priori constraints.

We utilize k -means method with metric w(., .) to parti-

tion data, and get salient degree of clustering result. The

centroids in clustering process of k -means are estimated as

{ }=1 [1].



=



 x x x  x w

,

where,



represents points

assigned to the cluster . The salient degree is defined as the

proportion of satisifed constraints by the clustering result

783

with

parameter

w,

namely

(w)

=

()+(  + 

)

,

here () means the satisfied constraints in the set .

Accordingly, the document clustering can be carried out

as an optimization problem, but with the salient degree as

a heuristic to decide whether it is necessary to carry out

further gradient descending search or not. The pseudo-code

of this algorithm is provided in Algorithm 1.

Algorithm 1: Document clustering with metric learning

Input: Dataset  , number of output clusters , mustlink constraints , cannot-link constraints .

Output: Clusters obtained with metric learning.

 = 1;

w

=

[

1 

,

...,

1 

];

while not convergent do

 = 1;

w = w;

Step 1: Select initial cluster centroids;

Randomly select cluster centroids, and make sure

(w+1) - (w) > 0,

where, w+1 = w +  (w).

Step 2: Iteration for optimization;

while the value of objective function decreases and

(w+1) - (w) > 0 do

w+1 = w +   (w);

 =  + 1;

end  =  + 1; w = w;

end

3. EXPERIMENTS AND RESULTS
Here we compare the performance of the proposed method with k -means and the hard-constraint algorithm COP-Kmeans implemented according to [4]. The 20Newsgroup dataset is used in our experiment.
From original 20Newsgroup dataset, we randomly select 100 documents for each category, and create 2 datasets: the     3 data set (alt.athei-sm, rec.sport.baseball, sci.space) consisting of 3 clusters on 3 distinct topics, and the    3 data set (comp.graphics, comp.os.ms-windows, comp.windows.x) contains 3 clusters with large overlaps between them. All the datasets have been pre-processed by removing stop-words, and words with too high or too low frequency, and each document is then represented by TFIDF.
We run 10 trials of 2-fold cross-validation for each dataset: 50% of the dataset is used as training set to obtain pairwise constraints, and the other half is used as input of compared algorithms after peering off its class/clustering information. The clustering results are then compared with the "ground truth" clustering using Normalized Mutual Information (  ) and   measures.
The results are shown as Figure 1 and Figure 2. On both data sets, we can see that the proposed method outperforms the other methods in both the    and the   measures. Additionally, the results in our method are more stable. Another important observation is that: When the amount of priori knowledge is adequate, our method performs similarly with the compared methods; but when the amount of priori knowledge is limited, our method can still achieve satisfactory clustering results, while the performance of other methods deterioriate significantly.

(a) NMI

(b) Purity

Figure 1: Clustering result on     3

(a) NMI

(b) Purity

Figure 2: Clustering result on    3

4. CONCLUSIONS
This paper proposes an efficient soft-constraint algorithm by obtaining a satisfactory clustering result so that the constraints will be respected as many as possible. Experiments show the advantage of the proposed algorithm especially when provided with little priori domain knowledge, the proposed method is more robust and accurate than the existing methods.
5. ACKNOWLEDGMENTS
This paper was partially supported by the National Natural Science Foundation of P.R.China (No.60802066), the Excellent Young Scientist Foundation of Shandong Province of China under Grant (No.2008BS01009) and the Science and Technology Planning Project of Shandong Provincial Education Department (No.J08LJ22).
6. REFERENCES
[1] S. Basu, M. Bilenko, and R. J. Mooney. A probabilistic framework for semi-supervised clustering. In KDD '04, pages 59­68, 2004.
[2] I. Davidson, K. Wagstaff, and S. Basu. Measuring constraint-set utility for partitional clustering algorithms. In PKDD '06, pages 115­126, 2006.
[3] A. Huang, D. Milne, E. Frank, and I. H. Witten. Clustering documents with active learning using wikipedia. In ICDM '08, pages 839­844, 2008.
[4] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. Constrained k-means clustering with background knowledge. In ICML '01, pages 577­584, 2001.
[5] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell. Distance metric learning with application to clustering with side-information. In NIPS '02, pages 505­512, 2002.

784


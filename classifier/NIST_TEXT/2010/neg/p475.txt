Combining Coregularization and Consensus-based Self-Training for Multilingual Text Categorization

Massih-Reza Amini

Cyril Goutte

Nicolas Usunier

National Research Council Canada Institute for Information Technology 283, boulevard Alexandre-Taché
Gatineau, J8X 3X7, Canada First.Last@nrc-cnrc.gc.ca

Université Pierre et Marie Curie Laboratoire d'Informatique de Paris 6 104, avenue du Président Kennedy
75016 Paris, France Nicolas.Usunier@lip6.fr

ABSTRACT
We investigate the problem of learning document classifiers in a multilingual setting, from collections where labels are only partially available. We address this problem in the framework of multiview learning, where different languages correspond to different views of the same document, combined with semi-supervised learning in order to benefit from unlabeled documents. We rely on two techniques, coregularization and consensus-based self-training, that combine multiview and semi-supervised learning in different ways. Our approach trains different monolingual classifiers on each of the views, such that the classifiers' decisions over a set of unlabeled examples are in agreement as much as possible, and iteratively labels new examples from another unlabeled training set based on a consensus across language-specific classifiers. We derive a boosting-based training algorithm for this task, and analyze the impact of the number of views on the semi-supervised learning results on a multilingual extension of the Reuters RCV1/RCV2 corpus using five different languages. Our experiments show that coregularization and consensus-based self-training are complementary and that their combination is especially effective in the interesting and very common situation where there are few views (languages) and few labeled documents available.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information Storage - Record classification; I.2 [Artificial Intelligence]: Learning
General Terms
Algorithms, Experimentation, Theory
Keywords
Multilingual Document Classification, Learning from Multiple Views, Semi-supervised Learning
Copyright 2010 Crown in Right of Canada. This article was authored by employees of the National Research Council of Canada. As such, the Canadian Government retains all interest in the copyright to this work and grants to ACM a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, provided that clear attribution is given both to the NRC and the authors. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

1. INTRODUCTION
In this paper, we address the problem of semi-supervised learning of document classifiers in a multilingual setting where documents are available as a parallel corpus with two or more languages for which labels are only partially available.
Our motivation is that multilingual collections are becoming more and more common in national and supranational contexts. However, the bulk of document classification and organization techniques and research is developed in the monolingual setting, most often for English. In addition, labeling text documents may require cost- and time-intensive human annotation, hence the widespread interest for semisupervised text classification approaches that leverage unlabeled documents to speed-up the learning process.
Our work addresses the two issues of limited annotation and multilingual setting. Using the different languages as different views on a document, we develop a multiview, semisupervised approach that learns from collection of multilingual documents.
We formalize the problem as follows. Given a collection of partially-labeled documents written in different languages and belonging to a set of classes that is fixed across languages, we wish to learn a number of monolingual classifiers for this common set of classes. Note that this problem is different from cross-language text categorization [5], where a document written in one language must be classified in a category system learned in another language.
In our setting, we assume that each document is available in several languages and we are interested in learning improved monolingual classifiers. We also emphasize that we wish to develop inter-dependent monolingual classifiers, rather than a single multilingual classifier, as we wish to be able to classify an incoming document in whatever language it is made available, without having to translate it beforehand.
There have been at least two approaches to multiview semi-supervised learning. One can use coregularization [19] to improve the view-specific classifiers by constraining them to agree on some unlabeled data, leveraging unlabeled data in a multiview learning framework. A more recent proposal [3], by contrast, leverages the multiple views in a semisupervised learning framework by using the consensus between the different views in a self-training framework. Our solution is to combine those two components into a single boosting-based algorithm. View-specific classifiers are trained using coregularization, and a consensus-based self-

475

training process iteratively labels unlabeled examples on which the view-specific classifiers agree.
Using a large publicly available corpus of multilingual documents extracted from the Reuters RCV1 and RCV2 corpora, we show that our approach consistently improves over both coregularization and self-training taken in isolation. We also analyze the conditions in which the combination is most profitable. It turns out that adding coregularization to consensus-based self-training helps most when there are few languages and few documents available. This is a particularly interesting setting when resources are limited, and corresponds in particular to the common situation of bilingual data.
In the next section, we position our work with respect to the state of the art. In Section 3, we then present the problem of multiview semi-supervised learning for multilingual text classification. Section 4 describes the boosting-based algorithm we developed to obtain the language-specific classifiers. In Section 5, we present experimental results obtained with our approach on a subcollection of the Reuters RCV1/RCV2 corpus. Finally, in Section 6 we discuss the outcomes of this study and give some pointers to further research.
2. RELATION TO STATE-OF-THE-ART
Document classification has been a very popular application domain for Machine Learning algorithms, and in particular for multiview [7] and semi-supervised learning [16, 12]. The setting of multilingual document classification, however, has been much less studied so far [1, 2].
Interestingly, the original work on co-training [7] introduced both multiview and semi-supervised learning on a document classification task. Since then, both fields have developed greatly but mostly independently. Semi-supervised learning approaches include generative approaches, densitybased or graph-based approaches (cf. [9] for an overview). Multiview learning techniques include multiple kernel learning [4] and techniques relying on kernel Canonical Correlation Analysis [11].
Some recent work more in line with the original co-training approach have introduced coregularization [19, 8], where classifiers are learnt in each view using a multiview regularizer that constrains predictions made in each view to be as similar as possible.
When this multiview regularizer is computed on unlabeled data, this provides a way to perform semi-supervised learning in a multiview setting. More recently, a semi-supervised multiview approach has been developed [3] where classifiers are learned on each view using standard single view training, but unlabeled examples are iteratively labeled in a selftraining manner using the consensus across the views. The multiview consensus ensures higher confidence in the labeling, which yields improved semi-supervised learning rates.
Our work analyses and illustrates the combination of these two techniques. We use a coregularization component similar to [19, 8], with the key difference that instead of the coregularized least squares, we penalize disagreement using a Kullback-Leibler divergence which has a more natural interpretation in the context of probabilistic classifier outputs. In addition, it allows us to develop a novel boosting-based algorithm for solving the coregularized multilingual classification problem.

We combine this coregularized learning with a consensusbased self-training framework similar to [3] where unlabeled documents are iteratively labeled using the consensus prediction across the multiple views.
As both coregularization and consensus-based self-training use multiview information and unlabeled data for training, the key question we address is to see whether the two techniques can be complementary and improve on each other, as opposed to being completely redundant. We also investigate in which conditions such a complementarity may be exploited. We are particularly interested in the effects of coregularization in the common situation where the number of views is small (eg bilingual documents) and few labeled data are available.

3. FRAMEWORK
We consider V input spaces Xv  Rdv ; v  {1, .., V }, and an output space Y. We take Y = {-1, +1} since we restrict our presentation to binary classification. Each multiview document x  X1 × ... × XV is a sequence
x =def (x1, ..., xV )
where each view xv provides a representation of the same document in a different vector space Xv. In the seminal work on co-training [7], web pages are represented by either their textual content (first view) or anchor text pointing to them (second view). In our setting of multilingual classification, each view is the textual representation in a different language. Although typically one of the views is the original version of the document and the others are its translations, we never rely on this information and treat all views equally. Note that in this framework all views of each document are present simultaneously, hence we deal with multilingual text classification in a parallel corpus.
We further assume that we have a labeled training set Z = {(xi, yi)|i  {1, .., l}} and a possibly much larger set of unlabeled training data that we split into two parts denoted respectively by XU1 = {xl+i|i  {1, .., m1}} and XU2 = {xl+m1+i|i  {1, .., m2}}. Our goal is to obtain V binary classifiers {hv : Xv  {-1, 1}|v  {1, .., V }}, working each on one view, such that the predictive performance as estimated for example from a test set is optimized. Note that by construction, the label for a given document is the same for all views.

4. MODEL
We iteratively learn each classifier hv, v  V, while keeping fixed the classifiers for the other views, hu, u  V u = v, by optimizing the loss

L(hv, Z

, XU1 , )

=

C(hv, Z

)

+

V

 -1

X V

d(hv, hu, XU1 ),

u=1,u=v

(1)

where C(hv, Z ) is the (monolingual) cost of hv on the labeled training set Z , d(hv, hu, XU1 ) measures the divergence

between the two classifiers hv and hu on the unlabelled documents in XU1 , and  is a discount factor which modulates

the influence of the disagreement cost on the optimization.

476

For the monolingual cost, we consider the standard misclassification error:

C(hv, Z

)

=

1 l

Xl [[yihv(xvi )



0]],

i=1

where [[]] is equal to 1 if the predicate  is true, and 0 otherwise. As this cost is non-continuous and non-differentiable,
it is typically replaced by an appropriate convex and dif-
ferentiable proxy. Following standard practice in Machine Learning algorithms, we replace [[z  0]] by the upper bound a log(1+ e-z), with a = (log 2)-1. The monolingual misclassification cost becomes:

C(hv, Z

)

=

1 l

Xl a log(1 + exp(-yihv(xvi ))),
i=1

Assuming that each classifier output may be turned into
a posterior class probability, we measure the disagreement
between the output distributions for each view using the
Kullback-Leibler (KL) divergence. Using the sigmoid function (z) = (1 + e-z)-1 to map the real-valued outputs of the functions hv and hu into a probability, and assuming that the reference distribution is the output of the classifier learned on the other views, hu, u  {1, ..., V }  u = v, the disagreement d(hv, hu, XU1 ) becomes

d(hv, hu, XU1 )

=

1 m1

X m1 kl( (hu (xul+i))|| (hv (xvl+i ))),
i=1

where for two binary probabilities p and q, the KL diver-

gence is defined as:

,,«

,,

«

kl(p||q) = p log

p q

+ (1 - p) log

1-p 1-q

There are two reasons for choosing the KL divergence: first, it is the natural equivalent in the classification context of the l2 norm used for regression in previous work on coregularization [19, 8, 18]; second, it allows the derivation of a boosting approach for minimizing the local objective function (1), as described in the following section.

4.1 A view-specific boosting-like algorithm
In order to learn the classifier hv for view v, we need to minimize

L(hv, Z

, XU1 , )

=

1 l

Xl a log(1 + exp(-yihv(xvi )))

i=1

+ (V

 - 1)m1

X m1
i=1

X V
u=1,u=v

kl(

(hu

(xul+i))||

(hv

(xvl+i

)))

(2)

We show how the loss-minimization of (2) is equivalent to
the minimization of a Bregman distance. This equivalence
will allow us to employ the boosting-like parallel-update op-
timization algorithm proposed by [10] to learn a linear classifier hv : xv  v, xv minimizing (2).
A Bregman distance BF of a convex, continuously differentiable function F :   R on a set of closed convex set  is defined as

p, q  , BF (p||q) =def F (p) - F (q) - F (q), (p - q) .

One optimization problem arising from a Bregman distance is to find a vector p  , closest to a given vector

q0   with respect to BF , under the set of linear constraints {p  |ptMv = p~tMv}, where p~   is a specified vector and Mv is a n × d matrix, with n the number of examples in the training set and d the dimension of the problem.1
Defining the Legendre transform as

Lf (q, Mv v) =def argmin(BF (p||q) + Mvv , p ), p

the dual optimization problem can be stated as finding a vector q in the closure Q¯ of the set Q = {LF (q, Mvv)|  Rp}, for which BF (p~||q) is the lowest, under the set of linear constraints {q  |qtMv = p~tMv}.
It has been shown that both of these optimization prob-
lems have the same unique solution [14]. Moreover, [10] have
proposed a single parallel-update optimization algorithm to
find this solution in the dual form.
They have further shown that their algorithm is a general
procedure for solving problems which aim to minimize the
exponential loss, like in Adaboost, or a log-likelihood loss,
like in logistic regression. Indeed, they showed the equiv-
alence of these two loss minimization problems in terms of
Bregman distance optimization.
In order to apply the boosting algorithm proposed by [10], we have to define a continuously differentiable function F such that by properly setting , p~, q0 and Mv, the Bregman distance BF (0||LF (q0, Mvv)) is equal to Eq. (2). Following [10], we choose:

X n p   = [0, 1]n, F (p) = vi (pi log pi + (1-pi) log(1-pi)) ,
i=1

where vi are non-negative real-valued weights associated to examples xvi .
This definition yields that p, q   × :

BF

(p||q)

=

X n
i=1

vi

,, pi

log

,,

pi qi

«

+

(1-pi)

log

,,

1-pi 1-qi

««

(3)

and,

i, LF (q, z)i

=

qi

-
e

zvii

1

-

qi

+

qi

-
e

zvii

(4)

Using Equations

(3)

and (4),

and

setting

q0

=

1 2

1,

the

vector

with

all

components

set

to

1 2

,

and

Mv

the

matrix

such that i, j, (Mv)ij = vi yixvij,2 the Bregman distance in

Equation (3) writes:

X n BF (0||LF (q0, Mvv )) = vi log(1 + e-yi

v ,xvi

).

(5)

i=1

1We have deliberately set the number of examples to n as in our equivalent rewriting of the minimization problem the
latter is not exactly m1. 2All vectors i  {1, .., n}, iyixvi should be normalized in order to respect the constraint Mv  [-1, 1]n×d.

477

Algorithm 1: Parallel-update optimization algorithm

Input : Matrix v, Mv  [-1, 1]n×d .

Initialize: Let v, v  0

for v = 1, ..., V do

for t = 1, 2, ... do

Let q(t) be the solution of LF (q0, Mvv(t));

for j = 1, ..., d do

Wv(,tj)+ Wv(,tj)-

 

P Pi:sign((Mv )ij
! i:sign((Mv )ij

)=+1 )=-1

qi(t)|(Mv qi(t)|(Mv

)ij )ij

|; |;

v(t,)j



1 2

log

Wv(t,j)+ Wv(t,j)-

;

end

v(t+1)  v(t) + v(t);

end

end Output : v, the sequence v(1), v(2), ... verifying

lim
t

BF

(0||LF

(q0,

Mv v(t) ))

=

inf
v Rd

BF

(0||LF

(q0,

Mvv ))

Algorithm 2: Coregularized semi-supervised Learning
Input : A set of labeled training examples Z ; Two sets of unlabeled training data XU1 and XU2 ; Initialize: Set ZU\  ; v, h(v0) =def argminh C(h, Z ); repeat
t  1;
repeat
for v = 1, .., V do Learn h(vt) = argminh L(h, Z  ZU\, XU1 , );
end
t  t + 1; until Convergence of (Vv=1h(vt), Z  ZU\, ) ; - Let XU\ be the set of unlabeled examples in XU2 on which all classifiers agree over the class label of
examples ; -XU2  XU2 XU\ ; -ZU\  ZU\  XU\ ; until XU2 =  or XU\ =  ; Output : Classifiers hv, v  {1, ..., V }

By developing Eq. (2), we get:

L(hv, Z

, XU1 , )

=

K

+

1 l

Xl a log(1

+

exp(-yihv (xvi )))

+

i=1

(V

 - 1)m1

X m1
i=1

X V
u=1,u=v

(hu(xul+i))

log(1

+

e-hv

) (xvl+i )

+

(V

 - 1)m1

X m1
i=1

X V (1
u=1,u=v

-

(hu(xul+i)))

log(1

+

ehv

) (xvl+i )

(6)

where K is a constant which does not depend on hv.

In order to make Eq. (6) identical to Eq. (5) (up to a constant), we create, for each unlabeled document xvi  XU1 , two examples (xvi , +1) and (xvi , -1) (which makes n = l +
2m1), and set the weights as follows:

8 >><

a l

if xi  Z ,

vi = >>: (V -1)m1u=X 1V,u=[v[yi = -1]]+ yi(hu(xui )) else.

(7)

As a consequence, minimizing Eq. (2) is equivalent to minimizing BF (0||q) over q  Q¯, where

Q = {q  [0, 1]l+2m1 | qi = (yi v , xvi ), v  Rdv }.

optimize each of the hv classifiers while keeping the classifiers for the other views fixed, until the global objective
X V (Vv=1hv, Z  ZU\, ) = L(hv, Z  ZU\, XU1 , ) (8)
v=1
has reached a (possibly local) minimum. This alternating optimization of partial cost functions bears
similarity with the block-coordinate descent technique [6]. At each iteration, block coordinate descent splits variables into different subsets, the set of the active variables and the sets of inactive ones, then minimizes the objective function along active dimensions while inactive variables are fixed at current values.
Once all language-specific classifiers have been trained we assign class labels to unlabeled examples in XU2 for which all mono-lingual classifiers predict the same class label. These newly labeled examples are added to the labeled training set. We then go back to the boosting-based coregularized classifier training using the combined labeled data, and so on until either no remaining unlabeled example can be labeled by consensus, or all unlabeled examples have been labeled. As shown by [3], focusing on functions which agree across several views reduces the complexity of the function class and therefore improves the prediction ability of the resulting model.
Algorithm 2 summarizes this coregularized self-training strategy.

This equivalence allows us to adapt the parallel-update optimization algorithm described in [10] to learn each specificview classifier, as described in Algorithm 1.
4.2 Coregularized semi-supervised algorithm
We embed the boosting-based coregularized classifier learning inside a self-training framework (cf. [22], Section 3) which relies on consensus across views in order to automatically label documents from an unlabeled document pool XU2 .
Each monolingual classifier hv, v  V is first initialized on the supervised monolingual cost alone, then we iteratively

5. EXPERIMENTS
We conducted a number of experiments aimed at evaluating how the combination of coregularization and consensusbased self-training can help to take advantage of multilingual unlabeled documents in order to learn efficient classification functions.
5.1 Data set
We perform experiments on a publicly available multilingual multiview text categorization corpus extracted from

478

Language English French German Italian Spanish Total

# docs 18,758 26,648 29,953 24,039 12,342 111,740

Class C15 CCAT E21 ECAT GCAT M11

# docs 18,816 21,426 13,701 19,198 19,178 19,421

Table 1: Number of documents per language (left) and per class (right) in Reuters RCV1/RCV2 subcollection used in our experiments.

the Reuters RCV1/RCV2 corpus [3].3 This corpus contains more than 110K documents from 5 different languages, (English, German, French, Italian, Spanish), distributed over 6 classes (Table 1). Documents that originally had more than one of these 6 labels were assigned to the smallest class. We reserved a test split containing 25% of the documents, respecting class and language proportions. Within the training set containing the remaining 75% of documents, we randomly sampled labeled documents (Z ), and split the remaining unlabeled data into two subsets: one for evaluating the coregularization term (XU1 ), and one for the self-training process (XU2 ). The motivation for that split is to avoid bias: as coregularization enforces agreement between classifiers, it may yield artificially high consensus for the examples used in the coregularization term.
This corpus of multilingual documents is originally a comparable corpus as it covers the same subset of topics in all languages. In order to produce multiple views for each documents, each original document extracted from the Reuters corpus was translated in all other languages using a phrasebased statistical machine translation system [20]. The indexed translations are part of the corpus distribution.
More precisely, each document is indexed by the text appearing in its title (headline tag) and body (body tag). As preprocessing, all text is lowercased, digits are mapped to a single digit token, and tokens containing non-alphanumeric characters are removed. For each language, words in a stoplist as well as tokens occurring in less than 5 documents were also filtered out. Documents were then represented as a bag of words, using a TFIDF weighting scheme based on BM25 [17].
Results are evaluated over the test set using the accuracy and the standard F1 measure [21], which is the harmonic average of precision and recall. The reported performance is averaged over the resulting five language-specific classifiers. In addition, we also averaged over 10 random (train/unlabeled/test) sets of the initial collection.
5.2 Experimental setup
To validate the coregularized consensus-based self-training approach described in the previous section, we test the following six classification methods. The first method is a purely supervised technique which does not make use of any unlabeled examples in the training stage. The following methods make use of the multiview and semi-supervised learning approaches in different ways, using coregularization and/or consensus-based self-training separately or in
3http://multilingreuters.iit.nrc.ca/

combination, over different subsets of the unlabeled training documents.
- Baseline method [Boost]: This baseline corresponds to a supervised monolingual boosting model optimizing Eq. 2 for  = 0.
- Coregularized boosting [reg-Boost]: Boosting using coregularization on XU1 , optimizing Eq. 2 for  = 0. This constrains the supervised monolingual boosting models to achieve high agreement among their predictions on XU1 .
- Boosting with self-training [Boost-cst]: Boosting using consensus-based self-training, but no coregularization. This is similar in spirit to the iterative co-training algorithm [7]. Given the language-specific classifiers trained on an initial set of labeled examples, we iteratively assign pseudo-labels to the unlabeled examples in XU2 for which all classifier predictions agree.

- SVM with self-training [SVM-cst]: This is simi-

lar to the previous method except that we use the

SVM-Perf package [13] to learn each language-specific

classifiers instead of boosting. For tuning the hyper-

parameter C, we first tried the leave-one-out cross-

validation we found

strategy. out that

tHheowdeevfearu,ltw(it1lhPsmli=a1ll||txria|i|n)-in1g

sets gave

similar, and in some cases, better results. We there-

fore used that default C in all of our experiments.

- Coregularization+self-training [reg-Boost-cst]: Coregularized boosting using the consensus-based selftraining: The coregularization term is computed over XU1 and self-training iteratively labels documents from XU2 .
- Boosting with full self-training [Boost-cst]: In order to determine when the combination of coregularization and self-training is the most useful, we also trained algorithm Boost-cst using all the unlabeled training examples XU = XU1  XU2 rather than just those in XU2 .
Our aim is to show the gradual effect of each of the multiview and semi-supervised learning approaches on the boosting algorithm, progressing from Boost to reg-Boost and Boost-cst, to reg-Boost-cst. Note that the reg-Boost and Boost-cst algorithms use the two separate unlabeled training subsets in different manners. SVM-cst is the same as Boost-cst using a SVM algorithm instead of Boosting. This will allow us to benchmark the boosting-based algorithm against the state of the art SVM model in a similar framework. Note that adding co-regularization in a SVM implementation requires some significant changes to the underlying code, which is why we do not provide reg-SVM variants. Finally, using all the unlabeled training examples in Boost-cst and comparing the results to reg-Boost-cst will allow us to uncover the situations in which it is beneficial to combine coregularization and self-training rather than use the latter alone on the combined unlabeled data. This gives an idea of the true benefit brought by coregularization.

479

Table 2: Test classification accuracy and F1 of different learning algorithms on the six classes, averaged over 10 random sets of 50 labeled examples per training set. For each class, the best result is in bold, and a  indicates a result that is statistically significantly worse than the best, according to a Wilcoxon rank sum test with p < .01.

Strategy
Boost reg-Boost Boost-cst SVM-cst reg-Boost-cst

C15

Acc.

F1

0.771 0.506 0.793 0.532 0.804 0.572

0.815 0.583

0.823 0.595

CCAT
Acc. F1
0.662 0.398 0.689 0.419 0.708 0.421 0.720 0.438
0.748 0.449

E21
Acc. F1
0.765 0.323 0.783 0.342 0.794 0.365 0.800 0.378
0.815 0.394

ECAT
Acc. F1
0.505 0.347 0.513 0.372 0.511 0.384 0.522 0.395
0.542 0.408

GCAT
Acc. F1
0.781 0.587 0.803 0.608 0.866 0.655 0.873 0.662
0.895 0.687

M11
Acc. F1
0.793 0.586 0.815 0.611 0.848 0.668 0.861 0.676
0.883 0.693

5.3 Experimental Results
We start our evaluation by analyzing the gains provided by coregularization, the consensus-based self-training and the combination of both, over the baseline boosting algorithm. We measure the classification accuracy and F1 for a fixed number of labeled and unlabeled examples in the training set. In order to study the role of unlabeled data on the learning behavior we begin our experiments with very few labeled training examples. The size of the labeled training sets in these first experiments is fixed to 50 (an average of 10 per language), with an equal sampling of 25 positive and 25 negative examples in Z . For coregularization, results are reported for the best discount factor  = 1, although as illustrated in Section 5.3.1, results are fairly stable across a wide range of values. We will later investigate the impact on the test performance of the number of labeled examples and the number of views (cf Sections 5.3.3 and 5.3.2).
Table 2 summarizes results obtained by Boost, reg-Boost, Boost-cst, SVM-cst and reg-Boost-cst averaged over five languages and 10 random splits of tests sets for our six main categories. We use bold face to indicate the highest performance rates, and the symbol  indicates that performance is significantly worse than the best result, according to a Wilcoxon rank sum test used at a p-value threshold of 0.01 [15]. From these results it becomes clear that:
1. Using the first part of the unlabeled training examples (XU1 ) to coregularize the boosting algorithm, algorithm reg-Boost always improves over Boost by an average of 2-3 points in F1.
2. The consensus-based self-training framework implemented in Boost-cst and SVM-cst also improves over the baseline. In addition, it always seems to outperform coregularization (reg-Boost) alone. In this self-training framework, the SVM classifiers SVM-cst tend to outperform the boosting-based classifiers Boost-cst.
3. Finally, the combination of coregularization and selftraining (reg-Boost-cst) produces a further improvement of around 1-2 points in F1 over the best semisupervised result (SVM-cst). The improvement is statistically significant in four classes out of six.
Our analysis of these results is that both coregularization and the consensus-based self-training provide consistent improvements over training independent monolingual classifiers. Both are instances of multiview learning, and both

rely in some way on the consensus between classifiers trained on the different views. The question therefore arises as to how redundant these two techniques are? Our experimental results suggest that these techniques are in fact complementary.
The gains provided by adding coregularization to the selftraining boosting-based model is in fact similar to the gain provided by coregularization in the supervised setting, which suggest that the two effects are essentially independent and additive. In order to analyze more finely the situations in which the combination of coregularization and consensusbased self-training is more advantageous, we compared all the algorithms, including Boost-cst, for different numbers of languages and different amounts of labeled documents. These results are reported in Section 5.3.2 and 5.3.3, right after we address the issue of the discount factor .
5.3.1 The effect of the coregularization factor 
We analyze the influence of the discount factor  on the performance of reg-Boost-cst for varying amounts of labeled training data.4 The results obtained on class E21 are presented in Figure 1. Note that  controls the relative importance of the unlabeled data in the coregularization (with  = 0 corresponding to no regularization). Figure 1 shows that unlabeled examples become relatively less important as more labeled data is available: as the amount of labeled training data increases from 50 to 300, the optimal discount factor  moves away from 1.
We recall that for  = 1, unlabeled data plays the same role in the training procedure as labeled data.
Note also that in all cases, the performance of the resulting classifiers seems relatively stable for a wide range of values of . This suggests that the results are not overly sensitive to a precise choice of discount factor .
5.3.2 The value of labeled data
We also analyze the behavior of the various algorithms for growing initial amounts of labeled data in the training set. Figure 2, illustrates this by showing the F1 measures on classes CCAT and ECAT with respect to the number of labeled documents in the initial labeled training set Z . For all labeled data sizes, the proportion of negative/positive examples is maintained at 50%. As expected, all performance curves increase monotonically with respect to the additional
4We always maintain the proportion of positive/negative documents in the labeled training set to 50%/50%.

480

F1 F1

F1

CCAT 0.55

0.5

0.45

0.4

0.35

0.3

Boost-cst* reg-Boost-cst

SVM-cst

0.25

reg-Boost

Boost

10

50 100 200 400 1000

# of labeled documents in the training set

ECAT 0.55

0.5

0.45

0.4

0.35

0.3

Boost-cst* reg-Boost-cst

SVM-cst

0.25

reg-Boost

Boost

10

50 100 200 400 1000

# of labeled documents in the training set

Figure 2: F1 on classes CCAT and ECAT with respect to the number of labeled documents in the initial labeled training set Z .

labeled data. When there are sufficient labeled examples, all algorithms actually converge to the same F1 value, suggesting that the labeled data carries sufficient information and that no additional information could be extracted from unlabeled examples. For a low number of labeled training data, the contribution of each of the algorithms that use unlabeled data is clearly shown. Note that these curves are obtained using five languages, such that the highest performance is achieved by Boost-cst, which is consistent with the findings of the previous section. When fewer views are available, the relative positions of the top algorithms are different, but the effect is similar in that the gains are more important when fewer initial labeled documents are available.
5.3.3 The effect of the number of languages
In our experiments, the unlabeled training set was split in two parts, one for coregularization and one for self-training. Our motivation was to examine the effect of each of the techniques individually without introducing any bias by per-

E21 0.46

0.44

0.42

F1

0.4

0.38

0.36

|||ZZZlll|||===51305000

Maxima

0

0.2

0.4

0.6

0.8

1



Figure 1: F1 with respect to the coregularization factor  for different labeled training sizes on class E21.

forming coregularization and self-training on the same unlabeled data. The previous results suggest that the performance gain is higher when unlabeled examples are iteratively labeled in the self-training framework than when they are used in coregularization to enforce agreement between the language-specific classifiers. The question therefore arises as to what the performance would be if all the unlabeled examples were used in consensus-based self-training rather than being split between coregularization and selftraining? In addition, the consensus is expected to be more reliable when there are many views than when there are few, in which case the language-specific classifiers could agree by chance but erroneously. We therefore investigate the effect of the number of views on the performance of the reg-Boost-cst and Boost-cst algorithms. Figure 3 depicts these results by comparing both algorithms for varying numbers of languages on two classes, E21 and C15. All re-

0.6

0.55

0.5

0.45

0.4

0.35 0.3 2

Boost-cst* reg-Boost-cst
C15 E21

3

4

5

# of languages

Figure 3: F1 with respect to the number of languages used for coregularization and self-training on classes
E21 (solid) and C15 (dash). Comparisons involve
reg-Boost-cst ( ) and the boosting algorithm using the unlabeled examples (XU1  XU2 ) for self-training Boost-cst ( ).

481

sults obtained for less than five languages are averaged over all possible such combinations of languages.
These results show that for five languages, using all the unlabeled data for self-training is slightly more efficient than reserving part of it for coregularization. However, when the number of views is smaller, the combination of both coregularization and consensus-based self-training is more advantageous. Note that this is a common situations, for example when only bilingual documents are available.
This result suggests that in the situation where we have few views, reducing the disagreement between language specific classifiers through coregularization may lead to a more effective use of consensus-based labeling, decreasing the number of noisy examples added to the training set during selftraining. On the other hand, when the number of views is large, the consensus is usually reliable enough without the need for coregularization.
6. CONCLUSION
In this paper we proposed a multiview semi-supervised boosting algorithm for multilingual document classification. We have shown how to embed a disagreement-based coregularization term into a classification objective function using a Bregman distance. This embedding allowed us to adapt an existing boosting algorithm to learn language-specific classifiers while enforcing consistency in prediction across languages. We then proposed a self-training algorithm which assigns class labels to unlabeled data based on the consensus of the classifier predictions across the different views.
Our results show clearly that the consensus based selftraining allows to reach high performance in the situation where few initial labeled training documents are available. We also showed that when there are fewer languages, combining coregularization with the consensus-based self-training approach provides a better leverage of the unlabeled data by improving the quality of the consensus.
Acknowlegdements
This work was supported in part by the IST Program of the European Community, under the PASCAL2 Network of Excellence, IST-2002-506778.
7. REFERENCES
[1] J. J. G. Adeva, R. A. Calvo, and D. L. de Ipin~a. Multilingual Approaches to Text Categorisation. UPGRADE: The European Journal for the Informatics Professional, VI(3):43­51, 2005.
[2] M.-R. Amini and C. Goutte. A Co-classification Approach to Learning from Multilingual Corpora. Machine Learning, 79(1-2):105­121, 2010.
[3] M.-R. Amini, N. Usunier, and C. Goutte. Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization. In Advances in Neural Information Processing Systems 22 (NIPS 2009), pages 28­36, 2009.
[4] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple Kernel Learning, Conic Duality, and the SMO Algorithm. In Proc. 21st International Conference on Machine Learning (ICML 2004), 2004.
[5] N. Bel, C. H. Koster, and M. Villegas. Cross-lingual Text Categorization. In ECDL-2003, pages 126­139, 2003.

[6] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
[7] A. Blum and T. M. Mitchell. Combining Labeled and Unlabeled Data with Co-Training. In Proc. 11th Annual Conference on Learning Theory (COLT 1998), pages 92­100, 1998.
[8] U. Brefeld, T. G¨artner, T. Scheffer, and S. Wrobel. Efficient Co-regularised Least Squares Regression. In Proc. 23rd International Conference on Machine Learning (ICML 2006), pages 137­144, 2006.
[9] O. Chapelle, B. Sch¨olkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006.
[10] M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, Adaboost and Bregman Distances. Machine Learning, 48(1-3):253­285, 2002.
[11] J. D. Farquhar, D. R. Hardoon, H. Meng, J. Shawe-Taylor, and S. Szedmak. Two View Learning: SVM-2k, Theory and Practice. In Advances in Neural Information Processing 18 (NIPS 2005), pages 355­362, 2005.
[12] T. Joachims. Transductive Inference for Text Classification using Support Vector Machines. In Proc. of the Sixteenth International Conference on Machine Learning (ICML 1999), pages 200­209, 1999.
[13] T. Joachims. Training Linear SVMs in Linear Time. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 217­226, 2006.
[14] J. D. Lafferty, S. D. Pietra, and V. D. Pietra. Statistical Learning Algorithms Based on Bregman Distances. In Canadian Workshop on Information Theory, 1997.
[15] E. Lehmann. Nonparametric Statistical Methods Based on Ranks. McGraw-Hill, New York, 1975.
[16] K. Nigam, A. McCallum, S. Thrun, and T. M. Mitchell. Learning to Classify Text from Labeled and Unlabeled Documents. In Proc. of the 15th National Conference on Artificial intelligence (AAAI/IAAI 1998, pages 792­799, 1998.
[17] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proc. 3rd Text Retrieval Conference (TREC), pages 109­126, 1994.
[18] D. S. Rosenberg and P. L. Bartlett. The Rademacher Complexity of Co-regularized Kernel Classes. In Proc. of the 11th International Conference on Artificial Intelligence and Statistics (AISTATS 2007), pages 396­403, 2007.
[19] V. Sindhwani, P. Niyogi, and M. Belkin. A Co-regularization Approach to Semi-supervised Learning with Multiple Views. In Proceedings of the ICML-05 Workshop on Learning with Multiple Views, pages 74­79, 2005.
[20] N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson. NRC's PORTAGE system for WMT 2007. In ACL-2007 Second Workshop on SMT, 2007.
[21] C. J. Van Rijsbergen. Information Retrieval. Butterworth-Heinemann, Newton, MA, 1979.
[22] X. Zhu. Semi-supervised Learning Literature Survey. Technical report, University of Wisconsin Madison, 2008.

482


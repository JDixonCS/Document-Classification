Acquisition of Instance Attributes via Labeled and Related Instances

Enrique Alfonseca
Google Inc. 110 Brandschenkestrasse
Zurich, Switzerland
ealfonseca@google.com

Marius Pas¸ca
Google Inc. 1600 Amphitheatre Parkway
Mountain View, California
mars@google.com

Enrique Robledo-Arnuncio
Google Inc. 110 Brandschenkestrasse
Zurich, Switzerland
era@google.com

ABSTRACT
This paper presents a method for increasing the quality of automatically extracted instance attributes by exploiting weakly-supervised and unsupervised instance relatedness data. This data consist of (a) class labels for instances and (b) distributional similarity scores. The method organizes the text-derived data into a graph, and automatically propagates attributes among related instances, through random walks over the graph. Experiments on various graph topologies illustrate the advantage of the method over both the original attribute lists and a per-class attribute extractor, both in terms of the number of attributes extracted per instance and the accuracy of top ranked attributes.
Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Information extraction, instance attributes, unstructured text, distributional similarities, labeled instances
1. INTRODUCTION
Motivation: Due in part to the quantitative limitations of the then-available textual data sources, early work on information extraction focuses on training supervised systems on small to medium-sized document collections, requiring relatively expensive manual annotations of data [5]. Some authors investigate the possibility of obtaining manual annotations more easily, through the creation of manual or semi-automatic annotations [6]. But as larger amounts of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

textual data sources have become available at lower computational costs, either directly as document collections or indirectly through the search interfaces of the larger Web search engines, information extraction has seen a shift towards large-scale acquisition of open-domain information [8]. In this framework, information at mainly three levels of granularity is extracted from text, with weak or no supervision: classes (having a label, e.g., painkillers), class elements or instances (e.g., vicodin, oxycontin); and relations among instances (e.g., france-capital-paris) or classes (e.g., countries-capital-cities).
Among other types of relations targeted by various extraction methods, attributes (e.g., side effects and maximum dose) have emerged as one of the more popular types, as they capture properties of their respective classes (e.g., painkillers), and thus serve as building blocks in many knowledge bases. Consequently, a variety of attribute extraction methods mine textual data sources ranging from unstructured [23] or structured [26, 4] text within Web documents, to human-compiled encyclopedia [25, 7] and Web search query logs [17, 16], attempting to extract, for a given class, a ranked list of attributes that is as comprehensive and accurate as possible. Contributions: This paper introduces a method that acquires instance relatedness information, and applies it for attribute extraction, in order to produce ranked lists of attributes of higher coverage and accuracy than current state of the art. The algorithm proposed is general enough that can be plugged to any generic attribute acquisition algorithm to increase precision and coverage. We explore this method, based on the identification of attributes of individual instances (vicodin), in contrast to most previous work on attribute extraction [23, 16], why acquire attributes of classes (e.g., painkillers). Thus, since a large majority of popular search queries are precisely instances of various kinds, the method operates over an input vocabulary (i.e., instances rather than classes) that better approximates the overall composition of the sets of millions of search queries submitted daily to Web search engines.
Instance relatedness is approximated through two types of data: pairwise distributional similarities [13], quantifying the extent to which any two instances occur in similar contexts in text; and labeled classes of instances, capturing the class labels (e.g., painkillers, prescription drugs, medications and substances) applicable to various instances (vicodin). It produces a significant improvement over either learning the attributes for class labels, and over learning the attributes for instances without propagation.

58

Applications: The special role played by attributes, among other types of relations, is documented in earlier work on language and knowledge representation [20, 9]. It inspired the subsequent development of text mining methods [14] aiming at constructing knowledge bases automatically. In Web search, the availability of instance attributes is useful for applications such as search result ranking and suggestion of related queries [2], and has also been identified to be a useful resource in generating product recommendations [19].
2. PREVIOUS WORK
Previous work on attribute extraction uses a variety of textual data sources for mining attributes. Taking advantage of structured and semi-structured text, the method presented in [26] submits list-seeking queries to general-purpose Web search engines, and analyzes retrieved documents to identify common structural (HTML) patterns around class labels given as input, and potential attributes. Similarly, layout and other HTML tags serve as clues to acquire attributes from either domain-specific documents such as those from product and auction Web sites [24] or from arbitrary documents, optionally relying on the presence of explicit itemized lists or tables [4]. As an alternative to Web documents, articles within online encyclopedia can also be exploited as sources of structured text for attribute extraction, as illustrated by previous work using infoboxes and category labels [22, 15, 25, 7] associated with Wikipedia articles.
Working with unstructured text within Web documents, the method described in [23] applies manually-created lexicosyntactic patterns to document sentences in order to extract candidate attributes, given various class labels as input. The candidate attributes are ranked using several frequency statistics. If the documents are domain-specific, such as documents containing product reviews, additional heuristicallymotivated filters and scoring metrics can be used to extract and rank the attributes [21]. In [2], the extraction is guided by a small set of seed instances and attributes rather than manually-created patterns, with the purpose of generating training data and extract new pairs of instances and attributes from text. The set of seeds is acquired automatically in [19], thus further reducing the overhead associated to the preparation of the input data, while exploiting words and part-of-speech labels as features during extraction. Web search queries have also been considered as a textual data source for attribute extraction, using lexico-syntactic patterns [17] or seed attributes [16] to guide the extraction.
Virtually all existing methods for attribute extraction produce ranked list of attributes for each input item (i.e., class label or, more rarely, class instance). Therefore, the method presented in this paper is generally applicable. It acts as a wrapper that takes as input the output from an existing method, propagating attributes across related instances to improve recall, while re-ranking top attributes to improve precision.
3. RELATED INSTANCES IN ATTRIBUTE EXTRACTION
3.1 Instance Attribute Extraction
Given an instance (e.g., cloxacillin as element of the set antibiotics), the goal of instance attribute extraction is the automatic acquisition of the set of relevant attributes (e.g.,

side effects, mechanism of action, cost, therapeutic range) capturing the most prominent properties of the instance. Ideally, the resulting set would be complete (perfect coverage), and contain only relevant attributes (perfect precision). In practice, the task of attribute extraction can be approximated by the acquisition of a ranked list of attributes [a1, a2, a3, ..., aN ], such that as many relevant attributes as possible are among the top items in the ranked list.
The task of instance attribute extraction is related to, and more difficult than, the task of class attribute extraction. Indeed, although both tasks generate ranked lists of attributes, the former does so for an individual instance (e.g., cloxacillin), whereas the latter works for a class of instances (e.g., antibiotics or penicillins). Since a class of instances is often available as input in the form of a set of instances (e.g., {ampicillin, oxacillin, cloxacillin, benzylpenicillin}) associated with a corresponding class label (e.g., penicillins), class attribute extraction methods [17] enjoy access to significantly more input data (i.e., a set of instances instead of a single instance), which allows them to mitigate the negative impact of data sparseness on the extraction outcome, by acquiring attributes of a class (e.g., car manufacturers), based on associations with some of the more popular instances of the class (e.g., jaguar, audi, toyota, chevrolet), even if a few of the input instances (e.g., tesla}) of the class may be absent from the underlying source of textual data from which attributes must be extracted;
Since instance attribute extraction is equivalent to class attribute extraction where the class contains only one element (the instance itself), none of the above positive properties hold in the case of instance attribute extraction. In particular, data sparseness may strongly affect the quality and coverage of the ranked lists of attributes extracted for long-tail instances, that is, instances that occur relatively infrequently in the underlying textual data source. Concretely, a previous method operating over query logs [17] extracts the following ranked lists as attributes for the respective instances: [solubility, analytical methods, inventor, dossier development, last patent, adverse effects] for bicalutamide; [map, opening] for agua caliente casino; and [] (i.e., the empty list) for ct scan. Clearly, the extracted lists suffer from occasional less-than-optimal extractions (e.g., dossier development for bicalutamide), but more importantly from occasionally too few or no elements in the lists, which suggests a low coverage.
3.2 Role of Related Instances
The operational advantage of class attribute extraction over instance attribute extraction stems from its ability to aggregate attributes of the class, from the attributes extracted for individual instances of the class. In order to emulate a similar behavior in instance attribute extraction, one would need a set of instances as input, which are simply not available. However, given the input instance, a rough approximation of a set of instances could be generated dynamically, based on access to a large resource of instance relatedness data. In that case, instance attribute extraction becomes equivalent to aggregating attributes of the instance, from the attributes extracted for other instances that are strongly related to it, based on the following observation:
Hypothesis 1: Let i1 and i2 be two instances. The more strongly related i1 and i2 are semantically, the more likely it is for them to share common attributes.

59

3.3 Sources of Instance Relatedness Data
Two types of relatedness data are explored in this paper for the purpose of attribute extraction: IsA pairs and distributional similarities.
Distributional similarities are a technique for calculating similarities among words or phrases [12]. They capture the extent to which the textual contexts in which phrases occur are similar, with the intuition that phrases that occur in similar contexts tend to have similar meanings. Distributional similarities scale well to large text collections, since their acquisition can be done through parallel implementations, yet they perform well against more expensive, knowledgebased similarity metrics [1]. Distributional similarities are assumed to be available, as pairs of similar phrases with an associated similarity score (e.g., cloxacillin and erythromycin with a score of 0.36; or cloxacillin and alcohol with a score of 0.05).
In addition to distributional similarities, IsA pairs are assumed to be available as weighted pairs of a class instance and an associated class label (e.g., audi and car manufacturers; or cloxacillin and antibiotics). IsA pairs indirectly capture instance relatedness, since any two instances involved in IsA pairs with the same class (e.g., audi and nissan, as distinct instances of the class car manufacturers) are related to each other.
4. ATTRIBUTE PROPAGATION METHOD
Graph Representation: Given an instance ij and some extraction method, let
Aj = [(a0, wj0), (a1, wj1), (a2, wj2), ..., (a|A|, wj|A|)]
be the weighted list of attributes extracted by the method for the instance ij , where A is the set of all attributes of all instances. One can define a probability of transitioning from instance ij to attribute ak by normalizing the weights to sum up to 1:
pjk = P|lwA=j0| kwjl
and represent the instances and attributes as a bipartite graph, with weighted edges from instance nodes to attribute nodes. Injecting Class Labels: The availability of IsA pairs allows for the extension of the instance-attribute graph, by adding a layer for the class labels. Given an instance ij , let
Cj = [(c0, wj0), (c1, wj1), (c2, wj2), ..., (c|C|, wj|C|)]
be the weighted list of the class labels of the instance, where C is the set of all class labels. The normalization of the weights produces a class-label probability distribution pcjm, where cm is a class label of the instance ij . In the scenario of a random walk across the graph representation, this probability distribution would provide, for each instance ij , the probability of transitioning to a class label cm. Conversely, the aggregation and normalization of the weights by class label rather than instance produces a probability distribution pimj , indicating the probability of transitioning from a class label cm to an instance ij within the graph, as illustrated in Figure 1. The resulting graph can be used to propagate attributes across instances of the same class, using a three-step random walk process:
1. From an instance (e.g., i0), execute a random step to one of its class labels (e.g., c0). The probability of each step

G@GG@@G@FAFFAAFAaaaaEBEEBBEB0123 PÕ Ò`ÑDCDDCCDCPÓqwfxÒ`Po PqwÒ`PÒ`qwPÒ`PqwPÒ`PqwppppÒ`PÒ`pp1010qwPÒ`P122301qwpPÒ`13Pqw0ppÒ`P0Ò`01qwPÒ`P40qwPÒ`PqwÒ`PÒ`qwPPÒ`qwPÒ`PqwÒ`PÒ`qwPPÒ`?8P>9i0=:9 @U<;B ~

G@FAaEB4 DCo

p14

?8>9i1=:<; p

b C E
F
~ 

p

pc00
wpi00



H P Rpi1p0c01

T

V

Y

 a pc1p0i01



Ø

Ö

Ô Ñ f

pc11
kpi11q

x


  d
H Ð

& 8?a 9>c# 0:=;< "
f 8?9>c1:=;< Ø

Figure 1: Graph for the transitions from instances to instances via class labels (i, a and c are instance, attribute, and class label nodes, respectively)

is governed by the distribution pc(·), following the dotted edges in Figure 1.
2. From a class label (e.g., c0), execute a random step to move to an instance of that class (e.g., i0). The probability is given by pi(·), following the dashed edges.
3. From an instance (e.g., i0), execute a random step to an attribute (e.g., a0), using the probability distribution p(·), following the solid edges.
This random walk process defines, for each original instance ij , a set of reachable attributes, and the probability of reaching each of them from ij . These attributes can be ranked by this probability.
Note that, if the input data does not contain any class label at all for any instance, no ranked list of attributes can be returned for that instance. This can be avoided by adding, for each instance i, a pseudo-class label ci representing a class that only contains that instance. In this way, for instances without any class label at all no propagation will be done, but they will at least retain their original ranked list of attributes. Injecting Distributional Similarities: The second type of information used to propagate attributes across instances are distributional similarities, used to identify related instances. Given an instance ij and its list of similar instances:
Ij = [(i0, wj0), (i1, wj1), (i2, wj2), ..., (i|I|, wj|I|)]
where I is the set of all instances, the graph can be extended by again normalizing the weights, and adding edges to the graph corresponding to transitions from each instance ij to similar instances. The resulting graph is depicted in Figure 2. In this case, the propagation would be defined by a two-step random walk, first transitioning from an instance to similar instances and then transitioning to attributes. Using distributional similarities the similarity of any instance with itself will always be 1.0, value which will be normalized together with other the similary scores. Injecting Class Labels and Distributional Similarities: Figure 3 shows an example of a graph topology that

60

ps00



G@GG@@G@FAFFAAFAaaaaEBEEBBEB0123 PÕ Ò`ÑDCDDCCDCPÓqwfx Ò`Po PqwÒ`PÒ`qwPÒ`PqwPÒ`PqwppppÒ`PÒ`pp1010qwPÒ`P122301qwpPÒ`13Pqw0ppÒ`P0Ò`qw01PÒ`P40qwPÒ`PqwÒ`PÒ`qwPPÒ`qwPÒ`PqwÒ`PÒ`qwPPÒ`8?P9>i0p:=s0 ;<1

ps10

G@FAaEB4 DCo

p14

8?t 9>i1:=;<

ps11

Figure 2: Graph for the transitions from instances to distributionally similar instances (i and a are instance and attribute nodes respectively)

includes both class labels and distributionally similar instances. The outgoing probabilities for every node are normalized so their sum is 1.
Using this graph topology, the propagation is a two-step algorithm. In the first step, the algorithm calculates, for each instance, the probability of transitioning to any instance in the dataset either by following the self-loop, by randomly walking in two steps through the class labels, or by stepping randomly to a distributionally similar instance. Next, the algorithm calculates the probability of transitioning to any of the attributes. These probabilities are used to rank the attributes, and thus generate a ranked list of attributes as output, for each instance.
5. EXPERIMENTAL SETTING
Textual Data Sources: The acquisition of instance attributes relies on unstructured text available within Web documents and search queries. The collection of queries is a random sample of fully-anonymized queries in English submitted by Web users in 2006. The sample contains about 50 million unique queries. Each query is accompanied by its frequency of occurrence in the logs. The document collection consists of around 200 million documents in English, as available on the Web in 2009. The textual portion of the documents is cleaned of html, tokenized, split into sentences and part-of-speech tagged using the TnT tagger [3]. Parameters for IsA Pairs: Pairs of a class label (e.g., antibiotics) and a class instance (e.g., cloxacillin), along with associated frequency-based scores, are extracted from the collection of Web documents by applying a few IsA extraction patterns selected from [11], as described in [18]. The pairs are organized as ranked lists of class labels per class instance, e.g., [antibiotics, resistant penicillins, lactams, peni-

ps00
 G@GGG@@@G@FAFFFAAAFAaaaaaEBEEEBBBEB01234 ÖIÓYÑDCDDDCCCDCIÓrvfx ÓYIoo IÓYrvIÓYrvIÓYIrvIÓYppppIÓYrvIpp1010ÓYIrv122301ÓYIpprvI13ÓY01ppIÓYrvI0401ÓYIrv40IÓYrvIÓYIÓYrvIÓYIrvIÓYIrvÓYIÓYrvIIÓY88??It 99>>pii01::==s0 1 Q;;<< qdxo StTspvs1V0xtXv`exbfdzhppppppppi0c1c1i1f|c0i0i1c00010i0111h~kpÐmÒrnÔtpÖruØtÙwG V8?8?69>9>% cc01:=:=;<;<
ps11
Figure 3: Graph including all possible transitions (i, a and c are instance, attribute, and class label nodes, respectively)
cillins, penicillinase-resistant penicillins, spectrum antibiotics, semisynthetic penicillins,..] for cloxacillin. Parameters for Distributional Similarities: When applied to the collection of Web documents, the pipeline for extracting distributional similarities described in [1], which scales to millions of instances and billions of Web documents, and identifies pairwise similarity scores among all instances involved in any IsA pairs. Following standard settings [12], the similarity score between two instances is the cosine between their vectors of context windows, where a window consists of three words to the left and three words to the right of each occurrence, within Web documents, of the two instances respectively. Parameters for Initial Instance Attributes: The extraction method introduced in [17] applies a few patterns (e.g., the A of I, or I's A, or A of I) to queries within query logs, where I is an instance from the IsA pairs, and A is a candidate attribute. For each instance, the method extracts ranked lists containing zero, one or more attributes, along with frequency-based scores. Parameters for Graph Representation: As described in the previous section, the complete version of the graph representation of the instances, attributes and class labels contains three types of nodes: instance, attribute and class label nodes. An instance node is created for each instance from the IsA pairs. For efficiency, at most 250 of the attributes extracted initially for each instance are used to populate attribute nodes and the associated instance-attribute edges in the graph; and distributional similarities below 0.001 are discarded. Even if they were kept, due to the low weights associated to the attributes and the similar queries below those thresholds, it would not have a notable impact on the relative ranking of attributes.

61

aaa, ac compressors, acheron, acrocyanosis, adelaide cbd, african population, agua caliente casino, al hirschfeld, alessandro nesta, american fascism, american society for horticultural science, ancient babylonia, angioplasty, annapolis harbor, antarctic region, arlene martel, arrabiata sauce, artificial intelligence, bangla music, baquba, bb gun, berkshire hathaway, bicalutamide, blue jay, boulder colorado, brittle star, capsicum, carbonate, carotid arteries, chester arthur, christian songs, cloxacillin, cobol, communicable diseases, contemporary art, cortex, ct scan, digital fortress, eartha kitt, eating disorders, file sharing, final fantasy vii, forensics, habbo hotel, halogens, halophytes, ho chi minh trail, icici prudential, jane fonda, juan carlos, karlsruhe, kidney stones, lipoma, loss of appetite, lucky ali, majorca, martin frobisher, mexico city, pancho villa, phosphorus, playing cards, prednisone, right to vote, robotics, rouen, scientific revolution, self-esteem, spandex, strattera, u.s., vida guerra, visual basic, web hosting, windsurfing, wlan
Table 1: Set of 75 target instances, used in the evaluation of instance attribute extraction
Target Instances: In order to assemble a set of instances whose attributes are evaluated for accuracy and relative coverage, the set of all instances from the graph is partitioned according to the number of attributes extracted initially for each instance. The partitions contain all instances with: 0 attributes; 1 to 5 attributes; 6 to 20 attributes; 21 to 50 attributes; and more than 51 attributes, respectively. From each partition, a random sample of 60 instances is automatically selected. The sample is further inspected manually, in order to eliminate instances for which human annotators would likely need a long time to become familiar with the instance and its meanings, before they can assign correctness labels to the attributes extracted for the instance. The only purpose of the manual selection step is to keep the costs associated with the subsequent, manual evaluation of attributes within reasonable limits. To remove any possible bias towards instances with more or better attributes, the extracted attributes, if any, remain invisible during the selection of instances. For example, the instance it (which, among other meanings, is as an acronym for information technology) is discarded due to extreme ambiguity. Conversely, agua caliente casino is retained, since it is relatively less difficult to notice that it refers to a particular casino. The manual selection of 15 instances, from the random sample of each of the 5 partitions, results in an evaluation set containing 75 target instances, as shown in Table 1.
While the comprehensiveness of any attribute extraction experiments increases with the the number of target instances used for evaluation, the time intensive nature of manual accuracy judgments often required in the evaluation of information extraction systems [8] sets a practical limit to the size of the test set. With this in mind, we choose what we feel to be a large enough size for our test set (75 instances with 4,833 attributes in total) to ensure varied experimentation on several dimensions. Experimental Runs: The experiments consist of four different runs: SnCn, SyCn, SnCy and SyCy, where S stands for transitions from instances to distributionally similar instances, C stands for transitions from instances to class labels, and y/n indicate whether the respective transitions are included in the graph topology (y) or not (n). The first run, SnCn, corresponds to using the ranked lists of attributes initially extracted from text, without any propagation.

Run
Sn Cy Sy Cn Sy Cy

Node Type

A

IC

0.49 2.12 3.50

0.49 2.12

0

0.49 2.12 3.50

Edge Type

I-C

I-I I-A

19

03

0 16.50 3

19 16.50 3

Table 2: Number of graph nodes and number of edges of various types, in millions (A=attribute; I=instance; C=class label)

Label vital
okay
wrong

Value 1.0
0.5
0.0

Examples of Attributes
capsicum: calorie count cloxacillin: side effects lucky ali: album songs jane fonda: musical theatre contributions mexico city: cathedral robotics: three laws acheron: kingdom berkshire hathaway: tax exclusion contemporary art: urban institute

Table 3: Correctness labels for the manual assessment of attributes

6. EVALUATION RESULTS
6.1 Quantitative Results
IsA Pairs: The IsA pairs, extracted according to the patterns from [18] from the document collection, cover a total of 2.12 million of the instances that each have two or more class labels, with an average of 19.72 class labels per instance. This set of 2.12 million instances was used for propagation via class labels. Distributional Similarities: The distributional similarities, for the set of 2.12 million instances that have two more class labels, capture one or more similar instances for 13% of them. For the ones that have at least one similar instance, an average of 9 similar instances are returned. Initial Instance Attributes: The initial set of instance attributes, extracted according to [17] from the collection of search queries, is available in the form of ranked lists containing zero, one or more attributes. There is one (possibly empty) ranked list for each of the 2.12 million instances. The extracted ranked lists contain 0 attributes for 1.63 million instances; 1 to 5 attributes for 315,052 instances; 6 to 20 attributes for 56,721 instances; 21 to 50 attributes for 13,902 instances; and more than 50 attributes for 9,249 instances. Graph Representation: Table 2 shows the number of nodes and edges of various types, for the three runs that apply propagation for extracting attributes.
6.2 Qualitative Results
Evaluation Procedure: The measurement of recall requires knowledge of the complete set of items (in our case, attributes) to be extracted. Unfortunately, this number is often unavailable in information extraction tasks in general [10], and attribute extraction in particular. Indeed, the manual enumeration of all attributes of each target instance, to measure recall, is unfeasible. Therefore, the evaluation focuses on the assessment of attribute accuracy.
To remove any bias towards higher-ranked attributes during the assessment of instance attributes, the top 50 attributes within the ranked lists of attributes produced by each run to be evaluated are sorted alphabetically into a

62

Instance: acheron
0.8 SnCn SyCn SnCy SyCy
0.6

Instance: habbo hotel
0.8 SnCn SyCn SnCy SyCy
0.6

Instance: spandex
0.8
0.6

Precision Precision Precision

0.4

0.4

0.4

0.2
0 0

0.2

0

10

20

30

40

50

0

Rank

0.2

0

10

20

30

40

50

0

Rank

SnCn SyCn SnCy SyCy

10

20

30

40

50

Rank

Instance: Average-Instance
0.8 SnCn SyCn SnCy SyCy
0.6

Instance: Average-Instance
0.8 SnCn SyCn SnCy SyCy
0.6

Precision Precision

0.4

0.4

0.2

0.2

0

0

0

10

20

30

40

50

0

10

20

30

40

50

Rank

Rank

Figure 4: Accuracy of the ranked lists of attributes extracted by various runs for a few target instances (top graphs); as an average over all instances (bottom left graph); and as average over every instance that has at least one distributionally similar instance (bottom right graph)

merged list. Each attribute of the merged list is manually assigned a correctness label within its respective instance. In accordance with previously introduced methodology, an attribute is vital if it must be present in an ideal list of attributes of the instance (e.g., side effects for cloxacillin); okay if it provides useful but non-essential information; and wrong if it is incorrect [16]. Thus, a correctness label is manually assigned to a total of 4,833 attributes extracted for the 75 target instances, in a process that confirms that evaluation of information extraction methods can be quite time consuming. Two computational linguists performed the evaluation, with each of the attributes rated by the two of them. The inter-annotator agreement of 88.79%, resulting in a Kappa score of 0.85, indicating substantial agreement.
To compute the precision score over a ranked list of attributes, the correctness labels are converted to numeric values (vital to 1, okay to 0.5 and wrong to 0), as shown in Table 3. Precision at some rank N in the list is thus measured as the sum of the assigned values of the first N attributes, divided by N . Accuracy of Instance Attributes: Figure 4 plots precision values for ranks 1 through 50, for each of the four experimental runs. The first three graphs in the figure show the precision over three individual target instances. Several conclusions can be drawn from these. First, the quality of the attributes extracted by a given run varies among instances. For instance, the attributes extracted for the instance spandex are better than for habbo hotel. Second, the experimental runs have variable levels of accuracy. The bottom left graph in Figure 4 shows the average precision over all target instances. Although none of the runs outperforms the others on each and every target instance, on average, SyCy performs the best and SnCn (i.e., the baseline) the worst, with SyCn placed in-between and SnCy almost as

accurate as SyCy. In other words, propagation based on distributionally similar instances (SyCn) gives a significant improvement over the baseline SnCn, and propagation based on class labels rather than distributionally similar instances (SnCy) gives an even larger improvement. In order to discover whether these results are due to the fact that class labels are more reliable or whether it is because class labels are available for more instances, The bottom right graph in Figure 4 shows the precision of the experimental runs over the subset of instances that have at least one distributionally similar instance. This subset contains exactly 50 of the 75 target instances. The results indicate that class labels and distributional similarities (as well as their combination), when available, lead to similar improvements when both types of relatedness data are available.
Table 4 provides a more detailed view on the accuracy of the extracted attributes, for various subsets of target instances obtained according to the number of initial attributes (i.e., extracted in run SnCn) for the respective instances. For example, rows with an attribute count "[1,5]" refer to instances that have one through five attributes in run SnCn. For this range, the baseline SnCn is outperformed by the three propagation-based runs at all ranks, with precision scores of 0.22 at rank 5 and 0.02 at rank 50 for SnCn, vs. 0.60 at rank 5 and 0.38 at rank 50 for SyCy, (fifth and eigth rows of the table). The table indicates that: 1) with few exceptions, SyCn, SnCy and SyCy outperform SnCn for all ranges, with SnCy and SyCy giving the best results; 2) SyCn, SnCy and SyCy produce more accurate attributes than SnCn even for the range [51,), which indicates that the attribute reranking caused by the propagation is better than the initial attribute ranking given by SnCn; and 3) attribute propagation is quite resistant to sparseness of initial attributes, as illustrated by precision scores of, e.g., SyCy that are com-

63

Instance angioplasty
bicalutamide

Run
Sn Cn Sy Cn Sn Cy
Sy Cy Per-class
Sn Cn Sy Cn Sn Cy
Sy Cy
Per-class

Precision @5 @10 0.00 0.00 1.00 0.75 1.00 0.95
1.00 0.75 1.00 0.90
0.60 0.40 0.60 0.40 0.60 0.80
0.60 0.80
0.40 0.40

Top Ten Attributes

Attributes

none extracted [history, types, complications, risks, definition, cost, pictures, symptoms, center, side effects] [complications, cost, history, types, risks, purpose, definition, pictures, side effects, advantages] [history, complications, types, cost, risks, definition, pictures, side effects, symptoms, center] [types, history, pictures, cost, principles, pros and cons, map, definition, different types, methods]
[solubility, analytical methods, inventor, dossier development, last patent, adverse effects] [solubility, analytical methods, inventor, dossier development, last patent, adverse effects] [solubility, analytical methods, inventor, last patent, dossier development, adverse effects, side effects, pharmacology, effects, pharmacokinetics] [solubility, analytical methods, inventor, last patent, dossier development, adverse effects, side effects, pharmacology, effects, pharmacokinetics] [symptoms, causes, side effects, types, effects, pathophysiology, treatment, american journal, definition, signs and symptoms]

Table 5: Ranked lists of attributes extracted by various runs for a sample of target instances

Att. Count
[0,0]
[1,5]
[6,20]
[21,50]
[51,)
[0,) (entire
set)

Run
SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy
SnCn (Rel) (Err) Sy Cn (Rel) (Err) SnCy (Rel) (Err) Sy Cy (Rel) (Err) Perclass

@5
0.00 0.61 0.80 0.77 0.22 0.24 0.60 0.60
0.61 0.63 0.64 0.64
0.67 0.71 0.69 0.71
0.68 0.72 0.75 0.77
0.44 0% -0%
0.58 31% -25% 0.69 57% -45% 0.69 57% -45% 0.52

@10
0.00 0.57 0.76 0.74 0.11 0.13 0.54 0.53
0.54 0.60 0.66 0.63
0.71 0.73 0.71 0.72
0.66 0.71 0.70 0.71
0.41 0% -0%
0.54 32% -22% 0.67 63% -44% 0.66 61% -42% 0.50

Precision @20 @30

0.00 0.52 0.65 0.66 0.05 0.09 0.43 0.43
0.33 0.49 0.55 0.58
0.68 0.70 0.68 0.68
0.65 0.66 0.68 0.67

0.00 0.50 0.64 0.65 0.04 0.08 0.40 0.40
0.22 0.44 0.53 0.54
0.60 0.66 0.64 0.64
0.64 0.63 0.65 0.66

0.34 0% -0%
0.49 44% -23% 0.59 74% -38% 0.60 76% -39% 0.47

0.30 0% -0%
0.46 53% -23% 0.57 90% -39% 0.58 93% -40% 0.44

@40
0.00 0.46 0.62 0.61 0.03 0.07 0.38 0.39
0.16 0.40 0.51 0.52
0.51 0.60 0.61 0.62
0.63 0.62 0.62 0.63
0.27 0% -0%
0.43 59% -22% 0.55 104% -38% 0.55 104% -38% 0.43

@50
0.00 0.46 0.60 0.59 0.02 0.06 0.37 0.38
0.13 0.38 0.50 0.50
0.42 0.57 0.60 0.60
0.61 0.61 0.62 0.62
0.24 0% -0%
0.41 71% -22% 0.53 121% -38% 0.54 125% -39% 0.42

Table 4: Precision scores at various ranks, as an average over subsets of the evaluation set of instances, where a subset contains the instances whose number of attributes extracted by the experimental run SnCn falls into a particular count range (Rel=increase relative to SnCn; Err=error reduction relative to SnCn). In each solid-line block in the table, the highest value and those that are indistinguishable at 95% confidence are bolded

petitive for the range [0,0], where sparseness is extreme (no initial attributes) vs. the range [51,), where sparseness over the top 50 attributes is non-existent.
In the lower part of Table 4, for the range [0,), the precision scores are computed over the entire set of target

instances, and therefore they correspond to points on the curves from the last graph of Figure 4. Also shown in the table are the relative increases (Rel) and the reduction in the error rates (Err) at various ranks, for each of SyCn, SnCy or SyCy, on one hand, relative to SnCn, on the other hand. Per-class attributes: For a comparison, we have used the procedure in [17] as another baseline. This is a per-class extraction procedure that works, from the same input data as the per-instance attribute extraction (SnCn) in the following way: first, from the IsA data, each instance is associated with its highest-ranking class label. For every class label, it looks for attributeness-denoting lexicosemantic patterns in the query logs (e.g. X of Y ) involving any element in the class. The extracted attributes are ranked by frequency in the whole class, and assigned, equally, to all the class elements. There are more than one million class labels, to ensure that they are fine-grained enough that only very closely related instances belong to the same class.
The last row in Table 4 shows the results using this method. The scores consistently improve over the per-instance attribute extraction (SnCn). This is mainly due to the sparse data affecting SnCn: the fact that the instances are grouped together and all their attributes are shared allows the perclass extraction to collect at least 50 attributes for 98% of the instances. On the other hand, since all the instances in the same class have the same relative weight, attributes that are vital only to some members of the class may receive a high standing for all of them. Some examples are king for u.s., or senator for countries that do not have a Senate. Using the graph structure this is partially avoided because (a) each instance has the highest distributional similarity with itself, and (b) every instance share all its class labels with itself. Therefore, the per-instance original attributes tend to end up higher in the final lists after the propagation. Examples: Table 5 shows the top items in the ranked lists of attributes extracted for a few of the 75 target instances. For angioplasty and bicalutamide, SnCn extracts 0 and 6 attributes respectively. The other three runs extract more instances via propagation, with the exception of SyCn for the instance bicalutamide. The per-class procedure does well for angioplasty, but bicalutamide shows some drawbacks of this procedure: although its class label is correctly identified in the IsA data (antiandrogens), this class mistakenly contains a couple of sicknesses, which pollute the list of attributes.

64

7. CONCLUSIONS
Data sparseness is a problem affecting the precision and coverage of open-domain information extraction tasks. Instance attribute extraction is not an exception. Data capturing the degree to which various instances may be related to one another is useful in re-ranking and expanding attributes extracted from text with standard techniques. The injection of instance relatedness data into a graph representing the initially extracted instance attributes, allows for relevant attributes to be propagated across related instances. The resulting ranked lists of attributes have higher accuracy levels than previous results. When both class labels and distributionally similar instances are available, the improvements using the two methods are comparable. Current work investigates the utility of distributional similarities among class labels, as signals during propagation.
8. REFERENCES
[1] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pa¸sca, and A. Soroa. A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches. In Proceedings of NAACL-2009, pages 19­27, 2009.
[2] K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liberman, A. McCallum, and M. Dredze. Lightly-Supervised Attribute Extraction. In NIPS Workshop on Machine Learning for Web Search, 2007.
[3] T. Brants. TnT - a statistical part of speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00), pages 224­231, Seattle, Washington, 2000.
[4] M. Cafarella, A. Halevy, D. Wang, and Y. Zhang. Webtables: Exploring the Power of Tables on the Eeb. Proceedings of the VLDB Endowment archive, 1(1):538­549, 2008.
[5] N. Chinchor. Overview of MUC-7/MET-2. In Proceedings of the Seventh Message Understanding Conference (MUC-7), volume 1, 1998.
[6] T. Chklovski and Y. Gil. An Analysis of Knowledge Collected from Volunteer Contributors. In Proceedings of the National Conference on Artificial Intelligence, page 564, 2005.
[7] G. Cui, Q. Lu, W. Li, and Y. Chen. Automatic Acquisition of Attributes for Ontology Construction. In Proceedings of the 22nd International Conference on Computer Processing of Oriental Languages. Language Technology for the Knowledge-based Economy, pages 248­259, 2009.
[8] O. Etzioni, M. Banko, S. Soderland, and S. Weld. Open Information Extraction from the Web. Communications of the ACM, 51(12), December 2008.
[9] N. Guarino. Concepts, Attributes and Arbitrary Relations. Data and Knowledge Engineering, 8:249­261, 1992.
[10] T. Hasegawa, S. Sekine, and R. Grishman. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 415­422, Barcelona, Spain, 2004.
[11] M. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th

International Conference on Computational Linguistics, pages 539­545, Nantes, France, 1992. [12] L. Lee. Measures of Distributional Similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pages 25­32, 1999. [13] D. Lin and P. Pantel. Concept Discovery from Text. In Proceedings of COLING, volume 2, pages 577­583, 2002.
[14] R. Mooney and R. Bunescu. Mining knowledge from text using information extraction. SIGKDD Explorations, 7(1):3­10, 2005.
[15] V. Nastase and M. Strube. Decoding wikipedia categories for knowledge acquisition. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1219­1224, Chicago, Illinois, 2008.
[16] M. Pa¸sca. Organizing and searching the World Wide Web of facts - step two: Harnessing the wisdom of the crowds. In Proceedings of the 16th World Wide Web Conference (WWW-07), pages 101­110, 2007.
[17] M. Pa¸sca and B. Van Durme. What you seek is what you get: Extraction of class attributes from query logs. In Proceedings of IJCAI-07, pages 2832­2837, 2007.
[18] M. Pa¸sca and B. Van Durme. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 19­27, Columbus, Ohio, 2008.
[19] K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu. Semi-Supervised Learning of Attribute-Value Pairs from Product Descriptions. IJCAI-07, 2007.
[20] J. Pustejovsky. The Generative Lexicon: a Theory of Computational Lexical Semantics, 1991.
[21] S. Raju, P. Pingali, and V. Varma. An Unsupervised Approach to Product Attribute Extraction. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, pages 796­800, 2009.
[22] F. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge unifying WordNet and Wikipedia. In Proceedings of WWW-2007, pages 697­706, 2007.
[23] K. Tokunaga, J. Kazama, and K. Torisawa. Automatic discovery of attribute words from Web documents. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05), pages 106­118, 2005.
[24] T. Wong and W. Lam. An Unsupervised Method for Joint Information Extraction and Feature Mining Across Different Web Sites. Data & Knowledge Engineering, 68(1):107­125, 2009.
[25] F. Wu, R. Hoffmann, and D. Weld. Information extraction from Wikipedia: Moving down the long tail. In Proceedings of the 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-08), pages 731­739, 2008.
[26] N. Yoshinaga and K. Torisawa. Open-Domain Attribute-Value Acquisition from Semi-Structured Texts. In Proceedings of the Workshop on Ontolex, pages 55­66, 2007.

65


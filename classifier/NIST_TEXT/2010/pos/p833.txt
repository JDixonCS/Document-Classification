Probabilistic Latent Maximal Marginal Relevance

Shengbo Guo
ANU & NICTA Canberra, Australia
shengbo.guo@nicta.com.au

Scott Sanner
NICTA & ANU Canberra, Australia
scott.sanner@nicta.com.au

ABSTRACT
Diversity has been heavily motivated in the information retrieval literature as an objective criterion for result sets in search and recommender systems. Perhaps one of the most well-known and most used algorithms for result set diversification is that of Maximal Marginal Relevance (MMR). In this paper, we show that while MMR is somewhat adhoc and motivated from a purely pragmatic perspective, we can derive a more principled variant via probabilistic inference in a latent variable graphical model. This novel derivation presents a formal probabilistic latent view of MMR (PLMMR) that (a) removes the need to manually balance relevance and diversity parameters, (b) shows that specific definitions of relevance and diversity metrics appropriate to MMR emerge naturally, and (c) formally derives variants of latent semantic indexing (LSI) similarity metrics for use in PLMMR. Empirically, PLMMR outperforms MMR with standard term frequency based similarity and diversity metrics since PLMMR maximizes latent diversity in the results.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms
Keywords
diversity, graphical models, maximal marginal relevance
1. INTRODUCTION
Maximal marginal relevance (MMR) [2] is perhaps one of the most popular methods for balancing relevance and diversity in set-based information retrieval and has been cited over 530 times1 since its publication in 1998.
The basic idea of MMR is straightforward: suppose we have a set of items D and we want to recommend a small subset Sk  D (where |Sk| = k and k  |D|) relevant to a given query q. MMR proposes to build Sk in a greedy
1According to Google Scholar.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

manner by selecting sj given Sj-1 = {s1, . . . , sj-1} (where Sj = Sj-1  {sj }) according to the following criteria

sj

= arg max [(Sim1(sj, q)) - (1
sj D\Sj-1

-

) max
si Sj-1

Sim2(sj, si)]

(1)

where Sim1(·, ·) measures the relevance between an item and a query, Sim2(·, ·) measures the similarity between two items, and the manually tuned   [0, 1] trades off relevance and similarity. In the case of s1, the second term disappears.
While MMR is a popular algorithm, it was specified in a

rather ad-hoc manner and good performance typically relies

on careful tuning of the  parameter. Furthermore, MMR is

agnostic to the specific similarity metrics used, which indeed

allows for flexibility, but makes no indication as to the choice

of similarity metrics for Sim1 and Sim2 that are compatible with each other and also appropriate for good performance.

In the next section, we address these concerns by taking a

more principled approach to set-based information retrieval

via maximum a posteriori probabilistic inference in a latent

variable graphical model of marginal relevance (PLMMR).

As an elegant and novel contribution, we note that natural

relevance and diversity metrics emerge from this derivation

(with no analogous manually tuned  parameter) and that

these metrics also formally motivate variants of similarity

metrics used in latent semantic indexing (LSI) [3].

2. PROBABILISTIC LATENT MMR

Figure 1: Graphical model used in PLMMR.
We begin our discussion of PLMMR by introducing a graphical model of (marginal) relevance in Figure 1. Shaded nodes represent observed variables while unshaded nodes are latent; we do not distinguish between variables and their assignments. The observed variables are the vector of query terms q and the selected items s1  D and s2  D. For the latent variables, let T be a discrete topic set; variables t1  T and t2  T respectively represent topics for s1 and

833

s2 and t  T represents a topic for query q. r1  {0, 1} and
r2  {0, 1} are variables that indicate whether the respective
selected items s1 and s2 are relevant (1) or not (0). The conditional probability tables (CPTs) in this discrete
directed graphical model are defined as follows. P (t1|s1) and P (t2|s2) represent topic models of the items and P (t|q) represents a topic model of the query. There are a variety of ways to learn these topic CPTs based on the nature of the items and query; for an item set D consisting of text documents and a query that can be treated as a text document, a natural probabilistic model for P (ti|si) and P (t|q) can be derived from Latent Dirichlet Allocation (LDA) [1]. Finally, the CPTs for relevance ri have a very natural definition:

P (r1|t, t1) =

1 0

if t1 = t if t1 = t

P (r2|t, r1 = 0, t1, t2) =

1 0

if (t2 = t1)  (t2 = t) if (t2 = t1)  (t2 = t)

Simply, s1 is relevant if its topic t1 = t (the query topic).

s2 is relevant with the same condition and the addition that

if s1 was irrelevant (r1 = 0), then topic t2 for s2 should also

not match t1. Following the click-chain model [4], we assume

the user only examines s2 if s1 was irrelevant (r1 = 0).

Let us assume that like MMR we use a greedy item set se-

lection algorithm given S1 = {s1},

and we have already we want to select s2

selected s1 = s1. Now in order to maximize

its marginal relevance w.r.t. q given S1, formally defined as

MR(S1, s2, q) and derived as a query in the graphical model:

s2 = arg max MR(S1, s2, q) = arg max P (r2|s1, s2, q)

s2 D\S1

s2D\{s1 }

= arg max

P (r2|r1 = 0, t1, t2, t)P (t1|s1)

s2 D\{s1 } t1,t2,t

P (r1 = 0|t1, t)P (t2|s2)P (t|q)

= arg max
s2 D\{s1 }

P (t|q)P (t2 = t|s2) -
t
relevance

P (t|q)P (t1 = t|s1)P (t2 = t|s2)

(2)

t

diversity

The basic insight leading to this fascinating result is the
exploitation of the indicator structure of the relevance vari-
ables r1 and r2 to make convenient variable substitutions. We note that in this special case for MR(S1, s2, q), a very
natural mapping to the MMR algorithm in (1) when  = 0.5 has emerged automatically from the derivation that maximized MR. This derivation automatically balances relevance and diversity without an analogous  and it suggests very specific (and different) relevance and diversity metrics, both effectively variants of similarity metrics used in latent semantic indexing (LSI) [3]. To make this clear, we examine
tlqehuteerTryeleqavananndcdeTitm2emebtersic2reSwsipimtehP1cLtviMevcM etRotrogpeivilceemnpebrnoytbsPaTbLiMliitM=y RPve(wctthoe=rrsei|fwqoer) and T2i = P (t2 = i|s2) and using ·, · for the inner product:

SimP1 LMMR(q, s2) = P (t|q)P (t2 = t|s2) = T, T2 .
t

A similar analysis gives diversity metric SimP2 LMMR(s1, s2), yielding a variant LSI similarity metric reweighted by the query topic probability P (t|q). This points out the important correction to MMR that item set diversity should be

Table 1: Weighted subtopic loss (WSL) of three methods using all words and first 10 words. Standard error estimates are shown for PLMMR-LDA.

Method MMR-TF MMR-TFIDF PLMMR-LDA

WSL (first 10 words) 0.555 0.549
0.458 ± 0.0058

WSL (all words) 0.534 0.493
0.468 ± 0.0019

query-relevant! Given these definitions of SimP1 LMMR and SimP2 LMMR, we can now substitute these into the MMR algorithm defined in (1) to arrive at a definition of PLMMR.

3. EXPERIMENTAL COMPARISON
We report experiments on a subset of TREC 6-8 data focusing on diversity. We follow the same experimental setup as [6] who measure the weighted subtopic loss (WSL) of recommended item sets where in brief, WSL gives higher penalty for not covering popular subtopics. We do not compare directly to [6] as their method was supervised while MMR and PLMMR are inherently unsupervised.
Standard query and item similarity metrics used in MMR applied to text data include the cosine of the term frequency (TF) and TF inverse document frequency (TFIDF) vector space models [5]. We denote these variants of MMR as MMR-TF and MMR-TFIDF. PLMMR specifically suggests the use of LSI-based similarity metrics defined in the last section; thus, we use LDA to derive these models, referring to the resulting algorithm as PLMMR-LDA. LDA was trained with  = 2.0,  = 0.5, |T | = 15; we note the results were not highly sensitive to these parameter choices.
Average WSL scores are shown in Table 1 on the 17 queries examined by [6]. We use both full documents and also just the first 10 words of each document. For both MMR algorithms, the best performing  = 0.5 is shown. We note that due to the power of the latent topic model and derived similarity metrics, PLMMR-LDA is able to perform better than MMR with standard TF and TFIDF metrics and without a  parameter to be tuned. In addition, PLMMRLDA works very well with short documents since intrinsic document and query similarities are automatically derived from the latent PLMMR relevance and diversity metrics.

4. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993­1022, 2003.
[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 335­336. 1998.
[3] S. Deerwester, S. T. Dumaisand, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. JASIS, 41:391­407, 1990.
[4] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y.-M. Wang, and C. Faloutsos. Click chain model in web search. In WWW-09, Madrid, Spain, 2009. ACM.
[5] G. Salton and M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983.
[6] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML, 1224­1231, 2008.

834


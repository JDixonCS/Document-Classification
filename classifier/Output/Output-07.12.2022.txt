
795

3. EXPERIMENTAL EVALUATION
We selected the 25 event topics of the latest TREC Novelty collection (2004) consisting of news articles. We annotated the documents associated with those topics using state of the art NLP tools1 in order to extract entities of type person, location, organization, and product based on WSJ annotations. The system detected 7481 entity occurrences in the collection: 26% persons, 10% locations, 57% organizations, and 7% products. Human judges assessed the relevance of the entities in each document with respect to the topic grading each entity on the 3-points scale: Relevant, Related, Not Relevant. An additional category was used, i.e., 'Not an entity', to mark entities which had been wrongly annotated by the NLP tool. A total of 21213 entitydocument-topic judgements were obtained in the collection2.
We compare the effectiveness of different features and some feature combinations using several performance metrics. We report values for Precision@3 (P@3), Precision@5 (P@5), and Mean Average Precision (MAP) considering Related entities as non-relevant and using tie-aware metrics [2].

Feature

P@3 P@5 MAP

All Ties

.34

.34

.42

Individual Features (Local and History)

F(e,d)

.65

.56

.60

FirstSenLen .37

.36

.45

FirstSenPos .31

.31

.43

Fsubj

.49

.44

.50

F (e, de,1)

.58

F (e, de,-1) .64

DF (e, H) .63

F (e, H)

.66

CoOcc(e, H) .62

.53
.56 .57 .59
.57

.56 .62 .65 .66 .65

Features combined with F(e,d)

F irstSenLen .65 F irstSenP os .67

.57 .58

.62 .62

Fsubj F (e, de,1) F (e, de,-1) F (e, H)

.65
.65 .68 .70

.56 .57 .60 .62

.61 .61 .65 .68

CoOcc(e, H) .68 .61 .67

DF (e, H)

.69 .61 .68

Table 1: Effectiveness of individual features and of features when combined with F (e, d). Bold values indicate the best performing runs. * (**) indicates statistical significance w.r.t. F(e,d) and () w.r.t. F(e,H) with paired t-test p<0.05(0.01).

Individual Features. The upper part of Table 1 shows effectiveness values obtained when ranking entities in a document according to individual features. For comparison, a feature that assigns the same value to each entity would obtain a MAP value of 0.42. The feature F (e, d) obtains the best MAP value (0.60) among features from the local article. In general, history features perform better than local features and the highest performance is obtained by ranking entities according to their frequency in the past documents. Interestingly, when identifying relevant entities for a docu-
1http://sourceforge.net/projects/supersensetag/ 2The evaluation collection we have created is available for download at: http://www.l3s.de/~demartini/deert/

ment, the frequency of the entity in the previous document in the story F (e, de,-1) is a better evidence than the frequency in the current document. This may be an indication of how people read news: some entities become relevant to readers after repeated occurrences. If an entity appears also in the previous documents it is more likely to be relevant.
Given these results we conclude that the evidence from the past is very important for ranking entities appearing in a document. We expect effectiveness of methods that exploit the past to improve as the size of H grows. That is, the more history is available the better we can rank entities for the current news. For |H|  20 the average effectiveness of F (e, H) grows together with |H| up to values of 0.7 MAP.
Combined Features. So far we have presented different features for ranking entities that appear in a document. Combining them in an appropriate manner yields a better ranking of entities; however, because the probability distribution of relevance given a feature is different among features we need a way for combining them. The following experiments rank entities in a document according to a score obtained after combining several features together. We consider linear combination of features (transformed with a function as explained in [1]).

Let the score for an entity e and a vector f of n features

be score(e, f ) =

n i=1

wig(fi,

i)

,

where

wi

is

the

weight

of each feature and g is a transformation function for the

feature fi using a given parameter i. In this paper we

employ a transformation function of the form: g(x, ) =

x x+

as suggested in [1], where x is the feature to transform

and  is a parameter. We also tried a linear transformation

but it did not perform as well (more complex non-linear

transformations could also be explored). In order to combine

features we then need to find a parameter i for the function

g and a weight wi for each feature fi. We tested two and

three features combinations, where the variables i, and the

combination weights wi have been tuned with 2-fold cross

validation of 25 topics training to optimize MAP. In order to

find the best values we used a optimization algorithm that

performs a greedy search over the parameter space [3].

Combining F (e, d) with another feature is able to outper-

form the baseline for some range of the weight w that can be

learned on a training set. The best effectiveness is obtained

when combining F (e, d) and F (e, H) obtaining an improve-

ment of 13% in terms of average precision. Other features,

when combined with the baseline, also obtain high improve-

ments performing as good as the combination with F (e, H)

(CoOcc(e, H) having 12% and DF (e, H) having 13% im-

provement in terms of MAP).

As future work, besides testing our features on different

time-aware document collections, we aim at adopting ma-

chine learning techniques to combine the proposed features.

4. REFERENCES
[1] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. Relevance weighting for query independent evidence. In SIGIR '05, USA. ACM.
[2] F. McSherry and M. Najork. Computing information retrieval performance measures efficiently in the presence of tied scores. In ECIR, 2008.
[3] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, (4), 2009.

796

Feature Subset Non-Negative Matrix Factorization and its Applications to Document Understanding

Dingding Wang
School of Computer Science Florida International University
Miami, FL 33199
dwang003@cs.fiu.edu

Chris Ding
CSE Department University of Texas at Arlington
Arlington, TX 76019
chqding@uta.edu

Tao Li
School of Computer Science Florida International University
Miami, FL 33199
taoli@cs.fiu.edu

ABSTRACT
In this paper, we propose feature subset non-negative matrix factorization (NMF), which is an unsupervised approach to simultaneously cluster data points and select important features. We apply our proposed approach to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval. General Terms: Algorithms, Experimentation. Keywords: Feature subset selection, NMF.
1. INTRODUCTION
Keyword (Feature) selection enhances and improves many IR tasks such as document categorization, automatic topic discovery, etc. Many supervised keyword selection techniques have been developed for selecting keywords for classification problems. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together.
The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. This work considers both theoretically and empirically feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering.
The selected keywords are discriminant for different topics in a global perspective, unlike those obtained in co-clustering, which typically associate with one cluster strongly and are absent from other clusters. Also, the selected keywords are not linear combinations of words like those obtained in Latent Semantic Indexing (LSI): our selected words provide clear semantic meanings of the key features while LSI features combine different words together and are not easy to interpret. Experiments on various document understanding applications demonstrate the effectiveness of our proposed approach.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. FEATURE SUBSET NON-NEGATIVE MA-

TRIX FACTORIZATION (FS-NMF)
Let X = {x1, · · · , xn) contains n documents with m keywords (features). In general, NMF factorizes the input nonnegative data matrix X into two nonnegative matrices,

X  F GT ,

where G  Rn+×k is the cluster indicator matrix for clustering columns of X and F = (f1, · · · , fk)  R+m×k contains k cluster centroids. In this paper, we propose a new objective

to simultaneously factorize X and rank the features in X as

follows:



W

min
0,F 0,G0

||X

-

F GT

||2W

,

s.t.

Wj = 1

(1)

j

where W  Rm + ×m which is a diagonal matrix indicating the weights of the rows (keywords or features) in X, and  is a
parameter (set to 0.7 empirically).

2.1 Optimization
We will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence.

2.1.1 Computation of W

Optimizing Eq.(1) with respect to W is equivalent to op-

timizing







J1 = Wibi - ( Wi - 1), bi = (X - F GT )2ij .

i

i

j

Now,

setting

J1 Wi

= bi - Wi-1

= 0,

we

obtain

the

fol-

lowing updating formula



1

Wi

=


i

1

bi-1




1
bi-1

(2)

2.1.2 Computation of G
Optimizing Eq.(1) with respect to G is equivalent to optimizing

J2 = T r(XT W T X - 2GF T W T X + F T W T F GT G).

Setting

J2 G

=

-2XT W F

+ 2GF T W T F

=

0,

we

obtain

the

following updating formula

Gik



Gik

(XT W F )ik (GF T W F )ik

(3)

The correctness and convergence of this updating rule can

be rigorously proved. Details are skipped here.

805

2.1.3 Computation of F
Optimizing Eq.(1) with respect to F is equivalent to optimizing
J3 = T r[W XXT - 2W XGF T + W F GT GF ].

Setting

J3 F

=

-2W XG + W F GT G + (GT GF T W )T

= 0,

we

obtain the following updating formula

Fik



Fik

(W XG)ik (W F GT G)ik

(4)

3. EXPERIMENTS

3.1 Document Clustering
First of all, we examine the clustering performance of FS-NMF using four text datasets as described in Table 1, and compare the results of FS-NMF with seven widely used document clustering methods: (1) K-means; (2) PCA-Km: PCA is firstly applied to reduce the data dimension followed by the K-means clustering; (3) LDA-Km [2]: an adaptive subspace clustering algorithm by integrating linear discriminant analysis (LDA) and K-means; (4)Euclidean coclustering (ECC) [1]; (5) minimum squared residueco-clustering (MSRC) [1]; (6) Non-negative matrix factorization (NMF) [5]; (7) Spectral Clustering with Normalized Cuts (Ncut) [9]. More description of the datasets can be found in [6]. The accuracy evaluation results are presented in Figure 2.

Datasets CSTR Log Reuters
WebACE

# Samples 475 1367 2900 2340

# Dimensions 1000 200 1000 1000

# Class 4 8 10 20

Table 1: Dataset descriptions.

K-means PCA-Km LDA-Km
ECC MSRC NMF Ncut FS-NMF

WebACE
0.4081 0.4432 0.4774 0.4081 0.4432 0.4774 0.4513 0.5577

Log
0.6979 0.6562 0.7198 0.7228 0.5655 0.7608 0.7574 0.7715

Reuters
0.4360 0.3925 0.5142 0.4968 0.4516 0.4047 0.4890 0.4697

CSTR
0.5210 0.5630 0.5630 0.5210 0.5630 0.5630 0.5435 0.6996

Table 2: Clustering accuracy on text datasets.
From the results, we clearly observe that FS-NMF outperforms other document clustering algorithms in most of the cases, and the effectiveness of FS-NMF for document clustering is demonstrated.
3.2 Document Summarization
In this set of experiments, we apply our FS-NMF algorithm on document summarization. Let X be the documentsentence matrix, which can be generated from the documentterm and sentence-term matrices, and now each feature (column) in X represents a sentence. Then the sentences can be ranked based on the weights in W . Top-ranked sentences are included into the final summary. We use the DUC benchmark dataset (DUC2004) for generic document summarization to compare our method with other state-ofart document summarization methods using ROUGE evaluation toolkit [7]. The results are demonstrated in Table 3.

Systems
DUC Best Random Centroid [8] LexPageRank [3] LSA [4] NMF [5] FS-NMF

R-1
0.382 0.318 0.367 0.378 0.341 0.367 0.388

R-2
0.092 0.063 0.073 0.085 0.065 0.072 0.101

R-L
0.386 0.345 0.361 0.375 0.349 0.367 0.381

R-W
0.133 0.117 0.124 0.131 0.120 0.129 0.139

R-SU
0.132 0.117 0.125 0.130 0.119 0.129 0.134

Table 3: Overall performance comparison on DUC2004 data using ROUGE evaluation methods.
From the results, we observe that the summary generated by FS-NMF outperforms those created by other methods, and the scores are even higher than the best results in DUC competition. The good results benefit from good sentence feature selection in FS-NMF.
3.3 Visualization
In this set of experiments, we calculate the pairwise document similarity using the top 20 word features selected by different methods, and Figure 1 demonstrates the document similarity matrix visually. Note that in the document dataset (CSTR dataset), we order the documents based on their class labels.

50

100

150

200

250

300

350

400

50

100

150

200

250

300

350

400

50

100

150

200

250

300

350

400

450

50

100 150 200 250 300 350 400 450

50

100

150

200

250

300

350

400

450

50

100 150 200 250 300 350 400 450

(a) FS-NMF

(b) NMF

(c) LSI

Figure 1: Visualization results on the CSTR Dataset. Note that CSTR has 4 clusters.

From Figure 1, we have the following observations. (1) Word features selected by FS-NMF can effectively reflet the document distribution. (2) NMF tends to select some irrelevant or redundant words thus Figure 1(b) shows no obvious patterns at all. (3) LSI can also find meaningful words, however, the first two clusters are not clearly discovered in Figure 1(c).
Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants DMS-0915110 and DMS-0915228.

4. REFERENCES
[1] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum sum squared residue co-clustering of gene expression data. In Proceedings of SDM 2004.
[2] C. Ding and T. Li. Adaptive dimension reduction using discriminant analysis and k-means c lustering. In ICML, 2007.
[3] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.
[4] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.
[5] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, 2000.
[6] T. Li and C. Ding. The relationships among various nonnegative matrix factorization methods for clustering. In ICDM, 2006.
[7] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.
[8] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.
[9] S. X. Yu and J. Shi. Multiclass spectral clustering. In ICCV, 2003.

806

Learning to Rank Query Reformulations

Van Dang, Michael Bendersky and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst, MA 01003
{vdang, bemike, croft}@cs.umass.edu

ABSTRACT
Query reformulation techniques based on query logs have recently proven to be effective for web queries. However, when initial queries have reasonably good quality, these techniques are often not reliable enough to identify the helpful reformulations among the suggested queries. In this paper, we show that we can use as few as two features to rerank a list of reformulated queries, or expanded queries to be specific, generated by a log-based query reformulation technique. Our results across five TREC collections suggest that there are consistently more useful reformulations in the first positions in the new ranked list than there were initially, which leads to statistically significant improvements in retrieval effectiveness.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query Formulation
General Terms
Algorithms, Measurement, Performance, Experimentation.
Keywords
Query reformulation, query expansion, query log, query performance predictor, learning to rank.
1. INTRODUCTION
Query logs have become an important resource for many tasks including query reformulation [3, 6]. Most log-based reformulation techniques, however, are evaluated using nonstandard approaches and proprietary query logs, making it hard to compare one to another. A more recent study [2] compares different techniques using TREC collections and finds that when intial queries have relatively high quality, query expansion is much more reliable than substitution.
Although the log-based expansion technique [2] can generate some good reformulations for high-quality TREC queries, it also produces many bad reformulations and it does not generate a reliable ranking of the reformulations by quality.
In this paper, we show that we can effectively rerank the list of reformulated queries obtained with this log-based expansion approach. By using as few as two features, SCQ
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

(Similarity Collection Query) [8] and query clarity [1], we can substantially improve the ranking of reformulated queries in terms of the quality of the reformulations in the top two ranks (measured by NDCG@2 ), which then leads to significant improvements in retrieval effectiveness.

2. METHOD

2.1 Log-based Query Expansion
The log-based query expansion method [2] (referred to as LQE) is a slight modification of the query substitution method proposed by Wang and Zhai [6]. It first estimates a context distribution for terms occuring in a query log. It then constructs a translation model that can suggest similar words based on their distributional similarity. Given any query, the expansion model will try to expand it with candidates suggested by the translation model for each query term. The model decides whether to expand the query based on how similar the candidate is to the query term and how appropriate it is to the context of the query. For more details, see [2].

2.2 The Reranking Approach
Query quality predictors aim to predict a query's quality without explicit relevance judgements. Thus, given a ranked list of reformulated queries, it is intuitive to think about reorganizing this list based on the "quality" score given by some predictor.
We tried some of the top-performing predictors that Kumaran and Carvalho [4] used in a similar task and found that
[8] and clarity score [1] are the most effective for our problem. Therefore, we rerank the list of expanded queries by

( )= 1×

( )+ 2×

()

where 1 and 2 are weight of the two predictors.

Table 1: Statistics of queries used for reformulation

AP WSJ Robust-04 WT10G Gov-2

Title Q. 133 133

200

66

119

Desc. Q. 150 150

246

94

134

807

Table 3: Evaluation of retrieval effectiveness in terms of MAP.  and  indicate significant difference to the

original query and LQE's ranked list respectively. Best result in each column is marked in bold.

Title Query

Description Query

AP

WSJ

RBT-04 WT10G Gov-2

AP

WSJ RBT-04 WT10G Gov-2

Orig-Q 0.1694 0.2594

0.2247

0.1904 0.2829

0.1660 0.2358 0.2519 0.1770 0.2518

LQE

0.1741 0.2563

0.2297

0.1911 0.2559

0.1694 0.2391 0.2538

0.1775

0.2497

Rerank 0.1749 0.2663 0.2382 0.1962 0.2901 0.1820 0.2374 0.2584 0.1836 0.2579

Table 2: Our approach ("Rerank") consistently out-

performs LQE in NDCG@2. All differences are signif-

icant at < 0.05

Collection

Title Query

Desc. Query

LQE Rerank LQE Rerank

AP

0.2434 0.4805 0.2307 0.3728

WSJ

0.2318 0.5040 0.2250 0.3296

Robust-04 0.2905 0.5559 0.2138 0.3687

WT10G 0.2673 0.5499 0.1680 0.3847

Gov-2 0.1933 0.5830 0.2059 0.4093

3. EVALUATION
3.1 Experiment Settings
In this section, we evaluate the performance of our reranking technique. Evaluation is done on five TREC collections: AP, WSJ, Robust-04, WT10G and Gov-2, with both title and description queries. We use the language modeling framework and remove all stop words at indexing time. We adopt the parameter settings for LQE from the authors [2].
Due to the limited coverage of the available query log [5], we use only a subset of TREC queries where the LQE can generate at least one reformulation. Information about these subsets is given in Table 1.
On each collection, we first use LQE to generate a list of expanded queries ( = 30) for each original query. We append to this list the original query - in the case when all generated reformulations are bad, the reranking approach has a chance to choose not to reformulate. We then use our approach to rerank this list and compare its performance with that of the intial list as well as original query.
3.2 Training Data
We run LQE with the MSN log to obtain a list of reformulations for each original query. We use all these queries to do retrieval and record their MAP and use them to create our dataset. Training and testing are done using 5-fold cross validation on this dataset. 1 and 2 are learned using AdaRank [7] to maximize the average NDCG@2. The algorithm ends up choosing either ( 1 = 1, 2 = 0) or ( 1 = 0, 2 = 1) depending on the collection.
3.3 Reranking Effectiveness
We use NDCG@2 to measure the quality of the ranked list of reformulations given by our approach. Reformulations are graded on a scale from zero to four with respect to the improvement they provide over the original query. In particular, improvement larger than 0.03 corresponds to a 4, or ( > 0.03)  4. Similarly, (0.01 <  0.03)  3, (0 <  0.01)  2, ( = 0)  1 and ( < 0)  0.
Table 2 summarizes the result: the list of reformulations ranked by our approach has a much higher average NDCG@2

than the initial list. All improvements are statistically significant at < 0.05 using a two-tailed t-test.
3.4 Retrieval Effectiveness
We define the MAP of a ranked list of reformulations as the best MAP observed among its top queries. In this section, we compare the MAP obtained by (i) the original query, (ii) the list of reformulations generated by LQE, and (iii) the list reranked by our method.
As can be seen in Table 3, the best of the top two reformulated queries ranked by our approach is almost always significantly better than the original query. This is not the case in LQE. In many cases, our method also provides significant improvements over LQE. This result suggests that the reranking can push better reformulations to the first two positions in the ranked list.
4. CONCLUSIONS
In this paper, we have shown that by reranking the list of reformulations generated by the log-based query expansion technique [2] with only two features, we can push more good reformulations into the first two positions in the list. This is reflected in the huge gain of NDCG@2 and statistically significant improvement in retrieval effectiveness. In the future, we will investigate more features. We hope this will lead to greater improvement in NDCG@1, helping retrieval systems to reformulate queries implicitly without user involvement.
5. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
6. REFERENCES
[1] S. Cronen-Townsend, Y. Zhou, and W.B. Croft. Predicting Query Performance. In Proc. of SIGIR, pages 299-306, 2002.
[2] V. Dang and W.B. Croft. Query Reformulation Using Anchor Text. In Proc. of WSDM, pages 41-50, 2010.
[3] R. Jones, B. Rey and O. Madani. Generating Query Substitutions. In Proc. of WWW, pages 387-396, 2006.
[4] G. Kumaran and V.R. Carvalho. Reducing Long Queries Using Query Quality Predictors. In Proc. of SIGIR, pages 564-571, 2009.
[5] Proc. of the 2009 workshop on Web Search Click Data, Barcelona, Spain. ACM New York, NY, USA, 2009.
[6] X. Wang and C. Zhai. Mining Term Association Patterns from Search Logs for Effective Query Reformulation. In Proc. of CIKM, pages 479-488, 2008.
[7] J. Xu and H. Li. AdaRank: A Boosting Algorithm for Information Retrieval. In Proc. of SIGIR, pages 391-398, 2007.
[8] Y. Zhao, F. Scholer, and Y. Tsegay. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proc. of ECIR, pages 52-64, 2008.

808

Many are Better Than One: Improving Multi-Document Summarization via Weighted Consensus

Dingding Wang Tao Li
School of Computer Science Florida International University
Miami, FL 33199
{dwang003,taoli}@cs.fiu.edu

ABSTRACT
Given a collection of documents, various multi-document summarization methods have been proposed to generate a short summary. However, few studies have been reported on aggregating different summarization methods to possibly generate better summarization results. We propose a weighted consensus summarization method to combine the results from single summarization systems. Experimental results on DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems, and our proposed weighted consensus summarization method outperforms other combination methods.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.
General Terms: Algorithms, Experimentation.
Keywords: Weighted consensus, summarization.
1. INTRODUCTION
Various multi-document summarization methods base on different strategies and usually produce diverse outputs. A natural question arises: can we perform ensemble or consensus summarization by combining different summarization methods to improve summarization performance? In general, the terms of "consensus methods" or "ensemble methods" are commonly reserved for the aggregation of a number of different (input) systems. Previous research has shown that ensemble methods, by combining multiple input systems, are a popular way to overcome instability and increase performance in many machine learning tasks, such as classification, clustering and ranking. The success of ensemble methods in other learning tasks provides the main motivation for applying ensemble methods in summarization. To the best of our knowledge, so far there are only limited attempts on using ensemble methods in multi-document summarization.
As a good ensemble requires the diversity of the individual members, here we study several widely used multi-document summarization systems based on a variety of strategies, and evaluate different baseline combination methods for obtaining a consensus summarizer to improve the summarization performance. Motivated from [5], we also propose a novel weighted consensus scheme to aggregate the results from
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

individual summarization methods, in which, the relative contribution of an individual summarizer to the consensus is determined by its agreement with other members of the summarization systems. Note that usually a high degree of agreements does not automatically imply the correctness since the systems could agree on a faulty answer. However, each of the summarization systems has shown its effectiveness individually, so the agreement measure can be used in the consensus summarization.

2. WEIGHTED CONSENSUS SUMMARIZATION (WCS)

2.1 Notations
Suppose there are K single summarization methods, each of which produces a ranking for the sentences containing in the document collection. Then we have K ranking lists {r1, r2, · · · , rK} and ri  RN , i = 1, · · · , K, where N is the total number of sentences in the documents. The task is to find a weighted consensus ranking of the sentences r with a set of weights {w1, w2, · · · , wK } assigning to each of the individual summarization methods.

2.2 Formulation
Our goal is to minimize the weighted distance between r and all the ri. Let w = [w1, w2, . . . , wK ]T  RK . The problem can be formulated as follows.

arg min
w
s.t.

K
(1 - ) wi r - ri 2 +  w 2
i=1
K
wi = 1; wi  0 i,
i=1

where 0    1 is the regularization parameter which spec-

ifies the tradeoff between the minimization of the weighted

distance and the smoothness enforced by w. In experiments,

is set to 0.3 empirically. For simplicity, we use Euclidean

distance to measure the discordance of the consensus rank-

ing r and each of individual sentence rankings ri.

We

initialize

wi

=

1 K

,

and

this

optimization

problem

can

be solved by iterating the following two steps:

Step 1: Solve for r while fixing w. The optimal solution

is

the weighted average r = Step 2: Solve for w while

i wi fixing

ri. r

.

Let

d = [ r - r1 2, r - r2 2, · · · , r - rK 2]  RK .

809

Note that

K
(1 - ) wi r - ri 2 +  w 2 = (1 - )d w + w w

i=1

=

w-

-1 2 d

2-

( - 1)2 4

d2

For fixing r, the optimization problem becomes

arg min
w

w

-



-

1 d

2

s.t.

2

K
wi = 1;
i=1

wi  0,

i

This is a quadratic function optimization problem with

linear constraints with K variables. This is a problem of

just about tens of variables (i.e., weights for each input sum-

marization system) and thus can be computed quickly. It

can

also

be

solved

by

simply

projecting

vector

-1 2

d

onto

(K - 1)-simplex. With step 1 and 2, we iteratively update

w and r until convergence. Then we sort r in ascending

order to get the consensus ranking.

3. EXPERIMENTS
In the experiments, we use four typical multi-document summarization methods as individual summarizers and compare our WCS method with other eight aggregation methods. The four individual summarization methods are: (a) Centroid [7], (b) LexPageRank [1], (c) LSA [2], and (d) NMF [4]. And the baseline aggregation methods are: (1) average score (Ave Score), which normalizes and averages the raw scores from different summarization systems; (2) average rank (Ave Rank), which averages individual rankings; (3) median aggregation (Med Rank); (4) Round Robin (RR); (5) Borda Count (BC); (6) correlation-based weighting (CW), which weights individual systems by their average Kendall's Tau correlation between the ranking list they generated and all the other lists; (7) ULTRA [3], which aims to find a consensus ranking with the minimum average Spearman's distance [8] to all the individual ranking lists; (8) graph-based combination (Graph), the basic idea of which is similar to the work proposed in [9], however, we use cosine similarity so that we can compare this method with other combination methods fairly. We conduct experiments on DUC benchmark data for generic multi-document summarization and use ROUGE [6] toolkit (version 1.5.5) to measure the summarization performance.
3.1 Overall Summarization Performance
Table 1 show Rouge-1, Rouge-2, and Rouge-SU scores of different individual and combination methods using DUC2004 data sets (intuitively, the higher the scores, the better the performance). From the results, we observe that (1) Most of the combination summarization systems outperform all the individual systems except the round robin combination. The results demonstrate that in general consensus methods can improve the summarization performance. (2) Weighted combinations (e.g., CW, ULTRA, and WCS) outperform average combination methods which treat each individual system equally. (3) Our WCS method outperforms other weighted combination methods because WCS optimizes the weighted distance between the consensus sentence ranking to individual rankings and updates the weights and consensus ranking iteratively, which is closer to the nature of consensus summarization than other approximation based weighted methods.

Systems
DUCBest Centroid LexPageRank
LSA NMF Ave Score Ave Rank Med Rank RR BC CW ULTRA Graph WCS

R-1
0.382 0.367 0.378 0.341 0.367 0.388 0.385 0.385 0.364 0.378 0.378 0.392 0.379 0.398

R-2
0.092 0.073 0.085 0.065 0.072 0.089 0.087 0.087 0.072 0.085 0.085 0.090 0.086 0.096

R-SU
0.132 0.125 0.130 0.119 0.129 0.132 0.131 0.131 0.126 0.129 0.131 0.133 0.132 0.135

Table 1: Overall performance comparison on DUC2004. Remark: DUCBest shows the best results from DUC 2004 competition.

3.2 Diversity of Individual Summarizers
In this set of experiments, we further examine if the four individual summarization methods are complementary to each other. We use our WCS method to aggregate any three of the four summarization methods and compare the results with the aggregation utilizing all the four methods. Table 2 shows the comparison results. From the results, we observe that adding any of the four individual methods improves the summarization performance. This is because these individual summarization methods are diverse and their performance is data dependant.

Systems
Centroid+LexPageRank+LSA Centroid+LexPageRank+NMF
Centroid+LSA+NMF LexPageRank+LSA+NMF
All

R-1
0.383 0.385 0.376 0.382 0.398

R-2
0.088 0.090 0.082 0.087 0.096

R-SU
0.132 0.133 0.131 0.132 0.135

Table 2: WCS results on DUC2004.

Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants IIS0546280 and DMS-0915110.

4. REFERENCES
[1] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.
[2] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.
[3] A. Klementiev, D. Roth, and K. Small. An unsupervised learning algorithm for rank aggregation. In ECML, 2007.
[4] D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, pages 788­791, 1999.
[5] T. Li and C. Ding. Weighted consensus clustering. SIAM Data Mining, 2008.
[6] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.
[7] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.
[8] C. Spearman. The proof and measurement of association between two things. Amer. J. Psychol., 1904.
[9] V. Thapar, A. A. Mohamed, and S. Rajasekaran. A consensus text summarizer based on meta-search algorithms. In Proceedings of 2006 IEEE International Symposium on Signal Processing and Information Technology, 2006.

810

Investigating the Suboptimality and Instability of Pseudo-Relevance Feedback

Raghavendra Udupa
Microsoft Research India, Bangalore 560080, India
raghavu@microsoft.com

Abhijit Bhole
Microsoft Research India, Bangalore 560080, India
v-abhibh@microsoft.com

ABSTRACT
Although Pseudo-Relevance Feedback (PRF) techniques improve average retrieval performance at the price of high variance, not much is known about their optimality1 and the reasons for their instability. In this work, we study more than 800 topics from several test collections including the TREC Robust Track and show that PRF techniques are highly suboptimal, i.e. they do not make the fullest utilization of pseudo-relevant documents and under-perform. A careful selection of expansion terms from the pseudo-relevant document with the help of an oracle can actually improve retrieval performance dramatically (by > 60%). Further, we show that instability in PRF techniques is mainly due to wrong selection of expansion terms from the pseudo-relevant documents. Our findings emphasize the need to revisit the problem of term selection to make a break through in PRF.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
1. INTRODUCTION
Pseudo-relevance feedback (PRF) uses terms from the top ranking documents of the initial unexpanded retrieval for expanding the query [5]. Although PRF improves average retrieval performance, improvements have been incremental despite years of research [1, 3]. It is not known whether PRF techniques are under-performing or have already given their best by making the fullest use of pseudo-relevant documents in Query Expansion. Further, there is no satisfactory explanation of instability of PRF techniques. It is commonly believed that PRF is unstable because the initial unexpanded retrieval brings many non-relevant documents in the top for some topics and therefore, query expansion produces topic drift. However, for a good number of queries, retrieval performance does not change significantly. It is not known why PRF techniques fail to make a difference to such queries.
It is important that the twin issues of optimality and instability be addressed to decide whether to continue investment on new research in PRF and to devise effective ways of combating instability. In this work, we take the first steps towards understanding the optimality and instability of PRF
1By optimality of PRF techniques, we mean their use of pseudo-relevant documents in such a way as to maximize retrieval performance.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

techniques by studying more than 800 topics from several test collections including the TREC Robust Track.
We develop DEX, an oracle method for extracting a set of expansion terms from the pseudo-relevant documents using discriminative learning. Being an oracle method, DEX can be viewed as a good approximation to the ideal PRF technique that we can hope to design. As state-of-the-art PRF techniques and DEX extract expansion terms from the same set of pseudo-relevant documents, the gap in their retrieval performance indicates the future potential for improvement in retrieval performance of PRF techniques.
2. THE DEX ORACLE
DEX is an oracle for extracting a set of useful expansion terms from the pseudo-relevant documents by using the knowledge of relevant documents. DEX first extracts a set of candidate expansion terms2 t1, ..., tN from the pseudorelevant documents and then partitions this set into a set of useful terms and a set of non-useful terms using statistical learning [6]. It treats relevant documents for the topic as +ve instances and top scoring non-relevant documents as ve instances3. It learns a linear discriminant function w to discriminate the +ve instances from the -ve instances. The linear discriminant function classifies a vector x as +ve if wT x > 0 and as -ve if wT x  0. Therefore, DEX treats terms ti : wi > 0 as useful terms and the rest as non-useful. Finally, DEX picks the largest weighted k > 0 terms for expansion.
3. EXPERIMENTAL STUDY
We employed a KL-divergence based retrieval system with two stage Dirichlet smoothing as our baseline [4]. We used model-based feedback technique (Mixture Model) as a representative PRF technique [3]. For expanded retrieval, we interpolated the feedback model with the original query model with  set to 0.5. For estimating the feedback model, we used the top 10 documents fetched by the initial retrieval. Topics as well as documents were stemmed using the well known Porter stemmer and stop-words were removed. We compared model-based feedback with DEX-based PRF. We used the DEX algorithm (Section 2) to extract k = 5 expansion terms from the top 10 documents of the unexpanded retrieval. We formed a feedback model from the expansion
2Candidate expansion terms are those terms from the pseudo-relevant documents whose idf > ln10 and collection frequency  5 [2]. 3Each labeled instance D has an associated feature vector x whose dimensions i = 1, ..., N correspond to the candidate expansion terms t1, ..., tN respectively.

813

Table 1: Comparitive Retrieval Performance

Collection
CLEF 00-02 CLEF 03,05,06
AP WSJ SJM Robust03 Robust04

LM MAP P@5 0.43 0.49 0.38 0.42 0.28 0.47 0.27 0.48 0.21 0.34 0.11 0.32 0.25 0.49

MF MAP P@5 0.44 0.50 0.41 0.43 0.33 0.50 0.30 0.52 0.24 0.36 0.13 0.34 0.28 0.49

DEX MAP P@5 0.74* 0.74 0.66* 0.72 0.48* 0.73 0.43* 0.72 0.43* 0.63 0.29* 0.64 0.40* 0.71

terms by assigning equal probability mass to the DEX terms. As with model-based feedback, we interpolated our feedback model with the query model with  set to 0.5. We call unexpanded retrieval, model-based feedback, and DEX-based PRF as LM, MF and DEX respectively.
3.1 Test Collections
We used the CLEF (LATimes 94, Glasgow Herald 95) and TREC (Associated Press 88-89, Wall Street Journal, San Jose Mercury, Disks 4&5 minus the Congressional Record) document collections in our experiments. We studied retrieval performance on the following sets of topics: CLEF Topics 1 - 140 (CLEF 2000-2002), Topics 141-200 (CLEF 2003), Topics 251-350 (CLEF 2005-2006), TREC Topics 51 - 200 (TREC Adhoc Tasks 1, 2, 3), Assorted Topics (TREC Robust 2003, Hard), Topics 301-450, 601-700 (TREC Robust 2004). There were totally 821 unique topics. Some topics were used for retrieval on multiple document collections.
3.2 Retrieval Performance
We used MAP and P@5 as the average performance measures to compare the three retrieval models. We say that a topic is hurt by expanded retrieval if the average precision decreases by 0.01 or more relative to the unexpanded retrieval. Similarly, we say that a topic is improved (benefitted) by a retrieval model if the average precision increases by 0.01 or more. We compare the performance of LM, MF, and DEX on all topics in Table 1. We see that MF fares better than LM overall but the improvement in retrieval performance is modest. In contrast, DEX gives dramatic improvement in retrieval performance relative to both LM and MF on all test sets despite using the same set of documents for estimating the feedback model. DEX improves MAP by > 60% over LM and by > 42% over MF in general and on Robust03, DEX improves MAP by 123% over MF. Not only the MAP has improved dramatically P@5 has also improved. The huge gap between the average retrieval performance of MF and DEX highlights a very important fact: PRF techniques are highly suboptimal. Their average retrieval performance is much lower than what can be potentially achieved using the same set of feedback documents.
Table 2 shows the percentage of topics which benefitted from MF and DEX, topics which got hurt by MF and DEX and topics which remained indifferent to MF and DEX. We observe that nearly 25% of topics in all collections are indifferent to MF whereas a smaller percentage of topics are hurt by MF. DEX reduces the percentage of topics in these two categories substantially. In the Robust03 track (hard), we see that the percentage of topics which got hurt reduced to 6% from 14% and the percentage of topics which remained indifferent from 28% to 4%. The relatively high robustness

Table 2: Effect of MF and DEX on individual topics

Collection Name
CLEF 00-02 CLEF 03,05,06
AP WSJ SJM Robust03(Hard) Robust04

% Topics improved MF DEX 54 91 56 89 68 97 61 93 56 98 52 90 55 85

% Topics indifferent MF DEX 23 8 25 8 24 1 27 2 29 1 34 4 28 4

% Topics

hurt

MF DEX

23 1

19 3

8

2

11 5

15 1

14 6

17 11

of DEX gives hope for PRF techniques to achieve a higher degree of robustness while not sacrificing the gain in average retrieval performance. Wrong selection of terms is at the root of instability and PRF techniques will need to relook term selection strategies [1, 2].
3.3 Discussion
To understand why MF is suboptimal and unstable, we computed the average rank of the DEX terms in the rank list of the MF expansion terms (ranked according to p(.|F )). We suspected that DEX terms would not be at the top of the rank list. Because otherwise MF would not be comparatively so worse. Our suspicion turned out to be true. For a large majority of the topics, DEX terms were deep down in the MF rank list. For instance, the average rank of DEX terms for Topic 193 from TREC 3 was 114 and for TREC Topics 350 and 190 it was 229 and 735 respectively. It is clear that MF fails to recognize the importance of DEX terms and ranks them poorly. As a consequence of not choosing the right terms, expanded retrieval fails to improve the retrieval performance of these topics.
4. CONCLUSION
Our study shows that current PRF techniques are highly suboptimal and also that wrong selection of expansion terms is at the root of instability of current PRF techniques. A careful selection of expansion terms from the pseudo-relevant documents with the help of an oracle can actually improve retrieval performance dramatically (by > 60%). We believe our findings will motivate PRF researchers to revisit the issue of term seleaction in PRF. It might be worthwhile to selectively extract expansion terms from the feedback documents. Further, term interactions may prove crucial in addressing the problems of suboptimality and instability[2].
5. REFERENCES
[1] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In Proceedings of SIGIR '08.
[2] R. Udupa, A. Bhole, and P. Bhattacharyya. On selecting a good expansion set in pseudo-relevance feedback. In Proceedings of ICTIR '09.
[3] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval.In Proceedings of CIKM '01.
[4] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst.(2)
[5] E. N. Efthimiadis. Query expansion. Annual Review of Information Systems and Technology.
[6] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning.

814

High Precision Opinion Retrieval using Sentiment-Relevance Flows

Seung-Wook Lee swlee@nlp.korea.ac.kr

Jung-Tae Lee jtlee@nlp.korea.ac.kr

Young-In Song yosong@microsoft.com

Hae-Chang Rim rim@nlp.korea.ac.kr

Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea  Microsoft Research Asia, Beijing, China

ABSTRACT
Opinion retrieval involves the measuring of opinion score of a document about the given topic. We propose a new method, namely sentiment-relevance flow, that naturally unifies the topic relevance and the opinionated nature of a document. Experiments conducted over a large-scaled Web corpus show that the proposed approach improves performance of opinion retrieval in terms of precision at top ranks.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms, Measurement, Experimentation
Keywords
opinion retrieval, sentiment analysis, sentiment-relevance flow
1. INTRODUCTION
Opinion retrieval is a new retrieval task which involves locating documents that express opinions about a topic of interest. With the rapid growth of user-centric media such as blogs and forums, opinion retrieval has been gaining considerable attention in recent years from academia and industry motivated by the huge business opportunities.
A key to success in opinion retrieval is to leverage both the topical relevance and the opinionated nature of documents simultaneously in ranking. However, most opinion retrieval systems separate the two components independently by adopting a re-ranking approach [1]. This approach involves finding as many relevant documents with regard to a given topic as possible regardless of their opinionated nature at first and then re-ranking them by combining the topical relevance scores with the opinion scores computed using some opinion detection techniques. Although a few approaches have shown attempts to unify the two components, for example, by using the proximity between topic words and opinion words within a document [3], the results are not yet conclusive and require more investigation.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Illustration of various flows.
In this paper, we present a new opinion retrieval method that adopts a very recently proposed technique involving relevance flow graphs [2]. A relevance flow graph of a document is a graphical plot of the topic relevance degree of individual sentences (with regard to a query) versus their positions in the document. The line graph labeled "Relevance" in Figure 1 illustrates an example; it visually shows the fluctuation of topic relevance levels within a document in regard to the query. [2] demonstrates that topically relevant documents have distinguishable flows from non-relevant ones in terms of the variance of relevance levels or the positions of high relevance levels (namely peaks), by designing a regression model based on such information and applying it to re-rank the retrieved results of conventional document ranking. Such method has shown promising results in improving the accuracy at top ranks.
Motivated by this achievement, we hypothesize that a truly relevant document in opinion retrieval not only has opinion sentences regarding the topic but does have them in predictable patterns within the document. Our idea is to create a new flow graph, namely sentiment flow, that plots the opinionated nature of individual sentences using some opinion scoring method, and then merge it with the topic relevance flow to generate a whole new flow graph, called sentiment-relevance flow. The dotted line graph labeled "Sent-Rel" in Figure 1 illustrates an example. The following section will elaborate on such technique.
2. SENTIMENT-RELEVANCE FLOW
The sentiment-relevance flow of a document (SRF) is a sequence of scores that reflect both the topic relevance with regard to the query and the opinionated nature of individual

817

sentences, ordered by the sentence positions. Given a query Q, we calculate the score of a sentence si at position i as follows:

score(Q, si) = topic(Q, si) · opinion(si)

(1)

where topic(Q, si) refers to the topic relevance score of si for Q calculated using some conventional relevance scoring function, such as the BM25 function. opinion(si) represents the opinion score of si computed as follows:

i+W |O|

opinion(si) =

f req(sj, owk)

(2)

j=i-W k=1

where W is the half size of the context window (empirically set to 15), O is a set of opinion word lexicon, and f req(sj, owk) is a function that returns the frequency of opinion word owk in sentence sj. When computing this score, we look at not only the sentence at hand but also its context, because we have observed that opinions often appear not in the same sentence where topic words occur but in preceding or succeeding sentences.
We normalize the individual sentence scores and the sentence positions in range 0 to 1 as in [2]. The graphical plot of such SRF, as shown in Figure 1, indicates the fluctuation of both topic relevance and opinionated nature within the document in a comprehensive way. As in [2], we refer to sentences that have scores higher than 0.5 as peaks.
In order to infer document relevance from SRFs, we use maximum entropy modeling to train a regression model that is able to predict the relevance of a document based on its SRF. The main features extracted from the SRFs for regression are the ones found useful in [2], which include the following: the variance of sentence scores, the fraction of peaks, and the first peak position.
For opinion retrieval, we rank documents by linearly combining the topic relevance scores with the new relevance scores inferred from their SRFs as follows:

score(Q, D) =  · topic(Q, D) + (1 - ) · srf (Q, D) (3)

where topic(Q, D) is the topic relevance of document D with regard to Q, and srf (Q, D) is the prediction score of the classifier for D.  is a weight parameter where 0    1.

3. EXPERIMENT
We conduct our experiments over Blogs06, a large-scaled Web blog collection. We use the title queries of 150 topics used in 2006, 2007, and 2008 TREC Blog Tracks. We validate our method in a re-ranking scheme. In other words, we initially retrieve the top n (=15) documents using two popular ranking models, the BM25 model and the query likelihood (QL) language model with dirichlet smoothing1, and then re-rank the results using Equation 3. When evaluating one query set (50 topics), other two query sets (100 topics) are used to train a regression model. 15 top ranked documents for every training query are divided into positive and negative training instance groups based on their relevance judgments. The main evaluation measures are P@1, P@3, and P@5 since our method aims to achieve high accuracy at top ranks via re-ranking. The relevance judgement set consists of two distict aspects, relevance and opinionated
1We empirically tuned k1=1.2, b=0.1 for BM25 and µ=5000 for QL smoothing parameter.

Table 1: Performance of topical retrieval

Method

P@1 P@3 P@5

BM25

N/A 0.5800 0.6334 0.6041

BM25+RF 0.15 0.6333 0.6422 0.6427

BM25+SRF 0.00 0.7200 0.7089 0.6907

QL

N/A 0.5933 0.6200 0.6160

QL+RF

0.02 0.6333 0.6711 0.6587

QL+SRF 0.01 0.7600 0.6956 0.6933

Table 2: Performance of opinion retrieval

Method

P@1 P@3 P@5

BM25

N/A 0.4400 0.4556 0.4320

BM25+SRF 0.01 0.5800 0.5800 0.5573

QL

N/A 0.4800 0.4756 0.4640

QL+FULL N/A 0.5600 0.5222 0.5373

QL+PROX N/A 0.5800 0.5556 0.5547

QL+SRF

0.01 0.6333 0.5689 0.5520

relevance. Thus, we report the topic P@Ns and the opinion P@Ns of the system separately. For sentence boundary detection, we use a public sentence splitter software2. We use the sentiment word list in the General Inquirer3, which is a public opinion dictionary in the linguistics field. We only collect adjectives, adverbs, and verbs from the opinion and emotion categories; as a result, the lexicon is made up of 1,496 entries.
Table 1 shows the performance of topical retrieval for the two initial results, relevance flow based re-ranking (RF) and our sentiment-relevance flow based re-ranking (SRF). As mentioned from previous study, RF successfully re-ranks high position documents. It is notable that our SRF also improves the performance of traditional topical retrieval since it basically aims to capture the pattern of relevant sentences.
In aspect of opinion retrieval, as shown in Table 2, SRF shows significant and consistant improvement. We compare our method with two previous re-ranking approaches for opinion retrieval with QL setting (since it outperforms BM25). Opinion score measured by QL+FULL is dominated by a number of opinion words that appeared in a whole document, while QL+PROX only considers opinion words located within W sentences from the query terms. We can observe that proximity is a helpful feature for opinion retrieval. It is remarkable that our sentiment-relevance flow based re-ranking scheme achieves better improvement. Note that the maximum performance are acheived on low  values which implies that the trained regression model based on SRF features are very accurate and reliable.
4. REFERENCES
[1] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the TREC-2008 blog track. In Proc. of TREC 2008.
[2] J. Seo and J. Jeon. High precision retrieval using relevance-flow graph. In Proc. of SIGIR 2009.
[3] M. Zhang and X. Ye. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In Proc. of SIGIR 2008.
2http://l2r.cs.uiuc.edu/~ecogcomp/tools.php 3http://www.wjh.harvard.edu/~inquirer/

818

Query Term Ranking based on Dependency Parsing of Verbose Queries

Jae-Hyun Park and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science
University of Massachusetts, Amherst, MA, 01003, USA
{jhpark,croft}@cs.umass.edu

ABSTRACT
Query term ranking approaches are used to select effective terms from a verbose query by ranking terms. Features used for query term ranking and selection in previous work do not consider grammatical relationships between terms. To address this issue, we use syntactic features extracted from dependency parsing results of verbose queries. We also modify the method for measuring the effectiveness of query terms for query term ranking.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation
General Terms
Algorithm, Experimentation, Performance
Keywords
Dependency Parse, Query Reformulation, Query Term Ranking
1. INTRODUCTION
Most search engines have a tendency to show better retrieval results with keyword queries than with verbose queries. Verbose queries tend to contain more redundant terms and these terms have grammatical meaning for communication between humans to help identify the important concepts.. Search engines do not typically use syntactic information.. For example, given a verbose query, "Identify positive accomplishments of the Hubble telescope since it was launched ...", search engines cannot recognize that "Hubble telescope" is the key concept of the query whereas "accomplishments" should be considered as a complementary concept, while people can readily identify this by analyzing the grammatical structure of the query. Therefore, search engines potentially need a method for exploiting this structure.
In this work, we rank terms in a verbose query and reformulate a new query using selected highly ranked terms. Good selection methods should be able to leverage the grammatical roles of terms within a query. To do this, we use syntactic features extracted from dependency parsing trees of queries. In addition, we suggest a new method for measuring the effectiveness of terms for query term ranking.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. QUERY TERM RANKING
2.1 Features extracted from Dependency Parsing
We use syntactic features extracted from dependency parsing to capture the grammatical properties of terms for a query. Features used by previous work in query term ranking [1, 6] are inadequate to reflect these characteristics. The limitation of these features is that they are based on individual terms. Features such as tf, idf, part-of-speech (PoS) tag, etc. will not change even if the role of the term changes according to the syntactic structure of queries. Even features for sub-queries [5] are also unlikely to reflect grammatical characteristics because they are not affected by the structure of queries.
Therefore, we propose to overcome this limitation by using dependency parsing trees. A typed dependency parse labels dependencies with grammatical relations [3]. Figure 1 shows an example of a typed dependency parse tree. Dependency parsing tree fragments of terms can provide grammatical information about terms in queries [2].
It is infeasible to use all dependency parse tree fragments as syntactic features. We limit the number of arcs in syntactic features to two arcs. Even if we limit the number of arcs, some of collected tree fragments are too specific to

Sentence: Identify positive accomplishments of the Hubble telescope since it was launched in 1991.

Identify

dobj

amod accomplishments prep_of

positive

telescope nn

Hubble

Figure 1: An example of dependency parsing trees. Labels attached to arcs are types of dependencies.

Identify

dobj

*

dobj

Identify

*

accomplishments accomplishments

(a)

(b)

accomplishments (c)

Figure 2: Three types of syntactic features for the term "accomplishments". (a) An original syntactic feature (b) The word is generalized to a * (c) The type of the dependency is generalized to a *

829

have a reliable amount of training data and not all of them are useful. We generalize syntactic features which consist of arcs labeled with dependency types and nodes representing words which are dependent. Figure 2 shows an example of an original syntactic feature and its generalized features. In the figure, "*" means any word or any type of dependency.
2.2 Query Term Ranking
Our approach aims to rank terms in a query and to reformulate the query using the ranking. To build training data for a ranking model, Bendersky and Croft [1] manually annotate the concept from each query that had the most impact on effectiveness. For given terms = { 1, 2, ..., }, they used labeled instances ( , ), where is a binary label, as training data. However, queries can have more than one effective term or concept. In addition, it is difficult for annotators to judge the effectiveness of a term. Therefore, we estimate the effectiveness of terms, i.e., the labels for training data, by evaluating the search results of terms in training data. By using these estimated scores, we expect that a ranking model can take account of all terms in a query and consider how effective they are.
Lee et al. [6] point out the importance of underlying correlations between terms. Previous work has evaluated the effectiveness of groups of terms instead of individual terms to capture these relationships [5, 6]. The problem is that the number of unique groups will grow exponentially with the size of the term groups and it will cause a data sparseness problem. We used the following equation for ( ), the effectiveness of a term to reflect the effects of relationships between terms in training labels.

( )=

1



(

(,

)-

( )),

(1)



where is all possible combinations of m terms except . N is the number of elements in and ( ) is the search performance of . Eq. (1) estimates the effectiveness of term
through aggregating the impacts of term on effectiveness when using it with other terms in . Thus, the scores of Eq. (1) reflects the correlations between and other terms.

3. EXPERIMENTS AND ANALYSIS
We evaluated our proposed method using two TREC collections: Robust 2004 (topic numbers are 301-450 and 601700) and Wt10g (topic numbers are 450-550). The average number of nouns, adjectives and verbs in queries of Robust2004 and Wt10g are 8.7 and 6.5 per a query, respectively. We used the language model framework with Dirichlet smoothing ( set to 1,500). Indexing and retrieval were conducted using the Indri toolkit.
To rank query terms, we used RankSVM [4]. We trained query term ranking models for each query using leave-oneout cross-validation in which one query was used for test data and the others were used for training data. We labeled training data based on Key concepts [1] and the effectiveness measured by Eq. 1 in which we chose nDCG as the performance measure. We used syntactic features in addition to tf, idf, and PoS tag features.
When we combined selected terms with original queries, we used two approaches. First, we assigned uniform weights to selected terms (binary). Alternatively, we used query term ranking scores as the weight for selected terms (weight).

Table 1: Mean Average Precision (MAP) of Ro-

bust04 and Wt10g collections, Key-Concept: using

key concept [1] as labels of training data, Auto: us-

ing effectiveness in retrieval as labels of training data

Robust04 Wt10g

<title>

25.17

18.55

<desc>

24.07

17.52

Key-Concept

binary weight

23.98 24.24

18.55 19.45

Auto

binary weight

25.40 26.21

17.91 19.15

Experimental results in Table 1 shows that selected terms by using query term ranking have better performance than description queries except for one result in which we used key concepts and uniform weighting. In this case, only the most important concepts in queries are labeled, whereas the effectiveness in retrieval is measured for all terms in queries. This difference makes the method using the effectiveness of terms (Auto) superior for the relatively longer queries in Robust2004, and the method using key concepts (Key Concept) better for the shorter queries in Wt10g.
4. CONCLUSIONS
In this paper, we propose a query term ranking method that uses syntactic features extracted from dependency parsing trees. By using syntactic features, we can take into account grammatical relationships between terms. We also modify the query term ranking method to measure the effectiveness of terms based on combinations of terms. Experimental results showed that the terms selected by the query term ranking method improved retrieval performance.
5. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0711348. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
6. REFERENCES
[1] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. ACM SIGIR, pages 491­498, 2008.
[2] A. Chanen. A comparison of human and computationally generated document features for automatic text classification. PhD thesis, The University of Sydney, 2009.
[3] M. De Marneffe, B. MacCartney, and C. Manning. Generating typed dependency parses from phrase structure parses. In Proc. LREC 2006, 2006.
[4] T. Joachims. Optimizing Search Engines Using Clickthrough Data. In Proc. ACM KDD, pages 133­142, 2002.
[5] G. Kumaran and V. Carvalho. Reducing long queries using query quality predictors. In Proc. ACM SIGIR, pages 564­571, 2009.
[6] C. Lee, R. Chen, S. Kao, and P. Cheng. A term dependency-based approach for query terms ranking. In Proc. CIKM, pages 1267­1276, 2009.

830

Probabilistic Latent Maximal Marginal Relevance

Shengbo Guo
ANU & NICTA Canberra, Australia
shengbo.guo@nicta.com.au

Scott Sanner
NICTA & ANU Canberra, Australia
scott.sanner@nicta.com.au

ABSTRACT
Diversity has been heavily motivated in the information retrieval literature as an objective criterion for result sets in search and recommender systems. Perhaps one of the most well-known and most used algorithms for result set diversification is that of Maximal Marginal Relevance (MMR). In this paper, we show that while MMR is somewhat adhoc and motivated from a purely pragmatic perspective, we can derive a more principled variant via probabilistic inference in a latent variable graphical model. This novel derivation presents a formal probabilistic latent view of MMR (PLMMR) that (a) removes the need to manually balance relevance and diversity parameters, (b) shows that specific definitions of relevance and diversity metrics appropriate to MMR emerge naturally, and (c) formally derives variants of latent semantic indexing (LSI) similarity metrics for use in PLMMR. Empirically, PLMMR outperforms MMR with standard term frequency based similarity and diversity metrics since PLMMR maximizes latent diversity in the results.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms
Keywords
diversity, graphical models, maximal marginal relevance
1. INTRODUCTION
Maximal marginal relevance (MMR) [2] is perhaps one of the most popular methods for balancing relevance and diversity in set-based information retrieval and has been cited over 530 times1 since its publication in 1998.
The basic idea of MMR is straightforward: suppose we have a set of items D and we want to recommend a small subset Sk  D (where |Sk| = k and k  |D|) relevant to a given query q. MMR proposes to build Sk in a greedy
1According to Google Scholar.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

manner by selecting sj given Sj-1 = {s1, . . . , sj-1} (where Sj = Sj-1  {sj }) according to the following criteria

sj

= arg max [(Sim1(sj, q)) - (1
sj D\Sj-1

-

) max
si Sj-1

Sim2(sj, si)]

(1)

where Sim1(·, ·) measures the relevance between an item and a query, Sim2(·, ·) measures the similarity between two items, and the manually tuned   [0, 1] trades off relevance and similarity. In the case of s1, the second term disappears.
While MMR is a popular algorithm, it was specified in a

rather ad-hoc manner and good performance typically relies

on careful tuning of the  parameter. Furthermore, MMR is

agnostic to the specific similarity metrics used, which indeed

allows for flexibility, but makes no indication as to the choice

of similarity metrics for Sim1 and Sim2 that are compatible with each other and also appropriate for good performance.

In the next section, we address these concerns by taking a

more principled approach to set-based information retrieval

via maximum a posteriori probabilistic inference in a latent

variable graphical model of marginal relevance (PLMMR).

As an elegant and novel contribution, we note that natural

relevance and diversity metrics emerge from this derivation

(with no analogous manually tuned  parameter) and that

these metrics also formally motivate variants of similarity

metrics used in latent semantic indexing (LSI) [3].

2. PROBABILISTIC LATENT MMR

Figure 1: Graphical model used in PLMMR.
We begin our discussion of PLMMR by introducing a graphical model of (marginal) relevance in Figure 1. Shaded nodes represent observed variables while unshaded nodes are latent; we do not distinguish between variables and their assignments. The observed variables are the vector of query terms q and the selected items s1  D and s2  D. For the latent variables, let T be a discrete topic set; variables t1  T and t2  T respectively represent topics for s1 and

833

s2 and t  T represents a topic for query q. r1  {0, 1} and
r2  {0, 1} are variables that indicate whether the respective
selected items s1 and s2 are relevant (1) or not (0). The conditional probability tables (CPTs) in this discrete
directed graphical model are defined as follows. P (t1|s1) and P (t2|s2) represent topic models of the items and P (t|q) represents a topic model of the query. There are a variety of ways to learn these topic CPTs based on the nature of the items and query; for an item set D consisting of text documents and a query that can be treated as a text document, a natural probabilistic model for P (ti|si) and P (t|q) can be derived from Latent Dirichlet Allocation (LDA) [1]. Finally, the CPTs for relevance ri have a very natural definition:

P (r1|t, t1) =

1 0

if t1 = t if t1 = t

P (r2|t, r1 = 0, t1, t2) =

1 0

if (t2 = t1)  (t2 = t) if (t2 = t1)  (t2 = t)

Simply, s1 is relevant if its topic t1 = t (the query topic).

s2 is relevant with the same condition and the addition that

if s1 was irrelevant (r1 = 0), then topic t2 for s2 should also

not match t1. Following the click-chain model [4], we assume

the user only examines s2 if s1 was irrelevant (r1 = 0).

Let us assume that like MMR we use a greedy item set se-

lection algorithm given S1 = {s1},

and we have already we want to select s2

selected s1 = s1. Now in order to maximize

its marginal relevance w.r.t. q given S1, formally defined as

MR(S1, s2, q) and derived as a query in the graphical model:

s2 = arg max MR(S1, s2, q) = arg max P (r2|s1, s2, q)

s2 D\S1

s2D\{s1 }

= arg max

P (r2|r1 = 0, t1, t2, t)P (t1|s1)

s2 D\{s1 } t1,t2,t

P (r1 = 0|t1, t)P (t2|s2)P (t|q)

= arg max
s2 D\{s1 }

P (t|q)P (t2 = t|s2) -
t
relevance

P (t|q)P (t1 = t|s1)P (t2 = t|s2)

(2)

t

diversity

The basic insight leading to this fascinating result is the
exploitation of the indicator structure of the relevance vari-
ables r1 and r2 to make convenient variable substitutions. We note that in this special case for MR(S1, s2, q), a very
natural mapping to the MMR algorithm in (1) when  = 0.5 has emerged automatically from the derivation that maximized MR. This derivation automatically balances relevance and diversity without an analogous  and it suggests very specific (and different) relevance and diversity metrics, both effectively variants of similarity metrics used in latent semantic indexing (LSI) [3]. To make this clear, we examine
tlqehuteerTryeleqavananndcdeTitm2emebtersic2reSwsipimtehP1cLtviMevcM etRotrogpeivilceemnpebrnoytbsPaTbLiMliitM=y RPve(wctthoe=rrsei|fwqoer) and T2i = P (t2 = i|s2) and using ·, · for the inner product:

SimP1 LMMR(q, s2) = P (t|q)P (t2 = t|s2) = T, T2 .
t

A similar analysis gives diversity metric SimP2 LMMR(s1, s2), yielding a variant LSI similarity metric reweighted by the query topic probability P (t|q). This points out the important correction to MMR that item set diversity should be

Table 1: Weighted subtopic loss (WSL) of three methods using all words and first 10 words. Standard error estimates are shown for PLMMR-LDA.

Method MMR-TF MMR-TFIDF PLMMR-LDA

WSL (first 10 words) 0.555 0.549
0.458 ± 0.0058

WSL (all words) 0.534 0.493
0.468 ± 0.0019

query-relevant! Given these definitions of SimP1 LMMR and SimP2 LMMR, we can now substitute these into the MMR algorithm defined in (1) to arrive at a definition of PLMMR.

3. EXPERIMENTAL COMPARISON
We report experiments on a subset of TREC 6-8 data focusing on diversity. We follow the same experimental setup as [6] who measure the weighted subtopic loss (WSL) of recommended item sets where in brief, WSL gives higher penalty for not covering popular subtopics. We do not compare directly to [6] as their method was supervised while MMR and PLMMR are inherently unsupervised.
Standard query and item similarity metrics used in MMR applied to text data include the cosine of the term frequency (TF) and TF inverse document frequency (TFIDF) vector space models [5]. We denote these variants of MMR as MMR-TF and MMR-TFIDF. PLMMR specifically suggests the use of LSI-based similarity metrics defined in the last section; thus, we use LDA to derive these models, referring to the resulting algorithm as PLMMR-LDA. LDA was trained with  = 2.0,  = 0.5, |T | = 15; we note the results were not highly sensitive to these parameter choices.
Average WSL scores are shown in Table 1 on the 17 queries examined by [6]. We use both full documents and also just the first 10 words of each document. For both MMR algorithms, the best performing  = 0.5 is shown. We note that due to the power of the latent topic model and derived similarity metrics, PLMMR-LDA is able to perform better than MMR with standard TF and TFIDF metrics and without a  parameter to be tuned. In addition, PLMMRLDA works very well with short documents since intrinsic document and query similarities are automatically derived from the latent PLMMR relevance and diversity metrics.

4. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993­1022, 2003.
[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 335­336. 1998.
[3] S. Deerwester, S. T. Dumaisand, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. JASIS, 41:391­407, 1990.
[4] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y.-M. Wang, and C. Faloutsos. Click chain model in web search. In WWW-09, Madrid, Spain, 2009. ACM.
[5] G. Salton and M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983.
[6] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML, 1224­1231, 2008.

834

A Framework for BM25F-based XML Retrieval

Kelly Y. Itakura
David R. Cheriton School of Computer Science University of Waterloo 200 University Ave. W. Waterloo, ON, Canada
yitakura@cs.uwaterloo.ca

Charles L.A. Clarke
David R. Cheriton School of Computer Science University of Waterloo 200 University Ave. W. Waterloo, ON, Canada
claclark@uwateloo.ca

ABSTRACT
We evaluate a framework for BM25F-based XML element retrieval. The framework gathers contextual information associated with each XML element into an associated field, which we call a characteristic field. The contents of the element and the contents of the characteristic field are then treated as distinct fields for BM25F weighting purposes. Evidence supporting this framework is drawn from both our own experiments and experiments reported in related work.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Theory, Experimentation
Keywords
XML retrieval, BM25, BM25F, Wikipedia, book search
1. INTRODUCTION
INEX [1], the annual Initiative for the Evaluation of XML retrieval, includes experiments on ad hoc focused XML element retrieval, where the task is to return a ranked list of document elements (e.g., paragraphs, sections, abstracts) in response to a previously unseen query. Elements are required to be non-overlapping, so that no returned element contains another, but otherwise any document element may be returned.
While various participating groups have reported attempts to exploit XML structure in order to improve performance on this task, none of these efforts have consistently outperformed the simple approach of applying Okapi BM25 [12] to score individual XML elements and then filtering the resulting ranked list to remove overlap. Under this approach, each element is scored as if it were an independent document. The context of the element -- such as information appearing in the elements that surround it -- is ignored. Runs using this basic approach ranked third in 2004, third in 2007, and first in 2008 [4, 5, 9].
Okapi BM25 is a well-established ranking formula, which has proven its value across a wide range of domains and ap-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

plications. For XML element retrieval, we use the following version:

s(E)



tQ

Wt

fE,t

fE,t(k + k(1 -

+ 1) b+b

elE avgdl

)

,

(1)

where Q is a set of query terms, Wt is the IDF weight of the term t, fE,t is the term frequencies in an element E, elE is an element length of E, and avgdl is the average document length in the collection. In applying BM25 to element retrieval, we continue to use document-level statistics for the computation of Wt.
Robertson et al. [2, 13] describe BM25F, an extension of BM25 that exploits structural information. Under BM25F, terms contained in a document's title, for example, may be given more weight than terms contained in the document's body.
In this paper, we explore a framework for adapting BM25F to XML element retrieval. Under this framework, we construct two fields for each element. One field contains the contents of the element itself; the other field, called the characteristic field, contains contextual information associated with the element. This contextual information will vary from domain to domain, but might include the title of the overall document, titles from containing elements, and document metadata.

2. BM25F

Using BM25F, an element's score is computed as follows:

BM 25F

(e)

=

tqe

K

xe,t + xe,t

Wt

,

where q is a query term, xe,t is a weighted normalized term frequency, K is a tunable parameter, and Wt is documentlevel IDF for a term t. To obtain xe,t, length normalization is first performed separately for each field f associated with an

element e, producing field-specific normalized term frequen-

cies. These normalized term frequencies xe,f,t are multiplied by field-specific weights Wf and summed to obtain xe,t

xe,f,t =

xe,f,t

1

+

Bf

(

le,f lf

- 1)

, xe,t =

f

Wf · xe,f,t ,

where xe,f,t is the term frequency of t in field f of element e, Bf is a tunable parameter, le,f is the length of f in e, and lf is the average field length of f . We report the results obtained by treating average document and field lengths as
a constant, but later experiments that treated them as pa-
rameters seem to give no advantage.

843

Table 1: Adhoc Focused Retrieval

Run

iP[0.01] rank

BM25F 0.6333 1

BM25 0.5940 12

For INEX 2005, Robertson et al. applied an earlier version of BM25F to XML element retrieval [8, 11], reporting 65% improvements over BM25 measured by nxCG(10) on INEX IEEE collection with a different task where overlap is allowed. In that work, an element's score is computed from multiple fields, which may include the body of the element, the document's title, the document's abstract, and ancestral section titles.
Trotman describes another effort to extend BM25F to XML element retrieval [15] on TREC Wall Street Journal collection, but showed that improvement obtained is 0.64% computed by mean average precision. BM25F has also been used for XML-encoded book retrieval, where the task was to return books not elements [7, 16] producing 9.09% improvement measured by NDCG@1.
3. THE CHARACTERISTIC FIELD
In order to simplify the application of BM25F to XML element retrieval, we propose a framework with only two fields for each element. The body field contains the element body, and the characteristic field contains any contextual or background information that characterizes the element. The precise contents of the characteristic field may vary from element to element. While this approach is similar to that of Robertson et al. [11] and Lu et al. [8] it avoids the complexity of multiple field types and allows a consistent approach to be applied across heterogenous elements.
4. EXPERIMENTS AND RESULTS
Ad Hoc Retrieval. We first report the results of runs on INEX 2009 ad hoc task. We trained on a 5.9GB INEX 2008 Wikipedia corpus [3] with 659,387 articles and 70 assessed topics and tested on a 50.7GB INEX 2009 Wikipedia corpus [14] with 2,666,190 articles and 68 assessed topics. Our training optimized the official metric of iP[0.01]. For these runs, we used a characteristic field formed from the titles of the article and the sections in which an element occurs.
Table 1 shows the official INEX results. The BM25F run that ranked first gives a 6.62% improvement over the BM25 run that ranked 12th.
Book Page Retrieval . We used INEX 2008 Book Track data [6] of 50239 books of size 37GB after pre-processing. Only 25 out of 70 topics had relevance judgements, thus we used 17 of them for training, and 8 for testing. The corpus comes with a file, machine readable cataloging (MARC) format [10], that contains information such as book category and library of congress classification (LCC) code.
The Book track task required to group the pages by the books and rank the books. Thus all of our runs did so and ranked the books by the highest scoring page returned for the book. Training maximized mean average precision.
Table 2 shows the results of our experiments. The runs with the plus signs indicate information used in the characteristic field. We see that using characteristic information gives up to 48.92% and 35.45% improvement over BM25 during training and testing respectively.

Table 2: Book Page Retrieval

Run

MAP (training) MAP (test)

BM25

0.0278

0.0110

BM25F+title

0.0412

0.0149

BM25F+title+cat

0.0413

0.0139

BM25F+title+cat+LCC

0.0414

0.0137

5. CONCLUSIONS
We propose a framework for applying BM25F to XML element retrieval through the addition of a single characteristic field. This characteristic field merges contextual information from multiple sources, which may include inherited titles and metadata. The proposal is inspired by previous work, but aims to avoid the complexity of multiple fields and heterogenous structure by merging contextual information into this single field.
The proposal is evaluated in the context of the INEX effort. While our results are preliminary, and the results of the INEX 2009 book track have not yet been fully judged, they suggest that the benefits of field weights may be obtainable even in this simplified framework.
Future work includes experimenting our version of BM25F on INEX heterogeneous track collection and taking advantage of more detailed structural information available in the new INEX 2009 Wikipedia collection.
6. REFERENCES
[1] Initiative for the Evaluation of XML retrieval. www.inex.otago.ac.nz.
[2] N. Craswell, H. Zaragoza, and S. Robertson. Microsoft Cambridge at TREC 14: Enterprise track. In Proceedings of the TREC 14, 2005.
[3] L. Denoyer and P. Gallinari. The Wikipedia XML corpus. SIGIR Forum, 40(1):64­69, 2006.
[4] N. Fuhr, J. Kamps, M. Lalmas, S. Malik, and A. Trotman. Overview of the INEX 2007 Ad Hoc Track. INEX 2007, 4862:1­23, 2007.
[5] J. Kamps, S. Geva, A. Trotman, A. Woodley, and M. Koolen. Overview of the INEX 2008 Ad Hoc Track. INEX 2008, 5631:1­28, 2009.
[6] G. Kazai, A. Doucet, and M. Landoni. Overview of the INEX 2008 Book Track. In INEX 2008, pages 106­123, 2009.
[7] G. Kazai and N. Milic-Frayling. Effects of social approval votes on search performance. In Proceedings of ITNG 2009, pages 1554­1559, 2009.
[8] W. Lu, S. Robertson, and A. MacFarlane. Field-weighted XML retrieval based on BM25. In INEX 2005, pages 161­171, 2006.
[9] S. Malik, M. Lalmas, and N. Fuhr. Overview of INEX 2004. LNCS, 3493:1­15, 2005.
[10] L. of Congress Network Development and M. S. Office. MARC standards. www.loc.gov/marc/.
[11] S. Robertson, W. Lu, and A. MacFarlane. XML-structured documents: Retrievable units and inheritance. In Proceedings of FQAS 2006, pages 121­132, 2006.
[12] S. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7: Automatic ad hoc, filtering, vlc and interactive track. Proceedings of TREC-7, 1998.
[13] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In Proceedings of CIKM 2004, pages 42­49, 2004.
[14] R. Schenkel, F. Suchanek, and G. Kasneci. YAWN: A semantically annotated Wikipedia XML corpus. In 12. GI-Fachtagung fu¨r Datenbanksysteme in Business, Technologie und Web, pages 277­291, 2007.
[15] A. Trotman. Choosing document structure weights. Inf. Process. Manage., 41(2):243­264, 2005.
[16] H. Wu, G. Kazai, and M. Taylor. Book search experiments: Investigating IR methods for the indexing and retrieval of books. LNCS, 4956:234, 2008.

844

Learning to Select Rankers
Niranjan Balasubramanian and James Allan
Department of Computer Science University of Massachusetts Amherst 140 Governors Drive, Amherst, MA 01003, USA
niranjan@cs.umass.edu, allan@cs.umass.edu

ABSTRACT
Combining evidence from multiple retrieval models has been widely studied in the context of of distributed search, metasearch and rank fusion. Much of the prior work has focused on combining retrieval scores (or the rankings) assigned by different retrieval models or ranking algorithms. In this work, we focus on the problem of choosing between retrieval models using performance estimation. We propose modeling the differences in retrieval performance directly by using rank-time features ­ features that are available to the ranking algorithms ­ and the retrieval scores assigned by the ranking algorithms. Our experimental results show that when choosing between two rankers, our approach yields significant improvements over the best individual ranker.
Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation, Theory
Keywords: Combining Searches, Learning to Rank, Metasearch
1. INTRODUCTION
Combining evidence from multiple sources has been studied in various contexts [2, 1, 4, 6]. The basic premise for combining evidence from multiple retrieval models is that there is no single model that performs the best on all queries. Several rank fusion [7] and rank aggregation [4] approaches have been proposed to re-rank documents based on retrieval scores (or rankings) obtained from individual rankers. However, most of these approaches either learn a fixed (query independent) set of weights that are used to combine document scores or utilize a voting scheme for combining the rankings.
Instead of learning to combine document scores in a query dependent manner, we consider the problem of selecting a ranker for a given query. We propose a simple framework that directly predicts the differences in effectiveness between the results of different retrieval models. In particular, we consider the web-search scenario, where a large number of features are often combined using sophisticated learning to rank algorithms (rankers). For the sake of simplicity, we assume that we have access to two different rankers that operate on the same set of features. We formally define the ranker selection problem as follows:
Problem Definition. Given two rankers, Ra and Rb, we choose one ranker to be the baseline ranker (say Rb) ­ either arbitrarily, or based on the prior knowledge about the average performance of the
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

rankers. Then, for each query, the selection problem is to determine whether to use the baseline ranker Rb or the alternate ranker Ra.
This problem definition and the selection framework that we propose can be extended to the scenario where we have access to multiple alternate rankers.
Ranker Selection Framework. We propose a simple framework for directly predicting the difference between the performance of two rankers(Rb and Ra) in terms of average precision (AP). We use the retrieval scores and the features of the top ranked documents (referred as retrieval features henceforth) to train a regressor. In addition to being closely related to the performance of the rankers, these features are also easy to compute, compared to typical performance prediction measures such as Clarity [3].
As shown in Figure 1, for a given test query, we first rank documents using both rankers. Then, for each ranked list, we compute mean and variance of the scores and the standard deviation of the retrieval features to generate aggregate feature vectors. The difference between the two aggregate vectors is input to the regressor which predicts the difference in effectiveness (AP(Ra) - AP(Rb)). If the predicted difference is positive, then we select the alternate ranker, otherwise, we use the baseline ranker.

Ra DDocoucRmuemesnuetlntst
Rb DDocoucRmuemesnuetlntst

Doc 1 f1,f2,...,fn
...

Score 1 ...

Doc k f1,f2,...,fn

Score k

Doc 1 f1,f2,...,fn
...

Score 1 ...

Doc k f1,f2,...,fn

Score k

Aggregator Aggregator

Ra Features
a11,a12,...,a1n ...
al1,al2,...,aln
Ra Features
b11,b12,...,b1n ...
bl1,bl2,...,bln

Regressor
AP(Ra) >
AP(Rb) ?

Figure 1: Ranker Selection Process

2. EXPERIMENTS
To evaluate our ranker selection approach, we use the LETOR 3.0 dataset [8] built on top of the TREC Gov2 collection. We conduct 5-fold cross validation experiments on a set of 225 queries created for the TREC named page finding tasks (NP 2003 and NP 2004). We use the results from three ranker baselines: RankBoost [5], Regression, and FRank [9]. To create features for the selection framework, we use the published test runs1 for these rankers to obtain the document scores for top 10 ranking documents, and the list of 64 features that are available as part of LETOR. We use a non-linear Random Forest regression model for our experiments. We compare the rankers using mean-average precision (MAP).
1http://research.microsoft.com/enus/um/beijing/projects/letor/letor3baseline.aspx

855

In terms of MAP, RankBoost is the best individual ranker, followed by FRank and Regression. Table 1 shows the potential for the use of query dependent ranker selection for named page finding. For example, RankBoost outperforms Regression on 90 queries, but performs worse on nearly half as many. Furthermore, we see that an oracle selection method can provide nearly 30% improvement over Regression, and nearly 15% improvement over FRank and 12% improvement over RankBoost.

Table 1: Potential for Improvement using Ranker Selection. Rb ­ Baseline ranker (RankBoost), Ra ­ Alternate ranker. Worse and Better indicate the number of queries for which Ra is worse than Rb in terms of MAP and vice versa. RankBoost has a MAP of 0.6596

Ra

MAP(Ra) Worse Better Oracle Selection

Regression 0.5476

90

42

0.7096

FRank

0.6429

62

45

0.7316

We conduct two sets of selection experiments one with RankBoost as the baseline ranker, and the other with RankBoost as the alternate ranker. Even though the rankers train on differences in performance, the distribution of the positive and negative differences change for each setting, thereby leading to different behavior in terms of the achieved improvements.

Table 2: Ranker Selection effectiveness on a set of 225 name page finding queries on the Gov2 collection. Rb ­ Baseline ranker, Ra ­ Alternate ranker. M AP (Rs) indicates the MAP achieved with ranker selection. Underline indicates best MAP. * indicates significant improvements over Regression/FRank when using a paired t-test with p < 0.05

Rb Regression FRank
RankBoost RankBoost

Ra RankBoost RankBoost
Regression FRank

MAP(Rb) 0.5476 0.6429
0.6596 0.6596

MAP(Ra) 0.6596 0.6596
0.5476 0.6429

MAP(Rs) 0.6623 0.6591
0.6722 0.6607

Results of the two selection experiments are tabulated in Table 2. When using RankBoost as the alternate ranker, selection yields improvements over both Regression and FRank. This is in part because RankBoost performs better than both these algorithms for most queries. However, selection does not provide substantial improvements over RankBoost, the best individual ranker. On the other hand, when using RankBoost as the baseline ranker and Regression as the alternate ranker, we obtain substantial improvements using selection. Interestingly, even though FRank has a higher MAP compared to Regression, using FRank as the alternate ranker yields smaller improvements. This suggests that effectiveness of the selection also depends on the type of ranking algorithm used, in addition to the performance of the ranker itself.
The distribution of gains achieved for ranker selection between RankBoost and Regression is shown in Figures 2 (a) and (b). In both cases, we see that for a large fraction of the queries, choosing the alternate ranker results in gains, and very few cases result in losses. When using RankBoost as the baseline ranker, selection uses Regression for a small number of queries (28), and provides gains for subset (14), but the choice results in fewer losses (4). However, when using RankBoost as the alternate ranker, selection uses RankBoost for a large number of queries (198), out of which 83 queries result in gains and 29 result in losses. This suggests that while ranker selection yields substantial gains, it can also benefit from limiting losses due to poor selection. For example, thresholding on the predicted differences can reduce the number of queries for which the alternate ranker is queried.

Difference in AP

-0.6

-0.2 0.0 0.2 0.4 0.6

(a) Rb: RankBoost

(b) Rb: Regression

Figure 2: Distribution of Ranker Selection Gains: (a) When using RankBoost as the baseline and (b) When using RankBoost as the alternate ranker

3. CONCLUSIONS
In this paper, we proposed a simple learning approach for querydependent selection of rankers. Our selection framework utilizes rank-time features ­ features that are available to the ranking algorithms during ranking. For selecting between two rankers, our experimental results show that a simple regression model that directly predicts differences in effectiveness, can achieve substantial improvements over the best individual ranker. As part of future work, we plan to investigate selection between multiple rankers using more sophisticated features for performance prediction.

4. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsor.

5. REFERENCES
[1] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In SIGIR '94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 173­181, 1994.
[2] W. B. Croft. Incorporating different search models into one document retrieval system. SIGIR Forum, 16(1):40­45, 1981.
[3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR '02: 25th Annual ACM SIGIR Conference Proceedings, pages 299­306, 2002.
[4] M. Farah and D. Vanderpooten. An outranking approach for rank aggregation in information retrieval. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 591­598, 2007.
[5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4, 2003.
[6] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In SIGIR '95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 180­188, 1995.
[7] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 139­146, 2006.
[8] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pages 3­10, 2007.
[9] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. Ma. Frank: a ranking method with fidelity loss. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 383­390, 2007.

Difference in AP

-1.0 -0.5

0.0

0.5

1.0

856

VisualSum: An Interactive Multi-Document Summarization System Using Visualization

Yi Zhang, Dingding Wang, Tao Li
School of Computing and Information Sciences Florida International University Miami, FL 33199
{yzhan004, dwang003, taoli}@cs.fiu.edu

ABSTRACT
Given a collection of documents, most of existing multidocument summarization methods automatically generate a static summary for all the users. However, different users may have different opinions on the documents, thus there is a necessity for improving users' interactions in the summarization process. In this paper, we propose an interactive document summarization system using information visualization techniques.
Categories and Subject Descriptors: H.3.3[Information Storage and Retrieval]: Information Search and Retrieval; H.5.2[User Interfaces]: Interaction styles.
General Terms: Algorithms, Experimentation, Performance
Keywords: Multi-Document Summarization, visualization
1. INTRODUCTION
With huge volume of text resources on the Internet, document summarization has been receiving a lot of attentions. Existing document summarization methods usually involve natural language processing and machine learning techniques. However, most of these methods exclude human from the summarization process, which is efficient in terms of reducing users' workload, but is not desired since the generated summaries are identical for all the users, contradicting to the subjective nature of summarization [6].
To address the issue that people with diverse interests may expect dynamic summaries based on their own preference, we develop VisualSum, an interactive visualized document summarization system, to help users select their preferred sentences to form the summaries.
The summarization process of VisualSum is performed in an iterative manner as illustrated in Figure 1. It starts with all the sentences in the documents, and stops when a satisfactory summary is obtained by a user. Each sentence selection iteration includes three steps as follows. Step (1): The system generates a 2-D view graph of current sentences, in which each node represents a sentence, and the location and color of the sentence are determined by the layout and clustering algorithms respectively. Step (2): The user selects a sentence based on the visualization results in Step (1). Step (3): The system removes the sentence clusters of the selected sentences from the current sentence set.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: The diagram of user interactive summarization

Experiments and a user study demonstrate the effectiveness of the VisualSum system.

2. METHODOLOGY
In this section, we introduce the components of VisualSum including sentence graph representation, layout and clustering algorithms, and user interaction function.

2.1 Sentence Graph Representation
Given a collection of documents, we first decompose them into sentences. An undirected graph G = (V, E) is then constructed to represent the relationships among the sentences, where V is the vertex set and E is the edge set. Each vertex in V is a sentence, and each edge in E is associated with the cosine similarity between two sentences (vertices). Two vertices are connected if their cosine similarity is greater than 0.

2.2 Linlog Layout Algorithm

Here, we use Linlog, a popular energy-based layout algo-

rithm[7], to display the sentence relationships. The energy

function in Linlog is



E(p) =

({u,v}pu - pv - dvdulnpu - pv)

{u,v}:u=v

Where {u,v} is the weight of the edge connecting vertices u and v, and du and dv are the degrees of u and v respectively. The optimal positions p of all the vertices are obtained by
minimizing E.

2.3 Clustering with Maximum Modularity

The node (sentence) positions displayed by the energybased layout algorithm are consistent with the clustering results obtained by maximizing graph modularity [2, 7]. Modularity can be defined as

[ wc cC wC

d(c)2 - ( d(C)2 )]

where wc, wC are the sum of edge weights in cluster c and

857

Figure 2: An example visualization and summarization by VisualSum

cluster set C respectively, and d(c) and d(C) are the sum of node degrees for all the nodes in cluster c and cluster set C.
The clustering results can be easily obtained by a bottomup algorithm, in which each sentence is treated as a singleton cluster at the beginning and then successively merge pairs of clusters until the maximum modularity is reached.
2.4 User Interaction
Now we show how VisualSum assists users to interactively select sentences to create summaries. The visualization in VisualSum clearly illustrates the following information for users. (1) Each node is a sentence and the color of the node indicates the cluster it belongs to. (2) The radius of each node is determined by the sentence's degree. The larger the node, the more important the corresponding sentence. (3) Important sentences in the largest cluster are labeled by their sentence IDs and recommended to users as candidates. (4) Large nodes in the overlapping area of two clusters may be the transition sentences between the clusters. (5) The larger the distance between two clusters, the dissimilar the two topics.
Since the visualization process clearly shows the relationships among the sentences, users can easily select the important sentences they are interested in to form the summary. Figure 2 shows an example of the visualization and sentence selection procedure.
3. EXPERIMENTS
3.1 Automatic Summarization
First of all, we examine the summarization performance of VisualSum using DUC 2006 dataset. Since the DUC evaluation is not personalized, we select the largest sentence node in the largest cluster at each iteration, until the required length of summaries is reached. Table 1 shows the evaluation results using ROUGE toolkit [5] (intuitively, the higher the scores, the better the performance). We compare VisualSum with four widely used baseline summarizers. From Table 1, we observe that the summarization performance of VisualSum outperforms LeadBase and Random and is comparable with NMF and LSA. Note that the motivation of VisualSum is not to build an automatic summarizer, but to

Systems VisualSum LeadBase [1]
Random NMF [4] LSA [3]

R-1 0.332 0.320 0.317 0.324 0.331

R-2 0.055 0.052 0.049 0.055 0.050

R-L 0.308 0.297 0.294 0.300 0.305

R-W 0.113 0.110 0.108 0.113 0.112

R-SU 0.107 0.104 0.101 0.106 0.102

Table 1: Summarization performance comparison.

help users to create their desired summaries using visualization. Thus in this experiment, we just demonstrate the comparable performance of VisualSum for automatic document summarization.
3.2 User Study
To better evaluate the summarization results of VisualSum, we conduct a user survey. The subjects of the survey are fifteen students at different levels and from various majors at Florida International university. Each participant randomly selects a set of news documents, and uses VisualSum to form a summary. Then they are asked to assign a score of 1 (the least satisfaction) to 10 (the highest satisfaction), according to their satisfaction of the use of VisualSum. The average scores of VisualSum and the baseline summarizers are 8.07, 7.5 respectively, which demonstrate the effectiveness of VisualSum.
Acknowledgements: The work is partially supported by NSF grants IIS-0546280 and DMS-0915110.

4. REFERENCES
[1] http://www-nlpir.nist.gov/projects/duc/pubs.html.
[2] G. Agarwal and D. Kempe. Modularity-maximizing graph communities via mathematical programming. The European Physical Journal B, 66(3):409­418, November 2008.
[3] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. SIGIR, 2001.
[4] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. NIPS, 2001.
[5] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. Association for Computational Linguistics, pages 71 ­ 78, 2003.
[6] S. Liu, M. X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian. Interactive, topic-based visual text summarization and analysislysis. CIKM, 2009.
[7] A. Noack. Modularity clustering is force-direced layout. Physical Review E, 79:026102, 2009.

858

Retrieval System Evaluation: Automatic Evaluation versus Incomplete Judgments

Claudia Hauff
University of Twente Enschede, The Netherlands
c.hauff@ewi.utwente.nl

Franciska de Jong
University of Twente Enschede, The Netherlands
f.m.g.dejong@ewi.utwente.nl

ABSTRACT
In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced (i.e. incomplete) amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of systems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particular case of TREC's Million Query track, we show that the automatic evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Experimentation, Performance Keywords: Automatic System Evaluation
1. INTRODUCTION
In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The two most important approaches in the first category are the determination of good documents to assess (the MTC approach) [6] and the proposal of alternative pooling methods (the statAP approach) [4]. Both, MTC and statAP, are now accepted system evaluation metrics at TREC1. They stand in contrast to the depth pooling methodology which has until recently been employed at TREC; due to the ever increasing size of test collections and query sets though, pooling the top 100 documents of each retrieval run participating in a benchmark and assessing those documents manually for their relevance, has become infeasible. The earliest method for a fully automatic evaluation was proposed by Soboroff et al.
1http://trec.nist.gov/
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

(the RS approach) [7]. It relies on drawing random samples from the pool of top retrieved documents.
The quality of statAP, MTC and RS is usually evaluated by comparing the performances of a set of retrieval runs for which sufficient relevance judgments are available according to a standard effectiveness metric (mean average precision) with the estimated system performances. Generally missing though is a direct comparison between statAP /MTC and an automatic method such as RS.
In recent work [3], we found the commonly reported problem of automatic evaluation approaches (the severe misranking of the very best retrieval runs [5]) not to be inherent to automatic system evaluation methods. The extent of this problem is strongly related to the degree of human intervention in the best retrieval runs: the larger the amount of human intervention, the less able automatic approaches are to identify the best runs correctly.
In this poster, we turn to investigating how closely the automatic evaluation of retrieval runs approximates the evaluation with incomplete manual relevance assessments. We perform this analysis in a setting which favors automatic evaluation: TREC's Million Query tracks of 2007 (MQ2007) [2] and 2008 (MQ-2008) [1]. Due to the size of the query sets, creating retrieval runs with a great amount of human intervention is virtually impossible. We thus expect the RS approach to lead to similar estimates of system performances as statAP and MTC respectively. If this would indeed be the case, it would bring into question the need for manual assessments in this type of setting.
2. EXPERIMENTS
For our experiments, we relied on the twenty-nine retrieval runs submitted to MQ-2007 and the twenty-four2 runs submitted to MQ-2008. Both sets of retrieval runs as well as their retrieval effectiveness scores according to statAP and MTC are available from the TREC website. Specifically, for MQ-2007, TREC provides the statAP measures3, while for MQ-2008 both, MTC and statAP, are provided. Of the 10000 queries that were released for each year, 1153 (MQ2007) and 564 (MQ-2008) queries respectively have valid statAP measurements; 784 (MQ-2008) queries have valid MTC measurements. These are the queries we also rely on in the RS approach.
2In total, twenty-five runs exist, though one is not accessible from the TREC website and thus had to be ignored. 3The MTC measures are not accessible from the TREC website.

863

Pool Depth p Avg. Sampled Documents

MQ-2007 statAP

10

4.6

50

22.2

100

43.0

250

102.6

MQ-2008 statAP

10

6.0

50

28.8

100

55.7

250

132.4

MQ-2008 MTC

10

6.1

50

29.4

100

56.8

250

135.0

Kendall's Tau
0.803 0.783 0.754 0.719
0.768 0.812 0.833 0.841
0.722 0.759 0.773 0.780

Table 1: Kendall's Tau rank correlation coefficient between the automatic RS approach and statAP/MTC respectively. All correlations are significant (p < 0.01). Column 2 contains the average number of sampled documents from the pool.

For the automatic evaluation, we implemented the random sampling approach [7]: first, the top p retrieved documents of all retrieval runs for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of documents are drawn at random from the pool; those are now considered to be the pseudo relevant documents. This process is performed for each query and the subsequent evaluation of each system is performed with pseudo relevance judgments instead of relevance judgments. Due to the randomness of the sampling, we performed 20 trials per query and averaged the pseudo relevance based system performance. We fixed the number m of documents to sample 5% of the number of unique documents in the pool and evaluated pool depths of p = {10, 50, 100, 250}.
In Table 1 (column 3) we report the rank correlation coefficient Kendall's Tau ( ) between the performance scores estimated by the automatic RS approach and the performance scores estimated by statAP /MTC which exploit manual relevance assessments. In the ideal case,  = 1.0, that is, RS leads to the same rank estimate of system performances as statAP /MTC. It is apparent, that although the correlations are not perfect, the correlation coefficients are consistently high; in the worst instance the correlation reaches  = 0.72 for MQ-2007 statAP and a pool depth of p = 250; at best the correlation reaches  = 0.84 for MQ-2008 statAP and p = 250.
Figures 1 and 2 show scatter plots of MQ-2007 statAP scores versus RS scores and of MQ-2008 MTC scores versus RS scores respectively. It is evident that the best retrieval runs as identified by statAP /MTC are also identified correctly by the automatic RS approach.
3. DISCUSSION AND CONCLUSION
In this poster, we investigated the ability of an automatic system evaluation approach (RS [7]) to approximate the system performance estimates as derived by two evaluation methods that rely on manually derived incomplete relevance judgments: statAP and MTC. Experiments on TREC's Mil-

random sampling

0.18 0.16 0.14 0.12
0.1 0.08 0.06 0.04 0.02
0

=0.803

0.05

0.1

0.15

0.2

0.25

0.3

statAP

Figure 1: MQ-2007 statAP scores (x-axis) versus RS scores (y-axis) for a pool depth of p = 10.

random sampling

0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

=0.780

0.02

0.04

0.06

0.08

0.1

MTC

Figure 2: MQ-2008 MTC scores (x-axis) versus RS scores (y-axis) for a pool depth of p = 250.

lion Query tracks showed that RS is highly correlated to statAP and MTC, an outcome which implies that retrieval runs, which are automatic in nature, can be evaluated by an automatic approach such as RS which requires no manual assessments at all.
One direction of future work will be the adaptation of RS to further improve the method's correlation with statAP and MTC by for instance taking advantage of the relationship between queries of a query set (as is possible for larger sets of queries) in contrast to the current approach where each query is viewed in isolation.
4. REFERENCES
[1] J. Allan, J. A. Aslam, V. Pavlu, E. Kanoulas, and B. Carterette. Million Query Track 2008 Overview. In TREC 2008, 2008.
[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million Query Track 2007 Overview. In TREC 2007, 2007.
[3] C. Hauff, D. Hiemstra, L. Azzopardi, and F. de Jong. A Case for Automatic System Evaluation. In ECIR '10, pages 153­165, 2010.
[4] J. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR '06, pages 541­548, 2006.
[5] J. A. Aslam and R. Savell. On the effectiveness of evaluating retrieval systems in the absence of relevance judgments. In SIGIR '03, pages 361­362, 2003.
[6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06, pages 268­275, 2006.
[7] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR '01, pages 66­73, 2001.

864

Aspect Presence Verification Conditional on Other
Aspects
Dmitri Roussinov
University of Strathclyde 16 Richmond Street, Glasgow, UK G1 1XQ
+44 141 548 3706
dmitri.roussinov@cis.strath.ac.uk

ABSTRACT
I have shown that the presence of difficult query aspects that are revealed only implicitly (e.g. exploration, opposition, achievements, cooperation, risks) can be improved by taking advantage of the known presence of other, easier to verify query aspects. The approach proceeds by mining a large external corpus and results in substantial improvements in re-ranking the subset of the top retrieved documents.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models; ­ Retrieval Models
General Terms
Algorithms, Experimentation, Theory.
Keywords
Information retrieval, machine learning, external corpus.
1. INTRODUCTION
It has been noticed that a common reason for the search results to be of poor quality is missing one or more aspects of the user information need [1], where an aspect can be represented by a subset of query words. E.g., in the query antarctica exploration, the word antarctica is relatively rare and specific. Along with its same-stem variants, it explicitly occurs in virtually all documents about Antarctica. On the other side, the word exploration, capturing the other aspect of the query, is not only more frequent, but also represents a higher level concept (theme, topic, etc.), often revealing itself only implicitly by such statements like "... scientists began to collect data..." The ranking of the retrieved documents by currently state-of-the-art algorithms would be primarily determined by antarctica, mildly affected by the explicit occurrence of the word exploration, and not affected by the implicit presence of the exploration theme. Although techniques aimed at detecting and representing missing aspects have been suggested [2][3] they have not been yet methodologically studied or revealed convincing improvements. I suggest that it was because they were essentially limited to the bag-of-words representations and the query expansion models. On the contrary, this research builds on the prior works studying the prediction of occurrence of given words (concepts) in a given text context (e.g. a sentence) such as [5][7]. Specifically, I proceeded by the exploring the following innovations: 1) Going beyond the bag of words by looking for the indicators of implicit aspect presence among all the sequences of words
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

(up to a certain length) from the document (e.g. polar station, oil drilling proposed, expedition ships, etc.) rather than relying on a finite number of expansion terms. 2) Considering the problem of aspect verification conditional on the presence of other, often easier to establish aspects. E.g., in a document about Antarctica, station is a good indication of the exploration theme, while not necessary so in unrestricted context. 3) Modeling subsumption of indicators, e.g. if the word station is a part of train station then its connection to the exploration theme is weaker.
2. FRAMEWORK
Here, I considered the simplest, but a common situation with the two aspects in a query, each can be specified by one or more keywords: 1) Ap is already known to be present in the document d and 2) the other aspect Am, explicit representation of which is missing in the document, thus simply referred below as the "missing" aspect. While Am is typically a difficult aspect to verify, as I have demonstrated here, the task of estimating the presence of Am conditional on the presence of Ap happens to be easier. As the possible indicators of the aspect implicit presence, I considered all the sequences of words (up to length of 3) in a document. For each indicator i, the algorithm estimated P(Am|i,Ap) , the probability of the occurrence of Am within the proximity (e.g. same sentence) of i, conditional on the occurrence of Ap. In order to do that, a regression (M5P model tree [6]) was trained independently for each topic, using the normalized frequency counts obtained from Microsoft's Bing search engine as independent variables. For example, the high ratio #((station NEAR exploration) AND Antarctica)/#(station AND A ntarctica) would indicate the high probability of occurrence of exploration near the word station in the external corpus, conditional on the presence of Antarctica. The snippets returned by Bing for the query representing both aspects (e.g. antarctica NEAR exploration) served as training examples, thus no parameter tuning was necessary. The advantage of using M5P was learning automatically the reliability thresholds for the frequency counts. The subsumption step of the algorithm discarded all the subsequences (e.g. station) within the word sequences (e.g. train station) for which the reliable estimates of the conditional probabilities were obtained. This resulted in the average of 531 preserved indicators per document, with 90% of the strongest 100 indicators having 2 or more words, some of those presented in Table 2 below. To rank the documents, the sum of all the conditional probability estimates over all the remaining indicators was treated as the total number of "implied" occurrences of the missing aspect:

865

tf m =

P ( Am | i , Ap ) , and the bm25 formula was

i d

applied.

3. EMPIRICAL EVALUATION

B1

B2

Entire Wikipedi

Web

a only

MAP

0.39

0.43

0.65**

.53*

Standard

0.13

0.11

0.16

.14

Deviation

% change -9% n/a

+51%

+23%

over B2

Table 1. The results of the evaluation. Statistically significant differences at the levels of 0.05 and .1 are marked with ** and * accordingly.

Since the approaches studied here involved a large number of time-consuming external queries, I built a small but "clean" data set that was sensitive enough to evaluate various configurations and the overall verification accuracy of the techniques suggested here based on the top ranked documents retrieved by bm25 ranking function (described as B1 below) using the HARD 2005 TREC topics, which are known to be difficult and often result in missing aspects [1]. I chose only those topics in which 1) It was possible to interpret the title as consisting of missing and present aspects. 2) The present aspect was occurring in at least 90% of the top 20 documents. 3) The missing aspect was not explicitly occurring in more than half of the top 20 documents. This left me with 18 topics, some examples of them listed in Table 2. Only up to 20 top ranked irrelevant and relevant documents were selected, and only those that did not have the missing aspect mentioned explicitly. I also removed (approx. 10% of) the documents assessed by TREC as irrelevant but still having both aspects present in order to reduce the sensitivity of the evaluation to the details supplied only in the narratives of the topics and thus not available to neither the baseline nor the suggested algorithms. The resulting data set was almost "balanced" with approximately 15 negative and 10 positive examples on average per each topic. I compared the performance against two strong baselines: B1 was obtained by applying bm25 ranking formula using the topic titles only, with

stemming and pseudo-relevance feedback using Lemur retrieval engine default settings. Since the approach here involves an external corpus, to make a fair comparison, I also involved B2 obtained and optimized the same way as in [4], which was essentially an application of a relevance model with an external corpus (here, using Bing's snippets).
4. CONCLUSIONS
As the results in Table 1 indicate, it was possible to significantly improve verifying the presence of the difficult implicit aspects by the techniques suggested here. Also, no specific topic has been harmed as a result. While the actual impact on a larger set of topics remains to be seen, I have nevertheless been able to handle a very common scenario with the implicit presence of a difficult aspect (e.g. exploration) and the explicit presence of an easier aspect (e.g. Antarctica). Although the dataset involved in experiments here was very small, it still represented the top ranked results from the state of the art techniques, and thus being able to improve them by re-ranking has great practical implications. The ablation studies (not reported here due to the space limitations) confirmed that each of the novelties 1-4 stated in the introduction was crucial. While sending queries to a search portal delayed the processing substantially, in order to achieve the real time speeds, future implementations can make use of the processed large corpus data such as Google's 1T or a faster access to a search engine index.
5. REFERENCES
[1] Buckley, C. Why current IR engines fail. SIGIR 2004. [2] Collins-Thompson, K., Callan, J. Query expansion using
random walk models. CIKM 2005. [3] Crabtree, D.W., Andreae, P. Gao, X. Exploiting
underrepresented query aspects for automatic query expansion. SIGKDD 2007. [4] Diaz, F. and Metzler, D. Improving the estimation of relevance models using large external corpora. SIGIR 2006. [5] Edmonds, P. Choosing the word most typical in context using a lexical co-occurrence network, ACL 1997. [6] Monz, C. Model Tree Learning for Query Term Weighting in Question Answering. Advances in Information Retrieval, Volume 4425/2007, pp. 589-596. [7] SzeWang F., Roussinov, D., Skillicorn, D.B. Detecting Word Substitutions in Text. IEEE TKDE, 2008.

Table 2. Top indicators of implicit aspect presence for some topics. The missing aspects are underlined. Bold font highlights the

indicators that are specific to the present aspect.

Topic

Strongest Indicators of Presence

Transportation train caught fire, people were killed, caused by sparks, the midst of, described the crash, in spite

Tunnel

of, millions of dollars, quickly as possible, billions of dollars, around the corner, was caused by,

Disasters

turned into a, the worst, rescue, emergency, explosion, coal, killed, fire

Black Bear

were forced to, bear attempted to, fire, the bear was, officials say, reduced, kill, defense,

Attacks

occurred in, officials said, carcass, attract, documented, bear was sighted, dangerous, killed

by, ripped, threatening

Iran Iraq

Iran will ship, minister to visit, delegation will visit, positive, visit to Iraq, oil exports, Iranian

Cooperation delegation, normalizing, relations, the joint, agreement with Iran, mutual, to bilateral, visit to Iran,

accepted an invitation, invitation to visit, normalization, war ended in, the withdrawal of, gesture

Journalist Risks journalist was shot, press freedom, murdered, circumstances, in prison, Iraq, Russia, body, killer,

gunned down, win, posed, detained

866

The Value of Visual Elements in Web Search

Marilyn Ostergren, Seung-yon Yu, Efthimis N. Efthimiadis
The Information School University of Washington, Box 352840
Seattle, WA 98195-2840 {ostergrn, syyu, efthimis}@uw.edu

ABSTRACT
We used eye-tracking equipment to observe 36 participants as they performed three search tasks using three graphicallyenhanced web search interfaces (Kartoo, SearchMe and Viewzi). In this poster we describe findings of the study focusing on how the presentation of SERP results influences how the user scans and attends to the results, and the user satisfaction with these search engines.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process H.5.2 [Information interfaces and presentation]: User Interfaces. - Graphical user interfaces.
General Terms
Design, Human Factors.
Keywords
Search Engine Results Page Display (SERP), Eye-tracking study, Search Engine Evaluation, User Study
1. INTRODUCTION
The results of a web search are generally presented as a collection of web-page surrogates. Each surrogate conveys information that

can be used to support the decision `should I follow this link?' These surrogates, along with other information on the page may also support the decision to reformulate the search query ­ either because the results don't seem relevant, or because they trigger ideas that alter the searcher's target or conceptualization of the information need [4].
Our study investigates how searchers interact with graphical, nontextual search engine results page user interfaces (SERP UIs) to reveal the potential value of these alternative display strategies. We study whether the unique characteristics of these displays facilitate the work of scanning the page for the clues that support the decision to follow a link or reformulate a query.
2. REVIEW OF RESEARCH
Others have also used eye-tracking to gain an understanding of what happens during search. A major finding of this work is that, when results are presented as a ranked list, users direct most of their attention to results near the top of the list [2,5]. This is true even if the list is manipulated so that more relevant results appear lower on the list [5], and even if the eye-tracking data shows that the viewer looked at those more relevant results [2]. Cutrell & Guan [1] found that this bias toward earlier results can also be affected by the content of the surrogates ­ in this case, by the length of the text snippet.

Figure 1: Screenshots of ViewZi, SearchMe, and KartOO

Figure 2: Hot spots for ViewZi, SearchMe, and KartOO

Copyright is held by the author/owner(s).
SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Some other observations revealed by eye-tracking that are relevant to our work are that placement and proximity of image search results affects a viewers gaze path [6], and that an organized display with an explicit hierarchy reinforced with

867

headings and indentations facilitates a more efficient visual search [3].
3. METHODOLOGY
To gather evidence for how graphical qualities of a SERP support or hinder a user, we collected and analyzed two sources of data: 1. self-report (both audio recordings of verbalizations during search and responses to a questionnaire), and 2. observation (cursor and gaze behavior). We collected this data while users completed a series of search tasks using three graphically-enhanced search interfaces. The interfaces were Kartoo (kartoo.com), SearchMe (searchme.com) and Viewzi (viewzi.com). We chose these three because they display the results using graphical representation rather than the typical text-based ranked list. They also have enough similarities to each other to allow for meaningful comparisons.
3.1 Participants
We recruited 36 participants, all undergraduate students at a major research university. They were divided into two groups: 18 were trained to search the three SE, 18 were untrained.
3.2 Search Tasks
We chose search tasks which require different types of cognitive processing and navigation to elicit a range of search behaviors. Task 1 asked the user to find the schedule of events at a local performance theater. Task 2 asked to find two sites where a specific camera model could be purchased. Task 3 asked the user to find two credible sites describing the side-effects of aspirin.
3.3 Data Collection
Data was collected using questionnaires, the think-aloud process, eye-tracking, transaction logging, and participant observation. We used the Tobii eye-tracking system and ClearView 2.7.1 software to record and analyze the eye-tracking data. After each search, we asked the users to reflect upon how the interface affected their searching. After the entire set of searches, we asked them to compare the interfaces with each other.
4. DISCUSSION
Each of these three SERP interfaces uses an arrangement of page surrogates which differs from the usual top-to-bottom linear display. Viewzi (upper left in Figure ) uses a grid. SearchMe (central image in Figure ) uses a horizontal display in which only one surrogate (the central one) is clearly displayed at a time, while the previous and subsequent surrogates are smaller and displayed as if in a stack set at an angle to the central surrogate. Kartoo (right in Figure ) spreads the surrogates across the page organized by major topic area and resembles a map. The three displays also differ in the content of each surrogate. All three include a thumbnail image of the site though they vary substantially in size (Figure ). All three include some text ­ a partial URL and keywords in Kartoo, a slightly-abbreviated Google-derived snippet in Viewzi, and a snippet plus access to the full page (using a magnifying window) in SearchMe. We can easily see differences in scanning behavior across the three interfaces. This is evident in the heat-map images in Figure which show the combined gaze data from 18 participants. Our analysis indicates that the Kartoo display elicited the most scanning. Participants generally spent time looking at all 10 surrogates before clicking to go to a page. The SearchMe Display elicited careful analysis of individual surrogates, but inhibited scanning beyond the first few results. Though it is possible to

scan sequentially through the entire result set using the slider control that is located at the bottom of the screen, none of our participants took advantage of this feature choosing instead to analyze each carefully in sequence. The Viewzi display facilitates scanning in a way that is similar to the typical ranked list. The results are displayed from left to right and continue in rows from top to bottom. The most attention is given to the surrogate in the upper-left hand corner with correspondingly less attention to the surrogates to the right and bottom. However we did find that this strong preference for the surrogates near the beginning of the sequence is less pronounced than that reported by Guan & Cutrell with the typical ranked list display [2]. The question remains how much of this scanning behavior was influenced by the arrangement of the surrogates and how much was influenced by the content of the surrogates. The fact that users viewed more surrogates using the Kartoo display appears to be mostly because the surrogates provide fewer textual cues as to the content of the target website. The results of our questionnaires indicate that Kartoo was the least popular interface while SearchMe, which heavily favored the first site in the list and elicited very little browsing, was the most popular. In terms of ease of interaction on a scale of 1-9, where 1 is most difficult, 5 is average, and 9 is least difficult, 30% found Kartoo most difficult (1-2), 70% found SearchMe least difficult (7-9), and 42% found Viewzi average.
5. CONCLUDING REMARKS
The major insight from our initial analysis is that the visual, nonlinear qualities of these SERP displays strongly influence the user interaction, in particular, the number and sequence in which the surrogates are explored. Satisfaction with the SERP displays is correlated more closely with the textual content of the surrogates. Our more detailed analysis will look at what elements of the surrogate were examined (e.g. the URL, the text snippet, the screenshot) and compare these results with a similar data for a standard text-based search result display. Familiarity to SE and knowledge about the functionality of the SE affected user satisfaction. Training how to search using the visual search engines enhanced the user's search effectiveness, e.g., less number of query reformulations, more efficient search by using search features, and better user satisfaction. When asked if they intend to use the visual search engine again, more trained users answered positively than non-trained users. The most popular search engine that people want to use again was SearchMe (72%), followed by Viewzi (36%), and then Kartoo (33%).
6. REFERENCES
[1] E. Cutrell & Z. Guan. What are you looking for?: An eyetracking study of information usage in Web search. CHI '07. ACM, New York, NY, (2007), 407-416.
[2] Guan, Z., & Cutrell, E. An eye tracking study of the effect of target rank on Web search. In CHI 2007, ACM (2007).
[3] Hornof, A.J., & Halverson, T. Cognitive strategies and eye movements for searching hierarchical computer displays. In Proceedings of CHI 2003, ACM Press (2003), 249-256.
[4] Johnson, F C. User interactions with results summaries. In Proceedings of the ACM SIGIR 2007 workshop on web information seeking and interaction (2007), 131-134.
[5] Lorigo, L., et al. 2008. Eye tracking and online search: Lessons learned and challenges ahead. J. Am. Soc. Inf. Sci. Technol. 59, 7 (May. 2008), 1041-1052.
[6] Tseng, Y.C., Howes, A. The Adaptation of Visual Search Strategy to Expected Information Gain. CHI 2008, 1075-84.

868

Diversification of Search Results using Webgraphs
Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu
Department of Computer and Information Sciences University of Delaware
Newark, DE, USA 19716

ABSTRACT
A set of words is often insufficient to express a user's information need. In order to account for various information needs associated with a query, diversification seems to be a reasonable strategy. By diversifying the result set, we increase the probability of results being relevant to the user's information needs when the given query is ambiguous. A diverse result set must contain a set of documents that cover various subtopics for a given query. We propose a graph based method which exploits the link structure of the web to return a ranked list that provides complete coverage for a query. Our method not only provides diversity to the results set, but also avoids excessive redundancy. Moreover, the probability of relevance of a document is conditioned on the documents that appear before it in the result list. We show the effectiveness of our method by comparing it with a query-likelihood model as the baseline.
Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage and Retrieval]
General Terms: Algorithms
Keywords: information retrieval, diversity, webgraphs.
1. INTRODUCTION
Users express information needs using a set of keywords. Current retrieval systems fail to capture the different information needs that could be expressed by users using the same set of keywords. Clearly this leads to multiple interpretations for a given query. For example, consider the query kcs. There are multiple interpretations for this query, one being the Kansas City Southern railroad; another, being Kanawha County Schools in West Virginia; one more interpretation is information on KCS Energy, Inc.
In order to maximize the user experience it appears reasonable to diversify the result set. Diversify means to examine the query with a broader perspective and account for the multiple information needs for the query. This diversification would provide complete coverage of subtopics for a given query to the user. Ranking with diversity requires moving away from the assumption that documents are independently relevant to the query. Each document must be ranked based not just on its similarity to the query but also based on the documents retrieved before it.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Our task is same as the "diversity" task of the TREC Web Track [4]; the goal of the system is to return a ranked list of documents that provides complete coverage for a given topic, while avoiding excessive redundancy in the result set. We have used the topics created for this task. These topics consists of a query, a description of an information need, and one or more subtopics or alternative interpretations of the query. These topics were developed from information extracted from the logs of a commercial Web search engine, thereby ensuring a good mix of user needs for a given query.
Most previous work, including the MMR approach of Carbonell & Goldstein [2] and the language modeling framework proposed by Zhai et al. [7], involve a greedy approach to finding subtopics. In this work, we propose a method using the link structure of the web to maximize the subtopics covered for a given query. Our method identifies authoritative documents in a set and assumes that these authoritative documents represent a subtopic. We evaluate our proposed method using the -nDGC measure proposed by Clarke [5] and intent aware precision (P-IA) proposed by Agarwal et al [1] and compare it to a query-likelihood baseline.
2. THE WEBGRAPH METHOD
The link structure has often provided a rich source of information about the content of the environment. Our method uses the information provided by the link structure to find several densely linked collections of hubs and authorities within a subset of the results. Each densely linked collection could potentially cover different subtopics for a given query.
In our approach, we re-rank an initial ranking of documents (query-likelihood results) to provide a diverse ranking of documents. The documents in this initial ranking consisting of hyperlinked pages are represented as a directed graph G = (V, E): nodes corresponds to pages and a directed edge (p, q)  E correspond to the presence of link from page p to q. We expand the subgraph to include all the in-links to the subgraph and out-links from the subgraph. The hubs and authorities scores are calculated for each document using the iterative procedure described by Kleinberg [6].
Kleinberg's procedure begins by representing the directed graph as an adjacency matrix. The principal and non-principal eigenvectors are calculated from this matrix multiplied by its transpose. Each value in an eigenvector represents a document score. The values in the principal eigenvector correspond to the Kleinberg's hub score for a document. The nonprincipal eigenvectors represent other densely-linked clusters in the graph; they have both positive and negative entries, but we consider only the positive entries.

869

No. of Eigenvectors
5 10 25 50 100 0 (baseline)

-nDCG10
0.100 0.154 0.143 0.169 0.142 0.124

P-IA10
0.038 0.050 0.051 0.057 0.047 0.061

Table 1: Diversity results for varied number of eigenvectors and 50 terms.

For each eigenvector we construct a language model using the documents corresponding to the k greatest values. Therefore, the m language models constructed from the documents correspond to the k greatest values in each of the first m eigenvectors. The intuition is that the link structure clusters the documents into subtopics, therefore these language models provide a hypothetical set of subtopic models. The language model corresponding to each subtopic is evaluated against the query and then we take the document with the greatest score. This produces a set of documents (possibly fewer than m) which are the highest scoring for the hypothesized subtopics that are then ranked in decreasing order of the original query-likelihood scores. We iterate in this way, taking the highest-scoring set of documents remaining, until we rank the top 200 documents in the original ranking. This method of iterating to obtain the final ranking is similar to the one described by Cartertte et al [3].

3. IMPLEMENTATION AND RESULTS
In our experiments, we used the ClueWeb09 dataset consisting of one billion web pages (5 TB compressed, 25 TB uncompressed), in ten languages, crawled in January and February 2009. We indexed the smaller set of "Category B" which consists of 50 million web pages in English. We used the webgraphs in the dataset which has about 428,136,613 unique URLs and 454,075,638 outlinks. This test collection was used for the diversity task at TREC'09. A total of 50 queries were evaluated and the subtopics for each query ranged from 3 to 8. We used the Lemur Toolkit and the Indri search engine in our experiment. The query-likelihood result set with Dirichlet smoothing (µ = 2000) was used as our baseline results for reranking.
Our method was evaluated using the two measures which reward novelty and diversity, namely -normalized discounted cumulative gain (-nDCG) and intent-aware precision (PIA). All our methods were evaluated at rank 10 with  = 0.5 in -nDCG. To see whether the setting of parameters such as m (the number of eigenvectors) and n (number of terms) may affect the performance, we compare the results for a range of values.
By comparing the results of the two parameters in Figure 1 we see that in general the performance increases and reaches a maximum at 50 eigenvectors and starts to decrease again. The number of terms in the model has less effect on the results. We report the diversity results by varying the number of eigenvectors along the Indri baseline model in Table 1. This table shows that our method did considerably well in diversifying the results set for all parameter values according to the -nDCG measure although for the P-IA measure the results were below the baseline.

alpha-nDCG Values

0.2 0.18 0.16 0.14 0.12 0.1 0.08 0.06
5

n = 5 n = 10 n = 25 n = 50 n = 100 baseline

10

25

50

100

Number of Eigenvectors

Figure 1: -nDCG averaged over 50 queries with increasing numbers of eigenvectors (subtopic models) and terms in each model.

4. CONCLUSIONS AND FUTURE WORK
In this work, we have proposed a novel method for diversifying search results. The webgraph method produces a diverse ranking from an initial set of documents for a given query by considering the underlying link structure of the retrieved documents. We believe more information can be harnessed from the hyperlink structure of retrieved documents; our work provides enough evidence for future work along these lines.
5. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of WSDM '09, pages 5­14, 2009.
[2] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR '98, pages 335­336, 1998.
[3] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceeding of CIKM '09, pages 1287­1296, 2009.
[4] C. L. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2009 web track. In Proceedings of TREC, 2009.
[5] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR '08, pages 659­666, 2008.
[6] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604­632, 1999.
[7] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR '03, pages 10­17, 2003.

870

Capturing Page Freshness for Web Search
Na Dai and Brian D. Davison
Department of Computer Science & Engineering Lehigh University
Bethlehem, PA 18015 USA
{nad207,davison}@cse.lehigh.edu

ABSTRACT
Freshness has been increasingly realized by commercial search engines as an important criteria for measuring the quality of search results. However, most information retrieval methods focus on the relevance of page content to given queries without considering the recency issue. In this work, we mine page freshness from web user maintenance activities and incorporate this feature into web search. We first quantify how fresh the web is over time from two distinct perspectives--the page itself and its in-linked pages--and then exploit a temporal correlation between two types of freshness measures to quantify the confidence of page freshness. Results demonstrate page freshness can be better quantified when combining with temporal freshness correlation. Experiments on a realworld archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance
Keywords: temporal correlation, web freshness, web search
1 Introduction
Web search engines exploit a variety of evidence in ranking web pages to satisfy users' information needs as expressed by the submitted queries. These information needs may contain distinct implicit demands, such as relevance and diversity. Recency is another such need, and so is utilized as an important criteria in the measurement of search quality. However, most information retrieval methods only match queries based on lexical similarity. Link-based ranking algorithms such as PageRank [1] typically favor old pages since the authority scores are estimated based on a static web structure and old pages have more time to attract in-links.
To overcome this problem, we quantify page freshness from web activities over time. We observe that pages and links may have diverse update activity distributions from inception to deletion time points. We infer that pages having similar activity distributions with their in-links suggest that such page activities have stronger influence on their parents' activities.
Motivated by the above analysis, in this work we incorporate a temporal freshness correlation (TFC) component in quantifying page freshness, and show that by using TFC, we can achieve a good estimate of how up-to-date the page tends to be, which is helpful to improve search quality in terms of both result freshness and rel-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Link activity
1 creation of link l : q  p 2 update on link l : q  p (changed anchor) 3 update on link l : q  p (unchanged anch.) 4 removal of link l : q  p
Page activity
1 creation of page q 2 update on page q 3 removal of page q

Infl. on p's InF

Infl. on q's PF


Gain of p's InF
3 2 1.5 -0.5 Gain of q's PF 3 1.5 -0.5

Table 1: Activities on pages and links and their influence on web freshness. (The link l points from page q to page p. : positive influence on web freshness. : negative influence on web freshness. The number of  or
indicates the magnitude.)

evance. We consider the effects of other aspects of freshness on retrieval quality elsewhere [4].

2 Page Freshness Estimation

We start by quantifying web freshness over time. We assign every
page two types of freshness: (1) page freshness (PF) inferred from
the activities on the page itself; and (2) in-link freshness (InF) in-
ferred from the activities of in-links. Table 1 lists the detailed web activities and their contributions1 to page and in-link freshness. To
simplify analysis, we break the time axis into discrete time points (t0, t1, . . . , ti, . . .) with a unit time interval t = ti - ti-1, where i > 0. It is reasonable to assume that any activities that occur in [ti-1, ti] can be considered as occurring at ti, especially when t is small. We assume that the influence of activity decays exponentially over time. Therefore, we estimate PF and InF at ti by aggregating the web activities with such a decay, written as:

X ti

X

P Fti (p) =

e(i-j)t

wk Ctj ,k (p)

tj =1

kP A

X ti

XX

InFti (p) =

e(i-j)t

wk Ctj ,k (l)

tj =1

l:qp kLA

where wk and wk are contributions associated with each type of page and link activities, and Ctj,k(p) is the number of the kth type of page activity on page p at tj , and Ctj,k(l) is the number of the kth type of page activity on link l at tj, and P A and LA are
the page and link activity sets. In this way, we estimate web page
freshness at multiple predefined time points from web activities.

1The sensitivity of activity weights with respect to freshness estimation is omitted due to space limitation.

871

We next quantify the temporal freshness correlation between
pages and their in-links. We exploit the method by Chien and Im-
morlica [3], in which the authors measure query semantic similarity
by using temporal correlation. Given a page p, its page and in-link freshness are denoted as (P Ftc (p), P Ftc+1 (p), . . . , P Ftr (p)) and (InFtc (p), InFtc+1 (p), . . . , InFtr (p)) covering p's life span. The temporal freshness correlation (TFC) between page p and its
in-links is given by:

T F C(p)

=

1 n

X tr " P Ft(p) - P F (p) "" InFt(p) - InF (p) "

t=tc

P F (p)

InF (p)

where P F (p) and InF (p) are the standard deviations of P F (p) and InF (p), respectively.
Once we calculate the temporal freshness correlation for every page (tr - tc  2t), we next combine it with page freshness score by ranks. Given a time point of interest ti, the combined page freshness rank of document d is written as:

Rankcombined(d) = (1 - )RankP Fti (d) + RankT F C (d)

where

=

a-1 n-1+a-1

,

and

n is the total number

of time points, and

a is the number of time points on which p exists. As a increases,

T F C(d) is more stable, and therefore we emphasize its contribu-

tion in the combined page freshness estimation.

3 Experimental Results and Discussion

Our goal is to improve web search quality on both relevance and freshness. To test the effect of combined page freshness on web search, we use an archival corpus of the .ie domain provided by the Internet Archive [5], covering from Jan. 2000 to Dec. 2007, and extract page and link activities. To minimize the influence of transient pages, we remove pages with fewer than 5 archival snapshots. The remaining sub-collection (with 3.8M unique URLs and 908M temporal links) is used for ranking evaluation.
We choose April 2007 as our time point of interest. 90 queries are selected from popular queries in Google Trends2 for evaluation. For each query, we have an average of 84.6 URLs labeled by at least one worker of Amazon Mechanical Turk3. Editors give judgments on each document with respect to a given query for both relevance and freshness. Relevance is judged from "highly relevant" (4) to "not related" (0). Freshness is judged from "very fresh" (4) to "very stale" (0). The document with an average score above 2.5 is marked as relevant/fresh.
To evaluate the effectiveness of the combined page freshness, we compare with PageRank, running on a single web snapshot of April 2007. The global ranking lists generated by the combined page freshness and PageRank scores are linearly combined with Okapi BM2500 [6] (baseline) by ranks individually. The parameters are the same as Cai et al. [2]. Precision@k and NDCG@k are used as metrics for ranking evaluation on both relevance and freshness. All methods are compared based on their best rank combination of query-specific scores and global scores on metric Precision@10 of relevance. The decay parameter  is set to 1 in this work.
Table 2 lists the ranking performance comparison varying the time span involved in the combined page freshness computation. For relevance, except for NDCG@3, the correlation between ranking performance and the time span is not consistent. Unlike relevance, freshness performance consistently improves with the increase of time span used in the combined page freshness computation. This suggests temporal freshness correlation calculated from

2http://www.google.com/trends 3http://www.mturk.com

NDCG@3 NDCG@3

Method Okapi BM2500
PageRank 200601-200704 200401-200704 200201-200704 200001-200704
Method Okapi BM2500
PageRank 200601-200704 200401-200704 200201-200704 200001-200704

P@10 0.4695 0.4894 0.5021 0.4893 0.5002 0.4986
P@10 0.3138 0.3325 0.3288 0.3342 0.3361 0.3374

Relevance NDCG@3
0.2478 0.2589
0.2917 0.3027 0.3081 0.3115 Freshness NDCG@3 0.2137 0.1946
0.2315 0.2329 0.2416 0.2477

NDCG@5 0.2740 0.2840 0.3152 0.3201 0.3157 0.3211
NDCG@5 0.2379 0.2345 0.2490 0.2552 0.2565 0.2617

NDCG@10 0.3344 0.3457 0.3675 0.3657 0.3642 0.3647
NDCG@10 0.2805 0.2838 0.2979 0.2988 0.3027 0.3028

Table 2: Ranking performance comparison. A  means the performance improvement is statistically significant (p-value<0.1) over Okapi BM2500. Performance improvement with p-value<0.05 is marked as .

0.32

combined page freshness

0.315

page freshness

temporal correlation

0.31

0.305

0.3

0.295

0.29

0.285

0.28

0.275 200601-

200501- 200401- 200301- 200201-
time span
(a) relevance

200101-

200001-

0.25

combined page freshness

0.245

page freshness

temporal correlation

0.24

0.235

0.23

0.225

0.22

0.215 200601-

200501- 200401- 200301- 200201-
time span
(b) freshness

200101-

200001-

Figure 1: Ranking performance on metric NDCG@3 while varying the time span involved in page freshness calculation.

long-term web freshness measures can benefit more on accurate page freshness estimation. Figure 1 shows the performance on NDCG@3 with the variance of the time span for both relevance and freshness. We observe that (1) the ranking performance of page freshness first decreases, and then keeps nearly constant with the increase of time span, indicating the page activities within the past 1-2 years influence page freshness estimation the most; (2) the ranking performance of temporal freshness correlation shows unstable trends with variance of time span; and (3) the combined page freshness shows promising performance, and demonstrates its superiority over either page freshness or TFC.
Acknowledgments
This work was supported in part by a grant from the National Science Foundation under award IIS-0803605 and an equipment grant from Sun Microsystems. We also thank Anlei Dong for helpful comments on the ranking evaluation criteria issue.
4 References
[1] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. In Proc. of 7th Int'l World Wide Web Conf., pages 107­117, Apr. 1998.
[2] D. Cai, X. He, J. Wen and W. Ma. Block-level link analysis. In Proc. 27th Annual Int'l ACM SIGIR Conf., pages 440­447, Jul, 2004.
[3] S. Chien and N. Immorlica. Semantic similarity between search engine queries using temporal correlation. In Proc. 14th Int'l World Wide Web Conf., pages 2­11, 2005.
[4] N. Dai and B. D. Davison. Freshness Matters: In Flowers, Food, and Web Authority. In Proc. of 33rd Annual Int'l ACM SIGIR Conf., Jul, 2010.
[5] The Internet Archive, 2010. http://www.archive.org/. [6] S. E. Robertson. Overview of the OKAPI projects. Journal of Documentation,
53:3­7, 1997.

872

S-PLSA+: Adaptive Sentiment Analysis with Application to Sales Performance Prediction

Yang Liu§,, Xiaohui Yu,§, Xiangji Huang, and Aijun An
§School of Computer Science and Technology, Shandong University Jinan, Shandong, China, 250101
York University, Toronto, ON, Canada, M3J 1P3
yliu@sdu.edu.cn,xhyu@yorku.ca,jhuang@yorku.ca,aan@cse.yorku.ca

ABSTRACT
Analyzing the large volume of online reviews would produce useful knowledge that could be of economic values to vendors and other interested parties. In particular, the sentiments expressed in the online reviews have been shown to be strongly correlated with the sales performance of products. In this paper, we present an adaptive sentiment analysis model called S-PLSA+, which aims to capture the hidden sentiment factors in the reviews with the capability to be incrementally updated as more data become available. We show how S-PLSA+can be applied to sales performance prediction using an ARSA model developed in previous literature. A case study is conducted in the movie domain, and results from preliminary experiments confirm the effectiveness of the proposed model.
Categories and Subject Descriptors
H.4.0 [Information Systems Applications]: General
General Terms
Algorithm, Experiment
Keywords
sentiment analysis, review mining, prediction
1. INTRODUCTION
Online reviews present a wealth of information on products and services, and if properly utilized, can provide vendors highly valuable intelligence to facilitate the improvement of their business. As such, a growing number of recent studies have focused on the economic values of reviews, exploring the relationship between the sales performance of products and their reviews [3, 2, 4]. Gruhl et al. [3] show that the volume of relevant postings can help predict the sales rank of books on Amazon, especially the spikes in sales ranks. Ghose et al. [2] also demonstrate that subjectivity of reviews can have an impact on sales performance.
Liu et al. [4] propose a probability model called Sentiment PLSA (S-PLSA for short) based on the assumption that sentiment consists of multiple hidden aspects. They develop a model called ARSA (which stands for Auto-Regressive Sentiment-Aware) to quantitatively measure the relationship between sentiment aspects and reviews. Our experience with running ARSA on several online review datasets reveals that the model is highly sensitive to the sentiment factors, which are constantly changing over time as new reviews become available. It is therefore essential to allow the SPLSA model to adapt to newly available review data.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

To this end, we take a Bayesian approach, and propose an adaptive version of the S-PLSA model that is equipped with the incremental learning capability for continuously updating the model using newly observed reviews. The proposed model is motivated by the principle of quasi-Bayesian (QB) estimation, which has found successful applications in various domains such as adaptive speech recognition and text retrieval [1]. We call the proposed model the S-PLSA+model, in which the parameters are estimated by maximizing an approximate posterior distribution. One salient feature of our modeling is the judicious use of hyperparameters, which can be recursively updated in order to obtain up-to-date posterior distribution and to estimate new model parameters. This modeling approach makes it possible to efficiently update the model parameters in an incremental manner without the need to re-train the model from scratch each time as new reviews become available.

2. S-PLSA

In the S-PLSA model [4], a review can be considered as being

generated under the influence of a number of hidden sentiment fac-

tors. The use of hidden factors provides the model the ability to

accommodate the intricate nature of sentiments, with each hidden

factor focusing on one specific aspect. What differentiates S-PLSA

from conventional PLSA is its use of a set of appraisal words [4]

as the basis for feature representation. The rationale is that those

appraisal words, such as "good" or "terrible", are more indicative

of the review's sentiments than other words.

For a given set of N reviews D = {d1, . . . , dN }, and the set

of M appraisal words W = {w1, . . . , wM }, the S-PLSA model

dictates that the joint probability of observed pair (di, wj) is gen-

erated by P (di, wj) = P (di)

K k=1

P (wj |zk)P (zk|di),

where

zk  Z = {z1, . . . , zK } corresponds to the latent sentiment fac-

tor, and where we assume that di and wj are independent condi-

tioned on the mixture of associated sentiment factor zk. The set of

parameters  of this model consist of {P (wj|zk), P (zk|di)}, the

maximum likelihood estimates of which can be obtained through

an expectation-maximization (EM) algorithm [4].

3. ADAPTIVE S-PLSA
The S-PLSA model can be trained in a batch manner on a collection of reviews, and then be applied to analyze others. In many cases, however, the reviews are continuously becoming available, with the sentiment factors constantly changing. We thus hope to adapt the model to the newly obtained reviews, in order to make it more suitable to the changing contexts. A naïve way to perform the adaptation is to re-train the model from scratch using all data available including the newly obtained data, which is clearly highly inefficient. Here, we propose a model called S-PLSA+, which performs incremental learning based on the principle of quasi-Bayesian

873

(QB) estimation. The basic idea is to perform updating and down-

dating at the same time by (i) incrementally accumulating statistics

on the training data, and (ii) fading out the out-of-date data. Let Dn be the set of reviews made available at epoch n (e.g.,
the reviews published on a certain day, but the time unit used can be set to be finer or coarser based on the need), and denote by n = {D1, . . . , Dn} the set of reviews obtained up to epoch n. QB S-PLSA estimates at epoch n are determined by maximizing the posterior probability using n:

(n) = arg max P (|n) = arg max P (Dn|)P (|n-1)





In order to allow closed-form recursive update of , we use the

closest tractable prior density g(|(n-1)) with sufficient statistics

to approximate the posterior density P (|n-1), where n-1 is

evolved from review sets n-1. This leads to (n)  arg max P (Dn|)g(|(n-1)). Note that at epoch n, only the new reviews

Dn and the current statistics (n-1) are used to update the S-PLSA+

parameters, and the set of reviews Dn are discarded after new parameter values (n) are obtained, which results in significant sav-

ings in computational resources.

The particular choice of the prior g(|) in our model is the

Dirichlet density, which can be expressed by


KM


N

g(|) =

P (wj |zk)j,k-1

P (zk|di)k,i-1

k=1 j=1

i=1

where  = {j,k, k,i} are the hyperparameters of the Dirichlet
distribution. Assuming for the moment that (n-1) is known, we can show that (n) can be obtained through an EM algorithm [1].
A major benefit of S-PLSA+ lies in its ability to continuously update the hyperparameters. We can show that the new hyperparameters are given by

|Dn |

(jn,k) =

c(d(in), wj(n))P (n)(zk |d(in), wj(n)) + (jn,k-1)

(1)

i=1

M

k(n,i) =

c(d(in), wj(n))P (n)(zk |d(in), wj(n)) + k(n,i-1).

(2)

j=1

where the posterior P (n)(zk|di(n), wj(n)) is computed using Dn and the current parameters (n), and c(di(n), wj(n)) denotes the number of (d(in), wj(n)) pairs.
To summarize, S-PLSA+works as follows. In the startup phase, initial estimates of the hyperparameters (0) are obtained. Then, at each learning epoch n, (i) new estimates of the parameters (n)
are computed based on the newly available data Dn and hyperpa-
rameters obtained from epoch n - 1; and (ii) new estimates of the hyperparameters (n) are obtained using (1) and (2). This way, the
model is continuously updated when new reviews (Dn) become available, and at the same time fades out historical data n-1, with the information contained in n-1 already captured by (n-1).

4. APPLICATION TO SALES PREDICTION

The proposed S-PLSA+model can be employed in a variety of

tasks, e.g., sentiment clustering, sentiment classification, etc. As a

sample application, we plug it into the ARSA model proposed in

[4], which is used to predict sales performance based on reviews

and past sales data. The original ARSA model uses S-PLSA as the

component for capturing sentiment information. With S-PLSA+,

the ARSA model can be formulated as follows:

p

qR

yt =

iyt-1 +

i,j t-i,j + t,

i=1

i=1 j=1

where (i) yt denotes the sales figure at time t after proper preprocessing such as de-seasoning, (ii) p, q, and R are user-chosen

parameters, (iii) i and i,j are coefficents to be estimated using

training data, and (iv) t,j

=

1 |Rt |

dRt p(zj |d), where Rt is

the set of reviews available at time t and p(zj|d) is computed based

on S-PLSA+. It reflects the sentiment "mass" that can be attributed
to factor zj. The ARSA model can be trained using linear least
squares regression. Note that the notion of time (t) in the ARSA model is different from the epoch (n) in S-PLSA+. For example,
sales prediction can be made for each day using ARSA, whereas the model adaptation of S-PLSA+can happen every other day.

5. EXPERIMENTS

Experiments were conducted on an IMDB dataset to evaluate

the effectiveness of the proposed approach by comparing the prediction accuracy of ARSA using S-PLSA+and that of the original

ARSA. The dataset was obtained from the IMDB Website by col-

lecting 28,353 reviews for 20 drama films released in the US from

May 1, 2006 to September 1, 2006, along with their daily gross

box office revenues. Half of the movies are used for batch training.

For the original ARSA, the trained model is then used to make pre-

dictions in the testing data consisting of the other half the movies. For the proposal model, adaptation of the S-PLSA+component is

performed for each movie in the testing set, in four epochs on four

different days v (v = 2, 4, 6, 8) using the review data available up

to day v. The up-to-date model at day v is then used for subsequent

prediction tasks.

We use the mean absolute percentage error (MAPE) to measure

the prediction accuracy:

MAPE

=

1 T

Ti=1(|Predi-Truei|/Truei),

where T is the number of instances in the testing set, and Predi and

Truei are the predicted value and the true value respectively. The

results on the accuracy of the original ARSA and that of the ARSA

using S-PLSA+updated at Epochs 1-4 (v = 2, 4, 6, 8) respectively

are shown in the table below.

Original Epoch 1 Epoch 2 Epoch 3 Epoch 4

0.352

0.295

0.241

0.247

0.240

The accuracy improves as the model is getting updated in the first two epochs, which demonstrates the benefits of having an incremental model to absorb new information; especially in our case, S-PLSA+allows the models to be adapted to the individual movies. The accuracy stays stable from Epoch 2 through Epoch 4, indicating that no significant new information is available from Epoch 2 to Epoch 4.

6. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented an adaptive S-PLSA model that is capable of incrementally updating its parameters and automatically downdating old information when new review data become available. This model has been used in conjunction with the ARSA model for predicting sales performance. Preliminary experimental results show that by allowing the model to be adaptive, we can capture new sentiment factors arising from newly available reviews, which can greatly improve the prediction accuracy. For future work, we plan to study the performance of S-PLSA+in other information retrieval and data mining tasks.

Acknowledgements
This work is supported by NSERC Discovery Grants, an Early Researcher Award of Ontario and an NSFC Grant (No. 60903108).
7. REFERENCES
[1] Jen-Tzung Chien and Meng-Sung Wu. Adaptive bayesian latent semantic analysis. IEEE TASLP, 16(1):198­207, 2008.
[2] Anindya Ghose and Panagiotis G. Ipeirotis. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In ICEC, pages 303­310, 2007.
[3] Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak, and Andrew Tomkins. The predictive power of online chatter. In KDD '05, pages 78­87, 2005.
[4] Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. ARSA: a sentiment-aware model for predicting sales performance using blogs. In SIGIR, pages 607­614, 2007.

874

Supervised Query Modeling Using Wikipedia
Edgar Meij and Maarten de Rijke
ISLA, University of Amsterdam, The Netherlands
{edgar.meij, derijke}@uva.nl

ABSTRACT
We use Wikipedia articles to semantically inform the generation of query models. To this end, we apply supervised machine learning to automatically link queries to Wikipedia articles and sample terms from the linked articles to re-estimate the query model. On a recent large web corpus, we observe substantial gains in terms of both traditional metrics and diversity measures.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval
General Terms
Algorithms, Experimentation, Measurement
Keywords
Machine Learning, Query Modeling, Wikipedia
1. INTRODUCTION
In a web retrieval setting, there is a clear need for precision enhancing methods [5]. For example, the query "the secret garden" (a novel that has been adapted into movies and musicals) is a query that is easily led astray because of the generality of the individual query terms. While some methods address this issue at the document level, e.g., by using anchor texts or some function of the web graph, we are interested in improving the query; a prime example of such an approach is leveraging phrasal or proximity information [8]. Besides degrading the user experience, another significant downside of a lack of precision is its negative impact on the effectiveness of pseudo relevance feedback methods. An example of this phenomenon can be observed for a query such as "indexed annuity" where the richness of the financial domain plus the broad commercial use of the web introduces unrelated terms. To address these issues, we propose a semantically informed manner of representing queries that uses supervised machine learning on Wikipedia. We train an SVM that automatically links queries to Wikipedia articles which are subsequently used to update the query model.
Wikipedia and supervised machine learning have previously been used to select optimal terms to include in the query model [10]. We, however, are interested in selecting those Wikipedia articles which best describe the query and use those to sample terms from. This is similar to the unsupervised manner used, e.g., in the context of retrieving blogs [9]. Such approaches are completely unsupervised in that they only consider a fixed number of pseudo relevant Wikipedia articles. As we will see below, focusing this set using machine learning improves overall retrieval performance.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. QUERY MODELING

We adopt a language modeling for IR framework in which doc-

uments are ranked according to their likelihood of generating the

query:

log P (D|Q)



log P (D)

+

P
tQ

P

(t|Q

)

log

P

(t|D

).

In our experiments we assume a uniform document prior and apply

Bayesian smoothing using a Dirichlet prior (set to the average doc-

ument length) to obtain each document model D. For the query model we use a linear interpolation: P (t|Q) = QP (t|~Q) + (1 - Q)P (t|^Q), where P (t|~Q) indicates the empirical estimate on the initial query and P (t|^Q) an expanded part which we obtain

using the formula below. Note that when we set Q = 1 we obtain

a query-likelihood ranking which will serve as our baseline.

We take relevance model 1 for our estimations of P (t|^Q) [6]:

P (t|^Q)

=

1 |R|

P
DR

P

(t|D)P

(Q|D).

(1)

Here, R indicates a set of (pseudo) relevant documents which we obtain in three ways: (i) on the collection ("normal" pseudo relevance feedback), (ii) on Wikipedia (similar to so-called "external expansion" [4, 9]), and (iii) using automatically linked Wikipedia articles, which are introduced in the next section.

3. LINKING QUERIES TO WIKIPEDIA
To be able to derive query models based on Wikipedia, we first need to link queries to Wikipedia articles. To this end, we follow the approach in [7] which maps queries to DBpedia concepts, without performing any subsequent query modeling as we do in this paper. We take their best performing settings, i.e., SVM with a polynomial kernel using full queries. Instead of using a proprietary dataset, however, we take two ad hoc TREC test collections, i.e., TREC Terabyte 2004­2006 (.GOV2) and TREC Web 2009 (ClueWeb09, Category A).1 In order to classify Wikipedia articles as being relevant to a query, the approach uses manual query-to-article annotations to train an SVM model. For new queries, a retrieval run is performed on Wikipedia which is then classified using the trained model. The output of this step is a binary classification on each Wikipedia article, where the class indicates the relevance status as predicted by the SVM.
Our features include those pertaining to the query, the Wikipedia article, and their combination. See [7] for an extensive description of each feature. Since we are using ad hoc test collections, we do not have session information and omit the history-based features used there. In order to obtain training data, we have asked 4 annotators to manually identify all relevant Wikipedia articles for each query. The average number of Wikipedia articles the annotators identified per query is around 2 for both collections. The average number of articles identified as relevant per query by SVM is slightly different, with 1.6 for TREC Terabyte and 2.7 for TREC
1http://trec.nist.gov/.

875

QL
RM (C) RM (WP)
WP-SVM WP-SVM WP-SVM

Q MAP MRR Recall P10

1 0.2803 0.7121 7874 0.5081

0.5 0.2882 0.6126 7599 0.5068 0.5 0.2680 0.7331 7364 0.5203

0.8 0.2856 0.7108 7902 0.5 0.2769 0.6937 7731 0 0.2284 0.6307 6965

0.5324 0.5176 0.4392

Table 1: Results on the TREC Terabyte 2004­2006 collection.

QL
RM (C) RM (WP)
WP-SVM WP-SVM WP-SVM

Q MAP MRP MPC30 MNDCG
1 0.02583 0.07765 0.08333 0.04443
0.5 0.02523 0.07612 0.07823 0.04107 0.5 0.02320 0.07274 0.07847 0.04359
0.8 0.03371 0.08882 0.11304 0.06188 0.5 0.03635 0.08961 0.13437 0.07529 0 0.02917 0.07403 0.12577 0.06480

Table 2: Results on the TREC Web 2009 collection (using stat measures [2]).

QL
RM (C) RM (WP)
WP-SVM WP-SVM WP-SVM

Q eMAP -NDCG@10 IA-P@10

1 0.03614 0.04200

0.01700

0.5 0.03919 0.03200 0.5 0.03474 0.03900

0.01300 0.01600

0.8 0.04702 0.05700 0.5 0.06364 0.06100 0 0.09418 0.03300

0.03000 0.03500 0.01800

Table 3: Results on the TREC Web 2009 test collection (using expectedMAP (eMAP) and diversity measures [1­3]).
Web 2009. This seems to be due to the differences in queries; the TREC Web queries are shorter and, thus, more prone to ambiguity.
For the TREC Web 2009 query (#48) "wilson antenna," it predicts ROBERT WOODROW WILSON as the only relevant article, classifying articles such as MOUNT WILSON (CALIFORNIA) as not relevant. For the query "the music man" (#42) it identifies the company, song, 1962 film, and 2003 film which indicates the inherent ambiguity of many web queries. The same effect can be observed for the query "disneyland hotel" (#39) with articles TOKYO DISNEYLAND HOTEL, DISNEYLAND HOTEL (CALIFORNIA), and DISNEYLAND HOTEL (PARIS). There are also mistakes however, such as predicting the article FLAME OF RECCA (a Japanese manga series) for the query (#49) "flame designs."
4. RESULTS AND DISCUSSION
To determine whether the automatically identified articles are a useful resource to improve the query model, we compare our approach (WP-SVM) against a query-likelihood (QL) baseline and against Eq. 1 on pseudo relevant documents. In the latter case, we use either the collection (RM (C)) or the top-ranked Wikipedia articles (RM (WP)). For both we use the top 10 retrieved documents. In order to make results comparable, we include the 10 terms with the highest probability in P (t|^Q) for all approaches. We leave the influence of varying these numbers for future work.
To train the SVM model, we split each test collection in a training and test set. For TREC Terabyte 2004­2006, we have 150 topics which are split equally. For TREC Web 2009 we have 50 topics and use five-fold cross validation.

Tables 1 and 2 show the results on TREC Terabyte and Web 2009 respectively (best scores in boldface). For TREC Terabyte, we observe that WP-SVM obtains highest recall and P10. Although pseudo relevance feedback on the collection obtains highest MAP, MRR is relatively low. An example of a topic helped by WP-SVM is "train station security measures" (#711) caused by the suggested article SECURITY ON THE MASS RAPID TRANSIT.
As to TREC Web 2009, performing pseudo relevance feedback on the collection introduces very general terms and thus does not improve overall retrieval effectiveness. Using WP-SVM to estimate the query model, however, introduces focused terms which improves overall performance. These results indicate that supervised query modeling using Wikipedia is helpful for large, noisy collections.
When we evaluate WP-SVM on the TREC Web 2009 collection using the diversity track's measures, cf. Table 3, we arrive at the same picture. Using WP-SVM we obtain an -nDCG@10 score of 0.06100 which would have placed this run in the top-7 of participating systems in that particular track. This finding, in conjunction with the examples provided earlier, indicates that our query modeling approach caters for multiple interpretations of the query since prominent terms from each identified Wikipedia article are included in the query model.
5. CONCLUSIONS
We have presented a query modeling method based on Wikipedia that is aimed at obtaining high-precision representations of the original query. We find limited improvements on a relatively small web collection, only beating state-of-the-art query expansion methods according to some metrics. On a much larger web corpus, we achieve improvements on all metrics, whether precision or recall oriented. When using diversity measures, we observe major improvements, especially when relying exclusively on externally derived contributions to the query model.
Acknowledgements This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802.
6. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM '09, 2009.
[2] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In SIGIR '08, 2008.
[3] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Büttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, 2008.
[4] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In SIGIR '06, 2006.
[5] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05, 2005.
[6] V. Lavrenko and B. W. Croft. Relevance models in information retrieval. In B. W. Croft and J. Lafferty, editors, Language Modeling for Information Retrieval. Kluwer, 2003.
[7] E. J. Meij, M. Bron, B. Huurnink, L. Hollink, and M. de Rijke. Learning semantic query suggestions. In ISWC '09, 2009.
[8] G. Mishne and M. de Rijke. Boosting web retrieval through query operations. In ECIR '05, 2005.
[9] W. Weerkamp, K. Balog, and M. de Rijke. A generative blog post retrieval model that uses query expansion based on external collections. In ACL-ICNLP 2009, 2009.
[10] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on wikipedia. In SIGIR '09, 2009.

876

A Two-Stage Model for Blog Feed Search

Wouter Weerkamp w.weerkamp@uva.nl

Krisztian Balog k.balog@uva.nl

Maarten de Rijke derijke@uva.nl

ISLA, University of Amsterdam, Science Park 107 1098 XG Amsterdam

ABSTRACT
We consider blog feed search: identifying relevant blogs for a given topic. An individual's search behavior often involves a combination of exploratory behavior triggered by salient features of the information objects being examined plus goal-directed in-depth information seeking behavior. We present a two-stage blog feed search model that directly builds on this insight. We first rank blog posts for a given topic, and use their parent blogs as selection of blogs that we rank using a blog-based model.
Categories and Subject Descriptors:
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval
General Terms: Algorithms, Measurement, Performance,
Experimentation
Keywords: Blog feed search, two-stage model
1. INTRODUCTION
We focus on blogs: the unedited, unregulated voice of an individual [5], as published on a web page containing time-stamped entries. The blogosphere has shown a huge increase in volume in recent years, and is now a major source of information online. To allow for end users to follow a blog that regularly covers a given topic, we can provide them with a ranking of blogs that are likely to show a recurring interest in the topic. This task of identifying topically relevant blogs is referred to as blog feed search. Even though the unit of retrieval is blogs, the indexing unit should be blog posts, as this allows for easy incremental indexing, and the use of a single index for both blog feed search and blog post retrieval. Indeed, all current approaches to this task use a post index [1, 2, 4, 6].
We propose a two-stage model to blog feed search. The model exploits the following observation about human strategies for identifying complex information objects such as blogs (or people, for that matter). Prior to in-depth examination of complex information objects, humans display exploratory search behavior triggered by salient features of such objects [3]. This insight gives rise to the following two-stage model for blog feed search. In stage 1, we take individual utterances (i.e., posts) to play the role of "attention triggers" and select an initial sample of blogs based on the most interesting posts given the query, using a post-based approach. Here, we define "interesting" as topically relevant, but more elaborate techniques can also be applied (e.g., credibility, novelty, etc. [8]). Then, in stage 2, we only consider these most interesting blogs, which we then examine more in-depth by considering all their posts to deter-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

mine the likelihood of the topic being a central theme of the blog, using a blog-based approach.
We hypothesize that by pruning the list of posts taken into account in stage 1, and thus focusing on the most interesting utterances, we can achieve improvements over a blog-based model baseline on effectiveness and, as a result of blog selection, also on efficiency. Furthermore, we expect to see a considerable improvement on precision when applying pruning, and more so, when using a lean, title-only document representation in stage 1.

2. RELATED WORK
Previous work in blog feed search centered around two families of approaches: blog-based models and post-based models. Blogbased models use posts to construct a model for each blog. Ranking is done by matching the query against these blog models. Examples of blog-based models include the Blogger model [1] and the Long Document model [2]. The other type of approaches, post-based models, start from a ranking of blog posts and scores of individual posts are aggregated in order to infer a ranking of blogs [1, 2, 4]; these models boil down to estimating the relevance of blog posts and associating these posts to their parent blogs, using some weighted association value. Various ways of aggregating scores [4] and association values [2, 9] have previously been discussed. Both families of approaches have problems: post-based models ignore the recurring interest requirement; just a few relevant posts in a blog can be enough for it to be ranked high, even though other posts are non-relevant. On the other hand, blog-based models can become quite inefficient when applied to large numbers of frequently updated blogs, where each consists of several posts.

3. A TWO-STAGE MODEL
We are working in a language modeling setting and rank documents (blogs, posts) based on their likelihood of being generated from the query. We use Bayes' Theorem to rewrite this probability: P (D|Q) = P (D)P (Q|D)/P (Q). As P (Q) does not influence the ranking, we drop this term; we assume P (D), i.e., the a priori belief of a document D being relevant, to be uniform. This term can therefore be ignored. We are left with P (D|Q)  P (Q|D). In our specific case we rank blogs, and try to estimate the probability P (Q|blog).
In stage 1, we are looking for salient utterances on the topic in blog posts. Therefore, we rank posts for a given query using:

P (Q|post) =

P (t|post)n(t,Q),

(1)

tQ

where P (t|post) = (1-post)P (t|post)+P (t) (i.e., smoothing with the background collection) and n(t, Q) is the number of times

877

term t is present in the query. We use the top N most relevant utterances (posts) to identify the set of possibly interesting blogs:

B = {blog|

P (Q|post)P (post|blog) > 0}, (2)

postN

where P (post|blog) denotes the importance of a post given a blog, which is assumed to be uniform. Note that the summation part in Eq. 2 corresponds to a post-based model for ranking blogs, however, in our approach it is only used for identifying blogs that deserve to be ranked for the topic.
Having identified the set of possibly interesting blogs, we now estimate the probability of each blog  B having generated the query, i.e., displaying a recurring interest in the topic:

P (Q|blog)  P (t|blog)n(t,Q).

(3)

tQ

We represent blogs as a multinomial probability distributions over the vocabulary terms, and infer a blog model blog, such that the probability of a term given the blog model is P (t|blog). To construct such a representation, we first aggregate all terms from posts
of the blog to estimate an empirical model:

P (t|blog) = P (t|post) · P (post|blog).

(4)

posts

Then, this probability P (t|blog) is smoothed using the background collection probability P (t), to arrive at P (t|blog) (smoothing blogs is done analogously to smoothing applied to posts).

4. EXPERIMENTS AND RESULTS
We test whether our two-stage model is capable of effective blog feed search. We perform two series of experiments. First, we investigate the amount of pruning applied, i.e., the value of N in Eq. 2. We consider three settings: no pruning, topic-independent pruning (train on 2007 topics, test on 2008, and vice versa), and topic-dependent pruning (best empirically found values per topic). The "no pruning" condition corresponds to the blog-based model (Blogger model in [1]). Our second set of experiments concerns the representation of posts for stage 1 of our approach. We compare the results of the two-stage model to a blog-based model only, since blog-based models usually outperform post-based models [1].
The blog feed search task ran at TREC 2007 and 2008 [7] and uses the TRECBlog06 corpus. We use the English blog posts, and ignore blogs that only have 1 post. We have a total of 95 topics and relevance judgments, and we only use the title field of the topics. In our experiments, we optimize for MAP. Testing for significance is done using a two-tailed paired t-test; significant differences are indicated using and ( = 0.01), and and ( = 0.05).
Table 1 lists the results on the 2007 and 2008 topics for the blogbased model, and the various settings for stage 1 in our two-stage model. Results show that our model is at least as effective as the blog-based model, while being considerably more efficient: The blog-based model examines all 2.5M blog-post associations, while our two-stage model considers just 1% (23,700) in stage 2. Topic independent pruning results in a slight improvement in effectiveness over the blog-based model, while topic dependent pruning results in significant improvements. The use of a lean document representation, with an average document length of just 12 words, results in very good overall precision scores.

5. CONCLUSIONS
We have proposed a two-stage model for blog feed search. The model only tries to rank bloggers that stand out because of salient

2007 topics

Blog-based model

Two-stage model Representation Pruning

full content full content title-only title-only title-only

1,700 topic-dep. 7,000 topic-dep.

2008 topics Blog-based model

Two-stage model Representation Pruning

full content full content title-only title-only title-only

1,700 topic-dep. 7,000 topic-dep.

MAP 0.3260
0.3348 0.3611 0.3549 0.3577 0.3813
0.2521
0.2551 0.2747 0.2363 0.2368 0.2571

P@5 0.5422
0.5422 0.5689 0.6444 0.6622 0.6889
0.4880
0.4960 0.5080 0.4880 0.4840 0.5080

MRR 0.7193
0.7213 0.7243 0.8476 0.8587 0.8604
0.7447
0.7483 0.7504 0.7524 0.7524 0.7591

Table 1: Results of the various instances of the two-stage model compared to the blog-based model. Significance tested against blog-based model.

posts and then determines whether the topic is a central concern. Experiments show that the two-stage model can improve over a blog-based model. Topic dependent pruning of the post list in stage 1 helps, and we can combine this with a lean document representation to improve early precision even further. Future work is aimed at learning the optimal pruning level per topic.
Acknowledgements This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802.
REFERENCES
[1] K. Balog, M. de Rijke, and W. Weerkamp. Bloggers as experts. In SIGIR 2008, pages 753­754, 2008.
[2] J. Elsas, J. Arguello, J. Callan, and J. Carbonell. Retrieval and feedback models for blog feed search. In SIGIR 2008, 2008.
[3] C. Kuhlthau. Seeking Meaning: A Process Approach to Library and Information Services. Libraries Unlimited, 2003.
[4] C. Macdonald and I. Ounis. Key blog distillation: Ranking aggregates. In CIKM 2008, pages 1043­1052, 2008.
[5] G. Mishne. Applied Text Analytics for Blogs. PhD thesis, University of Amsterdam, 2007.
[6] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM 2008, 2008.
[7] TREC Blog track wiki. http://ir.dcs.gla.ac.uk/wiki/TRECBLOG, 2010.
[8] W. Weerkamp and M. de Rijke. Credibility Improves Topical Blog Post Retrieval. In ACL-08: HLT, pages 923­931, 2008.
[9] W. Weerkamp, K. Balog, and M. de Rijke. Finding key bloggers, one post at a time. In ECAI 2008, pages 318­322, 2008.

878

Machine Learned Ranking of Entity Facets

Roelof van Zwol
Yahoo! Research
roelof@yahoo-inc.com

Lluis Garcia Pueyo
Yahoo! Research
lluis@yahoo-inc.com

Mridul Muralidharan

Börkur Sigurbjörnsson

Yahoo! Research

Yahoo! Research

mridulm@yahoo-inc.com borkur@yahoo-inc.com

ABSTRACT
The research described in this paper forms the backbone of a service that enables the faceted search experience of the Yahoo! search engine. We introduce an approach for a machine learned ranking of entity facets based on user click feedback and features extracted from three different ranking sources. The objective of the learned model is to predict the click-through rate on an entity facet. In an empirical evaluation we compare the performance of gradient boosted decision trees (GBDT) against a linear combination of features on two different click feedback models using the raw click-through rate (CTR), and click over expected clicks (COEC). The results show a significant improvement in ranking performance, in terms of discounted cumulated gain, when ranking entity facets with GBDT trained on the COEC model. Most notably this is true when evaluated against the CTR test set.
Categories and Subject Descriptors
H.3.3 [Information Retrieval]: Information Search and Retrieval; H.3.5 [Information Retrieval]: On-line Information Services
General Terms
Experimentation, Measurement, Performance
Keywords
ranking entity facets, click feedback, GBDT
1. ABOUT RANKING ENTITY FACETS
The major Web search engines are gradually changing the search experience. Most notably this is visible through the introduction of semantic search assistants, the enrichment of the search results shown to the user and other components that try to predict the user intent. Key to enriching the search experience is the wide-scale availability of user-generated content and other knowledge bases such as Wikipedia, the Internet Movie Database (IMDB), GeoPlanetTM, or Freebase to name a few.
The research presented here is part of the faceted search experience of the Yahoo! Web and Image search engines [4].
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Screen captions of entity facets show in the search engine interface.
Figure 1 shows a fragment of the search engine interface depicting the facets bar for the celebrity "Daniel Day-Lewis" and the location "Geneva, Switzerland".
To support the entity facet ranking application of Figure 1, we propose a machine learned ranking of entity facets based on user click feedback. Given an entity of interest, we have collected a large pool of related candidate facets, e.g. related entities. These entity facet pairs have been extracted from various knowledge bases such as Wikipedia, GeoPlanetTMand other sources. Typically this provides us with a few hundred candidate facets for an entity. The task is then to rank the candidate facets related to each entity in our pool based on its relevance.
Facets are ranked using statistical features extracted from three different sources that contain entity information in the context of images: query terms entered by users in query logs, query session information from users in query logs, and tags provided by users annotating their photos in Flickr. Query term information captures entities that co-occur in a single user query, while query sessions provide information about entities that frequently co-occur in a user session. Flickr tags have a good coverage of travel and location related entities, as well as topics of a more general nature, but

879

tend to be less focussed on, for instance, celebrity entities. For every source, we extract different unary, symmetric and asymmetric features such as query frequency, conditional probability, KL divergence, etc. [4]. For the initial launch of the faceted search experience, we constructed a ranking function that is a linear combination of the conditional probabilities extracted from the three ranking sources .
The main contribution of this paper is a machined learned approach for ranking entity facets based on user click feedback. We propose to learn a ranking using the full set of features extracted from the ranking sources that will predict the click-through rate (CTR) on an entity facet [1]. For that purpose we introduce two click models: raw clickthrough rate on the facets, and the click over expected click (COEC), which is claimed to be more robust towards the position-bias on a click as users tend to click more on those results shown high in the ranking [3].
The click-feedback is used as the ground truth for our training, development and test sets. We have experimented with various learners, but for the experiment reported here we limit ourselves to the discussion of the performance using stochastic gradient boosted decision trees (GBDT) [2]. We used least squares regression as our loss function.
Next we collected the user click feedback on the facets over a period of three months, based on which we compute the click-through rate and click over expected click for each entity facet pair that was shown at least 25 times to a user. The latter constraint is to ensure that the CTR and COEC values are stable enough to be used as the labels for our training, development, and test sets. We join the feature set with the CTR and COEC sets, using the entity facet pair as the key. Next we split the collection into training, development and test sets. When splitting, we ensure that an entity can only occur in one of the three collections.
2. EVALUATION
The objective of the experiment is to measure the prediction accuracy based on the click-through rate of an entity facet pair. We first present the setup of the experiment, followed by a discussion of the results of the evaluation.
Ranking strategies. Central in the experiment are the three
ranking strategies: (1) Baseline. A linear combination of the conditional probabilities. (2) GBDTctr. GBDT trained on the CTR click model and (3) GBDTcoec. GBDT trained on the COEC click model.
Test sets. For the experiment we use two test sets both
containing the same 100 entities and their 10+ facets that have not been used for training or parameter tuning. For a fair comparison of the performance between queries we have normalized the CTR and COEC values for each of the facets of the 100 selected entities to be in the range of [0, 1].
Evaluation metrics. To evaluate the performance on the
test collection, we adopt Discounted Cumulative Gain (DCG) as our metric. DCG is an effectiveness measure that is used frequently for information retrieval tasks, and allows for the use of a graded relevance scale.
Results. The overall performance of the baseline and the
two GBDT models is reported in Table 1. For each of the two test sets, CTR and COEC, the mDCG and mnDCG is included. The performance of all strategies, independent of

Table 1: Overall performance.

CTR

COEC

Run

mDCG mnDCG mDCG mnDCG

Ideal

2.375 -

2.594 -

Baseline 1.728 0.709

1.812 0.677

GBDTctr 2.090

0.874

-

-

GBDTcoec 2.343

0.986

2.436 0.930

Figure 2: nDCG comparison of performance of baseline and GBDT on CTR vs COEC test sets.
the test set is good (mnDCG > 0.67). It can be clearly seen that on both the CTR and COEC test sets, the GBDT models outperform the baseline strategy. Using the normalized metric (mnDCG) we see that can better estimate the actual COEC using the GBDTcoec model than predicting the raw CTR with our GBDTctr model. This gives us a first indication that the COEC click model is more effective than the CTR click model when learning to rank entity facets.
Figure 2 plots the nDCG scores at different points in the ranking. This allows for a direct comparison of the different strategies across the two test sets. In addition to the strategies already discussed, we introduce a new variant where we evaluate the performance of the GBDTcoec strategy, e.g. the GBDT model that was trained to optimized the click prediction on the COEC click model, against the CTR test set. As can be seen this gives a near optimal performance over the first ten positions in the ranking, and a perfect prediction of the most important facet for each of the 100 entities in our test set.
3. REFERENCES
[1] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM '08: Proceedings of the international conference on Web search and web data mining, pages 87­94, New York, NY, USA, 2008. ACM.
[2] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2001.
[3] Y. Zhang and R. Jones. Comparing click logs and editorial labels for query rewriting. In Query Log Analysis: Social And Technological Challenges, 2007.
[4] R. van Zwol, B. Sigurbj¨ornsson,et al. Faceted Exploration of Image Search Results. In WWW2010. Raleigh, NC, USA. 2010.

880

User Comments for News Recommendation in Social Media

Jia Wang


Qing Li

Southwestern Univ. of Finance Southwestern Univ. of Finance

and Economics

and Economics

55 Guanghua Cun Road

55 Guanghua Cun Road

Chengdu, China

Chengdu, China

wangjia@2008.swufe.edu.cn liq_t@swufe.edu.cn

Yuanzhu Peter Chen
Memorial Univ. of Newfoundland
St. John's, A1B 3X5 NL, Canada
yzchen@mun.ca

ABSTRACT
Reading and Commenting online news is becoming a common user behavior in social media. Discussion in the form of comments following news postings can be effectively facilitated if the service provider can recommend articles based on not only the original news itself but also the thread of changing comments. This turns the traditional news recommendation to a "discussion moderator" that can intelligently assist online forums. In this work, we present a framework to recommend relevant information in the forum-based social media using user comments. When incorporating user comments, we consider structural and semantic information carried by them. Experiments indicate that our proposed solutions provide an effective recommendation service.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Filtering
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Web is one of the most important vehicles for "social media", e.g. Internet forums, blogs, wikis, and twitters. One form of social media of particular interest here is self-publishing. In selfpublishing, a user can publish an article or post news to share with other users. Other users can read and comment on the posting and these comments can, in turn, be read and commented on. Digg (digg.com) and Yahoo!Buzz (buzz.yahoo.com) are commercial examples of self-publishing. A useful extension of this self-publishing application is to add a recommendation feature to the current discussion thread. That is, based on the original posting and various levels of comments, the system can provide a set of relevant articles, which are expected to be of interest of the active users of the thread.
Here, we explore the problem of news recommendation for dynamic discussion threads. A fundamental challenge in adaptive news recommendation is to account for topic divergence, i.e. the change of gist during the process of discussion. In a forum, the original news is typically followed by other readers' opinions, in
This research is supported by National Natural Science Foundation of China Grant No.60803106.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Multi-relation graph of comments
the form of comments. Concerns and intention of active users may change as the discussion continues. Therefore, news recommendation, if it were only based on the original posting, can not benefit the potentially changing interests of the users. Apparently, there is a need to consider topic evolution in adaptive news recommendation and this requires novel techniques that can help to capture topic evolution precisely to prevent wild topic shifting which returns completely irrelevant news to users. A related problem is content-based information filtering (or recommendation). Most information recommender systems select articles based solely on the contents of the original postings [1] [3] [4].
In this work, we propose a framework of adaptive news recommendation in social media. It has the following contributions. (1) It is the first attempt of incorporating reader comments for adaptive news recommendation. (2) We model the relationship among comments and that relative to the original posting in order to evaluate their overall impact on recommendations.
2. SYSTEM DESIGN
The proposed news recommender first constructs a topic profile for each news posting along with the comments from readers, and uses this profile to retrieve relevant news.
We first model the relationship among comments and that relative to the original posting in order to evaluate their overall impact. In our model, we treat the original posting and the comments each as a text node. This model both considers the content similarity between text nodes and the logic relationship among them. On one hand, the content similarity between two nodes can be measured by any commonly adopted metric, such as cosine similarity and Jaccard coefficient. This metric is taken over every node pair in the discussion thread. On the other hand, the logic relation between nodes takes two forms. First, a comment is always made in response to the original posting or an earlier comment. In graph theoretic terms, the hierarchy can be represented as a tree  = (,  ), where  is the set of all text nodes and  is the edge set. In particular, the original posting is the root and all the comments are ordinary nodes.

881

There is a directed edge    from node  to node , denoted (, ), if the corresponding comment  is made in response to comment (or original posting) . Second, a comment can quote from one or more earlier comments. From this perspective, the hierar-
chy can be modeled using a directed acyclic graph (DAG), denoted  = (, ). There is a directed edge    from node  to node , denoted (, ), if the corresponding comment  quotes from comment (or original posting) . As shown in Figure 1, for either graph  or , we can use a   ×   adjacency matrix, denoted  and , respectively, to record them. Inline with the adjacency matrices, we can also use a   ×   matrix defined on [0, 1] to record the content similarity between nodes and denote it by  . Thus, we can combine these three aspects linearly.
Intuitively, the important comments are those whose topics are
discussed by a large number of other important comments. There-
fore, we propose to apply the PageRank algorithm [2] to rank the
comments as
= /  + (1 - ) × (, ) × ,


where  is the damping factor as in PageRank and this value is recommended to be 0.85,  and  are node indices, and   denotes the number of text nodes in the thread. In addition, (, ) is the normalized weight of comment  referring to  defined as

(,  )

=



, ,

+



,



where , is an entry in the graph adjacency matrix and  is a constant to avoid division by zero.
Once the importance of comments on one news posting is quan-
tified by our model, this information along with the news itself are
fed into a synthesizer to construct a topic profile of this news dis-
cussion thread. The profile is a weight vector of terms to model the language used in the thread. Consider a news posting 0 and its comment sequence {1, 2,    , }. For each term , a compound weight  () is calculated. It is a linear combination of the contribution by the news posting itself, 1(), and that by the comments, 2(). The weight contributed by the news itself, 0, is:

1() = (, 0)/ma x (, 0)

The weight contribution from the comments {1, 2,    , } incorporates not only the language features of these documents but also their importance of leading a discussion in related topics. That is, the contribution of comment score is incorporated into weight calculation of the words in a text node.



2() =

(,



)/max

(

,



)

×



/max



=1

Such a treatment of compounded weight  () is essentially to recognize that readers' impact on selecting relevant news and the difference of their influence strength.
With the topic profile constructed as above, we can use it to select relevant news for recommendations. That is, the retriever returns an order list of news with decreasing relevance to the topic. Our model to differentiate the importance of each comment can be easily incorporated into any good retrieval model. In this work, our retrieval model is derived from [4].

3. EXPERIMENTAL EVALUATION
To gauge how well the proposed recommendation approach performs, we carry out a series of experiments on a synthetic data set

@10

Table 1: Overall performance The Proposed CF Okapi

0.94

0.789 0.827

0.932

0.8 0.833

LM
0.804 0.833

collected from Digg and Reuters news website. We randomly select 20 news articles with corresponding reader comments from Digg website. These news articles with different topics are treated as the original news postings, recommended news are selected from a corpus of articles collected from Reuters news website. This simulates the scenario of recommending relevant news from traditional media to social media readers for their further reading. We compared the proposed approach to three other retrieval approaches as the baseline: one is a simple content filter (CF) which treats news and comments as a single topic profile, the other two are well-known news recommendation methods [1], Okapi and LM.
To observe the impact of readers' concerns on original news posting in social media, we investigate the effect of the three forms of relationship among comments, i.e. content similarity, reply, and quotation. We carry out a series of experiments for this purpose. we find that replies are slightly more effective than quotations and both of these outperform pure content similarity. In other words, the importance of comments can be well evaluated by the logic organization of these comments. We also notice that the incorporation of content similarity decreases the system effectiveness. This may seem to contradict our intuition that the textual information should complement the logic-based models. By further investigating our results, we find that content similarity sometimes misleads the decision on the importance of the comments. Besides, the computation cost of calculating the content similarity matrix  is very high. Therefore, we only apply the structural information to determine the importance of each comment.
We have -tests using  @10 and MAP as performance measures, respectively, and the  values of these tests are all less than 0.05, which means that the results of experiments are statistically significant. We conduct a series of preliminary experiments to find the optimal performance obtained when the topic file word number is 60 and combination coefficient  is 0.7. As shown in Table 1, the overall performance of the proposed approach performed significantly better than the best baseline methods.

4. CONCLUSION
In this work, we present a framework for adaptive news recommendation that incorporates information from the entire discussion thread. This study can be extended in a few interesting ways. For example, we can use this technique to process personal Web blogs and email archives. The technique itself can also be extended by incorporating such information as reader scores on comments, chronological information of comments, and reputation of users. Indeed, its power is yet to be further improved and investigated.

5. REFERENCES
[1] T. Bogers and A. Bosch. Comparing and evaluating information retrieval algorithms for news recommendation. In Proc. of ACM Recommender systems, 2007.
[2] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107­117, 1998.
[3] J.-H. Chiang and Y.-C. Chen. An intelligent news recommender agent for filtering and categorizing large volumes of text corpus. International Journal of Intelligent Systems, 19(3):201­216, 2004.
[4] V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie, D. Jensen, and J. Allan. Language models for financial news recommendation. In Proc. of CIKM, 2000.

882

Incorporating Global Information into Named Entity Recognition Systems using Relational Context

Yuval Merhav

Filipe Mesquita

Denilson Barbosa

Illinois Institute of Technology University of Alberta, Canada University of Alberta, Canada

yuval@ir.iit.edu

mesquita@cs.ualberta.ca denilson@cs.ualberta.ca

Wai Gen Yee

Ophir Frieder

Illinois Institute of Technology

Georgetown University

yee@iit.edu

ophir@cs.georgetown.edu

ABSTRACT
The state-of-the-art in Named Entity Recognition relies on a combination of local features of the text and global knowledge to determine the types of the recognized entities. This is problematic in some cases, resulting in entities being classified as belonging to the wrong type. We show that using global information about the corpus improves the accuracy of type identification. We explore the notion of a global domain frequency that relates relationidentifying terms with pairs of entity types which are used in that relation. We use this to identify entities whose types are not compatible with the terms they co-occur in the text. Our results on a large corpus of social media content allows the identification of mistyped entities with 70% accuracy.
Categories and Subject Descriptors
I.2.7 [Natural Language Processing]: Text analysis
General Terms
Experimentation, Performance
1. INTRODUCTION
Named Entity Recognition (NER) is an important task for many Information Retrieval applications. One sub-task of particular importance is type identification: assigning meaningful types (e.g., Person, Organization, Location, etc.) to the extracted entities. The state-of-the-art NER systems rely on a mix of local information (statistics and results of lexical analysis) about small portions of the corpora and external knowledge (usually obtained through learning on training data) to perform type identification. For example, LBJ [3] analyzes the corpus in fixed-sized text windows ignoring document boundaries and relies on two sources of external information: high-precision lists of named entities (gazetteers) and clusters of commonly used words in different contexts. While the use of external information has been shown to improve accuracy over purely local methods, they are limited to the knowledge contained in a small collection of documents or tokens for every entity type assignment decision they make. Inevitably, these methods eventually assign incorrect types to the entities they extract. As an example, consider the snippet "[MISC Jewish] by birth, [ORG Alamo] married [PER Edith Opal] who was also [MISC Jewish]" which is tagged with LBJ. As one can see, Alamo is correctly identified
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

as an entity, but assigned type ORG (for Organization) instead of PER (for Person). As Alamo appears in LBJ's gazetteers as an organization, it is likely that this is the reason LBJ labeled Alamo incorrectly.
To overcome the limitation, we propose a scoring feature based on global information (extracted from the entire corpus) for improving the assignment accuracy of entity types. The feature is designed to be used as a supplement in different NER systems and other tasks. This work is motivated by our social network extractor system SONEX [2], that extracts latent social networks from social media text (in [2] we report results on the ICWSM'09 Spinn3r Blog dataset with 44 millions posts [1]). SONEX works by extracting named entities with LBJ, and individual sentences with LingPipe1. Then it identifies relations that associate pairs of entities (e.g., Alamo and Edith Opal) by clustering those sentences using a variety of features. In the example above, SONEX identifies that Alamo and Edith are married to each other, and thus assigns this term to the pair. In this work, we show how to automatically use the results of this relation extraction in SONEX to identify entities which are incorrectly typed (Alamo in the example).

2. DOMAIN FREQUENCY
It is natural to expect that the relations extracted in SONEX are strongly correlated with a given context. For instance, marriage is a relation between Persons, and thus, belongs to the domain PER-PER. We exploit this observation to identify mistyped entities. Starting from the social network extracted by SONEX, which we call the dataset in the sequel, we proceed as follows. For each relation-identifying term in the dataset (excluding stop words), we compute its relative frequency in every possible domain. We refer to the frequency of a term t in a given domain i as the term's Domain Frequency (DF) score, and refer to it as dfi (t).
We normalize the domain frequency of every term across all domains associated with the term. More precisely, let t be a term and let i1, . . . , in be all possible domains of pairs of entity types; let fi(t), . . . , fn(t) be the frequencies in which that term identifies a relation of a pair of entities in each of the domains. Then,

dfi (t) =

fi(t)

.

1jn fj (t)

The number of possible domains is the square of the number of types the NER system identifies. LBJ offers the following types: PER (Person), ORG (Organization), LOC (Location), and MISC

1http://alias-i.com/lingpipe

883

PER - PER ORG - ORG ORG - PER

0.8409 0.0681 0.0227

Table 1: Top-3 Domain Frequencies for "married" in the Spinn3r social network extracted with SONEX.

(miscellaneous)2. Table 1 shows the most significant relative DF scores for the term married across different domains. As expected, the DF score for the PER to PER domain is significantly larger than all other domains. If a term does not appear in a certain domain, its DF score for the domain is zero (e.g., "married" does not appear in the LOC to LOC domain, and hence, it does not appear in Table 1). The entire list of terms associated with their DF scores is available by request. This list can be used as an external knowledge source in different NER systems, in various dataset domains.
2.1 Detecting Incorrectly Typed Entities
Our premise is that a pair of entities (E1, E2) from a given domain d1 = T1 × T2 contains at least one mistyped entity if there is a relationship-identifying term t that connects them in the dataset, and the term's domain frequency for d1 is "significantly" lower than that of another different domain d2. Our hypothesis is that we can detect mistyped entities by comparing such domain frequencies. We validate it as follows.
Setup. Since the ICWSM'09 dataset does not include labels for
named entities, we identify all type errors through manual evaluation. The complete evaluation we performed is available by request. We compute the list of terms in the dataset and their associated DF scores. Then, we obtain two random entity sets for our evaluation. The first, RANDOM contains 70 entity pairs (thus 140 typed entities) randomly chosen from the dataset. The second, SUSPICIOUS, consists of 326 entity pairs for which the gap in domain frequencies (highest to lowest) is higher a threshold.
More precisely, SUSPICIOUS is obtained as follows. For each term t in the dataset, we compute the difference between its highest and the lowest DF scores. We keep those terms for which this difference is larger than 0.5 (empirically tested to produce high precision with reasonable recall), resulting in 482 unique terms (i.e., relations). From these, we randomly chose 30 to obtain the entity pairs in our tests. For each term, we gathered up to 15 entity pairs, resulting in a total of 326 unique entity pairs (some terms were associated with less than 15 pairs), that generated 375 entities for the SUSPICIOUS evaluation (for most of the entity pairs, only one entity out of the two in a domain is identified as a mistake).
Hypothesis. Our hypothesis is that pairs in the SUSPICIOUS
set are more likely to contain at least one mistyped entity. This is justified by the following observation: out of the 375 entities, 269 (or 72%) are of type ORG, 83 (22%) are of type LOC, and 23 (6%) are of type PER. This distribution is very different than the one that takes into account all entities in the dataset: 12% for ORG, 43% for LOC, and 45% for PER. The type ORG, which appears in only 12% of all entities, appears in 72% of the entities in the SUSPICIOUS set, implying a higher error rate for organizations compared to other entity types.
Table 2 validates this hypothesis. In the table, precision is the number of entities with correct types (we did not consider the cor-
2We do not consider the MISC type in our experiments. They are too generic, making it hard to evaluate whether the entities assigned to this type are misclassified by LBJ.

PER LOC ORG Weighted

RANDOM 0.79 0.70 0.43 0.62

SUSPICIOUS 0.43 0.44 0.25 0.30

Table 2: Correctness of the entity types in the 2 evaluation sets

rectness of the entity boundary) divided by the total number of entities in each set. Weighted is the weighted average of the precision for the three types. Observe that in the RANDOM set entities are correctly typed 62% of the times, whereas in the SUSPICIOUS set this happens only 30% of the time. This 30% reflects the entities we incorrectly identified as mis-typed. Also, observe that LBJ is much more accurate in correctly identifying persons and locations compared to organizations.
Method. The significant observation from our experiment is that
the difference in domain frequency scores may be an effective way of identifying mis-typed entities. It effectively yields a very simple and automatic procedure for detecting incorrect type assignments which has 70% precision. Given the much lower rate mistyping rate of just 38% for the RANDOM set, these results are promising.
3. CONCLUSION
We proposed the use of Domain Frequency scores to predict entities which are erroneously typed by NER systems. This measure can be readily incorporated into existing NER systems with ease. DF exploits terms between pairs of entities to estimate the likelihood of a term to appear between given entity types. DF relies on global (corpus-wide) information as is thus sensitive to the domain at hand. We showed experimentally that the difference in DF scores for a given term serves as a good indicator that the entities associated through that term are incorrectly typed, and that this simple rule was able to detect an entity with incorrect type in 70% of the cases, and that this rate is much higher than that of a random sample of the dataset.
We are investigating ways in which to use the DF scores to further improve not only entity type identification but also the extraction of the relations among the entities. We envision a mutual refinement scheme in which both tasks go hand-in-hand.
4. ACKNOWLEDGEMENTS
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada and the Alberta Ingenuity Fund.
5. REFERENCES
[1] K. Burton, A. Java, and I. Soboroff. The icwsm 2009 spinn3r dataset. In ICWSM '09: Proceedings of the 3rd Int'l AAAI Conference on Weblogs and Social Media, 2009.
[2] F. Mesquita, Y. Merhav, and D. Barbosa. Extracting information networks from the blogosphere: State-of-the-art and challenges. In ICWSM '10: Proceedings of the 4th Int'l AAAI Conference on Weblogs and Social Media, 2010.
[3] L. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition. In CoNLL '09: Proceedings of the 13th Conference on Computational Natural Language Learning, pages 147­155, Morristown, NJ, USA, 2009. Association for Computational Linguistics.

884

Achieving High Accuracy Retrieval using Intra-Document Term Ranking

Hyun-Wook Woo hwwoo@nlp.korea.ac.kr

Jung-Tae Lee jtlee@nlp.korea.ac.kr

Seung-Wook Lee swlee@nlp.korea.ac.kr

Young-In Song yosong@microsoft.com

Hae-Chang Rim rim@nlp.korea.ac.kr

Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea  Microsoft Research Asia, Beijing, China

ABSTRACT
Most traditional ranking models roughly score the relevance of a given document by observing simple term statistics, such as the occurrence of query terms within the document or within the collection. Intuitively, the relative importance of query terms with regard to other individual non-query terms in a document can also be exploited to promote the ranks of documents in which the query is dedicated as the main topic. In this paper, we introduce a simple technique named intra-document term ranking, which involves ranking all the terms in a document according to their relative importance within that particular document. We demonstrate that the information regarding the rank positions of given query terms within the intra-document term ranking can be useful for enhancing the precision of top-retrieved results by traditional ranking models. Experiments are conducted on three standard TREC test collections.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Retrieval models
General Terms
Algorithms, Experimentation
Keywords
inter-document term ranking, precision at top ranks
1. INTRODUCTION
With the rapid growth of Web document collection sizes in recent years, achieving high precision at the top of the retrieved result has become a major issue for search engine users [1]. In traditional IR ranking models [2, 3], the relevance of a document for a given query is scored primarily based on simple term statistics, such as the number of occurrence of query terms within the document or within the whole document collection. Basically, the more query terms occur in a document, the higher the chance that the document would be relevant to the query and thus would be
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

provided at the top part of the ranked list. Despite its simplicity, such a scoring method has been effective in many previous document ranking experiments.
However, traditional models virtually do not consider the relative importance of query terms compared to other individual non-query terms within a document in ranking. For example, consider the query word "q" and the following two equal-length documents at top ranks in which "q" occurs twice: "q q b c d e f " and "q q b b b b c". Traditional ranking models would assume that the weight of "q" is roughly the same for both documents. However, this is indeed not true if we examine the relative importance of "q" within individual documents. In the first document, "q" occurs relatively more than any of the other non-query terms, which implies that the document may be mainly about "q". In contrast, in the second document, "q" occurs relatively less than the word "b", which implies that "q" may be a subtopic of that document. Intuitively, given two documents that both contain the same number of query terms, we would like to rank the document in which the query is dedicated as a main topic above the one where the query is regarded as a minor topic.
The relative importance of query terms with regard to other terms in a document can be observed by ranking all the terms in the document with some weighting scheme, such as the tf-idf method. We refer to this technique as intra-document term ranking. In this paper, we present two heuristic document ranking methods based on the rank positions of given query terms in intra-document term ranking lists of individual documents. Our hypothesis is two-fold:
· First, when query terms are ranked in relatively higher term rank positions than other terms in a given document, it implies that the query is dedicated as the central topic of the document. Thus, there is a higher chance that the document is relevant to the query.
· Second, individual query terms having similar term rank positions to each other in a document reflect that they are treated equally. Thus, there is a chance that the query terms have close relationships to each other within the document, which implies higher relevance.
The succeeding section will elaborate on how we rank documents based on the two hypotheses. We validate our method on three standard TREC test collections.

885

2. PROPOSED METHOD
To analyze the relative importance of individual terms that occurred in a given document, we rank the terms by their standard tf-idf weights. We use the term frequency normalized with the document length as the tf component, and the logarithm of the inverse document frequency as the idf component. We normalize the ranks in range 0 to 1 so that the term with highest tf-idf weight will be at rank 0 and the term with the lowest weight at rank 1.
Given this ranked list of terms within a document, we derive two document ranking heuristics based on the rank positions of query terms within the list. The first heuristic, which corresponds to our first hypothesis, ranks documents according to the average rank position of query terms. Here we basically assume that the higher the rank of the query term within a document, the more likely that the document is relevant. Given a query Q and the term ranking result of a document D, the average rank position of query terms is calculated as follows:

R1(Q, D) =

qiQ{1 - rank(qi, D)} |Q|

(1)

where qi represents ith query term, |Q| is the size of the query in words, and rank(qi, D) is a function that returns the normalized term rank of qi in D.
The second heuristic, which corresponds to our second hypothesis, assumes that the more cohesive the rank positions of query terms (i.e. have similar rank positions to each other) in a document, the higher the probability that the document is relevant. There are a number of ways to measure the cohesiveness of the term ranks. In this paper, we calculate such measure by the maximum rank difference of all pairs of query terms as follows:

R2(Q, D) = 1 - { max {dif f (qi, qj , D)}} (2) qi ,qj QD,qi=qj

where dif f (qi, qj , D) is a function that returns the absolute value of the difference in rank positions of qi and qj . For example, if three query terms were ranked at 0, 0.1, and 0.4, the maximum difference would be 0.3.

3. EXPERIMENTS
For test collections, we use the following standard TREC collections: AP88-90 (Associated Press news 1988-90), WT2g (2 gigabyte Web data), and Blogs06 (Web data used in TREC Blog Track 2006-08). For queries, we use the title field of the TREC topics 50-150 for AP88-90, 401-450 for WT2g, and 851-950 and 1001-1050 for Blogs06.
To investigate whether the two new heuristics can infer the true relevance for documents at top ranks of the retrieved list by traditional ranking models, we retrieved the top 20 documents from the collections using query-likelihood language models [2] with dirichlet prior1 (LM) and computed the correlation between the output value of either of the heuristics and the relevance. We observe that both have positive correlations with document relevance (0.21 for R1 and 0.31 for R2). This result is consistent with our two hypotheses.
We validate the effectiveness of the two ranking heuristics in a re-ranking scheme. First, given a query, we generate a
1We tuned µ to be optimal for each collection (1000, 3000, and 5000 for AP88-90, WT2g, and Blogs06, respectively).

Table 1: Retrieval performance.

Corpus Method MRR

P@1

P@5

LM

0.4598 0.3100 0.2840

+R1

0.4433 0.3000 0.2740 (-3.59%) (-3.23%) (-3.52%)

AP88-90 +R2

0.4671 0.3200 0.2900 (1.59%) (3.23%) (2.11%)

+R1+R2

0.4743 (3.15%)

0.3200 (3.23%)

0.3020 (6.34%)

LM

0.6342 0.6000 0.4880

+R1

0.6874 0.5800 0.4640 (8.39%) (-3.33%) (-4.92%)

WT2g +R2

0.7151 0.6200 0.4640 (12.76%) (3.33%) (-4.92%)

+R1+R2

0.7097 (11.90%)

0.6200 (3.33%)

0.5000 (2.64%)

LM

0.6634 0.5333 0.5747

+R1

0.7090 0.6000 0.5987 (6.87%) (12.51%) (4.18%)

Blogs06 +R2

0.7404 0.6467 0.6067 (11.61%) (21.26%) (5.57%)

+R1+R2

0.7384 (11.31%)

0.6400 (20.01%)

0.6453 (12.28%)

ranked list consisting of top n documents from a test collection using LM, which represents a state-of-the-art baseline. Then, we create a new ranked list of the n documents using each heuristic function. We finally re-rank the documents in the initial list in the ascending order of the mean rank of documents among individual ranked lists. If a tie occurs, a document that has a higher rank in the initial ranked list is promoted. We compare the top results of the re-ranked result with the baseline result using Mean Reciprocal Rank (MRR) and Precision at k ranks (P@1 and P@5). The Indri implementation2 is used for indexing and retrieval; stemming and stopword removal are performed.
The retrieval performances of the baseline and the proposed method are presented in Table 1. We observe that when the baseline ranking is aggregated with either one of the heuristic rankings, the improvement is not consistent across different data collections. However, when the baseline ranking is merged with all two heuristic rankings, the improvement over the baseline is consistent across all collections for all evaluation measures. This demonstrates that the analysis of rank positions of query terms in interdocument term ranking is effective in improving the precision at top ranks. For future work, we plan to explore other features that capture various aspects of inter-document term ranking. We also plan to investigate on designing a unified ranking model that combines the proposed heuristics with the features of the traditional retrieval models.
4. REFERENCES
[1] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and L. Wong. High accuracy retrieval with multiple nested ranker. In SIGIR '06, 2006.
[2] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR '98, 1998.
[3] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, 1975.
2http://www.lemurproject.org/indri/

886

Author Interest Topic Model
Noriaki Kawamae
NTT Comware 1-6 Nakase Mihama-ku Chiba-shi, Chiba 261-0023 Japan
kawamae@gmail.com

ABSTRACT
This paper presents a hierarchical topic model that simultaneously captures topics and author's interests. Our proposed model, the Author Interest Topic model (AIT), introduces a latent variable with a probability distribution over topics into each document. Experiments on a research paper corpus show that AIT is very useful as a generative model.
Categories and Subject Descriptors
H.3.1 [Content Analysis and Indexing]:
General Terms
Algorithms, experimentation
Keywords
Topic Modeling, Latent Variable Modeling
1. INTRODUCTION
Attention is being focused on how to model users' interests in several fields. A model of interest allows us to infer which topics each user prefers and to measure the similarity between them in terms of their interests. For example, the Author-Topic(AT) [3] groups all papers associated with a given author by using a single topic distribution associated with this author. Author-Persona-Topic(APT) [2] introduces a persona, which is also a latent variable, under a single given author. Thus, these models allow each author's documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona
This paper presents the Author Interest Topic(AIT) model; it is a generalization of known author interest models such as AT and APT. AIT allows a number of possible latent variables to be associated with author's interest, while previous models limit this number. Therefore, AIT can describe a wider variety of authors' interests than other models, which reduces the perplexity. Moreover, AIT can infer the overall interest in the training data and so can assign probabilities to previously unseen documents.
2. AUTHOR INTEREST TOPIC MODEL
This section details our model. Table 1 shows the notations used in this paper. Figure 1 shows graphical models to
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Graphical models: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indicates a conditional dependency between variables and the plates indicate a repeated sampling with the iteration number shown. This figure shows that each author produces words from a set of topics that are preferred by the author in (a), persona associated with the author in (b), each document class in (c). In learning a document written by multiple authors, AIT makes copies of the document and associates one copy with each author.
describe the generative process. For modeling each author's interest, our proposal, AIT, incorporates document class cd; it provides an indicator variable that describes which mixture of topics each document d takes, into d. Accordingly, AIT represents documents of similar topics as the same document class in the same way that topic models represent cooccurrence words as the same topic variable. Therefore, the difference between AIT and AT, APT is that rather than representing author's interest as a mixture of topic variables a(AT) or Pa (APT) in each document layer, AIT represents each author's interest as a mixture of document classes a in each author layer. Although both a(AT) and Pa (APT) are associated with only authors, the document class can be shared among authors. This class allows AIT to represent documents having similar topics as the same document class by merging parameters; this reduces the number of possible parameters without losing generality. Accordingly, as the size of training data is increased, relatively fewer parameters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data. Moreover we decide the number of latent variables following CRP [1]. Consequently, AIT increases the number of possible latent variables for explaining all authors' interests.
AIT employs Gibbs sampling to perform inference approximation. In the Gibbs sampling procedure, we need to cal-

887

Table 1: Notations used in this paper

SYMBOL DESCRIPTION

A

number of authors

J

number of document classes

T

number of topics

D

number of documents

V

number of unique words

Ad

authors associated with document d

Da

number of documents written by author a

Nd

number of word tokens in document d

ai

author associated with ith token in document d

pd

persona associated with document d

cd

document class associated with document d

zdi

topic associated with the ith token in document

d

wdi

ith token in document d

a

multinomial distribution of document classes

specific to author a (a|  Dirichlet() )

j

multinomial distribution of topics specific to in-

terest j (j |  Dirichlet() )

t

multinomial distribution of words specific to

topic t (t|  Dirichlet() )

culate the conditional distributions. The predictive distribution of adding interest class cd in documents written by author a to topic cd = j is given by

8 (Pt

njt\d +t)

Q t

(njt +t )

>>< n , aj\d

Q t

(njt\d +t)

(Pt

njt +t )

P (j|c\d, a, z, , )  if j is an existing class class

> > :

j

(Pt njt\d +t)

Q t

(njt\d +t)

, Q t

(njt +t

)

(Pt njt+t)

otherwise

(1)

where naj\d represents the number of documents assigned to j in all documents written by author a, except d, and njt\di represents the total number of tokens assigned to topic t

in the documents associated with document class j, except

token di.

The predictive distribution of adding word wdi in docu-

ment d written by a to topic zd = t is given by

8
> > <

n , ntw\di+w

jt\di

PV v

(ntv\di +v )

P (t|j, z\di, w, , )  if t is an existing class

> > :

, ntw\di+w

t

PV v

(ntv\di +v

)

otherwise

(2)

where ntw\di represents the total number of tokens assigned to word w in topic t, except token di, and njt\di represents the total number of tokens assigned to topic t in

all tokens assigned to j, except token di.

3. EXPERIMENTS
We focus here on the extraction of interests from given documents, and demonstrate AIT's performance as a generative model. The dataset used in our experiments consisted of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that appeared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this dataset for training and comparison.
In our evaluation, the smoothing parameters ,  and

Table 2: Perplexity of AT, APT and AIT: This difference between AIT and APT is significant according to one-tailed t-test with the number of samples G = 100. For fair comparison, the number of topic variables T was fixed at 200, the number of document classes J was fixed at 40(AIT). Results that differ significantly by t-test p < 0.01, p < 0.05 from APT are marked with '**', '*' respectively. The value of Avg means the average computing time for each iteration in gibbs sampling.

Iteration AT APT AIT

2000 1529 1454 1321

4000 1488 1304 1217

6000 1343 1180 1103

8000 1339 1059 988

10000 1333 1027 964

Avg 3.2s 10.4s 11.7s

were set at 0.1, 10(APT),1(AIT) and 1, respectively. We ran single Gibbs sampling chains for 10000 iterations on machines with Dual Core 2.66 GHz Xeon processors.
To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated parameters and compared the resulting values.
Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are better). Table 2 shows the results of the perplexity comparison. This table shows that AIT yielded significantly lower perplexity on the test set than AT or APT, which shows that AIT is better as a topic model. This is due to the ability of AIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather than grouping documents by a given author or persona under a few topic distributions. This implies that clustered documents contain less noise than otherwise. If the number of document classes is overly restricted, the difference between the observed data and the data generated by the model under test increases, which raises the perplexity.
4. CONCLUSION
Our proposed model, AIT, supports the expression of topics in text documents and can identify the interests of authors in these documents. Future work includes extending AIT by taking other metadata such as time, references and link structure into account, for tracking the dynamics of interests and topics.
5. REFERENCES
[1] D. J. D. Aldous. Exchangeability and related topics, volume 1117 of Lecture Notes in Math. Springer, Berlin, 1985.
[2] D. Mimno and A. McCallum. Expertise modeling for matching papers with reviewers. In KDD, pages 500­509, 2007.
[3] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Griffiths. Probabilistic author-topic models for information discovery. In KDD, pages 306­315, 2004.

888

On the Relationship Between Effectiveness and Accessibility

Leif Azzopardi
Department of Computing Science University of Glasgow United Kingdom
leif@dcs.gla.ac.uk
ABSTRACT
Typically the evaluation of Information Retrieval (IR) systems is focused upon two main system attributes: efficiency and effectiveness. However, it has been argued that it is also important to consider accessibility, i.e. the extent to which the IR system makes information easily accessible. But, it is unclear how accessibility relates to typical IR evaluation, and specifically whether there is a trade-off between accessibility and effectiveness. In this poster, we empirically explore the relationship between effectiveness and accessibility to determine whether the two objectives i.e. maximizing effectiveness and maximizing accessibility, are compatible, or not. To this aim, we empirically examine this relationship using two popular IR models and explore the trade-off between access and performance as these models are tuned.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval - Retrieval Models
General Terms: Theory, Experimentation
Keywords: Information Retrieval, Accessibility, Findability, Retrievability, Evaluation
1. INTRODUCTION
Historically, there have been two main ways to evaluate an Information Retrieval (IR) system: efficiency and effectiveness [7]. A complementary and so-called higher order evaluation has been recently proposed based on accessibility [1]. Instead of assessing how well the system performs in terms of speed or performance, access-based measures provide an indication of how easily documents within the collection can be retrieved using a particular retrieval system [1]. Evaluations based on accessibility have been performed in a number of different contexts (see [2, 6, 3, 4]), but there has been little work examining the relationship between accessbased measures and effectiveness measures.
2. MEASURING ACCESSIBILITY
The accessibility of information in a collection given a system has been considered from two points of view, the system side i.e. retrievability [2] and the user side findability [6]. Retrievability measures provide an indication of how easily a
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Richard Bache
Department of Computing Science University of Glasgow United Kingdom
bache@dcs.gla.ac.uk

document could be retrieved using a given IR system, while findability measures provide an indication of how easily a document can be found by a user with the IR system. Here we consider the accessibility measures based on retrievability (see [2] for more details and [3, 4] for examples of its usage in practice.)
Retrievability: The retrievability r(d) of a document d with respect to an IR system, is the ease with which it can be retrieved given all possible queries Q1. Formally,

X

r(d)  f (kdq, c)

(1)

qQ

where q is a query in Q, kdq is the rank at which d is retrieved given q, and f (kdq, c) is a function which denotes how accessible the document d is for the query q given the rank cutoff c. A simple measure of retrievability employs an access function f (kdq, c), such that if d is retrieved in the top c documents given q, then f (kdq, c) = 1 else f (kdq, c) = 0. This is referred to as a cumulative-based retrievability measure [2]; and it provides an intuitive value for each document, i.e. it is the number of times that the document is retrieved in the top c documents. To provide a single measure of access given the retrievability scores for all documents, the Gini measure [5] was proposed in [2]. Intuitively, if all documents were equally accessible according to r(d), then the Gini would be zero (equality), while if all but one document had r(d) = 0 then the Gini would be one (total inequality). Usually, most documents have some level of retrievability and the Gini measures is somewhere between zero and one. Essentially, the Gini coefficient provides an indication of the level of inequality between documents given how easily they can be retrieved with a particular retrieval system.
It is important to note that retrievability can be estimated without recourse to relevance judgments, making it an attractive alternative for automatic evaluation. That is, if there is a positive correlation between accessibility and effectiveness based measures.
Relating Retrievability and Effectiveness: Given the definition of retrievability then a purely random IR system would provide equal access to all documents (i.e. Gini=0); however, this would also result in very poor effectiveness. While, if a (hypothetical) IR system retrieved the set of known relevant documents, and only these documents, regardless of the query, then there would be a very high inequality among documents and Gini would be close to one.

1Since we cannot know the set of all the possible queries, it is usually approximated using a large (order of 105) set of automatically generated queries[2, 3, 4].

889

Neither extreme is desirable, but to what extent do we need to trade-off retrievability for effectiveness. In this poster, we examine the relationship between the Gini measure (i.e. the summarized retrievability measure) and precision, by examining the change in each measure as the parameter values of different retrieval models are varied.
Experimental Setup Two TREC collections were used: Associated Press (AP) 1988-1989 and Wall Street Journal (WSJ) 1987-1992, both with TREC query sets 1, 2 and 3. Two popular IR models were selected: Multinomial Language Modelling with Bayes Smoothing and Okapi BM25. For Language modelling, the smoothing parameter  was varied from 10-4 to 105 in multiplicative steps of 10. For BM25, the b parameter, which adjusts length normalisation, was varied from 0.1 to 1.0 in steps of 0.1. Effectiveness was measured using both precision at 10 (P@10) and mean average precision (MAP). To estimate retrievability, the same methodology employed in [2] was applied, where we used 100,000 two-word queries derived from the most frequent collocations found in each corpus to estimate the retrievability values. Retrievability was measured using the cumulative measure (described above) where c = 10 and c = 100. The degree of equality was measured using the Gini coefficient denoted as Gini@10 and Gini@100, respectively.
Results In Figure 1, plots of the different measures are shown for each model (top: Language Model, bottom: BM25) and each collection (left: AP, right: WSJ) across the parameter values. From these plots, the first point of interest is that Gini varies considerably across the parameter ranges for both models, where minimizing the Gini coefficient translates into providing more access to documents in the collection (this is around  = 10 - 100 for the Language Model and b = 0.6 - 0.8 for BM25. Of note, is that the suggested/default value for b is usually 0.75, which is well within this range.). While this does not directly correspond to when performance is maximized, the difference in performance is quite small; in the range of (0.01-0.03 for both P@10 and MAP). While, these differences in performance were significantly difference (p < 0.05 using Student's T-test) for all but BM25 on WSJ, there does appear to be a positive correlation between the two measures, and this opens up the possibility of using access based measures to tune retrieval systems. These findings suggest that a systematic relationship appears to exist between the gini measurements (representing Accessibility) and the precision measurements (representing Effectiveness).
3. DISCUSSION AND FURTHER WORK
This preliminary analysis of the relationship between accessibility measures (specifically retrievability measures) and effectiveness measures shows that the two goals of maximizing access and maximizing performance are quite compatible. In fact, reasonably good retrieval performance is still obtained by selecting parameters that maximize access (i.e. when there is the least inequality between documents according to Gini given the r(d) values). This motivates the hypothesis that retrieval models/systems can be effectively tuned using access based measures. If this holds, then it suggests that when relevance information is not available a sensible approach to configuring a system is to ensure that users can access all documents as easily as possible. However, further research is needed to test this hypothesize more

Precision/Gini

Precision/Gini

conclusively and to explore this interesting and complex relationship in detail. In future work, we shall examine different models, and the influence of the different parameters on access and performance.
Acknowledgments: This work is supported by Matrixware. We would like to thank the Information Retrieval Facility for the use of their computing resources, and Tamara Polajnar for her helpful feedback and comments on this work.

0.8

1

0.6

0.8

Precision/Gini

0.6 0.4
0.4

0.2

0.2

0

10-4

10-2

100

102

104

LM Parameter  - AP

0

10-4

10-2

100

102

104

LM Parameter  - WSJ

0.8

P@10

MAP

0.7

Gini@10

0.5

Gini@100

0.6

Precision/Gini

0.4

0.5

0.4 0.3
0.3

0.2

0.2 0.4 0.6 0.8

1

BM25 Parameter b - AP

0.2

0.2 0.4 0.6 0.8

1

BM25 Parameter b - WSJ

Figure 1: Plots of Precision and Gini measures across parameters for the LM and BM25 models.

4. REFERENCES
[1] L. Azzopardi and V. Vinay. Accessibility in IR. In Proceedings of 30th ECIR 2008, 482­489, 2008.
[2] L. Azzopardi and V. Vinay. Retrievability: An evaluation measure for higher order information access tasks. In Proceedings of the 17th ACM CIKM 2008, 561­570, 2008.
[3] S. Bashir and A. Rauber. Improving retrievability of patents with cluster-based pseudo-relevance feedback documents selection. In Proceedings of the 18th ACM CIKM 2009, 1863­1866, 2009.
[4] S. Bashir and A. Rauber. Improving retrievability of patents in prior-art search. In Proceedings of 32nd ECIR 2010, 457­470, 2010.
[5] J. Gastwirth. The estimation of the lorenz curve and gini index. The Review of Economics and Statistics, 54:306­316, 1972.
[6] H. Ma, R. Chandrasekar, C. Quirk, and A. Gupta. Improving search engines using human computation games. In Proceedings of the 18th ACM CIKM 2009, 275­284, 2009.
[7] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, 1979.

890

Visual Concept-based Selection of Query Expansions for Spoken Content Retrieval
Stevan Rudinac, Martha Larson, Alan Hanjalic
Multimedia Information Retrieval Lab, Delft University of Technology, Delft, The Netherlands
{s.rudinac, m.a.larson, a.hanjalic}@tudelft.nl

ABSTRACT
In this paper we present a novel approach to semantic-themebased video retrieval that considers entire videos as retrieval units and exploits automatically detected visual concepts to improve the results of retrieval based on spoken content. We deploy a query prediction method that makes use of a coherence indicator calculated on top returned documents and taking into account the information about visual concepts presence in videos to make a choice between query expansion methods. The main contribution of our approach is in its ability to exploit noisy shot-level concept detection to improve semantic-theme-based video retrieval. Strikingly, improvement is possible using an extremely limited set of concepts. In the experiments performed on TRECVID 2007 and 2008 datasets our approach shows an interesting performance improvement compared to the best performing baseline.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance, Experimentation
Keywords: Semantic-theme-based video retrieval, video-level retrieval, query expansion, concept-based video indexing, query performance prediction
1. INTRODUCTION
The semantic theme (or subject matter) of a video is encoded in both its speech track and visual channel. This paper presents a novel multimodal retrieval approach aiming at videos covering the semantic theme expressed by the query. The approach improves the results of a speech-based retrieval system by exploiting concepts (e.g. human, car, house, female, children, indoor) detected in the visual channel. In contrast with most previous works on video retrieval, which focus on shot-level retrieval, e.g., [2, 5], our approach is designed to retrieve entire videos. These larger retrieval units are more appropriate than shots in cases where the searcher is looking for informational material or for entertainment, typical for the semantic-theme-based retrieval scenario (e.g., find a video about archaeology or psychology). Recent work on retrieval beyond the shot level includes [1]. Moving from the shot to the video level requires the combination of multiple shot-level concepts into an effective video-level representation. The novel contribution of our work is the successful use of such a video-level representation to combine the output of automatic speech recognition and visual-concept detection, both known to be noisy, and achieve an overall improvement in retrieval of videos on the basis of semantic theme. The key insight motivating our approach is that the presence, frequency and co-occurrence of
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

visual concepts are potentially powerful indicators of the similarity of videos with respect to their semantic themes. In this initial realization, we focus on exploiting the effects of concept presence and frequency.
In order to combine speech-based retrieval with visual concepts, we select a coherence-based query prediction framework [4]. This technique automatically chooses between results lists produced by a range of different query expansions by making use of a coherence-based indicator calculated over top documents of the results list returned by each expansion. Our specific innovation is use of concept-based representations of videos for calculation of the query prediction indicator. In the following, we first introduce our concept-based video representations and our query prediction method. Then we report on results of experiments that confirm the viability of our approach for effectively exploiting visual concepts to refine spoken content retrieval of videos at the semantic theme level.
2. APPROACH
The input for creating video-level representations is a vector in which each component represents a single concept. In order to weight each concept, we make use of term frequency (TF) and inverse document frequency (IDF) analogously to their use in text retrieval. However, concept detectors do not output binary concept occurrences, but rather shot-level lists of confidence scores. Each score represents the probability that a particular concept occurred in each shot. Our concept weights are created by accumulating the confidence scores for each concept over all shots in the video and then normalizing with the total shot count.
Following the previous feature generation step, we select in the next (feature selection) step the concepts that are potentially the most helpful in discriminating between videos with respect to their semantic themes. Because the output of concept detectors is notoriously noisy (in TRECVID 2009 the best performance failed to exceed 0.25 in the terms of MAP [5]), feature selection is a key aspect of our approach since it introduces a noise control effect. We developed our feature selection method by performing exploratory experimentation. Our experiments make use of TRECVID 2007 dataset and 46 semantic theme labels introduced by the VideoCLEF 2009 (www.multimediaeval.org) benchmark. The labels are manually assigned by professional archivists from the Netherlands Institute for Sound and Vision. We use a set of publicly available concept detection scores, generated for a set of 374 concepts selected from the LSCOM (www.lscom.org) ontology. In order to determine the most representative concepts, we trained classifiers that can identify videos related to each semantic theme and ranked the concepts according to their usefulness to the classifier. A simple voting approach was then applied to merge the lists into a single list that ranked the concept from most to least valuable for semantic discrimination. Our investigation revealed that it

891

is the most frequently occurring concepts in the videos that best

support discrimination between videos in terms of semantic class.

We use this result to select features in the coherence-based query

prediction step. The fact that the most frequent concepts are most

discriminative suggests that it is not so much the occurrence of a

particular visual concept in a video that distinguishes that video's

semantic class, but rather its relative frequency.

Our approach compares results of multiple query expansions

and returns, as the final output, the results list with the highest

coherence score over the Top N videos. The coherence score is

calculated as:

( ) Co(TopN ) =

v , v i j{1,...,N}

ij

1 N ( N -1)

(1)

2

Here  is a function defined on the pair of videos (vi,vj) that is

equal to 1 if their similarity is higher than the similarity of particu-

larly close video pairs from the collection (i.e., closer than TP% of

pair-wise similarities, where TP is the threshold defined below).

The sum is taken over all video pairs in the set of Top N videos.

As a similarity measure between vectors of concept frequencies

we used the cosine similarity. Use of alternative similar-

ity/distance measures such as Kullback-Leibler divergence and

Euclidean distance yielded similar results.

3. EXPERIMENTAL SETUP
We test our approach on TRECVID 2007 and TRECVID 2008 datasets, using the 46 VideoCLEF 2009 semantic labels as queries. We index the Dutch speech recognition transcripts and the English machine translation and carry out retrieval using the Lemur toolkit. The initial results lists are produced using the original query and three query expansions: 1) Conventional PRF, where the number of feedback documents and terms used for expansion are selected for the optimal performance, 2) WordNet (http://wordnet.princeton.edu/) expansion and 3) Google Sets (http://labs.google.com/sets) expansion, where each query is expanded with a set of up to 15 related items (words or multi-word phrases). For each query, the results list yielding the highest coherence indicator is selected. In the (rare) cases of the same indicator values the priority is given to the baseline or the expansions following the ordering as above. We use the concept detection output provided by [3], as mentioned above. In the experiments on both collections we swept the parameter space for the following parameters: number of most frequent (discriminative) semantic concepts (NC), number of documents from the top of results list (N) and the threshold TP used to calculate the coherence score. We reported the results for the optimal parameter setting.

4. EXPERIMENTAL RESULTS
The quality of the initial results list for the original query and three expansion methods is reported in terms of Mean Average Precision (MAP) in Table 1.

Table 1. MAPs of the Baseline and Expansion Methods

Baseline PRF WordNet Google Sets

TRECVID 2007 0.326 0.332 0.260

0.120

TRECVID 2008 0.245 0.265 0.268

0.142

Table 2 contains the MAPs of our concept-based selection of query expansion for TRECVID 2007 and TRECVID 2008 datasets. Statistical significance w.r.t. Wilcoxon Signed Rank Test, p < 0.02 is indicated with `^'.

Table 2. MAPs After Query Expansion Selection (QES)

QES Best Baseline

TRECVID 2007 0.355^ 0.332

TRECVID 2008 0.296 0.268

The results confirm the viability of exploiting visual concepts for refining the output of spoken-content-based video retrieval at the level of a semantic theme. Recall that our feature selection approach was optimized using TRECVID 2007 as a development set. The fact that TRECVID 2008 yielded similar performance demonstrates the ability of our feature selection method to generalize to new data. The optimal parameter settings for TRECVID 2007 and TRECVID 2008 are not the same for both datasets, but are in the similar range: N=5-10, TP=80-90%, NC = 5-10.

5. CONCLUSIONS AND OUTLOOK
We have proposed a multimodal approach to semantic-themebased retrieval of entire videos that exploits frequencies of (semantic) concepts detected in a video to enhance the initial retrieval result obtained at the spoken-content level. We have demonstrated that our approach can be effectively used to decide whether the query should be expanded and which of several query expansions to use. Further, we are making use of only a fraction (5-10) of the set of available concepts (374). This result suggests that concept detectors that focus on a very small number of concepts have large potential to be useful for improving the results of semantic-theme-based video retrieval. In our future work we will further study the characteristics of concept detector output that contribute to effective performance of our approach, investigating, for example, whether the relatively larger performance improvement achieved on the TRECVID 2008 set (cf. Table 2) can be attributed to better performing concept detectors. We will also work to take into account concept co-occurrences and to combine proposed concept-based and text-based indicators to further improve query prediction.

6. ACKNOWLEDGMENTS
The research leading to these results has received funding from the European Commission's 7th Framework Programme (FP7) under grant agreement n° 216444 (NoE PetaMedia).

7. REFERENCES
[1] Aly, R., Doherty, A., Hiemstra, D., and Smeaton, A. 2010. Beyond shot retrieval: searching for broadcast news items using language models of concepts. In ECIR, Milton Keynes, UK, 2010.
[2] Hsu, W. H., Kennedy, L. S., and Chang, S. 2006. Video search reranking via information bottleneck principle. In ACM MM, Santa Barbara, CA, USA, 2006.
[3] Jiang, Y-G., Yanagawa, A., Chang, S-F., and Ngo, C-W. 2008. CU-VIREO374: Fusing Columbia374 and VIREO374 for Large Scale Semantic Concept Detection. Columbia University ADVENT Technical Report #223-2008-1.
[4] Rudinac, S., Larson, M., and Hanjalic, A. 2010. Exploiting result consistency to select query expansions for spoken content retrieval. In ECIR, Milton Keynes, UK, 2010.
[5] Snoek, C. G. M., van de Sande, K. E. A., de Rooij, O., et al. 2009. The MediaMill TRECVID 2009 semantic video search engine. In TRECVID Workshop, Gaithersburg, USA, 2009.

892

Mining Adjacent Markets from a Large-scale Ads Video Collection for Image Advertising

Guwen Feng
Nanjing University Nanjing, Jiangsu, P.R.China
linvondepp@gmail.com

Xin-Jing Wang, Lei Zhang, Wei-Ying Ma
Microsoft Research Asia Beijing, P.R.China
{xjwang,leizhang,wyma}@microsoft.com

ABSTRACT
The research on image advertising is still in its infancy. Most previous approaches suggest ads by directly matching an ad to a query image, which lacks the power to identify ads from adjacent market. In this paper, we tackle the problem by mining knowledge on adjacent markets from ads videos with a novel Multi-Modal Dirichlet Process Mixture Sets model, which is a unified model of (video frames) clustering and (ads) ranking. Our approach is not only capable of discovering relevant ads (e.g. car ads for a query car image), but also suggesting ads from adjacent markets (e.g. tyre ads). Experimental results show that our proposed approach is fairly effective.
Categories and Subject Descriptors
I.5.4 [Pattern Recognition]: Applications ­ computer vision. G.3 [Probability and Statistics]: Nonparametric statistics.
General Terms
Algorithms, Performance.
Keywords
Image advertising, adjacent marketing, video retrieval.
1. INTRODUCTION
Though image has become an important media in the Web, how to monetize web images is a seldom touched problem. Few research works have been published in the literature. Among them, most of the works suggest directly mapping an ad to an image [2]. They suffer from the vocabulary impedance problem so that if a term does not appear in both an image and an ad' features, no connections will be built between them. The approach of Wang et al. [4] improves this by leveraging the ODP ontology to bridge the vocabulary gap, but it is still limited by the available texts.
Adjacent marketing means to develop additional items which compliment a customer's needs in some manner, e.g. suggest insurance when one buys a car. The insurance package thus makes up of qualified adjacent markets of cars. A key challenge is to discover the potential adjacent market (e.g. insurance) of a certain
This work was performed in Microsoft Research Asia.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Retrieved And Expanded Frames

... ...

Query

Ranking Results

Advertise

Benz

Insurance

Tires

Figure 1. Key idea: the story frames (in red blocks) and ads frames (cyan blocks) of video ads suggest adjacent markets.

product/object (e.g. car).
In this paper we propose a solution of adjacent marketing for image advertising based on a large-scale ads video collection. It is motivated by the fact that generally a video ad contains two types of frames ­ story frames and ads frames, as shown in Figure 1. Story frames in general provide the main concepts (e.g. cars) that are related to the corresponding ads (e.g. insurance, tires). They imply certain human knowledge on the adjacent markets, e.g. showing tire ads on car images. A novel Multi-Modal Dirichlet Process Mixture Sets (MoM-DPM Sets) model is proposed as the key technique behind.
2. SYSTEM OVERVIEW
Figure 2 shows the system overview. In the offline stage, we extract video keyframes and perform auto-tagging on them. The online stage contains three steps: 1) multimodal search on keyframes of ads videos given a query image (or query images). Both visual and textual similarities are taken into consideration. The texts come from three resources: image auto-tagging, OCR and surrounding texts; 2) search result expansion. Since the search step tends to find video frames of similar content, to incorporate the adjacent information between objects, we expand the retrieved frames with the rest frames in corresponding videos. For example, given the pizza image in Figure 2, the retrieved frames are generally about food, but by expanding them with the rest frames from the same ads videos, we are able to retrieve those soft drink and restaurant video frames which suggest the adjacent markets; and 3) ads clustering and ranking. The disadvantage brought by frame expansion is that the search results become noisy and with scattered topics. We propose the MoM-DPM Sets model to automatically learn the key topics from the expanded search results, and rank an ads database with the learnt topics. The top-

893

Query Image
Pizza 1. Search

Advertise

Annotate Using Arista
Visutal Features Detection Using ColorDescriptor

Large-Scale Video Frames DB

2. Cosine Measurement

Expanded Video Frames 3. Expand

Suggested Ads

Pizza

Pepsi Restaurant

6. Sorting Approach

Ads DB

MoM-DPM Sets

Clusters

5. Ranking

4. Clustering

...

...

Figure 2. System Overview.

ranked ads will be output. Therefore we find ads of Pepsi Cola and restaurants for the pizza image.

3. THE MoM-DPM SETS MODAL
The MoM-DPM Sets model addresses four challenges: 1) discover the latent topics shared among the frames, 2) automatically determine the number of topics, 3) leverage both the visual and textual features to ensure a better performance of topic discovery, and 4) unify the approaches of topic mining and ads ranking.

Let be the th latent topic and , be the visual and textual features of a query image respectively. Let , , denote the
concentration parameter and base distributions of visual and textual features respectively. Let , be model parameters and
, be the visual and textual features of the observed video
frames labeled by the topic respectively. The general form of is MoM-DPM Sets is given in Eq(1).

| , ,, ,

,

|

,



|

|,

(1)

if

for some ;



|

|

|

|

if

for all .

where and are the observed video frames corresponding to topic . is the normalization factor. is the number of observed video frames and , is the number of observed video frames (except the th) which belong to topic . We use Gibbs sampling to solve the model, which generally converges in 30 iterations.
MoM-DPM Sets has two key features which make it different from previous multimodal DP Mixture processes [3]. Firstly, rather than to learn an optimal parameter set of , , it intends to figure out the membership of each video frame given the observed video frames , . In our approach, , , and
are known ( and are learnt from the clustering step), while
the model parameters are going to be integrated out analytically. Such a set-based reasoning strategy [1] is more powerful in discovering analogical objects, e.g. given a frame set of Pepsi-cola and Coca-cola, this model is able to discover soda because they share the same concept of soft drinks. Secondly, since the model does not rely on certain parameter set, the clustering (topic mining) step and ranking step shares the same model formulation. The ranking process is as Eq.(2).

Average Precision

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

3

5

Our Approach

7

10

15

20

top N

Argo [4] DM [2]

Figure 3. Average precision performance @ top N.

| .. , ,

max



|

,

(2)



|

,

where ..

, , ... , defines the latent topic space.

4. EXPERIMENTS

We crawled about 32k videos from Youtube.com initiated by 30 popular concepts for advertising. In total 327,889 key frames were extracted, which make up of the ads videos collection for frame search. We randomly selected 450 ads as a separate ads DB for the ranking purpose. 100 Flickr images were used as queries.

Figure 3 illustrates the average precision at top 20 ads of our approach compared with those of the baselines Argo [4] and direct match [2]. It can be seen that our approach consistently outperforms the baselines. The gap between the blue curve and the green one indicates that our approach is able to identify the relevant ads from potential adjacent market, which have little overlap with the query image in both visual and textual features. And the gap between the red curve and the green one indicates that Argo [4] also tackles the adjacent marketing problem to a certain extent but it is not effective enough.
There are big gaps between our methods and the baselines in top 3 results, while the gap narrows down from top 5 to top 20. This may due to the limited size of our ads DB. Considering that generally a publisher such as Google shows less than five targeted ads, our method suggests a promising research direction for adjacent marketing.
5. CONCLUSION
Web image is an uncovered gold mine. Our method is the first work to tackle the adjacent marketing problem for image advertising. It leverages the human intelligence embedded in video ads to build the connections among ads objects based on a novel Multi-Modal Dirichlet Process Mixture Sets model.

6. REFERENCES
[1] Z. Ghahramani, and K. A. Heller. Bayesian Sets. Neural Information Processing Systems (NIPS). 2005.
[2] T. Mei, X.-S. Hua, and S.-P. Li. Contextual In-Image Advertising. 2008.
[3] A. Velivelli, and T.S. Huang. Automatic Video Annotation Using Multimodal Dirichlet Process Mixture Model. ICNSC 2008.
[4] X.-J. Wang, M. Yu, et al. Argo: Intelligent Advertising by Mining a User's Interest from His Photo Collections, in conjunction with SIGKDD (ADKDD), Paris, 2009.

894

A Co-learning Framework for Learning User Search Intents from Rule-Generated Training Data

Jun Yan1 Zeyu Zheng1
1Microsoft Research Asia Sigma Center, No.49, Zhichun Road Beijing, 100190, China
{junyan, v-zeyu, zhengc}@microsoft.com

Li Jiang2 Yan Li2 Shuicheng Yan3 Zheng Chen1

2Microsoft Corporation

3Department of Electrical and

One Microsoft Way

Computer Engineering

Redmond, WA 98004

National University of Singapore

{lij, roli}@microsoft.com

117576, Singapore
eleyans@nus.edu.sg

ABSTRACT
Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as "compare products", "plan a travel", etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data for learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated training data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different training datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently classified data by one classifier are added to other training datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process.

Though various popular machine learning techniques could be applied to learn the underlying search intents of the users, it is generally laborious or even impossible to collect sufficient and label high quality training data for such learning task [1]. Despite of the laborious human labeling efforts, many intuitive insights, which could be formulated as rules, can help generate small scale possibly biased and noisy training data. For example, to identify whether the users have intents to compare different products, several assumptions may help make the judgment. Generally, we may assume that if a user submits a query with explicit intent expression, such as "Canon 5d compare with Nikon D300", he/she may want to compare products. Though the rules satisfy the human common sense, there are two major limitations if we directly use them to infer ground truth. First, the coverage of each rule is often small and thus the training data may be seriously biased and insufficient. Second, the training data are usually not clean since no matter which rule we use, there may exist exceptions. In this paper, we propose a co-learning framework (CLF) for learning user intent from the rule-generated training data, which are possibly biased and noisy. The problem is,

Without laborious human labeling work, is it possible to train

user search intent classifier using the rule-generated training data,

which are generally noisy and biased? Given sets of rule-

generated training datasets , 1,2, ... , how to train the

classifier :

on top of these biased and noisy training data

sets with good performance?

General Terms
Algorithms, Experimentation
Keywords
User intent, search engine, classification.
1. INTRODUCTION
The classical relevance based search strategies may fail in satisfying the end users due to the lack of consideration on the real search intents of users. For example, when different users search with the same query "Canon 5D" under different contexts, they may have distinct intents such as to buy Canon 5D, to repair Canon 5D, etc. The search results about Canon 5D repairing obviously cannot satisfy the users who want to buy a Canon 5D camera. Learning to understand the true user intents behind the users' search queries is becoming a crucial problem for both Web search and behavior targeted online advertising.
Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. THE CO-LEARNING FRAMEWORK

Suppose we have sets of rule-generated training data ,

1,2, ... , which are possibly noisy and biased, and a set of

unlabeled user behavioral data . Each data sample in the

training datasets is represented by a triple , ,

1,

1,2, ... | |, where stands for the feature vector of the data

sample in the training data , is its class label and | | is the

total number of training data in . On the other hand, each

unlabeled data sample, i.e. the user search session that could not

be covered by our rules, is represented as , ,

0,

1,2, ... | |. Suppose for any

, all the features constituting

the feature space are represented as a set

| 1,2, ... .

Suppose among all the features F, some have direct correlation to

the rules, that is they are used to generate the training dataset .

These features are denoted by

, which constitute a subset

of F. Let =

be the subset of features having no direct

correlation to the rules used for generating training dataset .

Given a classifier :

, where

is any subset of F, we

use to represent an untrained classifier and use to represent

the classifier trained by the training data . Suppose

|

895

means to train the classifier by training dataset using the

features

, we have

trained classifier , let

| , 1,2, ... . For the | stands for classifying

using features F. We assume for each output result of trained

classifier , it can output a confidence score. Let

|

,

where is the class label of assigned by and the is the corresponding confidence score.

After generating a set of training data , 1,2, ... , based on rules, we first train the classifier by , 1,2, ... ,
independently. Then we can get a set of K classifiers,

| , 1,2, ... .

Note that the reason why we use to train classifier on top of

instead of using the full set of features F is that is generated

from some rules correlated to , which may overfit the classifier

if we do not exclude them. After each classifier is trained

by , we use to classify the training dataset itself. A basic assumption of CLF is that the confidently classified instances by

classifier , 1,2, ... , have high probability to be correctly

classified. Based on this assumption, for any

, if the

confidence score of the classification is larger than a threshold, i.e. > and the class label assigned by the classifier is different

from the class label assigned by the rule, i.e.

, then

is considered as noise in the training data . Note that here

is the label of assigned by classifier, is its observed

class label in training data, and is the true class label, which is

not observed. We exclude it from and put it into the unlabeled dataset . Thus we update the training data by

,

.

Then we use the classifier ,

1,2, ... , to classify the

unlabeled data independently. Based on the same assumption

that the confidently classified instances by classifier have high probability to be correctly classified, for any data belonging to ,

if the confidence score of the classification is larger than a

threshold, i.e. > , where

|

, we

include into the training dataset. In other words,

,

, 1,2 ... , .

Through this way, we can gradually reduce the bias of the rulegenerated training data.

On the other hand, some unlabeled data are added into the training

datasets. Suppose the ,

1| is the probability of a

data sample to be involved in the training data at the iteration n

conditioned on this data sample is represented as a feature vector

and

1 is the probability of any data sample in D is

considered as a training data sample. It can be proved that after n

iterations using CLF, for each training dataset, we have

,

1|

1.

The remaining questions are when to stop the iteration and how to

train the classifier after iteration stops. In this work, we define the

iteration stopping criteria as "if |{ |

,

}| < n

or the number of iterations reaches N, then we stop the iteration".

After the iterations stop, we obtain K updated training datasets

with both noise and bias reduced. Finally, we merge all these

training datasets into one. Thus we can train the final classifier as

.

3. EXPERIMENTS
In this short paper, we utilize the real user search behavioral dataset, which comes from the search click-through log of a commonly used commercial search engine. It contains 3,420 user search sessions, in each of which, the user queries and clicked Web pages are all logged. Six labelers are asked to label the user intents according to the user behaviors as ground truth for results validation. We name this dataset as the "Real User Behavioral Data". The n-gram features are used for classification in the Bag of Words (BOW) model. One of the most classical evaluation metrics for classification problems, F1, which is a tradeoff between Precision (Pre) and recall (Rec) is used as the evaluation metric. For comparison purpose, we utilize several baselines to show the effectiveness of the proposed CLF. Firstly, since we can use different rules to initialize several sets of training data, directly utilizing one training dataset or the combination of all rule-generated training datasets to train the same classification model can give us a set of classifiers. Among them, we take the classifier with the best performance as the first baseline, referred to as "Baseline" in the remaining parts of this section. The second baseline is the DL-CoTrain algorithm, which is a variant of cotraining algorithm. It also starts from the rule-generated training data for classification and thus has the same experiments configuration as CLF. The classification method selected in CLF is the classical Support Vector Machine (SVM). In Table 3, we show the experimental results of CLF after 25 rounds of iterations compared with the baseline algorithms. From the results we can see that, in terms of F1, the CLF can improve the classification performance as high as 47% compared with the baseline.
Table 3. Results of CLF after 25 iterations

Baseline

DL-CoTrain

Pre

0.78

0.78

Rec

0.24

0.12

F1

0.36

0.21

CLF 0.81 0.39 0.53

4. CONCLUSION
One bottleneck of user search intent learning for Web search and online advertising is the laborious training data collection. In this paper, we proposed a co-learning framework (CLF), which aims to classify the users' search intents without laborious human labeling efforts. We firstly utilize a set of rules coming from the common sense of humans to automatically generate some initial training datasets. Since the rule-generated training data are generally noisy and biased, we propose to iteratively reduce the bias of the training data and control the noises in the training data. Experimental results on both real user search click data and public dataset show the good performance of the co-learning framework.

5. REFERENCE
[1] Russell, D.M., Tang, D., Kellar, M. and Jeffries, R. 2009. Task behaviors during web search: the difficulty of assigning labels. Proceedings of the 42nd Hawaii International Conference on System Sciences (Hawaii, United States, January 05 - 08, 2009). HICSS '09. IEEE Press, 1-5. DOI= 10.1109/HICSS.2009.417.

896

Learning the Click-Through Rate for Rare/New Ads from Similar Ads

Kushal Dave
Language Technologies Research Centre International Institute of Information Technology
Hyderabad, India
kushal.dave@research.iiit.ac.in

Vasudeva Varma
Language Technologies Research Centre International Institute of Information Technology
Hyderabad, India
vv@iiit.ac.in

ABSTRACT
Ads on the search engine (SE) are generally ranked based on their Click-through rates (CTR). Hence, accurately predicting the CTR of an ad is of paramount importance for maximizing the SE's revenue. We present a model that inherits the click information of rare/new ads from other semantically related ads. The semantic features are derived from the query ad click-through graphs and advertisers account information. We show that the model learned using these features give a very good prediction for the CTR values.
Categories and Subject Descriptors
I.2.6 [Computing Methodologies]: Artificial Intelligence-- Learning; I.6.5 [Computing Methodologies]: Simulation and Modeling--model development; H.3.3 [Information Systems]: Information Storage and Retrieval
General Terms
Algorithms, Economics, Experimentation.
Keywords
Sponsored Search, Click-Through Rate Prediction, Ranking
1. INTRODUCTION
Sponsored search can be seen as an interaction between three parties - SE, User and the Advertiser. The user issues a query to a SE related to the topic on which he/she seeks information. Advertisers and SEs try to exploit the immediate interest of user in the topic by displaying ads relevant to the query topic. Advertisers bid on certain keywords known as bid terms and their ads may get displayed based on the match between bid term and the user query. SEs try to rank the ads in a way that maximizes its revenue.
Search engines typically rank ads based on the expected revenue ( ad(Rev)). Expected revenue from an ad is a function of both bid and relevance: ad(Rev) = BidRelevancead. The relevace of an ad is measured using its CTR. The CTR of an ad for a query is the no. of clicks normalized by no. of impressions for that query. CTR of an ad is a function of both ad and the query, i.e. an ad can have a different CTR for different queries. The CTR value for an ad-query pair
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

is calculated form past click logs. For new/rare ads, we do not have any/sufficient past click data. Hence CTR for such ads need to be predicted so that they can be ranked along with other frequent ads. Richardson et. al. [5] predict the CTR based on ad text, ad quality etc. Fain et. al. [4] predict the CTR based on term clusters. We propose similarity features derived from click logs and advertisers hierarchy to accurately predict the CTR for new ads.

2. DATASET

The dataset used in our experiments comprised 12 days search log from Yahoo! search engine's US market. After removing redundant fields, each record in the dataset contained following fields: 1.Query 2.Term Id 3. Creative ID 4.Adgroup ID 5.Campaign ID 6.Account ID 8.CTR. Fields 2-6 point to a unique ad. Creative id points to the ad text. An ad text comprises bid term, title, abstract & display URL. The CTR values are normalized by removing the position & presentation bias. After some preprocessing, we got 1,447,543 unique query-ad pairs from the click through logs. It contained 1,97,080 unique queries and 9,43,431 unique ads. We randomly divide this dataset into 65-25-10 ratio for training, testing and validation respectively. We use Gradient boosted decision trees (GBDT) as a regression model [3]. Using validation set, the number of trees and no. of nodes parameters of GBDT were set to 600 and 150 respectively.

3. FEATURES

Features from Query-ad click graph: These features

are based on the semantic relations of the queries and ads

with other similar queries and ads. Regelson [4] have shown

that similar ads (bid terms in their case) follow similar CTR

distribution. The idea here is to learn the CTR values of

query-ad pair from semantically similar queries and ads.

We derive the semantic similarity from the query ad click-

through graph. The click graph is built from 12 days query

log (same period from which we generated our dataset).

Queries are represented as vectors and these query vectors

are compared to find similarity amongst the queries. A

query q is represented as a vector of transition probability

from q to all the ads in the graph. Edges are weighted us-

ing click frequency-inverse query frequency (CF-IQF) model:

cf iqf (qi, aj ) = cij  iqf (aj ).

The transition probability from a query to an ad, P (aj|qi) =

cf iqf (qi, aj)/cf iqf (qi). Each query is represented as q =

(P (a1|qi), P (a2|qi), · · · , P (an|qi)). The similarity between

two queries qi and qj is the cosine similarity between the

two query vectors. Sim(qi, qj) = Cosine

qi  qj qi · qj

. This

897

Table 1: Improvement for various features (p-value  0.01)

Feature
Baseline Sim-Q Sim-A Sim-QA Term Creative Adgroup

RMSE (1e-3)
7.20 5.86 6.31 5.68 6.24 6.51 5.87

KL Diver-gence (1e-1)
1.72 1.42 1.53 1.38 1.45 1.50 1.35

% Improvement
18.61% 12.36% 21.11% 13.34%
9.6% 18.48%

Feature
Campaign Account AdH
SimQA+Camp QADL
SimQA+Camp +QADL

RMSE (1e-3) 5.67 5.94 6.20 5.28 6.50
5.14

KL Diver-gence (1e-1)
1.32 1.39 1.46 1.24 1.56
1.21

% Improvement
21.25% 17.50% 13.9% 26.67% 9.72%
28.61%

similarity is used to predict the CTR for new query-ad pair by retrieving top k queries similar to q' and calculating the weighted average of the CTR values for all the ads over query q' as in [1]. Using query similarity,the CTR is estimated as:
X CT R(qk)  Sim(qi, qk)
QCT R(ai) = k X Sim(qi, qk)
k
The similarity between ads is also calculated in a similar fashion, with each ad being represented by the transition probability from ad to query P (qj|ai) and similarity between two ads is reffered as Sim(ai, ak). Using ad similarity, The CTR of is estimated as follows:
X CT R(ak)  Sim(ai, ak)
ACT R(ai) = k X Sim(ai, ak)
k
Along with QCTR/ACTR We also consider the number of similar queries/ads retrieved (Nq/Na). The Query and ad similarity featuers are called Sim-Q & Sim-A.
Figure 1: A typical Ad hierarchy
Features from Ad Hierarchy: Advertisements on an ad engine are typically maintained in some kind of a hierarchy. One such hierarchy is shown in Fig. 1. There are numerous reasons for maintaining ads in a hierarchy: (1)Advertiser's business may span various business units (BU). Ads from the same advertiser but from different BUs are maintained in different accounts. (2)For each BU, the advertisers can have ads on a range of products. Advertisements from the same account on similar products fall under the same Campaign. (3)Adgroups do further granular classification of ads. (4) Finally, an ad comprises a bid term and ad text (creative). Combination of these two makes an ad. We aggregate ads at each level viz. Term, Creative, Adgroup, Campaign and Account, compute the average within each group and use them as features in our model. In addition, number of featuers in each group are also taken as

features. We call these features as AdH features. Detailed explanation of all the features is available in [2].
Features from Query-ad lexical match: In an attempt to capture how relevant an ad is to the query, we compute the lexical overlap between the query and these ad units. We compute various text matching features such as cosine similarity, word overlap, character overlap, and string edit distance for each combination of unigrams and bi-grams. We refer to this category of features as QADL. For all the set of features we also consider log of each feature as a feature. In all we have 50 features.
As shown in Table 1, Sim-Q & Sim-A give good improvements and when combined (Sim-QA) give an improvement of 21.11%. In the AdH category, Campaign (Camp) gave the best result and when Sim-QA was clubbed with Camp the improvement over baseline reached 26.67%. Finally, lexical feature did not yeild much improvement alone, but (SimQA+Camp+QADL) give the best performance with a good 28.61% improvement over the baseline. All these improvements are staistically significant at 99% significance level.
When all the features were ranked according to the feature importance [3]. Features like Campaign, ACTR, log(ACTR), No. of ads in campaign were amongst the top few.
4. CONCLUSIONS
We have proposed an approach to predict the CTR for new ads based on the similarity with other ads/queries. The similarity of ads is derived from sources like query ad clickthrough graph and advertisement hierarchies maintained by the ad engine. The model gives good prediction on the CTR values of new ads. Analysis of the feature's contribution shows that the features derived from the ad hierarchy and from the click-through graphs contribute the most to the model followed by some of the word overlap features.
5. ACKNOWLEDGMENTS
We are grateful to Yahoo! labs Bangalore for granting access to the ad click-through logs.
6. REFERENCES
[1] T. Anastasakos, D. Hillard, S. Kshetramade, and H. Raghavan. A collaborative filtering approach to ad recommendation using the query-ad click graph. In CIKM '09, pages 1927­1930, 2009.
[2] K. Dave and V. Varma. Predicting the click-through rate for rare/new ads. Technical report IIIT/TR/2010/15, IIIT-H, 2010.
[3] J. H. Friedman. Stochastic gradient boosting. Comput. Stat. Data Anal., 38(4):367­378, 2002.
[4] M. Regelson and D. C. Fain. Predicting click-through rate using keyword clusters. In Electronic Commerce (EC). ACM, 2006.
[5] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In WWW '07, pages 521­530, 2007.

898

Graphical Models for Text: A New Paradigm for Text Representation and Processing

Charu C. Aggarwal
IBM T. J. Watson Research Center Hawthorne, New York, USA
charu@us.ibm.com

Peixiang Zhao
University of Illinois at Urbana-Champaign Urbana, Illinois, USA
pzhao4@uiuc.edu

ABSTRACT
Almost all text applications use the well known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve distance and ordering information between the words, and provide a much richer representation of the underlying text. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms: Algorithms
1. INTRODUCTION
The most common representation for text is the vector-space representation. The vector-space representation treats each document as an unordered "bag-of-words". While the vector-space representation is very efficient because of its simplicity, it loses information about the structural ordering of the words in the document. For many applications, such an approach can lose key analytical insights. This is especially the case for applications in which the structure of the document plays a key role in the underlying semantics. The efficiency of the vector-space representation has been a key reason that it has remained the technique of choice for a variety of text processing applications. On the other hand, the vectorspace representation is very lossy because it contains absolutely no information about the ordering of the words in the document. One of the goals of this paper is to design a representation which retains at least some of the ordering information among the words in the document without losing its flexibility and efficiency for data processing.
While the processing-efficiency constraint has remained a strait-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

jacket on the development of richer representations of text, this constraint has become easier to overcome in recent years. This is because of advances in the computational power of hardware, and the increased sophistication of algorithms in other fields such as graph mining, which can be leveraged with representations such as those discussed in this paper. In this paper, we will design graphical models for representing and processing text data. In particular, we will define the concept of distance graphs, which represents the document in terms of the distances between the distinct words. We will show that such a representation can retain much richer information about the underlying data. It also allows for the use of many current text and graph mining algorithms without the need for new algorithmic efforts for this new representation. In fact, we will see that the only additional work required is a change in the underlying representation, and all existing text applications can be directly used with a vector-space representation of the structured data. In some cases, it also enables distance-based applications which are not possible with the vector-space representations.
2. DISTANCE GRAPHS
While the vector-space representation maintains no information about the ordering of the words, the string representation is at the other end of spectrum in maintaining complete ordering information. Distance graphs are a natural intermediate representation which preserve a high level of information about the ordering and distance between the words in the document. At the same time, the structural representation of distance graphs make it an effective representation for easy processing. Distance graphs can be defined to be of a variety of orders depending upon the level of distance information which is retained. Specifically, distance graphs of order k retain information about word pairs which are at a distance of at most k in the underlying document. We define a distance graph as follows:
DEFINITION 1. A distance graph of order k for a document D in corpus C is defined as graph G(C, D, k) = (N (C), A(D, k)), where N (C) is the set of nodes defined specific to the corpus C, and A(D, k) is the set of edges in the document. The sets N (C) and A(D, k) are defined as follows:
(a) The set N (C) contains one node for each distinct word in the entire document corpus C. Therefore, we will use the term "node i" and "word i" interchangeably to represent the index of the corresponding word in the corpus. Note that the corpus C may contain a large number of documents, and the index of the corresponding word (node) remains unchanged over the representation of the different documents in C. Therefore, the set of nodes is denoted by N (C), and is a function of the corpus C. (b) The set A(D, k) contains a directed edge from node i to node j

899

MARY HAD A LITTLE LAMB, LITTLE LAMB, LITTLE LAMB, MARY HAD A LITTLE LAMB, ITS FLEECE WAS WHITE AS SNOW

REMOVE STOPWORDS
CREATE

MARY LITTLE LAMB, LITTLE LAMB, LITTLE LAMB, MARY LITTLE LAMB, FLEECE WHITE SNOW

Order 0:

DISTANCE GRAPH

2

4

4

1

1

1

MARY

LITTLE

LAMB

FLEECE

WHITE

SNOW

Order 1:

2 2
MARY
Order 2:
2 2
MARY

4

4

4 1

LITTLE

LAMB

2

1

6

4

6 1

LITTLE

LAMB

3

2

1 1
FLEECE
1 1
FLEECE 1

1 1
WHITE
1 1
1 WHITE

1 SNOW
1 SNOW

Figure 1: Illustration of Distance Graph Representation

if the word i precedes word j by at most k positions. For example, for successive words, the value of k is 1. The frequency of the edge is the number of times that word i precedes word j by at most k positions in the document.
We note that the set A(D, k) always contains an edge from each node to itself. The frequency of the edge is the number of times that the word precedes itself in the document at a distance of at most k. Since any word precedes itself at distance 0 by default, the frequency of the edge is at least equal to the frequency of the corresponding word in the document.
Most text collections contain many frequently occurring words (known as stop-words), which are typically filtered out before text processing. Therefore, it is assumed that these words are removed from the text before the distance graph construction. In other words, stop-words are not counted while computing the distances for the graph, and are also not included in the node set N (C). This greatly reduces the number of edges in the distance graph representation. This also translates to better efficiency during processing.
We note that the order-0 representation contains only self loops with corresponding word frequencies. Therefore, this representation is quite similar to the vector-space representation. Representations of higher orders provide structural insights of different levels of complexity. An example of the distance graph representation for a well-known nursery rhyme "Mary had a little lamb" is illustrated in Figure 1. In this figure, we have illustrated the distance graphs of orders 0, 1 and 2 for the text fragment. The distance graph is constructed only with respect to the remaining words in the document, after the stop-words have already been pruned. The distances are then computed with respect to the pruned representation. Note that the distance graphs or order 0 contain only self loops. The frequencies of these self-loops in the order-0 representation corresponds to the frequency of the word, since this is also the number of times that a word occurs within a distance of 0 of itself. The number of edges in the representation will increase for distance graphs of successively higher orders. Another observation is that the frequency of the self loops in distance graphs of order 2 increases over the order-0 and order-1 representations. This is because of repetitive words like "little" and "lamb" which occur within alternate positions of one another. Such repetitions do not change the self-loop frequencies of order-0 and order-1 distance graphs, but do affect the order-2 distance graphs. We note that distance graphs of higher orders may sometimes be richer, though this is not necessarily true for orders higher than 5 or 10. For example, a distance graph with order greater than the number of distinct words in the document would be a complete clique. Clearly, this does not necessarily encode useful information. On the other hand, distance graphs of order-0 do not encode a lot of useful information either.
From a database perspective, such distance graphs can also be

represented in XML with attribute labels on the nodes corresponding to word-identifiers, and labels on the edges corresponding to the frequencies of the corresponding edges. Such a representation has the advantage that numerous data management and mining techniques for semi-structured data have already been developed. These can directly be used for such applications. Distance graphs provide a much richer representation for storage and retrieval purposes, because they partially store the structural behavior of the underlying text data.
An important characteristic of distance graphs is that they are relatively sparse, and contain a small number of edges for low values of the order k. As we will see in the experimental results presented in [1], it suffices to use low values of k for effective processing in most mining applications.
PROPERTY 1. Let f (D) denote the number of words in document D (counting repetitions), of which n(D) are distinct. Distance graphs of order k contain at least n(D)·(k+1)-k·(k-1)/2 edges, and at most f (D) · (k + 1) edges.
The modest size of the distance graph is extremely important from the perspective of storage and processing. In fact, the above observation suggests that for small values of k, the total storage requirement is not much higher than that required for the vectorspace representation. This is a modest price to pay for the semantic richness captured by the distance graph representation.
One advantage of the distance-graph representation is that it can be used directly in conjunction with either existing text applications or with structural and graph mining techniques, as follows: (a) Use with existing text applications: Most of the currently existing text applications use the vector-space model for text representation and processing. It turns out that the distance graph can also be converted to a vector-space representation. The main property which can be leveraged to this effect is that the distance-graph is sparse and the number of edges in it is relatively small compared to the total number of possibilities. For each edge in the distancegraph, we can create a unique "token" or "pseudo-word". The frequency of this token is equal to the frequency of the corresponding edge. Thus, the new vector-space representation contains tokens only corresponding to such pseudo-words (including self-loops). All existing text applications can be used directly in conjunction with this "edge-augmented" vector-space representation. (b) Use with structural mining and management algorithms: The database literature has seen an explosion of management and mining techniques for graph and XML mining. Since our distancebased representation can be naturally expressed as a graph or XML document, such techniques can also be used in conjunction with the distance graph representation. The advantages of such approaches are that they are specifically tailored to graph data, and can therefore determine novel insights in the underlying word-distances.
Both of the above methods have different advantages, and work well in different cases. The former provides ease in interoperability with existing text algorithms whereas the latter representation provides ease in interoperability with recently developed structural mining methods. In [1], we have presented details of methods and corresponding experimental results for problems such as clustering, classification, similarity search and plagiarism detection. We illustrate advantages both in terms of accuracy and the enablement of distance-based applications.
3. REFERENCES
[1] C. Aggarwal, P. Zhao. Graphical Models for Text: A New Paradigm for Text Representation and Processing, IBM Research Report, 2010.

900

A Survival Modeling Approach to Biomedical Search Result Diversification

Xiaoshi Yin1,2, Jimmy Xiangji Huang 2, Xiaofeng Zhou 2, Zhoujun Li 1
1School of Computer Science and Technology, Beihang University, Beijing, China. 2School of Information Technology, York University, Toronto, Canada.
xiaoshiyin@cse.buaa.edu.cn; jhuang@yorku.ca; lizj@buaa.edu.cn

ABSTRACT
In this paper, we propose a probabilistic survival model derived from the survival analysis theory for measuring aspect novelty. The retrieved documents' query-relevance and novelty are combined at the aspect level for re-ranking. Experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval. Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage & Retrieval]: Information Search & Retrieval General Terms: Performance, Experimentation
Keywords: Survival Modeling, Diversity, Biomedical IR
1. INTRODUCTION
In the biomedical domain, the desired information of a question (query) asked by biologists usually is a list of a certain type of entities covering different aspects that are related to the question [6], such as genes, proteins, diseases, mutations, etc. Hence it is important for a biomedical information retrieval (IR) system to provide comprehensive and diverse answers to fulfill biologists' information needs. In the TREC 2006 and 2007 Genomics tracks, the "aspect retrieval" was investigated. Its purpose was to study how a biomedical IR system can support a user gather information about the different aspects of a topic. Aspects of a retrieved passage could be a list of named entities or MeSH terms, representing answers that cover different portions of a full answer to the query. Aspect Mean Average Precision (Aspect MAP) was defined in the Genomics tracks to capture similarities and differences among retrieved passages. It is a measurement for diversity of the IR ranked list [5].
The Genomics aspect retrieval was firstly proposed in the TREC 2006 Genomics track and further investigated in the 2007 Genomics track. However, to the best of our knowledge, there is not too much previous work conducted on the Genomics aspect retrieval for promoting diversity in the ranked list. University of Wisconsin re-ranked the retrieved passages using a clustering-based approach named GRASSHOPPER to promote ranking diversity [4]. Unfortunately, for the Genomics aspect retrieval, this re-ranking method hurt their system's performance and decreased the Aspect MAP of the original results [4]. Later in the TREC 2007 Genomics track, most teams tried to obtain the aspect
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

level performance through their passage level results, instead of working on the aspect level retrieval directly [3, 6].
In this paper, we first propose a survival modeling approach to measuring the novel information provided by an aspect with respect to its occurrences. Then, the relevance and novelty of a retrieved document are combined at the aspect level. Evaluation results show that the proposed approach is effective in biomedical search result diversification.
2. SURVIVAL MODELING AND ANALYSIS FOR MEASURING NOVELTY
Survival analysis is a statistical methodology used for modeling and evaluating survival data, also called time-to-event data, where one is interested in the occurrence of events [2]. Survival time refers to a variable which measures the time from a particular starting time to a particular endpoint of interest. Events are usually referred as birth, death and failure that happen to an individual in the context of study. For example, in clinical trial, one may interested in the number of days that patient can survive in the study of the effectiveness of a new treatment for a disease. Formally, the survival function is defined as:
S(t) = P r(surviving longer than time t) (1)
= P r(T > t)
where t is a specific time, T is a random variable denoting the time of death, and "Pr" stands for probability. That is, the survival function gives the probability that the time of death is later than a specified time t. The survival function must be non-increasing: S(u)  S(t) if u > t. Usually one assumes S(0) = 1, that is, at the start of the study, the probability of surviving past time zero is one. The survival function is also assumed to approach zero as t increases without bound [2].
In the context of information retrieval, aspects covered by a document can be considered as treatments, a document can be considered as a patient in the clinical trial case. The number of times that an aspect has been observed can be considered as the survival time. The new information that can be provided by an aspect corresponds to the effectiveness of a treatment. One can expect that, in a ranked list, as the number of times that an aspect has been observed increases, the new information that this aspect can provide to a document decreases. For example, in a ranked document list, when aspect "stroke treatment" is observed in the jth document at the first time, the information provided by this aspect should be counted as completely new. We presume that "stroke treatment" in the jth document covers the

901

topic of "medications taken by mouth for long-term stroke

Aspect mean average precision (MAP), since our objective

treatment". Then, when aspect "stroke treatment" is ob-

is to promote diversity in the IR ranked list.

served again in the kth (k > j) document of the ranked list,

Evaluation results of using the proposed approach for doc-

it may provide new information about "injection for short-

ument re-ranking are shown in Table 1. The values in the

term stroke treatment", but it is also possible that it only

parentheses are the relative rates of improvement over the

provides redundant information about "medications taken

original results. As we can see, our approach achieves promis-

by mouth for long-term stroke treatment". As we can see,

ing and consistent performance improvements over all base-

this situation satisfies the properties of the survival function

line runs. Performance improvements can be observed on

described above.

both levels of evaluation measures. It is worth mention-

We assume that the occurrences of an aspect follow Pois-

ing that our approach can further improve the best result

son distribution. Then, the survival model derived from

(NLMinter) reported in the TREC 2007 Genomics track by

Equation 1 can be formally written as:

x
Saj (x) = P r(X > x) = 1 - e-
i=0

i i!

(2)

where  is the rate parameter of Poisson distribution. The

achieving 18.9% improvement on Aspect MAP and 11% improvement on Passage2 MAP.

on 2007's topics

MAP

Aspect Passage2

on 2006's topics MAP Aspect Passage2

value of Saj (x) states the probability of obtaining new information from aspect aj(j = 1, 2, ..., n; where n is the number
of observed aspects) after it has been observed x times. Note

NLMinter 0.2631 SvvModel 0.3117
(+18.5%) MuMshFd 0.2068

0.1148 0.1270 (+10.6%) 0.0895

Okapi06a 0.2176 SvvModel 0.2379
(+9.3%) Okapi06b 0.3147

0.0450 0.0472 (+4.9%) 0.0968

that in this paper, the aspects covered by a retrieved document are presented by concepts detected from Wikipedia [9].

SvvModel 0.2432 (+17.6%)
Okapi07 0.1428

0.0926 SvvModel 0.3236

(+3.5%)

(+2.8%)

0.0641 Okapi06c 0.2596

0.1009 (+4.2%)
0.0601

SvvModel 0.1660

0.0669 SvvModel 0.2697

0.0624

3. COMBINING NOVELTY AND RELEVANCE

(+16.2%) (+4.4%)

(+3.9%) (+3.8%)

For retrieved documents, the document rankings should depend on which documents the user has already seen. Suppose that we have ranked top i - 1 documents, and now we need to decide which document should be ranked at the ith position in the ranking list. The document that can deliver the most new and relevant aspects should be considered as the ith document in the ranking list. Assume that aspect novelty and aspect query-relevance are independent of each other. Then given previous ranked i - 1 documents, we rank the ith document using the following scoring function:

score(di; d1, ..., di-1) = P (N ew and Rel|di)

=

P (N ew and Rel|aj )P (aj )

aj Adi



P (N ew|aj )P (aj |Rel)

(3)

aj Adi

where aj is an aspect detected from document di, which follows Poisson distribution with an estimated rate parameter. P (N ew and Rel|aj) denotes the probability that aj is query-relevant and can provide new information as well.
P (N ew|aj) in Equation 3 states the probability of obtaining new information from aspect aj, which can be calculated using the survival models proposed in Section 2. P (N ew|aj) = 1 when i = 1. Since we do not usually have relevance information, P (aj|Rel) is unavailable. One possible solution, as introduced in [7], is to consider that
the best bet is to relate the probability of aspect aj to the conditional probability of observing aj given the query: P (aj|Rel)  P (aj|Q). P (aj|Q) can be calculated by the two-stage model presented in [9].

4. EXPERIMENTAL RESULTS
We conduct a series of experiments to evaluate the effectiveness of the proposed model on the TREC 2006 and 2007 Genomics collections. For the 2007's topics, three baseline runs are used, which are NLMinter [3], MuMshFd [8] and an Okapi run [1]. NLMinter and MuMshFd were two of the most competitive IR runs submitted to the TREC 2007 Genomics track. For 2006's topics, we test our approach on three Okapi runs. In this paper, we mainly focus on the

Table 1: Re-ranking Performance on Genomics topics
5. CONCLUSIONS
In this paper, we propose a survival modeling approach to promoting ranking diversity for biomedical information retrieval. The probabilistic survival model derived from the survival analysis theory measures the probability that novel information can be provided by an aspect with respect to its occurrences. Experimental results show that the proposed survival model can successfully capture the novel information delivered by aspects and achieve significant improvements on ranking diversity. We also show that combining the novelty and the relevance of a retrieved document at the aspect level is an effective way of promoting diversity of the ranked list, while keeping the relevance of retrieved documents. The proposed approach not only achieves promising performance improvements on the diversity based evaluation measure, but also on the relevance based evaluation measure.
Acknowledgements
This work is supported by NSERC Discovery Grant and an Early Researher Award of Ontario.
6. REFERENCES
[1] M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. William. Okapi at TREC-5. In Proc. of TREC-5, 1997.
[2] D. Cox and D. Oakes. Analysis of Survival Data. Chapman Hall. [3] D. Demner-Fushman and et. al. Combining resources to find
answers to biomedical questions. In Proc. of TREC-16, 2007. [4] A. B. Goldberg and et. al. Ranking biomedical passages for
relevance and diversity: University of Wisconsin, Madison at TREC Genomics 2006. In Proc. of TREC-15, 2006. [5] W. Hersh, A. Cohen, P. Roberts, and H. Rekapalli. TREC 2006 Genomics track overview. In Proc. of TREC-15, 2006. [6] W. Hersh, A. Cohen, L. Ruslen, and P. Roberts. TREC 2007 Genomics track overview. In Proc. of TREC-16, 2007. [7] V. Lavrenko and W. B. Croft. Relevance based language models. In Proceeding of the 24th ACM SIGIR. [8] N. Stokes and et. al. Entity-based relevance feedback for genomic list answer retrieval. In Proc. of TREC-16, 2007. [9] X. Yin, X. Huang, and Z. Li. Promoting ranking diversity for biomedical information retrieval using wikipedia. In Proc. of the 32nd ECIR, 2010.

902

Low Cost Evaluation in Information Retrieval
Ben Carterette
University of Delaware, carteret@cis.udel.edu
Evangelos Kanoulas
University of Sheffield, e.kanoulas@sheffield.ac.uk
Emine Yilmaz
Microsoft Research Cambridge, eminey@microsoft.com

Abstract:
Search corpora are growing larger and larger: over the last 10 years, the IR research community has moved from the several hundred thousand documents on the TREC disks to the tens of millions of U.S. government web pages of GOV2 to the one billion general-interest web pages in the new ClueWeb09 collection. But traditional means of acquiring relevance judgments and evaluating ­ e.g. pooling documents to calculate average precision ­ do not seem to scale well to these new large collections. They require substantially more cost in human assessments for the same reliability in evaluation; if the additional cost goes over the assessing budget, errors in evaluation are inevitable.
Some alternatives to pooling that support low-cost and reliable evaluation have recently been proposed. A number of them have already been used in TREC and other evaluation forums (TREC Million Query, Legal, Chemical, Web, Relevance Feedback Tracks, CLEF Patent IR, INEX). Evaluation via implicit user feedback (e.g. clicks) and crowdsourcing have also recently gained attention in the community. Thus it is important that the methodologies, the analysis they support, and their strengths and weaknesses are well-understood by the IR community. Furthermore, these approaches can support small research groups attempting to start investigating new tasks on new corpora with relatively low cost. Even groups that do not participate in TREC, CLEF, or other evaluation conferences can benefit from understanding how these methods work, how to use them, and what they mean as they build test collections for tasks they are interested in.
The goal of this tutorial is to provide attendees with a comprehensive overview of techniques to perform low cost (in terms of judgment effort) evaluation. A number of topics will be covered, including alternatives to pooling, evaluation measures robust to incomplete judgments, evaluating with no relevance judgments, statistical inference of evaluation metrics, inference of relevance judgments, query selection, techniques to test the reliability of the evaluation and reusability of the constructed collections.
The tutorial should be of interest to a wide range of attendees. Those new to the field will come away with a solid understanding of how low cost evaluation methods can be applied to construct inexpensive test collections and evaluate new IR technology, while those with intermediate knowledge will gain deeper insights and further understand the risks and gains of low cost evaluation. Attendees should have a basic
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

knowledge of the traditional evaluation framework (Cranfield) and metrics (such as average precision and nDCG), along with some basic knowledge on probability theory and statistics. More advanced concepts will be explained during the tutorial.
ACM Categories & Descriptors:
H.3.4 Information Storage and Retrieval; Performance evaluation (efficiency and effectiveness)
General Terms:
Experimentation, Measurement
Keywords:
information retrieval, evaluation, test collections
Bios
Ben Carterette is an Assistant Professor of Computer and Information Sciences at the University of Delaware, where he teaches Information Retrieval, Databases, and Artificial Intelligence. He has published extensively on constructing and using test collections for low cost. He has co-organized two SIGIR workshops on test collections that go beyond binary independent relevance judgments, and co-coordinated the TREC Million Query track from 2007­2009 as well as the TREC Session track in 2010. He is also co-organizing a SIGIR workshop on constructing the next generation of information retrieval test collections.
Evangelos Kanoulas is a postdoctoral researcher (Marie Curie fellow) in the Department of Information Studies at the University of Sheffield. He received his Ph.D. from Northeastern University. His main research interest is evaluation methods for information retrieval. He has published papers in SIGIR and CIKM. Further, Evangelos was actively involved in coordinating the Million Query Track in TREC 2007 ­ 2009 and he is one of the co-coordinators of the TREC 2010 Session Track.
Emine Yilmaz is a postdoctoral researcher in the Information Retrieval and Analysis Group at Microsoft Research Cambridge. She obtained her Ph.D. from Northeastern University. Most of her current work involves evaluation of retrieval systems, the effect of evaluation metrics on learning to rank problems and modeling user behavior. Her main interests are information retrieval and applications of information theory, statistics and machine learning. She has previously published research papers at major information retrieval venues, including SIGIR, CIKM and served as one of the organizers of the ICTIR Conference in 2009. She is also one of the organizers of a SIGIR workshop on crowdsourcing.

903

904

Learning to Rank for Information Retrieval
Tie-Yan Liu,
Microsoft Research Asia, tyliu@microsoft.com

Abstract: This tutorial is concerned with a comprehensive introduction to the research area of learning to rank for information retrieval. In the first part of the tutorial, we will introduce three major approaches to learning to rank, i.e., the pointwise, pairwise, and listwise approaches, analyze the relationship between the loss functions used in these approaches and the widely-used IR evaluation measures, evaluate the performance of these approaches on the LETOR benchmark datasets, and demonstrate how to use these approaches to solve real ranking applications. In the second part of the tutorial, we will discuss some advanced topics regarding learning to rank, such as relational ranking, diverse ranking, semi-supervised ranking, transfer ranking, query-dependent ranking, and training data preprocessing. In the third part, we will briefly mention the recent advances in statistical learning theory for ranking, which explain the generalization ability and statistical consistency of different ranking methods. In the last part, we will conclude the tutorial and show several future research directions.
ACM Categories & Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models; I.2.6 [Learning]: Parameter learning.

General Terms: Algorithms, Experimentation, Theory.
Keywords: Learning to Rank, Information Retrieval, Learning Theory, Ranking Models.
Bio: Tie-Yan Liu is a lead researcher at Microsoft Research Asia. He leads a team working on learning to rank for information retrieval, and large-scale graph learning. So far, he has more than 70 quality papers published in referred conferences and journals and over 50 filed US / international patents or pending applications. He is the co-author of the Best Student Paper for SIGIR 2008, and the Most Cited Paper for the Journal of Visual Communication and Image Representation (2004~2006). He is a Program Committee Co-Chair of RIAO (2010), an Area Chair of SIGIR (2008~2010) and AIRS (2009~2010), a Co-Chair of SIGIR workshop on learning to rank for IR (2007~2009), a Co-Chair of ICML workshop on learning to rank (2010), and a Program Committee member of many other international conferences such as WWW, ICML, KDD, WSDM, and ACL. He is on the Editorial Board of the Information Retrieval Journal (IRJ), and is a guest editor of the special issue on learning to rank of IRJ. He has given tutorials on learning to rank at several conferences including SIGIR 2008, WWW 2008, and WWW 2009.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
904

Estimating the Query Difficulty for Information Retrieval
David Carmel, Elad Yom-Tov
IBM Research lab in Haifa, {carmel,yomtov}@il.ibm.com

Abstract:
Many information retrieval (IR) systems suffer from a radical variance in performance when responding to users' queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries is poor. Thus, it is desirable that IR systems will be able to identify "difficult" queries in order to handle them properly. Understanding why some queries are inherently more difficult than others is essential for IR, and a good answer to this important question will help search engines to reduce the variance in performance, hence better servicing their customer needs.
The high variability in query performance has driven a new research direction in the IR field on estimating the expected quality of the search results, i.e. the query difficulty, when no relevance feedback is given. Estimating the query difficulty is a significant challenge due to the numerous factors that impact retrieval performance. Many prediction methods have been proposed recently. However, as many researchers observed, the prediction quality of state-of-the-art predictors is still too low to be widely used by IR applications. The low prediction quality is due to the complexity of the task, which involves factors such as query ambiguity, missing content, and vocabulary mismatch.
The goal of this tutorial is to expose participants to the current research on query performance prediction (also known as query difficulty estimation). Participants will become familiar with states-of-the-art performance prediction methods, and with common evaluation methodologies for prediction quality. We will discuss the reasons that cause search engines to fail for some of the queries, and provide an overview of several approaches for estimating query difficulty. We then describe common methodologies for evaluating the prediction quality of those estimators, and some experiments conducted recently with their prediction quality, as measured over several TREC benchmarks. We will cover a few potential applications that can utilize query difficulty estimators by handling each query individually and selectively based on its estimated difficulty. Finally we will summarize with a discussion on open issues and challenges in the field.
ACM Categories & Descriptors:
H.3.3 [Information Search
and Retrieval]: Retrieval models

General Terms:
Algorithms, Measurement
Keywords:
Retrieval robustness, Query difficulty estimation, Performance prediction
Bio/Bios:
David Carmel is a Research Staff Member at the Information Retrieval group at IBM Haifa Research Laboratory. David earned his PhD in Computer Science from the Technion, Israel Institute of Technology in 1997. David's research is focused on search in the enterprise, query performance prediction, social search, and text mining. For several years David taught the Introduction to IR course at the CS department at Haifa University.
At IBM, David is a key contributor to IBM enterprise search offerings. David is a co-founder of the Juru search engine which provides integrated search capabilities to several IBM products, and was used as a search platform for several studies in the TREC conferences. David has published more than 60 papers in Information retrieval and Web journals and conferences, and serves in the Program Committee of many conferences (SIGIR, WWW, WSDM, CIKM, ECIR) and workshops.
Elad Yom-Tov is a Research Staff Member at the Analytics Department at the IBM Haifa Research Laboratory. The main focus of his work is research into methods for large-scale machine learning, with a recent focus on social analytics. Prior to his current position he worked at Rafael Inc., where he applied machine learning to image processing. Elad is a graduate of Tel-Aviv University (B.Sc.) and the Technion, Haifa (M.Sc. and Ph.D). He is the author (with David Stork) of the Computer Manual to accompany Pattern classification, a book and a Matlab toolbox on pattern classification.
Elad's work in Information Retrieval includes query difficulty estimation, social tagging, and novelty detection.
Both David and Elad published many papers on query performance prediction, and organized a workshop on this subject in SIGIR 2005. Their paper on learning to estimate query difficulty won the Best Paper Award at SIGIR 2005.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
911

Learning Hidden Variable Models for Blog Retrieval
Mengqiu Wang
Computer Science Department Stanford University
Stanford, CA 94305, USA
mengqiu@cs.stanford.edu

ABSTRACT
We describe probabilistic models that leverage individual blog post evidence to improve blog seed retrieval performances. Our model offers a intuitive and principled method to combine multiple posts in scoring a whole blog site by treating individual posts as hidden variables. When applied to the seed retrieval task, our model yields state-of-the-art results on the TREC 2007 Blog Distillation Task dataset.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Retrieval Models

General Terms
Design, Algorithms, Experimentation, Performance

Keywords
Learning to Rank, Passage Retrieval, Blog Retrieval

1. INTRODUCTION
In blog seed retrieval tasks, we are interested in finding blogs with relevant and recurring interests for given topics. Rather than ranking individual blog posts, whole sites are ranked (i.e. all posts within a blog). We propose two discriminatively trained probabilistic models that model individual posts as hidden variables.

2. PROBABILISTIC PASSAGE MODELS

We make a modeling assumption that given a set of topranked passages of a document, the document is relevant if any one of the passages is relevant.
The first independent model (IND) assumes that the relevance of a specific top-ranked passage si is independent of the relevance of any other passage in s. We use the logistic function to model the relevance of a passage. Our second model (RBM) takes a step further and exploit the correlations among individual passages in a Restricted Boltzmann Machine framework.

P (z = 0|s) = e-f(s)
1+e-f (s)

P (z^|s))

=

1 Z

exp(Pi<j

f

(si

,

sj

,

zi,

zj

)

+

P|s|
i=1

g(si

,

zi

))

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

where f (s) is a feature vector of the passage s, and  is
the corresponding weight vector. Z is the partition function computed by summing over all possible relevance assignments. f (si, sj, zi, zj) are passage correlation features (cosine-sim, URL overlapping) and g(si, zi) are passage relevance feature (e.g., rank, score).

3. BLOG SEED RETRIEVAL
We evaluated our models on TREC 2007 Blog Distillation Track dataset. We would first obtain top 5 ranked passages for each document using Indri's LM-based retrieval system, and then apply our model to re-rank each document. Training and testing is done by performing 5-fold cross-validation. We compare our models with four strong baselines. The first two are the Indri language model passage and document retrieval systems (Indri-psg, Indri-doc). The third one is the CMU system, which gives the best performance in TREC 2007 and 2008 evaluations [1]. The last one is the ReDDE federated search algorithm used in [2]. Our IND model showed significant improvements over the Indri passage and document retrieval baselines (58.5% and 9.4% relative improvements). The RBM model gained a small improvement over the IND model, and significantly outperformed the baseline CMU and ReDDE models.

Baseline

This work

Indri-psg 0.2267

IND

0.3596

Indri-doc 0.3284

RBM

0.3702

CMU 0.3385 RBM+cosine sim 0.3779

ReDDE 0.3150 RBM+url 0.3685

4. CONCLUSIONS
In this paper, we introduced two probabilistic models that model individual blog posts as hidden variables for blog seed retrieval tasks. Our models produced state-of-the-art results on TREC 2007 Blog Distillation dataset.
5. REFERENCES
[1] J. Elsas, J. Arguello, J. Callan, and J. Carbonell. Retrieval and feedback models for blog distillation. In Proceedings of TREC, 2007.
[2] J. Elsas, J. Arguello, J. Callan, and J. Carbonell. Retrieval and feedback models for blog feed search. In Proceedings of SIGIR, 2008.

922

Investigation on Smoothing and Aggregation Methods in Blog Retrieval

Mostafa Keikha
Faculty of Informatics, University of Lugano Lugano, Switzerland
mostafa.keikha@usi.ch

ABSTRACT
Recently, user generated data is growing rapidly and becoming one of the most important source of information in the web. Blogosphere (the collection of blogs on the web) is one of the main source of information in this category. In my work for my PhD, I mainly focussed on the blog distillation task which is: given a user query find the blogs that are most related to the query topic [3].
There are some properties of blogs that make blog analysis different from usual text analysis. One of these properties is related to the time stamp assigned to each post; it is possible that the topics of a blog change over the time and this can affect blog relevance to the query. Also each post in a blog can have viewer generated comments that can change the relevance of the blog to the query if these are considered as part of the content of the blog. Another property is related to the meaning of the links between blogs which are different than links between websites. Finally, blog distillation is different from traditional ad-hoc search since the retrieval unit is a blog (a collection of posts), instead of a single document. With this view, blog distillation is similar to the task of resource selection in federated search [1].
Researchers have applied different methods from similar problems to blog distillation like ad-hoc search methods, expert search algorithms or methods from resource selection in distributed information retrieval.
Based on our preliminary experiments, I decided to divide the blog distillation problem into two sub-problems. First of all, I want to use mentioned properties of blogs to retrieve the most relevant posts for a given query. This part is very similar to the ad hoc retrieval. After that, I want to aggregate relevance of posts in each blog and calculate relevance of the blog. This part requires the development of a crossmodal aggregation model that combines the different blog relevance clues found in the blogosphere.
We use structure based smoothing methods for improving posts retrieval. The idea behind these smoothing methods is to change the score of a document based on the score of its similar or related documents. We model the blogosphere as a single graph that represents relations between posts and terms [2]. The idea is that in accordance with the Clustering Hypothesis, related documents should have similar scores for the same query. To model the relatedness between posts, we
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland ACM 978-1-60558-896-4/10/07.

define a new measure which takes into account both content similarity and temporal distance.
In more recent work, in the aggregation part of the problem, we model each post as evidence about relevance of a blog to the query, and use aggregation methods like Ordered Weighted Averaging operators to combine the evidence. The ordered weighted averaging operator, commonly called OWA operator, was introduced by Yager [4]. OWA provides a parametrized class of mean type aggregation operators, that can generate OR operator (M ax), AN D operator (M in) and any other aggregation operator between them.
For the next steps, I'm thinking about capturing the temporal properties of the blogs. Bloggers can change their interests over the time or write about different topics periodically. Capturing these changes and using them in the retrieval is one the future woks that I'm interested in. Also, studying the relations between blogs and news and their effect on each other is an interesting problem.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms
Keywords
Blog search, Temporal analysis, User generated data.
1. REFERENCES
[1] J. L. Elsas, J. Arguello, J. Callan, and J. G. Carbonell. Retrieval and feedback models for blog feed search. In SIGIR, pages 347­354, 2008.
[2] M. Keikha, M. J. Carman, and F. Crestani. Blog distillation using random walks. In SIGIR, pages 638­639, 2009.
[3] I. Ounis, M. De Rijke, C. Macdonald, G. Mishne, and I. Soboroff. Overview of the TREC-2006 blog track. In Proceedings of TREC, pages 15­27, 2006.
[4] R. Yager. On ordered weighted averaging aggregation operators in multicriteria decision making. IEEE Trans. Syst. Man Cybern., 18(1):183­190, 1988.

923

A Joint Probabilistic Classification Model for Resource Selection


Dzung Hong , Luo Si
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
{dthong, lsi}@cs.purdue.edu

Paul Bracke, Michael Witt
Purdue University Libraries Purdue University
504 West State Street West Lafayette, IN 47907, USA
{pbracke,mwitt}@purdue.edu

Tim Juchcinski
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
tjuchcin@purdue.edu

ABSTRACT
Resource selection is an important task in Federated Search to select a small number of most relevant information sources. Current resource selection algorithms such as GlOSS, CORI, ReDDE, Geometric Average and the recent classificationbased method focus on the evidence of individual information sources to determine the relevance of available sources. Current algorithms do not model the important relationship information among individual sources. For example, an information source tends to be relevant to a user query if it is similar to another source with high probability of being relevant. This paper proposes a joint probabilistic classification model for resource selection. The model estimates the probability of relevance of information sources in a joint manner by considering both the evidence of individual sources and their relationship. An extensive set of experiments have been conducted on several datasets to demonstrate the advantage of the proposed model.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Design, Performance
Keywords
Federated Search, Resource Selection, Joint Classification
1. INTRODUCTION
Federated text search provides a unified search interface for multiple search engines of distributed text information sources. There are three major research problems in federated search as resource representation, resource selection
Vietnam Education Foundation Fellow
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

and results merging. This paper focuses on resource selection, which selects a small number of most relevant information sources to search for any particular user query.
Resource selection for federated search has been a popular research topic in the last two decades. Many methods treat each information source as a single big document and rank available sources either by using statistics from the sample documents (CORI [6]), or by building a language model for each source (Xu and Croft [27], Si and Callan [22]). Other methods such as GlOSS [12], Geometric Average [17], ReDDE [20], CRCS [18] and SUSHI [24] look further inside an information source by estimating the relevance of each document and calculate the source's score as an aggregate function of the documents that the source contains. More recent methods such as the classification-based method [1] and the work in [2] treat resource selection as a classification problem and build probabilistic models by combining multiple types of evidence of individual sources.
Existing resource selection methods judge an information source by its own characteristics, but miss an important piece of evidence, which is the relationship between available sources. In practice, we notice that relationship can be meaningful and indicative. An information source that is "similar" to another highly relevant source has a better chance of being relevant. The evidence of source relationship can be very valuable for real world federated search solutions. In particular, the resource representation (e.g., sample documents) of each information source is often limited and prevents resource selection algorithms from identifying relevant sources, while the relationship between sources can help to alleviate the problem by providing more evidence from similar sources. Our study of a real world federated search application with digital libraries also suggests that it is difficult to obtain thorough resource representation from many sources (i.e., digital libraries). For example, some sources may only provide the abstracts of its documents instead of the full texts.
This paper proposes a novel probabilistic discriminative model for resource selection that explicitly models the relationship between information sources. In particular, the new research combines both the evidence of individual sources and the relationship evidence between sources into a single probabilistic model for estimating the joint probability of relevance of a set of sources. Different similarity metrics have been studied to explore the relationship between information sources. An extensive set of experiments have been conducted on two TREC testbeds for federated search

98

research and one real world application for searching digital libraries. The experiment results demonstrate the effectiveness and robustness of the proposed resource selection algorithm with the joint classification model.
The rest of this paper is organized as follows: the next section discusses the related research work. Section 3 presents the classification model for resource selection. Section 4 proposes the joint classification model. Section 5 discusses experimental methodology. Section 6 presents experimental results and related discussions. Section 7 concludes and points out some future research work.
2. RELATED WORK
There has been considerable research on all of the three subtasks of federated search as resource representation, resource selection and results merging. Since this paper focuses on the resource selection task, we mainly survey most related prior research work in resource selection and briefly talk about resource representation and results merging.
The first step in federated search is to obtain representative resource descriptions from available information sources. The START protocol [11] provides accurate information in collaborative federated search environments, but it does not work for uncooperative environments. On other side, the query-based sampling technique [4] has been widely used in federated search to obtain sample documents from each source by issuing randomly generated queries. In particular, the query-based sampling approach is used in this work to acquire sample documents from available sources. After that, all sample documents are merged together as a centralized sample database.
Resource selection selects a small set of most relevant sources for each user query [3][8][13]. Most early resource selection algorithms treat each individual source as a single big document which they extract summary statistics from. Those big document methods such as KL [27], CORI [6], and CVV [28] utilize different types of summary statistics of sources and finally rank available sources by matching the statistics with the user's query. These methods ignore the boundaries of individual documents within individual sources, which limits their performance of identifying sources with a large number of relevant documents.
Some recent resource selection algorithms such as ReDDE [20], DTF [9][10], CRCS [18] and SUSHI [24] step away from treating each source as a single big document. Those algorithms often analyze individual sample documents within resource representation for ranking sources. For example, the ReDDE selection algorithm estimates the distribution of relevant documents by treating top-ranked sample documents as a representative subset of relevant documents in available sources. Related algorithms such as UUM [22], RUM [23] and CRCS [18] have been proposed, which use different methods to weight top-ranked documents and estimate the probability of relevance.
More recent resource selection algorithms such as the classification-based resource selection in federated search [1] or vertical search [2] treat resource selection as a classification problem. A classification model can be learned from a set of training queries and is used to predict the relevance of a source for test queries. It has been shown [1] that the classification approach can outperform state-of-the-art resource selection algorithms like ReDDE.
Existing resource selection methods utilize evidence within

individual sources to judge their relevance but ignore the evidence of the relationship between available sources. However, the relationship evidence is a valuable piece of information, which promises to improve the accuracy of resource selection.
Two other related research work in [25][7] learn from the results of past queries for resource selection. However, these two methods do not model the relationship between information sources and do not use formal models based on classification.
The last step of federated search is results merging, which merges returned documents from selected sources into a single list. The most effective method is to download and recalculate scores for all returned documents within a centralized retrieval model, but this is often inefficient. More efficient methods such as the CORI merging formula, the SSL [21] and the SAFE merging algorithms [19] try to approximate the results of centralized retrieval in different ways.
3. CLASSIFICATION MODEL
3.1 Classification Approach
Many resource selection algorithms are unsupervised and provide one source of evidence. To combine different evidence in a unified framework, one needs a training dataset, usually in the form of binary judgments on sources. Specifically, given a set of sources C and a set of training queries Q, the objective is to find a mapping F of the form
F : Q × C  {+1, -1}
where +1 indicates the relevance between the query and the source, and -1 indicates irrelevance.
Arguello et al.[1] have proposed a method to construct those judgments. Each query q  Q will be issued to a fulldataset index for searching. A source Ci  C is considered to be relevant with q if more than  documents from Ci are present in top T of the full-dataset result. Otherwise, it is marked as irrelevant.
While this method can produce a rank list that mimics the rank list produced by a full-dataset retrieval, it is difficult to apply in a real world environment because of the absence of a full-dataset. We propose an alternative method that could be more feasible. A query q is now issued to each remote source Ci and we only count their returned documents that are relevant. Top T documents from each source will be inspected, then a source is marked as relevant if it has more than  relevant documents presenting in that list. In our work, we set T = 100. For dataset with a large average number of relevant documents per query (over 100), we set  = 3; otherwise  is equal to 1.
3.2 Sources of Evidence
This section presents different types of evidence of individual sources for building our classification model.
3.2.1 Big Document
Big Document (BIGDOC) approach treats each information source as a big document that contains all of its sample documents. A query is then issued to an index which contains a set of big documents, each representing one source. Sources are then ranked by how their merged sample documents match the query. The disadvantage of this method is that it does not take into account the variation of sources'

99

sizes. Assuming that the sampling process is uniform, for a very big source, the sampling process only covers a small fraction of its documents. Therefore, it may present fewer relevant documents in the centralized sample database than a much smaller one, although the absolute number of relevant documents in the big source is higher. Without considering the sources' sizes, it would be misleading to conclude that the small source is the better choice. Nevertheless, when combined with other features, BIGDOC could have a good contribution, especially in the case that many sources contain roughly the same number of documents. While CORI (discussed in the next part) also treats each source as one document, BIGDOC approach is more flexible since it can be used with different retrieval algorithms. In our experiments, the algorithm is Indri [14]. For each pair of a query and a source, one BIGDOC feature is built from the sample documents.

3.2.2 CORI
The CORI resource selection algorithm [6] uses Bayesian Inference Network model to rank sources. The belief P (q|Ci) that a source Ci satisfies query q is the combination of multiple P (rj|Ci), the belief corresponding to each term rj of query q. CORI applies a variant of tf.idf formula to determine each P (rj|Ci) and combine them together to calculate the final belief score of each source. CORI was proven to have robust performance for resource selection. In our experiments, one CORI feature is used for each pair of a query and a source.

3.2.3 Geometric Average

In this method, a query is first issued to a centralized sample database, which was mentioned in section 2. Then, each source Ci is scored according to the geometric average query likelihood of its top K sample documents [17],

K

1 K

GAV Gq(Ci) =

P (q|dij )

j=1

where dij is the j-th sample document in the rank list of source Ci. If Ci presents less than K documents in the rank list, the product above is padded with the minimum query likelihood score.

3.2.4 Modified ReDDE & ReDDE.top
Recall that ReDDE score [20] is calculated according to :

ReDDEq (Ci )

=

Niest Nisamp



×

I (d

dRsNamp



Ci) × Pq(rel|d)

where RsNamp is the top N documents returned from searching the centralized sample database. Niest is the estimated size of source Ci, Nisamp is the sample size of Ci, and I(.) is the indicator function. The number of top returned documents, N , is equal to  × Naelslt, where Naelslt is the estimated total number of documents of all sources and  is a constant,
which is usually in the range 0.002-0.005.
ReDDE uses a step function to estimate Pq(rel|d), the probability that the document d is relevant to query q. For
all top N documents, that probability is equal to a constant.
In our experiment, we use a modified version of ReDDE,
which replaces Pq(rel|d) by P (q|d), the retrieval score of document d with respect to query q. The Indri retrieval

algorithm [14] is used for searching the centralized sample database. The modified ReDDE feature has been shown empirically better than the original ReDDE feature. There is one modified ReDDE feature for each pair of a query and a source.
ReDDE.top [1] is another variant of ReDDE. Unlike ReDDE, ReDDE.top set a specific number to N . In our experiment, we add another two ReDDE.top features with N = 100 and N = 1000 respectively.

4. JOINT CLASSIFICATION MODEL

4.1 Probabilistic Discriminative Model
We propose a novel joint probabilistic model for the resource selection task. First of all, a logistic model is built to combine all the features of individual sources. We refer to this model as the independent model (Ind).
Let v = {v1, ..., vn} be the relevance vector. vi = 1 indicates that the i-th source is relevant, otherwise vi = 0. The relevance probability of a source ci given its feature vector f(ci) is calculated as:

P (vi

=

1|ci)

=

1

exp(f(ci) · ) + exp(f(ci) · )

where  denotes the combination weight vector. For simplicity, the vector f(ci) contains the bias feature (which is 1 for every pair of a query and a source) and the weight vector
contains the bias element 0. The conditional probability of v given n sources is:

P (v|c)

=

1 Z

n exp

log P

(vi

=

1|ci)vi P (vi

=

0|ci)1-vi

i

where Z is the normalizing constant. Our joint classification model (Jnt) expands the above
formula with a new term to model the relationship between sources. The conditional probability of v given n sources is now:

P (v|c)

=

1 Z

n exp

log P (vi

=

1|ci)vi P (vi

=

0|ci)1-vi

+

|v|

i


sim(ci, cj )vivj

i,j(i<j)

which can be rewritten as:

P (v|c)

=

1 Z

n exp

(1

-

vi

)(f(ci)

·

)

-

log(1

+

exp(f(ci)

·

))

+

|v|

i


sim(ci, cj )vivj

i,j(i<j)

where sim(ci, cj) denotes the similarity between two sources ci and cj, and Z is another normalizing constant.
The parameter  controls the influence of similarity. If || is high, the model tends to promote only similar (or dissimilar) sources. When  = 0, we get back to the independent model.
In the learning step, we learn the feature weight vector  from the independent model by using logistic regression. This vector is then used in the joint model. Learning , however, is generally intractable. One can see that the space of vector v is 2n, and so inferencing and estimation become

100

impossible when n is large. We resolve this issue by first ranking the sources using the independent model, and then apply the joint classification model only to the top K = 10 sources. This is equivalent to reranking the top K sources.
From the set of training queries, we use maximum loglikelihood estimation to learn the parameter . Because there is no closed-form solution for the maximum of this loglikelihood function, gradient search method is used instead.
In the prediction step, for a test query, the score of each source ci is assigned by its probability of being relevant:
R(ci) = P (vi = 1|c) = P (v1, v2, ..., vi = 1, ..., vn|c)
v\vi
where v\vi denotes the set of variables in v with variable vi omitted. In practice, the summation is taken over K - 1 variables and so is feasible when K is small. After that, the top K sources will be reranked according to the new score.
4.2 Similarity Metrics

4.2.1 Similarity Metric based-on Evaluation
Given a set of training queries, the similarity between two sources can be measured by looking at the set of queries for wich they are both relevant. The bigger that set is, the more related they are. Specifically, we apply a cross-product formula to measure this metric:
SM E(ci, cj) = rel(ci, q)rel(cj, q)
qQ
where Q is the set of training queries, rel(ci, q) is equal to 1 if source ci is relevant to query q based on the classification approach described above, otherwise it is 0. This method is called Similarity Metric based-on Evaluation (SME).

4.2.2 Similarity Metric based-on Query-specific
Evaluation
One issue with the SME is that it is independent of the query. A source may be highly related with another source with respect to a query but unrelated with that source with respect to another query. Therefore, it is better to incorporate the similarity between queries into this formula. By extending the above SME, we derive another metric called Similarity Metric based on Query-specific Evaluation (SMQE).



SM QEq(ci, cj ) =

sim(q, q)rel(ci, q)rel(cj , q)

q Q

where sim(q, q) denotes the correlation (or similarity) between the test query q and a training query q. There are many studies that explore the topicality or classification of queries, however, in this paper, we choose one simple approach. A query in consideration is issued to the centralized sample database, and the number of documents from each source that appear in top M documents of the result is recorded. In our work, M is equal to 100. The correlation between two queries is derived by a cosine-like formula:

sim(q, q)

=




i

numdoc(q,

ci)numdoc(q



,

ci

)

i numdoc(q, ci)2 i numdoc(q, ci)2

where numdoc(q, ci) is the number of documents of source ci that appear in the top M documents returned from query q.

Both the SME and SMQE metrics can be modified in many ways. First of all, the term rel(i, q) can be represented either by a binary number or the absolute number of relevant documents. Or we can set different thresholds to the searching on the centralized sample database. Another choice is to normalize the relevance vector. However, in our experiments, those changes do not have much effect on the results. In fact, SMQE provides the best result, proving that it better reflects the relationship between sources.
4.2.3 Similarity-Metric based-on Kullback-Leibler
divergence
This method tries to reveal the similarity between sources by looking at their own vocabularies. Specifically, a language model [16] is built for each sample source. Then we calculate the Kullback-Leibler divergence between those two language models. Recall that the Kullback-Leibler divergence is actually the distance between two probabilistic models, which is the inverse of their similarity. However, because our model can adapt this change by inferring a negative similarity coefficient , we keep the KL-value as it is. This metric is referred to as SMKL.
5. EXPERIMENTAL METHODOLOGY
We evaluate our proposed algorithms on 3 datasets. The first two datasets are well-known TREC testbeds, the last one comes from a real world application.
· TREC123-100col-bysource (TREC123): 100 collections (information sources) were created from TREC CDs 1,2 and 3 [3]. They are organized by publication source and publication date. The size of each source varies from 7,000 to 39,700 documents (see Table 1 for more statistics). This testbed comes with 100 queries (TREC topics 51-150) with judgments.
· TREC4-100col-bysource (TREC4): 100 collections were created according to the publication source of documents in TREC4 [26]. One actual publication source is distributed across a number of information sources depending on its total number of documents. Each information source has roughly 5,675 documents. This testbed comes with 50 queries (TREC topics 201250) with judgments. More statistics about both TREC123 and TREC4 are presented in Table 1.
· Digital Library (DIGLIB): This real world dataset contains 80 sources (i.e., digital libraries) that are accessible from Purdue University Libraries1. This testbed presents a heterogeneous sources of information. Each document from those sources composes of many fields. Three fields that convey rich information are the abstract, subject heading (or document's category) and full text. Not all sources provide all those three fields. A document from one source may not be provided with its full text, while a document from another source may not have the subject heading. Table 2 shows that only 65% of 80 sources provide abstracts, 65% provide subject headings, and only 30% provide full texts. In our current work, we temporally merge all those available information into one document. Future research may
1We make the dataset available as feature file at http://www.cs.purdue.edu/homes/dthong/

101

Table 1: Summary Statistics of TREC123 and TREC4

Testbed
TREC123 TREC4

Size (GB)
3.2 2.0

Number of Documents (x1000) Min Avg Max 0.7 10.8 39.7 5.6 5.6 5.6

Size(MB)
Min Avg Max 28 32 42 4 20 138

Rk

=

k ki=1
i=1

Ei Bi

Let Ei denote the number of relevant documents of the i-th source according to the ranking E, Bi denote the same thing with respect to ranking B. We also report the results at P @{1, 3, 5, 10} as in source level accuracy.

Table 2: Statistical Information about DIGLIB: Number of Sources Corresponding to their Available Information Fields
Abstract Subject Full Text Number of Sources 52(65%) 52(65%) 24(30%)

· Document Level, High Precision: To make it independent from result merging algorithm, we use fulldataset retrieval as our merging method. The chosen retrieval algorithm is Inquery in Lemur Toolkit [5]. For each query, top 5 sources from the joint classification rank list are selected for this step. Documents not from selected sources are filtered out from the full-dataset rank list. The remaining list is checked by their precision at P @{5, 10, 15, 20, 30} accordingly. We also report a full-dataset precision which includes all sources.

consider to treat each source differently according the type of information that is available.
We also build a set of 100 queries, some of them are extracted from the library log, which are real queries. For each pair of a query and a source, we manually assign a binary value to indicate their relevance.

On TREC123 and TREC4, all tests at different levels are presented. On DIGLIB dataset, we only report the results at source level because the document judgments are difficult to make as many sources do not provide their full text information. In most of the experiments, SMQE is used as our default similarity metric. However, in section 6.4, we also discuss the experimental results with different other metrics.

A note on resource specific retrieval algorithm: DIGLIB is a real world application of digital libraries, each of its sources implements a different retrieval algorithm, which is not known. We can only access those sources through a unified interface. For TREC123 and TREC4, we assign one retrieval algorithm to each source in a round-robin manner. The set of assigned algorithms is Inquery, Language Model and Vector Space (tf.idf). These algorithms influence the query-based sampling process, as well as the classification process. A less effective retrieval algorithm like Vector Space model may reduce a source's chance of being marked as relevant.
For each testbed, we repeat every experiment 5 times. In each trial, we randomly select 50% of the queries as training set, and test on the other 50%. All the results shown in the next section are averaged over 5 trials.
Each source is sampled with 300 documents. We also compare the main results with 100 sample documents. The experiments are measured on several levels:
· Source Level (Resource Selection), Accuracy: This level measures the precision of the resource selection algorithms. Top sources (i.e., top 10) are judged by their precisions at different levels. The judgments come from the classification method, as described in section 3.1. We report the results at P @{1, 3, 5, 10} accordingly.
· Source Level, Recall Metric (R-metric): This metric is widely used in comparing resource selection algorithms [3]. Let E denote the ranking produced by a resource selection algorithm, B denote the based line ranking, in this case, the Relevance-Based Ranking. At level k, the R-metric is defined as:

6. EXPERIMENTAL RESULTS
In all of our experiments, we use paired t-test on queries to check significance. A  denotes a significance on p < 0.1 level;  corresponds to p < 0.05 level and  corresponds to p < 0.01 level.
6.1 TREC123 & TREC4
First of all, we compare the joint classification model with the independent model on the two TREC testbeds. Table 3 represents the source level results in accuracy on TREC123 and TREC4. The second column of each dataset is the joint classification model. Numbers in parentheses show the relative improvement of the joint classification model (denoted as "Jnt") over the independent model (denoted as "Ind").
Table 4 shows the R-metric comparison between the independent model and joint classification model. Table 5 shows the high precision at document level. We also report the full centralized retrieval, which includes all sources. This is denoted as the "Full" column in the table.
It can be seen that joint classification model always leads to better results than independent model, as it shows in all three tables. Both models have the same source level accuracy and R-metric values at top 10 because of the fact that we rerank the top 10 sources. The results are more statistically significant on TREC123 than on TREC4. This can be explained as in TREC123, we have trained on 50 queries; whereas in TREC4, we use only 25 queries out of 50 for training.
6.2 Digital Library
The result at source level of Digital Library is reported in Table 6. In this real world dataset, the joint classification model significantly outperforms the independent model. This accounts to the fact that many sources only provide

102

Table 3: Source Level Results in Accuracy on TREC123 & TREC4 with 300 Sample Documents

Table 5: Document Level Results in High Precision on TREC123 & TREC4 with 300 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.512 0.456 0.451 0.439

Jnt 0.524(2.3%) 0.499(9.4%) 0.484(7.3%)
0.439(0%)

TREC4

Ind 0.480 0.451 0.430 0.414

Jnt 0.536(11.7%) 0.475(5.3%) 0.446(3.7%)
0.414(0%)

Docs Rank
@5 @10 @15 @20 @30

Full 0.446 0.444 0.435 0.430 0.414

TREC123

Ind 0.392 0.355 0.332 0.309 0.280

Jnt 0.410(4.6%) 0.360(1.4%) 0.347(4.5%) 0.326(5.5%) 0.300(7.1%)

Table 4: Source Level Results in R-metric on TREC123 & TREC4 with 300 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.262 0.309 0.354 0.426

Jnt 0.319(21.8%) 0.364(17.8%) 0.400(13.0%)
0.426(0%)

TREC4

Ind 0.287 0.324 0.343 0.414

Jnt 0.309(7.7%) 0.340(4.9%) 0.355(3.5%)
0.414(0%)

Docs Rank
@5 @10 @15 @20 @30

Full 0.549 0.459 0.422 0.384 0.354

TREC4

Ind 0.282 0.238 0.209 0.186 0.167

Jnt 0.290(2.8%) 0.254(6.7%) 0.224(7.2%) 0.200(7.5%) 0.170(1.8%)

partial information about themselves. This also shows that the joint classification model can alleviate the problem of missing information.
6.3 Tests with Different Sample Sizes
We conduct experiments on three datasets with only 100 documents sampled from each source. This test is to show the robustness of the model, as well as the effect of sampling size on the results. The results of source level (both in accuracy and R-metric) and document level are reported for TREC123 and TREC4 (Table 7, Table 8 and Table 9 respectively), while only source level is reported for DIGLIB (Table 10).
The sample size clearly affects TREC123. Its performance of the independent model drops significantly. However, this also leaves room for joint classification model to show its effectiveness: the accuracy on source level is statistically more significant. On document level, the improvement is a bit weaker. This can be explained as the initial choice of top 10 sources from the independent model is less precise, so is the joint classification model, which uses the initial ranking list directly.
Most results on TREC4 from Table 7 to Table 9 indicate the advantage of the joint classification model against independent model with a small number of sample documents, although the difference is smaller than TREC123 due to the limited amount of training information.
The results on DIGLIB (Table 10) are also consistent. The performances of both resource selection algorithms drop with 100 sample documents. However, the results of the joint classification method are still significantly better than those of the independent method.
6.4 Test with Different Similarity Metrics
We conduct tests on three testbeds with different similarity metrics discussed in Section 4.2. Figure 1 shows the

Table 6: Source Level Results in Accuracy on DIGLIB with 300 Sample Documents

Src Rank
@1 @3 @5 @10

DIGLIB

Ind 0.552 0.460 0.419 0.356

Jnt 0.640(15.9%) 0.536(16.5%) 0.487(16.2%)
0.356(0%)

Table 7: Source Level Results in Accuracy on TREC123 & TREC4 with 100 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.320 0.299 0.318 0.319

Jnt 0.380(18.8%) 0.373(24.7%) 0.357(12.3%)
0.319(0%)

TREC4

Ind
0.496 0.405 0.379 0.367

Jnt 0.480(-3.2%) 0.411(1.5%) 0.403(6.3%)
0.367(0%)

Table 8: Source Level Results in R-metric on TREC123 & TREC4 with 100 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.183 0.214 0.244 0.311

Jnt 0.233(27.3%) 0.262(22.4%) 0.279(14.3%)
0.311(0%)

TREC4

Ind 0.278 0.264 0.293 0.341

Jnt 0.317(14%) 0.293(11%) 0.311(6.1%)
0.341(0%)

103

Table 9: Document Level Results in High Precision on TREC123 & TREC4 with 100 Sample Documents

TREC123

0.3

0.42

SMQE

SME

0.4

Independent

0.28

SMKL

TREC4
SMQE SME Independent SMKL

Docs Rank
@5 @10 @15 @20 @30

TREC123

Ind 0.328 0.302 0.288 0.277 0.253

Jnt 0.329(0.3%) 0.316(4.6%) 0.306(6.2%) 0.296(6.9%) 0.268(5.9%)

TREC4

Ind 0.283 0.243 0.223 0.195 0.165

Jnt 0.301(6.4%) 0.254(4.5%) 0.227(1.8%) 0.204(4.6%) 0.166(0.6%)

Table 10: Source Level Results in Accuracy of DIGLIB with 100 Sample Documents

Precision Precision

0.38 0.36 0.34 0.32
0.3 0.28 0.26
0

10

20

Document Rank

0.26

0.24

0.22

0.2

0.18

0.16

30

0

10

20

30

Document Rank

Src Rank
@1 @3 @5 @10

DIGLIB

Ind 0.436 0.383 0.375 0.318

Jnt 0.620(42.2%) 0.531(38.6%) 0.474(26.4%)
0.318(0%)

results of TREC123 and TREC4 at document level. From this figure, we notice that the SMQE method outperforms all other metrics, due to the fact the it considers the similarity between queries. The SME produces a quite close-to-best result, but the SMKL tends not to be a good choice for the joint classification model. On TREC4, SMKL is comparable with independent model, but it is worse than SMQE and SME.
Figure 2 shows the results of DIGLIB at source level. In this case, both SMQE and SME are comparable, except for the precision at top 1. Again SMKL is not a good choice.

Figure 1: Document Level High Precision on TREC123 & TREC4 with Different Similarity Metrics

Precision

DIGLIB

0.65

SMQE

SME

Independent

0.6

SMKL

0.55

0.5

0.45

0.4

0.35

0

2

4

6

8

10

Source Rank

7. CONCLUSION & FUTURE WORK
This paper proposes a novel joint probabilistic classification model for the resource selection task in federated text search. Existing resource selection algorithms only utilize evidence of individual information sources to select relevant sources, but they do not model the valuable relationship information between the sources. The proposed algorithm estimates the probability of relevance of information sources in a joint manner by combining both the evidence of individual sources and the relationship between the sources. The importance of different types of evidence is determined in a discriminative manner for maximizing the accuracy of resource selection with some training queries. Different types of similarity metrics have been explored to model source similarity based on the performance of available sources on training queries and the Kullback-Leibler divergence on the contents of the sources. A set of experiments were conducted with two TREC datasets and one real world application with digital libraries. The empirical results in different configurations have demonstrated the effectiveness of the proposed joint classification model.
There are several directions to extend the research work in the paper. First, one advantage of the proposed joint probabilistic model is to integrate different types of evidence of

Figure 2: Source Level Accuracy on DIGLIB with
Different Similarity Metrics
individual sources and their relationship. We plan to explore more features for improving the performance of resource selection. For example, we can combine multiple types of similarity evidence in a single framework (with different  weights), which may better model sources' relationship for more accurate resource selection. Second, the joint model in this paper utilizes a reranking approach in resource selection with a small set of information sources (e.g., top 10) to avoid large computational complexity. It is possible to break this limit by utilizing some other approximate inference algorithms (e.g., the pseudo likelihood approach [15]) or making further assumptions on the sources' relationship. For example, one strategy is to first divide available sources into groups of closely related sources. Inference can be conducted by building a small model in each group and assuming independence of sources between different groups.
8. ACKNOWLEDGMENTS
This research was partially supported by the Vietnam Education Foundation (VEF) and the NSF grant IIS-0749462.

104

The opinions, findings, and conclusions stated herein are those of the authors and do not necessarily reflect those of the sponsors.
9. REFERENCES
[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. Proceeding of
the 18th ACM Conference on Information and Knowledge Management, 2009. [2] J. Arguello, F. D´iaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. In Proceedings of the
32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [3] J. Callan. Distributed information retrieval. Advances in Information Retrieval, pages 127­150, 2000. [4] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97­130, 2001. [5] J. Callan, W. B. Croft, and S. M. Harding. The inquery retrieval system. In Proceedings of the Third
International Conference on Database and Expert Systems Applications, 1992. [6] J. Callan, Z. Lu, and W. B. Croft. Searching distributed collection with inference networks. In
Proceedings of the 18th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval, 1995. [7] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. In Proceeding of the 18th
ACM Conference on Information and Knowledge Management. ACM, 2009. [8] N. Craswell, P. Bailey, and D. Hawking. Server selection on the world wide web. In Proceedings of the 5th ACM Conference on Digital Libraries. ACM, 2000. [9] N. Fuhr. A decision-theoretic approach to database selection in networked ir. ACM Transactions on Information Systems (TOIS), 17(3):229­249, 1999. [10] N. Fuhr. Resource discovery in distributed digital libraries. In In Digital Libraries '99: Advanced Methods and Technologies, Digital Collections, 1999. [11] L. Gravano, K. Chang, C-C., H. Garc´ia-Molina, and A. Paepcke. Starts: Stanford proposal for internet meta-searching. In Proceedings of the ACM-SIGMOD
International Conference on Management of Data (SIGMOD). ACM, 1997. [12] L. Gravano, H. Garc´ia-Molina, and A. Tomasic. Gloss: Text-source discovery over the internet. ACM Transactions on Database Systems, 24(2):229­264, 1999. [13] W. Meng, C. Yu, and K. Liu. Building efficient and effective metasearch engines. ACM Computing Surveys (CSUR), 34(1):48­89, 2002. [14] D. Metzler and W. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735­750, 2004. [15] S. Parise and M. Welling. Learning in markov random fields: An empirical study. In Joint Statistical Meeting (JSM2005), volume 4, 2005. [16] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of

the 25th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 1998. [17] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08: Proceeding of the
17th ACM Conference on Information and Knowledge Management, pages 1053­1062, New York, NY, USA, 2008. ACM. [18] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. In
Proceedings of the 29th European Conference on Information Retrieval, 2007. [19] M. Shokouhi and J. Zobel. Robust result merging using sample-based score estimates. ACM Transactions on Information Systems, 27(3):1­29, 2009. [20] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In
Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM, 2003. [21] L. Si and J. Callan. A semi-supervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457­491, 2003. [22] L. Si and J. Callan. Unified utility maximization framework for resource selection. In Proceedings of
13th ACM International Conference on Information and Knowledge Management (CIKM), 2004. [23] L. Si and J. Callan. Modeling search engine effectiveness for federated search. In Proceedings of the
28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2005. [24] P. Thomas and M. Shokouhi. Sushi: scoring scaled samples for server selection. In SIGIR '09: Proceedings
of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [25] E. Voorhees, N. K. Gupta, and B. Johnson-Laird. Learning collection fusion strategies. In Proceedings of
the 18th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 1995. [26] J. Xu and J. Callan. Effective retrieval with distributed collections. In Proceedings of the 21st
Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1998. [27] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of the 22nd
Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1999. [28] B. Yuwono and D. L. Lee. Server ranking for distributed text retrieval systems on the internet. In
Proceedings of the 5th Annual International
Conference on Database Systems for Advanced Applications, 1997.

105


C:\Users\z3696\Documents\Document-Classification\classifier\Imbalanced_Raw.py:92: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(pos_df)
C:\Users\z3696\Documents\Document-Classification\classifier\Imbalanced_Raw.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(neg_df)
                                                sentence  label
0      The Importance of Anchor Text for Ad Hoc Searc...      1
1                                                             1
2                           Marijn Koolen1 Jaap Kamps1,2      1
3      1 Archives and Information Studies, University...      1
4                            {m.h.a.koolen,kamps}@uva.nl      1
...                                                  ...    ...
48516  Jacques Savoy University of Neuchatel Africa &...      0
48517                                                         0
48518                                                 iv      0
48519                                                         0
48520                                                         0

[78229 rows x 2 columns]
(29708, 2)
(48521, 2)
[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\z3696\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\z3696\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
DF1: 31193
DF2: 34164
DF3: 37135
DF4: 40106
DF5: 40056
DF6: 38173
DF7: 36292
DF8: 34411
DF9: 32530
DF10: 30649
15596
17082
18568
20053
20028
19086
18146
17206
16265
15324
X1: (15596,)
X2: (15596,)
X3: (17082,)
X4: (17082,)
X5: (18568,)
X6: (18568,)
X7: (20053,)
X8: (20053,)
X9: (20028,)
X10: (20028,)
Y1: (15596,)
Y2: (15596,)
Y3: (17082,)
Y4: (17082,)
Y5: (18568,)
Y6: (18568,)
Y7: (20053,)
Y8: (20053,)
Y9: (15596,)
Y10: (15596,)
(15596, 64713)
(15596, 64713)
===Logistic Regression with TfidfVectorizer Imbalanced - 2010 Split 1 0.5 0.037
Logistic F1-score 13.714700368869806
Logistic ROCAUC score: 52.60158830046269
Logistic Recall score: 7.659057138997703
Logistic Precision Score: 65.51126516464471
Logistic Confusion Matrix [[23712   597]
 [13672  1134]]

Logistic Classification               precision    recall  f1-score   support

           0       0.63      0.98      0.77     24309
           1       0.66      0.08      0.14     14806

    accuracy                           0.64     39115
   macro avg       0.64      0.53      0.45     39115
weighted avg       0.64      0.64      0.53     39115


Logistic Accuracy Score 63.520388597724654
Execution Time for Logistic Regression Imbalanced:  0.11576509475708008 seconds
===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010 Split 1 0.5 0.037
DCT F1-score 21.159725882973113
DCT ROCAUC score: 52.34102929710412
DCT Recall score: 13.555315412670538
DCT Precision Score: 48.19884726224784
DCT Confusion Matrix [[22152  2157]
 [12799  2007]]

DCT Classification               precision    recall  f1-score   support

           0       0.63      0.91      0.75     24309
           1       0.48      0.14      0.21     14806

    accuracy                           0.62     39115
   macro avg       0.56      0.52      0.48     39115
weighted avg       0.58      0.62      0.54     39115


DCT Accuracy Score 61.764029144829344
Execution Time for Decision Tree Imbalanced:  0.5422077178955078 seconds
===Naive Bayes with TfidfVectorizer Imabalanced - 2010 Split 1 0.5 0.037
Naive F1-score 8.460515483503261
Naive ROCAUC score: 51.60382034122595
Naive Recall score: 4.511684452249088
Naive Precision Score: 67.81725888324873
Naive Confusion Matrix [[23992   317]
 [14138   668]]

Naive Classification               precision    recall  f1-score   support

           0       0.63      0.99      0.77     24309
           1       0.68      0.05      0.08     14806

    accuracy                           0.63     39115
   macro avg       0.65      0.52      0.43     39115
weighted avg       0.65      0.63      0.51     39115


Naive Accuracy Score 63.044867697814134
Execution Time for Naive Bayes Imbalanced:  0.04292559623718262 seconds
===XGB with TfidfVectorizer Imbalanced - 2010 Split 1 0.5 0.037
XGB F1-Score 17.75088809946714
XGB ROCAUC Score: 56.51495314088189
XGB Recall score: 49.81308411214953
XGB Precision Score: 10.799675807105228
XGB Confusion Matrix [[22698 13207]
 [ 1611  1599]]

XGB Classification               precision    recall  f1-score   support

           0       0.93      0.63      0.75     35905
           1       0.11      0.50      0.18      3210

    accuracy                           0.62     39115
   macro avg       0.52      0.57      0.47     39115
weighted avg       0.87      0.62      0.71     39115


XGB Accuracy Score 63.044867697814134
Execution Time for XGBoost Classifier Imbalanced:  1.6614222526550293 seconds
====RandomForest with Tfidf Imbalanced 2010 Split 1 0.5 0.037
RFC F1 score 19.708721611517216
RFC ROCAUC Score: 53.18910595936385
RFC Recall score: 56.69341894060995
RFC Precision Score: 11.927596920167499
RFC Confusion Matrix [[22960  1349]
 [13040  1766]]

RFC Classification               precision    recall  f1-score   support

           0       0.64      0.94      0.76     24309
           1       0.57      0.12      0.20     14806

    accuracy                           0.63     39115
   macro avg       0.60      0.53      0.48     39115
weighted avg       0.61      0.63      0.55     39115


RFC Accuracy Score 63.213600920363035
Execution Time for Random Forest Classifier Imbalanced:  200.78383111953735 seconds
Array of Prob Scores LR-Imb Test-Size 0.5 : [array([0.33122658, 0.38451019, 0.38451019, ..., 0.38451019, 0.37374142,
       0.50707199])]
Array of F1 Scores LR-Imb Test-Size: 0.5 : [0.13714700368869806]
Array of ROCAUC Scores LR-Imb: 0.5 : [0.5260158830046269]
Array of Recall Scores LR-Imb: 0.5 : [0.07659057138997703]
Array of Precision Scores LR-Imb: 0.5 : [0.6551126516464472]
Array of Accuracy Scores LR-Imb: 0.5 : [0.6352038859772465]
Array of Prob Scores DT-Imb: 0.5 : [array([0.40014577, 0.40014577, 0.40014577, ..., 0.40014577, 0.        ,
       1.        ])]
Array of F1 Scores DT-Imb: 0.5 : [0.21159725882973113]
Array of ROCAUC Scores DT-Imb: 0.5 : [0.5234102929710412]
Array of Recall Scores DT-Imb: 0.5 : [0.13555315412670538]
Array of Precision Scores DT-Imb: 0.5 : [0.4819884726224784]
Array of Accuracy Scores DT-Imb: 0.5 : [0.6176402914482935]
Array of Prob Scores NB-Imb: 0.5 : [array([0.29941999, 0.38389772, 0.38389772, ..., 0.39121631, 0.222277  ,
       0.47461368])]
Array of F1 Scores NB-Imb: 0.5 : [0.08460515483503261]
Array of ROCAUC Scores NB-Imb: 0.5 : [0.5160382034122595]
Array of Recall Scores NB-Imb: 0.5 : [0.04511684452249088]
Array of Precision Scores NB-Imb: 0.5 : [0.6781725888324873]
Array of Accuracy Scores NB-Imb: 0.5 : [0.6304486769781413]
Array of Prob Scores XG-Imb: 0.5 : [array([0.27995032, 0.38593566, 0.38593566, ..., 0.38593566, 0.638221  ,
       0.38593566], dtype=float32)]
Array of F1 Scores XG-Imb: 0.5 : [0.1775088809946714]
Array of ROCAUC Scores XG-Imb: 0.5 : [0.565149531408819]
Array of Recall Scores XG-Imb: 0.5 : [0.4981308411214953]
Array of Precision Scores XG-Imb: 0.5 : [0.10799675807105227]
Array of Accuracy Scores XG-Imb: 0.5 : [0.6211683497379522]
Array of Prob Scores RF-Imb: 0.5 : [array([0.27176573, 0.39988078, 0.39988078, ..., 0.39988078, 0.41361793,
       0.91491905])]
Array of F1 Scores RF-Imb: 0.5 : [0.19708721611517216]
Array of ROCAUC Scores RF-Imb: 0.5 : [0.5318910595936385]
Array of Recall Scores RF-Imb: 0.5 : [0.5669341894060995]
Array of Precision Scores RF-Imb: 0.5 : [0.11927596920167499]
Array of Accuracy Scores RF-Imb: 0.5 : [0.6321360092036303]
===Logistic Regression with TfidfVectorizer Imbalanced - 2010 Split 1 0.5 0.09
Logistic F1-score 22.95118674429019
Logistic ROCAUC score: 54.79406950389875
Logistic Recall score: 13.880425214977318
Logistic Precision Score: 66.23586429725363
Logistic Confusion Matrix [[23301  1045]
 [12719  2050]]

Logistic Classification               precision    recall  f1-score   support

           0       0.65      0.96      0.77     24346
           1       0.66      0.14      0.23     14769

    accuracy                           0.65     39115
   macro avg       0.65      0.55      0.50     39115
weighted avg       0.65      0.65      0.57     39115


Logistic Accuracy Score 64.8114534066215
Execution Time for Logistic Regression Imbalanced:  0.1806020736694336 seconds
===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010 Split 1 0.5 0.09
DCT F1-score 26.85562088547163
DCT ROCAUC score: 53.99791750097091
DCT Recall score: 18.091949353375313
DCT Precision Score: 52.085769980506825
DCT Confusion Matrix [[21888  2458]
 [12097  2672]]

DCT Classification               precision    recall  f1-score   support

           0       0.64      0.90      0.75     24346
           1       0.52      0.18      0.27     14769

    accuracy                           0.63     39115
   macro avg       0.58      0.54      0.51     39115
weighted avg       0.60      0.63      0.57     39115


DCT Accuracy Score 62.78921130001278
Execution Time for Decision Tree Imbalanced:  1.316521406173706 seconds
===Naive Bayes with TfidfVectorizer Imabalanced - 2010 Split 1 0.5 0.09
Naive F1-score 14.494163424124512
Naive ROCAUC score: 53.03531542300066
Naive Recall score: 8.070959442074615
Naive Precision Score: 70.99463966646815
Naive Confusion Matrix [[23859   487]
 [13577  1192]]

Naive Classification               precision    recall  f1-score   support

           0       0.64      0.98      0.77     24346
           1       0.71      0.08      0.14     14769

    accuracy                           0.64     39115
   macro avg       0.67      0.53      0.46     39115
weighted avg       0.66      0.64      0.54     39115


Naive Accuracy Score 64.04448421321743
Execution Time for Naive Bayes Imbalanced:  0.04487895965576172 seconds
===XGB with TfidfVectorizer Imbalanced - 2010 Split 1 0.5 0.09
XGB F1-Score 21.315192743764168
XGB ROCAUC Score: 61.156932255157415
XGB Recall score: 58.18236714975845
XGB Precision Score: 13.047599702078678
XGB Confusion Matrix [[22961 12842]
 [ 1385  1927]]

XGB Classification               precision    recall  f1-score   support

           0       0.94      0.64      0.76     35803
           1       0.13      0.58      0.21      3312

    accuracy                           0.64     39115
   macro avg       0.54      0.61      0.49     39115
weighted avg       0.87      0.64      0.72     39115


XGB Accuracy Score 64.04448421321743
Execution Time for XGBoost Classifier Imbalanced:  0.6487734317779541 seconds
====RandomForest with Tfidf Imbalanced 2010 Split 1 0.5 0.09
RFC F1 score 24.759032900759244
RFC ROCAUC Score: 54.69644516573607
RFC Recall score: 60.46817464492372
RFC Precision Score: 15.566389058162367
RFC Confusion Matrix [[22843  1503]
 [12470  2299]]

RFC Classification               precision    recall  f1-score   support

           0       0.65      0.94      0.77     24346
           1       0.60      0.16      0.25     14769

    accuracy                           0.64     39115
   macro avg       0.63      0.55      0.51     39115
weighted avg       0.63      0.64      0.57     39115


RFC Accuracy Score 64.27713153521667
Execution Time for Random Forest Classifier Imbalanced:  505.19520831108093 seconds
Array of Prob Scores LR-Imb Test-Size 0.5 : [array([0.33122658, 0.38451019, 0.38451019, ..., 0.38451019, 0.37374142,
       0.50707199]), array([0.38880602, 0.38880602, 0.41398308, ..., 0.3759446 , 0.3527457 ,
       0.38880602])]
Array of F1 Scores LR-Imb Test-Size: 0.5 : [0.13714700368869806, 0.22951186744290192]
Array of ROCAUC Scores LR-Imb: 0.5 : [0.5260158830046269, 0.5479406950389875]
Array of Recall Scores LR-Imb: 0.5 : [0.07659057138997703, 0.13880425214977318]
Array of Precision Scores LR-Imb: 0.5 : [0.6551126516464472, 0.6623586429725363]
Array of Accuracy Scores LR-Imb: 0.5 : [0.6352038859772465, 0.648114534066215]
Array of Prob Scores DT-Imb: 0.5 : [array([0.40014577, 0.40014577, 0.40014577, ..., 0.40014577, 0.        ,
       1.        ]), array([0.40239626, 0.40239626, 0.40239626, ..., 0.375     , 0.        ,
       0.40239626])]
Array of F1 Scores DT-Imb: 0.5 : [0.21159725882973113, 0.2685562088547163]
Array of ROCAUC Scores DT-Imb: 0.5 : [0.5234102929710412, 0.539979175009709]
Array of Recall Scores DT-Imb: 0.5 : [0.13555315412670538, 0.18091949353375314]
Array of Precision Scores DT-Imb: 0.5 : [0.4819884726224784, 0.5208576998050682]
Array of Accuracy Scores DT-Imb: 0.5 : [0.6176402914482935, 0.6278921130001278]
Array of Prob Scores NB-Imb: 0.5 : [array([0.29941999, 0.38389772, 0.38389772, ..., 0.39121631, 0.222277  ,
       0.47461368]), array([0.39048295, 0.39048295, 0.46342663, ..., 0.29796504, 0.25349534,
       0.39048295])]
Array of F1 Scores NB-Imb: 0.5 : [0.08460515483503261, 0.14494163424124512]
Array of ROCAUC Scores NB-Imb: 0.5 : [0.5160382034122595, 0.5303531542300066]
Array of Recall Scores NB-Imb: 0.5 : [0.04511684452249088, 0.08070959442074616]
Array of Precision Scores NB-Imb: 0.5 : [0.6781725888324873, 0.7099463966646814]
Array of Accuracy Scores NB-Imb: 0.5 : [0.6304486769781413, 0.6404448421321743]
Array of Prob Scores XG-Imb: 0.5 : [array([0.27995032, 0.38593566, 0.38593566, ..., 0.38593566, 0.638221  ,
       0.38593566], dtype=float32), array([0.38809112, 0.38809112, 0.38809112, ..., 0.38809112, 0.4274029 ,
       0.38809112], dtype=float32)]
Array of F1 Scores XG-Imb: 0.5 : [0.1775088809946714, 0.2131519274376417]
Array of ROCAUC Scores XG-Imb: 0.5 : [0.565149531408819, 0.6115693225515741]
Array of Recall Scores XG-Imb: 0.5 : [0.4981308411214953, 0.5818236714975845]
Array of Precision Scores XG-Imb: 0.5 : [0.10799675807105227, 0.13047599702078677]
Array of Accuracy Scores XG-Imb: 0.5 : [0.6211683497379522, 0.6362776428480122]
Array of Prob Scores RF-Imb: 0.5 : [array([0.27176573, 0.39988078, 0.39988078, ..., 0.39988078, 0.41361793,
       0.91491905]), array([0.40228974, 0.40228974, 0.409492  , ..., 0.37345138, 0.34251429,
       0.40228974])]
Array of F1 Scores RF-Imb: 0.5 : [0.19708721611517216, 0.24759032900759245]
Array of ROCAUC Scores RF-Imb: 0.5 : [0.5318910595936385, 0.5469644516573607]
Array of Recall Scores RF-Imb: 0.5 : [0.5669341894060995, 0.6046817464492372]
Array of Precision Scores RF-Imb: 0.5 : [0.11927596920167499, 0.15566389058162367]
Array of Accuracy Scores RF-Imb: 0.5 : [0.6321360092036303, 0.6427713153521667]
Iteration 1 Array:  [0.5, 0.11927596920167499, 0.10799675807105227, 0.4819884726224784, 0.6551126516464472, 0.6781725888324873]
Iteration 2 Array:  [0.5, 0.15566389058162367, 0.13047599702078677, 0.5208576998050682, 0.6623586429725363, 0.7099463966646814]
===Logistic Regression with TfidfVectorizer Imbalanced - 2010 Split 1 0.6 0.08
Logistic F1-score 19.37921469223779
Logistic ROCAUC score: 53.90210390631697
Logistic Recall score: 11.348868474120547
Logistic Precision Score: 66.27412495911024
Logistic Confusion Matrix [[28055  1031]
 [15826  2026]]

Logistic Classification               precision    recall  f1-score   support

           0       0.64      0.96      0.77     29086
           1       0.66      0.11      0.19     17852

    accuracy                           0.64     46938
   macro avg       0.65      0.54      0.48     46938
weighted avg       0.65      0.64      0.55     46938


Logistic Accuracy Score 64.08666751885465
Execution Time for Logistic Regression Imbalanced:  0.3251307010650635 seconds
===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010 Split 1 0.6 0.08
DCT F1-score 25.49572685555509
DCT ROCAUC score: 53.54204030976831
DCT Recall score: 16.96168496527
DCT Precision Score: 51.31333672258938
DCT Confusion Matrix [[26213  2873]
 [14824  3028]]

DCT Classification               precision    recall  f1-score   support

           0       0.64      0.90      0.75     29086
           1       0.51      0.17      0.25     17852

    accuracy                           0.62     46938
   macro avg       0.58      0.54      0.50     46938
weighted avg       0.59      0.62      0.56     46938


DCT Accuracy Score 62.29707273424518
Execution Time for Decision Tree Imbalanced:  1.1918153762817383 seconds
===Naive Bayes with TfidfVectorizer Imabalanced - 2010 Split 1 0.6 0.08
Naive F1-score 12.208497770488442
Naive ROCAUC score: 52.53124993789038
Naive Recall score: 6.67152139816267
Naive Precision Score: 71.79023508137432
Naive Confusion Matrix [[28618   468]
 [16661  1191]]

Naive Classification               precision    recall  f1-score   support

           0       0.63      0.98      0.77     29086
           1       0.72      0.07      0.12     17852

    accuracy                           0.64     46938
   macro avg       0.67      0.53      0.45     46938
weighted avg       0.66      0.64      0.52     46938


Naive Accuracy Score 63.50717968383825
Execution Time for Naive Bayes Imbalanced:  0.04987454414367676 seconds
===XGB with TfidfVectorizer Imbalanced - 2010 Split 1 0.6 0.08
XGB F1-Score 18.703955703627233
XGB ROCAUC Score: 60.571364345782705
XGB Recall score: 57.61780861520671
XGB Precision Score: 11.16401523638808
XGB Confusion Matrix [[27620 15859]
 [ 1466  1993]]

XGB Classification               precision    recall  f1-score   support

           0       0.95      0.64      0.76     43479
           1       0.11      0.58      0.19      3459

    accuracy                           0.63     46938
   macro avg       0.53      0.61      0.47     46938
weighted avg       0.89      0.63      0.72     46938


XGB Accuracy Score 63.50717968383825
Execution Time for XGBoost Classifier Imbalanced:  0.5894248485565186 seconds
====RandomForest with Tfidf Imbalanced 2010 Split 1 0.6 0.08
RFC F1 score 22.482563705155673
RFC ROCAUC Score: 54.12366325278067
RFC Recall score: 60.36719706242351
RFC Precision Score: 13.81357831055344
RFC Confusion Matrix [[27467  1619]
 [15386  2466]]

RFC Classification               precision    recall  f1-score   support

           0       0.64      0.94      0.76     29086
           1       0.60      0.14      0.22     17852

    accuracy                           0.64     46938
   macro avg       0.62      0.54      0.49     46938
weighted avg       0.63      0.64      0.56     46938


RFC Accuracy Score 63.77135796156632
Execution Time for Random Forest Classifier Imbalanced:  462.4668233394623 seconds
Array of Prob Scores LR-Imb: 0.6 : [array([0.37556539, 0.37556539, 0.32703701, ..., 0.34268318, 0.37556539,
       0.37556539])]
Array of F1 Scores LR-Imb: 0.6 : [0.19379214692237792]
Array of ROCAUC Scores LR-Imb: 0.6 : [0.5390210390631697]
Array of Recall Scores LR-Imb: 0.6 : [0.11348868474120546]
Array of Precision Scores LR-Imb: 0.6 : [0.6627412495911024]
Array of Accuracy Scores LR-Imb: 0.6 : [0.6408666751885466]
Array of Prob Scores DT-Imb: 0.6 : [array([0.38788278, 0.38788278, 0.30769231, ..., 0.38788278, 0.38788278,
       0.38788278])]
Array of F1 Scores DT-Imb: 0.6 : [0.2549572685555509]
Array of ROCAUC Scores DT-Imb: 0.6 : [0.5354204030976831]
Array of Recall Scores DT-Imb: 0.6 : [0.1696168496527]
Array of Precision Scores DT-Imb: 0.6 : [0.5131333672258939]
Array of Accuracy Scores DT-Imb: 0.6 : [0.6229707273424517]
Array of Prob Scores NB-Imb: 0.6 : [array([0.37743688, 0.37743688, 0.24530592, ..., 0.31126754, 0.37743688,
       0.37743688])]
Array of F1 Scores NB-Imb: 0.6 : [0.12208497770488443]
Array of ROCAUC Scores NB-Imb: 0.6 : [0.5253124993789038]
Array of Recall Scores NB-Imb: 0.6 : [0.0667152139816267]
Array of Precision Scores NB-Imb: 0.6 : [0.7179023508137432]
Array of Accuracy Scores NB-Imb: 0.6 : [0.6350717968383826]
Array of Prob Scores XG-Imb: 0.6 : [array([0.37555292, 0.37555292, 0.37555292, ..., 0.37555292, 0.37555292,
       0.37555292], dtype=float32)]
Array of F1 Scores XG-Imb: 0.6 : [0.18703955703627234]
Array of ROCAUC Scores XG-Imb: 0.6 : [0.6057136434578271]
Array of Recall Scores XG-Imb: 0.6 : [0.5761780861520671]
Array of Precision Scores XG-Imb: 0.6 : [0.11164015236388079]
Array of Accuracy Scores XG-Imb: 0.6 : [0.6308960756742937]
Array of Prob Scores RF-Imb: 0.6 : [array([0.38801952, 0.38801952, 0.3024869 , ..., 0.35422449, 0.38801952,
       0.38801952])]
Array of F1 Scores RF-Imb: 0.6 : [0.22482563705155673]
Array of ROCAUC Scores RF-Imb: 0.6 : [0.5412366325278067]
Array of Recall Scores RF-Imb: 0.6 : [0.603671970624235]
Array of Precision Scores RF-Imb: 0.6 : [0.1381357831055344]
Array of Accuracy Scores RF-Imb: 0.6 : [0.6377135796156632]
===Logistic Regression with TfidfVectorizer Imbalanced - 2010 Split 1 0.6 0.13
Logistic F1-score 23.646134766432407
Logistic ROCAUC score: 54.977822563844384
Logistic Recall score: 14.366244348942345
Logistic Precision Score: 66.78775298391282
Logistic Confusion Matrix [[27741  1280]
 [15343  2574]]

Logistic Classification               precision    recall  f1-score   support

           0       0.64      0.96      0.77     29021
           1       0.67      0.14      0.24     17917

    accuracy                           0.65     46938
   macro avg       0.66      0.55      0.50     46938
weighted avg       0.65      0.65      0.57     46938


Logistic Accuracy Score 64.5851974945673
Execution Time for Logistic Regression Imbalanced:  0.14461421966552734 seconds
===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010 Split 1 0.6 0.13
DCT F1-score 26.7059318468658
DCT ROCAUC score: 54.24188645829883
DCT Recall score: 17.715019255455715
DCT Precision Score: 54.22860071758073
DCT Confusion Matrix [[26342  2679]
 [14743  3174]]

DCT Classification               precision    recall  f1-score   support

           0       0.64      0.91      0.75     29021
           1       0.54      0.18      0.27     17917

    accuracy                           0.63     46938
   macro avg       0.59      0.54      0.51     46938
weighted avg       0.60      0.63      0.57     46938


DCT Accuracy Score 62.88295197920662
Execution Time for Decision Tree Imbalanced:  1.9029390811920166 seconds
===Naive Bayes with TfidfVectorizer Imabalanced - 2010 Split 1 0.6 0.13
Naive F1-score 14.55623085036918
Naive ROCAUC score: 53.10811625541931
Naive Recall score: 8.087291399229782
Naive Precision Score: 72.7409638554217
Naive Confusion Matrix [[28478   543]
 [16468  1449]]

Naive Classification               precision    recall  f1-score   support

           0       0.63      0.98      0.77     29021
           1       0.73      0.08      0.15     17917

    accuracy                           0.64     46938
   macro avg       0.68      0.53      0.46     46938
weighted avg       0.67      0.64      0.53     46938


Naive Accuracy Score 63.75857514167625
Execution Time for Naive Bayes Imbalanced:  0.04890298843383789 seconds
===XGB with TfidfVectorizer Imbalanced - 2010 Split 1 0.6 0.13
XGB F1-Score 20.284697508896794
XGB ROCAUC Score: 63.38669879881661
XGB Recall score: 62.98342541436463
XGB Precision Score: 12.089077412513255
XGB Confusion Matrix [[27748 15751]
 [ 1273  2166]]

XGB Classification               precision    recall  f1-score   support

           0       0.96      0.64      0.77     43499
           1       0.12      0.63      0.20      3439

    accuracy                           0.64     46938
   macro avg       0.54      0.63      0.48     46938
weighted avg       0.89      0.64      0.72     46938


XGB Accuracy Score 63.75857514167625
Execution Time for XGBoost Classifier Imbalanced:  0.6502618789672852 seconds
====RandomForest with Tfidf Imbalanced 2010 Split 1 0.6 0.13
RFC F1 score 25.835248364857698
RFC ROCAUC Score: 55.07652964952119
RFC Recall score: 62.046274676289535
RFC Precision Score: 16.314115086230956
RFC Confusion Matrix [[27233  1788]
 [14994  2923]]

RFC Classification               precision    recall  f1-score   support

           0       0.64      0.94      0.76     29021
           1       0.62      0.16      0.26     17917

    accuracy                           0.64     46938
   macro avg       0.63      0.55      0.51     46938
weighted avg       0.64      0.64      0.57     46938


RFC Accuracy Score 64.2464527674805
Execution Time for Random Forest Classifier Imbalanced:  777.9892585277557 seconds
Array of Prob Scores LR-Imb: 0.6 : [array([0.37556539, 0.37556539, 0.32703701, ..., 0.34268318, 0.37556539,
       0.37556539]), array([0.38142092, 0.57625842, 0.38142092, ..., 0.38142092, 0.38142092,
       0.80182276])]
Array of F1 Scores LR-Imb: 0.6 : [0.19379214692237792, 0.23646134766432408]
Array of ROCAUC Scores LR-Imb: 0.6 : [0.5390210390631697, 0.5497782256384438]
Array of Recall Scores LR-Imb: 0.6 : [0.11348868474120546, 0.14366244348942345]
Array of Precision Scores LR-Imb: 0.6 : [0.6627412495911024, 0.6678775298391282]
Array of Accuracy Scores LR-Imb: 0.6 : [0.6408666751885466, 0.645851974945673]
Array of Prob Scores DT-Imb: 0.6 : [array([0.38788278, 0.38788278, 0.30769231, ..., 0.38788278, 0.38788278,
       0.38788278]), array([0.40348743, 0.        , 0.40348743, ..., 0.40348743, 0.40348743,
       1.        ])]
Array of F1 Scores DT-Imb: 0.6 : [0.2549572685555509, 0.267059318468658]
Array of ROCAUC Scores DT-Imb: 0.6 : [0.5354204030976831, 0.5424188645829883]
Array of Recall Scores DT-Imb: 0.6 : [0.1696168496527, 0.17715019255455713]
Array of Precision Scores DT-Imb: 0.6 : [0.5131333672258939, 0.5422860071758073]
Array of Accuracy Scores DT-Imb: 0.6 : [0.6229707273424517, 0.6288295197920661]
Array of Prob Scores NB-Imb: 0.6 : [array([0.37743688, 0.37743688, 0.24530592, ..., 0.31126754, 0.37743688,
       0.37743688]), array([0.38086341, 0.21930794, 0.38086341, ..., 0.38086341, 0.38086341,
       0.2979854 ])]
Array of F1 Scores NB-Imb: 0.6 : [0.12208497770488443, 0.1455623085036918]
Array of ROCAUC Scores NB-Imb: 0.6 : [0.5253124993789038, 0.5310811625541931]
Array of Recall Scores NB-Imb: 0.6 : [0.0667152139816267, 0.08087291399229782]
Array of Precision Scores NB-Imb: 0.6 : [0.7179023508137432, 0.7274096385542169]
Array of Accuracy Scores NB-Imb: 0.6 : [0.6350717968383826, 0.6375857514167625]
Array of Prob Scores XG-Imb: 0.6 : [array([0.37555292, 0.37555292, 0.37555292, ..., 0.37555292, 0.37555292,
       0.37555292], dtype=float32), array([0.38048902, 0.5701597 , 0.38048902, ..., 0.38048902, 0.38048902,
       0.97430724], dtype=float32)]
Array of F1 Scores XG-Imb: 0.6 : [0.18703955703627234, 0.20284697508896793]
Array of ROCAUC Scores XG-Imb: 0.6 : [0.6057136434578271, 0.6338669879881661]
Array of Recall Scores XG-Imb: 0.6 : [0.5761780861520671, 0.6298342541436464]
Array of Precision Scores XG-Imb: 0.6 : [0.11164015236388079, 0.12089077412513255]
Array of Accuracy Scores XG-Imb: 0.6 : [0.6308960756742937, 0.6373087903191444]
Array of Prob Scores RF-Imb: 0.6 : [array([0.38801952, 0.38801952, 0.3024869 , ..., 0.35422449, 0.38801952,
       0.38801952]), array([0.40350642, 0.47995931, 0.40350642, ..., 0.40350642, 0.40350642,
       0.62030438])]
Array of F1 Scores RF-Imb: 0.6 : [0.22482563705155673, 0.258352483648577]
Array of ROCAUC Scores RF-Imb: 0.6 : [0.5412366325278067, 0.5507652964952119]
Array of Recall Scores RF-Imb: 0.6 : [0.603671970624235, 0.6204627467628954]
Array of Precision Scores RF-Imb: 0.6 : [0.1381357831055344, 0.16314115086230954]
Array of Accuracy Scores RF-Imb: 0.6 : [0.6377135796156632, 0.6424645276748051]
Iteration 1 Array: [0.6, 0.1381357831055344, 0.11164015236388079, 0.5131333672258939, 0.6627412495911024, 0.7179023508137432]
Iteration 2 Array: [0.6, 0.16314115086230954, 0.12089077412513255, 0.5422860071758073, 0.6678775298391282, 0.7274096385542169]
===Logistic Regression with TfidfVectorizer Imbalanced - 2010 Split 1 0.7 0.056
Logistic F1-score 17.319758986079368
Logistic ROCAUC score: 53.40371324395081
Logistic Recall score: 9.96557000765111
Logistic Precision Score: 66.09578179511576
Logistic Confusion Matrix [[32780  1069]
 [18828  2084]]

Logistic Classification               precision    recall  f1-score   support

           0       0.64      0.97      0.77     33849
           1       0.66      0.10      0.17     20912

    accuracy                           0.64     54761
   macro avg       0.65      0.53      0.47     54761
weighted avg       0.65      0.64      0.54     54761


Logistic Accuracy Score 63.665747521046
Execution Time for Logistic Regression Imbalanced:  0.12760543823242188 seconds
===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010 Split 1 0.7 0.056
DCT F1-score 23.10041716328963
DCT ROCAUC score: 53.22668718049599
DCT Recall score: 14.828806426931907
DCT Precision Score: 52.240566037735846
DCT Confusion Matrix [[31014  2835]
 [17811  3101]]

DCT Classification               precision    recall  f1-score   support

           0       0.64      0.92      0.75     33849
           1       0.52      0.15      0.23     20912

    accuracy                           0.62     54761
   macro avg       0.58      0.53      0.49     54761
weighted avg       0.59      0.62      0.55     54761


DCT Accuracy Score 62.29798579280875
Execution Time for Decision Tree Imbalanced:  0.8108320236206055 seconds
===Naive Bayes with TfidfVectorizer Imabalanced - 2010 Split 1 0.7 0.056
Naive F1-score 10.123227772624885
Naive ROCAUC score: 51.87887597497169
Naive Recall score: 5.4801071155317524
Naive Precision Score: 66.28108733371892
Naive Confusion Matrix [[33266   583]
 [19766  1146]]

Naive Classification               precision    recall  f1-score   support

           0       0.63      0.98      0.77     33849
           1       0.66      0.05      0.10     20912

    accuracy                           0.63     54761
   macro avg       0.65      0.52      0.43     54761
weighted avg       0.64      0.63      0.51     54761


Naive Accuracy Score 62.84034257957305
Execution Time for Naive Bayes Imbalanced:  0.054871559143066406 seconds
===XGB with TfidfVectorizer Imbalanced - 2010 Split 1 0.7 0.056
XGB F1-Score 17.45501913213384
XGB ROCAUC Score: 60.97623519492984
XGB Recall score: 58.675424192665574
XGB Precision Score: 10.252486610558531
XGB Confusion Matrix [[32339 18768]
 [ 1510  2144]]

XGB Classification               precision    recall  f1-score   support

           0       0.96      0.63      0.76     51107
           1       0.10      0.59      0.17      3654

    accuracy                           0.63     54761
   macro avg       0.53      0.61      0.47     54761
weighted avg       0.90      0.63      0.72     54761


XGB Accuracy Score 62.84034257957305
Execution Time for XGBoost Classifier Imbalanced:  0.5196127891540527 seconds
====RandomForest with Tfidf Imbalanced 2010 Split 1 0.7 0.056
RFC F1 score 20.214576982461697
RFC ROCAUC Score: 53.454146295409856
RFC Recall score: 58.730158730158735
RFC Precision Score: 12.208301453710787
RFC Confusion Matrix [[32055  1794]
 [18359  2553]]

RFC Classification               precision    recall  f1-score   support

           0       0.64      0.95      0.76     33849
           1       0.59      0.12      0.20     20912

    accuracy                           0.63     54761
   macro avg       0.61      0.53      0.48     54761
weighted avg       0.62      0.63      0.55     54761


RFC Accuracy Score 63.19826153649495
Execution Time for Random Forest Classifier Imbalanced:  335.28982377052307 seconds
Array of Prob Scores LR-Imb: 0.7 : [array([0.38261511, 0.3637574 , 0.60192188, ..., 0.38261511, 0.38261511,
       0.34317549])]
Array of F1 Scores LR-Imb: 0.7 : [0.17319758986079367]
Array of ROCAUC Scores LR-Imb: 0.7 : [0.5340371324395081]
Array of Recall Scores LR-Imb: 0.7 : [0.09965570007651109]
Array of Precision Scores LR-Imb: 0.7 : [0.6609578179511576]
Array of Accuracy Scores LR-Imb: 0.7 : [0.63665747521046]
Array of Prob Scores DT-Imb: 0.7 : [array([0.39119923, 1.        , 1.        , ..., 0.39119923, 0.39119923,
       0.        ])]
Array of F1 Scores DT-Imb: 0.7 : [0.2310041716328963]
Array of ROCAUC Scores DT-Imb: 0.7 : [0.5322668718049599]
Array of Recall Scores DT-Imb: 0.7 : [0.14828806426931906]
Array of Precision Scores DT-Imb: 0.7 : [0.5224056603773585]
Array of Accuracy Scores DT-Imb: 0.7 : [0.6229798579280875]
Array of Prob Scores NB-Imb: 0.7 : [array([0.37968037, 0.20692239, 0.2179846 , ..., 0.37968037, 0.37968037,
       0.24791272])]
Array of F1 Scores NB-Imb: 0.7 : [0.10123227772624885]
Array of ROCAUC Scores NB-Imb: 0.7 : [0.5187887597497169]
Array of Recall Scores NB-Imb: 0.7 : [0.05480107115531752]
Array of Precision Scores NB-Imb: 0.7 : [0.6628108733371891]
Array of Accuracy Scores NB-Imb: 0.7 : [0.6284034257957305]
Array of Prob Scores XG-Imb: 0.7 : [array([0.38802937, 0.2739626 , 0.88544226, ..., 0.38802937, 0.38802937,
       0.38802937], dtype=float32)]
Array of F1 Scores XG-Imb: 0.7 : [0.17455019132133842]
Array of ROCAUC Scores XG-Imb: 0.7 : [0.6097623519492984]
Array of Recall Scores XG-Imb: 0.7 : [0.5867542419266557]
Array of Precision Scores XG-Imb: 0.7 : [0.10252486610558531]
Array of Accuracy Scores XG-Imb: 0.7 : [0.6296999689560089]
Array of Prob Scores RF-Imb: 0.7 : [array([0.39111957, 0.3315806 , 0.44825   , ..., 0.39111957, 0.39111957,
       0.24362416])]
Array of F1 Scores RF-Imb: 0.7 : [0.20214576982461696]
Array of ROCAUC Scores RF-Imb: 0.7 : [0.5345414629540985]
Array of Recall Scores RF-Imb: 0.7 : [0.5873015873015873]
Array of Precision Scores RF-Imb: 0.7 : [0.12208301453710788]
Array of Accuracy Scores RF-Imb: 0.7 : [0.6319826153649495]
===Logistic Regression with TfidfVectorizer Imbalanced - 2010 Split 1 0.7 0.076
Logistic F1-score 18.922840913802393
Logistic ROCAUC score: 53.87186376562049
Logistic Recall score: 11.010856453558503
Logistic Precision Score: 67.23629935179729
Logistic Confusion Matrix [[32924  1112]
 [18443  2282]]

Logistic Classification               precision    recall  f1-score   support

           0       0.64      0.97      0.77     34036
           1       0.67      0.11      0.19     20725

    accuracy                           0.64     54761
   macro avg       0.66      0.54      0.48     54761
weighted avg       0.65      0.64      0.55     54761


Logistic Accuracy Score 64.29027957853218
Execution Time for Logistic Regression Imbalanced:  0.23437285423278809 seconds
===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010 Split 1 0.7 0.076
DCT F1-score 25.283156740957256
DCT ROCAUC score: 53.66853708150641
DCT Recall score: 16.69481302774427
DCT Precision Score: 52.06922498118887
DCT Confusion Matrix [[30851  3185]
 [17265  3460]]

DCT Classification               precision    recall  f1-score   support

           0       0.64      0.91      0.75     34036
           1       0.52      0.17      0.25     20725

    accuracy                           0.63     54761
   macro avg       0.58      0.54      0.50     54761
weighted avg       0.60      0.63      0.56     54761


DCT Accuracy Score 62.65590474973065
Execution Time for Decision Tree Imbalanced:  1.1818416118621826 seconds
===Naive Bayes with TfidfVectorizer Imabalanced - 2010 Split 1 0.7 0.076
Naive F1-score 10.792489854167595
Naive ROCAUC score: 52.20229173367985
Naive Recall score: 5.838359469240048
Naive Precision Score: 71.26030624263839
Naive Confusion Matrix [[33548   488]
 [19515  1210]]

Naive Classification               precision    recall  f1-score   support

           0       0.63      0.99      0.77     34036
           1       0.71      0.06      0.11     20725

    accuracy                           0.63     54761
   macro avg       0.67      0.52      0.44     54761
weighted avg       0.66      0.63      0.52     54761


Naive Accuracy Score 63.472179105567825
Execution Time for Naive Bayes Imbalanced:  0.056813955307006836 seconds
===XGB with TfidfVectorizer Imbalanced - 2010 Split 1 0.7 0.076
XGB F1-Score 18.270999386879215
XGB ROCAUC Score: 61.759689336266796
XGB Recall score: 59.759358288770045
XGB Precision Score: 10.784077201447527
XGB Confusion Matrix [[32531 18490]
 [ 1505  2235]]

XGB Classification               precision    recall  f1-score   support

           0       0.96      0.64      0.76     51021
           1       0.11      0.60      0.18      3740

    accuracy                           0.63     54761
   macro avg       0.53      0.62      0.47     54761
weighted avg       0.90      0.63      0.73     54761


XGB Accuracy Score 63.472179105567825
Execution Time for XGBoost Classifier Imbalanced:  0.572453498840332 seconds
====RandomForest with Tfidf Imbalanced 2010 Split 1 0.7 0.076
RFC F1 score 23.391630231105555
RFC ROCAUC Score: 54.444168389930134
RFC Recall score: 61.25536700061337
RFC Precision Score: 14.455971049457178
RFC Confusion Matrix [[32141  1895]
 [17729  2996]]

RFC Classification               precision    recall  f1-score   support

           0       0.64      0.94      0.77     34036
           1       0.61      0.14      0.23     20725

    accuracy                           0.64     54761
   macro avg       0.63      0.54      0.50     54761
weighted avg       0.63      0.64      0.56     54761


RFC Accuracy Score 64.16427749675864
Execution Time for Random Forest Classifier Imbalanced:  478.64438343048096 seconds
Array of Prob Scores LR-Imb: 0.7 : [array([0.38261511, 0.3637574 , 0.60192188, ..., 0.38261511, 0.38261511,
       0.34317549]), array([0.26240028, 0.37393989, 0.37393989, ..., 0.37393989, 0.11848526,
       0.38922914])]
Array of F1 Scores LR-Imb: 0.7 : [0.17319758986079367, 0.18922840913802394]
Array of ROCAUC Scores LR-Imb: 0.7 : [0.5340371324395081, 0.5387186376562049]
Array of Recall Scores LR-Imb: 0.7 : [0.09965570007651109, 0.11010856453558504]
Array of Precision Scores LR-Imb: 0.7 : [0.6609578179511576, 0.6723629935179729]
Array of Accuracy Scores LR-Imb: 0.7 : [0.63665747521046, 0.6429027957853217]
Array of Prob Scores DT-Imb: 0.7 : [array([0.39119923, 1.        , 1.        , ..., 0.39119923, 0.39119923,
       0.        ]), array([0.        , 0.38877266, 0.38877266, ..., 0.38877266, 0.        ,
       0.38877266])]
Array of F1 Scores DT-Imb: 0.7 : [0.2310041716328963, 0.2528315674095726]
Array of ROCAUC Scores DT-Imb: 0.7 : [0.5322668718049599, 0.5366853708150641]
Array of Recall Scores DT-Imb: 0.7 : [0.14828806426931906, 0.1669481302774427]
Array of Precision Scores DT-Imb: 0.7 : [0.5224056603773585, 0.5206922498118887]
Array of Accuracy Scores DT-Imb: 0.7 : [0.6229798579280875, 0.6265590474973065]
Array of Prob Scores NB-Imb: 0.7 : [array([0.37968037, 0.20692239, 0.2179846 , ..., 0.37968037, 0.37968037,
       0.24791272]), array([0.21338249, 0.37695542, 0.37695542, ..., 0.37695542, 0.10571331,
       0.43514119])]
Array of F1 Scores NB-Imb: 0.7 : [0.10123227772624885, 0.10792489854167596]
Array of ROCAUC Scores NB-Imb: 0.7 : [0.5187887597497169, 0.5220229173367985]
Array of Recall Scores NB-Imb: 0.7 : [0.05480107115531752, 0.05838359469240048]
Array of Precision Scores NB-Imb: 0.7 : [0.6628108733371891, 0.7126030624263839]
Array of Accuracy Scores NB-Imb: 0.7 : [0.6284034257957305, 0.6347217910556783]
Array of Prob Scores XG-Imb: 0.7 : [array([0.38802937, 0.2739626 , 0.88544226, ..., 0.38802937, 0.38802937,
       0.38802937], dtype=float32), array([0.24761814, 0.375031  , 0.375031  , ..., 0.375031  , 0.06949914,
       0.375031  ], dtype=float32)]
Array of F1 Scores XG-Imb: 0.7 : [0.17455019132133842, 0.18270999386879214]
Array of ROCAUC Scores XG-Imb: 0.7 : [0.6097623519492984, 0.617596893362668]
Array of Recall Scores XG-Imb: 0.7 : [0.5867542419266557, 0.5975935828877005]
Array of Precision Scores XG-Imb: 0.7 : [0.10252486610558531, 0.10784077201447527]
Array of Accuracy Scores XG-Imb: 0.7 : [0.6296999689560089, 0.6348678804258505]
Array of Prob Scores RF-Imb: 0.7 : [array([0.39111957, 0.3315806 , 0.44825   , ..., 0.39111957, 0.39111957,
       0.24362416]), array([0.2031509 , 0.38896588, 0.38896588, ..., 0.38896588, 0.04192858,
       0.40235979])]
Array of F1 Scores RF-Imb: 0.7 : [0.20214576982461696, 0.23391630231105556]
Array of ROCAUC Scores RF-Imb: 0.7 : [0.5345414629540985, 0.5444416838993014]
Array of Recall Scores RF-Imb: 0.7 : [0.5873015873015873, 0.6125536700061337]
Array of Precision Scores RF-Imb: 0.7 : [0.12208301453710788, 0.14455971049457178]
Array of Accuracy Scores RF-Imb: 0.7 : [0.6319826153649495, 0.6416427749675864]
Iteration 1 Array: [0.7, 0.10252486610558531, 0.12208301453710788, 0.5224056603773585, 0.6609578179511576, 0.6628108733371891]
Iteration 2 Array: [0.7, 0.10784077201447527, 0.14455971049457178, 0.5206922498118887, 0.6723629935179729, 0.7126030624263839]

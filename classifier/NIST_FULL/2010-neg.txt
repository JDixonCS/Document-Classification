SIGIR 2010 Industry Track

Following the success of the SIGIR 2007 Industry Event and the SIGIR 2009 Industry Track, this year's SIGIR conference includes again an Industry Track during the regular conference program, and in parallel with the technical tracks. The Industry Track's objectives are twofold. The first objective is presenting the state-of-the-art in search, delivered as keynote talks by influential technical leaders from the search industry. The second objective of the Industry Track is the presentation of interesting, novel, and innovative ideas from the search industry.
Future Search: From Information Retrieval to Information Enabled Commerce: Invited keynote talk by William Chang, Baidu. The China Economic Miracle has produced thirty years of sustained 10% GDP growth, allowing China to overtake Japan. Recently, concerned with social issues, debt safety, high commodity prices and weak exports, China has sought to tame that part of GDP derived from "real estate as securities" i.e. properties constructed for purpose of trade instead of use. Instead, China has turned to exhorting with urgency domestic consumption, but the country lacks many of the foundations of Information Enabled Commerce, contrastingly the epitome of American ingenuity and the very source of America's global competitive edge. We will survey some of these inventions for IEC from the viewpoint of IR. We will also survey China's demographic, social, cultural, and economic background and the role information now plays in people's daily lives, showcasing successful applications and business models that can suggest further opportunities for IR and IEC. Although China's 30-year development hasn't really built many of the things that the West takes for granted, people there are beginning to try: China's emerging Internet commerce has already exceeded 1% of GDP and is expected to double this year.
Search Flavours - Recent Updates and Trends: Invited keynote talk by Yossi Matias, Google. This talk will discuss some recent developments in Search, emerging in various shapes and forms. We will highlight some challenges, and point to some search trends that play an increasing role in multiple domains.
Query Understanding at Bing: Invited keynote talk by Jan Pedersen, Bing. Web Search is a modern marvel because it is able to produce very relevant results from relatively short queries evaluated over a vast database. Much of the magic is due to query understanding; the technology that analyzes a user query and produces a suitable backend search expression. This technology corrects common orthographic errors, expands terms to their semantically similar equivalents, and groups terms into concepts. I will discuss the language models behind these technologies and their role in canonical web search engine architecture.
Machine Learning in Search Quality at Yandex: Invited keynote talk by Ilya Segalovich, Yandex. This talk will discuss the machine learning approach to search quality problems at Yandex, the largest search engine in Russian Federation. We focus on a number of learning approaches that are vital in solving the large-scale IR problems, and explore the capabilities and prospects of machine learning in search quality, as well as the problems that appear in handling the real-world data sets based on our experiences at Yandex. We also describe Internet Mathematics 2009 contest which was organized by Yandex to stimulate research in the fields of data analysis and ranking methods.
Accepted Submission:
The new frontiers of Web search: going beyond the 10 blue links, Ricardo Baeza-Yates, Andrei Broder, Yoelle Maarek, and Prabhakar Raghavan, Yahoo! Labs
Cross-Language Information Retrieval in the Legal Domain, Samir Abdou and Thomas Arni, Eurospider
Building and Configuring a Real-Time Indexing System, Garret Swart, Ravi Palakodety, Mohammad Faisal, Wesley Lin, Oracle
Lessons and Challenges from Product Search, Daniel E. Rose, A9.com
Being Social: Research in Context-aware and Personalized Information Access @ Telefonica, Xavier Amatriain, Karen Church and Josep M. Pujol, Telefónica
Searching and Finding in a Long Tail Marketplace, Neel Sundaresan, eBay
When No Clicks are Good News, Carlos Castillo, Aris Gionis, Ronny Lempel, and Yoelle Maarek, Yahoo! Research

David Harper Google Organizer

Peter Schäuble Eurospider Organizer

xxix

SIGIR 2010 Organization

General Co-Chairs:
Technical Program Co-Chairs:
Posters and Demos Co-Chairs:
Tutorial Chair: Workshop Co-Chairs:
Industry Track Co-Chairs:
Mentor Chair: Best Paper Award Chair: Doctoral Consortium Co-Chairs:
Sponsorship Chair: Publicity Chair:
Local Arrangements: Finance Chair:
Conference Secretariat:

Fabio Crestani (University of Lugano, CH) Stéphane Marchand-Maillet (University of Geneva, CH)
Hsin-Hsi Chen (National Taiwan University, TW) Efthimis N. Efthimiadis (University of Washington, USA) Jacques Savoy (University of Neuchatel, CH)
Peter Bruza (Queensland University of Technology, AU) Gabriella Pasi (University of Milano-Bicocca, IT) Ellen Voorhees (NIST, USA)
Djoerd Hiemstra (University of Twente, NL)
Omar Alonso (Microsoft/Bing.com, USA) Giambattista Amati (Fondazione Ugo Bordoni, IT)
David Harper (Google, CH) Peter Schäuble (Eurospider, CH)
Ian Ruthven (University of Strathclyde, UK)
Norbert Fuhr (Universität Duisburg-Essen, DE)
Douglas W. Oard (University of Maryland, College Park, USA) Paul Thomas (CSIRO, AU)
Dawei Song (The Robert Gordon University, UK)
Fazli Can (Bilkent University, TR)
KUONI Destination Management, Petit Lancy, CH
Eric Bruno (University of Geneva, CH)
Germaine Gusthiot (University of Geneva, CH)

xviii

Senior Program Committee

Hsin-Hsi Chen (National Taiwan University, TW) - Chair Efthimis N. Efthimiadis (University of Washington, USA) ­
Chair Jacques Savoy (University of Neuchatel, CH) - Chair Eugene Agichtein (Emory University, USA) James Allan (University of Massachusetts Amherst, USA) Ricardo Baeza-Yates (Yahoo! Research, SP) Nicholas Belkin (Rutgers, The State University of New
Jersey, USA) Andrei Broder (Yahoo! Research, USA) Chris Buckley (Sabir Research, USA) Soumen Chakrabarti (IIT Bombay, IN) Tat-Seng Chua (National University of Singapore, SG) Charles Clarke (University of Waterloo, CA) Gordon V. Cormack (University of Waterloo, CA) W. Bruce Croft (University of Massachusetts Amherst,
USA) Franciska de Jong (University of Twente, NL) Maarten de Rijke (University of Amsterdam, NL) Edward Fox (Virginia Tech, USA) Norbert Fuhr (Universität Duisburg-Essen, DE) Eric Gaussier (Université Joseph Fourier, FR) Jaana Kekäläinen (University of Tampere, FI) Wai Lam (The Chinese University of Hong Kong, HK) Hang Li (Microsoft Research Asia, CN)

Tie-Yan Liu (Microsoft Research Asia, CN) Wei-Ying Ma (Microsoft Research Asia, CN) Yoelle Maarek (Yahoo! Research, IL) Marie-Francine Moens (Katholieke Universiteit Leuven,
BE) Alistair Moffat (University of Melbourne, AU) Isabelle Moulinier (Thomson Reuters, USA) Sung Hyon Myaeng (Korea Advanced Institute of Science
and Tech, KR) Hwee Tou Ng (National University of Singapore, SG) Jian-Yun Nie (University of Montreal, CA) Douglas W. Oard (University of Maryland, USA) Iadh Ounis (University of Glasgow, UK) Berthier Ribeiro-Neto (Federal University of Minas Gerais,
BR) Ian Ruthven (University of Strathclyde, UK) Tetsuya Sakai (Microsoft Research Asia, CN) Hinrich Schütze (University of Stuttgart, DE) Luo Si (Purdue University, USA) Ian Soboroff (NIST, USA) Ryen W. White (Microsoft Research Redmond, USA) Ross Wilkinson (ANDS, AU) Hugo Zaragoza (Yahoo! Research, SP) Cheng Xiang Zhai (University of Illinois at Urbana-
Champaign, USA)

Program Committee

Hsin-Hsi Chen (National Taiwan University, TW) - Chair Efthimis N. Efthimiadis (University of Washington, USA) ­
Chair Jacques Savoy (University of Neuchatel, CH) - Chair Eytan Adar (University of Michigan, USA) Maristella Agosti (University of Padua, IT) Fabio Aiolli (University of Padova, IT) Ram Akella (University of California Santa Cruz, USA) Khalid Al-Kofahi (Thomson Reuters, USA) Omar Alonso (Microsoft/Bing.com, USA) Gianni Amati (Fondazione Ugo Bordoni, IT) Xavier Amatriain (Telefonica Research, SP) Massih-Reza Amini (National Research Council of
Canada, CA) Peter Anick (Brandeis University, USA) Avi Arampatzis (Democritus University of Thrace, GR) Paavo Arvola (University of Tampere, FI) Javed Aslam (Northeastern University, USA) Leif Azzopardi (University of Glasgow, UK) Jing Bai (Yahoo!, USA) Peter Bailey (Microsoft Research, USA) Timothy Baldwin (University of Melbourne, AU)

Krisztian Balog (University of Amsterdam, NL) Judit Bar-Ilan (Bar-Ilan University, IL) Alvaro Barreiro (University of A Coruna, SP) Ziv Bar-Yossef (Google Inc., IL) Roberto Basili (University of Roma, IT) Hannah Bast (University of Freiburg, DE) Michel Beigbeder (École Nationale Supérieure des Mines
de Saint-Étienne, FR) Steve Beitzel (Telcordia Technologies, USA) Michael Bendersky (University of Massachusetts Amherst,
USA) Paul Bennett (Microsoft Research, USA) Gerald Benoit (Simmons College, USA) Ralf Bierig (Rutgers University, USA) Mikhail Bilenko (Microsoft Research, USA) Roi Blanco (Yahoo! Research, SP) Paolo Boldi (Università degli Studi di Milano, IT) Jose Borbinha (INESC-ID / IST, PT) Gloria Bordogna (CNR, IT) Mohand Boughanem (University Paul Sabatier, FR) Giorgio Brajnik (University of Udine, IT)

xix

Martin Braschler (Zurich University of Applied Sciences, CH)
Ulf Brefeld (Yahoo! Research, SP) Eric Brown (IBM Research, USA) Ron Brown (University of South Carolina, USA) Peter Brusilovsky (University of Pittsburgh, USA) George Buchanan (City University London, UK) Stefan Buettcher (Google, USA) Georg Buscher (DFKI, DE) Jamie Callan (Carnegie Mellon University, USA) Fazli Can (Bilkent University, TR) Guihong Cao (Microsoft Research, USA) Robert Capra (University of North Carolina at Chapel Hill,
USA) Jaime Carbonell (Carnegie Mellon University, USA) Mark Carman (University of Lugano, CH) David Carmel (IBM Research, IL) Ben Carterette (University of Delaware, USA) Lillian Cassel (Villanova University, USA) Carlos Castillo (Yahoo! Research, SP) Yllias Chali (University of Lethbridge, CA) Yee Seng Chan (University of Illinois at Urbana-
Champaign, USA) Raman Chandrasekar (Microsoft Research, USA) Edward Chang (Google Inc, USA) Olivier Chapelle (Yahoo! Research, USA) Chien Chin Chen (National Taiwan University, TW) Francine Chen (FX Palo Alto Laboratory, USA) Hsinchun Chen (University of Arizona, USA) Kuang-Hua Chen (National Taiwan University, TW) Pu-Jen Cheng (National Taiwan University, TW) Jean-Pierre Chevallet (Université de Grenoble, FR) Ed Chi (Palo Alto Research Center, USA) Yun Chi (NEC Laboratories America, USA) Abdur Chowdhury (Twitter, USA) Wei Chu (Yahoo! Labs, USA) Paul Clough (University of Sheffield, UK) Kevyn Collins-Thompson (Microsoft Research, USA) Gregory Crane (Tufts University, USA) Nick Craswell (Microsoft Research, USA) Silviu Cucerzan (Microsoft Research, USA) Sally Jo Cunningham (Wakaito University, NZ) Tonya Custis (Thomson Reuters, USA) Fernando Das Neves (Snoop Consulting, AR) Brian D. Davison (Lehigh University, USA) Arjen de Vries (Centrum Wiskunde & Informatica, NL) Giorgio Maria Di Nunzio (University of Padua, IT) Fernando Diaz (Yahoo!, USA) Ajay Divakaran (Sarnoff Corporation, USA) Debora Donato (Yahoo! Labs, USA) Dietmar Dorr (Thomson Reuters, USA) Antoine Doucet (University of Caen, FR) Kevin Duh (NTT, JP) Susan Dumais (Microsoft Research Redmond, USA) Georges Dupret (Yahoo! Labs, USA)

Koji Eguchi (Kobe University, JP) Maximilian Eibl (Chemnitz University of Technology, DE) David Eichmann (The University of Iowa, USA) Charles Elkan (University of California San Diego, USA) Jonathan Elsas (Carnegie Mellon University, USA) David Elsweiler (Friedrich-Alexander Universität
Erlangen, DE) Andrea Esuli (National Council of Research, IT) Jianping Fan (UNC-Charlotte, USA) Hui Fang (University of Delaware, USA) Shaolei Feng (Siemens Corporate Research Labs, USA) Juan M. Fernàndez-Luna (Universidad de Granada, SP) Nicola Ferro (University of Padua, IT) Radu Florian (IBM Research, USA) Marcus Fontoura (Yahoo! Research, USA) David Forsyth (UIUC, USA) Luis Francisco-Revilla (University of Texas at Austin, USA) Martin Franz (IBM, USA) Jim French (University of Virginia, USA) Luanne Freund (University of British Columbia, CA) Ophir Frieder (Georgetown University, USA) Ingo Frommholz (University of Glasgow, UK) Maria Fuentes (Universitat Politecnica de Catalunya, SP) Sumio Fujita (Yahoo! Japan, JP) Pascale Fung (Hong-Kong University of Science &
Technology, HK) Evgeniy Gabrilovich (Yahoo!, USA) Patrick Gallinari (Université Pierre et Marie Curie, FR) Bin Gao (Microsoft Research Asia, CN) Jianfeng Gao (Microsoft Research, USA) Shlomo Geva (Queensland University of Technology, AU) Frederic Gey (University of California, Berkeley, USA) C. Lee Giles (Pennsylvania State University, USA) Ayse Goker (City University London, UK) Marcos Goncalves (UFMG, BR) Julio Gonzalo (UNED, SP) Cyril Goutte (National Research Council of Canada, CA) Brigitte Grau (LIMSI, FR) Mark Greenwood (University of Sheffield, UK) Gregory Grefenstette (Exalead, FR) David Grossman (IIT, USA) Daniel Gruhl (IBM, USA) Cathal Gurrin (Dublin City University, IE) Karl Gyllstrom (Katholieke Universiteit Leuven, BE) Stephanie Haas (University of North Carolina at Chapel
Hill, USA) Ben Hachey (Macquarie University, AU) Eui-Hong (Sam) Han (Sears Holdings Corporation, USA) Alan Hanjalic (Delft University of Technology, NL) Sanda Harabagiu (University of Texas at Dallas, USA) Donna Harman (NIST, USA) Max Harper (University of Minnesota, USA) Claudia Hauff (University of Twente, NL) David Hawking (Funnelback, AU) Daqing He (University of Pittsburgh, USA)

xx

Lenwood Heath (Virginia Tech, USA) Andreas Henrich (University of Bamberg, DE) Paul Heymann (Stanford University, USA) Andrew Hickl (Language Computer Corporation, USA) David Hicks (Aalborg University, DK) Djoerd Hiemstra (University of Twente, NL) Katja Hofmann (University of Amsterdam, NL) Thomas Hofman (Google, CH) Steven Hoi (Nanyang Technological University, SG) Richang Hong (National University of Singapore, SG) Timo Honkela (Aalto University School of Science and
Technology, FI) Eric Horvitz (Microsoft Research, USA) Winston Hsu (National Taiwan University, TW) Xian-Sheng Hua (Microsoft Research Asia, CN) Jeff Huang (University of Washington, USA) Jimmy Huang (York University, CA) Yi-Fen Huang (Carnegie Mellon University, USA) Benoit Huet (EURECOM, FR) Juan Huete (University of Granada, SP) Peter Ingwersen (Royal School of Library and Information
Science Copenhagen, DK) Antti Järvelin (University of Tampere, FI) Adam Jatowt (Kyoto University, JP) CV Jawahar (IIIT Hyderabad, IN) Jiwoon Jeon (Google, USA) Heng Ji (City University of New York, USA) Daxin Jiang (Microsoft Researc Asia, CN) Jing Jiang (Singapore Management University, SG) Valentin Jijkoun (University of Amsterdam, NL) Rong Jin (Michigan State University, USA) Hideo Joho (University of Tsukuba, JP) Gareth J.F. Jones (Dublin City University, IE) Rosie Jones (Yahoo! Labs, USA) Joemon Jose (University of Glasgow, UK) Vanja Josifovski (Yahoo! Research, USA) Marko Junkkari (University of Tampere, FI) Jens Kürsten (Technische Universität Chemnitz, DE) Theodore Kalamboukis (Athens University of Economics
and Business, GR) Jaap Kamps (University of Amsterdam, NL) Min-Yen Kan (National University of Singapore, SG) Noriko Kando (National Institute of Informatics, JP) Mohan Kankanhalli (National University of Singapore, SG) Evangelos Kanoulas (University of Sheffield, UK) Sarantos Kapidakis (Ionian University, GR) Murat Karamuftuoglu (Bilkent University, TR) David Karger (MIT, USA) Jussi Karlgren (Swedish Institute of Computer Science, SE) Tsuneaki Kato (The University of Tokyo, Japan, JP) Judy Kay (University of Sydney, AU) Diane Kelly (University of North Carolina Chapel Hill,
USA) Kazuaki Kishida (Keio University, JP) Dietrich Klakow (Saarland University, DE)

Iraklis Klampanos (University of Glasgow, UK) Claus-Peter Klas (Distance University in Hagen, DE) Youngjoong Ko (Dong-A University, KR) Yehuda Koren (Yahoo Research Israel, IL) Wessel Kraaij (TNO and Radboud University, NL) Donald Kraft (Louisiana State University and U.S. Air
Force Academy, USA) Lun-Wei Ku (National Taiwan University, TW) Ravi Kumar (Yahoo! Research, USA) Giridhar Kumaran (Microsoft Corporation, USA) Oren Kurland (Technion, IL) Kui-Lam Kwok (Queens College City University of New
York, USA) Mounia Lalmas (University of Glasgow, UK) Monica Landoni (University of Lugano, CH) Guy Lapalme (Université de Montréal, CA) Birger Larsen (Royal School of Library and Information
Science Denmark, DK) Martha Larson (Delft University of Technology, NL) Ray Larson (University of California, Berkeley, USA) Neal Lathia (University College London, UK) Victor Lavrenko (University of Edinburgh, UK) Matthew Lease (University of Texas at Austin, USA) Guy Lebanon (Georgia Institute of Technology, USA) Gary Geunbae Lee (POSTECH, KR) Jin Ha Lee (University of Washington, USA) Miro Lehtonen (University of Helsinki, FI) Jochen Leidner (Thomson Reuters Corp., USA) Ronny Lempel (Yahoo Inc., IL) MunKew Leong (National Library Board Singapore, SG) Michael Lesk (Rutgers University, USA) Anton Leuski (University of Southern California, USA) Michael Lew (Leiden University, NL) Ping Li (Cornell University, USA) Tao Li (Florida International University, USA) Xiao Li (Microsoft Research, USA) Wenhui Liao (Thomson Reuters Corp., USA) Elizabeth D. Liddy (Syracuse University, Syracuse, USA) Ee-Peng Lim (Singapore Management University, SG) Chin-Yew Lin (Microsoft Research Asia, CN) Chuan-Jie Lin (National Taiwan Ocean University, TW) Jimmy Lin (University of Maryland, USA) Ming-Shun Lin (National Taiwan University, TW) Xia Lin (Drexel University, USA) Christina Lioma (Konstanz University, DE) Chao Liu (Microsoft Research, USA) Ting Liu (Harbin Institute of Technology, CN) Yi Liu (Google, USA) Fernando Llopis (University of Alicante, SP) Irene Lopatovska (Pratt Institute, USA) David Losada (University of Santiago de Compostela, SP) Jie Lu (IBM Research, USA) Robert Luk (The Hong Kong Polytechnic University, HK) Marianne Lykke (Royal School of Library and Information
Science Denmark, DK)

xxi

Michael Lyu (Chinese University of Hong Kong, HK) Craig Macdonald (University Glasgow, UK) Andrew MacFarlane (City University London, UK) Joao Magalhaes (Universidade Nova de Lisboa, PT) Prasenjit Majumder (DAIICT, IN) Thomas Mandl (University of Hildesheim, DE) Byron Marshall (Oregon State University, USA) Cathy Marshall (Microsoft Research Silicon Valley, USA) Maarten Marx (University of Amsterdam, NL) Yosi Mass (IBM Haifa Research Lab, IL) Yuji Matsumoto (Nara Institute of Science and Technology,
JP) Mark Maybury (The MITRE Corporation, USA) James Mayfield (JHU HLT/COE, USA) Paul McNamee (Johns Hopkins University, USA) Bhaskar Mehta (Google, CH) Qiaozhu Mei (University of Michigan, USA) Tao Mei (Microsoft Research Asia, CN) Edgar Meij (University of Amsterdam, NL) Massimo Melucci (University of Padua, IT) Donald Metzler (Yahoo!, Research, USA) Peter Mika (Yahoo! Research, SP) Natasa Milic-Frayling (Microsoft Research Ltd, UK) Teruko Mitamura (Carnegie Mellon University, USA) Mandar Mitra (Indian Statistical Institute, IN) Vibhu Mittal (Root-1 Research, USA) Stefano Mizzaro (University of Udine, IT) Dunja Mladenic (Jozef Stefan Institute, SI) Marie-Francine Moens (Katholieke Universiteit Leuven,
BE) Alessandro Moschitti (University of Trento, IT) Javed Mostafa (Univ of North Carolina at Chapel Hill,
USA) Josiane Mothe (Université Paul Sabatier, FR) Philippe Mulhem (Université de Grenoble, FR) Henning Müller (HES SO Valais, CH) Masaki Murata (NICT, JP) Vanessa Murdock (Yahoo! Research, SP) Seung-Hoon Na (National University of Singapore, SG) Preslav Nakov (National University of Singapore, SG) Gonzalo Navarro (University of Chile, CL) Michael Nelson (Old Dominion University, USA) Erich Neuhold (Universität Wien, AT) Chong-Wah Ngo (City University of Hong Kong, HK) Charles Nicholas (UMBC, USA) Maria Nikolaidou (Dept. of Informatics and Telematics,
GR) Alexandros Ntoulas (Microsoft Research, USA) Paul Ogilvie (mSpoke, USA) Jahna Otterbacher (Illinois Institute of Technology, USA) George Paliouras (NCSR Demokritos, GR) Rina Panigrahy (Microsoft Research, USA) Christos Papatheodorou (Ionian University, GR) Cecile Paris (CSIRO, AU) Marius Pasca (Google Inc., USA)

Virgil Pavlu (Northeastern University, USA) Jan Pedersen (Microsoft Corp., USA) Fuchun Peng (Yahoo! Inc, USA) José Perez Aguera (University of North Carolina at Chapel
Hill, USA) Jeremy Pickens (FX Palo Alto Lab, Inc., USA) Ari Pirkola (University of Tampere, FI) Benjamin Piwowarski (University of Glasgow, UK) Vassilis Plachouras (Athens University of Economics and
Business, GR) Panayiota Polydoratou (City University London, UK) John Prager (IBM Research, USA) Josep M. Pujol (Telefonica Research, SP) Tao Qin (Microsoft Research Asia, CN) Stephan Raaijmakers (TNO, NL) Balazs Racz (Google Inc., CH) Hema Raghavan (Yahoo! Labs, USA) Prabhakar Raghavan (Yahoo! Labs, USA) Vijay Raghavan (University of Louisiana at Lafayette,
USA) Naren Ramakrishnan (Virginia Tech, USA) Edie Rasmussen (University of British Columbia, CA) Andreas Rauber (Vienna University of Technology, AT) Jean-Michel Renders (Xerox Research Center Europe, FR) Matthew Richardson (Microsoft Research, USA) Hae-Chang Rim (Korea University, KR) Henning Rode (CWI Amsterdam, NL) Horacio Rodriguez (Universitat Politècnica de Catalunya,
SP) Thomas Roelleke (Queen Mary University of London, UK) Haggai Roitman (IBM Research Lab, Haifa, IL) Dmitri Roussinov (University of Strathclyde, UK) Patrick Ruch (University of Applied Sciences, Geneva, CH) Miguel Ruiz (University of North Texas, USA) Daniel Russell (Google Inc., USA) Shin'ichi Satoh (National Institute of Informatics, JP) Ralf Schenkel (Saarland University, DE) Frank Schilder (Thomson Reuters, USA) Jonathan Schler (Bar Ilan University, IL) Falk Scholer (RMIT University, AU) D. Sculley (Google, USA) Nicu Sebe (University of Trento, IT) Yohei Seki (Toyohashi University of Technology, JP) Erik Selberg (Amazon.com, USA) Pavel Serdyukov (Delft University of Technology, NL) Chirag Shah (UNC Chapel Hill, USA) Jayavel Shanmugasundaram (Yahoo! Research, USA) Dou Shen (Microsoft Research, USA) Jialie Shen (Singapore Management University, SG) Rao Shen (Yahoo!, USA) Xuehua Shen (Google, USA) Frank Shipman (Texas A&M University, USA) Milad Shokouhi (Microsoft, UK) Börkur Sigurbjörnsson (Yahoo! Research, SP) Màrio Silva (University of Lisbon, PT)

xxii

Fabrizio Silvestri (Information Science and Technology Institute, IT)
Malika Smail-Tabbone (LORIA - Nancy-Université, FR) Catherine Smith (Rutgers University, USA) Mark Smucker (University of Waterloo, CA) Cees Snoek (University of Amsterdam, NL) Aya Soffer (IBM Research - Haifa, IL) Ingeborg Solvberg (Norwegian University of Science and
Technology, NO) Dawei Song (The Robert Gordon University, UK) Ruihua Song (Microsoft Research Asia, CN) Eero Sormunen (University of Tampere, FI) Alessandro Sperduti (Universitè di Padova, IT) Amanda Spink (QUT, AU) Sofia Stamou (Patras University, Greece, GR) Benno Stein (Bauhaus-Universität Weimar, DE) Nicola Stokes (University College Dublin, Ireland) Trevor Strohman (Google, USA) Torsten Suel (Polytechnic Institute of NYU, USA) Hussein Suleman (University of Cape Town, ZA) Aixin Sun (Nanyang Technological University, SG) Le Sun (Chinese Academy of Sciences, CN) S.M.M (Saied) Tahaghoghi (Microsoft, USA) John Tait (Information Retrieval Facility, AT) Tuomas Talvensaari (University of Tampere, FI) Jie Tang (Tsinghua University, CN) Jinhui Tang (National University of Singapore, SG) Simone Teufel (University of Cambridge, UK) Martin Theobald (Max-Planck Institute Informatics, DE) Ulrich Thiel (IPSI-Darmstadt, DE) James Thom (RMIT, AU) Paul Thomas (CSIRO, AU) Paul Thompson (Dartmouth College, USA) Qi Tian (University of Texas at San Antonio, USA) Anastasios Tombros (Queen Mary University of London,
UK) Elaine Toms (Dalhousie University, CA) Richard Tong (Tarragon Consulting Corporation, USA) Ricardo Torres (University of Campinas (UNICAMP), BR) Andrew Trotman (University of Otago, NZ) Christos Tryfonopoulos (University of Peloponnese, GR) Ming-Feng Tsai (National University of Singapore, SG) Yuen-Hsien Tseng (National Taiwan Normal University,
TW) Theodora Tsikrika (Centrum Wiskunde & Informatica, NL) Jun'ichi Tsujii (University of Tokyo and University of
Manchester, JP) Howard Turtle (Syracuse University, USA) Antti Ukkonen (Aalto University, FI) Trystan Upstill (Google, USA) Quoc V. (Stanford University, USA) Pertti Vakkari (University of Tampere, FI) Antal van den Bosch (Tilburg University, NL)

Olga Vechtomova (University of Waterloo, Canada, CA) Svetha Venkatesh (Curtin University of Technology
Australia, AU) Phil Vines (RMIT University, Melbourne, Australia, AU) Stephen Wan (CSIRO, AU) Xiaojun Wan (Peking University, CN) Haifeng Wang (Baidu.com Inc., CN) Jenq-Haur Wang (National Taipei University of
Technology, TW) Jun Wang (University College London, UK) Peiling Wang (University of Tennessee, USA) Taifeng Wang (Microsoft Research Asia, CN) Xin-Jing Wang (Microsoft Research Asia, CN) Mark Wasson (LexisNexis, USA) William Webber (University of Melbourne, AU) Wouter Weerkamp (University of Amsterdam, NL) Xing Wei (Yahoo! Labs, USA) Thijs Westerveld (Teezir Search Solutions, NL) Lynn Wilcox (FXPAL, USA) Max Wilson (Swansea University, UK) René Witte (Concordia University, CA) Marcel Worring (University of Amsterdam, NL) Mingfang Wu (RMIT, AU) Iris Xie (University of Wisconsin-Milwaukee, USA) Dong Xu (Nanyang Technological University, SG) Gu Xu (Microsoft Research Asia, CN) Jun Xu (Microsoft Research Asia, CN) Rong Yan (Facebook, USA) Qiang Yang (Hong Kong University of Science and
Technology, HK) Yiming Yang (Carnegie Mellon University, USA) Emmanuel Yannakoudakis (AUEB, GR) Emine Yilmaz (Microsoft Research Cambridge, UK) Elad Yom-Tov (IBM Research, IL) Masaharu Yoshioka (Hokkaido University, JP) Kai Yu (NEC Laboratories America, USA) Shipeng Yu (Siemens, USA) Yong Yu (Shanghai Jiao Tong University, CN) Xiaojun Yuan (University at Albany, State University of
New York, USA) Yisong Yue (Cornell University, USA) Lei Zhang (Microsoft Research Asia, CN) Yi Zhang (UC Santa Cruz, USA) Zhongfei Zhang (State University of New York at
Binghamton, USA) Jun Zhao (Chinese Academy of Sciences, CN) Yan-Tao Zheng (Institute for Infocomm Research,
Singapore, SG) Dong Zhou (Trinity College Dublin, IE) Guodong Zhou (Soochow University, CN) Xiangdong Zhou (Fudan University, CN) Jianhan Zhu (University College London, UK) Nivio Ziviani (Federal University of Minas Gerais, BR)

xxiii

Additional Paper Reviewers

Jaewook Ahn (University of Pittsburgh, USA) Elshaimaa Ali (University of Louisiana at Lafayette, USA) Ismail Sengor Altingovde (Bilkent University, TR) Eduardo M. Ares (University of A Coruna, SP) Jaime Arguello (Carnegie Mellon University, USA) Avinash Atreya (UC San Diego, USA) Murat Ayhan (University of Louisiana at Lafayette, USA) Toine Bogers (Royal School of Library and Information
Science, DK) Savvas Chatzichristofis (Democritus University of Thrace,
GR) Max Chevalier (Université Paul Sabatier, FR) Doron Cohen (IBM Research, IL) Na Dai (Lehigh University, USA) Theodore Dalamagas (IMIS - Athena RC, GR) Zhicheng Dou (Microsoft Research Asia, CN) Giorgos Giannopoulos (IMIS - Athena RC, GR) Siddharth Gopal (Carnegie Mellon University, USA) Abhay Halpale (Carnegie Mellon University, USA) Ben He (York University, CA) Liangjie Hong (Lehigh University, USA) Vivian Hu (York University, CA) Gilles Hubert (Université Paul Sabatier, FR) Tom Johnsten (University of South Alabama, USA) Satya Katragadda (University of Louisiana at Lafayette,
USA) Maheedhar Kolla (University of Waterloo, CA) Aris Kosmopoulos (NCSR Demokritos, GR)

Anastasia Krithara (NCSR Demokritos, GR) Anagha Kulkarni (Carnegie Mellon University, USA) Abhimanyu Lad (Carnegie Mellon University, USA) Yize Li (University of California Santa Cruz, USA) Bingbing Ni (National University of Singapore, SG) Javier Parapar (University of A Coruna, SP) Dimitris Pierrakos (NCSR Demokritos, GR) Xiaoguang Qi (Lehigh University, USA) Konstantin Salomatin (Carnegie Mellon University, USA) Rodrygo Santos (University of Glasgow, UK) Amir Sharif (University of Louisiana at Lafayette, USA) Gianmaria Silvello (University of Padua, IT) Ranamathan Subramanian (University of Trento, IT) Giannis Tsakonas (Patras University, GR) Dimitris Vogiatzis (NCSR Demokritos, GR) Jian Wang (University of California Santa Cruz, USA) Shuhui Wang (Chinese Academy of Sciences, CN) Xuanhui Wang (Yahoo! Labs, USA) John Whissell (University of Waterloo, CA) Zaihan Yang (Lehigh University, USA) Jeff Ye (York University, CA) Xiaoshi Yin (York University, CA) Shiliang Zhang (Chinese Academy of Sciences, CN) Jiashu Zhao (York University, CA) Le Zhao (Carnegie Mellon University, USA) Wengang Zhou (University of Science and Technology of
China, CN)

Posters - Demonstrations Committee

Peter Bruza (Queensland University of Technology, AU) Chair
Gabriella Pasi (University of Milano-Bicocca, IT) - Chair Ellen Voorhees (NIST, USA) - Chair José R. Perez Aguera (University of North Carolina at
Chapel Hill, USA) Ram Akella (University of California Santa Cruz, USA) Giambattista Amati (Fondazione Ugo Bordoni, IT) Maik Anderka (Bauhaus-Universität Weimar, DE) Javed A. Aslam (Northeastern University, USA) Jing Bai (Yahoo! Research, USA) Roberto Basili (University of Roma, IT) Stefano Berretti (University of Firenze, IT) Roi Blanco (Yahoo! Research, SP) Jose Borbinha (INESC-ID / IST, PT) Gloria Bordogna (CNR, IT) Mohand Boughanem (University Paul Sabatier, FR) Giorgio Brajnik (University of Udine, IT) Nieves R. Brisaboa (University of Coruna, SP) Guillaume Cabanac (Université Paul Sabatier, FR)

Rodrigo Calumby (University of Campinas, BR) Fazli Can (Bilkent University, TR) Guihong Cao (Microsoft Research, USA) Mark J. Carman (University of Lugano, CH) Ben Carterette (University of Delaware, USA) Carlos Castillo (Yahoo! Research, SP) Raman Chandrasekar (Microsoft Research, USA) Yi Chang (Yahoo! Research, USA) Miao Chen (Yahoo! Research, USA) Zheng Chen (Microsoft Research Asia, CN) Yun Chi (NEC Laboratories America, USA) Paul Clough (University of Sheffield, UK) Silviu Cucerzan (Microsoft Research, USA) Tonya Custis (Thomson Reuters, USA) Theodore Dalamagas (IMIS ­ Athena RC, GR) Lorand Dali (Jozef Stefan Institute, SL) Christos Diou (Aristotle University of Thessaloniki, GR) Debora Donato (Yahoo! Labs, USA) J. Stephen Downie (University of Illinois at Urbana-
Champaign, USA)

xxiv

Georges Dupret (Yahoo! Labs, USA) Miles Efron (University of Illinois, USA) Koji Eguchi (Kobe University, JP) Andrea Esuli (NCR, IT) Jianping Fan (UNC - Charlotte, USA) Fabio Faria (University of Campinas, BR) Shaolei Feng (Siemens Corporate Research Labs, USA) Juan M. Fernández-Luna (Universidad de Granada, SP) Marcus Fontoura (Yahoo! Research, USA) Ingo Frommholz (University of Glasgow, UK) Xin Fu (Google, USA) Maria Fuentes (Universitat Politècnica de Catalunya, SP) Atsushi Fujii (Tokyo Institute of Technology, JP) Sumio Fujita (Yahoo! Japan Research, JP) Patrick Gallinari (Université Pierre et Marie Curie, FR) Susan Gauch (University of Arkansas, CA) Giorgos Giannopoulos (National Technical University of
Athens, GR) Marcos Goncalves (UFMG, BR) Julio Gonzalo (UNED, SP) Brigitte Grau (LIMSI, FR) Mark A. Greenwood (University of Sheffield, UK) Daniel Gruhl (IBM, USA) Karl Gyllstrom (Katholieke Universiteit Leuven, BE) Preben Hansen (Swedish Institute of Computer Science, SE) Donna Harman (NIST, USA) Claudia Hauff (University of Twente, NL) Daqing He (University of Pittsburgh, USA) Andreas Henrich (University of Bamberg, DE) Katja Hofmann (University of Amsterdam, NL) Richang Hong (National University of Singapore, SG) Xian-Sheng Hua (Microsoft Research Asia, CN) Jimmy Huang (York University, CA) Zi Helen Huang (University of Queensland, AU) Juan Huete (University of Granada, SP) Theo Huibers (University of Twente, NL) Adam Jatowt (Kyoto University, JP) Heng Ji (City University of New York, USA) Jing Jiang (Singapore Management University, SG) Valentin Jijkoun (University of Amsterdam, NL) Rong Jin (Michigan State University, USA) Hideo Joho (University of Tsukuba, JP) Gareth J.F. Jones (Dublin City University, IE) Marko Junkkari (University of Tampere, FI) Theodore Kalamboukis (Athens University of Economics
and Business, GR) Jaap Kamps (University of Amsterdam, NL) Evangelos Kanoulas (University of Sheffield, UK) Jussi Karlgren (Swedish Institute of Computer Science, SE) Claus-Peter Klas (Distance University in Hagen, DE) Donald H. Kraft (Louisiana State University and U.S. Air
Force Academy, USA) Ravi Kumar (Yahoo! Research, USA) Oren Kurland (Technion, IL) Monica Landoni (University of Lugano, CH)

Guy Lapalme (Université de Montréal, CA) Birger Larsen (Royal School of Library and Information
Science, DK) Martha Larson (Delft University of Technology, NL) Neal Lathia (University College London, UK) Hady Lauw (Microsoft Research, USA) Victor Lavrenko (University of Edinburgh, UK) Matthew Lease (University of Texas at Austin, USA) Miro Lehtonen (University of Helsinki, FI) Jochen L. Leidner (Thomson Reuters Corp., USA) Tao Li (Florida International University, USA) Yuefeng Li (Queensland University of Technology, AU) Qing Li (Southwestern University of Finance &
Economics, CN) Wenhui Liao (Thomson Reuters Corp., USA) Elizabeth D. Liddy (Syracuse University, USA) Xia Lin (Drexel University, USA) Christina Lioma (Konstanz University, DE) Irene Lopatovska (Pratt Institute, USA) David Losada (University of Santiago de Compostela, SP) Robert W.P. Luk (The Hong Kong Polytechnic University,
HK) Andrew MacFarlane (City University London, UK) Craig Macdonald (University of Glasgow, UK) Joao Magalhaes (Universidade Nova de Lisboa, PT) Prasenjit Majumder (DAIICT, IN) Thomas Mandl (University of Hildesheim, DE) Byron Marshall (Oregon State University, USA) Maarten Marx (University of Amsterdam, NL) James Mayfield (Johns Hopkins University, USA) Paul McNamee (Johns Hopkins University, USA) Tao Mei (Microsoft Research Asia, CN) Edgar Meij (University of Amsterdam, NL) Massimo Melucci (University of Padua, IT) Donald Metzler (Yahoo! Research, USA) Vibhu Mittal (Root-1 Research, USA) Dunja Mladenic (Jozef Stefan Institute, SL) Masnizah Mohd (University of Strathclyde, UK) Edleno Silva de Moura (Universidade Federal do
Amazonas, BR) Masaki Murata (NICT, JP) Vanessa Murdock (Yahoo! Research, SP) Henning Müller (HES SO Valais, CH) Seung-Hoon Na (National University of Singapore, SG) Preslav I. Nakov (National University of Singapore, SG) Michael L. Nelson (Old Dominion University, USA) Chong-Wah Ngo (City University of Hong Kong, HK) Charles Nicholas (UMBC, USA) Andreas Nuernberger (Otto-von-Guericke University, DE) Giorgio Maria Di Nunzio (University of Padua, IT) Smut Ozertem (Yahoo! Labs, USA) Christos Papatheodorou (Ionian University, GR) Virgil Pavlu (Northeastern University, USA) Daniel Pedronette (University of Campinas, BR) Fuchun Peng (Yahoo! Inc, USA)

xxv

Ari Pirkola (University of Tampere, FI) Benjamin Piwowarski (University of Glasgow, UK) Vassilis Plachouras (Athens University of Economics and
Business, GR) Xiaoguang Qi (Lehigh University, USA) Tao Qin (Microsoft Research Asia, CN) Stephan Raaijmakers (TNO, NL) Hema Raghavan (Yahoo! Labs, USA) Edie Rasmussen (University of British Columbia, CA) Henning Rode (CWI Amsterdam, NL) Haggai Roitman (IBM Research Lab, Haifa, IL) Dmitri Roussinov (University of Strathclyde, UK) Miguel E. Ruiz (University of North Texas, USA) Jan Rupnik (Jozef Stefan Institute, SL) Delia Rusu (Jozef Stefan Institute, SL) Ralf Schenkel (Saarland University, DE) Pavel Serdyukov (Delft University of Technology, NL) Chao Shen (Florida International University, USA) Dou Shen (Microsoft Research, USA) Jialie Shen (Singapore Management University, SG) Rao Shen (Yahoo! Research, USA) Milad Shokouhi (Microsoft, UK) Mário J. Silva (University of Lisbon, PT) Fabrizio Silvestri (Information Science and Technology
Institute, IT) Laurianne Sitbon (Queensland University of Technology,
AU) Mark D. Smucker (University of Waterloo, CA) Eero Sormunen (Finland University Tampere, FI) Sofia Stamou (Patras University, Greece Ionian University,
GR) Benno Stein (Bauhaus-Universität Weimar, DE) Hussein Suleman (University of Cape Town, ZA) Aixin Sun (Nanyang Technological University, SG) S.M.M. (Said) Tahaghoghi (Microsoft, USA) Lynda Tamine-Lechani (Université Paul Sabatier, FR) Giannis Tsakonas (Patras University, GR) Jaime Teevan (Microsoft Research, USA)

Martin Theobald (Max-Planck Institute Informatics, DE) Ulrich Thiel (IPSI-Darmstadt, DE) James A. Thom (RMIT, AU) Paul Thomas (CSIRO Australian National University, AU) Paul Thompson (Dartmouth College, USA) Qi Tian (University of Texas at San Antonio, USA) Anastasios Tombros (Queen Mary University of London,
UK) Ricardo Torres (University of Campinas (UNICAMP), BR) Mitja Trampus (Jozef Stefan Institute, SL) Andrew Trotman (University of Otago, NZ) Christos Tryfonopoulos (University of Peloponnese, GR) Theodora Tsikrika (Centrum Wiskunde & Informatica, NL) Eduardo Valle (University of Campinas, BR) Olga Vechtomova (University of Waterloo, CA) Sumithra Velupillai (Stockholm University, SW) Arjen de Vries (Centrum Wiskunde & Informatica, NL) Xiaojun Wan (Peking University, CN) Jun Wang (University College London, UK) Haifeng Wang (Baidu.com Inc., CN) Taifeng Wang (Microsoft Research Asia, CN) Xin-Jing Wang (Microsoft Research Asia, CN) William Webber (University of Melbourne, AU) Wouter Weerkamp (University of Amsterdam, NL) Mingfang Wu (RMIT, AU) Baoning Wu (Snap Technologies, Inc., USA) Jun Xu (Microsoft Research Asia, CN) Qiang Yang (Hong Kong University of Science and
Technology, HK) Emine Yilmaz (Microsoft Research Cambridge, UK) Elad Yom-Tov (IBM Research Lab, Haifa, IL) Xiaojun Yuan (University at Albany, State University of
New York, USA) Dell Zhang (Birkbeck, University of London, UK) Zhongfei Zhang (State University of New York at
Binghamton, USA) Dong Zhou (Trinity College Dublin, IE) Guodong Zhou (Soochow University, TW)

Tutorials Committee

Djoerd Hiemstra (University of Twente, NL) - Chair Eugene Agichtein (Emory University, USA) Omar Alonso (Microsoft/Bing.com, USA) Maristella Agosti (University of Padova, IT) Nick Craswell (Microsoft, USA) Norbert Fuhr (University of Duisburg-Essen, GE) Jussi Kalgren (Swedish Institute of Computer Science, SW) Birger Larsen (Royal School of Library and Information
Science, DK)

Thomas Mandl (University of Hildesheim, GE) Donald Metzler (Yahoo!, USA) Marie-Francine Moens (University of Leuven, BE) Jian-Yun Nie (University of Montreal, CA) Paul Ogilvie (mSpoke, USA) Maarten de Rijke (University of Amsterdam, NL) Ian Ruthven (University of Strathclyde, UK) James G. Shanahan (AT&T Interactive, USA) Arjen P. de Vries (Centrum Wiskunde & informatica, NL)

xxvi

Workshops Committee

Omar Alonso (Microsoft/Bing.com, USA) - Chair Giambattista Amati (Fondazione Ugo Bordoni, IT) - Chair Roi Blanco (Yahoo! Research, SP) Wray Buntine (NICTA, AU) Gene Golovchinsky (FX PAL, USA) Panos Ipeirotis (NYU, USA) Diane Kelly (Univ of North Carolina at Chapel Hill, USA) Yoelle Maarek (Yahoo! Research, IL)

Jian-Yun Nie (University of Montreal, CA) Iadh Ounis (University of Glasgow, UK) Igor Perisic (LinkedIn, USA) Jeremy Pickens (FX Palo Alto Lab, Inc., USA) Maria Stone (Microsoft Research, USA) Martin Theobald (Max-Planck Institute Informatics, DE) Daniel Tunkelang (Google, USA) Hugo Zaragoza (Yahoo! Research, SP)

Best Paper Award Committee

Norbert Fuhr (Universität Duisburg-Essen, DE) - Chair Eugene Agichtein (Emory University, USA) Nick Belkin (Rutgers University, USA) Chris Buckley (Sabir Research, USA)

Jaana Kekäläinen (University of Tampere, FI) Alistair Moffat (University of Melbourne, AU) Maarten de Rijke (University of Amsterdam, NL)

Doctoral Consortium Committee

Douglas W. Oard (University of Maryland, USA) ­ Chair Paul Thomas (CSIRO, AU) ­ Chair Charles L A Clarke (University of Waterloo, CA) Susan Dumais (Microsoft Research Redmond, USA) Julio Gonzalo (UNED, SP) Noriko Kando (National Institute of Informatics, JP) Diane Kelly (Univ of North Carolina at Chapel Hill, USA)

Tie-Yan Liu (Microsoft Research Asia, CN) Alistair Moffat (University of Melbourne, AU) Iadh Ounis (University of Glasgow, UK) Howard Turtle (Syracuse University, USA) Maarten de Rijke (University of Amsterdam, NL) ChengXiang Zhai (University of Illinois at Urbana-
Champaign, USA)

Mentoring Committee

Ian Ruthven (University of Strathclyde, UK) - Chair Krisztian Balog (University of Amsterdam, NL) Mark Carman (University of Lugano, CH) Paul Clough (University of Sheffield, UK) David Elsweiler (University of Erlangen, DE) Gareth Jones (Dublin City University, IE) Joemon Jose (University of Glasgow, UK) Jaap Kamps (University of Amsterdam, NL) Diane Kelly (Univ of North Carolina at Chapel Hill, USA) Oren Kurland (Technion, IL) Mandar Mitra (Indian Statistical Institute, IN)

Alistair Moffat (University of Melbourne, AU) Vanessa Murdock (Yahoo! Research, SP) Iadh Ounis (University of Glasgow, UK) Benjamin Piwowarski (University of Glasgow, UK) Vassilis Plachouras (Yahoo! Research, SP) Fabrizio Silvestri (ISTI, IT) Benno Stein (Bauhaus University Weimar, DE) Ryen W. White (Microsoft Research, USA) Rong Yan (IBM Research, USA)

xxvii

Keynote Talk
Is the Cranfield Paradigm Outdated?
Donna Harman
National Institute of Standards and Technology Gaithersburg, Maryland, USA donna.harman@nist.gov
Abstract
The Cranfield paradigm was designed in the early 1960s when information access was via Boolean queries against manually indexed documents and there was (virtually) no text online. Cyril Cleverdon, Librarian of the College of Aeronautics, Cranfield, England, built a test collection that modeled university researchers, including abstracts of aeronautical papers, one-line queries based on questions gathered from the researchers, and complete relevance judgments for each query submitted by these users. The idea of carefully modeling some user application continued with Prof. Gerard Salton and the SMART collections, such as searching MEDLINE abstracts using real questions submitted to MEDLINE, or searching full text TIME articles with real questions from several sources, etc. A 1969 paper by Michael Lesk and Salton used experiments on the ISPRA collection to show that relevance judgments made by a person who was not the user would still allow valid system comparison, a precursor to the paper by Ellen Voorhees in SIGIR 1998.
Implementation of the Cranfield paradigm has undergone extensive modifications over the years in TREC and other evaluation forums as the data and tasks have gotten more complex. However it still stands as the model of choice, both for these (mostly) academic evaluations and at least partially for commercial evaluations, especially for straight-forward searching tasks where clicks and dwell times can be used to predict relevance. However the world of information access has exploded in recent years to encompass online shopping, social networking, personal desktop organization, etc. Is it time to have a new paradigm, and if so, how do we ensure that information retrieval evaluation remains scientifically valid?
Categories & Subject Descriptors: H.3.4 [Systems and Software]: Performance
evaluation
General Terms: Measurement, Experimentation
Bio
Donna Harman graduated from Cornell University as an Electrical Engineer, and started her career working with Professor Gerard Salton in the design and building of several collections, including the first MEDLARS one. Later work was concerned with searching large volumes of data on relatively small computers, starting with building the IRX system at the National Library of Medicine in 1987, and then the Citator/PRISE system at the National Institute of Standards and Technology in 1988. In 1990 she was asked by DARPA to put together a realistic test collection on the order of 2 gigabytes of text, and this test collection was used in the first Text Retrieval Conference (TREC). TREC is now in its 19th year, and along with its sister evaluations such as CLEF, NTCIR, INEX, and FIRE, serves as a major testing ground for information retrieval algorithms.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
1

Person Name Disambiguation by Bootstrapping

Minoru Yoshida
University of Tokyo 7-3-1, Hongo, Bunkyo-ku
Tokyo, 113-0033

Masaki Ikeda
University of Tokyo 7-3-1, Hongo, Bunkyo-ku
Tokyo, 113-0033

Shingo Ono
University of Tokyo 7-3-1, Hongo, Bunkyo-ku
Tokyo, 113-0033

mino@r.dl.itc.utokyo.ac.jp

ikeda@r.dl.itc.utokyo.ac.jp

ono@r.dl.itc.u-tokyo.ac.jp

Issei Sato
University of Tokyo 7-3-1, Hongo, Bunkyo-ku
Tokyo, 113-0033

Hiroshi Nakagawa
University of Tokyo 7-3-1, Hongo, Bunkyo-ku
Tokyo, 113-0033

sato@r.dl.itc.utokyo.ac.jp

nakagawa@dl.itc.utokyo.ac.jp

ABSTRACT
In this paper, we report our system that disambiguates person names in Web search results. The system uses named entities, compound key words, and URLs as features for document similarity calculation, which typically show high precision but low recall clustering results. We propose to use a two-stage clustering algorithm by bootstrapping to improve the low recall values, in which clustering results of the first stage are used to extract features used in the second stage clustering. Experimental results revealed that our algorithm yields better score than the best systems at the latest WePS workshop.
Categories and Subject Descriptors
H.3.3 [Information Storage Retrieval]: Information Search and Retrieval--Clustering; I.2.7 [Artificial Intelligence]: Natural Language Processing--Text analysis
General Terms
Languages
Keywords
person name disambiguation, Web people search, clustering
1. INTRODUCTION
World Wide Web (WWW) search engines are commonly used for learning about real-world entities, such as people. In such cases, users key the name of the target entity in search engines to obtain a set of Web pages that contain that
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ....$10. 00.

name. However, ambiguity in names (i.e., many entities having the same name) typically causes the search results to contain Web pages about several different entities.
For example, if we want to know about a "George Bush" other than the former U.S. president, many pages about the former president are returned in the search results, which may be problematic. Depending on the circumstances, we may have to search once more to find Web pages about the target person buried in the numerous unrelated ones.
Hereon, we will use the term "person name" to mean a string indicating the name of a person. Many studies have recently been carried out on disambiguating people's names, as was reported at the recent WePS (Web People Search) workshops [2, 3]. In this disambiguation task, the typical approach is to define similarities between documents based on features extracted from the documents, and cluster Web pages returned by search engines for person name queries by using the similarity. In terms of performance, named entities (NEs) have been reported as one of the most effective features for this task [9]. NEs are good features for distinguishing people because they concisely represent real-world concepts related to the people. For example, the person's name Paul Allen or the organization's name Microsoft indicate real-world entities that are related to the person Bill Gates. In addition, we also focus on Compound key words (CKWs) and URLs as additional useful features. These features show similar properties to NEs. For example, the compound noun chief software architect indicates a concept strongly related to Bill Gates. (These examples can be found in the Wikipedia article on Bill Gates.) Links to relevant pages such as the Microsoft homepage are also good indicators for distinction.
The problem we observed with such features is that they show high precision values but low recall values (i.e., they are not observed frequently but work as strong evidence for entity identification). One typical approach to improve the recall value is to reduce the threshold value for document similarities, which involves the merging of non-similar documents and typically worsens the precision.
Another typical approach is to use weak features for calculation of document similarities. Features to represent documents for clustering are mainly categorized into two types:

10

strong features and weak features. Strong features have the ability to clearly distinguish between clusters and weak features do not. We categorized named entities (NEs), compound key words (CKWs), and URLs, as mentioned above, as strong features, and single words as weak features. Although we can improve the recall value by using weak features, it typically worsens the precision in the same way as reducing the threshold value discussed above.
We solve this problem by distinguishing reliable weak features from others by using two-stage clustering algorithm. The two-stage clustering algorithm clusters documents by using only strong features in the first stage, and revises them by using weak features in the second stage. The algorithm gives weights to weak features by using bootstrapping techniques, which is popular in the natural language processing community. For example, if a computer scientist and a baseball player share the same name, words like memory, algorithm are reliable weak features for the former, and words like ball and batting are reliable weak features for the latter. We report that we can use word features effectively by our feature weighting algorithm. We call the clusters produced in the first stage first-stage clusters and the clusters produced in the second stage second-stage clusters.
Bootstrapping is a category of algorithms for instance extraction, which start with some seed instances and iterate some process to repeatedly improve the instances (i.e., extracted collections such as a set of movie names) and patterns (i.e., linguistic rules that co-occur with movie names, in this case). Typical bootstrapping algorithm selects instances and patterns according to some reliability scores. We apply Espresso [20], one of such bootstrapping algorithms, to the person name disambiguation problem. In our case, patterns correspond to weak features and instances correspond to documents newly added to clusters, while seed instances correspond to documents in each first-stage cluster. Reliability scores give high weights to useful weak features and low weights to useless weak features. The experimental evaluation showed that using this algorithm dramatically improved the performance.
We compared the bootstrapping approach with two baseline methods for the second-stage clustering: compound key words and latent topics. The former uses strong features only, and the latter uses weak features. We observed that the bootstrapping algorithm showed the best performance, which suggests that bootstrapping approaches can get the most out of the ability of weak features.
Two-stage clustering has the same aim as pseudo-relevancefeedback for document retrieval in that both extract new features from a set of documents in the first stage. The main difference is that our purpose is not to produce words used as queries, but to refine clustering results. We therefore can choose other types of features to be extracted from documents than the features typically used for pseudo-relevancefeedback. Another difference is that we applied feature extraction to all the resulting clusters while pseudo-relevancefeedback does not focus on the ambiguity of a query itself. Because our purpose is to distinguish documents related to the same (person name) query, the differences between documents to be separated are often small. We therefore need more careful treatment of features to make distinctions among different clusters.
The word sense disambiguation (WSD) problem, extensively studied by the natural-language-processing commu-

nity, is a task usually compared to the person name disambiguation problem [3]. Although some of the results can be used to solve our name disambiguation problem, there are also several important differences between these two disambiguation tasks. For example, the number of true entities is not known in advance in name-disambiguation problems while in WSD the task is to categorize each word into one of several predefined senses. Another difference is the knowledge source. Dictionaries or thesauruses that describe each sense are available in most word disambiguation cases, and are used in the algorithm as sources to discriminate the sense.
The remainder of this paper is organized as follows. Sections 2 and 3 explain our task and describe related work, respectively. Sections 4 and 5 explain our framework. Section 6 evaluates our framework with an actual Web document dataset. Section 7 summarizes our work.
2. TASK DEFINITION
Our task, the disambiguation of person names appearing on Web pages, is formalized as follows. The query (target person name) is referred to as q. The set of Web pages obtained by inputting query q to a search engine is denoted by P = {d1, d2, · · · , dk}. Each Web-page di has at least one string q. We assume that q on the same page refers to the same entity. Therefore, person name disambiguation is achieved by document clustering where each cluster refers to a single entity. The input of the algorithm is query q. The output of the algorithm is a set of page clusters.
In this paper, we use the term features to indicate strings extracted from documents. The features include NEs, CKWs, URLs, and words.
3. RELATED WORK
Several important studies have tried to solve the task described in the previous section. Bagga and Baldwin [4] applied the vector space model to calculating similarities between names only using co-occurring words. Based on this, Niu et al. [17] presented an algorithm that uses informationextraction results in addition to co-occurring words. However, these methods had only been tested on small artificial test data, raising doubts as to their suitability in practical use. Mann and Yarowsky [15] employed a clustering algorithm to generate person clusters based on extracted biographic data. However, this method was also only tested on artificial test data. Wan et al. [24] proposed a system that rebuilt search results for person names. Their system, called WebHawk, was aimed at practical use like our systems, but their task was somewhat different. Their system was designed for actual queries that occurred frequently. The algorithm in their system was specialized for English person-name queries that consisted of three words: family name, first name, and middle name. They mainly assumed queries such as "<first name>" or "<first name> <family name>", and took middle names into consideration, which may have improved accuracy. So the problem setting of them is different from ours.
In another approach to this task, Bekkerman and McCallum [6] proposed two methods of finding Web pages that refer to a particular person. Their work consisted of two distinct mechanisms. The first was based on a link structure and the second used agglomerative/conglomerative double

11

clustering. However, they focused on disambiguating an existing social network of people, which is not the case when searching for people in real situations. In addition, as our experience is that the number of direct links between pages that contain the same name are fewer than expected, information on link structures would be difficult to use to resolve our task. Although there may be indirect links (i.e., one page can be found from another page via other pages), it is far too time consuming to find these.
The method proposed by Bollegala et al.[7] used extracted keywords to calculate similarities between documents. They further extracted keywords from resulting clusters. However, their research was aimed at keyword extraction itself.
Bunescu et al.[8] reported using Wikipedia knowledge to disambiguate named entities. They used Wikipedia to extract features for supervised learning. Using knowledge sources like Wikipedia is an interesting direction for named entity disambiguation in general, but in our case it is difficult to use them because our targets contain many minor person names that are not defined in Wikipedia.
3.1 Web People Search Task (WePS)
A large workshop for disambiguating person names, called WePS, was held in 2007[2]. The workshop provided a common dataset for research on person-name disambiguation. Sixteen systems were introduced for participation at the workshop. Most of their methods can be categorized into one of the approaches described in the previous section. Their methods typically involved some preprocessing such as POS tagging and NE extraction, calculating similarities between documents, and creating clusters according to the calculated similarities. In 2009, the second WePS workshop[3] was also held.
Named entities were one of the most effective features for the task[2][9]. Preprocessing such as filtering out feature values from some "valueless" pages was also important. The best system used both "cleaning" of documents and selection of useful features such as named entities.
Several studies that have used the WePS corpus have been reported[12][5].
[12] extensively used named entities. They extracted person names and organization names from the documents, and calculated the similarities between documents using web counts of their co-occurrence. Although their method performed extremely well, counting from the Web required massive amounts of time (because they needed to query the Web search engines about 40,000 times for each cluster), making it difficult to use in real-time systems (they stated their system was better suited for servers.)
[5] proposed a simple algorithm that used Single-Pass Clustering with a bag-of-words model, which attained performance that was comparable to the best system at WePS. Their idea of making use of positions (ranks) in search results and HTML trees to extract relevant blocks for the query was complementary to ours, which can improve the performance of our algorithm. We plan to incorporate their method in our framework in the future.
3.2 Two-Stage Clustering
Previous research has tackled clustering problems with two-stage clustering approaches. Tishby et al. [23] proposed the information-bottleneck method, which finds the optimal clusters according to an information-theoretic measure for

cluster goodness. Slonim et al. [22] applied this method to document clustering by a double-clustering approach, which extracts word clusters that preserve information about the document clusters, and used the extracted word clusters in turn to cluster documents. These previous methods were for document clustering in general, which are difficult to be directly applied to person name disambiguation problem because, as discussed in the introduction, general term (word) frequencies or TF-IDF scores are not so effective for this problem compared to general document clustering problems. The two-stage clustering approach was not used in any WePS-1 system. Two systems at WePS-2 used twostage clustering approaches[18][10], but both of them used no weak (term frequency) features.
Liu et al. [14] proposed extracting effective features from the first-stage clustering results and using the features for the second-stage clustering by feature voting. They observed that named entities and term pairs were salient features for identifying documents in the same clusters, and proposed to use them as well as ordinary term frequency features. One contribution by ours is applying a framework of bootstrapping algorithms to model such feature weighting methods via two-stage clustering in an elegant way. Moreover, they used all the features simultaneously. However, our preliminary investigation and previous reports suggest that, as mentioned above, simply blending term frequency features with NE features generally little contribution to person name disambiguation. This means that the strong features and term frequency features should be used separately. Our two-stage algorithm models this hierarchy of document features. Weak features are used only in the second-stage clustering while the first-stage clustering is performed by using strong features only.
4. FIRST-STAGE CLUSTERING
In this and the next section, we describe our two-stage clustering algorithm. As mentioned in the introduction, we categorize the features for representing documents into strong features, including NEs, CKWs, and URLs, and weak features, including single words, where the former have strong discriminative power and the latter do not. Our method uses only strong features to make the first-stage clusters and uses weak features to supplement them, where this use is guided by the first-stage clusters.
The algorithm proceeds as follows: (1) Make clusters on the basis of the similarities calculated by strong features, and (2) Find documents highly related to each cluster through weak features and add them to the cluster.
This section describes the first-stage clustering in detail, and the next section provides the second-stage clustering.
4.1 Preprocessing
We used lxml 1 and a sentence segmenter 2 to convert HTML files to text files that consist of sentences. We next extracted local text around each query string by using the window sizes set as parameters3. Tree Tagger 4 was used
1http://codespeak.net/lxml/ 2http://www.answerbus.com/sentence/ 3We tested four different window-size parameters (50, 100, 200, and all words) in preliminary experiments and chose the best setting for each feature (100 for CKW, all for NE). 4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/

12

to add part-of-speech tags to each word. We used Stanford NER5 for identifying person, place, and organization names.
In addition, URL strings were extracted from the original
HTML files.

4.2 Document Features
In the first stage, three types of features (strings) are extracted from documents: named entities, compound key words, and URLs. In this subsection, we explain these features in detail.

4.2.1 Named Entity Features
We used person names, organization names, and place names as the named entities used as features. They typically represent real-world entities related to the person.
While person names were used as is, some location names and organization names were filtered out by using stop-word lists that list the location/organization names that have high frequencies.

4.2.2 Compound Key Word Features
We also use compound key words as features. We describe how to extract them here.
First, we calculate the importance score for compound words in a document with the method proposed by Nakagawa et al. [16].
The importance score for the compound words is calculated as follows: Let CW (= W1W2 · · · WL) be a compound word, where Wi (i = 1, 2, · · · , L) is a simple noun. f (CW ) is the number of independent occurrences of compound word CW in a document where "independent" occurrence of CW means that CW is not a part of any longer compound nouns. The importance score of compound word CW is

Score(CW ) = f (CW ) · LR(CW ),

(1)

LR(CW ) is defined as follows:

!1

YL

2L

LR(CW ) = (LN (Wi) + 1)(RN (Wi) + 1)

(2)

i=1

LN (Wi) and RN (Wi) are the frequencies of nouns that directly precede or succeed simple noun Wi. We extracted the compound words that have a score higher than the threshold value CKW as CKW features.
This score is defined based on the intuition that some words are used as term units more frequently than others, and a phrase that contains such "good" term units is likely to be important. Figure 1 outlines example statistics for the appearance of compound words in a corpus, which include "disaster information", and "information security" three times each, "information system" once, and "information ethics" two times. In this case, for example, LN (Inf ormation) = 3 and RN (Inf ormation) = 3+1+2 = 6.

4.2.3 Link Features
We extract URLs (in <a> tags) from each document and use them as link features. Link features also include the URL of the document itself. URLs with high frequencies were discarded in the same way as in the location/organization name filtering described in section 4.2.1.
5http://nlp.stanford.edu/software/CRF-NER.shtml

4.3 Document Similarities
In this subsection, we describe how to calculate document similarities using the extracted features. We used the overlap coefficient[21] defined below to calculate similarities between documents.

Overlap(dx, dy)

=

|f x  fy| max(min(|fx|, |fy|), overlap)

where fx and fy are sets of features extracted from documents dx and dy, respectively. overlap is a threshold value to avoid too small denominator values in the equation, which
is currently set to overlap = 4 determined on the training data. Similarities by NEs (simNE) and by CKWs (simCKW) are defined by this overlap coefficient, where fx is a set of NEs in dx for simNE, and a set of CKWs in dx for simCKW.
The definition of similarities by URLs (simURL) is slightly different. Link similarity simURL is defined as follows.

(

simURL(dx, dy) =

1 if dx links to dy or vise versa. Overlap(dx, dy) otherwise

(3)

4.4 Calculation of Merged Similarities
In this subsection, we describe how to merge different similarity scores. First, we define the merged similarity of different types of NEs. Next, we define the merged similarity of NEs, CKWs, and URLs.

4.4.1 Merging Different Similarities: for Named Entities
Different similarity scores are calculated for different types of named entities, namely, person names, location names, and organization names. We take the linear interpolation of these different scores:

simNE(dx, dy) = P simP (dx, dy) + LsimL(dx, dy)

+OsimO(dx, dy)
where P + L + O = 1. We set these values to be P > O > L6.
4.4.2 Merging Different Similarities: for NE, CKW, and LINK
We define the merged similarity score given the set of these different similarity values. The new similarity score is
6Currently, we use the parameters P = 0.78, O = 0.16, and L = 0.06 tuned by the training data.

Disaster 3 times

Security 3 times

, ,

Information

, l

System 1 times

l l Ethics 2 times

Figure 1: Example of statistics for term unit connections

13

calculated by taking the maximum of the given similarity values of NE, CKW, and LINK as follows.

simmax(dx, dy) = max(simNE(dx, dy),
simCKW(dx, dy), simURL(dx, dy))(4)
4.5 Clustering Algorithm
With the document similarities calculated by the methods described above, the algorithm makes clusters of documents.
We used the standard hierarchical agglomerative clustering (HAC) algorithm for clustering. This algorithm starts from one-in-one clustering (each document is a size-one cluster) and iteratively merges the most-similar cluster pairs. The parameter of HAC is the similarity threshold value and does not need the number of clusters. We used the averagedistance approach for defining inter-cluster distances. In this approach, similarity between clusters Ci, Cj is defined as equation (5). simmax(dx, dy) is the above-mentioned similarity scores.

1 XX

sim(Ci, Cj ) = |Ci||Cj | dxCi dyCj simmax(dx, dy )

(5)

5. SECOND-STAGE CLUSTERING
This section describes the second-stage clustering. It uses the bootstrapping approach which is achieved by matrix multiplication.

5.1 Second Stage Clustering by Bootstrapping

Our approach is to apply bootstrapping algorithm to the

person name disambiguation. Bootstrapping is a method

used originally to extract a set of instances (e.g., country

names) and patterns iteratively. It starts with some seed

instances and finds patterns that are useful to extract such

seed instances (e.g., "*'s prime minister"). These patterns

are in turn used to harvest new instances, and from the

harvested new instances new patterns are induced. The al-

gorithm repeats these steps until convergence criteria are

fulfilled.

Espresso [20] is a well-known algorithm for information ex-

traction that was proposed for harvesting semantic relations.

It finds in documents word pairs that have relations simi-

lar to the given seed pairs, e.g., finds a pair (Ford, Nixon) given the pair (Bush, Regan).7 It extracts instances and ex-

traction patterns iteratively, and selects the instances and

patterns according to the reliability function defined based

on self-mutual-information values. Komachi et al. [13] pro-

vided a theoretical analysis for Espresso by representing it as

HITS-like matrix multiplication. We hereinafter borrow this

matrix representation for the Espresso algorithm from [13].

They represent Espresso algorithm8 by the matrix multipli-

cation

as

i(t+1)

=

1 |I||T |

· MT Mi(t).Here,

the

matrix

M

rep-

resents strength of connections between instances and pat-

terns, and vector i represents a cluster of instances where ij

is the weights (called reliability) to the j-th instance. Start-

ing with the initial vector i(0) which represents a cluster

7Both have the relation "succession". 8This representation is for the algorithm they call simiplfied Espresso in which some filtering steps after each iteration are omitted from the original Espresso. The bootstrapping algorithm we used is also this simplified Espresso algorithm.

of seed instances, we can calculate the weights of instances

obtained by bootstrapping by multiplying M, MT , and the

normalizing

factor

1 |I||T |

to

the

initial

vector

repeatedly.

We try to apply it to the person name disambiguation

problem. In our problem settings, instances and patterns

discussed above correspond to documents and (weak) fea-

tures. Given the first-stage clusters, the algorithm regards

them as seed instances, finds weak features related to them,

and finds new instances (new documents, in our case) by

using the weak features (as extraction patterns). One dif-

ference between our representation and the above one is that

we use matrix R instead of vector i because we have more

than one cluster. In our matrix representation, each column

vector in matrix R represents each cluster. We can represent

updating of clusters simultaneously by using R.

If we define a feature-document matrix P, which has strength

of relations between the ith feature and jth document in

its (i,j) element and denote a document-cluster matrix by

R(Dt) = {rd,C }, our algorithm can be formulated as R(Dt+1) =

1 |D||F |

· PTPR(Dt)

(|D|

and

|F |

are

constants

and

do

not

affect

the results in practice.)

Figure 2: Second-Stage Clustering by Bootstrapping
Figure 2 illustrates the meaning of this matrix multiplication. Here, feature fi is connected to document dj if fi is contained in dj (i.e., the element pi,j in feature-document matrix P is not zero). Documents d1, d2, d3, and d4 are in the same initial cluster and this is represented in the document-cluster matrix by setting r1(0,k) = r2(0,k) = r3(0,k) = r4(0,k) = 1 if the cluster ID is k. Such clustering information is propagated to the feature-cluster matrix R(f0,C) through the document-feature matrix P . That is, feature-cluster relation weights are obtained by multiplying R(d0,C) by P. In resulting weight matrix Rf,C, features strongly related to the k-th cluster are given high weights in the k-th column. After that, new document-cluster matrix R(d1,C) is obtained by multiplying R(f0,C) by PT , which propagates the featurecluster relation weights to the new document-cluster relation weights.
5.1.1 Algorithm
This section describes our bootstrapping algorithm. Here, we assume that first-stage clusters of size 2 or more are seed instances. The remaining documents (i.e., documents

14

Algorithm 1 Bootstrapping Algorithm for Person Name Disambiguation
Procedure: D, F, R(D0) Step-1: // Calculation of Feature-Document Matrix P

(

P[f, d] =

1 max pmi

log

p(f,d) p(f )p(d)

0

if

p(f,d) p(f )p(d)

>

1

, (f



F, d



D)

otherwise

where max pmi = max(P[f , d ]) (f  F, d  D)

fort  0, · · · , T - 1 // T : Number of Iterations

Step-2:

R(Ft)

=

1 |D|

PR(Dt)

Step-3:

R(Dt+1)

=

1 |F

|

PT R(Ft)

endfor

Step-4:
for C  C do Cd(T ) = arg maxC rd(T,C) where {C |(C  C  |C| > 1)  Cd(0)} endfor Define C(T ) based on Cd. return C(T )

not connected to any other document) are the sources from
which the algorithm extracts new instances for each cluster.
Algorithm 1 shows our bootstrapping algorithm. Here, P is a feature-document matrix, R(Dt) = {rd,C} is a documentcluster matrix, and R(Ft) = {rf,C } is a feature-cluster matrix. Our definition of document-feature matrix P is the same as
in the Espresso [20] algorithm, which uses the self-mutual information. The algorithm updates R(Dt) and R(Ft) iteratively by multiplying P by P T , resulting in refinement of
the document-cluster matrix. The initial document-cluster matrix R(D0) is generated by setting rd(0,C) = 1 when d  C in the first-stage clustering result C and rd(0,C) = 0 if otherwise.
The algorithm is explained in detail below.

6.1 Data Sets and Baselines
We used the latest dataset for person name disambiguation: WePS-2 clustering task data set. The WePS-2 test set9 consists of 30 names, each of which has 150 pages. The all-in-one baseline is the result when all documents belong to one cluster. The one-in-one baseline is the result when each document is a size-one cluster. The combined baseline is a mixture of these two baselines, where each document belongs to one cluster from the all-in-one baseline and another cluster from the one-in-one baseline. Note that the same document can refer to two or more entities in these data sets.
We used two baselines for the evaluation of the secondstage clustering. The first baseline is the TOPIC algorithm proposed by Ono et al.[19]. This algorithm estimate the latent topic (e.g., sports, computer science, arts, etc.) for each document by using the probabilistic generative model called the Dirichlet process unigram mixture where parameters are initialized by the first-stage clustering results. If two clusters share the same topic10, they are merged into one cluster.
The second baseline is the CKW algorithm proposed by Ikeda et al.[11]. This second-stage clustering algorithm reextracts CKWs from the resulting clusters of the first stage in contrast with the fact that CKWs for the first stage clustering are extracted from each document. The concept behind the algorithm is that CKWs extracted from clusters are more reliable than the ones extracted from documents because clusters contain more words than documents, and therefore the former provides more reliable statistics of term frequency for the keyword extraction algorithm than the latter. If documents in small-size clusters share the same reextracted CKWs with large clusters, these documents are moved to the large cluster.
We used 1-gram and 2-gram features, excluding some stop words, to represent the documents for the second-stage clustering. The weight for each feature was defined by using TFIDF scores, where the IDF values were estimated by using Web 1T 5-gram11.

1. In step 1: A feature-document matrix P is generated. Notice that entries whose corresponding self-mutual information is below zero are set to zero.
2. In step 2: A feature-cluster matrix R(Ft) is calculated from P and the current document-cluster matrix R(Dt).

6.2 Evaluation Measures
The extended B-Cubed measure[1] was the measure adopted at the second WePS workshop held in 2008­2009[3].
Assume that L(e) and C(e) correspond to the collect class and machine-predicted class of e. Multiplicity precision and recall between e and e are calculated as

3. In step 3: A document-cluster matrix R(Dt+1) is calculated from P and the current feature-cluster matrix R(Ft).
4. In step 4: Find cluster C  C for each document d that maximizes the relation value rd,C where C  C, |C |  2, and d  C (0).
The algorithm iterates steps (2) and (3), and the final result C is generated from document-cluster matrix.
6. EXPERIMENTS
In this section, we report our experimental results on the WePS-2 data set.

M P rec(e, e )

=

M in (|C(e)  C(e )|, |L(e)  L(e )|) |C(e)  C(e )|

M Rec(e, e )

=

M

in

(|C(e)  C(e |L(e) 

)|, |L(e) L(e )|



L(e

)|)

,

Extended B-Cubed precision (BEP) and recall (BER) are calculated as

9http://nlp.uned.es/weps/weps-2-data/ 10The topic of each cluster is defined as the most frequent topic for documents in the cluster. 11http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?catalogId=LDC2006T13

15

Table 1: Results: WePS-2 Data Set

Topic

BEP BER FB

Baseline

ALL IN ONE 0.43 1.00 0.53

ONE IN ONE 1.00 0.24 0.34

COMBINED 0.43 1.00 0.52

First-Stage Clustering

ORIGINAL 0.92 0.70 0.78

Second-Stage Clustering

TOPIC

0.94 0.70 0.79

CKW

0.87 0.77 0.81

BOOTSTRAP

1-gram, T = 1 0.89 0.82 0.85

1-gram, T = 2 0.66 0.91 0.73

1-gram, T = 3 0.53 0.95 0.63

2-gram, T = 1 0.92 0.70 0.78

WePS top 3

1st

0.87 0.79 0.82

2nd

0.85 0.80 0.81

3rd

0.93 0.73 0.81

"

#

BEP = Avg

^

~

Avg

M P rec(e, e )

e e , C(e)C(e )=

"

#

^

~

BER = Avg

Avg

M Rec(e, e )

e e , L(e)L(e )=

Here, Avg [·] is the average through e. The F-measure is

e

the

harmonic mean of the two, F

=

`1

1 2

1 BEP

+

1 ´.
BER

6.3 Results
We tested our algorithm on the WePS-2 data set with BEP-BER measures.
We show the results in Table 1. We have a parameter f that determines the threshold value in the HAC algorithm used in the first-stage clustering. The value of f was determined by using the training data.
"ORIGINAL" represents the results when we used the first-stage clustering only. "TOPIC" and "CKW" are baselines for the second-stage clustering. "BOOTSTRAP" represents our bootstrapping-based algorithm. The features used in the second-stage clustering are represented by "1gram" and "2-gram", and the number of iterations in the second-stage clustering is T .
The two-stage clustering algorithm with CKW improved the results from ORIGINAL. Using topic features also improved the performance but the improvement was small. We believe the reason the TOPIC features did not work well is that TOPIC treats all single words equally without giving any weight to each word, while the bootstrapping approach can give weight to each weak feature through the calculation of reliability scores. BOOTSTRAP with 1-gram features and T = 1 performed the best (0.85 in B-Cubed f-measure.) among all the systems. We believe that one of the reasons our method performed well was that the first-stage clustering had already shown good precision values for the WePS-2

data set. This was good for our algorithm, which assumed high-precision clusters at the first stage.
The score of "BOOTSTRAP" was better than that of "CKW" by 0.04 points, which was even higher than the score of top-ranked system at WePS-2.
The use of 2-gram features did not contribute to improving the performance. (We observed no changes in clusters at the first iteration, so the second and third iterations were omitted.) We believe the reason was that the 2-gram features were "sparse" and our bootstrapping algorithm requires a large number of appearances of weak features. Increasing the iteration number for "1-gram" makes the precision values severely worse, and the improvement of recall values could not cover this performance drop. We believe the reason was that, in T = 2, 3 cases, weak features were overestimated and documents having only weak relations to each cluster were merged into the cluster.
We also tested the Von Neumann Kernel and Graph Laplacian reported in [13], which models the infinite number of iterations in efficient ways, but no improvement was observed. This result and the previous result that T = 1 was better than T = 2, 3 suggest that it is sufficient to do the iteration only once, which is the weight propagation from documents to features followed by the weight propagation in the opposite direction (i.e., from features to documents.) Iterating this procedure more than once causes too much overspreading of features, which results in low precision. However, it is possible that there is the optimum solution is in between T = 0 and T = 1, or between T = 1 and T = 2. Searching for these "intermediate" states by introducing a method to slow the propagation of weights is an interesting future direction.
7. CONCLUSION
We proposed a new algorithm for person name disambiguation. It consists of two stages where the results of the first-stage clustering are used to extract features for the second-stage clustering by applying the Espresso bootstrapping algorithm. We compared our method with various methods including two baseline two-stage clustering methods and WePS top systems and found that our method outperformed all of them. Future work includes testing our method on a greater variety of domains and improving the preprocessing algorithm to filter out useless documents.
Acknowledgments This work was supported in part by MEXT Grant-in-Aid for Scientific Research on Priority Areas: "Cyber Infrastructure for the Information-explosion Era" and MEXT Grant-in-Aid for Scientific Research (A).
8. REFERENCES
[1] E. Amigo, J. Gonzalo, J. Artiles, and F. Verdejo. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4), 2009.
[2] J. Artiles, J. Gonzalo, and S. Sekine. The SemEval-2007 WePS evaluation: Establishing a benchmark for the web people search task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 64­69, 2007.
[3] J. Artiles, J. Gonzalo, and S. Sekine. WePS 2 Evaluation Campaign: overview of the Web People

16

Search Clustering Task. 2nd Web People Search Evaluation Workshop (WePS 2009), 2009.
[4] A. Bagga and B. Baldwin. Entity-based cross-document coreferencing using the vector space model. In Proceedings of COLING-ACL 1998, pages 79­85, 1998.
[5] K. Balog, L. Azzopardi, and M. de Rijke. Personal name resolution of web people search. In WWW2008 Workshop: NLP Challenges in the Information Explosion Era (NLPIX 2008), 2008.
[6] R. Bekkerman and A. McCallum. Disambiguating web appearances of people in a social network. In Proceedings of The 14th International World Wide Web Conference (WWW2005), pages 463­470, 2005.
[7] D. Bollegala, Y. Matsuo, and M. Ishizuka. Extracting key phrases to disambiguate personal name queries in web search. In Proceedings of the Workshop: How can Computational Linguistics improve Information Retreival? at COLING-ACL 2006, pages 17­24, 2006.
[8] R. Bunescu and M. Pasca. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), 2006.
[9] E. Elmacioglu, Y. F. Tan, S. Yan, M.-Y. Kan, and D. Lee. PSNUS: Web people name disambiguation by simple clustering with rich features. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 268­271, 2007.
[10] M. Ikeda, S. Ono, I. Sato, M. Yoshida, and H. Nakagawa. Person Name Disambiguation on the Web by Two-Stage Clustering. 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference, 2009.
[11] M. Ikeda, S. Ono, I. Sato, M. Yoshida, and H. Nakagawa. Person name disambiguation on the web by twostage clustering. In Proceedings of the 2nd Web People Search Evaluation Workshop (WePS 2009) at WWW-2009, 2009.
[12] D. Kalashnikov, R. Nuray-Turan, and S. Mehrotra. Towards breaking the quality curse.: a web-querying approach to web people search. In Proceedings of SIGIR '08, pages 27­34, 2008.
[13] M. Komachi, T. Kudo, M. Shimbo, and Y. Matsumoto. Graph-based Analysis of Semantic Drift in Espresso-like Bootstrapping Algorithms. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1010­1019, 2008.

[14] X. Liu, Y. Gong, W. Xu, and S. Zhu. Document clustering with cluster refinement and model selection capabilities. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 191­198, 2002.
[15] G. S. Mann and D. Yarowsky. Unsupervised personal name disambiguation. In Proceedings of CoNLL2003, pages 33­40, 2003.
[16] H. Nakagawa and T. Mori. Automatic term recognition based on statistics of compound nouns and their components. Terminology, 9(2):201­219, 2003.
[17] C. Niu, W. Li, and R. K. Srihari. Weakly supervised learning for cross-document person name disambiguation supported by information extraction. In Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL-2004), pages 598­605, 2004.
[18] R. Nuray-Turan, Z. Chen, D. Kalashnikov, and S. Mehrotra. Exploiting web querying for web people search in weps2. 2nd Web People Search Evaluation Workshop (WePS 2009), 2009.
[19] S. Ono, I. Sato, M. Yoshida, and H. Nakagawa. Person name disambiguation in web pages using social network, compound words and latent topics. In Proceedings of the 12th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD2008), pages 260­271, 2008.
[20] P. Pantel and M. Pennacchiotti. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 113­120, 2006.
[21] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl. Grouplens: An open architecture for collaborative filtering of netnews. In Proceedings of CSCW 94 Conference on Computer Supported Cooperative Work, pages 175­186. ACM Press, 1994.
[22] N. Slonim and N. Tishby. Document clustering using word clusters via the information bottleneck method. Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 208­215, 2000.
[23] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. Proceedings of the 37-thAnnual Allerton Conference on Communication, 2000.
[24] X. Wan, M. L. J. Gao, and B. Ding. Person resolution in person search results: WebHawk. In Proceedings of CIKM2005, pages 163­170, 2005.

17

Temporal Click Model for Sponsored Search

Wanhong Xu
Carnegie Mellon University 5000 Forbes Ave
Pittsburgh, PA 15213
wanhong@cmu.edu

Eren Manavoglu
Yahoo! Labs 4401 Great America Parkway
Santa Clara, CA 95054
erenm@yahoo-inc.com

Erick Cantú-Paz
Yahoo! Labs 4401 Great America Parkway
Santa Clara, CA 95054
erick@yahoo-inc.com

ABSTRACT
Previous studies on search engine click modeling have identified two presentation factors that affect users' behavior: (1) position bias: the same result will get a different number of clicks when displayed in different positions and (2) externalities: the same result might get more clicks when displayed with results of relatively lower quality than when shown with higher quality results. In this paper we focus on analyzing the sequence of user actions to model users' click behavior on sponsored listings shown on the search results page. We first show that temporal click sequences are good indicators of externalities in the advertising domain. We then describe the positional rationality hypothesis to explain both the position bias and the externalities, and based on this hypothesis we further propose the temporal click model (T CM), a Bayesian framework that is scalable and computationally efficient. To the best of our knowledge, this is the first attempt in the literature to estimate positional bias, externalities and unbiased user-perceived ad quality from user click logs in a combined model. We finally evaluate the proposed model on two real datasets, each containing over 100 million ad impressions obtained from a commercial search engine. The experimental results show that T CM outperforms two other competitive methods at click prediction.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; G.3 [Probability and Statistics]
General Terms
Algorithms, Experimentation
Keywords
Sponsored search, advertising, externalities, Bayesian model, click log analysis, click-through rate
This work was done when the first author was on a summer internship with Yahoo! Labs.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Commercial web search engines typically generate revenue by presenting sponsored results as well as organic web results to satisfy a user query. The most commonly employed payment model is "pay-per-click", where an advertiser pays the search engine when a user clicks on their ad [4]. The cost of a click depends on the quality and bid of competing ads, and is usually determined by a second price auction [10]. Sponsored search enables the advertisers (1) to target their campaigns to very specific markets by bidding only on the search terms interesting to them, (2) to explore different markets with minimal risk (there is no cost unless there is a click), and (3) to iterate and improve quickly their campaigns by using immediate feedback from performance. The combination of these features makes sponsored search one of the most attractive and profitable advertising approaches [11].
The objective of the search engine in the sponsored search model is to maximize its revenue over the long term [2]. This involves a delicate balance of possibly conflicting objectives: (1) maximize the revenue per search (RPS), (2) minimize the negative impact of ads on the user experience, and (3) maximize the return on investment for advertisers. Estimating the probability that users click on ads displayed in response to their queries is essential to sponsored search, because accurate predictions are necessary to address the objectives above. In particular, the click probability is a factor in ranking, placement, filtering, and pricing of ads.
Several studies have been published recently analyzing user click behavior in organic web search [1, 15, 16, 5, 9, 6, 13] and in sponsored search advertising [17, 19, 20]. Joachims et al. [15, 16] conducted an eye-tracking experiment to understand the decision making process of users when browsing search engine results. An important finding of this study, now well known as position bias, is that users tend to click less on documents that are shown in lower positions, even when the results were presented in reverse order. To explain the position bias phenomenon, Richardson et al. [19] proposed the examination hypothesis, which assumes that a result must be examined before being clicked, and the probability of being clicked after being examined depends on its user-perceived quality. Craswell et al. [7] later proposed the cascade hypothesis, which assumes that users always examine results in order from top to bottom. Under this assumption, results displayed at the top are more likely to be examined than results shown at the bottom, regardless of their quality. The cascade model proposed in [7] makes another strong assumption: the user session ends after the first click on a result. This assumption clearly fails to ex-

106

Figure 1: Statistical analyses on two real datasets SAD and NAD (see Sec. 2 for data description): The left four graphs are statistical analyses of NAD: The top two graphs show that the CTR of ads at one position does not always increase as that at the other position increases, which proves the existence of ad externalities; The bottom two graphs reveal that when the CTR at one position increases, the average percentage of the first click being at the other position decreases, which suggests that temporal click information could be useful to learn externalities. The right four graphs are statistical analyses for SAD.

plain sessions with multiple clicks. Most of the subsequent research on click modeling for search results have focused on relaxing this assumption. The Click Chain Model [13] and the Bayesian Browsing Model (BBM) [18] both allow the user session to continue with a probability that is dependent on the relevance of the clicked ad. The dynamic Bayesian network model proposed by Chapelle and Zhang [6] also lets the user session continue after a click, but in their model the probability of continuing the session depends on the quality of the landing page. Their model is the first to separate the perceived relevance of the ads from the actual quality of the landing page.
The click models discussed so far do not account for the attractiveness or relevance of the results below when considering the probability of click on a particular ad. In Guo's [13] and Chapelle's [6] models, the probability of click for the results might be affected by the results shown immediately above them, since the examination probability depends on the previous result's (or landing page's) quality. However, the click probabilities cannot be influenced by the results presented below. This can be a significant limitation, especially in sponsored search, where the advertisers compete and pay for the user's attention. In advertising, the effect of the set of ads on the user behavior is widely accepted and referred to as externalities. Ghosh et al. [12] proposed the rationality hypothesis to explain this behavior. Under their hypothesis users are assumed to be rational. Given a set of ads displayed, a user first compares the qualities of the ads and clicks on the best one. Kempe and Mahdian [17] later proposed a variation of the cascade model based on this hypothesis. Instead of using the top to bottom order of scanning the ads, their model allows each user to have a different ordering over the positions. Unfortunately both of these models were used only in the context of analyzing the auction mechanisms, therefore there are no results on the accuracy of the click models proposed, or even analysis to show that the externalities exist in the data.
In this paper, our objective is to first show that exter-

nalities are indeed present in the sponsored search domain and then investigate how to explain both position bias and externalities in a combined model. We exploit the temporal order of user's actions for this analysis. Our contributions can be summarized in three points. First, we present a statistical analysis demonstrating that temporal user click sequences are good indicators of ad externalities. Second, we propose the positional rationality hypothesis which explains both position bias and externalities. Third, based on this hypothesis we develop the temporal click model (T CM), which has the properties: (1) Foundation: To the best of our knowledge, this is the first attempt in the literature to estimate positional bias, externalities and unbiased userperceived ad quality from user click logs in a combined model; (2) Framework: T CM is based on a strict Bayesian framework. Closed-form representations of the ad quality and user behavior posteriors can be derived using this framework, making it scalable and computationally efficient to handle the challenges imposed by the voluminous click logs; (3) Effectiveness: T CM consistently outperforms two stateof-the art models in a number of metrics at click prediction.
2. DATA ANALYSIS
In the online advertising literature [12], externalities are often considered a primary factor influencing user click behavior. In this section we present a click log analysis indicating this effect exists in real data sets.
Our goal is to analyze how the click behavior of users on a particular ad changes depending on the quality of the rest of the ads. We focus on ad impression sequences with exactly two ads. Showing that externalities exist for ad sets with only two ads proves the existence of externalities for longer sequences as well. We collected two real data sets in March 2009 from a commercial search engine. One of the data sets consists of queries that have exactly two ads shown on top (north) of the organic web search results, and the other contains ad impressions that were shown at the

107

bottom (south). We will refer to these data set as North Ad Data(NAD) and South Ad Data(SAD), respectively.
Since there is no ground truth for ad quality, we use empirical click-through rates (CTR) as a proxy. For each ad impression sequence we calculate the CTR of the ad in Position 1 and Position 2 separately, and compare the CTRs at these two positions using the following approach: We group all impressions with similar CTRs at Position 1 in one bin and plot the average CTR at Position 2. This way we can use the CTRs at Position 1 as quality measure (since each bin contains ads with similar CTRs) and the average CTR at Position 2 as the click metric. Results are shown in the top row of Fig. 1. We see that as the quality in one position increases (x-axis) from 0 to about 0.2, CTR of the other position (y-axis) also increases. However, when the quality of one position keeps increasing further, the CTR of the other position starts dropping. This observation suggests that the high quality ads (CTR > 0.2 in these graphs) could affect the click rates of the other ads adversely. Note that the graphs for SAD dataset doesn't show the decrease, but that the absolute value of the max CTRs in these positions are much lower than the ones in North. Still, we see that around 0.1 the curve starts to saturate.
We also examine if the quality of ads in one position influences which ads user clicks first. For this, we group all impressions with similar CTR at Position 1 in one bin and plot the percentage of events where the first click occurred at Position 2 and vice versa. Results are shown in the bottom row of Fig. 1. We observe that when CTR in Position 1 increases users prefer to click first on Position 1. This observation verifies that users are rational: they compare the qualities of both ads and choose the better one to click first. Hence, the temporal click sequences of users can be used to learn ad externalities.
Analyzing the first click graphs in more detail shows that users aren't completely rational, and that they are biased by the position of the ads. For example, in the left bottom two plots of Fig. 1, when CTR at Position 1 is 0.2, the percentage of first click at Position 2 is around 0.2 too. In other words, the probability of clicking Position 1 first is around 0.8. But, when CTR at Position 2 is 0.2, the probability of clicking Position 2 first is around 0.6. The reason for this behavior could be that users may be trusting the ranking of the search engine more if they don't think the ad at Position 2 is much better than the one at Position 1 and don't use their ranking to take over the ranking of search engine.
Based on the above observations, we propose the positional rationality hypothesis: Users examine both ads together and assess their qualities, and then users compare their qualities. If the ad at Position 2 is much better than ad at Position 1, users would consider clicking the ad at Position 2 first instead of the ad at Position 1.
3. TEMPORAL CLICK MODEL
We first introduce definitions and notations, and next specify and discuss the temporal click model.
A user starts a query session by submitting a query to the search engine. The search engine retrieves the ads that match the user query and finds the ranking that optimizes the objective function and presents them to the user in different slots on the results page, alongside organic web results. The set of ads presented to the user can be represented as an ad impression sequence A =<a1 . . . aD>, where ai is the

ad presented in position i and D is the total number of slots.
An ad ai in this sequence is shown at a higher position (i.e. ranked higher) than ad aj if i < j. The clicked ads can be represented as a sequence as well, a sequence of click events ordered by their time of click: C =<c1 . . . cT >, where T  D and ci corresponds to the ad in position i. We call this sequence the temporal click sequence.
To study the impact of ad externalities, we focus on ad impression sequences with two ads only. For an ad impression sequence A =<a1a2>, there are five possible click sequences: <>, < a1 >, < a1a2 >, < a2 > and < a2a1 >. Multiple clicks on the same ad in the same position are discarded in this model. To simplify notation and have sequences of equal
lengths, we will use the symbol 2 to indicate a no-ad-clicked action. We can then rewrite the five possible click sequences as: <22>, <a12>, <a1a2>, <a22> and <a2a1>. We will refer to these sequences as click sequences type 1..5.

3.1 Model Specification

The temporal click model can be described as a generative process as illustrated in Fig. 2(Left). User submits a query and sees an ad impression sequence A =<a1a2 >. She can either examine or ignore the ads . We treat this user behavior as a probabilistic event, represented by a binary random variable E. The probability of examining ads, denoted as P (E = 1|A), is set to a global model parameter , i.e.,

P (E = 1|A) = .

(1)

If the user decides not to ignore the ads, she examines

both of them as the position rationality hypothesis states,

and picks one. We denote the perceived quality of ads a1 and a2 by Ra1 and Ra2 , respectively, where Rai  [0..1]. Our model says that the quality of a2 has to be greater than that of a1's by a margin of Ua1 for the user to ignore the search engine's ranking and pick a2. Otherwise she picks a1. Ua1 can be viewed as the advantage of being ranked in position

1. We treat this user behavior as a probabilistic event, too.

Let F be the random variable to represent the first ad the

user picks, then the probability of picking a1 or a2 would be:

P (F = a2|E = 1, A) = 1I[Ra2 - Ra1 > Ua1 ]

(2)

P (F = a1|E = 1, A) = 1I[Ra2 - Ra1  Ua1 ],

(3)

where 1I is the indicator function.

Once the user picks an ad ai, whether she clicks it or not
depends solely on its quality, Rai . Thus, the probability of ai being the first clicked ad c1 or not having a click at all is:

P (c1 = ai|F = ai) = Rai

(4)

P (c1 = 2|F = ai) = 1 - Rai , i  {1, 2}.

(5)

If the user does not click on the ad she picked our session

model terminates. If, however, she clicks on that ad she can

re-consider the ad that was skipped. Our model assumes

that the probability of re-considering the other ad depends

on the difference of the perceived qualities of the two ads.

The higher the relative perceived quality of the clicked ad,

the smaller the probability of re-considering the skipped ad.

In T CM, we model this probability as a linear function of

the difference in qualities with a scale factor  and a bias

term . We let S be the random variable representing the

skipped ad, and derive the following probabilities:

P (S = 2|c1 = 2) = 1

(6)

P (S = a3-i|c1 = ai, A) =  - (Rai - Ra3-i ), i  {1, 2}. (7) Again, once the user decides that she will consider the

108

Ua1

Ra1

Ra2

E

F

S

C1

C2

Figure 2: (Left) The user model of T CM; (Right) The graphical model representation of T CM for an ad impression A=<a1a2>: Ra1 and Ra2 are user-perceived quality variables of Ad a1 and a2 separately; Ua1 is the position advantage variable of Ad a1; and observed click sequence <c1c2> is shaded.

skipped ad aj , whether she clicks or not depends on its perceived quality only. Thus the probability of having a second
click c2 on aj or not having a second click at all has the form:

P (c2 = aj |S = aj ) = Raj

(8)

P (c2 = 2|S = aj ) = 1 - Raj , j  {1, 2}.

(9)

Since we ignore multiple clicks on the same ad at the same

position our session model ends after this step. However, we

still need to specify the probabilities for the case where the

user ignores the ads in the first step. If the user ignores both

of the ads, she considers neither, and if she doesn't consider

either one of the ads, she clicks none:

P (F = 2|E = 0, A) = 1

(10)

P (c1 = 2|F = 2) = 1

(11)

P (c2 = 2|S = 2) = 1

(12)

We complete the specifications of our Bayesian framework by introducing priors on Rai s and Uai s. Although any form priors are possible, we follow [13, 18] in choosing the iid non-informative uniform priors within [0, 1]. We show the graphical model representation of T CM in Fig. 2(Right).
To guarantee that we have a well-defined model, we have to put constraints on the parameters of the model such that the probability of any click sequence generated by this model is between 0 and 1. We describe these constraints in the following lemma.

Lemma 3.1. The temporal click model is well-defined if its parameters satisfy the constraints: -1    1, 0    1, 0    1,    and 0   +   1.
3.2 Discussion
Now that the T CM model is formally presented we illustrate how it explains the position bias and ad externalities.
Let's consider the two extreme cases of the T CM. If we set the position advantage variable Ua1 to be 1, then the quality difference Ra2 - Ra1 can not be greater than Ua1 because Ra2 -Ra1  1. In this case, user never considers clicking a2 first, regardless of its quality. Only after a1 is clicked could a2 have a chance. In other words, T CM with Ua1 = 1 would roughly correspond to the cascade model described in [7].
On the other hand, If we set the user variable Ua1 to be 0, the user would always be fully rational. She would always pick the ad with better quality first, regardless of its position, like the rationality hypothesis proposed in [12] that explains ad externalities.
In a nutshell, the position advantage variable Ua1 is a tradeoff between position bias and ad externalities, and it

gives us the flexibility to learn both of them from click data by finding its posterior probability.
To generalize our model to more than two ads we can assume that each ad except for the last one, has a corresponding position advantage; and that an ad would only be picked first if its quality beats all the other ads. After the first ad is picked the same process can be used to select the second ad and so on and so forth.
In reality, most search engines, for example Google, Yahoo! and Bing, receive a large fraction of ad clicks from the ads displayed on the top of organic search results. Improving the accuracy of click prediction in this premium slot will have a profound effect on the overall click yield and revenue. Those engines show less than five ads in the top. Therefore, we believe that the proposed model is potentially useable since it can be easily generalized and applied to the limited number of ads that can be displayed in the top. In fact, the proposal model on two ads is practicable too and it can be applied to the bottom slot of Bing and Yahoo! where at most two ads could be shown.
4. ALGORITHMS
In this section, we propose algorithms for the T CM. We first present the closed form posterior distributions of userperceived ad quality and ad position advantage; then explain how to estimate the model parameters  = {, , }; finally we show how to use them to predict the click-through rate of any given ad impression sequence.
4.1 Posterior Distribution
For a given query with N sessions, we let A = {A1, A2, . . . , AN } and C = {C1, C2, . . . , CN } be their corresponding ad impression and click sequences respectively. We assume that in A there are M distinct ads indexed from 1 to M, and let R = {R1, R2, . . . , RM } be the corresponding user-perceived ad quality variable and U = {U1, U2, . . . , UM } be the position advantage variable.
We assume that the ad impression and click sequences in different sessions are independent of each other, given Rm and Um. We can then compute the posteriors of Rm and Um using the Bayes principle:
P (Rm|C, A)  P (Rm)QNn=1P (Cn|Rm, An),
P (Um|C, A)  P (Um)QNn=1P (Cn|Um, An). Since P (Rm) and P (Um) are already known, we only need to figure out P (Cn|Rm, An) and P (Cn|Um, An). To calculate them, we have to integrate out all hidden random variables other than Rm and Um. We show that P (Cn|Rm, An) and

109

P (Cn|Um, An) have closed forms by demonstrating integration on a specific case of P (Cn|Rm, An). In this particular case, the ad impression is An =< a1a2 >, click sequence is Cn =<a2a1> and m = a2. After applying the Bayes rule and
uniform priors on Ra1 and Ua1 we get Eq.(13):
P (Cn =<ma1> |Rm, An =<a1m>)

=RRa1

R Ua1

P E,F,S

P

(C

n

,

E

,

F,

S,

Ra1

,

Ua1

|Rm

,

An

)

=RRa1

R Ua1

P E,F,S

P

(C

n

,

E

,

F,

S,

|Rm

,

Ra1

,

Ua1

,

An).

(13)

We first integrate out hidden variables E, F and S in

Eq.(13) by applying model specification. To generate the

click sequence <ma1> given ad impression <a1m>, E, F , S

must be 1, m, a1. Applying Eq.(1,2,4,7,8), we get Eq.(14).

P E

P F

P S

P

(C

n

=<ma1>,E

,F,S,|Rm,Ra1

,Ua1

,An

=<a1

m>)

=P (E = 1)P (F = m|E = 1,Rm,Ra1 ,Ua1 )P (C1n = m|F = m,Rm)

P (S = a1|C1n = m,Rm,Ra1 )P (C2n = a1|S = a1,Ra1 )

=RmRa1 ( -(Rm-Ra1 ))1I[Rm -Ra1 > Ua1 ].

(14)

Next, plugging Eq.(14) back into Eq.(13) and integrating

out random variables Ra1 and Ua1 , gives us the closed-form of P (Cn =<ma1> |Rm, An =<a1m>):

R1 0

R1 0



RmRa1

(

-(Rm-Ra1

))1I[Rm

-Ra1

>

Ua1

]dRa1dUa1

=R0RmR0Rm-Ua1 RmRa1 ( -(Rm-Ra1 ))dRa1dUa1

=

1 6



R4m

-

1 12

R5m

.

(15)

We can see that the closed form of P (Cn|Rm, An) depends

only on the click sequence type and the position of Ad m

in An, independent of the other ads. There are five click

sequence types (as described in 3) and two possible positions.
We let k,i(·) be the closed form of P (Cn|Rm, An) for k-th click sequence type and i-th position. The functional form

of k,i(·) for all k and i is listed in Table.1. Finally, the

posterior of Rm has the unnormalized closed form:

P

(Rm |C,

A)



Q5 k=1

Q2i=1(k,i

(Rm ))Nkm,i

,

where Nkm,i is the number of sessions that the click sequence

is k-th type and Ad m appears at i-th position in the ad

impression sequence.

We can get the unnormalized closed form of posterior of Um, similarly. We let k(·) be the closed form of P (Cn|Um, An) for k-th click sequence type and m at Position 1 of An. P (Cn|Um, An) will be a constant if m is at Position 2 of An because ad has position advantage only in Position 1 for

sequences of length 2. The functional form of k(·) is also

given in Table. 1. The closed form of the posterior of Um is

P

(Um|C,

A)



Q5 k=1

(k

(Um

))Nkm,1

.

Because of the potential dimensionality curse introduced

by large Nkm,i, we follow [13] and utilize the midpoint rule

to numerically normalize P (Rm|C, A) and P (Um|C, A). For

example, the normalizing constant of P (Rm|C, A) can be

approximated by

PB b=1

Q5 k=1

Q2 i=1

(k,i

(

b

- 0.5 B

))Nkm,i

with B equal-size bins on [0..1].

Once normalized with an appropriate B, both density

function and cumulative distribution function of posteriors

of Rm and Um could be evaluated at desired precision.

4.2 Parameter Estimation
We use the maximum likelihood principle to estimate the

model parameters  = {, , }. We want to maximize the

likelihood

of

C,

i.e.,

P (C|, A)

=

QN n=1

P

(C

n|,

An).

The

likelihood of P (Cn|, An) can be computed by integrating

out the Rai s and Uai s as described in the previous section. The exact derivation is omitted to save space.

By summing log likelihood of all click sequences, we get

the following log-likelihood function:

L()

=

N1 log(1 -

7 ) + 12

N2

log[(

11 24

-

13 60

+

 )] 72

+ N3

log[( 13 60

-

 )] + 72

N4

log[( 1 8

-

 30

+

 )] 72

 + N5 log[( 30 - 72 )],
where Ni represents the number of instances of click sequence type i in C .

By setting the derivative of log likelihood function L()

on  to zero and also considering model constraints listed in

Lemma 3.1, we get a closed form function to estimate :

(  = min 1,

12 7

(N2

+

N3

+

N4

+

N5 )

) .

(N1 + N2 + N3 + N4 + N5)

Unfortunately, there are no closed form solutions for 

and . But, it is not hard to verify that the log likelihood

function L() is concave in  and  under the parameter con-

straints listed in Lemma 3.1. Therefore, we can utilize con-

vex optimization techniques to find approximate solutions.

In our implementation we used a barrier method combined

with the Newton's method [3]. Since there are only two pa-

rameters to estimate, this is a simple optimization problem.

4.3 Click Through Rate Prediction

Given the model parameters and posteriors, we can use the mid-point rule to estimate the posterior probability of any click sequence. For example the probability of click sequence <a2a1> is:

P (C =<a2a1> |A)

=E(Ra2 Ra1 ( -(Ra2-Ra1 ))1I[Ra2 -Ra1 > Ua1 ])



1 B2

PB b2

=1

PB b1

=1

(b2

- 0.5) B

(b1

- 0.5) B

[

-

(

b2

- B

b1

)]

Pa2

(

b2

- 0.5 B

)Pa1

(

b1

- 0.5 B

)Fa1

(

b2

- B

b1

),

where Pa1 (·) and Pa2 (·) are normalized density functions of posteriors of Ra1 and Ra2 , and Fa1 (·) is cumulative distribution of posterior of Ua1 .
Once we have the probabilities of individual click sequences,

we can compute the click through rate of any ad, CT R(ai), in an ad impression sequence A =<a1a2> by taking the sum
of probabilities of all click sequences that include a click on

that particular ad ai:

CT R(a1) = P (C =<a12>C =<a1a2>C =<a2a1> |A)

CT R(a2) = P (C =<a22>C =<a1a2>C =<a2a1> |A).

4.4 MapReduce Implementation
To implement the inference algorithm proposed above, we first need to collect sufficient statistics {Nkm,i}k=1..5,i=1..2 for computing posteriors of Rm and Um, and {Nk}k=1..5 for estimating model parameters. Obviously, a sequential scan of data could provide those statistics. However, to be able to apply the T CM to petabyte-scale data requires parallelization. In this section we describe how to leverage the MapReduce paradigm [8] for this purpose.
MapReduce is a programming framework to parallelize

110

Algorithm 1 Map(A, C) - Mapping a query session

Input: A: ad impressions of a query, C: click log. Output (a, v): a is the ad index; v is sufficient statistics of a for one session.

1: for each ad impression sequencen = 1, . . . , N do

2: Set k be the click sequence type of Cn

3: for each position i = 1, 2 do

4:

v = 0;

5:

v[k  2 + i - 3] = 1;

6:

Emit(ani , v);

7: end for

8: end for

Algorithm 2 Reduce(a, vectList)
Input: a is the ad index; vectList is a list of vectors associated with a. Output (a, Na): Na is sufficient statistics of a for all sessions.
1: Na = 0; 2: for each v in vectList do 3: Na+ = v; 4: end for 5: return (a, Na);

computational tasks by expressing them as pairs of Map and Reduce functions. The implementation of MapReduce infrastructure frees programmers from the practical issues of running algorithms over a cluster of computers, such as loadbalancing, fault tolerance and data distribution problems.
Algorithm 1 and 2 are the implementation of a pair of Map and Reduce functions to collect {Nkm,i}k=1..5,i=1..2. Specifically, the Map function reads each session from input and emits a set of intermediate (key, value) pairs, where the key is the ad index and the value is this ad's sufficient statistics of that session. The MapReduce infrastructure then groups together all values with the same intermediate key and passes them to the Reduce function. The Reduce function accepts an intermediate key and a list of values for the key. It summarizes those statistics and outputs the final result. The idea behind the MapReduce framework is that a large scale dataset could be broken down into small chunks and each chunk is fed to a Map function that will be run on an individual computer. The Reduce function is used for summarizing results from all chunks to get the complete result on the whole dataset.
5. EXPERIMENTS
We conduct experiments to evaluate the performance of the T CM. The evaluation criterion in both cases was the accuracy of the predicted click-through rates. The ranking produced by ordering the results based on estimated CTRs are often used in literature [6] for model comparison purposes. However, in sponsored search, the final ranking presented to the user is a product of an auction mechanism. For our end goal, the accuracy of the click prediction matters most. One of the differentiating properties of T CM compared to other state of the art models [13, 18, 6] is that T CM considers temporal sequences, and predicts the probabilities of sequences of clicks. Since the existing methods completely ignore the time of clicks, we are unable to com-

Figure 3: Summary of Datasets: NAD (left) and SAD (right).
pare the performance of T CM on this task to any other method.
5.1 Experiment Setup
We evaluate our model on two data sets: NAD and SAD, which are introduced in Section 2. NAD and SAD have around 0.3 and 1.1 million distinct queries with 0.1 and 0.65 billion sessions each, respectively. For the very frequent queries, direct statistics can do well on CTR prediction because there are many historical observations for these queries. In order to prevent the model evaluation from being dominated by these samples, only queries with less than 103.1 sessions are kept for evaluation. This filtering removes 3% and 5% queries in NAD and SAD respectively. A summary of the query frequency distributions in the two data sets is provided in Fig. 3.
Because our model aims at learning externalities between ads, we evaluate our model on the estimating the CTR of ad impression sequences instead of an individual ad. Therefore, we design and use the following leave-one-out experimental protocol:
1. Retrieve all the sessions related to a given query; 2. Consider each distinct ad impression sequence in those
sessions; 3. Hold out as test sessions all the sessions with this ad
impression sequence; 4. Train our model and baselines on the remaining ses-
sions and the predict the CTR of this ad impression sequence; 5. Compute the true CTRs from test sessions; 6. Compute the Mean-Square-Error (MSE) between the true CTRs and the predicted CTRs; 7. Average the error on all ad impression sequences, weighted by the number of test sessions.
5.2 Baselines
We considered two baseline models for model comparisons. One is naive CTR statistics (NS), and the other is Bayesian browsing model (BBM) [18], a state-of-the-art model in click modeling literature.
Given training data A and C and an ad impression sequence A =<a1a2>, the baseline NS simply uses the empirical CTR of ai in training data, i.e., the percentage of clicks on ai at position i.
The BBM model follows the examination hypothesis as T CM does. But, T CM differentiates itself from BBM in several ways. For example, BBM examines one ad at a time and T CM examines both at the same time, in order to model externalities. Moreover, BBM ignores temporal information, and therefore it treats two click sequences <a1a2 > and <

111

Figure 4: Accuracy in predicting the CTR of ad impressions from queries with different frequencies (Left: NAD dataset, Right: SAD dataset). Both T CM and BBM are significantly better than NS for all query frequencies. T CM is noticeably better than BBM on less-frequent queries but shows similar performance on frequent ones.

Figure 5: Accuracy in predicting the CTR at Position 2 for varying query frequencies (Left: NAD dataset, Right: SAD dataset). Both T CM and BBM are significantly better than NS for all query frequencies on both datasets. T CM is noticeably better than BBM on all queries of NAD, and on less-frequent queries of SAD.

Figure 6: Average percentage of first click at Position 1 for varying query frequencies (Left: NAD dataset, Right: SAD dataset). The probability that users click Position 1 first for frequent queries tends to be higher than less frequent queries.

Figure 7: Percentage of queries where T CM performs as good or better than BBM for varying query frequencies (Left: NAD dataset, Right: SAD dataset)

a2a1> as the same. Given an ad impression sequence A and the click sequence C =< c1c2 >, the BBM specification is as follows:

P (F = a1) = 1, P (c1 = a1|F1 = 2) = 0, P (S = a2|c1 = a1) = 2, P (c2 = a2|S = 2, Ra2 ) = 0,

P (c1 = a1|F = a1) = Ra1 , P (S = a2|c1 = 2) = 3, P (c2 = a2|S = a2, Ra2 ) = Ra2 ,

where 1, 2 and 3 are model parameters. Please refer to [18] for the details of BBM inference algorithms.

5.3 Predicting CTR
We compare T CM to the two baselines on predicting the CTR of ad impressions. As described in the experimental protocol, mean square error is used as the evaluation metric [14]. Fig. 4 reports the prediction accuracy of three methods on both datasets NAD and SAD for varying query frequencies. Both T CM and BBM are significantly better than NS across all query frequencies. Our model T CM is noticeably better than BBM on less-frequent queries but shows similar performance on frequent ones.
One obvious reason for the similar performance of T CM and BBM on frequent queries is that frequent queries have more sessions available for training and have more uniform click patterns than less-frequent ones. Even the naive method NS tends to close the performance gap with T CM and BBM on frequent queries.
We present more detailed analysis of the data in Fig. 6 to better understand this behavior. This figure shows the percentage of first clicks at Position 1 for different query frequencies. We see that the probability of clicking the ad

in Position 1 first is higher for the frequent queries. This graph could suggest that the users consider ads in Position 1 more frequently in these very frequent queries. It's quite possible that search engine users are trained to consider top results more for the frequent queries via positive reinforcement: the more user feedback there is, the better the search engine ranking will be; as the search engine rankings improve users will trust search engine's ranking more. If this is the case, the assumption that the users will examine both of the ads before clicking would not hold for the frequent queries. Since T CM uses this positional rationality assumption, degradation in performance for the frequent queries would not be surprising. Fig. 7 further verifies this claim. It presents the percentage of queries that T CM performs as good or better than BBM for varying query frequencies on the two data sets. The percentage goes down as the query frequency increases.
5.4 Discussion
T CM performs noticeably better than BBM on less-frequent queries. But, what contributes to this improvement?
We compare T CM, BBM and NS on predicting the CTR at Position 2 of ad impressions. Fig. 5 presents the accuracy results for varying query frequencies as measured by mean square error. Both T CM and BBM are significantly better than NS for all query frequencies on both datasets NAD and SAD. T CM is noticeably better than BBM on all queries of NAD, and on less-frequent queries of SAD as before, but the magnitude of improvement is much bigger. Therefore, T CM performs better than BBM mainly because it predicts clicks at Position 2 better.

112

Case C = 22, m = a1 C = 22, m = a2 C = a12, m = a1 C = a12, m = a2 C = a1a2, m = a1 C = a1a2, m = a2 C = a22, m = a1 C = a22, m = a2 C = a2a1, m = a1 C = a2a1, m = a2

P (C|Rm, A =<a1a2>)

Function Expression

1

-

1 3



-

R2m

+

1 3



R3m

1

-

1 2



-

1 3

R3m

(

1 2

-

1 6



-

1 12

)

Rm

+

(1

-

1 2



-

1 6

)R2m

+

(-

1 2

+

1 2

)R3m

+

1 6

R4m

-

1 12

R5m

1 2



-

(

1 2



-

1 3

)Rm

-

1 2



R2m

-

1 6



R3m

+

1 6

R4m

+

1 12

R5m

(

1 6



+

1 12

)

Rm

+

(

1 2



+

1 6

)

R2m

-

1 2

R3m

-

1 6

R4m

+

1 12



R5m

(

1 2



-

1 3

)Rm

+

1 2

R2m

-

1 6



R4m

-

1 12

R5m

1 3



+

(-

1 2

-

1 3



+

1 4

)Rm

+

(

1 2



-

2 3

)R2m

+

(

1 6

+

1 2

)R3m

-

1 6

R4m

-

1 12

R5m

1 2

R3m

-

1 6

R4m

+

1 12



R5m

(

1 3



-

1 4

)Rm

+

(-

1 2



+

2 3

)R2m

-

1 2

R3m

+

1 6



R4m

+

1 12

R5m

1 6

R4m

-

1 12

R5m

Case C = 22, m = a1 C = a12, m = a1 C = a1a2, m = a1 C = a22, m = a1 C = a2a1, m = a1

P (C|Um, A =<a1a2>)

Function Expression

1

-

2 3



+

1 2



Um2

-

1 3

Um3

(

1 3

-

1 8



+

1 30

)

+

(

1 2

-

1 3

)Um

+

(-

1 2

+

1 4



-

1 6

)Um2

+

(

1 6

+

1 6

)Um3

-

1 24

Um4

-

1 30

Um5

(

1 8



-

1 30

)

+

1 3

Um

+

(-

1 4



+

1 6

)Um2

-

1 6



Um3

+

1 24



Um4

+

1 30



Um5

(

1 3

-

1 8



+

1 10

)

+

(-

1 2

+

1 3

)Um

-

(

1 4



+

1 6

)

Um2

+

(

1 6

+

1 6

)Um3

+

1 24



Um4

-

1 30

Um5

(

1 8



-

1 10

)

-

1 3

Um

+

(

1 4



+

1 6

)Um2

-

1 6



Um3

-

1 24

Um4

+

1 30



Um5

Table 1: Probabilities conditional on Rm and Um for different cases

6. CONCLUSIONS AND FUTURE WORK
Predicting user click behavior is crucial to the sponsored search business model. In this paper, we first presented statistical analyses demonstrating that externalities exist in this domain and that temporal click sequences are good indicators of externalities. We proposed the positional rationality hypothesis to explain both position bias and externalities. Based on this hypothesis, we further developed the temporal click model.
We evaluated the proposed model on two real data sets. We presented results showing that our model outperforms one of the state of the art click models, BBM [18], on mid to lower decile queries. We believe the results presented in this paper are convincing evidence that the time and order of user actions contain useful information about the user's click behavior.
We would like to extend our model to incorporate time users spend on the landing pages, which could provide us a direct measure of user satisfaction, and therefore improve our estimate of ad quality. Another extension is to model repeated clicks, which are not rare in click logs. Repeated clicks could be viewed as the re-judgement of users on ad quality. Being able to model them could be helpful to improve the accuracy of our click prediction.
7. REFERENCES
[1] Eugene Agichtein, Eric Brill, and Susan Dumais. Improving web search ranking by incorporating user behavior information. SIGIR, 2006.
[2] H. K. Bhargava and Juan Feng. Paid placement strategies for internet search engines. WWW, 2002.
[3] S. Boyd and L. Vandenberghe. Convex optimization. 2004. [4] R. Briggs and N. Hollis. Advertising on the web: Is there
response before click-through. Journal of Advertising Research, 37, 1997. [5] B. Carterette and R. Jones. Evaluating search engines by modeling the relationship between relevance and clicks. NIPS, 2007. [6] O. Chapelle and Y. Zhang. A dynamic bayesian network

click model for web search ranking. WWW, pages 1­10, 2009.
[7] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. WSDM, 2008.
[8] J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. OSDI, 2004.
[9] Georges E. Dupret and Benjamin Piwowarski. A user browsing model to predict search engine click data from past observations. SIGIR, pages 331­338, 2008.
[10] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. American Economic Review, 97(1), 2007.
[11] D. C. Fain and J. O. Pedersen. Sponsored search: A brief history. Bulletin of the American Society for Information Science and Technology, 32, 1997.
[12] A. Ghosh and M. Mahdian. Externalities in online advertising. WWW, 2008.
[13] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y. Wang, and C. Faloutsos. Click chain model in web search. WWW, 2009.
[14] F. Guo, C. Liu, and Y. Wang. Efficient multiple-click models in web search. WSDM, 2009.
[15] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. SIGIR, pages 154­161, 2005.
[16] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Transaction on Information System, 25(2), 2007.
[17] D. Kempe and M. Mahdian. A cascade model for externalities in sponsored search. Workshop on Ad Auctions, 2008.
[18] C. Liu, F. Guo, and C. Faloutsos. Bbm: Bayesian browsing model from petabyte-scale data. KDD, 2009.
[19] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. WWW, pages 521­530, 2007.
[20] Benyah Shaparenko, O¨ zgu¨r C¸ etin, and Rukmini Iyer. Data-driven text features for sponsored search click prediction. ADKDD, 2009.

113

Freshness Matters: In Flowers, Food, and Web Authority
Na Dai and Brian D. Davison
Department of Computer Science & Engineering Lehigh University
Bethlehem, PA 18015 USA
{nad207,davison}@cse.lehigh.edu

ABSTRACT
The collective contributions of billions of users across the globe each day result in an ever-changing web. In verticals like news and real-time search, recency is an obvious significant factor for ranking. However, traditional link-based web ranking algorithms typically run on a single web snapshot without concern for user activities associated with the dynamics of web pages and links. Therefore, a stale page popular many years ago may still achieve a high authority score due to its accumulated in-links. To remedy this situation, we propose a temporal web link-based ranking scheme, which incorporates features from historical author activities. We quantify web page freshness over time from page and in-link activity, and design a web surfer model that incorporates web freshness, based on a temporal web graph composed of multiple web snapshots at different time points. It includes authority propagation among snapshots, enabling link structures at distinct time points to influence each other when estimating web page authority. Experiments on a real-world archival web corpus show our approach improves upon PageRank in both relevance and freshness of the search results.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance
Keywords: Web search engine, temporal link analysis, web freshness, PageRank
1 Introduction
In a corpus of documents as large as the Web, relevance alone is often insufficient to produce good rankings. Thus it is necessary to differentiate pages, and for more than a decade search engines have considered page authority in addition to relevance with respect to queries (and other factors) in web search. Much previous work [10, 23, 25] has been studied to estimate page authority based on different assumptions and successfully generalized onto multiple tasks [5, 9, 32]. However, most of these studies accumulated the authority contributions only based on the evidence of links between pages, without considering the temporal aspects concealed in pages and their connections.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Freshness is important to the quality of much in our daily lives, such as flowers and food. The same is also true for web page authority estimation. Pages being fresh tend to be welcome. However, traditional link analysis algorithms such as PageRank [10] estimate page authority by simply accumulating contributions from in-links on a static web link structure, without considering whether pages are still fresh when web users search for them. Freshness of web links is also important to link-based ranking algorithms. The web is widely recognized as one of the networks in which the rich get richer as the networks grow, leading to power law effects [12]. Old pages have more time to attract in-links, but may contain stale information. For example, as of this writing, http://www.sigir2007.org/ has 902 in-links [33] while http://www.sigir2010.org/ only has 208. Assuming the same contribution from each in-link, methods like PageRank would render a higher authority score on the earlier version of the SIGIR conference homepage.
In addition, some web spam research [31, 14] has recognized that local link structures with sudden changes might indicate link spam. A single web snapshot is unable to detect such changes and further smooth or neutralize the undesirable influence automatically.
Motivated by these two points, in this work we propose an probabilistic algorithm to estimate web page authority by considering two temporal aspects. First, to avoid old pages from dominating the authority scores, we keep track of web freshness over time from two perspectives: (1) how fresh the page content is, referred to as page freshness; and (2) how much other pages care about the target page, referred as in-link freshness. To achieve this, we mine web authors' maintenance activities on page content, such as the creation and removal of out-links. Each activity is associated with the time at which it occurs. We build up temporal profiles for both pages and links. A random walk model is exploited to estimate the two predefined freshness measures. By modeling web freshness from these two perspectives, we can bias the authority distribution to fresh pages, and so neutralize the unfair preference toward old pages by traditional link analysis ranking algorithms.
Second, we use multiple web snapshots at distinct time points, instead of a single snapshot. To make the link graph more stable, we connect multiple web snapshots by propagating authority flows among them, and so smooth the impact of sudden changes to particular snapshots on web page authority estimation. We exploit several proximity-based density kernel functions to model such propagation. Combining web freshness measures, we utilize a semiMarkov process to model a web surfer's behavior in selecting and browsing web pages.
We show the superiority of our proposed approach by conducting experiments to evaluate the ranking performance of several representative temporal web link-based ranking algorithms on a real-

114

world archival web data set. Experimental results demonstrate that our method outperforms the original time-agnostic PageRank by 8% on both relevance and freshness of top 10 search results.
The contributions of this work are as follows.
· Quantify web freshness from authors' maintenance activities on web content over time, from the perspectives of page freshness and in-link freshness;
· Incorporate web freshness into authority propagation to favor fresh pages;
· Explore a series of proximity-based density kernel functions to model authority propagation among web snapshots;
· Conduct experiments on a real-world archival web data set and show the superiority of our approach on ranking performance in terms of both relevance and freshness.
The remainder of this paper is organized as follows. We review prior work in Section 2; introduce how we quantify web freshness and how we incorporate it into a web surfer model to estimate timedependent web page authorities in Section 3; present how we set up experiments in Section 4; and show the evaluation results of our proposed ranking algorithm in Section 5. We conclude and consider future work in Section 6.
2 Related Work
Link analysis which incorporates temporal aspects into search has been studied in [35, 4, 13, 8, 6, 7, 34, 24, 1]. Yu et al.'s work in [35] was among the earliest, which incorporated paper age into quantifying paper authority to improve academic search. In addition to utilizing paper citations, the authors modified PageRank by weighting each citation according to the citation date. However, their work only associated one type of activity, i.e., link (citation) creation, into link analysis in the scenario of academic search. Amitay et al. [4] attached a timestamp to each link, approximating the age of the page's content. However, they only gave bonus to the links from fresh pages, rather than combining the freshness of the page itself when estimate web page authority. Berberich et al.'s work [8] focused on temporal aspects of both web pages and links in web search via the web dynamics from page and link creation, modification and deletion. They assumed users are equally interested in recency of information, in addition to the quality. Their work specifically emphasized the freshness and activity of pages and links. However, activities occurring at different time points are not distinguished as long as they were all in the period of users' temporal interests, which could span wide ranges.
Our work differs from prior work in two ways. First, we model the web freshness from two different perspectives by building temporal link profiles and temporal page profiles from multiple types of activities over time. Second, the influence of activities on web freshness decays over time. We include some of these methods (e.g., [35, 8]) for comparison in Section 5.
Another direction in link analysis which incorporates temporal factors is to directly utilize or mine trends from multiple snapshots of the archival web [6, 7, 34, 24]. Berberich et al. [6] analyzed the potential of page authority by fitting an exponential model of page authority. The success with which web pages attract in-links from others in a given period becomes an indicator of the page authority in the future. However, one problem is how to normalize page authority at different time points, such that they are comparable. To solve it, Berberich et al. [7] normalized PageRank scores by dividing them by the minimum authority score in the same web snapshot, so that the minimum normalized PageRank score of the page in any snapshot equals 1.

Yang et al. [34] proposed a new framework which utilizes a kinetic model to explain the evolution of page authority over time from a physical point of view. The page authorities are viewed as objects subject to both "driving force" and "resistance", and so the page authority at any time point can be a combination of the current authority score resulting from "driving force" and the decayed historical authority score from "resistance". Empirical experiments demonstrated that authority estimation can benefit from increasing use of archival web content. However, their method did not consider the accumulation of incomparable authority scores caused by the inconsistent numbers of the pages in distinct snapshots. Other than web search, the idea of propagation of authority flows among different snapshots has been found in some other domains, such as social network analysis. Li and Tang [24] modeled the decayed effects of old publications in expertise search by allowing authority exchange only between successive snapshots of the time-varying social networks.
Our work differs from these in two ways. First, in our method each page in any snapshot is directly influenced by the same page in all the snapshots in a one-step transition decayed by time difference. This process captures a comprehensive interaction between pages at different time points naturally. Second, we propose and evaluate a series of proximity-based kernel functions to control the authority propagation among multiple snapshots. Again, we compare to some of these approaches (e.g., [6, 34, 24]) in Section 5.
Many link analysis methods compute page authority by a stochastic process via the link structure of the web. However, Liu et al. [25] utilized users' browsing behaviors to calculate page authority from a continuous-time Markov process which combines both how likely a web surfer reaches a page and how long the web surfer stays on a page. Their follow-up work [17] generalizes the page importance framework to be a semi-Markov process in which how long a web surfer stays on a page can partially depend on where the surfer comes from in one step transition. Since our work models web freshness from both how fresh a page is and how well other pages care about a particular page over time, we incorporate these two aspects into a semi-Markov process, which can model a temporal web surfer behavior in a natural and adaptive way. Other related work includes exploring temporal aspects of web behaviors [2] and utilizing the evolution patterns of pages and links to benefit webbased applications [16].
3 Methodology
In this section, we propose a temporal ranking model (denoted as T-Fresh) to incorporate web freshness and link structures at different time points into web page authority estimation. The main idea of T-Fresh is to utilize authors' maintenance activities on web pages and links to estimate web freshness at different time points, and then incorporate them into time-dependent page authority estimation by following proximity-based authority propagation rules on the time axis. T-Fresh outputs an authority score for each page at every predefined time point. The authority is estimated in an approximated way, partly depending on the link structure and web freshness of nearby snapshots, with the ones at farther time points having smaller influence.
3.1 Representing Web Freshness Over Time
As introduced in Section 1, web freshness reflects how fresh a web page is at a given time point ti by in-link freshness (InF) and page freshness (PF). The reasons we separate these two web freshness measures are: (1) InF and PF depict web freshness from the perspectives of information recommenders and information providers respectively; and (2) it prevents one type of web freshness from

115

Link activity
1 creation of link l : q  p 2 update on link l : q  p (changed anchor) 3 update on link l : q  p (unchanged anchor) 4 removal of link l : q  p
Page activity
1 creation of page q 2 update on page q 3 removal of page q

Infl. on p's InF
   
Infl. on
q's PF
  

Gain of p's InF
3 2 1.5 -0.5 Gain of q's PF 3 1.5 -0.5

Table 1: Activities on pages and links and their influence on web freshness. (The link l points from page q to page p. : positive influence on web freshness. : negative influence on web freshness. The number of  or  indicates the magnitude.)

dominating a single freshness score. Given a web page p, we assume that each update on p's parent page q is a direct validation of the link from q to p, and so an update on q implies that q pays attention to all of its out-linked pages, including p. Hence, we use InF to represent the attention from p's in-link pages, which is computed from the accumulation of activities on all the p's parent pages up to ti. Unlike InF, PF represents how fresh p is up to ti based on the activities on page p itself. For every page p at ti, it associates with InF and PF, denoted as InF (p)ti and P F (p)ti.
3.1.1 Building Temporal Page and Link Profiles
In order to compute InF and PF, we generate temporal page profiles (TPP) and temporal link profiles (TLP) in a manner inspired by Amitay et al. [4]. TPP and TLP record the web authors' activities on the pages and links over time. Given a page p, each item on its TPP records evidence of some type of activity on p at a specific time point. It is written as a 3-tuple <page ID, activity type, timestamp>, where activity type{creation, update, removal}. Given a link l with its associated anchortext, TLP records the evidence of some type of activity on l at a specific time point. Each item on TLP can similarly be represented as the 3tuple <link ID, activity type, timestamp>, where activity type{creation, update with unchanged anchor, update with changed anchor, removal}. In this way, each link and page is associated with a series of timestamped activities. Table 1 summarizes the influence of these activities on web freshness.
3.1.2 Quantifying Web Freshness
Based on TPP and TLP, we next quantify web freshness, i.e., InF and PF. In order to simplify analysis, we separate the continuous time axis into discrete time points, e.g., (t0, t1, . . . , tn, . . .), with a unit time interval t between successive time points, i.e., t = ti - ti-1. Web freshness at any time point ti is dependent on (1) the web freshness at ti-1, and (2) the activities on TPP and TLP, which occur between ti-1 and ti. When t is small enough, it is reasonable to assume that any activities in [ti-1, ti] occur at ti. In this way, we map all the web activities onto discrete time points. For web freshness at ti-1, we assume it decays exponentially over time. Thus, InF (p)ti and P F (p)ti can be given by:
I nF (p)ti = 1e-2tI nF (p)ti-1 + I nF (p)|ttii-1 (1)

P F (p)ti = 3e-4tP F (p)ti-1 + P F (p)|ttii-1

(2)

where P F (p)|ttii-1 and InF (p)|ttii-1 are the incremental freshness scores from the activities in [ti-1, ti], and 1e-2t is a co-
efficient that controls the decay of historical web freshness.

In the next step, we compute the incremental in-link freshness InF (p)|ttii-1 for the given page p. Since in-link freshness depends on the activities on TLP, we compute InF (p)|ttii-1 by accumulating all the activities on p's in-links in [ti-1, ti]. Let Cj (l) be the number of the jth type of link activity on link l in [ti-1, ti]. Let wj be the unit contribution of the jth type of link activity. The
incremental in-link freshness is written as:

I nF0(p)|ttii-1 = X

X wj Cj (l)

(3)

l:qp jLA

where LA is the set of link activity types. However, it is not enough to propagate such influence in one step; we additionally propagate in-link activities in an iterative way, leading to smoother in-link freshness scores. Let InF0(p)|ttii-1 in Equation 3 be an initial score. For each iteration, every page receives in-link freshness scores from its parent pages, and also holds its initial score. The process converges and produces a stable score for every page determined by both its parents' scores and its own in-link activities. Thus, the incremental in-link freshness is given by:

I nF (p)|ttii-1 = InF I nF0(p)|ttii-1
+(1 - InF ) X mqpInF (q)|ttii-1(4)
l:qp

where mqp is the weight on the link from q to p. Equation 4 is actually the personalized PageRank (PPR) [19]. We use one-step transition probability from q to p based on link structure to represent mqp, where P mq = 1 if q has at least one out-link.
We next compute the incremental page freshness P F (p)|ttii-1. Similar to InF (p)|ttii-1, we argue that how fresh one page is depends on both the page itself and its out-linked pages, since the
out-linked pages are extensions of the current page. We thus prop-
agate page freshness backward through links in an iterative way.
For each iteration, every page receives page freshness scores from
its out-linked pages, and also holds its initial score. This process
converges finally and generates a stable page freshness score on every page. Let Cj (p) be the number of the jth type of page activity on p in time period [ti-1, ti]. Let wj be the unit contribution of the jth type of page activity. The initial incremental page freshness score P F0(p)|ttii-1 is defined as:

P F0(p)|ttii-1 = X wj Cj (p)

(5)

jP A

where P A is the set of page activity types. The incremental page freshness is given by:

P F (q)|ttii-1 = P F P F0(q)|ttii-1
+(1 - P F ) X mqpP F (p)|ttii-1 (6)
l:qp

where mqp is the weight on the link from q to p. We use the inverted one-step transition probability to represent mqp, where P mp = 1 if page p has at least one in-link. Once achieving InF (p)|ttii-1 and P F (p)|ttii-1 , we compute InF (p)ti and P F (p)ti by Equation 1 and 2.
3.2 Temporal Ranking Model

Now that we quantify web freshness at distinct time points, the next problem comes to how to utilize web freshness to control authority propagation in an archival link graph, so that we achieve a timedependent authority score for every page.
We start by describing a "temporal random surfer model" which motivates our method T-Fresh. The "temporal random surfer

116

t

i

m

e

C A

t

3

E

C A

t

2

F B
D

A
B D

t

1

A B

t

0

Figure 1: The process of T-Fresh. Each node represents one web page.

model" is similar to the "random surfer model", which explains

PageRank [10]. However, our surfer model differs from the tra-

ditional model in two aspects. First, the web surfer has specific

temporal intent, which controls her choice of target snapshot. Note

that the surfer's temporal intent varies with her current snapshot.

Second, the web surfer prefers fresh web resources. Figure 1 de-

picts one simple example of how the surfer behaves on an archival

web of four snapshots.

Consider a web surfer wandering on an archival web corpus,

which includes multiple web snapshots collected at different time

points (t0, t1, . . ., tn). For every move, the surfer takes the following steps. First, she can choose either to follow one of the out-

linked pages or to randomly jump to any page at the same time

point. However, unlike PageRank in which a web surfer has equal

probabilities to follow out-going links, the preference of our surfer

choosing out-going links is a function of the page freshness of out-

linked pages. Consider the example in Figure 1. Suppose the surfer

is currently on page A at t2. She follows the link to B at t2 (solid

link) with probability (1 - d)Ft2 (B, A), where Ft2 (B, A) is a function which depends on the page freshness of all A's out-linked

pages

at

t2

and

P
P :AP

Ft2 (P,

A)

=

1.

The

probability

that

the

surfer randomly jumps to any page within snapshot t2, such as B,

is d/Nt2 , where Nt2 is the total number of pages at t2. After the surfer reaches the page chosen in the first step, she next

selects the specific snapshot of the page to which to jump based

on her temporal intent, which correlates to the time difference be-

tween the current snapshot and the target snapshot. This process

propagates authority among snapshots and uses the link structure at

one time point to influence the authority computation at other time

points. The propagation decays with time difference between snap-

shots. For the example in Figure 1 (dashed bi-directed links), sup-

pose the surfer reaches B at t2 after the first step, she can jump to B

at any time point as long as it exists, i.e., t2, t1, and t0. Specifically,

the probability that she jumps to B at t1 is written as Pt1|t2 (B), which depends on the time difference between t1 and t2.

Once the surfer reaches the page at the chosen time point, e.g.,

page B at t1, she browses it with the mean stay time µt1 (B), which incorporates B's in-link freshness at t1 before the next move.

In this way, the surfer's behavior on the archival web can be sep-

arated as (1) moving from one page to another; and (2) staying on a

page and browsing it. It leads to the semi-Markov process [30] for

page authority estimation.

Definition 1. A semi-Markov process is defined as a process that can be in any one of N states 1, 2, . . ., N , and each time it enters

a state i it remains there for a random amount of time having mean µi, and then makes a transition into state j with probability Pij.

Suppose the time that the process spends on each state is 1; then the semi-Markov process leads to a Markov chain. Assuming all states in such a Markov chain communicate with each other, the process can generate a stationary probability i for any state i. The long-run proportion of time that the original semi-Markov process is in state i is given by:

A(i)

=

iµi

PN
j=1

j

µj

,

i

=

1, 2, . . . , N

(7)

This solution divides the time-dependent page authority estimation into (1) computing the stationary probability that a surfer reaches every page in the archival corpus; and (2) computing the mean time of a surfer staying on every page.

3.2.1 Estimating Stationary Probability

We now introduce the computation of probability p,ti that a web surfer enters a page p at snapshot ti. In the first step of each move,
the surfer reaches page p at any time point tj by: (1) following p's
in-link at tj to reach p; (2) jumping from any page at tj to p at tj.

Ptj (F ollow|q) = (1 - d), Ptj (p|q, F ollow) = Ftj (p, q) (8)

Ptj (Jump|q) = d,

Ptj (p|q, Jump) = 1/Ntj (9)

where d is 0.15 by default. Ftj (p, q) is the web surfer's preference for following out-linked pages. Intuitively, fresh web resources are
likely to attract surfer's attention. We define Ftj (p, q) as:

Ftj (p, q)

=

P Ftj (p)

P
p :qp |tj

P Ftj (p)

(10)

In the second step of each move, the surfer reaches page p at ti from page p at tj is given by:

Pti|tj (p)

=

w(ti, tj)

P
qVi ,qVj

w(ti, tj)

(11)

where Vi and Vj are the sets of pages at time point ti and tj respectively, and w(ti, tj ) is the weight that represents the influence between the snapshots at ti and tj. Motivated by previous work [15, 22, 26, 28] which used proximity-based methods, we consider six kernel functions to model the authority propagation between snapshots: gaussian kernel (equation 12), triangle kernel (equation 13), cosine kernel (equation 14), circle kernel (equation 15), passage kernel (equation 16) and PageRank kernel (equation 17). We formally define them as follows.

w1(ti, tj)

=

» exp

-

(ti - tj)2 2|T |2

­

(12)

w2(ti, tj )

=

1

-

|ti - tj | |T |

(13)

w3(ti, tj)

=

1 2

» 1

+

cos

,,

|ti

- tj| «­ |T |

(14)

s

w4(ti, tj) =

1

-

,,

|ti

- tj |T |

|

«2

(15)

w5(ti, tj) = 1

(16)

117

 w6(ti, tj) =

0.85
0.15

|T -1|

ti = tj ti = tj

(17)

where |T | is the window size of one step authority propagation between snapshots. Other than Equation 12, all kernels require |ti - tj| < |T |; that is, the one step authority propagation proceeds only within a window of a specified size. Larger |T | results in more choices for the web surfer at each move between snapshots, while smaller |T | leads to influence mainly from nearby time points. In this work we set |T | to the total number of snapshots involved in authority propagation by default.
Combining the analysis above, the probability that a web surfer reaches page p at snapshot ti can be written as:

X

X

p,i =

Pti|tj (p)

Ptj (F ollow|q)Ptj (p|q, F ollow)

tj Ti

q:qp|tj

X

X

+

Pti|tj (p) Ptj (Jump|q)Ptj (p|q, Jump)

tj Ti

q|tj

X

=

Pti|tj (p)

tj Ti

» × (1

-

d) X
q:qp|tj

Ftj (p, q)q,j

+

dX
q|tj

q,j Ntj

­

(18)

where Ti is the set of snapshots which can directly distribute authority to ti within one step. Based on the surfer's behavior, this Markov process guarantees all states to communicate with each other, leading to a transition matrix that is irreducible and aperiodic [30]. As a result, it converges and generates a stationary probability on every page existing in any snapshot.

3.2.2 Estimating Staying Time

Pages with more in-link activity are likely to attract a surfer to spend time browsing it. We assume the web surfer prefers fresh web resources, and so the mean time (µp,i) of the surfer staying on page p at ti can be proportional to p's web freshness at ti. As discussed in Section 3.2.1, the web surfer prefers pages with high page freshness when choosing among out-going links; we use inlink freshness to model the time of a surfer staying on a web page. In this way, pages with both high in-link freshness and page freshness are more likely to be given high authority scores. Specifically, we utilize a sliding window and compute p's weighted in-link freshness centroid within it as the estimation of µp,i, which is formally given by

µp,i = k X w(ti, tj)InF (p)tj
tj Tti

(19)

where Tti is the set of

centered

on

ti,

and

P
tj

snapshots Tti w(ti

included in the sliding , tj ) = 1. In this work

window we eval-

uate

one special

case,

in

which

w(ti, tj)

=

1 |Tti |

for any

tj



Tti .

In this way, the authority score A(i) in Equation 7 is determined by

both p,i in Equation 18 and µp,i in Equation 19.

4 Experimental Setup

4.1 Data set and Evaluation
Many standard data sets such as TREC [27] usually only contain one snapshot of a web corpus, and so are not suitable to show the effectiveness of ranking models utilizing temporal information. To evaluate our proposed method, we utilize a corpus of archival web pages in the .ie domain collected by Internet Archive [20] from January 2000 to December 2007. This corpus contains 158 million

Notation of T-Fresh variants: T-Fresh(kernel, window, snapshot) kernel The kernel controlling authority propagation among
different web snapshots, where kernel  {1, 2, 3, 4, 5, 6} window The window size used in calculating average in-link
freshness for estimating staying time, where window  N snapshot The number of months spanned over the temporal graph
where 1  snapshot  88 (from Jan. 2000 to Apr. 2007)
Table 2: Notation of T-Fresh variants.
unique web pages, and approximately 12 billion temporal links. To avoid the influence of transient web pages, we extract one web graph for each month from the sub-collection of pages for which we have at least 5 crawled copies. These graphs comprise a collection of 3.8M unique pages and 435M temporal links in total.
For ranking evaluation, we choose April 2007 as our time period of interest. Ninety queries are selected from a set of sources, including those frequently used by previous researchers, and popular queries from Google Trends [18]. For each query, we have an average of 84.6 URLs judged by at least one worker of Amazon's Mechanical Turk [3]. When human editors judge each pair of <query,URL>, they are required to give a score based on (1) how relevant the page is to the query; and (2) how fresh the page would be as a result for the requested time period. The relevance score is selected from among highly relevant, relevant, borderline, not relevant and not related, which is translated to an integer gain from 4 to 0. A page with score higher than 2.5 is marked as relevant. Similar to the relevance judgement, the freshness score is selected from very fresh, fresh, borderline, stale, and very stale, which we translate into an integer scaled from 4 to 0. A page with a score higher than 2.5 is marked as fresh. All human editors were asked to give the confidence of their provided judgments, in the selection of high, medium and low. Judgements with low confidence are not included in ranking evaluation. A random sample with 76 <query, URL> pairs judged by 3 editors show that the average standard deviations of relevance and freshness judgements are 0.88 and 1.02 respectively.
Based on these judgements, we evaluate the ranking quality of our approach on both relevance and freshness over the Normalized Discounted Cumulative Gain (NDCG) [21] metric. It penalizes the highly relevant or fresh documents appearing at lower positions. Precision@k is also utilized to measure the ranking quality, which calculates the number of relevant or fresh documents within the top k results across all queries.
4.2 Compared Methods
To show the effectiveness of T-Fresh, we compare with PageRank [10] (the baseline) and several representative link-based ranking algorithms, which incorporate temporal information, including TimedPageRank [35], T-Rank [8], BuzzRank [6], TemporalRank [34], and T-Random [24]. All these algorithms combine with Okapi BM2500 [29] linearly by ranks, defined as:
(1 - )rankauthority(p) + rankBM (p)
The parameters used in Okapi BM2500 are the same as Cai et al. [11]. The variants of T-Fresh are summarized in Table 2.
4.3 Web Activity Detection
While accurate web maintenance activities might be recorded on Web servers' logs, we must infer such activities from the comparison between successive web snapshots in this work. Specifically, we assume that each page was created at the time at which it was first crawled, and each link was created when it was first found. Although some pages can automatically change a portion of its con-

118

Correlation to future page activity

Correlation to future link activity

tent in every crawl, we suppose one page has an update when its content has any difference from the previous version, or its metadata can show the last-modified time is after the crawling time of the previous one. To identify the link update, we simply assume that once a page has an update, all its out-links are considered to be updated. We admit that perfect quantification of link update activity may depend on a variety of factors, including the distance to page blocks being changed, page editing concentration inferred from content maintenance patterns, and so on. We leave the sensitivity of web activity detection accuracy on ranking performance to future work. We also assume that a page disappears when its returned HTTP response code is 4xx or 5xx. While the gain associated with each type of link and page activity can influence the ranking performance, as a preliminary study, we define these gains in Table 1, and again leave the sensitivity of ranking performance with respect to gains on web activity to future work.
5 Experimental Results
In this section, we report the results of our ranking evaluation and compare T-Fresh to representative link-based algorithms. Results demonstrate that by incorporating web freshness and propagating authority among different web snapshots, we can achieve more relevant and fresh search results.
5.1 Correlation of InF and PF
As introduced in Section 3.1.2, each page in the temporal graph is associated with InF and PF. A reasonable criteria for the good estimation of InF and PF would be their potential capability of predicting future web activities even though the correlation between them would be rather small. To better consider this idea, we compute the average correlation between web freshness scores at t and web activities at future time points, i.e., t + 1, t + 2, etc., given by Equation 3 and 5.
From Figure 2(a), P F |tt-1 and future in-link activity show positive correlation, with the strength inversely proportional to the time difference between the incremental page freshness and future in-link activities. In most cases, the correlation is maximized when P F and InF are 0.6. It indicates pages can achieve freshness scores from both activities on themselves and their neighbor pages via propagation. The correlations between InF |tt-1 and future page activities show similar trends (Figure 2(b)). One may notice that the average correlation between P F |tt-1 and in-link activities at t+1 is 0.0519, which is higher than that between InF |tt-1 and page activities at t + 1 over 13.5%. One interpretation is that a page with very fresh content tends to attract new in-links or existing in-links to validate in next time periods. From Figure 2(c) and (d), the cumulative web freshness scores can show stronger correlation to future web activities, varying with the decay parameter 2 and 4 given 1 = 3 = 1 constantly. For both P Ft and InFt, the correlations achieve the highest when 2 and 4 are 1 in most cases. To meet our criteria of good estimation about web freshness, we set P F = InF = 0.6 and 2 = 4 = 1 in the following ranking evaluation.
5.2 Ranking Performance
Figure 3 demonstrates the ranking performance in terms of relevance and freshness on metric P@10 over all the compared algorithms, under the variance of combination parameter  from 0.8 to 1. The variant of T-Fresh we choose for comparison is T-Fresh(1,1,30). For relevance evaluation, PageRank achieves its highest P@10 at 0.4894 when  is 0.97. T-Fresh performs the best among all the algorithms, achieving its highest P@10 at 0.5051 when  is 0.91, exceeding PageRank by 3.2%. The TimedPageR-

Correlation to future inlink activity

0.055

0.05

0.045

0.04

0.035

t+1

0.03

t+2

t+3

t+4

0.025

0.1

0.2

0.3

0.4

 0.5 0.6
PF

0.7

0.8

0.9

1

(a) P F |tt-1

0.046

0.044

0.042

0.04

0.038

0.036

0.034 0.032
0.03 0.1

0.2

0.3

0.4

 0.5 0.6
InF

0.7

0.8

(b) InF |tt-1

t+1 t+2 t+3 t+4

0.9

1

Correlation to future page activity

0.058

t+1

0.056

t+2

t+3

0.054

t+4

0.052

0.05

0.048

0.046

0.05

t+1

0.048

t+2

t+3

0.046

t+4

0.044

0.042

0.04

0.038

0.036

0.044

0.034

0.042

0.032

0.0001

0.001

 0.01

0.1

4

1

10

0.0001

0.001

 0.01

0.1

1

10

2

(c) P Ft

(d) InFt

Figure 2: Correlation between web freshness and future web activities.

P@10

0.54

BuzzRank

0.53

PageRank TemporalRank

0.52

TimedPageRank

T-Random

0.51

T-Rank

0.5

T-Fresh(1,1,30)

0.49

0.48

0.47

0.46

0.45

0.44

0.8

0.82 0.84 0.86 0.88

0.9

0.92 0.94 0.96 0.98

1

(a) Relevance performance: P@10.

0.37

BuzzRank

PageRank

0.36

TemporalRank TimedPageRank

T-Random

0.35

T-Rank

T-Fresh(1,1,30)

0.34

P@10

0.33

0.32

0.31

0.3

0.8

0.82 0.84 0.86 0.88

0.9

0.92 0.94 0.96 0.98

1

(b) Freshness performance: P@10.

Figure 3: Sensitivity of P@10 with respect to combination parameter .

ank places second on metric P@10, which reaches 0.5031 when  is 0.92. Under the combination parameter achieving the best P@10 for every method, we compare the ranking performance over all metrics in Table 3. T-Fresh performs the best among all the algorithms over all the metrics. Specifically, it outperforms PageRank over 24.7%, 17.8% and 7.8%, in terms of NDCG@3, NDCG@5 and NDCG@10. Single-tailed student t-tests at a confidence level of 95% demonstrate the improvements are statistically significant over PageRank on NDCG@3, NDCG@5 and NDCG@10, with pvalues 0.0001, 0.0001, 0.0016 respectively.
For freshness evaluation, Figure 3(b) shows ranking performance on metric P@10, varying with combination parameter . T-Fresh demonstrates a stable trend for P@10, which exceeds PageRank on all the experimental points. Unlike relevance evaluation in which improvements of other temporal link-based algorithms are

119

Method BM25 PageRank BuzzRank TemporalRank TimedPageRank T-Random T-rank T-Fresh(1,1,30)
Method BM25 PageRank BuzzRank TemporalRank TimedPageRank T-Random T-rank T-Fresh(1,1,30)

P@10 0.4695 0.4894 0.4770 0.4841 0.5031 0.4904 0.4875 0.5051
P@10 0.3138 0.3325 0.3327 0.3473 0.3398 0.3316 0.3356 0.3412

Relevance NDCG@3
0.2478 0.2589 0.2770 0.2706 0.2830 0.2690 0.2669 0.3229
Freshness NDCG@3
0.2137 0.1946 0.2043 0.2312 0.2443 0.2054 0.2269 0.2411

NDCG@5 0.2740 0.2840 0.2980 0.2875 0.3063 0.2877 0.2870 0.3347
NDCG@5 0.2379 0.2345 0.2234 0.2510 0.2514 0.2403 0.2498 0.2662

NDCG@10 0.3344 0.3457 0.3460 0.3524 0.3587 0.3495 0.3496 0.3729
NDCG@10 0.2805 0.2838 0.2797 0.2992 0.2972 0.2879 0.2950 0.3076

Table 3: Performance Comparison.

not obvious, more methods can produce fresher search results than PageRank. One reason is that these temporal link-based algorithms incorporate diverse temporal factors which favor fresh web pages. T-Fresh reaches its best P@10 at 0.3412 when  is 0.88, which is only inferior to TemporalRank with its highest P@10 at 0.3473 when  is 0.98. PageRank has its best P@10 at 0.3325 when  is 0.97. With individual best combination parameter  on P@10, we compare all the ranking algorithms over other metrics in Table 3. T-Fresh outperforms PageRank in terms of NDCG@3, NDCG@5 and NDCG@10 over 23.8%, 13.5% and 8.3%, with pvalues 0.0090, 0.0260 and 0.0263 respectively. One observation is the performance of PageRank on metric NDCG@3 is extremely low while its performance on NDCG@5 and NDCG@10 are not so bad. We infer that stale web pages can achieve high authority scores by PageRank, and so dominate top positions in search results.
5.3 Deeper Analysis
We study the effects of propagation kernels and window sizes used in staying time estimation on ranking performance in this section.
Figures 4(a) and (b) show the best ranking performance of TFresh(*,1,*) on metric NDCG@10 for relevance and freshness. For most kernels, the relevance performance improves with the time span of the temporal graph, and reaches the highest in [30, 60], i.e., from 2.5 to 5 years. The improvements upon using single snapshot are 4.9%, 4.1%, 4.2%, 4.9%, 5.0% and 2.8% for gaussian, triangle, cosine, circle, passage and PageRank kernels respectively. Passage kernel renders both a stable and best overall performance, followed by gaussian and circle kernels. Results from triangle and cosine kernels show larger fluctuations over time span of the temporal graph. Combining with the kernel expressions defined in Equations 12-17, we conclude that the ranking performance with respect to relevance can take advantage of appropriate emphasis on authority propagation among far away snapshots.
The ranking performance on freshness shows similar trends to relevance, though the variance is typically larger. Except for PageRank kernel, all others achieve their highest performance in the time interval [30, 60]. Passage kernel gets the best performance 0.3171 on metric NDCG@10 by outperforming the baseline (using a single snapshot) by 4.5%. One observation is that the performance of PageRank kernel suddenly falls down to around 0.295

0.375

0.37

NDCG@10

0.365

0.36

0.355 0.35
0.345 0

Gaussian kernel Triangle kernel Cosine kernel Circle kernel Passage kernel PageRank kernel
Numb1e5r of Mon3t0hs Spann45ed over 6T0emporal75Graph 90
(a) Relevance performance: NDCG@10

0.33 0.325
0.32 0.315

Gaussian kernel Triangle kernel Cosine kernel Circle kernel Passage kernel PageRank kernel

NDCG@10

0.31

0.305

0.3

0.295

0.29

0

15

30

45

60

75

90

Number of Months Spanned over Temporal Graph

(b) Freshness performance: NDCG@10

Figure 4: T-Fresh(*,1,*): Sensitivity of NDCG@10 with respect to kernel for authority propagation.

when the graph time span is beyond 30 months. One possible reason is that the authority propagation among any distinct web snapshots become very weak in PageRank kernel when the graph time span is large enough, and so historical link structures only have tiny influence on page authority estimation at the current time point. In addition, the freshness performance tends to stablize when the graph time span is over 70 months, which indicates temporal web graphs with long time span render more stable ranking performance on freshness, and it reflects the long-term freshness of web resources.
Figures 5(a) and (b) show the best ranking performance of TFresh(5,*,*) on metric NDCG@10 in terms of relevance and freshness. For relevance evaluation, our results demonstrate: (1) To use the average in-link freshness on several adjacent time points is better than to use it at a single time point when estimating staying time. We infer that average in-link freshness can render a good estimation about how active the page in-links are during a time period; (2) It does harm to ranking performance on relevance when the window size is too large; (3) Large window sizes result in large variance of ranking performance when varying the number of snapshots in the temporal web graph; (4) The ranking performance improves with the increase of graph time span in general for all the window sizes. For freshness evaluation, a clear trend in Figure 5(b) shows that a larger window size used in staying time estimation helps generate fresher search results with smaller deviation.
6 Conclusion and Future Work
Dynamic web resources can reflect how active web pages are over time. From the perspectives of in-links and the page itself, we quantify web freshness from web creators' activities. We argue that web freshness is an important attribute of web resources and can benefit a series of time-sensitive applications including archival search, news ranking, twitter message recommendation, tag recommendation and so on.
In this work we propose a temporal web link-based ranking algorithm to estimate time-dependent web page authority. It incorpo-

120

0.38

0.375

NDCG@10

0.37

0.365

0.36

1 month

2 months

0.355

4 months

8 months

12 months

0.35
0 Numb1e5r of Mon3t0hs Spann45ed over 6T0emporal75Graph 90

(a) Relevance performance: NDCG@10

0.33

0.325

0.32

NDCG@10

0.315

0.31

0.305 0.3

1 month 2 months 4 months 8 months 12 months

0.295
0 Numbe5 r of Mon1t0hs Span1n5ed over 2T0 empora2l5Graph 30

(b) Freshness performance: NDCG@10

Figure 5: T-Fresh(5,*,*): Sensitivity of NDCG@10 with respect to window size used in the stay time estimation.

rates web freshness at multiple time points to bias the web surfer's behavior on a temporal graph composed of multiple web snapshots. Experiments on a real-world archival corpus demonstrate its superiority over PageRank on both relevance and freshness by 17.8% and 13.5% in terms of NDCG@5. Results show ranking performance can benefit more from long-term historical web freshness and link structure. The best period covers the past 2.5 to 5 years.
There are a few interesting extensions. The web surfer's temporal interest changes with her position in this work. We could fix her temporal interest, and use the estimated authority score to support archival search, which can select highly authoritative page instances that best match a specific temporal interest, in addition to relevance to a given query. On the other hand, the historical information at the Internet Archive comes from an external source for commercial search engines, which means a large amount of pages may lack archival copies. In the future, we hope to find a way to mitigate the gap caused by some pages having archival copies and some without in the searching process, so that the method can be applicable to search engines seamlessly.
Acknowledgments
We thank the anonymous reviewers for their useful comments. We also thank Fernando Diaz for valuable suggestions related to recency ranking evaluation. This work was supported in part by a grant from the National Science Foundation under award IIS0803605 and an equipment grant from Sun Microsystems.
7 References
[1] A. Acharya, M. Cutts, J. Dean, P. Haahr, M. Henzinger, U. Hoelzle, S. Lawrence, K. Pfleger, O. Sercinoglu, and S. Tong. Information retrieval based on historical data. US Patent 7,346,839, USPTO, Mar. 2008.
[2] E. Adar, J. Teevan, S. Dumais, and J. L. Elsas. The web changes everything: understanding the dynamics of web content. In Proc. of 2nd ACM WSDM Conf., pages 282­291. Feb, 2009.
[3] Amazon, Inc. Amazon mechanical turk home page, 2010. http://www.mturk.com/.
[4] E. Amitay, D. Carmel, M. Herscovici, R. Lempel, and A. Soffer. Trend detection through temporal link analysis. Journal of the American Society for Information Science and Technology, 55(14):1270­1281, 2004.

[5] Z. Bar-Yossef, A. Z. Broder, R. Kumar, and A. Tomkins. Sic transit gloria telae: Towards an understading of the web's decay. In Proc. of 13th Int'l World Wide Web Conference, pages 328­337. May 2004.
[6] K. Berberich, S. Bedathur, M. Vazirgiannis, and G. Weikum. Buzzrank... and the trend is your friend. In Proc. of 15th Int'l World Wide Web Conference, pages 937­938, May 2006.
[7] K. Berberich, S. Bedathur, G. Weikum, and M. Vazirgiannis. Comparing apples and oranges: Normalized PageRank for evolving graphs. In Proc. of 16th Int'l World Wide Web Conference, pages 1145­1146, May 2007.
[8] K. Berberich, M. Vazirgiannis, and G. Weikum. Time-aware authority ranking. Internet Mathematics, 2(3):301­332, 2005.
[9] J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. Learning to recognize reliable users and content in social media with coupled mutual reinforcement. In Proc. of 18th Int'l World Wide Web Conference, pages 51­60, Apr. 2009.
[10] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. In Proc. of 7th Int'l World Wide Web Conference, pages 107­117, Apr. 1998.
[11] D. Cai, X. He, J. R. Wen, and W. Y. Ma. Block-level link analysis. In Proc. of 27th Annual Int'l ACM SIGIR Conf., Sheffield, UK, July 2004.
[12] D. Chakrabarti and C. Faloutsos. Graph mining: Laws, generators, and algorithms. ACM Comput. Surv., 38(1):2, 2006.
[13] J. Cho, S. Roy, and R. E. Adams. Page quality: In search of an unbiased web ranking. In Proc. of ACM SIGMOD, Baltimore, MD, June 2005.
[14] Y. J. Chung, M. Toyoda, and M. Kitsuregawa. A study of link farm distribution and evolution using a time series of web snapshots. In Proc. of the 5th Int'l Workshop on Adversarial Information Retrieval on the Web, pages 9­16, New York, NY, USA, 2009. ACM.
[15] O. de Kretser and A. Moffat. Effective document presentation with a locality-based similarity heuristic. In Proc. of 22nd Annual Int'l ACM SIGIR Conf., pages 113­120, New York, NY, USA, 1999. ACM.
[16] J. L. Elsas and S. T. Dumais. Leveraging temporal dynamics of document content in relevance ranking. In Proc. of 3nd ACM WSDM Conf., pages 1­10. Feb, 2010.
[17] B. Gao, T. Y. Liu, Z. Ma, T. Wang, and H. Li. A general markov framework for page importance computation. In Proc. of 18th ACM CIKM Conf., pages 1835­1838, New York, NY, USA, 2009. ACM.
[18] Google Inc. Google trends home page, 2010. http://www.google.com/trends.
[19] T. Haveliwala, S. Kamvar, A. Kamvar, and G. Jeh. An analytical comparison of approaches to personalizing pagerank. Technical report, Stanford University, 2003.
[20] Internet Archive. The Internet Archive. 2010. http://www.archive.org/.
[21] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In Proc. of 23rd Annual Int'l ACM SIGIR Conf., pages 41­48, July 2000.
[22] K. Kise, M. Junker, A. Dengel, and K. Matsumoto. Passage Retrieval Based on Density Distributions of Terms and Its Applications to Document Retrieval and Question Answering. In volume 2956 of LNCS, pages 306­327. Springer, Berlin/Heidelberg, 2004.
[23] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. In Proc. of the ACM-SIAM Symposium on Discrete Algorithms (SODA-98), pages 668­677, San Francisco, CA, Jan. 1998.
[24] Y. Li and J. Tang. Expertise search in a time-varying social network. In Proc. of 9th Int'l Web-Age Information Management Conf. (WAIM 08), July 2008.
[25] Y. Liu, B. Gao, T. Y. Liu, Y. Zhang, Z. Ma, S. He, and H. Li. Browserank: letting web users vote for page importance. In Proc. of 31st Annual Int'l ACM SIGIR Conf., pages 451­458, New York, NY, USA, 2008. ACM.
[26] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proc. of 32nd Annual Int'l ACM SIGIR Conf., pages 299­306, New York, NY, USA, 2009. ACM.
[27] NIST. Text REtrieval Conference (TREC) home page, 2010. http://trec.nist.gov/.
[28] D. Petkova and W. B. Croft. Proximity-based document representation for named entity retrieval. In Proc. of 16th ACM CIKM Conf., pages 731­740, New York, NY, USA, 2007. ACM.
[29] S. E. Robertson. Overview of the OKAPI projects. Journal of Documentation, 53:3­7, 1997.
[30] S. M. Ross. Introduction to Probability Models, Ninth Edition. Academic Press, Inc., Orlando, FL, USA, 2006.
[31] G. Shen, B. Gao, T.Y. Liu, G. Feng, S. Song, and H. Li. Detecting link spam using temporal information. In Proc. of IEEE International Conference on Data Mining, pages 1049­1053, 2006.
[32] B. Wu, V. Goel and B. D. Davison. Propagating Trust and Distrust to Demote Web Spam. In Proc. of WWW2006 MTW Workshop, 2006.
[33] Yahoo!, Inc. Yahoo! site explorer, 2010. http://siteexplorer.search.yahoo.com/.
[34] L. Yang, L. Qi, Y. P. Zhao, B. Gao, and T.Y. Liu. Link analysis using time series of web graphs. In Proc. of 16th ACM CIKM Conf., pages 1011­1014, New York, NY, USA, 2007. ACM.
[35] P. S. Yu, X. Li, and B. Liu. On the temporal dimension of search. In Proc. of 13rd Int'l World Wide Web Conference, pages 448­449. ACM Press, May 2004.

121

Ready to Buy or Just Browsing? Detecting Web Searcher Goals from Interaction Data

Qi Guo
Emory University qguo3@mathcs.emory.edu

Eugene Agichtein
Emory University eugene@mathcs.emory.edu

ABSTRACT
An improved understanding of the relationship between search intent, result quality, and searcher behavior is crucial for improving the effectiveness of web search. While recent progress in user behavior mining has been largely focused on aggregate server-side click logs, we present a new class of search behavior models that also exploit fine-grained user interactions with the search results. We show that mining these interactions, such as mouse movements and scrolling, can enable more effective detection of the user's search goals. Potential applications include automatic search evaluation, improving search ranking, result presentation, and search advertising. We describe extensive experimental evaluation over both controlled user studies, and logs of interaction data collected from hundreds of real users. The results show that our method is more effective than the current state-of-the-art techniques, both for detection of searcher goals, and for an important practical application of predicting ad clicks for a given search session.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Storage and Retrieval
General Terms
Design, Experimentation, Performance
Keywords
user intent inference; search behavior modeling; search advertising
1. INTRODUCTION
An improved understanding of searcher needs and interests is crucial for search engines to generate satisfactory search results, with applications ranging from search evaluation to improving search ranking, presentation, and usability. What makes the problem particularly daunting is that the same query may reflect different goals not only different users [29], but even for the same user at different times. For example, a user may search for "blackberry" initially to learn about the Blackberry smartphone; however, days or weeks later the same user may search for "blackberry" to identify the best deals on actually purchasing the device. Thus, identifying the most
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

popular or majority meaning for a query is not sufficient; rather, the challenge is to identify the intent of the given search, contextualized within a search task (e.g., buying a smartphone, which may involve goals such as researching the device, comparing data plans, lookup of customer reviews, and eventual purchase).
While previous studies have shown the effectiveness of eye tracking to identify user interests (e.g., [9]), unfortunately, it requires expensive equipment, limiting the applicability. However, recent work has shown the existence of coordination between the searcher gaze position and mouse movement over the search results [19, 28]. Our hypothesis is that searcher interactions such as mouse movement and scrolling can help more accurately infer searcher intent and interest in the search results. That is, like eye movements, such interactions can reflect searcher attention. These interactions can be captured with Javascript code that could be returned as part of a Search Engine Result Page (SERP). This would allow estimating which parts of the SERP the user is interested in (e.g., whether the searcher is paying more attention to the organic or the sponsored results), and provide additional clues about the search intent.
To test this hypothesis, we develop a novel model of inferring searcher intent that incorporates both search context and rich interactions with the results, such as mouse movements, hovering, scrolling, and clicking on the results (Section 3). The model is operationalized by converting these interactions into features, which can then be used as input to classification algorithms to infer the search intent from the interaction data (Section 4).
While many other dimensions of search intent have been studied (e.g., [29]), in this paper we focus on characterizing two general types of commercial intent: research and purchase, illustrated in the examples above. We focus on these intent classes for two reasons: a) these are broad classes of informational queries, and b) distinguishing between the two has significant practical applications, in particular, for search advertising. For example, a searcher issuing a seemingly commercial query (e.g., "blackberry") may not be interested in the search ads if the organic (non-sponsored) search results are sufficient for their needs. In this case, showing ads could annoy the searcher, and contribute to "training" them to ignore the ads [8]. Thus, knowing the searcher intent (and consequently, interest in viewing sponsored results) would allow search engines to target ads better; and for advertisers to better target the appropriate population of "receptive" searchers. So, if we could infer a user's current interests based on her search context and behavior, a search engine may then show more or fewer ads (or none at all) if the current user is in the "research" mode.
The experiments in this paper follow a similar progression. First, we show that the proposed interaction model helps distinguish between known "research" and "purchase" search intents in a controlled used study (Section 5.2). Then, we show that when our

130

model is applied to the search data of real users, the searches predicted to have "purchase" intent indeed have significantly higher ad clickthrough rates than those predicted to have "research" intent (Section 5.3). Finally, we apply our model to the task of predicting ad clickthrough for an individual user within the current search session - which could have significant practical applications for commercial search engines (Section 6). In summary, the contributions of this paper include:
· A richer model of searcher intent, that incorporates searcher interactions with session-level state for jointly modeling searcher goals and behavior.
· Empirical evidence that the predictions of the searcher's goals correlate with empirical ad clickthrough rates.
· A large-scale experimental evaluation of our model on predicting ad clickthrough, an important problem in its own right.
Next, we briefly survey the background and related work to put our contribution in context and to motivate our approach.
2. BACKGROUND AND MOTIVATION
Search intent detection, and more generally information seeking behavior, has been an active area of research in information science and information retrieval communities. We briefly summarize these efforts next. We then motivate our approach by considering two concrete kinds of commercial intent, "research" and "purchase" (Section 2.3). Finally, we describe a particularly important application of distinguishing between research and purchase intent to search advertising (Section 2.4).
2.1 Related Work
The origins of user modeling research can be traced to library and information science research of the 1980s. An excellent overview of the traditional "pre-Web" user modeling research is available in [5]. With the explosion of the popularity of the Web, and with increasing availability of large amounts of user data, Web usage mining has become an active area of research. In particular, inferring user intent in Web search has been studied extensively, including references [22, 1]. There is a broad consensus on the top 3 levels of intent taxonomy, namely the navigational, transactional and informational intents introduced by Broder[7]. Recently, a more specific query intent classification was presented in [29], where informational and resource (transactional) user goals are further divided into specific sub-goals.
Previous research on user behavior modeling for Web search focused on aggregated behavior of users to improve Web search or to study other general aspects of behavior [16]. Another approach is to model random walk on the click graph [13], which considers a series of interactions within a search page, but not session-level context or behavior information. Most of the previous research on predicting ad clickthrough focuses on learning from the content of displayed ads (e.g., [12, 27]), but did not take into account the session-level search context and the individual user behavior. Reference [4] considered the result (and ad) relative position and presentation features to improve clickthrough estimation, within a single page. Reference [15] addressed the detection of commercial intent in the aggregate using page-level context modeling. Another dimension of work somewhat similar in approach to ours considered query chains and browsing behavior to infer document relevance (e.g., [26]). Earlier references [11], [10] and [25], attempted to capture and identify user goals based on the query context. Most recently, a model to estimate searcher's viewing behavior based on observable click data was introduced in [32]. Our work expands

on these efforts by exploiting additional evidence (namely, richer user interaction features) for both general intent classification and for applications such as ad click prediction.
Furthermore, it has been shown that specific user goals and experience vary widely and have substantial effect on user behavior [33]. Some queries have substantial variation in intent [31], and searcher behavior can help distinguish user intent in such ambiguous cases, as we attempt to do in this paper. Recently, eye tracking has started to emerge as a useful technology for understanding some of the mechanisms behind user behavior (e.g., [14, 21]). Our work expands on the observations described in [28] and operationalized in [19], which explored eye-mouse coordination patterns. Other previous work considered mouse movement in a different setting (e.g., windows desktop operations) for biometric identification based on interactions [2], and for website usability evaluation [3]. In contrast to previous work, we address different problems - that is, we aim to predict the searchers current goal (for research goal identification), or future behavior, e.g., whether the user is likely to click on an ad sometime during the current session.
2.2 General Search Intent Detection
While it has been shown previously that the most popular intent of a query can be detected for sufficiently frequent queries (e.g., [20, 23, 6]), out aim is to detect the intent of the specific search ­ that is, for queries that could plausibly be navigational or informational in intent. Previous work has shown preliminary indications that at a coarse level, the navigational vs. informational intent of a query can be distinguished for a given search by mining user's behavior [18]. However, informational intent combines many different types of search, varying from direct lookup to exploratory search (e.g., [34]). Furthermore, specific kinds of informational intent are more difficult to identify due to (typically) lower frequency of informational queries, and consequently smaller amounts of interaction data available. Finally, informational searchers tend to exhibit higher variation in intent [31], making informational search modeling a particularly challenging problem, as described next.

(a)

(b)

Figure 1: Searcher gaze position and corresponding mouse tra-

jectory (Research intent)

2.3 Deconstructing Informational Searches
Unlike navigational intent, informational query intent encapsulates many different types of important search tasks, for example Directed vs. Undirected (Exploratory) searches, as well as more specific Lookup or Locate goals, e.g., to verify that a particular product can be obtained. Many other dimensions of informational queries have been identified, including topical [29], exploratory vs. specific [29], commercial vs. non-commercial [15]. In particular,

131

we focus on one important intersection of informational and commercial intent categories, namely Research vs. Purchase intent.
As a concrete example, consider how users with research intent examine the search engine result page (SERP) for a query "nikkor 24-70 review". This query is commercial (the searcher is probably considering whether to buy this digital camera model), but could also be research-oriented (the searcher is interested in reviews, and not yet in making an immediate purchase). Figure 1 (a) shows the gaze position "heat map" (different colors represent amount of time spent examining the corresponding page position). Figure 1 (b) shows the mouse movements performed by the subject as they were examining the SERP. This example illustrates the possible connection between user interactions on the SERP and interest in the specific results. Thus, it is important to model not only the "popular" intent of a query, but also the searcher's immediate intent based on the context (within a search session) as well as on the interactions with the search results. In addition to research importance, this capability has important practical applications to search advertising, as described next.
2.4 Application: Search Advertising
An important practical application of our methods is predicting whether the user is likely to click on search ads shown next to the "organic" results. This problem is related to the research vs. purchase orientation of the user's goals: a user is more likely to click on a search ad if they are looking to make a purchase, and less likely if they are researching a product. We verify this observation empirically by comparing the ad clickthrough of "research" and "purchase" searches as classified by our model.
Furthermore, one could predict whether the searcher is more or less likely to click on an ad in future searches within the current session. This idea is illustrated in Figure 2, which shows an example where the user hovers the mouse over the ads before she clicks on an organic result in her first search for the query "green coffee maker". In her following search for the same query, in the same session, she clicks on an ad. We call this predisposition "advertising receptiveness", and show that the user's interest in a search ad shown for a future search within the same session can be predicted based on the user interactions with the current search result page.

3. SEARCH AND USER MODEL
In this section we first describe the definitions of search tasks and search goals. We then introduce our approach to mine the contextualized fine-grained interactions.
3.1 Search Model: Tasks and Goals
Our work assumes a simplified model of search following recent information behavior literature, where a user is attempting to accomplish an overall search task by solving specific search goals, as illustrated in Figure 3.
Figure 3: Relationship between a search task, immediate goals and specific searches to accomplish each goal.
Many user information needs require multiple searches until the needed information is found. Thus, it is natural to organize individual queries into overall tasks and immediate goals, which correspond to a common information need. For this, we use the idea of a search task, which, in turn, requires more immediate goals to accomplish by submitting and examining related searches. Our operational definition of a search task is that it consists of a consecutive sequence of queries that share at least one non-stopword term with any previous query within the task. A example search session consisting of two search tasks is reported in Figure 4. We verified this simple definition of a search task manually, and out of more than 100 tasks examined, in all but 3 tasks the searches shared at least one non-stopword term with some other search in the task. In our dataset, while the 30-minute sessions tend to be 6.77 searches long on average, tasks tend to contain 2.71 searches on average, which is consistent with previous finding [33] that users perform on average only two or three query reformulations before giving up.

Figure 2: Mouse trajectory on a SERP for query "green coffee maker" with an ad click on the next search result page
If a search engine could be notified that a searcher is (or is not) interested in search advertising for their current task, the next results returned could be more accurately targeted towards this user. For example, if the user appears interested in buying a hybrid car, ads for hybrids as well as deals in the organic results should be returned. In contrast, if the user appears to be just researching hybrid technology, then the search engine should privilege customer reviews or technical articles. To achieve this real-time behavioral targeting, we argue that contextualized user interaction models are required.

Figure 4: An example user session, consisting of two consecutive disjoint search tasks.
3.2 User Model: Goal-driven Search
Our user model naturally follows our search model. A user, while solving a search task, has a number of immediate goals. While these goals are "hidden" - that is, not directly observable, the user searches (queries) and their interactions on the corresponding search results can be observed. Thus, we model a user as a nondeterministic state machine with hidden states representing user goals, and observable actions that depend on the user's current state. Our model is illustrated in Figure 5: searcher actions such as queries, result clicks, and mouse movements are observations generated by the hidden states corresponding to the user's goals. We restrict the interactions to those on the SERP to make our work more realistic: search engines are able to capture user interactions

132

over their own results, but capturing actions on other pages require significant additional effort.
For example, if the immediate user goal is informational, then longer mouse trajectories are more likely to be observed on search result pages (as the user is more likely to examine the results to decide which one is most relevant); in contrast, if the immediate user goal is navigational, he or she can quickly recognize the target site, resulting in shorter mouse trajectory and faster response time. Similarly, ad clicks are more likely to be emitted if the user is in a receptive state to search advertising (e.g., has a Purchase goal), and less likely if the user is in a non-receptive state (e.g., has a Research goal). Hence, observations including the search context and user interactions are related to the states (goals) of the users. If we can infer the hidden states using the observations, we can both recover the user's immediate search goal and potentially the overall task, as well as predict future user actions such as ad clicks, in subsequent searches.
Figure 5: Sample states and observations for a single search within a task.
Note that the predictions in our model are dependent on both the current and the previous goal state of the user, thus naturally allowing us to maintain the user's "mental state" across individual searches. Furthermore, this formalism allows for arbitrary number of hidden states which may correspond to different types of goals and more complex tasks. For example, this would allow us to model different variations of ad receptiveness. These sequential user models can (and have been) implemented in previous work (see Section 2.1). However, what makes our model unique is the rich representation of user actions on the search result pages, allowing us to potentially capture the mental state of the user while he or she is examining the search results, as we describe next.
4. INFRASTRUCTURE, FEATURES AND ALGORITHMS
We now describe the actual implementation of our system. First, we describe the infrastructure for extracting and storing user interactions (Section 4.1); then, we describe the concrete representation of these interactions as features (Section 4.2) for incorporating the interaction information into classification algorithms (Section 4.3).
4.1 Infrastructure
The user interaction data was captured by instrumenting the LibX toolbar 1. The toolbar was modified to buffer the GUI events such as mouse movements, scrolling, and key press events, and send them to our server for logging. These instrumented Web browsers were installed on approximately 150 public-use computers (mostly 1Original LibX toolbar available at www.libx.org

Windows PCs) at the Emory University library. The usage was tracked only for users who have explicitly opted in to participate in our study. No identifiable user information was stored.
Note that the instrumentation described above for the search result pages does not necessarily require a user download or installation: JavaScript code similar to what we run in the toolbar can be easily returned in the header of a Search Engine Result Page (SERP). Also, we are only modeling searcher behavior on a SERP, and do not consider the external result pages visited by clicking on the results; therefore, all the data we collected would be available to the search engine via light-weight server-side instrumentation.
In our prototype implementation we sample mouse movements and scroll events at every 5 pixels moved, or every 50 ms, whichever is more frequent, and keep all other events (e.g., MouseDown, KeyPress events) without downsampling.
4.2 Features
We now describe the types of interactions captured and the corresponding feature representations. The major feature groups and representative features are summarized in Table 1. Each feature group is described below to the extent permitted by space limitations. Complete feature specification and the dataset from our user study experiments described in Section 5.1 are available online2. The features used to represent each search are illustrated in Figure 5, with some features spanning multiple searches to provide session-level context, as described below.
Query group: This group of features is designed to capture the same information as was used in the previous studies of capturing clicks in the context of previous and subsequent queries [25, 26]. Specifically, we include the tokens from the text of the query (after frequency thresholding); the length of the query in characters and words, and binary features such as IncludesTLD, which is 1 if the query includes a TLD token such as ".com" or ".org".
SERPContent group: These features represent the text (and markup) content of the SERP. Specifically, both the organic results and the sponsored results are allocated a separate feature space, and include the tokens from OrganicText and AdText, respectively (after frequency filtering). Additionally, tokens and markup is extracted from the whole SERP, regardless of the type of result (organic or sponsored) and represented as the SERPText features (after frequency filtering).
ResultQuality group: These features aim to capture coarse information about the SERP relation to the query, namely how many words in the organic result summaries match the query terms (SnippetOverlap); how many words in the text of the ads match the query terms (AdOverlap); as well as the normalized versions of these features computed by dividing by the query length, in words. We also capture the number of ads, number of ads at the top of the SERP (NorthAds), and number of ads on the side (EastAds). These features has been shown in previous work to correlate with the degree of commercial interest in the query.
Interaction group: The interaction features aim to capture the client-side interaction events. Specifically, the features include: the number of SERP GUI events, such as number of mouse events (TotalMouse), scroll events (TotalScroll) and keypress events (TotalKeypress); time features, such as SERP deliberation time, measured as seconds until first GUI event (DeliberationTime), the time until the first result click (SERPDwellTime); and hovering features, that
2http://ir.mathcs.emory.edu/intent/sigir2010/.

133

Feature group Query SERP Content
Result Quality Interaction
Click Context All

Count 4 3
7 99
7 7 127

Description
QueryTokens* (unigram), QueryLengthChars, QueryLengthWord, IncludesTLD (1 if contains ".com", ".edu"). AdText* (unigram), OrganicText* (unigram), SERPText* (unigram). Each feature contains 100 most frequent terms from each area of the SERP (e.g., 100 most frequent tokens in the ads). TotalAds, NorthAds, EastAds, SnippetOverlap, SnippetOverlapNorm, AdOverlap, AdOverlapNorm MouseRange, MouseCoordinates, MouseSpeed, MouseAcceleration, TotalMouse, TotalScroll, TotalKeypress, SERPDwellTime, DeliberationTime, HoverEastAd, HoverNorthAd, HoverOrganic, etc (see main text) ClickUrl* (unigram), NumBrowseAfterClick, AverageDwellTime, TotalDwellTime, SAT, DSAT, ClickType IsInitialQ, IsSameQ, IsReformulatedQ, IsExpansionQ, IsContractedQ, RepeatQ, SERPIndex
All features and feature classes used for experiments

Table 1: Summary of the features used for representing searcher context and interactions. The full feature specification and sample data are available at http://ir.mathcs.emory.edu/intent/sigir2010/. .

measure how the time that the mouse hovers over an area of interest such as north ads, east ads, and organic results regions.
Our interaction features also aims to capture the physiological characteristics hidden in mouse movements, following reference [24]. In particular, the mouse trajectory representation is split into two subgroups, (Mouse (Global)) and (Mouse (Local)):
· Mouse (Global): the features include the length, vertical and horizontal ranges of mouse trajectory, in pixels; also, the features describing the general statistics of the trajectory, namely, the means and the standard deviations of the mouse coordinates, the difference in distance and time between two adjacent mouse points, the velocity, acceleration, slope and rotation (computed from the difference of slopes between neighboring points).
· Mouse (Segment): to distinguish the patterns in different stages of the user interactions with the search results, we split each mouse trajectory into five segments: initial, early, middle, late, and end. Each of the five segments contains 20% of the sample points of the trajectories. We then compute the same properties (e.g., speed, acceleration, slope etc.) as above, but computed for each segment individually. The intuition is to capture mouse movement during 5 different stages of SERP examination (e.g., first two segments correspond to the visual search stage, and last segment corresponds to moving the mouse to click on a result).
Click group: Captures the types and properties of result clicks and SERP revisits. Specifically, the features include tokens in the clicked URL (ClickUrl); the number of URLs visited after a result click (NumBrowseAfterClick), the average and total dwell time on each visited result URL. We also identify satisfied URL visits (those with dwell time greater than 30 seconds [17]), and "dissatisfied" visits (those with dwell time less than 15 seconds [30]), as well as the number and the position of the satisfied and dissatisfied URL visits within the task. Finally, we capture the type of the click, such as a click on organic result, on a menu item, or on a search ad (ClickType).
Context group: Captures where the search belongs to within a task context, the features include: whether the query is initial in session (IsInitialQ), whether the query is identical to previous query (IsSameQ), whether the query overlap with previous query submitted; respectively true if a word is replaced (IsReformulatedQ), or added(IsExpansionQ), or removed (IsContractedQ); whether the query was issued within same session (RepeatQ); the current position (progress) within a search session, e.g., whether this was a first, second, or 5th search in the session (SERPIndex). In summary, the features attempt to capture properties of the query, the search context, and the interaction on the SERP page itself.

4.3 Classifier Implementation
We now describe the details of classifier implementations we considered. We experiment with two different families of classifiers: Support Vector Machine (SVM) that supports flexible feature representation, and Conditional Random Fields (CRF), which naturally supports modeling sequences.
Figure 6: Our SVM classifier configuration.
Support Vector Machine (SVM): We chose the Weka implementation of the SVM implementation with the SMO optimization algorithm, using polynomial kernel with degree 4 (chosen during preliminary development experiments). Conditional Random Field (CRF): The SVM representation allows only limited representation of the search state: that is, whether the searcher is still in "exploratory" stage of the search or is now on the next goal of verifying specific information found during exploration "lookup" stage. To explicitly model these different states (goals) within a session, CRF allows us to define a conditional probability over hidden state sequences given a particular observation sequence of searches. Take predicting future ad clicks as an example: at training, the hidden state is assigned according to whether an ad click was observed in the future searches within the task. Note that an ad click on current SERP is simply an observation, and is not necessarily an indication whether a user remains likely to click on future search ad in the same task. At test time, we identify the intent sequence that maximizes the conditional probability of the observation sequence. In the current system we used the Mallet3 implementation. 3Available at http://mallet.cs.umass.edu/.

134

5. EXPERIMENTAL RESULTS: BUYING OR BROWSING?
This section presents the experimental setup and results for our case study of informational intent detection, namely research vs. purchase intent. The goal of the experiments is to validate whether interaction data can help more accurately distinguish searcher intent classes. We first describe the data gathering and construction of the "ground truth" data and our evaluation metrics. Then, we report and analyze the experimental results (Section 5.2) obtained in a small user study. Finally, we verify our results over a large log of real user interaction data.
5.1 Search Task and Data Collection
Task: the problem is to detect, given a user's behavior on a SERP, whether the query had research or purchase intent. Data collection: We performed a user study with 10 subjects, who were graduate and undergraduate students and university staff, that is, were technically savvy and had some experience with Web search. The subjects were asked to perform two search sessions. Each subject was asked first to research a product of interest to them for potential future purchase. Then, the same subject was asked to attempt to "buy" an item of immediate interest to the subject, which may or may not be the same item the subject was researching in the previous stage. The subjects were not restricted on time, and could submit any queries (usually, to the Google search engine) and click on any results.
All the interactions were tracked using our Firefox plugin. At the same time, the searcher gaze position was tracked using the EyeTech TM3 integrated eye tracker at approximately 30Hz sampling rate, for subsequent analysis. Additionally, each search and corresponding SERP interactions were labeled as parts of a research or purchase session, according to the explicitly stated intent of the corresponding session.
Methods Compared:
· Baseline: always guesses the majority class (Research).
· SVM(Query): similar to the state-of-the-art models using query features (e.g., [26, 25]), implemented using Query group features described in Section 4.2, and trained using the SVM model.
· SVM(All): the SVM classifier implemented using the features described in Section 4.2 to infer the user goal for each search (independently of other searches in the session).
5.2 Results
We now report the results for classifying whether the searcher had research or purchase intent. We split the data by time, using the first 90% of searches for each subject's data for training, and the rest for testing (recall, that each subject had two sessions, one research, and one purchase). In this experiment, the intent of each search is predicted independently of other searches in the session (we will use a full session-level model in Section 6). To evaluate classification performance, we use the standard Precision, Recall and Macro-averaged F1. Table 2 shows that our system, SVM(All), outperforms both baselines, resulting in accuracy of almost 97%.
To identify the most important features contributing to the classification, we performed feature ablation by removing one feature group at a time from the classifier(Table 3). All the feature groups provide significant contributions, but the most important features appear to be SERPContent and Interaction features: with these features removed, accuracy degrades to 86.7% from 96.7% with these features included. This makes sense since the SERP content can help enrich the context of a query, while the Interaction

features provide additional clues about the searcher interest. However, since this user study was done over a rather small number of subjects, further investigation and additional user study is needed to fully understand the connection between various feature groups. To complement these results, we validate our model on an objective ad clickthrough metric on a much larger user population, as described next.

Method

Acc.

Research

Purchase

F1

Prec. Recall Prec. Recall

Baseline

56.7 56.7 100

0

0 36.2

SVM(Query) 86.7 93.3 82.4 80.0 92.3 86.6

SVM(All)

96.7 100 94.1 92.9 100 96.6

Table 2: Classification performance for research vs. purchase.

Method

Acc. Research

Purchase F1

Prec. Rec. Prec. Rec.

SVM(All) SVM(-Query)

96.7 100 94.1 92.9 100 96.6 93.3 94.1 94.1 92.3 92.3 93.2

SVM(-SERPContent) 86.7 93.3 82.4 80.0 92.3 86.6

SVM(-ResultQuality) 93.3 100 88.2 86.7 100 93.3

SVM(-Click)

90.0 93.8 88.2 85.7 92.3 89.9

SVM(-Interaction)

86.7 100 76.5 76.5 100 86.7

SVM(-Context)

93.3 100 88.2 86.7 100 93.3

Table 3: Feature ablation results for intent classification.

5.3 Ad Clickthrough on Real Search Data
To better understand the effectiveness of our classifier, we evaluated our model on a large dataset of real user searches collected in the Emory University libraries using the infrastructure described in Section 4.1. We hypothesize that for research searches, clickthrough on search ads should be lower than for purchase searches. Therefore, we can evaluate the effectiveness of our intent classification model by comparing the ad clickthrough on the searches classified as research by our model, to those classified as purchase. To avoid "cheating", no click group or result URL features were used, as they could provide information to the classifier about the ad click on the SERP.
Data: The data was gathered from mid-August through mid-December 2008. To ensure data consistency, we generated a longitudinal dataset of the usage for 440 opted-in users, who clicked a search ad at least once during this period. For this universe of users we include all the search sessions attempted during this period. The resulting dataset contains 4,377 login sessions, comprising 6,476 search sessions, 16,693 search tasks and 45,212 searches.
Results: The predicted purchase searches have substantially higher ad clickthrough rates (9.7%) compared to research searches (4.1%), and all searches with at least one ad displayed (5.9%). These statistics are summarized in Table 4. As hypothesized, our research and purchase predictions indeed correlate with ad clickthrough of real users. What makes this result remarkable is that our model was trained on a small dataset compiled from just 10 subjects in the user study (with clear intent labels), yet still provides promising performance on unconstrained user data obtained "in the wild".

Search class #ACLK (%) #SERP with Ads Ad CTR (%)

All

854

14545

5.9

Research

417

10054

4.1 (-29%))

Purchase

437

4491

9.7 (+66%)

Table 4: Search ad clickthrough statistics on all search pages

(All), and for searches classified as "Research" and "Purchase".

135

6. PREDICTING AD CLICKTHROUGH
We now turn to the practical application of predicting future search ad clicks for the current user session. We first define the problem more formally, then describe the data and metrics (Section 6.2) used for this task, the methods compared (Sections 6.3 and 6.4), followed by the empirical results and discussion (Section 6.5, which concludes this section.
6.1 Problem statement
This problem of predicting future ad clickthrough for the current user is distinct from predicting ad clickthrough in aggregate for many users. We define our problem as follows: Given the first i searches in a search task S(s1, ..., si, ...., sm), and the searcher behavior on these first i SERPs, predict whether the searcher will click on an ad on the SERP within the current search task S, for any of the future searches si+1, si+2, ..., sm.
6.2 Data and Evaluation Metrics
For this task, the dataset was based on the interaction data collected from the opted-in users in the Emory Libraries, and consists of the same log data as described in Section 5.3.

Evaluation Metrics: To focus on the ad click prediction, we report the results for the positive class, i.e., the "advertising-receptive" state. Specifically, we report Precision (P), Recall (R), and F1measure (F1) calculated as follows:

· Precision (P): Precision is computed with respect to the positive (receptive) class, as fraction of true positives over all predicted positives. Specifically, for each search task, the precision is the fraction of correct positive predictions over all positive predictions for the task, averaged across all the search tasks.

· Recall (R): for each task, the recall is computed as the fraction of correct positive predictions over all positive labels in the task. This value is then averaged over all the tasks.

·

F1-measure (F1): F1 measure, computed as

2P ·R P +R

.

6.3 Methods Compared

· CRF(Query): CRF model, implemented using the Query group features as described in Section 4.2.

· CRF(Query+Click): CRF model, implemented using Query group and Click group features as described in Section 4.2.

· CRF(All): CRF model, implemented using all features as described in Section 4.2.

· CRF(All-Interaction): Same as above, but with the Interaction group features removed.

6.4 Classifier Configuration
We configured the CRF to have two hidden states, A+ and A-, corresponding to "Receptive" (meaning that an ad click is expected in a future search within the current session), and "Not receptive" (meaning to not expect any future ad clicks within the current session).

6.5 Results and Discussion
To simulate an operational environment, we split the data by time, and use the first 80% of the sessions for training the system, and the remaining 20% of the sessions for test. The results on the test set are reported in Table 5. As we can see, our system achieves the highest performance on all metrics, compared to the baselines. Specifically, CRF(Query+Click) outperforms CRF(Query) on the

Figure 7: CRF configuration Model with two hidden states, A+ (receptive), and A- (non-receptive), with labels assigned according to the observed future ad clickthrough - here on the third search result pages within the session.

ad receptiveness prediction task by incorporating the click information, and the CRF(All) system further increases both precision and recall by incorporating additional behavior features. Interestingly, removing the Interaction group of features from the full system (CRF(All-Interaction)) degrades the recall and overall F1 performance of the system, while precision is somewhat improved. This suggests that interaction features help detect additional cases (compared to query and click information alone) where a searcher may be interested in the ads, while occasionally introducing additional false positives. We discuss the performance of the system in more detail, next.

Method CRF(Query) CRF(Query+Click) CRF(All) CRF(All-Interaction)

Precision
0.05 (-) 0.14 (+153%) 0.15 (+170%) 0.16 (+206%)

Recall
0.11 (-) 0.12 (+11%) 0.21 (+99%) 0.14 (+32%)

F1
0.07 (-) 0.13 (+77%) 0.17 (+141%) 0.15 (+112%)

Table 5: Precision, Recall, and F1 for predicting ad receptiveness within a search task

Potential limitations: While our ad click prediction experiments were performed over a relatively large dataset collected over thousands of real search sessions for hundreds of users, we acknowledge some limitations of our study. Specifically, our user population is relatively homogeneous (college and graduate students, and faculty and staff), and substantially more training data may be required to achieve this performance for the general population. Another limitation is lack of conversion data: ad clickthrough is just one evaluation metric, and may not be predictive of the ultimate intent of the searcher (e.g., a searcher may click on an ad out of curiosity). Despite the limitations above, our population is large enough that useful conclusions could be drawn. To better understand the system performance and guide follow-up research, we describe representative case studies to provide better understanding of our system's performance:
Not using a mouse as a reading aid: this is the most frequent source of error introduced by the interaction features: when a mouse is not used to mark or focus user interest, interaction information could be misleading. One possible approach is to classify users into different groups according to mouse usage patterns, and train separate prediction models for each group.
Long difficult research sessions with ad clicks: in such cases, the searcher began with a research intent, and in her first several searches, no interest in ads were shown. However, as the session progresses, the user eventually clicks on an ad as the promising organic results are exhausted. For example, one searcher submitted

136

a query "comcast basic cable channels" in her task to find Comcast's basic cable line-up, and finally clicked on an ad because of the unsatisfactory organic results. Such ad clicks appear to be different from cases where a user clicks on ads because of a premeditated purchasing intent. We plan to investigate the different types of clicks in our future work.
Commercial purchase sessions without ad clicks: in such cases, a searcher examined the ads but did not click on any. This could be due to poor quality of search ads, or to availability of more promising organic search results. For example, one searcher submitted a query "george boots" and clicked on a Google's Product Search result. In this case, the searcher might be actually receptive to search advertising. However, we label such sessions as "non-receptive" since there's no future ad click to use as evidence. One natural extension of our model is to expand our labels by considering clicks on product search results to be similar to ad clicks with respect to purchasing intent. Another possibility may be that particular users could be generally "less-receptive" to advertising. To tackle this problem, personalizing our user models is a promising direction for future work.
7. CONCLUSIONS
In this paper we introduced a rich searcher behavior model that captures not only the queries and clicks, but also the fine-grained interactions with the search results, contextualized within a search session. Our experimental results on three related tasks demonstrate the generality and flexibility of our approach. The first task, predicting research vs. purchase goal of the user, provided insights about the feature groups most important for distinguishing these variants of commercial intent. In the second task, we validated our model by showing correlation of the predicted search intents with the ad clickthrough rates of real users. Finally, we demonstrated the performance of our model for an important practical application of predicting future search ad clickthrough within the current search session of each user.
In the future, we plan to expand our model to consider user interactions and page context other than the search result pages. In particular, we plan to incorporate the interactions on the intermediate result pages visited between successive searches, which may provide additional contextual information. We also plan to incorporate the user's history to enable the personalization of intent inference and search behavior prediction.
Acknowledgments
The authors thank Microsoft Research and Yahoo! Research for partially supporting this work through faculty research grants.
8. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. In Proc. of SIGIR, 2006.
[2] A. Ahmed and I. Traore. Detecting computer intrusions using behavioral biometrics. In Proc. of PST, 2005.
[3] R. Atterer, M. Wnuk, and A. Schmidt. Knowing the user's every move: user activity tracking for website usability evaluation and implicit interaction. In Proc. of WWW, 2006.
[4] H. Becker, C. Meek, and D. Chickering. Modeling Contextual Factors of Click Rates. In Proc. of AAAI, 2007.
[5] N. J. Belkin. User modeling in information retrieval. Tutorial at UM97, 1997.
[6] D. J. Brenes, D. Gayo-Avello, and K. Pérez-González. Survey and evaluation of query intent detection methods. In Proc. of WSCD workshop, pages 1­7, 2009.

[7] A. Broder. A taxonomy of web search. SIGIR Forum, 2002.
[8] A. Z. Broder, M. Ciaramita, M. Fontoura, E. Gabrilovich, V. Josifovski, D. Metzler, V. Murdock, and V. Plachouras. To swing or not to swing: Learning when (not) to advertise. In Proc. of CIKM, 2008.
[9] G. Buscher, A. Dengel, and L. van Elst. Query expansion using gaze-based feedback on the subdocument level. In Proc. of SIGIR, 2008.
[10] H. Cao, D. H. Hu, D. Shen, D. Jiang, J.-T. Sun, E. Chen, and Q. Yang. Context-aware query classification. In Proc. of SIGIR, 2009.
[11] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. Context-aware query suggestion by mining click-through and session data. In Proc. of KDD, 2008.
[12] M. Ciaramita, V. Murdock, and V. Plachouras. Online learning from click data for sponsored search. In Proc. of WWW, 2008.
[13] N. Craswell and M. Szummer. Random walks on the click graph. In Proc. SIGIR, 2007.
[14] E. Cutrell and Z. Guan. What are you looking for?: an eye-tracking study of information usage in web search. In Proc. of CHI, 2007.
[15] H. K. Dai, L. Zhao, Z. Nie, J.-R. Wen, L. Wang, and Y. Li. Detecting online commercial intention (oci). In Proc. of WWW, 2006.
[16] D. Downey, S. T. Dumais, and E. Horvitz. Models of searching and browsing: Languages, studies, and application. In Proc. of IJCAI, 2007.
[17] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Transactions on Information Systems, 23(2), 2005.
[18] Q. Guo and E. Agichtein. Exploring mouse movements for inferring query intent. In Proc. SIGIR, 2008.
[19] Q. Guo and E. Agichtein. Towards predicting web searcher gaze position from mouse movements. In Proc. CHI Extended Abstracts, 2010.
[20] B. J. Jansen, D. L. Booth, and A. Spink. Determining the user intent of web search engine queries. In Proc. of WWW, 2007.
[21] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. Inf. Syst., 25(2), 2007.
[22] U. Lee, Z. Liu, and J. Cho. Automatic identification of user goals in web search. In Proc. of WWW, 2005.
[23] X. Li, Y.-Y. Wang, and A. Acero. Learning query intent from regularized click graphs. In SIGIR, pages 339­346, 2008.
[24] J. G. Phillips and T. J. Triggs. Characteristics of cursor trajectories controlled by the computer mouse. Ergonomics, 2001.
[25] B. Piwowarski, G. Dupret, and R. Jones. Mining user web search activity with layered bayesian networks or how to capture a click in its context. In Proc. of WSDM, 2009.
[26] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In Proc. of KDD, 2005.
[27] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In Proc. of WWW, 2007.
[28] K. Rodden, X. Fu, A. Aula, and I. Spiro. Eye-mouse coordination patterns on web search results pages. In Proc. of CHI, 2008.
[29] D. E. Rose and D. Levinson. Understanding user goals in web search. In Proc. of WWW, 2004.
[30] D. Sculley, R. G. Malkin, S. Basu, and R. J. Bayardo. Predicting bounce rates in sponsored search advertisements. In Proc. of KDD, 2009.
[31] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: modeling queries with variation in user intent. In Proc. of SIGIR, 2008.
[32] K. Wang, N. Gloy, and X. Li. Inferring search behaviors using partially observable Markov (POM) model. In Proc. of WSDM, 2010.
[33] R. W. White and S. M. Drucker. Investigating behavioral variability in web search. In Proc. of WWW, 2007.
[34] R. W. White and R.A. Roth. Exploratory Search: Beyond the Query-Response Paradigm. Morgan & Claypool Synthesis Lectures on Information Concepts, Retrieval, and Services, 2009.

137

Ranking for the Conversion Funnel
Abraham Bagherjeiran, Andrew Hatch, Adwait Ratnaparkhi
Yahoo! Santa Clara, CA, USA
{abagher,aohatch,adwaitr}@yahoo-inc.com

ABSTRACT
In contextual advertising advertisers show ads to users so that they will click on them and eventually purchase a product. Optimizing this action sequence, called the conversion funnel, is the ultimate goal of advertising. Advertisers, however, often have very different sub-goals for their ads such as purchase, request for a quote, or simply a site visit. Often an improvement for one advertiser's goal comes at the expense of others. A single ranking function must balance these different goals in order to make an efficient system for all advertisers. We propose a ranking method that globally balances the goals of all advertisers, while simultaneously improving overall performance. Our method has been shown to improve significantly over the baseline in online traffic at a major ad network.
Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Classifier Design and Evaluation
General Terms
Experimentation, Measurement
1. INTRODUCTION
An advertiser creates an online ad for one of two main purposes: brand awareness or performance. In brand awareness, the advertiser simply wants to make the user aware of some message but does not expect the user to take any immediate action such as an ad click. In performance advertising, the advertiser wants the user to click on the ad so that the advertiser can offer a product or service. Traditionally contextual advertising has been associated with performance advertising where advertisers pay per click.
Although they pay for clicks, for some advertisers, showing an ad because a click is likely is not enough. When a user visits a web page, an ad search engine searches a database of ads with the content of the page as the query [2].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Ads are then ranked to optimize for the click rate or CTR [3]. Budget-conscious advertisers would prefer to spend their limited budget only on clicks that will convert.
A conversion is the business term for an action that has some benefit to the advertiser and happens after the click. In order for a conversion to occur a specific set of events, called the conversion funnel, has to occur. As shown in Figure 1, the ad impression is served, the user clicks on the impression, visits the advertiser's site, and then converts. A user is then said to convert from a regular user into a potential business lead or customer. However, not all conversions have the same meaning or value to all advertisers. Some advertisers want the user to purchase some product whereas others want the user to request information about a product by signing up for a newsletter. Both actions are important to the advertisers but if we treat them as equivalent we would say that the latter is more important because it is more frequent.
Our ranking problem is as follows: how to rank ad impressions for conversions but still charge for clicks. Within the conversion funnel, ranking can only influence the impression. The ideal ranking would rank impressions into three groups: conversions followed by clicks followed by non-clicking impressions. Since advertisers define conversions differently, the ranking should also take into account the importance of conversions when ranking the ads.
Our main contributions are as follows. First, we discuss some of the challenges and constraints in optimizing for conversions. Second, we propose generally useful feature construction methods for dealing with differently valued examples. Finally, we show how to rank documents into groups when the groups are highly imbalanced.
The rest of the paper is organized as follows. Section 2 gives a more detailed introduction to contextual advertising and conversions. Section 3 discusses related work. Section 4 describes the feature construction. Section 5 describes the ranking method. Finally, Section 6 describes the offline and online experiments.
2. CONTEXTUAL ADVERTISING
The following section provides a general overview of the contextual advertising system and ranking. The system operates as follows: Let's say that a user navigates to a web page that serves contextual ads. We refer to the event of user u viewing page p as an impression. For every impression of p, the system retrieves a set of candidate ads from an ad index. The candidate ads are selected to maximize the

146

Impressions
Clicks
Conversions
Figure 1: The conversion funnel is the sequence of events that a user experiences after visiting a web page with contextual ads. The size of each segment indicates the relative volume of the event. Typical rates are on the order of 1 click in 1,000 impressions and 1 conversion in 100 clicks­1 conversion in 100,000 impressions.
degree to which various terms and features in the ad match the given page.
A click model is then used to estimate the click probability for each ad a. The ads are then ranked according to their expected cost per mille (ECPM):
P (click (a) |p, u, a) V (a)
where (p, u, a) denotes the context of the impression and V (a) is the advertiser's bid, which represents the maximum cost-per-click of the ad. For the remainder of the discussion, the context is implicit in each expression, so we will denote P (click(a) | p, u, a) as P (C). Given the ECPM-ranked set of ads, the system returns the top k ads to be displayed on the given page. The number k is determined by the publisher and is typically equal to 3 or 4 ads.
2.1 Conversion Funnel
The conversion funnel, depicted in Figure 1, is a threestep process. Just before a web page is rendered to a user, an ad call is issued to a server. The ad server selects an appropriate slate of ads which are rendered on the page. When the user sees these ads, we say that an impression has occurred. The user views the page and the ad and decides whether to click. If the user clicks on the ad, the user visits the landing page and other pages created by the advertiser.
An advertiser places a beacon on a conversion page, which is a specific page on his or her web site. The beacon fires when a user visits this page after having first clicked on an ad. The conversion event is associated only with the page visited. It is up to the advertiser to decide what is on the page. The page could be the receipt page after purchasing a product, but it could also be a visit to a product description page. In these cases, the conversion is attributed to the latest click and all previous clicks are designated as supporting clicks.
The key distinction between clicks and conversions is that clicks are defined and instrumented by the ad network but conversions are defined and instrumented by advertisers. Like clicks, conversions have different values for the advertisers. Unlike clicks, however, conversions are defined at the discretion of the advertiser and their rates relative to clicks vary considerably. For example, one advertiser's conversion could be filling out a form for more information while another advertiser's conversion could be to purchase the advertised product.

2.2 Pricing for Contextual Advertising
An ad server plays a dual role in contextual advertising, both making the market and participating in it. The ad server's market-making role is to select the appropriate ad a to maximize revenue for the publisher. It conducts an auction wherein each advertiser submits a bid for the impression and the highest bidder wins. Ad servers are usually run by ad networks so the ad server not only conducts an auction but also sets the bids on behalf of the advertiser. The ad server consults a click model to estimates P (C) and then computes the following bid for each impression:
B(a) = P^(C)V (C)

where V (C) is assumed to be the true value and bid for the click. The auction run by the ad server is a generalized second-price auction which means that the true price paid is less than V (C) [4]. The final price charged by the ad network for each click is:

V

(C2

)

P^ P^

(C2) (C1)



V (C1)

(1)

where P^(Ci) is the estimated click rate for ad ai. In this pricing formula, the advertiser receives a discount if the ad is more clickable than the next ad. The discount is greater the more clickable the ad is.

2.3 Ranking for Ad Serving
When deciding what ad to serve, the ad server has considerable flexibility in ranking. Because it acts on behalf of the publisher and advertiser it must find a compromise behind highly clickable ads and high value ads. The pricing formula above gives us the method to achieve both these goals. We will show how the ad server can decrease the expected costs to the advertiser by better predicting probability of click.
A performance-focused advertiser is only interested in one thing­maximizing the return on the advertising investment. Performance ads are designed to be measurable and accountable such that each ad has an expected value E [V (a)]. An impression can result in one of three outcomes: user does not click (I), user clicks but does not convert (C), user clicks then converts (N ). The advertiser has a value defined for each possible outcome. The expected value of an impression for an advertiser is as follows:

E [V (a)] = P (I)V (I) + P (C)V (C) + P (N )V (N ) (2)

In a market that sells impressions (cost-per-impression) we would expect the bid to converge to the expected value E [V (a)].
Because the final cost of a click is determined not by the expected value in Equation 2 but by the pricing formula from Equation 1, the discount applied to the bid of ad a1 is proportional to the difference in quality of the ad. A new ranking method can significantly alter the cost of a click with only a slight change to the scores of a pair of ads. Consider the following example, two ads have the same bid for a click and the same CTR. The cost of a click on a1 is equal to its bid because P (C2) = P (C1) = P (C). If a new ranking method changes P (C1) to P (C1) for  > 1 but leaves P (C2)unchanged, then the cost of clicks to a1 decrease exponentially by a factor , but the overall rank does not change. In the case of ranking for conversions, we would like to separate converting clicks from non-converting

147

clicks with a wide margin. This will cause the price of some non-converting click to increase but the cost of converting clicks will decrease. Our proposed ranking methods will have to be careful not to make the price differences too extreme.
3. RELATED WORK
Contextual advertising and ranking in this field have recently received much attention in the literature. Most major online ad networks (Yahoo!, Microsoft, and Google) offer some contextual advertising service with a pay-per-click (PPC) system, where the advertiser is charged a fee every time a user clicks on an ad. These systems use click models to automatically estimate the probability that a given ad impression will receive a click. Click models are typically trained on a variety of different signals. For example, most click models incorporate historical click-through-rate (CTR) information for specific ads or for specific page-ad pairs. Only recently has post-click behavior emerged for textual advertising, where it was shown that the page visited after clicks are somewhat relevant to predicting post-click activity [1].
A number of studies have used the co-occurrence of words and phrases within pages and ads to measure ad relevance (for example, see [9],[5],[2]). In these studies, the problem of matching ads with pages is translated into a similarity search in a vector space. Each page or ad is represented as a vector of features, which can include words and phrases, along with higher-level semantic classes (see [2]). The matching problem then reduces to the task of finding the set of ads that are closest to a given page.
In [9], Ribeiro-Neto et al. examine vector-space representations where the page and ad vectors are based on extracted keywords. The authors show that the vocabulary used in ads does not always match the vocabulary used in pages; hence, there exists an impedance mismatch between pages and ads. A number of techniques have been proposed to correct this mismatch. For example, in [9], the authors use a form of query expansion, where the page vocabulary is augmented with terms from other similar pages. Other studies have examined the use of semantic classes to match pages with ads. For example, in [2], Broder et al. map pages and ads to a common set of semantic classes, which are then used as features in a vector-space model. In [8], Ratnaparkhi introduces a page-ad probability model in which semantic relationships between page terms and ad terms are modeled with hidden classes. Another approach uses features from machine translation techniques to improve the matching between pages and ads (see [7]).
4. FEATURE CONSTRUCTION
Our training data consists of click logs from a major contextual ad network. Each log entry consists of a page-ad pair and a class label indicating whether the ad was clicked, converted or not. We extract the following raw features from the logs:
· HTML of the web page.
· Text of the ad creative.
· Keywords bidded on by the advertiser.
· Metadata such as price paid, id of the advertiser, etc.

In this section we discuss methods to construct higher-level features from the logs.
4.1 Matched Keywords
Advertisers annotate their ads with keywords in addition to the words that are shown to the user. For example, the title of an ad could be "Discounts on car insurance" but the advertiser wants to show the ad on pages about new cars, so the advertiser adds keywords such as "2010 honda" or "mazda 3". These additional words are added to the term vector of the creative. The intensity of the terms for the ad is the relative frequency with which the term occurs in the ad.
A publisher chooses to place text on a web page to provide information to the user. A similar feature extraction function is created for a web page. Terms are extracted from different zones of the page such as the title, bold text, headings, etc. For each term, the final term frequency is the weighted combination of its frequency of the term in each zone.
Matched keywords are defined as the intersection of the terms that occur both in the page and in the ad. The intensity of the matched features are the product of the intensities of the page and ad features.
4.2 Site-Level Interactions
Although semantic similarity between page and ad influences the propensity to click, we have observed that there is little influence on conversions. We consider what other factors might be useful for discriminating conversions from clicks.
One obvious feature is the quality of the ad. If the ad comes from a reputable advertiser then, a user is likely to find what he or she needs. Alternatively, many advertisers engage in click arbitrage. A click on their ad leads to a page with more ads. Arbitrageurs implement a distributed price setting mechanism in the marketplace. They will buy unused inventory by bidding on keywords on pages that do not have any other ads, but are priced low. They then show other ads (or in some cases more of the same ads) that are presumably priced higher. These ads may themselves be very appealing to the user but of poor quality in that the advertiser is not actually selling the product advertised.
An additional feature is the site hosting the page. Consider how users arrive to different web pages. If the page is shown on a known shopping site, then the user is "in the mood" to purchase a product. If the user is reading the news, the user might simply ignore the ad altogether or click by accident. In these cases, the content on the page may be similar, as in a site for booking plane travel and an article that mentions a tragic airplane crash. We can construct features that condition the text matches on the domain of the web page.
Post-click features describe what is to the user after clicking on the ad. For example, the page immediately after the click­the landing page has some influence over actions. The specificity of the ad and whether it is a call to action or simply brand awareness also play a large role in conversions.
Much of the information provided by features such as ad quality and post-click features is highly correlated with the web page hosting the ad. Quality pages promote competition among ads such that poor-quality ads tend to selfselect themselves off the page. Low quality pages often con-

148

tain questionable content and easily detected by the domain name.

4.3 Normalized Conversions

Unlike clicks, each advertiser defines its own conversions differently. This means that ranking must be aware of the different definitions. Consider the example of two advertisers. Advertiser a measures the number of purchases for a high-end shoe, where each conversion is worth $10 in profit. Advertiser b simply collects e-mail addresses for the shoe mailing list, each list is worth about $1. Both ads are similar in content and have similar CTR. If we are ranking ads by click propensity, we would be indifferent to the two ads. However, we can significantly improve the ROI for advertiser a if we can rank based on the value of a conversion.
Instead of eliciting private information from the advertisers about the sales value of conversions, we can normalize across all advertisers. The cost to an advertiser for each conversion is measured by the average cost per action (CPA), defined as follows:

CPA(a)

=

cost to adv. #of conversions

where the cost is the total amount paid for all clicks on a over some time period. We do not directly observe the true value V (N ), but we can approximate the relative difference as follows:

PV (N (a)) a V (N (a))



PCPA(a) a CPA(a)

(3)

where a  A. This measure tells us that if an advertiser a pays more for the conversions relative to other advertisers, then a must value these conversions with a similar relative value. When evaluating the overall impact of a ranking system we will use this normalized measure.

4.4 Feature Representation
The final feature representation contains:

· Matched Keywords: unigrams and frequently occurring n-grams. Features are TF-IDF normalized and must occur on both the page and ad text.

· Site Name: Hostname of the publisher's site on which the ad was shown.

· Site-Specific Matched Keywords: Keywords matched conditioned on each site. For example, if "Mazda" was matched on an ad on the site autos.yahoo.com, then the feature would be "Mazda:autos.yahoo.com".

5. RANKING FOR CONVERSIONS
The purpose of conversion modeling in contextual advertising is to improve the ranking of ads so as to improve the conversion rate without sacrificing CTR. In addition we would like to decrease the cost of a converting click.
Several constraints are imposed on the solutions for conversion modeling in our production environment. First, the ranking is generally for a pay-per-click advertising system, so ranking for clicks is still very important. The conversion model is intended as a replacement for a click model, which means that it must outperform a comparable click model with respect to clicks. Second, the conversion model must not use any second-order features, such as the score

of component models or data that is not accessible at run time. Finally, several other components require that the score output by the model is the probability of a click.

5.1 Baseline Click Model
We are restricting our discussion of learning algorithms to linear models. Regularized linear have been shown to be effective for text data in many studies. Linear models are also efficient to train and score.

5.2 Problem Description
We seek to induce a very weak partial order on the set of impressions. The rank among examples in each group are not constrained, but across the different groups conversions should precede clicks which should precede impressions. The click modeling problem is defined as follows. When a user visits a page p, a sequence of text ads a = a1, . . . , ak will be displayed on the page each at a position 1  i  k. We denote an impression as the tuple (p, a). For modeling, the impression is decomposed into several examples, one for each displayed ad such that x = (p, ai) for an ad displayed on the page in position i. We assume that there is a class label defined over the examples, l which is 0 for non-clicks, 1 for clicks, and 2 for conversions. We define the following sets:

I = {(x, l) | l = 0} C = {(x, l) | l = 1} N = {(x, l) | l = 2}

where examples are pairs (x, l). Click modeling is thus posed as a classification problem such that the output of the classifier g(x) approximates the posterior probability p(xl = 1 | x). Ads are ranked according to this score given a page impression. Conversions are actions that occur after the click, so the set of conversions N  C is entirely contained in the set C. In general, conversion modeling aims to find a classifier with decision function g : P × A  R such that g(x)  g(x ) for x  N and x / N .

5.2.1 Conversion v. Rest Model

A na¨ive approach to conversion modeling is to predict

P (l = 2 | x) directly. For this model, we define the positive

and negative class labels as follows:



l=

0 x / N 1 xN

where N denotes the set of converting clicks. This class label definition describes exactly what we want in the model, namely that conversions be ranked higher than all other impressions and clicks. The model treats all non-conversions equally, so clicks are treated as negatives.
There are several potential problems with a model like this, which is why we refer to it as the na¨ive model. First, it treats non-converting clicks as negative examples. Just because a click is not a conversion does not mean it is undesirable. Indeed, advertisers pay per click rather than conversions. A model that performs well at the conversion prediction task may be arbitrarily bad for clicks.

5.2.2 Click Re-Weighting Model
A conversion model must improve the conversion rate but not sacrifice CTR. A standard click model treats all clicks

149

equally. In reality clicks are not equal; some clicks lead

to conversions. These clicks should have a higher score and

thus rank. Because advertisers have different values for their

conversions, we weight conversions by their cost. We assume

that the most expensive conversions (with respect to CPA)

are the most important. We define a weight for each example

as follows:

8

<

1 2

if x  I

w(x)

=

:

1 w(A(x))

if x  C\N if x  N

where A(x) is the ad id of the pair and w(A(x)) is the rela-

tive conversion value defined in Equation 3. The weights of

the conversions are then normalized to satisfy the following

constraint:

X w(x) = |C|

xN

where |C| is the total number of clicks. The total weight of all examples can be adjusted to produce well-calibrated probabilities.
The main advantage of this model is that it still predicts clicks. The disadvantage is that predicting clicks, even with re-weighting, does not guarantee performance on conversions. For example there is no constraint that a conversion should be ranked higher than a non-converting click.

5.2.3 Ordinal Regression
The ideal ranking of page-ad pairs is: conversions, clicks, and impressions. Ordinal regression is well-suited to this problem because the class labels have a preference and the class labels are nested, such that N  C  I. Recent work gives a reduction from ordinal regression to binary classification. In the reduction, we learn multiple parallel hyperplanes separating the different classes [6]. With a single linear decision function g(x) we have the desired ranking g(n)  g(c)  g(i) for all conversions n, clicks c, and impressions i.
In the ordinal regression formulation, the class label l takes values such that l  {0, 1, 2} where impressions have label 0, clicks have label 1, and conversions have label 2. The ordinal regression problem can be reduced to a binary classification problem by extending the feature space such that X = P × A × {b01, b12}, such that dataset now contains the following examples:

((x, b01, b12) , l) ((x, 1, 0) , 0) ((x, 1, 0) , 1) ((x, 1, 0) , 1) ((x, 0, 1) , 0) ((x, 0, 1) , 0) ((x, 0, 1) , 1)

xI xC xN xI xC xN

where the feature vectors are simply copied and have the new features appended. As shown in Figure 2, the bias parameters b01 and b12 cause the model to learn an offset between the different classes. The key assumption in the parallel hyperplane approach is that the same features that discriminate conversions also discriminate clicks. Therefore we can view the training procedure as learning two separate tasks: clicks and conversions versus impressions, conversions versus clicks and impressions. In learning these separate tasks

we constrain the weights to be the same for both models­

information from both tasks shapes the weights. The offsets

translate the different sets into the same range.

One difficulty with the original ordinal regression formu-

lation is that it assumes that classes have an equal number

of examples. In our problem, the imbalance between the

different sets is severe. Without some tuning the model

will converge to the degenerate case where the conversion

hyperplane is the same as the click hyperplane. A popu-

lar solution for class imbalance in binary classification is to

simply increase the relative weight on the rare class. This

strategy can be extended to ordinal regression by weighting

the examples as follows:

8

><

1 2

P

(C

|

I)

xI

w(x) = >:1 1
2P (N |C)

xC xN

(4)

where the probabilities are over all ads. With this weighting scheme examples in each group have roughly the same importance. The factor of 2 is necessary because conversions appear as positive examples twice and impressions appear as negative examples twice. Because clicks appear as both positive and negative examples, there is no need to adjust their weights. As we will see later in Figure 3 this theoretically justified value also happens to be a good break-even point empirically.
To score a new example, the ordinal regression model operates as a binary classifier where only one bias is used. The resulting model assigns higher scores to conversions without the bias.
There are several advantages to the ordinal regression model. It guarantees (in training) the desired ranking. Second, it can be easily deployed as a drop-in replacement for an existing linear model that optimizes for clicks. The scores are reasonably well-calibrated to clicks. We expect any score difference to result in a larger score for converting clicks than non-converting clicks and impressions. Unfortunately this may mean that clicks become less well-separated from each other and the cost will increase. Finally, unlike pairwise classifiers, the training is still linear in the size of the original training data.

6. EXPERIMENTAL RESULTS
Conversion models must be evaluated as a replacement for the click models. As such, any conversion model must have a comparable performance on clicks but simultaneously improve performance relative to conversions.
6.1 Offline Evaluation
A conversion model is evaluated as a classifier for multiple learning tasks. We measure the area under the ROC curve, AUC, on the test set as a single measure of performance for the conversion models in the offline analysis. One conversion model is considered superior to another if it improves on all metrics.
6.1.1 Training Data
For the offline analysis, we collected impression logs from a major ad network over 7 weeks. We used the first 6 weeks for training and the following 1 week for evaluation. In the training data, there were approximately 200,000 clicks,

150

C v. I N v. I N v. C

Conv Only -24.33% -9.27% 25.29%

Re-weight 0.04% 0.47% 2.46%

Ordinal -0.78% 2.09% 10.65%

Table 1: Comparing conversion and click models in offline results, where C is clicks, I is impressions, and N is conversions. Performance metric is percent gain in ROC area relative to the click model baseline.

100300000

Performance Tradeoff for Ordinal Regression

500 300

140 80

0.15

Relative Area Under ROC Curve of Conversion v. Click

0.10

Figure 2: Ordinal regression reduction to binary classification. Two parallel hyperplanes are learned to jointly classify examples in each group.
10,000 conversions, and 2B impressions. Impressions were sub-sampled uniformly at 2%.
6.1.2 Results
Table 1 compares the conversion models on offline data. All models improve over the baseline click model for predicting conversions. This is not surprising because the baseline is unaware of conversions. The conversion v. rest model has the best performance at predicting conversions but is inferior to the click model for predicting clicks. This is because non-converting clicks are treated as negative examples. The model separates conversions from clicks but because it is unaware of clicks, it loses significant performance in separating clicks from impressions. The re-weighting model improves on conversions relative to clicks but is nearly identical with respect to clicks. Finally, the ordinal regression improves further on the re-weighting model in the conversion v. click task, with only slight decrease in click performance.
The results provide some insight on the relationship between clicks and conversions. In the conversion v. rest model, although we can distinguish conversions from clicks, we could not distinguish clicks or conversions from impressions. Because conversions logically occur after clicks, a good strategy for finding conversions is simply to find the clicks. Clearly the information in the clicks helps distinguish conversions. The other two models utilize this information. The re-weighting model still predicts clicks, so it only implicitly uses the conversion information. The ordinal regression relies heavily on the clicks to distinguish conversions because it learns both tasks jointly.
6.1.3 Tradeoff Analysis
Table 1 also demonstrates a key finding in the conversion ranking problem­a tradeoff between the click and conversion performance. In the ordinal regression model, the importance of the classes can be adjusted with an appropriate

20 10

0.05

2 1

0.0

0.10

0.15

0.20

Relative Area Under ROC Curve of Click v. Impressions

Figure 3: Comparing the performance of ordinal regression for predicting clicks and impressions. Each label on the curve indicates the relative importance of the conversions v. clicks. The vertical line indicates the break-even point in performance.

weight factor. We trained and evaluated several models with different values for the weight of conversions. As shown in Figure 3, as the weight given to the conversions increases so does the performance. However, this improvement of prediction for conversions comes at the expense of predicting clicks. Although any tradeoff is Pareto-optimal, we select the break-event point at which the model meets the performance of the baseline click model with respect to predicting clicks. From the previous discussion of the effect of weights, we see that to meet the performance of the click model and click should have the same importance in the ordinal model as in the click model during training. This corresponds to the case when the weights are balanced as in Equation 4. The best compromise in performance comes at this recommended value of weights at the class prior. We chose this weight for the subsequent analysis.

6.2 Online Evaluation

The online evaluation of a model tests the model on a sample of real traffic. The following metrics are computed for conversion models:

CTR

=

clicks impressions

CPC

=

cost to adv. clicks

CVR

=

conversions clicks

CPA

=

cost to adv. conversions

where the impressions, clicks, and conversions come from those actually shown on pages to users. The cost to the

151

CTR CPC CVR CPA Ordinal -2.46% 7.00% 7.98% -0.91%
Table 2: Relative performance of ordinal regression model versus the click model baseline in an online test.
advertiser is the total amount charged to the advertiser for the clicks.
6.2.1 Online Testing
In the online tests, our models were used to rank ads and serve impressions for a uniform sample of 2% traffic for each model. Because conversions are very rare, the model ran online for 4 weeks to collect enough conversions for significance testing.
6.2.2 Results
The ordinal regression model was selected for bucket test against the baseline click model. Table 2 shows the change in performance relative to the click model. The online results are consistent with the offline results. CTR decreases slightly but there is a significant increase in CVR (p-value less than 1%). The CVR metric is computed with the normalized conversion method described in Section 4.3. The cost per click (CPC) has increased as well. There are two main causes for this. First, the model ranks conversions higher than clicks, which means that non-converting clicks are ranked lower. The advertiser has to either improve the conversion rate or increase the bid to compete. Consequently, the advertiser has to pay a premium associated for ads that do not lend themselves to conversions, but is discounted for converting clicks. Secondly, it was noted during the analysis that the score distribution (not shown) of the estimated P (C) measure is slightly skewed upwards. The scores from the ordinal regression model are over-estimating the click probability because of the weighting of the examples, but can be easily adjusted with a corresponding corrective factor on the bid. Although the CPC has increased, the cost per conversion (CPA) has decreased slightly. This is desirable and to be expected because the improved performance makes converting click cheaper. However, because there are many more non-converting clicks, the impact to the CPA is overshadowed by the CPC changes. Again this is a simple correction.
As we have seen from the overall metrics, there is a confounding relationship between predictive performance and overall ranking. Because the ranking is multiplied by the bid and there are numerous business constraints regarding the ranking of ads, the overall metrics do not convey the entire picture. The plots in Figure 4 show the precision recall curves when ranking by the model scores P^(C) as logged in the impression events during the online test. The performance is not as good as we would have expected from the offline analysis discussed earlier. This discrepancy is because the final ranking is actually the ECPM: P^(C)V (C). We see from the plots in Figure 5 that when ranking by ECPM, the performance improves. Despite the improvement, it is misleading to conclude that the bid is responsible solely for the improvement. The ECPM ranking biases the results toward those examples with a high bid, which is the result of position bias. However, even with this bias, the model improves over the baseline. The bid improves the performance of the

conversion model more relative to the click model. The primary reason for the further improvement is that the ECPM measures the inherent worth of an impression. In the case of a conversion model, the score can take into account the fact that conversions are worth more than click even if only the relative value is being modeled.
7. CONCLUSION
In performance advertising, advertisers are ultimately interested in finding users who would like to buy their products. An ad ranking method that optimizes for the conversion funnel must simultaneously optimize both for clicks as well as for the conversions for all advertisers. We show that although the conversion goals for advertisers are very different, they can be modeled with an ordinal regression ranking function. Our ordinal regression provides a single linear model that ensures conversions are ranked higher than clicks and clicks are ranked higher than conversions. The overall performance of the global ranking function can be controlled to balance the performance of conversions versus clicks. The models were shown to be effective both in offline log data and in online tests. The model not only improves on the ranking of conversions but also makes conversions cheaper for the advertisers. This brings efficiency to the advertising marketplace as advertisers can tailor their ads towards conversions.
8. REFERENCES
[1] Hila Becker, Andrei Z. Broder, Evgeniy Gabrilovich, Vanja Josifovski, and Bo Pang. What happens after an ad click?: quantifying the impact of landing pages in web advertising. In CIKM, pages 57­66, 2009.
[2] Andrei Broder, Marcus Fontoura, Vanja Josifovski, and Lance Riedel. A semantic approach to contextual advertising. In SIGIR 2007, pages 559­566. ACM, 2007.
[3] Deepayan Chakrabarti, Deepak Agarwal, and Vanja Josifovski. Contextual advertising by combining relevance with click feedback. In WWW 2008. ACM, 2008.
[4] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. American Economic Review, 97, 2005.
[5] Anisio Lacerda, Marco Cristo, Marcos Goncalves, Fan Weiguo, Nivio Ziviani, and Berthier Ribeiro-Neto. Learning to advertise. In SIGIR 2006, pages 549­556. ACM, 2006.
[6] Ling Li and Hsuan-Tien Lin. Ordinal regression by extended binary classification. In Advances in Neural Information Processing Systems 19, pages 865­872, 2007.
[7] Vanessa Murdock, Massimiliano Ciaramita, and Vassilis Plachouras. A noisy-channel approach to contextual advertising. In ADKDD 2007. ACM, 2007.
[8] Adwait Ratnaparkhi. A hidden class page-ad probability model for contextual advertising. In Workshop on Targeting and Ranking for Online Advertising, at WWW 2008, Beijing, China, April 2008.
[9] Berthier Ribeiro-Neto, Marco Cristo, Paulo Golgher, and Edleno Silva de Moura. Impedance coupling in content-targeted advertising. In SIGIR 2005. ACM, 2005.

152

2.0 2.5 3.0 3.5

100

Conversion v. Impression Precision Recall Curve

Baseline Click Ordinal

Conversion v. Click Precision Recall Curve

Baseline Click Ordinal

50

20

Lift in Precision

10

Lift in Precision

1.5

5

1.0

2

1

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Recall

Recall

Figure 4: Precision and recall curves from online traffic comparing performance of predicting (a) conversions
versus impressions and (b) conversions versus clicks. The impressions are ordered in decreasing order of model score P^(C).

Conversion v. Impression Precision Recall Curve

Baseline Click Ordinal

Conversion v. Click Precision Recall Curve

Baseline Click Ordinal

2.2

8

2.0

6

Lift in Precision

1.8

Lift in Precision

1.6

4

1.4

1.2

2

1.0

0.2

0.4

0.6

0.8

1.0

Recall

0.2

0.4

0.6

0.8

1.0

Recall

Figure 5: Precision and recall curves from online traffic comparing performance of predicting (a) conversions
versus impressions and (b) conversions versus clicks. The impressions are ordered in decreasing order of ECPM estimated by the model­P^(C)V (C).

153

How Good is a Span of Terms? Exploiting Proximity to Improve Web Retrieval

Krysta M. Svore
Microsoft Research 1 Microsoft Way Redmond, WA
ksvore@microsoft.com

Pallika H. Kanani
UMass., Amherst 140 Governors Drive
Amherst, MA
pallika@cs.umass.edu

Nazan Khan
Microsoft 1 Microsoft Way Redmond, WA
nazanka@microsoft.com

ABSTRACT
Ranking search results is a fundamental problem in information retrieval. In this paper we explore whether the use of proximity and phrase information can improve web retrieval accuracy. We build on existing research by incorporating novel ranking features based on flexible proximity terms with recent state-of-the-art machine learning ranking models. We introduce a method of determining the goodness of a set of proximity terms that takes advantage of the structured nature of web documents, document metadata, and phrasal information from search engine user query logs. We perform experiments on a large real-world Web data collection and show that using the goodness score of flexible proximity terms can improve ranking accuracy over state-ofthe-art ranking methods by as much as 13%. We also show that we can improve accuracy on the hardest queries by as much as 9% relative to state-of-the-art approaches.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--retrieval models, selection process; D.0 [Software]: General
General Terms
Algorithms, Experimentation
Keywords
Web Search, Retrieval Models, Proximity, Learning to Rank, BM25
1. INTRODUCTION
Hundreds of millions of users issue queries to search engines daily. In order to improve the ranking of web documents, effective ranking features are necessary. One class of ranking features considered in both ad-hoc and web retrieval is proximity and phrasal information. In ad-hoc retrieval,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

relevance contributions from proximity and phrasal information have varied. For example, if a ranking function is already strong in retrieval effectiveness, then the addition of proximity and phrasal features can be neutral or even negative [10]. On the other hand, when considering only top retrieved documents, term proximity information can lead to improved retrieval effectiveness [12].
Despite relevance contributions from phrase and proximity terms varying in ad-hoc retrieval, the use of phrase and proximity terms in web retrieval may be more effective, in part due to the difference in query statistics; in ad-hoc retrieval queries are on average 4.9 terms compared to an average of 1.5­2.6 terms per query in web retrieval [9]. Intuitively, retrieval for longer queries should benefit more from proximity information, yet it has been shown previously that when using proximity features, query length is inversely correlated with retrieval effectiveness [9]. This may be because long queries are often a sequence of terms, whereas short queries form a linguistic phrase, thus proximity information is more informative. The structured nature of web documents may also contribute to the significant improvements that can be obtained through the use of phrases and term proximity information in web retrieval [9]. Recent work by Song et al. indicates that using flexible proximity terms within an information retrieval model such as BM25 results in improved retrieval effectiveness [16].
Our work is motivated by the previous experimental results in ad-hoc and web retrieval. In this paper, we exclusively focus on web retrieval. We address how much benefit phrases and proximity information have for web retrieval on the top retrieved documents. Rather than proposing changes on the query level through query alteration or query expansion, we propose changes to the features used in the ranking function. Our features are based on flexible proximity term information, called spans, as introduced in [16]. We introduce the notion of a "good" span by developing novel features of the document spans; we determine the "goodness" of a span by evaluating the presence of third-party phrasal information within spans, the formatting and structure of the spans, the density of query terms in the span, and so on. We then evaluate our span-based features within a larger ranking model on real-world Web data.
Through evaluation of our proximity features, we seek to answer if phrases and proximity terms have different impact on retrieval effectiveness for short 2­3 term queries versus long queries containing more than 4 terms. We also look at the retrieval effectiveness of phrases and proximity terms on popular, head queries versus rare, tail queries. We attempt

154

to not only introduce a novel set of features for determining the importance of proximal terms that significantly improve web retrieval, but also attempt to unify previous work into a more cohesive story on proximity information in web retrieval. Throughout our work, we take a machine learning approach to ranking, allowing the model to determine where and how to use the phrase and proximity information while directly optimizing for the retrieval measure under consideration.
Our specific contributions in this paper include:
· Introduction and analysis of a novel approach for determining the "goodness" of a span of terms in a document (Section 5.2)
· Novel span features that exploit document metadata, structure, formatting, and third-party phrasal information from sources such as query logs and Wikipedia (Section 5.3)
· A large scale evaluation on a real-world Web dataset that shows significant improvements in retrieval effectiveness from using our span features (Section 7)
· An in-depth evaluation of the contributions of our span features on query segments based on length and popularity (Section 7)
2. RELATED WORK
The use of proximity information in retrieval has recently become a more interesting research avenue due to the large number of search engine users. Several retrieval models have been developed to capture proximity. Peng et al. proposed a statistical language model applied to both the query and relevant documents [11]. The model predicts query stemming operations; query term matches in the document are considered valid only if the match appears in the same context and order as in the stemmed query. A simple retrival model that focuses on efficiency rather than accuracy is proposed in [15]. Beigbeder and Mercier [1] use a model based on fuzzy proximity degree of term occurrences, but focus mainly on boolean queries. A formal, Markov Random Field model for term proximity is introduced in [8]. More recently, Lv and Zhai [7] proposed a positional language model that accounts for proximity and passage retrieval scores.
One of the most widely used models for information retrieval, BM25 [13, 6], does not include proximity information. One method of adding proximity information is to determine the frequencies of n-grams of the query in the document, such as bigrams or trigrams, and to incorporate such frequencies into BM25 [12, 3]. Rasolofo and Savoy took such an approach and reported mixed effectiveness for web retrieval, showing almost negligible results on MAP [12]. Tao et al. [18] used a span-based relevance score, where a span must contain all query terms, in conjunction with existing retrieval models. They also reported mixed results when using proximity in web retrieval. Recently, flexible, nonoverlapping spans were introduced in [16]; Song et al. account for proximity by segmenting a document into flexible spans of terms and performing a weighted count of the matched query terms in the segments, where the weighting is based on the number of query terms in the span and the length of the span, and finally incorporating the counts into the BM25 function.

We build upon the work of Song et al. by proposing novel features of spans based on the formatting of the document and third-party phrasal information. In addition, we extend the notion of a span to more general ranking features that can be used within a larger machine learning model and not restricted for use within BM25. We also unify the previous approaches to proximity information in BM25 [16, 12, 18] with a recent machine learning method for improving upon information retrieval functions [17].

3. PROXIMITY IN BM25
In this section, we review several retrieval methods that incorporate proximity into BM25; these techniques will serve as baselines for our experiments in Section 7. We refer the reader to the original papers for details. We choose to compare against BM25-based baselines since BM25 is an important feature in a trained ranking model [14, 21]. We ultimately seek to improve ranking features for web retrieval, and significance over BM25 typically indicates the new feature will be effective for web retrieval.
BM25 [13, 6] is a probabilistic model of information retrieval. The BM25 relevance score S for document d and query q is computed as follows:

S

=

X wt
tq

·

(k + 1) · ft K + ft

,

(1)

» K = k · (1 - b) + b ·



­ ,

(2)

av

where t is a term in query q, ft is the frequency of t in document d,  is the length of document d, av is the average document length in the collection, and k and b are tuning parameters. wt is the Robertson-Sparck-Jones inverse document frequency of term t:

wt

=

log

N

- dft dft +

+ 0.5 0.5

,

(3)

where N is the number of documents in the collection and dft is the document frequency of term t.
One technique of introducing proximity into BM25 incorporates matches of adjacent and non-adjacent query bigram frequencies1 in the document into the BM25 formula [12]. We denote this technique as BM25-P1. The relevance score for BM25-P1 is calculated as:

BM25-P1 = S +

(4)

X
ti,tj q|i<j

" min(wi, wj)

·

(k

+

1)

·

P
occ(ti,tj )

|pj

K

+

P
occ(ti,tj )

|pj

-

- pi|-2 pi|-2

#

,

where S is the BM25 relevance score (Eq 1), K is defined in Eq 2, min(wi, wj) is the minimum of the Robertson-SparckJones inverse document frequencies of term i and term j, occ(ti, tj) are the occurrences of a query term pair ti, tj in the document, and pi, pj are the respective positions of query terms ti, tj in the document.

1An adjacent query n-gram is an n-gram comprised of only query terms, where the terms appear adjacent in the document. A non-adjacent query n-gram is any n-gram formed from only query terms where the terms appear within some distance in the document.

155

We consider a variation that employs matches of adjacent query bigrams in the document, which we call BM25-P2:

BM25-P2

=

S

+

» X
wi,i+1
ti ,ti+1 q

·

(k + 1) · fi,i+1 K + fi,i+1

­

,

(5)

where wi,i+1 and fi,i+1 are the document and term frequencies of query bigram ti, ti+1, respectively.

4. PROXIMITY THROUGH SPANS

Previous approaches to proximity mainly consider n-grams of query terms and their matches in the document. Song et al. [16] propose a different, more flexible approach to proximity that segments a document into spans based on query term matches and their positions in the document. Spans are constructed as follows. Each term position in the document, beginning at the first term position, is checked for a match against a query term. When the first query term is found, a new span begins. Terms, including non-query terms, are added to the span until it is closed or split into two spans. A span is closed or split when one of three conditions occurs: (1) the distance between the current query term match position and the next query term match position in the document is greater than a predetermined threshold, (2) the current and next query term matches in the document contain the same query term, (3) if the current and the next query term matches are different, and the previous query term match and the next query term match are identical, then the span is split into two spans based on the larger dis-

tance. Note that spans cannot overlap and need not contain every query term. Previous span methods have required that every query term be contained in the span. Spans may, and likely will, contain non-query terms, however the span be-

ginning and end positions must always be query term match positions. We use this method of span construction throughout our paper and determine the "goodness" of each span in

a document through span-based features (Section 5.3).

Song et al. incorporate spans into BM25 by replacing ft, the frequency of term t, in Eq 1 with a relevance contri-

bution, rc, based on spans in which term t occurs, rc =

P
i|tsi

ni d(si)-

[16],

where

 d(si) =

pi,e - pi,b + 1 dmax

pi,b = pi,e otherwise

(6)

is the length of span si, pi,b, pi,e are the span's beginning and end positions in the document, ni is the number of query terms that occur in span si, dmax is the distance threshold, and  and  are tuning parameters. We denote this technique as BM25-P3.

5. THE GOODNESS OF A SPAN
In Song et al. [16], the relevance contribution of a span is calculated based on only the number of query terms in the span and the total number of terms in the span. We believe that by additionally exploiting the structured nature of web documents, the availability of third-party data, linguistic features, and by taking advantage of machine learning techniques, we can improve the calculation of the relevance of a span. We propose to determine the "goodness" of a span through the development of span-based features. In this section, we first introduce how span-based features can be used within a ranking model framework. Following [17],

we believe that a machine learning model will have improved retrieval effectiveness over a BM25-based model. We therefore combine the span-based features into a larger ranking model. We then develop the "goodness" of a span and develop a technique for including the span "goodness" within a ranking model. Finally, we describe a novel set of span-based features based on formatting and third-party data that together represent the relevance of a span.

5.1 Ranking with Span-based Features
Web search engines rank results based on a large number of features including query dependent features, such as matches against query n-grams in the content (i.e., anchor text, body text, title, URL) or the BM25 score of a document, and query independent features such as the PageRank of the document. Most modern search engines use automatic methods for developing the ranking model based on learning to rank techniques. In this work, we perform two types of evaluation of our span-based features: (1) an evaluation against BM25 and state-of-the-art proximity methods that employ BM25, and (2) an evaluation against a modern ranking model based on a large number of features. We perform (1) since any scoring function can be used as a feature in (2) and because BM25 is an extremely powerful feature in any ranking model. We particularly want to understand improvements that can be made to individual features. We perform (2) since any feature needs to be effective in a larger ranking model. In this work, we evaluate the effectiveness of features on the body content of the document. Thus all features discussed are extracted from only the body text, but could be applied to other content, such as anchor text and so on.
In both cases, we take a machine learning approach and train models using LambdaRank [2], a state-of-the-art neuralnet based ranking algorithm that has been shown to be empirically optimal for several IR measures [4, 20]. When comparing against BM25 in evaluation (1), we specifically compare against the machine learning approach to a BM25style function in [17], called LambdaBM25 (BM25), that achieves significant improvements over BM25. LambdaBM25 learns a ranking model using LambdaRank [2] on the input attributes of BM25, namely term frequency, document frequency, and body content length. When evaluating our features, we add them to the LambdaBM25 feature input set and train LambdaRank over the entire set of features. Details of the models will be given in Section 6.

5.2 Deriving Span Goodness
In this section, we propose a machine learning method to determine the "goodness" of a span. We then describe how the span "goodness" scores can be combined and input as a feature into LambdaRank [2] or any ranking model.
We derive a goodness score for a document by learning on labeled training data (labeled query-URL pairs), and span features. Each span is described by a vector of feature values; the features are described in detail in Section 5.3. We assign a "goodness" score gs to each span s in the document based on a weighted linear combination of each span's feature vector. We calculate the span goodness score gs as

X

gs = f vf,s,

(7)

f

where f is a feature of span s, f is the weight of feature

156

f , and vf,s is the value of feature f for span s. To calculate the score, we must assign a weight to each f . If we had a labeled training set indicating the goodness of a span for a query-document pair, then we could apply machine learning to learn the weights. Unfortunately, acquiring such labels is challenging and costly. We can, however, choose to model the overall span goodness of a document. We model the goodness score for document d, gd, based on the spans contained in the document:

gd = X X f vf,s

(8)

sf

By reversing the summations,

XX

gd = f ( vf,s),

(9)

f

s

we

can

input

for

each

feature

f,

P
s

vf,s,

the

sum

of

the

document's spans' feature vectors, as a document feature

in LambdaRank and learn the feature weights f over the

labeled training data. Our method provides the flexibility

to easily add additional span-based features to any ranking

model.

5.3 Span-based Features
Our feature vector for a span consists of several types of features. Table 1 lists the features used in our "goodness" approach. The first set of span features are basic query match

Query Match Features Span contains  2 query terms (binary) Span contains  4 query terms (binary) Span length (number of terms in span) Count of query terms in span Density of span
Formatting Features (F) Count of indefinite articles in spans Count of definite articles in spans Count of stopwords in span Span contains only stopwords (binary) Span contains a sentence boundary (binary) Span contains a paragraph boundary (binary) Span contains html markup (bold, italic, tags) (binary)
Third-party Phrase Features (P) Span contains important phrase (binary) Count of important phrases in span Density of important phrases in span
Table 1: Span goodness features.

features that determine how many query terms are matched in the span and how many total terms are in the span. The density of the span is calculated as the number of query terms in the span divided by the number of terms in the span. The second set of features are formatting and linguistic features. These features include information about the definite and indefinite articles in the span, the html markup contained in the span, and so on. The third set of features determines if the span contains an "important" phrasing of the query. The list of important phrases was extracted from Wikipedia titles and by mining a search engine's query logs for common n-gram occurrences. The features express if query terms found in the span match an important phrase.

I. BM25 Features Term frequency of query unigrams Document frequency of query unigrams Length of body content (number of terms)
II. BM25-2 Features Term frequency of query bigrams Document frequency of query bigrams
III. Proximity Match Features Relevance contribution per query term (Sec 4) Number of spans in the document Max, avg span length Max, avg count of query matches in spans Max, avg span density Length of span with highest term frequency Term frequency of span with longest length Term frequency of span with largest density
Table 2: Model feature sets.
We also consider adding additional features to our models that express the attributes of specific span features. The features are listed in (III) of Table 2. In particular, we add features such as the total number of spans in the document, the max and average span length, and the max and average span density. In addition, we add features such as the length of the span with the highest term frequency, the term frequency of the longest span, and the term frequency of the most dense span since they are representative features of the "best" spans in the document.
In our evaluations, we perform feature ablation studies to determine which features are most impactful and effective for improving web retrieval.
6. EXPERIMENTAL SETUP
6.1 Data
We evaluate our methods on a real-world Web data collection containing queries sampled from query logs of a commercial search engine and corresponding URLs. All queries are English queries and contain up to 10 query terms. Each query is associated with on average 150-200 documents (URLs), each with a vector of feature attributes extracted for the query-URL pair and a human-generated relevance label from 0 to 4, with 4 meaning document d is the most relevant to query q and 0 meaning d is not relevant to q.
The training set consists of 27,959 queries. During model training, we use 20% of the training data for validation. The test set contains 11857 queries. We examine two splits of our test set to understand the performance of our methods on different query types. One split separates short queries (< 4 terms in query) from long queries ( 4 terms in query). The other split separates head (more popular) queries from tail (less popular) queries. We use the amount of click and anchor information as an indicator of query popularity2. Table 3 lists the respective split sizes of our test dataset.
6.2 Evaluation Measure
We evaluate using NDCG, Normalized Discounted Cumulative Gain (NDCG) [5], a widely used measure for search
2We could also split based on the frequency (number of times issued by users) of the query.

157

Dataset
Full Head Tail Short Long

Query Split Description
Full Dataset Queries with anchor and clicks Queries without anchor and clicks
Queries < 4 terms Queries  4 terms

# Queries
11857 9166 2691 8766 3091

Table 3: Description of test dataset.

metrics. NDCG for a given query q is defined as follows:

NDCG@Lq

=

100 Z

L
X

2l(r) - 1 log(1 + r)

r=1

(10)

where l(r)  {0, . . . , 4} is the relevance label of the doc-

ument at rank position r and L is the truncation level to

which NDCG is computed. Z is chosen such that the perfect

ranking would result in NDCG@Lq = 100. Mean NDCG@L

is the normalized sum over all queries:

1 N

PN
q=1

NDCG@Lq

.

NDCG is particularly well-suited for Web search applica-

tions since it accounts for multilevel relevance labels and

the truncation level can be set to model user behavior. In

our work, relevance is measured on a 5-level scale. We evalu-

ate our results using mean NDCG@1, 3, 10. For brevity, we

write NDCG@1, 3, 10. We also perform a signficance test,

i.e., a t-test with a significance level of 0.05 (95% level). To

improve readability of our tables, we only report significance

numbers when the gap between models is small, or when we

are performing a feature ablation study. Significance is also

stated in the text. Note that a gain of 0.3 - 0.5 NDCG is

considered substantial.

6.3 Ranking Model Comparison
For each scoring function, we tuned the parameters using grid search on our validation set as described in [19]. For each Span model variant, the model was trained using LambdaRank on the training set. The learning rate, found to be 10-5 in each case, and epoch were chosen based on our validation set. For evaluation (1), the models contain only the listed scoring function as a feature, unless additional features are explicitly listed in the descriptions below. For evaluation (2), the models contain additional features such as traditional query-dependent and query-independent ranking features, such as BM25, the PageRank of the document, and so on, as well as the features and scoring functions (input as features) listed below.

· BM25: The BM25 scoring function given in Eq 1 [13, 6]. BM25 has been used in the best performing TREC Web track systems [14, 21].

· BM25: The method of training LambdaRank over the input features of BM25 [17]. The features used in our model are given in (I) of Table 2. We trained LambdaBM25 on our training set and found the learning rate of 10-5 and epoch according to accuracy on our validation set.

· BM25-P1: The scoring function given in Eq 4 [12].

· BM25-P2: The scoring function given in Eq 5. It is a slight modification to the function of BM25-P1.

· BM25-P3: The scoring function described in Section 4 that incorporates spans into BM25 [16].

Model
BM25 BM25-P1 BM25-P2 BM25-P3 BM25 BM25-2 BM25-2RC Span Span-P Span-F

N@1
24.60 26.06 25.27 25.97 26.22 26.34 26.96 29.56 28.90 26.03

N@3
27.74 29.54 28.72 29.36 29.41 29.54 30.51 32.23 31.81 29.45

N@10
34.34 36.00 35.35 35.84 35.92 36.42 37.17 38.47 38.20 36.81

Table 4: NDCG results on the full test set. Bold indicates statistical significance over all other listed models.

· BM25-2: BM25 with the additional features listed in (II) of Table 2 to incorporate bigrams.
· BM25-2RC: BM25-2 with an additional feature, the relevance contribution score per query term based on spans (see Sec 4 and (III) of Table 2) [16]. This model is used to evaluate the effectiveness of using span information as a basic feature in a machine learned ranking model versus its incorporation into an information retrieval function.
· Span: A model containing all of our "goodness" features listed in Table 1. Note that all of the features input into the model are a sum of all of the span feature vectors. The model also contains all features listed in Table 2; we want to compare if adding span-based features improves retrieval accuracy over BM25-based proximity functions and LambdaBM25.
· Span-F: The Span model above, but without the features listed in the formatting section (F) of Table 1.
· Span-P: The Span model above, but without the features listed in the third-party phrase section (P) of Table 1.
7. EXPERIMENTAL RESULTS
In this section, we evaluate our proposed features against the baseline models and perform feature ablation studies. Improvements cannot be attributed to a low baseline; each baseline is a state-of-the-art technique and has shown sufficiently high retrieval effectiveness. We first evaluate our features against BM25 and baselines using proximity in BM25. We then evaluate our features within larger ranking models.
7.1 Evaluation of Features versus BM25
We begin by evaluating our span-based features against BM25 and proximity versions of BM25 to understand how effective our span-based features are as simple ranking models. The models are described in the previous section. Table 4 lists the results of the various models on the full test set.
We first observe that BM25 is statistically better than BM25 and BM25-P2 at all truncation levels. It is particularly notable that BM25-2 exhibits significantly superior accuracy at truncation levels 3 and 10 over all models listed above it in Table 4. The strong retrieval effectiveness of BM25-2 demonstrates the power of machine learning over

158

set retrieval functions and further supports the results found in [17].
We next evaluate the addition of the relevance contribution score (see (III) in Table 2) as a feature to BM25-2. We find that BM25-2RC outperforms BM25-2 with statistical significance at all truncation levels. The experimental result supports the claims in [16] that spans are an improved method of segmenting the document into flexible spans of terms and offer improvements when incorporating proximity into a ranking model over simple n-gram matches and term frequencies.
We now evaluate the effectiveness of our new span-based features. Our Span model outperforms all baselines, including BM25-2RC, with statistical significance at all truncation levels with a gain of almost 3 points NDCG@1. The result implies that by utilizing the "goodness" feature vectors, we can enhance the effectiveness of spans for web retrieval. It is also a very flexible method that allows for easy insertion of new span features.
How effective are the span-based phrase and formatting features for web retrieval? We find that when removing phrase features (Span-P), retrieval accuracy on the full test set drops significantly at all truncation levels. In particular, the drop at level 1 is over 0.5 points NDCG@1. Most remarkable is the effectiveness of the span-based formatting features. We observe that when removing formatting features (Span-F), retrieval accuracy on the full set drops significantly at all truncation levels by as much as 12% and by over 3.5 points NDCG@1.
Table 5 lists the results of our evaluations on various splits of the test set. Note that on the head queries, our Span model significantly outperforms all other models, indicating that both phrase and formatting span-based features contribute significantly to our model's superior accuracy. On short queries, which tend to highly correlate with head queries, we find very similar results, except that the removal of phrase span features has no effect on accuracy. Thus, we find that formatting features are effective for short queries, but phrase features contribute negligible gains.
On tail queries, we remark that Span is significantly better than BM25-2RC at truncation levels 3 and 10. In addition, the removal of phrase span-based feature causes no significant difference compared to the Span model. Formatting span-based features, however, cause a significant drop in accuracy, indicating that tail queries benefit from these features. Tail queries show a significant benefit from spanbased features within a machine learning framework. Results on long queries indicate that phrase features are not overly effective for long query retrieval, but that formatting features remain significantly effective. It is also noteworthy that on long and tail queries, the differences between BM25-2RC and Span are negligible, indicating that a few span features are important for long and tail queries, but more specialized span features may not be needed. This result corresponds with previous results indicating that proximity is more beneficial for retrieval of short queries [9].
7.2 Evaluation of Features in a Full Ranking Model
In this section, we evaluate the retrieval effectiveness of span-based features within a full ranking model on the body content of web documents. Our goal is to determine how much proximity information, in particular in the form of

Split Head Tail Short Long

Model
BM25 BM25-P1 BM25-P2 BM25-P3 BM25 BM25-2 BM25-2RC Span Span-P Span-F BM25 BM25-P1 BM25-P2 BM25-P3 BM25 BM25-2 BM25-2RC Span Span-P Span-F BM25 BM25-P1 BM25-P2 BM25-P3 BM25 BM25-2 BM25-2RC Span Span-P Span-F BM25 BM25-P1 BM25-P2 BM25-P3 BM25 BM25-2 BM25-2RC Span Span-P Span-F

N@1
25.59 26.89 25.95 26.58 27.37 26.94 29.73 30.27 29.65 26.46 21.23 23.21 22.93 23.91 22.31 24.31 26.04 26.23* 26.34 24.56+ 24.77 25.49 22.93 25.75 26.05 25.62 28.15 28.73* 28.16 24.74+ 24.13 27.68 25.08 26.60 26.72 28.38 30.99 31.15 31.00 29.67+

N@3
28.05 29.77 28.98 29.65 30.06 29.76 32.04 32.63 32.10 29.40 25.13 28.73 27.82 28.38 27.17 28.77 30.71 30.87* 30.80 29.62+ 28.08 29.08 27.82 29.24 29.29 29.02 31.16 31.82* 31.43 28.27+ 26.75 30.83 28.61 29.73 29.73 31.02 33.37 33.41 32.88 32.81+

N@10
35.01 35.99 35.48 36.13 36.3 36.45 38.18 38.61 38.27 36.77 32.05 36.04 34.91 34.85 34.62 36.31 37.86 37.99 37.96 36.94+ 34.86 35.76 34.91 35.87 35.93 36.07 37.76 38.23* 37.91 36.09+ 32.86 36.68 35.43 35.75 35.88 37.41 39.09 39.13 39.02 38.08+

Table 5: NDCG results on test set splits. Bold indicates statistical significance over all other models. * indicates statistical signficance of Span over BM252RC. + indicates statistical signficance of Span over Span-F. Other significance markers have been removed for readability of the table and are stated when necessary in the text.

159

Model
R+BM25 R+BM25-P3 R+BM25 R+BM25-2 R+BM25-2RC R+Span R+Span-P R+Span-F

N@1
36.86 37.09 37.51 37.24 37.94 38.18 38.43 37.57+

N@3
39.17 39.14 39.58 39.12 39.93 40.29* 40.49 39.69+

N@10
44.62 44.49 44.93 44.66 45.34 45.65* 45.75 45.01+

Table 6: NDCG results on the full test set using features within a full ranking model. * indicates statistical signficance of R+Span over R+BM252RC. + indicates statistical signficance of R+Span over R+Span-F.

span-based features, can improve web retrieval. We also seek to determine if previous results in web retrieval indicating that proximity is not that effective when paired with a larger ranking model remain true on a large real-world Web data collection. Additionally, we ask how effective formatting, linguistic, and phrase span-based features are in the presence of a larger ranking model.
For each model listed in Section 6, we combine traditional query-dependent and query-independent ranking features, such as BM25, the PageRank of the document, and so on, with the features listed for each model. The models are denoted with "R+" to indicate full ranking model. For a scoring function model, the scoring function is input as one of the features into the larger ranking model. We train LambdaRank on the various feature sets and determine the learning rate and epoch according to the highest accuracy on the validation set.
Table 6 lists the results of training a full ranking model on the various sets of features. We do not list R+BM25-P1 or R+BM25-P2 since R+BM25-P3 includes span information and performs similarly (see Table 4). R+BM25 performs significantly better than the two R+BM25 models at all truncation levels. Interestingly, the bigram features contained in R+BM25-2 do not cause an increase in accuracy over R+BM25. However, proximity information expressed through spans in R+BM25-2RC causes significant gains over R+BM25 and R+BM25-2 at all truncation levels. The results show the importance of representing span information within a machine learning framework and including the span information through individual features separate from BM25, as in [17].
An even better approach is to include additional spanbased features directly in the model, as shown by using R+Span. R+Span is a significantly better model than R+ BM25-2RC at truncation levels 3 and 10. By removing phrase features, the model improves insignificantly at all truncation levels (R+Span-P), which may indicate that the method of determining important phrases or the list of important phrases could be further improved. When we remove formatting features, the model's accuracy decreases significantly at all truncation levels, indicating that our formatting features are an important class of span features to include in a full ranking model. Note that R+Span-P is significantly better than all other models except Span.
Table 7 lists the results of various full ranking models on splits of the test data. Most interestingly, the gains of

Split Head Tail Short Long

Model
R+BM25 R+BM25-P3 R+BM25 R+BM25-2 R+BM25-2RC R+Span R+Span-P R+Span-F R+BM25 R+BM25-P3 R+BM25 R+BM25-2 R+BM25-2RC R+Span R+Span-P R+Span-F R+BM25 R+BM25-P3 R+BM25 R+BM25-2 R+BM25-2RC R+Span R+Span-P R+Span-F R+BM25 R+BM25-P3 R+BM25 R+BM25-2 R+BM25-2RC R+Span R+Span-P R+Span-F

N@1
39.11 39.20 39.68 39.17 40.29 40.29 40.55 39.66+ 29.22 29.91 30.15 30.67 29.91 30.98* 31.19 30.43 37.83 38.05 38.49 38.25 39.17 39.25 39.45 38.49+ 34.12 34.39 34.76 34.39 34.46 35.15* 35.55 34.96

N@3
40.73 40.62 41.19 40.63 41.70 41.97 42.09 41.20+ 33.86 34.09 34.10 33.96 33.88 34.55* 35.04 34.58 40.22 40.13 40.55 40.09 41.16 41.48* 41.53 40.75+ 36.20 36.32 36.84 36.36 36.44 36.89 37.54* 36.69

N@10
45.79 45.59 46.13 45.84 46.70 46.96 47.01 46.29+ 40.64 40.73 40.81 40.66 40.86 41.19* 41.44 41.04 45.78 45.62 46.09 45.84 46.67 46.93 47.02 46.27+ 41.32 41.3 41.61 41.31 41.72 42.01 42.13 41.76

Table 7: NDCG results on test set splits using features within a full ranking model. * indicates statistical signficance of R+Span over R+BM25-2RC. + indicates statistical signficance of R+Span over R+Span-F.

BM25-2RC over BM25-2 are significant on head and short queries, with over 1 point gain at truncation levels 3 and 10, but on tail and long queries there is no significant difference. The result may indicate that span features are more beneficial for short queries, which matches previous results showing proximity helps short queries more than long queries [9].
R+Span shows significant gains over BM25-2RC on tail queries at all truncation levels, and on long queries at position 1, but negligible differences on short and head queries. The lack of formatting span-based features has a significant impact on short and head queries, but little impact on long and tail queries. The phrase span-based features have a negligible effect on all query splits, although Span-P shows significant gains over BM25-2RC on all query splits and truncation levels except short at 1. Thus, our span-based features, without the important phrase features, significantly improve web retrieval accuracy.
8. CONCLUSIONS AND FUTURE WORK
We have proposed a new approach for combining term proximity into a machine learning framework. Specifically, we have introduced the goodness of a span and a correspond-

160

ing framework for incorporating it into a machine learning ranking model. We have also introduced novel span-based ranking features based on document formatting, linguistics, and important phrases from Wikipedia and a search engine query log. Our in-depth analysis indicates that proximity information is best extracted using spans, originally introduced in [16]. Moreover, we find that span-based features outperform an information retrieval function such as BM25 that includes proximity information. Our feature ablation studies indicate that formatting span-based features are significantly effective, while important phrase features may not be effective in a larger ranking model. They also indicate that improvements of features in small ranking models may not necessarily correlate with gains when used in a larger ranking framework. We have also shown that head and short queries benefit from different span-based features than tail and long queries. Proximity information appears more effective for short and head queries than for long and tail queries, but span-based proximity features lead to significant gains across all query sets compared to ranking models without span-based features.
Future work includes extending our approach to additional document fields, such as anchor text and title fields. We also plan to explore novel sources of phrasal information that may further improve the important phrase span-based features. Finally, our training set contains a significant number of head and short queries. We would like to train our models on a training set consisting of only long and tail queries to determine if the models can better take advantage of the span-based features.
9. ACKNOWLEDGEMENTS
P.K. is supported in part by the Center for Intelligent Information Retrieval and in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. P.K. conducted the work while interning at Microsoft Research.
10. REFERENCES
[1] M. Beigbeder and A. Mercier. An information retrieval model using the fuzzy proximity degree of term occurences. In SAC, page 1018, 2005.
[2] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to rank with nonsmooth cost functions. In NIPS, page 193, 2006.
[3] S. Buttcher, C. L. A. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In ACM SIGIR Conference on Research and Development in Information Retrieval, page 621, 2006.
[4] P. Donmez, K. M. Svore, and C. J. C. Burges. On the local optimality of LambdaRank. In ACM SIGIR Conference on Research and Development in Information Retrieval, 2009.
[5] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In ACM SIGIR Conference on Research and Development in Information Retrieval, pages 41­48, 2000.

[6] S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments - part 1. Information Processing & Management, 36:779, 2000.
[7] Y. Lv and C. Zhai. Positional language models for information retrieval. In ACM SIGIR Conference on Research and Development in Information Retrieval, 2009.
[8] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472­479, 2005.
[9] G. Mishne and M. de Rijke. Boosting web retrieval through query operations. In ECIR, pages 502­516, 2005.
[10] M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An analysis of statistical and syntactic phrases. In Proceedings of RIAO-97, 1997.
[11] F. Peng, N. Ahmed, X. Li, and Y. Lu. Context sensitive stemming for web search. In ACM SIGIR Conference on Research and Development in Information Retrieval, 2007.
[12] Y. Rasolofo and J. Savoy. Term proximity scoring for keyword-based retrieval systems. In ECIR, page 207, 2003.
[13] S. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In ACM SIGIR Conference on Research and Development in Information Retrieval, pages 345­354, 1994.
[14] S. E. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In CIKM, page 42, 2004.
[15] R. Schenkel, A. Broschart, S. won Hwang, M. Theobald, and G. Weikum. Efficient text proximity search. In SPIRE, page 287, 2007.
[16] R. Song, M. J. Taylor, J. R. Wen, H. W. Hon, and Y. Yu. Viewing term proximity from a different perspective. In ECIR, page 346, 2008.
[17] K. M. Svore and C. J. C. Burges. A machine learning approach for improved BM25 retrieval. In CIKM, 2009.
[18] T. Tao and C. Zhai. An exploration of proximity measures in information retrieval. In ACM SIGIR Conference on Research and Development in Information Retrieval, page 295, 2007.
[19] M. Taylor, H. Zaragoza, N. Craswell, S. Robertson, and C. Burges. Optimisation methods for ranking functions with multiple parameters. In ACM Conference on Information Knowledge Management (CIKM), 2006.
[20] Y. Yue and C. J. C. Burges. On using simultaneous perturbation stochastic approximation for IR measures, and the empirical optimality of LambdaRank. In NIPS Machine Learning for Web Search Workshop, 2007.
[21] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson. Microsoft cambridge at TREC 13: Web and hard tracks. In Proceedings of TREC 2004, 2004.

161

Optimal Meta Search Results Clustering

Claudio Carpineto
Fondazione Ugo Bordoni Rome, Italy
carpinet@fub.it

Giovanni Romano
Fondazione Ugo Bordoni Rome, Italy
romano@fub.it

ABSTRACT
By analogy with merging documents rankings, the outputs from multiple search results clustering algorithms can be combined into a single output. In this paper we study the feasibility of meta search results clustering, which has unique features compared to the general meta clustering problem. After showing that the combination of multiple search results clusterings is empirically justified, we cast meta clustering as an optimization problem of an objective function measuring the probabilistic concordance between the clustering combination and the single clusterings. We then show, using an easily computable upper bound on such a function, that a simple stochastic optimization algorithm delivers reasonable approximations of the optimal value very efficiently, and we also provide a method for labeling the generated clusters with the most agreed upon cluster labels. Optimal meta clustering with meta labeling is applied to three descriptioncentric, state-of-the-art search results clustering algorithms. The performance improvement is demonstrated through a range of evaluation techniques (i.e., internal, classificationoriented, and information retrieval-oriented), using suitable test collections of search results with document-level relevance judgments per subtopic.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Clustering
General Terms
Algorithms, Experimentation, Measurement, Performance
Keywords
Meta clustering, search results clustering, optimization
1. INTRODUCTION
Search results clustering (hereafter referred to as SRC) has become a popular means of approaching the problem
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

of information disorganization and redundancy in lists of search results returned by current search engines in response to multi-topic queries. If the items that relate to the same topic have been correctly placed within the same cluster and if the user is able to choose the right cluster from the cluster labels, such items can be accessed in logarithmic rather than linear time. Because there are so many available SRC systems employing very different techniques and algorithms (see [4] for a review), it is tempting to combine their outputs, just as the results of several search engines can be merged into a meta search engine. To the best of our knowledge, this problem has not been addressed so far.
As every clustering algorithm implicitly or explicitly assumes a certain data model, the main rationale for combining multiple clusterings is to try to produce a clustering with improved accuracy and robustness when such assumptions are not satisfied by the sample data. Meta SRC is clearly related to the the general field of meta clustering, also known as consensus clustering or clustering ensembles (e.g., [6], [15], [7]), but it poses unique challenges that cannot be easily addressed by available techniques:
· the features or algorithms that determined the clusterings are not accessible (characterized as the hard ensemble clustering in [14];
· the need to label the generated clusters with linguistic descriptions of high quality is very important for SRC applications, whereas this aspect is usually ignored in the meta clustering field;
· the clusterings may have been formed from non-identical sets of objects;
· high computational efficiency of the meta clustering algorithm is required to support real-time applications.
In this paper we first show that the characteristics of the outputs returned by multiple SRC algorithms suggest the adoption of a meta clustering approach. Based on this observation, we introduce a novel criterion for measuring the concordance of two partitions of n objects into m disjoint clusters, using the information content associated with the series of decisions made by the partitions on single pairs of objects.1 We then cast meta clustering as an optimization problem of the concordance between the clustering combination and the given set of clusterings. The optimization framework is the first main contribution of the paper.
1The terms `partition' and `clustering' are often used interchangeably throughout this paper.

170

To solve this problem, we test several metaheuristic methods: through an easily computable upper bound on the objective function, we show that a simple stochastic optimization method delivers fast approximations of the optimal value. The clusters of the meta clustering are then labeled with the most agreed upon cluster labels of the input clusterings. The procedure for building the meta clustering and labeling its clusters is our second main contribution.
The superior performance of meta SRC over individual clusterings is demonstrated through a range of evaluation techniques; i.e., using internal, classification-oriented, and information retrieval-oriented measures. The set of experiments are carried out on two test collections explicitly designed to evaluate the performance of clustering algorithms that post-process search results, including a newly created test collection made available for reuse. The extensive evaluation part is our third contribution. Overall, the proposed approach is empirically motivated, theoretically wellfounded, computationally efficient, and highly effective.
The remainder of the paper has the following organization. We first compare the results produced by some well known SRC algorithms, making use of methods and concepts on which we build in the following sections. Then we introduce the theoretical framework of optimal probabilistic meta clustering and study approximate solutions to the problem. After describing the procedure for meta labeling the generated clusters, we present the evaluation experiments. We finally discuss related work on meta clustering and offer some conclusions.
2. COMPARING SEARCH RESULTS CLUSTERINGS
Method combination usually works well when the results of the individual methods are different and of good quality. In this section we experimentally analyze whether SRC complies with these requirements. We use four state-of-theart SRC algorithms: Clusty, KeySRC, Lingo, and Lingo3G. KeySRC [1] and Lingo [11] are research systems2, while Clusty and Lingo3G are commercial web clustering engines. These systems are characterized by highly descriptive phrases as cluster labels, and are known to perform well on browsing retrieval tasks [4].
The clusterings to be compared were generated in the following way. We used the 100 most popular web queries provided by Google Trends as of February 2009. The queries were submitted to Clusty (with the clustering being restricted to the first 100 documents retrieved for each query), and the resulting set of snippets was collected and given as input to other SRC algorithms. In this way we have been able to include Clusty in the evaluation while ensuring that all the algorithms operate on the same set of snippets, for full comparability of the results.
In SRC, although the algorithms may produce a variable number of clusters, only those with the highest coverage are usually displayed on the first results page. Considering the first ten clusters is a typical choice, and is what we do in this paper. We group all the documents that are not covered by the first ten clusters in a dummy cluster `other topics'.
We first measured coverage and overlap of truly classified
2KeySRC and Lingo and can be tested, respectively, at http://keysrc.fub.it/Keysrc and http://search. carrot2.org/stable/search

documents; i.e., documents not grouped under the `other topics' cluster. We found that the first ten clusters of each method covered, on average, 58% of the 100 input documents. The overlap of truly classified documents across pairs of methods was 45%, the documents uniquely classified by either method were 31%, and the remaining 24% of documents were unclassified in both methods.
The next step was to measure the similarity of the clusterings produced by the different methods. In the following set of experiments we considered all 100 search results, thus including those that were grouped under the `other topics' cluster. While this cluster may be not so useful for information retrieval, it is relevant from the point of view of comparing the two partitions because it contains documents that cannot be grouped with similar documents. In the limit, if all the documents were placed into the `other topics' cluster by both systems, the two systems would be useless but very similar. To evaluate clustering similarity, we used the Rand index [13], explained below.
Given a set O of n objects and two partitions of O to compare, 1 and 2, the Rand index (R) is defined as:

a+d

a+d

R=

=

(1)

a+b+c+d

n

2

where:

· a: number of pairs of objects in O that are in the same cluster in 1 and in the same cluster in 2.
· b: number of pairs of objects in O that are in the same cluster in 1 and in different clusters in 2.
· c: number of pairs of objects in O that are in different clusters in 1 and in the same cluster in 2.
· d: number of pairs of objects in O that are in different
clusters in 1 and in different clusters in 2.

Intuitively, one can think of a + d as the number of agreements between 1 and 2, and b + c as the number of disagreements between 1 and 2.3 The Rand index has a value between 0 (i.e., no agreement on any pair of objects) and 1 (i.e., when the two partitions coincide).4
In Table 1, in the triangle above the main diagonal of the matrix, we report the mean Rand index value for each pair of methods, averaged over the set of queries. From these figures, the similarity among clusterings seems consistently high across method pairs. However, we must consider that a great contribution to the value of R comes from the pairs of objects that belong to different clusters both in 1 and 2; i.e., term d in Equation 1. As term a and term d do not have equal information in the SRC domain, a more reliable

3The Rand index assumes that clusters do not overlap. The clusters generated by the SRC systems were not strictly disjoint, but their overlap was very small: we found that on average less than one document in ten was assigned to more than one cluster. When multiple clusters contained the same document, we simulated a true partition by considering only the most highly ranked one in the list displayed by the system. 4As the expected value of the Rand index for random partitions does not take a constant value, the adjusted Rand index [8] is sometimes preferred, but we did not use it because its assumptions (e.g., a fixed number of objects in each cluster) do not fit the SRC data.

171

Clusty KeySRC Lingo Lingo3G

Clusty 1
J = 0.28 J = 0.26 J = 0.25

KeySRC R = 0.67
1 J = 0.27 J = 0.30

Lingo R = 0.66 R = 0.63
1 J = 0.26

Lingo3G R = 0.63 R = 0.64 R = 0.60
1

Table 1: Pairwise similarity of four SRC methods on 100 popular web queries. We show the Rand index values in the upper triangle of the matrix and the Jaccard coefficient values in the lower triangle.

Method
KeySRC Lingo Lingo3G Best combination Search engine

kSSL (k=1) 24.07 24.40 24.00 21.65 21.60

kSSL (k=2) 32.39 30.64 32.37 29.28 35.47

kSSL (k=3) 38.19 36.57 39.55 33.15 41.96

kSSL (k=4) 42.13 40,69 42.97 37.29 47.55

Table 2: Retrieval performance on the Ambient test collection measured as mean kSSL over the set of queries, for several values of k.

measure than the Rand index should probably underweigh the contribution of term d to the similarity.
A drastic solution [2] is to argue that pairs of type d are not clearly indicative either of similarity or of dissimilarity, as opposed to counts of "good pairs" (term a) and "bad pairs" (terms b and c), thus ending up with a formula conceptually similar to Jaccard's coefficient:

J= a

(2)

a+b+c

In the lower triangle of Table 1 we report pairwise Jaccard's coefficient values. The results clearly show that the inter-clustering similarity becomes dramatically lower than that measured with the Rand index, according to such a strong interpretation. In the next section we will define a more balanced way of assessing the importance of each term in Equation 1, depending on the number of clusters.
As the primary objective of meta SRC is to improve retrieval performance, it is useful to evaluate the effectiveness of the individual methods and analyse whether there is scope for improvement using a method combination. To this aim, we used AMBIENT, a test collection introduced in [3] and downloadable from http://credo.fub.it/ambient. AMBIENT is explicitly designed for evaluating the subtopic retrieval effectiveness of systems that post-process search results. It consists of 44 topics extracted from the ambiguous Wikipedia entries, each with a set of subtopics and a list of 100 ranked search results collected from a plain web search engine and manually annotated with subtopic relevance judgments.
As an evaluation measure, we used the Subtopic Search Length under k document sufficiency (kSSL), introduced in [1]. It is defined as the average number of items (cluster labels or search results) that must be examined before finding a sufficient number (k) of documents relevant to any of the query's subtopics, assuming that both cluster labels and search results are read sequentially from top to bottom, and that only cluster with labels relevant to the subtopic at hand are opened. The main features of kSSL are that: (i) it allows evaluation of full-subtopic retrieval (i.e., retrieval of multiple documents relevant to any subtopic) rather than focusing on subtopic coverage (i.e., retrieving at least one relevant document for some subtopics, as e.g. with subtopic recall at n); (ii) the modelization of the user search behavior is realistic because the role played by cluster labels is taken into account, whereas most earlier clustering evaluation studies assume that the user can choose the best cluster regardless of its label. The kSSL measure can be applied not only to clustered results but also to ranked lists, thus allowing clustering performance to be compared to the per-

Figure 1: Retrieval performance variation of SRC methods on individual Ambient queries using kSSL with k=2 (search engine list is the baseline).
formance of the original ranked list of search results given as input to the clustering algorithms (used as a baseline).
The systems being tested were ran on the 100 search results associated with each AMBIENT query and the performance of the corresponding output was evaluated using kSSL, with k = 1, 2, 3, 4. Again we considered the systems introduced above, except for Clusty, whose data were not available to us. The results, averaged over the set of queries, are reported in Table 2. Note that, for k = 1, SRC did not improve over using the plain list of ranked results, whereas its superiority becomes clear as the value of k increases, with a comparable mean performance improvement across different methods.
As we were interested in testing the hypothesis that individual methods behave differently on single queries, we also performed a query-by-query analysis. In Figure 1, we show the range of performance variations exhibited by the three clustering methods over the baseline on each query, using kSSL with k = 2 as evaluation measure. The differences were ample, with a lot of scope for performance improvement: if we were able to select the best method for each query, we would get the kSSL values reported in the penultimate row in Figure 1.
To summarize the results reported in this section, the individual clusterings were different and presented considerable variations of retrieval performance on single queries, while demonstrating comparable mean retrieval performance. Taken together, these findings indicate the use of a method combination strategy with the goal of maximizing the agreement with the individual clusterings, much in the same spirit

172

as multiple classifiers are combined through a majority vote, in the hope that the single classifiers make uncorrelated errors. This issue is dealt with in the next section.

3. OPTIMAL PROBABILISTIC META
CLUSTERING: PROBLEM DEFINITION
The Rand index and the Jaccard coefficient can be used even when considering partitions with a different number of clusters. However, when the partitions to be compared have a fixed number m of clusters, it may be more convenient to weigh the different types of agreements/disagreements on the basis of their probability of occurring by chance, rather than just counting them. As m grows, the chance for a pair of objects to be placed in the same cluster decreases, while at the same time the chance to be placed in different clusters increases. We can estimate the relative importance of terms a, b, c, d as a function of m.
Assuming that each object is randomly assigned to one cluster, the probability that two objects are in a same cluster in both partitions is

1

pa = m2 ,

(3)

the probability that two objects are in different clusters

in both partitions is

m-1 2

pd =

m

,

(4)

and the probability that two objects are in the same cluster in one partition and in different clusters in the other partition is

(m - 1)

pb = pc = m2 ,

(5)

with h ph = 1. The smaller the probability, the larger the information content associated with the observation that

the event indeed occurred. We estimate the weights associ-

ated with each of the four possible types of agreements or

disagreements with the self information, i.e:

wh = -log2 (ph)

(6)

Such weights (taken with positive sign for agreements and negative sign for disagreements) can be used to define a measure of probabilistic concordance of two partitions, as detailed below.
Given a set of n objects O = {o1, o2, ....oi, ...., on}, consider two partitions 1, 2 of O into m clusters, with corresponding object-cluster assignments ci 1 and ci 2 . The concordance of the two partitions at the object-pair level, denoted OC (object-pair concordance), is:



     

-log2 +log2

OC1,2 (i, j) =      

+log2 -log2

1 m2
m-1 m2
m-1 m2 m-1 2
m

if (ci 1 = cj 1 )  (ci 2 = cj 2 ) if (ci 1 = cj 1 )  (ci 2 = cj 2 ) if (ci 1 = cj 1 )  (ci 2 = cj 2 ) if (ci 1 = cj 1 )  (ci 2 = cj 2 )
(7)

where i, j are two objects, with i = j.

The concordance between the two partitions, denoted P C

(partition concordance), is defined as the average OC value

of all pairs of objects:

1

P C(1, 2) = n

OC1,2 (i, j) (8)

2

i, j

i<j

Based on this pairwise measure of partition concordance,
we define the meta partition concordance (M P C) between a single (meta) partition  and a set of q partitions
1, 2, ..., q as:

M P C(, 1, 2, ..., q)

=

1 q

q

P C(, r)

(9)

r=1

This is our objective function. The optimal partition opt

is the one that has maximal concordance with the given

partitions:

q

opt = arg max M P C(, r)



r=1

(10)

As the optimization of M P C is computationally expen-
sive, it is useful to derive an upper bound that can be easily computed. An upper bound M P CV on M P C can be de-
fined by considering for each pair of objects a contribution

to M P C equal to its maximum theoretical contribution ac-

cording to the given partitions:

MPCV = 1

q

n 2

i, j i<j

arg max ci V = cj V ci V = cj V

q
OCV ,r (i, j)
r=1

(11)

In other words, it suffices to consider the two possible cases

(either oi and oj in the same cluster, or oi and oj in different

clusters) and compute the corresponding concordance values

at the object-pair level with each given partition. Note that,

in general, there is no guarantee that that there will be an

admissible partition V that fulfills the decisions made when computing M P CV ; e.g., think of ci V = cj V , cj V = ck V , and ci V = ck V . The upper bound corresponds to an actual value if the input partitions coincide: the optimal partition

in this case is the given partition.

As an illustration, consider the following three partitions

of ten objects into four clusters (each position of the vector

corresponds to an object and the value is the cluster to which

the object has been assigned):

1 = (1, 1, 2, 2, 3, 4, 3, 3, 4, 4) 2 = (1, 1, 3, 4, 2, 2, 3, 4, 3, 4) 3 = (3, 4, 1, 1, 2, 2, 3, 4, 4, 3)
The partitions were chosen in such a way that the same number of object pairs are grouped together in any pair of partitions, i.e., the first two objects in 1 and 2, the third and fourth in 1 and 3, the fifth and sixth in 2 and 3.
With such small partitions, the optimal meta clustering can be found by brute force search. The number N of distinct partitions of n objects into m non-empty clusters is [5]:

N (n, m) = 1

m
(-1)m-i

m!

m i

in

i=0

(12)

173

For n = 10, m = 4, we get: N = 34, 105, wa = 4.0, wb = wc = 1.41, wd = 0.83. We generated all possible partitions and computed the M P C score associated with each, seen as a meta clustering of 1, 2, 3. The optimal partition is:
opt = (1, 1, 2, 2, 3, 3, 4, 4, 4, 4),
with M P C = 0.66, while the value of the upper bound M P CV is 0.77. Note that if we chose one of the original clusterings as meta clustering, we would get the same MPC value for each ( i.e., M P C = 0.59), due to their regularities. Intuitively, the optimal meta clustering retained the three clusters shared by pairs of individual methods and placed the remaining objects in the fourth cluster. In this example, the improvement attainable by the optimal M P C value is limited, because with few clusters and few objects choosing one of the original partitions as meta clustering ensures a fair concordance with the other partitions (including itself). We will see in Section 4.6 that in practical situations the increase of the M P C value is larger.
Clearly, brute force methods cannot be applied to find opt for values of interest in the SRC domain. For instance, with 100 objects and 10 clusters, we get  1039 distinct partitions. In the next section we study approximate solutions to this problem.
4. METAHEURISTIC OPTIMIZATION OF CLUSTERING CONCORDANCE
The impracticability of examining every possible partition naturally leads to the adoption of a hill climbing strategy, which essentially consists of iteratively rearranging existing partitions by moving individual objects to different clusters, and keeping the new partition only if it provides an improvement of the objective function. Metaheuristic algorithms [9] are elaborate combinations of hill climbing and random search to deal with local maxima. In this section we study the applicability of some well known metaheuristic algorithms to the clustering concordance optimization problem. Any of the following algorithms starts with a random (potentially poor) solution, because we observed that choosing more valuable starting points, such as using one of the given partitions, did not have a clear impact on the algorithm performance.
4.1 Steepest ascent hill climbing
In steepest ascent hill climbing all successors of a current partition are evaluated and the partition with the highest M P C is chosen. The computation halts when the movement of single objects no longer causes the objective function to improve.
4.2 Stochastic hill climbing
Stochastic hill climbing does not examine all successors before deciding how to move. Rather, it selects a successor at random, and moves to that successor provided that there is an improvement of M P C. The computation usually halts when we have not been able to choose a better successor after a fixed number of attempts. In our case, consistently with the termination criterion used for the steepest ascent hill climbing algorithm, we test all possible successors before halting the search.

Method
SAHC SHC

MPC
2.08 2.07

Imprv of meta over best i
12.3% 12.1%

Upper bound
2.38 2.38

Num. of candidates
14,950 4,967

Table 3: Performance of stochastic optimization methods on the Ambient collection.

Method
SAHC SHC

MPC
1.29 1.29

Imprv of meta over best i
21.9% 21.4%

Upper bound
1.70 1.70

Num. of candidates
23,147 5,891

Table 4: Performance of stochastic optimization methods on the ODP-239 collection.

4.3 Random restart hill climbing
Stochastic hill climbing with random restart iteratively does hill climbing for a random amount of time, each time with a new random partition. The best partition is kept: if a new run of hill climbing produces a better solution, it replaces the stored one.

4.4 Simulated annealing

In simulated annealing, the current state may be replaced

by a successor with a lower quality. That is, the algorithm

sometimes goes down hills. If the objective function value of

the successor (M P C ) is lower than that of the current best

partition (M P C), we move to the successor with a prob-

M P C -M P C
ability equal to e T , where T is a parameter that

decreases slowly, eventually to 0, at which point the algo-

rithm is doing plain hill climbing.

We used T

=

T0 log2 (i+1)

,

with T0 = 10 and i set to the iteration index.

4.5 Quantum annealing
Unlike previous algorithms, in quantum annealing the successors of the current partition are not generated by moving a single object. Ideally, the neighborhood of states explored by the method should initially extend over the whole search space, and then should shrink through the computation to the nearest states. We mimic this behavior by allowing moves involving both pairs of objects and single objects. Note that in this way the number of successors grows from nm to n2m2.

4.6 Testing metaheuristic optimization methods
The last three algorithms introduced above favor global optimization, at the cost of exploring a larger portion of the search space. However, preliminary tests suggested that they converged to acceptably good solutions more slowly. Based on this observation, we focused on the first two algorithms, namely steepest ascent hill climbing (SAHC) and stochastic hill climbing (SHC), and made a systematic evaluation of their performance.
For each query and each method, we computed the M P C value of the meta partition, the improvement over the initial partition with the best M P C score, the upper bound value on opt, and the number of candidate partitions generated during the search. In Table 3 we show the results for Ambient, averaged over the query set. In Table 4 we show the analogous results on a different test collection, termed ODP-239, that will be discussed in Section 6.1.

174

The meta partition was always much better than the best initial partition across both data sets. Furthermore, the results show that the meta clusterings generated by the two methods were reasonable approximations of the optimal meta clusterings because the gap between the heuristic values and the upper bound of the optimal value was not large. This was especially true for the Ambient collection, where the heuristic M P C value was indeed very close to the upper bound value for several queries. The results also show that the two methods built meta clusterings with very similar M P C values, but SHC explored a much smaller portion of the search space than SAHC. In practice, the processing times were in the order of hundreds of milliseconds on a computer of medium power (2.8 Ghz CPU, 4GB RAM), with SHC being about four times faster than SAHC.
5. META LABELING
After finding the optimal partition, we need to label its clusters. This is a very important key to the success of meta SRC as a browsing retrieval system, because a cluster with a poor label is very likely to be entirely omitted by the user even if it points to a group of strongly related and relevant documents. We do not generate cluster labels on our own, because this is a difficult task and it would require accessing the input documents. We take advantage of the fact that the individual SRC algorithms return phrase labels of high quality, and devise a procedure to select the most agreed upon labels from those given as input.
The procedure takes into account the characteristics of the set of labels provided by the individual SRC algorithms. We observed that, on average, the number of labels that are shared by a pair of SRC algorithms is only 9% of the total labels generated. By contrast, if we consider the single distinct non-stop words contained in the set of cluster labels generated for a given query, the similarity between pairs of methods is much higher, with a mean Jaccard index value of 0.24.
The meta labeling algorithm consists of three steps: 1. We associate with each cluster of the meta partition a set of candidate labels, formed by all labels under which each document in the cluster has been classified in at least one individual method. 2. We assign a score to each candidate label based on both its extensional coverage of the set of objects and intensional coverage of the set of labels. The purpose of the intensional factor is to promote syntactically different labels that refer to the same concept. The exact formula is the following:

Score(l) = count(obj) · count(w),

(13)

wl

where count(obj) is the number of search results in the cluster that are labeled by l, w is a non-stop word contained in l, and count(w) is the number of distinct labels in the cluster that contain word w.
3. We select the label with the highest score. Hereafter, the full clustering method consisting of generation of the meta partition with stochastic hill climbing followed by meta labeling will be referred to as OPTIMSRC (OPTImal Meta Search Results Clustering). As an illustration, consider the query `Bronx'. We show in Table 5 the set of cluster labels generated by each clustering algorithm (including OPTIMSRC) that were judged to be relevant to at

OPTIMSRC Imprv over KeySRC Imprv over Lingo Imprv over Lingo3G Imprv over search eng Imprv over best comb

kSSL (k=1) 20.56 14.5%* 15.7%* 14.3%* 4.8% 5.0%

kSSL (k=2) 28.93 10.6%* 5.5% 10.6%* 18.4%* 1.1%

kSSL (k=3) 34.05 10.8%* 6.8% 13.9%* 18.8%* -2.7%

kSSL (k=4) 38.94 7.5%* 4.3% 9.4%* 18.1%* -4.4%

Table 6: Retrieval performance improvement of OPTIMSRC over individual clustering methods, baseline, and best combined method.

least one of the subtopics of `Bronx' defined in the Ambient collection.
6. EVALUATION
In this section we describe the test collections used in the experiments and three complementary techniques to validate the results of OPTIMSRC compared to those of its input algorithms.
6.1 Test collections
There is no standard test collection for evaluating SRC algorithms. In addition to using Ambient, introduced in Section 2, we created a new, larger test collection, termed ODP-239. ODP-239 combines the features of search results data with those of classification benchmarks. It consists of 239 topics, each with about 10 subtopics and 100 documents. Each document is represented by a title and a very short snippet. The topics, subtopics, and their associated documents were selected from the top levels of the Open Directory Project (http://www.dmoz.org), in such a way that the distribution of documents across subtopics reflects the relative importance of subtopics. Unlike Ambient, all documents are relevant to at least one subtopic and the document-subtopic assignment comes for free. ODP-239 and Ambient have complementary aspects: the former collection deals with ambiguous queries and is suitable for information retrieval, the latter is about truly multi-topic queries and is aimed at classification. ODP-239 is available for download at http://credo.fub.it/odp239.
6.2 Subtopic retrieval
In this section we evaluate the subtopic retrieval effectiveness of OPTIMSRC. We use the same experimental setting as previous experiments with individual methods reported in Table 2. For each topic we found the meta partition associated with the clusterings produced by KeySRC, Lingo, and Lingo3G, and computed the corresponding kSSL values. This experiment was limited to the Ambient collection.5. The results are shown in Table 6. Asterisks are used to denote that the difference is statistically significant, using a two-tailed paired t test with a confidence level in excess of 95%.
OPTIMSRC obtained better results than any individual method for all evaluation measures, with most differences being statistically significant. Unlike the individual meth-
5The computation of kSSL requires that we know which cluster labels are relevant to each query's subtopic. Such relevance judgments were available to us for Ambient, but not for ODP-239, which is why this set of experiments could not be replicated on the former test collection.

175

Bronx query Subtop1
Subtop2 Subtop3 Subtop4

OPTIMSRC
Borough of New York city / Bronx County Bronx River Bronx Music Bronx Zoo

KeySRC city's borough / Bronx NY / Bronx district Bronx River
Bronx Zoo

Lingo
Borough of New York city / Bronx County / Bronx district Bronx River Music Bronx Zoo

Lingo3G
New York / Bronx County Bronx River Bronx Music Bronx Zoo

Table 5: Cluster labels relevant to the Ambient subtopics for the query `Bronx'. The subtopic definitions are the following. Subtop1: `The Bronx, one of the five boroughs of New York City'; Subtop2: `Bronx River, a river that flows south through The Bronx'; Subtop3: `The Bronx(band), an American punk rock band'; Subtop4: `Bronx Zoo'.

Collection OPTIMSRC KeySRC Lingo Lingo3G

Ambient

0.27

0.21

0.22

0.14

ODP-239

0.25

0.20

0.19

0.15

Table 7: Mean silhouette coefficient value of SRC systems on the Ambient and ODP-239 collections.

ods, OPTIMSRC improved on the baseline not only for k  2 but also for k = 1. Note that for k = 1, k = 2, the performance of OPTIMSRC was even better than the performance that we would obtain by selecting the best individual method for each query. Although somewhat surprising, this is perfectly consistent with the approach proposed here, because the meta strategy does not combine the performance results of individual methods but truly integrates their performance components.
6.3 Internal measures: the silhouette coefficient
Internal indices of cluster validity use only information present in the data set. Most of them (see [12] for a comprehensive summary) are based on the notions of cluster cohesion, i.e., how close the objects in a cluster are, and cluster separation, i.e., how distinct a cluster is from other clusters. The popular method of silhouette coefficients combines both cohesion and separation. The silhouette coefficient si for an individual object i is defined as:

si

=

bi - ai , max (ai, bi)

(14)

where ai is the average distance of object i from all other objects in its cluster, and bi is the minimum average distance to objects in another cluster. The value of the silhouette coefficient can vary between -1 and 1; a negative value is undesirable because this corresponds to ai > bi, meaning that the distance of data objects to the center of their cluster is greater than the distance to the next nearest cluster. The silhouette coefficient S of a clustering is defined as the average silhouette coefficient of all objects.
We computed the S value of the individual clusterings and of OPTIMSRC for each query, representing objects as tf-idf weighted term vectors (up to text normalization) and using the formula 1 - cosine similarity as distance function. In Table 7, we show the results for each collection averaged over the corresponding query set.
As OPTIMSRC achieved much better results than the individual methods, it may be conjectured that the use of meta clustering helped to remove noisy object-cluster assignments (i.e., objects that could not be clearly assigned to one clus-

ter), thus leading to more recognizable cluster structures in the feature space.
On the other hand, the relatively low mean absolute values of S for all SRC systems, including OPTIMSRC, indicate that the separation between clusters was not always clear. This is not surprising, given that neither the individual clusterings nor OPTIMSRC were explicitly optimized for cohesion or separation. Also, perhaps more importantly, we must consider that such algorithms perform sophisticated forms of feature construction to detect inter-document similarities that go beyond the bag-of-words approach. Thus, the feature space in which we defined the distance function used to compute S is not the same as that employed to do the clustering. The former is the space of the original single terms describing the documents, the latter is a space of phrases that do not necessarily occur in exactly the same form in the documents to which they are assigned by the algorithms.
6.4 Ground-truth validation
Ground truth validation is aimed at assessing how good a clustering method is at recovering known clusters (referred to as classes) from a gold standard partition. For this experiment we use only the ODP-239 collection, because many documents in Ambient are not assigned to any category (i.e., those search results that were not relevant to any of the query's subtopics listed in Wikipedia).
Several evaluation measures are available for this task (see [10] for a detailed summary), including the well known F measure [16] that combines precision P and recall R:

(2 + 1)P R

F = 2P + R ,

(15)

with

P = TP , R = TP

(16)

TP +FP

TP +FN

where T P , T N , F P , and F N are respectively, the number of true-positives (i.e., two documents of the same class assigned to the same cluster), true-negatives (i.e., two documents of different classes assigned to different clusters), false-positives (i.e., two documents of different classes assigned to the same cluster), and false-negatives (i.e., two documents of the same class assigned to different clusters). The parameter  is a weighting factor for the importance of the recall (or precision).
Because separating documents of a same class usually has a worse effect than placing pairs of documents of different classes in the same cluster, at least in the SRC domain,

176

Method OPTIMSRC KeySRC Lingo Lingo3G

F1 0.313 0,295 0.273 0.311

F2 0.341 0.318 0.283 0.292

F5 0.380 0.341 0.294 0.285

Table 8: Classification performance on the ODP239 collection measured as mean (micro-averaged) F over the set of queries, for several values of .

we give more weight to recall. In Table 8 we show the mean (micro-averaged) F values for each clustering method, with  = 1, 2, 5.
OPTIMSRC clearly outperformed the other methods for all evaluation measures, with higher values of  leading to greater performance improvements, up to 11.76 % over the best individual method for  = 5. Note that although OPTIMSRC was a clear improvement over individual methods, its performance is, on an absolute scale, still relatively low. This is probably due to the intrinsic difficulty of the classification task on the ODP-239 collection, where documents are very short and subtopics do not always have very distinct meanings.

7. RELATED WORK

While there is no earlier work on meta SRC, the gen-

eral problem of finding a meta (or consensus) clustering

from multiple partitions has been approached from various

perspectives (e.g., graph-based, statistical, and combinato-

rial), using, among others: hypergraph partitioning [14], co-

association matrix [6], mixture model [15], and Bayesian ap-

proach [18].

Most relevant to us is the work on finding the median

partition, that is the partition that minimizes the distance

to the given partitions. Similar to our paper, the key to

finding the most important commonalities and differences

among partitions are the decisions made on single pairs of

objects. Under the hypothesis that the distance between

partitions is measured by the number of disagreements, i.e.,

b+c =

n 2

- (a + d), the median partition problem is

known to be NP-complete [17] and various heuristics can be used for approximating it [7]. In contrast to our work, the objective function is strictly modeled after the notion of binary agreements/disagreements, in a Rand index style.

8. CONCLUSIONS AND FUTURE WORK
In this paper we studied the problem of meta search results clustering. We introduced a novel probabilistic criterion for combining the results of a given set of partitions of n objects into m clusters, and showed that a simple stochastic optimization algorithm delivers fast approximations of the optimal value. Through a range of evaluation techniques, we showed that optimal meta SRC, enriched with meta labeling, is more effective than the individual SRC algorithms, and it is also efficient for real-time applications.
Two natural research directions are the extension of the proposed framework to partitions with a variable number of clusters and to partitions of different but overlapping sets of objects (e.g., web clustering engines that fetch their search results from distinct search engines). Future work will also

include an experimental comparison with other meta clustering methods such as finding the median partition.
9. ACKNOWLEDGMENTS
We would like to thank Stanislaw Osin´ski and Dawid Weiss for providing us with the results of Lingo and Lingo3G, and four anonymous reviewers for their comments.
10. REFERENCES
[1] A. Bernardini, C. Carpineto, and M. D'Amico. Full-Subtopic Retrieval with Keyphrase-Based Search Results Clustering. In Proc. Web Intelligence 2009, Milan, Italy, pages 206­213. IEEE Computer Society, 2009.
[2] R. J. G. B. Campello. A fuzzy extension of the Rand index and other related indexes for clustering and classification assessment. Pattern Recognition Letters, 28(7):833­841, 2007.
[3] C. Carpineto, S. Mizzaro, G. Romano, and M. Snidero. Mobile Information Retrieval with Search Results Clustering: Prototypes and Evaluations. JASIST, 60(5):877­895, 2009.
[4] C. Carpineto, S. Osin´ski, G. Romano, and D. Weiss. A survey of Web clustering engines. ACM Computing Survey, 41(3), 2009.
[5] G. L. Liu. Introduction to Combinatorial Mathematics. McGraw Hill, 1968.
[6] A. Fred and A. Jain. Data clustering using evidence accumulation. In ICPR, pages 276­280, 2002.
[7] A. Goder and V. Filkov. Consensus Clustering Algorithms: Comparison and Refinement. In Proc. ALENEX 2008, San Francisco, CA, USA, pages 109­117, 2008.
[8] L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2(1):193­218, 1985.
[9] S. Luke. Essentials of Metaheuristics. 2009. available at http://cs.gmu.edu/sean/book/metaheuristics/.
[10] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[11] S. Osin´ski and D. Weiss. A Concept-Driven Algorithm for Clustering Search Results. IEEE Intelligent Systems, 20(3):48­54, 2005.
[12] M. S. P.-N. Tan and V. Kumar. Introduction to Data Mining, chapter 8: Cluster analysis: basic concepts and algorithms, pages 487­568. Addison Wesley, 2005.
[13] W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of American Statistical Association, 66:846­850, 1971.
[14] A. Strehl and J. Ghosh. Cluster Ensembles ­ A Knowledge Reuse Framework for Combining Multiple Partitions. JMLR, (3):583­617, 2002.
[15] A. Topchy, A. K. Jain, and W. Punch. Clustering ensembles: models of consensus and weak partitions. IEEE Trans. PAMI, 27(12):1866­1881, 2005.
[16] K. van Rijsbergen. Information Retrieval. Butterworth-Heinemann, 1979.
[17] Y. Wakabayashi. The Complexity of Computing Medians of Relations. Resenhas, 3(3):323­349, 1998.
[18] H. Wang, H. Shan, and A. Banerjee. Bayesian Cluster Ensembles. In Proc. SDM 2009, pages 209­220, 2009.

177

Self-Taught Hashing for Fast Similarity Search

Dell Zhang
DCSIS Birkbeck, University of London
Malet Street London WC1E 7HX, UK
dell.z@ieee.org
Deng Cai
State Key Lab of CAD&CG Zhejiang University 100 Zijinggang Road
Hangzhou 310058, China
dengcai@cad.zju.edu.cn

Jun Wang
Dept of Computer Science University College London
Gower Street London WC1E 6BT, UK
jun.wang@cs.ucl.ac.uk
Jinsong Lu
DEMS Birkbeck, University of London
Malet Street London WC1E 7HX, UK
jingsong.lu@gmail.com

ABSTRACT
The ability of fast similarity search at large scale is of great importance to many Information Retrieval (IR) applications. A promising way to accelerate similarity search is semantic hashing which designs compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes (within a short Hamming distance). Although some recently proposed techniques are able to generate high-quality codes for documents known in advance, obtaining the codes for previously unseen documents remains to be a very challenging problem. In this paper, we emphasise this issue and propose a novel SelfTaught Hashing (STH) approach to semantic hashing: we first find the optimal l-bit binary codes for all documents in the given corpus via unsupervised learning, and then train l classifiers via supervised learning to predict the l-bit code for any query document unseen before. Our experiments on three real-world text datasets show that the proposed approach using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine (SVM) outperforms stateof-the-art techniques significantly.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications-- data mining; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology--classifier design and evaluation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

General Terms
Algorithms, Experimentation, Performance
Keywords
Similarity Search, Semantic Hashing, Laplacian Eigenmap, Support Vector Machine.
1. INTRODUCTION
The problem of similarity search (aka nearest neighbour search) is: given a query document1, find its most similar documents from a very large document collection (corpus). It is of great importance to many Information Retrieval (IR) [30] applications, such as near-duplicate detection [18], plagiarism analysis [43], collaborative filtering [26], caching [32], and content-based multimedia retrieval [28].
Recently, with the rapid evolution of the Internet and the increased amounts of data to be processed, how to conduct fast similarity search at large scale has become an urgent research issue. A promising way to accelerate similarity search is semantic hashing [34] which designs compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes (within a short Hamming distance). It is extremely fast to perform similarity search over such binary codes [42], because
· the encoded data are highly compressed and thus can be loaded into the main memory;
· the Hamming distance between two binary codes can be computed efficiently by using bit XOR operation and counting the number of set bits [25, 46]: an ordinary PC today would be able to do millions of Hamming distance computation in just a few milliseconds.
Furthermore, we usually just need to retrieve a small number of the most similar documents (i.e., nearest neighbours) for a given query document rather than computing its similarity to all documents in the collection. In such situations, we can simply return all the documents that are hashed into a tight
1In similarity search, a document is used as the query for retrieval, which is fundamentally different with the standard keyword search paradigm, e.g., in TREC.

18

Hamming ball centred around the binary code of the query document. For example, assuming that we use 4-bit binary codes, if the query document is represented as `0000', then we can just check this code as well as those 4 codes within one Hamming distance to it (i.e., having one bit difference with it) -- `1000', `0100', `0010', and `0001' -- and return the associated documents back. It will also be easy to filter or re-rank the very small set of "good" documents (returned by semantic hashing) based on their full content, so as to further improve the retrieval effectiveness with just a little extra time [42].
In addition, similarity search serves as the basis of a classic non-parametric machine learning method, the k-NearestNeighbours (kNN) algorithm [31], for automated text categorisation [37] and so on. By enabling fast similarity search at large scale, semantic hashing makes it feasible to exploit "the unreasonable effectiveness of data" [14] to accomplish traditionally difficult tasks. For example, researchers recently achieved great success in scene completion and scene recognition using millions of images on the Web as training data [15, 44].
Although some recently proposed techniques are able to generate high-quality codes for the documents known in advance, obtaining the codes for previously unseen documents remains to be a very challenging problem [42]. Existing methods either have prohibitively high computational complexity or impose exceedingly restrictive assumptions about data distribution (see Section 3.2). In this paper, we emphasise this issue and propose a novel Self-Taught Hashing (STH) approach to semantic hashing. As illustrated in Figure 1, we first find the optimal l-bit binary codes for all documents in the given corpus via unsupervised learning, and then train l classifiers via supervised learning to predict the l-bit code for any query document unseen before.
Our experiments on three real-world text datasets show that the proposed approach using binarised Laplacian Eigenmap (LapEig) [3] and linear Support Vector Machine (SVM) [23, 36] outperforms state-of-the-art techniques significantly, while maintaining a high running speed.
The rest of this paper is organised as follows. In Section 2, we review the related work. In Section 3, we present our approach in details. In Section 4, we show the experimental results. In Section 5, we make conclusions.
2. RELATED WORK
There has been extensive research on fast similarity search due to its central importance in many applications. For a low-dimensional feature space, similarity search can be carried out efficiently with pre-built space-partitioning index structures (such as KD-tree) or data-partitioning index structures (such as R-tree) [7]. However, when the dimensionality of feature space is high (say > 10), similarity search aiming to return exact results cannot be done better than the naive method -- a linear scan of the entire collection [45]. In the IR domain, documents are typically represented as feature vectors in a space of more than thousands of dimensions [30]. Nevertheless, if the complete exactness of results is not really necessary, similarity search in a highdimensional space can be dramatically speeded up by using hash-based methods which are purposefully designed to approximately answer queries in virtually constant time [42].
Such hash-based methods for fast similarity search can be considered as a means for embedding high-dimensional fea-

ture vectors to a low-dimensional Hamming space (the set of all 2l binary strings of length l), while retaining as much as possible the semantic similarity structure of data. Unlike standard dimensionality reduction techniques such as Latent Semantic Indexing (LSI) [5, 8] and Locality-Preserving Indexing (LPI) [17, 16], hashing techniques map feature vectors to binary codes, which is key to extremely fast similarity search (see Section 1). One possible way to get binary codes for text documents is to binarise the real-valued low-dimensional vectors (obtained from dimensionality reduction techniques like LSI) via thresholding [34]. An improvement on binarised-LSI that directly optimises a Hamming distance based objective function, namely Laplacian Co-Hashing (LCH), has been proposed recently [50].
The most well-known hashing technique that preserves similarity information is probably Locality-Sensitive Hashing (LSH) [1]. LSH simply employs random linear projections (followed by random thresholding) to map data points close in a Euclidean space to similar codes. It is theoretically guaranteed that as the code length increases, the Hamming distance between two codes will asymptotically approach the Euclidean distance between their corresponding data points. However, since the design of hash functions for LSH is dataoblivious, LSH may lead to quite inefficient (long) codes in practice [34, 48].
Several recently proposed hashing techniques attempt to overcome this problem by finding good data-aware hash functions through machine learning. In [34], the authors proposed to use stacked Restricted Boltzmann Machine (RBM) [19, 20], and showed that it was indeed able to generate compact binary codes to accelerate document retrieval. Researchers have also tried the boosting approach to Similarity Sensitive Coding (SSC) [38] and Forgiving Hashing (FgH) [2] -- they first train AdaBoost [35] classifiers with similar pairs of items as positive examples (and also non-similar pairs of items as negative examples in SCC), and then take the output of all (decision stump) weak learners on a given document as its binary code. In [44], both stacked-RBM and boosting-SSC were found to work significantly better and faster than LSH when applied to a database containing tens of millions of images. In [48], a new technique called Spectral Hashing (SpH) was proposed. It has demonstrated significant improvements over LSH, stacked-RBM and boostingSSC in terms of the number of bits required to find good similar items. There is some resemblance between the first step of SpH and the unsupervised learning stage of our STH approach, because both are related to spectral graph partitioning [6, 13, 40]. Nevertheless, we use a different spectral method and take a different way to address the entropy maximising criterion (see Section 3.1). More importantly, in order to process query documents, SpH has to assume that the data are uniformly distributed in a hyper-rectangle, which is apparently very restrictive. In contrast, our proposed STH approach can work with any data distribution and it is much more flexible (see Section 3.2). The superiority of STH to SpH has been confirmed by our experimental results (see Section 4).
A somewhat related, but different, line of research is to use hashing representations for machine learning [41, 47]. The objective of such techniques is to accelerate complex learning algorithms, but not similarity search. Our work is basically the other way around.

19

Figure 1: The proposed STH approach to semantic hashing.

3. APPROACH
The proposed Self-Taught Hashing (STH) approach to semantic hashing is a general learning framework that consists of two distinct stages, as illustrated in Figure 1. We call the approach "self-taught" because the hash function is learnt from the data that are auto-labelled by itself in the previous stage2 .
3.1 Stage 1: Unsupervised Learning of Binary Codes
Given a collection of n documents which are represented as m-dimensional vectors {xi}ni=1  Rm. Let X denote the m × n term-document matrix: [x1, . . . , xn]. Suppose that the desired length of code is l bits. We use yi  {-1, +1}l to represent the binary code for document vector xi, where the p-th element of yi, i.e., yi(p), is +1 if the p-th bit of code is on, or -1 otherwise. Let Y denote the n × l matrix whose i-th row is the code for the i-th document, i.e., [y1, . . . , yn]T .
A "good" semantic hashing should be similarity preserving to ensure effectiveness. That is to say, semantically similar documents should be mapped to similar codes within a short Hamming distance.
Unlike the existing approaches (such as SpH [48]) that aim to preserve the global similarity structure of all document pairs, we focus on the local similarity structure, i.e., k-nearest-neighbourhood, for each document. Since IR applications usually put emphasis on a small number of most similar documents for a given query document [30], preserving the global similarity structure is not only unnecessary but also likely to be sub-optimal for our problem. Therefore, using the cosine similarity3 [30], we construct our n×n local
2It is, however, worth noticing that the term "self-taught learning" has been mentioned in [33] where the intention was to describe a strategy for transfer learning based on sparse coding, whereas in this paper the term has a rather different meaning. 3Our approach can work with any legitimate similarity measure, though we focus on cosine similarity in this paper.

similarity matrix W as

Wij =

xTi xj xi · xj
0

if xi  Nk(xj) or xj  Nk(xi) otherwise

(1)

where Nk(x) represents the set of k-nearest-neighbours of document x. In other words, W is the adjacency matrix of

the k-nearest-neighbours graph for the given corpus [3]. A

by-product of focusing on such a local similarity structure

instead of the global one is that W becomes a sparse ma-

trix. This not only leads to much lower storage overhead,

but also brings a significant reduction to the computational

complexity of subsequent operations. Furthermore, we in-

troduce a diagonal n × n matrix D whose entries are given

by Dii =

n j=1

Wij

.

The

matrix

D

provides

a

natural

mea-

sure of document importance: the bigger the value of Dii is,

the more "important" is the document xi as its neighbours

are strongly connected to it [3].

The Hamming distance between two binary codes yi and

yj (corresponding to documents xi and xj) is given by the

number of bits that are different between them, which can

be calculated as

1 4

yi - yj

2.

To meet the similarity pre-

serving criterion, we seek to minimise the weighted average

Hamming distance (as in SpH [48])

1n 4

n
Wij yi - yj 2

(2)

i=1 j=1

because it incurs a heavy penalty if two similar documents

are mapped far apart. After some simple mathematical

transformation, the above objective function can be rewrit-

ten

in

matrix

form

as

1 4

Tr(Y

T

LY

),

where

L

=

D-W

is

the graph Laplacian [6], and Tr(·) means the matrix trace.

We found the above objective function (2) actually pro-

portional to that of a well-known manifold learning algo-

rithm, Laplacian Eigenmap (LapEig) [3], except that LapEig does not have the constraint yi  {-1, +1}l. So, if we relax

this discreteness condition but just keep the similarity pre-

serving requirement, we can get the optimal l-dimensional

real-valued vector y~i to represent each document xi by solv-

20

ing the following LapEig problem:

arg min Tr(Y~ T LY~ )

(3)

Y~

subject to Y~ T DY~ = I

Y~ T D1 = 0

where Tr(Y~ T LY~ ) gives the real relaxation of the weighted average Hamming distance Tr(Y T LY ), and the two constraints prevent the collapse into a subspace of dimension less than l. The solution of this optimisation problem is given by Y~ = [v1, . . . , vl] whose columns are the l eigenvectors corresponding to the smallest eigenvalues of the follow-
ing generalised eigenvalue problem (except the trivial eigen-
value 0):

Lv = Dv

(4)

The above LapEig formulation (3) may look similar to the first step of SpH [48]. This is because SpH is motivated by a spectral graph partitioning method ratio-cut [13], while LapEig is closely connected to another spectral graph partitioning method normalised-cut [40]. Many independent studies have shown that normalised-cut has better theoretical properties and empirical performances than ratio-cut [6, 40].
We now convert the above l-dimensional real-valued vectors y~1, . . . , y~n into binary codes via thresholding: if the p-th element of y~i is larger than the specified threshold, yi(p) = +1 (i.e., the p-th bit of the i-th code is on); otherwise, yi(p) = -1 (i.e., the p-th bit of the i-th code is off).
A "good" semantic hashing should also be entropy maximising to ensure efficiency, as pointed out by [2]. According to the information theory [39]: the maximal entropy of a source alphabet is attained by having a uniform probability distribution. If the entropy of codes over the corpus is small, it means that documents are mapped to only a small number of codes (hash bins), thereby rendering the hash table inefficient. To meet this entropy maximising criterion, we set the threshold for binarising y~1(p), . . . , y~n(p) to be the median value of vp. In this way, the p-th bit will be on for half of the corpus and off for the other half. Furthermore, as the eigenvectors v1, . . . , vl given by LapEig are orthogonal to each other, different bits y(1), . . . , y(l) in the generated binary codes will be uncorrelated. Therefore this thresholding method gives each distinct binary code roughly equal probability of occurring in the document collection, thus achieves the best utilisation of the hash table.

3.2 Stage 2: Supervised Learning of Hash Function
Mapping all documents in the given corpus to binary codes does not completely solve the problem of semantic hashing, because we also need to know how to obtain the binary codes for query documents, i.e., new documents that are unseen before. This problem, called out-of-sample extension in manifold learning, is often addressed using the Nystrom method [4, 9]. However, calculating the Nystrom extension of a new document is as computationally expensive as an exhaustive similarity search over the corpus (that may contain millions of documents), which makes it impractical for semantic hashing. In LPI [17, 16], LapEig [3] is extended to deal with new samples by approximating a linear function to the embedding of LapEig. However, the computational

complexity of LPI is very high because its learning algorithm
involves eigen-decompositions of two large dense matrices. It
is infeasible to apply LPI if the given training corpus is large.
In SpH [48], new samples are handled by utilising the latest
results on the convergence of graph Laplacian eigenvectors
to the Laplace-Beltrami eigenfunctions of manifolds. It can
achieve both fast learning and fast prediction, but it relies
on a very restrictive assumption that the data are uniformly
distributed in a hyper-rectangle.
Overcoming the limitations of the above techniques [4, 9,
17, 16, 48], this paper proposes a novel method to com-
pute the binary codes for query documents by considering it as a supervised4 learning problem: we think of each bit yi(p)  {+1, -1} in the binary code for document xi as a binary class label (class-"on" or class-"off") for that document, and train a binary classifier y(p) = f (p)(x) on the given corpus that has already been "labelled" by the above
binarised-LapEig method, then we can use the learned binary classifiers f (1), . . . , f (l) to predict the l-bit binary code y(1), . . . , y(l) for any query document x. As mentioned in the previous section, different bits y(1), . . . , y(l) in the generated binary codes are uncorrelated. Hence there is no redundancy among the binary classifiers f (1), . . . , f (l), and they can also be trained independently.
In this paper, we choose to use the Support Vector Ma-
chine (SVM) [23, 36] algorithm to train these binary classifiers. SVM in its simplest form, linear SVM f (x) = sgn(wT x) consistently provides state-of-the-art performance for text classification tasks [10, 22, 49]. Given the documents x1, . . . , xn together with their self-taught binary labels for the p-th bit y1(p), . . . , yn(p), the corresponding linear SVM can be trained by solving the following quadratic optimisation problem

arg min
w,i 0

1 wT 2

w

+

C n

n

i

i=1

(5)

subject to

ni=1 : yi(p)wT xi  1 - i

A notable advantage of using SVM classifiers here is that we can easily achieve non-linear mappings if necessary by plugging in non-linear kernels [36], though we do not explore this potential in this paper.

3.3 Summary of Approach
We name the above proposed two-stage approach SelfTaught Hashing (STH). In this paper, we choose binarisedLapEig [3] for the unsupervised learning stage and linearSVM [23, 36] for the supervised learning stage, but obviously it is possible to use other machine learning algorithms.
The learning process of STH for a given corpus can be summarized as follows.

1. unsupervised learning of binary codes:

· construct the k-nearest-neighbours graph for the given corpus;
· embed the documents in an l-dimensional space through LapEig (4) to get an l-dimensional realvalued vector for each document;
4Since in the second stage, the supervised learning algorithm uses only the pseudo-labels input from the previous unsupervised learning stage, the entire STH approach remains to be unsupervised.

21

· obtain an l-bit binary code for each document via thresholding the above vectors at their median point, and then take each bit as a binary class label for that document;
2. supervised learning of hash function:
· train l SVM classifiers (5) based on the given corpus that has been "labelled" as above.
Let s denote the average number of non-zero features per document. In the first stage, constructing the k-nearestneighbours graph takes O(n2s+n2k) time using the selection algorithm [7], solving the LapEig problem (4) takes O(lnkt) time using the Lanczos algorithm [12] of t iterations (the value of t is usually quite small), and the median-based binarisation takes O(ln) time again using the selection algorithm [7]. In the second stage, thanks to the recent advances in large-scale optimisation, each of the l linear SVM classifiers can be trained in O(sn) time or even less [24, 21], so all training can be done in O(lsn) time. Both the value of l and the value of k can be regarded as small constants, as usually a short code length is desirable and just a few nearest neighbours are needed. For example, l  64 and k = 25 in our experiments (see Section 4). Therefore the overall computational complexity of the learning process is roughly quadratic to the number of documents in the corpus while linear to the average size of the documents in the corpus.
The predicting process of STH for a given query document is simply to classify the query document using those l learned classifiers and then assemble the output l binary labels into an l-bit binary code. For linear SVM, classifying a document only requires one dot-product operation between two vectors, the aggregated support vector and the document vector (with s non-zero features), which can be done quickly in O(s ) time. Therefore the overall computational complexity of the prediction process for each query document is linear to the size of the query document.
4. EXPERIMENTS
We now empirically evaluate our proposed STH approach (using binarised-LapEig and linear-SVM), and compare its performance with binarised-LSI [34], LCH [50], and SpH [48] that represents the state of the art (see Section 2).
In the following STH experiments, the parameter k = 25 when constructing the k-nearest-neighbours graph for LapEig5, and the SVM implementation is from LIBLINEAR [11] with the default parameter values6.
4.1 Data
We have conducted experiments on three publicly available real-world text datasets: Reuters215787, 20Newsgroups8 and TDT29.
5In principle, the value of k for LapEig should be set to the desired number of original nearest neighbours to be retrieved (see Section 4.2). 6It is not necessary to fine tune the SVM parameters (such as C) because it has already worked very well with its default parameter values. 7http://www.daviddlewis.com/resources/testcollections/ reuters21578/ 8http://people.csail.mit.edu/jrennie/20Newsgroups/ 9http://www.nist.gov/speech/tests/tdt/tdt98/index.htm

The Reuters21578 corpus is a collection of documents that appeared on Reuters newswire in 1987. It contains 21578 documents in 135 categories. In our experiments, those documents appearing in more than one category were discarded, and only the largest 10 categories were kept, thus leaving us with 7285 documents in total. We use the ModeApte split here which gives 5228 (72%) documents for training and 2057 (28%) documents for testing.
The 20Newsgroups corpus was collected and originally used for document categorisation by Lang [27]. We use the popular `bydate' version which contains 18846 documents, evenly distributed across 20 categories. The time-based split leads to 11314 (60%) documents for training and 7532 (40%) documents for testing.
The TDT2 (NIST Topic Detection and Tracking) corpus consists of data collected during the first half of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). It consists of 11201 on-topic documents which are classified into 96 semantic categories. In our experiments, those documents appearing in more than one category were discarded, and only the largest 30 categories were kept, thus leaving us with 9394 documents in total. We randomly selected 5597 (60%) documents for training and 3797 (40%) documents for testing. The averaged performance based on 10 such random selections is reported in this paper.
All the above datasets have been pre-processed by stopword removal, Porter stemming, and TF-IDF weighting [30].
For the purpose of reproducibility, we shall make the datasets and code used in our experiments publicly available at the first author's homepage upon paper publication.
4.2 Evaluation
Given a dataset, we use each document in the test set as a query to retrieve documents in the training set within a specified Hamming distance, and then compute standard retrieval performance measures: precision, recall, and their harmonic mean (F1 measure) [30].
precision = the number of retrieved relevant documents the number of all retrieved documents (6)
recall = the number of retrieved relevant documents the number of all relevant documents (7)
The reported performance scores in the following Section are averaged over all test queries in the dataset.
To determine whether a retrieved document is "relevant" to the given query document, we adopt the following two evaluation methodologies:
1. retrieving original nearest neighbours -- the k most similar documents, i.e., nearest neighbours, in the original vector space are considered as the groundtruth relevant documents (k = 25 in our experiments);
2. retrieving same-topic documents -- the documents on the same topic, i.e., in the same category, are considered as the ground-truth relevant documents.
The former methodology is used in [48]10, while the latter methodology is used in [34]. In our opinion, these two
10Actually only precision is used in [48], which is appropriate

22

methodologies emphasise different aspects of semantic hashing, and thus are suitable for different target IR applications. Therefore we use both of them in this paper.
The absolute performance scores of STH are not as important as how they compare with those of other semantic hashing techniques. As previously mentioned in Section 1, if necessary, we can always spend a little extra time to filter or re-rank the similarity search results based on their full content, thus achieve higher performance scores [42].
4.3 Results
Figure 2 and Figure 3 show the F1 measure of STH for retrieving original nearest neighbours and same-topic documents respectively11. We vary the code length from 4-bit to 64-bit and also the Hamming ball radius (i.e., the maximum Hamming distance between any retrieved document and the query document) from 0 to 3, in order to show their influences on the retrieval performance. It can be seen that when the code length increases, STH is able to achieve a higher F1 measure (using a bigger Hamming ball radius). However, longer binary codes demand more memory and a bigger Hamming ball radius requires more computation. The optimal trade-off between effectiveness and efficiency can be found by using a validation set of query documents.
Figure 4 and Figure 5 compare STH with several other typical semantic hashing methods in terms of their precisionrecall curves (created by varying the code length from 4bit to 64-bit while fixing the Hamming ball radius at 1), for retrieving original nearest neighbours and same-topic documents respectively12. It is clear that on all datasets and under both evaluation methodologies, STH outperforms binarised-LSI, LCH, and the state-of-the-art technique SpH (that has already been shown to work much better than LSH [1], stacked-RBM13 [34] and boosting-SSC [38]). Using 16bit codes and Hamming ball radius 1, the performance improvements are all statistically significant (P value < 0.01) according to one-sided micro sign test (s-test) [49].
We think the superior performance of STH is due to two reasons:
· the binary codes produced by binarised-LapEig effectively preserve the semantic similarity structure while maximising the entropy of the hash table;
· the maximum-margin hyperplane produced by linearSVM ensures high generalisation ability [36].
for their application of pattern recognition but obviously insufficient from the IR perspective. Due to this difference in performance measurement, their results are not directly comparable with ours. 11The F1 measure scores reported here should not be directly compared with those in text categorisation papers, as we are addressing a very different problem even though the same datasets may have been used for experimentation. 12Although we could achieve higher retrieval performance by utilising a bigger Hamming ball radius (e.g., 4), a large number of binary codes (e.g., C644 = 635376 for 64-bit codes) would need to be checked for each query and then the efficiency gain brought by semantic hashing would diminish. 13For example, on the 20Newsgroups dataset, stacked-RBM achieves a maximum of F1 = 0.276 for retrieving same-topic documents with 128-bit codes, while the same level of performance can be obtained using our STH approach with just 8-bit codes.

We have also examined the approximation errors accumulated in each step of STH (see Section 3.3). Our anatomy reveals that almost all approximation errors come from the dimensionality reduction step using LapEig. However, LapEig does work better than alternative methods (such as LSI) for this step in our experiments, and it is a well-known hard problem to accurately detect the (intrinsic) dimensionality of data or effectively reduce the dimensionality of data. The median-based binarisation and SVM-based outof-sample extension both work perfectly incurring little approximation errors.
The proposed STH approach (using binarised-LapEig and linear-SVM) to semantic hashing is pretty fast: on an ordinary PC with Intel Pentium4 3.00GHz CPU and 2GB RAM, our Matlab implementation of 64-bit STH takes approximately 0.0165 second per document for training (which is about 10 times faster than SpH), and 0.0007 second per document for prediction.
5. CONCLUSIONS
The main contribution of this paper is a novel Self-Taught Hashing (STH) approach to semantic hashing for fast similarity search. By decomposing the problem of finding small codes for large data into two stages -- unsupervised learning and supervised learning -- we achieve great flexibility in choosing learning algorithms. Using binarised-LapEig for the first stage and linear-SVM for the second stage, STH significantly outperforms binarised-LSI, LCH, and the state-ofthe-art technique SpH [48]. Since STH is a general learning framework, it is promising to achieve even higher effectiveness and efficiency if more powerful unsupervised or supervised learning algorithms can be employed.
We shall apply this technique to text mining tasks (such as automated text categorisation [37]) and content-based multimedia retrieval [28] in the near future. It would also be interesting to combine semantic hashing and distributed computing (e.g., [29]) to further improve the speed and scalability of similarity search.
Acknowledgements
We are grateful to Dr Xi Chen (Alberta) for his valuable discussion and the London Mathematical Society (LMS) for their support of this work (SC7-09/10-6). We would also like to thank the anonymous reviewers for their helpful comments.
6. REFERENCES
[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 459­468, Berkeley, CA, USA, 2006.
[2] S. Baluja and M. Covell. Learning to hash: Forgiving hash functions and applications. Data Mining and Knowledge Discovery (DMKD), 17(3):402­430, 2008.
[3] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373­1396, 2003.
[4] S. Belongie, C. Fowlkes, F. Chung, and J. Malik. Spectral partitioning with indefinite kernels using the nystrom extension. In Proceedings of the 7th European Conference on Computer Vision (ECCV), pages 531­542, Copenhagen, Denmark, 2002.
[5] M. W. Berry, S. T. Dumais, and G. W. O'Brien. Using linear algebra for intelligent information retrieval. SIAM Review, 37(4):573­595, 1995.

23

F1 measure

F1 measure

0.8

d<=0

0.7

d<=1

0.6

d<=2 d<=3

0.5

0.4

0.3

0.2

0.1

0

10

20

30

40

50

60

code length

(a) Reuters21578

F1 measure

0.8

d<=0

0.7

d<=1

0.6

d<=2 d<=3

0.5

0.4

0.3

0.2

0.1

0

10

20

30

40

50

60

code length

(b) 20Newsgroups

F1 measure

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 10

20

30

40

code length

(c) TDT2

d<=0 d<=1 d<=2 d<=3

50

60

Figure 2: The F1 measure of STH for retrieving original nearest neighbours.

0.8

d<=0

0.7

d<=1

0.6

d<=2 d<=3

0.5

0.4

0.3

0.2

0.1

0

10

20

30

40

50

60

code length

(a) Reuters21578

F1 measure

0.8

d<=0

0.7

d<=1

0.6

d<=2 d<=3

0.5

0.4

0.3

0.2

0.1

0

10

20

30

40

50

60

code length

(b) 20Newsgroups

F1 measure

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 10

20

30

40

code length

(c) TDT2

d<=0 d<=1 d<=2 d<=3

50

60

Figure 3: The F1 measure of STH for retrieving same-topic documents.

1

LSI

LCH

0.8

SpH

STH

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

(a) Reuters21578

precision

1

LSI

LCH

0.8

SpH

STH

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

(b) 20Newsgroups

precision

1

LSI

LCH

0.8

SpH

STH

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

(c) TDT2

Figure 4: The precision-recall curve for retrieving original nearest neighbours.

1

LSI

LCH

0.8

SpH

STH

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

(a) Reuters21578

precision

1

LSI

LCH

0.8

SpH

STH

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

(b) 20Newsgroups

precision

1

LSI

LCH

0.8

SpH

STH

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

(c) TDT2

Figure 5: The precision-recall curve for retrieving same-topic documents.

precision

precision

24

[6] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[7] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. MIT Press and McGraw-Hill, 2nd edition, 2001.
[8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society of Information Science (JASIS), 41(6):391­407, 1990.
[9] P. Drineas and M. W. Mahoney. On the nystrom method for approximating a gram matrix for improved kernel-based learning. Journal of Machine Learning Research (JMLR), 6:2153­2175, 2005.
[10] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms and representations for text categorization. In Proceedings of the 7th ACM International Conference on Information and Knowledge Management (CIKM), pages 148­155, Bethesda, MD, 1998.
[11] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871­1874, 2008.
[12] G. H. Golub and C. F. V. Loan. Matrix Computations. The Johns Hopkins University Press, 3rd edition, 1996.
[13] L. W. Hagen and A. B. Kahng. New spectral methods for ratio cut partitioning and clustering. IEEE Transactions on CAD of Integrated Circuits and Systems, 11(9):1074­1085, 1992.
[14] A. Y. Halevy, P. Norvig, and F. Pereira. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2):8­12, 2009.
[15] J. Hays and A. A. Efros. Scene completion using millions of photographs. ACM Transactions on Graphics (TOG), 26(3):4, 2007.
[16] X. He, D. Cai, H. Liu, and W.-Y. Ma. Locality preserving indexing for document representation. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 96­103, Sheffield, UK, 2004.
[17] X. He and P. Niyogi. Locality preserving projections. In Advances in Neural Information Processing Systems (NIPS), volume 16, pages 153­160, Vancouver and Whistler, Canada, 2003.
[18] M. R. Henzinger. Finding near-duplicate web pages: A large-scale evaluation of algorithms. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 284­291, Seattle, WA, USA, 2006.
[19] G. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527­1554, 2006.
[20] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504­507, July 2006.
[21] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 408­415, Helsinki, Finland, 2008.
[22] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning (ECML), pages 137­142, Chemnitz, Germany, 1998.
[23] T. Joachims. Learning to Classify Text using Support Vector Machines. Kluwer, 2002.
[24] T. Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 217­226, Philadelphia, PA, 2006.
[25] D. Knuth. The Art of Computer Programming. Addison-Wesley, 3rd edition, 1997.
[26] Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 426­434, Las Vegas, NV, USA, 2008.
[27] K. Lang. Newsweeder: Learning to filter netnews. In Proceedings of the 12th International Conference on Machine Learning (ICML), pages 331­339, Tahoe City, CA, 1995.
[28] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain. Content-based multimedia information retrieval: State of the art and challenges. ACM Transactions on Multimedia Computing,

Communications, and Applications (TOMCCAP), 2(1):1­19, 2006.
[29] J. Lin. Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 155­162, Boston, MA, USA, 2009.
[30] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[31] T. Mitchell. Machine Learning. McGraw Hill, international edition, 1997.
[32] S. Pandey, A. Broder, F. Chierichetti, V. Josifovski, R. Kumar, and S. Vassilvitskii. Nearest-neighbor caching for content-match applications. In Proceedings of the 18th International Conference on World Wide Web (WWW), pages 441­450, Madrid, Spain, 2009.
[33] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: Transfer learning from unlabeled data. In Proceedings of the 24th International Conference on Machine Learning (ICML), pages 759­766, Corvalis, OR, USA, 2007.
[34] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning (IJAR), 50(7):969­978, 2009.
[35] R. E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear Estimation and Classification. Springer, 2003.
[36] B. Scholkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[37] F. Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1­47, 2002.
[38] G. Shakhnarovich, P. A. Viola, and T. Darrell. Fast pose estimation with parameter-sensitive hashing. In Proceedings of the 9th IEEE International Conference on Computer Vision (ICCV), pages 750­759, Nice, France, 2003.
[39] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27:379­423, 623­656, 1948.
[40] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 22(8):888­905, 2000.
[41] Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, and S. Vishwanathan. Hash kernels for structured data. Journal of Machine Learning Research (JMLR), 10:2615­2637, 2009.
[42] B. Stein. Principles of hash-based text retrieval. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 527­534, Amsterdam, The Netherlands, 2007.
[43] B. Stein, S. M. zu Eissen, and M. Potthast. Strategies for retrieving plagiarized documents. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 825­826, Amsterdam, The Netherlands, 2007.
[44] A. B. Torralba, R. Fergus, and Y. Weiss. Small codes and large image databases for recognition. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1­8, Anchorage, AK, USA, 2008.
[45] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In Proceedings of 24th International Conference on Very Large Data Bases (VLDB), pages 194­205, New York City, USA, 1998.
[46] P. Wegner. A technique for counting ones in a binary computer. Communications of the ACM (CACM), 3(5):322, 1960.
[47] K. Q. Weinberger, A. Dasgupta, J. Langford, A. J. Smola, and J. Attenberg. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), page 140, Montreal, Quebec, Canada, 2009.
[48] Y. Weiss, A. B. Torralba, and R. Fergus. Spectral hashing. In Advances in Neural Information Processing Systems (NIPS), volume 21, pages 1753­1760, Vancouver, Canada, 2008.
[49] Y. Yang and X. Liu. A re-examination of text categorization methods. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 42­49, Berkeley, CA, 1999.
[50] D. Zhang, J. Wang, D. Cai, and J. Lu. Laplacian co-hashing of terms and documents. In Proceedings of the 32nd European Conference on IR Research (ECIR), page 577­580, Milton Keynes, UK, 2010.

25

Social Media Recommendation based on People and Tags
Ido Guy, Naama Zwerdling, Inbal Ronen, David Carmel, Erel Uziel
IBM Research Lab Haifa 31905, Israel
{ido,naamaz,inbal,carmel,erelu}@il.ibm.com

ABSTRACT
We study personalized item recommendation within an enterprise social media application suite that includes blogs, bookmarks, communities, wikis, and shared files. Recommendations are based on two of the core elements of social media­--people and tags. Relationship information among people, tags, and items, is collected and aggregated across different sources within the enterprise. Based on these aggregated relationships, the system recommends items related to people and tags that are related to the user. Each recommended item is accompanied by an explanation that includes the people and tags that led to its recommendation, as well as their relationships with the user and the item. We evaluated our recommender system through an extensive user study. Results show a significantly better interest ratio for the tag-based recommender than for the people-based recommender, and an even better performance for a combined recommender. Tags applied on the user by other people are found to be highly effective in representing that user's topics of interest.
Categories and Subject Descriptors: H.3.3
[Information Search and Retrieval]: information filtering
General Terms: Algorithms, Experimentation
Keywords: Personalization, Recommender Systems, Social
Media, Social Networks, Social Software, Collaborative Tagging
1. INTRODUCTION
Social media has been enjoying a great deal of success in recent years, with millions of users visiting sites like Facebook for social networking; Wordpress for blogging; Twitter for micro-blogging; Flickr and YouTube for photo and video sharing, respectively; Digg for social news reading; and Delicious for social bookmarking. These social media sites rely principally on their users to create and contribute content; to annotate others' content with tags, ratings, and comments; to form online relationships; and to join online communities.
As social media sites continue to proliferate, and their volumes of content keep growing, users are having more difficulty choosing sites in which to become actively involved. Furthermore, users are "flooded" with information from feed readers, news alert systems,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

and many other resources. Easy access to so much information along with difficulty in judging the validity of so much content can lead to information overload, i.e., having more information available than a user can readily assimilate. Social media sites are increasingly challenged to attract new users and retain existing ones, due to these same factors.
One way site address these issues is by providing users with personalized recommendations. As in traditional taste-related domains or e-commerce (movies, books, hotels), the goal of a personalized recommender system is to adapt the content based on characteristics of the individual users. Social media and personalized recommender systems can mutually benefit from one another: on the one hand, social media introduces new types of public data and metadata, such as tags, ratings, comments, and explicit people relationships, which can be utilized to enhance recommendations; on the other hand, recommender technologies can play a key role in the success of social media applications and the social web as a whole, ensuring that each user is presented with the most attractive and relevant content, on a personal level.
In recent years, quite a few personalized recommendation services for social media have emerged. For instance, StumbleUpon1 is a personalized recommender engine that suggests web pages based on a user's past ratings, ratings by friends, ratings by users with similar interests, and topics of interest selected by the user from a list of nearly 500 subjects. More recently, some of the leading social media sites have also added personalized recommendation features: video-sharing site YouTube has launched a personalized homepage that includes recommendations based on past views and favorites. This feature is reported to have led to an increase in the number of users visiting the homepage, the frequency of visits, and the number of subscriptions users make over time [25]. Social news aggregator service Digg has added a personalized recommender engine for presenting stories presumed to be most interesting to a user, based on preferences of similar users [24].
Following the proliferation of social media sites on the web, analogous sites have emerged within organizations, gaining popularity as well [8]. Similarly to their counterparts on the web, enterprise social media sites also face challenges stemming from a continuously growing number of applications and the expanding volumes of information within them [8,11].
1.1 Contribution
In this work, we study personalized recommendation of social media items within an enterprise social software application suite, Lotus Connections (LC) [18]. LC consists of various types of social media applications, including social bookmarking, file
1 www.stumbleupon.com

194

sharing, blogging, communities, and wikis. Our recommender suggests items across the different applications based on two of the main characteristics of social media--people and tags.
In a previous work, we studied the recommendation of social media items based purely on related people [17]. We showed that items that are strongly related to people in a user's social network are likely to interest that user. Our hypothesis in this work is that recommending items related to a user's tags can also increase the quality of recommendation. Such a combination may be viewed as a social media variation of a traditional hybrid recommender that has been proven to be effective in taste-related domains [4].
Previous work has suggested tag-based recommendations, highlighting the value of tags as concise and accurate content descriptors that take into account human perception of the content [22,29]. User-tag relationships have been inferred through direct usage of tags or through indirect links, such as tags applied to resources rated positively by a user or those that were clickedthrough by a user. In this work, we only use information that is already publicly available and that does not require any explicit input, such as rating. We do not use any private information, such as click-through rates or query logs. We evaluate three methods to extract user-tag relationships based on public information: (1) direct usage of tags across the different LC applications ("used tags"); (2) indirect link between a user and a tag through an item, e.g., tags related to documents that are related to the user ("indirect tags"); and (3) tags applied to the user by others, within a people-tagging feature that allows users to tag one another [9] ("incoming tags"). To the best of our knowledge, our study is the first to suggest using incoming people tags to recommend content.
Our recommender engine is based on the social aggregation system SaND [5,27], which aggregates relationships among people, items, and tags, across the different LC components. SaND is used to extract, for each user, weighted lists of related people and related tags that constitute the user's personal profile. In addition, SaND provides weighted lists of items related to given people and/or tags. Ultimately, the system recommends to the user items that are related to people and tags within his personal profile. For each recommended item, two-level explanations illustrate why the item is recommended. On the first level, the related people and/or tags that yielded the recommended items are presented. On the second level, by hovering over the name of a specific person or a tag, the user may see its relationship to the recommended item and to himself as inferred by SaND.
Our approach has several advantages: (1) users are not required to provide explicit input to the system, e.g., by rating a set of items (we infer both their social relationships and topics of interest from other online information); (2) coping with the cold start problem of new users [28], as SaND allows aggregation of data which is external to LC (see [11]); (3) transparency [31]--intuitive explanations can be provided based on public tags and social relations; (4) performance--our recommendations are based on the rich aggregated index and do not require clustering or other computationally-intensive methods; and (5) generality--both people and tags can be used to recommend virtually any type of item, including music, photos, and videos.
While the SaND infrastructure has been used before for providing people-based recommendations, in this work we describe how it can be exploited to provide effective tag-based recommendations

as well. Furthermore, we present a novel approach for a hybrid recommender based on people and tags that leverages the unified modeling of relationships among people, tags, and resources. Another benefit of this approach is a uniform presentation of "hybrid explanations" based on both people and tags.
1.2 Evaluation
Our evaluation aims at comparing five types of recommenders: a people-based recommender (PBR); a tags-based recommender (TBR); two types of a hybrid recommender (PTBR): a combination of people or tags (or-PTBR), and a combination of people and tags (and-PTBR, suggesting only items related to both people and tags); and a popularity-based recommender (POPBR), as a benchmark. To the best of our knowledge, this is the first comprehensive study to compare people-based recommenders with tag-based recommenders and their hybridizations.
Our evaluation involves the following elements: (1) an offline comparison of the recommended items yielded by the five recommenders over 1,410 LC users, to examine the diversity across the recommenders, and in particular to compare the items stemming from related people with the items stemming from related tags; (2) a user survey with 65 participants who were asked to evaluate tags as indicators of topics of interest, based on four different methods: indirect tags, used tags, incoming tags, and a combination of both used and incoming tags; (3) the main element of our evaluation is a survey of over 400 LC users, who were randomly divided into five groups, receiving recommendations based on the five recommenders. All groups received recommendations in two phases--without explanations and with explanations. Participants were asked to provide feedback on their interest in the recommended items.
Our primary results show that the combination of incoming tags and used tags is the most effective in representing a user's topics of interest, with users rating nearly 70% of the topics as very interesting. Recommendations based on a TBR, with a tag profile that combines incoming and used tags, are rated significantly more interesting than the most effective PBR studied in our previous work. Recommended items are shown to be highly different between the PBR and the TBR, with less than 2% overlap. A hybrid PTBR recommender including explanations improves the results slightly further, leading to an over 70:30 ratio between interesting and non-interesting items. It also presents other potential benefits over a TBR, such as a lower percentage of already known items and higher diversity of item types.
In the next section, we discuss how existing work relates to our research. We then present our recommender system, followed by a detailed description of our experiments and their results. We conclude by discussing our findings and suggesting future work.
2. RELATED WORK
There are two prevalent approaches for building recommender systems: content-based (CB) [26] and collaborative filtering (CF) [13]. The CB approach is based on recommending items that are similar to those in which the user has shown interest in the past. The CF approach, on the other hand, recommends items to the user based on other individuals who are found to have similar preferences or tastes. Traditionally, both CB and CF systems have been based on explicit input from the user, usually provided by rating a set of items. To avoid this extra burden on the user,

195

leveraging implicit interest indicators [6], such as purchase history, views, clicks, or queries, has recently become more popular in recommender systems.
With the current prosperity of social media in general, and of social network sites (SNSs) in particular, several studies have suggested incorporating direct social relationships in CF systems. ReferralWeb [19] was one of the first systems to suggest the combination of direct social relations and CF to enhance searching for documents and people. Several studies suggest incorporating explicit social network information in CF systems to improve the quality of recommendation in domains such as movies and books (e.g., [3,12,30]), music [20], clubs [14], and news stories [21]. In this work, we infer social relationships from many different data sources, such as an enterprise SNS, a wiki system, and an organizational chart. Previous work has shown the value of aggregating social network information in yielding a richer and more accurate social graph [15].
On the other hand, as tagging has emerged as a popular way to let users annotate social media content, several works propose using tags as content descriptors for CB systems. Li et al. [22] analyze data from the social bookmarking site Delicious and find a high similarity between the tag vector of a URL and its keyword vector, as extracted from the corresponding web page. Firan et al. [10] study personalized recommendation of tracks within the popular music portal Last.Fm, and show that tag-based profiles can produce better recommendations than conventional ones based on track usage. Vatturi el al. [32] study personalized bookmark recommendation using a CB approach that leverages tags, assuming that users would be interested in pages annotated with tags similar to ones they have already used. Sen et al. [29] introduce Tagommenders--recommender algorithms that extend existing CB techniques by making use of tags. Their evaluation is based on the MovieLens system, and findings indicate that tagbased algorithms generate better recommendation rankings than state-of-the-art CF-based algorithms. The value in generating intuitive explanations through tags is highlighted in another MovieLens study by the same authors [33]. Our own tag-based approach is based on aggregating tags across various social media systems and considering both tags used by the user as well as tags with which the user has been tagged.
In this paper, we use the combination of related people and related tags to recommend social media items. Our system can be viewed as a variation of a hybrid CF-CB recommender system, in which related people and tags are used analogously to traditional CF and CB systems, respectively. Some research suggests combining traditional CF and CB systems, mostly in taste-related domains (see [4] for a summary). In particular, several studies point to the value of hybridizing CF and CB over each of the pure methods on its own. For example, Fab [2], a hybrid recommender system for web pages, is one of the first systems that combined CB and CF, suggesting that such a combination may eliminate many of the weaknesses found in each approach when individually applied. Claypool et al. [7] present a new filtering approach that combines the "coverage and speed" of CB filters with the "depth" of CF, and provides personalized filtering of an online newspaper. Melville et al. [23] present a hybrid recommender approach-- Content-Boosted Collaborative Filtering (CBCF), which uses a CB predictor to enhance existing user data, and then provides personalized suggestions through CF.

Evaluation is based on a movie rating dataset and indicates that CBCF performs better than pure CB or pure CF. The hybrid recommender presented in this work is based on implicit interest indicators and does not require explicit ratings by users, as most of the previous work. The unique hybridization algorithm is based on a unified index [1], which allows integrated retrieval of recommended items based on both people and tags.
3. RECOMMENDER SYSTEM
3.1 Social Media Platform
Our research platform for personal recommendation is Lotus Connections (LC) [18]--a social software application suite for organizations. It includes seven social media applications: profiles (of all employees), activities, bookmarks, blogs, communities, files, and wikis. We focus on recommending items of the last five applications, disregarding the first two, since profiles pose a different challenge regarding people recommendation [16], and an activity is generally restricted to a limited number of users. In our work, recommended items may originate from one of the following five applications, which are part of LC's deployment within our organization: (1) social bookmarking application, which allows users to store and tag their favorite web pages. It includes 900K bookmarks with 2M tags by 21K users; (2) blogging service that contains 7.5K public blogs, 130K entries, 350K tags and 17K users; (3) online community system that contains 6K public communities, each with shared resources (such as feeds and discussion forums), with a total of 174K members and 19.5K tags; (4) system for file sharing with 15K public files (presentations, photos, articles, etc.), 24K tags, and 8K users; and (5) wiki system with 3K public wikis including 20K pages edited by 5K users, and with 10K tags.
3.2 Relationship Aggregation
SaND [5,27] is an aggregation system that models relationships among people, items, and tags, through data collected across the enterprise, and in particular across all LC applications. SaND aggregates any kind of relationships between its three core entities--people, items, and tags. The implementation of SaND is based on a unified approach [1], in which all entities are searchable and retrievable. As part of its analysis, SaND builds an entity-entity relationship matrix that maps a given entity to all related entities, weighted according to their respective relationship strengths. The entity-entity relationship strength is composed of two types of relations:
· Direct Relations: Figure 1 shows all direct relations among entities that are modeled by SaND. Particularly, a user is directly related to: (1) another person: as a friend, as a tagger of or tagged by that person, or through the organizational chart (direct manager or employee); (2) an item (e.g., a shared file or a community): as an author, a commenter, a tagger, or a member; or (3) a tag: when used by the user or applied on the user by others. In addition, an item is directly related to a tag if it has been tagged with it. SaND does not currently model any direct tag-tag and item-item relations.
· Indirect Relations: Two entities are indirectly related if both are directly related to another common entity. For example, two users are indirectly related if both are related to the same user, e.g., if both have the same manager or friend, or if both have tagged or were tagged by the same person.

196

Figure 1. Direct entity-entity relations in SaND.
3.3 User Profile
The user profile, P(u), is given as an input to the recommender engine once the user u logs into the system. The profile is used to personalize the recommended items for u. It consists of 30 related people, N(u), and 30 related tags, T(u), retrieved through SaND, as explained in the paragraphs below.
The set of people related to the user is extracted by considering both direct and indirect people-people relations, scoring them, and aggregating them into a single person-person relationship strength, in the same way as was performed in previous studies ([16,17]). In principle, each direct relation adds a score of 1 to the overall relationship score, while an indirect relation adds a score in the range of (0,1], determined by various parameters, such as the number of common files or number of other wiki co-authors. More details on person-person score calculation can be found in [15,16,17].
Our previous work on purely people-based recommendation [17] distinguished between familiarity relationships (people the user knows) and similarity relationships (people whose social activity overlaps with the user's social activity). Familiarity relationships include all direct people-people relations, as well as two types of indirect relations: co-authorship (e.g., of a file or a wiki), and having the same manager. Similarity relationships include indirect relations only, such as co-usage of the same tag, co-tagging of the same item, co-commenting on the same blog entry, or comembership in the same community. Findings of that work have indicated that familiarity relationships are more effective in yielding interesting recommended items, yet similarity relationships are also productive and may diversify the recommended items. Based on our previous work's conclusions, all similarity relationships are multiplied by a factor of 1/3, so that familiarity relationships are favored, yet do not completely prevail. The user's set of related people is ultimately determined by retrieving the 30 related people who are found to have the highest relationship strength with the user, as done in [17].
To extract the user's related tags, we consider the following usertag relations: (1) used tags--direct relation based on tags the user has used; (2) incoming tags--direct relation based on tags applied on the user by others; and (3) indirect tags--indirect relation based on tags applied on items related to the user (note that this subsumes relation 1). We conducted a user survey to evaluate the quality of these tags as indicators for the user's topics of interest. Results of this evaluation are used to configure SaND to return the 30 tags that are most strongly related to the user's topics. The survey results are described in more detail in Section 4.1.

3.4 Recommendation Algorithm
Given the user profile, P(u) = (N(u),T(u)), we suggest items to the user that are related to people and/or tags in his profile. The recommendation score of item i for user u is determined by:
 RS (u, i) = e -d (i) [ w(u, v)  w(v, i) vN (u ) + (1 -  )  w(u,t)  w(t,i)] tT (u )
where d(i) is the number of days since the creation date of i;  is a decay factor (set in our experiments to 0.025, as in [17]);  is a parameter that controls the relative weight between people and tags, and is used in our experiments to evaluate different recommenders; w(u,v) and w(u,t) are the relationship strengths of u to user v and tag t, as given by the user profile; w(v,i) and w(t,i) are the relationship strengths between v and t, respectively, to item i, as determined by SaND, based on direct relations as described in Figure 1. User-item direct relation types are weighted as in previous studies [1,5,17]: authorship (0.6), membership (0.4), commenting (0.3), and tagging (0.3). Tag-item relations are weighted relative to the number of users who applied the tag on the item, normalized by the overall popularity of the tag, as in [1]. Ultimately, the recommendation score of an item, reflecting its likelihood to be recommended to the user, may increase due to the following factors: more people and/or tags within the user's profile are related to the item; stronger relationships of these people and/or tags to the user; stronger relationships of these people and/or tags to the item; and freshness of the item. We exclude items that are found to be directly related to the user. For example, we will not recommend an item on which the user has already commented or has already tagged.
3.5 Recommender Widget
Figure 2 depicts our UI widget for item recommendations based on the algorithm described in the previous section. The user is presented with a number of items (three, in this example) that may include a mix of the five LC item types. Each item has a title that links to the original document, and a short description when available. The icon to the left of each item represents its type-- the first item in Figure 2 is a blog entry, the second is a community, and the third is a wiki.
Figure 2. Item Recommendation Widget.

197

Each item includes a list of up to five related person names and/or up to five related tags that yielded this item's recommendation. The related people and tags serve as a first level explanation of why the item is recommended. On the second level, when hovering over a person's name or a tag, the user is presented with a popup detailing the relations of the person/tag to the user and to the item. In Figure 2, the popup indicates that Inbal is a member of the recommended community, and is also related to the user through several detailed direct and indirect relations. In the case of hovering over a tag, the popup indicates whether the user has used the tag, was tagged by the tag, or both.

4. EVALUATION

4.1 Tag Profile Survey
As a first step of our evaluation we set out to explore how to effectively build a user's tag profile based on the information represented in SaND. As described in the previous section, we examine three types of user-tag relations: used tags, indirect tags, and incoming tags. While the first two types have been used in previous studies around tag-based personalization, to the best of our knowledge, this is the first study that examines incoming tags for personalized content recommendation.
Our evaluation is based on a user survey sent to 200 LC users with at least 30 used tags and 30 incoming tags. User-related topics were assumed to be represented by tags associated with the user through four types of user-tag relations: (1) used tags; (2) incoming tags; (3) indirect tags; and (4) direct tags. The last group considers both types of direct relations (used tags and incoming tags) as retrieved through SaND. We extracted the user's four top related tags based on each of the relation types and randomized their order. Overall, we produced up to 16 tags for each of the participants, for which they were asked to indicate their level of interest, according to following three options: "Not Interested", "Interested", and "Highly Interested". We sent invitations to the survey by email, and received responses from 65 users, who rated a total of 1,037 tags.
Table 1. Rating results of tags as topics of interest

% used incoming direct indirect

Not Interested 16.84 15.48 7.46 35.38

Interested 38.25 31.75 22.81 45.38

Highly Interested 44.91 52.78 69.74 19.23

Table 1 shows the rating results of the tags as topics of interest for each of the four relation types. Direct tags clearly yield the most interesting topics--nearly 70% are rated as highly interesting and only 7.5% are rated not interesting. Incoming tags are slightly more effective in representing topics of interest than used tags, while indirect tags are evidently the least effective, with only 19% rated as highly interesting. One-way ANOVA indicates that ratings across the four types are significantly different (F(3,1068)=51.89, p<.0001). Tukey post-hoc comparisons of the four types indicate that direct tags are rated significantly higher than the rest of the types, indirect tags are rated significantly lower than the rest, and that the difference in interest levels between incoming and used tags is not significant.

Due to these results we opted to use the direct user-tag relation for retrieving the user's tag profile. We did not further weight

incoming vs. used tags, as the differences between them in the survey were not statistically significant. Consequently, we used SaND's indirect relations only for retrieving the list of people related to a user (as has been shown useful by a previous study [15]).
4.2 Recommended Items Survey
4.2.1 Methodology
The main part of our evaluation is based on an extensive user survey, designed to compare the people-based recommender (PBR), the tag-based recommender (TBR), and two combinations of these two recommenders (PTBRs). Participants of the survey were asked to evaluate 16 recommended items in two randomly ordered phases (each phase included eight items): with and without explanations. Each participant was assigned to one of five groups in a round-robin order, receiving recommendations based on one of the following five recommenders: (1) PBR (=1 in the equation in Section 3.4); (2) TBR (=0); (3) or-PTBR--each item may be recommended due to related people, related tags, or both (=0.5); (4) and-PTBR--each item is recommended due to at least one person and at least one tag in the user's profile (=0.5 with the constraint that both parts of the summation in brackets are nonzero); and (5) POPBR--popular item recommendation (as a benchmark). The popularity of items was determined based on the number of people they were directly related to in SaND, and on the items' freshness. For explanations, we pointed out the types and numbers of the different direct relations with people as well as the last-update date. For example, an explanation for a popular item would be: "tagged by 57, commented by 12, last updated Jan. 17th, 2010". Recommended items in each of the two phases were presented using the widget described in Figure 2, allowing to rate them as "Very Interesting", "Interesting", "I already know this", or "Not Interesting".
Our target population for the survey consisted of 1,410 LC users who were directly related to at least 30 other people, 30 tags, and 30 items. We note that this group does not represent the entire population of our organization, but rather active users of the LC system, who are the target population for our recommender system. A link to the survey with an invitation to participate was sent to each of these 1,410 individuals. In addition, we ran the five recommenders for each of these users to retrieve the top 16 items, and calculated average overlap between the items returned from the different recommenders. The average overlap across the 1,410 users between the items returned by the PBR and the TBR was 1.58%, indicating that these two recommenders return very dissimilar items. The POPBR had very low overlap with all other recommenders, ranging from 0.87% to 1.83%. Overlap between the two PTBRs was 38.6%. The or-PTBR had higher overlap with the PBR (57.3%) and the TBR (32.6%) than the and-PTBR (24.1% and 9.7%, respectively). This indicates that the or-PTBR recommends mostly items that are either recommended by the PBR or the TBR, while the and-PTBR recommends more items that are further down the list of the PBR and the TBR.
4.2.2 Results
In total, 412 participants completed our survey, originating from 31 countries and spanning the different organizational units: 32% sales, 28% software, 18% services, 11% headquarters, 4% research, 4% systems, and 3% others.

198

Table 2 summarizes the rating results of the survey for each of the five recommenders, with and without explanations. The rightmost column displays the interest ratio--the ratio between interesting (including very interesting) and non-interesting items. The best ratio is achieved by the or-PTBR with explanations. One-way ANOVA indicates that ratings across the five recommenders are significantly different (F(4,5496)=66.823, p<.0001). Tukey posthoc comparisons indicate that differences between the POPBR and the other four recommenders, as well as between the PBR and the other four recommenders, are significant, while differences among the TBR, and-PTBR, and or-PTBR are not significant.

Table 2. Item rating results across the five recommenders

Rec.

Expl.

Not Int.

Int.

Very Alrd. Int. Int. Know Ratio

POPBR

no 52.72 30.95 11.05 5.27 0.80

yes 58.12 29.96 7.58 4.33 0.65

no 37.70 35.25 18.03 9.02 1.41 PBR
yes 34.94 38.35 18.32 8.39 1.62

TBR

no 26.41 36.46 23.17 13.97 2.26

yes 26.97 35.13 21.14 16.76 2.09

no 29.87 36.95 21.07 12.11 1.94 or-PTBR
yes 26.10 40.00 23.39 10.51 2.43

no 31.41 37.50 21.28 9.70 1.87 and-PTBR
yes 25.90 37.52 24.02 12.56 2.38

4.2.2.1 Baseline ­ POPBR
We opted to use popular items as a benchmark, in order to examine whether personalized recommendations are "worth the effort", and add substantial value over a general non-personalized recommendation of the most popular items. Results show that all types of personalized recommenders significantly outperform the popularity-based recommender. Interestingly, when accompanied by explanations, popular items are rated slightly lower, possibly as the numbers indicating an item's popularity are not found to be a compelling justification by the participants.
4.2.2.2 PBR vs. TBR
Results for the PBR are consistent with the results from our previous work about people-based recommendation [17]. In this case, the explanations slightly increase the interest rate in recommended items, reinforcing the instant value of people-based explanations.
In this work, we suggest using tags to improve the quality of social media item recommendation and as described in Section 4.1, we leverage both used tags and incoming tags. Results for the TBR, as displayed in Table 2, reveal that tag-based recommendations significantly outperform people-based recommendations, both with and without explanations. When tags come into play, interest ratio jumps from around 1.5 to over 2. As opposed to the PBR, tag-based explanations did not instantly lead to more interest in items. In fact, items without explanations even led to a slightly higher interest ratio (2.26 vs. 2.09). This indicates that while a TBR outperforms a PBR, tag-based explanations are not as effective as people-based explanations in increasing interest in items. This may be due to the fact that related tags are already reflected, to some extent, in an item's title or description.

Figure 3 depicts aggregated rating results across the five recommenders, regardless of whether explanations were provided or not. The "All Interesting" bar includes items rated interesting and very interesting. It can be seen that the number of items rated non-interesting drops by almost 10% when moving from the PBR to the TBR. The percentage of already known items is also notable--it is the highest for the TBR (15.5%), the lowest for the POPBR (4.8%) and the next-to-lowest for the PBR (8.7%). Tukey post-hoc comparisons indicate that the percentage of already known items in the TBR is significantly higher than in any of the other recommenders (other differences among the recommenders are not significant). These results are understandable: tags yield items that are more similar to ones the user preferred in the past and are thus likely to be less diverse and surprising than peoplebased items. Diversity is a well known advantage of CF recommenders over CB ones [13]. Popular items are even less expected than people-based items, however, as mentioned before, their interest ratio is significantly lower. The high diversity of the PBR relative to the TBR is also reflected in the types of items each of them yielded: 80% of the items recommended by the TBR for the survey's participants were bookmarked web pages, while for PBR 32.6% were files, 29.3% communities, 22.3% bookmarks, 12.3% blogs, and 3.4% wikis.

55.3 39.8 4.8 36.3
55.0 8.7 26.7
57.8 15.5 28.6
60.2 11.2 28.1
60.6 11.3

70.0 60.0 50.0 40.0 30.0 20.0 10.0
0.0 POPBR

PBR

TBR

and-PTBR

or-PTBR

% Not Interesting % All Interesting % Already Know

Figure 3. Item rating results across the five recommenders, summing over both phases.
4.2.2.3 PTBR vs. TBR
Figure 3 indicates that both the or-PTBR and the and-PTBR produce the highest percentage of items rated interesting or very interesting--over 60%. Table 2 demonstrates that when explanations are included, both PTBRs also have the highest percentage of very interesting items, the lowest percentage of non-interesting items, and an overall interest ratio of over 70:30. However, the differences between the two PTBRs and the TBR are statistically insignificant. Moreover, when explanations are excluded, the TBR performs slightly better than both PTBRs. These differences between the two phases may be due to the effectiveness of the people-based explanations included in the PTBRs, but not in the TBR (as discussed before).
Figure 3 also shows that both PTBRs have a significantly lower percentage of already known items than the TBR, indicating that they produce less expected items while maintaining high interest ratios. Diversity of item types is also higher for the PTBRs, as compared to the TBR: only 44% of the or-PTBR and 51% of the and-PTBR items are bookmarks (compared to 80% for the TBR).

4.2.2.4 Item Type Diversity
42.5% of the recommended items in our survey (over all 412 participants) were bookmarked web pages, 26.9% were shared

199

files, 15.6% were communities, 10.2% were blog entries, and 4.8% were wikis. These differences may be ascribed to the fact that applications are different in various parameters, such as the level of usage within the organization, the frequency of item creation in the system (e.g., a bookmark is more frequently created than a wiki), and so on. Our recommendation algorithm does not explicitly consider the item type, and does not impose predefined item type diversity.

53.1 40.4 6.5 46.0 45.7 8.3 36.9
54.9 8.1 33.9
56.2 9.9 28.7
58.3 13.0

70.0 60.0 50.0 40.0 30.0 20.0 10.0
0.0 wikis

blogs

files

communities bookmarks

% Not Interesting % All Interesting % Already Know

Figure 4. Item rating results across the five types, summing over both phases.
Figure 4 depicts the rating results across the five item types. Bookmarks have the highest interest ratio, followed by communities and files. Ratings of these three types are significantly higher than those of blogs and wikis (using ANOVA with Tukey post hoc analysis). Note that the order of items in terms of proportion in the overall recommendations, as detailed in the previous paragraph, is very similar to their order in terms of interest ratio. Our recommenders suggest more items of types that are likely to be interesting, but also maintain some level of item type diversity. The percentage of already known items per type increases in accordance with the interest ratio, indicating a trade-off between accuracy and expectedness.

5. DISCUSSION AND FUTURE WORK
The results presented in the previous section indicate that using tags for social media recommendation can be highly beneficial. The combination of directly used tags and incoming tags produces an effective tag-based user profile. A TBR that makes use of this profile yields significantly more interesting recommendations than the most effective PBR presented in a previous work [17]. In addition, the items produced by the TBR are almost completely disjoint from the items produced by the PBR (less than 2% average overlap across the top 16 items), indicating that related tags produce very different recommendations as compared to related people. Combining both related tags and people in the user profile does not significantly increase the interest in recommended items over a pure tag-based approach; however, it significantly lowers the percentage of already known items, increases the diversity of item types, and makes explanations more effective.
The higher effectiveness of the TBR over the PBR may be attributed to the fact that tags are better filters for topics of interest than are people. People related to the user may broaden the scope of recommended items (and increase diversity), yet they are also likely to add irrelevant items, as they may have interest areas that are different from those of the user.
In our previous work on PBRs [17], some of the feedback we received highlighted the need for additional filtering based on topics

(since related people often have many different topics of interest). Indeed, PTBRs are found to perform significantly better than the optimal PBR. However, this improvement does not occur over the TBR, a finding that surprised us to some extent. We expected that adding people to tags as filters would significantly improve the recommendations (similarly to traditional hybrid recommender systems), yet the improvement was small. Our findings suggest that a TBR without explanations performs well, and can be used as a starting point, or in cases that require a simple social media recommender system.
We examine two PTBRs that combine related people and related tags in different manners, and produce fairly dissimilar recommended items (less than 40% mutual overlap). Yet, the differences in their performance are very small. Future studies may examine whether other methods for combining related people and related tags in user profiles can further enhance the recommender's performance.
In addition to the people- and tag-based recommenders, we also experimented with a non-personalized, popularity-based recommender. While the interest ratio of this recommender is significantly lower than all personalized recommenders, it has the potential to provide more unexpected recommendations, as reflected in its very low percentage of already known items. In a future work, we plan to examine whether and how a popularity recommender can be combined with the personalized recommenders, so that more unexpected items are suggested to the user, but not so often as to become an annoyance.
Integration of traditional CB methods within the recommender should also be explored and can be helpful in addressing two key issues that are acute in both TBRs and PBRs: (1) the cold start problem for new items, as these are not yet related to people or tags, and (2) language issues--items that users cannot understand might be accidentally recommended (e.g., when the tag's language is different than the language of the content).
Our recommender engine is based on the rich relationship data aggregated and modeled by SaND. The fact that we do not apply computationally-intensive algorithms over this data allows us to compare recommenders in a more direct way, provide intuitive explanations, and maintain generality. Future research should examine whether applying such algorithms can further improve the results presented in this work.
A future study is also required to validate the results of our experiments in a non-enterprise environment, where tags are used on a larger scale, related people are mostly personal friends rather than colleagues, and multiple identities must be managed.
We also plan to examine how to maintain high interest in recommended items over time. While the evaluation in this study is mostly based on rating an initial set of recommended items, maintaining that same level of interest for users who regularly access the system is more challenging. One approach we intend to explore, which could help overcome this challenge, is based on user feedback. The approach would address how to elicit such feedback, on what levels to allow it (an item, a person, a tag, etc.), and how to adapt the recommendations accordingly.
6. CONCLUSION
In this work, we propose a novel method for recommending social media items based on both related people and related tags. An

200

extensive experimentation is conducted to compare people-based and tag-based recommenders as well as their hybridizations. We show that a combination of directly used tags and tags applied by others is most effective in representing the user's topics of interest. A recommender based on this tag profile yields items that are significantly more interesting to the user than the most effective people-based recommender demonstrated in a previous work [17]. Combining related people and tags in the user profile improves the results slightly further, leading to a 70:30 ratio between interesting and non-interesting items when explanations are included. In addition, a hybrid people-tag-based recommender has other advantages, such as low proportion of expected items, high diversity of item types, richer explanations, and the simple fact that for some users, recommendations based on people work better, while for others, recommendations based on tags are more effective. Future work should thoroughly examine whether the results presented here can be further improved by means such as integration of other recommenders (e.g., content-based or popularity-based), execution of more sophisticated algorithms (e.g., clustering of people, tags, or items), or optimization of the parameters used by the recommender engine.
7. ACKNOWLEDGMENTS
We thank Sigalit Ur and Tal Daniel for designing and implementing the recommender widget. We are grateful to Shila Ofek-Koifman and Sivan Yogev for many useful discussions.
8. REFERENCES
[1] Amitay, E., Carmel, D., Har'el, N., Soffer, A., Golbandi, N., OfekKoifman, S., & Yogev, S. 2009. Social Search and Discovery using a Unified Approach. Proc. HYPERTEXT '09.
[2] Balabanovic, M. & Shoham, Y. 1997. Fab: Content-based, Collaborative Recommendation. Commun. ACM 40, 3 (Mar. 1997), 66-72.
[3] Bonhard, P. & Sasse, M. A. 2006. Knowing me, Knowing you Using Profiles and Social Networking to Improve Recommender Systems. BT Technology Journal 24, 3 (Jul. 2006), 84-98.
[4] Burke, R. 2002. Hybrid Recommender Systems: Survey and Experiments. User Modeling and User-Adapted Interaction 12, 4 (2002), 331-370.
[5] Carmel, D., Zwerdling, N., Guy I., Ofek-Koifman, S., Har'el N., Ronen, I., Uziel, E., Yogev, S., & Chernov, S. 2009. Personalized Social Search based on the User's Social Network. Proc. CIKM '09, 1227-1236.
[6] Claypool, M., Le, P., Wased, M., & Brown, D. 2001. Implicit Interest Indicators. Proc. IUI '01, 33­40.
[7] Claypool, M., Gokhale, A., Miranda, T., Murnikov, P., Netes, D., & Sartin, M. 1999. Combining Content-Based and Collaborative Filters in an Online Newspaper. Workshop on Recommender Systems, SIGIR '99.
[8] DiMicco, J., Millen, D. R., Geyer, W., Dugan, C., Brownholtz, B., & Muller, M. 2008. Motivations for Social Networking at Work. Proc. CSCW '08, 711-720.
[9] Farrell, S., & Lau T. 2006. Fringe Contacts: People Tagging for the Enterprise. Workshop on Collaborative Web Tagging, WWW '06.
[10] Firan, C. S., Nejdl, W., & Paiu, R. 2007. The Benefit of Using Tag-Based Profiles. Proc. LA-WEB '07, 32-41.
[11] Freyne J., Jacovi, M., Guy I., & Geyer W. 2009. Increasing Engagement through Early Recommender Intervention. Proc RecSys '09, 85-92.

[12] Golbeck J. 2006. Generating Predictive Movie Recommendations from Trust in Social Networks. Proc. 4th Int. Conf. on Trust Management. Pisa, Italy.
[13] Goldberg, D., Nichols, D., Oki, B. M., and Terry, D. 1992. Using Collaborative Filtering to Weave an Information Tapestry. Commun. ACM 35, 12 (Dec. 1992), 61-70.
[14] Groh, G., & Ehmig, C. 2007. Recommendations in Taste Related Domains: Collaborative Filtering vs. Social Filtering. Proc. GROUP '07, 127-136.
[15] Guy, I., Jacovi, M., Shahar, E., Meshulam, N., Soroka, V., & Farrell, S. 2008. Harvesting with SONAR: The Value of Aggregating Social Network Information. Proc. CHI '08, 10171026.
[16] Guy I., Ronen I., & Wilcox E. 2009. Do You Know? Recommending People to Invite into Your Social Network. Proc. IUI '09, 77-86.
[17] Guy, I., Zwerdling, N., Carmel, D., Ronen, I., Uziel, E., Yogev, S., & Ofek-Koifman, S. 2009. Personalized recommendation of social software items based on social relations. Proc. RecSys '09, 53-60.
[18] IBM Social Software for Business ­ Lotus Connections: http://www-01.ibm.com/software/lotus/products/connections/.
[19] Kautz, H., Selman, B., & Shah, M. 1997. ReferralWeb: Combining Social Networks and Collaborative Filtering. Commun. ACM 40, 3 (Mar. 1997) 63-65.
[20] Konstas, I., Stathopoulos, V., & Jose, J. M. 2009. On social networks and collaborative recommendation. Proc. SIGIR '09, 195-202.
[21] Lerman, K. 2007. Social Networks and Social Information Filtering on Digg. Proc. ICWSM '07.
[22] Li, X., Guo, L., & Zhao, Y. E. 2008. Tag-based Social Interest Discovery. Proc. WWW '08, 675-684.
[23] Melville, P., Mooney, R. J., & Nagarajan, R. 2002. Contentboosted collaborative filtering for improved recommendations. Proc. AAAI `02, 187-192.
[24] Official Digg Blog: http://blog.digg.com/?p=127.
[25] Official YouTube Blog: http://youtube-global.blogspot.com/2008/06/new-personalizedhomepage-and-improved.html
[26] Pazzani, M.J., & Billsus D. 2007. Content-based recommendation systems. The Adaptive Web, 325-341.
[27] Ronen, I., Shahar, E., Ur, S., Uziel, E., Yogev, S., Zwerdling, N., Carmel, D., Guy, I., Har'el, N., & Ofek-Koifman, S. 2009. Social networks and discovery in the enterprise (SaND). Proc. SIGIR '09, 836.
[28] Schein, A. I., Popescul, A., Ungar, L. H., & Pennock, D. M. 2002. Methods and Metrics for Cold-start Recommendations. Proc. SIGIR '02, 253-260.
[29] Sen, S., Vig, J., & Riedl, J. 2009. Tagommenders: Connecting Users to Items through Tags. Proc. WWW '09, 671-680.
[30] Sinha, R. & Swearingen, K. 2001. Comparing Recommendations Made by Online Systems and Friends. DELOS-NSF Workshop on Personalization and Recommender Systems in Digital Libraries.
[31] Sinha, R. & Swearingen, K. 2002. The Role of Transparency in Recommender Systems. CHI Extended Abstracts '02, 830-831.
[32] Vatturi, P. K., Geyer, W., Dugan, C., Muller, M., & Brownholtz, B. 2008. Tag-based filtering for personalized bookmark recommendations. Proc. CIKM '08, 1395-1396.
[33] Vig, J., Sen, S., & Riedl, J. 2009. Tagsplanations: Explaining Recommendations using Tags. Proc. IUI '09, 47-56.

201

Prototype Hierarchy Based Clustering for the Categorization and Navigation of Web Collections

Zhao-Yan Ming1,2, Kai Wang2 and Tat-Seng Chua2
1NUS Graduate School for Integrative Sciences and Engineering 2Department of Computer Science, School of Computing
National University of Singapore
{mingzy,kwang,chuats}@comp.nus.edu.sg

ABSTRACT
This paper presents a novel prototype hierarchy based clustering (PHC) framework for the organization of web collections. It solves simultaneously the problem of categorizing web collections and interpreting the clustering results for navigation. By utilizing prototype hierarchies and the underlying topic structures of the collections, PHC is modeled as a multi-criterion optimization problem based on minimizing the hierarchy evolution, maximizing category cohesiveness and inter-hierarchy structural and semantic resemblance. The flexible design of metrics enables PHC to be a general framework for applications in various domains. In the experiments on categorizing 4 collections of distinct domains, PHC achieves 30% improvement in F1 over the state-of-the-art techniques. Further experiments provide insights on performance variations with abstract and concrete domains, completeness of the prototype hierarchy, and effects of different combinations of optimization criteria.
Categories and Subject Descriptors
H.3.3 [ Information Storage and Retrieval]: Information Search and Retrieval--clustering
General Terms
Algorithms, Performance, Experimentation.
Keywords
Hierarchical Clustering, Prototype Hierarchy, Hierarchy Induction, Criterion Function
1. INTRODUCTION
With the flourishing of user contributed services like Yahoo! Answers, discovering the utility of user-generated-contents becomes a research topic of interest to many researchers. The utility of user-generated-contents comes in two major aspects, the quality and accessibility. Efforts have been put
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

to distinguish the good and bad quality content [1]. To make contents more accessible, state-of-the-art retrieval models like translation based language model [18] and syntactic tree matching [15] have achieved promising performance. Organizing the huge collections of data for information navigation is another important direction in exploring web collections. Categorization, especially hierarchical clustering with labels and descriptions of clusters, enables browsing style of information access. Users can navigate through the hierarchy driven by their information needs [11, 17].
Currently, web services rely on users to construct topic hierarchies and assign objects into their nodes. Open Directory Project (ODP) and Wikipedia are both examples of hierarchically organized web collections formed by community of editors. Yahoo! Answers (YA) is organized in a hierarchical tree containing 728 nodes with 26 top-level categories, relying on users to select a category for their postings. Besides the reliance on manual assignment, a hierarchy as large as YA's directory is too coarse to contain a category like IPod (it is in Music & Music players) whose subtopics might be of interest to many users. These suggest the necessity of automatic fine-grained hierarchical categorization.
Toward automatic categorization of web collections into hierarchies, supervised techniques that require manuallylabeled corpora are not appropriate for dynamic Web information services [9]. Existing unsupervised techniques generally focus either on clustering the collections into smaller groups [5, 17], or extracting labels for clustered groups [4]. SnakeT [6] is a successful hierarchical clustering engine that performs sequential clustering and labeling on snippets returned by search engines. However, the resulting clusters and labels may not be consistent and systematic because of its data-driven nature. LiveClassifier [9] addresses the categorization and navigation in one go by utilizing predefined topic hierarchies and searching the training instances to feed into a supervised learner. This approach, however, ignores the underlying topic structure of the target collection; and the result is confined to the predefined hierarchy which may not be a perfect match to the collection.
In this paper, we propose an unsupervised approach called Prototype Hierarchy based Clustering (PHC) to tackle the problem of web collection categorization and navigation. PHC utilizes the world knowledge in the form of prototype hierarchies, while adapts to the underlying topic structures of the collections. By following the structure of the prototype hierarchy, PHC eliminates the problem of determining the number of clusters and assigning initial clusters.
Moreover, the PHC results are interpretable, comprehen-

2

Figure 1: Categorizing a Yahoo! Answers dataset using the Prototype Hierarchy of IPhone
sive, and organized. Unlike in general clustering schemes where no label is provided for the resulting groups, or in conceptual clustering [12] where extra efforts are needed to extract labels, in PHC the labels or the descriptions of clusters are already provided by the prototype hierarchy used. Therefore the trouble of inventing cluster labels is eliminated. What's more, the labels provided by the supervision hierarchy are logically and systematically arranged, while the labels extracted from unsupervised clustering might be inconsistent and disorganized.
PHC allows flexible forms of supervision: the prototype hierarchy can come in different level of granularity, in different forms, and even tailored to any specific applications. Thanks to the diversity of web content, the hierarchy can even be automatically extracted. It is thus less rigid than the example-based learning and constraining.
In the rest of this paper, we first provide an overview of PHC in Section 3. Section 4 gives details of the PHC problem and algorithm. Section 5 presents experiments and results analysis. Related work is reviewed in Section 5, and with the conclusion in Section 6.
2. PROTOTYPE HIERARCHY BASED CLUSTERING
Generally, PHC takes in as input a prototype hierarchy and a target collection on the same topic, and produces as output a data hierarchy that contains all the data items from the collection. For ease of discussion, we first give some basic definitions and notations.
2.1 Preliminaries and Notations
Preliminary 1. A Hierarchy (H) is defined as a tree that consists of a set of uniquely labeled nodes V and a set of parent-child relations R between these nodes. A Concept Hierarchy (CH) is a hierarchy whose V represents a set of concepts , with each being used as a label for each V .
Definition 1. A Prototype Hierarchy (P H) is defined as a hierarchy whose nodes set V represents a set of < ,  > tuples, with  a Prototype serving as a typical example, description, or standard for the concept .
Definition 2. Data Hierarchy (DH) is a hierarchy that organizes a collection of objects d. Each node of DH represents a category of objects CO. Non-leaf nodes subsumes

their child nodes in a recursive manner. The root of DH consists of a single broadest category containing all objects, and the leaves correspond to the finest categories.
P H is the hierarchy that supervises the categorization process. It can be seen as a concept hierarchy CH with each
labeled node embodied by a prototype . With P H and DH, we define the problem of Prototype Hierarchy based Clustering (PHC) as follows. Given a collection D of objects on a topic  , PHC partitions and maps D into the categories that are predefined by a P H on  , such that the formed objects clusters CO1, CO2,..., COk are organized in a DH with similar structures. The output DH is readily labeled by the P H, and thus could be easily browsed by users to find information at different granularity.
2.2 An Example
Figure 1 illustrates how the prototype hierarchy based categorization works. Suppose the problem is to organize an archive of Yahoo! Answer questions on iPhone. Given the dataset of questions and the predefined prototype hierarchy as shown in Figure 1, PHC assigns each object (question) of the dataset into the leaf nodes of the hierarchy. For instance, question 1 is categorized into MobileMe and naturally becomes a member of Online Services. Note that the category IPhone:Software:Interface does not have a single object and question 7 has no appropriate category to assign to. These are two typical cases to be handled in the PHC algorithm. With all the objects being assigned, a data hierarchy that has the same structure with the prototype hierarchy is formed. A user may thus easily browse the organized dataset by navigating the prototype hierarchy and clicking on any node to view the questions from the corresponding node of the data hierarchy.
This example suggests that clustering of a dataset according to a supervision hierarchy is not trivial. We identify the following requirements in the study of this paper:
1. The data hierarchy, like a taxonomy or an ontology, is incrementally evolving into a compact structure encoding the underlying topics of the collection.
2. The data and prototype hierarchy are to be matched at both the node and relation level, while special techniques are needed to handle the mis-match between the data hierarchy and the prototype hierarchy.
3. The distance between objects are measured by appropriate metrics, so as to partition the objects into homogeneous clusters that are far apart from each other.
These requirements suggest the criteria in constructing a data hierarchy from a dataset. They form the basis of the PHC framework. In the following Section, we address the requirements in the first three subsections, and induce a multi-criterion optimization function in the last subsection.
3. PROBLEM FORMULATION
3.1 Data Hierarchy Structure Evolution
3.1.1 Hierarchy Metric and Information Function
To characterize the structure of a hierarchy, we introduce Hierarchy Metric and Information Function inspired by research in automatic taxonomy induction [19] and ontology

3

obj1 suggests that the optimal DH on a collection is the one that contains the least information. It makes intuitive sense that the DH that compactly "encodes" the collection into topic categories is the best.

Figure 2: Illustration of Prototype Hierarchy (i-iii) and Data Hierarchy (iv). The prototype hierarchy (i) is a full match of (iv), (ii) an incomplete match of (iv), and (iii) an excess match of (iv).

learning, due to the similar nature of hierarchy and taxonomy.
We define a hierarchy metric as a function that operates on all the nodes in a hierarchy, similar to ontology metric [19] on an ontology. Formally, it is a function h : V × V  R+, where V is the set of nodes in H. h(., .) is recursively defined.
For an adjacent pair of nodes vp and vq, the hierarchy metric is defined as the edge weight w(evpvq ). For the other pairs, the hierarchy metric h(., .) on H with edge weights w for any node pair vi, vj  V is the sum of all edge weights along the shortest path between the pair:

hH,w(vi, vj ) =

w(evpvq )

(1)

evpvq P (vi,vj )

where P (vi, vj) is the set of edges defining the shortest path from nodes vi to vj . The quality of the structure of a hierarchy is measured by the amount of information carried in H, defined as the sum of all hierarchy metrics in H:

Inf o(H) =

h(vi, vj )

(2)

i<j,vi,vj V

where i < j reduces duplicated entries of h(., .) since the hierarchy metric is a symmetric measure.
Figure 2 (i) gives an example of a 6-node hierarchy. We can calculate the hierarchy metric between A and F as h(A, F ) = h(A, C) + h(C, F ) = 2.8, and the Information Function of the hierarchy as the sum of 15 pairs of nodes, resulting in Inf o(H) = 38.5.

3.1.2 Objective Functions
Minimum Evolution(obj1) is designed to monitor the structural evolution of the data hierarchy. The data hierarchy is incrementally hosting more objects until the whole collection is categorized and allocated. We assume that DH(n+1) with n + 1 nodes to be the one that introduces the least changes of information from its previous status DH(n):
DH(n+1) = arg minDH ||Inf o(DH(n)) - Inf o(DH )|| (3)
Therefore the optimal DH organizes the whole collection so as to introduce the least information changes since the initial data hierarchy DH(0): D^H = arg minDH ||Inf o(DH(0))- Inf o(DH )||, where Inf o(DH(0)) = 0 since the initial DH is empty. By plugging in Equation 1 and 2, the minimum evolution objective function becomes:

minimize obj1 =

w(evpvq ) (4)

i<j,vi ,vj V evpvq P (i,j)

3.1.3 Data Hierarchy Centroid

The hierarchy metric and information function discussed

above are defined in "node space". For P H, the nodes are

represented by prototypes. For DH, we use centroid to rep-

resent a node of objects CO. Note that in previous work [7]

on classification, centroid and prototype are two equivalent

and interchangeable concepts. In this paper, we distinguish

the two concepts by emphasizing that prototype is knowl-

edge oriented and centroid is data oriented.

The centroids for DH nodes are generated in an incre-

mental manner. When the first object is categorized into

a category, it acts as the initial centroid of the category

(and its ancestor categories). With subsequent objects be-

ing inserted into the same category, the centroid is updated

incrementally upon its previous status.

Suppose by vectors

that the centroids and on the term space as

-tvhceenotbrjoeicdtsanadre -rde.prWeshenenteda

new object d is inserted into a node, its centroid is updated

by ta-king the and d , as (n

algorithmic -v (cnen) troid +

-average of all d )/(n + 1).

the

existing

objects

The new object in a leaf node automatically becomes

members of its ancestor nodes whose centroids are to be

updated too. We consider that the magnitude of the change

decreases with the levels from the leaf node. The updating

formula for a data hierarchy centroid is defined as

-v (cnen+t1r)oid

=

n

-v (cnen) troid

+

- g(t) d

n+1

(5)

where t is the number of edges on the shortest path be-
tween the updated node and its descendent leaf nodes, and g(t) is a monotonically decreasing updating coefficient. In the implementation, a heuristic function g(t) = 1 - t/|H| is utilized, |H| is the height of the data hierarchy H, and g(t)  (0.0, 1.0]. For example,when v is a leaf node, t = 0, and g(t) = 1.0.

3.2 Matching of Prototype Data Hierarchy
Preliminary 2. A full match between two hierarchies H1 and H2 is defined such that nodes V1 = V2 and relations R1 = R2. A partial match between H1 and H2 can be either an incomplete match or an excess match. When H1 is an incomplete match of H2, V1 + Vin = V2, R1 + Rin = R2, and the incomplete rate is defined as |Vin|/|V2|; when H1 is an excess match of H2, V1 = V2 + Ve, R1 = R2 + Re, and the excess rate is defined as |Ve|/|V2|. In the case of a partial match, the matched nodes and relations constitute a sub-hierarchy called a common hierarchy.

Figure 2(iv) shows a data hierarchy, and Figure 2(i-iii) show three prototype hierarchies that are respectively a full match, an incomplete match, and an excess match of the data hierarchy. The common hierarchies of Figure 2(iii) and (iv) are (A,B,C,D,E,F) and (a,b,c,d,e,f), and G is an excess node.
3.2.1 Objective Functions
Prototype Centrality (obj2) We assume that a prototype is located at the center of an object cluster in the object

4

space. Formally, the prototype centrality is expressed as

1

maximize obj2 = |V |

c(vpi , voi )

voi =

where |V | is the number of nodes in the hierarchy, vpi is represented by its prototype i, and voi is represented by its centroid. voi is updated with incrementally added new objects. c(., .) measures the similarity between a vpi and a voi that is not empty. For this study, c(., .) employs the simple and effective cosine similarity function on the term
vectors of vpi and voi . The maximization of the prototype centrality objective is
actually equivalent to adding a data object into a node, so
that the updated centroids (including the parental node cen-
troids), are most similar to their corresponding prototypes.
Prototype-Data Hierarchy Resemblance (obj3) considers the common part of the data hierarchy and the pro-
totype hierarchy. Formally, the prototype-data hierarchy
resemblance is defined as follows:

1 minimize obj3 = |M |

||h(vpi , vpj ) - h(voi , voj )|| (6)

i<j

where |M | is the size of the common hierarchy, vpi , vpj  Vp and voi , voj  Vo are the corresponding nodes of P H and DH respectively.
Since P H is predefined and static, it is usually a partial
match of DH. To enable DH to flexibly adjust to the col-
lection distribution by having more or less nodes, obj2 and obj3 are measured on the common hierarchy.

3.2.2 Considerations for Partially Matched Prototype Hierarchy
In the ideal setting, the prototype hierarchy is the one that fully represents the underlying topic structure in the target collection, i.e., the resulting data hierarchy is a full match. However, it is not possible to define such an ideal prototype hierarchy. Therefore the implementation should consider the cases when the predefined PH is a partial match.
If the predefined P H is an incomplete match of the underlying DH of a collection, we expand P H by adding more nodes to accommodate more categories. This is solved by adding dummy child nodes to the existing nodes in P H. For instance, in the Figure 1 example, equestion 7 has no appropriate category to assign to. This question will thus be assigned to an unnamed (dummy) node as the child of category IPhone:Online Service: Itunes Store1.
The added nodes, however, do not have a specific concept label and prototype description. For such scenario, we adopt the existing label extraction algorithms [4] for these new categories. The disadvantage of post-categorizing labeled concepts is that they may be less consistent with the existing concept space.
If the predefined P H is an excess match of the underlying DH of a collection, some of the nodes in PH may result in empty category in DH, such as IPhone:Software:Interface in the Figure 1 example. For such cases, the empty nodes will be labeled as empty or removed from the browsing interface.

1This is a combined effect of all the objectives, if only
obj2 and obj3 are considered, question 7 will be assigned to IPhone:Online Service: Itunes Store:audio category.

3.3 Object Metric
Under Data Hierarchy, Object Metric M (di, dj ) is defined as the distance (similarity) between a pair of objects di and dj within a node. Theoretically, any metric that satisfies the non-negativity, symmetricity, and triangular inequality criteria is a valid metric. For text clustering, a desirable metric is the one that captures the lexical, syntactic, and semantic features of the texts. We explore two state-of-theart text retrieval models for estimation the distance metrics, the translation-based language model and the syntactic tree kernel matching model.

3.3.1 Translation-based Language Model
Translation-based language model (TBLM) is originally proposed to solve the lexical gap problem in document retrieval. The monolingual translation probabilities capture the lexical semantic relatedness between mismatched terms in the query and the documents. We employ TBLM to measure the semantic similarity between two texts d1 and d2. The similarity score function is similar to the retrieval function proposed by Xue et al [18]:

PT BLM (d1|d2) = wd1 P (w|d2) P (w|d2) = (1 - )Pmx(w|d2) + Pml(w|D)

Pmx(w|d2) = (1 - )Pml(w|d2) +  P (w|t)Pml(t|d2)
td2
where P (w|d2), the probability that w is generated from document d2, is smoothed using Pml(w|D), the prior probability that w is generated from the document collection D.  is the smoothing parameter. Pmx(w|d2) is the interpolated probability of Pml(w|d2) and the sum of the probabilities that w is a translation of t, P (w|t), weighted by Pml(t|d2). Pml is computed using the maximum likelihood estimator.
Due to its asymmetricity, PT BLM (d1|d2) cannot be directly applied as a metric between a pair of text d1 and d2. We thus define a symmetric distance metric by taking the average of the two scores that switch the role of d1 and d2 as the "query" and "document":

MT BLM (d1, d2)

=

1 2

(PT

BLM

(d1|d2

)

+

PT

BLM

(d2|d1

))

(7)

3.3.2 Syntactic Tree Kernel Matching Model
The tree kernel function is one of the most effective ways to represent the syntactic structure of a sentence [15]. Syntactic Tree Kernel Matching Model(STKM) is designed based on the idea of counting the number of tree fragments (subtrees) that are common to both parsing trees:

Sim(T1, T2) =

C(w1, w2)

w1 W1,w2 W2

where W1 and W2 are sets of nodes(terms) in two syntactic trees T1 and T2, and C(w1, w2) is the number of common tree fragments rooted in nodes w1 and w2. Wang et al [15] improved node matching function C(w1, w2) by adapting the tree kernel function to take into account the syntactic as well as semantic variations.
STKM is originally designed to measure the similarity between two sentences. We generalize it into a similarity metric between multiple-sentence texts as follows:

MST KM (d1, d2) =

si d1

sjd2 sim(T (si), T (sj)) (8) |d1||d2|

5

Table 1: Statistics of Dataset ( indicates webpages,  indicates question answer pairs).

statistics

ODP

Yahoo! Answers

Computer Religion & Dental IPod Science Spirituality

Prototype Max.depth 6

5

54

Hierarchy Collection

concepts 145

leaf concepts 83

ob jects

2085

177 106 3909

104 87 62 51 6735 4381

where si and sj are sentences from d1 and d2 respectively. MT BLM and MST KM emphasize on semantic and syn-
tactic similarity of text objects respectively, we propose an integrated object metrics by taking their interpolation:
MT B-ST (d1, d2) = MT BLM (d1, d2)+(1-)MST KM (d1, d2) (9)
With the above defined M (., .), similar objects can be better clustered into the same category.
3.3.3 Objective Functions
Category Cohesiveness (obj4) objective requires that the collection is categorized such that objects in the same category are similar to each other and those in different categories are dissimilar to each other. More specifically, when the intra-category similarity is the highest, and the intercategory similarity is the lowest, the categorization achieves the highest cohesiveness. Formally, the cohesiveness of the data hierarchy is defined as:

maximize obj4 =

vk=root dp,dqvk M (dp, dq) (10) vp=leaf vi,vj child(vp) c(vi, vj )

where c(vi, vj) measures the cosine similarity between the centroids of vi and vj . Note that in the denominator, only sibling categories compared, rather than as a whole as in [4]. The assumption is that categories at different levels could be different in terms of abstractness and thus not comparable.

3.4 Multi-Criterion Optimization Function
In Section 3.1-3.3, we have discussed the criteria on constructing a data hierarchy, and induced four objective functions obj1,obj2, obj3, and obj4. In our proposed prototype hierarchy based clustering framework, all the criteria are to be satisfied, therefore the four objectives are to be optimized simultaneously:

minimize Om = 1obj1 - 2obj2 + 3obj3 - 4obj4 (11)

where 1, 2, 3, and 4 are introduced to control the contribution of each objective within the range of 0 to 1.
The multi-criterion optimization function leads to a greedy optimization algorithm, which at each object insertion step, produces a new data hierarchy by adding the new object into an appropriate node, which minimizes Om.

4. EXPERIMENTS

4.1 Datasets
To evaluate the proposed prototype hierarchy based clustering scheme, we apply the techniques developed to reconstruct the subdirectories of ODP, and to organize Yahoo!

Answers questions according to prototype hierarchies from external knowledge source. Table 1 shows the statistics of the prototype hierarchies and the associated collections of the four datasets.
For the ODP datasets, the prototype hierarchies are constructed by extracting the subcategories of two topics, Computer Science (CS) and Religion and Spirituality (RS). The subcategory descriptions are extracted as prototypes. Some subcategories for portals (e.g., classified, directory) or those labeled by alphabetic orders, which are uninformative for the purpose of categorization and navigation, are removed. The two collections contain websites belonging to the categories of the extracted prototype hierarchies; where homepages of these websites are the objects of the collections.
The datasets under the topics Dental and IPod are collected from YA. The prototype hierarchy for the Dental dataset is directly extracted from Wikipedia hierarchy under Dentistry, where the prototype for each subcategory is the first part (the definition) of the corresponding Wikipedia article. The prototype hierarchy for IPod is a manually constructed hybrid hierarchy by combining Wikipedia IPod article hierarchy, Wordnet IPod meronyms, and product specification from IPod website). The corresponding prototypes are also the combined descriptions from these three sources. The objects of the two collections are questions downloaded using YA API from Dental and Music & Music players. We asked two dentistry graduate students and two computer science graduate students to organize the two collections by reading through the downloaded archives. Inter-rater agreements in terms of Kappa statistics are 85% and 91% for Dental and IPod respectively. The differences between the two annotators are made consistent by discussion.
Each of the four collections are equally divided into two parts (C1 and C2) for training/developing and testing. The results are presented by taking the average of the two suits of experiments, using either part as testing sets.
The four datasets are carefully constructed to represent different scenarios. CS is a topic with a deep hierarchy, while RS has a broad hierarchy. IPod represents a concrete domain and RS an abstract domain. ODP hierarchies (CS and RS) are noisier than Wikipedia hierarchy (Dental); while the semi-manually constructed hierarchy (IPod) has better quality.
4.2 Overall Performance
4.2.1 Experimental Setting
To evaluate the performance of the proposed PHC model, we compare the following systems:
1) proKmeans: a prototype hierarchy enhanced K-means divisive hierarchical clustering. We choose a divisive algorithm as our baseline, as divisive algorithms has been found to be better solutions than agglomerative algorithms [20]. K-means works well when K and the initial partitioning are properly set. At each step of the division, we set K to be the number of leaf nodes in the prototype hierarchy; and the associate prototypes as the initial centroids. In this way, an intuitive unsupervised method is enhanced to be a relatively strong baseline.
2) LiveClassifier [9]: a state-of-the-art hierarchical classifier. We employ the approach 3 and KNN as the learning algorithm as in [9]. We adopt Yahoo BOSS API2 as the
2http://developer.yahoo.com/search/boss/

6

Table 2: Comparison of the proposed PHC, the two

baselines, and a supervised method CFC in terms of

F1 and mF1. Methods CS

RS

Dental

IPod

F1 mF1 F1 mF1 F1 mF1 F1 mF1

proKmeans-B1 0.623 0.547 0.63 0.644 0.601 0.592 0.612 0.608

LiveClassifier-B2 0.656 0.641 0.618 0.625 0.683 0.663 0.667 0.629

PHC-BOW 0.732 0.713 0.741 0.729 0.755 0.703 0.764 0.713 over B1 17.5% 30.3% 17.6% 13.2% 25.6% 18.8% 24.8% 17.3% over B2 11.6% 11.2% 19.9% 16.6% 10.5% 6.0% 14.5% 13.4%
PHC-TBLM 0.757 0.732 0.749 0.756 0.803 0.783 0.822 0.767 over B1 21.5% 33.8% 18.9% 17.4% 33.6% 32.3% 34.3% 26.2% over B2 15.4% 14.2% 21.2% 21.0% 17.6% 18.10% 23.2% 21.9%
PHC-STKM 0.791 0.78 0.775 0.788 0.764 0.721 0.778 0.719 over B1 26.9% 42.6% 23.0% 22.4% 27.1% 21.8% 27.1% 18.3% over B2 20.5% 21.7% 25.4% 26.1% 11.9% 8.7% 16.6% 14.30%
PHC-TB-ST 0.869 0.853 0.842 0.851 0.885 0.879 0.893 0.889 over B1 39.5% 55.9% 33.7% 32.1% 47.2% 48.5% 45.9% 46.2% over B2 32.5% 33.1% 36.2% 36.2% 29.6% 32.6% 33.9% 41.3%

supervised CFC 0.904 0.884 0.851 0.857 0.879 0.904 0.866 0.854

search engine to gather snippets as training examples. The number of pseudo nodes for leaf node is set to be 6. We set K=1 for result evaluation. This method is used as the second baseline.
3) PHC-BOW, PHC-TBLM, PHC-STKM, and PHC-TBST : 4 variations of PHC using Bag-of-Word, TBLM, STKM, and the combined TBLM and STKM respectively as the object metric. For 1, 2, 3, and 4 in Equation 10, we perform an exhaustive grid search of step size 0.1 on [0, 1] to find parameters that produce the best F1 on the developing set. For TBLM,  is set to 0.8 and  to 0.5; the translation probabilities are trained using the set C1/C2 and tested on C2/C1. For STKM, the four parameters (the node/size/depth weighting factors, and the weight of the matching tree fragment) are tuned by using the set C1/C2 as the development set. For TB-ST,  is set to 0.5.
4) CFC Classifier [7]: a state-of-the-art supervised text categorization technique. It is included to test the effectiveness of semi-supervised PHC against a supervised method. Experimental results are averaged using either set C1/C2 as the training set.
We use the average accuracy of categorizing the leaf categories as the performance measure for each dataset. In particular, we use the micro-averaging F1 (F1) and macroaveraging F1 (mF1) as the performance metrics. F1 is a combined form for precision (p) and recall (r), which is defined as F1 = 2rp/(r + p).
4.2.2 Results and Discussion
The results are evaluated on all the leaf nodes and displayed in Table 2. By comparing the vertical entries by different methods, we draw the following observations:
(1) Both (unsupervised) baselines achieve reasonably high F1 of about 0.6. For LiveClassifier, the results are consistent with that reported in the original experiment in [9] which shown it to be comparable to a supervised approach. For proKmeans, it achieves better F1 than that reported in a state-of-the-art K-means clustering algorithm [8] that employs the sophisticated semantic features. This suggests that by specifying a prototype hierarchy for a collection,

even a simple method like divisive K-means can categorize the collection reasonably well.
(2) PHC with TB-ST surpasses all the other unsupervised systems. All the four PHC systems perform significantly better than the two baselines. Even the simplest PHC design with BOW-based metric achieves improvement of above 10% over both baselines. This indicates that PHC is superior in terms of utilizing the prototype hierarchy. proKmeans makes use of the prototypes and the number of children under each node; whereas LiveClassifier makes use of the relations of nodes and node labels (concepts). Either baseline benefits from the prototype hierarchy but not as comprehensively as the PHC's.
(3) PHC achieves the best performance when the object metrics (TBLM and STKM) are used in combination (TBST). It is however difficult to compare the PHC with TBLM and PHC with STKM. Generally, syntactic tree kernel based method works better on the two ODP collections; and translation based method works better on the two YA collections. We conjecture that the ODP collections are more standard than YA in term of English grammar and thus more suitable for syntactic tree parsing; while the YA collections use parallel question-answer corpus which is more suitable for generating translation probabilities. The combination of TBLM and STKM yields significant performance improvement when compared to the individual model. This shows that semantic and syntactic features complement each other and contribute to the overall result.
(4) PHC with TB-ST even achieves comparable result with CFC, a state-of-the-art supervised classification algorithm. CFC can be deemed as using the hand-labeled corpora, while PHC makes use of hand-built hierarchy. This implies that a prototype hierarchy created by experts or web community is enough to help create good categorization of a large web collection, instead of needing to manually organize and label the large corpus. Moreover, PHC provides the additional benefit of facilitating navigation.
(5) PHC introduces new nodes into predefined hierarchy. In PHC-TB-ST setting, the numbers of new nodes introduced are 7 for CS, 11 for RS, 5 for Dental, and 3 for IPod. Since we deem the prototype hierarchies and collections in Table 1 as fully matched, the objects in the newly added nodes are simply evaluated as incorrect for ease of experimentation. This indicates that the results in Table 2 may underestimate the actual performance of PHC.
4.3 Impact of Domain Abstractness and Prototype Quality
By comparing the horizontal entries of Table 2, we draw the following observations of PHC performance on different domains and prototype hierarchies:
(1) PHC works better on concrete domains than on abstract domains. This is evidenced by comparing PHC performances on the two ODP collections. Computer Science achieves higher F1 scores than Religion and Spirituality for the last 3 settings of PHC. It can be explained by two reasons: (i) the concepts in concrete domains like CS are more specific, therefore the prototypes associated with the concepts are more precise, informative, and have potentially more word overlapping with the target objects; (ii) the subtopics of abstract domains like RS are less systematically arranged in a hierarchical structure and the ancestordescendent relations between the subtopics are less obvious;

7

Table 3: Objectives analysis. % of change in F1 when a single objective is removed.

objectives CS

RS Dental IPod

All-obj1 -1.4% +3.2% +2.7% 0.0

All-obj2 -11.1% -10.6% -8.7% -9.6%

All-obj3 -5.8% -4.0% -6.6% -7.4%

All-obj4 -10.5% -6.7% -9.3% -8.8%

therefore PHC is harder to benefit from the "loose" hierarchies of the abstract domains.
(2) A better prototype hierarchy can potentially enhance the categorization performance. Table 2 shows that in the PHC settings, IPod collections (with semi-manually compiled hierarchy) attains the best results, followed by Dental (with Wikipedia hierarchy), and the worst are the two ODP collections. This indicates that high quality hierarchy will lead to better results. Besides the quality of prototype hierarchy, the quality of the prototypes used may also influence the categorization performance. Prototypes from ODP category descriptions are of various qualities, depending on the devotion of the category editor. Prototypes from Wikipedia articles are mediated by a larger community and thus maintain a high level of quality. We conjecture that when more high quality hierarchies are available in digital form, PHC can achieve even better performance and wider adaptability.
4.4 Ablation Study on Optimization Objectives
We do a leave-one-out study on the optimization objectives to analyze the effects of each objective on the categorization. In the implementation, we set one of the parameters (1, 2, 3, and 4) in Equation 11 to 0, and optimize the rest using grid search.
Table 3 shows that removing an objective from the multicriterion optimization generally results in degraded performance. Prototype Centrality (obj2) influences all the four collections greatly, followed by Category Cohesiveness (obj4). Prototype-Data Hierarchy Resemblance (obj3) influences Dental and IPod more than CS and RS.
obj2 is a bit more complex. A prototype may not represent the perfect center for a cluster. Two cases exist: (i) the prototype provided is roughly located at the center of a category's object space; the objects that are closer to the true center but farther away from the prototype may be adversely influenced by the prototype centrality score. (ii) the target collection is not large enough to form typical categories; even though the prototype provided might be good, the object center itself may shift from general knowledge.
An interesting observation is that removing obj1(minimum evolution) increases the F1 measure on RS and Dental. By examining the clustering output, we find that the data hierarchy varies less from the prototype hierarchy without the minimum evolution. For example, two websites about Youth for Human Rights are under ODP category Religion and Spirituality: Scientology: Church of Scientology: Volunteer and Community Activities; the two websites are correctly categorized when the minimum evolution objective is removed. Under the full-fledge multi-criterion setting, the output data hierarchy has a new node created to accommodate the two websites under Religion and Spirituality: Scientology: Church of Scientology, as a sibling of Volunteer and Community Activities. Since there is no standard on

uF1 uF1

0.95 0.9
0.85 0.8
0.75 0.7
0.65 0.6 0

Excess Match

CS RS Dental IPod

5

10

15

Excess Rate(%)

(i)

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

20

0

Incomplete Match

CS RS Dental IPod

5

10

15

20

Incomplete Rate(%)

(ii)

Figure 3: The influence of partially matched prototypes on PHC performance.

whether some objects should be assigned into a new node or an existing node, the phenomenon implies that minimum evolution objective leads to a self-contained data hierarchy.
4.5 Robustness with Mismatched Prototype Hierarchy
To further study the tolerance of PHC with partially matched prototype hierarchies, we deliberately manipulate the prototype hierarchies and the collections to examine the two types of mismatching effects: incomplete and excess prototype hierarchy.
We mimic an incomplete (insufficient) prototype hierarchy by deleting nodes from the completely matched prototype hierarchy. Similarly, we mimic an excess(overfitted) prototype hierarchy by inserting dummy nodes into the current hierarchy. Alternatively, we remove objects belong to a certain node from the collection, such that the node in the prototype hierarchy becomes redundant. We adopt the second approach because lacking certain groups of objects is more common in practical applications.
In Figure 3, we plot F1 of PHC results on the four collections with excess rate and incomplete rate ranging from 0 to 20%. The two rates are defined in Section 3.2. From Figure 3(i), we can see that the F1 measures on all the four collections slight degrade over the 0 - 15% excess rate. By examining the output object hierarchy, we find that for most excess nodes, PHC produces empty clusters. It suggests that PHC's is robust against overfitted prototype hierarchies.
From Figure 3(ii), we can see that incomplete match is well tackled at 0 - 5% range and degrades drastically from 10% onwards. In the experimental setting, at the lower incomplete rate, only some leaf nodes are removed; whereas at higher incomplete rate, even those sub-hierarchies are removed. This suggests that PHC has only limited ability to "create" categories. We conjecture that it is because PHC is designed to add one level of child nodes to existing nodes in the prototype hierarchy without considering multiply levels. In other words, our system needs further improvement to tackle the high incomplete rate, or seek more complete match prototype hierarchies to avoid the problem.
5. RELATED WORK
Hierarchical clustering has long been recognized as a natural way to organize and navigate text collections [10, 5, 17]. Existing algorithms for hierarchical clustering are generally either agglomerative, divisive, or combined [20]. The automatically generated clusters are however less neatly organized as a manually constructed hierarchical tree like the ODP and Wikipedia hierarchies. Another limitation is that the clusters do not have labels to indicate the topics con-

8

tained. Further more hierarchical clustering, labeling is more complicated since an internal node in the hierarchy has to be distinguished from its siblings, parent, and children.
World knowledge has been found to be useful in enhancing clustering and labeling. For clustering, metric based [16] and constraints based [14] approaches utilize knowledge in the form of a small amount of labeled samples. Bilenko et al [2] integrated the two approaches and obtained further improvement over either approach. Hu et al [8] proposed to enrich short texts representation for clustering with syntactic and semantic features from WordNet and Wikipedia. For cluster labeling, Carmel et al [4] successfully enhanced it by extracting candidate labels from Wikipedia, in addition to the important terms that are extracted directly from the text. In our work, world knowledge comes in the form of a topic hierarchy and prototype descriptions.
Various criterion functions for document clustering have been studied in [20]. These functions represent some of the most widely used criteria for document clustering, but not cover the structural aspects of the hierarchies.
Hierarchy and taxonomy induction has long been studied on concepts [13], noun-phrases [19], and word-based topics [3]. For the first time it has been applied as a criterion on hierarchical clustering in this work. Moreover, the concept hierarchies [13] and taxonomies [19] could be used to automatically extract prototype hierarchies for the PHC framework.
Our work is similar in spirit to conceptual clustering [12] which is distinguished from ordinary data clustering by generating a concept description for each generated class. Another similar work is LiveClassifier [9]. It tries to tackle the clustering and labeling problem in a reverse order. Assuming a predefined topic hierarchy, it augments the hierarchy and uses search engines to automatically gather training corpus for classifying websites into the hierarchy. Our framework is similar to theirs in term of specifying a topic hierarchy, but exploits more on the structure and evolution of the hierarchy rather than searching for training data.
6. CONCLUSION
This paper proposed a prototype hierarchy based clustering framework for web collection categorization and navigation. By minimizing the hierarchy evolution, maximizing category cohesiveness and inter-hierarchy structural and semantic resemblance, the hierarchical clustering task is modeled as a multi-criterion optimization problem. Empirical results on categorizing 4 web collections of various domains have shown that PHC is superior to the two strong unsupervised baseline methods and comparable to a state-of-the-art supervised method.
In future work, we plan to optimize the efficiency of the proposed PHC algorithm and explore its applicability to multimedia collections with domain specific hierarchy and object metrics.
7. REFERENCES
[1] R. Baeza-Yates. User generated content: how good is it? In Proc. of the 3rd workshop on Information credibility on the web, pages 1­2, Spain, 2009. ACM.
[2] M. Bilenko, S. Basu, and R. J. Mooney. Integrating constraints and metric learning in semi-supervised clustering. In Proc. Machine learning, page 11, Banff, Alberta, Canada, 2004. ACM.

[3] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In Advances in Neural Information Processing Systems, page 2003. MIT Press, 2003.
[4] D. Carmel, H. Roitman, and N. Zwerdling. Enhancing cluster labeling using wikipedia. In Proc. SIGIR, pages 139­146, Boston, MA, USA, 2009. ACM.
[5] S. Dumais and H. Chen. Hierarchical classification of web content. In Proc. SIGIR, pages 256­263, Athens, Greece, 2000. ACM.
[6] P. Ferragina and A. Gulli. A personalized search engine based on web-snippet hierarchical clustering. In Proc. WWW, pages 801­810, Japan, 2005. ACM.
[7] H. Guan, J. Zhou, and M. Guo. A class-feature-centroid classifier for text categorization. In Proc. WWW, pages 201­210, Spain, 2009. ACM.
[8] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploiting internal and external semantics for the clustering of short texts using world knowledge. In Proc. CIKM, pages 919­928, Hong Kong, China, 2009. ACM.
[9] C.-C. Huang, S.-L. Chuang, and L.-F. Chien. Liveclassifier: creating hierarchical text classifiers through web corpora. In Proc. WWW, pages 184­192, New York, NY, USA, 2004. ACM.
[10] S. Johnson. Hierarchical clustering schemes. Psychometrika, 32(3):241­254, 1967.
[11] D. J. Lawrie and W. B. Croft. Generating hierarchical summaries for web searches. In Proc. SIGIR, pages 457­458, Toronto, Canada, 2003. ACM.
[12] R. Michalski and R. Stepp. Learning from observation: Conceptual clustering. Machine Learning, 1:331­363, 1983.
[13] M. Sanderson and B. Croft. Deriving concept hierarchies from text. In Proc. SIGIR, pages 206­213, New York, NY, USA, 1999. ACM.
[14] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. Constrained k-means clustering with background knowledge. In Proc. Machine Learning.
[15] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree matching approach to finding similar questions in community-based qa services. In Proc. SIGIR, pages 187­194, Boston, MA, USA, 2009. ACM.
[16] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric learning with application to clustering with side-information. Advances in neural information processing systems, pages 521­528, 2003.
[17] G.-R. Xue, D. Xing, Q. Yang, and Y. Yu. Deep classification in large-scale text hierarchies. In Proc. SIGIR, pages 619­626, Singapore, 2008. ACM.
[18] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question and answer archives. In Proc. SIGIR, pages 475­482, Singapore, 2008. ACM.
[19] H. Yang and J. Callan. A metric-based framework for automatic taxonomy induction. In Proc. ACL, pages 271­279, Suntec, Singapore, 2009. ACL.
[20] Y. Zhao, G. Karypis, and U. Fayyad. Hierarchical clustering algorithms for document datasets. Data Min. Knowl. Discov., 10(2):141­168, 2005.

9

A Network-Based Model for High-Dimensional Information Filtering

Nikolaos Nanas
Centre for Research and Technology Thessaly Volos, 58300, Greece
n.nanas@cereteth.gr

Manolis Vavalis
Centre for Research and Technology Thessaly Volos, 58300, Greece
m.vavalis@cereteth.gr

Anne De Roeck
Computing Department The Open University
Milton Keynes, MKA 6AA, U.K.
a.deroeck@open.ac.uk

ABSTRACT
The Vector Space Model has been and to a great extent still is the de facto choice for profile representation in contentbased Information Filtering. However, user profiles represented as weighted keyword vectors have inherent dimensionality problems. As the number of profile keywords increases, the vector representation becomes ambiguous, due to the exponential increase in the volume of the vector space and in the number of possible keyword combinations. We argue that the complexity and dynamics of Information Filtering require user profile representations which are resilient and resistant to this "curse of dimensionality". A user profile has to be able to incorporate many features and to adapt to a variety of interest changes. We propose an alternative, network-based profile representation that meets these challenging requirements. Experiments show that the network profile representation can more effectively capture additional information about a user's interests and thus achieve significant performance improvements over a vector-based representation comprising the same weighted keywords.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software
General Terms
Algorithms, Experimentation, Performance
Keywords
Content-based Information Filtering, User Profiling, Curse of Dimensionality
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
When today on the WWW everyone can be both a consumer, but also a producer of information, a dual information overload problem arises. On one hand, it is impossible to keep track of the information that is being dynamically generated and disseminated in the context of the so called real-time Web, or to spot interesting sources of information out of the available glut. On the other hand, nobody can ensure the individual publisher that broadcasted information will reach the right audience. Information Filtering (IF) and the personalisation of information delivery that it achieves, can have a radical impact on the way we interact with information media. Already, Collaborative Filtering (CF) has been successfully deployed for calculating recommendations of movies, music tracks and books, but is admittedly not well suited for dynamic domains, like news publishing. Unlike CF, content-based IF has not yet produced similar success stories. After two decades of research on content-based IF, there is a surprising lack of publicly available and broadly adopted content-based IF applications.
Some of the reasons for this absence are discussed in [10], where we argued that IF is a complex and dynamic problem with its own particular characteristics and requirements, which differentiate it from Information Retrieval and Text Classification. IF is complex and dynamic because user interests and the information environment are complex and dynamic. Unlike Text Classification, the notion of a "topic" of interest is not that distinct in the case of IF. A user is typically interested in a variety of topics, which are fluid and interrelated. Over time, the level of interest in each topic may vary, new topics of interest can emerge and a previously interesting topic may wane and even become obsolete. Furthermore, there is an immense variety of topics to choose from in the information space. From general topics of interest, such as news categories (e.g., economy, technology, etc.) to a "long-tail" of more personal and specific interests. Of course, the information space itself continuously changes with the new material dealing with new combinations of concepts, even new concepts, the development of new technologies, the occurrence of temporal events, etc. Successful IF requires a user profile representation that can capture the various topics of interest and can continuously adapt to interest drifts and changes in the information space.
One significant implication of the above specification is that a user profile has to be able to incorporate a large number of features. For example, if we focus on textual information, then a larger number of keywords is required to represent multiple topics of interest than to represent a

202

single topic. More keywords are also required when the topics are specific rather than general. But, as we will further discuss in the next section, the Vector Space Model (VSM), the most popular choice for profile representation in IF, has inherent problems when the number of keywords, i.e., the dimensions of the vector space, increases. This "curse of dimensionality" [3] has forced vector-based approaches to IF to adopt radical dimensionality reduction techniques and to approach the modelling of user interests by requiring a separate profile for each topic. Furthermore, the VSM is typically coupled with the "bag of words" assumption and hence any information encoded in the correlated placement of words in text is not captured.
In this paper, we propose an alternative to the VSM. The user profile is no longer a weighted feature vector but a weighted network of descriptive features, which has been assigned to, or can be automatically extracted from interesting information items. Links in this network represent correlations between features appearing within the same context. Relevance evaluation of information items is not performed with trigonometric measures of similarity between keyword vectors, but with a directional spreading activation process. In the case of textual information, the user profile is a weighted network of keywords extracted from the content of interesting documents. We argue and experimentally support that the proposed network-based profile can incorporate a large number of keywords, more effectively than a vector-based profile. In doing so, the network-based profile captures additional information about a user's multiple interests, it becomes more specific and achieves significant performance improvements. We substantiate this claim in section 4. This resistance to the "curse of dimensionality" is a significant property of the proposed profile representation, offering many practical advantages and new perspectives.
In the rest of this paper we first identify and discuss the causes for the inherent dimensionality problems of vectorbased approaches to IF. Then in section 3 we describe the proposed network-based representation. The experiments in section 4 have been performed with a methodology that adopts the Reuters-21578 and simulates users with multiple topics of interest. We compared a vector-based to a networkbased profile comprising the same weighted keywords. The results indicate that as the number of keywords in these profiles increases, the existence of links in the network profile contributes to an increase in performance of up to 50% on average, compared to the vector-based profile. We discuss the implications of these positive results and we conclude with a summary and future research plans in section 5.
2. DIMENSIONALITY PROBLEMS IN IF
The VSM [15] is the most popular choice for profile representation and has had fundamental impact for research in IF. According to the VSM both documents and profiles are represented as, typically weighted, keyword vectors in a multidimensional space with as many dimensions as the number of keywords in the documents' vocabulary. This abstraction allows the application of trigonometric measures of similarity, like the inner product or cosine similarity, for assessing how "close" to a user profile a given document is [6]. The profile's goal is to define decision boundaries between relevant and non-relevant documents, or represent regions in the vector space which are dense with interesting documents. In

multidimensional spaces however, the discriminatory power of pairwise distances is significantly affected.
In [7], the authors thoroughly analyse and discuss in the context of Artificial Immune Systems, the issues that undermine vector-based approaches in multidimensional spaces. Their argumentation is directly applicable to vector-based approaches to IF and so here we recapitulate some basic points:
· As the number of dimensions increases, the volume of the space increases exponentially faster, and distance based metrics become increasingly meaningless, because all points in such a space become essentially equidistant [1].
· In a multidimensional space small amounts of noise along many dimensions can cause significant displacement to a vector. Along the same lines, "A scalar metric cannot differentiate between two vectors that differ slightly across all dimensions (and may be the same after accounting for noise) or differ significantly on just a few dimensions (and are clearly different in some respect)" [7].
· When instance-based methods (such as kNN [2]) are deployed, then typically the number of required data points scales with the number of dimensions causing a significant increase in memory and time requirements. These problems are exaggerated in the case of continuous learning problems, such as IF, where there is no stopping criterion.
We would like to complement the above issues with an intuitive argument. Let's assume that a user is interested in two topics: "long river" and "bank holidays". If we use a keyword vector, containing these four words, then we equally represent any possible combination of these four words, including for instance the combinations, "river banks" and "long holidays", which do not necessarily represent topics of interest to the user. In general, as the number of keywords in a user profile increases, the number of possible combinations increases exponentially and the profile becomes ambiguous, because the majority of keyword combinations will be irrelevant to the user's interests. To counteract this effect one should at least avoid the common term-independence assumption, which is inherent to the VSM and its orthogonal dimensions, and move away from the "bag of words" simplification. In the opposite case, valuable information about the user's interests is lost and the profile's specificity drops. Our experimental results, clearly support this argument.
Although in this paper we concentrate on a static IF problem, we will briefly touch on dynamic aspects. As both the user's interests and the information space change over time, a fixed keyword space becomes an inadequate choice. Tackling the problem's dynamics requires a fluid keyword space where additional dimensions can be added and removed. But even if this is the case, the adoption of a common vector space where all profiles and documents are represented is still impractical. If an IF system needs to accommodate a large number of users, then it is only safe to assume that their interests will vary. To cover all possible topics of interest with a common vector space, a very large number of keywords and hence dimensions are required and the computational and memory costs would significantly increase. For instance, experiments performed in [9] demonstrate that

203

covering 23 of the topics in Reuters-21578 requires a common vector space comprising more than 30000 documents.
Although seldom clearly stated, the above dimensionality problems are evident in current vector-based IF practices. They heavily depend on dimensionality reduction techniques, such as stop word removal, stemming, term weighting [23], Latent Semantic Indexing (LSI) [5] and more. This pre-processing of documents takes place in advance and typically results in a fixed vector space with manageable dimensions. Furthermore, they tend to break up the problem into distinct pre-defined topics and built a separate single-topic profile for each individual topic [21, 22]. This practice has been inherited from Text Classification and is reflected by evaluation standards for IF [14]. In reality though, a user's topics of interest cannot be easily predefined and they are definitely not fixed. Even within a general topic (e.g., "economy") different users will develop specific interests in various subtopics (e.g., "credit card fraud", "equity markets" etc.). Furthermore, given the plenitude of documents (e.g., news stories, blog posts, etc.) being published daily on a topic, a user is only interested in and has the time to read a very small percentage. So specificity is essential for successful IF, but it requires profiles with the ability to capture any available information about a user's interests. Although outside the scope of the current work, we would also like to note that many machine learning algorithms, such as Rocchio's linear learning algorithm, which have been a popular solution for profile adaptation [17, 24], lack an inherent mechanism for adding or removing keywords to a user profile. This means that they assume a fixed vector space that predefines the possible repertoire of profile keywords. The algorithm can only modify the weights of keywords in the profile's vector. It has also been argued, that learning algorithms cannot easily cope with radical interest shifts [20]. A new profile is typically generated whenever a new topic of interest emerges and a profile that corresponds to a no longer interesting topic is destroyed [21]. This is of course only a partial solution to the problem that unnecessarily complicates the task with additional system parameters and in any case, no profile will be able to represent a topic that is not already covered by the keywords in the predefined vector space.
3. A NETWORK-BASED PROFILE
To alleviate the above issues, we propose an alternative to the VSM. We need a model for IF that satisfies the following basic requirements:
· It does not require a fixed (and common) vector space that a priori defines the available features for representing user profiles and documents.
· The new model should not ignore the additional information encoded in correlations between features that appear in the same context. In the case of textual information, this means capturing dependencies between terms in text.
· The user profile should be dynamic, capable of continuously adapting its structure to the changing user interests and the evolving information space. A user profile that can not maintain a satisfactory level of performance will eventually dissatisfy the user and will be abandoned.

In the proposed model, the user profile is a weighted network with nodes representing features extracted from interesting information items and links representing their correlations. Since we will concentrate on textual information, nodes will represent terms (i.e., single words) extracted from the content of documents and links will represent correlations between terms in text. The discussion however can be easily extended to any type of descriptive feature that can be automatically extracted from the content of information items, or have already been assigned to them (e.g., tags). Every node is assigned a weight that measures the importance of the corresponding term given the user's interests. Every link between two nodes is assigned a weight1 measuring the degree of correlation between the respective terms. The weights are of course important, but the way they are calculated is not constrained by the model itself. Any keyword weighting method can be used to calculate the weights of nodes in the profile's network. To calculate the weights of links, co-occurence statistics between terms appearing in the same context are required. Various appropriate methods appear in the literature and have been used to construct similar network structures for capturing term correlations in Text Retrieval and even Information Filtering. For example, "collocation maps" [12] and "dependence trees" [19] have been proposed for query expansion and "concept hierarchies" [16] for navigating document collection and search results. In IF, correlations between terms have been generally neglected. One notable exception is the adoption of an associative graph for capturing syntactic correlations between terms that appear next to each other and of a spreading activation process for document evaluation in [18].
The essential contribution of the proposed model is not the network itself, but a new way of using the weighted network to evaluate the relevance of documents. We treat content-based IF as the general problem of assigning a relevance score to each document based on its content, rather than making a binary classification between relevant and non-relevant documents. Document evaluation is based on a non-iterative, directional, spreading activation process that takes into account not only the weight of profile nodes (terms), but also the weight of links between them. The process can be deployed to assign a relevance score to any portion of a document's text, ranging from a single sentence to the complete document. The document is not treated as a "bag of words" and it does not have to be represented as a keyword vector. Nevertheless, the terms in the text can be weighted with methods such as Term Frequency Inverse Document Frequency (TFIDF). To assign a relevance score to a portion of text T , each term in the profile's network that also appears in T is assigned an initial activation equal to the term's weight in T . The activation phase is followed by a dissemination phase, which starts with the activated node with the smaller weight in the profile's network and proceeds sequentially with the remaining activated nodes in increasing weight order, until the activated node with the largest weight is reached. Every activated node is triggered once to disseminate part of each current activation to the activated nodes with larger weights that it is linked to. The amount of activation that is disseminated between two nodes is proportional to the weight of the link between them. The relevance
1Here we focus on symmetric links but the model could also account for non-symmetric links.

204

(A)

wn n

wmn

wkn wm m wkm
wk k

(B)
n an m am
k ak

c kn = a k * w kn

(C)
n an + ckn
m am + ckm
ckm = ak * wkm
k ak - ckm - ckm

(D)
n an + ckn + cmn
cmn = (am + ckm) * wkm
m am + ckm - cmn
k ak - ckm - ckm

Figure 1: Directional Spreading Activation: (A) idle network, (B) activation phase, (C) node k disseminates, (D) node m disseminates.

score of T is calculated as the weighted sum of the final activation of nodes.
Figure 3 illustrates the above process. The portion of text T activates the initially idle nodes k, m, n, out of the complete profile network, which is not depicted in the figure.
The weights of the three nodes are wk, wm, wn respectively, with 0 < wk < wm < wn, and the weights of the links between the three nodes have weights wkn, wkm and wmn with positive values. The initial activation of the three nodes (ak, am, an) is equal to the weight of the corresponding terms in T . The dissemination process starts with the activated node with the least weight2. Node k disseminates an amount of activation equal to ckn = ak · wkn to node n and an amount equal to ckm = ak · wkm to node m. To avoid the situation where a node disseminates more than its current activation, i.e., when the sum of the weights of links to activated nodes is more than one, we normalise the weight of these links so
that they add up to one. Once node k has disseminated its activation, the new activation of the three nodes k, m and n, becomes ak - ckn - ckm, am + ckm and an + ckn respectively. It is now the turn of the next node in the order of increasing weight to disseminate part of its current activation to acti-
vated nodes with larger weight that it is linked to. So node m disseminates the amount cmn = (am + ckm) · wmn to node n and their respective activation becomes am + ckm - cmn and an + ckn + cnm. At this point the dissemination process terminates because node n is not linked to any other activated nodes with larger weights. The relevance score RT of T is given by the following formula:

RT = wk · (ak - ckn - ckm) + wm · (am + ckm - cmn)

+ wn · (an + ckn + cnm)

= (wk · ak + wm · am + wn · an)

(1)

+ (wm - wk) · ak · wkm + (wn - wk) · ak · wkn

+ (wn - wm) · (am + ak · wkm) · wmn

Note that the first term in the above sum, is actually the inner product between a weighted keyword vector of the three profile terms and a weighted keyword vector of the three terms in T . In other words, the above formula specialises to the inner product if there are no links between the activated nodes (wkn = wkm = wmn = 0). However, when the activated nodes are linked then the relevance score of T increases by an additional amount equal to the sum of the last three terms in equation 1. It is clear that this additional amount is

2If two terms have the same weight then they are ordered alphabetically

positive because all weights are positive and wk < wm < wn. So every time a portion of text activates correlated nodes in the profile's network and not isolated ones, it receives an additional relevance reward. In this way the profile becomes more specific, because it concentrates on those term combinations that are relevant to the user's interests and it does not equally represent every possible combination of terms in the profile. Our experimental results clearly show that this property results in significant improvements in filtering accuracy, especially when the number of terms in the profile increases.
Overall, the proposed model, as described so far, satisfies the first two of the three requirements described earlier. The user profile is not represented as a weighted keyword vector on a common, central, predefined and fixed space. For each individual user, the profile is a separate network structure containing only those terms that are representative of the user's interests. Furthermore, the number of profile terms is neither predefined nor fixed. It is dynamically controlled during the profile's adaptation to interest changes. The profile's network captures correlations between terms in text and a directional spreading activation process takes them into account during document evaluation. Note also, that unlike existing spreading activation processes that are typically iterative and involve the complete network [18], thus increasing the computational cost, the proposed approach involves only the subset of activated profile nodes and its directionality ensures that each activated node is visited only once and thereafter, each link between activated nodes is also traversed only once. So in the worst case of a fully connected and fully activated network, its complexity is O(Np2), where Np is the number of profile terms. The inclusion of links increases the complexity of the user profile, in comparison to the linear complexity O(NL) of a vector-based profile in a NL-dimensional space, using the inner product for document evaluation. Note however, that if a common vector space is used to represent the profiles of many users with a variety of interests, then a large number of keywords would be required to cover all possible interests. In contrast, the proposed model is inherently distributed and comprises only the terms required to represent the interests of a single user. So Np << NL and the difference in complexity between O(Np2) and O(NL) is alleviated. If needed, the computational and memory requirements of each profile can be controlled with upper limits on the number of profile terms and links. In any case, if the user profile resides on the user's machine, scalability issues do not arise.

205

topic size topic size topic size

earn (1) 3987
ship (9) 305
veg-oil (17) 137

Table 1:
acq (2) 2448
corn (10) 254
gold (18) 135

Topics involved in the experiments and their corresponding size

money-fx (3) crude (4)

grain (5)

trade (6)

interest (7) wheat (8)

801

634

628

552

513

306

dlr (11)

oilseed (12) money-supply (13) sugar (14)

gnp (15) coffee (16)

217

192

190

184

163

145

nat-gas (19) soybean (20)

bop (21)

livestock (22) cpi (23)

130

120

116

114

112

The theoretical background of the proposed model is discussed in detail in [8] and is biologically-inspired. The user profile is modelled after the network of interacting antibodies in the immune system of vertebrates and through the chains of suppression and reinforcement that the spreading activation process generates, it defines the host organism's "self", i.e., the user's interests. In the same paper, we describe an algorithm that allows the profile to continuously adapt to a variety of interest changes, through a biologicallyinspired process of self-organisation. The algorithm adjust the profile's structure in response to user feedback, through variations in the weight of profile terms, recruitment of new terms that cover emerging topics of interest and removal of terms that correspond to no longer interesting topics. So the profile is not constrained by the terms in a pre-defined vector space. Experiments show that through this process the profile can adapt to both short-term variations and more longterm, radical changes in user interests, while autonomously controlling both its size and connectivity [8]. Furthermore, comparative experiments show that this algorithm outperforms the popular Rocchio's learning algorithm on a continuous learning problem [11]. So, although outside the scope of the current work, the third of the aforementioned requirements can also fulfilled.
4. COMPARATIVE EXPERIMENTS
In this section, we evaluate experimentally a specific realisation of the model and compare it with a vector-based profile. In [10], we argued that existing evaluation methodologies do not accurately reflect the particularities of contentbased IF, mainly because they usually treat it as a Text Classification problem, with each user profile representing a single topic of interest and trained with a large number of relevant documents. Furthermore, since the removal of the filtering track from the Text Retrieval Conference (TREC) in 2001, there is no established evaluation standard for contentbased IF. Here we adopt a stripped down version of the methodology in [11], which simulates users with multiple interests, but ignores interest changes.
The methodology uses the Reuters-21578 document collection, but could be applied for any pre-classified collection of documents. The evaluation concentrates on the 23 topics in Reuters-21578 with more than 100 relevant documents (table 1). Each topic is assigned a serial number (in parenthesis) to identify it when presenting the experimental results. For each topic we use the first fifty relevant documents in the collection for training and the complete collection as a test set. This is a significant departure from current practices for the evaluation of Text Classification systems, such as the ModApte split, that uses three quarters of the documents for training and the remaining quarter for testing. By using the same small number of training documents per topic and given that the number of relevant documents range from 112 to 3987 (table 1), a variable percentage of the rel-

evant documents is used to build the profile. So the user profile has to be specific for topics with a small number of relevant documents and exhaustive for topics with a large number of relevant documents.
The most significant contribution of the proposed methodology, is the simulation of users with multiple interests. In particular, we simulated users with parallel interest in one, two, three, four and five topics. For instance, to simulate a user interested in two topics we train a single profile for each combination of two consequent topics (e.g., earn and acq (1:2), acq and money-fx (2:3), money-fx and crude (3:4) and so on). We combine consequent topics with similar sizes to avoid biases towards topics with a larger number of relevant documents. For each topic combination we use the first 50 relevant documents per topic to train a single profile, which is then used to evaluate the complete collection. The 21578 documents are then ranked according to their relevance score and the Average Uninterpolated Precision (AUP) measure is calculated for each individual topic and also for their combination, i.e., an aggregate topic that includes all documents relevant to the constituent topics. A topic's AUP is defined as the sum of the precision ­ i.e., the percentage of documents relevant to that topic ­ at each point in the ordered list where a relevant document appears, divided by the topic's size. We use an evaluation list comprising all documents in the collection and not just the best 1000 scoring documents (as in TREC's routing subtask). This way, we obtain more accurate and unbiased measurements, since a list of the best 1000 scoring documents can be easily populated when a topic of interest has a far larger number of relevant documents. Furthermore, such a list can be biased towards one of the topics in a combination, because it can be dominated by the topic's relevant documents at the expense of documents relevant to the rest of the topics. For similar reasons, we preferred Reuters-21578 and not the more recent RCV1, because the latter causes evaluation problems due to the very large number of relevant documents per topic in the collection [14].
Overall, the methodology defines a challenging IF task that more accurately reflects the problem's complexity and proposes an alternative to existing practices. As the number of topics of interest increases from one to five the necessary number of profile terms also increases. Our aim is to experimentally support our argument regarding the effectiveness of the proposed model in comparison to the VSM when this happens.
4.1 Experimental Setting
The documents in the collection are first pre-processed using stop word removal and stemming with Porter's algorithm. We then used Information Gain (IG) [4] to weight the remaining terms in the training documents. Terms with positive weight were extracted to build a user profile for each topic or topic combination. Two different types of profile were constructed. The baseline profile is a vector-based

206

AUP

23

22:23

combined AUP

combined AUP

combined AUP

combined AUP
1

0.900 0.675 0.450 0.225
0 0.900 0.675 0.450 0.225 0
0.900 0.675 0.450 0.225
0
0.900 0.675 0.450 0.225
0
0.900 0.675 0.450 0.225
0

1:2

2

2:3

3

3:4

4

4:5

5

5:6

6

6:7

7

7:8

8

8:9

9

9:10

10

10:11

11

11:12

12

12:13

13

13:14

14

14:15

15

15:16

16

16:17

17

17:18

18

18:19

19

19:20

20

single topic two topic combinations three topic combinations four topic combinations five topic combinations

20:21

21

21:22

22

1:2:3 2:3:4 3:4:5 4:5:6 5:6:7 6:7:8 7:8:9 8:9:10 9:10:11 10:11:12 11:12:13 12:13:14 13:14:15 14:15:16 15:16:17 16:17:18 17:18:19 18:19:20 19:20:21 20:21:22 21:22:23

1:2:3:4 2:3:4:5 3:4:5:6 4:5:6:7 5:6:7:8 6:7:8:9 7:8:9:10 8:9:10:11 9:10:11:12 10:11:12:13 11:12:13:14 12:13:14:15 13:14:15:16 14:15:16:17 15:16:17:18 16:17:18:19 17:18:19:20 18:19:20:21 19:20:21:22 20:21:22:23

1:2:3:4:5 2:3:4:5:6 3:4:5:6:7 4:5:6:7:8 5:6:7:8:9 6:7:8:9:10 7:8:9:10:11 8:9:10:11:12 9:10:11:12:13 10:11:12:13:14 11:12:13:14:15 12:13:14:15:16 13:14:15:16:17 14:15:16:17:18 15:16:17:18:19 16:17:18:19:20 17:18:19:20:21 18:19:20:21:22 19:20:21:22:23

Figure 2: AUP scores for the five experiments: onetopic to five-topic.

Table 2: Summary of Results

topics:

one two three four

per. increase (%) 10.47 33.9 45.68 50.24

st. deviation

9.83 17.92 22.68 23.55

paired t-test

.001 .001 .001 .001

av. no. terms

1123.2 1820.5 2333.5 2750.1

five 46.39 19.95
.001 3094.0

profile comprising the extracted weighted terms. The second type, is the proposed network-based profile comprising exactly the same weighted terms, but an additional process is deployed to generate the links between profile terms and calculate their weights. In particular, a sliding window approach is used to define the context of terms in text and identify their co-occurences. The window defines a span of 10 contiguous terms3. Every time two profile terms appear within the window in the training documents, a link between them is established. The weight wkn of the link between two terms k and n is calculated using the following equation, which is similar to the one adopted in [12], extended with an additional factor based on the average distance, i.e., the number of terms that intervene between k and n in the sliding window.

wkn

=

f rk2n f rk · f rn

·

1 dkn

(2)

where: f rkn
f rk and f rn
dkn

is the number of times k and n cooccur within the sliding window
are respectively the number of occurrences
of k and n in the training documents is the average distance between k and n, within the sliding window.

To evaluate each document in the collection, the same sliding window is deployed, so that only terms found in the same context get activated. Terms in the document are not weighted. Every position of the window defines a portion of the document's text. In the case of the vector-based profile, we assign a relevance score to the window by calculating the inner product between the profile and a keyword vector comprising the terms in the window. The network-based profile assigns a relevance score to the window using the proposed spreading activation process. In both cases, the result of the document evaluation process is a relevance score for each position of the window in the document's text, which indicates the distribution of relevance through out the document. For practical purposes though, we calculate a single relevance score for each document as the sum of the individual window scores, normalised to the logarithm of the number of terms in the document. It is important to note, that since both types of profile comprise exactly the same weighted terms, any difference in their performance is due only to the additional relevance that links contribute to documents, according to equation 1.

4.2 Experimental Results
Figure 2 includes five graphs one for each of the singletopic, two-topic, three-topic, four-topic and five-topic experiments. The x-axis shows the serial numbers of topics or topic combinations (e.g., 1:2 corresponds to earn:acq) and
3The window's size was chosen based on systematic experiments.

207

0.50

0.38

average AUP

0.25

0.13

average number or profile terms

0

0

250

500

750 1000 1250 1500 1750 2000 2250 2500

Figure 3: Average AUP for different numbers of profile terms.

the y-axis the corresponding AUP value, or combined AUP value in the case of multi-topic experiments. Table 2 summarises for each of the five experiments the average percent increase, the standard deviation, the p value of the paired, two-tailed t-test, and the average number of profile terms.
The results clearly show that as the number of topics of interest increases, causing an increase in the number of profile terms, the network-based profile achieves significant performance improvements of up to 50.2% on average, over the vector-based profile. These differences are consistent through out the 23 topics, as highlighted by the standard deviation, and are statistically significant, since all the p values of the t-test are less than 0.05. It is also evident, that the difference is more pronounced for topics with a small number of relevant documents in the collection, where specificity is more important. As expected, the achieved AUP values get smaller as the number of topics of interest increases, because the IF task becomes more difficult.
The observed differences in performance are not only substantial, but also of computational interest, because they are due only to the existence of links in the network-based profile, since both types of profile comprise exactly the same weighted terms. Furthermore, although not reported here due to space limitation, we have also performed the same experiments with a different term weighting method, called Relative Document Frequency (RelDF) [13]. The results achieved for RelDF were overall worse than those presented here for IG, but the network-based profile achieved improvements of up to 75% on average, over the vector-based profile4. It is also evident, that the observed differences in AUP scores relate to the increase in the number of profile terms required to represent multiple topics of interest.
To further investigate this effect we repeated the experiments for different numbers of profile terms. In particular, we present here results for the three-topic experiments, where we progressively increase a threshold and we only extract from the training documents terms with weight larger than this threshold. Table 3, summarises the average AUP values of the vector-based and network-based profiles for different numbers of profile terms, the percent increase, the standard deviation and the p value of the paired, two-tailed t-test. Figure 3 presents a plot of the average AUP (columns 3 and 4 in table 3) on the y-axis, as the average number
4All experimental results can be found at http://www.scribd.com/IF SIGIR10

Table 3: 3-Topic Experiment: overall results for dif-

ferent threshold values

threshold av. no. vector network increase st.

t-test

(×10-6) of terms

(%) dev. (p-value)

0 2333.5 0.329 0.469 45.68 22.68 4.17E-10

100 2308.7 0.330 0.469 45.64 22.58 3.91E-10

300 1976.6 0.330 0.465 43.69 21.52 4.17E-10

500 1489.7 0.330 0.456 40.96 20.30 6.43E-10

1000 763.0 0.329 0.437 34.91 18.19 2.61E-09

1500 510.7 0.327 0.422 30.73 17.04 2.61E-09

2000 369.0 0.325 0.409 27.26 15.90 2.89E-08

3000 228.0 0.319 0.388 22.41 14.19 1.74E-07

4000 154.2 0.312 0.368 18.46 13.80 2.06E-06

5000 111.3 0.308 0.354 15.40 12.41 7.87E-06

6000

82.2 0.302 0.341 13.26 11.77 2.12E-05

7000

62.9 0.294 0.330 12.00 10.67 5.23E-05

8000

48.4 0.285 0.314 10.01 9.83 1.78E-04

9000

37.9 0.274 0.299

9.17 9.35 2.65E-04

10000

31.1 0.269 0.295

9.33 8.95 1.96E-04

15000

13.2 0.212 0.228

6.16 9.35 7.35E-03

of profile terms (column 2 in table 3) increases on the xaxis. When the number of profile terms is small the AUP scores of the two types of profile are also small, because there is a limited scope for capturing three topics using a small number of terms. As the number of profile terms increases the AUP scores of both profile types increases, but in the case of the vector-based profile the curve quickly flattens. Approximately, after extracting the first 400 terms with the highest weights the remaining 2000 terms do not essentially contribute to the profile's accuracy. On the contrary, the AUP score of the network-based profile increases more rapidly with the increase in the number of terms and keeps on increasing until all terms with positive weight have been incorporated. The p-values show that as the number of profile terms increases the confidence in the comparison also increases. Although not reported here due to space limitations, these differences are more pronounced for more topics of interest.
These findings are revealing, because they demonstrate that the network-based profile can effectively incorporate a large number of terms (or features in general). Unlike the vector-based profile that does not distinguish between relevant and non-relevant term combinations, the term network can exploit the additional information about the user's interests, that a large number of terms and their correlations encode. We are confident that these findings generalise beyond textual information, to any type of information that can be described or associated with correlated features. A user profile that is "resistant" to dimensionality problems can not only represent multiple topics of interest more accurately than a vector-based profile, but in principle, it can incorporate a greater diversity of features, including context dependent ones. A large number of features is no longer a problem, but an advantage that can be exploited to incorporate, in the profile, additional features (such as tags) that have been assigned to, or can be automatically extracted from information items. In this way, the scope of IF can be easily extended beyond textual information and the specificity of a user profile can be augmented.
5. SUMMARY AND FUTURE WORK
The problem of information overload still impedes the dissemination of information on today's Web, where everyone can be both a receiver and a transmitter of information.

208

IF has an important role to play in achieving personalised information delivery to ensure that the right information reaches the right people. However, unlike the success stories of Collaborative Filtering that produced popular applications for recommending books, movies and music tracks, content-based IF has not yet lead to the development of widely adopted Web applications for personalised information delivery. In this paper, we argued that one possible explanation is the reliance on the VSM, which leads to inherent dimensionality problems. Instead, we proposed an alternative network-based model for profile representation that, in the case of textual information, captures correlations between terms appearing in the same context. The network profile can evaluate any portion of text with a directional spreading activation process.
We performed a series of experiments comparing the network profile to a vector-based profile containing the same weighted terms. Unlike existing evaluation practices, our experimental methodology simulates users with multiple concurrent interests. Representing multiple topics of interest requires a large number of profile terms and reveals the dimensionality problems of the VSM. The existence of links allows the network profile to capture additional information about the user's interests, become more specific and achieve significant performance improvements. We specifically investigated the effect of the number of terms on the profile's performance and found out that after just a few hundred terms the vector-based profile reaches its maximum representational capacity, while the network profile can effectively incorporate thousands of terms.
This is a significant property that extends beyond textual information and terms as features. We envision user profiles that do not only incorporate the necessary number of terms for representing a user's multiple interests, but are also hybridised with additional features, such as tags or even user ratings. In this way, the user profile will become more specific to the user's interests and will enable a variety of personalisation services. Given that the proposed model is also distributed and dynamic, it proposes a new perspective towards IF and may establish a new research paradigm. This paper is part of ongoing effort towards this direction, that involves further experiments, theoretical analysis and real world prototype implementations.
6. REFERENCES
[1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On the surprising behaviour of distance metrics in high dimensional space. Database Theory -- ICDT 2001, Volume 1973/2001:420­434, 2001.
[2] T. Ault and Y. Yang. knn, rocchio and metrics for information filtering at trec-10. In TREC, 2001.
[3] R. Bellman. Adaptive Control Processes: A Guided Tour. Princeton University Press, 1961.
[4] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. Wadsworth International Group, Belmont, CA, 1984.
[5] P. W. Foltz. Using latent semantic indexing for information filtering. In Proceedings of the ACM SIGOIS and IEEE CS TC-OA conference on Office information systems, pages 40­47, 1990.
[6] W. P. Jones and G. W. Furnas. Pictures of relevance: A geometric analysis of similarity measures. Journal of

the American Society of Information Science, 38(6):420­442, May 1986. [7] C. McEwan and E. Hart. Representation in the (artificial) immune system. Journal of Mathematical Modelling and Algorithms, 8(2):125­149, 2009.
[8] N. Nanas and A. De Roeck. Autopoiesis, the immune system and adaptive information filtering. Natural Computing, 8(2):387­427, 2009.
[9] N. Nanas, S. Kodovas, and M. Vavalis. Revisiting evolutionary information filtering. To appear in Congress on Evolutionary Computation, 2010.
[10] N. Nanas, M. Vavalis, and A. De Roeck. What happened to content based information filtering? In Advances in Information Retrieval Theory, Second International Conference on the Theory of Information Retrieval (ICTIR 2009), pages 249­256, 2009.
[11] N. Nanas, M. Vavalis, and L. Kellis. Immune learning in a dynamic information environment. In Artificial Immune Systems, 8th International Conference (ICARIS 2009), pages 192­205, 2009.
[12] Y. C. Park and K.-S. Choi. Automatic thesaurus construction using bayesian networks. Information Processing and Management., 32(5):543­553, 1996.
[13] M. F. Porter. Implementing a probabilistic information retrieval system. Information Technology: Research and Development, 1:131­156, 1982.
[14] S. Robertson and I. Soboroff. The TREC 2001 filtering track report. In The Tenth Text Retrieval Conference (TREC-10), pages 26­37, 2001.
[15] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill Inc., 1983.
[16] M. Sanderson and B. W. Croft. Deriving concept hierarchies from text. In 22nd ACM SIGIR Conference, pages 206­213, 1999.
[17] R. Schapire, Y. Singer, and A. Singhal. Boosting and Rocchio applied to text filtering. In 21st ACM SIGIR Conference, pages 215­223, 1998.
[18] H. Sorensen, A. O' Riordan, and C. O' Riordan. Profiling with the informer text filtering agent. Journal of Universal Computer Science, 3(8):988­1006, 1997.
[19] C. J. van Rijsbergen. A theoretical basis for the use of co-occurrence data in information retrieval. Journal of Documentation, 33(2):106­199, 1977.
[20] G. I. Webb, M. J. Pazzani, and D. Billsus. Machine learning for user modeling. User Modeling and User-Adapted Interaction, 11:19­29, 2001.
[21] D. H. Widyantoro, T. R. Ioerger, and J. Yen. An adaptive algorithm for learning changes in user interests. In ACM/CIKM'99 Conference, pages 405­412, 1999.
[22] Y. Yang, A. Lad, N. Lao, A. Harpale, B. Kisiel, and M. Rogati. Utility-based information distillation over temporally sequenced documents. In 30th ACM SIGIR Conference, pages 31­38, 2007.
[23] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In 14th International Conference on Machine Learning (ICML '97), pages 412­420, 1997.
[24] Y. Zhang. Using bayesian priors to combine classifiers for adaptive filtering. In 27th ACM SIGIR Conference, pages 345­352, 2004.

209

Temporal Diversity in Recommender Systems
Neal Lathia§, Stephen Hailes§, Licia Capra§, Xavier Amatriain
§Dept. of Computer Science, University College London, Gower Street WC1E 6BT, UK Telefonica Research, Via Augusta 177, Barcelona 08021, Spain
n.lathia, s.hailes, l.capra@cs.ucl.ac.uk, xar@tid.es

ABSTRACT
Collaborative Filtering (CF) algorithms, used to build webbased recommender systems, are often evaluated in terms of how accurately they predict user ratings. However, current evaluation techniques disregard the fact that users continue to rate items over time: the temporal characteristics of the system's top-N recommendations are not investigated. In particular, there is no means of measuring the extent that the same items are being recommended to users over and over again. In this work, we show that temporal diversity is an important facet of recommender systems, by showing how CF data changes over time and performing a user survey. We then evaluate three CF algorithms from the point of view of the diversity in the sequence of recommendation lists they produce over time. We examine how a number of characteristics of user rating patterns (including profile size and time between rating) affect diversity. We then propose and evaluate set methods that maximise temporal recommendation diversity without extensively penalising accuracy.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Filtering
General Terms
Algorithms, Performance, Theory
Keywords
Recommender Systems, Evaluation
1. INTRODUCTION
Recommender systems have become essential navigational tools for users to wade through the plethora of online content. Many of them use collaborative filtering (CF) algorithms, that compute recommendations using the ratings that each user has input. CF algorithms are often evaluated according to how accurately they predict these ratings,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$5.00.

or by measuring the precision and recall of ranked recommendations [1]. However, the set of evaluation measures available disregard the fact that the process of inputting ratings and receiving recommendations happens iteratively over time. Users update their profiles in an incremental manner, rating content as they consume it, and typical CF algorithms are retrained regularly (e.g., weekly [2]) to reflect this. As recommender systems grow dynamically, a problem arises: current evaluation techniques do not investigate the temporal characteristics of the produced recommendations. Researchers have no means of knowing whether, for example, the system recommends the same items to users over and over again, or whether the most novel content is finding its way into recommendations. The danger here is that, as results may begin to stagnate, users may lose interest in interacting with the recommender system.
In this work, we investigate one dimension of recommendations: the diversity of top-N lists over time. We first examine why temporal diversity may be important in recommender system research (Section 2) by considering temporal rating patterns and the results of a user survey. Based on these observations, we evaluate three CF algorithms' temporal diversity from 3 perspectives (Section 3): by comparing the intersection of sequential top-N lists, by examining how diversity is affected by the number of ratings that users input, and by weighing-in the trade-off between accuracy and diversity over time. We finally design and evaluate a hybrid mechanism to promote temporal diversity (Section 4), comparing its performance to a range of baseline techniques. We conclude in Section 5 by discussing future research directions.
2. WHY TEMPORAL DIVERSITY?
We explore the importance of temporal diversity from two perspectives: (a) changes that CF data undergoes over time and (b) how surveyed users respond to recommendations with varying levels of diversity.
(a) Changes Over Time. Recommender systems grow over time: new users join the system and new content is added as it is released. Pre-existing users can update their profiles by rating previously unrated content; the overall volume of data thus grows over time. As a consequence of the continuous influx of ratings, any summary statistics related to the recommender system's content may also change. We visualised these changes in the Netflix prize dataset1, a large-scale collection of ratings for movies. The ratings in
1http://www.netflixprize.com

210

(a) Movie Growth

(b) User Growth

Figure 2: Survey Results for (S1) Popular Movies With No Diversity (S2) Popular Movies With Diversity and (S3) Randomly Selected Movies

(c) Average

(d) Std Dev

Figure 1: Growth of Netflix Dataset Over Time & Average/Standard Deviation of Returning Users' Ratings Input Per Week

this set span a total of 2, 243 days. We plot the movie and user growth in Figures 1(a) and 1(b) respectively; from these plots we see that there is a continuous arrival of both new users and movies. While new movies seem to be added at a relatively linear pace, the number of users grows exponentially over time. We now turn to the rating frequency: in Figures 1(c) and 1(d) we plot the average and standard deviation of ratings input by returning users (i.e., users who have already previously rated at least once) per week. The plots show the high variability in how users interact with the recommender system. After the initial high fluctuation in average user ratings per week, the mean value flattens out at approximately five ratings per user per week. However, the standard deviation of this mean falls within [5, 20]: rating behaviour also varies over time, and viewing rating sets from a non-temporal viewpoint does not account for these changes. These changes affect the ratings' summary statistics: in [3], Koren shows how global summary statistics vary over time. Similarly, our previous work looks at how changes are further reflected in the global rating median and mode [4]. All the summary values fluctuate over time, reflecting how the overall distribution of ratings shifts as more users interact with the system.
What do we learn from observing these changes? The datasets do not only remain incredibly sparse, but they also do not stabilise; recommender systems continuously have to make decisions based on incomplete and changing data, and the range of the changes we observe in the Netflix data have a strong impact on the predictability of ratings. Furthermore, the continuous rating of content means that the data that an algorithm will be trained with at any particular time is likely to be different than data it trained with previously.

The question we explore in this paper is: do these changes translate into different recommendations over time?
(b) User Survey. In order to determine whether temporal diversity is important for recommender system users, we designed three surveys that simulate systems that produce popular movie recommendations over the course of five "weeks." We opted to recommend popular movies in order to avoid a variety of confounds that would emerge had we selected a personalised CF algorithm (e.g., the quality of the algorithm itself and the cumbersome process of asking users to rate films). Survey 1 (S1) and Survey 2 (S2) both recommended popular movies drawn from a list of the 100 all time most profitable box office movies2. S1 had no diversity: it consistently recommended the top-10 box office hits. S2's recommendations, instead, did change over time. Each week, approximately seven of the previous week's ten recommendations would be replaced by other movies in the top-100 box office list. Lastly, Survey 3 (S3) recommended movies that were randomly selected from the Netflix dataset: the recommendation process included full diversity, but was very unlikely to recommend popular movies.
Each survey was structured as follows. The users were first queried for demographic data. They were then offered the first week's recommendations, represented as a list of ten movie titles (and the relative DVD covers and IMDB links) and asked to rate these top-10 recommendations on a 1-5 star scale. After submitting their rating, they were presented with a buffer screen containing thirty DVD covers, and had to click to continue to the subsequent week; this aimed at diverting users' attention before presenting them with the next week's recommendations. After rating all five week's worth of recommendations, they were asked to comment on the recommendations themselves and answer a number of questions relating to diversity over time. Users were invited to participate in one or more of the surveys: S1 was completed 41 times, S2 had 34 responses, and S3 was completed 29 times. Due to the surveys' anonymity, we do not know how many users completed more than one survey. We therefore treat each completed survey individually. Of the 104 total responses, 74% of the users were male, 10% were 18-21 years old, 66% were 22-30 years old, and 24% were between 31 and 50 years of age. On average, the users
2http://www.imdb.com/boxoffice/alltimegross

211

claimed to watch 6.01 ± 6.12 movies per month, and while 61% of them said they were familiar with recommender systems, over half of them claimed they used them less than once a month. On the other hand, 29% use recommender systems weekly or daily: our respondents therefore include a wide variety of movie enthusiasts and both people who do and do not use recommender systems.
We averaged the users' ratings for each week's recommendations and plot the results in Figure 2. The S2 results (popular movies with diversity) achieve the highest scores: on average, these five weeks of recommendations were rated 3.11 ± 0.08 stars. The low temporal standard deviation reflects the fact that the rating trend remains relatively flat; the average for each week is about 3 stars. S3's results (randomly selected movies), were consistently disliked: the average rating peaks at 2.34 stars for week 5. In fact, some users commented on the fact that recommendations "appeared to be very random," "varied wildly" and the system "avoid[ed] box office hits." The main result of our surveys is reflected in S1's results (popular movies with no diversity): as the same recommendations are offered week after week, the average ratings monotonically decrease. The average for week 1 was 2.9, which falls within the range of values measured in S2, while by week 5 the average score is 2.3, which is lower than the average score for the random movies. Not all commented on the lack of recommendations diversity; however, most modified their ratings for the recommendations as the lack of diversity persisted. This shows that when users rate they are not only expressing their tastes or preferences; they are also responding to the impression they have of the recommender system. In the case of S1, users commented on the fact that the algorithm was "too naive" or "not working," and the lack of diversity "decreased [the respondent's] interest."
In order to test the statistical significance of the three different survey's results, we performed an analysis of variance (ANOVA): in this case, the null hypothesis is that the ratings for each survey are the same. We can reject the null hypothesis with 99% confidence with p-values less than 0.01: the p-value we measured for the three methods is 9.72e -14. A pairwise t-test between each survey further shows that the ratings input for each survey cannot be attributed to the sampling of the study; we omit the boxplots due to lack of space.
The final part of the surveys asked users about qualities they sought in recommendations. Overall, 74% said it was important for recommender systems to provide results that accurately matched their taste (23% selected the 'neutral' option to this question). 86% said it is important for recommendations to change over time; in fact, 95% stated it is important that they are offered new recommendations. It thus quickly becomes apparent that temporal diversity is a highly important facet of recommender systems, both in terms of the direct responses and rating behaviour of the surveyed users. In the following sections, we evaluate the temporal diversity of three state of the art CF algorithms.
3. EVALUATING FOR DIVERSITY
Given the above, we now aim to examine how diverse CF algorithms are over time. We focus on three algorithms: a baseline, where a prediction for an item is that item's mean rating, the item-based k-Nearest Neighbour (kNN) algorithm, and a matrix factorisation approach based on

Singular Value Decomposition (SVD). Due to limited space, we do not detail each algorithm; Adomavicius and Tuzhilin [5] thoroughly review these and many more approaches. We chose these algorithms since they not only reflect state-ofthe-art CF, but also each manipulate the rating data in a different way and may thus produce varying recommendations.
All of the algorithms share a common theme: they produce predicted ratings that can then be used to recommend content. The idea is to use the predictions in order to generate a personalised ranking of the system's content for each user (note that, in our scenario, items that have been rated cannot be recommended). However, it may be the case that items share the same predicted rating. For example, a number of items may all have 5-star predictions. In this case, the predicted rating alone is not conducive to a meaningful ranking. We solve this problem by introducing a scoring function to rank items, regardless of the model used to generate predicted ratings. The scoring function uses two pieces of information: the predicted rating, and the confidence in the prediction (i.e., number of data points used to derive it) as used in [6]. Assuming a 5-star rating scale, we first subtract the scale mid-point (3 stars) from the prediction and then multiply by the confidence:

sr^,i = (r^u,i,t - 3.0) × conf idence(r^u,i,t)

(1)

This scoring function ensures that items with high prediction and confidence are promoted, and low prediction with high confidence are demoted. For example, an item with a predicted 5 star rating, derived from 2 ratings, will be ranked lower than another item with a 4 star prediction based on 50 ratings. If two items had the same score, then we differentiated them based on their respective average rating date: the item that had been rated more recently is ranked higher. The greatest advantage of this method, as detailed in [6], is the heightened explainability of recommendations.
Methodology. In order to examine the sequence of recommendations produced by a system, we explore CF algorithms that iteratively re-train on a growing dataset. Given a dataset at time t and a window size µ (how often the system will be updated), we train the algorithm with any data input prior to t and then predict and rank all of the unrated items for each user. The t variable is then incremented by µ, and the entire process is repeated, except that now the latest ratings become incorporated into the training data. In other words, at time t we generate a set of top-N lists--corresponding to the top-N recommendations each user would receive--in order to then examine how the sequence of ranked items that we produce will vary as the system is updated. This method includes a number of advantages: we test the algorithms as data grows (and view more than a single iteration of this process), making predictions based only on ratings that are currently available. We simulate the iterative update of deployed systems, and stay true to the order users input ratings.
Since users do not necessarily log-in consistently to the system, we cannot be certain that each top-N list would have been viewed by each user. We therefore only generate a top-N list for the users who will rate at least one item in time (t + µ); we assume that if the user is rating an item then they have logged into the system and are likely to have seen their recommendations. The benefit of this is that we compare the current recommendations to those

212

that users are likely to have seen before. It remains possible that users viewed their recommendations without rating any content; however, given this uncertainty in the data, we only consider the scenario where there is evidence that the users have interacted with the system.
We use 5 subsamples of the Netflix dataset for crossvalidation purposes. Each subsample includes 50, 000 users and any rating input (by any user) prior to a pre-defined "edge" time = 500 days after the first date in the Netflix dataset. Our final subsets have approximately 60, 000 users; setting the value as we did is roughly equivalent to bootstrapping a new recommender system with 10, 000 users rating content for over a year (we thus hope to avoid settings where our results will be skewed by system-wide cold start problems). We set the window size µ = 7 days; the system will be updated weekly [2]. Since the Netflix data spans a total of 2, 243 days, selecting µ = 7 days allows us to explore 249 system updates.

3.1 Measuring Diversity Over Time

We define a way to measure the diversity between two

ranked lists as follows. Assume that, at time t, a user is

offered a set of 10 recommendations. The next time the user

interacts with the system only 1 of the 10 recommendations

is different. Therefore, the diversity between the two lists

is

1 10

= 0.1.

More formally, given two sets L1 and L2, the

set theoretic difference (or relative complement) of the sets

denotes the members of L2 that are not in L1:

L2\L1 = {x  L2|x / L1}

(2)

In our example above, only 1 of the 10 recommendations was not the same: the set theoretic difference of the two recommendation lists has size 1. We thus define the diversity between two lists (at depth N) as the size of their set theoretic difference over N :

diversity(L1, L2, N )

=

|L2\L1| N

(3)

If L1 and L2 are exactly the same, there is no diversity: diversity(L1, L2, N ) = 0. If the lists are completely different, then diversity(L1, L2, N ) = 1. This measure disregards the actual ordering of the items: if a pair of lists are re-shuffled copies of each other, then there continues to be no diversity. However, we can measure the extent that recommendations change as a result of the same content being promoted or demoted by measuring diversity at varying depths (N ).
One of the limitations of this metric is that it measures the diversity between two lists; it only highlights the extent that users are being sequentially offerered the same recommendations. In order to see how recommendations change, in terms of new items appearing in the lists, we define a top-N list's novelty. Rather than, as above, comparing the current list L2 to the previous list L1, we compare it to the set of all items that have been recommended to date (At):

novelty(L1, N )

=

|L1\At| N

(4)

In this context, we define novelty in terms of what items have been previously recommended to a user. A list's novelty will be high if all of the items have never been recommended before, and low if all of the items have been recommended at some point in the past (not just in the last update). We further define the average diversity t and average novelty

(a) Top-10 Diversity

(b) Top-20 Diversity

(c) Top-10 Novelty

(d) Top-20 Novelty

Figure 3: Top-10 and 20 Temporal Diversity and Novelty for Baseline, kNN and SVD CF

t that is generated by a given CF algorithm (at time t) as the average of the values computed between all the current top-N lists and the respective previous list for each user.
3.2 Results and Analysis
We computed t and t for each of the 3 algorithms over all 249 simulated system updates outlined in Section 3, and report the results for the top-10 and top-20 recommendations in Figure 3. These results provide a number of insights into recommender system's temporal diversity. As expected, the baseline algorithm produces little to no diversity. On average, users' top-10 recommendations differ by (at most) one item compared to the previous recommendations. Both the factorisation and nearest neighbour approaches increment diversity; furthermore, the kNN algorithm is, on average, consistently more diverse than the sequence of recommendations produced by the SVD.
The novelty values (Figures 3(c) and 3(d)) are lower than the average diversity values. That means that when a different recommendation appears, it is more often a recommendation that has appeared at some point in the past, rather than something that has not appeared before. There are a variety of factors that may cause this; for example, new items may not be recommended because they lack sufficient ratings: the CF algorithm cannot confidently recommend them. However, this metric does not tell us whether the new recommendations are new items to the system, or simply content that has (to date) not been recommended. A full analysis of the novelty of recommendation warrants a closer inspection of when items join the system and when they are recommened. In order to focus our analysis, we thus separate the problems of recommending new content from that of diversifying sequential recommendations: in this work, we focus on the latter.

213

(a) Baseline

(b) kNN

(c) SVD

Figure 4: Profile Size vs. Top-10 Temporal Diversity for Baseline, kNN and SVD CF

(a) Baseline

(b) kNN

(c) SVD

Figure 5: Ratings Added vs. Top-10 Temporal Diversity for Baseline, kNN and SVD CF

Both Figure 3(a) and 3(b) also look very similar: the di-

versity values for the top-10 and top-20 recommendations

are nearly the same. In order for this to happen (i.e., for a

comparison between two top-10 lists and two top-20 lists to

produce the same value) there must be more diversity be-

tween the larger lists. For example, if only 1 item changes in

the

top-10,

the

diversity

is

1 10

= 0.1,

and

the

pair

of

top-20

lists will only produce this diversity value if 2 items have

changed,

2 20

.

What

this

means

is

that

not

all

of

the

changes

in the item rankings are occuring in the top-10: new items

are also being ranked between the 11th and 20th positions.

At the broadest level, we thus observe that (a) both the

baseline and SVD produce less temporal diversity than the

kNN approach, and (b) across all CF algorithms, diversity

is never higher than appproximately 0.4. However, these

are averaged results across many users, who may be each

behaving in very different ways: we now perform a finer

grained analysis of temporal diversity to explore the relation

between users and the diversity they experience.

Diversity vs. Profile Size. The metric in Section

3.1 does not factor in the fact that the distribution of rat-

ings per user is not uniform. Some users have rated a lot of

items, while others have very sparse profiles. Users' profile

size (i.e., the number of ratings per user) may affect their

recommendation diversity. We therefore binned the above

temporal results according to users' current profile size and

then averaged the diversity of each group. We plot the re-

sults in Figure 4. The baseline (Figure 4(a)) continues to show next to no diversity, regardless of how many items users have rated. The rationale behind this is that the only profile information that the baseline factors in when it computes recommendations is whether the user has rated one of the popular items; results will only be diverse if the user rates all the popular content. The kNN (Figure 4(b)) and SVD (Figure 4(c)) results, instead, show a negative trend: diversity tends to reduce as users' profile size increases.
Diversity vs. Ratings Input. Our temporal diversity metric is based on pairwise comparisons; we compare each sequential pair of top-N lists. One factor that may thus play an important role when determining how diverse a pair of lists will be from one another is how much the user rates in a given session. For example, one user may log in and rate two items while another may log in and rate fifty; the temporal diversity that each user subsequently experiences may be affected by these new ratings. We therefore binned users according to how many new ratings they input, and plot the results in Figure 5. As before, the baseline remains unaffected by how many new ratings each user inputs. The kNN (Figure 5(b)) and SVD (Figure 5(c)), instead, show a positive trend. These results can be interpreted as follows: the more you rate now, the more diverse your next recommendations will be in the next session.
Diversity and Time Between Sessions. The previous analysis was concerned with how diversity is influenced

214

(a) Baseline

(b) kNN

(c) SVD

(d) RMSE vs. Diversity

Figure 6: Time Passed vs. Top-10 Temporal Diversity for Baseline, kNN and SVD CF and Comparing Accuracy with Diversity

by a single user rating content. However, users do not rate alone: an entire community of users rate content over extended periods of time. We highlight this point with an example: some users may consistently log in and rate items every week; others may rate a few items now and not return for another month (and, in their absence, other users will have continued rating). In other words, diversity may be subject to the time that has passed from when one list and the next are served to the user. In order to verify this, we binned our diversity results according to the number of weeks that had passed between each pair of lists, and plot the results in Figure 6. In this case, all three of our algorithms show a positive trend: the longer the user does not return to the system, the more diversity increases. Even the baseline diversity increases: if a user does not enter the system for a protracted period of time, the popular content will have changed.
Lessons Learned. Overall, average temporal diversity is low. Ranking content based on popularity offers next to no diversity, while the kNN method produces the largest average temporal diversity. Larger profile sizes negatively affect diversity; it seems that users who have already rated extensively will see the least diverse recommendations over time. Pairwise diversity between sequential lists is largest when users rate many items before receiving their next recommendations; users should be encouraged to rate in order to change what they will be recommended next. Diversity will naturally improve as users extend the time between sessions when they interact with the system (even popular content eventually changes).
The final question we ask is how diversity relates to accuracy, the metric of choice is traditional CF research. To do so, we take the predictions we made at each update, and compute the Root Mean Squared Error (RMSE) between them and the ratings the visiting users will input. We then plot RMSE against average diversity in Figure 6(d). A plot of this kind has four distinct regions: low accuracy with low diversity (bottom right), high accuracy with low diversity (bottom left), low accuracy with high diversity (top right), and high accuracy with high diversity (top left). We find that the results for each algorithm cluster into different regions of the plot, corresponding to the different diversity results that they obtain. In terms of RMSE, different algorithms often overlap; for example, kNN CF is sometimes less accurate than the baseline. The baseline sits toward the

bottom right of the plot: it offers neither accuracy nor diversity. The SVD, on the other hand, tends to be more accurate than the baseline, although there is little diversity gain. The kNN results, finally, sit between the two others--in terms of accuracy--and above them when considering diversity.
Based on what we have observed, it seems that improving the temporal diversity of a recommender system is an important task for system developers. In the following section, we describe and evaluate a number of techniques that may be implemented to do so. We then discuss the potential implications that modifying top-N lists to promote diversity may have.
4. PROMOTING TEMPORAL DIVERSITY
The easiest way of ensuring that recommendations will be diverse is to do away with predicted ratings and simply rank items randomly. However, diversity then comes at the cost of accuracy: recommendations are no longer personalised to users' tastes. The random survey (Section 2) showed that this is not a viable option, since the recommendations were rated very low. We can thus anticipate that, when promoting diversity, we must continue to take into account users' preferences. We do so with two methods: temporal hybrid switching, from a system (a) and user (b) perspective, and re-ranking individual users' recommendations (c).
(a) Temporal Switching. Many state of the art approaches to CF combine a variety of algorithms in order to bolster prediction accuracy [5]. However, as described by Burke [7], another approach to building hybrid CF algorithms is to switch between them. Instead of combining prediction output, a mechanism is defined to select one of them. The rationale behind this approach is as follows: given a set of CF algorithms, that each operate in a different way, are likely to produce different recommendations for the same user; the top-N produced by a kNN may not be the same as that produced by an SVD. We thus switch between the two algorithms: we cycle between giving users kNN-based recommendations one week, and SVD-based recommendations the following week.
We plot the top-10 diversity over time for this switching method in Figure 7(a). Diversity has now been incremented to approximately 0.8: on average, 8 of the top-10 recommendations ranked for each user is something that was not recommended the week before. How does this affect accuracy? Intuitively, the overall accuracy that the system will

215

(a) Switching Temporal Di- (b) Switching RMSE vs Di-

versity

versity

Figure 7: Diversity (a) and Accuracy (b) of Temporal Switching Method

(a) Temporal Diversity (b) RMSE vs. Diversity
Figure 8: Temporal Diversity and Accuracy vs. Diversity With User-Based Temporal Switching

achieve will be somewhere between the accuracy of each individual algorithm. We compare the accuracy and diversity of our switching technique in Figure 7(b). The results for the switching method now cluster into two groups; each group lies above the candidate algorithms we selected. In other words, accuracy fluctuates between the values we reported for kNN and SVD CF, but the fact that we are switching between these two techniques ensures that diversity has been greatly increased.
(b) Temporal User-Based Switching. The method described in the previous section is very straightforward: the system changes the CF algorithm that is used from one week to the next in order to favour diversity. However, this method does not take into account how users behave; in particular, we previously noted that not all users have regular sessions with the recommender system. In fact, if their sessions were every other week, then the switching technique described in the previous section would be of no use at all. We therefore also tested a user-based switching algorithm. It works as follows: the system keeps track of when a user last appeared, and what algorithm was used to recommend content to that user during the last session. When the user reappears, the system simply picks a different algorithm from what it previously used. As before, we switched between using an item-based kNN and an SVD-based approach in our experiments. The results are shown in Figure 8.
The temporal diversity (Figure 8(a)) is now near 1: on average, users are being offered different recommendations to what they were shown the last time they interacted with the system. On the other hand, accuracy (Figure 8(b)) now falls between the kNN and SVD results. In other words, we sacrifice the low-RMSE of the SVD, but still do better than simply using the kNN approach: in return, the average diversity has been greatly amplified.
The only overhead imposed by user-based switching is a single value per user that identifies which algorithm was last used to compute recommendations; however, unlike the temporal switching method in the previous section, we are now required to compute both kNN and SVD at every update, albeit for a subset of users. We do not consider this to be an unsurmountable overhead, given that state of the art algorithms already tend to ensemble the results of multiple CF algorithms.
(c) Re-Ranking Frequent Visitors' Lists. An immediate problem with a temporal switching approach is that it requires multiple CF algorithm implementations. In this

(a) Temporal Diversity (b) RMSE vs. Diversity
Figure 9: Temporal Diversity and Accuracy vs. Diversity When Re-Ranking Frequent Visitors' Lists
section, we provide a means of diversifying recommendations to any desired degree of diversity when only a single CF algorithm is used.
One of the observations we made above is that users who have very regular sessions with the recommender system have low top-N temporal diversity. One way of improving overall average temporal diversity thus entails catering to the diversity needs of this group. To do so, we take advantage of the fact that they are regular visitors, and only re-rank their top-N recommendations.
The re-ranking works in a very straightforward manner: given a list that we wish to diversify with depth N (e.g., N = 10), we select M , with N < M (e.g., M = 20). Then, in order to introduce diversity d into the top-N , we replace (d× N ) items in the top-N with randomly selected items from positions [(N + 1)...M ]. In the case of d = 1, all elements in the first [1...N ] positions are replaced with elements from positions [(N + 1)...M ]. This is the method that we used to diversify the recommendations in the user survey S2 (Section 2); in that case, N = 10 and M = 100 (the 100 all time box office hits).
In our experiments, we opted to re-rank the top-10 results for any users who had previously visited the system less than two weeks before (recall that our system is updated weekly). The temporal diversity results, shown in Figure 9(a), clearly improve the overall average. Furthermore, the accuracy (Figure 9(b)) remains the same: the diversity has simply been shifted in the positive direction. However, how does this not hurt accuracy? There are three points to keep in mind: (a) we are only reranking the lists for frequent visitors, others' recommendations are untouched; (b) the items

216

in the top-N are there due to both high prediction value and high confidence (there is a good chance the user will like those items); and (c) we do not promote items that it is unlikely the user would like would not like (by only reranking the top-M ).
How do these techniques affect recommendation novelty? If we aggregate the temporal results of Figure 3(c), we find that the baseline top-10 recommends, on average, 13.53 ± 2.86 items over time; the SVD top-10 suggests 26.17 ± 12.51 items over time, and the kNN top-10 recommends the highest number of items over time: 79.86 ± 59.33. This ensures that kNN will also produce the highest number of new recommendations. Weekly switching slightly lowers kNN's average, to 75.36 ± 53.98; repeatedly visiting the SVD recommendations reduces the number of total items that can be recommended. However, user based switching maintains the average number of recommended items over time at 79.86 ± 55.12; it essentially highly promotes temporal diversity without impacting the number of new items that enter the top-10 list over time. However, re-ranking bolsters both the average and standard deviation to 97.93 ± 78.82; re-ranking thus seems like a promising approach to solving the related problem of temporal novelty in recommendation.
5. RELATED WORK AND CONCLUSION
Diversity is a theme that extends beyond recommender systems; for example, Radlinski and Dumais examine how it can be used in the context of personalised search [8]. In other cases, diversifying search results is done in order to reduce the risk of query misinterpretation [9]. Similarly, diversity relates to user satisfaction; more specifically, to users' impatience with duplicate results [10]. We have observed similar `impatience' in our survey: users who completed the survey with no diversity began to rate recommendations lower as they saw that they were not changing.
It is certainly possible to envisage a finer grained notion of diversity that takes semantic data into account--by measuring, for example, the extent that the same genre or category of items are being recommended. To that end, diversity may also be measured within a single top-N list, rather than a pair or sequence of recommendations; such a metric may, for example, take into account the number of highly related items (such as a movie and its sequels, or multiple albums by the same artist) that are being simultaneously recommended. For example, Smyth and McClave [11] apply strategies to improve recommender systems based on casedbased reasoning; diversity, in this case, is viewed as the complement of similarity. Zhang and Hurley [12] also focus on intra-list diversity, and optimize the trade off between users' preferences and the diversity of the top-N results. In this work, we focus on the temporal dimension (inter-list diversity) and whether the exact same items are being offered to users more than once; we do not take semantic relationships between the recommended items into account nor improve the diversity of individual top-N lists. However, both lines of research are not in conflict: ideally, one would like a recommender system that offers diverse results that change over time to suit each users' tastes. All of the work we have done here diversifies recommendations based on rating data: additional data (such where users click, or what reviews they read) may further inform this process.
When investigating how recommendations change over time, we found that state of the art CF algorithms generally pro-

duce low temporal diversity; they repeatedly recommend the same top-N items to users. We then defined a metric to measure temporal diversity, based on the set theoretic difference of two sequential top-N lists, and performed a finegrained analysis of the factors that may influence diversity. We found that, while users with large profiles suffer from lower diversity, those who rate a lot of content in one session are likely to see very diverse results the next time. We also observed that diversity will naturally improve if a lot of time passes between user sessions. We then designed and evaluated three methods of improving temporal diversity without extensively penalising recommendation accuracy. Two were based on switching CF algorithm over time; users are first given recommendations produced with (for example) a kNN approach, and then offered the results of an SVD algorithm. The last method was based on re-ranking the results of frequent visitors to the system.
In future work, we plan on extending the evaluation methodology that we have applied here in order to examine how novel items find their way into recommendations, and how user rating patterns can be used to improve recommender system's resilience to attack.
6. REFERENCES
[1] J. Herlocker, J. Konstan, L. Terveen, and J. Riedl. Evaluating Collaborative Filtering Recommender Systems. In ACM TOIS, volume 22, pages 5­53, 2004.
[2] M. Mull. Characteristics of High-Volume Recommender Systems. In Proceedings of Recommenders '06, Bilbao, Spain, September 2006.
[3] Y. Koren. Collaborative Filtering With Temporal Dynamics. In ACM KDD, Paris, France, June 2009.
[4] N. Lathia, S. Hailes, and L. Capra. Temporal Collaborative Filtering With Adaptive Neighbourhoods. In ACM SIGIR, Boston, USA, 2009.
[5] G. Adomavicius and A. Tuzhilin. Towards the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions. IEEE TKDE, 17(6), June 2005.
[6] S.M. McNee, S.K. Lam, C. Guetzlaff, J.A. Konstan, and J. Riedl. Confidence Displays and Training in Recommender Systems. In ACM CHI, 2003.
[7] R. Burke. Hybrid Recommender Systems: Survey and Experiments. User Modeling and User-Adapted Interaction, 12(4), 2002.
[8] F. Radlinski and S. Dumais. Improving Personalized Web Search Using Result Diversification. In ACM SIGIR, Seattle, USA, 2006.
[9] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying Search Results. In ACM WSDM, Barcelona, Spain, 2009.
[10] D. Hawking, T. Rowlands, and P. Thomas. C-TEST: Supporting Novelty and Diversity in Testfiles for Search Evaluation. In ACM SIGIR Workshop on Redundancy, Diversity and Interdependent Document Relevance, Boston, USA, 2009.
[11] B. Smyth and P. McClave. Similarity vs. Diversity. In 4th International Conference on Case-Based Reasoning, Vancouver, Canada, 2001.
[12] M. Zhang and N. Hurley. Avoiding Monotony: Improving the Diversity of Recommendation Lists. In ACM RecSys, Lausanne, Switzerland, 2008.

217

Serendipitous Recommendations via Innovators
Noriaki Kawamae
NTT Comware 1-6 Nakase Mihama-ku Chiba-shi, Chiba 261-0023 Japan
kawamae@gmail.com

ABSTRACT
To realize services that provide serendipity, this paper assesses the surprise of each user when presented recommendations. We propose a recommendation algorithm that focuses on the search time that, in the absence of any recommendation, each user would need to find a desirable and novel item by himself. Following the hypothesis that the degree of user's surprise is proportional to the estimated search time, we consider both innovators' preferences and trends for identifying items with long estimated search times. To predict which items the target user is likely to purchase in the near future, the candidate items, this algorithm weights each item that innovators have purchased and that reflect one or more current trends; it then lists them in order of decreasing weight. Experiments demonstrate that this algorithm outputs recommendations that offer high user/item coverage, a low Gini coefficient, and long estimated search times, and so offers a high degree of recommendation serendipitousness.
Categories and Subject Descriptors
H.3.3 [Information filtering]: Information filtering by ranking
General Terms
Algorithms, experimentation
Keywords
Personalization, Ranking, Serendipitous Recommendations, Innovator, User Flow, Collaborative filtering, Trend
1. INTRODUCTION
Collaborative filtering(CF) aims to improve the user's experience and discovery by providing a better interface to the potentially overwhelming set of choices. The volume of items now exceeds the ability of any individual to accurately assess their desirability. This information overload problem is also
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

serious for many service providers and thus has heightened demands for personalized recommendations. CF is used in web-based services, where there are a vast numbers of items such as Web pages, digital music, and video contents for browsing(download or purchase), and is regarded as one of the most promising recommendation algorithms.
Traditional CF algorithms focus on optimizing accuracy measurements such as precision/recall and neglect other factors, e.g, novelty and serendipity. For example, many CF algorithms use similarity scores between all pairs of users in an attempt to improve the top-N precision, and present items in proportion to their popularity among like-minded users [10] [12]. However, the most popular items in any log collection are the ones that a given user will recognize with high probability, or be broadly familiar with. Hence, highly accurate recommendations appear far too obvious and of little help to users, and seem to fail to enhance user satisfaction [3] [17]. In fact, recommendation systems that appropriately discount popularity lead to an increase in the total sales volume [2] [4]. Moreover, novel and serendipitous recommendations are necessary to broaden the user's view in the recommendation flow.
In this paper, we aim to emphasize the surprise of each user with the recommendation, instead of posing the typical CF task: how well will the user like a candidate item? For this goal, our algorithm focuses on the estimated search time that the user would take to find the item by himself. This algorithm is based on the assumption that an item recently purchased by the innovator will surprise the follower (the other user) more than other items. Namely, this item will have a longer estimated search time than other items meaning that the purchased item would be time-consuming for the user to find. Following this idea, we extend the Personal Innovators Degree (PID) [8] into the Personal Innovator Probability (PIP) for identifying innovators and define a User flow probability (UFP) for measuring how likely the user is to purchase the item just after purchasing an arbitrary item. In ranking candidate items for a given user, this algorithm weights each item by both PIP and UFP scores of the most latest purchases of his/her personal innovators and including trends.
One key advance of our algorithm is that it reduces the time spent by the user in seeking novel items compared to the conventional alternatives. This earned time is in proportion to the difference between the estimated purchase time when the target user is likely to purchase the item without any recommendation and the time when this item is recommended to the user. To weight items in proportion to the

218

value of this difference, we use the time lead and the number of users in both PIP and UFP. We define PIP using the purchase time offset, the number of users, and the relationship over multiple steps. Likewise, UFP is determined by using these time factors and the multiple steps. Consequently, we weight item serendipitousness by innovators and then item novelty by trends, where this weight is in proportion to the length of estimated search time.
2. RELATED WORK
Novelty and serendipity metrics measure the degree of "non-obviousness"of recommendations with the goal of avoiding "cherry-picking" [6]. The system of [16] can infer whether an item, one that is considered relevant, contains any novel information as indicated by five proposed measures that are intended to capture redundancy. Yang et al [15] define novelty in terms of user knowledge, and his/her degree of interest in an item. Ziegler et al [17] propose a topical diversification approach to balance and diversify personalized recommendation lists; the goal is to use intra-list similarity to reflect the complete spectrum of the user's interests. Fouss et al [5] incorporate Euclidean commute time distance, which is one of random-walk-based techniques, when computing the similarities of nodes. Recently, Celma et al [2] presented two complementary methods to analyze and evaluate novel recommendations. The item-centric evaluation method involves analyzing the item-based recommendation network to detect whether the intrinsic topology of the network has a pathology that hinders novel recommendations. The user-centric evaluation aims to measure the perceived quality of the recommendations.
Although these approaches focus on providing novel and serendipitous recommendations, they ignore the dynamics, which describes the changes in user preferences over time. For example, new items are different from old items in terms of their serendipity, even if they share almost the same popularity. While the former items are novel and as such are unknown to many people, the latter items have been recognized but not purchased by them. Identifying these differences is valuable if the serendipitous recommendation flow is to retrieval worthwhile items for the user.
To distinguish these differences, recommendation algorithms need to consider both the change in user preferences and the trends of items in computing user-user and itemitem relationships, respectively. In this paper, we use PIP, which measures how much earlier the innovator purchased the item, to compute user-user relationships and UFP, which measures how likely the user will be to purchase each item just after purchasing an arbitrary item to determine itemitem relationships.
3. RECOMMENDATION FOCUSED ON ESTIMATED PURCHASE TIME
Here, we intuitively explain why the estimated purchase time is useful for improving serendipitousness and explain the ideas behind our algorithm.
A serendipitous recommendation helps the user find a surprisingly interesting item he might not have otherwise discovered [6]. The distinction between novelty and serendipitousness is important when evaluating collaborative filtering algorithms, because the potential for serendipitous recommendations is one facet of collaborative filtering that

Figure 1: Reducing search time by basing recommendations on the estimated purchase time: For example, t^a2 denotes the estimated purchase time when user a would purchase item i2 without a recommendation
traditional content-based information filtering systems do not provide. To provide a clear example of the difference between novelty and serendipitousness, consider the case in which a simple recommendation system presents movies to a user whose favorite director is "James Cameron". If the system recommends his latest movie "Avatar(2009)" to these users, this movie is certainly novel, but probably not serendipitous. Since the user is his fan, he is likely to already know of that movie via advertisements, discussion groups, or like-minded friends, or will find it in the near future. On the other hand, however, a recommender system that shows "Princess Mononoke(1997)", which has themes similar to "Avatar", and which is not an obvious recommendation, provides serendipity to the user. Because this is a foreign movie produced by a different director, many users would take much longer for find it by themselves in the absence of any help. Consequently, serendipitous recommendations need to find the users with well-proven predictive ability (innovators) and use their logs.
First, we propose to use "time" as a novel view for quantifying serendipity. As shown in Figure 1, time here means the difference between the time at which target user ua would purchase items (estimated time) i2(t^a2), i3(t^a3), and the times at which these items are recommended to user t0. Our algorithm is based on the assumption that this difference is useful as a measure of serendipity, since items with long estimated search times will surprise the user. Accordingly, the proposed recommendation algorithm ranks items in decreasing order of this difference; it yields serendipitous items and greatly reduces the time need for the user to find these items. For example, |t^a3-t0| indicates a longer acquisition delay for ua than |t^a2-t0|. Moreover, i3 seems to offer more serendipity to ua than i2. That is, serendipitous recommendations are real time-savers for consumers.
Second, we focus on "innovative" consumers for identifying the user logs that contain serendipitous items. In Figure 1, ub is an innovator for ua. Among like-minded consumers, innovators become aware of items well before their release and purchase these items soon after their release. According to Rogers' innovator theory, the early consumers who focus on undiscovered or unreleased items are called "innovative" [13]. The purchase logs of innovators include more serendipitous items that other like-minded consumers would like to acquire but have not yet become aware of.
Actually, many innovators can be found in CD/DVD purchase logs. Innovators are able to discover even poorlymarketed movies (e.g. Napoleon Dynamite(2004), The Visitor(2007) and The Hangover(2009), which become popu-

219

Figure 2: Typical temporal flow of consumers observed in purchase history logs: The direction denotes the purchase order time
lar long after its release) ahead of others. These movies, once "discovered," are rapidly acquired by like-minded followers. Since Cameron likes the films of Hayao Miyazaki, like "Princess Mononoke","Nausicaa: Valley of the wind(1984)", and "Laputa: Castle in the Sky(1986)", where themes are pacifism, harmony with nature, flying aircraft, floating castles, etc, and has watched these films, innovative consumers get to know this fact and then watch these movies, too. Consequently, innovators have the shortest offset, |t^uij - t0| of innovator ui for item j.
Although innovators discover serendipitous items ahead of others, these items lose their serendipitousness over time. In Figure 1, i3 better satisfies ub's preference than i2, since ub will be purchased i3 after i2. Therefore we incorporate the trend of items in computing item-item relationships. Figure 2 shows a simple example of consumer transitions for three items: ia, i1 and i2. Let ia be the specified item, i.e., the item observed in the log of the target user. Since CF processes user purchase logs to decide which candidate items are to be recommended to the target user, its quality depends on how to weight items i1 and i2 for the user who has purchased ia given the preference order of this user.
Conventional algorithms based on item-item similarity compute the similarity scores among items using the number of consumers who purchased both items. This is because the underlying assumption of these algorithms is that those who agreed in terms of past behavior will tend to agree in the future, and the similarity score is proportional to the degree of agreement. For example, in Figure 2, each item of set i1-3 has been purchased by the same users in common with ia, and thus each item is treated equally according to the similarity defined for ia. Accordingly, CF assigns equal weights to i1-3.
We, however, need to differentiate these items by focusing on the order of item purchase time. Here, i3 was purchased by three users after ia, while i2 was purchased by three users before ia. As stated before, we naturally assume that i3 better matches the future preferences of users who have just purchased ia than i1 or i2, and will be preferred by them. Accordingly, i3 is a more appropriate item for a user who has purchased ia than i1 or i2. Therefore, information on item purchase time is useful in identifying the trend in items. Consequently, it is more reasonable to compare the number of common users as well as purchase times in computing item-item relationships.

4. RECOMMENDATION VIA PERSONAL INNOVATOR PROBABILITY AND USER FLOW PROBABILITY

4.1 Personal Innovator Probability
The aim of personal innovator probability (PIP) is to identify the innovators. We extend PID to model how innovators are followed by multiple users and steps for ranking how likely each user is to be an innovator among all users. This approach is based on the assumption that if uc is an innovator for ua and ud is an innovator for uc, ud seems to be more useful to ua than uc. The aim of PID is to weight user logs appropriately given a target user and to identify logs that match the precedence preference of the target user. PID consists of two time factors and one item factor, and is given by:

X

PID(ub, ua) =

r(i; b, a) × w(i; a) × v(i). (1)

iCab

Here, Cab is the set of common items that both ua and ub have purchased, r(i; b, a) denotes the degree of ub as a personal innovator to ua for common item i, w(i; a) denotes how recently ua purchased common item i, and v(i) denotes how important common item i is as a discriminator of likeminded users. We describe each of the factors below:

8
< r(i; b, a) =

exp(-

tb,i -Ti t¯i

)

-

exp(-

ta,i -Ti t¯i

),

if tb,i  ta,i

(2)

: 0, otherwise

where Ti denotes the release time of i and t¯i denotes the average time i was purchased after its release, ta,i and tb,i denote the times at which ua and ub purchased i, respectively.

w(i,

a)

=

exp(-

ea,i e¯a

),

(3)

where ea,i denotes the time passed since ua purchased i, and e¯a denotes the average of ea,i over all items purchased by ua.

v(i)

=

1 log(1 +

, Ui )

(4)

where Ui denotes the number of users who purchased i (taken from the logs).
It is clear that determining the user-user relationships via common items will encounter the problem of data sparseness, since the ratio of users who share common items decreases rapidly in inverse proportion to the number of items.
To solve this problem, our approach is to model the innovator relationship as an ergodic Markov chain; this guarantees the convergence of the power of the Markov chainbased matrix as follows: First, we define innovator probability p(b|a) according to the Bayesian rule:

p(b|a)

=

PID(ub, ua) uPID(u, ua)

,

(5)

where p(b|a) represents the probability that, given ua, how likely ub is to be regarded as the best innovator among all users for ua. The innovator probability p(b|a) is a relative measure since it is normalized over all users and selects b from among all users, while PID is a absolute measure. Second, we define the revised innovative probability p(b, a) for

220

implementing the ergodic Markov chain.

8 p(b|a),

<

p(b, a) =

if up(u, a) = 0

(6)

:

1 U

,

otherwise

Moreover, we consider the revised innovator probability as the primitive matrix P¨ for making the Markov chain ergodic:

P¨ = P + (1 - ) eeT ,

(7)

U

where P¨ is the innovator probability matrix consisting of p(b|a); e is the column vector of all ones and  is a weight parameter. By this definition, P¨ is constructed as a primitive stochastic matrix that models the probabilities of personal innovator transition and that converges to a stationary distribution. Finally, we calculate the innovator relationship over multiple steps and paths over the users. Since short paths indicate a more direct relationship among users and more important innovators, we consider assigning lower weights to long paths:

P¯

=

((P¨)+

1 2!

(

P¨

)2

+

·

·

·

+

(N

1 -

1)! (P¨)N-1 +···)exp(-),

(8)

where

1 N!

assigns

lower

weight

to

longer

paths,



is

a

pa-

rameter to control the effects of transitivity, and exp(-) is

a normalization factor. The larger  is, the more strongly

are longer paths weighted. Clearly, PIP enables us to iden-

tify users who could not, according to PID, be regarded as

innovators. We call innovator relationship p¯(b|a) the Person-

alized Innovator Probability P IP (ub, ua), and use it instead

of P ID(ub, ua).

4.2 User flow probability
The aim of user flow probability (UFP) is to follow trends, and detect novel items by employing the idea discussed in the previous section. UFP weights each item appropriately given a specified item; it is an estimate of how many consumers will purchase this item after the specified item. We consider the function of consumers who have purchased item b at t after a in assessing transition probability pab, and use this probability to predict which item will be purchased.
Similar to PIP, we model the probability by using CTMC, which satisfies the Markov property; it takes its value from a set called the state space. Continuous-time Markov processes are most easily defined by specifying the transition rates qij , and these are typically given as the ij-th elements of the transition rate matrix Q, which contains all information about the transitions of the Markov chain. For the probabilities to be conserved, i.e., to add up to one, the off-diagonal elements of Q must be non-negative and the diagonal elements, which we call jump-rate, must satisfy

qi = X qij ,

(9)

j=i

which represents the chain from item i with rate qi. When

the stochastic process leaves item i, it will next settle on

item j with probability pij, which is independent of the time

spent

at

item

i,

and

that

satisfies

pii

=

0

and

P
j=i

pij

=

1.

Accordingly, we gain

pij

=

qij qi

if i = j.

(10)

We assume that the stay period at item i follows an exponential distribution with shift-rate qi. According to the property of exponential distributed random variables, ti with rate qi is given by

E(ti)

=

1. qi

(11)

Figure 3 shows the age of a DVD rating, i.e., the time passed since the release of an item as the horizontal axis, and the number of users who rated a DVD of that age as the vertical axis as determined from actual online music and video download services in Japan. Details of the data will be described in a later section. The fitting lines, exponential functions of time, are also shown, where the number of users who evaluated an item decreases exponentially as the item ages. This figure supports our approach of utilizing exponential distributions.
Given the jump-rate, we calculate the transition probability from item i to item j as

X pij = qiexp(-qitu,ij).
u

(12)

where tu,ij denotes the time that user u took to move from item i to item j. Determining qij from Eq. (10), we gain

qij = qipij if i = j.

(13)

To predict the trends at time t = tf , we calculate the probability of the user flows from item i to others in the period[0, tf ], u(j|i, tf ), which are the ij-th elements of matrix U with

Z tf

lim U (tf ) = lim

P (t)dt,

t

t 0

(14)

where P (t) denotes the transition matrix with the ij-th elements of matrix pij. Formally, when the state space is finite, the transition probability can be estimated by using

P (t) = P (t)Q P(0) = I,

(15)

where I is the identity matrix. The solution is

P

(t)

=

etQ

=


X

(tQ)n n!

.

n=0

(16)

We then get the transition matrix function that satisfies the
Kolmogorov forward and backward equations. If Q can be diagonalized by Q = MDM-1, then

P (t) =


X

M (tD)nM -1 n!

=

M etDM -1.

n=0

(17)

For large Q, a Taylor approximation can also be used,

P

(t)



lim (I
n

+

Q

t n

)n.

(18)

Consequently, UFP consists of u(ib|ia, ) and is given by:

UFP(ib, ia) = lim u(ib|ia, t).

(19)

t

Unlike the traditional item-item relationship based on pairwise similarity, UFP ranks how similar users will become finally; it prevents novel items from being overwhelmed by well-known items with high popularity.

221

1e+06 100000

disnumusers fitting line

10000

1000

100 0

500

1000

1500

2000

2500

Figure 3: Distribution of the number of users who rated a DVD at a specific age of the DVD (time passed since its release) in Netflix. The horizontal axis corresponds to DVD age (measured in days) while the vertical axis shows the number of users who rated DVDs of that age.

4.3 Recommendation based on both PIP and UFP
We identify the informative logs for target user ua by using PIP and then recommend items to ua according to the weight based on both PIP and UFP. By using PIP defined in Eq. (8), p(ib|ua), the probability that ua will purchase item ib, is defined as follows:
p(ib|ua)  X UFP(ib, ia)(ia|uj )PIP(uj , ua), (20)
j
where, (ia|uj ) represents the evidence that uj has purchased item ia, namely, (ia|uj ) = 1 when ia is in the purchase history logs of uj , otherwise (ia|uj ) = 0. Clearly, p(ib|ua) becomes high when there are many personal innovators with high PIP and high UFP who purchased i. When p(ib|ua) is computed for all items, i, we can simply recommend the top-N items, the N largest p(ib|ua), to ua. In order to avoid creating a list of "trivial" recommendations, we must remove items that have already been purchased by the target user, from the recommendation list before presenting it to the target user.
A recommendation algorithm based on UFP and PIP is shown in Figure 4. This figure shows that we can calculate PIP over each pair of users by comparing just their purchase history logs in a manner similar to conventional methods. The computational cost of this process is linear, O(I), against the number of items, I, and thus is no more expensive than the conventional methods. The very low calculation costs of UFP and PIP mean that they can be easily introduced into any personalized recommendation service.
5. EXPERIMENTS
5.1 Data sets
We conducted simulations on the following four data sets, two of which consist of purchase histories obtained from real on-line music and video download services in Japan, one from the rating logs in Netflix, and one from search query logs.

Input: user logs Output: item ranking p(i|ua)

Ni: Number of users who purchased item i I: Total number of unique items observed in user logs Nu: Number of items purchased by user u U : Total number of unique users observed in user logs T0: Present time (=the time to recommend items)

calculate t¯i and e¯u for all items and users

for i = 1 to I do

t¯i



1 Ni

PNi
j=1

(tj,i

- Ti)

Eq.

(2)

end for

for u = 1 to U do

e¯u end

 for

1 Nu

PNu
i=1

(T0

- tu,i)

=

1 Nu

PNu
i=1

eu,i

Eq.

(3)

calculate P IP (ub, ua) on each pair of ua and ub

for a = 1 to U do

for b = 1 to U do

P IP (ub, ua)



P
iCab

w(i;

a)

×

r(i; b, a)

×

v(i)

Eq. (1), (4), (5), (6), (7) and (8)

end for

end for

calculate U F P (ib, ia) on each pair of ua and ub

for a = 1 to I do

for b = 1 to I do

UFP(ib, ia)



u(ib|ia, )

Eq. (11), (12), (14)and (19)

end for

end for

rank items for each target user ua

for a = 1 to U do

p(i|ua)



P
b

UFP(i,

j ) (j |ub )PIP(ub ,

ua)

Eq.

(20)

rank items in descending order of p(i|ua)

end for

Figure 4: Recommendation Algorithm

5.1.1 Content download services
The first purchase log data set is a group of music download purchase records from April 1st, 2005 to July 31st, 2006. It lists 44,527 items purchased by 84,620 users. Each purchase record consists of title of music purchased, artist name, CD album title, purchase time stamp, and price. The second set is a set of movie video download purchase records from September 1st, 2005 to February 28th, 2006. It lists 4,064 items purchased by 7,537 users. Each purchase record consists of the title of the video purchased, director's name, purchase time stamp, and price. Most items in these data sets are "newly released" items that became available for purchase from the service as soon as they were first released as a CD or Video (DVD).
5.1.2 Netflix
Netflix contains a set of 100,480,507 rating records from Nov 11th, 1999 to Dec 31st, 2005, that list 17,770 movies rated by 480,189 users. We first selected only those users who rated at least 20 movies and movies that were rated by

222

at least 100 users. This pre-processing downsized the data set to 85,730,203 rating records from Nov 11th, 1999 to Dec 31st, 2005; it consists of 9,264 movies rated by 136,589 users. Each rating record consists of movie title id, user id, rating, and timestamp.
Unlike the first two purchase log data sets, Netflix consists of user rating logs of movies with multi-valued ratings. We have to convert the ratings to binary values, namely the value is 1 (purchase) if the user rates an item, and 0 (no purchase) otherwise and made two data sets, Netflix(o), Netflix(p), in the same manner as in [8].
5.1.3 Query logs
The data set Query was generated from a search engine server log from April 1st, 2006 to May 31st, 2006. This data set consists of 35,325,842 query records, each of which consists of query keyword id, user id, and timestamp.
5.2 Experiment design
Our approach aims to predict which piece of music, video or movie (query) a user will purchase (submit) given the past purchase history of the user. We conducted simulations to evaluate the predictive performance of recommendations via K fold cross-validation where the original data was partitioned into K subsamples at random. Of the K subsamples, a single subsample was retained as the validation data for testing the model, and the remaining K - 1 subsamples were used as training data. We repeated this process K times, with each of the K subsamples being used just once as the validation data. Each subsample was divided into two periods: a learning period and a test period. We call the data in the test period the test data and that in the learning period the learning data; K was set to 10.
In the simulations, we treated each user in the test data as a target user to whom we applied each of the recommendation methods by using user logs collected from the learning data. We then presented the top N ranked items to the target user and confirmed that these recommended items existed in the test data. This is the traditional ranking based recommendation scenario. To evaluate the quality of the proposed method, we used top N precision, a measure commonly used for evaluating the predictive performance of CF [9].
5.3 Comparison of Personalization Performance
We applied a total of nine proposed and conventional recommendation methods to the data sets and compared the precision of their top-N recommendations (N values were 1, 5 and 10). Popular recommends the most popular items during the last one month of the learning period and thus it is not personalized to the user. Pearson and Cosine are based on user similarity as measured by Pearson's correlation coefficient and cosine similarity, respectively. On the other hand, Item is based on content similarity as measured by Pearson's correlation coefficient proposed in [1]. bPLSA is based on Probabilistic Latent Semantic Analysis [7] with Bernoulli distribution. MEA is the maximum entropy approach proposed in [11]. EABIF represents the early-adoption-based information-flow approach proposed in [14]. PIP+UFP is the proposed method. In this paper, we set  = 1 in Eq. (7),  = 1 and N = 5 in Eq. (8). The results are shown in Table 1.
In addition to the top-N precision above, we also evalu-

ated performance by two coverage measures, the Gini coefficient, the elapsed time, and the difference time. The values of the two coverage terms, Gini coefficient, avg elapsed and avg difference results are shown in Table 2.
5.3.1 Coverage: IC & UC
As the coverage measures, we calculated the percentage of the number of unique items appearing in the top-N recommendation list from the total number of unique items; we denote this coverage measure as "Item coverage"(IC). The item coverage of a recommender system is a measure of the size of the item domain in the system from which the system can recommend [6]. Systems with lower coverage may be less valuable to users, since they can offer only limited choices in assisting users to make decisions. We also calculated the percentage of the number of users to whom each method could recommend any item from the total number of all users who purchased any item in the test period; we denote this coverage measure as "User coverage"(UC).
5.3.2 Gini
The Gini coefficient is a measure of the statistical dispersion of the distribution of users over items and is defined as the ratio of the areas on the Lorenz curve diagram; it takes values between 0 and 1. A low Gini coefficient indicates that the distribution is flat, while a high Gini coefficient indicates that the distribution is extremely biased: 0 corresponds to perfect equality (every item has been purchased by exactly the same number of users) and 1 corresponds to perfect inequality (where one item is purchased by all users, while none of the other items are purchased by any user). In other words, a result with a high Gini coefficient means that a few particular items tend to be ranked highly by most users and thus recommendations are not strongly personalized.
5.3.3 AE
The elapsed time measures how much time has passed from each item's release day to its day of purchase by each user. A short elapsed time indicates that the item is novel. We measured the average over all user-item pairs in the test data that each method could predict and denote this as avg elapsed (AE).
5.3.4 AD
The difference time is the difference between the observed time when each user u purchased item i in the test data without recommendations tui and the time when divided into two periods (learning/test) t0 (Music:July 1st, 2006, Video:February 15th, 2006, Netflix(h),Netflix(p):December 1th, 2005, Query:May 31th, 2006) and defined over each user and each item, as discussed in Section 2. Therefore, an item with a large difference, |tui - t0|, indicates a time-consuming item for u; that is, it takes a long time for u to find this item on his/her own in the absence of any recommendation. We measured the average over all user-item pairs that each method could predict and denote this as the avg difference (AD).
5.3.5 Comparison of top-N precision
From Table 1, we can see that PIP+UFP offers the highest accuracy like PID. These results can be explained by the characteristics of the data sets; they consist of "luxury items". When an individual user desires and purchases

223

Table 1: Top N precision of personalized recommendations yielded by applying different methods to Music,

Video, Netflix, and Query data sets. Results that indicate a significant inter-method difference, t-test p < 0.01,

p < 0.05, are marked with '**', '*', respectively.

Data set top N Popular Cosine Pearson Item bPLSA MEA EABIF PID PIP PIP+UFP

Video

1

8.31

8.31

7.35 7.20 12.44 13.21 19.88 25.11 23.16 22.85

5

27.73 27.75 24.51 24.02 28.36 28.93 36.27 42.22 39.52 38.22

10

34.23 34.26 30.26 29.65 32.62 33.14 41.02 46.02 44.23 46.12

Music

1

6.90

6.91

6.10 5.98 10.78 10.86 16.32 22.41 20.37 21.57

5

23.01 23.03 20.34 19.93 21.23 22.52 29.83 35.74 32.26 34.38

10

32.25 31.47 29.84 27.85 31.02 31.26 36.97 39.22 37.56 39.87

Net(h)

1

5.12

5.32

4.76 5.23 5.63 5.77 7.59 8.74 8.03

8.52

5

6.33

7.21

6.35 8.12 8.59 8.61 9.78 12.03 10.59 11.58

10

7.56

8.46

7.22 10.21 10.98 10.97 13.44 16.51 15.33 16.82

Net(P)

1

5.83

6.01

5.26 6.68 6.71 7.22 8.31 9.77 9.03

9.52

5

7.27

7.54

6.98 10.37 10.94 10.96 12.77 15.12 14.21 14.78

10

8.32

9.57

8.51 12.86 13.12 13.05 15.28 17.93 16.67 18.04

Query

1

4.66

4.82

5.16 5.68 6.31 6.35 7.25 7.65 7.03

7.13

5

6.89

6.92

6.73 7.52 10.88 10.90 13.66 14.27 13.87 14.31

10

7.14

8.32

8.38 9.52 11.65 11.34 14.23 15.88 14.98 15.23

these items, they are generally motivated by their preferences rather than their needs. PIP+UFP differentiates likeminded users by using time decay factors to assign higher weights to more recently purchased items, and introduces the trend of items into item ranking. Consequently, the proposed method ranks items that match the latest preferences, which improves the precision.
5.3.6 Comparison of Coverage, Gini, AE and AD
From Table 2, we can see that PIP + UFP exhibits the highest user/item coverage, the lowest Gini coefficient, close to 0, and the longest average difference. Although the top-N results don't show the significant improvement of the proposed algorithm, this result indicates that PIP + UFP can rank and recommend various different items with much less bias than the other methods. In fact, those conventional algorithms, which do not consider the dynamics of user preference or the user-user relationship, achieve lower item coverage and higher Gini coefficients, close to 1. They use logs of like-minded users who are later adopters as well as earlier adopters and thus recommend generally popular and trivial items. With regard to user coverage, the algorithms that employ the dynamics of the user-user relationship offer slightly lower values than the others. Although users who are innovators are not covered by EABIF, PID anyway, PIP + UFP algorithms can recommend some novel items to these innovators, most of these items have not yet been purchased by the others, which raises precision.
6. DISCUSSION
Here, we discuss the potential of the proposed algorithm from the viewpoints of serendipity and novelty in recommendation flow.
Although we are not able to distinguish the effect of innovators from that of trends from our experiments completely, the fact that the PIP improves AD, supports our assumption that innovators know more serendipitous items than the others. As shown in Table 2, EABIF and PID, which include the time factors, offers longer AD than others (i.e. those without time factors). The main difference between PID

and PIP is that the former considers only pairwise comparison, while PIP uses an ergodic Markov chain to model how innovators are followed through multiple steps. Accordingly, PIP enables us to discover more items that would take more time to discover because of these multiple steps than is possible with the other methods; it can recommend surprisingly interesting items that might not otherwise be discovered.
As shown in Table 2, the results that the proposed method yields the shortest, on average, elapsed time indicate that it is best at identifying novel items. Because these items are so new th they are not yet generally known to many other like-minded users, users would take some time for them to find the items by themselves. Accordingly they are novel items and their popularity is low. Since UFP suppresses well-known items in the recommendations, it cannot offer high top-N precision in the simulation comparison. However, PIP improves the IC and ED; it can identify more novel items since its Gini coefficient is close to 0.
7. CONCLUSION
The contribution of this paper to the recommendation research field is to show the impact of trends on the transition probability of items, and that the estimated time offset to purchase in the absence of recommendations is a useful metric of serendipitousness. Our approach offers improvements on the user/item coverage, the Gini coefficient, the elapsed time, the difference in estimated time, and the predictive performance simultaneously.
Following the hypothesis that items that innovators have recently purchased offer serendipity and will be adopted by their like-minded users more willingly than items purchased by others, we use PIP for identifying innovators and define UFP that estimates how likely the user is to finally purchase each item.
In future work, we would like to extend the proposed model to incorporate user specialty and level of expertise in a particular domain, and apply this extended model to personalized information retrieval more generally.

224

Table 2: Comparison of various methods as applied to Music, Video, Netflix, and Query data sets: Results that indicate significant inter-method difference, t-test p < 0.01, p < 0.05, are marked with '**', '*', respectively.

Data set Music Video
Netflix(h) Netflix(p)
Query

Evaluation
UC IC Gini AE AD
UC IC Gini AE AD
UC IC Gini AE AD UC IC Gini AE AD UC IC Gini AE AD

Popular
100 6.50 0.51 15.73 8.52
100 8.43 0.46 22.25 5.45
100 5.36 0.72 27.86 15.67 100 5.88 0.67 26.54 15.33 100 3.22 0.92 13.92 4.21

Cosine
98.9 6.90 0.47 39.74 3.38
99.2 8.87 0.42 42.27 5.52
92.3 6.12 0.76 40.53 13.58 90.9 5.93 0.62 38.24 12.25 82.3 3.35 0.91 14.22 2.38

Pearson
98.9 6.91 0.51 38.53 3.21
99.2 8.72 0.47 40.74 5.87
92.3 6.17 0.74 41.74 13.47 90.9 5.85 0.63 39.52 12.36 82.3 3.37 0.91 14.43 2.41

Item
98.9 6.10 0.49 38.57 3.12
99.2 12.31 0.43 41.43 5.79
92.3 6.88 0.71 40.75 13.31 90.9 7.21 0.58 39.38 12.96 82.3 3.41 0.91 14.57 2.27

bPLSA
98.9 5.98 0.51 38.46 3.55
99.2 11.56 0.46 40.58 5.23
92.3 9.54 0.69 41.24 13.85 90.9 6.35 0.61 39.89 12.51 82.3 3.38 0.90 14.23 2.33

MEA
98.9 11.21 0.47 36.62 3.46
99.2 12.32 0.29 41.36 5.88
92.3 10.19 0.62 40.32 13.21 90.9 9.74 0.56 39.01 12.78 82.3 3.35 0.91 14.93 2.16

EABIF
97.7 19.55 0.45 26.21 8.69
98.6 18.55 0.22 33.23 6.18
91.5 10.32 0.58 41.71 16.68 90.3 11.1 0.52 38.86 16.24 77.3 4.21 0.83 14.88 4.98

PID
97.7 20.81 0.37 22.23 8.97
98.6 33.12 0.22 28.68 7.32
91.5 12.75 0.48 31.23 16.87 90.3 15.88 0.48 29.12 16.43 77.3 5.88 0.81 13.22 6.22

PIP
100 21.53 0.35 17.52 9.06
100 30.22 0.20 24.76 7.95
100 13.37 0.44 28.42 18.51 100 15.92 0.42 27.23 18.19 100 5.92 0.77 12.82 6.56

PIP+UFP
100 22.34 0.31 12.39 9.26
100 34.32
0.19 17.62 8.38
100 14.53 0.41 19.72 19.53
100 16.21 0.38 18.96 18.72
100
5.95
0.77 10.69 6.69

8. REFERENCES
[1] J. K. B. Sarwa, G. Karypis and J. Riedl. Item-based collaborative filtering recommendation algorithms. In WWW, pages 285­295, 2001.
[2] O` . Celma and P. Lamere. A new approach to evaluating novel recommendations. In ACM Recsys, pages 179­186, 2008.
[3] D. Cosley, S. Lawrence, and D. M. Pennock. Referee: An open framework for practical testing of recommender systems using researchindex. In VLDB, pages 35­46, 2002.
[4] D. Fleder and K. Hosanagar. Blockbuster culture's next rise or fall: The impact of recommender systems on sales diversity. In SSRN eLibrary, pages 697­712, 2007.
[5] F. Fouss, J.-M. Renders, A. Pirotte, and M. Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transaction on Knowledge and Data Engineering, 19(3):355?369, 2007.
[6] J. L. Herlocker, J. Konstan, L. Terveen, and J. Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems, 22(1):5­53, 2004.
[7] T. Hofmann. Collaborative filtering via gaussian probabilistic latent semantic analysis. In ACM SIGIR, pages 259­266, 2003.
[8] N. Kawamae, H. Sakano, and T. Yamada. Personalized recommendation based on the personal innovator degree. In ACM Recsys, pages 329­332, 2009.

[9] J. Konstan, B. Miller, J. H. D. Maltz, L. Gordon, and J. Riedl. Grouplens: Applying collaborative filtering to usenet news. Communications of the ACM, 40(3):77­87, 1997.
[10] G. Linden, B. Smith, and J. York. Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Internet Computing, 7(1):76­80, 2003.
[11] D. Pavlov and D. Pennock. A maximum entropy approach to collaborative filtering in dynamic, sparse, high-dimensional domains. In NIPS, pages 1441­1448, 2002.
[12] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl. An open architecture for collaborative filtering of netnews. In CSCW, pages 175­186. ACM Press, 1994.
[13] E. M. Rogers. Diffusion of Innovations. The Free Press, New York, 1995.
[14] X. Song, C. Lin, B. Tseng, and M. Sun. Personalized recommendation driven by information flow. In ACM SIGIR, pages 509­516, 2006.
[15] Y. Yang and J. Z. Li. Interest-based recommendation in digital library. Journal of Computer Science, 1(1):40­46, 2005.
[16] Y. Zhang, J. Callan, and T. Minka. Novelty and redundancy detection in adaptive filtering. In ACM SIGIR, pages 81­88, 2002.
[17] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and G. Lausen. Improving recommendation lists through topic diversification. In WWW, pages 22­32, 2005.

225

Keynote Talk
Refactoring the Search Problem
Gary William Flake
Microsoft Corporation Redmond, WA, USA flake@microsoft.com
Abstract
The most common way of framing the search problem is as an exchange between a user and a database, where the user issues queries and the database replies with results that satisfy constraints imposed by the query but that also optimize some notion of relevance. There are several variations to this basic model that augment the dialogue between humans and machines through query refinement, relevance feedback, and other mechanism. However, rarely is this problem ever posed in a way in which the properties of the client and server are fundamentally different and in a way in which exploiting the differences can be used to yield substantially different experiences.
I propose a reframing of the basic search problem which presupposes that servers are scalable on most dimensions but suffer from low communication latencies while clients have lower scalability but support vastly richer user interactions because of lower communication latencies. Framed in this manner, there is clear utility in refactoring the search problem so that user interactions are processed fluidly by a client while the server is relegated to pre-computing the properties of a result set that cannot be efficiently left to the client.
I will demonstrate Pivot, an experimental client application that allows the user to visually interact with thousands of search results at once, while using facetted-based exploration in a zoomable interface. I will argue that the evolving structure of the Web will tend to push all IR-based applications in a similar direction, which has the algorithmic intelligence increasingly split between clients and servers. Put another way, my claim is that future clients will be neither thin nor dumb.
Categories & Subject Descriptors: H.1.2 [Models and Principles]: User/Machine
Systems-human information processing; H.3.3 [Information Storage and Retrieval] Information Search and Retrieval-Information filtering; H.5.2 [Information Interfaces and Presentation]: User Interfaces-Graphical user interfaces (GUI)
General Terms: Algorithms, Performance, Design, Human Factors.
Bio
Dr. Gary William Flake is a Technical Fellow at Microsoft, where he focuses on Internet products and technologies including search, advertising, content, portals, community, and application development. In this capacity, he helps define and evolve Microsoft's product vision, technical architecture, and business strategy for online services. He is also the founder and director of Live Labs, a "skunk works" team that bridges research and development. Prior to joining Microsoft, Dr. Flake founded Yahoo! Research Labs, ran Yahoo!'s corporate R&D activities and company-wide innovation effort, and was the Chief Science Officer of Overture ­ the company that invented the paid search business model. Dr. Flake also wrote the award-winning book, The Computational Beauty of Nature, which is used in college courses worldwide.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
250

Personalizing Information Retrieval for Multi-Session Tasks: The Roles of Task Stage and Task Type
Jingjing Liu, Nicholas J. Belkin
School of Communication and Information, Rutgers University 4 Huntington Street, New Brunswick, NJ 08901, USA
jingjing@eden.rutgers.edu, belkin@rutgers.edu

ABSTRACT
Dwell time as a user behavior has been found in previous studies to be an unreliable predictor of document usefulness, with contextual factors such as the user's task needing to be considered in its interpretation. Task stage has been shown to influence search behaviors including usefulness judgments, as has task type. This paper reports on an investigation of how task stage and task type may help predict usefulness from the time that users spend on retrieved documents, over the course of several information seeking episodes. A 3-stage controlled experiment was conducted with 24 participants, each coming 3 times to work on 3 sub-tasks of a general task, couched either as "parallel" or "dependent" task type. The full task was to write a report on the general topic, with interim documents produced for each sub-task. Results show that task stage can help in inferring document usefulness from decision time, especially in the parallel task. The findings can be used to increase accuracy in predicting document usefulness and accordingly in personalizing search for multi-session tasks.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ relevance feedback, search process
General Terms
Performance, Design, Human Factors.
Keywords
Personalization, contextual factors in IR, dwell time, task stage, task type.
1. INTRODUCTION
Aimed at helping people find documents that meet their particular information needs, personalization of information retrieval (IR) takes account of information about users and their contexts beyond the queries that they submit. This additional information is often obtained implicitly from user behaviors and/or contextual data, such as dwell time, topic knowledge, and task information [1]. The time that a user spends on a retrieved document has attracted some research attention in the areas of implicit feedback and personalization of IR. It has been suggested that time alone is not a reliable factor for predicting document relevance in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

interactive IR; instead, it seems to differ significantly according to different tasks [6]. White & Kelly [17] found that information about tasks can be helpful in personalization, specifically, in setting a threshold for predicting web pages' relevance from dwell time, in supporting implicit relevance feedback.
In everyday life, information search usually happens in a certain scenario when people have a problem to solve, a goal to attain, or a "work task" at hand [2][4]. It is often seen that a work task is not accomplished in one session but requires multiple sessions for various reasons, such as the complexity of the task, the difficulty in locating desired information, time constraints, and so on. To study how search personalization can be applied to multi-session work tasks could result in helping people to address such tasks, especially when these tasks are prolonged due to difficulties in locating useful information.
In multi-session tasks, one task feature, task stage, seems to be more salient than in single-session (especially quick and simple) tasks. Task stage has been found to affect users' search behaviors and search performance (e.g., [7], [9]). Multi-session work tasks are often complex and consist of multiple sub-tasks. Sub-task interrelationship is another salient feature in multi-session work tasks. For example, the subtasks can be independent of one another (termed "parallel" in this paper), or they may need to be performed in some hierarchical or serial order (termed "dependent" in this paper) [13]. It is a pending issue whether contextual factors of task stage and task type in multi-session tasks can be helpful in implicitly predicting document usefulness, which is a key to personalization. To this end, a longitudinal study was conducted which was aimed at answering the following research questions (RQs):
RQ1. Can the usefulness of retrieved documents be reliably predicted solely from the time that users have spent on them?
RQ2. Does the stage of users' tasks help predict the usefulness of retrieved documents from the time that users have spent on them?
RQ3. If the stage information helps in predicting document usefulness from time, does its role vary in different types of tasks?
2. RELATED LITERATURE 2.1 Task, Behavior, and Usefulness
Task has been found in previous research to be helpful in predicting document usefulness by the user's behavioral factors, such as time spent on retrieved documents. Kelly & Belkin [6] found that using display time averaged over a group of users to predict document usefulness is not likely to be accurate, nor is it accurate using display time for a single user without taking into account contextual factors. Specifically, display time differs

26

significantly according to specific tasks and specific users. This demonstrated that inferring the usefulness of a document from dwell time should be tailored towards individual tasks and/or users. Their study, however, did not examine how to incorporate the contextual factors and what the actual effectiveness would be.
The issue of the influence of contextual factors was addressed by White & Kelly [17]. They explored the interactions between dwell time and two contextual factors: user and task. They examined if additional information about the user and/or the task helps reliably to establish a dwell time threshold to predict document usefulness, and how effective this method would be. The results showed that tailoring display time threshold based on task information improved implicit relevance feedback algorithm performance. In other words, display time was shown to be able to successfully predict document usefulness when the task information is considered. This research shed light on studying the roles that other contextual factors may also play in helping predicting document usefulness from user behaviors, including the task features in multi-session tasks.
2.2 Task Stage
Task stage has received significant research attention in terms of how it influences users' search behavior and performance. It has been suggested that information seekers go through various stages or phases with different levels of information need (e.g., [7], [9], [11]), and that users' feelings, thoughts, and actions vary along the different stages. This body of research indicates that stage of task may be an important factor that relates to the user's judgment of document usefulness. Vakkari and colleagues (e.g., [14], [15], [16]) found that users' search tactics change along stages, for example, their users were, at the beginning of their tasks, less likely to start their initial queries by introducing all the search terms, were more likely to enter only a fraction of the terms, and tended to use more synonyms and parallel terms than in later stages. They also found that users' relevance criteria depend on the stage of their task performance process, although their results did not show a statistical significance of the changes. Taylor et al. [12] found statistically significant relationships between users' relevance criteria and task stage choices corresponding to those of selection, exploration, formulation, and presentation in Kulthau's ISP model [7]. This branch of research demonstrates the differences in users' relevance judgments in different stages of the task, however, it leaves as an open issue how such differences could be modeled through the user's behaviors. In addition, the above described research considered only the effect of task stage but did not correlate it with user behaviors to suggest how to build user models and improve system support for search. More research is needed to study how task stage can be helpful in adapting search to different users.
It is not always easy to accurately identify task stages because they do not always have apparent boundaries. One way is to divide the logged user-system interaction data into three stages with equal time length: "start", "middle", and "end" [18]. Another way to operationalize task stage is provided by Lin [9], in which the user's task was manipulated with different sub-tasks to be completed in different search sessions. The task scenario in his study required participants to make a vacation plan through three steps in three sessions: identifying candidate places, choosing one place, and making a trip plan. Both ways to operationalize task

stage are arbitrary to some extent, but the latter is closer to the situation in people's everyday life when solving complex tasks.

2.3 Task Type
There have been many studies on task type classification (e.g., [3], [5], [8]), and Li & Belkin ([8]) have proposed an extensive scheme to classify tasks based on many dimensions of task features. Among the many task features, for work tasks which consist of multiple sub-tasks, the relationship between the subtasks seems salient and it is necessary to take it into account because the orders of sub-tasks may vary during the process of accomplishing the work tasks. To our knowledge, no previous research has classified multi-session tasks based on this dimension, but similar ideas can be found in some works, for example, Toms et al. [13] which classifies single-session tasks based on their conceptual structures. The two types of tasks in their approach are: the parallel task, where the search uses multiple concepts that exist on the same level in a conceptual hierarchy, and the hierarchical task, where the search uses a single concept for which multiple attributes or characteristics are sought.
In sum, there is a relatively rich body of previous work on tasks, but additional approaches are needed to better utilize the information that a user's task can provide to personalize search for multi-session work tasks.

3. METHOD

3.1 Study Design

The study was a 3-session controlled lab experiment, where the 3

sessions were treated as 3 stages. The design was 2*2 factorial

with two between-subjects factors (Table 1). One was task type,

with two levels: parallel or dependent. The other was search

system, with two levels: query suggestion (QS) or non-query

suggestion (NQS). These tasks and systems are described in more

detail below.

Table 1. Study design

System version

Condition

Task

Session (stage) 1

Session (stage) 2

Session (stage)
3

1

Dependent NQS

NQS

NQS

2

Parallel

NQS

NQS

NQS

3

Dependent NQS

QS

QS

4

Parallel

NQS

QS

QS

3.2 Tasks
Tasks were designed to mimic journalists' assignments since they could be relatively easily set as realistic tasks in different domains. Among the many dimensions of task types, e.g., in [8], this study focused on task stage and sub-task interrelationship, varying them while keeping other facets (e.g., task difficulty, task product format) as constant as possible. Two tasks types were used in the study: one parallel and one dependent. They both had three sub-tasks, each of which was worked on by the participant during one separate session, for three sessions in total.
The tasks asked the participants to write a three-section feature story on hybrid cars for a newspaper, and to finish and submit each article section at the end of each experiment session. At the end of the 3rd session, they were asked to integrate the 3 sections into one article. In the dependent task, the three sub-tasks were: 1)

27

collect information on what manufacturers have hybrid cars; 2) select three models that you will mainly focus on in this feature story; and 3) compare the pros and cons of three models of hybrid cars. In the parallel task, the three sub-tasks were finding information and writing a report on three models of cars from auto manufacturers renown for good warranties and fair maintenance costs: 1) Honda Civic hybrid; 2) Nissan Altima hybrid, and 3) Toyota Camry hybrid. It was hypothesized that the sub-tasks in the parallel task were independent of one another, but that in the dependent task there would be perceived to be at least some notional order. To maintain consistency, sub-task orders in task description in both tasks were rotated and users were allowed to choose whatever order of sub-task performance they preferred.
In each session, participants were allowed to work up to 40 minutes to write and submit their reports. They were allowed to search freely on the Web for resources in report writing. For logging purpose, users were allowed to keep only one Internet Explorer (IE) window open and use back and forward buttons to move between web pages.
3.3 Participants
Undergraduate Journalism/Media Studies students were invited to participate in the study via email to the student mailing list at the Journalism/Media Studies undergraduate program in the authors' school. The first 24 students who responded and eventually came were study participants. Out of them, 21 were female and 3 were male. Their mean age was 20.4 years. They self reported to have an average of 8.4 years of online searching experience, and rated their levels of expertise with searching as slightly above average (M=5.38) (1=novice, 7=expert). Each of them came 3 times within a 2-week period based on their schedule. Each was assigned randomly to a task/system condition. After finishing all 3 sessions, each participant obtained $30 payment. To encourage them to search in a serious manner, participants were told in the beginning of the experiment that the top 6 who submitted the most detailed reports would receive a bonus of $20.
3.4 Procedures
Participants came individually to a usability lab in the authors' school to take part in the experiment. Upon arrival in the first session, they completed a consent form and a background questionnaire eliciting their demographic information and search experience. They were then given the general work task to be finished in the whole experiment. A pre-session task questionnaire followed to collect their familiarity with the general task topic. Then they were asked to pick one sub-task to work with in the current session. A pre-session sub-task questionnaire followed to collect their familiarity with the sub-task topic. Then they worked with the subtask: searching for useful sources and writing reports. After report submission, participants went through an evaluation process in which they were asked to rate on a 7-point scale each document that they had viewed, in the order of viewing them in the actual search process, with respect to its usefulness to the overall task. A post-session sub-task questionnaire and a postsession general task questionnaire were then administered to elicit user perceptions on the difficulty of the task and sub-task, as well as their satisfaction with their reports. This ended the first session.
In the 2nd and the 3rd sessions, participants went through the same processes except for the consent form and background questionnaire, as well as an instruction step on using query

suggestion features for those assigned with the QS version system. In the 3rd session, after the post-session general task questionnaire, an exit interview asked them to reflect their overall knowledge gain and to comment on the whole experiment.
3.5 Work Station and Logging
The experiment was conducted using a two-monitor workstation: the main monitor was an eye-tracker in which the users searched and worked on writing their reports; the 2nd monitor was a regular monitor sitting beside the search monitor, which displayed the online questionnaires and the task and sub-task descriptions. Users' eye movements were captured but are not reported here. Logging software Morae (http://www.techsmith.com/morae.asp) was used to record all the user-system interactions (such as mouse and keyboard activities, window display) in the main monitor.
3.6 Search Systems
One aspect of the study as a whole was aimed at exploring whether query terms extracted from useful pages in previous sessions were helpful for the users in their current search, and to this end, two versions of the search system were designed. One version (QS) offered query term suggestions based on previous sessions; the other (NQS) did not. For those who were assigned to the NQS condition, the regular IE was used throughout all three sessions. For those who were assigned to the QS condition, the regular IE was used in their first sessions, but a different interface with query term suggestions was provided for the second and the third sessions. This different interface (Figure 1) consisted of two parts: the right side panel was the regular IE browser, and the user could search the open Web through whatever search engine they like; the left-side panel displayed a list of terms extracted from their previous session(s) for the users to select and add to their queries.
Figure 1. Search interface for the QS condition
4. RESULTS 4.1 Three Types of Time
In the literature, time that users spend on a document or webpage is generally called dwell time, or reading time, or display time, but it is often not clear how this time users spent on documents was defined, although all these different names of time seem to refer to the same thing. In the current study, an examination of the data as well as observation during the experiment showed that there were different types, instead of a single type, of time period that users

28

Open a page Webpage A

Other pages or windows Webpage A

Close a page Webpage A

Re-open a page Webpage A

Close a page Webpage A

Dwell time a1 Decision time b

Dwell time a2 Display time d1

Dwell time a3 Time line

Dwell time a4

Dwell time a5

Display time d2

Dwell time a Display time d Decision time b

Total dwell time = a1 + a2 + a3 + a4 + a5 Total display time = d1 + d2

Figure 2. Illustration of three types of times that a user spends on a retrieved document in a session
spent on documents. These types are first explained here before other results are reported.

Dwell time (DwT) (denoted as a in Figure 2) is defined as the time duration from each point when the user starts reading a document to when the user leaves the document (either closing the document or leaving it open and going to other applications). Each dwell time is the time that a user dwells on the document.
Display time (DisT) (denoted as d in Figure 2) refers to the total duration of a document between when it is opened to when it is closed. This is also the total time that the document remains open, no matter if it is active, i.e., if the user views it or not, after it is opened. It is possible that a document was opened for multiple times in an experiment session, making it possible to have multiple display times at different points.
Decision time (DecT) (denoted as b in Figure 2) in this study is the first dwell time. When a document was opened for the first time, the user looked at it and then left it to do something else, e.g., either invoking some other application, or leaving the document to do some other searching. This we interpret as indicating that the user had made a decision on the usefulness (being useful or not, or to use the document or not) of the document. For example, going to an MS WORD document and starting writing most likely meant that the web page that the user just viewed was useful; leaving a web page and going back to a search result page (to refine queries or open another search result) perhaps meant that the page just viewed was not useful. In defining decision time, we are not concerned with the nature of the decision, but just with the fact that a decision had been made.
At times, some documents were viewed and/or opened for multiple times in a session. In such cases, total dwell time (tDwT) was used in data analysis, which was the sum of the dwell time that a user spent on a document each time it was viewed. Similarly, total display time (tDisT) was also used in data analysis, which was the sum of the total times that a document was open. For the usefulness scores of the documents viewed multiple times in a session, most of the time, the users gave the same scores to the same document opened at multiple times. At rare times they gave different scores, in which case the final rating score was used in data analysis.

Figure 3-a. Distribution of tDisT in both tasks
Figure 3-b. Distribution of log(10) of tDisT in both tasks In order to explore the relationships between/among various factors, including both the main effects and any possible interaction effects, General Linear Model (GLM) tests were conducted, where time was used as the dependent variable, and stage and usefulness were used as factors. Examinations of distributions of all three types of time revealed that they were far from normally distributed, which was consistent with what has been found in the literature (e.g., [6]). As an example, Figure 3-a shows the distribution of tDisT in both tasks combined. Similarly to what has been done in the literature, these times were transformed by logarithm using a base of 10 in data analysis for

29

the relationships between/among factors. The transformed time data were normally distributed, or close to normally distributed (see the example in Figure 3-b for the distribution of transformed tDisT), which improves the interpretability of GLM analyses.
Data were analyzed for both tasks combined as well as for each task individually. The original usefulness scores were on a 7-point scale for its appropriateness for collecting user assessment [10], but this is too fine-grained for a system to differentiate, so the scores were collapsed into groups in the analysis similarly to what has been done in the literature (e.g., [17]). Collapsing was based on the distribution of usefulness scores, where 1-2 was put into a low useful group, 3-5 into a somewhat useful group, and 6-7 into a very useful group. The three combined groups were rather balanced in frequency.
4.2 Results with Both Tasks Considered
4.2.1 Total Display Time
We first looked at the relations among factors of usefulness, time, and stage when no task type information was taken into account, i.e., when both tasks in general were considered. Results indicated that (Table 2), when tDisT was used, both stage and usefulness had a significant effect. There was also a significant interaction effect between stage and usefulness (p<.05).
As can be seen from Figure 4-a, across all three stages, the longer a page was displayed in a session, the more useful this page was. For all documents, in general, the average page display time was longer in later stage(s) than in earlier stage(s). The interaction effect between stage and usefulness was shown in the following way: for not useful documents, they were displayed longer in stages 1 and 3 than in stage 2; for somewhat useful documents, the later the stages, the longer they were displayed; however, for very useful documents, their display time was longer in stages 2 and 3 than that in stage 1.
4.2.2 Total Dwell Time
As can be seen in Table 2, when tDwt was used, usefulness showed as the single main factor (p<.001), but stage did not. Descriptively, as can be seen from Figure 4-b, across all three stages, the longer a page was displayed in a session, the more useful this page was. For all documents, in general, the average page display time did not differ across stages, and in fact, total dwell time in stages 2 and stage 3 were almost identical.
4.2.3 Decision Time
When DecT was used, neither stage nor usefulness showed a significant main effect, but the interaction between stage and usefulness showed a significant effect (p<.01). Figure 4-c shows that when stage information is not considered, users spent roughly the same average decision time on documents with various usefulness levels. For all documents, in general, the average decision time did not vary in different stages. However, there was an interaction effect between stage and usefulness, demonstrated in the following way: for not useful documents, users spent longer decision time in stages 1 and 3 but shorter decision time in stage 2; for somewhat useful documents, users spent short decision time in stage 1, but much longer decision time in stage 2, and in stage 3, their decision time for somewhat useful documents was

between that in stages 1 and 2; for very useful documents, users spent a long time deciding document usefulness in stage 1, but in stage 2, they spent shorter decision time, and in stage 3, their decision time was even shorter. It can be clearly seen that there is no general rule that the longer the decision time, the more useful the document. So decision time cannot reliably predict usefulness without consideration of information about task stage.
4.3 Results in the Dependent Task
Relations among stage, usefulness, and time were also examined in individual tasks. In the dependent task, it was found that usefulness was the single significant factor contributing to tDisT (p<.001), tDwT (p<.001), as well as DecT (p<.05). Stage did not play a significant role contributing to any type of time.
4.3.1 Total Display Time
Across all three stages, the longer a page was displayed in a session, the more useful this page was (Figure 5-a). For all documents, in general, the average page display time did not have differences in the three stages.
4.3.2 Total Dwell Time
As can be seen from Figure 5-b, across all three stages, the longer the users dwelled on a page, the more useful this page was. For all documents, in general, the average total dwell time did not have differences in three stages. In fact, total dwell time in stage 2 and that in stage 3 were almost identical.
4.3.3 Decision Time
When DecT was used, the findings were essentially the same as those using tDisT or tDwT. Although Figure 5-c looks a bit different from Figures 5-a and 5-b, there was no statistically significant difference. Across all three stages, the longer the decision time, the more useful the page was. For all documents, the average decision time did not show differences in the three stages.
4.4 Results in the Parallel Task
In the parallel task, the patterns of relationships among time, stage, and usefulness were similar to those obtained when both tasks were examined together. In contrast, they were very different to those obtained in the dependent task alone.
4.4.1 Total Display Time
When tDisT was used, usefulness had a significant effect (p<.001) but stage did not. As can be seen from Figure 6-a, across all three stages, the longer a page was displayed in a session, the more useful this page was. For all documents, when their usefulness levels are not differentiated, the average page display time did not have differences in three stages.
4.4.2 Total Dwell Time
When tDwT was used, usefulness had a significant effect (p<.001) but stage did not. As can be seen from Figure 6-b, across all three stages, the longer the user dwelled on a page, the more useful this page was. For all documents, when their usefulness levels are not differentiated, the average total dwell time did not have differences in the three stages.

30

Table 2. F and p values of factors

Time Type

Both Tasks Combined

Stage

Usefulness

Stage* Usefulness

tDisT 4.15(.016) 123.78(.000)
tDwT 1.68(.187) 75.40(.000) DecT .33(.722) 2.16(.116) (Those in bold were significant.)

2.66(.032) .82(.514) 3.62(.006)

Dependent Task

Stage

Usefulness

Stage* Usefulness

1.96(.142)

63.40 (.000)

1.91(.108)

1.29(.276) 35.23(.000) .83(.507) .79(.454) 3.34(.036) 1.57(.180)

Stage

Parallel Task

Usefulness

Stage* Usefulness

2.40(.092)
.48(.621) .45(.639)

61.11(.000)
40.78(.000) .14(.869)

1.39(.236)
.43(.791) 2.48(.043)

Legend for Figures 4, 5, & 6: Stage: 1.

2.

3.

4-a. Total Display Time (tDisT)

4-b. Total Dwell Time (tDwT)

4-c. Decision Time (DecT)

Figure 4. Relationship of time, usefulness, and stages in both tasks combined

5-a. Total Display Time (tDisT)

5-b. Total Dwell Time (tDwT)

5-c. Decision Time (DecT)

Figure 5. Relationship of time, usefulness, and stages in the dependent task

6-a. Total Display Time (tDisT)

6-b. Total Dwell Time (tDwT)

6-c. Decision Time (DecT)

Figure 6. Relationship of time, usefulness, and stages in the parallel task

31

4.4.3 Decision Time
When DecT was used, neither usefulness nor stage had a significant effect, but their interaction did (p<.05) (Figure 6-c). In stage 1, users spent the shortest decision time for medium useful document and the longest decision time for very useful documents. In stage 2, they spent shorter decision time on low usefulness and very useful documents but longer decision time on somewhat useful documents. In stage 3, they spent the shortest time for very useful documents and the longest time for medium useful documents. These results indicated that in the parallel task, stage played a significant role contributing to DecT.
4.5 About System Versions
There is a need to look at the possible effect of system version on the above results since participants used 2 versions of the system. It was found that compared with the participants who were assigned to the NQS condition, those assigned to the QS condition had less initial knowledge on task topics even before they worked with their sub-tasks in session 1. Therefore, the 2 groups of users were not sampled from the same population, which accordingly makes it inappropriate to conduct any comparison between these 2 groups of users with respect to system performance. This also suggests that we need to treat all participants as a whole instead of examining each group independently for analyses on the effect of task stage and task type. In addition, it was found that system condition did not seem to confound the relationships discovered because it did not have a significant main effect on users' decision time, nor did it have a significant interaction effect with usefulness on decision time.
5. DISCUSSION 5.1 Usefulness of Three Types of Time
Among the three types of time, tDisT and tDwT both measured time at the whole session level. These times cannot be captured until a session is finished, so their applicability for personalizing search for the current session is limited. They may be nonetheless useful as a basis for personalizing subsequent search sessions. On the other hand, DecT can be captured in a much earlier phase in a session; therefore, it can be used for adapting search in the current session in addition to subsequent sessions, making it more useful for immediate personalization than the other two types of time.
5.2 Time as a Predictor of Usefulness
Results showed that in both tasks combined, i.e., when task type was not distinguished, usefulness had a significant main effect on tDwT. This is reasonable given the fact that the users often moved back and forth between reading documents and writing reports, and those documents which had longer dwell time were more likely to be useful. This finding indicates that when task type was not distinguished, tDwT was a rather reliable predictor of usefulness. However, when tDisT or DecT was used, neither of them seemed able to reliably predict usefulness.
In the dependent task, all three types of time, tDisT, tDwT, and DecT, appeared to be able to reliably predict usefulness. In contrast, in the parallel task, tDisT and tDwT were shown to be able to reliably predict usefulness, but DecT was not.
In sum, tDwT was shown to be able to reliably predict usefulness in both tasks combined, and for both the parallel and the dependent task. tDisT can predict usefulness in the parallel and

the dependent tasks individually, but not in both tasks combined. In contrast, DecT can only predict usefulness in the dependent task. Considering the practical limitation that tDwT and tDisT can be captured only at the end of the task session, if one is to use time alone as a reliable predictor of usefulness to personalize for the current session, only DecT is available and so one can only make predictions for the dependent task.

Table 3. Summary of usefulness predictors

Role as

Applicable Task

Time Predictor

Type

Applicable

Type

of usefulness

Both

Depd.

Para.

Sessions

tDisT

Single With stage

 





Following Following

tDw T

Single







Following

DecT

Single With stage





Current/following  Current/following

(Note: Depd.: dependent task; Para.: parallel task)

5.3 Stage as a Helpful Contextual Factor in Predicting Usefulness
Although in the dependent task, stage did not seem to play a role in contributing to use of any of the three types of time, in the parallel task and in both tasks together, stage was found to have significant main or interaction effects. Specifically, in both tasks combined, stage had a significant main effect as well as a significant interaction effect with usefulness on tDisT. Stage also had a significant interaction effect with usefulness on DecT. In the parallel task, stage had a significant interaction effect with usefulness on DecT.
Task stage plays a role in predicting usefulness from tDisT in both tasks combined. This role can be used for subsequent search/work sessions but it cannot be applied to the ongoing session due to the limitation that tDisT cannot be captured until the end of a session. However, the role that task stage plays in predicting usefulness from DecT can be applied to the ongoing session. The significant interaction effects between stage and usefulness on DecT in both tasks combined and in the parallel task are important because in these tasks, usefulness in fact did not have any main effect. This indicates that in both tasks combined and in the parallel task, DecT alone cannot predict usefulness, but by taking into account task stage information, it can. However, it should also be noted that although the above mentioned interaction effects between stage and usefulness were significant, the effect size was not large (partial eta squared was .015 in both tasks combined and .022 in the parallel task).
In sum, when no task type was distinguished, task stage was found to help in predicting usefulness from tDisT, which can be used for personalization in subsequent work/search sessions in a multisession task. When no task type was distinguished, or in the parallel task, task stage was found to be an important factor to take account of when predicting usefulness from DecT, which can be used for personalizing search for the ongoing session.
These findings are helpful for personalizing search for specific users in that decision time can help predict the usefulness of documents given the task stage and task type information. In addition, unlike the tDwT which would not be available until a

32

session is over, DecT can be easily obtained in an early phase of a session.
5.4 Task Type as a Helpful Contextual Factor in Predicting Usefulness
Our results show that in the dependent task, task stage did not play a role in predicting document usefulness, but in the parallel task, it did. A likely explanation is that in the dependent task, subtasks that the users worked with in the three sessions were different not only in their topics, but also in the sub-task patterns. However, in the parallel task, sub-tasks that the users worked on in the three sessions differed only in their topics; they had the same sub-task patterns, and users only changed car models across sessions. This made it possible for users to gain knowledge across stages regarding the usefulness of some documents. Therefore, their decision time on useful documents in later stages was shorter, while in the dependent task, it was still long.
When task information was not specified, stage also played a role. This is due, we think, to the strong role of task stage in the parallel task. Although inferring document usefulness based on time and stage still works while ignoring task type, it will enhance the prediction accuracy by considering this task type information.
5.5 Further Studies
The findings reported here are only about examination of the role of task stage in multi-session tasks in personalization. There are certainly other factors that may also play significant roles in usefulness prediction, for example, users' knowledge of the task topics. This was elicited in the experiment and will be analyzed in future studies. Also, it would also be interesting to examine the 2nd, the 3rd, and more dwell times besides the 1st dwell time (i.e., DecT) with respect to how they can be used for personalization. In addition, the results in this paper showed that task stage is an important contextual factor to be considered in predicting usefulness from time. Nevertheless, more work is needed to estimate the extent that task stage can help. Further studies will also design and build systems that automatically personalize search based on time threshold and stage information.
6. CONCLUSIONS
In this paper, we examined the relationships among task stage, task type, document usefulness, and three types of time. Decision time can be used for personalizing search for an ongoing session as well as for subsequent sessions. It was found that in the parallel task, one needs to take account of task stage in order to accurately predict usefulness from decision time. However, this is not the case with the dependent task. This suggests that it is important to be able to identify task type in addition to task stage in order to predict document usefulness based on decision time. Total display time and total dwell time can be used for personalizing search for subsequent sessions in multi-session tasks. Either of them alone appears to be a reliable indicator of document usefulness, and task stage and task type information does not necessarily need to be considered. While these findings can only be generalized to the task types and task stages as defined in this paper, they suggest that there may be similar results when considering other task types, and other definitions of task stage.

7. ACKNOWLEDGMENTS
This research is sponsored by IMLS grant LG#06-07-0105. We thank the reviewers for their insightful comments.
8. REFERENCES
[1] Belkin, N.J. (2008). Some(What) grand challenges for information retrieval. ACM SIGIR Forum 42, 1, 47-54.
[2] Byström, K., & Järvelin, K. (1995). Task complexity affects information seeking and use. Inform Process Manag, 31, 191213.
[3] Freund (2008). Exploring task-document relations in support of information retrieval in the workplace. Unpublished Dissertation. University of Toronto.
[4] Ingwersen, P. & Järvelin , K. (2005). The turn: Integration of information seeking and retrieval in context. Springer-Verlag New York, Inc. Secaucus, NJ, USA.
[5] Kellar, M., Watters, C., & Shepherd, M. (2007). A field study characterizing Web-based information-seeking tasks. J. Am. Soc. Inf. Sci. Tec., 58(7), 999-1018.
[6] Kelly, D. & Belkin, N.J. (2004). Display time as implicit feedback: Understanding task effects. Proc. SIGIR, 377-384.
[7] Kuhlthau, C.C. (1991). Inside the search process: information seeking from the user's perspective. J. Am. Soc. Inf. Sci., 42, 361-371.
[8] Li, Y. & Belkin, N.J. (2008). A faceted approach to conceptualizing tasks in information seeking. Inform Process Manag, 44, 1822-1837.
[9] Lin, S.-J. (2001). Modeling and Supporting Multiple Information Seeking Episodes over the Web. Unpublished dissertation. Rutgers University.
[10] Tang, R., Shaw, W.M., & Vevea, J.L. (1999). Towards the identification of the optimal number of relevance categories. J. Am. Soc. Inf. Sci. Tec, 50(3), 254-264.
[11] Taylor, R.S. (1968). Question negotiation and information seeking in libraries. College & Research Libraries, 29, 178-194.
[12] Taylor, A. R., Cool, C., Belkin, N.J., & Amadio, W.J. (2007). Relationships between categories of relevance criteria and stage in task completion. Inform Process Manag 43, 1071-1084.
[13] Toms, E., MacKenzie, T., Jordan, C., O'Brien, H., Freund, L., Toze, S., et al. (2007). How task affects information search. In Workshop Pre-proceedings in Initiative for the Evaluation of XML Retrieval (INEX), 337-341.
[14] Vakkari, P., & Hakala, N. (2000). Changes in relevance criteria and problem stages in task performance. J. Doc. 56(5), 540-562.
[15] Vakkari, P., Pennanen, M., & Serola, S. (2003). Changes of search terms and tactics while writing a research proposal: A longitudinal research. Inform Process Manag, 39, 445-463.
[16] Vakkari, P. H., N. (2000). Changes in relevance criteria and problem stages in task performance. J. Doc., 56(5), 540-562.
[17] White, R., & Kelly, D. (2006). A study of the effects of personalization and task information on implicit feedback performance. Proc. CIKM, 297-306.
[18] White, R.W., Ruthven, I., & Jose, J.M. (2005). A study of factors affecting the utility of implicit relevance feedback. Proc. SIGIR, 35-42.

33

Active Learning for Ranking through Expected Loss Optimization

Bo Long
Yahoo! Labs Sunnyvale, CA
bolong@yahoo-inc.com

Olivier Chapelle
Yahoo! Labs Sunnyvale, CA
chap@yahoo-inc.com

Ya Zhang
Shanghai Jiao Tong University Shanghai, China
yazhang@sjtu.edu.cn

Yi Chang
Yahoo! Labs Sunnyvale, CA
yichang@yahoo-inc.com

Zhaohui Zheng
Yahoo! Labs Sunnyvale, CA
zhaohui@yahoo-inc.com

Belle Tseng
Yahoo! Labs Sunnyvale, CA
belle@yahoo-inc.com

ABSTRACT
Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation system. In learning to rank, the performance of a ranking model is strongly affected by the number of labeled examples in the training set; on the other hand, obtaining labeled examples for training data is very expensive and time-consuming. This presents a great need for the active learning approaches to select most informative examples for ranking learning; however, in the literature there is still very limited work to address active learning for ranking. In this paper, we propose a general active learning framework, Expected Loss Optimization (ELO), for ranking. The ELO framework is applicable to a wide range of ranking functions. Under this framework, we derive a novel algorithm, Expected DCG Loss Optimization (ELO-DCG), to select most informative examples. Furthermore, we investigate both query and document level active learning for raking and propose a two-stage ELO-DCG algorithm which incorporate both query and document selection into active learning. Extensive experiments on real-world Web search data sets have demonstrated great potential and effectiveness of the proposed framework and algorithms.
Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous
General Terms
Algorithms, Design, Theory, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
Active Learning, Ranking, Expected Loss Optimization
1. INTRODUCTION
Ranking is the core component of many important information retrieval problems, such as web search, recommendation, computational advertising. Learning to rank represents an important class of supervised machine learning tasks with the goal of automatically constructing ranking functions from training data. As many other supervised machine learning problems, the quality of a ranking function is highly correlated with the amount of labeled data used to train the function. Due to the complexity of many ranking problems, a large amount of labeled training examples is usually required to learn a high quality ranking function. However, in most applications, while it is easy to collect unlabeled samples, it is very expensive and time-consuming to label the samples.
Active learning comes as a paradigm to reduce the labeling effort in supervised learning. It has been mostly studied in the context of classification tasks [19]. Existing algorithms for learning to rank may be categorized into three groups: pointwise approach [8], pairwise approach [25], and listwise approach [21]. Compared to active learning for classification, active learning for ranking faces some unique challenges. First, there is no notion of classification margin in ranking. Hence, many of the margin-based active learning algorithms proposed for classification tasks are not readily applicable to ranking. Further more, even some straightforward active learning approach, such as query-by-committee, has not been justified for the ranking tasks under regression framework. Second, in most supervised learning setting, each data sample can be treated completely independent of each other. In learning to rank, data examples are not independent, though they are conditionally independent given a query. We need to consider this data dependence in selecting data and tailor active learning algorithms according to the underlying learning to rank schemes. There is a great need for an active learning framework for ranking.
In this paper, we attempt to address those two important and challenging aspects of active learning for ranking. We first propose a general active learning framework, Expected

267

Loss Optimization (ELO), and apply it to ranking. The key idea of the proposed framework is that given a loss function, the samples minimizing the expected loss are the most informative ones. Under this framework, we derive a novel active learning algorithm for ranking, which uses function ensemble to select most informative examples that minimizes a chosen loss. For the rest of the paper, we use web search ranking problem as an example to illustrate the ideas and perform evaluation. But the proposed method is generic and may be applicable to all ranking applications. In the case of web search ranking, we minimize the expected DCG loss, one of the most commonly used loss for web search ranking. This algorithm may be easily adapted to other ranking loss such as NDCG or Average Precision. To address the data dependency issue, the proposed algorithm is further extended to a two-stage active learning schema to seamlessly integrate query level and document level data selection.
The main contributions of the paper are summarized as follows.
· We propose a general active learning framework based on expected loss optimization. This framework is applicable to various ranking scenarios with a wide spectrum of learners and loss functions. We also provides a theoretically justified measure of the informativeness .
· Under the ELO framework, we derive novel algorithms to select most informative examples by optimizing the expected DCG loss. Those selected examples represent the ones that the current ranking model is most uncertain about and they may lead to a large DCG loss if predicted incorrectly.
· We propose a two stage active learning algorithm for ranking, which addresses the sample dependence issue by first performing query level selection and then document level selection.
2. RELATED WORK
The main motivation for active learning is that it usually requires time and/or money for the human expert to label examples and those resources should not be wasted to label non-informative samples, but be spent on interesting ones.
Optimal Experimental Design [12] is closely related to active learning as it attempts to find a set of points such that the variance of the estimate is minimized. In contrast to this "batch" formulation, the term active learning often refers to an incremental strategy [7].
There has been various types of strategies for active learning that we now review. A comprehensive survey can be found in [20]. The simplest and maybe most common strategy is uncertainty sampling [18], where the active learning algorithm queries points for which the label uncertainty is the highest. The drawback of this type of approach is that it often mixes two types of uncertainties, the one stemming from the noise and the variance. The noise is something intrinsic to the learning problem which does not depend on the size of the training set. An active learner should not spend too much effort in querying points in noisy regions of the input space. On the other hand, the variance is the uncertainty in the model parameters resulting from the finiteness of the training set. Active learning should thus try to minimize this variance and this was first proposed in [7].

In Bayesian terms, the variance is computed by integrating over the posterior distribution of the model parameters. But in practice, it may be difficult or impossible to compute this posterior distribution. Instead, one can randomly sample models from the posterior distribution [9]. An heuristic way of doing so is to use a bagging type of algorithm [1]. This type of analysis can be seen as an extension of the Query-By-Committee (QBC) algorithm [14] which has been derived in a noise free classification setting. In that case, the posterior distribution is uniform over the version space ­ the space of consistent hypothesis with the labeled data ­ and the QBC algorithm selects points on which random functions in the version space have the highest disagreement.
Another fairly common heuristic for active learning is to select points that once added in the training set are expected to result in a large model change [20] or a large increase in the objective function value that is being optimized [4].
Compared with traditional active learning, there is still limited work on the active learning for ranking. Donmez and Carbonell studied the problem of document selection in ranking [11]. Their algorithm selects the documents which, once added to the training set, are the most likely to result in a large change in the model parameters of the ranking function. They apply their algorithm to RankSVM [17] and RankBoost [13]. Also in the context of RankSVM, [24] suggests to add the most ambiguous pairs of documents to the training set, that is documents whose predicted relevance scores are very close under the current model. Other works based on pairwise ranking include [6, 10]. In case of binary relevance, [5] proposed a greedy algorithm which selects document that are the most likely to differentiate two ranking systems in terms of average precision. Finally, an empirical comparison of document selection strategies for learning to rank can be found in [2].
There are some related works about query sampling. [23] empirically shows that having more queries but less number of documents per query is better than having more documents and less queries. Yang et. al. propose a greedy query selection algorithm that tries to maximize a linear combination of query difficulty, query density and query diversity [22].
3. EXPECTED LOSS OPTIMIZATION FOR ACTIVE LEARNING
As explained in the previous section, a natural strategy for active learning is based on variance minimization. The variance, in the context of regression, stems from the uncertainty in the prediction due to the finiteness of the training set. Cohn et. al [7] proposes to select the next instance to be labeled as the one with the highest variance. However, this approach applies only to regression and we aim at generalizing it through the Bayesian expected loss [3].
In the rest of the section, we first review Bayesian decision theory in section 3.1 and then introduce the Expected Loss Optimization (ELO) principle for active learning. In section 3.2 we show that in the cases of classification and regression, applying ELO turns out to be equivalent to standard active learning method. Finally, we present ELO for ranking in section 3.3.
3.1 Bayesian Decision Theory
We consider a classical Bayesian framework to learn a

268

function f : X  Y parametrized by a vector . The training data D is made of n examples, (x1, y1), . . . , (xn, yn). Bayesian learning consists in:

1. Specifying a prior P () on the parameters and a likelihood function P (y|x, ).

2. Computing the likelihood of the training data, P (D|) =

n i=1

P

(yi

|xi

,

).

3. Applying Bayes rule to get the posterior distribution of the model parameters, P (|D) = P (D|)P ()/P (D).

4. For a test point x, computing the predictive distribution P (y|x, D) =  P (y|x, )P (|D)d.
Note that in such a Bayesian formalism, the prediction is a distribution instead of an element of the output space Y. In order to know which action to perform (or which element to predict), Bayesian decision theory needs a loss function. Let (a, y) be the loss incurred by performing action a when the true output is y. Then the Bayesian expected loss is defined as the expected loss under the predictive distribution:

(a) := (a, y)P (y|x, D)dy.

(1)

y

The best action according to Bayesian decision theory is the one that minimizes that loss: a := arg mina (a). Central to our analysis is the expected loss (EL) of that action, (a) or

EL(x) := min (a, y)P (y|x, D)dy.

(2)

ay

This quantity should be understood as follows: given that we have taken the best action a for the input x, and that the true output is in fact given by P (y|x, D), what is, in expectation, the loss to be incurred once the true output is revealed?
The overall generalization error (i.e. the expected error on unseen examples) is the average of the expected loss over the input distribution: x EL(x)P (x)dx. Thus, in order to minimize this generalization error, our active learning strategy consists in selecting the input instance x to maximize the expected loss:

arg max EL(x).
x

3.2 ELO for Regression and Classification
In this section, we show that the ELO principle for active learning is equivalent to well known active learning strategies for classification and regression. In the cases of regression and classification, the "action" a discussed above is simply the prediction of an element in the output space Y.

Regression.
The output space is Y = R and the loss function is the squared loss (a, y) = (a - y)2. It is well known that the prediction minimizing this square loss is the mean of the distribution and that the expected loss is the variance:
arg min (a - y)2P (y|x, D)dy = µ
ay
and min (a - y)2P (y|x, D)dy = 2,
ay

where µ and 2 are the mean and variance of the predictive distribution. So in the regression case, ELO will choose the point with the highest predictive variance which is exactly one of the classical strategy for active learning [7].

Classification.
The output space for binary classification is Y = {-1, 1} and the loss is the 0/1 loss: (a, y) = 0 if a = y, 1 otherwise. The optimal prediction is given according to arg maxaY P (y = a|x, D) and the expected loss turns out to be:
min(P (y = 1|x, D), P (y = -1|x, D)),
which is maximum when P (y = 1|x, D) = P (y = -1|x, D) = 0.5, that is when we are completely uncertain about the class label. This uncertainty based active learning is the most popular one for classification which was first proposed in [18].
3.3 ELO for Ranking
In the case of ranking, the input instance is a query and a set of documents associated with it, while the output is a vector of relevance scores. If the query q has n documents, let us denote by Xq := (x1, . . . , xn) the feature vectors describing these (query,document) pairs and by Y := (y1, . . . , yn) their labels. As before we have a predictive distribution P (Y |Xq, D). Unlike active learning for classification and regression, active learning for ranking can select examples at different levels. One is query level, which selects a query with all associated documents Xq; the other one is document level, which selects documents xi individually .
Query level. In the case of ranking, the "action" in ELO
framework is slightly different than before because we are not directly interested in predicting the scores, but instead we want to produce a ranking. So the set of actions is the set of permutations of length n and for a given permutation , the rank of the i-th document (i). The expected loss for a given  can thus be written as:

(, Y )P (Y |Xq, D)dY,

(3)

Y

where (, Y ) quantifies the loss in ranking according to  if the true labels are given by Y . The next section will detail the computation of the expected loss where is the DCG loss.
As before, the ELO principle for active learning tells us to select the queries with the highest expected losses:

EL(q) := min


Y

(, Y )P (Y |Xq, D)dY.

(4)

As an aside, note that the ranking minimizing the loss (3) is not necessarily the one obtained by sorting the documents according to their mean predicted scores. This has already been noted for instance in [26, section 3.1].

Document level. Selecting the most informative document
is a bit more complex because the loss function in ranking is defined at the query level and not at the document level. We can still use the expected loss (4), but only consider the predictive distribution for the document of interest and consider the scores for the other documents fixed. Then we take an expectation over the scores of the other documents.

269

This leads to:

EL(q, i) =

min
Yi 

yi

(, Y )P (Y |Xq, D)dyidY i,

(5)

where EL(q, i) is the expected loss for query q associated with the i-th document and Y i is the vector Y after removing yi.

4. ALGORITHM DERIVATION
We now provide practical implementation details of the ELO principle for active learning and in particular specifie how to compute equations (4) and (5) in case of the DCG loss.
The difficulty of implementing the formulations of the previous section lies in the fact that the computation of the posterior distributions P (yi|xi, D) and the integrals is in general intractable. For this reason, we instead use an ensemble of learners and replace the integrals by sums over the ensemble. As in [1], we propose to use bootstrap to construct the ensemble. More precisely, the labeled set is subsampled several times and for each subsample, a relevance function is learned. The predictive distribution for a document is then given by the predicted relevance scores by various functions in the ensemble. The use of bootstrap to estimate predictive distributions is not new and there has been some work investigating whether the two procedures are equivalent [16].
Finally note that in our framework we need to estimate the relevance scores. This is why we concentrate in this paper on pointwise approaches for learning to rank since pairwise and listwise approaches would not produce such relevance estimates.

4.1 Query Level Active Learning
If the metric of interest is DCG, the associated loss is the difference between the DCG for that ranking and the ranking with largest DCG:

(, Y ) = max DCG( , Y ) - DCG(, Y ),

(6)



where DCG(, Y ) =

. 2yi -1
i log2(1+(i))

Combining equations (4) and (6), the expected loss for a

given q is expressed as follows:

EL(q) =

Y

max DCG(, Y


)P (Y

|Xq, D)dY

- max


DCG(, Y )P (Y |Xq, D)dY.
Y

(7)

The maximum in the first component of the expected loss can easily be found by sorting the documents according to Y . We rewrite the integral in the second component as:

DCG(, Y )P (Y |Xq, D)dY

Y

=

i

1 log2(1 + (i))

(2yi - 1)P (yi|xi, D)dyi,
yi

(8)

:=ti

with which the maximum can now be found by sorting the ti.
The pseudo-code for selecting queries based on equation (7) is presented in algorithm 1. The notations and definitions are as follows:

· G is the gain function defined as G(s) = 2s - 1.

· The notation · means average. For instance, di =

1 N

N i=1

di.

· BDCG is a function which takes as input a set of gain values and returns the corresponding best DCG:

BDCG({gj}) =

j

log2

(1

gj +



(j

))

,

where  is the permutation sorting the gj in decreasing order.

Algorithm 1 Query Level ELO-DCG Algorithm

Require: Labeled set L, unlabeled set U

for i=1,. . . ,N do

N =size of the ensemble

Subsample L and learn a relevance function

sij  score predicted by that function on the j-th document in U .

end for

for q=1,. . . ,Q do

Q = number of queries in U

I  documents associated to q

for i=1,. . . ,N do

di  BDCG({G(sij )}jI ) end for

tj  G(sij ) d  BDCG({tj }jI )

EL(q)  di - d

end for

Select the queries q which have the highest values EL(q).

4.2 Document Level Active Learning
Combining equations (5) and (6), the expected loss of the i-th document is expressed as:

EL(q, i) =

max DCG(, Y )P (Y |Xq, D)dyi

Yi

yi 

- max


DCG(, Y )P (Y |Xq, D)dyi
yi

dY i,

(9)

which is similar to equation (7) except that the uncertainty is on yi instead of the entire vector Y and that there is an outer expectation on the relevance values for the other documents. The corresponding pseudo-code is provided in algorithm 2.

4.3 Two-stage Active Learning
Both query level and document level active learning have their own drawbacks. Since query level active learning selects all documents associated with a query, it is tend to include non-informative documents when there are a large number of documents associated with each query. For example, in Web search applications, there are large amount of Web documents associated for a query; most of them are non-informative, since the quality of a ranking function is mainly measured by its ranking output on a small number of top ranked Web documents. On the other hand, document level active learning selects documents individually. This selection process implies unrealistic assumption that documents are independent, which leads to some undesirable results. For example, an informative query could be

270

Algorithm 2 Document Level ELO-DCG Algorithm

Require: Labeled set L, unlabeled doc U for a given query

for i=1,. . . ,N do

N =size of the ensemble

Subsample L and learn a relevance function

sij  score predicted by that function on the j-th document in U .

end for

for all j  U do

EL(j)  0

Expected loss for the j-th document

for i=1,. . . ,N do

Outer integral in (5)

tk  sik, k = j for p=1,. . . ,N do

tj  spj dp  BDCG({G(tk)})

end for

gk  G(sik), k = j gj  G(sij ) EL(j)  EL(j) + dp - BDCG({gk})

end for

end for

Select the documents (for the given query) which have the

highest values of EL(j).

missed if none of its documents is selected; or only one document is selected for a query, which is not a good example in ranking learning.
Therefore, it is natural to combine query level and document level into two-stage active learning. A realistic assumption for ranking data is that queries are independent and the documents are independent given on a query. Based on this assumption, we propose the following two-stage ELO-DCG algorithm: first, applying Algorithm 1 to select most informative queries; then, applying Algorithm 2 to select the most informative queries for each selected query.
5. EXPERIMENTAL EVALUATION
As a general active learning algorithm for ranking, ELODCG can be applied to a wide range of ranking applications. In this section, we apply different versions of ELO-DCG algorithms to Web search ranking to demonstrate the properties and effectiveness of our algorithm. We denote query level, document level, and two-stage ELO-DCG algorithms as ELO-DCG-Q, ELO-DCG-D, and ELO-DCG-QD, respectively.
5.1 Data Sets
We use Web search data from a commercial search engine. The data set consists of a random sample of about 10,000 queries with about half million Web documents. Those query-document pairs are labeled using a five-grade labeling scheme: {Bad, Fair, Good, Excellent, Perfect}.
For a query-document pair (q; d), a feature vector x is generated and the features generally fall into the following three categories. Query features comprise features dependent on the query q only and have constant values across all the documents, for example, the number of terms in the query, whether or not the query is a person name, etc. Document features comprise features dependent on the document d only and have constant values across all the queries, for example, the number of inbound links pointing to the document, the amount of anchor-texts in bytes for the document,

Data set base set 2k base set 4k base set 8k
AL set test set

Number of examples 2,000  4,000 8,000
160,000 180,000

Table 1: Sizes of the five data sets.
and the language identity of the document, etc. Querydocument features comprise features dependent on the relation of the query q with respect to the document d, for example, the number of times each term in the query q appears in the document d, the number of times each term in the query q appears in the anchor-texts of the document d, etc. We selected about five hundred features in total.
We randomly divide this data set into three subsets, base training set, active learning set, and test set. From the base training set, we randomly sample four small data sets to simulate small size labeled data sets L. The active learning data set is used as a large size unlabeled data set U from which active learning algorithms will select the most informative examples. The true labels from active learning set are not revealed to the ranking learners unless the examples are selected for active learning. The test set is used to evaluate the ranking functions trained with the selected examples plus base set examples. We kept test set large to have rigorous evaluations on ranking functions. The sizes of those five data sets are summarized in Table 1.
5.2 Experimental Setting
For the learner, we use Gradient Boosting Decision Tree (GBDT) [15].
The input for ELO-DCG algorithm is a base data set L and the AL data set U . The size of the function ensemble is set as 8 for all experiments. ELO-DCG algorithm selects top m informative examples; those m examples are then added to the base set to train a new ranking function; the performance of this new function is then evaluated on the test set. Each algorithm with each base set is tested on 14 different m, ranging from 500 to 80,000. For every experimental setting, 10 runs are repeated and in each run the base set is re-sampled to generate a new function ensemble.
For the performance measure for ranking models, we select to use DCG-k, since users of a search engine are only interested in the top-k results of a query rather than a sorted order of the entire document collection. In this study, we select k as 10. The average DCG of 10 runs is reported for each experiment setting.
5.3 Document Level Active Learning
We first investigate document level active learning, since documents correspond to basic elements to be selected in the traditional active learning framework. We compare document level ELO-DCG algorithm with random selection (denoted by Random-D) and a classical active learning approach based on Variance Reduction (VR) [7], which selects document examples with largest variance on the prediction scores.
Figure 1 compares the three document level active learning methods in terms of DCG-10 of the resulting ranking functions on the test set. Those ranking functions are trained with base data set and the selected examples. X-axis de-

271

DCG10 DCG10 DCG10

2k Base Data Set

12.4

REVLRaOnd-DoCmG-D-D

12.2

12.0

11.8

11.6

11.4

29

210

211

212

213

214

215

216

Number of Examples Selected

4k Base Data Set

12.5 12.4

REVLRaOnd-DoCmG-D-D

12.3

12.2

12.1

12.0

11.9

11.8

29

210

211

212

213

214

215

216

Number of Examples Selected

8k Base Data Set

12.5

REVLRaOnd-DoCmG-D-D

12.4

12.3

12.2

12.1

12.0

29

210

211

212

213

214

215

216

Number of Examples Selected

Figure 1: DCG comparison of document level ELO-DCG, variance reduction based document selection, and random document selection with base sets of sizes 2k,4k, and 8k shows that ELO-DCG algorithm outperforms the other two document selection methods at various sizes of selected examples.

notes number of examples selected by the active learning algorithm. For all three methods, the DCG increases with the number of added examples. This agrees with the intuition that the quality of a ranking function is positively correlated with the number of examples in the training set. ELO-DCG consistently outperforms the other two methods. An possible explanation is that ELO-DCG optimizes the expected DCG loss that is directly related to the objective function DCG-10 used to evaluate the ranking quality; on the other hand, VR reduces the score variance that is not directly related to the objective function. In fact, VR performs even worse than random document selection when the size of the selected example is small. An advantage of the ELO-DCG algorithm is its capability to optimize directly based on the ultimate loss function to measure ranking quality.
5.4 Query Level Active Learning
In this section, we show that query level ELO-DCG algorithm effectively selects informative queries to improve the learning to rank performance. Since traditional active learning approaches cannot directly applied to query selection in ranking, we compare it with random query selection (denoted by Random-Q) used in practice.
Figure 2 shows the DCG comparison results. we observe that for all three base sets, ELO-DCG performs better than random selection for all different sample sizes (from 500 to 80,000) that are added to base sets. Moreover, ELO-DCG converges much faster than random selection, i.e., ELODCG attains the best DCG that the whole AL data set can attain with much less examples added to the train data.
5.5 Two-stage Active Learning
In this section, we compare two-stage ELO-DCG algorithm with other two two-stage active learning algorithms. One is two-stage random selection, i.e. random query selection followed by random document selection for each query. The other one is a widely used approach in practice, which first randomly selects queries and then select top k relevant documents for each query based on current ranking functions (such as top k Web sites returned by the current search engine)[23]. In our experimental setting, this approach corresponds to randomly query selection followed by selecting k documents with highest mean relevance scores within each selected query. We denote this approach as top-K. In all

three two-stage algorithms, we simply fix the number of documents per query at 15 based on the results from [23].
Figure 3 shows the DCG comparison results for two-stage active learning. We observe that among all three base sets, ELO-DCG performs the best and top-K performs the second. This result demonstrates that two-stage OLE-DCG effectively select most informative documents for most informative queries. A possible reason that top-K performs better than random selection is that top-k selects more perfect and excellent examples. Those examples contribute more to DCG than bad and fair examples.
We have observed that ELO-DCG algorithms perform best in all three active learning scenarios, query level, document level, and two stage active learning. Next, we compare three versions of ELO-DCG with each other.
Figure 4 shows DCG comparisons of two-stage ELO-DCG, query level ELO-DCG, and document level ELO-DCG. We observe that for all three based sets, two stage ELO-DCG performs best. The reason that two-stage algorithm performs best may root in its reasonable assumption for the ranking data: queries are independent; the documents are conditionally independent given a query. On the other hand, the document level algorithm makes the incorrect assumption about document independence and may miss informative information at the query level; the query level algorithm selects all documents associated with a query, which are not all informative.
5.6 Cost reduction of Two-stage ELO-DCG Algorithm
In this section, we show the reduction in labeling cost achieved by ELO-DCG compared with the widely used topK approach in practice.
In Table 2, the saturated size means that when the examples of this size are selected and added back to the base set to train a ranking function, the performance of the learned ranking function is equivalent to the ranking function trained with all active learning data. From the first row of Table 2, we observe that for the base set of size 2k, 64k is the saturated size for two-stage ELO-DCG algorithm and 80k is the saturated size for top-K approach; hence, 64k selected examples from two-stage ELO-DCG algorithm is equivalent to 80k selected examples from top-K approach. This means that two-stage ELO-DCG algorithm can reduce the cost by

272

DCG10 DCG10 DCG10

2k Base Data Set Random-Q 12.4 ELO-DCG-Q

12.2

12.0

11.8

11.6

11.4

29

210

211

212

213

214

215

216

Number of Examples Selected

4k Base Data Set 12.5 Random-Q
ELO-DCG-Q
12.4
12.3
12.2
12.1
12.0
11.9
11.8

29

210

211

212

213

214

215

216

Number of Examples Selected

8k Base Data Set 12.5 Random-Q
ELO-DCG-Q
12.4

12.3

12.2

12.1

12.0

29

210

211

212

213

214

215

216

Number of Examples Selected

Figure 2: DCG comparisons of query level ELO-DCG and random query selection with base sets of sizes 2k,4k, and 8k shows that ELO-DCG algorithm outperforms random selection at various sizes of selected examples.

DCG10 DCG10 DCG10

2k Base Data Set Random-QD 12.4 ELO-DCG-QD Top-K
12.2
12.0
11.8
11.6

29

210

211

212

213

214

215

216

Number of Examples Selected

4k Base Data Set 12.5 Random-QD
ELO-DCG-QD 12.4 Top-K

12.3

12.2

12.1

12.0

11.9

11.8

29

210

211

212

213

214

215

216

Number of Examples Selected

8k Base Data Set 12.5 Random-QD
ELO-DCG-QD Top-K
12.4
12.3
12.2
12.1

29

210

211

212

213

214

215

216

Number of Examples Selected

Figure 3: DCG comparisons of two-stage ELO-DCG, two-stage random selection, and top-K selection with base set 2k,4k, and 8k shows that ELO-DCG algorithm performs best.

20%. The largest percentage of cost reduced, 64%, is from base set 8k.
6. CONCLUSIONS
We propose a general expected loss optimization framework for ranking, which is applicable to active learning scenarios for various ranking learners. Under ELO framework, we derive novel algorithms, query level ELO-DCG and document level ELO-DCG, to select most informative examples to minimize the expected DCG loss. We propose a two stage active learning algorithm to select the most effective examples for the most effective queries. Extensive experiments on real-world Web search data sets have demonstrated great potential and effectiveness of the proposed framework and algorithms.
7. REFERENCES
[1] N. Abe and H. Mamitsuka. Query learning strategies using boosting and bagging. In Proceedings of the Fifteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc., 1998.
[2] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 468­475. ACM, 2009.

[3] J. Berger. Statistical decision theory and Bayesian analysis. Springer, 1985.
[4] C. Campbell, N. Cristianini, and A. Smola. Query learning with large margin classifiers. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 111­118. Morgan Kaufmann, 2000.
[5] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2006.
[6] W. Chu and Z. Ghahramani. Extensions of gaussian processes for ranking: semi-supervised and active learning. In Nips workshop on Learning to Rank, 2005.
[7] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information Processing Systems, volume 7, pages 705­712. The MIT Press, 1995.
[8] D. Cossock and T. Zhang. Subset ranking using regression. In Proc. Conf. on Learning Theory, 2006.
[9] I. Dagan and S. P. Engelson. Committee-based sampling for training probabilistic classifiers. In In Proceedings of the Twelfth International Conference on Machine Learning, pages 150­157. Morgan Kaufmann, 1995.
[10] P. Donmez and J. Carbonell. Active sampling for rank

273

DCG10 DCG10 DCG10

2k Base Data Set 12.5 ELO-DCG-Q 12.4 EELLOO--DDCCGG--DQD
12.3

12.2

12.1

12.0

11.9

11.8

11.7

29

210

211

212

213

214

215

216

Number of Examples Selected

4k Base Data Set 12.5 ELO-DCG-Q
EELLOO--DDCCGG--DQD
12.4

12.3

12.2

12.1

12.0

11.9

29

210

211

212

213

214

215

216

Number of Examples Selected

8k Base Data Set 12.5 ELO-DCG-Q
EELLOO--DDCCGG--DQD
12.4

12.3

12.2

12.1

29

210

211

212

213

214

215

216

Number of Examples Selected

Figure 4: DCG comparisons of two-stage ELO-DCG, query level ELO-DCG, and document level ELODCG,with base sets of sizes 2k,4k, and 8k shows that two-stage ELO-DCG algorithm performs best.

Base set size 2k 4k 8k

Saturated size for ELO-DCG 64k 48k 32k

Saturated size for top-K 80k 80k 80k

Percentage of cost reduced 20% 40% 64%

Table 2: Cost reduction of two-stage algorithm compared with top-K approach.

learning via optimizing the area under the ROC curve. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, pages 78­89, 2009.
[11] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In ICML '08: Proceedings of the 25th international conference on Machine learning, pages 248­255, New York, NY, USA, 2008. ACM.
[12] V. Fedorov. Theory of Optimal Experiments. Academic Press, New York, 1972.
[13] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933­969, 2003.
[14] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133­168, 1997.
[15] J. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 1189­1232, 2001.
[16] T. Fushiki. Bootstrap prediction and bayesian prediction under misspecified models. Bernoulli, 11(4):747­758, 2005.
[17] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. In Smola, Bartlett, Schoelkopf, and Schuurmans, editors, Advances in Large Margin Classifiers. MIT Press, Cambridge, MA, 2000.
[18] D. Lewis and W. Gale. Training text classifiers by uncertainty sampling. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3­12, 1994.
[19] A. McCallum and K. Nigam. Employing EM and pool-based active learning for text classification. In

Proceedings of the 5th international conference on Machine learning, pages 359­367, 1998.
[20] B. Settles. Active learning literature survey. Technical Report Computer Sciences 1648, University of Wisconsin, Madison, 2009.
[21] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank: theory and algorithm. In ICML '08: Proceedings of the 25th international conference on Machine learning, pages 1192­1199, New York, NY, USA, 2008. ACM.
[22] L. Yang, L. Wang, B. Geng, and X.-S. Hua. Query sampling for ranking learning in web search. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 754­755, New York, NY, USA, 2009. ACM.
[23] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 662­663, New York, NY, USA, 2009. ACM.
[24] H. Yu. SVM selective sampling for ranking with application to data retrieval. In KDD '05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 354­363. ACM, 2005.
[25] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen, and G. Sun. A general boosting method and its application to learning ranking functions for web search. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1697­1704, 2008.
[26] O. Zoeter, N. Craswell, M. Taylor, J. Guiver, and E. Snelson. A decision theoretic framework for implicit relevance feedback. In NIPS Workshop on Machine learning for web search, 2007.

274

Image Search by Concept Map 
Hao Xu Jingdong Wang Xian-Sheng Hua Shipeng Li
MOE-MS KeyLab of MCC, University of Science and Technology of China, Hefei, 230026, P. R. China Microsoft Research Asia, Beijing 100190, P. R. China
xuhao657@ustc.edu, {jingdw, xshua, spli}@microsoft.com

ABSTRACT
In this paper, we present a novel image search system, image search by concept map. This system enables users to indicate not only what semantic concepts are expected to appear but also how these concepts are spatially distributed in the desired images. To this end, we propose a new image search interface to enable users to formulate a query, called concept map, by intuitively typing textual queries in a blank canvas to indicate the desired spatial positions of the concepts. In the ranking process, by interpreting each textual concept as a set of representative visual instances, the concept map query is translated into a visual instance map, which is then used to evaluate the relevance of the image in the database. Experimental results demonstrate the effectiveness of the proposed system.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.5 [Information Storage and Retrieval]: Online Information Services
General Terms
Algorithms, Experimentation, Performance
Keywords
image search, concept map
1. INTRODUCTION
Digital image is nowadays the second most prevalent media in the Web only after text. Image search engines play an important role in enabling people to easily access to the desired images. A variety of search interfaces have been employed to let users submit the query in various forms, e.g., textual input, image input, and painting based input, to indicate the search goal. To facilitate image search, query formulation is required not only to be convenient and effective for users to indicate the search goal clearly, but also to
This work was performed at Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

be easily interpreted by image search engines. Therefore, recently more and more research attention has been paid on search interface design [26, 4, 15, 18, 23] in developing image search engines. In this paper, we focus on designing a new image search system to address a specific kind of image search scenario: the user concerns not only the presence but also the spatial distribution of the specified semantic concepts. An example is illustrated in Fig. 1, where the user wants to "find the images containing a butterfly on the topleft of a flower". Such image search intention concerning the spatial distribution of concepts, which is called concept layout hereafter, is common when people try to visually convey ideas, such as making illustration for a slide show.
Existing commercial image search engines, e.g., Google image search [10] and Microsoft Bing image search [16], provide a textbox for users to type one or more keywords to indicate the search goal. Then the text-based search technique is adopted to match the textual query with the textual metadata associated with images. This type of search interface is easy to use. However, besides the limitation that the associated texts may not reveal the image content, it is not easy to perform image search with the requirement on the concept layout. The search results by "butterfly flower", as shown in Fig. 1, do not come up to expectation (only two relevant images in top ten images). Adding the spatial description, "on the top-left of", to the query will not improve the search results, since such spatial description rarely appears in the metadata of Web images.
Conventional content-based image retrieval techniques [12] usually require users to input a visual query, e.g., an example image or a painted sketch, to indicate the desired visual content, such as shape or color distribution [20, 7, 22]. Then visual features extracted from the visual query are used to match with the images in the database. Such system is often inconvenient to use because the users have to submit an image that may not be available at hand, or paint a sketch that may not be easy to precisely indicate the search goal. Besides, such techniques suffer from at least two drawbacks: visually similar images may have different semantic contents; visual query has limited capability to represent the semantic search goal, since same semantic content may have different visual appearances. Therefore, content-based image search techniques are difficult to handle the image search task in Fig. 1. This is in line with the unsatisfactory search results by visual queries in Fig. 1.
Recently, some efforts have been made to take advantage of both textual queries and visual queries. The "show similar images" feature of Microsoft Bing image search enables users to pick an image as the example image from the search results of a textual query, and then promote the visually similar images. Another investigation was conducted by painting color strokes to indicate the desired spatial distribution of colors and promoting the images satisfying the color distribution from the search results of a textual query [21].

275

butterfly flower text-based search

show similar image

sketch-based search

butterfly flower
our approach
Figure 1: Illustration of four types of image search schemes: text-based image search, "show similar images" of Microsoft Bing image search, sketch-based image search, and our approach. The user's search intention is "finding the images containing a butterfly on the top-left of a flower". The images accord with the search intention are highlighted in red boxes.

These techniques may, to some extent, help users to find the images that satisfy some specific semantic and visual requirements, but still suffer from the drawback that the search intention concerning the concept layout is difficult to indicate. The search results of "show similar image" are shown in Fig. 1. Selecting a good example image that exactly accords with the search intention does not improve the search results significantly.
In this paper, we propose a novel image search system, which presents a novel interface to enable users to intuitively indicate the search goal by formulating the query in a visual manner, i.e., explicitly indicating where the concepts should appear. This new interface is very intuitive and only requires users to type textual queries at the arbitrary positions of a blank canvas to formulate the query, which is called concept map in this paper. Compared with the traditional textual query, the concept map query allows users to indicate not only what concepts should appear in the desired images, but also where these concepts should appear. Compared with the visual query, the concept map query allows users to semantically indicate the search goal rather than submitting an image or a sketch that cannot convey the clear semantic intention. We have previously demonstrated an early version of our system [22]. In this paper, we report on extensions, technical details and experimental analysis of the system. Fig. 1 shows a concept map and the corresponding search results of the proposed system, in which most retrieved images accord with the search intention.
Technically, it becomes possible to compute the relevance of an image according to the user's spatial intention, since desired spatial distribution of concepts is visually expressed by a concept map query in an explicit manner. We introduce an effective scheme to translate a concept map to a visual instance map, which represents each concept as a set of visual instances exploiting the techniques of text-based image search and representative image selection. The visual instance map is then used to evaluate the relevance of the image in the database. Besides, we present an interactive tool to allow users to select a few visual examples to assist to visually specify what a concept looks like. Experiments demonstrate the effectiveness of the proposed image search system, including the new query formulation interface and the relevance evaluation scheme.
In summary, the key contributions of this paper are as follows: 1) We present a novel image search system to enable users to search images with the requirement on the spatial distribution of semantic

concepts. 2) We propose a novel image search interface to enable users to intuitively input a so-called concept map by typing textual queries in a blank canvas to indicate what concepts should be included and where they should appear in the desired images. 3) We introduce an effective technique to translate the concept map into visual instance map, and a robust scheme to evaluate the relevance of an image with the visual instance map.
2. RELATED WORK
The image search intention concerning the spatial layout has been investigated in the previous works [20, 7]. The VisualSEEk system presented in [20] uses joint color/spatial queries to perform image search. The users are allowed to freely draw color strokes in a canvas to indicate the desired color and spatial position of visual content in the desired images. A similar idea is explored in [7], only that, instead of color, the desired shape and spatial position of the visual content is considered. These works concern for the search intention of the spatial layout of visual content from the perspective of the low level signal similarity. In contrast, the proposed system concerns for the search intention of the layout of semantic concepts, that is, what expected to appear in a specific position of the desired images is defined by a semantic keyword but not particular color or sketch.
Zavesky and Chang propose an image search scheme called CuZero in [25]. It adopts a concept navigation map to assist users to view the image search results. Note that the concept navigation map in this work is distinct from our concept map, because concept navigation map is a tool for users to straightforwardly select the weights of the concepts to generate a weighted textual query, thus it has nothing to do with the spatial distribution of the concepts in the desired images.
Even more recently, Chen et al. propose a novel system called Sketch2Photo in [3] to synthesize an image by drawing some sketches in a blank canvas and defining each sketch with a keyword. Assisted by the keyword, it is able to find the visual object matched with the input sketch more accurately from the Web, and thus synthesize a more satisfactory image. Our proposed system differs from this work at least in two aspects. First, our goal is to search for the existing images but not synthesize a new one. Second, the Sketch2Photo system requires users to input some sketches and

276

Query Formulation Canvas
Visual Assistor
Figure 2: User interface of the proposed system.
generate the image by matching these sketches, while our system does not require users to draw sketch to describe the shape of the concept but focuses on the spatial relation of the concepts.
3. OVERVIEW
The snapshot of the user interface of the proposed system is shown in Fig. 2. To perform image search, the user first formulates a concept map query, by typing one or more keywords (concepts) in query formulation canvas. As the example in Fig. 2, the concept map consists of three concepts, "sky", "house" and "lawn", which are expected to appear from top to bottom. After submitting the query, the system returns a list of images according to their relevances with the query, shown in search result panel. We also find that in some cases it may be insufficient to merely use a textual keyword to indicate a desired concept, and hence provide an advanced tool, called visual assistor, to allow the user to select a few visual examples to assist to visually specify or narrow down what the desired concept looks like. The user may click advanced function button to popup the visual assistor window. For each concept, a set of visual examples are presented, as shown in Fig. 2. The user may select the examples that are visually close to what are desired.
To evaluate the relevance of an image in the database with the concept map, we propose to transform the concept map to the visual instance map, which replaces each keyword with a set of visual instances and generates a probabilistic map, by estimating the spatial intention, to tolerate the roughness of the input spatial position in the concept map. A layout-sensitive relevance evaluation scheme is then used to compare the visual instance map with the images in the database.
4. APPROACH
4.1 Query Formulation
The procedure of formulating a concept map query is not only convenient, as it only requires the user to simply select a position and type textual keywords, but also intuitive, since the user can express the spatial intention in a visually straightforward manner. To make the interface even more user-friendly, a series of intuitive manipulations are supported to let the user formulate and edit the query more easily, i.e., the user may delete or edit an existing keyword, or modify the position of a concept by dragging the corresponding keyword.

Concept Map Translation
Visual Instance Transformation

Query Formulation
Concept Map

butterfly flower

Web Images
Visual Instance Transformation

Images to rank
Visual Instance
Map
Layout Sensitive Relevance Evaluation

Spatial Intention Estimation

Search Results

Figure 3: The flowchart of concept map translation. A concept map with two concepts "butterfly" and "flower" is used as the example input.
In practice, besides the positions of the concepts, the user may also care about the sizes of the concepts, i.e., the sizes of the regions occupied by the concepts. Therefore, a rectangle is associated with each keyword to indicate the influence scope of the keyword, i.e., the scope of the spatial distribution of the corresponding concept. Once a keyword has been input, a rectangle with the default size, one-ninth of the size of the query formulation canvas, is assigned to it. Our interface allows the user to explicitly specify the influence scope of a keyword by stretching the corresponding rectangle. The rectangle is visible around a keyword for editing when the user moves the mouse over the keyword.
4.2 Concept Map Translation
Relevance evaluation will be straightforward if the images in the database are also represented in the form of concept map, i.e., the information of what concepts appear in the image and where they appear is available. Recently, a lot of efforts, including human tagging and automatic annotation, have been taken to extract such information. Flickr [8] provides a feature to let users add notes in local regions of the images. The LabelMe project [19] also presents a tool to users to help manually assign tags to local regions of the images. The precision of manual annotation may be well guaranteed, but it has some difficulties in the practical applications since we are facing Web-scale images and Web-scale concepts. The computer vision community has tried to investigate automatic techniques [5, 2, 14], which need to collect training images and train a discriminative classifier for each concept, and hence also suffer from the Web-scale concepts.
Instead, we follow the instance-based classification methodology to translate a concept map into a visual instance map, which encodes both the visual appearance and desired spatial distribution of each concept in the concept map, and use the visual instance map to evaluate the relevance of the image in the database. The flowchart of concept map translation is illustrated in Fig. 3.
4.2.1 Visual Instance Transformation
Existing text-based image search engines are successful if the textual query is relatively simple, e.g., a single concept without spatial intention. This means that the search results in some degree can visually represent the concept. On the other hand, the research in the pattern recognition community has shown that the instance-based classifier is promising and can obtain competitive performance with discriminative classifier. These motivate us to exploit the text-based image search technique to transform the textual concept to a few visual instances for the subsequent relevance evaluation.

277

We first collect a set of images by querying a text-based image search engine with the keyword of the concept and then adopt the affinity propagation (AP) algorithm [9] to find the visual instances, so as to obtain the representative appearances of the concept. AP has been shown to be effective and efficient to find a set of exemplars from a set of images [6]. To reduce the computation cost, we run AP only for the top ranked images returned by the text-based image search engine as we found that the main visual appearances of a concept are almost covered by the top ranked images. In our implementation, the visual similarities are evaluated on the regions containing salient objects instead of the entire images. This helps alleviating the influence of the background. The salient object is detected offline using the learning based technique [13]. Considering that small image groups are more likely to be formed of irrelevant images, we sort the obtained exemplars in a descending order of the sizes of their associated image groups, and take the detected salient objects of the top V exemplars as the visual instances of the concept. Experimentally, we found our algorithm is to some extent robust to the quality of the mined visual instances, and feeding the top 50 images of the text-based image search results to AP is adequate to guarantee the satisfactory performance.

4.2.2 Spatial Intention Estimation

A concept map includes raw information related to the spatial intention, the position and the influence scope, of each concept. Our goal here is to estimate the spatial intention and represent it by a group of spatial distributions, each of which corresponds to a concept. The spatial intention estimation follows two principles: 1) a concept has larger probability to appear near the specified position; 2) a concept is not expected to appear in the position where another concept should appear.
We denote a concept map as {(wk, rk)}k=1..K, with wk and rk being the kth keyword and the associated rectangle respectively, and K the number of concepts in the concept map. The desired spatial distribution Dk of concept k is estimated by

Dk(x, y) =

Gk(x, y) Gk(x, y) = max Gj (x, y)

j=1..K

, (1)

0

otherwise

where Gk(x, y) is a 2D Gaussian distribution, with the mean k = [xk, yk]T and the covariance matrix k = Diag((wk)2, (hk)2). Here, hk, wk and (xk, yk) are height, width and center coordinates of rectangle rk respectively. The shape of the resulting distribution is determined by the height and the width of the rectangle.  is set to a constant (2 log(2))-1 to make the distribution degrade to a
half near the boundary of the rectangle. The smaller the rectangle
is, the more rapidly the distribution degrades to zero from its center,
indicating the concept is expected to appear within a smaller area.

4.3 Layout Sensitive Relevance Evaluation
In this section, we present the approach to calculate the relevance of an image with the visual instance map. Two aspects are taken into consideration during the relevance evaluation: 1) whether each concept occurs in the image; 2) whether the occurrence is spatially consistent to user expectation.

4.3.1 Image Representation
We follow the state-of-the-art image representation technique, and extract a Bag-of-Words (BoW) feature. Rather than building a global BoW model, we build a partially spatial BoW model, which will be helpful to compute the spatial occurrences of the concepts. We divide the image uniformly into n × n cells, and extract visual features for each cell. An image is thus represented as {fc}c=1..n×n, where fc denotes a feature vector extracted from

the cell c. In our implementation, each entry fcj in fc is the number of the associated visual word (e.g., quantized SIFT feature [17] and
color feature) in the corresponding cell. Experimentally we find n = 9 works well.
Accordingly, we extract visual features to represent the visual instance map with a feature map, {(Fk, Dk)}k. Here Fk = {fvk}v=1..V is a set of BoW vectors, with each vector corresponding to a visual
instance of concept k.

4.3.2 Relevance Evaluation
The relevance evaluation consists of two main steps: calculating a relevance score for each concept and combining these relevance scores to an overall relevance score.
Relevance Evaluation Per Concept Since a concept is represented as a few visual instances, we re-
duce the problem of estimating a relevance score for a concept to checking whether there appears a visual instance of the concept at the expected position in the image.
For a visual instance v of concept k, we first compute an occurrence map Ovk = [okv1, · · · , okvc, · · · , okvn2 ]T by calculating the similarity between v and each cell of the image:

okvc = sim(fvk, fc) = j min(fvkj , fcj ).

(2)

This similarity measure is similar to histogram intersection. The
slight difference is that the BoW vectors are not normalized be-
cause we do not aim to check whether the concept appears in a
single cell, but aim to inspect how much part of the concept ap-
pears in the cell by counting how many common visual words they
share. Appearance consistency: Given the occurrence map Ovk, we
evaluate the appearance consistency, i.e., the degree that visual instance v appears in the image, as kv1 = c okvc, which can be viewed as the count of common visual words of visual instance v and the image.
Spatial consistency: In order to check whether the occurrence
of the concept is spatially consistent to user expectation, we compare the spatial distribution of visual instance v in the image, which is approximated by the normalized occurrence map O~vk, with the desired spatial distribution of the concept k, say Dk. The spatial consistency, denoted as kv2, is calculated as:

kv2 = o~kvc = dkc =

c sign(dkc ) min(|dkc |, o~kvc),

(3)

okvc c okvc

,

(4)

1 z1 z2

(Dk (xc , (Dk (xc ,

yc) yc)

- -

) )

Dk(xc, yc)   otherwise

,

(5)

where (xc, yc) is the center coordinates of cell c.  and  con-

trol the degree of penalty to the case that the concept appears at

the position not expected.



=

1 3

maxx,y

Dk (x,

y)

and



=

0.5

in our implementation. z1 and z2 are partition factors to make

c:dkc >0 dkc = 1 and c:dkc <0 dkc = -. There are two parts

contribute to kv2: a positive part by {dkc |dkc > 0}, and a negative

part by {dkc |dkc < 0}. Note that the negative part is introduced

to penalize the inconsistent spatial distribution in a harsher manner

and the degree of penalty is controlled by parameter .

The relevance score for concept k is then calculated by combin-

ing the appearance consistency and spatial consistency of all related

visual instances:

k

=

max v

kv1 kv2 .

(6)

278

Relevance Fusion Finally, a scalar relevance score of the image is obtained by com-
bining the relevance scores of all the concepts,  = {k}k=1..K . A fusion function that naively takes the average will overestimates the images having perfect scores for some concepts but very poor scores for the others. To take account of every concept, the final relevance score is calculated with a fusion function sensitive to both the average and the variance of the input vector:

rel

=

E ( )

-

 K

K

|k - E()|,

(7)

k=1

where

E ( )

=

1 K

k k is the average of the vector.  is a posi-

tive parameter controls the penalization degree for the input vector

with big variance, which is heuristically set to 0.8 in our imple-

mentation. Given two vectors with the same average, it is straight-

forward that the one with smaller variance will get larger score by

Eqn. (7).

4.4 Advanced Functions
In the proposed system, two advanced functions, influence scope selection (ISS) and visual example selection (VES), are provided to assist users to further indicate the search goal.
The user may have a specific spatial intention for a concept, e.g., expecting the concept "lawn" to be filled in a long and narrow region at the bottom of the image. In such a case, the user can indicate the spatial intention in an explicit manner by ISS. Specifically, ISS associates each keyword with a rectangle and enables the user to explicitly control the influence scope of a keyword by stretching the rectangle. The desired spatial distribution for the concept is then estimated taking account of the shape of the rectangle as mentioned in Section 4.2.2.
The user may be interested in a particular appearance of a concept, e.g., expecting only yellow flower for the concept "flower". In such a case, the user can indicate what the concept looks like by VES. Specifically, VES assists the user to express the intention by visual assistor. Candidate visual examples of each concept are listed in a showcase, as shown in Fig. 2, from which the user can select the desired ones for a concept. The visual instances of a concept are then obtained from the selected visual examples.

5. EXPERIMENTS
5.1 Setting
To evaluate the proposed system, we design 42 image search tasks with the requirement on the concept layout. Fig. 4 illustrates some examples of the tasks. Among all the tasks, 22 tasks involve only a single concept, 16 tasks involve two concepts and 4 tasks involve three concepts. There are totally 33 concepts in these tasks, which are categorized as follows: 6 scenes (sky, grass, desert, beach, lawn, fire), 4 landmarks (Pyramid, Sphinx, Great Wall, Colosseum), 5 cartoon characters (hello kitty, garfield, pooh, snoopy, teddy bear) and 18 real world objects (polar bear, tiger, dolphin, seagull, butterfly, panda, bamboo, Christmas tree, tulip, flower, jeep, car, bicycle, house, windmill, keyboard, mouse, us flag). The concepts that do not have stable appearance, such as "rain", are not supported in our system and handling concepts like people's names by introducing face-related features is our future work.
To perform quantitative evaluation, some volunteers are recruited to label the ground truth. Given a task, an image is labeled with a relevance score, according to how well the image accords with the search intention of the task. To differentiate the relevance degrees,

snoopy

house car

jeep grass

sky windmill
tulip

(a)

(b)

(c)

(d)

Figure 4: Examples of the image search tasks in the experiment. Each task is represented as a concept map (desired sizes of the concepts are represented by the rectangles) to describe the desired concept layout.

the relevance score is defined in four levels from level 0 to level 3. Level 3 corresponds to the most relevant (all concepts appear as expected), and level 0 the least relevant (some concepts missed or the layout of concepts is quite different from what is expected). With the ground truth, Normalized Discounted Cumulative Gain (NDCG) [11] is adopted to measure the image search performance.
In our implementation, two kinds of visual features are adopted: color feature and SIFT feature. For color feature, each pixel is represented in the HSV color space and quantized into 192 levels. For SIFT feature, the extracted SIFT features are quantized into visual words with a visual vocabulary of size 6K in the way of [1]. Given a concept, we select which feature to use adaptively to measure the visual similarity, exploiting query classification method in [24]. The image database is dependent on the specific task. Specifically for a given task, the image database is obtained by querying a textbased image search engine using the keywords of the task. All possible combinations of keywords are used as the textual query one at a time, and the top 500 images in the search results are merged to form the database.
5.2 Quantitative Search Performance
The text-based image search system is taken as the baseline in the quantitative comparison. To accomplish a task with a text-based image search system, the keywords of the task are used to query the system. Taking the task of Fig. 4(c) as an example, four queries, "jeep", "grass", "jeep grass" and "grass jeep", can be formulated to get four different search results. An NDCG score is calculated for each search result and the best one is taken as the baseline performance for this task. Free-text query, such as "jeep at center and grass at bottom", is not adopted in the experiment, since the search results are too noisy.
We also involve the "show similar image" search in the quantitative comparison. The basic idea of this method is to pick an example image from the search results of the text-based image search system to rerank the initial search results. To accomplish a given task, we collect the most relevant images of the task from the top 20 images of the search results of the baseline system, and use them one at a time as the example image to perform "show similar image" search. The performance is evaluated by averaging the NDCG scores obtained in all these trials. This setting is to simulate the searching process in a practical case, i.e., from the top 20 images displayed in the screen, the user may select one of the most relevant images she think to conduct "show similar image" search, expecting more relevant images to be returned.
To accomplish a task with the proposed system, we use the concept map of the task as the query, except that we only input the keywords in the specified positions without explicitly indicating the influence scopes of the keywords. The other advanced function, visual example selection, is also disabled.
The performance of each of the three methods is measured by averaging the NDCG scores over all tasks and depicted in Fig. 5(a). As can be anticipated, the text-based image search system works

279

0.8

Baseline

ShowSimilarImage

0.7

proposed system

0.6

0.6

0.55

V=3

V=6

V=9

0.6

V=12

0.55

Initial ISS

NDCG NDCG NDCG

0.5

0.5

0.5

0.45

0.45

0.4

0.3
0.2 @1

0.4

0.35

0.3

@5

@10

@20

@30

@50

@1

(a)

0.4

0.35

0.3

@5

@10

@20

@30

@50

@1

(b)

@5

@10

@20

@30

@50

(c)

Figure 5: (a) Quantitative comparison of the image search performance for the three different methods. (b) Illustration of the influence of the number of visual instances V . (c) Illustration of the effectiveness of the advanced function influence scope selection.

poorly, since it is difficult to handle the tasks with the spatial intention. The "show similar image" method gets a good score for NDCG@1, because the image ranked at the first place is supposed to be the example image itself, which must be relevant otherwise the user would not select it as the example image. However, the performance of "show similar image" method drops rapidly along with growth of the NDCG depth. It is not difficult to explain, since visually similar images are not ensured to have the same semantic concepts. The proposed system shows a satisfactory performance. It outperforms the baseline remarkably in all NDCG depth and is superior to "show similar image" method except of NDCG@1. This means the search intention is well interpreted by the proposed system through the concept map query.
In Fig. 5(b), we show the influence of parameter V , which is the number of visual instances selected for each concept. As can be seen, the performance peaks at the medium values, say V = 6 and V = 9. We explain this as follows. When the number of visual instances is too small, the various appearance of a concept cannot be covered, so that many relevant images in the database are not found. When the number of visual instances is too large, a certain number of noisy visual instances irrelevant to the concept are selected, so that many irrelevant images are mistakenly ranked high. This experiment shows that setting parameter V = 6 is a good tradeoff between the search performance and computation cost. The experiments in this section all adopt V = 6.
5.3 Visual Results
In this subsection, we present some visual results to visually demonstrate the advance of the proposed system. In Fig. 6, we compare the search results by different methods for the 3 tasks related to the concept "snoopy": "snoopy at left", "snoopy at right" and "snoopy at top". The text-based image search system uses the same query for the 3 tasks (free-text query like "snoopy at left" is not considered in the experiment) and produces the identical search results (Fig. 6(a)). Since it has no sense of the desired layout of the concepts in the task, the position of snoopy is quite inconsistent in the search results. The search results of "show similar image" for the task "snoopy at right" is depicted in Fig. 6(b). The image ranked at the first place is the example image used to perform the search. Observed from the search results, this method ranks the images mainly according to the color similarity, which mistakenly interprets the search intention. In Figs. 6(c), 6(d), 6(e), we show the concept maps adopted for accomplishing the 3 tasks and the corresponding search results of the proposed system. Clear spatial correspondence of the concept "snoopy" between the input concept map and the retrieved images can be observed. We also illustrate the mined visual instances for the concept "snoopy" in Fig. 6(f). Note that not all the visual instances perfectly represent the appear-

ance of "snoopy". The third visual instance is the back of snoopy and the fifth is a dog which has nothing to do with snoopy. This justifies that our algorithm can produce satisfactory search results in the presence of imperfect visual instances, which relaxes the requirement of the accuracy of the algorithm for visual instance transformation.
More visual results for the tasks involving multiple concepts are illustrated in Fig. 7. The demonstrated search results of the textbased image search system is the best one selected from the set of the search results, which is obtained using all the possible keyword combinations as query one at a time. The proposed system produces much better results compared with baseline method. For example, for the task of "finding a jeep at center and grass at bottom" (Fig. 4(c)), only one desired image can be found using the text-based search system, while 8 of top 10 images in the search results accord with the search intention using the proposed system.
5.4 Evaluation of Advanced Functions
In this subsection, we evaluate the effectiveness of the advanced functions provided in the proposed system: influence scope selection (ISS) and visual example selection (VES).
We first perform an experiment to evaluate ISS quantitatively using our image search tasks. To accomplish a task, we use the concept map of the task as the query, indicating explicitly the influence scopes of the keywords this time. The other advanced feature, visual example selection, is still disabled. Fig. 5(c) shows the performance of the proposed system with ISS enabled, together with the initial results with both advanced functions disabled. We can see that the performance is improved considerably by enabling ISS. This shows that the proposed system can well interpret the indication of the influence scopes of the keywords.
We demonstrate the effectiveness of VES and ISS with a few visual examples in Fig. 8 and Fig. 9. In Fig. 8, we show two search results for the task of "finding a jeep at the center" together with the selected visual examples for the concept "jeep". It is obvious that the search results are influenced by selecting different visual examples for the concept. The jeeps in front view are retrieved by selecting three front view visual examples, while the jeeps in side view are retrieved by selecting three side view visual examples. In Fig. 9, we show two concept maps with the same keywords but different rectangles, indicating the different spatial distributions of the concept "windmill" are desired. The search results satisfactorily reflect such search intention.
5.5 User Study
A user study with 20 participants is performed to justify our system. The participants taking part in the experiments are college students from nearby universities, and all have the experience of

280

(a)

(b)

snoopy
(c)

snoopy snoopy

(d)

(e)

(f)
Figure 6: Visual results for the image search tasks related to the concept "snoopy". (a) shows the top 10 images in the search results of the text-based image search system using textual query "snoopy". (b) shows the top 10 images returned by "show similar image" for the task "snoopy at right". (c,d,e) are the three concept maps (the rectangle reflecting the influence scope of the keyword is with default size and not displayed) and the corresponding search results of the proposed system. (f) shows the mined visual instances of "snoopy" (in greed boxes). The relevant images are highlighted in red boxes.

(a)

(b)

(c)

(d)

Figure 8: Illustration of the effectiveness of visual example selection. (a) and (c) show two different sets of visual examples selected for the concept "jeep", indicating to find a jeep in side view and a jeep in front view respectively. (b) and (d) are the search results corresponding to (a) and (c) respectively, using the same concept map query.

using image search engines. After a brief introduction of our system, they are asked to try our system for a few minutes and then fill a questionnaire.
To the question "have you ever had any image search intention concerning the concept layout?", 20% of respondents replied with "yes" and 50% of respondents replied with "no, but probably in the future". It shows there are a certain amount of image search intentions concern about the concept layout. Moreover, several respondents gave us the concrete examples that they think the system may help them: "It is useful for me to search a wallpaper that Michael Jordan appears at the right of the picture." "I was searching for an interesting picture that I saw before to share with my friends. I remembered there is a cute bear lying at the bottom of the picture, but failed to find it by the search engines with the textbox interface. It should be much easier to search it using this interface." In summary, the user study justifies that the proposed image search system, which handles specifically the image search

windmill

tulip

(a)

(b)

windmill

tulip

(c)

(d)

Figure 9: Illustration of the effectiveness of influence scope selection. (a) and (c) show two concept maps with the same concepts but different influence scopes for the concept "windmill", indicating to find a big windmill and a small windmill respectively. (b) and (d) are the search results obtained by (a) and (c) respectively.

intentions concerning the concept layout, is meaningful in practical use.

6. CONCLUSION
We have presented a novel image search system to enable users to search images with the particular requirement on the concept layout. A new formulation of image search query called concept map is introduced so that the user can explicitly indicate not only the desired concepts but also the expected layout of the concepts in the desired images. In the implementation, a concept map is first translated into a visual instance map by mining visual instances for each concept from the Web and estimating the spatial intention concerning the concept layout. The relevance of an image is evaluated by comparing the visual instance map with the image. Experiments demonstrate that the proposed system is effective to handle the particular kind of image search tasks.

281

jeep grass

keyboard mouse

sky Colosseum
grass

house car

Figure 7: Visual results for the image search tasks involving multiple concepts. The left column shows the search results of the textbased image search system. The middle column shows the concept maps for the tasks (the rectangles reflecting the influence scopes of the keywords are all with default size and not displayed). The right column shows the search results of the proposed system. The relevant images are highlighted in red boxes.

7. REFERENCES
[1] J. Sivic, B. C. Russell, A. Efros, A. Zisserman, and W. T. Freeman. Discovering object categories in image collections. In ICCV '05, 2005.
[2] M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regression. In ECCV '08, pages 2­15, 2008.
[3] T. Chen, M.-M. Cheng, P. Tan, A. Shamir, and S.-M. Hu. Sketch2photo: internet image montage. In SIGGRAPH Asia '09, pages 1­10, 2009.
[4] J. Cui, F. Wen, and X. Tang. Intentsearch: interactive on-line image search re-ranking. In MM '08, pages 997­998, 2008.
[5] C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for multi-class object layout. In ICCV '09, 2009.
[6] D. Dueck and B. J. Frey. Non-metric affinity propagation for unsupervised image categorization. In ICCV '07, 2007.
[7] M. J. Egenhofer. Spatial-query-by-sketch. In IEEE Symposium on Visual Languages, pages 60­67, 1996.
[8] Flickr. http://www.flickr.com/. [9] B. J. Frey and D. Dueck. Clustering by passing messages
between data points. Science, 315(5814):972, 2007. [10] Google Image Search. http://images.google.com/. [11] K. Jarvelin and J. Kekalainen. IR evaluation methods for
retrieving highly relevant documents. In SIGIR '00, pages 41­48, 2000. [12] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain. Content-based multimedia information retrieval: State of the art and challenges. TOMCCAP, 2(1):1­19, 2006. [13] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum. Learning to detect a salient object. IEEE Trans. Pattern Anal. Mach. Intell., to appear. [14] X. Liu, B. Cheng, S. Yan, J. Tang, T. S. Chua, and H. Jin.

Label to region by bi-layer sparsity priors. In MM '09, pages 115­124, 2009. [15] Y. Luo, W. Liu, J. Liu, and X. Tang. Mqsearch: image search by multi-class query. In CHI '08, pages 49­52, 2008. [16] Microsoft Bing Image Search. http://images.bing.com/. [17] K. Mikolajczyk and C. Schmid. Scale & affine invariant interest point detectors. IJCV, 60(1):63­86, 2004. [18] G. P. Nguyen and M. Worring. Optimization of interactive visual-similarity-based search. TOMCCAP, 4(1), 2008. [19] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. LabelMe: a database and web-based tool for image annotation. IJCV, 77(1):157­173, 2008. [20] J. R. Smith and S.-F. Chang. Visualseek: A fully automated content-based image query system. In MM '96, pages 87­98, 1996. [21] J. Wang, X. Hua, and Y. Zhao. Color-Structured Image Search. MSR-TR-2009-82. July 7, 2009. [22] H. Xu, J. Wang, X. Hua, and S. Li. Interactive Image Search by 2D Semantic Map. In WWW '10, 2010. [23] R. Yan, A. Natsev, and M. Campbell. Multi-query interactive image and video retrieval -: theory and practice. In CIVR '08, pages 475­484, 2008. [24] R. Yan, J. Yang, and A. G. Hauptmann. Learning query-class dependent weights in automatic video retrieval. In MM '04, pages 548­555, 2004. [25] E. Zavesky and S.-F. Chang. Cuzero: embracing the frontier of interactive visual search for informed users. In MIR '08, pages 237­244, 2008. [26] Z.-J. Zha, L. Yang, T. Mei, M. Wang, and Z. Wang. Visual query suggestion. In MM '09, pages 15­24, 2009.

282

Generalized Syntactic and Semantic Models of Query Reformulation

Amaç Herdag delen
University of Trento Rovereto, Italy
amac@herdagdelen.com

Massimiliano Ciaramita
Google Zürich, Switzerland
massi@google.com

Daniel Mahler
Google Zürich, Switzerland
mahler@google.com

Maria Holmqvist
Linkopings University Linkopings, Sweden
marho@ida.liu.se

Keith Hall
Google Zürich, Switzerland
kbhall@google.com

Stefan Riezler
Google Zürich, Switzerland
riezler@google.com

Enrique Alfonseca
Google Zürich, Switzerland
ealfonseca@google.com

ABSTRACT
We present a novel approach to query reformulation which combines syntactic and semantic information by means of generalized Levenshtein distance algorithms where the substitution operation costs are based on probabilistic term rewrite functions. We investigate unsupervised, compact and efficient models, and provide empirical evidence of their effectiveness. We further explore a generative model of query reformulation and supervised combination methods providing improved performance at variable computational costs.
Among other desirable properties, our similarity measures incorporate information-theoretic interpretations of taxonomic relations such as specification and generalization.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Query formulation, Search process, Retrieval models.
General Terms
Algorithms, Experimentation.
Keywords
Query reformulation, query rewriting, generalized edit distance, similarity metrics.
1. INTRODUCTION
Query reformulation is the process of iteratively modifying a query to improve the quality of a search engine results, in order to satisfy one's information need. Search engines
Work carried out during internships at Google.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

support users in this task explicitly; e.g., by suggesting related queries or query completions, and implicitly; e.g., by expanding the query to improve quality and recall of organic and sponsored results. The close interaction between users and algorithms makes this a central topic in search technology and research [10, Ch. 6].
Successful refinements are closely related to the original query [22]. This is not surprising as reformulations involve spelling corrections, morphological variants, and tend to reuse parts of the previous query. More precisely, reformulations are close to the previous query both syntactically, as sequences of characters or terms,1 and semantically, often involving transparent taxonomic relations. As an example, for the query "becoming a dentist", the reformulation "becoming an oral surgeon" might have a higher chance of producing relevant results than "becoming a doctor". In this paper we address the following question: how can we model query reformulation as a process involving syntactic and semantic operations within a unified and principled framework?
String distance metrics model the similarity between two queries as a function of the edit operations (insertion, deletion, substitution) that are necessary to generate one string from the other. Jones et al. [14] noticed that edit distance can be an accurate query similarity measure as it approximates well the users' conservative disposition in query refinement. Semantic approaches are based on the linguistic notion that similar words (queries) occur in similar contexts; an intuition that can be captured by statistical association measures extracted from simple document counts [5], or involving deeper analyses; e.g., of search results snippets [24]. Here we investigate a class of models for query reformulation which combines the syntactic and semantic aspects. We call these models generalized in the sense that they aim at capturing both syntactic and semantic properties of a reformulation. These models build upon the generalized edit distance framework. In our formulation, the cost of an edit operation, rather than being fixed, is weighted by probabilistic interpretations of the semantic relation between two terms. Our approach, while conceptually simple, unsupervised, and efficient, outperforms several competitors and baselines. We
1Through the paper with the term syntactic we refer purely to the surface properties of queries as sequences of symbols, without any reference to their constituent structure.

283

provide empirical evidence from extensive evaluations on two datasets in Section 6.
Pushing the framework further, we investigate a generative model previously applied to biological sequence alignment problems [20]. We show that in this direction improved performance can be expected, although at increased computational cost and additional complexities in parameter estimation, leaving room for further research. While most of the focus is on single unsupervised signals for query reformulation, we show that our measures provide mutually complementary information: weighted combinations further improve performance.
The paper also touches upon a related topic. Recently, Boldi et al. [4] proposed the idea of capturing explicitly the relation between two queries with respect to a taxonomic representation (e.g., specification, generalization, etc.) to improve query reformulations. With respect to this issue, we show how asymmetric and symmetric probabilistic similarity measures, and their combinations, can be loosely interpreted as information-theoretic approximations of categorical notions such as "generalization" or "specification".
2. RELATED WORK
Query reformulation is an important topic in web search as a large fraction of the queries issued to search engines are modified after examination of the results [12]. Query modification is supported in several ways to improve search experience; e.g., via automatic spelling correction [6]. Query reformulation also requires editing or expanding the query. Several techniques have been proposed based on relevance feedback, query log analysis and distributional similarity [2, 18, 23, 24, 29, 30]. A related task is that of session segmentation where properties of query transitions can be used to identify session boundaries [9, 13].
As relevance and pseudo relevance feedback impose additional cognitive load on the user, and can lead to query drift or costly computations, Jones et al. [14] proposed to pre-compute reformulations by ranking candidate queries extracted from query logs, using several types of features and learning methods. Interestingly, they notice how simple linear combinations of a few edit distance features provide powerful ranking functions, comparable to more complex methods. Previously, Wen et al. [27] clustered queries combining several sources of information such as coclick and IR document similarity, including string distance. They also suggest using a smaller fixed cost for pairs of terms occurring in Wordnet in the edit distance computation, but did not carry out a systematic evaluation. Generalizations of string matching metrics are the focus of our study. Generalized Levenshtein distance algorithms [17] have been intensively investigated in bioinformatics for solving sequence alignment problems. Oommen and Kashyap proposed a model which generates the probability of a string being rewritten into another accounting for all possible combinations of edit operations [20] that has been used successfully in peptide classification [3] and optical character recognition [15].
Previous studies have teased apart the semantic aspects of query reformulations. Rieh and Xie [22] (see also [11, 16]) analyzed query transitions in terms of syntactic and semantic operations and found that when reformulating previous queries users adopt several tactics including generalization, replacement with synonyms, parallel movement (approximately 50% of the time) and specification (approximately

30% of the time). Boldi et al. [4] proposed a query reformulation approach based on classifying reformulation types (QRTs) as belonging to a small taxonomy. They represent query transitions in a feature space including properties extracted from sessions and similarity features including edit distance, Jaccard and term vector cosine. Hence, they build a supervised QRT decision tree classifier which achieves 92% accuracy in a four-class task (specialization, generalization, correction, parallel move). Their methodology includes an unspecified feature selection process, thus we don't know the contribution of each feature. However, the high accuracy suggests that a few features, at least partially based on simple string matching metrics, can go a long way in capturing taxonomic aspects of query reformulations (see also Huang and Efthimiadis [11] for a related unsupervised approach). In evaluation they find that recommendations limited to specializations provide the best accuracy while introducing other types of QRTs decreases the quality of the recommendations.

3. PRELIMINARIES
Let (qs, qt) be an ordered pair where qt is a candidate reformulation of a query qs. We call qs the source and qt the target. A similarity measure between two queries is a function f : (qs, qt)  IR which takes (qs, qt) as input and returns a score. In particular, we are interested in functions which correlate well with human judgments of how good a reformulation qt is for qs.
3.1 Semantic similarity
For several of the similarity measures described below, we employ pointwise mutual information (PMI) as a measure of the association between two terms or queries. PMI has been applied extensively to model semantic similarity ­ e.g., Turney [26] uses it to discover synonyms on web data ­ and correlates well with human judgments [21]. Let x and y be two strings that we want to measure the amount of association between. Let p(x) and p(y) be the probability of observing x and y in a given model; e.g., relative frequencies estimated from occurrence counts in a corpus. We also define p(x, y) as the joint probability of x and y; i.e., the probability of the two strings occurring together. An abstract definition of PMI for our purposes is as follows:

,, PMI(x, y) = log

p(x, y)

« .

(1)

p(x)p(y)

PMI can yield negative values, if p(x, y) < p(x)p(y). For the purposes of normalization, below in this section, we discard negative PMI values and assign zero to such cases. PMI is also used as a basis for the substitution score of two terms (see Section 5.2). Limiting PMI to positive values is further motivated by the assumption that substituting two terms occurring together less frequently than random should not be penalized more than two unrelated terms.2
As pointed out in [16, 22] query transitions tend to correlate with taxonomic relations such as generalization and specialization. Boldi et al. [4] show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than

2As a matter of fact, such occurrences are extremely rare in our data and within noise levels.

284

building a dedicated supervised classifier for this task we try to capture it directly at the source. We propose that by manipulating PMI we can directly model taxonomic relations to some extent. In the following definitions we interpret (x, y) as a transition from x (i.e. source) to y (i.e. target) to break the symmetry without loss of generalization.

3.1.1 Joint normalization
The first type of normalization, called joint normalization, uses the negative log joint probability and is defined as:

PMI(J)(x, y) = PMI(x, y)/ - log(p(x, y)).

(2)

As we limit PMI to positive values the normalization bounds the range between 0 and 1. The jointly normalized PMI(J) is a symmetric measure between x and y in the sense that PMI(J)(x, y) = PMI(J)(y, x). Intuitively it is a measure of the amount of shared information between the two strings relative to the sum of individual strings information.

3.1.2 Specialization normalization
To capture asymmetries in the relation between two strings, we apply two non-symmetric normalizations also bounding the measure between 0 and 1. The first asymmetric normalization is called specialization and is defined as:

PMI(S)(x, y) = PMI(x, y)/ - log(p(x)).

(3)

The reason we call it specialization is that PMI(S) favors pairs where the second one is a specialization of the first one. For instance, PMI(S) is at its maximum when p(x, y) = p(y) and that means the conditional probability p(x|y) is 1 which is an indication of a specialization relation.

3.1.3 Generalization normalization
The second asymmetric normalization is called generalization and is defined in the reverse direction as:

PMI(G)(x, y) = PMI(x, y)/ - log(p(y)).

(4)

PMI(G) is at maximum when p(y|x) is 1. The three normalizations provide a richer representation
of the association between two strings and approximate the generalization-specialization dimension from an informationtheoretic perspective. As an example, for the query transition "apple" to "mac os" PMI(G)=0.2917 and PMI(S)=0.3686; i.e., there is more evidence for a specialization. Conversely for the query transition "ferrari models" to "ferrari" the measures yield PMI(G)=1 and PMI(S)=0.5558; i.e., the target is a "perfect" generalization of the source3.

3.2 Syntactic similarity

Let V be a finite vocabulary and  be the null symbol. An

edit operation: insertion, deletion or substitution, is a pair

(a, b)  {V {}×V {}}-{(, )}. An alignment between

two sequences x and y is a sequence of edit operations  =

(a1, b1), ..., (an, bn). Given a non-negative cost function c the

cost

of

an

alignment

is

c() =

Pn
i=1

c(i

).

The

Levenshtein

distance, or edit distance, defined over V , dV (x, y) between

x and y is the cost of the least expensive sequence of edit

operations which transforms x into y [17]. The distance

computation can be performed via dynamic programming in

time O(|x||y|). Edit distance captures the amount of overlap

between the queries as sequences of symbols and have been

previously used in information retrieval [4, 14, 28].

3The values are computed from Web counts.

4. QUERY-LEVEL MEASURES

We calculate the PMI for a pair (qs, qt) using the num-

ber of documents retrieved by a search engine for qs, qt and

qs,t where qs,t is a shorthand for the concatenation of qs

and qt. Formally, let Ns and Nt be the number of docu-

ments retrieved for qs and qt respectively. Similarly, let Ns,t

be the number of documents retrieved for the concatenated

joint query. We define the probability of, respectively, the

two

queries

and

the

joint

query

as

p(qs)

=

Ns N

,

p(qt)

=

Nt N

,

and p(qs, qt)

=

Ns,t N

where N

is a constant large enough

to approximate the total number of documents that can be

retrieved. In our implementation we use Google's search en-

gine. The number of results returned determine Ns, Nt, and

Ns,t. We denote this PMI measure between two queries by

PMIW eb(qs, qt). We renormalize the PMI values as described

above thus generating three query-level similarity measures.

5. TERM-LEVEL MEASURES
5.1 Syntactic measures
We use two Levenshtein distance models as basic syntactic measures. The first, called Edit1 (E1), employs a unit cost function for each of the three operations. That is, given a finite vocabulary T of all terms occurring in queries:
a, b  T, cE1(a, b) = 1 if a = b, 0 otherwise. (5)
The second, called Edit2 (E2), uses unit costs for insertion and deletion, but computes the character-based edit distance between two terms to determine the substitution cost. If two terms are similar at the character level, the cost of substitution is lower. Given the vocabulary T of terms and a finite vocabulary A of characters the cost function is defined as:
a, b  T, cE2(a, b) = dA(a, b) if a  b = , 1 otherwise (6)
where 0  dA(a, b)  1, normalizing by max(|a|, |b|). We also investigate a variant in which the input sequences
are alphabetically sorted before the edit distance computation. The motivation is the observation that queries may be often formulated as sets of terms in which the order of the terms is irrelevant. Thus, "Brooklyn pizza" and "pizza Brooklyn" may denote same user intent but the edit distance is unable to capture the similarity. By presorting the terms in the queries we compute an order-free version of edit distance. We prefix the names of these models with "Sorted".
5.2 Generalized edit distance
Extending the Levenshtein distance framework to take into account semantic similarities between terms is conceptually simple. As in the Edit2 model above we use a modified cost function. For our purposes, the cost function should have the following properties: whenever there is evidence of semantic association between two terms, it should be "cheaper" to substitute these terms instead of deleting one and inserting the other. For an unrelated term pair, a combination of insertion and deletion should always be less costly then a substitution. We also assume that the cost of the substitution of a term with itself (i.e. identity substitution) is always 0. Considering these requirements, we define the cost function as a cost matrix S based on the normalized PMI measures defined above. Given a normalized similarity measure f , an entry in a cost matrix S for a term pair

285

(wi, wj) is defined as:

s(wi, wj ) = 2 - 2f (wi, wj ) +

(7)

The correction, coupled with unit insertion and deletion costs, guarantees that these requirements are fulfilled. We call these models GenEdit (GE). Given a finite term vocabulary T and cost matrix S the cost function is defined as:

a, b  T, cGE(a, b) = s(a, b) if a  b = , 1 otherwise. (8)

5.2.1 Cost matrix estimation

To estimate an appropriate cost matrix we used session logs consisting of actual transitions of consecutive queries. The data consists of approximately 1.3 billion English queries generated from the U.S. A session is identified as a sequence of queries from the same user. Let qs and qt be a query pair observed in the session data where qt is issued immediately after qs in the same session. Let qs = qs \ qt and qt = qt \ qs, where \ is the set difference operator. The co-occurrence count of two terms wi and wj from a query pair qs, qt is denoted by ni,j(qs, qt) and is defined as:

8

1

<

ni,j (qs, qt) = 1/(|qs| |qt|)

:

0

if wi = wj  wi  qs  wj  qt if wi  qs  wj  qt otherwise.

(9)

If a term occurs in both queries, it has a co-occurrence

count of 1. For all other pairs we make sure the sum of co-

occurrence counts for a term wi  qs is 1 for a given query pair. The normalization is an attempt to avoid the under-

representation of terms occurring in both queries. The fi-

nal co-occurrence count of two arbitrary terms wi and wj

is denoted by Ni,j and it is defined as the sum over all

query

pairs

in

the

session

logs,

Ni,j

=

P
qs ,qt

ni,j (qs,

qt).

Let

N

=

P
i,j

Ni,j

be

the

sum

of

co-occurrence

counts

over

all term pairs. Then we define a joint probability for a

term

pair

as

p(x, y)

=

. Ni,j
N

Similarly,

we

define

the

single-

occurrence counts and probabilities of the terms by com-

puting the marginalized sums over all term pairs. Namely,

the probability of a term wi occurring in the source query is

p(i, ·)

=

P
j

Ni,j /N

and

similarly

the

probability

of

a

term

wj

occurring

in

the

target

query

is

p(·, j)

=

P
i

Ni,j

/N

.

Plugging these values in Eq. (1), we obtain the PMI(i, j)

for term pair wi and wj, which are further normalized as described in Section 3.1. Any term pair that is not co-

occurring in the session data is considered to be unrelated

and is assigned a PMI value of zero.

5.3 A generative model
The edit distance measures considered so far generate a score based on the least-costly alignment of two queries. This can be viewed as finding the shortest path in a query space constructed on the atomic edit operations. A natural extension would calculate the probability of producing the target from the source not only considering the least-costly alignment of two queries but computing over all possible ways the target can be obtained from the source. Oommen and Kashyap [20], proposed a syntactic transition probability model (referred to as the OK model) which shows how the probability of a string, in our case a query, rewrite can be computed with a generative model consisting of random insertion, deletion, and substitution operations. The model has been successfully applied to problems such as peptide classification and OCR correction [3, 15].

Let x = (x1x2 . . . xn) and y = (y1y2 . . . ym) be the source and target strings respectively such that x  V and y  V where V is the finite alphabet of symbols. We introduce two additional symbols  and , which are not in V , as input and output null symbols, respectively. The OK model computes the probability of obtaining y from x under a generative model which takes as input two probability distributions G and S, and works in the following steps.

1. Distribution G specifies the number of insertions to be applied to the source. In each independent string generation process z terms are inserted with probability G(z). The intermediate output at the end of this step is x = x1x2 . . . xn+z where x is the string x modified by inserting z  symbols at random positions in x.

2. The distribution S is over {T  {} × T  {}}. The value S(yj|xi) for two symbols xi and yj is the probability that xi is substituted by yj. The output of this step is y which is obtained by substituting all symbols in x according to the probabilities specified by S.

3. Last step mirrors the original deletion operation: all  characters remaining in y are deleted. The remaining string is y, the output of the process.

There are two constraints on S. The first states that

xi

P
yj

T

{}

S(yj

|xi)

=

1.

This

ensures

that

each

symbol

in x is either substituted, left intact or deleted. The second

constraint S(|) = 0 guarantees that exactly z insertions

are made and no input null symbol inserted at the first step

is deleted in the second step (i.e.  is always substituted by

a symbol in T not by ).

Using the OK model, one can compute the probability of

a transition from source to target query by integrating the

individual probabilities of all possible paths allowed by the

generative model. The explicit form of this probability is:

m
X

G(z)n!z!

X

X

n+z
Y

p(y|x) =

(n + z)!

S(yk, xk) (10)

z=max(0,m-n)

x y k=1

where m = |x| and n = |y|. The outermost summation is

over all possible values of number of insertions. The factor

n!z! (n+z)!

is

the

number

of

different

ways

in

which

z



char-

acters can be inserted in x to obtain x . Although the ex-

plicit calculation of this probability is too expensive, due to

the combinatorial element, Oommen and Kashyap provide a

dynamic program which runs in approximately cubic time,

O(mn min(m, n)) [20].

As in the generalized edit distance models, we represent

the queries as strings and the terms as characters. That is,

qs = w1w2 · · · wn is the source query and qt = w1w2 · · · wm

is the target query where wi,wj  T .

5.3.1 Parameter estimation
In order to actually employ the OK model we need to estimate distributions G and S. The model accepts arbitrary probability distributions and estimating meaningful parameters is not trivial. We devise an estimation strategy similar to that used by Kolak and Resnik [15] who apply the OK model to optical character recognition (OCR). The idea is to generate an alignment with a simpler model and then estimate all substitutions in S directly from a large-enough dataset. As a corpus of pairs to align we used the session

286

data of Section 5.2.1, under the assumption that contigu-

ous query pairs represent reasonable candidates of naturally

occurring query transitions. Subsequently, we ran a general-

ized edit-distance model to find the least-costly alignment of

each pair and then count the edit operations that make up

this alignment. The application of the edit-distance model

thus provides a way to reverse engineer the query transi-

tions and obtain estimates for the term insertion, deletion

and substitution probability distributions.

Summarizing, we run a generalized edit distance model on

the query pairs in our session dataset4, and count the num-

ber of times each term insertion, deletion, and substitution

occurred directly off the alignments. By integrating these

counts over all pairs and normalizing them into probability

distributions, we obtain the necessary estimates for G and

S. The inspection of the outcome of this procedure revealed

that the deletion probability (i.e. S(|xi)) is largely over-

estimated (e.g. over 0.3 for some terms). This is possibly

due to the noisy alignment procedure. To solve this problem

we introduce one adjustable parameter called damping fac-

tor, denoted by DF. For each term, the deletion probability

is corrected as S(|t)  S(|t)/DF , then S is re-normalized

so

that

xi

P
yj T {}

S(yj |xi)

=

1.

6. EXPERIMENTS
We evaluate all models discussed so far on two datasets. As an external benchmark we use the unsupervised distributional similarity system (DistSim) of Alfonseca et al. [1]. DistSim implements an extension of the vector-space model of distributional similarities to query-to-query similarity, by combining different vectors using the geometric mean of the frequencies for each of the features separately. Features are n-grams collected in the context of each query in hundreds of millions of documents. The score of a query reformulation is the cosine of the vectors representing each query. DistSim generates richly lexicalized high-dimensional models which in evaluation [1] outperformed web kernel methods [24].

6.1 Experimental setup
The evaluation involves a query reformulation task in which several source queries are provided, each with a set of candidate reformulations scored by raters. Each model predicts a real-valued score for each source-target reformulation pair. The score represents the quality of the reformulation according to the system. While the absolute value of the score might not be meaningful in itself it is used to rank the queries in the set of possible targets for the same source. Several evaluation metrics are used to quantify the performance of a system: Spearman rank correlation, precision at N and mean average precision. We use Spearman correlation as our primal evaluation measure as it is independent of the choice of a threshold which is necessary for precision.
6.2 Combined models
We evaluate all the similarity measures individually as well as in combination. We experiment with one unsupervised combination method, a baseline which simply averages all signals5. We also evaluate a supervised combination, and a supervised optimization of the OK model. The OK
4A sorted joint-normalized generalized edit distance model. 5All non-normalized individual signals are re-normalized before combination.

Number of query pairs Number of source queries Average log-probability qs Average log-probability qt Average Number of terms qs Average Number of terms qt

QS1500 1486 57
-10,61 -9.33 3,40 2,83

CC2000 1995 500
-10,57 -10,27
2,08 2,24

Table 1. Statistics of the evaluation datasets.

model involves an adjustable parameter DF that should be picked empirically. Hence, we optimize the OK model separately by a supervised leave-one-out procedure. All values for DF = 10i were evaluated at i = 1, 2, .., 10. As a full supervised combination we used a neural network regression model using all of the features introduced in the paper, excluding OK. This type of approach lets us exploit potential non-linearities in the signals. For each network model three parameters are optimized: learning rate, number of hidden units and number of iterations (epochs) over the training data. Predictions are generated in a leave-one-out scheme where in turn a source query qs is excluded for prediction.
6.3 Evaluation data
The first evaluation set, QS1500 is based on the gold standard from [1]. It contains 57 source queries, each paired with up to 20 target queries. The candidate reformulations are generated from the top-20 ranked suggestions using several different systems, based on the web kernel approach [24], and distributional similarity. Two raters evaluated each pair, using the 5-Likert scale defined in [24]. The weighted Cohen's Kappa was 0.7111 on a binary split at level 1.5, indicating substantial agreement amongst the raters for a binary decision. In the computation of the precision at N scores we use the pairs with a score of 1.5 or more [1] as positive pairs.
The second evaluation set, called CC2000, was built from scratch based on the hypothesis that two different queries are related if they lead to user clicks on the same documents. This approach is similar to the method proposed by Fitzpatrick and Dent [8]. Our technique adds click information, thus strengthening the preference for precision over recall in the extraction of related queries. For a randomly extracted set of 500 source queries, we randomly sampled 4 targets. 3 out of 4 targets are queries that have been co-clicked with at least 10 different results. The remaining one has been co-clicked only once. The latter pair acts as a control on the quality of click as a measure of relatedness. The 2,000 pairs were judged by 5 raters, with access to the search result, in blind evaluation according to a 4-point scale: Unrelated(1), Slightly Related(2), Very Related(3), Same Meaning(4). Inter-rater agreement of 5 raters on a binary classification task (class 1 = Unrelated or Slightly Related, class 2 = Very Related or Same Meaning) gave a Kappa value of 0.65. A connection between the co-click hypothesis and human ratings can be seen from computing average human scores for the automatically created distinction. This results in an average human score of 3.1 for pairs with more than 10 co-clicks, and an average human score of 2.3 for pairs with 1 co-click. This shows that the co-click hypothesis yields positively related pairs that are judged on average as Very Related by human raters, while the control set are judged only as Slightly Related. The gold standard for each pair is the average of the 5 ratings. Choosing a fixed threshold

287

QS1500

# Similarity Function Spearman mAP Prec@1 Prec@3 Prec@5 Sig.

1 NN

0.500 0.806 0.836 0.741 0.637 7

2 Oommen-Kashyap

0.470 0.747 0.782 0.698 0.637 10

3 DistSim

0.438 0.744 0.768 0.679 0.628 12

4 Mean all

0.435 0.772 0.818 0.691 0.633 13

5 SortedGenEdit(S)

0.429 0.774 0.845 0.709 0.638 13

6 SortedGenEdit(G)

0.428 0.775 0.828 0.712 0.648 13

7 PMIW eb(S) 8 PMIW eb(J) 9 SortedGenEdit(J)

0.417 0.713 0.409 0.730 0.408 0.771

0.764 0.782 0.832

0.630 0.679 0.701

0.589 13 0.594 13 0.639 13

10 GenEdit(S)

0.382 0.743 0.796 0.695 0.619 15

11 GenEdit(G)

0.380 0.745 0.795 0.698 0.625 15

12 GenEdit(J)

0.365 0.737 0.790 0.692 0.609 15

13 SortedEdit2

0.320 0.714 0.757 0.668 0.630 18

14 SortedEdit1

0.314 0.697 0.763 0.660 0.595 18

15 PMIW eb(G) 16 Edit2

0.283 0.670 0.270 0.649

0.627 0.715

0.547 0.618

0.570 18 0.571 18

17 Edit1

0.252 0.633 0.683 0.615 0.550 18

18 Length-target(Char)

0.139 0.519 0.435 0.456 0.437 20

19 Length-target(Term)

0.112 0.506 0.453 0.423 0.413 20

20 log-prob target

-0.161 0.452 0.309 0.309 0.341

-

Table 2. The grand table for QS1500. The column Sig. gives the index of the model with the highest Spearman correlation that the corresponding model is significantly higher than with p < 0.05. Length and log probability of target are additional baselines.

Similarity Measure Oommen-Kashyap SortedGenEdit(S) SortedGenEdit(G) SortedGenEdit(J) GenEdit(S) GenEdit(G) GenEdit(J) SortedEdit2 SortedEdit1 Edit2 Edit1

QS1500 0.470* 0.429* 0.428*
0.408 0.382 0.380 0.365 0.320 0.314 0.270 0.252

CC2000 0.391* (6) 0.407* (4) 0.419* (2) 0.391* (7) 0.414* (3) 0.424* (1) 0.402* (5) 0.288 (11)
0.298 (9) 0.292 (10)
0.299 (8)

Table 3. Generalized edit distances for QS1500 and CC2000. The ranks of the features for CC2000 are given in parentheses; * indicates a higher Spearman correlation than the highest performing edit distance baseline (SortedEdit2 for QS1500 and Edit1 for CC2000) at a significance level of 0.95.

for the precision scores is not straightforward; e.g., using a threshold at three produces 132 all positive sets and 90 all negative sets, therefore we would not be able to compute a meaningful precision score for too many sets. To avoid this problem we choose in each set the positive pair as the one with the highest score. In this way we obtain 774 positive pairs and 1221 negative pairs. Thus in terms of precision we evaluate the performance of systems at identifying the best available pair. The following table summarizes some datasets statistics: Table 1 summarizes the basic properties of the datasets.
7. RESULTS AND DISCUSSION
In the following sections, we discuss the performance of generalized edit distance with respect to baselines (simple edit distance and distributional similarity models), comment on the effect of taxonomic normalization of PMI, and report the performance of combining different measures in supervised and unsupervised settings. In order to give a birds-eye overview, the results for all models are given in Tables 2 and 4 for QS1500 and CC2000, respectively. In these tables, we report Spearman correlation, mean average precision and precision at various positions. Since, there are only 4 target

queries per source in CC2000, we report precision values at 1, 2 and 3 for that dataset. For QS1500, the precision values at 1, 3 and 5 are reported.
7.1 Generalized Edit Distance
The Spearman rank correlations obtained for all edit distance models are given in Table 3 for QS1500 and CC2000. Several points are worth discussing in these results.
Generalized edit distance is better than simple edit distance. For both datasets, the generalized edit distance models (all variants of GenEdit and SortedGenEdit) outperform the simple edit-distance based features (Edit1 and Edit2). This observation is also supported by the significance results obtained by one-tailed t-tests reported in the same table. This result proves that our method is a powerful, yet simple, generalization of an already robust query similarity measure. To the best of our knowledge ours is the first (successful) application of such generalized algorithms to IR.
Sorting has an effect on results. For QS1500, sorted editdistance based features (SortedGenEdit) outperformed their unsorted counterparts (GenEdit) by margins of more than 4 percent (though, we were unable to confirm a significant difference between them). The pattern is different in CC2000 where the unsorted features outperform their sorted counterparts albeit with smaller margins. We hypothesize that this effect might be related to the query length, greater in QS1500 both for qs and qt. It is possible that as longer queries are more subject to permutations, sorted distance measures emerge as more robust.
Generative models are promising. Especially for QS1500, we see that the increased complexity of the OK model pays off in terms of performance. Since the OK model uses the substitution probability matrices computed by the alignments obtained by SortedGenEdit(J) model, the difference between the SortedGenEdit(J) and the OK model becomes even more encouraging. Although we picked the damping factor value for each set by supervision (i.e., leave-one-out), we should note that the OK model is robust with respect to varying values of DF. In the optimization experiments we observed that the Spearman scores of the OK models remain

288

CC2000

# Similarity Function Spearman mAP Prec@1 Prec@2 Prec@3 Sig.

1 NN

0.432 0.739 0.583 0.511 0.451 10

2 GenEdit(G)

0.424 0.716 0.545 0.485 0.447 11

3 SortedGenEdit(G)

0.419 0.712 0.550 0.476 0.446 11

4 GenEdit(S)

0.414 0.714 0.543 0.488 0.447 11

5 SortedGenEdit(S)

0.407 0.710 0.546 0.477 0.445 11

6 GenEdit(J)

0.402 0.714 0.541 0.482 0.448 11

7 Oommen-Kashyap

0.391 0.704 0.515 0.484 0.451 11

8 SortedGenEdit(J)

0.391 0.706 0.538 0.474 0.445 11

9 Mean all

0.386 0.711 0.531 0.485 0.448 12

10 PMIW eb(G) 11 PMIW eb(J) 12 DistSim

0.369 0.698 0.330 0.692 0.322 0.707

0.506 0.485 0.532

0.473 0.474 0.492

0.449 13 0.444 17 0.427 17

13 Edit1

0.299 0.656 0.441 0.438 0.418 17

14 SortedEdit1

0.298 0.662 0.456 0.444 0.420 17

15 Edit2

0.292 0.676 0.478 0.458 0.432 17

16 SortedEdit2

0.288 0.685 0.487 0.461 0.432 17

17 PMIW eb(S) 18 log-prob target

0.264 0.681 0.114 0.626

0.477 0.384

0.461 0.416

0.437 17 0.404 19

19 Length-target(Char)

-0.036 0.603 0.362 0.390 0.392

-

20 Length-target(Term)

-0.077 0.603 0.358 0.387 0.388

-

Table 4. The grand table for CC2000. The column Sig. gives the index of the model with the highest Spearman correlation that the corresponding model is significantly higher than with p < 0.05. Length and log probability of target are additional baselines.

at levels either comparable to or superior than other generalized models' in a range of almost 7 orders of magnitude for DF. However, the simpler generalized models are even more robust. Better formulations of the generative model provide an interesting direction for future research.
7.2 Taxonomic normalization
Type of normalization is important. Especially for the query-level similarity measures (variants of PMIW eb), the type of normalization has a significant effect on performance. E.g., as it can be seen in Table 2, PMIW eb(G) performs badly in QS1500 (Spearman 0.283, rank 15), but PMIW eb(S) is more competitive (Spearman 0.417, rank 7) even though both measures are based on the same PMI values and only differ by the type of normalization. Interestingly, a similar but reversed pattern is observed for CC2000 in Table 4. For this dataset, PMIW eb(G) is the best measure among the query-level measures with a Spearman correlation of 0.369 and overall rank of 10. PMIW eb(S), on the other hand, can achieve a correlation of 0.264 and is placed quite low in the overall ranking. The difference between PMIW eb(S) and PMIW eb(G) is significant (p < 0.05) for both datasets in opposite directions. This evidence alone suggests that different normalizations can capture different properties of different datasets. A similar pattern also emerges from the generalized edit distance models, specialization works best for QS1500 and generalization for CC2000.
One intriguing explanation for this pattern involves the dominant directionality in the datasets. We know that in QS1500 target queries are shorter than source queries on average. Hence, transitions are more likely in the generalization direction. It is possible that a feature which favors generalizations loses its discriminative power and do not correlate well with human judgments because of the bias in the dataset. A similar effect in the reversed direction is compatible with results on CC2000. This explanation seems supported also by the performance of length and log probability baselines, see Tables 2 and 4, which are characterized by opposite signs.

7.3 Combined models
Supervision works. In Tables 2 and 4, we see that the neural network model constructed on all features (NN) outperformed all other methods in both datasets. This was rather expected but it is important to see that there is room for further improvements, and that the features we propose provide complementary information. Experiments with linear regression were less successful which suggests there might be useful non-linear interactions between features that can be captured by the neural network. It is also interesting to notice that supervised combination improves but not by a large margin indicating that our single generalized features have good discriminative power in absolute terms. The comparison with the high-dimensional distributional similarity model (DistSim) is also positive, DistSim performs only marginally better than the GenEdit models on one dataset (QS1500) ­ although, in terms of precision, GenEdit models are still better ­ and worse on CC2000. Naive unsupervised combination yields mixed results.
8. CONCLUSION
In this paper we proposed an approach to query reformulation aiming at the combination of string similarity and corpus-based semantic association measures. Generalized Levenshtein distance algorithms provide a principled framework for this combination. By manipulating the edit distance cost function our models can incorporate naturally useful statistical association measures, including variants of pointwise mutual information which, to some extent, capture directly taxonomic relations between terms. The models we proposed are mostly unsupervised, compact and efficient, and we provided empirical evidence of their effectiveness. We also explored a generative query reformulation model which can provide further improvements at additional computational cost and estimation complexity. Finally, we evaluated supervised combinations proving that the features capture complementary aspects of the data.
This framework offers several opportunities for further research. In a related work [7] we investigated supervised

289

models based on our features trained on noisy data within a learning to rank framework. Another interesting topic involves, as in bioinformatics, controlling the costs of all edit operations by applying algorithms such as NeedlemanWunsch [19]. Another promising topic involves moving beyond context-free reformulation in the generalized framework. In our approach substitution costs involve pairs of terms independent of the surrounding context while it seems reasonable that dependencies between terms should be useful in computing the best reformulation. Finally, probabilistic interpretations of semantic notions such as those investigated here deserve further research, possibly in combination with linguistic structure matching; e.g., as in [25]
9. REFERENCES
[1] E. Alfonseca, K. Hall, and S. Hartmann. Large-scale computation of distributional similarities for queries. In Proceedings of NAACL-HLT, pages 29­32. Association for Computational Linguistics, 2009.
[2] J. Allan. Relevance feedback with too much data. In Proceedings of SIGIR, pages 337­343. ACM, 1995.
[3] E. Aygu¨n, B. Oommen, and Z. Cataltepe. On utilizing optimal and information theoretic syntactic modeling for peptide classification. In Pattern Recognition in Bioinformatics, volume 5780 of Lecture Notes in Computer Science, pages 24­35. Springer Berlin, 2009.
[4] P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. From 'Dango' to 'Japanese cakes': Query reformulation models and patterns. In Proceedings of Web Intelligence. IEEE Cs Press, 2009.
[5] K. Church, W. Gale, P. Hanks, and D. Hindle. Using statistics in lexical analysis. In Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, pages 115­164. Erlbaum, 1991.
[6] B. Cucerzan and E. Brill. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of EMNLP, pages 293­300. Association for Computational Linguistics, 2004.
[7] F. De Bona, S. Riezler, K. Hall, M. Ciaramita, A. Herdagdelen, and M. Holmqvist. Learning dense models of query similarity from user click logs. In Proceedings of NAACL-HLT. Association for Computational Linguistics, 2010.
[8] L. Fitzpatrick and M. Dent. Automatic feedback using past queries: Social searching? In Proceedings of SIGIR, pages 306­313. ACM, 1997.
[9] D. He, A. G¨oker, and D. Harper. Combining evidence for automatic web session identification. Information Processing and Management, 38(5):727­742, 2002.
[10] M. Hearst. Search user interfaces. Cambridge University Press, 2009.
[11] J. Huang and E. Efthimiadis. Analyzing and evaluating query reformulation strategies in web search logs. In Proceedings of CIKM, pages 77­86. ACM, 2009.
[12] B. Jansen, A. Spink, and S. Koshman. Web searcher interaction with the dogpile.com metasearch engine. Journal Of The American Society For Information Science And Technology, 58(5):744­755, 2007.
[13] R. Jones and K. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of

search topics in query logs. In Proceedings of CIKM, pages 699­708. ACM, 2008. [14] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In Proceedings of WWW, pages 387­396. ACM, 2006. [15] O. Kolak and P. Resnik. OCR error correction using a noisy channel model. In Proceedings of HLT, pages 29­32. Association for Computational Linguistics, 2002. [16] T. Lau and E. Horvitz. Patterns of search: analyzing and modeling web query refinement. In Proceedings of the seventh international conference on User modeling, pages 119­128. Springer-Verlag New York, Inc., 1999. [17] V. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707­710, 1966. [18] M. Mitra, A. Singhal, and C. Buckley. Improving automatic query expansion. In Proceedings of SIGIR, pages 206­214. ACM, 1998. [19] S. Needleman and C. Wunsch. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443­453, 1970. [20] B. Oommen and R. Kashyap. A formal theory for optimal and information theoretic syntactic pattern recognition. Pattern Recognition, 31(8):1159­1177, 1998. [21] G. Recchia and M. Jones. More data trumps smarter algorithms: comparing pointwise mutual information with latent semantic analysis. Behavioral Research Methods, 41(3):647­656, 2009. [22] S. Rieh and H. Xie. Analysis of multiple query reformulations on the web: the interactive information retrieval context. Inf. Process. Manage., 42(3):751­768, 2006. [23] S. Riezler, Y. Liu, and A. Vasserman. Translating queries into snippets for improved query expansion. In Proceedings of Coling, pages 737­744, 2008. [24] M. Sahami and T. Heilman. A web-based kernel function for measuring the similarity of short text snippets. In Proceedings of WWW, pages 377­386. ACM, 2006. [25] M. Surdeanu, M. Ciaramita and H. Zaragoza. Learning to rank answers on large online QA collections, In Proceedings of ACL-HLT, pages 719­727. Association for Computational Linguistics, 2008. [26] P. Turney. Mining the web for synonyms: PMI­IR versus LSA on TOEFL. Lecture Notes in Computer Science, 2167:491­503, 2001. [27] J. Wen, J. Nie, and H. Zhang. Clustering user queries of a search engine. In Proceedings of WWW, pages 162­168. ACM, 2001. [28] J. Wen, J. Nie, and H. Zhang. Query clustering using user logs. ACM Trans. Inf. Syst., 20(1):59­81, 2002. [29] J. Xu and B. Croft. Query expansion using local and global document analysis. In Proceedings of SIGIR, pages 4­11. ACM, 1996. [30] Z. Zhang and O. Nasraoui. Mining search engine query logs for query recommendation. In Proceedings of WWW, pages 1039­1040. ACM, 2006.

290

Evaluating Verbose Query Processing Techniques

Samuel Huston and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science
University of Massachusetts, Amherst Amherst, MA 01003
{sjh, croft}@cs.umass.edu

ABSTRACT
Verbose or long queries are a small but significant part of the query stream in web search, and are common in other applications such as collaborative question answering (CQA). Current search engines perform well with keyword queries but are not, in general, effective for verbose queries. In this paper, we examine query processing techniques which can be applied to verbose queries prior to submission to a search engine in order to improve the search engine's results. We focus on verbose queries that have sentence-like structure, but are not simple "wh-" questions, and assume the search engine is a "black box." We evaluated the output of two search engines using queries from a CQA service and our results show that, among a broad range of techniques, the most effective approach is to simply reduce the length of the query. This can be achieved effectively by removing "stop structure" instead of only stop words. We show that the process of learning and removing stop structure from a query can be effectively automated.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation
General Terms
Algorithms, Experimentation, Perfomance
Keywords
Verbose Queries, Query Reformulation, Black Box
1. INTRODUCTION
It has been observed that most queries submitted to search engines are short. For example, the average length of the queries in the MSN search log, a sample of about 15 million queries collected over one month, is 2.4 words [3]. A surprising number of queries, however, are longer. For example, in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

the MSN log, about 10% of the queries are five words or longer (not including extremely long run-on queries, such as automatically generated queries). The reason this is surprising is that current search engines do not perform particularly well with long queries. Long queries are important in other search applications, such as Collaborative Question Answering (CQA) services and certain vertical search domains, such as patent searches.
Long queries can potentially express more complex information needs and they can provide much more context for ranking algorithms than the one-word queries used as examples in many papers (e.g., "java" and "jaguar"). Given their potential, there has been some recent research on techniques for improving search effectiveness with long queries. Bendersky and Croft [2], for example, show that key concepts can be extracted from long queries and used to improve ranking. Lease et al [14] described a technique for improving ranking by weighting terms in long queries. Kumaran and Carvalho [12] use learning techniques to rank subsets of the original long query based on quality prediction measures. These and some other similar studies have used TREC collections and open-source search engines to study the effectiveness of query processing techniques for long queries.
In this paper, we focus on identifying query processing techniques that can be used to improve the effectiveness of long queries where the search engine is a "black box". As such, we have limited ourselves to techniques that generate queries that a standard search engine application programming interface (API; in our case, the Yahoo! API and the Bing API) is capable of processing. This means that some techniques, such as term weighting, cannot be used because they are not supported by the API. There is some risk in this decision as it is possible that the search engine will attempt to learn from the submitted queries or that our results will reflect the query processing done by the search engine. To reduce these risks, we used two different search engines for our experiments. We also attempted to limit the learning capability of each search engine by ensuring that the links returned were never `clicked.' In order to reduce the chance that the search engine is modified during our search experiments, we ensure that all queries produced by applying these pre-processing techniques are submitted to the search APIs within a very short time span. We note that "black box" experiments are quite common in the literature and are becoming more important in the search industry. One of our goals is to make the assumptions and limitations underlying these experiments more explicit.
In addition to limiting our query processing techniques

291

to account for the black box search environment, we also restrict our study to long queries that form a clause or sentence containing a verb. Bendersky and Croft [3] described a classification of long queries and found that queries containing verbs made up about 15% of the long queries in the MSN query log. Our interest in these type of queries stems from the fact that their structure allows for more linguistic processing and that they are representative of the queries that people use in less constrained search environments, such as a CQA service. Verbose queries are long queries in which people use many words to say what could have been expressed in a few keywords. An example of a verbose query is "Would the meteor shower that hits this weekend be better to watch tonight or tomorrow night?", taken from Yahoo! Answers1. A number of the words in this query do not help to retrieve relevant documents, but the query as a whole conveys considerably more about the information needed or the user's intent than a query such as "meteor shower". Verbose queries may also include simple "wh-" (who, what, when, where) or "factoid" queries. However, because this subset of verbose queries has been studied extensively in the TREC QA (Question Answering) track and have had specific techniques developed for them, we have omitted them from this work.
The query transformation techniques we consider include stopword removal, phrase detection, key concept identification, and stop structure removal. Each of these techniques is applied individually and in combination to the verbose queries in our test sets. We measure the performance of the original queries and of the queries produced by these techniques and compare the results to evaluate the techniques' effectiveness. The most effective single technique found in this first round of experiments was stop structure removal.
A stop phrase has been defined as a phrase that does not provide any information on the user's information need [6]. We define a stop structure as a stop phrase that begins at the first word in a query. Though it may be possible to effectively remove many stop structures from search engine queries using static lists, similar to those used for stopwords, doing so may inadvertently remove relevant words from the query. To address this problem we describe a sequential classifier that enables us to automatically identify the stop structure in each query. We show that the queries produced by this classifier perform similarly to those produced by manually identified stop structure removal.
In the next section we discuss related work. We describe how the test sets of queries were created in section 3. In section 4, we describe the query processing techniques used. We describe the experimental setup and the results from the retrieval experiments in section 5. The automatic stop structure classifier is described and analyzed in section 6. In section 7 we conclude and discuss possible future work.
2. RELATED WORK
There has been a large amount of recent work on query transformation techniques. Much of this work focuses on the accurate, efficient extraction of linguistic features from queries. Bergsma and Wang [4] present a method of learning noun phrase segmentations. Tan and Peng [16] present a generative model based method of query segmentation. Guo et al [8] present a Weakly Supervised Latent Dirichlet
1http://answers.yahoo.com/

Allocation (WS-LDA) based method of identifying named entities in queries. Each of these studies evaluate their performance over samples of commercial query logs. It should be noted that these studies do not show how the extracted noun phrases or query segments might be used in a retrieval system, either internally or externally as a pre-processing stage.
There is some recent work on query processing that evaluates results using a "black box" approach. For example, Gu et al [9] describe a CRF-based framework for applying several types of transformations to a query. These transformations are tested on queries sampled from a commercial search engine log. Both the original and transformed queries are submitted to a commercial search engine, and the top retrieved documents are assessed for relevance. Although there are some similarities to our study, we focus on verbose queries and processing techniques that are appropriate for longer queries, including automatic stop structure removal.
3. TEST QUERIES
In order to carry out experiments with query processing techniques, we needed a test collection of verbose queries. Rather than use the TREC description queries that have been studied in other recent papers (e.g., [2],[14]), we decided to use queries from the Yahoo! Answers CQA service. We believed these queries would be less artificial than TREC queries and less constrained than queries from a web search query log, where users tend to think in terms of keywords because that is what works well with current search engines. We would expect, therefore, that many of the CQA queries will not work well without query processing.
In order to construct a sample of verbose queries, we extracted questions from a crawl of Yahoo! Answers and filtered the results to identify queries which match the desired type. As mentioned above, the desired queries consist of clauses or sentences containing verbs, but are not standard "factoid" or QA questions. In Bendersky and Croft's study [3], these were called non-composite-verb queries, where the "non-composite" indicates that the query is not just a simple combination of shorter web-style queries.
Our purpose in extracting queries is to construct two nonoverlapping sample sets of 50 queries, such that one set is composed of more "difficult" queries, and the other set is composed of a representative sample of this type of query. In order to construct the "difficult" query set, candidate queries were submitted to a web search engine (Yahoo! Search). If the search yielded a relevant document in the top position, then the query was rejected, otherwise we added the query to our test set.
In this paper we refer to the set of "difficult" queries as Set 1. The representative set of verbose queries is referred to as Set 2.
Set 1 is used to study the query processing techniques that could have the most impact when current search engines fail to perform well. Set 2 is used to test that the same query processing techniques are beneficial over a more representative sample query set.
By inspecting the query log we found that capitalization and punctuation appeared to be optional for this type of query. Accordingly, we removed all punctuation from the queries in order to ensure that the performance of our techniques would not be dependent on correct grammar or captialization. We view spelling correction as a necessary step

292

for any verbose query processing system. Therefore we modified the queries so that all words were correctly spelled.
4. TECHNIQUES
4.1 Stopword Removal
Removing stopwords has long been a standard query processing step [7]. We used three different stopwords lists in this study: the standard INQUERY stopword list [1], as well as two query-stopword lists derived from the Yahoo! Answers query log. We chose to construct new stopword lists because the language that is used in some of the test queries is more informal than the sample of English text from which the INQUERY stopword list is derived.
Lo, He and Ounis [17] present several methods of automatically constructing a collection dependent stopword list. Their methods generally involve ranking collection terms by some weight, then choosing some rank threshold above which terms are considered stopwords. Their weighting methods all produce similar retrieval performances, but an inverse document frequency (IDF) based weighting scheme is the most effective.
In accordance with their findings, we applied an IDF based weighting scheme to the set of verbose queries in the Yahoo! Answers query log. As a result we were able to construct two stopword lists which might be more appropriate for these queries. The two stopword lists were extracted by taking the top 100 and 200 ranked words from this ordering. It should be noted that words are rarely repeated in a single query. Thus, in this particular case, sorting by term frequency is almost rank equivalent to sorting by inverse document frequency.
This process identified words such as "help", "find", and "know" which do not occur in the INQUERY stopword list, but are not necessarily useful search words.
The key problem in trying to construct a query-stopword list is that common query phrases such as "high blood pressure" occur as frequently as some stop phrases. These frequent phrases mean that words such as "blood" and "pressure" occur in the query log as frequently as the words "did" or "am".
We chose to apply these stopword lists by removing all words in the query which occur on the stopword list. For example, if using the INQUERY stopword list, and given the query:
"Can i work while study in Europe"
this technique produces the query:
"work study Europe"
It is worth noting that this technique can significantly change the meaning of the query. For example when applying the INQUERY stopword list to the query:
"i would like to know the origin
of the phrase to be or not to be"
the resulting query is:
"know origin phrase"
4.2 Noun Phrase Detection
By definition, a verbose query is composed by the user as a sentence or phrase. This allows us to apply a variety

of linguistic tagging and chunking techniques to the query. Additionally we know from previous work that extracting noun phrases from the query can help identify the key concepts within the query [2, 18, 10]. We used the MontyLingua toolkit [15] to automatically identify noun phrases.
Given that we are using a search engine as a black box, we are limited in how we can use the identified noun phrases contained within the query. We are unable to assign weights to terms or phrases according to confidence or importance. Using the black box interface, we are only able to identify important phrases using quotation marks.
There are several methods by which the identified noun phrases can be communicated to the search engine within this limited interface. We show results for two such methods. The first method wraps each of the detected noun phrases in the query in quotation marks; no words are removed from the query. The second method removes all words which are not part of some detected noun phrase and quotation marks are not used.
For example, 3 noun phrases: `me', `a name' and `my next horror script', are detected in the query:
give me a name for my next horror script
Using the first method we would produce the query:
give "me" "a name" for "my next horror script"
Using the second method we would produce the query:
me a name my next horror script
4.3 Key Concept Detection
The term key concept in this context is a short set of sequential words that express an important idea contained within the query. As presented by Bendersky and Croft [2], the identification of key concepts in long queries can produce performance improvements in retrieval engines. Their research demonstrated that key concepts in TREC queries can be automatically identified with around 80% accuracy.
For this study, we use the same type of classifier as Bendersky and Croft. The classifier is based upon the AdaBoost.M1 meta-classifier using C4.5 Decision Trees. The features used in this classifier included: GOV2 collection term frequency, inverse document frequency, residual inverse document frequency, weighted information gain, google n-grams term frequency, and query frequency. These features are detailed in their paper, [2]. The GOV2 query set was used to train this classifier.
Key concepts are structurally similar to noun phrases and thus present similar problems for utilizing them. In order to effectively communicate key concepts to the search engine, therefore, we again employ the same two methods. The first method wraps the detected key concepts in quotation marks. The second method removes all words that are not part of a given key concept.
Given the query:
i read that ions cant have net dipole moments why not
The key concepts "i", "ions" and "net dipole moments" were identified by the classifier. The first method outlined above would produce the query:
"i" read that "ions" cant have "net dipole moments" why not

293

and the second method would produce:
i ions net dipole moments
4.4 Stop Structure
Removal of stop phrases was originally presented by Callan and Croft as a query processing technique [6]. They define a stop phrase as a phrase which provides no information about the information need. One example they give is "find a document". Within their study they created a static list of stop phrases by inspecting 50 TIPSTER queries. This list was reused in later studies using TIPSTER [5].
The majority of our test queries contained one relatively large stop phrase at the start of the query. We call these stop phrases stop structure. That is, a stop structure is a stop phrase which begins at the first word in the query. For this study other stop phrases in the query are considered semantically meaningful; we did not remove them.
Importantly, stop structure often contains words which are not stopwords, but which nonetheless do not add any meaning to the query. For example "My husband would like to know more about cancer" This information need could be equivalently served by the query "cancer". As in the case of stopwords, stop structure removal carries some inherent risk. In the above query, one could say that there is implied information contained within the term "husband". That is, the underlying information need may correspond to specific types of cancers that are common among males.
However, within the framework of this study we have access to additional information about the user's information need. We used the user-selected answer corresponding to their query to guide the manual identification of stop structure. In this way, we eliminated the risk of losing meaningful terms through the removal of stop structure. Later, we will show that automatic, non-answer guided stop structure removal can closely approximate the retrieval performance of answer-guided manual stop structure removal.
Similar to stopwords, stop structure is removed from queries prior to submission to a search engine. For example given the query:
if i am having a lipid test can i drink black coffee
The stop structure "if i am having a" is removed, leaving:
lipid test can i drink black coffee
Note that this query has another stop phrase, "can i", which is not removed.
5. RETRIEVAL EXPERIMENTS
5.1 Setup
The aim of these experiments is to see which of the above techniques, if any, has the greatest effect on retrieval performance. As mentioned above we choose to use two commercial search engines as black box search engines, as this matches the limitations of a meta-search engine designer. For this study we chose to use Yahoo! Search, and Bing as `black boxes'. Yahoo! BOSS API and Bing API 2.0 were used to submit queries and retrieve documents. 10 results were collected for each query from each search engine.
There are two problems which may arise through the use of commercial search engines for a scientific study. First the search engine may learn how to respond to our particular set

of test queries. Second the search engine may be modified at any time. In order to limit the amount of learning that each search engine would be able to do over our set of test queries, we ensures that no returned links were clicked for any of the queries.
Since the search engines we are using are commercial, we won't necessarily know if the underlying system changes at any time. In order to minimize this risk we chose to submit all queries generated by all of the above techniques to both APIs within a single short search session. This search session occurred on September 29th, 2009.
After all queries were submitted, all returned pages were manually judged for relevance. These judgements were guided by the answers provided for the queries in the CQA system. A three-valued judgement system was used. The three values were `not relevant', `partially relevant', and `relevant', denoted by the numerical values {0,1,2} respectively.
It should be noted that both search engines occasionally returned the page in Yahoo! Answers from which the original query was gathered. These pages were removed from the search results, and the remaining pages re-ranked. This adjustment was made in order to avoid skewed results in favor of the original queries.
We show the evaluation metrics normalized discounted cumulative gain (nDCG) at ranks 5 and 10 for each processing technique. The normalization constant was computed from the sum of all annotations for each query. Thus the values are comparable across query processing techniques, the two search engines, and the subsequent retrieval experiments described below.
5.2 Retrieval Experiments Results
Table 1 shows the retrieval results for all of the query processing techniques when applied to query set 1 using the Yahoo! and Bing search engines. The results from the two search engines are very similar in terms of relative effectiveness, confirming that inconsistencies are unlikely to have been caused by internal search engine processing. The use of quotations in formulating queries is clearly not effective. Both noun phrase and key concept identification produce significant improvements, when combined with stopword removal. The most effective technique, however, is the removal of stop structure. Manual removal of these words resulted in consistent and very significant improvements for both NDCG measures and in both search engines.
Manual removal of stop structure was guided by the user's selected answer, so we view this as an oracle result under the best possible circumstances.
Interestingly, the removal of any remaining stopwords from the queries after removing stop structure, degrades the retrieval performance of the queries Within almost all of the queries produced through the removal of manual stop structure semantically meaningless terms are present. The performance degradation stems from the removal of semantically meaningful terms. For example given the query:
for a year ive been getting some
tightening from within my chest
Removal of manual stop structure produces the query:
tightening from within my chest
Clearly "from" and "my" could be removed without changing the meaning of the query. However, removing the INQUERY

294

Original INQUERY Stopword List
Query Stopword List 1 Query Stopword List 2 Quoted Noun Phrases Quoted Noun Phrases + Stopwords Quoted Key Concepts Quoted Key Concepts + Stopwords
Only Noun Phrases Only Noun Phrases + Stopwords
Only Key Concepts Only Key Concepts + Stopwords
Manual Stop Structure Manual Stop Structure + Stopwords

Query Test Set 1 Yahoo! API

nDCG@5 nDCG@10

0.1760 0.2263+

0.1573 0.2060+

0.2116
0.1844 0.0854- 0.1172- 0.1217-

0.1900
0.1846 0.0735- 0.1071- 0.1110-

0.1401

0.1340

0.2164 0.2808+
0.2344 0.2851+ 0.3604+ 0.3280+

0.1975 0.2530+ 0.2165+ 0.2531+ 0.3298+ 0.3042+

Query Test Set 1 Bing API

nDCG@5 nDCG@10

0.1939 0.2530+ 0.2551+
0.2142 0.0947-
0.1384 0.1261-

0.1875
0.2152
0.2266
0.1952 0.0763- 0.1262- 0.1201-

0.1749

0.1536

0.2305 0.2854+

0.2107 0.2507

0.2612 0.2894+ 0.3588+ 0.3798+

0.2447 0.2684+ 0.3274+ 0.3371+

Table 1: Performance of preprocessing techniques from the first search session for query test set 1. nDCG@5
and nDCG@10 are shown for both search APIs. Paired t-tests were performed between each result shown and the baseline (Original), results which show significant improvements, (p-value < 0.05) are marked +. Similarly results which significantly degraded performance, with a p-value less than 0.05, are marked -.

Original INQUERY Stopword List
Query Stopword List 1 Query Stopword List 2 Manual Stop Structure Manual Stop Structure + Stopwords

Query Test Set 2 Yahoo! API
nDCG@5 nDCG@10

0.3355 0.3610

0.2926 0.3311

0.3289

0.3009

0.3118 0.4288+ 0.4071+

0.3020 0.3805+ 0.3789+

Query Test Set 2 Bing API
nDCG@5 nDCG@10

0.2757 0.3356+
0.3359 0.3546+ 0.4201+ 0.4054+

0.2400 0.3157+ 0.3172+ 0.3329+ 0.3815+ 0.3882+

Table 2: Performance of preprocessing techniques from the first search session for query test set 2. nDCG@5
and nDCG@10 are shown. Paired t-tests were performed between each result shown and the baseline (Original), results which show significant improvements, (p-value < 0.05) are marked +.

stopwords produces the query:
tightening chest
Both search engines returned websites which advertised exercise regimes for this query. Similarly the performance of the stopword based techniques was hindered by this type of inadvertent removal of semantically meaningful terms.
Recall that query test set 1 was sampled such that the queries are more difficult for commercial search engines. We repeated the experiments with stopword and stop structure removal using the second set of queries. These results are shown in Table 2. This second set of experiments are designed to test the performance of the removal of stop structure over a more representative sample of verbose queries.
These results show that stop structure removal is indeed very effective. Given these, very significant effectiveness improvements from manual stop structure removal, in the remainder of this paper, we focus on describing and evaluating an automatic method for classifying and removing query stop structure.

6. AUTOMATIC STOP STRUCTURE CLASSIFICATION
We have defined stop structure to be a stop phrase beginning at the first word in the query. It it important to note that a stop structure can have many variations without significantly changing its meaning. Indeed, any phrase that does not provide some insight into the underlying information need may be considered stop structure for some particular query. Additionally, it is possible that the stop structure from one query may not necessarily be stop structure in another query. For example "please tell me why" may be identified as stop structure in the query:
please tell me why the sky is blue
But it would not be considered stop structure in the query:
please tell me why song lyrics
So we can see that using a single static stop structure list is not an appropriate approach to stop structure removal.
Therefore, we have developed a method of automatically classifying stop structure within some input query. The purpose of our classifier is to identify each of the words in each of the queries as a "stop structure term" or a "query term".

295

It is natural to formulate this type of problem as a sequential binary class tagging problem.
6.1 Features
In order to obtain the best possible classifier a range of features were extracted for term in each query. Broadly these features fall into two categories; multinomial and numerical. The features extracted for each term in a query were then used as input to the classifiers which are discussed in the next section.
First I will outline the multinomial features generated for each term in the input query. Using the MontyLingua toolkit [15], we extract the part of speech tag for each term (NN, VB, etc) The next feature generated was the position of the term (1,2,3,4,5,...) in the query. Two binary features were also extracted: whether or not the term is present in the INQUERY stopword list, and whether or not there is any non-stopwords in the current query which precede the current word.
The numerical features were extracted from three TREC collections, and from three query logs. We used the WT10G, GOV, and GOV2 TREC collections. The query logs we used are: the Yahoo! Answers CQA log, Wondir CQA log, and the MSN query log. We found that each of these collections and query logs conveyed some unique information to the classifier. We believe that this is due to the vocabulary mismatch between the test queries and any one of the collections or query logs.
All three TREC collections are composed of web documents, the GOV and GOV2 collections were limited to the `.gov' domain, while the WT10G was not limited to any particular domain. Further details can be found on the TREC Web Test collections website. 2
The Yahoo! Answers CQA log consists of 216,563 user submitted question and answer sets. The log was constructed by a web crawl of the Yahoo! Answers website 3. The Wondir CQA log consists of 401,560 user submitted question and answer sets from the Wondir community question answer service. The MSN query log consists of 15 million queries submitted to the MSN search engine during a period of one month.
We define subsets of each of the query logs were created based on the length of the queries; the first subset contains very short queries ( 2 words), the second contains short queries ( 4 words) and the final subset contains long queries (> 4 words). Our hope is that features extracted from these subsets will enable a classifier to distinguish between commonly searched phrases and common stop structures. A common search phrase should have a higher frequency amongst the short or very short queries, while a common stop structure should have a lower frequency in these subsets. Within the long subsets, common query phrases and stop structures may have similar, relatively high frequencies.
Three types of numeric features were extracted for each term in a query; term frequency, bi-term frequency, and inverse document frequency (IDF). Term frequency refers to the number of times the term occurs in the collection; f (ti). Bi-term frequency referse to the number of times the term and it's predecessor occurs in the corpus; f (ti|ti-1). IDF
2http://ir.dcs.gla.ac.uk/test collections/ 3http://answers.yahoo.com/

refers

to

the

inverse

document

frequency;

idf (ti)

=

log

|D| df

.

Where the document frequency, df , is the number of doc-

uments which contains the term, and |D| is the number of

documents in the collection.

We extracted all three numeric features from each of the

TREC collections. Term frequencies and bi-term frequencies

were extracted from each of the three query logs and each

of the nine query log subsets.

We also experimented with features based on phrase chunk

tags and n-word frequencies. However these features were

found to be detrimental to the classifier's performance. This

may have stemmed from the inaccuracy of the phrase chunks,

and corpus sparsity problems for the frequency of longer

phrases.

6.2 Classification Evaluation
As has been mentioned above, the purpose of this classifier is to mark each of the words in the queries as a "stop structure term" or a "query term". We experimented with two implementations of sequential taggers; CRF++ and YamCha. CRF++ is an open source implementation of Conditional Random Fields for sequential tagging, based on work done be Lafferty, McCallum and Pereira [13]. YamCha (Yet Another Multipurpose CHunk Annotator) is a sequential tagger based on support vector machines [11].
The training set used for each of these classifiers was composed of 100 new verbose queries identified from the Yahoo! Answers query log. These newly identified queries have similar properties to the two sets of queries already identified. The stop structure in these queries was manually identified in a similar manner to the test sets of queries. The test sets for this classifier are the two sets of queries which we have already identified for this study.
The CRF++ classifier is constructed such that all features must be multinomial. This means that numerical features are unsuitable for this classifier. In order to convert the frequency-based features detailed above into multinomial features, we applied binning after taking the log of each numerical value. After taking the log of the frequency values the numerical features ranged from 0 and 20. The most effective binning method we were able to find segmented the number plane at multiples of 5, with an extra bin for values 0 < x < 1. This method produced 6 bins; (0, 1, 5, 10, 15, 15).
The best performance from the YamCha classifier that we were able to obtain used a context window of 3 terms; the previous term, the current term and the next term. The previously assigned tag was also used as a feature. Using larger window sizes degraded performance. Similarly the best performance for the CRF++ that we were able to obtain used a context window of 3 terms.
We used two types of evaluation metrics to measure the performance of each classifier. First we used precision, recall, and accuracy of the tags applied to each term in the queries. A true positive, with respect to precision and recall, refers to a semantically meaningful term being correctly classified as a `query term'.
Recall that stop structure begins with the first word in the query, and extends to the first semantically meaningful term, or `query term'. The second measure is the mean squared error distance (MSE) of the position of the first `query term'. This metric is intended to measure the error distance of the boundary between stop structure terms and query terms.

296

CRF++ Yamcha

Precision 0.891 0.915

Test Set 1 Recall Accuracy 0.825 0.816 0.921 0.894

MSE 5.1 2.64

Precision 0.930 0.938

Test Set 2 Recall Accuracy 0.867 0.866 0.926 0.905

MSE 2.68 1.86

Table 3: Classification results for two automatic stop structure classifiers. Precision, recall, and accuracy refer to per word classification accuracy. Precision and recall metrics refer to a correctly classified semantically meaningful query words. Mean squared error distance (MSE) refers to the distance between the first semantically meaningful query word, and the first classified semantically meaningful word.

Original Stopword List Manual Stop Structure Manual Stop Structure + Stopwords Classified Stop Structure Classified Stop Structure + Stopwords

Query Test Set 1 Yahoo API
nDCG@5 nDCG@10

0.1963
0.2264 0.3705+ 0.3631+ 0.3810+ 0.3530+

0.1718
0.2011 0.3338+ 0.3177+ 0.3412+ 0.3177+

Query Test Set 1 Bing API
nDCG@5 nDCG@10

0.2043
0.2354 0.3330+ 0.3551+ 0.3176+ 0.3299+

0.1803
0.2162 0.3059+ 0.3267+ 0.2893+ 0.3038+

Table 4: Performance of preprocessing techniques from the automatically classified stop structure focused
retrieval experiments over query test set 1. Performance metric nDCG at 5 and at 10 are shown. Paired
t-tests were performed between each result shown and the baseline (Original), results which show significant improvements, (p-value < 0.05) are marked +.

Original Stopword List Manual Stop Structure Manual Stop Structure + Stopwords Classified Stop Structure Classified Stop Structure + Stopwords

Query Test Set 2 Yahoo API
nDCG@5 nDCG@10

0.3689

0.3394

0.3914 0.4661+ 0.4873+

0.3770 0.4312+ 0.4525+

0.4083 0.4442+

0.3923 0.4148+

Query Test Set 2 Bing API
nDCG@5 nDCG@10

0.3249 0.4444+ 0.4916+ 0.5291+ 0.4628+ 0.5047+

0.2710 0.3821+ 0.4507+ 0.4782+ 0.4253+ 0.4692+

Table 5: Performance of preprocessing techniques from the automatically classified stop structure focused
retrieval experiments over query test set 2. Performance metric nDCG at 5 and at 10 are shown. Paired
t-tests were performed between each result shown and the baseline (Original), results which show significant improvements, (p-value < 0.05) are marked +.

Results for classification evaluation are shown in Table 3. We are most interested in maximizing recall within this classifier. A high recall value means that all semantically meaningful terms are retained within the processed query. That is not to say that precision is not valuable; higher precision should dramatically improve the performance of the query, as semantically meaningless words are removed from the processed query.
Due to the effectiveness of the YamCha classifier, the output of this classifier was used as the automatically classified stop structure in the following retrieval experiments.
6.3 Retrieval Experiments
We performed these retrieval experiments in a similar way to the previous retrieval experiments in section 5. The aim of these experiments is to evaluate the performance of the queries produced by removing automatically identified stop structure.
Because the commercial search engines we are using may have changed in the months between this search session and the previous search session, we chose to re-submit queries

produced by previously analyzed pre-processing techniques. In particular, the original query, and techniques involving the removal of INQUERY stopwords, and the removal of manually identified stop structure were re-submitted as part of these experiments. Queries produced by these techniques and queries produced through the automatic removal of stop structure were submitted to Yahoo! API and Bing API during a search session on January 9th, 2010.
The retrieval performance for each of the processed queries as submitted in the second search session is shown in Tables 4 and 5. We show nDCG@5 and nDCG@10 for each processing technique. We use the same normalizing factor used in the above retrieval experiments, thus these results are directly comparable to the results in the previous retrieval experiments.
Comparing these results to the results of the previous session, we can see that the performance of these queries and pre-processing techniques have improved somewhat. in the months since the last search session. However, we can see that the relative performances of the processing techniques has not significantly changed. Queries produced by remov-

297

ing stop structure still significantly outperform the original queries and the queries with stopwords removed.
We can infer from these results that stop structure can be effectively automatically classified. When compared with the manually identified stop structure, similar performance increases over the baseline are achieved. The differences between the manually identified stop structure and the classified stop structure generally stemmed from misclassified semantically meaningful query words. That, is some meaningful terms are incorrectly classified as part of the stop structure. For example, in the query "define turf toe as in football", the automatic stop structure classifier identified "define turf " as stop structure, thereby missing documents on the medical condition "turf toe".
7. CONCLUSION
We have shown that pre-processing techniques are a viable option to improve the performance of verbose queries for commercial web search engines. In particular, we found that the removal of stop structure, a stop phrase beginning at the first word in the query, can dramatically improve the performance of the query.
We also investigated methods of automatically identifying stop structure within a given query. Using a YamCha based classifier, we were able to classify stop structure with high degree of accuracy. It was also shown that the retrieval performance of automatically identified stop structure closely matches the performance of manually identified stop structure.
Future work may include investigating the automatic removal of stop phrases that may occur at any position within a verbose query. This is based on the observation that it is possible to formulate queries which contain stop phrases which do not occur at the beginning of the query.
It would also be interesting to investigate methods of combining the results of several partial query searches. It is possible that some rank fusion methods might produce better results by applying several pre-processing techniques to the query then combining the results from several searches.
Acknowledgments
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS0534383. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
8. REFERENCES
[1] J. Allan, W. Croft, D. Fisher, M. Connell, F. Feng, and X. Li. Inquery and TREC-9. In Proc. of TREC-9, pages 551­562, 2000.
[2] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. of SIGIR '08, pages 491­498, New York, NY, USA, 2008. ACM.

[3] M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. In Workshop on Web Search Click Data (WSCD 2009), 2009.
[4] S. Bergsma and Q. Wang. Learning Noun Phrase Query Segmentation. In Proc. of EMNLP-CoNLL 2007, pages 819­826, 2007.
[5] J. Callan, W. Croft, and J. Broglio. TREC and TIPSTER experiments with INQUERY. Information Processing and Management, 31(3):327­343, 1995.
[6] J. P. Callan and W. B. Croft. An evaluation of query processing strategies using the TIPSTER collection. In Proc. of SIGIR '93, pages 347­355, New York, NY, USA, 1993. ACM.
[7] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison-Wesley Publishing Company, USA, 2009.
[8] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity recognition in query. In Proc. of SIGIR '08, pages 267­274, New York, NY, USA, 2009. ACM.
[9] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In Proc. of SIGIR '08, pages 379­386, New York, NY, USA, 2008. ACM.
[10] A. Hulth. Improved automatic keyword extraction given more linguistic knowledge. In Proc. of EMNLP 2003, volume 2, pages 16­223, 2003.
[11] T. Kudoh and Y. Matsumoto. Use of support vector learning for chunk identification. In Proc. of CoNLL-2000, page 144. Association for Computational Linguistics, 2000.
[12] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In Proc. of SIGIR '09, pages 564­571. ACM, 2009.
[13] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML 2001, pages 282­289, 2001.
[14] M. Lease, J. Allan, and W. B. Croft. Regression Rank: Learning to Meet the Opportunity of Descriptive Queries. In Proc. of ECIR 2009, pages 90­101, 2009.
[15] H. Liu. MontyLingua: An end-to-end natural language processor with common sense, 2004.
[16] B. Tan and F. Peng. Unsupervised query segmentation using generative language models and Wikipedia. In Proc. of WWW '08, pages 347­356, New York, NY, USA, 2008. ACM.
[17] R. Tsz-Wai Lo, B. He, and I. Ounis. Automatically building a stopword list for an information retrieval system. In Journal on Digital Information Management: Special Issue on the 5th Dutch-Belgian Information Retrieval Workshop (DIR), volume 5, pages 17­24, 2005.
[18] J. Xu and W. Croft. Query expansion using local and global document analysis. In Proc. of SIGIR 1996, page 11. ACM, 1996.

298

SED: Supervised Experimental Design and Its Application to Text Classification

Yi Zhen and Dit-Yan Yeung
Department of Computer Science and Engineering Hong Kong University of Science and Technology
Hong Kong, China
{yzhen, dyyeung}@cse.ust.hk

ABSTRACT
In recent years, active learning methods based on experimental design achieve state-of-the-art performance in text classification applications. Although these methods can exploit the distribution of unlabeled data and support batch selection, they cannot make use of labeled data which often carry useful information for active learning. In this paper, we propose a novel active learning method for text classification, called supervised experimental design (SED), which seamlessly incorporates label information into experimental design. Experimental results show that SED outperforms its counterparts which either discard the label information even when it is available or fail to exploit the distribution of unlabeled data.
Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Statistics--Experimental Design; H.3 [Information Storage and Retrieval]: Information Search and Retrieval--Clustering
General Terms
Algorithms, Theory
Keywords
Active Learning, Supervised Experimental Design, Text Classification, Convex Optimization
1. INTRODUCTION
There has been a long tradition of research on text classification in both the information retrieval and machine learning communities. In order to learn a good text classifier, a large number of labeled documents are often needed for classifier training. However, labeling documents always needs domain knowledge and thus is difficult, time consuming and costly. On the other hand, it is much easier to obtain a large
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

number of unlabeled documents, such as web pages, newspapers and journal articles. In recent years, a new approach called active learning [1, 3, 5, 6, 9, 11, 13, 14, 15, 16, 18, 20, 25] has been developed in the machine learning community with the goal of reducing the labeling cost by identifying and presenting the most informative examples from the unlabeled examples for the human experts to label.
Although a lot of work has been done in active learning research, most of the existing active learning methods are still far from satisfactory with apparent shortcomings. In particular, many methods only take into consideration partial information to determine the informativeness of examples. Some methods consider information conveyed by the class boundaries, some consider information conveyed by the distribution of unlabeled data, and some consider the disagreement between learners when multiple learners are involved. Unfortunately, none of these methods is consistently better than others in all situations. Another drawback is that most active learning algorithms select only one example at a time for labeling. Compared with a batch approach [7, 9, 10] which selects multiple examples in one iteration, this greedy incremental approach is at best suboptimal and is not suitable for large-scale and parallel computing applications.
Experimental design [2, 20, 21], which is one of the stateof-the-art active learning approaches for text classification, can effectively exploit the distribution of unlabeled data while supporting batch selection at the same time. Despite their appealing properties, existing methods based on experimental design cannot make use of label information even when labeled data are available. Thus, these methods are intrinsically unsupervised in nature.
In this paper, we propose a novel batch mode active learning algorithm, called supervised experimental design (SED), which incorporates label information into the experimental design procedure. SED is a supervised extension of experimental design with a new regularization term that incorporates label information added to the objective function. To the best of our knowledge, no work has been done so far to utilize label information in the experimental design procedure. Some favorable properties of SED are highlighted here:
 To the best of our knowledge, SED is the first work that incorporates label information into experimental design.
 SED outperforms (unsupervised) experimental design, which discards the label information even when it is available. This shows that label information does pro-

299

vide useful information for active document selection and SED can utilize the information very effectively.
 SED outperforms margin-based active learning under highly unbalanced data distributions which are often encountered in practice.
 SED is convex and thus global optimality can be guaranteed.
The remainder of this paper is organized as follows. In Section 2, we will introduce the notations and some related work. In Section 3, we first introduce transductive experimental design and then present our SED model and algorithm in detail. Extensive empirical studies conducted on two real-world text corpora are presented in Section 4. Section 5 concludes our paper.
2. NOTATIONS AND RELATED WORK
Throughout this paper, we use boldface uppercase letters (e.g. X) to denote matrices and boldface lowercase letters (e.g. x) to denote vectors. We use tr(X) to denote the trace of X and X to denote its transpose. Moreover, we use calligraphic letters (e.g. ) to denote sets and   to denote the size of .
Given the whole data set represented as X   × or = {x1, . . . , x }, in which each data point x is a × 1 vector, a generic active learning problem [4, 11] can be defined as selecting a subset of unlabeled data points from a candidate set X   × or = {x1, . . . , x }, such that if the selected data points are labeled and added to the training set for re-training the classifier, the improvement of the classifier will be maximized. We call the subset of selected data the active set and denote it as X   × or = {x1, . . . , x }.1 The promise of active learning is appealing because it can help to alleviate the labeled data deficiency problem commonly encountered in many supervised learning applications. Existing active learning algorithms for text classification either select the most uncertain data given the current classifier [11], select the data with the smallest margin [18], select the data on which multiple classifiers disagree most with each other [5, 14, 17], or select the data that optimize some information gain [6, 13, 16, 25]. Closely related to active learning is experimental design in statistics [2]. Conventionally, experimental design considers the problem of learning a predictive function (x) from experiment-measurement pairs (x , ). Given that conducting an experiment is expensive, experimental design seeks to select the most informative experiments to conduct such that the number of measurements needed can be reduced. Traditional experimental design considers the following linear regression model:
=w x+ ,
where is the measurement, x is the × 1 feature vector of the experiment, w is the × 1 model parameter vector and is the noise term.
1The reader should note that points in different sets with the same index are not necessarily the same point, although we require that the points in should appear in and the points in should appear in .

Given a set of labeled data {(x , )} =1, the maximum likelihood estimate (MLE) of the model parameter vector w can be obtained by minimizing the residual sum of squares:

{ w^ = argmin

(w; X, y) = (w

x

-

)2}

w =1

= (X X)-1X y,

(1)

where X = [x1, . . . , x ] is a matrix of the labeled data and y is a vector of the corresponding target outputs.2
If we put a spherical Gaussian prior on the noise , i.e.,  (0, 2), it can be proved easily that w^ is an unbiased estimate of w with covariance:
cov[w^ ] = cov[(X X)-1X y]
= (X X)-1X cov[y] X(X X)-1
= 2(X X)-1.

Traditional experimental design aims at minimizing the covariance of w^ , which characterizes the model uncertainty in some sense. Three criteria have been commonly used in the literature:
 D-optimal design: minimizing the determinant of cov[w^ ];

 A-optimal design: minimizing the trace of cov[w^ ];

 E-optimal design: minimizing the maximum eigenvalue of cov[w^ ].
Recently, Yu et al. [20] proposed a method, called transductive experimental design (TED), which selects the most informative examples by reducing the model uncertainty on all of the unlabeled data and thus effectively exploits the distribution of the unlabeled data. He et al. [8] applied similar ideas to content-based image retrieval (CBIR), where a Laplacian regularization term is added and then the model uncertainty, represented by a new covariance matrix, considers the smoothness among data points.
Despite the appealing properties which include clear mathematical formulation and the ability of batch selection, algorithms based on experimental design often have to deal with combinatorial complexity and are NP-hard. Since the optimization problems involved are non-convex, the solutions obtained may correspond to poor local minima. To address this problem, some approximation methods based on convex relaxation have been developed [21, 23].

3. SUPERVISED EXPERIMENTAL DESIGN
Existing active learning methods based on experimental design, such as TED, are formulated under the setting that all available data are unlabeled. As such, they cannot make use of the label information even when it is available.
Since label information has been found very useful to example (or document) selection [5, 11, 14, 16, 18], incorporating label information into the example selection procedure of experimental design is a very worthwhile direction to explore.
In this section, we first briefly review TED in Section 3.1 and then present our method, SED, in Section 3.2. The algorithm for SED will be summarized in Section 3.3 and its complexity analysis will be presented in Section 3.4.
2In the sequel, we will also refer to them as labels even though the term `label' is more appropriately used for classification problems.

300

3.1 Transductive Experimental Design

TED [20] seeks to choose X from X such that a function learned from X has the smallest predictive variance on X . The goal can be achieved by solving the following optimization problem:

min

[ tr X

(X X

+

I)-1 X ]

X

s.t.

X  X ,  = ,

(2)

where I is an identity matrix whose dimensionality is determined by the problem and is the number of examples selected. The objective function may also be considered as model uncertainty over X . We note that it only depends on the input features of the training examples and thus is independent of the labels. This is because in the error function
(w; X, y) of the linear regression model in Equation (1), the model parameter vector w is only coupled with the labels linearly and hence a second derivative with respect to w makes all the terms disappear.
Since the TED optimization problem is non-convex and can easily get stuck in local optima, Yu et al. [21] proposed a convex relaxation of TED (Convex TED). The optimization problem of Convex TED is defined as follows:

(

min


=1

x - X

,

2)

2 +  =1

+  1

s.t.

x  X ,   ,   ×1,  0,

(3)

where the variables , = 1, . . . , , control the inclusion of examples in X into the training set X , the 1-norm  1 enforces the sparsity of , and denotes the th element of . According to [20, 21], TED and Convex TED tend to select examples representative of all the unlabeled data and hence exploit the distribution of the whole data space.
Since experimental design based methods do not use label information, we call them unsupervised active learning methods here. In the next subsection, we will present our supervised extension, SED, which can effectively utilize the available label information to select the most informative examples.

3.2 Supervised Experimental Design

Given a set of labeled data points (training set), we can learn a classifier from the data. In a typical active learning setting in which labeled data are scarce, may not be accurate enough and hence it is desirable to select some unlabeled data points for labeling to enlarge the training set. However, although is not accurate enough, it still carries some useful information about the data points. Let f be a vector of decision values on the candidate set X and ~f be the vector after taking the absolute value of each element of f . For example, in support vector machine (SVM), ~f indicates the uncertainty of the current classifier about the labels of the examples. The smaller the th element ~ of ~f is, the less certain is the classifier about the example. Intuitively speaking, the most informative examples should be those with the smallest ~ values.
Based on the above intuition and the formulation of TED, we can choose the most informative X by solving the following optimization problem,

min

[ tr X

(X X

+

I)-1 X

] +

~f

X

s.t.

X  X ,  = ,

(4)

where ~f is similar to ~f but is defined only on the active set X and is a user-defined parameter controlling the contribution of model uncertainty due to the current labeled data. In other words, controls the contribution of label information.
Since the optimization problem in Equation (4) is NPhard, non-convex and can easily get trapped in local minima, we borrow ideas from [21] to reformulate it in a convex form and define our SED problem as follows.

Definition 1. Supervised Experimental Design (SED)

(

min


=1

x - X

,

2)

2 +  =1

+ 1 1 + 2

~f

s.t.

x  X ,   ,   ×1,  0.

(5)

We can further prove that SED is a convex problem.

Theorem 1. SED is convex w.r.t. and { }.

Proof. Let the objective function of SED be = 1 +

(

2)

2, where 1 =  =1 x - X 2 +  =1

+ 1 1

and 2 = 2 ~f . Because ~f is constant, 2 is linear in .
Thus 2 is convex with respect to . Since 1 is also convex with respect to and { }3 and 1 + 2 is a convex combi-
nation of two convex functions 1 and 2, is thus convex with respect to and { }. This completes the proof.

3.3 Algorithm
It is convenient to find the local optimum of Problem (5), which is also the global optimum, by updating and { } iteratively. More specifically, we can find the analytical solution for updating one variable while fixing the other as follows:

=

1 1 + 2 ~ =1

2,

= 1, . . . , ,

(6)

= (diag( )-1 + X X )-1X x , = 1, . . . , . (7)

The proposed algorithm is summarized in Algorithm 1.

3.4 Complexity Analysis
The main computation of SED is to update and { }. The time complexity of updating (Equation (6)) is ( ) and that of updating { } (Equation (7)) is ( 3). Hence, the time complexity of one iteration is ( 3+ ). Though our algorithm converges very quickly in practice, it is interesting and worthwhile to explore more efficient techniques to solve the problem, and we leave it as future work.

4. EMPIRICAL ANALYSIS
We conduct several experiments to compare SED with some other related methods. We have the following questions in mind while designing and conducting the experiments:
1. How does SED perform when compared with other state-of-the-art active learning methods?
2. How effective is label information for experimental design?
3The proof can be found in [21].

301

Algorithm 1 Algorithm for SED

1: INPUT:

0 ­ set of labeled data points

0 ­ set of unlabeled data points

­ number of active learning iterations

­ number of examples selected in each iteration

2: for = 1 to do

3: Train classifier based on  -1. 4: Compute absolute decision values ~f .

5: Initialize { }.

6: repeat

7:

Fix { }, update using Equation (6).

8:

Fix , update { } using Equation (7).

9: until converge w.r.t. objective value of Problem (5).

10: Choose examples with the largest values into X -1 and request their labels.
11: Update    -1  X -1 and  -1  X -1.

12: end for

13: Train classifier based on  .

14: return

3. How does varying the size of the candidate set affect the performance of SED?
These questions are answered in separate subsections: question 1 in Section 4.3, question 2 in Section 4.4.1, and question 3 in Section 4.4.2.
4.1 Data Sets
We conduct experiments on two public benchmark data sets. The first one is a subset of the Newsgroups corpus [21], which consists of 3, 970 documents with TFIDF features of 8, 014 dimensions. Each document belongs to exactly one of the four categories: autos, motorcycles, baseball and hockey. The other one is the Reuters data set, which is a subset of the RCV1-v2 data set [12]. We randomly choose from the original data set 5, 000 documents with TFIDF features of 6, 451 dimensions. Each document belongs to at least one of the four categories: CCAT, ECAT, GCAT and MCAT. Some characteristics of the two data sets are summarized in Table 1 respectively.

Table 1: Characteristics of Data Sets

Data Sets Newsgroups
Reuters

Category
Autos Motorcycles
Baseball Hockey
CCAT ECAT GCAT MCAT

# of Documents
988 993 992 997
907 1, 259 1, 524 2, 337

# of Features 8, 014
6, 451

4.2 Experimental Settings and Metrics
In the experiments, we simply treat the multi-class/label classification problem as a set of binary classification problems by using the one-versus-all scheme, i.e., documents from the target category are labeled as positive examples and those from the other categories are labeled as negative examples. We use area under the ROC curve (AUC) as the performance measure to measure the overall classification

performance, because in our setting, each binary classification task is unbalanced (only about 25% of the documents in each Newsgroups data set and about 30% of the documents in each Reuters data set are positive). Note that a larger value of AUC indicates a better performance.
At each iteration of our experiments, an active learning method selects a set of = 5 unlabeled examples from the candidate set. The selected examples are then labeled and added to the training set . The classifier is then trained on the expanded training set and used to predict the class labels of all documents. An AUC score is then computed based on the predictions. In order to randomize the experiments as well as to reduce the computational cost, we restrict the candidate set to randomly cover only a fraction of all the unlabeled documents. Ten different candidate sets are generated for each experiment and the average AUC value, together with the standard deviation, is reported.
We compare SED with four popular active learning methods for text classification:
 Convex TED [21], which is a convex relaxation of TED.
 Sequential TED [20], which sequentially selects examples using TED.
 Margin, which chooses the examples closest to the class boundary. This method implements the basic idea of [18] but uses the squared loss instead of the hinge loss. We use this method because it performs much better than [18] in practice.
 Random Sampling, which randomly selects examples from the candidate set.
We note that all the methods use kernel ridge regression, which is essentially equivalent to least squares SVM (LS-SVM), as the base classifier. LS-SVM has been reported to give state-of-the-art performance in text classification tasks [22, 24]. Since no labeled data exists in the beginning of each experiment, we use Convex TED to select the first = 5 examples for SED and Margin.
4.3 Performance Evaluation

4.3.1 Comparison of Methods on Newsgroups Data

We first compare the five methods on the Newsgroups data set. For each method, we restrict the candidate set to cover 50% of the unlabeled data and set the parameters as
= 0.01, 1 = 0.1 max, 2 = 1.4 The AUC values averaged over four binary classification tasks are reported in Table 2, where each row corresponds to one iteration. We use boldface numbers to indicate the best results among the five methods. It is obvious that SED consistently outperforms the other methods. To evaluate how significant SED outperforms other methods, we have conducted paired t-tests [19] on the results of SED and the second best method, Convex TED. The p-value is 2.37 × 10-5, indicating that SED achieves a significantly better result. It is not surprising that Random Sampling performs the worst because the randomly selected examples may not provide much useful information to the classifier. We also note

4 1

max = max 




(x

x )2 is a necessary condi-

tion for the cardinality constraint  0  1. The reader is

referred to [21] for details.

302

Table 2: Comparison of Methods (in Average AUC) on Newsgroups Data



SED

Convex TED Sequential TED

Margin

Random Sampling

5 0.8854±0.0256 0.8854±0.0256 0.8195±0.0299 0.8854±0.0256 0.7138±0.0295

10 0.9179±0.0133 0.9057±0.0134 0.8501±0.0244 0.8800±0.0274 0.7703±0.0354

15 0.9327±0.0115 0.9186±0.0120 0.9023±0.0120 0.8914±0.0270 0.8006±0.0234

20 0.9456±0.0067 0.9244±0.0103 0.9219±0.0122 0.9027±0.0163 0.8261±0.0231

25 0.9512±0.0061 0.9361±0.0076 0.9304±0.0105 0.9115±0.0103 0.8460±0.0169

30 0.9546±0.0042 0.9407±0.0058 0.9362±0.0095 0.9171±0.0093 0.8639±0.0159

35 0.9573±0.0055 0.9446±0.0049 0.9406±0.0081 0.9212±0.0090 0.8782±0.0164

40 0.9609±0.0041 0.9471±0.0054 0.9434±0.0073 0.9262±0.0098 0.8904±0.0138

45 0.9631±0.0051 0.9493±0.0051 0.9460±0.0062 0.9321±0.0081 0.9009±0.0132

50 0.9655±0.0043 0.9514±0.0046 0.9486±0.0068 0.9363±0.0074 0.9076±0.0126

AUC AUC

that the methods based on experimental design, i.e., Convex TED and Sequential TED, perform better than Margin. This indicates that exploiting the distribution of unlabeled data can provide more useful information than selecting only examples near the class boundary. We also observe that in the beginning of the learning procedure, examples selected by Margin actually degrade the performance. This is because the labeled data are scarce at that time and hence the class boundary learned by training on the labeled data is not accurate enough and hence may be misleading for document selection.

0.98

0.96

0.94

0.92

0.9

0.88

0.86 0.84
5

SED Convex TED Margin
10 15 20 25 30 35 40 45 50 Number of Training Examples

Figure 1: Learning Curves on Newsgroups Data

We plot the learning curves of SED, Convex TED and Margin in Figure 1. As we can see, SED performs better than its two counterparts by a large margin. This observation validates that considering label information and the distribution of unlabeled data together can provide more useful information for active selection than only considering either of them.
To further understand the properties of SED, we plot the learning curves of SED, Convex TED and Margin for four binary classification tasks in Figure 2. For three categories, i.e., autos, baseball and hockey, SED consistently outperforms the second best, Convex TED, by a large margin. For the motorcycles category, SED and Convex TED perform similarly. We note that Margin is consistently the worst with the largest variance for all tasks. We conjecture that Margin always selects the outliers, which stay close to the class boundary but are not useful to the learner. On the other hand, SED and TED can exploit the distribution of unlabeled data and hence have a smaller chance to select the outliers.

4.3.2 Comparison of Methods on Reuters Data
We now compare the five methods on the Reuters data set. Each candidate set covers 20% of the unlabeled documents. The parameters are set as = 0.01, 1 = 0.1 max, 2 = 10.
The AUC values averaged over the four tasks are reported in Table 3. Again the best results are shown in bold. As in the Newsgroups data set, SED significantly outperforms Convex TED (the p-value of paired t-test is 2.26 × 10-5), validating the effectiveness of label information. It is interesting to find that Margin performs better than Convex TED. This can be attributed to two reasons. First, the data are very balanced in this data set and Margin selects the most discriminative examples without querying the outliers. Second, the representative examples selected by TED might not be as helpful as those discriminative ones. However, SED can take advantage of both criteria and always performs the best, especially in the early stage.

0.96

0.92

0.88

0.84

0.8

0.76

0.72

SED

Convex TED

Margin

0.68 5 10 15 20 25 30 35 40 45 50

Number of Training Examples

Figure 3: Learning Curves on Reuters Data

The learning curves of SED, Convex TED and Margin are plotted in Figure 3. From the figure, SED outperforms Convex TED and Margin especially in the early stage. This observation again validates the contribution of label information to experimental design.
We also plot the learning curves of SED, Convex TED and Margin for the four tasks in Figure 4. SED again outperforms its counterpart, Convex TED, for all tasks. It is interesting to observe that when the data are rather balanced, such as in MCAT, Margin performs better than Convex TED. This is actually possible, because when the data are balanced, discriminative examples near the class boundary will provide the most useful information to the learner. Note that the effectiveness of SED can be further improved if we put more weight on the label information for this task. Nevertheless, we leave the issue of automatically learning the weight of label information to our future research.

303

AUC

0.98

0.96

0.94

0.92

0.9

0.88

SED

Convex TED

Margin

0.86 5 10 15 20 25 30 35 40 45 50

Number of Training Examples

(a) Autos

AUC

0.98

0.96 0.94

0.92

0.9 0.88 0.86

0.84

0.82

SED

0.8

Convex TED

Margin

0.78 5 10 15 20 25 30 35 40 45 50

Number of Training Examples

(b) Motorcycles

AUC

0.98

0.96 0.94

0.92

0.9 0.88 0.86

0.84

0.82

SED

0.8

Convex TED

Margin

0.78 5 10 15 20 25 30 35 40 45 50

Number of Training Examples

(c) Baseball

AUC

0.98

0.96

0.94

0.92

0.9

SED

Convex TED

Margin

0.88 5 10 15 20 25 30 35 40 45 50

Number of Training Examples

(d) Hockey

Figure 2: Learning Curves for Four Binary Classification Tasks on Newsgroups Data

Table 3: Comparison of Methods (in Average AUC) on Reuters Data



SED

Convex TED Sequential TED

Margin

Random Sampling

5 0.7347±0.0416 0.7347±0.0416 0.7910±0.0250 0.7347±0.0401 0.6932±0.0669

10 0.8215±0.0201 0.8039±0.0267 0.8137±0.0232 0.7942±0.0264 0.7579±0.0434

15 0.8565±0.0216 0.8327±0.0190 0.8397±0.0135 0.8341±0.0203 0.7945±0.0246

20 0.8720±0.0117 0.8416±0.0154 0.8551±0.0117 0.8538±0.0215 0.8235±0.0227

25 0.8842±0.0066 0.8550±0.0126 0.8668±0.0131 0.8646±0.0233 0.8383±0.0190

30 0.8902±0.0078 0.8645±0.0106 0.8725±0.0137 0.8775±0.0168 0.8536±0.0226

35 0.8963±0.0069 0.8709±0.0091 0.8807±0.0139 0.8897±0.0114 0.8684±0.0162

40 0.8992±0.0088 0.8774±0.0085 0.8850±0.0118 0.8993±0.0129 0.8749±0.0130

45 0.9024±0.0085 0.8789±0.0080 0.8898±0.0126 0.9042±0.0117 0.8864±0.0108

50 0.9048±0.0087 0.8856±0.0068 0.8932±0.0120 0.9093±0.0101 0.8948±0.0096

4.4 Discussions
4.4.1 Effectiveness of Label Information
As we have discussed in Section 3, the contribution of label information is controlled by the parameter 2. If 2 = 0, we do not use the label information at all and hence our method degenerates to Convex TED; as 2 increases, we put larger weight on the label information. To evaluate the contribution of label information, we carry out a set of experiments by varying the value of 2 in the autos task of the Newsgroups data set. As before, each candidate set covers 50% of the unlabeled documents and the parameters are set to be = 0.01, 1 = 0.1 max.
The learning curves of SED with different 2 values are plotted in Figure 5. As we can see, using a large enough
2 value, e.g. 2 = 1, can greatly speed up the learning procedure, while using small values, e.g. 2 = [0, 0.1], will not improve much.
This observation validates the effectiveness of label information for experimental design. It should be noted that if

2 is too large, e.g. 2 = 10 or 100, the learning rate will be slower than that with moderate 2 values in the early stage of learning.
This is because the training set is too small in this stage and the class boundary learned is not very accurate, so adopting too large 2 values will mislead example selection by querying the outliers. The risk can be mitigated as the size of the training set increases. We also note that choosing 2 = 1 will achieve the best performance not only in the early stage but also in the later stage.
Similar experiments are conducted for the MCAT task of the Reuters data set. As before, the random candidate sets cover 20% of the unlabeled documents and the parameters are set to be = 0.01, 1 = 0.1 max. The learning curves of SED with different 2 values are plotted in Figure 6.
From Figure 6, it is interesting to observe that, different from what we have found in Figure 5, adopting a larger value of 2 will always improve the active learning procedure. This is because in the MCAT task, about 50% of the documents are positive, but in the autos task, only 25% of

304

AUC

0.95

0.9

0.85

0.8

0.75

0.7

0.65

SED Convex TED Margin

5 10 15 20 25 30 35 40 45 50 Number of Training Examples

(a) CCAT

AUC

0.95

0.9

0.85

0.8

0.75

0.7 0.65
5

SED Convex TED Margin
10 15 20 25 30 35 40 45 50 Number of Training Examples

(b) ECAT

0.95 0.9
0.85

AUC

0.9 0.85
0.8 0.75

AUC

0.8

0.7

0.75

SED Convex TED Margin

5 10 15 20 25 30 35 40 45 50 Number of Training Examples

(c) GCAT

0.65 5

SED Convex TED Margin
10 15 20 25 30 35 40 45 50 Number of Training Examples

(d) MCAT

Figure 4: Learning Curves for Four Binary Classification Tasks on Reuters Data

AUC

0.98

0.97

0.96

0.95

0.94

0.93

0.92

0.91

0

0.01

0.9

0.1

1

0.89

10

100 0.88
5 10 15 20 25 30 35 40 45 50 Number of Training Examples

Figure 5: Effect of Varying 2 on Autos Task

the documents are positive. Since the data distribution is more balanced in the MCAT task, adopting a larger value of
2 will always choose those discriminative examples without taking the risk of querying the outliers.

4.4.2 Effect of Candidate Set Size
In this section, we conduct several experiments to investigate the effect of the candidate set size by randomly choosing 20%, 40%, 60% and 80% of the unlabeled documents to form different candidate sets. For the autos task, the parameters are set to be = 0.01, 1 = 0.1 max, 2 = 1, and the learning curves for different candidate set sizes are plotted in Figure 7. For the MCAT task, the parameters are set to be = 0.01, 1 = 0.1 max, 2 = 10, and the learning curves are plotted in Figure 8.
As we can see from Figure 7, using a larger candidate set will greatly speed up the learning procedure. We note that as the size of the candidate set increases, the performance gap between the learned classifiers becomes smaller. How-

0.95

0.9

0.85

0.8

AUC

0.75

0

0.7

0.01

0.1

0.65

1

10

100

5 10 15 20 25 30 35 40 45 50 Number of Training Examples

Figure 6: Effect of Varying 2 on MCAT Task

ever, in Figure 8, the learning curves are not so sensitive to the candidate set size as those in Figure 7. This can again be explained by the distribution of data. The more balanced the data are, the less sensitive is the method to the candidate set size. We should note that the candidate set size has great impact on the optimization problem. Specifically, the larger the candidate set is, the longer time we need to solve the problem. Thus, in practice, we should maintain a tradeoff between performance and the time cost and use a candidate set of a reasonable size.

5. CONCLUSION
In this paper, we have proposed a novel active learning method, SED, to seamlessly incorporate label information into the document selection procedure of experimental design. To the best of our knowledge, SED is the first work that uses label information to improve experimental design. One promising property of SED is that it can effectively use label information and the distribution of unlabeled data in a

305

0.98

0.96

0.94

AUC

0.92

0.9

20%

0.88

40%

60%

80% 0.86
5 10 15 20 25 30 35 40 45 50

Number of Training Examples

Figure 7: Effect of Candidate Set Size on Autos Task

0.95

0.9

0.85

0.8

AUC

0.75

0.7

20%

0.65

40%

60%

80%

5 10 15 20 25 30 35 40 45 50 Number of Training Examples

Figure 8: Effect of Candidate Set Size on MCAT Task

unified framework. In particular, SED can greatly speed up the learning procedure when the distribution of unlabeled data is balanced, while existing methods based on experimental design always perform badly in this case. Moreover, SED can greatly outperform margin-based active learning when the distribution of unlabeled data is unbalanced. As another promising property, SED is convex and thus global optimality is guaranteed. Experiments conducted on two text corpora demonstrate that SED outperforms state-ofthe-art active learning algorithms, such as TED and marginbased methods, which take into consideration only partial information.
One of our future research directions is to automatically learn from data the contribution of label information, i.e.
2. Another possible research direction is to apply SED to other information retrieval applications.

6. ACKNOWLEDGMENTS
The authors thank Wu-Jun Li and Ning Zhu for some helpful discussions. This research has been supported by General Research Fund 622209 from the Research Grants Council of Hong Kong.

7. REFERENCES
[1] D. Angluin. Queries and concept learning. Mach. Learn., 2(4):319­342, 1988.
[2] A. Atkinson and A. Donev. Optimum Experimental Designs. Oxford University Press, USA, 1992.
[3] D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Mach. Learn., 15(2):201­221, 1994.

[4] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. J. Artif. Intell. Res., 4:129­145, 1995.
[5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4:933­969, 2003.
[6] Y. Guo and R. Greiner. Optimistic active learning using mutual information. In IJCAI, 2007.
[7] Y. Guo and D. Schuurmans. Discriminative batch mode active learning. In NIPS, 2007.
[8] X. He, W. Min, D. Cai, and K. Zhou. Laplacian optimal design for image retrieval. In SIGIR, 2007.
[9] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text categorization by batch mode active learning. In WWW, 2006.
[10] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode active learning and its application to medical image classification. In ICML, 2006.
[11] D. D. Lewis and W. A. Gale. A sequential algorithm for training text classifiers. In SIGIR, 1994.
[12] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361­397, 2004.
[13] D. MacKay. Information-based objective functions for active data selection. Neural Comput., 4(4):590­604, 1992.
[14] A. McCallum and K. Nigam. Employing EM and pool-based active learning for text classification. In ICML, 1998.
[15] H. T. Nguyen and A. Smeulders. Active learning using pre-clustering. In ICML, 2004.
[16] N. Roy and A. McCallum. Toward optimal active learning through sampling estimation of error reduction. In ICML, 2001.
[17] H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In COLT, 1992.
[18] S. Tong and D. Koller. Support vector machine active learning with applications to text classification. J. Mach. Learn. Res., 2:45­66, 2002.
[19] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, 1999.
[20] K. Yu, J. Bi, and V. Tresp. Active learning via transductive experimental design. In ICML, 2006.
[21] K. Yu, S. Zhu, W. Xu, and Y. Gong. Non-greedy active learning for text categorization using convex transductive experimental design. In SIGIR, 2008.
[22] J. Zhang and Y. Yang. Robustness of regularized linear classification methods in text categorization. In SIGIR, 2003.
[23] L. Zhang, C. Chen, W. Chen, J. Bu, D. Cai, and X. He. Convex experimental design using manifold structure for image retrieval. In ACM MM, 2009.
[24] T. Zhang and F. J. Oles. Text categorization based on regularized linear classification methods. Inform. Retrieval, 4(1):5­31, 2001.
[25] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using gaussian fields and harmonic functions. In ICML Workshop, 2003.

306

Temporally-Aware Algorithms for Document Classification

Thiago Salles
Fed. Univ. of Minas Gerais Computer Science Dep. Belo Horizonte, Brazil
tsalles@dcc.ufmg.br
Fernando Mourão
Fed. Univ. of Minas Gerais Computer Science Dep. Belo Horizonte, Brazil
fhmourao@dcc.ufmg.br

Leonardo Rocha
Fed. Univ. São João Del Rei Computer Science Dep.
of São João Del Rei, Brazil
lcrocha@ufsj.edu.br
Wagner Meira Jr.
Fed. Univ. of Minas Gerais Computer Science Dep. Belo Horizonte, Brazil
meira@dcc.ufmg.br

Gisele L. Pappa
Fed. Univ. of Minas Gerais Computer Science Dep. Belo Horizonte, Brazil
glpappa@dcc.ufmg.br
Marcos Gonçalves
Fed. Univ. of Minas Gerais Computer Science Dep. Belo Horizonte, Brazil
mgoncalv@dcc.ufmg.br

ABSTRACT
Automatic Document Classification (ADC) is still one of the major information retrieval problems. It usually employs a supervised learning strategy, where we first build a classification model using pre-classified documents and then use this model to classify unseen documents. The majority of supervised algorithms consider that all documents provide equally important information. However, in practice, a document may be considered more or less important to build the classification model according to several factors, such as its timeliness, the venue where it was published in, its authors, among others. In this paper, we are particularly concerned with the impact that temporal effects may have on ADC and how to minimize such impact. In order to deal with these effects, we introduce a temporal weighting function (TWF) and propose a methodology to determine it for document collections. We applied the proposed methodology to ACM-DL and Medline and found that the TWF of both follows a lognormal. We then extend three ADC algorithms (namely kNN, Rocchio and Na¨ive Bayes) to incorporate the TWF. Experiments showed that the temporallyaware classifiers achieved significant gains, outperforming (or at least matching) state-of-the-art algorithms.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.5.4 [Applications]: Text processing;
General Terms
Algorithms, Experimentation
This work was partially supported by CNPq, CAPES, FINEP, Fapemig, and INWEB.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
Classification and Clustering, Text Mining
1. INTRODUCTION
Text classification is still one of the major information retrieval problems, and developing robust and accurate classification models continues to be a relevant demand, as a consequence of the increasing complexity and scale of current application scenarios, such as the Web. The task of Automatic Document Classification (ADC) aims to create models that associate documents with semantically meaningful categories, and these models are key for building spam filters and topic directories, identifying documents writing style, creating digital libraries, and guiding a user's search on the World Wide Web.
ADC usually follows a supervised learning strategy, in which a classification model is built using some training (preclassified) documents and later employed to classify a new set of unseen documents. The majority of supervised algorithms consider that all documents provide equally important information. However, in practice, a document may be considered more or less important to build the classification model according to several factors, such as its timeliness, the venue where it was published in, its authors, among others.
In this work we are particularly concerned with the impact that temporal effects may have on ADC and how to minimize such impact. Consider, for instance, the terms pheromone and ant colony. Before the 1990s, they referred exclusively to documents in the area of Natural Sciences. However, after the introduction of the technique of Ant Colony Optimization in the area of Artificial Intelligence, these terms became relevant for classifying Computer Science documents too. Previous work has demonstrated that temporal effects, such as the variation of the strenght of term-class relationship over time, may have a significant impact on ADC, and strategies for assessing these effects and their impact on ADC have already been devised [19].
In general, methods proposed to deal with temporal effects are based on three main approaches: instance selection, instance weighting, and ensembles. Instance selection [20] uses heuristics to decide which instances should be used (or the time intervals that contain those instances) to create a classification model. However, tuning these heuristics to select the most relevant documents is a challenge, since we

307

may easily gather too many or too few documents. Methods based on instance weighting [11] may ameliorate this problem 1. However, this strategy raises the additional challenge of determining the weighting functions and their parameters, which are collection-dependent and usually performed in ad-hoc way. Finally, approaches based on ensembles, that correspond to the combination of various classification models generated from different classification algorithms, present the challenge of how to manage, efficiently, several models simultaneously [7].
This paper proposes a strategy to incorporate temporal models to document classifiers, aiming to address the two main drawbacks of instance selection and instance weighting approaches. Our strategy is based on the evolution of the term-class relationship over time, captured by a metric of dominance. We start by determining a temporal weighting function for a collection according to its characteristics. We found that this function follows a lognormal distribution for the datasets we used.
The next step is to incorporate the temporal weighting function to ADC algorithms and we propose two strategies that follow a lazy classification approach. In both strategies, the weights assigned to each example depend on the notion of a temporal distance , defined as the difference between the time of creation p of a training example and a reference time point pr. The first strategy, named temporal weighting on documents, weights training instances according to . The second strategy, called temporal weighting on scores, is based on an ensemble of classifiers, one for each pair class c,time point p . In this case, the scores (e.g., similarities, probabilities) returned by the respective classifiers for each pair c, p are weighted according to . The combined weighted scores are then used to take the final classification decision. We specifically show how these two strategies are implemented in three traditional ADC algorithms, namely, Rocchio, k Nearest Neighbors (KNN), and Na¨ive Bayes.
We evaluated our strategies using two actual digital libraries that span for decades, ACM-DL and MedLine, and achieved significant improvements on classification effectiveness for all classifiers. For instance, the temporal-aware version of Na¨ive Bayes outperformed by up to 10% the state-ofthe-art classifier (SVM), while presenting an execution time up to hundreds of times faster.
2. RELATED WORK
Although document classification is a widely studied subject, the analysis of temporal aspects in this class of algorithms is quite recent ­ it has been studied just in the last decade. As previously mentioned, strategies to deal with these effects involve one or more of three approaches, namely: instance selection, instance weighting, and ensembles. Here we review the most relevant works of two broad areas where there has been significant efforts in terms of temporal effects on classification: adaptive document classification and concept drift.
Adaptive Document Classification [4] encompasses a set of techniques related to temporal aspects with the goal of improving the effectiveness of document classifiers through their incremental and efficient adaptation. Adaptive Do-
1The first strategy may be seem as an instance weighting strategy with binary weights.

cument Classification brings three main challenges to text mining [17]. The first one, and most relevant to this research, is the notion of context and how it may be exploited towards better classification models. Previous research in document classification identified two essential forms of context: neighbor terms that are close to a certain keyword [14] and terms that indicate the scope and semantics of the document [2]. The second challenge is creating the models incrementally [10]. The third challenge has to do with the computational efficiency of the document classifiers. Methods for Adaptive Document Classification usually follow an instance selection approach, as they select semantic contexts based on, for instance, the co-occurrence of terms, not taking into account all documents in the training set.
Concept or topic drift [22] comprises another relevant set of efforts to deal with temporal effects in classification. To deal with concept drift, a prevailing approach in the literature is to completely retrain the classifier according to a sliding window. This involves instance selection and instance weighting techniques [12, 11, 23, 15]. The method presented in [12], for instance, maintains a window with documents sufficiently "close" to the current target concept and automatically adjusts the window size so that the estimated generalization error is minimized. In [11], the methods presented either maintain an adaptive time window on the training data, select representative training examples, or weight the training examples. In [23] the authors describe a set of algorithms that react to concept drift in a flexible way and can take advantage of situations where contexts reappear. The main idea of these algorithms is to keep only a window of currently trusted examples and hypothesis, and store concept descriptions in order to reuse them if a previous context reappears. Unlike previous works, which use a single window to determine drift in the data, in [15] the authors present a method that uses three windows of different sizes to estimate the change in the data. While algorithms that use a window of fixed size impose hard constraints over drift patterns, those that use heuristics to adjust the window size to the current extent of concept drift often involve lots of parameters to be calibrated. The approach proposed in this paper relies on statistical properties of the collection to assess the temporal effects, solving such drawbacks, while promoting a high quality classification.
Other common approach to deal with concept drift focuses on the combination of various classification models generated from different algorithms (ensembles) for classification, pruning or adapting the weights according to recent data [21, 13, 7]. In [21], the authors propose a boosting-like method to train a classifier ensemble from data streams. It naturally adapts to concept drift and allows to quantify the drift in terms of its base learners. The algorithm was shown to outperform learning algorithms that ignore concept drift. In this same direction, Kolter et al. [13] present a technique that maintains an ensemble of base learners, predicts instance classes using a weighted-majority vote of these "experts", and dynamically creates and deletes experts in response to changes in performance. In [7], a method that builds an ensemble of classifiers using Genetic Programming (GP) to inductively generate decision trees is presented. However, how to manage, efficiently, several models simultaneously remains a challenge. To address such drawback, we propose an approach based on the combination of vari-

308

ous classification models, but with a simpler way to manage them.
Another approach that can be easily compared to ours, and is based on instance selection, is the one proposed in [20]. In [20], the authors introduce the concept of temporal context, defined as a subset of the documents collection that minimizes the impact of temporal effects in classifiers' performance. An algorithm named Chronos was proposed to identify these contexts based on the stability of the terms in the training set. The temporal contexts were then used to sample the training documents for the classification process. Hence, training documents that were considered to be outside the temporal context were discarded by the classifier.
In contrast with the aforementioned works, here we propose an approach to classify documents in scenarios where we may have information about both the past and the future, and this information may change over time. It should be noticed, however, that our approach may be easily adapted for scenarios where we only have past information, such as Adaptive Document Classification and Concept Drift. Moreover, we address the drawbacks of which instances to select by approximating a temporal weighting function using a lognormal distribution, and may easily tune its parameters using statistical methods.
3. TEMPORAL WEIGHTING FUNCTION
As mentioned before, the potential impact that certain temporal effects have on term-class relationships may have a great influence on the results of the classification process, as showed in [19]. Thus, incorporating information about these changes into the classification process has the potential to improve its effectiveness.
We address this issue through a temporal weighting function (TWF) that quantifies the influence of a training document while classifying a test document, as a function of the temporal distance between their creation times. We distinguish two major steps in determining such function: its expression and its parameters. The expression is usually harder to determine, since it may express the generative process behind the function, while the parameters are usually obtained using approximation strategies.
Intuitively, given a test document to be classified, the TWF must set higher weights to training documents that are more similar to that test document w.r.t. the strength of term-class relationships. One metric that expresses such strength is the dominance [20], since the more exclusive a term is to a given predefined class, the stronger this relationship. Dominance can be formally defined as:
Dominance(t, c) = PNtc , c Ntc
where Ntc stands for the number of documents in class c that contain term t.
For ease of understanding, before we continue the discussion about the temporal weighting function, we describe the two document collections for which we want to determine the functions: ACM Digital Library (ACM-DL) and the MedLine. The ACM-DL has 24.897 documents containing articles related to Computer Science created between the years of 1980 and 2002. We considered only the first level of the taxonomy adopted by ACM, including 11 categories, which did not vary during this period of time. The second one is

derived from the MedLine collection, and has 861.454 documents, classified into 7 distinct classes related to Medicine and created between the years of 1970 and 1985. In both collections, each document is assigned to a single class.
We start by defining the temporal granularity of the weighting function, which should be the minimum time interval between relevant changes in the collection (e.g., days, weeks, or years). Since both collections contain scientific documents, it is intuitive that a year granularity is representative, once documents are usually published yearly (scientific conferences are usually annual).
The simplest approach would be to use a pulse function at temporal distance 0, that is, the pulse magnitude is proportional to the term dominance associated with the training documents produced in the same year of the test document. However, as pointed by [19], considering a larger time interval instead of a single time point is better, since the influence decreases with the increase of the temporal distance. We then need to determine the time period that must be considered, which we call stability period. Notice that each term may present a different stability period for each year when it occurred in the collection. We first determine the stability period for each term and then combine them, as follows.
One approach for the first step is presented in [20], where a stability period St,r of a term t, considering the reference time point pr in which the test document was created, consists of the largest continuous period of time, starting from pr and growing both to the past and the future, where Dominance(t, c) >  (for some predefined  and any class c). In the case of the collections ACM-DL and Medline, we investigated different values for  when computing stability periods and, as they lead to similar results, we adopted  = 50%, ensuring that the terms will have a high degree of exclusivity with some class.
We then combine the stability periods St,r for each term t and each reference time point pr in the collection. A difficulty in this case is related to the fact that a term may present different stability periods for different reference years. In order to avoid this problem, we mapped all the time points in a stability period to temporal distances, where the reference year is considered as distance 0. For instance, a term t1 may have different stability periods when considering the years 1989 or 2000 as a reference. More specifically, if the stability period of t1 is {1999,2000,2001} regarding pr = 2000, and {1988,1989,1990} regarding pr = 1989, these periods would be both mapped to {-1,0,1}. Considering St as the set of temporal distances that occur on the stability periods of term t (considering all reference moments r, then St = {  pn - pr|rpn  St,r}. Making the stability periods easily comparable is important because our real interest is to know what kind of distribution this temporal distances follow w.r.t. different terms.
The next step is to determine the function expression and, towards this goal, we considered the stability period of each term as a random variable (RV), where the occurrence of each possible temporal distance in its stability period is an event. More formally, as Table 1 shows, we are interested in the frequencies of the temporal distances 1 to n, for terms t1 to tk. An interesting property that we may test is whether these RV's are independent. This hypothesis can be corroborated by the Fisher's Exact Test to assess the independence of each RVi and RVj, i = j [3], where, as

309

1 2

t1 f11 f21

t2 f12 f22

... ... ...

tk f1k f2k

PkD Pik=1
i=1

f1i f2i

... n

fn1

fn2

...

fnk

Pk
i=1

fni

Table 1: Temporal distances versus terms

mentioned, each RV represents the occurrence of a temporal

distance  for a term t.

We applied this test to both ACM-DL and Medline and

obtained a p-value of 0.99 through a Monte Carlo simulation,

which allows us to state that the random variables consid-

ered are indeed independent. Thus, the observed variability

of occurrences of  for different terms is a result of indepen-

dent effects [16]. However, it is still not clear whether the ef-

fects responsible for the observed variability can be additive

(leading to a normal distribution) or multiplicative (leading

to a lognormal distribution). We then apply a statistical

normality test. According to D'Agostino's D-Statistic Test

of Normality [6], with 0.01 significance level, we found that

the lognormal distribution best fits both the ACM-DL and

Medline collections, as presented in Table 3.

Consider that the RV D related to the occurrences of ,

which represents the distribution of each i over all terms

t, is lognormally distributed if lnD is normally distributed.

More generally, since i are RV's under the independence

atrsaslumLipmtiiotnTwhietohrefimni,telnmDean=anPdni=va1rlinanicew, itlhl eans,ybmyptthoetiCcaelnlyapproach a normal distribution and, by definition, converges

to a lognormal distribution [5]. For a lognormal distribution,

the asymptotically most efficient method for estimating its

associated parameters relies on a log-transformation [16].

Using a Maximum Likelihood method, we estimated those

parameters for both collections, and then back-transformed

them, as shown in Table 2. We considered a 3-parameter

gaussian function, F

=

a e . i

-

(x-bi 2c2i

)2

The parameter ai is

the height of the curve's peak, bi is the position of the cen-

tre of the peak, and ci controls the width of the curve. The

last one, also called the shape parameter, reflects the nature

of the variations of term-class relationships over time. Since

abrupt or smooth variations lead to small or greater stability

periods, respectively, the shape of the distribution changes

accordingly, being a matter of parameter estimation to cap-

ture such distinct natures. We performed two curve fitting

procedures, considering a single gaussian F and a mixture

of two gaussians, given by G = G1 + G2, where each Gi de-

notes a gaussian function. The last one was the model that

best fitted D, and its parameters are presented in Table 2, along with the goodness of fitting measure Adjusted-R2. The Adjusted-R2 measure denotes the percentage of variance ex-

plained by the model and, for both collections, the obtained

model explains 99% of such variance.

The greater the frequency of  on stability periods, the

more suitable training documents created in  are to build

an accurate classification model, making the modelling of the

Temporal Weighting Function as a lognormal distribution an

effective strategy. To account for the problem faced when

the scale of the score is not compatible with the algorithm

input, we include a scaling factor   R, that is algorithm

specific and will be defined in Section 5.

Figure 1 shows the distribution of temporal scores, when

Param.
a1 b1 c1 a2 b2 c2 Adj. R2

ACM-DL Value Conf. Interval 0.325 (0.288, 0.362) -0.028 (-0.309, 0.253) 3.636 (3.117, 4.154) 0.616 (0.589, 0.643) 0.037 (-0.395, 0.470) 20.14 (20.93, 23.35)
0.990

Value 0.089 -0.013 1.635 0.901 0.092 24.51

Medline Conf. Interval (0.066, 0.113) (-0.349, 0.324) (1.099, 2.17) (0.891, 0.911) (-0.130, 0.314) (23.71, 25.3)

0.992

Table 2: Estimated parameters for both collections,

with 99% confidence intervals.

 = 1, for each possible temporal distance between the creation time of test document d and the training documents for both the ACM-DL and the Medline collections.

Data Original Log-Transformed

ACM-DL 4.497e-6
0.2144

Medline 0.002762 0.6802

Table 3: D'Agostino's D-Statistic Test of Normality.

Bold-face for tests that we can not reject the null

hypothesis of normality.

(a) ACM-DL Collection
(b) MedLine Collection Figure 1: Fitted temporal weighting function with log-transformed data.
4. TEMPORALLY-AWARE ADC
This section shows how three well-known text classifiers, namely Rocchio, KNN and Na¨ive Bayes [18], can be modified to take into account the temporal weighting function defined in Section 3. The three algorithms are modified following two strategies: temporal weighting on documents and temporal weighting on scores, as detailed below.
4.1 Temporal Weighting on Documents
The temporal weighting on documents strategy weights each training document by the temporal weighting function according to its temporal distance to the test document d , as detailed next.
The strategy to incorporate the weight of each training document to a given classifier depends inherently on the characteristics of the classification algorithm being modified. For example, while Rocchio and KNN classify new instances

310

based on a distance metric, Na¨ive Bayes is a probabilistic classifier that assigns to a test document the most probable class that would have generated d , adopting some na¨ive assumptions such as positional and conditional independence of terms.
In the case of distance-based classifiers, the temporal weighting function can be easily applied when calculating the distance between the training and test documents, by weighting each training document (T F -IDF vector) by its associated temporal weight. In the case of the Na¨ive Bayes, the temporal function can be used to weight the impact of each training example in both the a priori and conditional probabilities, in order to generate a more accurate a posteriori probability.
Rocchio Rocchio is an eager classifier that uses the centroid of a class to find boundaries between classes. As an eager classifier, Rocchio does not require any information from d to create a classification model. Hence, we will have to adapt it to become a lazy classifier when using the temporal weighting function, since the weights depends on the creation time of a test document.
The centroid of a class is defined as the average value of all its training examples. When classifying a new document d , Rocchio associates it to the class represented by the centroid closest to d . In order to make Rocchio a lazy classifier, we have to change the separation boundaries of classes according to the temporal weights produced by our function.
Hence, it needs to calculate each Rocchio's class centroid based on the creation time pr of a test document d . Consider the set of training documents d of class c. This set can be partitioned into subgroups of documents created at the same temporal distance  from pr, i.e., subgroups of documents created at a temporal distance of 1 or -1 , 2 or -2, and so on. The centroid - c for class c is defined by weighting the documents vector representations with the score produced by the temporal function T W F (), obtained using the temporal distance  between the creation time point of d and d . Thus, a centroid - c is given by:

- c =

1X Dc dDc

!

X

- d

·

T

W

F

()

,



where Dc is the number of documents in class c,  is the

set of all possible temporal distances between the training

documents

and

the

test

document

d,

and

- d



Dc

is

a

training document with temporal distance  from d .

This approach redefines the centroid's coordinates in the

vectorial space considering document's representativeness on

class c w.r.t. the reference time point pr. Both training and

classification procedures are presented in Algorithm 1.

Algorithm 1 Rocchio-TWF-Doc: Rocchio with Temporal Weighting on Documents

1: function Train(C, D)

2:

Dc,  {d : d, c,   D}

!

3: 4:

 - c  return

1P {D -c cd:cDc

P - d

C}

·

T W F ()

5: 6:

function Classify({c : c  return arg maxc cos( - c,

C},  - d)

d

)

KNN KNN is a lazy classifier that assigns to a test document d the majority class among those of its k nearest

neighbor documents in the vector space. Determining the test document's class from the k nearest neighbors training documents may not be ideal in the presence of term-class relationships that vary considerably over time. To deal with it, we apply the proposed temporal weighting function during the computation of similarities among d and the documents in the training set, aiming to select the closest documents, in terms of both similarity and temporality.
Let s be the cosine similarity between a training document d and d . If d is similar to d but is temporally distant, then it is moved away from d , reducing the probability of being among the k nearest documents of d . Let T W F () be the temporal weight associated with the temporal distance between the time of creation of documents d and d . Then, the documents' similarity is given by:

sim(d, d )  cos(d, d ) · T W F ().
Both training and classification procedures are presented in Algorithm 2.

Algorithm 2 KNN-TWF-Doc: KNN with Temporal Weighting on Documents

1: function getKNearestNeighbors(D, d , k)

2: for each d  D do

3:

sim(d, d )  cos(d, d ) · T W F ()

4:

priorityQueue.insert(sim, d)

5: return priorityQueue.first(k)

6: function Classify(D, d , k)

7:

knn  getKNearePstNeighbors (d , D, k)

8: return {arg maxc knn.nextDoc(c)}

c

Na¨ive Bayes Na¨ive Bayes is a probabilistic learning method that aims to infer a model for each class, assigning to d the class associated to the most probable model that would have generated d . Here we adapt the Multinomial Na¨ive Bayes approach [18], since it is widely used for the probabilistic text classification. Similarly to the previously defined "temporal weighting on documents" approaches, here we apply the temporal weighting function on the information used by the learning method, namely the relative frequencies of documents and terms, as follows:

P (d

|c) =

·

P Pp(Ncp
p(Np

· T W F ()) · T W F ())

Y ·
td

P

P

Pp(ftcp · T (ft cp ·

W F ()) ,
T W F ())

p t V

where  denotes a normalizing factor, Ncp is the number of training documents of D assigned to class c and created at time point p, Np is the number of training documents created at time point p, ftcp stands for the frequency of occurrence of term t in training documents of class c that were created on time point p and, finally,  denotes the temporal distance between time point p and the creation time of d .
The main goal of this strategy is to reduce the impact that temporally distant information have when estimating a
posteriori probabilities. Algorithm 3 presents this strategy.

4.2 Temporal Weighting on Scores
A more sophisticated approach to exploit the temporal weighting function considers the "scores" produced by the traditional classifiers, as listed in Algorithm 4. By score we mean: (i) the smallest distance from the test document d to a class centroid for Rocchio; (ii) the smallest sum of the

311

Algorithm 3 Na¨ive Bayes TWF-Doc: Na¨ive Bayes with Temporal Weighting on Documents

1: function ClassifyP(D, d )

2: 3:

aP riori[c]  termC ond[c]

Pp (Ncp ·T W F ())



p (Np ·T
Q
td

WF
P p

()) P P p (ftcp ·T W F
t V (ft cp ·T

()) WF

())

4: return {arg maxc  · aP riori[c] · termCond[c]}

distances of the K-nearest neighbors to document d assigned to class c in the case of KNN; or (iii) the probability to generate d with the model associated to some class c for Na¨ive Bayes. From now on, we refer to this approach as temporal weighting on scores.
Let C and P be the set of classes and creation time points of the training documents. First, each training document class c  C is associated with the corresponding creation time point p  P, generating a new class defined as c, p . Then, we use a traditional classification algorithm to gener-
ate scores for each new class c, p . Note that this scenario isolates term-class relationship variations, since it considers only one time point. To decide to which class c the document d should be assigned to, we sum up all the scores c, p , for all pr  P, weighting them by the T W F (), where  = p - pr corresponds to the temporal distance between p and the creation moment of d . At the end of this process, d will be assigned to the class c with highest score, as listed in Algorithm 4.

Algorithm 4 TWF-Sc: Temporal Weighting on Scores

1: function Classify(d , C, P, D)

2: for each c  C do

3:

for each p  P do

4:

c  c.p

5:

score[c]+ =TraditionalClassifier(d , D, c ) · T W F ()

6: return {arg maxc score}

5. RESULTS
In order to evaluate the impact that the proposed TWF has on the classification task, we evaluate both the traditional and temporally-aware versions of Rocchio, KNN and Na¨ive Bayes in the ACM-DL and Medline collections, and contrast them. The methods were compared using two standard information retrieval measures: Accuracy and macro average F1 (MacroF1). While the Accuracy measures the classification effectiveness over all decisions, the MacroF1 measures the classification effectiveness for each individual class and averages them. All experiments were executed using a 10-fold cross-validation [1] procedure considering training, validation and test sets. The parameters were set using the validation set, and the effectiveness of the algorithms measured in the test partition.
5.1 Parameter settings
In order to run the experiments, two important parameters had to be set: the value of k for KNN and the scaling factor .
We first performed some experiments with KNN to define the value of k. This parameter significantly impacts the quality of classifier, and must be carefully chosen. Four values were tested for each version of the traditional and temporally-aware algorithms: 30, 50, 150, and 200. For the traditional version of the algorithm k = 30 presented better results, while for both temporally-aware versions of KNN

the best value of k was 50. The intuition for the traditional KNN to perform better with smaller values of k is that, as the number of neighbors increases, the variation on term-class relationships also increases, and the probability of misclassification increases. When considering temporal information by means of the proposed temporal weights, in contrast, more consistent information becomes available, allowing a more accurate model.
As discussed in Section 4, the TWF scale must be compatible with the classifiers scores, ensuring that it effectively improves the classifier's decision rules without dismissing them. We empirically tested three values for : 1, 10, and 100. The best value of each version of each classifier was considered. For Rocchio and KNN, the best results were obtained with  = 1. For Na¨ive Bayes, the best value was  = 10. This is due to the multiplicative nature of this classifier: many conditional probabilities are multiplied, leading to even smaller values.
5.2 Experiments
After setting the parameters, we perform experiments comparing the traditional and the proposed temporally-aware versions of Rocchio, KNN and Na¨ive Bayes. The results for the ACM-DL and Medline collections are reported in Tables 4 and 5. In both tables, each line presents the results achieved by the versions of the classifiers identified in the first row and column. The values obtained for MacroF1 ("macF1") and accuracy ("acc.") are reported, as well as the percentage difference between values achieved by the temporally-aware methods and the traditional version of the classifiers. This percentage difference is followed by a symbol that indicates whether the variations are statistically significant according to a 2-tailed paired t-test, given a 99% confidence level. denotes a significant positive variation, · a non significant variation and a significant negative variation.
As we can see in Tables 4 and 5, all modified versions of Rocchio and KNN achieved better results than the baseline in ACM-DL. In Medline, the versions on score achieved gains, while the versions on documents were statistically the same as the baseline. In particular, Rocchio with TWF on scores presents the most significant improvements in both collections, with gains up to +18.87 and +11.46 for MacroF1 and Accuracy, respectively. Similarly, KNN with TWF on scores achieves the best results among all KNN variations, with gains of +8.85% and +3.80% for MacroF1 and Accuracy in the ACM-DL collection. In the case of Rocchio, the improvements achieved using the TWF can be explained by the fact that, in the traditional version, the documents are summarized in a unique representative vector (centroid), aggregating documents from distinct creation time points, and affecting the prediction ability of the classifier. In the case of KNN, the definition of class boundaries is done considering each training document independently. KNN assumes that documents of same class are located close by on the vectorial space. By using the TWF, the k nearest documents are reorganized, and the most temporally relevant are placed closer to the example being classified.
The Na¨ive Bayes with TWF on documents presents better results for MacroF1 on both ACM-DL and Medline, and better Accuracy in Medline. Note that the best improvement was achieved in MacroF1, pointing out that this strategy effectively reduces the Na¨ive Bayes bias towards the most

312

Algorithm Metric Baseline TWF
on documents TWF
on scores

Rocchio

macF1(%) 55.12

acc.(%) 66.42

57.24

68.64

(+3.85)

(+3.34)

59.07

71.54

(+7.17)

(+7.71)

KNN

macF1(%) 61.80

acc.(%) 72.29

63.40

73.98

(+2.59)

(+2.34)

64.18

74.56

(+3.85)

(+3.14)

Na¨ive Bayes

macF1(%) acc.(%)

57.33

73.69

60.78

74.14

(+6.02)

(+0.61) ·

40.62

48.76

(-41.14)

(-51.13)

Table 4: Results obtained when incorporating TWF to Rocchio, KNN, and Na¨ive Bayes ­ ACM-DL

Algorithm Metric Baseline TWF
on Documents TWF
on scores

Rocchio

macF1(%)

acc.(%)

55.36

70.17

56.21

70.83

(+1.53) · (+0.94) ·

65.81

78.21

(+18.87)

(+11.46)

KNN

macF1(%) acc.(%)

73.33

84.63

73.94

84.07

(+0.83) · (+0.66) ·

76.82

87.01

(+4.54)

(+2.81)

Na¨ive Bayes

macF1(%) acc.(%)

76.81

84.69

81.58

87.48

(+6.21)

(+3.29)

51.54

48.02

(-49.03)

(-76.36)

Table 5: Results obtained when incorporating TWF to Rocchio, KNN, and Na¨ive Bayes ­ Medline

frequent classes. However, in contrast with Rocchio and

KNN, the Na¨ive Bayes with TWF on scores performs poorly

in both collections. We attribute this to two major weak-

nesses of traditional Na¨ive Bayes version. First, when facing

skewed data distributions, Na¨ive Bayes unwittingly prefers

larger classes over others, causing decision boundaries to be

biased. Second, when data is scarce, there is not enough

information to perform accurate estimates, leading to bad

results.

The skewness of data distribution among classes c, p can

be quantified by the Coefficient of Variation CV

=

 

of their

sizes, where  and  stand for the standard deviation and

mean. To explore the impact of data skewness on Na¨ive

Bayes, we sampled MedLine, creating two sub-collections

composed by the least and most frequent classes c, p , min-

imizing data skewness. While the entire collection presents

CV = 1.33, the sub-collections with the least and most fre-

quent classes present CV equal to 0.57 and 0.43, respec-

tively. As we can observe in Tables 5 and 6, the greater the

CV, the worse are the results.

ACM-DL has an even more skewed data distribution over

each time point, preventing us to sample it in sub-collections

with smaller CV. Figure 2 shows that in the ACM-DL col-

lection data scarcity is also prominent, contributing to the

poor performance. Notice that 70% of classes c, p have

less than 100 documents, a number too low to guarantee

accurate estimates.

Relative Frequencies (%)

0.5 0.45
0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
0 0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 Size
Figure 2: c, p sizes ­ ACM-DL
As observed, using TWF on scores in most cases led to better results than those applying TWF on documents. We believe this is because, in many applications, terms present distinct evolutive patterns, and the proposed function neglects this fact. Hence, when the temporal TWF is applied on documents, all terms are multiplied by the same score (i.e., using a fine-grained representation of the document), given in function of the temporal distance . Thus, we consider an uniform evolution among terms. In contrast, the TWF on scores minimizes this problem, as it applies

the score in a coarse-grained representation of a document. Hence, one could argue that the TWF is more suitable to the "on scores" approach. The definition of a temporal weighting for each term independently is left for future work.
Finally, we also compared the best version of the methods previously proposed, i.e., KNN with TWF on scores and Na¨ive Bayes with TWF on documents, to the state of the art classifier Support Vector Machine [8], in terms of effectiveness (classification quality) and efficiency (execution time). We run an efficient SVM implementation, SVM Perf [9], which is based on the maximum-margin approach and can be trained in linear time. We used an one-against-all [18] methodology to adapt binary SVM to multi-class classification, since the collections present 11 (ACM-DL) and 7 (MedLine) classes. The results are presented in Table 7. For ACM-DL collection, the significant gains are of 3.74% and 2.66% in macroF1 and accuracy, respectively. For MedLine collection, the most significant gains are of 12.87% and 5.06% in macroF1 and accuracy, respectively. Considering that SVM is a state of the art classifier, and that both collections are very unbalanced (which significantly difficulties classification), our results evidence the quality of the proposed solution, with better performance.
6. CONCLUSION AND FUTURE WORK
This work discussed the impact that temporal effects may have in ADC, and proposed two new strategies for instance weighting that leads to more accurate classification. We started by proposing a methodology to model a Temporal Weighting Function (TWF) that captures changes in termclass relationships for a given period of time. For our real datasets, we showed that TWF follows a lognormal distribution, whose parameters may easily be tuned using statistical methods. In order to incorporate this TWF to classifiers, we presented two approaches: TWF on documents and TWF on scores. TWF on documents weights each training document by the TWF according to its temporal distance to the test document. TWF on scores, in contrast, takes into account scores produced by the traditional classifiers on scenarios without temporal variability on term-class relationships, performing a weighted sum of them, where the weights come from the TWF. Both strategies were incorporated to three traditional classifiers, namely Rocchio, KNN, and Na¨ive Bayes.
Results with the traditional versions of these classifiers and the temporally-aware ones showed that considering temporal information significantly improves the results of the

313

Na¨ive Bayes Metric Baseline TWF
on Scores

Least frequent classes c, p

CV macF1(%) acc.(%)

87.32

88.10

0.57

92.03

92.21

(+5.40)

(+4.66)

Most frequent classes c, p

CV macF1(%) acc.(%)

91.09

91.92

0.43

93.19

93.86

(+2.31)

(+2.12)

Table 6: Results obtained for the least and most frequent classes c, p sampling for Na¨ive Bayes ­ Medline

Collection Metric SVM
KNN with TWF on scores Na¨ive Bayes with TWF on documents

macF1(%) 60.07
64.18 (+6.84)
60.78 (+1.18) ·

ACM-DL acc.(%) 73.03
74.56 (+2.09)
74.14 (+1.52) ·

Time (s) 4200 62
51

macF1(%) 72.28
76.82 (+5.90)
81.58 (+12.87)

Medline acc.(%) 83.27
87.01 (+4.49)
87.48 (+5.06)

Time (s) 1300000
4757
2840

Table 7: Results obtained when adding TWF on scores to KNN and TWF on Documents to Na¨ive Bayes

versus SVM for both collections

traditional classifiers. Also, both temporally-aware KNN and Na¨ive Bayes achieved better results than SVM, with better performance. Considering that SVM is a state of the art classifier, and that both collections are very unbalanced, our results evidence the quality of our solution, coupled with an efficient implementation.
Given the results obtained when comparing SVM to the temporal versions of KNN and Na¨ive Bayes, as a future work, we will incorporate temporal information to the SVM classifier, by defining kernel functions that use the proposed TWF. We also plan to refine our TWF, in order to account for distinct evolutive patterns that terms may present, by means of a more sophisticated statistical analysis. Moreover, similarly to time, social and geographical aspects may induce variations on term-class relationships, and we will also explore those dimensions in order to achieve an even better classification quality.
7. REFERENCES
[1] L. Breiman and P. Spector. Submodel selection and evaluation in regression ­ the x-random case. International Statistical Review, 60:291­319, 1992.
[2] N. H. M. Caldwell, P. J. Clarkson, P. A. Rodgers, and A. P. Huxor. Web-based knowledge management for distributed design. IEEE Intelligent Systems, 15(3):40­47, 2000.
[3] D. B. Clarkson, Y.-a. Fan, and H. Joe. A remark on algorithm 643: Fexact: an algorithm for performing fisher's exact test in r x c ACM Trans. Math. Softw., 19(4):484­488, 1993.
[4] W. W. Cohen and Y. Singer. Context-sensitive learning methods for text categorization. ACM Trans. Inf. Syst., 17(2):141­173, 1999.
[5] S. K. Crow EL. Log-normal distributions: Theory and application. New York: Dekker, December 1988.
[6] P. E. D'Agostino R.B. Tests for departure from normality. Biometrika, 60:613­622, 1973.
[7] G. Folino, C. Pizzuti, and G. Spezzano. An adaptive distributed ensemble approach to mine concept-drifting data streams. In ICTAI '07, Volume 2, pages 183­188, Washington, DC, USA, 2007. IEEE Computer Society.
[8] T. Joachims. Making large-scale support vector machine learning practical. Advances in kernel methods: support vector learning, pages 169­184, 1999.
[9] T. Joachims. Training linear svms in linear time. In Proc. of the 12th ACM SIGKDD Conference, pages 217­226, New York, NY, USA, 2006. ACM.

[10] Y. S. Kim, S. S. Park, E. Deards, and B. H. Kang. Adaptive web document classification with mcrdr. In ITCC '04, Volume 2, page 476, Washington, DC, USA, 2004. IEEE Computer Society.
[11] R. Klinkenberg. Learning drifting concepts: Example selection vs. example weighting. Intell. Data Anal., 8(3):281­300, 2004.
[12] R. Klinkenberg and T. Joachims. Detecting concept drift with support vector machines. In P. Langley, editor, ICML '00, pages 487­494, Stanford, US, 2000. Morgan Kaufmann Publishers, San Francisco, US.
[13] J. Kolter and M. Maloof. Dynamic weighted majority: A new ensemble method for tracking concept drift. Technical report, Department of Computer Science, Georgetown University, Washington, DC, June 2003.
[14] S. Lawrence and C. L. Giles. Context and page analysis for improved web search. IEEE Internet Computing, 2(4), 1998.
[15] M. M. Lazarescu, S. Venkatesh, and H. H. Bui. Using multiple windows to track concept drift. Intell. Data Anal., 8(1):29­59, 2004.
[16] E. Limpert, W. A. Stahel, and M. Abbt. Log-normal distributions across the sciences: Keys and clues. BioScience, 51(5):341­352, 2001.
[17] R. Liu and Y. Lu. Incremental context mining for adaptive document classification. In Proc. of the 8th ACM SIGKDD, pages 599­604. ACM Press, 2002.
[18] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.
[19] F. Mourao, L. Rocha, R. Arau´jo, T. Couto, M. Gon¸calves, and W. Meira Jr. Understanding temporal aspects in document classification. In Proc. of the WSDM '08, 2008.
[20] L. Rocha, F. Mourao, A. Pereira, M. A. Gon¸calves, and W. Meira Jr. Exploiting temporal contexts in text classification. In Proc. of the CIKM '08, 2008.
[21] M. Scholz and R. Klinkenberg. Boosting classifiers for drifting concepts. Intell. Data Anal., 11(1):3­28, 2007.
[22] A. Tsymbal. The problem of concept drift: Definitions and related work. Technical report, Department of Computer Science, Trinity College, Dublin, Ireland, December 2004.
[23] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69­101, 1996.

314

Multilabel Classification with Meta-level Features

Siddharth Gopal
Carnegie Mellon University Pittsburgh PA 15213
sgopal1@andrew.cmu.edu

Yiming Yang
Carnegie Mellon University Pittsburgh PA 15213 yiming@cs.cmu.edu

ABSTRACT
Effective learning in multi-label classification (MLC) requires an appropriate level of abstraction for representing the relationship between each instance and multiple categories. Current MLC methods have been focused on learning-to-map from instances to ranked lists of categories in a relatively high-dimensional space. The fine-grained features in such a space may not be sufficiently expressive for characterizing discriminative patterns, and worse, make the model complexity unnecessarily high. This paper proposes an alternative approach by transforming conventional representations of instances and categories into a relatively small set of link-based meta-level features, and leveraging successful learning-to-rank retrieval algorithms (e.g., SVM-MAP) over this reduced feature space. Controlled experiments on multiple benchmark datasets show strong empirical evidence for the strength of the proposed approach, as it significantly outperformed several state-of-the-art methods, including Rank-SVM, ML-kNN and IBLR-ML (Instance-based Logistic Regression for Multi-label Classification) in most cases.
Categories and Subject Descriptors
I.5.2 [PATTERN RECOGNITION] Design Methodology;
Classifier design and evaluation; H.1.0 [INFORMATION
SYSTEMS]: General;
General Terms
Algorithms, Design, Experimentation, Performance
Keywords
Multi-label classification; model design; learning to rank; comparative evaluation
1. INTRODUCTION
Multi-label classification (MLC) refers to the problem of instance labeling where each instance may have more than one correct label. MLC has a broad range of applications. For example, a news article could belong to both topics politics and economics, and also could be related to China and USA as the regional categories. An image
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

(picture) could have flower as the object type, yellow and red as the colors, and outdoor as the background category. A computer trouble report could be simultaneously related to a hardware failure, a software problem, an urgency-level category, a regional code, and so on.
MLC is technically challenging as it goes beyond the scope of well-studied two-way classifiers, such as binary Support Vector Machines (SVM), Naïve Bayes probabilistic classifiers, etc. Approaches to MLC typically reduce the problem into two subproblems: the first is learning to rank categories with respect to each input instance, and the second is learning to place a threshold on each ranked list for a yes/no decision per category. The first subproblem is the most challenging part and therefore has been the central focus in MLC. A variety of approaches has been developed and can be roughly divided into two types: binary-classifier based methods versus global optimization methods, and the latter can be further divided into model-based and instance-based methods.
Binary-classifier based methods are the simplest. A representative example is to use a standard SVM ("binary-SVM") [17] to learn a scoring function for each category independently from the scoring functions for other categories. Other kinds of binary classifiers could also be used for such a purpose, such as logistic regression, Naïve Bayes probabilistic classifiers, boosting algorithms, neural networks etc. In the testing phase, the ranked list of categories is obtained for each test instance by scoring each category independently and then sorting the scores. Binaryclassifier based methods have been commonly used due to their simplicity, but also have been criticized for the lack of global optimization in category scoring. These methods are common baselines in benchmark evaluations of stronger methods in MLC.
Elisseeff and Weston [5] proposed a large-margin approach, Rank-SVM, which is a representative example of model-based methods. Unlike conventional binary SVM which maximizes the margin for each category independently, Rank-SVM maximizes the sum of the margins for all categories simultaneously, conditioned on partial-order constraints. That is, ranking the relevant categories of each training instance higher than the irrelevant categories is an explicit optimization criterion in this method. The scoring function is parameterized by the weights of input features for every category; thus, the number of parameters in Rank-SVM is the product of the number of categories and the size of the feature set. In other words, the model complexity (measured using the number of free parameters) is the same as that of m binary-SVMs where m is the number of target categories. Experiments by the authors of RankSVM show that this method significantly outperformed binary SVM in gene classification on a micro-array dataset (namely "the yeast dataset").
Zhang and Zhou [25] proposed an instance-based approach which is named as Multi-label k-Nearest Neighbor (ML-kNN).

315

Cheng and Hullermeier proposed another variant called InstanceBased Logistic Regression (IBLR) [3]. Multi-label versions of kNN have been studied in text categorization for a long time and commonly used as strong baselines in benchmark evaluations [2][20][22]. ML-kNN and IBLR are relatively new variants which are similar in the sense that both use Euclidean distance to identify the k nearest neighbors for each test instance, but differ in how the local probabilities are estimated for categories. ML-kNN gives an equal weight to each label occurrence in the neighborhood of the input instance while IBLR varies the weight of each label according to how distant it is to the test instance. Further, ML-kNN assumes independence among category occurrences while IBLR explicitly models pairwise dependencies among categories using logistic regression. IBLR also makes combined use of instance-based features (such as the similarity score of each neighbor) and conventional features (such as words in the test document) in the logistic regression. The evaluations by the authors of ML-kNN showed its superior performance over Rank-SVM, BoosTexter [14] and Adaboost.MH [15] , and the experiments by the authors of IBLR showed further performance improvement by IBLR over the results of ML-kNN on multiple datasets [3].
MLC methods, including the ones discussed above, have been focused on learning-to-rank in a relatively high-dimensional space. In Rank-SVM for text categorization the features are words in the training-set document vocabulary. In IBLR for text categorization the feature set is the union of word-level features and kNN-based features (used to model the interdependencies among categories). Both methods learn a linear function per category, so the total number of features is md where d is the feature-set size and m is the number of categories. In machine learning, it is generally understood that when the number of model parameters is unnecessarily large, the model tends to overfit training data and does not generalize well on test data. To what extent would this be an issue in current MLC methods? Further, can we find a better solution for MLC by transforming lower-level features to higher-level ones, and then learning to rank more effectively in the reduced space? Thorough investigation is needed to answer these questions, and such is the primary focus of this paper. Specifically, we address these questions by the following means:
1) We propose a generic framework that allows automated transformation of a conventional data representation (such as a bag of words per instance and a set of training instances per category) into meta-level features. This enables a broad range of learning-to-rank algorithms in information retrieval (IR) to be deployed for ranking categories in MLC.
2) We use SVM-MAP, a large-margin method for optimizing ranked lists of documents with respect to the Mean Average Precision in IR, as the choice of algorithm in this paper to illustrate effective MLC learning with meta-level features. For convenience we call this instantiation of our framework SVM-MAP-MLC in distinction from the use of SVM-MAP in ad-hoc information retrieval.
3) We conduct controlled experiments on multiple benchmark datasets to evaluate SVM-MAP-MLC in comparison with other state-of-the-art methods, including Rank-SVM, MLkNN, IBLR and Binary SVM.

4) We provide strong empirical evidence for the strengths of the proposed method, with p-values at the 1% level or smaller on all the datasets in statistical significance tests for comparing our approach with the other state-of-the-art methods.
The rest of the paper is organized as follows. Section 2 introduces the meta-level features for MLC. Section 3 describes the method for category ranking optimization, i.e., SVM-MAPMLC and the strategy for optimizing the threshold on each ranked list. Section 4 presents design of controlled experiments. Section 5 reports the main results. Section 6 summarizes our findings and conclusion.
2. META-LEVEL FEATURES FOR MLC
Following a standard notation in machine learning, we define X as the input space (of all possible instances), Y as the output space (of all possible ranked lists of target variables), and T = {(x1, y1), (x2 , y2 ),L, (xn , yn )} as a training set of n pairs of xi  X and yi Y . The learning-to-rank problem is generally defined as to find the optimal mapping f : X  Y given T. Now we define a transformation from x  X to z  Z with the following properties:
1) The feature-set size of Z should be relative small.
2) The features in z  Z should be highly informative about how each instance is related to multiple categories, and should discriminate relevant instances from irrelevant instances with respect to any possible subset of categories.
3) The transformation from x  X to z  Z should be learnable based on a labeled training set and automatically computable for each test instance on the fly.
4) The transformed training data should allow a broad range of learning-to-rank algorithms to be used for MLC optimization with respect to various loss functions.
Based on these criteria we define vector z as the concatenation of the sub-vectors below, which are defined for each test instance x0 and category c j  C for j = 1,2,L, m as:
( ) · vL2 (x0 , c j ) = d (x1j , x0 ), d (x2j , x0 ),L, d (xkj , x0 ) , a k-
dimensional vector where xij  kNN (x0 , c j ) is the ith nearest neighbor ( i = 1,2,L, k ) of x0 among the members of c j in the training set, and d (xij , x0 ) || xij - x0 ||2 is
the L2 -norm of the difference between the two vectors1;
( ) · vL1 (x0 ,c j ) = d '(x1j , x0 ), d '(x2j , x0 ),L, d '(xkj , x0 ) , a k-
dimensional vector where d '(xij , x0 )  || xij - x0 ||1 is the L1 -norm of the difference between the two vectors;
1 For rare categories, since the number of training instances in each category is small, there might not be k nearest neighbors. In such a case we duplicate the furthest neighbor so that the subvector reaches size k.

316

( ) · vcos (x0 ,c j ) = cos(x1j , x0 ),cos(x2j , x0 ),L,cos(xkj , x0 ) , a
k-dimensional vector whose elements are the cosine similarity of the corresponding vector pairs;
( ) · vmod (x0 ,c j ) = d (x j , x0 ),cos(x j , x0 )) , a 2-dimensional
vector where x j is the centroid (vector average) of all the positive training examples in category c j .
Of course the listed features are not necessarily exhaustive for all possibly informative features but rather a set of concrete examples for illustrating the principle in our approach. The number of
features in vector z is 3k + 2 per category, and the total of (3k + 2)m for m categories. Parameter k can be tuned on a
held-out validation dataset which is typically in the range from 10 to 100. The size of such a meta-level feature set is much smaller than those typically found in current MLC methods.
More importantly, these meta-level features are induced in a supervised manner, taking the labels of training instances into account. Also, the meta-level features make a combined use of local information (through kNN-based features) and global information (through category centroids) in the training set.
Vector z  Z synthetically represents a pattern for each instance
about how it is related to multiple categories. Figure 1 illustrates the concept geometrically in a 2-D space. For simplicity we only plot the L2-norm links in this graph and omit the other types of links.
Figure 1: The link-based representation of one particular instance (the dot in the center) in relation to multiple categories is shown. Each category is represented using its positive examples (points in the same color) in the training set and its centroid (triangle). Each instance is represented using the "average links" (i.e., the distance to each category centroid, shown by thick lines) and multiple "single links" (i.e., the distance to each of the k nearest neighbors, shown by the thin lines) per category. These links together portray an informative picture about how the instance is related to multiple categories.

3. OUR APPROACH: SVM-MAP-MLC
3.1 Learning to rank for MLC
Our goal now is to solve the mapping f MLC : Z  Y based on
the transformed training set
T MLC = {(z1, y1), (z2 , y2 ),L, (zn , yn )}
where zi = z(xi )  Z is an transformed input vector whose elements are meta-level features, and yi  Y is a true ranked list
of categories for the input2. Any learning-to-rank algorithm could be deployed in principle; among the successful ones, we choose SVM-MAP in the remainder of this paper.
SVM-MAP is originally designed for ranking documents with respect ad-hoc queries where ad-hoc means that queries could be any combination of words, not fixed in advance. The learning-to-rank problem is to find the optimal mapping f IR : Q  Y where Q the input space (of all possible
queries), Y as the output space (of all possible ranked lists of documents), and optimal means to minimize the training-set loss as well as the model complexity. A variety of learning-to-rank algorithms have been developed recently in machine learning for ad-hoc retrieval, with different loss functions and model selection criteria. SVM-MAP [24] is designed to maximize the Mean Average Precision (MAP) of ranked lists, which is a common metric for method comparison in IR evaluations. Methods focusing on other optimization criteria include a multivariate version of SVM [8] that maximizes ROC-Area for classification, MCRank and AdaRank [11] [19] that use boosting algorithms to maximize the Normalized Discounted Cumulated Gain (NDCG) of ranked lists, and so on.
Most learning-to-rank algorithms in ad-hoc retrieval rely on a shared representation between queries and documents, i.e., a bag-of-words per query and per document. Such a shared representation facilitates a natural way to induce features for discriminating the relevance of query-document pairs. For example, SVM-MAP uses a conventional search engine (Indri) to produce the cosine similarity and language-model based scores for each query-document pair, discretizes those scores into bins, and treats the bins as the features in a dimension-reduced vector space where each query-document pair with relevance judgment is treated as a training instance. Optimizing the mapping from queries to ranked lists of documents therefore reduces to the learning of feature weights in the dimension-reduced vector space.
In order to apply SVM-MAP to MLC, or to apply any learning-to-rank retrieval algorithm to MLC in general, we need to find discriminative features to represent instance-category pairs and the one-to-many mapping from each instance to categories. The meta-level features we introduced in the previous section are exactly designed for such a purpose, allowing a broad range of learning-to-rank algorithms in IR to be deployed for MLC. The
category-specific links ( 3k + 2 per category) in vector z (Section 2)
and the corresponding label (yes or no with respect to the category)
2 There may be more than one true ranked list for an instance. That is, any list that places all the relevant categories above all the irrelevant categories is truly correct.

317

is treated as an instance-category pair in the training set. We focus on SVM-MAP in this paper because it explicitly optimizes MAP which is one of the primary metrics we use in our evaluation of MLC methods (Section 4.3). SVM-MAP and other learning-to-rank retrieval methods have not been used for MLC before, to our knowledge. We name our novel application of SVM-MAP as SVM-MAP-MLC, in contrast to its conventional use in ad-hoc information retrieval.
3.2 Learning to threshold for MLC
In order to enable the system to make classification decisions in MLC, we need to apply a threshold to the ranked list of categories for each test instance. A variety of thresholding strategies have been studied in the literature [21],[5], among which we choose the linear regression approach proposed in [5]. Unlike binary-SVM where the natural choice of threshold is zero and probabilistic classifiers (such as logistic regression, Naïve Bayes, IBLR, etc.) where the default choice of threshold is 0.5, SVM-MAP-MLC produces non-probabilistic scores to rank categories with partialorder preferences. Obviously, neither 0 nor 0.5 is the appropriate optimal threshold on a ranked list of categories given an instance. The strategy proposed in Rank-SVM [5] is designed for optimizing the threshold conditioned on each ranked list. That is, the system uses a training set to learn a linear mapping from an arbitrary ranked list of categories to the optimal threshold as:
g :L T
where L  R m is the space of all possible vectors of system-scored categories, and T  R is the space of all possible thresholds. The
optimal mapping is defined as the linear-least-squared-fit (LLSF) solution given a training set of ranked lists with the optimal threshold per list. The optimal threshold given a list is defined as the threshold that minimizes total error, i.e. the sum of type-I errors (the false positives) and type-II errors (the false negatives). Such a training set can be automatically generated by 1) learning a SVMMAP-MLC model to score all categories conditioned on each input instance, and 2) finding the optimal threshold for each vector of scored categories given an instance. The LLSF function is applied in the testing phase, to the system-scored categories conditioned on each test instance. The resulting threshold is then applied to the corresponding ranked list of categories: the categories above or at the threshold receive a yes decision, and the categories below the threshold receive a no decision. We modified the original method by rescaling scores of each instance to make it in the unit norm.
4. EVALUATION DESIGN
4.1 Methods for Comparison
We conducted controlled experiments to evaluate SVM-MAPMLC3 in comparison with the following methods:
· Binary-SVM is a standard version of SVM for one-versus-rest classification, and a common baseline in comparative evaluation of classifiers (including MLC methods) [5] [9]4.

· Rank-SVM, the method proposed by [5], is representative among the model-based methods which explicitly optimize ranked lists of categories for MLC5.
· IBLR, the instance-based method recently proposed by [3], is a new method in MLC evaluations. The method has two versions, one using kNN-based features only (IBLR-ML) and another (IBLR-ML+) using world-level features in conjunction with kNN- based features. We tested both versions and found that IBLR-ML performed consistently better than IBLR-ML+, which agrees with the conclusion by the authors of IBLR [3]. We therefore use the results of IBLR-ML for method comparison in the rest of the paper. We used the Weka [7] implementation provided by the authors.
· ML-kNN, the instance-based method proposed by [25] is another strong baseline in MLC evaluations [3]. We used the publicly available Weka implementation [7] of this method in our experiments.
All the systems produce a ranked list of categories given a test instance. The ranked lists can be directly evaluated using rank-based metrics, and indirectly evaluated based on binary classification decisions (yes or no on each category) after applying a threshold to each ranked list.
The choice of thresholding strategy depends on the nature of each classification method. In Binary-SVM, for each category, we used the conventional threshold of zero to obtain a yes/no decision for that category. In ML-kNN, IBLR-ML and Rank-SVM we follow the same thresholding strategies as proposed by the authors. Specifically, for ML-kNN and IBLR we set the threshold to 0.5, and for Rank-SVM we use the linear least squares fit solution as the threshold, as proposed in [5].
4.2 Datasets
We used five datasets6 in our experiments, namely emotion, scene, yeast, Reuters-21578 and Citeseer. These datasets form a representative sample across different fields and they vary in training-set size and feature-space size. All the datasets except Citeseer have been used in previous evaluations of MLC methods, with conventional train-test splits. We follow such conventions in order to make our results comparable to the previously published ones. Table 1 summarizes the datasets statistics:
· Emotions [16] is a multi-label audio dataset, in which each instance is a song, indexed using 72 features such as amplitude, beats per minute etc. The songs have been classified under six moods such as sad/lonely, relaxing/calm, happy/pleased, amazed/surprised, angry/aggressive and quiet/still.
· Scene [1] is an image dataset. The images are indexed using a set of 294 features which decompose each image into smaller blocks and represent the color of each block. The images are classified based on the scenery (Beach, Sunset etc) they portray.

3 We used the publicly available SVM-MAP software at
http://projects.yisongyue.com/svmmap/ as the core algorithm. 4 We used the publicly available SVMlight software package at
http://svmlight.joachims.org/ in our experiments.

5 We thank the authors of ML-kNN for sharing their
implementation of Rank-SVM. 6 The emotions, scene and yeast datasets were obtained from
http://mlkd.csd.auth.gr/multilabel.html

318

Dataset Name
Emotions Scene Yeast Citeseer
Reuters21578

Table 1. Dataset Characteristics

Training Size Testing Size

#Categories

391

202

6

1211

1196

6

1500

917

14

5303

1326

17

7770

3019

90

Avg #Categories per instance 1.87 1.07 4.24 1.26 1.23

#Features
72 294 103 14601 18637

· Yeast dataset [5] is a biomedical dataset. Each instance is a gene, represented using a vector whose features are the micro-array expression levels under various conditions. The genes are classified into 14 different functional classes.
· Citeseer is a set of research articles we collected from the Citeseer web site7. Each article is indexed using the words in its abstract as the features, with a feature-set size of 14,601. We use the top level of 17 categories in the Citeseer classification hierarchy as the labels in this dataset, and randomly split 80% of the corpus into training and the rest as testing instances. The dataset will be made publicly available along with the publication of this paper.
· Reuters-21578 is a benchmark dataset in text categorization evaluations. The instances are Reuters news articles during the period 1987 to 1991, and labeled using 90 topical categories. We follow the same train-test split in [21].

4.3 Metrics
We select two standard metrics for evaluating ranked lists, and two standard metrics for evaluating classification decisions.

· Mean Average Precision (MAP) [18] is a popular metric in
traditional IR evaluations for comparing ranked lists. It is
defined as the average of the per-instance (or per-ranked-list)
Average precision (AP) over all test instances. Let
{ } D = xi i=1,L,n be the test set of instances, Li be the ranked

list of categories for a specific instance, ri (c) be the rank of

category c in list Li , and Ri be the set of categories relevant

to instance xi . MAP is defined as:

MAP(D)

=

1 n

n
 AP(xi
i=1

)

AP(xi )

=

1 | Ri

 | cRi

{c' Ri , s.t., ri (c') <
ri (c)

ri (c)}

· Ranking Loss (RankLoss) is a popular metric for comparing MLC methods in ranking categories [14][3][5][25]. It measures the average number of times an irrelevant category is ranked above a relevant category in a ranked list as:

{ } RankLoss(xi

)

=

|

Ri

1 ||R

i

|

(c, c')  Ri × Ri , s.t.ri (c) > ri (c')

7 http://citeseer.ist.psu.edu/directory.html

· Micro-averaged F1 is a conventional metric for evaluating

classifiers in category assignments to test instances

[10],[22],[23] . The system-made decisions on test set D with
respect to a specific category c  C  {c1,L, cm }can be

divided into four groups: True Positives ( TPc ), False

Positives ( FPc ), True Negatives ( TNc ) and False Negatives

( FNc ), respectively. The corresponding evaluation metrics

are defined as:

Global Precision

P

=

cC TPc
cC (TPc + FPc

)

Global

Recall

R

=

cC TPc
cC (TPc + FNc

)

Micro-averaged

F1

=

2PR P+R

· Macro-averaged F1 is also a conventional metric for evaluating classifiers in category assignments, defined as:

Category-specific Precision

Pc

=

TPc TPc + FPc

Category-specific Recall

Rc

=

TPc TPc + FPc

Macro-averaged

F1 =

1 m

cC

2Pc Rc Pc + Rc

Both micro-averaged F1 and macro-averaged F1 are informative for method comparison. The former gives the performance on each instance an equal weight in computing the average; the latter gives the performance on each category an equal weight in computing the average.
We choose the evaluation measures so as to evaluate the performance of both the ranking algorithms as well as the thresholding strategies. MAP and RankLoss measure how well a system ranks the categories; Micro-F1 and Macro-F1 evaluate the effectiveness of the thresholding strategies for making classification decisions.

4.4 Experimental Setting Details
All parameters are tuned to optimize MAP and the tuning is done through a five-fold cross validation on the training set for each corpus. The tuned parameters include the number of nearest

319

neighbors in SVM-MAP-MLC, ML-kNN and IBLR-ML, and the regularization parameter in SVM-MAP-MLC, Binary-SVM, Rank-SVM. For the number of nearest neighbors we tried values from 10 to 100 with the increments of 10; for the regularization parameter we tried 20 different values between 10-3 to 106 . For IBLR, on Citeseer and Reuters-21578, the best choice for the number of nearest neighbors through cross validation was found to be 300 and 190 respectively.
Feature selection was not performed on any of the datasets for any method. For term weighting in Citeseer and Reuters documents, we used a conventional TF-IDF scheme (namely "ltc") [22]. On the Emotions dataset, each feature was rescaled to a unit variance representation since we observed that the original values of the features are not comparably scaled.
5. RESULTS
The results of all the systems on the five datasets are summarized in Table 2. The methods with the best scores are highlighted in bold, and the relative ranks among the methods on each dataset in

a specific metric are provided inside parentheses. We report the value of 1-RankLoss instead of RankLoss just to make the scores consistent to each other, i.e., higher values are better. The total rank of each system is provided at the bottom line of the table.
5.1 Comparative Analysis
Let us call each line in Table 1 a case, which corresponds to a particular dataset and a specific metric. SVM-MAP-MLC is the best in 18 out of the 20 cases while Binary-SVM is the best on the two remaining cases. That is, the proposed approach consistently outperformed other methods in most cases.
Comparing Rank-SVM with binary-SVM, both are largemargin methods but the former outperformed the latter on 12 out of the 20 cases. These results are consistent with the previously reported evaluation on the Yeast dataset [5], showing some success of Rank-SVM by reinforcing partial-order preferences among categories. Comparing Rank-SVM with SVM-MAP-MLC, on the other hand, the latter outperformed the former in 19 out of the 20 cases although both methods have partial-order preferences in their objective functions for optimization. The advanced

Table 2. Results Summary: Each method is evaluated on five datasets using four metrics. The bold-faced numbers indicate the best system on a particular dataset given the metric; the numbers in parentheses are the ranks of the systems accordingly.

Dataset

Metric

SVM-MAP-MLC

ML-kNN

Rank-SVM Binary-SVM IBLR-ML

Emotions

MAP 1-Rankloss Micro-F1 Macro-F1

0.8286 (1) 0.8631 (1) 0.7285 (1) 0.7201 (1)

0.7936 (3) 0.8262 (3) 0.6693 (3) 0.6562 (3)

0.7819 (5) 0.8228 (4) 0.6423 (4) 0.6096 (4)

0.7842 (4) 0.8184 (5) 0.6309 (5) 0.5875 (5)

0.8147 (2) 0.8469 (2) 0.6738 (2) 0.6593 (2)

Scene

MAP 1-Rankloss Micro-F1 Macro-F1

0.8796 (1) 0.9315 (1) 0.7416 (1) 0.7563 (1)

0.8511 (5) 0.9069 (5) 0.6986 (3) 0.6916 (3)

0.8519 (4) 0.9193 (2) 0.6611 (5) 0.6687 (5)

0.8567 (3) 0.9179 (3) 0.6753 (4) 0.6742 (4)

0.8580 (2) 0.9173 (4) 0.7094 (2) 0.7122 (2)

Yeast

MAP 1-Rankloss Micro-F1 Macro-F1

0.7662 (1) 0.8373 (1) 0.6730 (1) 0.4306 (1)

0.7584 (3) 0.8284 (4) 0.6249 (5) 0.3361 (4)

0.7577 (4) 0.8288 (3) 0.6515 (2) 0.3597 (3)

0.7465 (5) 0.8010 (5) 0.6313 (4) 0.3240 (5)

0.7599 (2) 0.8307 (2) 0.6371 (3) 0.3709 (2)

Citeseer

MAP 1-Rankloss Micro-F1 Macro-F1

0.7690 (1) 0.9298 (1) 0.5955 (1) 0.5969 (1)

0.7329 (4) 0.9143 (3) 0.5139 (4) 0.4924 (3)

0.7508 (2) 0.9216 (2) 0.5673 (2) 0.5640 (2)

0.7357 (3) 0.8904 (5) 0.5142 (3) 0.4919 (4)

0.6926 (5) 0.8906 (4) 0.4478 (5) 0.4352 (5)

Reuters 21578
Rank Total

MAP 1-Rankloss Micro-F1 Macro-F1

0.9423 (2) 0.99485 (1) 0.8225 (3) 0.5451(1)
23

0.9248 (4) 0.9935 (4) 0.8140 (4) 0.2864 (5)
75

0.9390(3) 0.9939(3) 0.8278(2) 0.4420(3)
64

0.9543 (1) 0.99483 (2) 0.8708 (1) 0.5266 (2)
73

0.8537 (5) 0.9691 (5) 0.7280 (5) 0.3093 (4)
65

320

Table 3. P-Values in signed-rank tests for comparing SVM-MAP-MLC with other methods (`*' denotes lower performance of SVM-MAP-MLC).

Dataset/Method

ML-kNN

Rank-SVM

Binary-SVM

IBLR-ML

Emotions

0.003

7.490e-4

2.50e-04

0.243

Scene

6.975e-07

0.001

.001

4.976e-04

Yeast

1.706e-04

7.603e-04

3.056e-06

0.005

Citeseer

9.815e-12

3.109e-04

2.776e-15

3.156e-26

Reuters-21578

1.970e-10

.452

.999*

3.399e-72

performance of SVM-MAP-MLC, evidentially, comes from the use of meta-level features instead of the conventional features as that in Rank-SVM.
Comparing SVM-MAP-MLC, ML-kNN and IBLR-ML, these methods have one property in common: they are either fully instance-based or partially instance-based leveraging kNN-based features. IBLR-ML outperformed ML-kNN in 13 out of the 20 cases in our experiments; this is more or less consistent with the previous report by [3] in terms of the relative performance of the two methods. Nevertheless, both IBLR-ML and ML-kNN underperformed SVM-MAP-MLC in all the 20 cases, showing the advantage of using of the meta-level features in the learning-torank framework with SVM-MAP.
Comparing Rank-SVM with ML-kNN, the former outperformed the latter in 13 out of the 20 cases. The relative performance of these two methods compared to each other is different from the previously reported evaluation in [25] where ML-kNN outperformed Rank-SVM on average. In order to clarify this issue, we compared the performance of Rank-SVM with different values of its regularization parameter which controls the balance between the training-set loss and model complexity. We found Rank-SVM performed suboptimally with the default parameter setting,, and performed better when this parameter was tuned through cross validation. Our results of Rank-SVM are based on the properly tuned parameters, and this should explain why Rank-SVM performed stronger than ML-kNN in our experiments. Comparing Rank-SVM with IBLR-ML, each method outperformed the other in 10 out of the 20 cases. Thus the two methods have comparable performance; both are strong baselines for method comparison in MLC.
6. SIGNIFICANCE TESTS
We used the Wilcoxon signed-rank test to compare the performance of each method with that of SVM-MAP-MLC. Signed-rank is a non-parametric statistical test for pairwise comparison of methods, and a better alternative to the paired t-test when the performance scores are not normally distributed. Due to the space limit of the paper, we only present the tests based on the MAP scores of the systems. Each test instance is treated as a random event, and the average precision scores of systemgenerated ranked lists over all test instances are used to compare each pair of systems. The null hypothesis is that the system being compared with SVM-MAP-MLC is equally good; the alternative is that SVM-MAP-MLC is better. The p-values are presented in Table 3 where the null hypothesis is rejected with strong evidence in most cases. That is, the performance difference is statistically significant in 17 out of the 20 tests if using 1% p-value as the threshold.

We did not use ANOVA tests for multi-set comparison of systems because such tests have a normal-distribution assumption about the data which is inappropriate for the performance scores we use for system comparison. The Friedman test has been recently advocated for comparing classifiers on multiple datasets [4] [6], which does not have the normal assumption. However, it treats each dataset as a random event, and requires a relatively large number of datasets for meaningful testing. It is not recommended to use the Friedman test when the number of datasets is 10 or less8.
6.1 Experiments with Feature Subsets
In order to analyze the usefulness of different types of meta-level features (links), we conducted experiments with the following combinations: using the single links in L1 -norm only, using the single links in L2 -norm only, using the single links in cosine similarity, and using all the links -- including those in L1 -norm, L2 -norm, cosine similarity and the two average links per category. Figure 2 compares the performance of SVM-MAPMLC under these conditions. The different types of links have complementary effects: the single links in L1 -norm are more useful in the datasets (Emotions, Scene and Yeast) whose feature-
Figure 2. SVM-MAP-MLC with different feature sets (Performance in MAP, 1-RankLoss, Micro-F1, Macro-F1)
8 This is according to personal communication with the author of [4].

321

set sizes are relatively small than they are on the datasets (Citeseer and Reuters-21578) whose feature-set sizes are large. On the other hand, cosine-similarity based single links have a different performance pattern; single links in L2 -norm have comparable performance across the datasets. Using all the features together performs the best, showing that SVM-MAPMLC is able to assign appropriate weights to different features, and improve the robustness of its predictions by making a combined use of different features types.
7. CONCLUDING REMARKS
In this paper we produced a new approach for learning to rank categories in multi-label classification. By introducing meta-level features that effectively characterize the one-to-many relationship from instances to categories in MLC, and by formulating the category ranking problem as a standard ad-hoc retrieval problem, our framework allows a broad range of learning-to-rank retrieval algorithms to be deployed for MLC optimization with respect to various performance metrics. Using SVM-MAP-MLC as a specific instantiation of this framework, and with controlled experiments on multiple benchmark datasets, the strength of the proposed approach is strongly evident, as it significantly outperformed all the state-of-the-art methods (Rank-SVM, MLkNN and IBLR-ML) being evaluated in our experiments.
We hope this study provides useful insights into how to enhance the performance of MLC methods by improving the representation schemes for instances, categories and their relationships, and by creatively leveraging dimensionality reduction. A line of future research would be to explore our framework with other learning-to-rank algorithms, using different dimensionality reduction techniques (such as SVD or LDA), and for different optimization metrics (such ach NDCG and other types of loss functions).
ACKNOWLEDGEMENTS
This work is supported, in part, by the National Science Foundation (NSF) under grant IIS_0704689. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
REFERENCES
[1] M.R. Boutell, J.Luo, X.Shen and C.M. Brown. Learning multilabel scene classification. Pattern Recognition 2004, pages 1757--1771
[2] Robert H. Creecy, Brij M. Masand, Stephen J. Smith, David L. Waltz: Trading MIPS and Memory for Knowledge Engineering. Communications of the ACM 1992, pages 48-64 (1992)
[3] W.Cheng and E.Hüllermeier. Combining Instance-Based Learning and Logistic Regression for Multilabel Classification. Journal of Machine Learning Research 2009, pages 211-255.
[4] J. Demsar. Statistical comparisons for classifiers over multiple data sets. Journal of Machine Learning Research 2006, pages 1-30.
[5] A Elisseff and J. Weston. A kernel method for multi-labeled classification. Advances in Neural Information Processing Systems 2002, pages 681-687.
[6] S Garcia and F. Herrera. An extension on "statistical comparisons of classifiers over multiple data sets" for all

pairwise comparisons. Journal of Machine Learning Research 2008, pages 2677-2694.
[7] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann and I.H. Witten; The WEKA Data Mining Software: An Update. SIGKDD Explorations, Volume 11, 2009.
[8] T. Joachims: A support vector method for multivariate performance measures. International Conference on Machine Learning 2005, pages 377-384.
[9] T. Joachims, Making large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning, MITPress, 1999.
[10] D.D. Lewis, R.E. Schapire, J.P. Callan and R.Papka. Training algorithms for linear text classifiers. ACM SIGIR 1996, pages 298-306.
[11] F. Li and Y. Yang. A loss function analysis for classification methods in text categorization. International Conference on Machine Learning 2003, pages 472-479.
[12] P Li, C Burges, Q Wu, JC Platt, D Koller, Y Singer and S Roweis. McRank: Learning to rank using multiple classification and gradient boosting. Advances in Neural Information Processing Systems 2007.
[13] S Har-Peled, D Roth and D Zimak. Constraint classification: a new approach to multiclass classification and ranking. Advances in Neural Information Processing Systems 2002, pages 365-379.
[14] R.E. Schapire and Y. Singer . BoosTexter: A boosting-based system for text categorization. Journal of Machine learning Research 2000, pages 135-168.
[15] R.E. Schapire and Y. Singer . Improved boosting algorithms using confidence-rated predictions. Journal of Machine learning Research 1999, pages 297-336.
[16] K. Trohidis ,G. Tsoumakas, G. Kalliris, I. Vlahavas. Multilabel classification of music into emotions International Conference on Music Information Retrieval 2008.
[17] V. Vapnik, The nature of statistical learning theory, Springer verlag, New York, 2005.
[18] E.Voorhees Overview of TREC 2002, NIST Special Publication SP 2003.
[19] Jun Xu and Hang Li. AdaRank: a boosting algorithm for information retrieval. ACM SIGIR 2007, pages 391-398.
[20] Y. Yang: Expert Network: Effective and Efficient Learning from Human Decisions in Text Categorization and Retrieval. ACM SIGIR 1994, pages 13-22.
[21] Y. Yang. A Study of thresholding strategies for text categorization. ACM SIGIR 2001, pages 137-145.
[22] Y. Yang. An evaluation of statistical approaches for text classification. Information Retrieval 1999. Vol 1, No. ½, pages 67-88.
[23] Y. Yang and J.O. Pederson. A comparative study of features selection in text categorization. International Conference on Machine Learning 1997, pages 412-420.
[24] Yisong Yue, T. Finley, F. Radlinski and T. Joachims. A Support Vector Method for Optimizing Average Precision. ACM SIGIR Conference 2007.
[25] ML Zhang and ZH Zhou. ML-kNN: A lazy learning approach to multi-label learning. Pattern Recognition 2007, pages 20382048.

322

DivQ: Diversification for Keyword Search over Structured

Databases

Elena Demidova1, Peter Fankhauser1, 2, Xuan Zhou3 and Wolfgang Nejdl1
1L3S Research Center, Hannover, Germany 2Fraunhofer IPSI, Darmstadt Germany 3CSIRO ICT Centre, Australia

{demidova, fankhauser, nejdl}@L3S.de

xuan.zhou@CSIRO.au

ABSTRACT
Keyword queries over structured databases are notoriously ambiguous. No single interpretation of a keyword query can satisfy all users, and multiple interpretations may yield overlapping results. This paper proposes a scheme to balance the relevance and novelty of keyword search results over structured databases. Firstly, we present a probabilistic model which effectively ranks the possible interpretations of a keyword query over structured data. Then, we introduce a scheme to diversify the search results by re-ranking query interpretations, taking into account redundancy of query results. Finally, we propose nDCG-W and WS-recall, an adaptation of -nDCG and S-recall metrics, taking into account graded relevance of subtopics. Our evaluation on two real-world datasets demonstrates that search results obtained using the proposed diversification algorithms better characterize possible answers available in the database than the results of the initial relevance ranking.
Categories and Subject Descriptors: H.3.3
[Information Storage and Retrieval]: Information Search and Retrieval ­ Retrieval Models
General Terms: Algorithms.
Keywords: diversity, ranking in databases, query intent.
1. INTRODUCTION
Diversification aims at minimizing the risk of user's dissatisfaction by balancing relevance and novelty of search results. Whereas diversification of search results on unstructured documents is a well-studied problem, diversification of search results over structured databases attracted much less attention. Keyword queries over structured data are notoriously ambiguous offering an interesting target for diversification. No single interpretation of a keyword query can satisfy all users, and multiple interpretations may yield overlapping results. The key challenge here is to give users a quick glance of the major plausible interpretations of a keyword query in the underlying database, to enable user to effectively select the intended interpretation.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

For example, a user who issued a keyword query "London" may be interested either in the capital of the United Kingdom or a book written by Jack London, an American author. In contrast to document search, where data instances need to be retrieved and analyzed, rich database structures offer a more direct and intuitive way of diversification. For instance, if keyword "London" occurs in two database attributes, such as "location" and "name", each of these occurrences can be viewed as a keyword interpretation with different semantics offering complementary results. In addition, as in a database the query disambiguation can be performed before the actual execution, the computational overhead for retrieving and filtering redundant search results can be avoided. In the final step the database system executes only the top-ranked query interpretations to retrieve relevant and diverse results.
Applying diversification techniques for unstructured documents to keyword queries over structured databases, calls for two main adaptations. First, keyword queries need to be interpreted in terms of the underlying database, such that the most likely interpretations are ranked on top. Second, diversification should take advantage of the structure of the database to deliver more diverse and orthogonal representations of query results. In this paper we present DivQ - a novel approach to search result diversification in structured databases. We first present a probabilistic query disambiguation model to create semantic interpretations of a keyword query over a structured database. Then, we propose a diversification scheme for generating the topk most relevant and diverse query interpretations.
For evaluation, we propose -nDCG-W and WS-recall, an adaptation of -nDCG [6] and S-recall [4], to measure both relevance and novelty of search results. These two new metrics take into account graded relevance of subtopics, which is important for evaluating search results on structured data. We performed a user study to assess the quality of the disambiguation model and the diversification scheme. Our evaluation results on two real world datasets demonstrates that search results obtained using the proposed algorithms are able to better characterize possible answers available in the database than the results obtained by the initial relevance ranking.
The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 presents the diversification scheme. Section 4 introduces the new -nDCG-W and WS-recall measures. Section 5 contains the results of our empirical investigation. Section 6 provides a conclusion.
2. RELATED WORK
Recently, a number of schemes have been proposed for diversifying results of document retrieval. Several evaluation

331

schemes for result diversification have also been introduced [1, 2, 4, 6, 9, 20]. Most of the techniques (e.g. [2]) perform diversification as a post-processing or re-ranking step of document retrieval. These techniques first retrieve relevant results and then filter or re-order the result list to achieve diversification. However, this approach can hardly be applied to structured databases, where retrieval of all relevant data is usually computationally expensive [11], especially when search results have to be obtained by joining multiple tables. In contrast, DivQ embeds diversification in the phase of query disambiguation, before retrieving any search results. This offers two advantages. Firstly, as the query interpretations generated during query disambiguation have clear semantics, they offer quality information for diversification. Secondly, we avoid the overhead of generation of all relevant results. Only the results of the top ranked interpretations are retrieved from the database.
Another conventional approach to achieve diversification is clustering or classification of search results. Both techniques group search results based on similarity, so that users can navigate to the right groups to retrieve the desired information. Clustering and classification have been applied to document retrieval [1, 15], image retrieval [18], and database query results [5, 14]. Similar to result re-ranking, clustering is usually performed as a postprocessing step, and is computationally expensive. Moreover, it lacks semantic interpretations, making results less understandable by end users [10]. Although classification is more understandable,classes are usually pre-defined, without taking into account the intent of the actual query. The query interpretations of DivQ can be regarded as a special kind of classes which have well-defined semantics. In contrast to pre-defined classes, query interpretations are generated on the fly based on users' keyword queries. Thus, query interpretations are both query aware and understandable for end users. Most importantly, in contrast to existing work, we consider the similarity between query interpretations to avoid redundant search results. This enables us to further improve user satisfaction.
Chen et al. [4] employ pseudo-relevance feedback to achieve diversification of search results. Different from DivQ, they consider the intent of a query only tacitly. Wang et al. [20] focus on a theoretical development of the portfolio theory for document ranking. They propose to select top-n documents and order them by balancing the overall relevance of the list against its risk (variance). We believe that the portfolio technique can be adopted to compute diverse query interpretations in DivQ too.
Apart from diversification for document retrieval, little work has focused on diversification of search results over structured data. In [5] the authors propose to navigate SQL results through categorization, which takes into account user preferences. In [19], the authors introduce a pre-indexing approach for efficient diversification of query results on relational databases. As the categorization and diversification in both approaches is performed on the result level, these approaches are complementary to DivQ which conducts diversification on the interpretations of keyword queries without retrieving any search results.
Recent approaches to database keyword search [8, 13, 16, 17, 21] translate a keyword query into a ranked list of structured queries, also known as query interpretations, such that the user can select the one that represents her informational need. This disambiguation step is also a crucial step of DivQ. We utilize a similar probabilistic model as [8] for query disambiguation.

However, the existing query disambiguation approaches consider only the likelihood of different query interpretations rather than their diversity. As a result, users with uncommon informational needs may not receive adequate results [4]. For example, if the majority of users who issued the keyword query "London" were
interested in the guide of a city, the results referring to books written by Jack London may receive a low rank and even remain invisible to users. DivQ alleviates this problem by providing not only relevant but diverse query interpretations.

3. The Diversification Scheme
The user interface of our database keyword search is similar to that of faceted search. DivQ translates a keyword query to a set of structured queries, also known as query interpretations. Given a keyword query, a database can offer a broad range of query interpretations with various semantics. The ranked query interpretations work as facets and provide a quick overview over the available classes of results. These facets enable users to easily
navigate over the result set, choose the query interpretations that are relevant to the specific informational need and click on them to retrieve the actual search results (like in [8]). To minimize the risk of user's dissatisfaction in this environment, diversification is required to provide a better overview of the probable query interpretations, rather than a ranking based only on relevance.

Table 1. Structured Interpretations for a Keyword Query Keyword query:

CONSIDERATION CHRISTOPHER GUEST

Relev Top-3 ance interpretations

Relev Top-3 ance interpretations

ranking

diversification

0.9 A director

0.9

A director

CHRISTOPHER

CHRISTOPHER

GUEST of a movie

GUEST of a movie

CONSIDERATION

CONSIDERATION

0.5 A director

0.4

An actor

CHRISTOPHER

CHRISTOPHER

GUEST

GUEST

0.8 An actor

0.2

A plot containing

CHRISTOPHER

CHRISTOPHER

GUEST in a movie

GUEST of a movie

CONSIDERATION

...

...

...

...

Table 1 gives an example of the query interpretations for the keyword query "CONSIDERATION CHRISTOPHER GUEST", once ranked only by relevance, and once re-ranked by diversification. In this scenario, both rankings provide several possible interpretations of the query, so that the user can choose the intended one. However, ranking by estimated relevance bears
the danger of redundant results. For example, the results of the partial interpretation "A director CHRISTOPHER GUEST" which is ranked second in the top-3 ranking clearly overlap with the results of the complete query interpretation ranked first. In contrast, the diversified ranking shows a set of possible complementary interpretations with increased novelty of results.

332

3.1 Bringing Keywords into Structure
In the context of a relational database, a structured query is an
expression of relational algebra. To translate a keyword query K to a structured query Q, DivQ first obtains a set of keyword interpretations Ai:ki, which map each keyword ki of K to an element Ai of an algebraic expression. DivQ then joins the keyword interpretations using a predefined query template T [3,
11], which is a structural pattern that is frequently used to query the database. We call the structured query resulting from the translation process described above a query interpretation.

For instance, "CONSIDERATION CHRISTOPHER GUEST" is first translated into a set of keyword interpretations, which are "director:CHRISTOPHER", "director:GUEST" and "movie: CONSIDERATION". Then, these keyword interpretations are connected to a template "A director X of a movie Y" to form a query interpretation "A director CHRISTOPHER GUEST of a movie CONSIDERATION".

A query interpretation iscomplete if it contains interpretations for all keywords from the initial user query. Otherwise we talk about partial query interpretation. Given a keyword query K, the interpretation space of K is the entire set of possible
interpretations of K. In this paper, we focus on interpretations that retrieve non-empty results from the database.

3.2 Estimating Query Relevance
We estimate relevance of a query interpretation Q to the informational need of the user as the conditional probability P(Q|K) that, given keyword query K, Q is the user intended interpretation of K. A query interpretation Q is composed of a query template T and a set of keyword interpretations
I={Aj:{kj1,kjn} |AjÎT, {kj,kjn} Ì K, {ki1,kim}  {kj1,kjn} = {} for ij}. Note that {ki1,kim} need not to be consecutive in the query. Thus, the probability P(Q|K) can be expressed as:

P(Q | K ) = P(I,T | K ).

(1)

To simplify the computation, we assume that (i) each keyword has one particular interpretation intended by the user; and (ii) the probability of a keyword interpretation is independent from the part of the query interpretation the keyword is not interpreted to. Based on these assumptions and Bayes' rule, we can transform Formula 1 to:

Õ ( { } ) Õ P(Q |

K)

µ

çæ çè

Aj

Î T

P

Aj

:

k j1 , k jn

| Aj

÷ö ÷ø

´

ççèæ

Pu
kuÎK ÇkuÏQ

÷÷øö

´

P(T

),

(2)

where P(T) is the prior probability that the template T is used to form a query interpretation. P(Aj:{kj1,kjn}|Aj) represents the probability that, given that Aj is a part of a query interpretation, keyword interpretations Aj:{kj1,kjn} are also a part of the query interpretation. In case a keyword kuÎK is not mapped to any keyword interpretation in Q, we introduce a smoothing factor Pu, which is the probability that the user's interpretation of keyword ku does not match any available attribute in the database.

P(Aj:{kj1,kjn}|Aj)) can be estimated using attribute specific term frequency, i.e., the average number of occurrences of the keyword combination {k1,kjn} in the attribute Aj. Note that, when {kj1,kjn} co-occur in an attribute Aj, the joint probability P(Aj:{kj1,kjn}|Aj) will usually be larger than the product of the marginal probabilities P(Aj:{kj1}|Aj)... P(Aj:{kjn}|Aj). Thus, query interpretations that bind more than one keyword to the same attribute, for example, a first name and a last name of a person to

attribute "name", will get higher ranked than query interpretations that bind keywords to different attributes. Pu is a constant, whose value is smaller than the minimum probability of any existing keyword interpretation, such that the function assigns higher probabilities to complete query interpretations than to partial interpretations. P(T) can be estimated as a frequency of the template's occurrence in the database query log. When the query log is not available, we assume all templates to be equally probable. As indicated in Section 3.1, query interpretations with an empty result are assigned zero probability. In this case the independence assumption (ii) used in Equation 2 is obviously violated, because the query interpretation maps keywords k1 and k2 to attributes A1 and A2, such that the marginal probabilities P(A1:{k1}|A1) and P(A2:{k2}|A2) are larger than zero, but, given the instances of the database, the joint probability P(A1:{k1},A2:{k2}|A1,A2) is zero.
3.3 Estimating Query Similarity
As our objective is to obtain diverse query results, we want the resulting query interpretations to be not only relevant but also as dissimilar to each other as possible. Let Q1 and Q2 be two query interpretations of a keyword query K. Let I1 and I2 be the sets of keyword interpretations contained by Q1 and Q2 respectively. To assess similarity between the two query interpretations, we compute the Jaccard coefficient between I1 and I2.
Def. 1 (Query Similarity): We define similarity between two query interpretations Qi, and Qj as the Jaccard coefficient between the sets of keyword interpretations they contain, that is,

Sim(Q1 , Q2 ) =

I1 Ç I2 I1 È I2

.

(3)

The resulting similarity value should always fall in [0, 1], where 1 stands for the highest possible similarity.

3.4 Combining Relevance and Similarity
To generate the top-k query interpretations that are both relevant and diverse, we employ a greedy procedure. We always select the most relevant interpretation as the first interpretation presented to the user. Then, each of the following interpretations is selected based on both its relevance and novelty. Namely, given a query interpretation Q and a set of query interpretations QI that are
already presented to the user, we estimate the score of Q as its relevance score discounted by the average similarity between Q and all the interpretations in QI:

Score(Q)

=

l

×

P(Q

|

K

)-

(1-

l)×

åqÎQI

Sim(Q,
| QI |

q).

(4)

Relevance and similarity factors are normalized to equal means before -weighting is applied. The interpretation with the highest score is selected as the next interpretation to be presented to the user. In Formula 4,  is a parameter to trade-off query interpretation relevance against novelty. For example, with =1 the score of the query interpretation takes only relevance into account; = 0.5 corresponds to a balance between relevance and novelty, whereas <0.5 emphasizes novelty of the interpretation.
3.5 The Diversification Algorithm
To create a set R of the most relevant and diverse query interpretations in an efficient way we first materialize the top-k most probable query interpretations of a keyword query and sort the interpretations according to the relevance scores. Then we go

333

through the query interpretations and output the most relevant and diverse interpretations one by one. The pseudo-code of the algorithm is presented in Algorithm 1. Let L be the list of top-k query interpretations sorted by probability of their relevance to the user's informational need. The process starts with the most relevant query interpretation at the top of L. To compute the ith relevant and diverse element, i.e. R[i], we scan the remaining candidate elements in L, compare their scores in Formula 4, and add the element with the highest score to R. As the diversity value of each item is always larger than 0, it is not necessary to scan the entire L to obtain each R[i]. The scan stops when we are sure that the rest of L cannot possibly outperform the current optimal item, which is evaluated by "best_score > lP(L[j])". The algorithm terminates after r elements are selected.
Input: list L[l] of top-k query interpretations ranked by relevance Output: list R[r] of the relevant and diverse query interpretations Proc Select Diverse Query Interpretations R[0]=L[0]; i=1; //less than r elements selected while (i<r){ //select the best candidate for R[i]
j=i; best_score=0; //more candidates for R[i]in L while(L[j]!=null){
//check score upper bound if (best_score>lP(L[j])) break; if (score(L[j])>best_score){
best_score=score(L[j]); c=j); } j++; } //add the best candidate to R R[i] =L[c]; Swap L[i...c-1] and L[c]; i++; } End Proc;
Algorithm 1. Select Diverse Query Interpretations
The worst case complexity of the Algorithm 1 is O(l*r), where l is the number of query interpretations in L and r is the number of query interpretations in the result list R. The maximal total number of similarity computations is (l2-l)/2.
4. EVALUATION METRICS
-NDCG [6] and S-recall [4] are established evaluation metrics for document retrieval in presence of diversity and subtopics. As results of keyword search over structured data differ from conventional documents, these metrics require some adaptation.
A search result of DivQ is a ranked list of query interpretations. Therefore, a "document" in traditional IR corresponds to the union of tuples returned for one particular query interpretation in DivQ. Each tuple can be represented by its primary key in the database. Thus, a primary key corresponds to the notion of information nugget in -NDCG and to subtopic in S-recall. However, the correspondence is loose: whereas -NDCG and S-recall assume equal relevance of information nuggets and subtopics contained in a document, relevance of primary keys in a query result may vary

a lot. Thus it is important to take into account their relevance for estimating gain and recall explicitly. In the following, we adapt NDCG and S-recall to this end.

4.1 Adapting Gain for -NDCG-W
nDCG (normalized Discounted Cumulative Gain) has established itself as the standard evaluation measure when graded relevance values are available [12, 6]. The first step in the nDCG computation is creation of a gain vector G. The gain G[k] at rank k can be computed as the relevance of the result at this rank to the user's keyword query. The gain may be discounted with increasing rank, to penalize documents lower in the ranking, reflecting the additional user effort required to reach them. The discounted gain is accumulated over k to obtain the DCG
(Discounted Cumulative Gain) value and normalized using the ideal gain at rank k to finally obtain the nDCG value.

To balance relevance of search results with their diversity, the authors of [6] proposed -nDCG, where the computation of the gain G[k] is extended with a parameter , representing a tradeoff between relevance and novelty of a search result. To assess novelty of a document in the search result, -nDCG views a document as the set of information nuggets. If a document at rank i contains an information nugget n, -NDCG counts how many documents containing n were seen before and discounts the gain of this document accordingly.  has a value in the interval [0, 1];
=0 means that -NDCG is equivalent to the standard nDCG measure. With increasing , novelty is rewarded with more credit. When  is close to 1, repeated results are regarded as completely redundant such that they do not offer any gain. In [20], the authors fix  as 0.5 for a balance between relevance and novelty.

In the context of database keyword search, where an information nugget corresponds to a primary key, the relevance of nuggets with respect to the user query can vary a lot. To reflect the graded relevance assessment on the nuggets in the evaluation metrics, NDCG-W measures the gain G[k] of a search result at rank k as the relevance of the query interpretation at rank k, i.e., Qk. We penalize the gain of an interpretation retrieving overlapping results using the following formula:

G[k] = relevance(Qk ) × (1-a )r ,

(5)

where r is the factor, which expresses overlap in the results of the query interpretation Qk with results of the query interpretations at ranks 1...k-1.

To compute r, for each primary key pki in the result of Qk we count how many query interpretations with pki were seen before (i.e. at ranks 1...k-1), and aggregate the counts:

r = åpkiÎQk å jÎ[1,k-1] pki ÎQ j .

(6)

Note that we consider primary keys in the result of one interpretation to be distinct (each primary key counts only once).

As in document retrieval the presence of a particular information nugget in a document is uncertain, the gain computation in [6] focuses on the number of nuggets contained in a document and does not take into account graded relevance of information
nuggets. In contrast, in the context of database keyword search an information nugget in -nDCG-W corresponds to a primary key in the result of a query interpretation, such that the presence of an information nugget in the result is certain. At the same time, relevance of retrieved primary keys with respect to the user query can vary a lot. This graded relevance is captured by Equation 5.

334

Agrawal et al. [1] suggest an alternative approach called NDCGIA (for Intent Aware NDCG) to take into account graded relevance of information nuggets to queries. However, a drawback of NDCG-IA is that it may not lie between [0, 1]. Moreover, NDCG-IA does not take into account result overlap. In contrast,
the value of relevance-aware -nDCG takes into account result overlap and is always in the interval [0, 1], where 1 corresponds to ranking according to the user assessment of query interpretation relevance averaged over users.

4.2 Weighted S-Recall
Instance recall at rank k (S-recall) is an established recall measure which is applied when search results are related to several subtopics. S-recall is the number of unique subtopics covered by the first k results, divided by the total number of subtopics [4, 20].

In database keyword search, a single primary key in the search
result corresponds to a subtopic in S-recall. However, other than in document retrieval, where all subtopics can be considered equally important, relevance of retrieved primary keys (tuples) can vary a lot with respect to the user query. To take the graded relevance of subtopics into account, we developed a WS-recall measure (weighted S-recall). WS-Recall is computed as the aggregated relevance of the subtopics covered by the top-k results (in our case query interpretations) divided by the maximum possible aggregated relevance when all relevant subtopics are covered:

WS

-

recall@ k

=

( ) åpkÎQ1...k relevance pk åpkÎU relevance( pk)

,

(7)

where U is the set of relevant subtopics (primary keys). In case only binary relevance assessments are available, WS-recall corresponds to S-recall. We average WS-recall at k and -NDCG at k over a number of topics to get their means over the query set.

5. EXPERIMENTS
To assess the quality of the disambiguation and diversification schemes we performed a user study and a set of experiments.
5.1 Dataset and Queries
In our experiments, we used two real-world datasets: a crawl of the Internet Movie Database (IMDB) and a crawl of a lyrics database from the web. The IMDB dataset contains seven tables, such as movies, actors and directors, with more than 10,000,000 records. The Lyrics dataset contains five tables, such as artists, albums and songs, with around 400,000 records. As these datasets do not have any associated query log, we extracted the keyword queries from the query logs of MSN and AOL Web search engines. We pruned the queries based on their target URLs, and obtained thousands of queries for the IMDB and lyrics domains.
To obtain the most popular keyword queries, we first sorted the queries based on frequency of their usage in the log. For each domain, we selected 200 most frequent queries for which multiple interpretations with non-empty results exist in the database. These queries were mostly either single keyword or single concept queries, often referring to actor/artist names or movie/song titles. We refer to this part of the query set as single concept queries

(sc). To obtain an additional set of more complex queries, we manually selected about 100 queries for each dataset from the query log, where we explicitly looked for queries containing more than one concept, e.g. a movie/song title and an actor/artist name. We refer to this set as multi-concept queries (mc).

As diversification of results is potentially useful for ambiguous queries [7], we estimated ambiguity of the resulting keyword
queries using an entropy-based measure. To this end, for each keyword query, we ranked interpretations of this query available in the database using Formula 2 and computed the entropy in the top-10 ranks of the resulting list. Intuitively, given a keyword query, high entropy over the top ranked interpretations indicates potential ambiguity. Finally, we selected 25 single concept and 25 multi-concept queries with the highest entropy for each dataset.

5.2 User Study
To assess relevance of possible query interpretations we performed a user study. We selected a mix of single and multiconcept queries as described in Section 5.1, for which we generated all possible interpretations sorted by their probability. As the set of possible interpretations grows exponentially with the number of concepts involved, we took at most the top-25 interpretations. This should not rule out meaningful interpretations, as probabilities fall very quickly with their rank: Figures 1a and 1b give the maximum and the average ratio of the
probability of a query at rank i and the aggregated probabilities of
( ) queries at rank j<i: PRi = P(Qi | K) / åj<i P Q j | K .

1

Max PR

0.8

Avg PR

0.6

0.4

0.2

0

0

5

10

15

20

25

Figure 1a. Maximum and Average Probability Ratio, IMDB.

1 Max PR

0.8

Avg PR

0.6

0.4

0.2

0

0

5

10

15

20

25

Figure 1b. Maximum and Average Probability Ratio, Lyrics.

335

1

0.9

0.8

0.7

0.6

0.5

0.4 0.3 0.2 0.1
0

-NDCG-W, =0.0 Rank mc
-NDCG-W, =0.0 Div mc -NDCG-W, =0.0 Rank sc -NDCG-W, =0.0 Div sc

1 2 3 4 5 6 7 8 9 10

1

0.9

0.8

0.7

0.6

0.5

0.4 0.3 0.2 0.1

-NDCG-W, =0.0 Rank mc -NDCG-W, =0.0 Div mc -NDCG-W, =0.0 Rank sc -NDCG-W, =0.0 Div sc

0

1 2 3 4 5 6 7 8 9 10

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4 0.3 0.2 0.1
0

-NDCG-W, =0.5 Rank mc -NDCG-W, =0.5 Div mc -NDCG-W, =0.5 Rank sc -NDCG-W, =0.5 Div sc

0.4 0.3 0.2 0.1
0

-NDCG-W, =0.5 Rank mc -NDCG-W, =0.5 Div mc
-NDCG-W, =0.5 Rank sc -NDCG-W, =0.5 Div sc

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4 0.3 0.2 0.1
0

-NDCG-W, =0.99 Rank mc -NDCG-W, =0.99 Div mc -NDCG-W, =0.99 Rank sc
-NDCG-W, =0.99 Div sc

0.4 0.3 0.2 0.1
0

-NDCG-W, =0.99 Rank mc -NDCG-W, =0.99 Div mc -NDCG-W, =0.99 Rank sc -NDCG-W, =0.99 Div sc

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Figure 2a. -NDCG-W, IMDB.

Figure 2b. -NDCG-W, Lyrics.

(-NDCG-W for single concept (sc) and multi-concept (mc) queries for diversification (Div) and ranking (Rank) for =0, 0.5, and 0.99)

Each data point on the X-axis of Figures 1a and 1b presents the rank. The Y-axis presents the corresponding average and maximum PRj value. As can be seen, queries at rank 10 already are only 0.01 as likely as queries at rank < 10, and queries at rank 25 are at most 2.95E-04 as likely as queries at rank < 25.
For each query we pruned all query interpretations Qi whose probability constituted less than 0.1% of the aggregated probability of all possible interpretations. Additionally, for each query we included at most five more interpretations with probability below this threshold and randomized the order in which the interpretations were presented for user assessment, in order to avoid a bias towards top ranked queries.

In total, each user had to evaluate 630 interpretations for IMDB
and 517 interpretations for the Lyrics dataset. For each interpretation of a given keyword query, the participants were asked to indicate on a two-point Likert scale, if they think that this interpretation could reflect an informational need implied by the keyword query. Multiple interpretations of one query were possible and explicitly encouraged. In total, we had 16 participants, from whom 10 completed all evaluation tasks in both datasets and the rest completed 30% of tasks in IMDB and 9% of
tasks in lyrics dataset on average. We computed agreement between the participants using kappa statistics [15]. We observed average kappa values of 0.33 in IMDB and 0.28 in Lyrics. We consider this low agreement as an additional indication of

336

ambiguity of the selected queries. Finally, we computed the relevance scores of each query interpretation by averaging scores over the participants.
5.3 -nDCG-W
Given a keyword query, DivQ first creates a ranked list of query interpretations and then applies the diversification algorithm to this list to obtain the most relevant and novel results. To assess quality of query ranking and diversification, we measured NDCG-W by varying  parameter from 0, to 0.5 and to 0.99. In the case of =0, novelty of results is completely ignored, and NDCG-W corresponds to the standard NDCG. With =0.5, novelty is given a certain credit. With =0.99, novelty becomes crucial, and results without novelty are regarded as completely redundant. As the optimal ranking for normalization of DCG we ranked query interpretations by their user score. To achieve better overview of the available results in this experiment we set =0.1 (Equation 4); this enables the system to emphasize novelty of results in both datasets. We discuss the influence of  value in Section 5.5. The results of the -NDCG-W evaluation are presented in Figures 2a, 2b.
Each diagram of Figures 2a and 2b corresponds to a different  value. Each data point of the X-axis of a diagram represents the k for top-k query interpretations. The Y-axis represents the corresponding -NDCG-W value. We use the symbol Rank to denote the ranking algorithm without diversification, and Div to denote the ranking algorithm with diversification. The -NDCGW values in the diagrams are averaged on single concept queries (sc) and multi-concept queries (mc) respectively. As we can see, in our experiments on the IMDB dataset (Figure 2a), the average -NDCG-W for top-1 result of both ranking and diversification on single concept queries was always 0.58, given any value of . For top-5 results, the gain of single concept queries increased to 0.9 in both datasets. For multi-concept queries, with =0 the gain of ranking reaches 0.8 and 0.9 at top-6 in IMDB and Lyrics respectively. The relatively high -NDCG-W values for =0 confirm the quality of the ranking function.
As Figures 2a and 2b show, for =0 ranking dominates diversification in all the cases. This is expected, as for -values below 0.5, relevance is rewarded over novelty. In this case, diversification does not show its benefit. In the Lyrics dataset, the first benefits of diversification for single concept queries become visible already with =0.5 at k=4, where -NDCG-W improves by about 4%. This advantage increases with growing , and achieves 8% at =0.99. For single concept queries on IMDB, we did not observe any difference between ranking and diversification (the lines Rank sc and Div sc almost overlap in all diagrams of the Figure 2a). This is because the top query interpretations returned by ranking already deliver distinct results. In this case diversification preserves the high gain values achieved by ranking. For multi-concept queries, the gain of diversification grows with increasing . When =0.99 and k>3, diversification on mc queries outperforms ranking by about 7% in both datasets. The results of the paired ttest confirm statistical significance of this result for the confidence level of 95%. In summary, diversification performed on top of query ranking achieves significant reduction of result redundancy, while preserving retrieval quality in the majority of the cases.
5.4 WS-recall
We evaluate recall quality of the system using the WS-recall measure presented in Section 4.2. WS-recall computation requires

user assessments of subtopic relevance. As graded relevance assessments of top query interpretations were available to us as a result of the user study, we compute relevance of a subtopic (primary key) as the relevance of the interpretation which returns this primary key. As one and the same primary key can be returned by multiple distinct query interpretations, we take the maximal score. As user judgments were available only for a subset of the interpretation space, the absolute recall values obtained by this approach might be too optimistic. However, they enable a fair comparison of the algorithms. We present the results of the WS-recall evaluation in Figures 3a and 3b.
1

0.8

0.6

0.4

0.2

avg WS-recall Rank

0

avg WS-recall Div

1 3 5 7 9 11 13 15 17 19 21 23 25

Figure 3a. WS-recall for Ranking and Diversification, IMDB. 1

0.8

0.6

0.4

0.2

avg WS-recall Rank

0

avg WS-recall Div

1 3 5 7 9 11 13 15 17 19 21 23 25

Figure 3b. WS-recall for Ranking and Diversification, Lyrics.
Each data point of the X-axis of Figures 3a and 3b corresponds to k for top-k interpretations. The Y-axis represents the corresponding WS-recall value of ranking (Rank) and diversification (Div) averaged over a set of queries. For example, in the Lyrics dataset (Figure 3b) the WS-recall of ranking increased from 0.2 in top-1 to 0.8 in top-6. As Figures 3a, 3b show, on average, ranking and diversification perform similar with respect to recall. We observed a slight improvement by diversification for k=2...11 in the IMDB dataset, whereas in Lyrics WS-recall at corresponding k values slightly decreased. Inspection of the actual query interpretations reveals that this is mainly due to the fact that ranking by relevance in Lyrics prefers complete query interpretations with large result sizes (i.e. large total number of returned tuples), whereas diversification pushes partial query interpretations with smaller result sizes. All other things equal, a larger result size increases WS-recall more. Normalizing result sizes for WS-recall is subject to future work.
In total we did not observe any significant effect of diversification on WS-recall values.
5.5 Balancing Relevance and Novelty
In Equation 4,  is a parameter to balance query interpretation relevance against novelty. We evaluated influence of  on NDCG-W at top-5 by =0.99.

337

1 0.95

Div sc

0.9 Rank 0.85 sc

Div mc

0.8

Rank

0.75

mc

0.7

=0.01

=0.1

Rank sc Div sc Rank mc

=0.5

=1

Figure 4. -NDCG-W, k=5, =0.99, Lyrics.
The results on the lyrics dataset are presented on Figure 4. The Xaxis of Figure 4 presents the values of . The Y-axis represents the corresponding value of -NDCG-W at top-5 by =0.99. Each bar on Figure 4 presents -NDCG-W for ranking and diversification of single concept (sc) and multi-concept (mc) queries averaged over a set of queries. For example, the average -NDCG-W of diversification for single concept queries increased from 0.82 by =1 to 0.91 by =0.01. As can be seen, high -NDCG-W values achieved by diversification of both, single concept and multiconcept queries decrease with increasing , until they meet NDCG-W of the original ranking in =1. The smaller the value of , the more visible is the impact of diversification and the more NDCG-W values of diversification outperform the original ranking. In contrast, with increasing , relevance of query interpretations dominates over novelty and the amount of reranking achieved by diversification becomes smaller. For example, for = [0.01, 0.5] the Spearman's rank correlation coefficient between the ranks of the query interpretations in the initial ranking and their ranks after diversification ranges between [0.74, 0.82] for single concept queries and [0.4, 0.67] for multiconcept queries in Lyrics. In the IMDB dataset multi-concept queries perform similar with rank correlation of [0.36, 0.69]. As different interpretations of single concept IMDB queries already deliver distinct results, we did not observe any significant reranking by varying  on this query set.
6. CONCLUSION
In this paper we presented an approach to search result diversification over structured data. We introduced a probabilistic query disambiguation model to create relevant query interpretations over the structured data and evaluated the quality of the model in a user study. Furthermore, we proposed query similarity measure and a greedy algorithm to efficiently obtain relevant and diverse query interpretations. We proposed an adaptation of the established evaluation metrics to measure quality of diversification in database keyword search. Our evaluation results demonstrate the quality of the proposed model and show that using our algorithms the novelty of keyword search results over structured data can be substantially improved. Search results obtained using the proposed algorithms are also better characterize possible answers available in the database than the results obtained by the initial relevance ranking.
7. ACKNOWLEDGMENTS
This work is partially supported by the FP7 EU Projects OKKAM (contract ICT-215032) and LivingKnowledge (contract 231126).

8. REFERENCES
[1] Agrawal, R., Gollapudi, S., Halverson, A., & Leong, S. Diversifying Search Results. WSDM 2009.
[2] Carbonell, J., & Goldstein, J. The use of MMR, DiversityBased Reranking for Reordering Documents and Producing Summaries. In Proceedings of the SIGIR 1998.
[3] Chakaravarthy, V. T., Gupta, H., Roy, P., & Mohania, M. Efficiently Linking Text Documents with Relevant Structured Information. In Proceedings of the VLDB 2006.
[4] Chen, H., & Karger, D. R. Less is More. Probabilistic Models for Retrieving Fewer Relevant Documents. SIGIR'06
[5] Chen, Z., & Li, T. Addressing Diverse User Preferences in SQL-Query-Result Navigation. SIGMOD 2007
[6] Clarke, C. L., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., Büttcher, S., MacKinnon, I. Novelty and Diversity in Information Retrieval Evaluation. SIGIR 2008.
[7] Clough, P., Sanderson, M., Abouammoh, M., Navarro, S., Paramita, M.: Multiple Approaches to Analysing Query Diversity. In Proceedings of SIGIR2009.
[8] Demidova, E., Zhou, X. & Nejdl, W. IQP: Incremental Query Construction, a Probabilistic Approach. In ICDE 2010.
[9] Gollapudi, S., Sharma, A. An Axiomatic Approach for Result Diversification. In Proceedings of WWW 2009.
[10] Hearst, M. A. Clustering versus Faceted Categories for Information Exploration. Commun, ACM 49, April 2006.
[11] Hristidis, V., Gravano, L., Papakonstantinou, Y. Efficient IRStyle Keyword Search over Relational Databases. VLDB 03.
[12] Järvelin, K., & Kekäläinen, J. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst., 2002.
[13] Kandogan, E., Krishnamurthy, R., Raghavan, S., Vaithyanathan, S., & Zhu, H. Avatar Semantic Search: A Database Approach to Information retrieval. SIGMOD 2006.
[14] Liu, B., & Jagadish, H. V. Using Trees to Depict a Forest. In Proceedings of the VLDB 2009.
[15] Manning, C. D., Raghavan, P. and Schütze, H. Introduction to Information Retrieval, Cambridge University Press. 2008.
[16] Tata, S., & Lohman, G. M. SQAK: doing more with keywords. In Proceedings of the SIGMOD 2008.
[17] Tran, T., Cimiano, P., Rudolph, S., & Studer, R. Ontologybased Interpretation of Keywords for Semantic Search. In Proceedings of the ISWC 2007.
[18] vanLeuken, R., Pueyo, L., Olivares, X., & Zwol, R. Visual Diversification of Image Search Results. WWW 2009.
[19] Vee, E., Srivastava, U., & Shanmugasund, J. Efficient Computation of Diverse Query Results. ICDE 2008.
[20] Wang, J., & Zhu, J. Portfolio Theory of Information Retrieval. In Proceedings of the SIGIR 2009.
[21] Zhou, Q., Wang, C., Xiong, M., Wang, H., Yu, Y. SPARK: Adapting Keyword Query to Semantic Search. ISWC 2007.

338

Finding Support Sentences for Entities

Roi Blanco
Yahoo! Research Barcelona, Spain
roi@yahoo-inc.com

Hugo Zaragoza
Yahoo! Research Barcelona, Spain
hugoz@yahoo-inc.com

ABSTRACT

entities and the number of entity search applications has

We study the problem of finding sentences that explain the relationship between a named entity and an ad-hoc query, which we refer to as entity support sentences. This is an important sub-problem of entity ranking which, to the best of our knowledge, has not been addressed before. In this paper we give the first formalization of the problem, how it can be evaluated, and present a full evaluation dataset. We propose several methods to rank these sentences, namely retrievalbased, entity-ranking based and position-based. We found that traditional bag-of-words models perform relatively well when there is a match between an entity and a query in a given sentence, but they fail to find a support sentence for a substantial portion of entities. This can be improved by incorporating small windows of context sentences and ranking them appropriately.

increased: finding the most important dates or events for a query, the most important locations, companies and so on.
However, presenting a list of entities to the user without any explanation is not sufficient: the entity needs to be contextualized for the user to decide its relevance and relationship with the query, very much like snippets help users to select documents from a ranked list of query results. In this paper we tackle the problem of retrieving and ranking sentences that explain the interest (or relevance) of an entity with respect to a query; we call these sentences entity support sentences. Note that we are not interested in the entity ranking task (choosing which entities are relevant). We are only interested in finding explanations for relevant entities; this is in fact a sentence ranking task.
As an illustration, we discuss some examples of good entity support sentences. One can observe different types of

Categories and Subject Descriptors

support sentences, depending on the generality of the query and the entity, their type of relationship, etc. We tried to

H.3.3 [Information Storage And Retrieval]: Informa-

represent these types in the examples in Table 1. For exam-

tion Search and Retrieval--retrieval models; H.3.4 [Information ple, support sentence (5) is a typical definition: it defines the

Storage And Retrieval]: Systems and Software--perfor-

entity, and in doing so, it clarifies the relationship with the

mance evaluation (efficiency and effectiveness)

query. This is perhaps the easiest type of sentence, since it is

query independent, and often used in practice (for example,

General Terms

systems that display the first paragraphs of the Wikipedia entry corresponding to the entity, regardless of the query).

Algorithms, Experimentation, Performance

Nevertheless, definitions are insufficient in many cases. For

example, support sentence (1) is not a simple definition of

Keywords
Sentence Retrieval, Entity Ranking

the entity Picasso (e.g. a XXth century painter), it specifically addresses the "peace" aspect of the query. Note that some support sentences have partial or no matches with the

1. INTRODUCTION

query terms. There are several reasons why this might happen: the usual synonymy or anaphora problems in IR (as in

Ranking entities with respect to a query has become a standard information retrieval (IR) task, often referred to as entity ranking. People and expert search are the best known entity ranking tasks, which have been conveniently

(3), where Picasso is referred to as "the author"), or more complex sentences requiring some domain knowledge and inference (as in (5)).
The main contributions of this paper are:

evaluated in the Text REtrieval Conference (TREC [27]) in the past years [21, 22, 2]. Recently, the different types of

· a formalization of this problem, which to our knowledge has not been addressed by the IR community so

far,

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

· an evaluation framework and an initial evaluation dataset,
· an empirical evaluation of several families of methods, including bag-of-words, entity-ranking based and position-dependent ranking, showing that including a small weighted context window of surrounding sentences improves performance of sentence retrieval.

339

Table 1: Examples of entities support sentences. Query: Picasso and peace Entity: Picasso. Support Sentence: "In 1944 Picasso joined the French Communist Party, attended an international peace conference in Poland, and in 1950 received the Stalin Peace Prize from the Soviet government." (1) Entity: 1944 Support Sentence: (same as above) Entity: Northern Spain Support Sentence: "Although it was not conceived by the author as a representation of the disasters of war, but the Nazi bombing of Guernica (a town in Northern Spain), it is now considered an iconic representation of the disasters of war." (3) Query: Spanish Civil War Entity: International Brigades Support Sentence: "The International Brigades were Republican military units in the Spanish Civil War, formed of many non-state sponsored volunteers of different countries who traveled to Spain, to fight for the republic in the Spanish Civil War between 1936 and 1939." (4) Entity: Franco Support Sentence: "In 1936, Franco participated in a coup d'etat against the elected Popular Front government." (5) Query: Lennon and religion Entity: George Harrison Support Sentence: "During the late 1960s, bandmate George Harrison became interested in Eastern mysticism; Lennon dismisses Harrison 's beloved gurus and Hare Krishna mantra as "pie in the sky". (6)
Undoubtedly this task is closely related to other tasks in IR, such as sentence retrieval, query expansion, and entity ranking, as detailed in Section 2. However, we believe it is important to clearly formalize this task separately, because its particular characteristics make this problem hard and interesting. A fundamental difference is that the number of entities potentially covered by a single query might be in the order of thousands or millions, which prevents to devise a solution consisting of issuing a single query to find the support sentences for every different entity. Instead, it is more appealing to work with the original set of retrieved sentences and perform some re-ranking among them. In fact, we will extend on this idea to develop context-aware ranking methods which outperform single bag-of-words ranking for traditional sentence retrieval.
The remainder of this paper is organized as follows. Section 2 reviews related work on the IR field and compares it to the support sentence retrieval task. Section 3 presents some sentence-entity ranking features, and Section 4 describes more in depth the dataset manually annotated to evaluate support sentences retrieval. Section 5 presents results of different ranking models and a discussion about the importance of the role of context sentences. The paper ends with conclusions and future work.

2. RELATED WORK
Entity search first appeared in corporate search engines such as Microsoft Office SharePoint Server or Autonomy. These applications deal mostly with metadata entities (like author, document type, or other company-defined annotations) but have recently started to include entities directly extracted from sentences (SAS TextMiner for example). Unfortunately, there is no literature publicly available about the methods used in these applications.
More recently, a number of commercial online applications have sprung dealing specifically with entity searching on the Web (either on the entire Web or on high informational online content such as news and blogs). There are too many to list them all here and little is known about their algorithms, some examples are Evri1, TextMap 2, Yahoo! Correlator3 or ZoomInfo4. They differ greatly on their sources, user interfaces, entity types, search capabilities and features, as well as the modality of the information. However with respect to their entity ranking functionality they are all very similar: they allow users to search and browse entities related to a topic or to another entity. And they all require solving the task discussed here: finding support sentences for entities.
For example Evri proposes an entity search and browsing site which shows relations between entities as well as links to news stories or web pages where the entities are mentioned. Although it does not provide (yet) a free ad-hoc keyword search, it is possible to search for an entity in the context of another. For example a search for the query Bill Clinton returns a definition of the entity, but when one then selects the entity Korea (in the context of Bill Clinton), Evri brings up snippets relating these two entities, instead of a definition of Korea. Finding those snippets is a special case of the task discussed here, where the queries have to be entities themselves.
Another example is Yahoo! Correlator where one can type ad-hoc queries and choose a search type (names, locations, events or concepts). For each search type there is an associated user interface which shows entities relevant to the query; when one hovers over the entities, sentences are shown explaining the relationship between the query and the entity. For example, for the query "Picasso and peace" the entity Neruda appears; when one hovers over it one obtains a sentence that does not define Neruda, but rather states the relationship of Neruda to the query5. This is another example of the task of entity support sentence ranking.
Academic research became interested in entity ranking recently, and several evaluation campaigns and competitions have been launched, the most important ones being TREC and INEX. So far research has concentrated with the main problem: identification and ranking of entities. To our knowledge there have been no papers published addressing other entity ranking problems such as generation or ranking of sentences.
Named Entities Recognition (NER) has recently attracted
1http://www.evri.com/ 2http://www.textmap.com/ 3http://sandbox.yahoo.com/Correlator 4http://www.zoominfo.com/ 5"Pablo Picasso arranged his entrance into Paris and Neruda made a surprise appearance there to a stunned World Congress of Peace Forces, the Chilean government meanwhile denying that the poet could have escaped the country."

340

much research interest in the IR community. For instance, the 1st Entity Workshop held in 2009 in TREC is composed of three search tasks which involve entity-related search on Web data. The tasks to be addressed are motivated by the expert finding task at the TREC Enterprise track [21, 22, 2]. INEX has also been running constantly since 2006 an entity track, where the goal is to match a textual query to a predefined set of entities that are usually mentioned in text documents and/or described explicitly [11, 28, 5].
The task of finding support sentences can be seen as a specialized form of sentence retrieval, where sentences need to be relevant to both a query and the entity being supported. There is an important body of work on sentence retrieval (see the book from Murdock [13], and references therein). Research in sentence retrieval has been driven by two main topics: relevance retrieval and novelty detection, both mainly geared towards text summarization, question answering [3], topic detection and tracking [24] or a combination of any of them [25]. Li and Croft [9] employ named entity recognition techniques to improve novelty detection in sentence retrieval. They filter out sentences which do not contain specific entity-patterns in the result set and re-rank sentences accordingly. Sentence retrieval has also been employed to assist Question Answering systems (QAs); the motivation is to select a small set of sentences which may contain the answer to a given question and employ QA strategies to them instead to whole documents [3]. None of these works deal with the issue of explaining a query-entity relationship. The closest is perhaps [4] which discusses sentence ranking models where the query includes a constraint on a type of entity (e.g. a location, a person). While these models are interesting and could probably be used to speed up some of the entity support sentence rankings, this work is not directly related to ours.
One way to map our task to the problem of sentence retrieval is to merge the query and the entity into a normal query. In that way we can obtain sentences that are relevant both to the entity and the query, obtaining a good candidate entity support sentence. However, this approach is impractical due to its computational complexity: it requires executing one query per retrieved entity (potentially thousands). . This is also the reason precluding snippet generation techniques [8] from being applied directly to this problem. Instead, we explore methods that do not require issuing any subsequent queries to the retrieval system. On the contrary, all the methods introduced in this paper select one or more support sentences for a query and entity by re-ranking the top retrieved sentences for a given query. A problem of this approach is that of exact-match methods: we would only re-rank and retrieve sentences that partially match the original query. The vocabulary mismatch problem is particularly problematic in sentence retrieval [10] because sentences are short pieces of text and their probability of being relevant to a query term that is not mentioned explicitly in them is higher. We address this issue by introducing a relatively small context window of non-matching sentences surrounding matching sentences (section 5.3). Although non standard, this is quite a natural thing to try, and can be integrated in a relatively simple way in most ranking model paradigms. As an example, this is equivalent to smooth locally a sentence language model using surrounding sentences as proposed in [15]. In this paper we propose to use BM25F to integrate context into ranking, and show that

it is effective even for the standard TREC sentence retrieval task.

3. FEATURES FOR RANKING SUPPORT SEN-
TENCES
In this section we introduce the notation used in the paper and describe several features for the problem of entity support sentence ranking.
First we assume that we have a collection of documents, which can be segmented into sentences s  S (more generally these could be paragraphs or text windows of fixed size). It will also be useful to consider the sentence's context Cs; this context can be defined as surrounding sentences, a passage, the document's title or even the entire document.
We further assume that entities in the collection have been annotated in the text (as a result of automatic or manual information extraction). Entities in the collection are denoted by e  E. We represent the presence of an entity in a sentence via the matrix G  {0, 1}|S|×|E|, where Gij = 1 if entity j is present (mentioned) in sentence i, and 0 otherwise. Alternatively we can see G as a bipartite graph connecting each sentence to the entities mentioned in it. Matrix G is sometimes referred to as an entity containment graph [29, 18]. We will sometimes use the shorthand notation e  s to denote Gse = 1.
Our goal is to find a good model for ranking entity support sentences (Hqe(s)), that scores triples (sentence,query,entity) using sentence scores coming from a retrieval function (Fq(s)) and entity scores (E(q, e)).
We define the top-k relevant sentences for query q as:

Sq = {s | rankq(s) < k} ,

(1)

where k is a global parameter. One possible way to incorporate the context into the result set is to augment Sq with:

S^q = {s | s  Sq, s  Cs }.

(2)

The set of candidate support sentences6 for an entity e is

defined as:

Sqe = {s | s  Sq , Gse = 1} ,

(3)

and

S^qe = {s | s  S^q , Gse = 1}.

(4)

The problem of finding entity support sentences can now be formalized as that of assigning a score Hqe(s) to the candidate support sentences.
Now let us discuss some features. A first trivial feature is to use the original score of the sentence. The sentence score Fq(s) can be obtained by any ad-hoc ranking method such as TF-IDF, BM25, language models, etc. In this paper we will use BM25 (see Section 5.1). Formally:

Hqe(s) = Fq(s)  s  Sqe

(5)

Ranking sentences using Equation (5) trusts a retrieval scoring function for determining the relevance between query and entity. In other words; it is equivalent to order sentences with respect to their relevance score with respect to

6In theory, it is possible that a support sentence does not mention e (due to anaphora) but this is rare and not studied it in this paper.

341

the given query (without considering the entity) and then filtering out all the sentences not containing the entity. We can also take into account the context of a sentence inside the ranking function, replacing Fq(s) by a context-aware model Fq(s, Cs). There are different ways to do this, such as using query expansion, smoothing, or structural ranking functions. In this paper we will use BM25F (see Section 5.1), which allows to score multiple fields; we put Cs in a context field, separate from the sentence field s. Formally:

Hqe(s) = Fq(s, Cs)  s  Sqe

(6)

These two features take into account the statistics and distribution of terms in sentences. Instead, we can also look into the statistics of the entities. Therefore, a second ranking method could take the scores of entities in the sentence into account (E(q, e), detailed next in section 3.1):

(P

Hqe(s) =

e s E(q, e ), 0,

if e  s if e / s

 s  S^qe (7)

Note that here we employ a summation but other aggregation functions (such as the average, max and min) are also possible.
Another interesting feature of sentences is the position in which the entity and the query terms are found. We tried several heuristic position-dependent models; as an example we report the best performing one: the distance between the last match of query and entity, and the length of the sentence:
Heq(s) = length(s) - max(position(q), position(e)) (8)
where position(q) = 0 if none of the query terms are present in s. This can be regarded as a proxy for deeper linguistic features, since important elements of a sentence tend to occur in higher levels of the syntactic dependencies tree; terms that are central to a given sentences tend to appear in lower levels.
We have also explored features derived from the analysis of the entity containment graph topology as discussed in [18], but we could not obtain interesting results. For lack of space we decided to omit their description and results.

3.1 Entity Ranking
In order to find different models for Hqe(s) we will take advantage of some entity ranking methods and incorporate them into Hqe.
We describe next some simple and efficient methods for ranking entities; all of them are applied in the context of Equation (7) substituting the E(q, e) function.
One of the simplest entity ranking methods is the number of relevant sentences containing it (hereinafter frequency):

EF REQ(q, e) = |Sqe|

(9)

To penalize very frequent entities (such as entity descriptors like "person"), we use the entity inverted sentence frequency [29] (hereinafter rarity):

ERARIT Y

(e,

q)

=

log

P |S| sS Gse

(10)

This is similar to the traditional inverse document frequency [16].

Combining these two measures we obtain a very accurate entity ranking function [29], which resembles the well-known TF-IDF weighting scheme [7] (hereinafter combination)

ECOMB(q, e) = EF REQ(q, e) · ERARIT Y (e) (11)

In [26] a slightly different measure was presented, inspired in the cross entropy of the query and collection distributions and measured with KL-divergence (hereinafter KLD):

EKLD (q,

e)

=

P

(e|q )

log

P (e|q) P (e|S)

(12)

where

P

P (e|q)

=

|Sqe | |Sq |

,

P (e|S) =

sS Gse |S|

(13)

We note that the query and sentence models q and S can be parametrized to account for smoothed probability estimations, though in Equation (13) we assume simple parameterfree count-based models.
More complex entity ranking methods have been proposed in the literature, but their computational cost is orders of magnitude higher than the techniques just presented, especially when dealing with millions of potential entities. Since the purpose of this paper is not to evaluate entity ranking methods, we have limited ourselves to these methods, which have been proved to be effective and have a very low computational cost and memory footprint. However we remark that the described support ranking methods account for word-based relevance (Equations (5) and (6)) and entitybased relevance (Equation (7) and subsequent definitions of E(q, e)).

4. TASK EVALUATION
We can evaluate this task similarly to how standard retrieval ad-hoc tasks are evaluated. First, we ask human subjects to produce queries about topics they know well. We then produce a (large) set of candidate entities and ask the subject to eliminate the entities that are not relevant to the query (this is similar to how entity ranking could be evaluated [21, 29, 26]). Finally, for every entity selected, we produce a number of candidate sentences for each (query,entity) pair and ask the subject to evaluate them as good or bad support sentences for that query and for that entity. By definition, we require that the sentence should mention the entity, otherwise it becomes very difficult to judge them.
Human subjects evaluate sentences (for example as bad, adequate or excellent) assigning them grades, (noted Grade(q, e, s)). Using these grades we can now compute standard IR performance measures such as Precision and Recall, Mean Reciprocal Rank (MRR), Discounted Cumulative Gain (DCG), etc. These measures can be normalized in two ways: by (entity, query) pair, or first by entity and then by query. In this paper we report scores normalized by (entity,query) pair since we are interested in providing sentences for all entities, regardless of their number.
We will be interested in precision much more than recall, for two reasons. First, there may be many sentences explaining the relationship between a query and an entity, but the user is typically interested in one (except for very specific settings like in forensics or legal applications). Furthermore, entity retrieval applications tend to have very crowded user

342

interfaces and leave little space for sentences and typically only one or two sentences are displayed.
We now describe how we built our evaluation collection. As corpus, we used the Semantically Annotated Snapshot of the English Wikipedia v.1 (SW1) [1]. The collection contains around 75M sentences (coming from 1.5M documents) and 20.3M unique named entities (with type). From all the annotations we used only the 12 first level Wall Street Journal entity types (e.g. person, location, facility, etc.), removing DESC entities (description) within their tag, as it is too hard to provide an accurate evaluation due to its broad semantic generality.
Firstly, we built a manually evaluated dataset of 226 (query, entity) pairs with 45 unique queries (see Table 1 for examples). These queries were entered manually by the assessors and a random selection of relevant entities was considered for evaluation. The total number of obtained relevance judgments was 4814. Judges were asked to assign a Grade(q, e, s) using four levels of relevance: 1 for non-relevant, 2 for fairly relevant, 3 for relevant and 4 for very relevant. A triple (q, e, s) is considered relevant iff Grade(q, e, s)  3. We consider that if there is a support sentence for a (query,entity) pair then the entity must be relevant for the query, thus separating entity and sentence ranking evaluations. The collection is available trough Yahoo's! Webscope program, and we have made available the evaluation data7.
The particular task described through the paper requires to retrieve one or two relevant support sentences per (query, entity) pair; this is why we focus on top-precision retrieval performance metrics: NDCG8, MAP, P@1 and MRR (which is arguably the best suited one). Because in principle many sentences scores might be the same, the evaluation can be biased: if a number of sentences have the same score the models introduced are not able to decide how to order them. This is potentially problematic not only for some of our models but also for standard ranking methods for sentences. Our solution is to employ tie-aware evaluation [12] which takes into account the fact that score-tied sentences could have been ranked randomly. The final performance value is the average over all possible permutations on the ties, which can be efficiently computed in linear time. As a consequence, all these measures (including high-precision measures like P@1, MRR) might be affected by adding more results from the lower part of the ranked documents, which could introduce ties in the top rank.
5. EXPERIMENTS
In this section we evaluate the new dataset just described in Section 4 using different models for Hqe.
5.1 Ranking Models
The support sentences ranking functions Hqe described (Equations (5), (6) and (7)) use different models for F and E. The specific models tested are described in this section.
We employ BM25 [19] as a model for Fq(s), BM25F [17] for Fq(s, Cs) and different options for E. BM25 models use a standard parametrization with parameters k1 and b [17]. In our setting, Hqe functions operate on a top-k set for a given query (Sq) that can be augmented with a context Cs.
7http://barcelona.research.yahoo.net/dokuwiki /doku.php?id=support sentence evaluation set 8gains of (0, 1, 3, 7) and decaying factor of log(1 + rank)

The context of a sentence was defined as the surrounding four sentences (two preceding, two following) plus the title of its Wikipedia entry. We represented each sentence in three fields: the first with the sentence s, the second with surrounding sentences, and the third with the title. This resulted in BM25F parameters k1, (w1, w2, w3) and (b1, b2, b3). As customary in BM25F, and without loss of generality, we set w1 = 1. To reduce the number of parameters we tied all b parameters to a single parameter b. This resulted in the four parameters (k1, w2, w3, b).
In both cases parameters were optimized, for each value of k, by a greedy algorithm (described in [17]) using 2-fold cross validation; results reported are the average over the two test sets9.
We also experimented with a simpler context model, where surrounding sentences and title are concatenated to the sentence which was ranked with BM25. This led to poor results, and therefore were omitted from this paper.
Another batch of features for sentence ranking are aggregations of entity ranking functions, as stated by Equation (7) which exemplifies the sum; we also report on the average aggregator. Aggregated functions are the models selected for E(q, e). We include results for Frequency (Equation (9)), Rarity (Equation (10)), Combination (Equation (11)) and KLD (Equation (12)).
We experimented with k = 1000 and k = 4000. We note that in practice, increasing k has a high performance cost. The main reason is that in order to find candidate support sentences for an entity e we need to check at run time if each scored sentence contains e or not. This is in fact a query operation on the entity containment graph G; even with fastaccess dedicated structures the cost of this increases linearly with k and it is costly even for k values in the order of a few thousands.
5.2 Results
We will evaluate here the different methods discussed in Section 3 on the SW1 collection. Table 2 summarizes the results.
First we note that BM25 with k = 1000 obtains reasonable results in this task; e.g., it achieves a MRR of 0.61. However, context can be exploited to obtain better results. Using the sentence context with BM25F produces a large improvement in the results, obtaining a 16% relative improvement in MRR and 20% in NDCG.
We next investigate the interest of entity-ranking features. First we note that most improve over BM25, but none over BM25F. Recall that these features work re-ranking the extended set S^q. It is quite remarkable that some of these features can improve over BM25 given that they are parameterfree. This indicates that these features are very informative for the task. The sum aggregator stands out slightly among
9For k = 1000 the resulting BM25 parameters for MRR in each run was k1 = (1, 1) and b = (0.18, 0.22). For this set, the over-fitted maximum (using the entire set) is k1 = 1 and b = 0.18, so the trained values are quite close to the optimum. For BM25F, the resulting parameters were: k1 = (0.2, 1), b = (0.62, 0.11), w2 = (0.64, 0.22) and w3 = (0.10, 0.50), being the over-fitted maximum at k1 = 0.26, b = 0.15, w2 = 0.23 and w3 = 0.23. In this case there is a higher variance in the parameter selection, although performance on the test sets is not far from the optimal values; this indicates that BM25 and BM25F parameters are quite robust.

343

Table 2: Performance of Entity Support Ranking

Methods (* = statistical significance at p < 0.05 us-

ing the Wilcoxon signed-ranks test with respect to

BM25).

MRR NDCG P@1 MAP

k = 1000

BM25 BM25F Position dependent Sum Frequency

.61 .71 .69 .67

.59 .71 .64
.67

.57 .66
.65 .58

.45 .53 .51 .52

Sum Rarity Sum Combination Sum KLD Average Frequency

.55 .60 .71 .67 .67 .66 .62 .63

.42 .42 .60 .53 .59 .51 .51 .50

Average Rarity

.47 .54

Average Combination .63 .64

Average KLD

.63 .64

.32 .40 .53 .51 .53 .50

k = 4000

BM25 BM25F

.63 .61 .76 .75

.57 .53 .69 .58

the rest, especially if used together with the combination entity ranking scorer. The features could be ranked in the following way: Combination > KLD > Frequency > Rarity, and the aggregators as Sum > Average. We also experimented with other aggregation techniques besides the sum and average but their performances are always inferior and we do not report them.
Nevertheless, these features are working on a candidate set that is much larger than Sq (as explained in Section 5.3). To make a fair comparison, we also report (bottom of Table 2) results for BM25 and BM25F for k = 4000, which yields result sets of size comparable to S^q. We see that increasing k improves only slightly BM25; this demonstrates that these features are informative. BM25F with k=4000 improves even more (25% relative in MRR and 27% in NDCG). This is further investigated in the next section.
5.3 The Role of Context
Context plays a crucial role in the successful retrieval of support sentences. In Section 3 we introduced context formally and in Section 5 we saw that features exploiting context improved results significantly. In this section we explore the different roles of context and provide a more in-depth and intuitive introduction of the role of context in this task and in sentence retrieval in general.
First, let us look at the problem, illustrated in figure 1 Given a fixed query q and a fixed entity e, there is a relevant set Rq,e of sentences which are correct support sentences (i.e., good explanations of the relevance of e to q). Now consider the top retrieved sentences for q, noted Sq. We know it intersects Rq,e because otherwise simple models such as BM25 could not perform well. But it is often the case that some support sentences do not contain a match of any of the query terms at all, which are the ones outside of Sq. There are many reasons for this, such as anaphora and synonymy among others. This is a typical problem in IR, but the extremely short length of sentences (compared to documents) exacerbates this problem
We have addressed this problem by considering sentences preceding and following matching sentences. Indeed, we

Figure 1: Venn Diagram of sets Rq,e,Sq, Sq and S^q.
found that good support sentences for an entity are often found in the sentences immediately before or after the sentence that mentions the entity. The rationale behind this is that if a sentence provides a good description to a concept related to a information need, the referred query and entity must appear somehow nearby in a document. Pronouns for example usually refer to entities mentioned in the one or two preceding sentences; beyond pronouns, many complex linguistic relations conspire to make nearby words relevant to each-other. We have followed two alternative paths to take the context into account.
Our first method is to score sentences without context first, obtaining Sq, and then extending this set with the context of each of its sentences (S^q). Note that by construction |Sq| = k and Sq  S^q, and therefore |^Sq|  k. Therefore the top-k sentences in Sq are necessarily somewhere in S^q. Note that not all of the new sentences introduced had an initial score of zero; some might have had a small score that did not put them in the top-k. The second method is to score sentences taking into account the context in the ranking function itself. In this case we obtain a different Sq. It is important to understand that at as k   we would have Sq = S^q and Sq  Sq, but for fixed k this is not the case. Instead |Sq| = k and S^q = Sq = Sq. These two methods are fundamentally different although complementary.
Figure 2 shows the sizes of the sets Sq (BM25 no context) and Sq (BM25 context window) with respect to the "percentage of queries answered". By this we mean the number of (query,entity) pairs for which at least one relevant (support) sentence was found. This number provides an upper bound on precision, since it is impossible for a ranking function to find a relevant sentence outside of the S set scored. We see that without the context, BM25 is not able to find any relevant results for approximately 20% of the queries, whereas including the context this number is less than 1%. The figure also reveals that a pure bag of words approach struggles to cover a high percentage of the queries which require a very large k value in order for BM25 to find any answer for them.
It was mentioned before that BM25F is one way of introducing context into sentence retrieval. In order to demonstrate that BM25F performs well for early-precision metrics in standard sentence retrieval, we experiment with the three TREC Novelty collections. The TREC Novelty Track ran for three years (2002-2004) [6, 23, 20]. The tracks included

344

% of queries answered

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 1

BM25 no context BM25 using context window

10

100

1000 10000

Number of retrieved sentences k

Figure 2: Percentage of support sentences found when varying the number of top retrieved sentences with and without using a context window of size 5.

an ad-hoc sentence retrieval task consisting of 50 topics for each year. We use the title of the topics as queries (as these are more representative of Web search) and do very little pre-processing to the collections (no stemming or stop-words removal). Results are presented in table 5.3.
A common baseline for ranking sentences is the TF-IDF measure [13], that we include for comparison. The standard performance metric is the F measure which only takes into account the proportion of relevant elements over the total number of sentences retrieved; however, as explained before, in our particular approach we are more interested in early precision metrics (like P@X and MRR). We also introduce results using BM25 for comparison. We tuned BM25 and BM25F's parameters using two-fold cross validation (denoted ), as well as using the entire collection (denoted ). Note that we tune for P@10 only and report on every measure for the best P@10 run.
BM25F outperforms BM25 and TF-IDF in almost every case for P@10, MRR and MAP, whereas being inferior in terms of the F measure in two collections. This may be due to the fact that the context introduces many non-relevant low-ranked sentences which affect the F measure but are mostly unlikely to appear in the top results presented to the user. Although a more thorough comparison with other context and query expansion approaches for sentences retrieval is out of the scope of this paper, the performance numbers obtained are better than ones reported in previous work (for example [14] reports a P@10 of 0.11 in the TREC Novelty 2002 collection) and comparable or better than well-tuned query expansion techniques [10] in every collection. These results agree with the findings reported on the Wikipedia collection - adding small context windows is beneficial for sentence retrieval if weighted appropriately.
6. FUTURE WORK AND CONCLUSIONS
In this paper we presented the novel task of finding support sentences which explain the relationship between a query q and an entity e. We developed a framework which consisted of a definition, formalization and a thorough evaluation dataset for the problem at hand. For tackling the problem we developed several features embracing different paradigms (entity score-based, position-based, retrieval-based).
We show that the most interesting feature is the context of a sentence which can be effectively exploited using the BM25F ranking algorithm. Other entity-score de-

Table 3: Results on TREC Novelty 2002, 2003,

2004 sentence retrieval tasks. Parameter values are obtained using two-fold cross validation  and the whole test set .
P@10 MAP MRR F

TREC Novelty 2002

TF-IDF 0.19 0.12 0.44 0.19 BM25 0.19 0.12 0.45 0.20 BM25F 0.25 0.15 0.49 0.12
BM25 0.18 0.12 0.39 0.20
BM25F 0.22 0.14 0.44 0.12

TREC Novelty 2003

TF-IDF 0.71 0.40 0.84 0.53 BM25 0.72 0.40 0.86 0.54 BM25F 0.79 0.57 0.90 0.54
BM25 0.69 0.38 0.84 0.54
BM25F 0.76 0.53 0.87 0.54

TREC Novelty 2004

TF-IDF BM25 BM25F
BM25
BM25F

0.46 0.47 0.51 0.46 0.48

0.27 0.65 0.37 0.27 0.73 0.38 0.35 0.74 0.34 0.27 0.67 0.38 0.34 0.68 0.34

rived features show promise but further research is required to capitalize them. Furthermore, we experimented with TREC Novelty collections and found out that weighting with BM25F can improve on best published results for high precision metrics.
Some possible extensions to the ranking formulae presented in section 3 could take into account some additional features, mostly related to sentence normalization; we found out that the methods might have a bias for longer sentences, similarly to standard document retrieval. For instance, the sum-based entity ranking measures will score higher if there are many entities inside a sentence, which may be not necessarily more relevant than another that have just a few very highly-ranked entities; averaging the scores for different entities in a sentence might suffer for very entity-crowded sentences, etc. Consequently, these models can be normalized with respect to document length and number of entities using traditional document length normalization functions, for instance BM25's term frequency normalization factor [19], using sentence length, number of entities in a sentence or a weighted mixture of both.
We are interested in pursuing other linguistic features of sentences in the future. For example, it is likely to be important to detect matches in lists or long coordinations, since these are likely to be less relevant. Also, finding the entity and the query matches in a subject/object relation is likely to be relevant. The notion of what determines a proper context for a given candidate support sentence is also subject to variability and tuning.
Finally, if more sophisticated definitions of context are taken into account, or any other features (linguistic, statistic) are to be incorporated into the ranking models, it could be necessary to devise more fine-grained parametrization and tuning.
Acknowledgments: This work is partially funded by FP7 EU Project LivingKnowledge (ICT-231126).

345

7. REFERENCES
[1] J. Atserias, H. Zaragoza, M. Ciaramita, and G. Attardi. Semantically annotated snapshot of the english wikipedia. In LREC'08, 2008.
[2] P. Bailey, A. P. de Vries, N. Craswell, and I. Soboroff. Overview of the trec 2007 enterprise track. In Proceedings of TREC 2007 the 16th Text REtrieval Conference, 2007.
[3] C. Cardie, V. Ng, D. Pierce, and C. Buckley. Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system. pages 180­187, 2000.
[4] S. Chakrabarti, K. Puniyani, and S. Das. Optimizing scoring functions and indexes for proximity search in type-annotated corpora. In WWW '06, pages 717­726, New York, NY, USA, 2006. ACM Press.
[5] D. Gianluca, A. P. de Vries, T. Iofciu, and J. Zhu. Overview of the inex 2008 entity ranking track. In 7th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2008 Dagstuhl Castle, Germany, 2008.
[6] D. Harman. Overview of the trec 2002 novelty track. In Proceedings of TREC 2002, the 11th text retrieval conference, 2002.
[7] K. S. Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28:11­21, 1972.
[8] T. Kanungo and D. Orr. Predicting the readability of short web summaries. In WSDM '09: Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 202­211. 2009, ACM.
[9] X. Li and W. B. Croft. Improving novelty detection for general topics using sentence level information patterns. In CIKM '06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 238­247, New York, NY, USA, 2006, ACM .
[10] D. Losada and R. T. Fern´andez. Highly frequent terms and sentence retrieval. In Proceedings of 14th String Processing and Information Retrieval Symposium, SPIRE'07, pages 217­228, Santiago de Chile, October 2007.
[11] S. Malik, A. Trotman, M. Lalmas, and N. Fuhr. Overview of inex 2006. In 5th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2006, Dagstuhl Castle, Germany, pages 1­11, 2006.
[12] F. McSherry and M. Najork. Computing information retrieval performance measures efficiently in the presence of tied scores. In ECIR 2008, Proceedings of the 30th European Conference on IR Research, pages 414­421, Glasgow, Scotland, 2008, Springer.
[13] V. Murdock. Exploring Sentence Retrieval. VDM Verlag Dr. Mueller e.K., 2008.
[14] V. Murdock and W. B. Croft. A translation model for sentence retrieval. In HLT '05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 684­691, Morristown, NJ, USA, 2005. Association for Computational Linguistics.
[15] T. Okamoto, T. Honda, and K. Eguchi. Locally contextualized smoothing of language models for

sentiment sentence retrieval. In TSA '09: Proceeding of the 1st workshop on Topic-sentiment analysis for mass opinion, pages 73­80, New York, NY, USA, 2009, ACM.
[16] S. Robertson and K. Sparck Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27:129­146, 1976.
[17] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond, foundations and trends in information retrieval. Volume 3, pages 333­389, 2009.
[18] H. Rode. From document to entity retrieval: Improving precision and performance of focused text search. PhD thesis, University of Twente, CTIT. 2008.
[19] S. Robertson and S. Walker. Some simple effective approximations to the 2 poisson model for probabilistic weighted retrieval. In SIGIR '94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 232­241, New York, NY, USA, 1994. ACM/Springer Verlag.
[20] I. Soboroff. Overview of the trec 2004 novelty track. In Proceedings of TREC 2004, the 13th text retrieval conference, 2004.
[21] I. Soboroff, A. P. de Vries, and N. Craswell. Overview of the trec 2005 enterprise track. In Proceedings of TREC 2005 the 14th Text REtrieval Conference, 2005.
[22] I. Soboroff, A. P. de Vries, and N. Craswell. Overview of the trec 2006 enterprise track. In Proceedings of TREC 2006 the Fifteenth Text REtrieval Conference, 2006.
[23] I. Soboroff and D. Harman. Overview of the trec 2003 novelty track. In Proceedings of TREC 2003, the 12th text retrieval conference, 2003.
[24] N. Stokes and J. Carthy. First story detection using a composite document representation. In Proceedings of HTL01, the Human Language Technology Conference, San Diego, USA, 2001.
[25] S. Sweeney, F. Crestani, and D. Losada. Show me more: incremental length summarisation using novelty detection. Information Processing and Management, 44(2):663­686, 2008.
[26] D. Vallet and H. Zaragoza. Inferring the most important types of a query: a semantic approach. In SIGIR '08, pages 857­858, New York, NY, USA, 2008. ACM.
[27] E. M. Voorhees and D. K. Harman. TREC: Experiment and Evaluation in Information Retrieval. The MIT Press, 2005.
[28] A. P. Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. Overview of the inex 2007 entity ranking track. In 6th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2007 Dagstuhl Castle, Germany, pages 245­251. Springer-Verlag, 2008.
[29] H. Zaragoza, H. Rode, P. Mika, J. Atserias, M. Ciaramita, and G. Attardi. Ranking very many typed entities on wikipedia. In CIKM '07. ACM Press, 2007.

346

Predicting Searcher Frustration

Henry Feild
Dept. of Computer Science University of Massachusetts
Amherst, MA 01003 USA
hfeild@cs.umass.edu

James Allan
Dept. of Computer Science University of Massachusetts
Amherst, MA 01003 USA
allan@cs.umass.edu

Rosie Jones
Yahoo! Labs 4 Cambridge Center Cambridge, MA 02142 USA
jonesr@yahoo-inc.com

ABSTRACT
When search engine users have trouble finding information, they may become frustrated, possibly resulting in a bad experience (even if they are ultimately successful). In a user study in which participants were given difficult information seeking tasks, half of all queries submitted resulted in some degree of self-reported frustration. A third of all successful tasks involved at least one instance of frustration. By modeling searcher frustration, search engines can predict the current state of user frustration and decide when to intervene with alternative search strategies to prevent the user from becoming more frustrated, giving up, or switching to another search engine. We present several models to predict frustration using features extracted from query logs and physical sensors. We are able to predict frustration with a mean average precision of 65% from the physical sensors, and 87% from the query log features.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process
General Terms
Experimentation, Measurement
Keywords
user modeling, searcher frustration, query logs, emotional sensors
1. INTRODUCTION
In this work, we investigate searcher frustration. We consider users frustrated in the context of information retrieval (IR) when their search process is impeded. A frustration model capable of predicting how frustrated searchers are throughout their search is useful retrospectively as an effectiveness measure. More importantly, it allows for real-time
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

system intervention to help frustrated searchers, hopefully preventing users from leaving for another search engine or abandoning the search altogether.
This work investigates what aspects of users' interactions with a search engine during a task can be used to predict frustration. Depending on the level of frustration, we may wish to change the underlying retrieval algorithm or the user interface. For example, one source of difficulty in retrieval is a user's inability to sift through the results presented for a query [13, 17]. One way that a system could adapt to address this kind of frustration is to show the user a conceptual breakdown of the results: rather than listing all results, group them based on the key concepts that best represent them [13]. Using a well worn example, if a user enters `java', they can see the results based on `islands', `programming languages', `coffee', etc. Of course, most search engines already strive to diversify result sets, so documents relating to all of these different facets of `java' are present, but they might not be clear to some users, causing users to become frustrated.
An example from the IR literature of a system that adapts based on a user model is work by White et al. [15]. They used implicit relevance feedback to detect changes in users' information needs and alter the retrieval strategy based on the degree of change. The focus of our work is to detect frustrated behavior, and adapt the system based on the type of frustration, regardless of the information need itself.
The goals for our line of research are as follows: first, determine how to detect a user's level of frustration; second, determine what the key causes or types of frustration are; and third, determine the kinds of system interventions that can reduce different types of frustration. This work explores the question of whether frustration can be accurately predicted and what features derived from query logs and physical sensors are the most useful in doing so in a controlled lab study.
Our key contributions are (1) the first user study of frustration in web search (2) a publicly available data set of the data collected, and (3) a comparison of on-line classification models derived from sensor and query log data to predict frustration.
The remainder of this paper is organized as follows. In Section 2 we discuss related work from the IR, intelligent tutoring systems (ITS), and information science (IS) literature. We then describe the task and evaluation in Section 3, followed by a description of the user study we conducted in Section 4. In Section 5 we describe the models used followed in Section 6 by a description and analysis of the experiments.

34

In Sections 7 and 8 we discuss the results and design implications of our findings. We end with a summary and future work in Section 9.
2. RELATED WORK
In this section, we first describe frustration in the context of IR. We then touch upon some of the related work in three areas: searcher satisfaction modeling, work carried out in the field of ITS where frustration has been modeled, and various work pertaining to user modeling in IR, such as predicting when users will switch to another search engine. These works helped to shape the user study we conducted and the models used to predict searcher frustration.
2.1 Frustration in Information Retrieval
We define frustration in the context of IR as the impediment of search progress. Xie and Cool [17] explored helpseeking or problematic situations that arise in searching digital libraries. They identified fifteen types of help-seeking situations that their 120 novice participants encountered. The authors' use of `help-seeking situations' aligns well with our definition of frustration, since the issues encountered by the subjects impeded their search progress. The authors created a model of the factors that contribute to these helpseeking situations from the user, task, system, and interaction aspects. The qualitative nature of the study is useful in designing general help systems for digital library systems. However, there was no attempt to model frustration using logged interaction data, which is the goal of our work.
In a study examining how children search the Internet, Druin et al. [7] found that all of the twelve participants experienced frustration while searching. The authors point out that children make up one of the largest groups of Internet users, making frustration a major concern. In a similar study, Bilal and Kirby [2] compared the searching behavior of graduate students and children on Yahooligans! They found that over 50% of graduate students and 43% of children were frustrated and confused during their searches. In addition, they found that while graduate students quickly recovered from "breakdowns" (where users were unable to find results for a keyword search), children did not.
Kuhlthau [12] found that frustration is an emotion commonly experienced during the exploration phase of a search process. She states that encountering inconsistent information from various sources can cause frustration and lead to search abandonment.
2.2 Satisfaction in Information Retrieval
While frustration prediction has not been directly studied in the field of IR, searcher satisfaction has. Satisfaction in search can have different meanings [1, 8, 9]. We define searcher satisfaction as the fulfillment of a user's information need. While satisfaction and frustration are closely related, they are distinct. As a consequence, searchers can ultimately satisfy their information need, but still be quite frustrated in the process [3].
In previous work, satisfaction has been examined at the task or session level1 [1, 8, 9, 10]. These satisfaction models only cover user satisfaction after a task has been completed, not while a task is in progress. As such, satisfaction models are useful for retrospective analysis and improvement, but
1We use task and session interchangeably in this research.

not as a real-time predictor. In contrast, with a frustration model that is defined throughout a search, these real-time solutions are available.
In web search study, Fox et al. [8] found there exists an association between query log features and searcher satisfaction, with the most predictive features being click-through, the time spent on the search result page, and the manner in which a user ended a search. They also analyzed browsing patterns and found some more indicative of satisfaction than others, such as entering a query, clicking on one result, and then ending the task. Clicking four or more results was more indicative of dissatisfaction.
Huffman and Hochster [10] found a relatively strong correlation with session satisfaction using a linear model encompassing the relevance of the first three results returned for the first query in a search task, whether the information need was navigational, and the number of events in the session. In a similar study of search task success, Hassan et al. [9] used a Markov model of search action sequences to predict success at the end of a task. The model outperformed a method using the DCG of the first query's result set, suggesting that a model of the interactions derivable from a query log is better than general relevance in modeling satisfaction.
2.3 Frustration in Tutoring Systems
While we have not found any discussion of predicting frustration in the IR literature, we did find studies that model frustration in the ITS literature. Cooper et al. [4] describe a study in which students using an intelligent tutoring system were outfitted with four sensors: a mental state camera that focused on the student's face, a skin conductance bracelet, a pressure sensitive mouse, and a chair seat capable of detecting posture.
Cooper et al. found that across the three experiments they conducted, the mental state camera was the best standalone sensor to use in conjunction with the tutoring interaction logs for determining frustration. However, using features from all sensors and the interaction logs performed best. They used step-wise regression to develop a model for describing each emotion. In another study using the same sensors, but different features, Kapoor, Burleson, and Picard [11] created a model that was capable of classifying when the user of an ITS was going to click an I'm frustrated! button with 79% accuracy and a chance accuracy of 58%.
2.4 User Modeling in Information Retrieval
In this section, we summarize several models used in IR prediction tasks that rely, at least in part, on query log data [6, 9, 10, 16]. We are specifically interested in the types of models used (e.g., linear regression) and the key features.
Huffman and Hochster [10] predicted session satisfaction useing a regression model incorporating the relevance of the top three results returned for the first query, the type of information need, and the number of actions in the session. Hassan et al. [9] used a Markov model to predict task success and found that sequences of actions, as well as the time between the actions, are good predictors.
Downey et al. [6] created a Bayesian dependency network to predict the next user browsing action given the previous n actions, parameterized by a long list of user, session, query, result click, non-search action, and temporal features. They

35

found that using an action history with more than just the immediately preceding action hurt performance.
White and Dumais [16] explored predicting when users would switch between search engines. Their goal was "not to optimize the model but rather to determine the predictive value of the query/session/user feature classes for the switch prediction challenge." They used a logistic regression model that encompassed query, session, and user level features. They found that using all three feature classes outperformed all other combinations of feature classes and did much better than the baseline for most recall levels.
3. TASK AND EVALUATION
In this section, we outline the details of the frustration modeling task and its evaluation.
3.1 Task
Our goal is to predict whether a user is frustrated at the end of each query interaction during a session. We define a query interaction as all interactions between a user and the the browser pertaining to a specific query up until another query is entered or the session ends. We will refer to these as searches. A session consists of one or more searches directed at fulfilling a specific information need or task. We will refer to these as tasks. At the end of a search, we ask, "Is the user frustrated at this point of the task?" To make the prediction, we can derive features from the search just completed or from all the searches conducted in the task so far. We refer to these feature sets as search and task features, respectively. In addition, features can be derived from a user's other tasks, which we call user features.
In this paper, we consider frustration prediction as a binary task. However, multi-class prediction may also be useful, using either regression or a multi-class machine learning method. We also focus on general frustration, but predicting types of frustration may also be useful, e.g., predicting the fifteen types of frustration outlined by Xie and Cool [17].
3.2 Evaluation
In this section, we describe the metrics that we use to evaluate frustration models. Our ultimate goal is to use frustration models to decide when to intervene to help the user during the search process. Since many interaction methods with which we would like to intervene are not typically used because of their undesirable, frustration-causing attributes (i.e., interaction and latency), we are interested in minimizing our false-positives (non-frustrated searchers that our models say are frustrated), potentially at the cost of recall. For that reason, our predominant evaluation metric is a macro-average (across users) F-score with  = 0.5, which gives increased weight to precision over recall. We also use 11-point interpolated average precision to compare models across users. This metric tells us how well, on average, a model can rank instances of frustration by user.
Comparing across users rather than with a micro approach avoids one frustrated searcher in the test data skewing the results. Un-weighted macro-averaging treats all users equally. A desirable model is one that performs well across all users, not just on one specific user. In Section 6 we report macro accuracy, precision, F=0.5, and mean average precision (MAP). To be clear, MAP is uninterpolated, in contrast to 11-point interpolated average precision.

1. What is the average temperature in [Dallas, SD/Albany, GA/Springfield, IL] for winter? Summer?
2. Name three bridges that collapsed in the USA since 2007. 3. In what year did the USA experience its worst drought? What
was the average precipitation in the country that year? 4. How many pixels must be dead on a MacBook before Apple will
replace the laptop? Assume the laptop is still under warranty. 5. Is the band [Snow Patrol/Greenday/State Radio/Goo Goo
Dolls/Counting Crows] coming to Amherst, MA within the next year? If not, when and where will they be playing closest? 7. What was the best selling television (brand & model) of 2008? 8. Find the hours of the PetsMart nearest [Wichita, KS/Thorndale, TX/Nitro, WV]. 9. How much did the Dow Jones Industrial Average increase/decrease at the end of yesterday? 10. Find three coffee shops with WI-FI in [Staunton, VA/Canton, OH/Metairie, LA]. 11. Where is the nearest Chipotle restaurant with respect to [Manchester, MD/Brownsville, Oregon/Morey, CO]? 12. What's the helpline phone number for Verizon Wireless in MA? 13. Name four places to get a car inspection for a normal passenger car in [Hanover, PA/Collinwood, TN/Salem, NC].
Table 1: The information seeking tasks given to users in the user study. Variations are included in brackets.
We use an approximation of Fisher's randomization test to obtain a double sided p-value for significance. Using 100,000 trials for every model comparison, the error at  = 0.05 is ±0.001 (2% error) [14].
4. USER STUDY
In Fall 2009, we conducted a user study with thirty participants from the University of Massachusetts Amherst. The mean age of participants was 26. Most participants were computer science or engineering graduates, others were from English, kinesiology, physics, chemical engineering, and operation management. Two participants were undergraduates. Twenty-seven users reported a `5' (the highest) on a five-point search experience scale; one reported a `4' and two a `3'. There were seven females and twenty-three males.
Each participant was asked to complete seven2 tasks from a pool of twelve (several with multiple versions) and to spend no more than seven minutes on each, though this was not strictly enforced. The order of the tasks was determined by four 12 × 12 Latin squares, which removed ordering effects from the study. Users were given tasks one at a time, so they were unaware of the tasks later in the order. Most of the tasks were designed to be difficult to solve with a search engine since the answer was not easily found on a single page. The complete list of tasks is shown in Table 1.
The study relied on a modified version of the Lemur Query Log Toolbar3 for Firefox.4 To begin a task, participants had to click a `Start Task' button. This prompted them with the task and a brief questionnaire about how well they understood the task and the degree to which they felt they knew the answer. They were asked to use any of four search engines: Bing, Google, Yahoo!, or Ask.com and were allowed to switch at any time. Links to these appeared on the toolbar and were randomly reordered at the start of each task. Users were allowed to use tabs within Firefox.
For every query entered, users were prompted to describe their expectations for the query. Each time they navigated
2Two participants completed eight tasks, but it took longer than expected, so seven tasks were used from then on. 3http://www.lemurproject.org/querylogtoolbar/ 4http://www.mozilla.com/en-US/firefox/firefox.html

36

Query Frustration None

Extreme

Feedback value: 1

2

3

4

5

Frequency: 235 128 68 25

7

Percentage: 51% 28% 15% 5%

1%

Table 2: Distribution of user-reported frustration for searches.

Task Success Feedback value:
Frequency: Percentage:

Bad 1 14 7%

Fair&Good 2&3 66 31%

Excellent 4 48
23%

Perfect 5 83
39%

Table 3: Distribution of user-reported task success. An error in the logging software caused the `fair' and `good' levels to be conflated.

away from a non-search page, they were asked the degree to which the page satisfied the task on a five point scale, with an option to evaluate later. At the end of a search (determined by the user entering a new query or clicking `End Task'), users were asked what the search actually provided relative to their expectations, how well the search satisfied their task (on a five point scale), how frustrated they were with the task so far (on a five point scale), and, if they indicated at least slight frustration (2­5 on the five-point scale), we asked them to describe their frustration.
When users finished the task by clicking `End Task', they were asked to evaluate, on a five point scale, how successful the session was, what their most useful query was, how they would suggest a search engine be changed to better address the task, and what other resources they would have sought to respond to the task.
A total of 211 tasks were completed (one participant completed one fewer task because of computer problems), feedback was provided for 463 queries, and 711 pages were visited. On the frustration feedback scale, `1' is not frustrated at all and `5' is extremely frustrated. In Table 2 we see that users reported frustration for half of their queries. The most common reasons given for being frustrated were: (1) off-topic results, (2) more effort than expected, (3) results that were too general, (4) un-corroborated answers, and (5) seemingly non-existent answers.
4.1 Success and Frustration
We find that users become frustrated even when they succeed at their information seeking task. Table 3 shows the breakdown of user-reported task success. The majority of users reported their tasks were satisfied at the `excellent' or `perfect' levels. Table 4 shows that while not finding the information can be frustrating, even when the information is found, users can get frustrated. Users were successful in 62% of all tasks, but experience some degree of frustration in over a third of those successful tasks. This evidence supports the exploration of frustration modeling and differentiates it from task success or satisfaction prediction.
4.2 Individual Variation
Since we measure self-reported frustration, the results may depend on the individual's temperament as well as introspection. In Figure 1 we see that individuals from the test set do indeed vary in their self-reported frustration. The training set shows a similar trend. One phlegmatic individual did not report any frustration for any task. In our experimental section we will conduct leave-one-user-out

Success Failure
Total

Frustration 46 72 118

No Frustration 85 8 93

Total 131 80 211

Table 4: The number of tasks for which users were highly successful (levels 4­5) or not versus whether or not the task had any searchers for which the user was at least somewhat frustration.

cross-validation to concentrate on the aspects of frustration that generalize across users.

5. MODELING SEARCHER FRUSTRATION
In this section, we describe the models we use to predict frustration. We consider a number of features that have been used in previous studies, both in the IR and ITS fields. The first set of features include those derived from a client-side query log, while the second set includes those from three physical sensors.

5.1 Query Log Features
The query log used in this study is client-side. Interactions between the user and the Web were recorded by means of a Firefox plug-in, adapted from the Lemur Query Log Toolbar. The toolbar captures data including page focuses, click events, navigation events such as pressing the back and forward buttons, copy and paste actions, page scrolling, and mouse movements, among others. Every event includes a timestamp.
Given the section of the log that corresponds to a particular task, we can derive search and task features (Section 3.1). The search features include search duration, pages visited, query length, max page scroll, and others. The task features span the start of the current task through the end of the most recent search. They include aggregates of the search features such as task duration, queries entered, average search duration, total pages visited, average pages visited per search, etc. Due to space constraints, we have not included a full listing of the forty-seven features. However, they are very similar to features used in previous query log analyses [8, 16].

5.2 Sensor Features
We used three physical sensors in our study: a mental state camera, a pressure sensitive mouse, and a pressure sensitive chair. These are three of the four sensors used by Cooper et al. [4]; we use the same high-level readings. The camera software provides confidence values for six mental states: agreeing, disagreeing, unsure, interested, thinking, and confident. The mouse consists of six pressure sensors-- two on top and two on either side. Following Cooper et al. [4], we calculate the following feature with the values:

mouse

=

P6
i=1

M

Si

,

(1)

1023

where M S represents the six mouse sensors and the denominator is the maximum pressure reading provided by any one sensor. This feature has a range from 0 to 6. Finally, the chair has three pressure sensors on the back and three on the seat. We derive three aggregate features: net seat change,

37

net back change, and leaning forward [4]:

netS eatC hang e(t)

=

P3  i=1

SSi[t

-

1]

-

SSi[t]

,

(2)

netB ackC hang e(t)

=

P3  i=1

BSi[t

-

1]

-

BSi[t]

,

(3)

8 >0 <
sitF orward(t) = 1

if

W3
i=1

BSi

> 200,

if

V3
i=1

200



BSi

> -1,

(4)

>:N A otherwise,

where SS corresponds to the three seat sensors, BS the three back sensors, and t is the time step at which the feature is being computed. These were found to be useful features by both Cooper et al. [4] and D'Mello et al. [5].
To derive features, we find the minimum, maximum, mean, and standard deviation for each reading over some time frame. Previous studies used window sizes of 150 seconds preceding the event being predicted [4, 11]. In our setting, we used three appropriate time frames: aggregating the features from the beginning of the task, from the beginning of the search, and thirty seconds preceding the end of the search where we are predicting frustration. The first two are equivalent to the query log task and search features, respectively. In addition, we decided to use two versions of each window: one that ignores any segments of time where a user was responding to a feedback prompt and a version that uses those time segments. See Section 4 for details about the feedback prompts.
In total, this yields (6 camera readings + 1 mouse reading + 3 chair readings) × {min | max | mean | stddev} × {task | search | 30-seconds} × {prompts | no-prompts} = 240 features.

5.3 Models
We consider two baselines for this study: (1) always predicting users are frustrated and (2) predicting they are frustrated only when they have abandoned their query (i.e., they did not click on anything). Prior to the study, we believed this to be a reasonable approximation of frustration.
We construct seven additional models using logistic regression. All features were normalized per user prior to training. One model uses all of the features from both the sensors and the query logs and is referred to as QL+Sensors. Three of the models are based on sequential forward feature selection on just the query log features, just the sensor features, and all the features. We name these SFS-QL, SFSSensors, and SFS-QL+Sensors, respectively. The sequential forward selection process starts with an empty feature set, considering all features `unused.' On each iteration of the algorithm, the unused feature that performs best in combination with the current pool of `used' features is moved from the `unused' to the `used' pool. The algorithm stops when no improvement in performance is made. The `used' features are the final selection. For each of the SFS sets, all of the features from the set were available for selection.
We optimized our feature selection for F=0.5 using macro precision and recall at the optimal logistic regression score threshold. For example, if a subset of features achieved a macro F-score of 0.6 with a score threshold of 0.5 and another subset achieved an F-score of 0.7 at a score threshold of 0.4, the latter would be selected and the corresponding threshold noted. The features selected for each model are listed in Table 5. The models show the order in which the features were selected. For the query log features, task MaxQrCharLen is the maximum length (in characters) of any

SFS-QL

1. task Duration

4. search RsltsVisitedPrev,

2. task QryPropUnq

5. task AvgPgMaxScroll

3. task MaxQryCharLen

SFS-Sensors 1. wind30s inclPrmpts unsureConf-mean, 2. wind30s noPrmpts sitForward-min, 3. task inclPrmpts netSeatChange-min

SFS-QL+Sensors

1. task Duration,

2. task QryPropUnq,

3. wind30s noPrmpts unsureConf-mean,

4. search inclPrmpts unsureConf-min

5. wind30s noPrmpts concentratingConf-stddev,

6. search noPrmpts netBackChange-min,

7. search noPrmpts concentratingConf-min,

search QryCharLen, task Duration, user AvgURLCount

W&D search AvgTokenLen, task ActionCount,

Table 5: The top three models were greedily learned from subsets of the query log and sensor feature sets by sequential forward selection. The bottom model is derived from features that worked well for detecting when users would switch search engines [16]. The meaning of the sensor feature names are selfevident based on Section 5.2; query log features are described in Section 5.3.

query seen so far in a task; task Duration is the time, in seconds, of the task; task QryPropUnq specifies the number of unique queries seen so far in a task; task AvgPgMaxScroll is the mean average max scroll per page per query in the task; search RsltsVisitedPrev is the number of results visited during a search that were visited previously in the task.
We create a seventh model based on the features that White and Dumais [16] found were most important for predicting when users would switch search engines, which we refer to as W&D. The features in this model (Table 5) are: the most recent query's length in characters (search QryCharLen), the average token length of the most recent query (search AvgTokenLen), the duration of the task in seconds (task Duration), the number of user actions in the task (task ActionCount), and the average number of URLs visited per task for the current user (user AvgURLCount).
The eighth model we explore is the Markov Model Likelihood (MML) used by Hassan et al. [9] to predict task success. The input to this model is a sequence of events with time lapses between events included. After being trained on event sequences leading up to frustrated and non-frustrated instances, the model produces two scores: one calculates the likelihood of the event sequence being indicative of frustration; the other calculates the likelihood of the event transition times being indicative of frustration.
The scores produced range from 0 - , with scores closer to 0 meaning the sequence is more consistent with frustration, 1 being indifferent, and scores greater than 1 meaning the sequence is more consistent with non-frustration. We use the following variation of Platt smoothing to map the scores into the range 0 - 1.

1.0

M M L(x) =

,

(5)

1 + e(-x)+

where x is the ratio and  and  are the smoothing parameters, set to 4 as determined by our personal judgment on the range of ratios output in the training and development phases.

38

Event
Q RF RC S OF OC

Description Enter query. Focus on a search results page. Click on a link on a results page. Scroll. Focus on a non-results list page. Click on a link on a non-results list page.

Table 6: The event types used in the Markov Model sequences.

15

20

Total number of instances Frustrated instances

10

Frequency

The sequence events we used for the MML model are listed in Table 6. The MML uses task-level event sequences--each instance consists of the sequence of events starting from the beginning of the task up until the point where frustration is being predicted. Duplicate events were ignored and the time between events was recorded in seconds. We used the scores as features with a logistic regression classifier. We refer to this model as MML-time in the rest of the paper.
On the training/development data (Section 7), the W&D model performed very well, so we decided to add the MMLtime as an additional feature, creating the ninth model W&D+MML-time. We felt that the sequence information captured by the MML model would benefit the static features used by the W&D model.
6. RESULTS
We randomly selected twenty of the thirty participants' data for training and development. In the training/development set, we put each user's data into its own fold, giving us a total of twenty folds. This avoids using a particular user's data for both training and testing for cross validation experiments. We used twenty-fold cross validation to select features and tune the score threshold at which F=0.5, precision, and accuracy are computed.
For the results presented here, we re-trained our models on all twenty users in the training/development set and tested on the remaining ten users. The macro-averages were calculated across users.
The training set contained 323 queries (51% of which were frustrated) for which there was feedback across 136 tasks for twenty users. The test set contained 137 query-feedback instances (44% of which were frustrated) across seventy-one tasks for ten users. One query from the training set and two from the testing set were removed due to logging errors that prevented the queries from being properly processed. We should note that during the study, two participants were accidentally given the same ordering of tasks and both users were randomly selected for testing. While this does increase the chances of ordering bias, we believe the effect is small due to the similar performance of the models on the training and testing set. Figure 1 shows the number of total and frustrated instances per user in the test set.
Table 7 shows the results of the experiments. Accuracy is measured across all users. The other three metrics are only measured for nine of the users, as user `25' never indicated frustration, causing the metrics to be undefined. Accuracy, precision, and F=0.5 are calculated using the binary predictions created by thresholding each models' output according to the development set. The no-clicks baseline is undefined for three of the metrics. For precision and F, this is because of undefined values for certain users. For MAP, both no-clicks and always-frustrated are undefined since

0

5

12_2 28 03 11 02 22 07 12 30 25

User ID
Figure 1: The total number of feedback instances and frustrated instances per user, ordered by total instances.

W&D QL+Sens SFS-QL+Sens SFS-QL SFS-Sens MML-time W&D+MML-time No clicks Always frustrated

Accuracy 0.75 0.54 0.69 0.69 0.55 0.56 0.66 0.57 0.44

Precision 0.81 0.50 0.74 0.74 0.58 0.57 0.85 -- 0.49

F=0.5 0.80 0.49 0.72 0.73 0.61 0.62 0.69 -- 0.55

MAP 0.87 0.59 0.85 0.80 0.65 0.65 0.76
-- --

Table 7: Macro-level results for the models on the test set. Accuracy is over all ten users. The other three metrics do not include user `25'.

it involves ranking scores and both baselines produce binary scores.
The metrics show that the relatively simple W&D model outperforms the rest for most metrics. Not all differences are significant, however. As there is no concise way to illustrate significance for all pairs of systems for each metric, we will describe the most critical differences for F=0.5. The W&D model is statistically different from all other systems with respect to F=0.5 except SFS-QL. In turn, SFS-QL is statistically different from all the other systems except SFS-all and W&D+MML-time.
Figure 2 shows the 11-point interpolated average precision across users for each model. Using all features outperforms the baseline, but is much worse then selecting only a subset of the features (SFS-QL+Sensors). This graph shows the W&D, W&D+MML-time, and SFS-QL+Sensors models in contention for the highest precision at different recall levels, all resulting in precision above 70% with 100% recall.
While not shown, when average precision is broken down by user, it is clear that no one model performs well for all users. This suggests that some form of personalization could be useful in determining how much influence each feature type should have for a particular user.
As we mentioned in Section 3.2, macro F=0.5 is the metric we are most interested in. While we calculated this value using the score threshold for each model selected during development as shown in Table 7, observing how the score threshold affects the F-score can help us understand how stable each model is between the development and test sets. Figure 3 shows F=0.5 as the score threshold ranges from 0 to 1 for select models. The W&D model reaches its optimum on

39

0.80

1.0

W&D W&D+MML-time MML-time SFS-Sensors Always frustrated

0.75

0.9

0.70

0.8

F (  =0.5)

Precision

0.7

0.6

G

G

G

G

G

G

W&D W&D+MML-time SFS-QL+Sensors SFS-QL MML-time SFS-Sensors G QL+Sensors f Always frustrated (Prec=0.49)

G

G

G

G

G

f

0.0

0.2

0.4

0.6

0.8

1.0

Recall

0.5

Figure 2: Macro 11-point interpolated average precision across the test users for each model.

Feature task Duration user AvgTaskURLCount search QryCharLen search AvgTokenLen task ActionCount

Weight 2.049 -1.293 0.670 -0.497 -0.089

Table 8: The weights learned for the features in the W&D model ranked by influence (intercept = -0.406).

the test set at nearly the same score threshold as in the development set. In contrast, the W&D+MML-time and MML-time models peak at a lower and higher threshold, respectively, on the test set. While the maximum for SFS-Sensors on the test set is close to its score threshold, there is a substantial decrease in performance just past its optimal threshold. This suggests that several of the trained models are sensitive to new data.
Turning to the learned weights for the best performing model, the W&D feature weights are listed in Table 8. The model is likely to predict frustration for a lengthy session in which few URLs have been visited and actions taken, and where the most recent query has many characters overall, but very few per term.
7. DISCUSSION
Lab studies such as this one are highly controlled--in terms of users, tasks, timing, environment, etc.--and as a result the results are not necessarily directly transferable to more realistic settings. Despite this, we feel that our findings are still generally applicable to real Web search and from which many practical observations can be gleaned. In this section we will discuss a few of these observations as well as some questions that arose.
The results suggest that a few relatively simple query log features can reliably predict frustration. This is useful in developing a search system that predicts frustration. However, some information necessary for the best performing model, W&D, is client-side, such as the average number of URLs visited in other tasks and the number of actions performed by

0.55

0.60

0.65

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Score threshold

Figure 3: F=0.5 using macro precision and recall for select models over the ten test users. The points denote the threshold that was chosen in development for each model.

the user during the current task. This means a system would likely need to be implemented an a browser add-on.
The features we found most useful for detecting frustration are the same as those White and Dumais [16] found most useful for detecting when a user will switch search engines. This suggests that this feature set may have a broader scope of predictive power for related tasks, such as task satisfaction and query abandonment.
One of the surprises of this research for us was the performance of the sensors. Given the results of Cooper et al. [4], we expected the sensors would strongly correlate with user-reported frustration and that we would struggle to find a set of query log features to even come close to the performance of the sensors. In fact, the opposite was true. There may be several reasons for this. One reason might be that the study occurred in an open room and up to five participants were active in the study at a given time. The presence of other participants may have affected how an individual maintained their composure. Arguments could be made that this is or is not a realistic situation; it probably varies by where people search (e.g., in an open office space vs. a cubicle vs. a living room). Another possibility is that the way the feedback was gathered upset the natural reactions of the participants. However, we hope that including a set of features that ignores sensor readings during the time intervals when prompts were shown removed such bias. A third reason may be that the users' physical reaction to frustration may not have aligned with their report of the emotion and our attempts to discretize the sensor readings may have affected the sensors' predictive ability.
Another surprise was the performance of the MML-time model. We thought that the sequence data would have been more helpful than it was. One reason for its performance may be the event language. We used a simple, high-level set of events. This is in contrast to Hassan et al. [9], who used events such as the type of link clicked on a search results page. Adding more advanced features may be more useful. However, there is another problem with sequences on our data set: data sparsity. While our data set is suffi-

40

cient for static feature classification, there is likely an insufficient number of unique sequences to build a reliable model. A Web-scale data set, such as those used in other studies [6, 9], would be more useful in combination with this model. The trade off is that user-reported frustration is not included with Web-scale search logs.
Lastly, the greedy sequential forward selection algorithm did not provide a feature set that performed as well as or better than the W&D model on the training data. Future work should explore more advanced selection techniques to find a better approximation for this task.
8. DESIGN IMPLICATIONS
This study has several implications for designing systems that detect frustration. First, the best performing models do not rely on sensor data. Using query log data alone is sufficient to predict frustration well. Second, in light of the first implication, a Web browser add-on would be a viable vehicle to deliver frustration detection. Browser add-ons have access to all of the query log statistics needed to predict frustration. Third, the model that performed best uses five relatively simple query log features in a logistic regression model, making classification fast, simple, and usable in realtime. A fourth implication is that one model, such as the W&D model, will not work equally well across all users. Thus, personalization may be important and future work should investigate how it would be useful.
9. CONCLUSIONS
In this paper we used features derived from a client-side query log and three physical sensors to predict user-reported frustration during a controlled study of Web search. We compared several models based on the information retrieval [8, 9, 16] and the intelligent tutoring systems [4, 5, 11] literature. We found that using a few simple query log features performed best.
In addition, the toolbar and all of the data collected during the study are being made publicly available.5 Much more data was collected than was used in this paper, such as user-reported page relevance, query satisfaction, and task satisfaction. All pages viewed were downloaded, including search results pages. Mouse movements were also collected, from which a useful feature could be derived for tasks such as frustration prediction, among others. The data set serves as a means for others to compare against the results of this paper, as well as provide insights into ways to build on the study design used. Future work should explore incorporating other features from this rich data set, such as mouse movements, into frustration models.
One direction of future work is investigating how the models presented in this paper transfer to real searching environments, where there are user-defined, and likely interleaved, tasks. Another direction of future work is finding a set of useful features from the less-rich server-side information. This would allow a system to be built that does not depend on add-ons or other client-side instrumentation.
We intend to explore what types of interventions are appropriate for addressing searcher frustration and when to use them. One plausible design is to present a user with a list of search assisting technologies (e.g., explicit relevance
5See http://ciir.cs.umass.edu/downloads/.

feedback) when frustration is detected. Alternatively, classifying types of frustration may be helpful; one or more interventions could be presented to the user based on the type of frustration. Future work should consider the effects of interventions and presentation on frustration so that the interventions are not counterproductive.
Acknowledgments
This work was supported in part by the Center for Intelligent Information Retrieval, by NSF IIS-0910884, and through collaborations with Yahoo! scientists as part of the Yahoo! Faculty Research and Engagement Program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors. We would like to thank Beverly Woolf, David Cooper, and Winslow Burleson for loaning the sensors and logging software.
References
[1] A. Al-Maskari, M. Sanderson, and P. Clough. The relationship between IR effectiveness measures and user satisfaction. In Proc. of SIGIR, pages 773­774, 2007.
[2] D. Bilal and J. Kirby. Differences and similarities in information seeking: children and adults as Web users. Info. Proc. and Mngt., 38(5):649­670, 2002.
[3] I. Ceaparu, J. Lazar, K. Bessiere, J. Robinson, and B. Shneiderman. Determining Causes and Severity of End-User Frustration. Intl. J. of HCI, 17(3):333­356, 2004.
[4] D. G. Cooper, I. Arroyo, B. P. Woolf, K. Muldner, W. Burleson, and R. Christopherson. Sensor model student self concept in the classroom. In Proc. of UMAP, pages 30­ 41, June 2009.
[5] S. D'Mello, R. Picard, and A. Graesser. Toward an affectsensitive AutoTutor. IEEE Intel. Sys., pages 53­61, 2007.
[6] D. Downey, S. Dumais, and E. Horvitz. Models of searching and browsing: languages, studies, and applications. In Proc. of ICAI, pages 2740­2747, 2007.
[7] A. Druin, E. Foss, L. Hatley, E. Golub, M. L. Guha, J. Fails, and H. Hutchinson. How children search the internet with keyword interfaces. In Proc. of IDC, pages 89­96, 2009.
[8] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Trans. on Info. Sys., 23(2):147­168, 2005.
[9] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg: User behavior as a predictor of a successful search. In Proc. of WSDM, pages 221­230, 2010.
[10] S. Huffman and M. Hochster. How well does result relevance predict session satisfaction? In Proc. of SIGIR, pages 567­ 574, 2007.
[11] A. Kapoor, W. Burleson, and R. Picard. Automatic prediction of frustration. Intl. J. of Human-Computer Studies, 65 (8):724­736, 2007.
[12] C. C. Kuhlthau. Inside the search process: Information seeking from the user's perspective. JASIST, 42(5):361­371, 1991.
[13] D. J. Lawrie. Language models for hierarchical summarization. PhD thesis, 2003.
[14] M. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. of CIKM, pages 623­632, 2007.
[15] R. White, J. Jose, and I. Ruthven. An implicit feedback approach for interactive information retrieval. Info. Proc. and Mngt., 42(1):166­190, 2006.
[16] R. W. White and S. T. Dumais. Characterizing and predicting search engine switching behavior. In Proc. of CIKM, pages 87­96, 2009.
[17] I. Xie and C. Cool. Understanding help seeking within the context of searching digital libraries. JASIST, 60(3):477­494, 2009.

41

Incorporating Post-Click Behaviors into a Click Model
Feimin Zhong12, Dong Wang12, Gang Wang1, Weizhu Chen1,Yuchen Zhang12, Zheng Chen1,Haixun Wang1
1 Microsoft Research Asia, Beijing, China 2Tsinghua University, Beijing, China
{v-fezhon, v-dongmw, gawa, wzchen, v-yuczha, zhengc, haixunw } @microsoft.com

ABSTRACT
Much work has attempted to model a user's click-through behavior by mining the click logs. The task is not trivial due to the well-known position bias problem. Some breakthroughs have been made: two newly proposed click models, DBN and CCM, addressed this problem and improved document relevance estimation. However, to further improve the estimation, we need a model that can capture more sophisticated user behaviors. In particular, after clicking a search result, a user's behavior (such as the dwell time on the clicked document, and whether there are further clicks on the clicked document) can be highly indicative of the relevance of the document. Unfortunately, such measures have not been incorporated in previous click models. In this paper, we introduce a novel click model, called the post-click click model (PCC), which provides an unbiased estimation of document relevance through leveraging both click behaviors on the search page and post-click behaviors beyond the search page. The PCC model is based on the Bayesian approach, and because of its incremental nature, it is highly scalable to large scale and constantly growing log data. Extensive experimental results illustrate that the proposed method significantly outperforms the state of the art methods merely relying on click logs.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]:
General Terms
Algorithms, Experimentation, Performance
Keywords
Post-Click Behavior, click log analysis, Bayesian model
1. INTRODUCTION
It is one of the most important as well as challenging tasks to develop an ideal ranking function for commercial search
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

engine. Most of existing works depend on manually labeled data, where professional editors provide the relevance ratings between a query and its related documents. According to manually labeled data, machine learning algorithms [5, 10, 13] are used to automatically optimize the ranking function and maximize user satisfaction. However, the labeled data is very expensive to be generated and is difficult to keep up with the trend over time. For example, given a query "SIGIR", a search engine is expected to return the most up-todate site such as the SIGIR 2010 website to users, instead of SIGIR 2009. Thus, it is very difficult to maintain the relevance labels up to date.
Compared with manually labeled data, terabytes of implicit user clicks are recorded by commercial search engines every day, which implies that a large scale of click-through data can be collected at a very low cost and it usually reveals the latest tendency of the Internet users. User preference on search results is encoded into user clicks, as such, the click logs provide a highly complementary information to manually labeled data. Many studies have attempted to discover the underlying user preferences from the click-through logs and then learn a ranking function, or regard the click logs as a complementary data source to overcome shortcomings in manually labeled data. Following the pioneered works by Joachims et al.[14] that automatically generated the preferences from the click logs to train a ranking function, many interesting works have been proposed to estimate the document relevance from user clicks, including [1, 2, 3, 6, 18].
Previous works have noticed that the main difficulty in estimating the relevance from click data comes from the so-called position bias: a document appearing in a higher position is more likely to attract user clicks even though it is irrelevant. Recently, Richardson et al.[19] suggested to reward the document relevance at a lower position by multiplying a factor and this idea was later formalized as the examination hypothsis [8] and the position model [7], which indicates the user will click a document only after examining it. Craswell et al. [8] extended the examination hypothesis and proposed the cascade model by assuming that the user will scan search results from top to bottom. Furthermore, Dupret and Piwowarski[9] included the positional distance into the proposed UBM model. Guo et al.[11] proposed the CCM model and Chappell and Zhang[7] proposed the DBN model that generalizes the cascade model by introducing that the conditional probability of examining the current document is related to the relevance of the document at the previous position.
Despite their successes in solving the position-bias prob-

355

260
Dwell Time
220

Seconds

180

140

100

Bad

Good

Perfect

Relevance Rating
Figure 1: The average dwell time on three levels of

relevance rating.

lem, previous works mainly investigate user behaviors on the search page, without considering user subsequent behaviors after a click. Nevertheless, as pointed in the DBN model, a click only represents user is attracted by the search snippet, rather than indicates the clicked document is relevant or user is satisfied with the document. Although there is a correlation between clicks and document relevance, they often differ with each other in many cases. For example, given two documents with similar clicks, if users often dwell longer to read the first document while close the second document immediately, it is likely that users feel satisfied with the first document while disappointed with the second one. Obviously, the relevance difference between these two documents can be discovered from user post-click behaviors, such as the dwell time on the clicked document. As shown in Figure 1, we calculate the average dwell time on three relevance levels in a manually labeled data set1 It is clear that there is a strong correlation between the dwell time and the relevance rating, which validates the importance of incorporating user post-click behaviors to build a better click model.
User subsequent behaviors after a click have been studied for evaluating and improving the quality of the results returned by search engine. Sculley et al.[20] attempted to predict the bounce rates and Attenberg et al.[4] attempted to predict expected on-site actions in sponsored search. Agichtein et al.[2] optimized the ranking function through including some features extracted from post-click behaviors. Postclick behaviors can act as an effective measure of user satisfaction, thus, are very useful to improve the ranking function. However, there are few works investigate how to integrate both click behaviors and post-click behaviors into a click model.
In this paper, we propose a novel click model, called postclicked click model (PCC), to provide an unbiased estimation of the relevance from both clicks and post-click behaviors. In order to overcome the position bias in clicks, the PCC model follows the assumptions in the DBN model [7] that distinguishes the concepts of the perceived relevance and the actual relevance. It assumes that the probability that user clicks on a document after examination is determined by the perceived relevance, while the probability that user examines the next document after a click is determined by the actual relevance of the previous document. Different from DBN, the post-click behaviors are used to estimate the user satisfaction in the PCC model. Some measures such as
1The data set information is introduced in Section 4.

the user dwell time on the clicked page, whether user has the next click, etc are extracted from the post-click behaviors, and used as features that are shared across queries in the PCC model.
The PCC model is based on the Bayesian framework that is both scalable and incremental to handle the computational challenges in the large scale and constantly growing log data. The parameters for the posterior distribution can be updated in a closed form equation. We conduct extensive experimental studies on the data set with 54931 distinct queries and 140 million click sessions. Manually labeled data is used as the ground truth to evaluate the PCC model. The experimental results demonstrate that the PCC model significantly outperform two state of the art methods such as the DBN and CCM models that do not take post-click behaviors into account. Because the PCC model can provide much more number of accurate preference data complementary to manually labeled data, the ranking function trained on the relevance labels from both the PCC model and manually labeled data can produce better NDCG value than merely trained on manually labeled data.

2. PRELIMINARIES
We firstly introduce some background before delving into the algorithm details. When a user submits a query to the search engine, the search engine returns the user some ranked documents as search results. The user then browses the returned documents and clicks some of them. One query session corresponds to all the behaviors the user does under one input query, and we assume there are M displayed documents in each query session.

2.1 Examination and Cascade Hypotheses
The studies on click model attempted to solve the click bias problem in user implicit feedback. There are two important hypotheses, i.e., the examination hypothesis and the cascade hypothesis, that are widely used in various click model implementations. These two hypotheses are quite natural to simulate user browsing habits, and our proposed PCC model also depends on them.
We use two binary random variables Ei and Ci to represent the examination and click events of the document at the position i (i = 1, ..., M ). Ei = 1 indicates the document at the position i is examined by the user, while Ei = 0 indicates this document is not examined. Ci = 1 indicates the user clicks the document at the position i, while Ci = 0 indicates the user does not click this document.
The examination hypothesis assumes that when a displayed document is clicked if and only if this document is both examined and perceived relevant, which can be summarized as follows:

P (Ci = 1 | Ei = 0) = 0

(1)

P (Ci = 1 | Ei = 1) = aui ,

(2)

where ui is the document at the position i, and the parameter aui measures the relevance2 of the document ui indicating the conditional probability of click after examination.
The cascade hypothesis assumes that the user scans linear
to the search results, thus, a document is examined only if
all the above documents are examined. The first document

2aui is the perceived relevance in the DBN model

356

is always examined.

P (Ei+1 = 1 | Ei = 0) = 0

(3)

P (E1 = 1) = 1.

(4)

2.2 DBN Click Model
Since the proposed model follows similar assumptions in the DBN model, we briefly introduce the formulation in DBN. A click does not necessarily indicates that the user is satisfied with this document. Thus, the DBN model [7] distinguish the document relevance as the perceived relevance and the real relevance, where whether the user clicks a document depends on its perceived relevance while whether the user is satisfied with this document and examines the next document depends on the real relevance. Thus, besides the examination and the cascade hypotheses, the DBN click model is characterized as:

P (Si = 1|Ci = 1) = sui

(5)

Ci = 0  Si = 0

(6)

Si = 1  Ei+1 = 0

(7)

P (Ei+1 = 1|Ei = 1, Si = 0) = ,

(8)

where Si is a binary variable indicating whether the user is satisfied with the document ui at the position i, and the parameter sui measures the real relevance of this document. The DBN model uses the EM algorithm to find the maxi-
mum likelihood estimation of the parameters.

2.3 Post-Click Behaviors
Behavior logs in this study are the anonymized logs provided by users who opted in through a widely-distributed browse toolbar. These log entities include a unique anonymous identifier for the user, the issued query to search engine, the visited document, and a timestamp for each page view or search query.
We process behavior logs, and extract all the post-click behaviors after there is a document click on the search page, Thus, for each pair of query and document, several behavior sessions from different users are extracted and the length of each session is fixed no longer than 20 minutes. We then define some measures extracted from the post-click sessions:

· Dwell time on the next clicked page;

· Dwell time on the clicked pages in the same domain;

· Interval time that user inputs another query;

· Whether user has the next click on the clicked document ;

· Whether user switches to another search engine.

For each query and document pair, we calcuate the average value of the above measures over related sessions and the averaged values are used as features into the proposed algorithm.

3. POST-CLICKED CLICK MODEL
We now introduce a novel model, post-clicked click model (PCC), that leverages both click-through behaviors on the search page and the post-click behaviors after the click.

Ei-1

Ai

Ei

Ci

Ei+1

Si

aui sui
f1
......
fn

ui ui
ui ui
m1 1
......
mn n

Figure 2: The PCC model. The variables Ci and fi (i) are the observed variables given a query session.

3.1 Model
The PCC model is a generative Bayesian model and is explained in Figure 2, where the variables inside the box are defined at the session level, and the variables outside are defined at the query level. The variables Ei, Ci, and Si are defined the same as in the Section 2. Here we assume there are n features extracted from user post-click behaviors and fi is the feature value of the ith feature.
au  N (u, u2), su  N (u, 2u), fi  N (mi, i2). (9)
Thus, u and u2 are the parameters of the perceive relevance variable au, u and 2u are the parameters of the real relevance variable su, and mi and i2 are the parameters of the ith feature variable fi.
The PCC model is characterized by the following equations:

E1 = 1

(10)

Ai = 1, Ei = 1  Ci = 1

(11)

P (Ai = 1 | Ei = 1) = P (au + > 0)

P (Si = 1 | Ci = 1) = P (su +

n i=1

yu,i fi

+

Ci = 0  Si = 0

(12) > 0) (13)
(14)

Si = 1  Ei+1 = 0

(15)

P (Ei+1 = 1 | Ei = 1, Si = 0) = 

(16)

Ei = 0  Ei+1 = 0,

(17)

where  N (0, 2) is an error parameter and yu,i is a binary value indicating whether we can extract the value of the ith feature on the document u. It is possible that, for a document u, no user has clicked this document, thus, there is no information extracted from post-click behaviors on the ith feature. Thus, yu,i = 0 in this case. Otherwise, yu,i = 1.
The PCC model simulates user interactions with the search engine results. When a user examines the ith document, he will read the title and the snippet of this document, and whether the document attracts him depends on the perceived relevance of this document aui . If the user is not attracted by the snippet (i.e., Ai = 0), he will not click the document which also indicates he is not satisfied with this document (i.e., Si = 0). Thus, there is a probability  that the user will examine the next document at the position i + 1, and a probability 1 -  that the user stops his search on this query. If the user is attracted by the snippet (i.e., Ai = 1), he will click and visit the document. User postclick behaviors on the clicked document are very indicative

357

to infer how much the user is satisfied with this document. If the user is satisfied (i.e., Si = 1), he will stop this search session; Otherwise, he will either stop this search session or examine the next document depending on the probability .
The equations (10) and (17) is the cascade hypothesis and the equation (11) is the examination hypothesis. The equation (12) shows that when a user examines the document, whether the user would click or not depends on the variable aui and the error term. The equation (13) shows that when the user clicks and visits the document, the value of the post-click behavior features will affect whether the user is satisfied or not. The equation (14) and (15) mean that the user will not be satisfied if he does not click the document, while the user will stop the search when he is satisfied. The equation (16) shows that if user is not satisfied by the clicked document, the probability he continues browsing the next search results is  while the probability he abandons the session is 1 - .

3.2 The Parameter Update
After observing one query session, we update the related parameters of each document in this session. For each document in one query session, it can be distinguished into five cases and the parameter update for these five cases are different. We denote l as the last clicked position. When l = 0, it corresponds to the session with no click, and when l > 0, it corresponds to the session with clicks. We define two sets of positions: A is the set of positions before the last click and B is the set of positions after the last click. Thus, the five cases are defined as follows:

· Case 1 : l = 0, which indicates there is no click in the session. In this case, we update the parameters of the kth document with the equation (23).

· Case 2 : l > 0, k  A, Ck = 0, which indicates the kth document is at the non-clicked position before the last click. In this case, we update the parameters with the equation (24).

· Case 3 : l > 0, k  A, Ck = 1, which indicates the kth document is at the clicked postion before the last click. In this case, we update the parameters with the equations (25) , (26) and (27).

· Case 4 : l > 0, k = l, Ck = 1, which indicates the kth document is at the last clicked position. In this case, we update the parameters with the equations (28) , (29) and (30).

· Case 5 : l > 0; k  B, Ck = 0, which indicates the kth document is at the position after the last click. In this case, we update the parameters with the equation (31).

For a fixed k(1  k  M ), suppose x is the parameter we want to update, we follow the equation:

p(x | C1:k)  p(x) × P (C1:k | x)

(18)

to get the posterior distribution. Then we approximate it to Gaussian distribution use KL-divergence. The method to derive the updating formula is based on the message passing [15] and the expectation propagation[17]. Since the space limitation, we omit the proof of these formula. For convenience, we will introduce some functions that will be used

in the following update equations:

1 -c2 N (c) = e 2 ;
2

(19)

c

(c) =

N (x)dx;

(20)

-

N (c)

v(c, )

=

(c)

+

 1-

;

(21)

w(c, ) = v(c, )(v(c, ) + c).

(22)

3.2.1 Case 1:
For the kth document, the observation is A1 = 0, E1 = 1, Ci = 0, 1  i  k . We update the parameters related to the ith document. This is the update of the parameter in the perceived relevance:


     
     

uk
u2k c=


 -

 - uk
2
 (1 - uk
uk

u2k v(c,1,k ) 1
(2u2+kwu2(kc,)21,k )  2 +u2 k

1

)

(2+u2k ) 2

(23)

where 1,k is a coefficient whose value is given in Appendix. The parameters of the features and the real relevance are kept the same.

3.2.2 Case 2:
For the kth document, the observation is Ak = 0, Ek = 1. Thus, we update the parameters related to the kth document. The update of the parameter in the perceived relevance is:


     
     

uk
u2k c=

-u2uukkk(1-1-(.vu2(cu2k,u2k0+k)w+(2u2c)k,2012) )

(u2k +2) 2

(24)

The parameters of the features and the real relevance are kept the same.

3.2.3 Case 3:
For the kth document, the observation is Ak = 1, Ek = 1 and Sk = 0. Thus, we update the parameters related to the kth document. The update of the parameter in the perceived relevance is:


     
     

uk

 uk

+

v(c,0)u2 k 1
(u2k +2) 2

u2k c=



u2k (1 - . uk
1

) u2k w(c,0)
u2k +2

(u2k +2) 2

(25)

The update of the parameter in the feature is:


     
     

mi  mi -
(
i2  i2(1 -
c = -(uk +

v(c,0)i2yuk ,i

). nj=1yi2uwk(,cj,0j2)y+uk2u,ki

+2

)

1 2

njnj==11yyuukk,j,jmjj2)+2uk +2

1

(

n j=1

yuk ,j

j2 +2uk

+ 2 )

2

(26)

358

The update of the parameter in the real relevance is:


     
     

v(c,0)2uk

   - uk
2

uk 2

(

n j=1

yuk,2ujkwj2 +(c,02u)k

+ 2 )

1 2

   (1 - ) uk

uk -(uk +

n j=1

n j=1
yuk

yuk ,j j2 ,j mj )

+2uk

+2

c= (

n j=1

yuk ,j

j2 +2uk

+ 2 )

1 2

(27)

3.2.4 Case 4
For the last clicked document, the observation is Cl = 1, Ci = 0(i = l + 1 to M ) and we update the parameters related to the lth document. The update of the parameters in the perceived relevance is:


     
    

ul

 ul

+

v(c,0)u2 l 1
(u2l +2) 2

u2l c=



u2l (1
ul

- 1.

u2l w(c,0) u2l +2

)



(u2l +2) 2

The update of the parameters in the feature is:

(28)



 

mi

 mi +

 

(



v(c,2 )i2

n j=1

yul ,j

j2 +2ul

+ 2 )

1 2


     

2

2

i2 w(c,2 )

   (1 - ) i

i (ul +

nj=nj1=y1uyl,ujlm,jj)j2+2ul +2

c= (

n j=1

yul ,j

j2 +2ul

+ 2 )

1 2

(29)

where 2 is a coefficient whose value is given in Appendix. The update of the parameters in the real relevance is:


     
     

v(c,2 )2ul

   + ul
2

ul 2

(

n j=1

yul ,j 2ul

j2+2ul +2) w(c,2 )

1 2

   (1 - ) ul

ul (ul +

n j=1

n j=1

yul,j j2+2ul

yul,j mj )

+2

c= (

n j=1

yul ,j

j2 +2ul

+ 2 )

1 2

(30)

3.2.5 Case 5
For the kth document, the observation is Cl = 1, Ck = 0(k = l+1 to M ). Thus we update the parameter related to the kth document. The update of the parameter in the perceived relevance is:


     
    

   - ui

ui

u2i v(c,3,k ) 1
(2+u2i ) 2

u2i c=

 -

u2i (1
ui

- u2i w(c,3,k )
 2 +u2 i 1

)



(2+u2i ) 2

(31)

where 3,k is a coefficient whose value is given in Appendix. The parameters in the features and the real relevance are kept the same.

3.3 Algorithm

Following the above update formula, we can easily build

the PCC training algorithm as follows:

1. Initialize au, fi and su (u, i) to the prior distribu-

tion N (-0.5, 0.5).

2. For each session

3. If l = 0, update each document with (23)

4. Else

5.

For k = 1 to M

6.

If k < l , Ck = 0, update (24)

7.

If k < l , Ck = 1, update (25),(26) and (27)

8.

If k = l, update (28),(29) and (30)

9.

If k > l, update (31)

10.

Endfor

11. Endif

12. End

Given a collection of training sessions, we sequentially up-

date the parameters according to the five cases. Since the

update formula is in a closed form, the algorithm can be

trained on a large scale and constantly growing log data.

After training the PCC model, we set the user satisfaction

probability to zero, i.e., P (S = 1 | C = 1) = 0, for those

documents that have never been clicked.

The PCC model follows the assumption in DBN to dis-

tinguish the document relevance as the perceived relevance

P (A = 1|E = 1) and the real relevance P (S = 1|C = 1).

We define the document relevance inferred from the PCC

model as:

relu = P (A = 1 | E = 1)P (Su = 1 | C = 1)

=

( (u2

u

+

2)

1 2

)( (2u

u + + 2 +

n i=1

yu,i mi

n i=1

yu,i i2 )

1 2

).(32)

This document relevance relu will be evaluated on the ground truth ratings in manually labeled data.

4. EXPERIMENTAL RESULTS
In the experiment, we evaluate the document relevance and the click perplexity inferred from the PCC model, and the results are compared with other click models including DBN and CCM. The experiments are organized into four parts. In the first part, we analyze the pairwise accuracies of the relevance among different click models. In the second part, we use the generated relevance to rank the documents directly and evaluate the ranking function according to the normalized discounted cumulative gain (NDCG) [12]. In the second part, we use the RankNet algorithm to learn a ranking function on the preference pairs extracted from both the click model and manually labeled data, and illustrate the ranking improvement. Finally, we illustrate the click perplexity among different click models.
4.1 Data Set
The click logs used to train the click models are collected from a large commercial search engine which comprises 54,931 randomly sampled queries and about 2 million related documents from the U.S. market in English language, and the total number of search sessions from one month click-through log is about 143 million. For each search session, we have one input query, a list of returned documents on browsed pages and a list of positions of the clicked documents. The information on the click logs is summarized in Table 1.
For each query and document pair, we collect corresponding post-click sessions in 20 minutes from one month behavior log. We calculate the average values of five features, as introduced in Section 2.3, from post-click behaviors and they are used to train and evaluate the PCC model.
The manully labeled data is used as the ground truth to evaluate the relevance from click models. In the human relevance system (HRS), editors provided the relevance ratings for 4,521 queries and 127,519 related documents. On average, 28.2 documents per query are labeled. A five grade

359

Query Frequency 1 to 30
30 to 100 100 to 1,000 1,000 to 10,000
>10,000 all

Query 33,519 5,836 8,270 5,282 2,024 54,931

Document 437,610 163,133 425,594 578,198 401,083 2,005,618

Total Sessions 182,312 332,194 3,031,827
17,827,303 121,589,355 142,962,991

Table 1: The summary of the search sessions from one month click logs.

rating is assigned to each query and document (4: perfect, 3: excellence, 2: good, 1: fair, 0: bad ). The documents without judgement are labeled as 0. The summary of the HRS is introduced in Table 2.

Query Frequency 1 to 30
30 to 100 100 to 1,000 1,000 to 10,000
>10,000 all

Query 772 666 1,342 1,074 662 4,516

Document 11,328 12,335 33,568 37,092 33,196 127,519

Table 2: The summary of the data in human relevance system (HRS).

4.2 Pairwise Accuracy

The document relevance is derived from the PCC model according to Equation (32), and we compute the relevance for those queries and related documents that are overlapped with the HRS data in the experiment. Since the relevance value is a real number between [0, 1], while the rating in HRS, denoted as hrsu, is a discrete number from 0 to 4, it is unable to match them directly. We evaluate the relevance according to the pairwise accuracy based on the number of concordances and discordances in preference pairs. Given two documents ui and uj under the same query, the concordant pair is that if hrsui > hrsuj and relui > reluj , or if hrsui < hrsuj and relui < reluj . An discordant pair is that if hrsui > hrsuj and relui < reluj , or if hrsui < hrsuj and relui > reluj . This pairwise accuracy is calculated as follows:

acc = 1 - D

(33)

N

Here, D represents the number of discordant pairs and N represents the total number of pairs generated by the click model.
Similarly, we compute the document relevance from the DBN and the CCM model according to the probability P (C = 1|E = 1). After training click model, we generate the preference pair with respect to each pair of documents under the same query. However, we notice that the number of generated preference pairs from different click models varies significantly different. Thus, even one algorithm reaches better accuracy than another one, since the number of preference pairs is different, we cannot conclude which algorithm is better. In order to provide a fair evaluation, we introduce a threshold  such that the preference pair ui > uj is generated only when

relui - reluj > ,

(34)

where   0. Thus, we can generate different set of preference pairs through setting different  value. When we set  as a larger value, less number of preference pairs are generated. Moreover, since the relevance difference becomes large, the generated preference pairs are more reliable. Accordingly, we evaluate the pairwise accuracy among different algorithms in terms of the similar number of preference pairs.
Figure 3 reports the result of pairwise accuracies among three click models. For each click model, we set a series of  values to generate different number of preference pairs and compute related pairwise accuracies. As  increases, the number of pairs decreases and the pairwise accuracy increases correspondly. When the pair number is 1 million, the PCC model reaches to the pairwise accuracy 82.8% while DBN and CCM reaches to 81.7% and 78.2% respectively. When the number of pairs is 0.5 million, PCC reaches to the accuracy 86.3% while DBN and CCM reaches to 83.9% and 78.6% respectively. On average, the PCC model achieves 2% and 5% accuracy improvement than that of the DBN and CCM models.

Accuracy

0.92 0.9
0.88 0.86 0.84 0.82
0.8 0.78 0.76
0

PCC DBN CCM

500000

1000000

1500000

Pair Number

2000000

Figure 3: The pairwise accuracy comparison among three click models in terms of the number of preference pairs.

4.3 Ranking by Predicted Relevance
In the part, we use the predicted relevance to rank the documents directly. For one query and their related documents, every document is treated equally in computing the pairwise accuracy in the above. However, the ranking evaluation such as NDCG often put more emphasis on the documents at top positions. As such, the relative order of the documents with higher predicted relevance is more important than the documents with lower relevance.
For each query, we rank the returned documents according to the relevance value relui (i) and compute NDCG@1 and NDCG@3 scores for the PCC, DBN and CCM models. The results are shown in Figure 4 and 5, where we decompose the NDCG score in terms of query frequency. We can see when the query frequency is between 100 to 1000, NDCG@1 of the PCC model is 63.1%, which has 3% and 17% improvement than that of DBN and CCM, respectively. For extremely low frequent queries, the NGCG@1 improvement of the PCC model over DBN and CCM becomes less significant. The main reason is because the post-click features cannot be extracted for these queries and their related documents so that the post-click behaviors cannot contribute

360

to the click model, which proves the effectiveness of incorporating post-click behavior into click model.
The overall NDCG@1 for all queries is 63.2%, which has 2% and 13% improvement over DBN and CCM. We also observe very similar results in NDCG@3, which demonstrates that the relevance inferred from PCC is consistently better than that from DBN and CCM.

NDCG@1

0.75

PCC

0.7

DBN

0.65

CCM

0.6

0.55

0.5

0.45

0.4

0.35

1~30

30~100

100~1,000 1,000~10,000 >10,000

all

Query Frequency

Figure 4: The NDCG@1 comparison among three click models in terms of query frequency.

NDCG@3

0.7
PCC
0.65
DBN
0.6
CCM
0.55

0.5

0.45

0.4

0.35

0.3

1~30

30~100

100~1000 1000~10000 >10000

all

Query Frequency

Figure 5: The NDCG@3 comparison among three click models in terms of query frequency

4.4 Integrating Predicted Relevance and HRS
Learning to rank is to optimize a ranking function from a set of documents with relevance ratings. We follow the RankNet [5] method which is a pairwise ranking algorithm receiving the pairwise preferences to optimize the ranking function. For each query and document, we extract about three hundred of features in the experiment, where the features are similar to those defined in LETOR[16]). Since the document relevance inferred from the PCC and DBN models is better than that from the CCM model in the above two experiments, we only consider the PCC and DBN models in this part of experiment.
We partition the HRS data as described in Table 2 into the training and testing sets. We randomly choose 3,000 queries and related 85,173 documents into the training set, and other queries and documents are in the testing data. There are totally about 5.1 million preference pairs generated from HRS as the training data. In addition, the click model are trained on the click log as described in Table 1, thus, there are about 7.4 million preference pairs generated from the PCC and the DBN. We construct three training

sets for the RankNet: 1. only HRS; 2. PCC + HRS; 3. DBN + HRS, and evaluate the ranking function on the HRS testing data. The results on NDCG@1 and NDCG@3 are shown in Figure 6 and 7.

0.8
PCC+HRS

0.75

DBN+HRS

HRS
0.7

NDCG@1

0.65

0.6

0.55

0.5

1~30

30~100

100~1,000 1,000~10,000 >10,000

All

Query Frequency

Figure 6: The NDCG@1 results from the RankNet algorithm on three different training sets.

NDCG@3

0.8

0.75

PCC+HRS

DBN+HRS

0.7

HRS

0.65

0.6

0.55

0.5

1~30

30~100

100~1,000 1,000~10,000 >10,000

All

Query Frequency

Figure 7: The NDCG@3 results from the RankNet algorithm on three different training sets.

The NDCG@1 and NDCG@3 results illustrate that the ranking function trained on the "PCC + HRS" data consistently outperform the function on the "DBN + HRS" data, while the function on the "DBN + HRS" data outperforms the function trained only on the "HRS" data. The overall NDCG@1 from "PCC+HRS" is 1.9% higher than that from "HRS", which is a significant improvment of the ranking function on such large scale training and evaluation data.

4.5 Click Perplexity
Click perplexity is used as an evaluation metric to evaluate the accuracy of the click-through rate prediction. We assume that qij is the probability of click drived from the click model, i.e. P (Ci = 1|Ei = 1) at the position i and Cij is a binary value indicating the click event at the position i on the jth session. Thus, the click perplexity at the position i is computed as follows:

p = 2 i

-

1 N

N n=1

(Cin

log2

qin

+(1-Cin

)log2

(1-qin

))

(35)

Thus, a smaller perplexity value indicates a better prediction.
The result on click perplexity is shown in Figure 8. We can see that the PCC model performs the best for the clicks in the first position. As for the other positions, the click

361

perplexity from PCC are very similar to that from CCM. Although CCM has not inferred the document relevance very well in the above experiment, its click perplexity performs as well as PCC. The click perplexity obtained from PCC significantly outperforms the perplexity from DBN, which indicates that incorporating post-click behaviors into a click model can also produce a much better click prediction.

Perplexity

1.9

1.7

PCC

DBN 1.5
CCM 1.3

1.1

0.9

0.7

0.5

1 2 3 4 5 6 7 8 9 10 Position

Figure 8: The click perplexity comparisons among three click models in terms of search position.

5. CONCLUSION AND EXTENSION
Besides user behaviors on the search result page, postclick behaviors after leaving the search page encodes very valuable user preference information. Different from previous works, this paper firstly investigates how to incorporate post-click behaviors into a click model to infer the document relevance. It proposes a novel PCC model by leveraging both click behaviors and post-click behaviors to estimate the degree of user satisfaction via a Bayesian approach. We conduct extensive experiments on a large scale data set and compare the PCC model with the state of the art works such as DBN and CCM. The experimental results show that PCC can consistently outperform baselines models on four different experimental setting. It is worth noting that the update of the PCC model is in a close form, which is capable of processing very large scale data sequentially.
The proposed method of incorporating post-click behaviors in the paper is a very general solution and can be extended to other click models such as CCM, UBM, etc. In the PCC model, the post-click behaviors are used as the features to estimate the user satisfication on the clicked document. However, it is not the only approach of incorporating postclick behaviors into click model. Another possible approach is to simulate user post-click behaviors through constructing a separate user browse model and then integrate it with the click models. We will explore these directions in future works.
6. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. In proceedings of SIGIR2006, 2006.
[2] E. Agichtein, E. Brill, and D. Susan. Improving web search ranking by incorporating user behavior information. In proceedings of SIGIR2006, 2006.
[3] R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra, and P. Tsaparas. Generating labels from clicks. In proceedings of WSDM2009, 2009.

[4] J. Attenberg, S. Pandey, and T. Suel. Modeling and predicting user behavior in sponsored search. In proceedings of KDD2009, 2009.
[5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In proceedings of ICML2005, 2005.
[6] B. Carterette and R. Jones. Evaluating search engines by modeling the relationship between relevance and clicks. In proceedings of NIPS20, 2008.
[7] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In proceedings of WWW2009, 2009.
[8] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In proceedings of WSDM2008, 2008.
[9] G. Dupret and B. Piwowarski. User browsing model to predict search engine click data from past observations. In proceedings of SIGIR2008, 2008.
[10] Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. The Journal of Machine Learning Research, 4:933­969, 2003.
[11] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y. Wang, and C. Faloutsos. Click chain model in web search. In proceedings of WWW2009, 2009.
[12] K. Jarvelin and J. Kekalainen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems 20(4), 422-446 (2002), 2002.
[13] T. Joachims. Optimizing search engines using clickthrough data. In proceedings of KDD2002, 2002.
[14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In proceedings of SIGIR2005, 2005.
[15] F. R. Kschischang, B. J. Frey, and H-A. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Throry, 1998.
[16] T.-Y. Liu, T. Qin, J. Xu, X. Wenying, and H. Li. Letor: A benchmark collection for research on learning to rank for information retrieval. http://research.microsoft.com/enus/um/beijing/projects/letor/.
[17] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of Technology, 2001.
[18] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In proceedings of KDD2005, 2005.
[19] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In proceedings of WWW2007, 2007.
[20] D. Sculley, R.G. Malkin, S. Basu, and R.J. Bayardo. Predicting bounce rates in sponsored search advertisements. In proceedings of KDD2009, 2009.

7. APPENDIX
Since the computation of the coefficients 1, 2 and 3 is rather complicated, we move their equations into this section:

g(k - 1, 0)

1,k = 1 - (1 - )

k-2 j=0

g(j,

0)

+

g(k

-

1,

0)

M -1

2 = (1 - ) g(j, l) + g(M, l)

j=l

3,k = 1 -

P (Sul = 0)g(k - 1, l)

P (Sul = 1) + P (Sul = 0) (1 - )

k-2 j=l

g(j,

l)

+

g(k

-

1,

l)

where g(i, j) =

i-j P (Aj+1 = 0) × · · · × P (Ai = 0) i > j

1

ij

362

A Comparison of General vs Personalised Affective Models for the Prediction of Topical Relevance

Ioannis Arapakis, Konstantinos Athanasakos, Joemon M. Jose
Department of Computing Science University of Glasgow Glasgow, G12 8QQ
{arapakis,athanask,jj}@dcs.gla.ac.uk

ABSTRACT
Information retrieval systems face a number of challenges, originating mainly from the semantic gap problem. Implicit feedback techniques have been employed in the past to address many of these issues. Although this was a step towards the right direction, a need to personalise and tailor the search experience to the user-specific needs has become evident. In this study we examine ways of personalising affective models trained on facial expression data. Using personalised data we adapt these models to individual users and compare their performance to a general model. The main goal is to determine whether the behavioural differences of users have an impact on the models' ability to determine topical relevance and if, by personalising them, we can improve their accuracy. For modelling relevance we extract a set of features from the facial expression data and classify them using Support Vector Machines. Our initial evaluation indicates that accounting for individual differences and applying personalisation introduces, in most cases, a noticeable improvement in the models' performance.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Relevance Feedback, Search Process; I.5.1 [Computing Methodologies]: Pattern Recognition--Models
General Terms
Experimentation, Human Factors, Performance
1. INTRODUCTION
The main challenge information retrieval (IR) systems face nowadays originates from the semantic gap problem: the semantic difference between a user's query representation and the internal representation of an information item The research leading to this paper was supported by the European commission, under the contract FP6-027122 (SALERO).
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

in a collection. Although progress has been made, the effectiveness of existing systems is still limited. The gap is further widened when the user is driven by an ill-defined information need, often the result of an anomaly in his/her current state of knowledge [7]. The formulated search queries, which are used by the retrieval systems to locate potentially relevant items, produce results that do not address the users' true needs.
To deal with information need uncertainty IR systems have employed in the past a range of feedback techniques, which vary from explicit [13, 21] to implicit [1, 6]. The notion of explicit feedback was present from the early years of IR, but it soon became apparent that users could not cope with the cognitive burden of explicit relevance judgments. Alternative paths had to be discovered, which led to the unobtrusive, yet less robust, implicit feedback techniques [12, 15]. Even though this was a step towards the right direction a need to personalise and tailor the search experience to the user-specific needs was progressively made evident.
Personalisation emerged as an appealing technique in dealing with the issues caused by the variation of online behaviour and the individual differences observed in user interests, information needs, search goals, difficulties encountered, and other. To apply personalisation an IR system must initially employ a modelling technique that will capture certain user characteristics. At a later stage, information filtering is performed to refine the aggregated information and adjust the system's responses to accommodate users' needs, thus providing a more personalised experience.
Several attempts have been made in the past to develop user models, using implicit feedback. In [16], Oard and Kim define a set of application-specific observable behaviours (examination, retention, etc.) and introduce the concept of learning user interests and building user profiles from implicit data. In [20], Puolam¨aki et al. combine implicit feedback with explicitly created user profiles. In the latter work, the authors use mixture models to combine different sources of relevance judgments. The implicit feedback information derives from eye-movement data, used in combination with a probabilistic collaborative filtering model.
In [2] the authors make the assumption that, apart from user modelling, query-specific behaviours are also important and should be considered when attempting to predict topical relevance. Following this work, Liu et al. [14] constructed user profiles based on users' search history and developed algorithms that mapped query terms to predefined categories. The latter information was used to extract users' interests and address issues related to word ambiguity. Teevan et

371

al. showed in [23] that richer representations of the user lead effectively to more accurate relevance predictions. This improvement is achieved by combining different sources of information, such as a search history, webpages visited, documents created and viewed, etc., which is used to re-rank the results obtained by a search system.
However, although the identification of user interests is a definite step, it is important to examine how these interests evolve, interact and lose focus, from a temporal perspective. In [9], Daoud et al. consider in this context the problem of search-session boundary recognition. In the above study the users were represented by long-term interests and short-term contexts, which were both essentially ontologies of semantically linked concepts. Their approach to personalisation yielded significant improvements compared to the conventional query handling paradigm.
In this paper we examine ways of personalising affective models, trained on facial expression data gathered by many individuals. Our work is limited to that of modelling users' affective behaviour and does not involve information filtering or adaptation of content. Using personalised data we adapt these models to individuals and compare their performance to a general model. For modelling relevance we extract a set of features from the facial expression data and classify them using Support Vector Machines. Our initial evaluation indicates that accounting for individual differences and applying personalisation introduces, in most cases, a noticeable improvement in the models' performance. To our knowledge, no prior work has ever applied personalisation on the affective level interaction, in the context of online information seeking.
1.1 Research Questions
The major goal of this study is to develop personalised affective models, adapted to the individual characteristics of specific users, and compare their ability to discriminate between relevant and irrelevant items against general affective models. To achieve this goal we had to expose our participants to stimuli of varied intensity. As a result, the information that we collected covered a much wider spectrum of affective behaviour and allowed the comparability of results with previous work. We, furthermore, explore different ways of combining the personalised data with the general data, to optimise the models' performance. Overall, we examined the following research hypothesis:
H1: By adapting a general affective model with personalised data, to a specific user, we can improve its accuracy in predicting topical relevance.
H2: Merging general with personalised data is more effective personalisation method compare to training separate models and applying information fusion on a decision level.
2. EXPERIMENTAL METHODOLOGY
By definition an experimental study introduces the participants to an artificial situation that takes place at a laboratory setting, therefore lacking the ecological validity of a naturalistic study. In addition, when analysing facial expressions several critical issues arise [22]. Firstly, emotional expressions are highly idiosyncratic in nature and may vary significantly from one individual to another (depending on personal, familial or cultural traits). Secondly, spontaneous expressive behaviour may not be easily elicited, especially when participants are aware of being recorded. Finally,

while interacting with researchers and other authorities the participants may intentionally try to mask or control their emotional expressions, in an attempt to act in appropriate ways.
While taking into consideration the above factors we devised an experimental setup, similar to the one adopted in [3], that mitigated most of the unwanted effects. In our approach we: (i) employed a facial expression recognition system of reasonably robust performance and accuracy across all individuals, (ii) applied hidden recording, thus increasing the chance of observing spontaneous behaviour, and (iii) made our presence in the laboratory setting as unobtrusive as possible.
2.1 Design
This study used a repeated-measures design. There were two independent variables: task difficulty (with three levels: "easy", "average", "difficult") and personalisation technique (with two levels: "adaptation" and "weighted voting"). The task difficulty levels were controlled by re-ranking the returned results to include 8 relevant - 2 irrelevant, 5 relevant 5 irrelevant, and 2 relevant - 8 irrelevant documents, accordingly. The set of relevant documents consisted of top-ranked results, while the set of irrelevant documents consisted of bottom-ranked results. This way we improved or decreased the chances of locating relevant items among the results. The personalisation technique was controlled by adopting a different approach (mixing general with personalised data, or using them separately to train different models). The dependent variables were: (i) task (difficulty, complexity, etc.), (ii) search process, and (iii) models' performance, in terms of accuracy.
2.2 Apparatus
For our experiment we used one desktop computer, equipped with monitor, keyboard, mouse and a web-camera. The computer provided access to a custom-made search interface, which allowed the participants to perform their search tasks. The search interface was designed to re-rank the results for each submitted query, according to the level of task difficulty, without the participants being aware of it. In addition, a custom-made script logged participants' desktop actions, such as starting, finishing and elapsed times for interactions, and click-throughs. The web-camera (Creative Live! Cam Optia AF with a 2.0 megapixels sensor) was used in combination with eMotion [24], for the application of real-time facial expression analysis. Finally, we used entry-, post- and exit- questionnaires in each session.
2.2.1 Search Tasks
We prepared a number of search tasks that covered a variety of context, from entertainment to health-related issues, in order to capture participants' interest as best as possible. All tasks were performed manually, prior to the experiment, to ensure the availability of relevant documents. The search tasks were presented using the structural framework of the simulated information need situations [8]. By doing so, we introduced short cover stories that helped us describe to our participants the source of their information need, the environment of the situation and the problem to be solved. We believe that this way we facilitated a better understanding of the search objective and, in addition, we introduced a layer

372

Topic 1: A task of digging cheesy gossips and scandals.
Topic 2: Formulate an opinion about existing social networking sites.
Topic 3: A task of investigating, obtaining advance knowledge, or doing research on a particular sport.
Topic 4: A task of finding information regarding contraception methods.
Topic 5: A task of investigating, obtaining new knowledge, or doing research on global warming.
Topic 6: A task of planning your Christmas holidays.
Table 1: A list of the available search tasks
of realism, while preserving well-defined relevance criteria. An indicative list of the topics is presented in Table 1.
2.2.2 Search Interface
For the completion of the search tasks we used a custommade search environment (Zoogle) that was designed to resemble the basic layout of existing search services, while retaining a minimum of graphical elements and distractions. Zoogle works on top of Yahoo! API. For every submitted query it returned a list of ten results, stripped of their title, snippet or any other metadata. This layout was intentional to ensure that the participants would not be able to judge the topical relevance of the returned documents, prior to examining them.
Even though this approach introduced our participants into artificial search situations, which differ from real-life experiences, it was a necessary trade-off for capturing affective responses exhibited towards the viewed content. In addition, we ensured that the participants viewed an equal number of relevant and irrelevant documents. This allowed us to develop balanced sets of affective data.
Zoogle applies a layered architecture approach, similar to that adopted in [5]. The first layer of the interface is dedicated for supporting any interaction that occurs during the early stages of the search process (such as query formulation and search execution). Any output generated during this phase is presented in the second layer. From there, the participants can select and preview any of the retrieved documents. The content of an item is shown in a separate panel in the foreground, which constitutes the third layer of our system.
The main purpose of this layered architecture is to isolate the viewed content from all possible distractions that reside on the desktop screen; therefore, establishing additional ground truth that allowed us to relate participants' affective responses to the source of stimuli (in our case, the perused documents). This was an important aspect of our experimental methodology, since we were interested in isolating content-particular emotions. Upon examining a document, the participants had the option to either bookmark or ignore it. The first option would classify the document as relevant, while the latter as irrelevant.
2.2.3 Questionnaires
The participants completed an Entry Questionnaire at the beginning of the study, which gathered background and demographic information, and, furthermore, inquired about previous experience with online searching. A Post-Search Questionnaire was also administered at the end of each task,

to elicit subjects viewpoint on certain aspects of the search process. The questions were divided into three sections that covered the search session, the encountered task and the returned results.
Finally, an Exit Questionnaire was introduced at the end of the study. The questionnaire gathered information on participants' views about the importance of affective feedback, with respect to usability and ethical issues. All of the questions included in the questionnaires were forced-choice type.
2.2.4 Facial Expression Recognition
Facial expressions have been associated in the past with universally distinguished emotions, such as happiness, sadness, anger, fear, disgust, and surprise [11]. Recent findings also indicate that emotions are primarily communicated through facial expressions [17] and are generally regarded as essential aspects of human social interaction. The face provides conversational signals, which do not only clarify our current focus of attention [18] but also regulate our interactions with the surrounding environment and the organisms that inhabit it.
In this study we applied real-time facial expression analysis using eMotion, an automatic facial expression recognition system with emotion-detection capabilities. The process of recognition occurred as follows: initially, eMotion would locate certain facial landmark features (eyebrows, corners of the mouth, etc.) and construct a 3-dimensional wireframe model of the face, consisting of surface patches wrapped around it. After the construction of the model, head motion or any other facial deformation would be tracked and measured in terms of motion-units (MU's), and, finally, classified into one of the seven detectable emotion categories.
Automatic systems are an alternative approach to facial expression analysis [19] and have exhibited performance comparable to that of trained human recognition (87%). eMotion applies a generic classifier that has been trained on a diverse data set, combining data from the Cohn-Kanade database. Its main advantage is its reasonable performance across all individuals, irrespectively of the variation introduced from mixed-ethnicity groups. Results of the persondependent and person-independent tests presented in [24] support our performance-related assumptions. For additional information regarding the advantages and limitations of eMotion the reader is referred to [24, 4]
2.3 Participants
Sixteen healthy participants of mixed ethnicity and educational background (8 MSc students, 4 BSc. and 4 other) applied for the study through a campus-wide ad. They were all proficient with the English language (1 native, 14 advanced, and 1 intermediate speakers). Out of 16, 7 were male and 9 were female and were between 21-32 years of age (M =25.83, SD=2.57). They had an average of 7.33 years of online search experience and all claimed to have been using at least one search service in the past (with the most popular being "Google" and "Yahoo!"). On average, the participants reported carrying out online searches once or twice a day (M =5.33, SD=0.84). The frequency was measured using a 6-point scale (1="Never", 2="Once or twice a year", 3="Once or twice a month", 4="Once or twice a week", 5="Once or twice a day", 6="More often").

373

2.4 Procedure
The user study was carried out in the following manner. The formal meeting with the participants took place in the laboratory setting. At the beginning of the session the participants were given an information sheet, which explained the conditions of the experiment. They were then asked to sign a Consent Form and were notified about their right to withdraw at any point during the study, without having their legal rights or benefits affected. Finally, they were given an Entry Questionnaire to fill in. The session proceeded with a a brief tutorial on the use of the search interface, followed by a calibration of the web-camera. The participants' were told that the web-camera was used for eye-tracking purposes, thus concealing it's true operation.To ensure that their faces would be visible to the camera at all times we encouraged them to keep a proper posture, by indicating the need to stay within the visual field of the eye-tracker.
Each participant completed three search tasks, one for each level of difficulty (see Section §2.1). In every task they were handed six topics and were asked to proceed with the one they found most interesting. For each topic the subjects were given 15 minutes, during which they had to locate as many relevant documents as possible. For every submitted query the search interface would return ten results, which they were asked to evaluate one by one. If a document was judged as relevant the participants had the option to bookmark it, or otherwise ignore it and continue with the evaluation of the remaining items. Depending on the level of task difficulty ("easy", "average", "difficult") the ratio of relevant-irrelevant documents varied accordingly (the participants were unaware of this uneven distribution of relevant/irrelevant documents). To negate the order effects we counterbalanced the task distribution by using a Latin Squares design. At the end of each task, the participants were asked to complete a Post-Search Questionnaire.
An Exit Questionnaire was administered at the end of each session. The participants were informed about the unknown conditions of the study and were asked to sign a second Consent Form, which was granting us permission to retain the accumulated facial expression data. Finally, the participants were asked to sign a Payment Form, prior to receiving the fee of £10.
3. DATA ANALYSIS
Out of the 1534 browsing instances that took place during the study, 696 correspond to relevant documents and 838 correspond to irrelevant documents. Overall, we collected 440557 feature vectors, out of which 224165 are associated to relevant documents and 216392 to irrelevant documents. Our main objective was to accumulate a sufficiently rich and balanced set of affective data that would allow us to experiment with different personalisation approaches. The analysis was performed on a frame-basis.
3.1 Features
From the output of eMotion we concluded to a subset of 12 features that have directly measured values and were used to train our models. Most of these attributes have been associated in the past with important affective and cognitive processes. Even though eMotion follows the categorical approach (i.e., interprets facial expressions in terms of emotion categories) we did not employ categorical data

for the training of our models. Instead, we used the motionunits (MU's) data, which is a low-level category of features very similar to Ekman's action-units (AU's) [10]. MU's measure the intensity of an emotion indirectly, by tracking the presence and degree of changes in all facial regions associated with it. Moreover, MU's allowed us to associate the captured facial expressions with a wider range of affective and cognitive states, which are not accounted for during the meta-classification that eMotion applies.
3.2 Preprocessing
We shuffled and split the data of each participant into three subsets, two of which were used for training purposes (S1 & S2) and one for testing (S3). Each time we used an equal number of documents. We also resampled datasets S1, S2 and S3, based on the participant with the least number of instances. This resulted in three sets with approximately the same number of feature vectors, across all participants. By balancing the training and test sets we prevented over-fitting and, additionally, compensated for the originally uneven size of the datasets. Since eMotion did not pre-processes the data we had to scale them before applying any classification method, to avoid having attributes in greater numeric ranges dominating those in smaller numeric ranges.
4. MODELS
We explore the effect of personalisation on the affective models' performance. The modelling goal is to develop affective models that can predict with reasonable accuracy the topical relevance of viewed documents. We employ sensory data that derive from facial expressions as the only implicit feedback information. From the latter signals we extract a set of features, and perform discriminant analysis, using Support Vector Machines (SVM). Additional classification techniques were evaluated in [4], but proved to be less efficient. Therefore they were omitted from this study. We do not assume anything about the relationship between these features, which we consider indicative of users' affective behaviour and topical relevance. We, rather, follow a straightforward classification approach, using the ground truth that is associated with our training data.
4.1 Support Vector Machines
We used libSVM1, an implementation of SVM, to discriminate between two classes of documents: (i) relevant, and (ii) irrelevant. Our approach utilises an efficient method that can deal with a difficult, multi-dimensional classification problem. We trained our models using a radial basis function (RBF) kernel, which, based on previous work [4] that evaluated all basic SVM kernels (linear, polynomial, radial basis function, sigmoid), proved to be the optimal choice. Moreover, the RBF kernel is preferable, since it encounters less numerical difficulties and has a limited number of hyper-parameters.
To optimise the performance of our SVM model we performed a grid-search on the parameters C (cost) and  (gamma) using cross-validation, during which we tried exponentially growing sequences of C and . However, since performing a full grid-search can be time consuming, we initially used a coarse grid and then, after identifying a "good" region, we performed a finer grid search on that region. The
1http://www.csie.ntu.edu.tw/~cjlin/libsvm/

374

Task
T1 T2 T3

Easy - Difficult

M

SD

1.750

0.8563

1.6875

0.8732

2.6875

1.1383

Clear - Unclear

M

SD

1.4375

0.6292

1.3750

0.6191

1.5000

1.0954

Simple - Complex

M

SD

2.1875

1.1087

1.7500

1.0646

2.0000

1.4606

Interesting - Boring

M

SD

2.0000

1.1547

2.1333

1.1255

1.9375

1.2366

Table 2: Descriptive statistics on tasks

end-purpose was to identify the optimal set of (C,) so that every classifier we trained could achieve the best possible (tuning-wise) accuracy score on in testing data.
4.2 Personalisation
We followed two different training approaches: (i) we merged general data, gathered from many individuals, with personalised data from a single participant and trained a single SVM model, and (ii) we used general and personalised data separately, to train two different models and combined their predictions using weighted voting.
In the first approach we used a total of 19157 instances of general data, acquired from [3], in combination with 4300 instances (in three sets of 1430 instances) per participant. For every participant we originally tested the performance of the SVM model (general model) trained on the 19157 instances against S3 (the predestined test set of the participant). Then we retrained the model using the same general data merged with additional N instances of general data, or N instances of personalised data (where N or N equals 1430 feature vectors), and tested its performance against S3. Finally, we repeated the same process using N+N instances of general data, or N+N instances of personalised data. This way we were able to examine if by adding personal data we improved the performance of our model more than by adding general data.
In the second approach we examined whether predictions from two different sources (general model and personalised model) could be fused, on a decision level, to determine the topical relevance of a document. For each participant we trained a personalised SVM model using the subsets S1 and S2. A general SVM model was also trained using the same general data as in the previous method. We then used each participant's test set (S3) to acquire the predictions from both classifiers and combine their output using the following formula (each time with a different weighting scheme):

pigen · wgen + pipers · (1 - wgen) = pi

(1)

Assume pigen is the probability estimate of instance i being relevant, as given by the general model, while pipers denotes the probability of the same instance being relevant, as determined by the personalised model. We then calculate the probability pi of the instance i being relevant using Formula 1. Where wgen  [0, 1], is the weight we assign for the prediction of the general model. The prediction pi will then be transformed to a binary decision classifying instance i as either relevant or irrelevant, based on a predefined threshold value t. The probability estimates of both models were tested for different combinations of weights and threshold, using a step of 0.1.

5. RESULTS
In this section we present the experimental findings of our

study, based on 48 search sessions that were carried out by 16 subjects. Out of the many results, we are reporting those that refer to our models and present only the questionnaire data that refer to the tasks, due to limited space. We measured the performance of all models using the standard metric of accuracy. Accuracy was computed as the fraction of items in the test set for which the models' predictions were correct.
5.1 Questionnaires
A 5-point Likert scale was used in all questionnaires. Questions that ask for participants' rating on a bipolar dimension have the positive concept corresponding to the value of 1 (on a scale of 1-5) and the negative concept corresponding to the value of 5. Questions that ask for user rating on a scale of 1-5 represent in our analysis stronger perception with high scores and weaker perception with low scores. Friedman's ANOVA and Pearson's Chi-Square test were used to establish the statistical significance (p < .05) of the differences observed among the three tasks (T1: easy, T2: average, and T3: difficult). When a difference was found to be significant the Wilcoxon Signed-Ranked Test was applied to isolate the significant pair(s), through multiple pair-wise comparisons. To take an appropriate control of Type I errors the Bonferroni correction was applied, and so all effects are reported at a .016 level of significance.
Table 2 shows the means and standard deviations for participants' assessment of the performed tasks. With respect to the assessment of the difficulty level it appears that there is a trend, with T3 considered the most difficult. Friedman's ANOVA was applied to evaluate the significance of this variance. The results indicate that participants' perception of the task difficulty was significantly different (2(3, N = 16) = 9.042, p < .05). The post hoc tests show that the differences for the pairs T1 & T3 (Z = -2.434, p < .016) and T2 & T3 (Z = -2.683, p < .016) are statistically significant, but the same condition does not apply for T1 & T2.
This is further supported by participants' view of the retrieved results. The participants were asked to provide their assessments through the following questions: (i) "Overall, the results that were presented to you were: (Range: 1-5, Lower = Relevant - Higher = Irrelevant)", and (ii) "You feel satisfied with the retrieved results (Range: 1-5, Lower = Agree - Higher = Disagree) ". For the first question, the participants considered the retrieved results less relevant for T3 (M =2.8125, SD=0.9106), compared to T1 (M =2.0625, SD=0.8539) and T2 (M =2.1875, SD=0.9811). Furthermore, they were less satisfied with the retrieved results in T3 (M =2.8750, SD= 0.9574), than in T1 (M =2.0625, SD= 0.9287) or T2 (M =2.1250, SD= 0.9574). Table 2 also shows participant's assessment of the ambiguity, complexity and interest of the three tasks. Friedman's ANOVA test did not reveal a significant difference for any of the above aspects.

375

Model  Accuracy  

80.00  

70.00  

60.00  

50.00  

40.00  

30.00  

20.00  

10.00  

0.00  

1  

2  

3  

4  

5  

6  

7  

8  

9   10   11   12   13   14   15   16  

Par9cipants  

General   General+S1   General+S1+S2  

Figure 1: Results for models adapted using personalised data

5.2 Models
For each personalisation approach we present the performance of our models in terms of accuracy. The Dependent t-Test was applied, when possible, to determine if the difference between the experimental conditions is statistically significant. The baseline, which represents random choice, is set to 50%, since the class of a document can be either relevant or irrelevant.
5.2.1 Adaptation
The results of the first approach are shown in Figures 1, 2, and 3. Figure 1 illustrates the performance of three different classifiers, per participant: (i) a classifier trained exclusively on general data, (ii) a classifier trained using general data merged with the personalised dataset S1, and (iii) a classifier trained using general data merged with the datasets S1 and S2. For every participant we tested these three combinations against the corresponding S3. Only for this case, the subsets S1-S3 were not balanced. Therefore, the contribution of personalised data by each participant varied. The progression of the columns in Figure 1 suggests that, in most cases, an improvement was achieved by introducing personalised data to the training set, reaching classification rates that exceed 70%.
Figures 2 and 3 show the results of the same personalisation approach, as described above, with the exception that this time we used balanced sets of personalised data (we re-sampled the datasets S1, S2 and S3 to ensure that each participant contributed the same number of instances). This was a necessary step to allow for testing the significance of the variation introduced in the models' performance. Figure 2 shows the performance of a classifier trained using the original set of general data, merged with an additional N instances of general data, and a classifier trained using the same general set of data, merged with an additional N instances of personalised data. On average, the second classifier (M =52.74, SD=3.31) performed slightly better than the first classifier (M =51.23, SD=4.33). Therefore, we can argue that by adding N number of personalised data we achieved a slightly better performance, compared to adding the same number of general data. However, the post-hoc tests did not reveal a significant difference.
Figure 3 illustrates the performance of the two classifiers after adding N+N general data, or N+N of personalised data, to the original training set and tested against the cor-

responding test set S3, for each participant. The results show that the second classifier attained a significantly higher performance (M = 55.94, SD=6.62) than the first classifier (M =50.83, SD=4.34), t(15)=-3.848, p < .01. In this graph the enhancement of the model's performance, due to the integration of additional personalised data, is much more evident.
5.2.2 Weighted voting
The results of the second approach are presented in Figure 4. In this approach we used the general and the personalised data separately, to train two classifiers and combine their predictions using weighted voting. The graph illustrates the performance of the classifiers for different thresholds. Each line in the graph is a different weight combination, e.g., for wgen=0.0 and wpers=1.0 we see the progression of the performance, between thresholds 0.0 to 1.0. The graph indicates that, on average, the best performance was held by the classifier with combination of weights wgen=0.3 and wpers=0.7, for threshold t=0.3. This suggests that the voting scheme worked better when more emphasis was put on the personalised model. However, the contribution of the general model was equally important, to keep the classification rates optimal. When higher weights were given to the general model the performance dropped considerably, which supports further the positive effect of personalisation on the models' performance.
6. DISCUSSION & CONCLUSIONS
In this paper we explored two different approaches to personalising affective models that are capable of discriminating between two categories of documents: relevant and irrelevant. We devised an experimental setup that exposed our participants to search tasks of varying difficulty, which was achieved through the re-ranking of the return documents. This manipulation of task difficulty resulted in a much wider spectrum of affective reactions, thus making the accumulated affective data not only more authentic but also comparable to data gathered from previous studies. Our analysis also indicates that this variation was perceived by the participants, as it was found statistically significant.
For modelling relevance we extracted from facial expression data a set of features and classified them using Support Vector Machines. In the first approach we adapted a general model to the behavioural characteristics of a number

376

Model  Accuracy   Model  Accuracy  

75.00  

General+N   General+N*  

70.00  

65.00  

60.00  

55.00  

50.00  

45.00  

40.00  

35.00  

1  

2  

3  

4  

5  

6  

7  

8  

9   10   11   12   13   14   15   16  

Par9cipants  

Figure 2: Performance of general model after adding N general or N personalised data

of participants, using personalised data, and established its performance against a model trained exclusively on general data. In the second approach we trained a general and a personalised model separately and combined their predictions using a combination of weighting schemes. We, finally, examined the effect of personalisation on the model's performance and tested it's significance.
One facet of affect recognition is developed here for the first time: the personalisation of affective models, trained on facial expression data, for the prediction of topical relevance. Our experimental evidence supports our first hypothesis, namely that by adapting a general affective model to a specific user we introduce a noticeable improvement in its discriminating ability. Our best performing model attained an accuracy of 72.52%, which is substantially better than the baseline or any other performance presented in [4].This difference was found to be highly statistically significant, which is an encouraging finding.
Using weighted voting we provided additional evidence in favour of accounting for the behavioural differences of users. Our analysis indicates that by fusing, on a decision level, the output of both general and personalised classifiers (with the emphasis on the latter) we can attain the optimal performance. Regarding our second hypothesis, we cannot suggest which approach was more effective, since our findings did not favour one method over the other. Clearly, there is more than one alternative to personalising user models, especially those built on affective data. Additional work is necessary before we determine if these two approaches perform equally well under different experimental conditions.
Finally, the evidence accumulated from both approaches suggests that personalisation works better for some users, than others. We speculate that the variation in the models' performance might be correlated with the ability of the participants to behave naturally and be expressive in a laboratory setting, as in their home environment. However, the choice of setting was a necessity, guided by the need to allow for comparability between data from previous studies.
In conclusion, we feel that the quality of our results is good enough to indicate that personalisation of affective feedback

General+N+N   General+N*+N*  

75.00  

70.00  

65.00  

60.00  

55.00  

50.00  

45.00  

40.00  

35.00  

1  

2  

3  

4  

5  

6  

7  

8  

9   10   11   12   13   14   15   16  

Par9cipants  

Figure 3: Performance of general model after adding N+N general or N+N personalised data
is a promising area of research and that it can potentially influence other aspects of the search process, such as relevance feedback, ranking, recommendation techniques, as well as offer new insight to the semantic gap problem. Finally, since there are no other systems available for direct comparison, our system holds the best accuracy achieved, so far, in the deduction of topical relevance using affective information.
7. REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 19­26, New York, NY, USA, 2006. ACM.
[2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 3­10, New York, NY, USA, 2006. ACM.
[3] I. Arapakis, J. M. Jose, and P. D. Gray. Affective feedback: an investigation into the role of emotions in the information seeking process. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 395­402, New York, NY, USA, 2008. ACM.
[4] I. Arapakis, I. Konstas, and M. Jose, J. Using facial expressions and peripheral physiological signals as implicit indicators of topical relevance. In Proceedings of the seventeen ACM international conference on Multimedia, pages 461­470, NY, USA, 2009. ACM.
[5] I. Arapakis, Y. Moshfeghi, H. Joho, R. Ren, D. Hannah, and J. M. Jose. Enriching user profiling with affective features for the improvement of a multimodal recommender system. In Proceeding of the ACM International Conference on Image and Video Retrieval, pages 1­8, NY, USA, 2009. ACM.
[6] R. Badi, S. Bae, J. M. Moore, K. Meintanis, A. Zacchi, H. Hsieh, F. Shipman, and C. C. Marshall.

377

-./01%23345637%

!"'&%

!"'$%

!"!%

!"(%

!"''%

!"*%

!")%

!"')%

!"#%

!"'%

!"'(%

!"+%

!"$%

!"#&%

!",%

!"&%

!"#$%

!%

!"(%

!"*%

!")%

!"#%

!"'%

!"+%

!"$%

!",%

!"&%

(%

("!%

8950:9.1/%

Figure 4: Results for the weighted voting (for different combinations of weights and threshold)

Recognizing user interest and document value from reading and organizing activities in document triage. In Proceedings of the 11th international conference on Intelligent user interfaces, pages 218­225, New York, NY, USA, 2006. ACM.
[7] N. J. Belkin. Anomalous states of knowledge as a basis for information retrieval. Canadian Journal of Information Science, 5:133­143, 1980.
[8] P. Borlund. Experimental components for the evaluation of interactive information retrieval systems. Journal of Documentation, 56(1):71­90, 2000.
[9] M. Daoud, L. Tamine-Lechani, and M. Boughanem. Learning user interests for a session-based personalized search. In Proceedings of the second international symposium on Information interaction in context, pages 57­64, New York, NY, USA, 2008. ACM.
[10] P. Ekman. Facial Expressions, chapter 16, pages 301­320. The Handbook of Cognition and Emotion. U.K.: John Wiley & Sons, Ltd, 1999.
[11] P. Ekman. Emotions Revealed: Recognizing Faces and Feelings to Improve Communication and Emotional Life. Times Books, New York, 2003.
[12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 154­161, New York, NY, USA, 2005. ACM.
[13] J. Koenemann and N. J. Belkin. A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 205­212, New York, NY, USA, 1996. ACM.
[14] F. Liu, C. Yu, and W. Meng. Personalized web search by mapping user queries to categories. In Proceedings of the eleventh international conference on Information and knowledge management, pages 558­565, New York, NY, USA, 2002. ACM.
[15] M. Morita and Y. Shinoda. Information filtering based on user behavior analysis and best match text retrieval. In Proceedings of the 17th annual international ACM SIGIR conference on Research and

development in information retrieval, pages 272­281, NY, USA, 1994. Springer-Verlag New York, Inc.
[16] D. W. Oard and J. Kim. Modeling information content using observable behavior, 2001.
[17] M. Pantic and L. Rothkrantz. Expert system for automatic analysis of facial expression. Image and Vision Computing Journal, 18(11):881­905, August 2000.
[18] M. Pantic and L. J. M. Rothkrantz. Toward an affect-sensitive multimodal human-computer interaction. Proceedings of the IEEE, 91(9):1370­1390, Sept. 2003.
[19] M. Pantic, N. Sebe, C. J. F., and T. Huang. Affective multimodal human-computer interaction. In Proceedings of the 13th annual ACM international conference on Multimedia, pages 669­676, New York, NY, USA, 2005. ACM.
[20] K. Puolam¨aki, J. Saloj¨arvi, E. Savia, J. Simola, and S. Kaski. Combining eye movements and collaborative filtering for proactive information retrieval. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 146­153, New York, NY, USA, 2005. ACM.
[21] Y. Rui and T. Huang. Optimizing learning in image retrieval. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 236­243, 2000.
[22] N. Sebe, M. S. Lew, Y. Sun, I. Cohen, T. Gevers, and T. S. Huang. Authentic facial expression analysis. Image Vision Comput., 25(12):1856­1863, 2007.
[23] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing search via automated analysis of interests and activities. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 449­456, New York, NY, USA, 2005. ACM.
[24] R. Valenti, N. Sebe, and T. Gevers. Facial expression recognition: A fully integrated approach. 14th International Conference on Image Analysis and Processing Workshops, pages 125­130, 2007.

378

Understanding Web Browsing Behaviors through Weibull Analysis of Dwell Time

Chao Liu
Microsoft Research One Microsoft Way Redmond, WA 98052
chaoliu@microsoft.com

Ryen W. White
Microsoft Research One Microsoft Way Redmond, WA 98052
ryenw@microsoft.com

Susan Dumais
Microsoft Research One Microsoft Way Redmond, WA 98052
sdumais@microsoft.com

ABSTRACT
Dwell time on Web pages has been extensively used for various information retrieval tasks. However, some basic yet important questions have not been sufficiently addressed, e.g., what distribution is appropriate to model the distribution of dwell times on a Web page, and furthermore, what the distribution tells us about the underlying browsing behaviors. In this paper, we draw an analogy between abandoning a page during Web browsing and a system failure in reliability analysis, and propose to model the dwell time using the Weibull distribution. Using this distribution provides better goodness-of-fit to real world data, and it uncovers some interesting patterns of user browsing behaviors not previously reported. For example, our analysis reveals that Web browsing in general exhibits a significant "negative aging" phenomenon, which means that some initial screening has to be passed before a page is examined in detail, giving rise to the browsing behavior that we call "screen-and-glean." In addition, we demonstrate that dwell time distributions can be reasonably predicted purely based on low-level page features, which broadens the possible applications of this study to situations where log data may be unavailable.
Categories and Subject Descriptors
H.1.2 [Information Systems]: Models and Principles ­ User/Machine Systems
General Terms
Algorithms, Measurement, Human Factors
Keywords
Weibull analysis, User behaviors, Web browsing, Dwell time
1. INTRODUCTION
Real-world information retrieval (IR) heavily relies on effective usage of implicit feedback, which comes in various
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

forms such as document clickthrough, viewing, scrolling, and bookmarking. Many researchers have studied the correlations between implicit feedback and document relevance (e.g., [6, 21, 5, 10]), and revealed that document dwell time (i.e., the length of time a user spends on a document), is generally the most significant indicator of document relevance besides clickthrough, although the extent of the relationship may vary depending on the information seeking task [14, 15]. Because of the correlation between dwell time and document relevance, dwell time has been successfully used in various applications, such as learning to rank [2, 3], query expansion [5], and inferring query-independent page importance [19]. Specifically, Agichtein et al. [2, 3] demonstrate that user browsing features, a major component of which is Web page dwell time, significantly improve the retrieval performance of a competitive search engine, even with the presence of other important features such as BM25 and search-result clickthrough. Although post-query browsing is intuitively more relevant to IR than general browsing, general browsing is still an important component in information seeking [20, 25]. Indeed, some of aforementioned studies (e.g., [19]) and real-world search engines also leverage the general browsing activities for improved efficacy and coverage.
Although dwell time has been extensively studied, some important questions have not been sufficiently addressed. For example, what distribution is appropriate to model the dwell time on a Web page1 across all visits, i.e., what does
(  ) look like? Furthermore, how does the distribution depend on the features of ? And finally, what does the distribution tell us about users' general browsing behaviors? These questions are not only interesting in themselves, but are also useful for various IR applications, as we now explain.
First, accurately modeling (  ) would enable the construction of generative models involving dwell time for Web text analysis. For example, when dwell time is properly modeled, topic discovery can be guided by considering both
(  ) and content. Second, (  ) can readily answer questions such as "what is the probability that a user will stay longer than 1 on the page?" (answer: (  1 )) or "what is the expected remaining time that a user will spend on a page that he has dwelled on for 1?" (answer:
(   1, )). Answers to such questions could help publishers optimize advertising and content placement. Third, understanding (  ) would help us gain insights into user browsing behaviors that can help inform the design of search
1We use page, Web page, URL and document interchangeably in this paper

379

and advertising technologies, as we will explain later in the paper.
Precise modeling of dwell time is not straightforward since duration on a Web page depends on many factors, some of which may not even be fully captured by log data (e.g., the mood of the user). In addition, the distribution family may vary with different information seeking tasks in different settings (e.g., time of day). As the first step towards precise modeling of dwell time, we choose to model the overall distribution of user dwells on each Web page, which we believe will help us better understand the dwell time distributions in general across all users.
In this paper, we draw an analogy between abandoning a page during Web browsing and a system failure in reliability analysis, and use Weibull analysis techniques which are commonly used in reliability engineering [1] to characterize general browsing behaviors. Furthermore, we demonstrate that it is possible to predict the dwell time distribution based on page-level features. We make the following contributions in this study:
 Weibull analysis of Web dwell time data: To the best of our knowledge, this is the first time an analogy has been drawn between abandoning a Webpage and a system failure, which leads to a principled way of analyzing dwell time data. The same or similar analogies can be made for other kinds of temporal data on the Web (e.g., time-to-first-click on search result pages and session length).
 Discoveries about user browsing behaviors: Our analysis leads to some interesting new insights regarding users' Web browsing behaviors. Specifically, we find that Web browsing exhibits a significant "negative aging" phenomenon (i.e., the rate of Web page abandonment decreases over time), and that this effect is stronger for less entertaining pages. These discoveries, together with the application of Weibull analysis to a new domain, enhance our understanding of user browsing behaviors.
 Predicting dwell time distribution: We demonstrate that the dwell time distribution (in Weibull parametric form) can be effectively predicted from low-level page features. Not only does this broaden the applicability of dwell time data, but it also reveals what page features correlate with the dwell time distribution.
The remainder of this paper is organized as follows. We first discuss the related work in Section 2, and then examine the goodness-of-fit of the Weibull distribution in Section 3. We present the Weibull analysis results in Section 4, and elaborate on the predictive model in Section 5. With extensions and future work discussed in Section 6, Section 7 concludes this study.
2. RELATED WORK
This paper is related to work on implicit feedback within IR. Research on implicit feedback has sought to address the high cost of soliciting explicit feedback from users by unobtrusively observing their natural interactions and building models for activities such as query expansion and user profiling [17]. Although implicit feedback may be less accurate than explicit feedback [22], it is available in significantly greater quantity than explicit feedback. Implicit

measures include document retention (e.g., printing, saving, bookmarking) and document interaction (e.g., viewing, scrolling, dwell time) [21, 6, 14]. Morita and Shinoda [21] measured the relationship between dwell time, saving, following-up and copying of a document and users' explicit ratings, and showed that there was a relationship between dwell time and interest, but no relationship between interest and any other measures. Claypool et al. [6] examined mouse clicks, scrolling, dwell time, and requested explicit ratings, and found that dwell time and the amount of mouse scrolling had a strong positive correlation with explicit ratings. Studies by Kelly and Belkin [14, 15] further found that special attention is needed to interpret dwell time as relevance because of the implications of different tasks. On the application side, besides being incorporated into learning to rank for Web IR [2, 3, 19], dwell time is also widely used in other information seeking tasks (e.g., [16, 5]). In this paper, we do not focus on particular search and retrieval tasks as done in most previous work, but instead try to model the dwell time distribution across all users engaged in general Web browsing.
This work is also related to online user behavior modeling, which has been attracting significant attention in recent years (e.g., [24, 25, 9]). There are two main complementary approaches to uncovering user behavior models: one based on controlled user studies (e.g., [12, 13, 24]), and the other based on large-scale log analysis (e.g., [25, 9, 26, 4, 19]). This work falls into the second category, and is mostly related to BrowseRank [19], which tries to infer a query-independent score for each page from page dwell time in general browsing. In particular, BrowseRank assumes (mainly for tractability) that the dwell time for a given page follows an exponential distribution. In this paper, we show that the Weibull distribution is more versatile than the exponential distribution used in [19]; it better fits the real-world dwell time data and provides insights on browsing behaviors.
This work also relates to research on Weibull analysis, which has been extensively and successfully applied in nearly all scientific disciplines, such as biological, demographical, reliability sciences (c.f. [1, 23]). This paper therefore adds a new application area to the rich literature of Weibull analysis, and meanwhile introduces a disciplined method for analyzing temporal data on the Web, e.g., time-to-first-click on search result pages and the session length in time, in addition to the page dwell time as studied here.
3. MODEL FITTING AND COMPARISON
In this section, we fit the dwell time data with exponential and Weibull distributions (Section 3.2), and compare their goodness-of-fit in Section 3.3. The data used for this comparison and throughout the paper are discussed in Section 3.1.
3.1 Experimental Data
We collected two-weeks of log data from a popular Web browser plug-in operating in the English (US) market, which records the searches and browsed pages for opted-in users. The log data is organized in sessions, each of which is defined as a series of Web page visits that extends until either the browser is closed or a period of 30-minutes of inactivity. Based on the visit time of consecutive page visits within sessions, the dwell time of each page visit is calculated. We do this for all pages apart from the last page in the session,

380

2  = 1.0, k = 0.5

 = 1.0, k = 1.0

 = 1.0, k = 3.0

1.5

 = 2.0, k = 3.0

1

PDF

0.5

0

0

1

2

3

4

t

Figure 1: Example Weibull Distributions

which is then discarded from the analysis because we do not have a succeeding page visit from which to calculate its dwell time. For accurate parameter estimation, only pages with 10,000 or more visits are used. This results in a set of 205,873 URLs, each of which is accompanied by at least 10,000 dwell time observations.

3.2 Model Fitting with Maximum Likelihood
The probability density function (PDF) of Weibull distribution is given by

( ) -1 {

}

( , )=

-( )

 0, (1)

with (  , ) = (1+1/ ). and are the scale and shape parameters, respectively. Figure 1 plots the PDFs of some typical parameterizations, which exemplify the versatility of the Weibull distribution, and how parameters and affect the scale and shape of the distribution, respectively.
When = 1, the Weibull distribution reduces to the exponential distribution with PDF

( )= 1

{} -

0

and (  ) = . Given a sample of observed dwell time for a page, { } =1,
we choose to fit the model through maximum likelihood estimation (MLE), and denote the fitted model with Weibull by and that with exponential by . While fitting is as simple as



^ = =1 ,

(2)

fitting

is nontrivial because the MLE of ( , ) has no

closed form. Instead, we need to use an iterative approach

proposed in [7]. For completeness, we briefly outline the

estimation.

Given the likelihood function as

( 1, 2,    ,  , ) = 
=1
we set the partial derivative w.r.t.

( ) -1 -( ) , and to 0, i.e.,

( 

)=

-

( )+ ( )-( )

=1

=1

( ) = 0, (3)

( 

) =-

+

+1 = 0.

(4)

=1

Eqn 4 gives



= =1 ,

(5)

which, once plugged into Eqn 3, renders



+ ( )-

=1


( ) = 0,

(6)

=1

=1

which only involves and can be solved through NewtonRaphson iterations. Specifically, let

( )=
=1



+

=1


=1

( )- 
=1

( ),

then

 ( ) = =1

( ) (

+

=1

=1

( )) - 
=1

2( ).

Then the MLE of , denoted by ^, is obtained by

^( +1)  ^( ) -

( ( )) ( ( ))

= 1, 2,   

We terminate the iterations when the change of is less than 10-6. Once ^ is obtained, ^ immediately follows from Eqn 5. We try initial value (1) = 0.1, 1, 10, and choose the final (^, ^) with the largest likelihood. Readers interested in a thorough treatment of parameter estimations (besides MLE) for Weibull distributions are referred to [23]. The above estimation can be trivially parallelized across URLs for distributed computing, which affords Web-scale dwell time data analysis.

3.3 Goodness-of-fit Comparison
We use the log-likelihood (LL) and the Kolmogorov-Smirnov distance (KS-distance) [8] to evaluate the goodness-of-fit of
and . In general, a better fit corresponds to a bigger LL and/or a smaller KS-distance.
The KS-distance as defined below

( , ) =

 ( ) - ( )

(7)

is the test statistic for Kolmogorov goodness-of-fit test, which

tests whether a random sample 1, 2,    , , whose em-

pirical cumulative distribution function (eCDF) is described

by ( ), comes from a completely specified hypothesis dis-

tribution whose cumulative distribution function (CDF) is

given by ( ).

Because the exponential distribution is a special case of

the Weibull distribution,

is guaranteed to be no worse

than if fitted and evaluated on the same dataset. To be

fair, the dwell time observations for each page are randomly

split into training and testing portions with a ratio of 4:1.

In other words, the data are split within the dwell time

observations for each page rather than across pages.

and are fit using the training portion and evaluated on

the testing portion for each page. LL and KS-distance on

the testing portion are used to determine which model wins.

The number of pages on which a model wins are listed in

Table 1, which clearly shows the superiority of

to .

Specifically, outperforms on more than 85% of the

pages in terms of both metrics, and a sign test for each result

gives a p-value that is very close to zero.

381

Wins Wins

Log-Likelihood 176,242 29,631

KS-distance 178,892 26,981

Table 1: Comparison of Goodness-of-Fit

4. WEIBULL ANALYSIS OF DWELL TIME
In this section, we discuss the implications of a fitted Weibull distribution for understanding user browsing behaviors (Section 4.2). We first provide a brief introduction to Weibull analysis in Section 4.1.
4.1 A Primer on Weibull Analysis
Weibull analysis dates back to 1937 when Waloddi Weibull invented the Weibull distribution. It has been successfully applied to nearly all scientific disciplines, such as biological, environmental, health, physical and social sciences, but, to the best of our knowledge, not in the Web data analysis domain. By fitting time-to-failure data to Weibull distributions, Weibull analysis enables principled failure interpretation, risk assessment, failure forecasting, and planning of corrective actions. Since a full introduction to Weibull analysis is neither realistic nor necessary here, we will highlight those aspects that pertain to our analysis and referring interested readers to [23] and [1] for a thorough treatment of Weibull analysis and applications.
The most popular characteristic function of a Weibull distribution is the Hazard function, which is defined as
 ( ) = lim (  < +   ) .
0
If an item that has survived time is called an -survivor, the hazard function gives the probability that an -survivor fails immediately at time , and it is also known as the instantaneous failure rate or the hazard rate. Usually, the hazard rate is interpreted as the amount of risk associated with an -survivor at time in reliability study and as the force of mortality in demography and actuarial science.
The hazard function of a Weibull distribution is given by

( )=

-1 ,

(8)

whose first-order derivative is

 ( ) =

- 1(

) -2 .

(9)

When  (0, 1), the first-order derivative,  ( ), is less than 0, so the hazard rate monotonically decreases w.r.t. . This phenomenon is often termed "negative aging," which means that the longer one survives, the less likely it would fail instantaneously. Since the hazard rate is high at the onset, it is also called the "infant mortality" phenomenon. In abstract terms, negative aging means that a screening is taken place at the early stage so that weak items with hidden defects are sorted out while leaving robust and healthy ones in the population, or as Lehman [18] suggests "So once the obstacle of early youth have been hurdled, life can continue almost indefinitely." We will reveal the implication of negative aging for Web browsing in Section 4.2.
In contrast, > 1 corresponds to the "positive aging" phenomenon, which means that the longer one survives, the more likely it fails instantaneously. Finally, = 1 results

2.5  Negative Aging
2
 Positive Aging 1.5

 = 1.0, k = 0.5  = 1.0, k = 0.8  = 1.0, k = 1.0  = 1.0, k = 1.5

No Aging 
1

Hazard Rate

0.5

Cumulative Probability (CDF) Cumulative Probability (CDF)

0

0

1

2

3

4

5

t

Figure 2: Example Weibull Hazard Functions

1 0.8 (70, 0.8) 0.6 0.4

1 
(1, 0.985) 0.8
0.6
0.4

0.2

0.2

0

0

0

200 400 600 800 1000 1200

0

0.5

1

1.5

2



k

(a) eCDF of

(b) eCDF of

Figure 3: Distributions of the Fitted and Values
in a constant hazard function, indicating a constant failure rate, which is the physical model of the exponential distribution.
The hazard functions of some example Weibull distributions are plotted in Figure 2, which illustrates different types of aging. Note that when  (0, 1), we see negative aging, or a decrease in the failure rate over time. In the context of Web browsing this would mean a decrease in Web page abandonment rate over time. Conversely, when > 0, we see positive aging, or an increase in the failure rate over time.
4.2 Weibull Analysis on Dwell Time
Using the data set as described in Section 3.1, we now examine the fitted and values on the training portion for each page. Figure 3 plots the empirical cumulative distribution function (eCDF) for the fitted and values. Figure 3(a) shows the eCDF for the scale parameter of the estimated dwell time distribution. We see that the dwell time is no more than 70 seconds on 80% of the 205,873 pages, which gives us an overall estimate of the dwell time scales across pages. Figure 3(b) shows the eCDF for the shape parameter, . We see that is less than 1 on 98.5% pages. Recalling that < 1 indicates a negative aging effect. Thus, Figure 3(b) suggests that Web browsing exhibits a strong "negative aging" phenomenon, that is, some "screening" is carried out at the early stage of browsing a page, and the rate of subsequent abandonment decreases over time.
This discovery agrees well with the intuition about how a user browses a page: upon landing on a Web page, the user would first skim through the page, assessing the potential benefit of further reading, before delving into it and gleaning needed information. During the screening, the probability of abandoning the page is high (i.e., a high hazard rate), but once the page survives the screening (e.g., is regarded

382

0.35 Pr(Category | k < 1)

0.3

Pr(Category | k > 1)

0.25

0.2

0.15

0.1

0.05

En0tertainmeCntomputerRsecreaRtieolnationshipsFinancial

Travel Society ScienceEducation Vehicles

(a) (

 < 1) vs. (

 > 1)

k

1.5

1

0.5

0EducationFinancial SciencCeomputers SocieRtyecreEatniotenrtainment

TRraevlealtionships Vehicles

(b) ( 

)

Figure 4: Relationship between Categories and Aging Effect as Characterized by

as useful by the user), the abandonment rate decreases. We

therefore suspect that users do in general adopt a "screen-

and-glean" type browsing behavior, which gives rise to the

dwell time distribution showing the observed negative aging

effect.

We now examine whether and how the "negative aging"

phenomenon relates to the topic of the page, as defined by

category membership, i.e., do people impose equal screen-

ing on pages of different categories? For this purpose, we

employed a proprietary document classifier that assigned

each page into one of 23 top-level categories in a taxonomy similar to dmoz2. The categorization succeeded on 136,395

pages. We then analyzed how category information relates

to the aging effect from two complementary aspects: first,

we compare (

 < 1) with (

 > 1),

and second, we examine ( 

) for different cate-

gories. In order to have sufficient data in each category, only

the top-10 categories were retained, which included 106,169

(77.8%) pages.

Figure 4(a) compares the category distributions for Web

pages with < 1 and > 1. We show the category distri-

butions for each of these two types of pages. We see that

Entertainment, Recreation, Relationships, Travel and Vehi-

cles have a proportionally greater presence when > 1 than

when < 1 (recall the sets of pages with > 1 and < 1

are highly imbalanced). Thus pages exhibiting positive ag-

ing ( > 1) are more likely to fall into these categories,

which we can characterize as more entertaining, than those

showing negative aging effect. Conversely, we see that the

presence of Computers and Education is stronger in < 1

than > 1, indicating that people are more likely to screen

pages in these two categories before examining them in more

details. This observation leads to a hypothesis that nega-

tive aging is more common on less-entertaining pages than

on fun pages, which in turn suggests that people tend to

screen less-entertaining pages more harshly.

Figure 4(b) shows boxplots of ( 

) for the 10

categories. The line in the middle of each box is the median

2http://www.dmoz.org/

of the data, and the lower and upper lines of the box represent the 25  and 75  percentiles of the data, respectively. The categories are ordered in ascending order of the median values from left to right, which median value of 0.6506 for Education and a median value of 0.7979 for Vehicles. Again, we observe that less-entertaining categories appear on the left of the figure, supporting the hypothesis that lessentertaining pages may be more harshly screened.
5. PREDICTING DWELL TIME
DISTRIBUTION
In this section, we investigate the feasibility of predicting dwell time distribution from page-level features. A successful prediction from page features will not only enable thirdparties without access to browsing logs to use dwell time information, but will also provide us with an opportunity to identify page features that are most related to dwell time distributions. We describe the experimental setup and page features in Section 5.1, report on the prediction results in Section 5.2, and inspect the learned model in Section 5.3.
5.1 Experimental Setup
We randomly sampled 5000 pages from the set of pages whose test KS-distance is less than 0.05 from the experiments in Section 3.3. By choosing pages with a high goodness-of-fit to the Weibull distribution (small KSdistance), we can provide good training examples to the classifier. For each sampled page, the and values fitted on the training portion of data are taken as the learning labels. In order to extract page features, we crawled these pages using a dynamic crawler, which employs an Internet Explorer object to execute all dynamic components (e.g., flash, javascript, etc.) and download the final rendered page. Pages containing the term "login" are excluded because these login pages are usually automatically loaded through a timeout redirection. This, together with failed crawling, gave us a set of 4,771 pages, which are randomly partitioned into training, validation and testing sets with a ratio of 7:1:2.
Since we want to inspect the learned model we use Mul-

383

Feature
PageSize PageHeight PageWidth DownloadCount DownloadTime SecDownloadCount SecDownloadTime ParseTime RenderTime

Description
Size (in bytes) of the rendered page Height of the rendered page Width of the rendered page Number of total downloaded URLs Time to download all URLs Number of secondary URLs Time to download secondary URLs Time to parse all URLs Time to layout and render the page

Table 2: Details of Dynamic Features
tiple Additive Regression Trees (MART) [11], which provide good interpretability and high accuracy. We used the validation set to locate the optimal parameters, which include the maximum number ( ) of leaf nodes of the base learner tree and the shrinkage parameter ( ). We varied
 {2, 3, 4, 6, 11, 21, 25},  {1, 2-1, 2-2,    , 2-6}, and recorded the number ( ) of iterations that achieved the minimum error. The tuple ( , , ) that achieved the lowest error on the validation set was used in the final testing phase.
We constructed the following three sets of features, ranked in ascending order based on their closeness to what users would actually experience when viewing that page in a Web browser:
 HtmlTag: The frequency of each of the 93 HTML tags (obtained from http://www.quackit.com/html/ tags/) is taken as an independent feature, comprising the first set. These features represent the underlying elements that are determinants of page formatting and layout but are not visible to users.
 Content: We leverage the "6of12 list" of the English words, which contains 32,153 most commonly used English words that "approximates the common core of the vocabulary of American English."3 The top-1000 most frequent terms that appear in the training set of pages, together with one more dimension about the document length, are taken as the second set of features. They correspond to the most frequent words users would see. The value of each feature, except the document length, is the word frequency in each page.
 Dynamic: We also recorded nine measures during the dynamic crawling of each page. The dynamic crawler first downloads the backbone page, parses it, downloads any secondary URLs (e.g., javascript, flash, image, etc.) if any, calculates the page layout, and finally renders the page. The nine features based on these measures are listed in Table 2. Because the crawler executes the scripts and renders the page, these features are meant to closely estimate users' browsing experience with the page.
We intentionally chose not to include advanced features such as PageRank, number of inlinks and any log-based features in the feature set, because these features are generally not available to researchers outside search engine companies. By restricting to page-level features, anyone can crawl the page, construct the features, reproduce the result, and more importantly, utilize the predictive model to asses dwell time for any pages that can be downloaded.
3http://wordlist.sourceforge.net/

5.2 Prediction Results

In order to determine how different sets of features in-

teract, we tested seven feature configurations as listed in

Table 3. Also listed in the table are the optimal parameters

determined by the validation set. In particular, the MART

parameters for predicting and are denoted by () and

(), respectively.

Log-likelihood (LL) and KS-distance are again used as the

evaluation metrics. For each test page, we evaluate the LL

and KS-distance on the test portion data with the predicted

and values, and compare it with the baseline model, which returns the mean value (¯, ¯) across all training pages,

which resembles, and is stronger than, the exponential model

(c.f. Eq. 2).

Results are presented in Table 3, in which "Predict Win"

means that the predictive model achieves a higher LL (or a

smaller KS-distance) than the baseline model. As the two

metrics give very similar results, we will focus on the result

based on LL in the following.

First, we see that the prediction model outperforms the

baseline method on all seven configurations with statisti-

cal significance. Sign tests for 0 :



all

return -values that are very close to zero for the seven con-

figurations. This result shows that low-level page features

do carry some prediction power that can be leveraged for

effective dwell time prediction.

Second, HtmlTag is as effective as Dynamic when used in-

dividually, and when combined, they bring further improve-

ment. This observation indicates that the nine Dynamic fea-

tures are as predictive as the 93 HtmlTag features, and their

prediction power is complementary. Conversely, Content in

itself outperforms Content+Dynamic, and adding HtmlTag

to Content only provides some marginal improvement.

Finally, the best performance is actually achieved by

Content+Dynamic. This is reasonable in that Dynamic repre-

sents what users would experience immediately after clicking

through to a page while Content corresponds to what con-

tent users would see once the page is loaded. Note that

adding HtmlTag does not provide much benefit. Given the

promising results from Content+Dynamic and the fact that

only the top-1,000 frequent words are used in Content, we ex-

pect further improvements from better feature engineering,

for example, by choosing words with high inverse document

frequencies rather than simply the most frequent ones, or by

including the number of graphics or tables in the page. We

will explore how to fully utilize the content together with

other kinds of features in future work. For now, let us in-

spect the learned models to understand what page features

are the most useful for dwell time prediction.

5.3 Feature Importance
By virtue of the interpretability of MART, we could estimate the importance of each feature and sort them in descending order of the estimated importance. Figure 5 depicts the six most important features for predicting and respectively under each configuration. The figure for "HtmlTag+Content" is dropped due to space constraints.
Figure 5(a) shows that Html tags about scripts and links are the most important ("<!--" is for comments in Html). In Figure 5(b), it is unsurprising to see that the document length is the most relevant feature, followed by words related to pornography, games and news. This looks reasonable as the dwell times for those topics are likely very different, since

384

Feature Rank

Features
HtmlTag Content Dynamic HtmlTag+Content HtmlTag+Dynamic Content+Dynamic HtmlTag+Content+Dynamic

Training & Validation

( ) () ( ) ( ) ()

25 2-6 113 25 2-5 67 25 2-6 124 25 2-6 126 25 2-5 65 25 2-6 123 25 2-6 133

25 2-4

25 2-3

6

2-2

21 2-3

25 2-3

25 2-4

21 2-4

Test by Log-Likelihood

Test by KS-Distance

( ) Predict Win Baseline Win Predict Win Baseline Win

244

654

301

159

702

253

186

653

302

684

271

727

228

685

270

199

706

249

120

669

286

724

231

701

254

195

724

231

727

228

198

717

238

725

230

Table 3: Prediction Efficacy with Different Feature Configurations

Feature Relative Weight

0

0.5

1

1.5

1

<!-- <a

Feature Relative Weight

0

0.5

1

1.5

1

DocLength DocLength

Feature Relative Weight

0

0.5

1

1.5

1

PageHeight ParseTime

2

<s <script

2

"sex" "online"

2

PageSize PageSize

Feature Rank

Feature Rank

3

<script <s

3

"game" "download"

3

PageWidth PageHeight

4

<meta <li

4

"porn" "game"

4

ParseTime DownloadCount

5

<link <meta

5

"latest" "photo"

5

DownloadCount PageWidth

6

<div <div

Predicting 

6

"follow" "news"

Predicting 

6

DownloadTime DownloadTime

Predicting 

Predicting k

Predicting k

Predicting k

(a) HtmlTag

(b) Content

(c) Dynamic

Feature Rank

Feature Relative Weight

0

0.5

1

1.5

0

1

PageSize PageSize

1

2

PageWidth ParseTime

2

Feature Rank

3

PageHeight PageHeight

3

4

<!-- PageWidth

4

5

ParseTime DownloadTime

5

6

<meta <s

Predicting 

6

Predicting k

(d) HtmlTag+Dynamic

Feature Relative Weight

0.5

1

1.5

PageHeight PageSize

PageSize PageHeight

PageWidth PageWidth

DocLength DocLength

"sex" ParseTime

"game" DownloadCount

Predicting  Predicting k

(e) Content+Dynamic

Feature Rank

Feature Relative Weight

0

0.5

1

1.5

1

PageHeight PageSize

2

PageSize PageHeight

3

"porn" PageWidth

4

"game" DocLength

5

PageWidth ParseTime

6

DocLength DownloadCount

Predicting 

Predicting k

(f) HtmlTag+Content+Dynamic

Figure 5: Feature Importance in Different Feature Configurations

users may interact with pages on these topics in different ways. Similarly, in Figure 5(c) we see that the height of the rendered page is the top feature for , followed by the page size and width. Interestingly, the time to parse the page is the most relevant feature for predicting in Figure 5(c). This may suggest when parsing takes a comparably long time, the page will have a lower chance to survive users' screening. Finally, for the remaining three figures involving feature combinations, we see that Dynamic, although only comprising nine features, always appears near the top. This confirms our belief that the nine dynamic features are strong predictors; but because of the limited number of features, complementary support from Content is necessary for the best performance.
6. DISCUSSION AND FUTURE WORK
This paper presents the first step in Weibull analysis of Web page dwell time data, which can be extended in both

breadth and depth. In breadth, there are many characteristic functions/quantities for Weibull analysis besides the hazard function, e.g., the cumulative hazard rate function and the mean residual life, each of which has a natural correspondence to interesting aspects of Web browsing. In depth, it is interesting to investigate how sophisticated models (e.g., mixture of Weibulls) would bring better goodness-of-fit and more insights into understanding user browsing behaviors.
The predictive models as presented here demonstrate the possibility of predicting Web page dwell time distributions. While better feature engineering and algorithm improvements would likely further improve performance, the current approach has an inherent shortcoming: it predicts and separately whereas it is their combination that determines the goodness-of-fit of the predicted model. So instead of predicting and separately, a more principled approach could be to optimize the likelihood directly, which would Likely provide a much better goodness-of-fit.

385

The Weibull analysis in this paper reveals some implications for understanding the browsing behaviors of all users. Alternatively, the user population can be partitioned along explicit dimensions such as time-of-day and geographical locations or implicit dimensions such as user intent and domain expertise estimates. For the latter, we can partition the dwell time based on how users reach the page, e.g., through a search clickthrough, an advertisement clickthrough, or a link from a general Web page. In this way, we would gain more detailed understanding of user browsing dwell time in different scenarios.
7. CONCLUSION
This paper has drawn an analogy between abandoning a browsed page and the failure of a system, and presented the first Weibull analysis on Web page dwell time data. We found that general Web surfing exhibits a significant "negative aging" phenomenon, suggesting that users adopt a "screen-and-glean" browsing behavior where they vet the page prior to more detailed examination. This study brings a new approach to analyzing implicit feedback involving dwell time, complementing previously conducted user studies in that area. We have proposed some directions for building more sophisticated dwell time models and presented some implications for understanding user browsing behavior. Future work will build on our application of Weibull analysis, as well as the numerous successes of it in other application domains, to improve search and advertising.
8. ACKNOWLEDGEMENTS
The authors would like to thank Yutaka Suzue for the dynamic crawler, Krysta Svore and Qiang Wu for the MART implementation, Wen-tau Yih and Alice Zheng for the discussion on analyzing dwell time, and Xiaoxin Yin for the proofreading and helpful comments. The authors also appreciate the anonymous reviewers who not only offered us detailed review comments but also insightful suggestions on future work.
9. REFERENCES
[1] R. Abernethy. The New Weibull Handbook. fifth edition, 2006.
[2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR, pages 19­26, 2006.
[3] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. In SIGIR, pages 3­10, 2006.
[4] J. Attenberg, S. Pandey, and T. Suel. Modeling and predicting user behavior in sponsored search. In KDD, pages 1067­1076, 2009.
[5] G. Buscher, L. van Elst, and A. Dengel. Segment-level display time as implicit feedback: a comparison to eye tracking. In SIGIR, pages 67­74, 2009.
[6] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit interest indicators. In IUI, pages 33­40, 2001.
[7] A. C. Cohen. Maximum likelihood estimation in the weibull distribution based on complete and on censored samples. Technometrics, 7(4):579­588, 1965.
[8] W. J. Conover. Practical Nonparametric Statistics. Wiley, third edition, 1998.

[9] D. Downey, S. Dumais, D. Liebling, and E. Horvitz. Understanding the relationship between searchers' queries and information goals. In CIKM, pages 449­458, 2008.
[10] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Trans. Inf. Syst., 23(2):147­168, 2005.
[11] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:579­588, 1999.
[12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR, pages 154­161, 2005.
[13] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Transaction on Information System, 25(2):7, 2007.
[14] D. Kelly and N. J. Belkin. Reading time, scrolling and interaction: exploring implicit sources of user preferences for relevance feedback. In SIGIR, pages 408­409, 2001.
[15] D. Kelly and N. J. Belkin. Display time as implicit feedback: understanding task effects. In SIGIR'04, pages 377­384, 2004.
[16] D. Kelly and C. Cool. The effects of topic familiarity on information search behavior. In JCDL, pages 74­75, 2002.
[17] D. Kelly and J. Teevan. Implicit feedback for inferring user preference: a bibliography. SIGIR Forum, 37(2):18­28, 2003.
[18] E. Lehman. Shapes, moments and estimators of the weibull distribution. IEEE Transactions on Reliability, 12:32­38, 1963.
[19] Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He, and H. Li. BrowseRank: letting web users vote for page importance. In SIGIR, pages 451­458, 2008.
[20] G. Marchionini and B. Shneiderman. Finding facts vs. browsing knowledge in hypertext systems. Computer, 21(1):70­80, 1988.
[21] M. Morita and Y. Shinoda. Information filtering based on user behavior analysis and best match text retrieval. In SIGIR, pages 272­281, 1994.
[22] D. Nichols. Implicit ratings and filtering. In Proceedings of the 5th DELOS Workshop on Filtering and Collaborative Filtering, pages 31­36, 1997.
[23] H. Rinne. The Weibull Distribution: A Handbook. Chapman & Hall, first edition, 2008.
[24] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R. Karger. The perfect search engine is not enough: a study of orienteering behavior in directed search. In CHI, pages 415­422, 2004.
[25] R. W. White and S. M. Drucker. Investigating behavioral variability in web search. In WWW, pages 21­30, 2007.
[26] R. W. White and S. T. Dumais. Characterizing and predicting search engine switching behavior. In CIKM, pages 87­96, 2009.

386

Segmentation of Multi-Sentence Questions: Towards
Effective Question Retrieval in cQA Services
Kai Wang, Zhao-Yan Ming, Xia Hu, Tat-Seng Chua
Department of Computer Science School of Computing
National University of Singapore
{kwang, mingzy, huxia, chuats}@comp.nus.edu.sg

ABSTRACT
Existing question retrieval models work relatively well in finding similar questions in community-based question answering (cQA) services. However, they are designed for single-sentence queries or bag-of-word representations, and are not sufficient to handle multi-sentence questions complemented with various contexts. Segmenting questions into parts that are topically related could assist the retrieval system to not only better understand the user's different information needs but also fetch the most appropriate fragments of questions and answers in cQA archive that are relevant to user's query. In this paper, we propose a graph based approach to segmenting multi-sentence questions. The results from user studies show that our segmentation model outperforms traditional systems in question segmentation by over 30% in user's satisfaction. We incorporate the segmentation model into existing cQA question retrieval framework for more targeted question matching, and the empirical evaluation results demonstrate that the segmentation boosts the question retrieval performance by up to 12.93% in Mean Average Precision and 11.72% in Top One Precision. Our model comes with a comprehensive question detector equipped with both lexical and syntactic features.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ Retrieval Models; I.2.7 [Artificial Intelligence]: Natural Language Processing ­ Text Analysis
General Terms
Algorithms, Design, Experimentation
Keywords
Question Answering, Question Segmentation, Question Matching, Yahoo! Answers
1. INTRODUCTION
Community-based Question Answering (cQA) services begin to emerge with the blooming of Web 2.0. They bring together a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

network of self-declared "experts" to answer questions posted by other people. Examples of these services include Yahoo! Answers (answers.yahoo.com) and Baidu Zhidao (zhidao.baidu.com) etc. Over times, a tremendous amount of historical QA pairs have been built up in their databases, and this transformation gives information seekers a great alternative to web search [2,18,19]. Instead of looking through a list of potentially relevant documents from the Web, users may directly search for relevant historical questions from cQA archives. As a result, the corresponding best answer could be explicitly extracted and returned. In view of the above, traditional information retrieval tasks like TREC [1] QA are transformed to similar question matching tasks [18,19].
There has been a host of work on question retrieval. The stateof-the-art retrieval systems employ different models to perform the search, including vector space model [5], language model [5,7], Okapi model [7], translation model [7,14,19] and the recently proposed syntactic tree matching model [18]. Although the experimental studies in these works show that the proposed models are capable of improving question retrieval performance, they are not well designed to handle questions in the form of multiple sub-questions complemented with sentences elaborating the context of the sub-questions. This limitation could be further viewed from two aspects. From the viewpoint of user query, the input to most existing models is simply a bag of keywords [5,19] or a single-sentence question [18]. It leads to a bottleneck in understanding the user's different information needs when the user query is represented in a complex form with many subquestions. From the viewpoint of the archived questions, none of the existing work attempts to distinguish context sentences from question sentences, or tries to segment the archived question thread into parts that are topically based. It prevents the system from presenting the user the most appropriate fragments that are relevant to his/her queries.
Figure 1 illustrates an example of a question thread extracted from Yahoo! Answers. There are three sub-questions (Q1, Q2 and Q3) asked in this thread, all in different aspects. If a user posts such example as a query, it is hard for existing retrieval systems to find all matches for the three sub-questions if the query is not well segmented. On the other hand, if a new similar query such as "what are the requirements of being a dentist?" is posted, it is also difficult for existing retrieval systems to return Q3 as a valid match if Q3 is not explicitly separated from its surrounding subquestions and contexts. Given all these constraints, it is thus highly valuable and desirable to topically segment multi-sentence questions, and to properly align individual sub-questions with their context sentences. Good segmentation not only helps the question retrieval system to better analyze the user's complex information needs, but also assists it in matching the query with the most appropriate portions of the questions in the cQA archive.

387

C1: i heard somewhere that in order to become a dentist, you need certain hours of volunteering or shadowing.
Q1: is that true?
Q2: if it is, how many hours?
C2: i have only a few hours of such activity...
Q3: and can you write down other requirements that one would need to become a dentist
C3: i know there are a lot of things but if you can write down as much as you can, that'd be a lot of help.
C4: thanks
Figure 1: Example of multi-sentence questions extracted from Yahoo! Answers
It appears to be natural to exploit traditional text-based segmentation techniques to segment multi-sentence questions. Existing approaches to text segment boundary detection include similarity based method [3], graph based method [13], lexical chain based method [10], text tiling algorithm [6] and the topic change detection method [12] etc. Although experimental results of these segmentation techniques are shown to be encouraging, they mainly focus on general text relations and are incapable of modeling the relationships between questions and contexts. A question thread from cQA usually comes with multiple subquestions and contexts, and it is desirable for one sub-question to be isolated from other sub-questions while closely linked to its context sentences.
After extensive study of the characteristics of questions in cQA archive, we introduce in this paper a new graph based approach to segment multi-sentence questions. The basic idea is outlined as follows. We first attempt to detect question sentences using a classifier built from both lexical and syntactic features, and use similarity and co-reference chain based methods to measure the closeness score between the question and context sentences. We model their relationships to form a graph, and use the graph to propagate the closeness scores. The closeness scores are finally utilized to group topically related question and context sentences.
The contributions of this paper are threefold: First, we build a question detector on top of both lexical and syntactic features. Second, we propose an unsupervised graph based approach for multi-sentence segmentation. Finally, we introduce a novel retrieval framework incorporating question segmentation for better question retrieval in cQA archives.
The rest of the paper is organized as follows: Section 2 presents the proposed technique for question sentence detection. Section 3 describes the detailed algorithm and architecture for multisentence segmentation, together with the new segmentation aided retrieval framework. Section 4 presents our experimental results. Section 5 reviews some related works and Section 6 concludes this paper with directions for future work.
2. QESTION SENTENCE DETECTION
Human generated content on the Web are usually informal, and it is not uncommon that standard features such as question mark or utterance are absent in cQA questions. For example, question mark might be used in cases other than questions (e.g. denoting uncertainty), or could be overlooked after a question. Therefore, traditional methods using certain heuristics or hand-crafted rules become inadequate to cope with various online question forms. To overcome these obstacles, we propose an automated approach to extracting salient sequential and syntactic patterns from question sentences, and use these patterns as features to build a question detector. Research on sequential patterns has been well discussed in many literatures, including the identification of

comparative sentences [9], the detection of erroneous sentences [17] and question sentences [4]. However, works on syntactic patterns have only been partially explored [17,18]. Grounded on these previous works, we next explain our pattern mining process, together with the learning algorithm for the classification model.
2.1 Sequential Pattern Mining
Sequential Pattern is also referred to as Labeled Sequential
Pattern (LSP) in the literatures. It is in the form of S  C , where
S is a sequence <t1,...,tn>, and C is the class label that the sequence S is classified to. In the problem of question detection, a sequence is defined to be a series of tokens from sentences, and the class is in the binary form of {Q, NQ} (resp. question and non-question). The purpose of sequential pattern mining is to extract a set of frequent subsequence of words that are indicative of questions. For example, the word sequence "anyone know what ... to" is a good indication to characterize the question sentence "anyone know what I can do to make me less tired". Note that the mined sequential tokens need not to be contiguous as appeared in the original text.
There is a handful of algorithms available to find all frequent subsequences, and the Prefixspan algorithm [11] is reported to be efficient in discovering all relative frequents by using a pattern growth method. We adopt this algorithm in our work by imposing the following additional constraints: 1) Maximum Pattern Length: We limit the maximum number of
tokens in a mined sequence to 5. 2) Maximum Token Distance: The two adjacent tokens tn and
tn+1 in the pattern need to be within a threshold window in the original text. We set it to 6. 3) Minimum Support: We set the minimum percentage of sentences in database D containing the pattern p to 0.45%. 4) Minimum Confidence: We set the probability of the pattern p being true in database D to 70%. To overcome the sparseness problem, we generalize the tokens by applying Part-of-Speech (POS) taggers to all tokens except some keywords including 5W1H words, modal words, stop words and the most frequent occurring words mind from cQA such as "any1", "im", "whats" etc. For example, the pattern <any1, know, what> will be converted to <any1, VB, what>. Each generalized pattern makes up a binary feature for the classification model as we will introduce in Section 2.3.
2.2 Syntactic Shallow Pattern Mining
We found that sequential patterns at the lexical level might not always be adequate to categorize questions. For example, the lexical pattern <when, do> presumes the non-question "Levator scapulae is used when you do the traps workout" to be a question, and the question "know someone with an eating disorder?" could be missed out due to the lack of indicative lexical patterns. These limitations, however, could be alleviated by syntactic features. The tree pattern (SBAR(WHADVP(WRB))(S(NP)(VP))) extracted from the former example has the order of NP and VP being switched, which might indicate the sentence to be a non-question, whereas the tree pattern (VP(VB)(NP(NP)(PP))) may be evidence that the latter example is indeed a question, because this pattern is commonly observed in the archived questions.
Syntactic patterns have been partially explored in erroneous sentence detection [17], in which all non-leaf nodes are flattened for frequent substructure extraction. The number of patterns to be explored, however, grows exponentially with the size of the tree, which we think is inefficient. The reason is that the syntactic

388

pattern will become too specific if mining is extended to a very deep level, and nodes at certain levels do not carry much useful structural information favored by question detection (e.g., the production rules NPDT·NN at the bottom level).
For better efficiency, we focus only on certain portion of the parsing tree by limiting the depth of the sub-tree patterns to be within certain levels (e.g. 2  D  4). We further generalize each syntactic pattern by removing some nodes denoting modifiers, preposition phrases and conjunctions etc. For instance, the pattern SQ(MD)(NP(NN))(ADVP(RB))(VP(VP)(NP)(NP)) extracted from the question "can someone also give me any advice?'' could be generalized into SQ(MD)(NP(NN))(VP(VP)(NP)(NP)), where the redundant branch ADVP(RB) that represents the adverb "also" is pruned. The pattern extraction process is outlined in Algorithm 1. The overall pattern mining strategy is analogous to the mining of sequential patterns, where the measures including support and confidence are taken into consideration to control the significance of the mined patterns. The discovered patterns are used together with the sequential patterns as features for the learning of classification model.

Algorithm 1 ExtractPattern (S , D)

Input: A set of syntactic trees for sentences (S); the depth range (D)

Output: A set of sub-tree shallow patterns extracted from S

1: Patterns = {};

2: for all Syntactic tree T  S do

3: Nodes  level order traversal of T from top to bottom;

4: for all node n  Nodes do

5:

Extract subtree p rooted under node n, with depth within the range D;

6:

p  generalize (p); // remove modifier nodes etc.

7:

Patterns.add (p);

// add p as a candidate

8: end for

9: end for

10: return Patterns;

2.3 Model Learning
The input to an algorithm that learns a binary classifier consists normally of both positive and negative examples. While it is easy to discover certain patterns from questions, it becomes unnatural to identify characteristics for non-questions. The imbalanced data distribution leads normal classifiers to perform poorly on the model learning. To address this issue, we propose to learn with the one-class SVM method. One-class SVM is built on top of the standard two-class SVM method, and its basic idea is to transform features from only positive examples via a kernel to a hyperplane, and treats the origin as the only member of the negative class. It further uses relaxation parameters to separate the image of positive class from the origin, and finally applies the standard two-class SVM techniques to learn a decision boundary. As a result, data points outside the boundary are considered to be outliers, i.e. non-questions in our problem.
The training data as used by traditional supervised learning methods usually require human labelling, which is not cheap. To save human efforts on data annotation, we take a shortcut by assuming all questions ending with question marks as an initial set of positive examples. This assumption is acceptable, as according to the results reported in [4], the rule-based method using only question mark achieves a very high precision (97%) in detecting questions. It in turn indicates that questions ending with "?" are highly likely to be real questions. To reduce the effect of possible outliers (e.g. non-questions ending with "?"), we need to purify the initial training set. There are many techniques available for training data refinement, such as bootstrapping, condensing, and editing. We choose a SVM-based data editing and classification method proposed by [15] to iteratively remove the samples likely

to be outliers. The detail is not covered here as it is beyond the scope of this paper.
For one-class SVM training, the linear kernel is used, as it is shown to outperform other kernel functions. In the iterations of training data refinement, the parameter  that controls the upper bound percentage of outliers is set to 0.02. The question detector model learned ultimately serves as a component for the multisentence question segmentation system.
3. Multi-Sentence Question Segmentation
Unlike traditional text segmentation, question segmentation ought to group each sub-question with its context sentences while separating it from the other sub-questions. Investigations show that the user posting styles in the online environment are largely unpredictable. While some users ask multiple questions in an interleaved manner, some prefer to list the whole description first and ask all sub-questions later. Therefore, naive methods such as using distance based metrics will be inadequate, and it is a great challenge to segment multi-sentence questions especially when the description sentences in various aspects are mixed together.
In the remainder of this section, we present a novel graphbased propagation method for segmenting multi-sentence questions. While the graph based method has been successfully applied in many applications like web search, to the best of our knowledge, this is the first attempt to apply it to the question segmentation problem. The intuition behind the use of graph propagation approach is that if two description sentences are closely related and one is the context of a question sentence, then the other is also likely to be its context. Likewise, if two question sentences are very close, then the context of one is also likely to be the context of the other. We next introduce the graph model of the multi-sentence question, followed by the sentence closeness score computation and the graph propagation mechanism.
3.1 Building Graphs for Question Threads
Given a question thread comprising multiple sentences, we represent each of its sentences as a vertex v. The question detector is then applied to divide sentences into question sentences and non-question sentences (contexts), forming a question sentence vertex set Vq and a context sentence vertex set Vc respectively.
We model the question thread into a weighted graph (V, E)
with a set of weight functions w : E   , where V is the set of
vertices VqVc, E is the union of three edge sets EqEcEr, and w(E) is the weight associated with the edge E. The three edge sets Eq, Ec and Er are respectively defined as follows:
- Eq: a set of directed edges uv, where u, v  Vq; - Ec : a set of directed edges uv, where u, v  Vc; - Er : a set of undirected edges u­v, where u  Vq and v  Vc. While the undirected edge indicates the symmetric closeness relationship between a question sentence and a context sentence, the directed edge captures the asymmetric relation between two question sentences or two context sentences. The intuition of introducing the asymmetry relationship could be explained with the example given in Figure 1. It is noticed that C1 is the context of the question sentence Q1 and C2 is the context of the question sentence Q2. Furthermore, Q2 is shown up to be motivated by Q1, but not in the opposite direction. This observation gives us the sense that C1 could also be the context of Q2, but not for C2 and Q1. We may reflect this asymmetric relationship using the graph model by assigning higher weight to the directed edge Q1Q2 than to Q2Q1. As a result, the weight of the chain C1Q1Q2 becomes much stronger than that of C2Q2Q1, indicating that

389

C1 is related to Q2 but C2 is not related to Q1, which is consistent to our intuition. From another point of view, the asymmetry helps

to regulate the direction of the closeness score propagation.

We give two different weight functions for edges depending on

whether they are directed or not. For the directed edge (uv) in

Eq and Ec, we consider the following factors in computing weight: 1) KL-divergence: given two vertices u and v, we construct the

unigram language models Mu and Mv for the sentences they represent, and use KL-divergence to measure the difference

between the probability distributions of Mu and Mv. We use DKL(Mu||Mv) to model the connectivity from u to v:

DKL (M u

||

Mv)



w

p(w |

M u ) log

p(w | p(w |

Mu) Mv)

(1)

Generally, the smaller the divergence value, the stronger the

connectivity, and the value of DKL(Mu||Mv) is usually unequal to DKL(Mv||Mu), thereby representing the asymmetry. 2) Coherence: it is observed that the subsequence sentences are

usually motivated by the earlier sentences. Given two vertices

u, v, we say that v is motivated by u (or u motivates v) if v

comes after u in the original post, and there are conjunction or

linking words connecting in-between. The coherence score

from u to v is determined as follows:

Coh(v

|

u)



1 0

if v is motivated by u otherwise

(2)

3) Coreference: coreference commonly occurs when multiple

expressions in a sentence or multiple sentences have the same

referent. We observe that sentences having the same referent

are somehow connected, and the more the referents two

sentences share, the stronger the connection. We perform the

coreference resolution on a question thread, and measure the

coreference score from vertex u to vertex v as follows:

Ref

(v

|

u)



1  



e

| referent{u
0

v}|

,

if v comes after u otherwise

(3)

Note that all the metrics introduced above are asymmetric, meaning that the measure from u to v is not necessarily the same as that from v to u. Given two vertices u, v  Eq or Ec, the weight of the edge uv is computed by a linear interpolation of the three factors as follows:

w1 (u



v)



1

1

DKL

1 (M

u

||

M

v

)

 2Coh(v

|

u)

 3Ref

(v

|

u)

where 0  1,2 ,3  1 .

(4)

Since DKL(Mv||Mu)  0, 0  Coh(v|u)  1, and 0  Ref(v|u)  1, the interval range of w1(uv) is between 0 to 1, and we do not need to apply normalization on this weight. We employed grid

search with 0.05 stepping space in our experiments and found that

the combination of {1 = 0.4, 2 = 0.25, 3 = 0.35} gives the most satisfactory results.

While the weight of the directed edges in Eq and Ec measures the throughput of the score propagation from one to another, the

weight of the undirected edge (u­v) in Er demonstrates the true closeness between a question and a context sentence. We consider

the following factors in computing the weight for edges in Er : 1) Cosine Similarity: given a question vertex u and a context

vertex v, we measure their cosine similarity weighted by the

word inverse document frequency (idfw) as follows:

Sim(u, v) 

wu,v fu (w)  fv (w)  (idfw )2

(5)

wu ( fu (w)  idfw )2 wv ( fv (w)  idfw )2

where fu(w) is the frequency of word w in sentence u, idfw is the inverse document frequency (# of posts containing w). We do not employ KL-divergence as we believe that the similarity between question and context sentences is symmetric. 2) Distance: questions and contexts separated far away are less likely to be relevant as compared to neighboring pairs. Hence, we take the following distance factor into account:

Dis(u, v)  e (u,v)

(6)

where  (u,v) is proportional to the number of sentences

between u and v in the original post. 3) Coherence: the coherence between a question and a context
sentence is also important, and we take it into account with the exception that the order of appearance is not considered:

Coh(u,

v)



1 0

if linked by conjunction words otherwise

(7)

4) Coreference: similarly, it measures the number of the same

referents in the question and context, without considering their

ordering:

Ref (u, v)  1  e|referent{uv}|

(8)

The final weight of the undirected edge (u­v) is computed by a linear interpolation of the abovementioned factors:

w2 (u  v)  1Sim(u,v)  2 Dis(u,v)  3Coh(u,v)  4Ref (u,v)

where 0  1, 2 , 3, 4  1

(9)

The combination of {1 = 0.4, 2 = 0.1, 3 = 0.3, 3 = 0.2} produces best results with grid search. Note that normalization is

not required as each factor is valued between 0 and 1. With the

weight of each edge defined, we next introduce the propagation

mechanism of the edge scores.

3.2 Propagating the Closeness Scores
For each pair of vertices, we assign the initial closeness score to be the weight of the edge in-between using the weight function introduced in Section 3.1, depending on whether the edge is in Eq, Ec or Er. Note that if the edge weight is very low, two sentences might not be closely related. For fast processing, we use a weight threshold  to prune edges with weight below . The parameter  is empirically determined, and we found in our experiments that the results are not very sensitive to  value below 0.15.

Algorithm 2 MapPropagation (G(V,E))

Input: The map model with initial scores assigned to every edge

Output: The map with updated closeness scores between questions and contexts

1: for every context cVc and every question qVq do // initialization

2:

w(q,c) = w2(q,c);

3: end for

4: while score is not converged do

5:

for every context cVc and question qVq do // propagate from c to q

6:

w'(q,c) = MAXqi Vq { w(qi,c)w1(qiq) }

7:

if (w(q,c) < w'(q,c))

8:

w(q,c) = w'(q,c)

9:

end for

10:

for every question qVq and context cVc do // propagate from q to c

11:

w'(c,q) = MAXci Vc { w(ci,q)w1(cic) }

12:

if (w(c,q) < w'(c,q))

13:

w(c,q) = w'(c,q)

14:

end for

15: end while

With the initial closeness scores, we carry out the score propagation using the algorithm outlined in Algorithm 2. The basic idea of this propagation algorithm is that, given a question sentence q and a context sentence c, if there is an intermediate question sentence qi such that the edge weight w1(qiq), together with the closeness score w(qi,c) between qi and c, are both

390

relatively high, then the closeness score w(q,c) between q and c could be updated to w1(qiq)w(qi,c) in case the original score is lower than that. In other words, qi becomes the evidence that q and c are related. The propagation algorithm works similarly in propagating scores from question sentences to context sentences, where an intermediate context ci could be the evidence that c and q are related. Notice that the direction of propagation is not arbitrary. For example, it makes no sense if we propagate the score along the path of cciq, because ci is simply the receiver of c, which could not be the evidence that a question and a context are correlated. When considering a pair of q and c, the possible directions of propagation are illustrated in Figure 2, in which the dashed lines indicate invalid propagation paths.

q1
q2 . . .
qn

w1(qiq) Invalid! w(q1,c)

q

c

w(q,c)

c1
c2 . . .
cn

q1
q2 . . .
qn

Invalid! w1(cic) w(c1,q)

q

c

w(q,c)

c1
c2 ...
cn

Propagating from c to q only nodes in Vq are considered to be intermediate nodes for propagation

Propagating from q to c only nodes in Vc are considered to be intermediate nodes for propagation

Figure 2: Illustration of the direction of score propagation

The damping factor  in the algorithm controls the transitivity

among nodes. In some circumstances, the propagated closeness

score might not indicate the true relatedness between two nodes,

especially when the score is propagated through an extremely

long chain. For example, {ABC} is close to {BCD}, {BCD} is

close to {CDE}, and {CDE} is close to {DEF}. The propagation

chain could infer {ABC} to be related to {DEF}, which is not

true. The introduction of damping factor  can leverage this

propagation issue by penalizing the closeness score when the

chain becomes longer. We empirically set  to 0.88 in this work.

The propagation of the closeness score will eventually

converge. This is controlled by our propagation principle that the

updated closeness score is a multiplication of two edge weights

whose value is defined to fall between 0 and 1. Hence the score is

always upper bounded by the maximum weight of the edges in E.

After the propagation reaches the stationary condition, we need

to extract all salient edges in Er for the alignment of questions and contexts. One straightforward method is to pre-define a threshold

, and remove all edges weighted under . However, this method

is not very adaptive, as the edge weights vary greatly for different

questions and a pre-defined threshold is not capable to regulate

the appropriate number of alignments between questions and

contexts. In this work, we take a dynamical approach instead: we

first sort edges in Er by the closeness score and extract them one by one in descending order <e1, e2, ... , en>. The extraction process terminates at em when one of the following criteria is met:

1.

ewm



ewm1





(

1 m

m i1

ewi



ewm )

,

where

ewi

is

the

i-th

edge weight in the order and  is the control parameter.

2. ewm+1 <  , where  is a pre-defined threshold controlling the overall connection quality (we set it to 0.05).

3. m = n, meaning all edges have been extracted out from Er. When the extraction procedure terminates, the extracted edge

set {e1,...,em} represents the final alignment between questions and contexts. For each edge ei connecting between a context c and a question q, c will be considered as the context to question q, and

they belong to the same question segment. For example, a final

edge set {(q1,c1), (q2,c2), (q1,c2), (q2,c4), (q3,c1), (q2,c3)} produces three question segments: (q1 ­ c1,c2), (q2 ­ c2,c3,c4) and (q3 ­ c1). Note that the segmentation works in a fuzzy way such that no explicit boundaries are defined between sentences. Instead, a question could have multiple context sentences, whereas a context sentence does not necessarily belong to only one question.

3.3 Segmentation-aided Retrieval
By applying segmentation on the multi-sentence questions from cQA, sub-questions and their corresponding contexts that are topically related could be grouped. Figure 3 shows an improved retrieval framework with segmentation integrated. Different from existing models, the question matcher matches two question sentences with the assistance of additional related contexts such that the users' query can be matched with the archived cQA questions more precisely. More specifically, the user query is no longer restricted to a short single-sentence question, but can be in the form of multiple sub-questions complemented with many description sentences. An archived question thread asking in various aspects could also be indexed into different questioncontext pairs such that the matching is performed on the basis of each question-context pair.

Segmentation Module

Input
Question Thread

Question Detection
1

Questions Contexts

Question Segmentation
2

Question Repository
Question Threads

Query

Segmentation

RS

Q1

C11, C12, ...

...Q2

C21..., C22, ...

Qn

Cn1, Cn2, ...

Questions Contexts

Indexer

Segmentation Module

Segmentation

QS

Question Segments

Q1
...Q2

C11, C12, ...
... C21, C22, ...

Qn

Cn1,Cn2, ...

Questions Contexts

Q index C index

Question Matcher

Q1

C11,C12, ...

...Q2

... C21,C22, ...

Qn

Cn1,Cn2, ...

Output

M1 M2 M4

M3
M5 ... ...

Question Retrieval System

Matched Questions

Figure 3: Retrieval framework with question segmentations

4. EXPERIMENTS
In this section, we present empirical evaluation results to assess the effectiveness of our question detection model and multisentence segmentation technique. In particular, we conduct experiments on the Yahoo! Answers QA archive and show that our question detection model outperforms traditional rule based or lexical based methods. We further show that our segmentation model works more effectively than conventional text segmentation techniques in segmenting multi-sentence questions, and it gives additional performance boosting to cQA question matching.

4.1 Evaluation of Question Detection
Dataset: We issued getByCategory API query to Yahoo! Answers, and collected a total of around 0.8 million question threads from Healthcare domain. From the collected data, we generate the following three datasets for the experiments: - Pattern Mining Set: Around 350k sentences extracted from 60k
question threads are used for lexical and syntactic pattern

391

mining, where those ending with "?" are treated as question sentences and the others as non-question sentences1. - Training Set: Around 130k sentences ending with "?" from another 60k question threads are used as the initial positive examples for one-class SVM learning method. - Testing Set: Two annotators are asked to tag some randomly picked sentences from a third post set. A total of 2004 question sentences and 2039 non-question sentences are annotated.
Method: To evaluate the performance of our question detection model, we use five different systems for comparison: 1) 5W1H (baseline1): a rule based method determines that a
sentence is a question if it contains 5W1H words. 2) Question Mark (baseline2): a rule based method judges that a
sentence is a question if it ends with the question mark "?". 3) SeqPattern: Using only sequential patterns as features. 4) SynPattern: Using only syntactic patterns as features. 5) SeqPattern+SynPattern: Using both sequential patterns and
syntactic patterns as features for question sentence detection. A grid search algorithm is performed to find the optimal number of features used for model training, and a set of 1314 sequential patterns and 580 syntactic patterns are shown to give the best performance. Table 1 illustrates some pattern examples mined.

Table 1: Examples for sequential and syntactic patterns

Pattern Type Sequential Pattern
Syntactic Pattern

Pattern Example

< anyone VB NN >

< what NN to VB NN >

< NNS should I >

< can VB my NN>

< JJS NN to VB >

...

(SBARQ (CC)(WHADVP (WRB))(SQ (VBP)(NP)(VP)))

(SQ (VBZ)(NP (DT))(NP (DT)(JJ)(NN)))

(VP (VBG)(S (VP)))

...

Metrics & Results: We employ Precision, Recall, and F1 as metrics to evaluate the question detection performance. Table 2 tabulates the comparison results. From the table, we observe that 5W1H performs poorly in both precision and recall. Question mark based method gives the highest precision, but the recall is relatively low. This observation is in line with the reported results in [4]. On the other hand, SeqPattern gives relatively high recall and SynPattern gives relatively high precision. The combination of both augments the performance in both precision and recall by a lot, and it achieves statistically significant improvement (t-test, p-value<0.05) as compared to SeqPattern and SynPattern. We believe that the improvement stems from the ability of the detection model to capture the salient characteristics in questions at both the lexical and syntactic levels. The results are also consistent with our intuition that sequential patterns could misclassify a non-question to a question, but syntactic patterns may leverage it to certain extent. It is noted that our question detector exhibits a sufficiently high F1 score for its use in the multi-sentence question segmentation model in the later phase.

Table 2: Performance comparisons for question detection on different system combinations

System Combination (1) 5W1H (2) Question Mark (3) SeqPattern (4) SynPattern (5) SeqPattern+SynPattern

Precision (%) 75.37 94.12 88.92 90.06 92.11

Recall (%) 49.50 77.50 88.47 78.19 89.67

F1(%) 59.76 85.00 88.69 83.71
90.87

1 This is acceptable for a large dataset, as a question ending with "?" is claimed to have high precision to be a true question.

4.2 Direct Assessment of Multi-Sentence
Question Segmentation via User Study
We first evaluate the effectiveness of our multi-sentence question segmentation model (denoted as MQSeg) via a direct user study. We set up two baselines using the traditional text segmentation techniques for comparison. The first baseline (denoted as C99) employs the C99 algorithm [4], which uses a similarity matrix to generate a local sentence classifier so as to isolate topical segments. The second baseline (denoted as TransitZone) is built on top of the method proposed in [12]. It measures the thematic distance between sentences to determine a series of transition zones, and uses them to locate the boundary sentences. To conduct the user study, we generate a small dataset by randomly sampling 200 question threads from the collected data. We run the three segmentation systems for each question thread, and present the segmentation results to two evaluators without telling them from which system the result was generated. The evaluators are then asked to rate the segmentation results using a score from 0 to 5 with respect to their satisfaction. Figure 4 shows the score distributions from the evaluators for three different segmentation systems. We can see from Figure 4 that users give relatively moderate scores (avg. 2 to 3) to the results returned by two baseline systems, whereas they seem to be more satisfied with the results given by MQSeg. The score distribution in MQSeg largely shifts towards high end as compared to the two baseline systems. The average rating scores for three different systems are 2.63, 2.74, and 3.6 respectively. We consider two evaluators to be agreeable to the segmentation result if their score difference does not exceed 1, and the average level of peer agreement obtained between the two evaluators is 93.5%.

35 %

Baseline 1 - C99

30

EV1

25 EV2

20

15

10

5

0

Score 0 1 2 3 4 5

35 % Baseline 2 - TransitZone

30

EV1

25

EV2

20

15

10

5

0

Score 0 1 2 3 4 5

40 % Our System - MQSeg

35

EV1

30

EV2

25

20

15

10

5

0

Score 0 1 2 3 4 5

Avg StDev Peer Agrmt Avg StDev Peer Agrmt Avg StDev Peer Agrmt

2.63 1.34

93.5% 2.74 1.35

92.5% 3.6 1.28

94.5%

Figure 4: Score distribution of user evaluation for 3 systems

It is to our expectation that MQSeg performs better than C99 or TransitZone segmentation systems. One straightforward reason is that MQSeg is specifically designed to segment multi-sentence questions, whereas the traditional systems are designed for generic purpose and do not distinguish question sentences from contexts. While the conventional systems fail to capture the relationship between questions and their contexts, our system aligns the questions and contexts in a fuzzy way that one context sentence could belong to different question segments. As online content is usually freely posted and does not strictly adhere to the formal format, we believe that our fuzzy grouping mechanism is more suitable to correlate sub-questions with their contexts, especially when there is no obvious sign of association.

4.3 Performance Evaluation on Question
Retrieval with Segmentation Model
In cQA, either archived questions or user queries could be in the form of a mixture of question and description sentences. To further evaluate our segmentation model and to show that it can improve question retrieval, we set up question retrieval systems coupled with segmentation modules for either question repository or user query.

392

Methods: We select BoW, a simple bag-of-word retrieval system that matches stemmed words between the query and questions, and STM, a syntactic tree matching retrieval model proposed in [18] as two baseline systems for question retrieval. For each baseline, we further set up three different combinations: 1) Baseline+RS: a baseline retrieval system integrated with
question repository segmentation. 2) Baseline+QS: a baseline retrieval system equipped with user
query segmentation. 3) Baseline+RS+QS: the retrieval system with segmentations for
both repository questions and user queries. It gives rise to a total of 6 different combinations of methods for comparison.
Dataset: We divide the collected 0.8 million question dataset from Yahoo! Answers into two parts. The first part (0.75M) is used as a question repository, while the remaining part (0.05M) is used as a test set. For data preprocessing, systems coupled with RS will segment and index each question thread in the repository accordingly, whereas systems without RS simply performs basic sentence indexing tasks. From the test set, we randomly select 250 sample questions, each of which is in the form of one singlesentence question with some context sentences. The reason that we do not take queries of multi sub-questions as test cases is that traditional cQA question retrieval systems cannot handle complex queries, making it impossible to conduct the comparison test. Nevertheless, it is sufficient to use single-question queries here as our purpose is to testify that the context extracted by the segmentation model could help question matching.
For systems equipped with user query segmentation (QS), we simply use the testing samples as they are, whereas for systems without QS, we manually extract the question sentences from the samples and use them as queries without their corresponding context sentences. For each retrieval system, the top 10 retrieval results are kept. For each query, we combine the retrieval results from different systems, and ask two annotators to label each result to be either "relevant" or "irrelevant" without telling them from which system the result is generated. The kappa statistic for identifying relevance between two evaluators is reported to be 0.73. A third person will be involved if conflicts happen. By eliminating some queries that have no relevant matches, the final testing set contains 214 query questions.

Table 3: Performance of different systems measured by MAP, MRR, and P@1 (%chg shows the improvement as compared to BoW or STM baselines. All measures achieve statistically significant improvement with t-test, p-value<0.05)

Systems
BoW BoW+RS BoW+QS BoW+RS+QS
STM STM+RS STM+QS STM+RS+QS

MAP
0.5807 0.6389 0.6245 0.6558
0.6653 0.7310 0.7238 0.7415

%chg
­ 10.02
7.54 12.93
­ 9.88 8.79 11.45

MRR
0.7138 0.7565 0.7429 0.7690
0.7429 0.7774 0.7893 0.7984

%chg
­ 5.98 4.07 7.73
­ 4.64 6.24 7.46

P@1
0.5981 0.6542 0.6355 0.6682
0.6308 0.6776 0.6916 0.7009

%chg
­ 9.38 6.25 11.72
­ 7.41 9.63 11.11

Metrics & Results: We evaluate the performance of retrieval systems using three metrics: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Precision at Top One (P@1). The evaluation results are presented in Table 3.
We can see from Table 3 that STM consistently outperforms BoW. Applying question repository segmentation (RS) over both BoW and STM baselines boosts system performance by a lot. All

RS coupled systems achieve statistically significant improvement in terms of MAP, MRR and P@1. We believe that the improvement stems from the ability of the segmentation module to eliminate irrelevant content that is favored by traditional BoW or STM approaches. Take the query question "What can I eat to put on weight?" as an example, traditional approaches may match it to an irrelevant question "I'm wearing braces now. what am I allowed to eat?" due to their high similarity on the questioning part. The mismatch however, could be alleviated if repository segmentation gets involved, where the context sentence can give clear clue that the above archived sentence is not relevant to the user query.
Performing user query segmentation (QS) on top of baseline systems also brings in large improvements in all metrics. This result is in line with our expectation. The introduction of QS is based on the intuition that contexts could complement questions with additional information, which help the retrieval system to better understand the user's information need. For example, given an example question from our testing set "Questions about root canal?", it makes no sense for retrieval systems to find its related questions if the context is absent, because there could be hundreds of irrelevant questions in the QA archive as long as they are concerned about "root canal".
Interestingly, STM+QS gives more improvement over STM as compared to BoW+QS over BoW. Our reading is that, BoW is less sensitive to the query context as compared to STM. To be more specific, the query context provides information at the lexical level, and BoW handles bad-of-word queries at the lexical level, whereas STM matches questions at the syntactic level. As such, it is reasonable that matching at both lexical and syntactic levels (STM+QS) gives more performance boosting as compared to only at lexical level (BoW+QS). Similar interpretation could be applied to explain the finding that BoW+RS system gives more significant improvement over BoW as compared to BoW+QS. Furthermore, we conjecture that, without RS, BoW is likely to match the query with some context sentences, whereas having question repository properly segmented overcomes this issue to a large extent.
Lastly, the combination of both RS and QS brings in significant improvement over the other methods in all metrics. The MAP on systems integrated with RS and QS improves by 12.93% and 11.45% respectively over BoW and STM baselines. RS+QS embedded systems also yield better top one precision by correctly retrieving questions at the first position on 143 and 150 questions respectively, out of a total of 214 questions. These significant improvements are consistent to our observations that RS and QS complement each other in not only better analyzing the user's information need but also organizing the question repository more systematically for efficient question retrieval.
Error Analysis: Although we have shown that RS together with QS improves question retrieval, there is still plenty of room for improvement. We perform micro-level error analysis and found that the segmentation sometimes fails to boost retrieval performance mainly for the following three reasons: 1) Question detection error: The performance of question
segmentation highly depends on the reliability of the question detector. Although we have shown that the performance of our question detection model is very competitive, the noisy online environment still leads many questions to be miss-detected. Examples are the question in abbreviated form such as "signs of a concussion?" and the question in declarative form such as "I'm going through some serious insomniac issues?" etc.

393

2) Closeness gaps: The true closeness score between sentences is relatively hard to measure. For simplicity and efficiency, the relatedness measure in this work is more at the lexical level, and the only semantic factor we have taken is coreference resolution. These measures may become insufficient when the sentences grow in complexity, especially when there is a lack of lexical evidence (e.g. cue words or phrases etc.) indicative of the connection between two sentences. This is a difficult challenge, and a good strategy may be to apply more advanced NLP techniques or semantic measures.
3) Propagation errors: The propagated closeness score could be unreliable even when the propagation chain is short. Given three questions "is it expensive to see a dentist instead?" (Q1), "if it is not, how long it takes to get my teeth whitened?" (Q2), and "How many ways to get my teeth whitened?" (Q3), Q1 is considered to be the predecessor of Q2, and Q3 is closed to Q2, but the linkage between Q1 and Q3 is so weak that assigning the context of Q1 to Q3 becomes inappropriate. We conjecture that selecting the damping factor  in a more dynamic way (e.g. associating  with the actual question) could help to adjust the propagation trend. We leave it to our future work.
5. RELATED WORK
There have been many literature works in the direction of question retrieval, and these works could generally be classified into two genres: the early FAQ retrievals and the recent cQA retrievals. Among FAQ related works, many retrieval models have been proposed, including the conventional vector space model [8], noisy channel model [16], and translation based model [14] etc. Most of these works tried to extract a large number of FAQ pairs from the Web, and use the FAQs dataset to do training and retrieval.
The cQA archive is different from FAQ collections in the sense that the content of cQA archive is much noisier and the scope is much wider. The state-of-the-art cQA question retrieval systems also employ different models to perform the search, including the vector space model [5], language model [5,7], Okapi model [7], and translation model [7,14,19] etc. Claiming that purely lexical level models are not adequate to cope with natural languages, Wang et al. [18] proposed a syntactic tree matching model to rank historical questions.
However, all these previous works handle bag-of-words queries or single-sentence questions only. On the contrary, we take a new approach by introducing a question segmentation module, where the enhanced retrieval system is capable of segmenting a multisentence question into parts that are topically related and perform better question matching thereafter. To the best of our knowledge, no previous work has attempted to look into this direction, or use question segmentation to improve the question search.
6. CONCLUSION AND FUTURE WORK
In this paper, we have presented a new segmentation approach for segmenting multi-sentence questions. It separates question sentences from non-question sentences and aligns them according to their closeness scores as derived from the graph based model. The user study showed that our system produces more satisfactory results as compared to the traditional text segmentation systems. Experiments conducted on the cQA question retrieval systems further demonstrated that segmentation significantly boosts the performance of question matching.
Our qualitative error analysis revealed that the segmentation model could be improved by incorporating a more robust question

detector, together with more advanced semantic measures. One promising direction for future work would be to also analyze the answers to help question segmentation. This is because answers are usually inspired by questions, where certain answer patterns could be helpful to predict the linkage between question and context sentences. The segmentation system in this work takes all noisy contexts as they are, without further analysis. The model could be further improved by extracting the most significant content and align them with question sentences. Finally, it is important to evaluate the efficiency of our proposed approach as well as to conduct additional empirical studies of the performance of question search with segmentation model incorporated.
7. REFERENCES
[1] Trec proceedings. http://trec.nist.gov/. [2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G.
Mishne. Finding high-quality content in social media. In WSDM, 2008. [3] F. Y. Y. Choi. Advances in domain independent linear text segmentation. In NAACL, 2000. [4] G. Cong, L. Wang, C.-Y. Lin, Y.-I. Song, and Y. Sun. Finding question-answer pairs from online forums. In SIGIR, 2008. [5] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching questions by identifying question topic and question focus. In HLTACL, 2008. [6] M. A. Hearst. Multi-paragraph segmentation of expository text. In ACL, 1994. [7] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in large question and answer archives. In CIKM, 2005. [8] V. Jijkoun and M. de Rijke. Retrieving answers from frequently asked questions pages on the web. In CIKM, 2005. [9] N. Jindal and B. Liu. Identifying comparative sentences in text documents. In SIGIR, 2006. [10] M.-Y. Kan, J. L. Klavans, and K. R. McKeown. Linear segmentation and segment significance. In WVLC, 1998. [11] J. Pei, J. Han, B. Mortazavi-asl, H. Pinto, Q. Chen, U. Dayal, and M. chun Hsu. Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth. In ICDE, 2001. [12] V. Prince and A. Labadi´e. Text segmentation based on document understanding for information retrieval. 2007. [13] J. C. Reynar. Topic segmentation: Algorithms and applications, 1998. [14] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. Statistical machine translation for query expansion in answer retrieval. In ACL, 2007. [15] X. Song, G. Fan, and M. Rao. Svm-based data editing for enhanced one-class classification of remotely sensed imagery. Geoscience and Remote Sensing Letters, IEEE, 2008 [16] R. Soricut and E. Brill. Automatic question answering: Beyond the factoid. In HLT-NAACL, 2004. [17] G. Sun, G. Cong, X. Liu, C.-Y. Lin, and M. Zhou. Mining sequential patterns and tree patterns to detect erroneous sentences. In AAAI, 2007. [18] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree matching approach to finding similar questions in community-based qa services. In SIGIR, 2009. [19] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question and answer archives. In SIGIR, 2008.

394

Evaluating and Predicting Answer Quality in Community QA

Chirag Shah
School of Communication & Information (SC&I) Rutgers, The State University of New Jersey 4 Huntington St, New Brunswick, NJ 08901
chirags@rutgers.edu

Jefferey Pomerantz
School of Information & Library Science (SILS) University of North Carolina at Chapel Hill 100 Manning Dr, Chapel Hill, NC 27599
pomerantz@unc.edu

ABSTRACT
Question answering (QA) helps one go beyond traditional keywords-based querying and retrieve information in more precise form than given by a document or a list of documents. Several community-based QA (CQA) services have emerged allowing information seekers pose their information need as questions and receive answers from their fellow users. A question may receive multiple answers from multiple users and the asker or the community can choose the best answer. While the asker can thus indicate if he was satisfied with the information he received, there is no clear way of evaluating the quality of that information. We present a study to evaluate and predict the quality of an answer in a CQA setting. We chose Yahoo! Answers as such CQA service and selected a small set of questions, each with at least five answers. We asked Amazon Mechanical Turk workers to rate the quality of each answer for a given question based on 13 different criteria. Each answer was rated by five different workers. We then matched their assessments with the actual asker's rating of a given answer. We show that the quality criteria we used faithfully match with asker's perception of a quality answer. We furthered our investigation by extracting various features from questions, answers, and the users who posted them, and training a number of classifiers to select the best answer using those features. We demonstrate a high predictability of our trained models along with the relative merits of each of the features for such prediction. These models support our argument that in case of CQA, contextual information such as a user's profile, can be critical in evaluating and predicting content quality.
Categories and Subject Descriptors
H.3: INFORMATION STORAGE AND RETRIEVAL H.3.3: Information Search and Retrieval: Search process; H.3.5: Online Information Services: Web-based services
General Terms
Experimentation; Human Factors; Measurement.
Keywords
Community question answering; answer quality evaluation and prediction.

1. INTRODUCTION
Community Question Answering (CQA) sites have emerged in the past few years as an enormous market, so to speak, for the fulfillment of information needs. Estimates of the volume of questions answered are difficult to come by, but it is likely that the number of questions answered on CQA sites far exceeds the number of questions answered by library reference services [4], which until recently were one of the few institutional sources for such question answering. CQA sites make their content ­ questions and associated answers submitted on the site ­ available on the open web, and indexable by search engines, thus enabling web users to find answers provided for previously asked questions in response to new queries.
Yahoo! Answers1 and AnswerBag2 are examples of such CQA services, the popularity of which have been increasing dramatically for the past several years.3 The fact that CQA sites receive such a high volume of use, and that there is such a large ocean of information needs that may be fulfilled by these sites, makes it critical to establish criteria for evaluating the quality of answers provided by these sites. Library reference services have a long tradition of evaluation to establish the degree to which a service is meeting user needs [9]. Such evaluation is no less critical for CQA sites, and perhaps even more so, as these sites do not have a set of professional guidelines and ethics behind them, as library services do.
A small but growing body of literature exists on evaluating the quality of answers provided on CQA sites. Some work has been done to predict asker satisfaction [8], but little work exists on what factors contribute to a quality answer, beyond simply being satisfactory to the asker.
As Liu et al. [8] point out, research on CQA draws on research on interactive Question Answering. One significant distinction, however, is that in studying CQA there is an actual user, while the questions used in the TREC QA track, for example, are questions submitted to the FAQ Finder system, and the found answers (by participating sites) are evaluated by trained assessors [15]. It is of course appropriate to reduce the subjectivity of evaluating answers in a large-scale evaluation of systems like a TREC track. However, in CQA, the subjectivity of relevance assessments is

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

1 http://answers.yahoo.com 2 http://www.answerbag.com 3 According to a March 2008 Hitwise report
(http://www.hitwise.com/press-center/hitwiseHS2004/question-
and-answer-websites.php), visits to CQA websites have increased
118% year-over-year since 2006.

411

central. Furthermore, in evaluations of CQA, it becomes possible to glean relevance assessments from actual users ­ rather than trained assessors ­ possibly even the asker's own relevance assessments.
Beyond the benefit to the user of having better metrics for evaluating the quality of answers, it would benefit the management of CQA sites themselves to have such metrics. Many CQA sites have implemented reputation systems, where answerers can gain points, or advance levels, based on their participation on the site: number of questions answered, number of answers voted as best answers, etc. Having metrics for evaluating the quality of an answer would enable CQA sites to add this as a factor in building an answerer's reputation. A reputation system that incorporated such quality metrics would also benefit askers: an asker could view the profiles of answerers who provided answers to his question, and see the quality of past answers provided by that answerer.
Some researchers suggest that the context that gives rise to an information need is unique for every individual, and that an answer that is useful to an individual in a particular context will therefore be at best only partially useful to others in other contexts [1]. Identifying the factors that contribute to the quality of answers would therefore be critical for informing web users in deciding if a previously provided answer is appropriate for answering their question.
In this paper we present a novel approach to measuring the quality of answers on CQA sites, and use it to predict which of the answers to a given question the asker will pick as the best answer. The specific problem of answer quality prediction is defined in Section 3. This is followed by our two approaches to measure the quality of the answers. In Section 4, we discuss how we used human assessment of different aspects of answer quality, and in Section 5, we show how we used automatically extracted features for measuring and predicting quality of answers. An overview of some of the related work is provided in the following section.
2. BACKGROUND
Community Question Answering, according to Shah et al. [13], consists of three components: a mechanism for users to submit questions in natural language, a venue for users to submit answers to questions, and a community built around this exchange. Viewed in that light, online communities have performed a question answering function perhaps since the advent of Usenet and Bulletin Board Systems, so in one sense CQA is nothing new. Websites dedicated to CQA, however, have emerged on the web only within the past few years: the first CQA site was the Korean Naver Knowledge iN, launched in 2002, while the first Englishlanguage CQA site was Yahoo! Answers, launched in 2005. Despite this short history, however, CQA has already attracted a great deal of attention from researchers investigating information seeking behaviors [5], selection of resources [3], social annotations [2], user motivations [12], comparisons with other types of question answering services [14], and a range of other information-related behaviors.
While CQA sites are new, they have been in existence long enough for product differentiation to begin to occur. Many CQA sites allow questions to be submitted on any topic: for example, Yahoo! Answers, WikiAnswers, AnswerBag, and others. Several CQA sites, however, restrict their scope in a variety of ways.

Some sites are subject-specific, such as Stack Overflow, which limits its scope to questions about programming,4 and Math Overflow, which limits its scope to research level math questions.5 Some sites serve a specific user community, such as HeadHunterIQ, which targets business recruiters.6 Some sites answer only specific types of questions, such as Homework Hub, which is limited to homework help.7 From the standpoint of user satisfaction ­ with both the answer and the site ­ it would be a benefit to CQA sites for there to be a mechanism to triage questions between sites. The topic of a question would obviously be a factor in such a mechanism, but other factors in evaluating the quality of answers provided on the site could also be valuable for this purpose.
This sort of triage is comparatively simple for a human to perform ­ and while time-consuming, is in fact commonly performed by librarians for digital reference services [9,10]. The QuestionPoint reference service,8 an application for managing a global collaborative of library reference services, also performs this sort of triage automatically, by matching questions to profiles of individual libraries [6]. The level of complexity that such triage systems are currently capable of, however, pale in comparison to the complexity that a human triage can manage.
Both triage and the evaluation of the quality of answers are, of course, largely classification problems: in triage, the question must be placed in one and only one "bin" corresponding to a specific answering service, while in evaluation it must be determined if the question does or does not meet an evaluation criterion. Most classification tasks are sufficiently complex that, by and large, humans are able to perform them more effectively ­ if not efficiently ­ than machine classification. To improve machine classification, of course, a sufficiently large dataset and a sufficiently rich set of factors are needed. The work reported here investigates two methods for developing sets of factors: first, a set of 13 quality metrics identified from the literature on CQA are used for human quality assessments, then a set of features are automatically extracted.
CQA presents interesting problems for automatically extracting criteria to guide classification. Automatic classification must of course make use of a corpus of some kind. For CQA, this corpus consists of the questions and their associated answers submitted on the site, and could also include other data such as user profiles, answer ratings, etc. There is, however, much more going on CQA sites than just answering questions and reputation building. CQA sites are communities, however loosely defined, and so there is a wealth of contextual information about that community, and the interpersonal dynamics within it, that could be utilized, if only we understood how to make use of it.
In an article mapping out a research agenda for CQA, Shah et al. [13] identify two primary threads in this work: research on the content of CQA sites, which includes analyses of the content and quality of the questions and answers, and research on the community around these sites, which includes analyses of the reputation systems and social norms on these sites. While, of
4 http://www.stackoverflow.com 5 http://www.mathoverflow.net 6 http://www.headhunteriq.com 7 http://www.stackexchangesites.com/homework-hub 8 http://questionpoint.org

412

course, the content of a CQA site would not exist without the community, here we focus only on the former.
CQA sites may be mined to identify the information needs within a specific domain [7], but Yahoo! Answers (YA) enables questions to be asked on nearly any possible topic. This broad diversity of topics, as well as the diversity of users asking questions, makes evaluating answers challenging. Indeed, Shah et al. [12] found that while 95% of questions that they investigated on YA received at least one answer, the quality of those answers varied widely, from correct answers to partial and unhelpful answers, to spam. Further, as in any instance of information retrieval or question answering, the quality of the answer is heavily dependent user-specific factors.
One of the difficulties of studying CQA, however, is the difficulty ­ sometimes the impossibility ­ of gaining access to the members of the community themselves, askers or answerers. As a result, a range of approaches has emerged for making use of proxies for the askers themselves. Unable to contact the askers directly, Kim et al. [5] analyzed the comments provided by askers when selecting a best answer. Others have used third parties to stand in for askers: Harper et al. [3] used undergraduate students as proxies for askers, while Liu et al. [8] used both subject experts and paid workers from Amazon Mechanical Turk.9 Both of these approaches have clear advantages: the former makes use of the asker's own words and evaluation criteria, while the latter is able to collect more detailed evaluative data. The study presented here makes use of the latter approach.
A range of approaches has also emerged for developing evaluation criteria used in studies of CQA. Liu et al. [8] had proxies evaluate answers according to the same 5-star rating system available to the asker. Kim et al. [5] let evaluation criteria emerge from the comments provided by askers when selecting a best answer. Harper et al. [3] had proxies evaluate answers according to criteria derived from the evaluation of library reference services. Perhaps the most fully developed set of evaluation criteria for answers in CQA, however, was proposed by Zhu et al. [16], who identified a set of 13 criteria from both CQA sites' guidelines for answerers, and the comments provided by the community on questions and answers. For our work reported here, we will use these 13 criteria when asking human assessors to judge the quality of an answer.
3. PROBLEM OF ANSWER QUALITY PREDICTION
The quality of an answer, or of any information content for that matter, can be subjective. A quality assessment may depend on the relevance of that content, among other factors, and relevance itself is difficult to measure in the context of CQA. We, therefore, provide our own interpretation of quality with respect to the data and the task we have on our hand.
On YA, a question is considered to be resolved if either the community votes and selects one of the answers to be the best, or the asker himself chooses one as the best answer from the set of answers he received for his question. It is possible that multiple answers are of high quality, but only one of them gets picked as the best answer. Liu et al. [8] considered the act of an asker
9 http://www.mturk.com

choosing one as the best answer an indication of satisfaction. If the asker does not select any answer as the best one, and/or if the community votes for the best answer, the asker is assumed to be unsatisfied. For the work reported here, we will follow this notion of asker satisfaction. To extend it to indicate the quality of an answer, we will add another constraint that the asker has to give the chosen answer a rating of at least 3 out of 5. Thus, we consider an answer to be a high quality answer, if (1) the asker chose it as the best answer, and (2) gave it a rating of at least 3.
Given this, we define the problem of answer quality prediction to be one where we need to predict if a given answer will be selected by the asker as a high quality answer. Thus, the goal of our work is to predict if an answer was chosen by the asker as the best answer with a high rating. In order to do that, we will evaluate each answer's quality according to several measures. First, we will use the 13 criteria used by Zhu et al. [16] as different aspects of answer quality. Next, we will extract a number of features from questions, answers, and the profiles of their posters as a way to measure answer quality. For both of these approaches, we will demonstrate how we extracted necessary features and constructed fairly reliable models, and used these models to classify an answer to be in the `yes' class (chosen as the best), or in the `no' class (not chosen as the best).
4. PREDICTION MODEL USING HUMAN ASSESSMENTS
In this section we will describe how we used the quality assessment data obtained from Amazon Mechanical Turk (MTurk) human assessors (also known as Turk workers) to build a regression model for predicting answer quality. It is important to note that in a strict sense, we are not actually trying to evaluate the quality of answers. Rather, we are trying to predict if a given answer will be selected by the asker as the best answer or not. It is possible that an answer is of good quality, but was not chosen by the asker as the best answer. Conversely, an asker could select an answer as the best that another (a librarian or a subject expert, for example) would not have evaluated highly. But for the purpose of our work here, we will assume that an answer declared as the best answer by the asker with a rating of at least 3 is a high quality answer.
4.1 Data
Yahoo! Research makes several datasets available to universityaffiliated researchers, through their WebscopeTM Program.10 One of these datasets is Yahoo! Quest, which consists of the full text of approximately 8 million questions from Yahoo! Answers, the full text of all answers to each question, which was voted the best answer, the subject of the question as chosen by the asker, and other associated data.
In the Yahoo! Quest dataset, questions are categorized into four broad types: advice, informational, opinion, and polling. From this dataset, we randomly sampled 30 questions of each of these four types, for which there were at least five answers. For each question, we picked five answers ­ the answer voted as the best answer, and four others randomly sampled. Thus, our data consisted of 120 questions with 600 answers. Beyond being stratified into the aforementioned four types, the questions in our
10 http://sandbox.yahoo.com

413

dataset were quite wide-ranging, covering a broad range of subject areas and depth of subject knowledge.
4.2 Obtaining quality assessment using Mechanical Turk
As mentioned above, it is often impossible to gather evaluative data about answers from the askers themselves, and that was the case here. Users of YA may create a profile, which may include an email or IM address, but the Yahoo! Quest dataset does not include any user profile data. It was, therefore, impossible to ask the asker how they had evaluated the various answers to their question. As a proxy for the original asker, we therefore used Amazon Mechanical Turk. MTurk has been used successfully to predict asker satisfaction, and indeed ratings from Turk workers have been shown to be more closely correlated with actual asker satisfaction than ratings from experts [8].
We created a Human Intelligence Task (HIT) on MTurk, in which Turk workers evaluated each of the 600 answers in our dataset according to the 13 quality criteria identified by [16] (discussed above), on a 5-point Likert scale. In particular, we presented a worker with a question-answer pair, and instructed the worker to rate the following 13 statements on scale of 1 to 5.
1. This answer provides enough information for the question. (informative)
2. This answer is polite (not offending). (polite) 3. This answer completely answers the whole question (rather
than a part of it). (complete) 4. This is an easy to read answer. (readable) 5. This answer is relevant to the question. (relevant) 6. The answer is concise (not too wordy). (brief) 7. I trust/believe this answer. (convincing) 8. The answer contains enough detail. (detailed) 9. I believe this answer is original (not copied from another
place). (original) 10. I believe the answer is objective and impartial. (objective) 11. The answer has new ideas or concepts that made me
somewhat surprised. (novel) 12. This answer is useful or helpful to address the question.
(helpful) 13. I believe this answer has been written by an expert. (expert)
Since these are subjective criteria, it is likely that different people have different levels of agreement while rating them. We, therefore, employed five different Turk workers to obtain ratings on each of these statements. Thus, we created a total of 3000 HITs. To reduce the order effect, we randomized the ordering of the above statements for each HIT.
In order to ensure that the worker who completed our HIT was an actual human and not a bot, we added a final question: "What is two plus two," and provided a text field for the answer. Human workers, of course, had no trouble answering this correctly, while bots supplied nonsensical responses that were easily filtered out. Workers took about 90 seconds on average to work on a HIT (read a question-answer pair and rate 13 different criteria).
4.3 Constructing a model with logistic regression
As described before, each worker on MTurk rated each of the thirteen quality aspects for an answer on scale 1 to 5. To convert

this rating to a two-class decision (`yes' or `no' for an aspect), we assumed that a rating of 3 or higher meant `yes', and a rating of 1 or 2 meant `no'. Thus, for a statement "This answer contains enough details.", if a worker rated 3 or higher, we considered his decision to be `yes' for the answer containing enough details to address the question. Since we had five different workers rating each answer, we used a simple voting scheme to arrive to their collective decision. We counted the votes for a factor to be `yes' and normalized it by the number of raters (5). Thus, if a factor received 4 `yes' decisions, it had 0.8 as its weight. Using such weighted aspects for each answer, we constructed a model for our dataset using logistic regression. In this model, the independent variables were the 13 aspects, and the dependent variable was the asker's decision about the quality of an answer. As described earlier, the dataset consisted of 120 questions with total of 600 answers. Once again, an answer was considered to be a high quality answer, if (1) the asker chose it as the best answer, and (2) gave it a rating of 3 or higher.

The model constructed with logistic regression is shown in Table 1.
As we can see, most factors do not contribute significantly in explaining the variability of the data. In fact, overall, pseudo R2 is reported to be 0.0902 with (p>2)=0.0000, which means only about
9% of the variability in the data could be explained by this model.

Table 1: Logistic regression model for 13 quality factors in predicting asker-rated quality of an answer.

Logistic regression Log pseudolikelihood = -265.40593

Number of obs =

Wald chi2(13) =

Prob > chi2

=

Pseudo R2

=

600 55.48 0.0000 0.0902

(Std. Err. adjusted for 120 clusters in qid)

rating

Robust Coef. Std. Err.

z P>|z|

[95% Conf. Interval]

informative polite
complete readable relevant
brief convincing
detailed original objective
novel helpful
expert _cons

.4429017 -.5958797
.8192924 -2.118669 -.6858092 -.7298307 -.0764392
1.299867 1.648606 -1.247356 2.298834 1.023509 -.9137472 -2.08764

.7485814 .7714024 .6996266 .8350697 .7688801 .7512585 .8450847 .7694943 .7450068 .7662476 .6831346 .7075909 .7107884 .8108309

0.59 -0.77
1.17 -2.54 -0.89 -0.97 -0.09
1.69 2.21 -1.63 3.37 1.45 -1.29 -2.57

0.554 0.440 0.242 0.011 0.372 0.331 0.928 0.091 0.027 0.104 0.001 0.148 0.199 0.010

-1.024291 -2.107801 -.5519505 -3.755375 -2.192787
-2.20227 -1.732775 -.2083146
.1884198 -2.749174
.9599153 -.363344 -2.306867 -3.67684

1.910094 .9160413 2.190535 -.4819622 .8211681
.742609 1.579896 2.808048 3.108793 .2544616 3.637754 2.410361 .4793725 -.4984407

This indicates the low quality of the model. We do see that aspects `novel', `original', and `readable' have significant impact on model's ability to predict the best answer. Doing further data mining revealed that losing aspects with high p>|z| values do not affect the performance of this model by much. Removing aspects `convincing' and `informative' reduced the power of this model by only a little, with pseudo R2=0.0896. However, this model is not very appropriate for doing feature selection since all of the aspects are strongly correlated with each other as shown in Table 2. This may also be the reason for the model not being able to explain much of the variability in the data.
Applying this model on the same data for testing, we achieved 80.33% classification accuracy. Doing a 10-fold cross-validation with this model resulted in classification accuracy of 79.50%, with almost all of the best answers classified in the wrong class. In fact, given the kind of data we have, where 4 out of 5 answers are not selected as the best answers, we could achieve 80.00% classification accuracy by simply declaring every answer to be not the best answer. It should be noted that in reality, a question on YA may receive many more answers than this, and thus, constructing a classifier to identify the best answer is extremely difficult.

414

Table 2: Pair-wise correlation of aspects. Significant relations (p<0.05) are indicated with `*'.

inform~e polite complete readable relevant brief convin~g

informative polite
complete readable relevant
brief convincing
detailed original objective
novel helpful
expert

1.0000 0.3916* 0.5369* 0.2464* 0.4421* 0.1422* 0.4743* 0.6342* 0.2795* 0.3651* 0.4219* 0.5501* 0.5144*

1.0000 0.4357* 0.3392* 0.4361* 0.2348* 0.4612* 0.4206* 0.3165* 0.4261* 0.3104* 0.4858* 0.3761*

1.0000 0.2453* 0.4151* 0.1831* 0.4606* 0.6125* 0.2403* 0.3996* 0.4035* 0.5737* 0.4650*

1.0000 0.3556* 0.3634* 0.3789* 0.2545* 0.3035* 0.2769* 0.1804* 0.3647* 0.1882*

1.0000 0.2531* 0.4968* 0.4414* 0.3784* 0.4560* 0.3386* 0.5121* 0.3491*

1.0000 0.2868* 0.1271* 0.3003* 0.2390* 0.1570* 0.2239* 0.1427*

1.0000 0.4969* 0.3389* 0.3812* 0.3450* 0.5068* 0.4198*

detailed original object~e novel helpful expert

detailed original objective
novel helpful
expert

1.0000 0.2413* 0.4233* 0.4454* 0.5240* 0.4998*

1.0000 0.3226* 0.2848* 0.2840* 0.2473*

1.0000 0.3009* 0.4262* 0.3262*

1.0000 0.3651* 0.4921*

1.0000 0.4723*

1.0000

Continuing to do data mining with this model to identify relative impact of individual aspects did not provide us any significant results. Any subset of these aspects could achieve a classification accuracy of about 80.00%, but this could simply be by chance given the nature of the data.
While the classifier built with this model was not highly successful, we did find the evidence of its correct functioning when we compared the probabilities computed for the instances of both the classes (`yes' and `no'). Table 3 shows the result of a two-tailed paired t-test between the probability distributions of `yes' class and `no' class. In the table, they are represented by `1' and `0' group respectively.
Table 3: Two-tailed paired t-test for probability distributions of `yes' and `no' classes.

As we can see, the mean probability for `no' class instances was 0.1734, whereas for `yes' class it was 0.2607. The test shows that the difference in these means was statistically significant. This indicates that even though the probabilities for class `yes' were not higher than 0.5 (for their correct classification), they were significantly higher than those for class `no'.
In summary, we learned that it is extremely hard to pick the best answer from a set of answers for a given question. This is due to large variability in the subject and the kind of questions asked, the asker's expectations, and the kind of answers given. In addition to this, only one out of many answers is selected as the best answer and it is very difficult to train a classifier using such training data, where not chosen answers may still be of good quality. We also realized that the 13 aspects of quality that we identified are highly correlated, but do not help us create a robust model for explaining the variability in the data, let alone predicting the best answer.
We did, however, find a high level of agreement among the workers while rating different aspects of answer quality (Figure 1). This informs us that while people have similar understanding or perception about the quality of an answer, their collective assessment was not enough to predict the decision of the asker.

Figure 1: Agreement among 5 different workers while rating aspects of answer quality.
This led us to believe that there may be other aspects or features that we need to consider. In the next section we describe how we automatically extracted such features and used them for evaluating and predicting answer quality.
5. PREDICTION MODEL USING AUTOMATICALLY EXTRACTED FEATURES
Now we will describe a set of experiments done using automatically extracted features from questions and/or answers. As we discussed in the previous section, we may have a common perception of what constitutes to the quality of an answer, but in case of CQA, there are many other variables that may contribute to an asker choosing an answer to be the best answer for his information need. In particular, the profile of the asker as well as the answerer may play an important role in determining if the asker would find an answer appropriate for his posed question or not. In this section we will show how to use such user profile information to build a prediction model.
5.1 Extracting QA features
We extracted the following features for each question and answer in our dataset:
· Length of the question's subject (qsubject) · Length of the question's content (qcontent) · Number of answers for the question (numanswers) · Number of comments for the question (numcomments) · Information from the asker's profile (q_points, q_level,
q_total_answers, q_best_answers, q_questions_asked, q_questions_resolved, q_stars) · Length of the answer's content (acontent) · Inclusion of references with the answer (reference) · Reciprocal rank of the answer in the list of answers for the given question (a_rr) · Information from the answerer's profile (a_points, a_level, a_total_answers, a_best_answers, a_questions_asked, a_questions_resolved, a_stars) A user's profile (asker or answerer) contained the following information about that user: number of questions asked, number of those questions resolved, number of questions answered, number of those answers chosen as the best answers, level achieved in YA (1 to 7), number of points earned, and number of stars received.

415

Some of the data points had to be deleted because we could not obtain one or more of the features (e.g., a user's profile taken down). This resulted in a total of 116 questions and 575 answers. Table 4 provides a summary of some of these features based on our dataset.

Table 4: Summary of various features used from questions, answers, and users (asker or answerer).

Feature

Min value Max value

Mean

Question

qsubject

12

110

54.92

qcontent

0

1030

195.46

numanswers

5

50

19.94

numcomments

0

3

0.15

Answer

acontent

2

10625

182.77

reference

0

1

0.05

a_rr

0.02

1.00

0.35

User (Asker/Answerer)

points

8

63785

521.35

level

1

7

1.19

stars

0

547

9.58

total_answers

0

16239

123.78

best_answers

0

2576

22.72

questions_asked

0

1071

26.47

questions_resolve d

0

920

25.64

We also found that these different features are not all significantly correlated as we found earlier with the quality aspects assessed by human workers. This indicates that not all the features extracted about questions and/or answers are geared toward predicting the quality of an answer.

Table 5: Logistic regression model for 21 automatically extracted QA features in predicting asker-rated quality of an
answer.

Logistic regression Log pseudolikelihood = -240.42392

Number of obs =

Wald chi2(21) =

Prob > chi2

=

Pseudo R2

=

575 938.18 0.0000 0.1562

(Std. Err. adjusted for 116 clusters in qid)

rating

Robust Coef. Std. Err.

z P>|z|

[95% Conf. Interval]

qsubject qcontent numanswers numcomments q_points
q_level q_total_an~s q_best_ans~s q_questi~ked q_questi~ved
q_stars acontent reference a_points
a_level a_total_an~s a_best_ans~s a_questi~ked a_questi~ved
a_stars a_rr
_cons

-.0038755 -.0007107
.017057 -.034717 -.0022464 .1517189 .0048579 .0234524 -.0354473 .0330828 .0008161 .0020836 .2374713 .0053807 -.4164522 -.0110888 -.0594911 .1146048 -.1071046 -.0251896 -2.906039 -.9648778

.0022726 .0003948 .0090116 .0907147 .0021258
.20077 .0049749 .0215382 .0260461 .0242647 .0049889 .0014807 .6453088 .0038277 .3714064 .0085822 .0397994 .0913479 .0871774 .0180339 1.122684 .5064439

-1.71 -1.80
1.89 -0.38 -1.06
0.76 0.98 1.09 -1.36 1.36 0.16 1.41 0.37 1.41 -1.12 -1.29 -1.49 1.25 -1.23 -1.40 -2.59 -1.91

0.088 0.072 0.058 0.702 0.291 0.450 0.329 0.276 0.174 0.173 0.870 0.159 0.713 0.160 0.262 0.196 0.135 0.210 0.219 0.162 0.010 0.057

-.0083297 -.0014845 -.0006055 -.2125146 -.0064128
-.241783 -.0048927 -.0187616 -.0864966 -.0144751 -.0089619 -.0008185 -1.027311 -.0021215 -1.144395 -.0279096 -.1374966 -.0644339 -.2779692 -.0605353
-5.10646 -1.95749

.0005787 .0000632 .0347195 .1430805
.00192 .5452208 .0146085 .0656664 .0156021 .0806407 .0105941 .0049857 1.502253 .0128829 .3114909 .0057321 .0185143 .2936435
.06376 .0101561 -.7056184
.027734

Table 6: Two-tailed paired t-test for probability distributions of `yes' and `no' classes.

5.2 Constructing a model with logistic regression
Using these 21 features, we constructed a model with logistic regression; similar to what we did with human assessed 13 quality aspects reported earlier. This model is presented in Table 5.
As we can see, the model is quite good in terms of its power to explain the variability in the data. Since pseudo R2=0.1562 with statistical significance, we could say that more than 15% of the variability in the data can be attributed to the factors used here.
The model was successful 84.17% times in selecting the best answer on the same training set, which was better than the prediction accuracy obtained from human assessments of quality factors. Doing 10-fold cross-validation resulted in 81.74% accuracy, which was once again better than the one achieved using human rated judgments.
Similar to the earlier model, we performed a two-tailed paired ttest between the probability distributions of the `yes' or the `1' class and the `no' or the `0' class. The test results are reported in Table 6. Once again, we find that even though the probabilities for class `yes' are not higher than 0.5 on average, they are significantly higher than those of for `no'.

From Table 5, it appears that number of comments, existence of references with an answer, and the stars received by the asker are not helping much in the prediction. Once again, data mining with this model revealed that removing these three features retains the model's ability to select the best answer without much loss (pseudo R2=0.1404 and prediction accuracy=83.83%). Extending this further, we could get the most parsimonious model by having a_rr (reciprocal rank of an answer) only in the model (pseudo R2=0.0823 and prediction accuracy=80.34%).
Not very surprisingly, features extracted from questions only do not help in prediction at all (Table 7, with significance reported to be 0.9594), whereas features extracted from answers only achieve quite high power (Table 8) with pseudo R2=0.1386 and statistical significance. Once again, we find that the single most important feature that contributes to predicting if an answer would be selected as the best answer or not is the reciprocal rank of that answer (a_rr).
Finally, we performed likelihood-ratio test on the models constructed with 13 human-assessed aspects and 21 automatically extracted features. We found that both the models were significantly different (2=17.29, p=0.0040). This shows that the latter model was successful in outperforming the former one with statistical significance.

416

Table 7: Logistic regression model for 11 automatically extracted question features in predicting asker-rated quality
of an answer.

Logistic regression Log pseudolikelihood = -284.91394

Number of obs =

Wald chi2(11) =

Prob > chi2

=

Pseudo R2

=

575 4.33 0.9594 0.0001

(Std. Err. adjusted for 116 clusters in qid)

rating

Robust Coef. Std. Err.

z P>|z|

[95% Conf. Interval]

qsubject qcontent numanswers numcomments q_points
q_level q_total_an~s q_best_ans~s q_questi~ked q_questi~ved
q_stars _cons

-8.33e-06 .0000279
-.0007592 -.0209016 -8.62e-06
.0156938 .0000205 .0000674 -.0001519 .0001205 -.0000234 -1.403788

.0009781 .0000349
.003645 .0326157 .0000264 .0094876 .0000413 .0004137 .0006919 .0006348 .0000971 .1206554

-0.01 0.80
-0.21 -0.64 -0.33
1.65 0.50 0.16 -0.22 0.19 -0.24 -11.63

0.993 0.423 0.835 0.522 0.744 0.098 0.619 0.871 0.826 0.849 0.810 0.000

-.0019253 -.0000404 -.0079033 -.0848271 -.0000603 -.0029016 -.0000605 -.0007435
-.001508 -.0011238 -.0002136 -1.640268

.0019086 .0000963 .0063848
.043024 .0000431 .0342892 .0001016 .0008784 .0012042 .0013647 .0001669 -1.167308

Table 8: Logistic regression model for 10 automatically extracted answer features in predicting asker-rated quality of
an answer.

Logistic regression Log pseudolikelihood = -245.44934

Number of obs =

Wald chi2(10) =

Prob > chi2

=

Pseudo R2

=

575 444.51 0.0000 0.1386

(Std. Err. adjusted for 116 clusters in qid)

rating

Robust Coef. Std. Err.

z P>|z|

[95% Conf. Interval]

acontent reference
a_points a_level
a_total_an~s a_best_ans~s a_questi~ked a_questi~ved
a_stars a_rr
_cons

.0017766 .3128832 .0031013 -.2672806 -.0062671 -.0350234 .0741423 -.0684709 -.0262627 -2.859143 -.9574806

.0012478 .6102943 .0021472 .2239349 .0046254 .0229574 .0724542 .0692063 .0208062 1.118054
.40994

1.42 0.51 1.44 -1.19 -1.35 -1.53 1.02 -0.99 -1.26 -2.56 -2.34

0.155 0.608 0.149 0.233 0.175 0.127 0.306 0.322 0.207 0.011 0.020

-.000669 -.8832717 -.0011071
-.706185 -.0153326
-.080019 -.0678654 -.2041127
-.067042 -5.050488 -1.760948

.0042222 1.509038 .0073096 .1716237 .0027985 .0099723 .2161499 .0671709 .0145166 -.6677986 -.1540131

5.3 Additional training and testing
Encouraged by the success of automatically extracted features for predicting answer quality, we extended our experiments to include another set of answers. Using YA APIs,11 we randomly extracted 5032 answers from YA, associated with 1346 questions in different categories. We extracted all the 21 features reported earlier for each question-answer pair and constructed a logistic regression model. The model had pseudo R2=0.1312, and gave 84.72% classification accuracy on the same data (Figure 2). When tried with 10-fold cross-validation, the model gave 84.52% accuracy, showing its robustness for classification.
When we used this model that we constructed with 5032 datapoints for training and the previous model with 575 data-points for testing, the training model achieved 80.35% accuracy (Figure 3). This, once again, demonstrates the robustness of the selected features and the models constructed using them.

Figure 2: Result of classification on the training data. 11 http://developer.yahoo.com/answers/

Figure 3: Result of classification on the test data.
All of the classification results reported in the present and the previous section are summarized in Table 9. It is clear that the models constructed with QA features (automatically extracted) tend to give reliable results on training as well as testing data, indicating their robustness.

Table 9: Summary of various classification models.

# Model

Training Testing

Classification accuracy

1 Quality aspects 600

Same data 80.33%

(human assessed) samples

2 Quality aspects (human assessed)

600 samples

10-fold crossvalidation

79.50%

3 QA features (automatically extracted)

575

Same data 84.17%

samples

4 QA features (automatically extracted)

575 samples

10-fold crossvalidation

81.74%

5 QA features (automatically extracted)

5032

Same data 84.72%

samples

6 QA features (automatically extracted)

5032 samples

10-fold crossvalidation

84.52%

7 QA features (automatically extracted)

5032

Data from 80.35%

samples model 3

6. CONCLUSION
Measuring quality of content in community question-answering (CQA) sites presents a unique set of issues and challenges. As with many other IR problems, evaluating content quality is a significant challenge in CQA, and may require us to go beyond the traditional notion of "relevance" as suggested by Saracevic [11]. To address this challenge of evaluating answer quality, we used 13 different criteria to assess the overall quality of an answer on Yahoo! Answers (YA) site. We assumed that an answer is the best for a given question, if (1) the answer is chosen by the asker as the best, and (2) the asker gives it a rating of 3 or higher. This gave us a gold standard against which we could compare our models for evaluating and predicting answer quality. With the help of Amazon Mechanical Turk workers, we discovered that people have a good understanding and high level of agreement over what constitutes as a good quality answer. We also found that different aspects of the overall quality of an answer, such as informativeness, completeness, novelty, etc., are highly correlated. However, when these features were used for creating a model for predicting the best quality answers, we found it

417

limiting. The human assessors were given the answers without any context (other than the question); they did not know who asked the question or who answered it, nor did they have any information about the conditions under which a question was asked or an answer provided. We realized that such information is critical in evaluating content quality in CQA.
We, therefore, extracted several features of the questions, the answers, and the users who provided them from YA. Via model building and classifying experiments, we discovered that indeed, the answerer's profile, as measured by the points earned on YA (social capital in that community), and the order of the answer in the list of answers for a given question, as measured by the reciprocal rank of that answer, are the most significant features for predicting the best quality answers. We demonstrated the robustness of our models by doing cross-validations and applying the trained models to a larger test set.
Beyond developing models to select best answers and evaluate the quality of answers, there are several important lessons to learn here for measuring content quality in CQA. It was pointed out, above, that there is huge variety in the kind of questions and answers found on CQA services, and that a given question may receive several answers from the community.12 Given that only one of these answers can be picked by the asker as the best answer, it is extremely hard to create a classifier that outperforms random selection, based on a priori information about the data. However, with appropriate features, we could build models that can have significantly higher probability of identifying the best answer in the `yes' class than of classifying a non-best answer in that class.
CQA provides unique opportunities to consider social factors and other contextual information, while evaluating its content. For instance, the placement of information, as well as the profile or social capital of its producer are some of the data that may help in better prediction and evaluation of content quality. A great many CQA sites exist, with different asking, answering, and rating mechanisms; and we believe that there are several other possible features of CQA questions and answers worth exploring. Future work will benefit from the unique issues presented here while evaluating and predicting content quality in CQA.
7. ACKNOWLEDGMENTS
We are grateful to anonymous workers of Amazon Mechanical Turk service for providing us valuable human assessments for the answers that we used here. We are also thankful to Yahoo! for making their QA datasets available to us.
8. REFERENCES
[1] Dervin, B. (1998). Sense-making theory and practice: An overview of user interests in knowledge seeking and use. In Journal of Knowledge Management, 2(2), 36-46.
[2] Gazan, R. (2008). Social annotations in digital library collections. D-Lib Magazine, 11/12(14). Available from http://www.dlib.org/dlib/november08/gazan/11gazan.html.
[3] Harper, M. F., Raban, D. R., Rafaeli, S., & Konstan, J. K. (2008). Predictors of answer quality in online Q&A sites. In
12 Using more than a million questions from YA, we have found that on average a question receives about 6 answers.

Proceedings of the 26th Annual SIGCHI Conference on Human Factors in Computing Systems (pp. 865-874). New York: ACM.
[4] Janes, J. (2003). The Global Census of Digital Reference. In 5th Annual VRD Conference. San Antonio, TX.
[5] Kim, S., Oh, J-S., & Oh, S. (2007). Best-Answer Selection Criteria in a Social Q&A site from the User Oriented Relevance Perspective. Proceeding of the 70th Annual Meeting of the American Society for Information Science and Technology (ASIST `07), 44.
[6] Kresh, D. N. (2000). Offering High Quality Reference Service on the Web: The Collaborative Digital Reference Service (CDRS). D-Lib Magazine, 6.
[7] Lee, J. H., Downie, J. S., & Cunningham, S. J. (2005). Challenges in cross-cultural/ multilingual music information seeking. In Proceedings of the 6th International Society for Music Information Retrieval (pp. 1-7). London, UK.
[8] Liu, Y., Bian, J., & Agichtein, E. (2008). Predicting Information Seeker Satisfaction in Community Question Answering. Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval.
[9] Pomerantz, J. (2008). Evaluation of Online Reference Services. Bulletin of the American Society for Information Science and Technology, 34(2), 15-19. Available from http://www.asis.org/Bulletin/Dec-07/pomerantz.html.
[10] Pomerantz, J., Nicholson, S., Belanger, Y., & Lankes, R. D. (2004). The Current State of Digital Reference: Validation of a General Digital Reference Model through a Survey of Digital Reference Services. Information Processing & Management, 40(2), 347-363.
[11] Saracevic, T. (1995). Evaluation of evaluation in information retrieval. Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval (pp. 138-146). Seattle, USA.
[12] Shah, C., Oh, J. S., & Oh, S. (2008). Exploring characteristics and effects of user participation in online social Q&A sites. First Monday, 13(9). Available from http://www.uic.edu/htbin/cgiwrap/bin/ojs/index.php/fm/articl e/view/2182/2028.
[13] Shah, C., Oh, S., & Oh, J-S. (2009). Research Agenda for Social Q&A. Library and Information Science Research, 11(4), 205-209.
[14] Su, Q., Pavlov, D., Chow, J., & Baker, W. (2007). Internetscale collection of human- reviewed data. In C. L. Williamson, M. E. Zurko, P. E. Patel-Schneider, & P. J. Shenoy (Eds.), Proceedings of the 16th International Conference on World Wide Web (pp. 231-240). New York: ACM.
[15] Voorhees, E. M (2003). Overview of the TREC 2003 question-answering track. In TREC 2003.
[16] Zhu, Z., Bernhard, D., & Gurevych, I. (2009). A Multidimensional Model for Assessing the Quality of Answers in Social Q&A Sites. Technical Report TUD-CS-2009-0158. Technische Universität Darmstad.

418

Adaptive Near-Duplicate Detection via Similarity Learning

Hannaneh Hajishirzi
University of Illinois 201 N Goodwin Ave
Urbana, IL, USA
hajishir@uiuc.edu

Wen-tau Yih
Microsoft Research One Microsoft Way Redmond, WA, USA
scottyih@microsoft.com

Aleksander Kolcz
Microsoft One Microsoft Way Redmond, WA, USA
ark@microsoft.com

ABSTRACT
In this paper, we present a novel near-duplicate document detection method that can easily be tuned for a particular domain. Our method represents each document as a real-valued sparse k-gram vector, where the weights are learned to optimize for a specified similarity function, such as the cosine similarity or the Jaccard coefficient. Near-duplicate documents can be reliably detected through this improved similarity measure. In addition, these vectors can be mapped to a small number of hash-values as document signatures through the locality sensitive hashing scheme for efficient similarity computation. We demonstrate our approach in two target domains: Web news articles and email messages. Our method is not only more accurate than the commonly used methods such as Shingles and I-Match, but also shows consistent improvement across the domains, which is a desired property lacked by existing methods.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; I.2.6 [Artificial Intelligence]: Learning
General Terms
Algorithms, Experimentation, Performance, Reliability
Keywords
Near-duplicate Detection, Similarity Learning, Spam Detection
1. INTRODUCTION
Inspired by the needs of many real-world tasks when handling large document collections, near-duplicate detection (NDD) has been an important research problem for more than a decade [4, 6, 10, 20]. In many scenarios, two documents that are not exactly identical may still contain the same content and should be treated as duplicates. However, which portion of the documents should be considered important in such comparison depends on the final
This work was done while the author was an intern at Microsoft Research.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

application and may vary from task to task. For example, Web pages from different mirrored sites may only differ in the header or footnote zones that denote the site URL and update time [7, 17]. News articles shown on different portals could come from the same source (e.g., Associated Press) and thus have identical content, but can be rendered in different site templates with advertisements [20]. In both cases, a search engine should not show these near-duplicate documents together since they carry identical information. In a plagiarism detection scenario, the definition of near-duplicate documents may be even looser. When a portion of one document, such as a sentence or a paragraph, is contained in another document, these two documents could be seen as near-duplicates. In contrast, perhaps the most extreme definition of a near-duplicate exists in an anti-adversarial scenario. Spam messages that belong to the same campaign may look very different because spammers often need to randomize the messages by obfuscating terms or adding unrelated paragraphs to pass the filter [13, 14, 16]. However, as long as the core payload text (e.g., a URL pointing to the spammer's site) is identical, two email messages are treated as near-duplicates.
There are two main considerations when solving an NDD problem: efficiency and accuracy. Applications of NDD typically need to handle a very large collection of documents. A practical algorithm will need to determine whether a document is a duplicate of some other documents in the repository in real-time [17]. As a result, efficiency has been the main focus of existing popular NDD approaches, where various techniques of generating short signatures using hash functions (e.g., [4, 2, 5]) and pruning inverted index searching (e.g., [20]) were invented. In contrast, the problem of how to improve NDD accuracy has received less attention. Most existing NDD methods encode documents as sets of token sequences (i.e., k-grams). In such binary vector representation, all the elements, no matter where the corresponding fragments come from (e.g., header, body or navigation block) of the document, are treated equally when comparing two documents. Although heuristic approaches have been developed for selecting important terms using IDF values (e.g., I-Match [6]) or special patterns based on stopwords (e.g., SpotSigs [20]), such approaches do not perform consistently across different domains. As a result, manually tuning the configuration or even selecting the right NDD method becomes a necessity to achieve acceptable accuracy for the target domain.
In this paper, we introduce a novel Adaptive Near-Duplicate Detection (ANDD) method to address the above issues. Given a small set of labeled documents that denote the near-duplicate clusters, ANDD learns a vector representation for documents in the target domain. Each element in the vector corresponds to a short token sequence (i.e., k-gram) associated with a real-valued weight that indicates its importance when determining whether two document are near-duplicate. Similarity scores, such as cosine or Jaccard, calcu-

419

lated based on this new vector representation provide higher nearduplicate detection accuracy. Such improvement does not come at the cost of sacrificing the computational efficiency, as established signature generation techniques can be easily adapted to reduce the dimensionality of the original vector representation.
Our contributions in this work are twofold. First, by observing that existing NDD approaches consist of two main steps, raw vector construction and fast similarity computation, we identify the former is critical to achieve better accuracy. Hashing schemes or truncated inverted index provide fast ways to approximate the similarity score calculated using the raw vectors. Theoretically, the prediction performance is upper bounded by the quality of the original representation. As we show experimentally, the accuracy difference between different hashing schemes is often limited and much less than the gain brought by a better document representation. Second, we extend a recently proposed similarity learning framework [21] to learn the vector construction. This allows ANDD to easily capture the implicit and often vague notion of "near-duplicate" through the editorial data for the target domain. As a result, comparable and often higher accuracy can be achieved compared to existing NDD methods, and such advantage is consistent across domains.
2. RELATED WORK
In this section, we briefly describe the representative NDD approaches with focus on their unique characteristics.
2.1 Shingling
As one of the earliest NDD methods, the Shingling [4, 2] algorithm views each document as a sequence of tokens and first encodes a document as a set of unique k-grams (i.e., contiguous subsequences of k tokens). For the ease of further processing, each k-gram is encoded by a 64-bit Rabin fingerprint [19] and is called a shingle. The similarity between two documents is measured using the Jaccard coefficient between the shingle vectors. Documents with high similarity scores are considered near-duplicate.
One issue of applying the Jaccard similarity directly is the variable and potentially large size of the shingle sets, as they grow linearly in the number of document tokens. Broder et al. [4] introduced the technique of min-wise independent permutations to solve this problem by mapping each set of shingles to an m-dimensional vector, with m typically much smaller than the original number of tokens in a document. In this process, m different hash functions h1, · · · , hm are created and applied to all shingles. Let the set of shingles of the target document d be S(d) = {s1, s2, · · · , sn}. The j-th element in the final vector is defined as the minimum hash value of hj . Namely, minl{1..n} hj (sl).
Each m-dimensional vector can be further mapped to a smaller set of super shingles by first separating the elements into m disjoint subsets of equal size and then fingerprinting each subset of elements using a different hashing function. This process effectively reduces the dimensionality of each vector from m to m , and thus saves the storage space and also speeds up the computation.
Notice that in the standard shingling methods, the construction of document signature vectors is purely syntactic ­ all the k-grams in the documents are treated equally. Alternatively, Hoad and Zobel [11] experimented with various strategies of selecting k-grams when encoding shingles, such as based on their TFIDF scores.
2.2 I-Match
Unlike Shingling, which creates a sequence of hash values based on a random sample of k-grams of the original document, I-Match maps each individual document into a single hash value using the SHA1 hash algorithm. Two documents are considered near-duplicate

if and only if their hash values are identical [6]. The signature generation process of I-Match views a document as a single bag of words (i.e., terms, unigrams). In addition, only the "important" terms are retained in the bag. It first defines an I-Match lexicon L based on collection statistics of terms using a large document corpus. A commonly used option is the inverse document frequency (IDF), where L consists of only terms with mid-range IDF values. For each document d that contains the set of unique terms U , the intersection S = L  U is used as the set of terms representing d for creating the signature.
One potential issue with I-Match occurs when the retained word bag S is small (i.e., |S| |U |). Because the documents are effectively represented using only a small number of terms, different documents could be mistakenly predicted as near-duplicates easily. To deal with such cases, a constraint is placed on the minimum length of a document for which a valid signature can be produced. To make I-Match more robust to such false-positive errors, Kolcz et al. propose using m randomized lexicons concurrently instead of one, following the same I-Match signature generation process [15, 14]. As a result, an m-dimensional vector is used to present a document. Two documents are considered near-duplicate only if they have enough number of signatures matched.
2.3 Random projection
As pointed out by Charikar [5], the min-wise independent permutations method used in Shingling is in fact a particular case of a locality sensitive hashing (LSH) scheme introduced by Indyk and Motwani [12]. The probability that the two hash values match is the same as the Jaccard similarity of the two k-gram vectors. In contrast, the random projection based approach proposed by Charikar [5] and later applied to the web document domain by Henzinger [10] is another special LSH scheme for the cosine similarity based on term (i.e., unigram) vectors. According to the implementation described in [10], this algorithm generates a binary vector with m bits to represent documents using the following steps. First, each unique term in the target document is projected into an m-dimensional real-valued random vector, where each element is randomly chosen from [-1, 1]. All the random vectors generated from the terms in this document are then added together. The final m-dimensional binary vector representing this document is derived by setting each element in the vector to 1 if the corresponding real value is positive and 0 otherwise.
Noticing that the Shingling method often generates more falsepositive cases, Henzinger invented a hybrid approach that applies Charikar's random projection method to the potential near-duplicate pairs detected by Shingling [10]. As a result, the precision is significantly improved without sacrificing too much recall.
2.4 SpotSigs
In determining which k-grams in a document should be used for creating signatures, Theobald et al.'s SpotSigs method is perhaps the most creative and interesting one [20]. When developing nearduplicate detection methods for clustering news articles shown on various Web sites, they observe that stopwords seldom occur in the unimportant template blocks such as navigation sidebar or links shown at the bottom of the page. Based on this observation, they first scan the document to find stopwords in it as anchors. k tokens right after an anchor excluding stopwords are grouped as a special k-gram, or so called a "spot signature" in their terminology. The raw representation of each target document is therefore a set of spot signatures. To some extent, the construction of spot signatures can be viewed as a simple and efficient heuristic to filter terms in template blocks so that the k-grams are extracted from the main

420

content block only. Once the spot signatures have been extracted, the same techniques of using hash functions as seen in other NDD methods can be directly applied to reduce the length of the spot signature vectors. In addition, Theobald et al. propose an efficient algorithm for directly computing the Jaccard similarity measures on the raw spot signature vectors, with the help of a pruned inverted index data structure [20].

3. ADAPTIVE NDD
Although their algorithm designs seem different, all the NDD methods surveyed in Sec. 2 can be described within a unified framework that consists of two main steps: (1) generating k-gram vectors from documents and (2) computing the similarity score efficiently based on the desired function operating on the vectors. Two documents are considered near-duplicate if their similarity score is above a predefined threshold. In order to have an efficient NDD method with improved prediction accuracy, our strategy is to follow the same pipeline but to ensure that the raw k-gram vector is a good representation of the document for computing reliable similarity scores. It will be a real-valued vector where the weights of the active k-grams are learned from labeled document pairs.
There are two main differences in our approach, compared to most existing NDD algorithms. First, each k-gram in the vector is associated with a real weight, which is used in computing document similarity scores. In contrast, existing methods typically decompose documents into bags of k-grams (i.e., binary vectors). Although some weighting schemes (e.g., removing terms based on idf values) may be used to retain only a subset of k-grams, all the active k-grams are treated equally during signature generation and/or similarity computation. As in many NDD applications, not all terms in the same document are equally important (e.g., some may occur in the title; others may appear in the navigation sidebar), treating terms indistinguishably important will obviously impact the accuracy of the NDD predictions [6].
The second difference of our approach is that we leverage labeled data in the target domain to learn these weights. Unlike existing NDD methods, which are more or less static, the learning approach provides a principled way to adjust the model to better fit the target domain. Specifically, we extend a recently proposed term-weighting learning framework [21] by using general k-gram vectors instead of just unigrams and by optimizing for different similarity functions such as the Jaccard coefficient. In what follows, we describe the learning approach in detail and demonstrate how the better document representation can be encoded to signatures that support efficient similarity computation.

3.1 Learning real-valued k-gram vectors
We first formally define the raw document representation in our approach, the real-valued k-gram vector. Let V = {g1, g2, · · · , gn} be the vocabulary that contains all possible k-grams occurring in all documents. Each document d is mapped to a sparse vector v, which consists of all the k-grams G  V that can be found or selected in d. For each k-gram g  G, its score is decided by a function that d¯eipsetnhdesmonodthelepka-rgarmametegrsanleda/ronretdhefrdoomcutmheelnatbde:legdwd¯a(tga., d), where
Conceptually, this weighting function indicates how important the k-gram is with respect to the document, when computing the similarity between two vectors. While there are many choices of the functional form, we use a simple linear combination of features extracted for each k-gram gi occurring in document d:

gw¯ (gi, d) = j · j (gi, d),

(1)

j

Feature
Bias TF DF DF-Avg DF-Med QF Loc Len Cap InTitle InURL

Note
1 for all examples Frequency of the k-gram in the document Document frequency of the k-gram Average of document frequencies of terms in the k-gram Median of document frequencies of terms in the k-gram Query log frequency of the k-gram The location the k-gram in the document Length of the document Whether the first term of the k-gram is capitalized Whether the k-gram appears in the title or email subject Whether the k-gram is part of the URL of the document

Table 1: Features used in ANDD. DF-Avg and DF-Med are only used when k > 1. InURL is used only for HTML documents.

where j is the j-th feature function and j is the corresponding model parameter. The goal of the training procedure is thus to determine ¯ so that two near-duplicate documents can have a high similarity score.
The learning framework enables a principled way of incorporating additional information regarding the raw text in features. For each k-gram, the features used are listed in Table 1. Note that these features are all very easy to compute. Some are statistics derived from scanning the whole document once (e.g., Loc and Len). Others can be retrieved from a direct table look-up (e.g., DF and QF).
Given two documents dp and dq, their similarity score is given by a specified similarity function fsim operating on their corresponding k-gram vectors vp and vq. We develop models for two commonly used similarity functions in the task of NDD, cosine and the (extended) Jaccard coefficient in this work.

cos(vp, vq) =

vp · vq vp vq

(2)

jacc(vp, vq) =

vp · vq vp 2 + vq 2 - vp · vq

(3)

Having human subjects score the importance of each k-gram that leads to a robust similarity measure is a difficult annotation task. Instead, we assume that we are given clusters of documents as the labeled data. Documents belonging to the same clusters are nearduplicate and unrelated otherwise. How to use such labeled documents to train the model parameters depends on the learning setting. We choose the setting of learning the preference ordering because of its slightly superior performance reported previously [21]. In this setting, the absolute similarity score of two documents is not important. Instead, we would like the model to assign higher scores to near-duplicate documents, compared to other unrelated pairs of documents. Due to the lack of space, we omit the derivations of gradients for both cosine and Jaccard functions here.
A training set of N examples in this setting is formally denoted as {(y1, (xa1 , xb1 )), (y2, (xa2 , xb2 )), · · · , (yN , (xaN , xbN ))}, where xak = (dpak , dqak ) and xbk = (dpbk , dqbk ) are two pairs of documents and yk  {0, 1} indicates the pairwise order preference, where 1 means xak should be ranked higher than xbk and 0 otherwise. The loss function we used for this setting is:

N
L(¯) = log(1 + exp(-yk · k - (1 - yk) · (-k))), (4)
k=1
where k is the difference of the similarity scores of two document pairs, computed based on the corresponding vectors. Namely,

k = fsim(vpak , vqak ) - fsim(vpbk , vqbk )

The

loss

function

(Eq.

4)

is

regularized

by

adding

a

term

 2

||w||2

.

421

We use L-BFGS to minimize the loss function for its guarantee to find a local minimum. We omit the derivations of gradients for both cosine and Jaccard functions due to lack of space.
3.2 Generating document signatures
In many applications, the NDD method needs to handle a large collection of documents. Therefore, being able to efficiently determine whether the similarity score of two documents is high enough to declare they are near-duplicate is not only crucial but also a requirement to a practical NDD algorithm. Based on the traditional vector space model and cosine similarity, one simple way to detect near duplicates is to sequentially submit each document in the collection as a query to search for highly similar documents in the collection. By the use of inverted index as adopted in most search engines, this kind of document search is sub-linear (to the number of documents in the collection) in time complexity in practice, however it can be O(n2) in the worst case.
Recall that the weights in each vector can be interpreted as the importance scores of the corresponding k-grams, the straightforward way to shrink the vector size is thus by eliminating k-grams with low weights. Efficient similarity computation can be supported by techniques like pruned inverted index [20]. While this could be an effective strategy as shown in our experiment (see Sec. 4.2.2), having variable sizes of document representation is less appealing in terms of engineering. Meanwhile, although lowweight k-grams are not as important, they still contain information that could affect the similarity measure. In contrast, signature generation techniques map vectors into strings of a fixed number of bits. Even with the same complexity order, the basic bit comparison operation is much faster than comparing whether two strings (i.e., terms) or two integers (i.e., term id) are identical. Moreover, efficient sub-linear algorithms exist for similarity search in the signature space [5, 12, 1].
In this paper, we rely on locality sensitive hashing (LSH) schemes to map the raw vector to a sequence of hash values as the document signature. An LSH scheme has the following defining property:
DEFINITION 3.1 ([5, 12]). Let fsim(·, ·) be a given similarity function defined on the collection of objects O. A distribution on a family H of hash functions operating on O is a locality sensitive hashing scheme if for x, y  O,
ProbhH[h(x) = h(y)] = fsim(x, y)
Using this scheme, hash functions h1, h2, · · · , hm drawn from H are applied to raw vectors to encode them into signatures of m hash values. The similarity score of two documents is derived by counting the number of identical hash values, divided by m. As m increases, this scheme will approximate asymptotically the true similarity score given by the specific function fsim. Since the similarity functions that our learning method optimizes for are cosine and Jaccard, we apply the corresponding LSH schemes when generating signatures.
For the cosine function, we use the random hyperplane based hash function, which is the essential part of Charikar's random projection algorithm [5]. For a given collection of vectors in Rd, each hash function is created by first choosing a random vector r¯ from the d-dimensional Gaussian distribution. When applied to a vector u¯  Rd, the corresponding binary hash function hr¯ returns 1 if the dot product r¯ · u¯  0; otherwise it's 0. For vectors u¯ and v¯, this LSH scheme has the following property:
Prob[hu¯ = hv¯] = 1 - cos-1(u¯, v¯)/,
which has a monotonic mapping of the cosine function.

When applying this scheme to our k-gram vectors, each k-gram in the vocabulary is associated with m different random numbers drawn from the Gaussian distribution. The signature of each vector/document is a bit-string of m bits. The value of the i-th bit is decided by the sign of summing the product of the i-th random number and the weight of each k-gram. Notice that this scheme works for both binary and real vectors, and the number of bits (i.e., m) does not need to increase when handling real vectors.
For the Jaccard function, the LSH scheme that we use is the min-hash [12, 8] function, which are designed originally for binary vectors. To handle our real k-gram vectors, we first transfer each real-valued weight to a binary vector as suggested by Gionis et al. [8]. The weight of each k-gram in the vector is multiplied by a big integer and then the number is bucketed and mapped to a bit-string. The original real vector thus becomes a binary vector by concatenating these bit-strings.
It is impractical to choose a hash function h uniformly among all the possible functions. Therefore, we limit the search among a specific class of functions (linear in our experiments) as suggested by [3, 20]. Each hash function hi in this family is defined by two random numbers i and i that are smaller than the length of the mapped binary vectors. Let X be the set of indices of the "1" bits in vector u¯. The i-th hash value of this vector is defined as hi(u¯) = minxX (i · x + i mod p) where p is the first prime number bigger than the length the mapped binary vector. Similarly, a complete document signature consists of m such min-hash values, and the Jaccard coefficient is approximated by the fraction of identical hash values in the corresponding signature vectors.
4. EXPERIMENTS
In this section, we compare our near-duplicate detection methods with other state-of-the-art approaches on two important tasks: Deduping Web news articles and email campaign detection. Although in both tasks, the goal to identify semantically identical documents is the same, the syntactic presentation of near-duplicates in fact differs significantly from one domain to the other. As we demonstrate, our approach can adapt to the target domain easily while existing methods often suffer from such domain change and their predictions cannot be consistently reliable. Other practical issues when applying our method, such as the number of training examples it needs and whether it increases the computational cost during runtime, will be discussed at the end of this section.
4.1 De-duping Web news articles
In this set of experiments, we focus on determining whether news articles shown on different sites are identical and thus are potentially from the same source. As observed by Theobald et al. [20], Web sites showing these articles have their own unique style or template. These news pages may differ substantially in their presentation and only the core content can decide whether they are near-duplicate articles. As a result, correctly distinguishing the important portion of the text and encoding such information in the document representation become a key to improve the accuracy for this task. Below, we start from describing the data used for this set of experiments and the detailed experimental settings. We then compare our method with others in detail.
4.1.1 Experimental setting
The dataset we used in this task is the gold set of near-duplicate news articles collected by Theobald et al. [20], which contains 2,160 news articles crawled in 2006. These articles are manually clustered into 68 directories, where documents in the same directory have the same content news and are considered near-duplicate.

422

ANDD-Raw TFIDF Binary
SpotSigs

Unigram (k = 1) Cosine Jaccard

0.956 0.884 0.861
0.953

0.952 0.874 0.852
0.952

Trigram (k = 3) Cosine Jaccard

0.936 0.875 0.869
-

0.910 0.873 0.867
-

Table 2: MAX F1 of ANDD-Raw k-gram vectors with k  {1, 3} using the cosine and Jaccard similarity functions, compared to other k-gram weighting functions (binary and TFIDF) in the News domain.

We use this dataset for a five-run experimental setting. In each run of the experiment, we divide these clusters randomly into two disjoint sets of equal size; one set of clusters of documents is used for training and the other is used for testing. In other words, no cluster has documents in both the training and testing sets.
We construct our training instances as pairs of documents drawn from the training set. Each instance is associated with a binary label. If two documents are from the same cluster and thus nearduplicates, then the label is positive; otherwise, it's negative. Notice that the number of negative instances in the training set is much more than the number of positive instances. We balance the class distribution by randomly selecting the same number of negative instances as positive instances from the current set. 80,000 random pairs of these instances are used to train the model.
Likewise, testing instances are all the pairs of documents created from the testing set. To mimic the true distribution, we do not balance the number of positive and negative instances as done previously and evaluate various methods using all the testing instances. In particular, we evaluate the accuracy by the number of these instances with their labels correctly predicted. Following [20], we report the averaged Max F1 scores of our experimental results as the major evaluation metric.
4.1.2 Raw similarity measure
We first show the quality of the similarity measures computed on the k-gram vectors that include all the k-grams, when used for detecting near-duplicate documents. As discussed previously, existing NDD methods essentially approximate the raw similarity measure efficiently by either using the hashing trick or inverted index. In other words, the quality of the raw similarity score dictates the final NDD prediction accuracy.
We compare several configurations of the raw similarity measures based on three variables: k  {1, 3} (i.e., unigram or trigram), similarity function fsim  {cosine, jaccard} and the kgram weighting function, which can be binary, TFIDF or learned (denoted as ANDD-Raw). Table 2 presents the averaged Max F1 scores by varying the decision threshold when using the similarity measures of all the configurations. As can be observed from the table, regardless of the choice of k and the similarity function, ANDD-Raw yields a better similarity measure for near-duplicate detection. Their Max F1 scores are statistically significantly better than their TFIDF counterparts. Not surprisingly, without pruning k-grams using heuristics such as idf values, the raw similarity measure based on binary vector representation leads to the lowest Max F1 scores in our experiments. Again, the differences are statistically significant1. We also found that on this dataset, the cosine function performs slightly better than the Jaccard coefficient,
1We run a student's paired-t test on individual scores from the five rounds of the compared two configurations. The results are considered statistically significant when the p-value is lower than 0.05.

1

0.95

Averaged Max F1 Score

0.9

0.85

0.8

0.75

0.7 100

ANDD-LSH-Cos ANDD-Raw-Cos
1000

Number of Bits
Figure 1: Averaged Max F1 scores of ANDD-LSH-Cosine for the News domain when encoded using different numbers of bits.

although the differences are not statistically significant. This phenomenon is shown on both unigram (k = 1) and trigram (k = 3) representations, where using unigram derives better NDD results. It is also worth noticing that with a small set of simple features, our approach can perform comparable to SpotSigs, which is based on a heuristic designed specifically for this problem domain.

4.1.3 Signature generation
While the raw similarity measures derived from ANDD-Raw k-gram vectors can achieve highly accurate results, reducing the size of such document representation is essential to efficient computation. In this section, we first apply locality sensitive hashing schemes described earlier to map unigram vectors to signatures with different lengths and examine the degree of performance degradation in terms of NDD accuracy. We then compare our approach with other signature-based NDD algorithms.
Taking the unigram ANDD-Raw vectors where the weights are learned to optimize for the cosine function, we map each document to a signature of m bits via the LSH-Cosine scheme. The similarity score of two documents is then determined by the number of identical bits their corresponding signatures share, divided by m. We vary m from 64 to 4096 and report the averaged Max F1 scores of the 5 rounds in Figure 1. As can be observed from the figure, the accuracy of the NDD prediction based on the bit-string signatures improves quickly as the number of bits (i.e., m) increases. The performance degradation compared to the raw vector representation is limited when m is not too small. In fact, when m  512, the difference in Max F1 when compared to either the raw vector or the SpotSigs method is not statistically significant. In practice, the choice of m is determined by the trade-off between the efficiency constraint and the desired prediction accuracy, as longer signatures take more storage space and processing time.
We conduct similar experiments applying the LSH-Jaccard scheme to the unigram ANDD-Raw vectors where the weights are learned to optimize for the Jaccard function. Each vector is mapped to a set of m min-hash values. Because each min-hash value takes roughly 20 bits to store, to get roughly the same size of signatures, we vary m from 8 to 256 and report the averaged Max F1 scores of the 5 rounds in Figure 2. Compared with the LSH-Cosine scheme, LSHJaccard is less stable and also needs a larger size of signature to approximate the raw similarity measure. In this set of experiments, we found that the signature needs to have at least 80 min-hash values to achieve a Max F1 score that is not statistically significantly different from the one derived from the original Jaccard score.
We next compare our approach with other signature-based NDD methods, including Shingles, Charikar's random projection algorithm and I-Match. Following the setting chosen by Henzinger [10],

423

1

0.95

Averaged Max F1 Score

0.9

0.85

0.8

0.75

0.7 10

ANDD-LSH-Jacc ANDD-Raw-Jacc
100

Number of min-hash functions
Figure 2: The averaged Max F1 scores of ANDD-LSH-Jaccard for the News domain when encoded using different numbers of min-hash functions.

Method
ANDD-LSH-Cos ANDD-LSH-Jacc
Shingles Charikar 1-Shingles I-Match

IDF
[0,1] [0,1] [0.2,0.85] [0.2,0.85] [0.2,0.85] [0.4,0.75]

Bytes
48 200 672 48 48 24

Prec
0.965 0.957 0.886 0.868 0.994 0.983

Rec
0.923 0.907 0.867 0.777 0.013 0.001

F1
0.943 0.931 0.876 0.820 0.026 0.002

Table 3: Results of signature-based NDD methods for the News domain. ANDD-LSH-Cos achieves the best Max F1 despite its short signature length.

we use 384 bits (48 bytes) to encode the signatures in ANDD-LSHCos and Charikar's random projection method, and represent each document using 84 shingles (672 bytes) and 6 super shingles (48 bytes) for the Shingles and 1-Shingles methods. Because the unigram model performs better than trigram model, we set the window size in Shingles to 1. For ANDD-LSH-Jacc, each signature consists of 80 min-hash values (200 bytes) since it gives the best trade-off between accuracy and storage based on Figure 2. I-Match uses only one hash-value for each document and the length is 24 bytes.
Table 3 summarizes the performance of these algorithms in terms of the averaged Max F1 scores of the five rounds, as well as the corresponding precision and recall numbers. Among them, ANDDLSH-Cos achieves the best Max F1 score despite the fact that its signature length is relatively short. The Jaccard-based counterpart achieves pretty competitive result, but with longer signatures. A regular Shingles method with 84 hash values performs better than Charikar's random projection approach with 384 bits on this domain. Both have Max F1 scores higher than 0.8. Surprisingly, despite having impeccable precision, both 1-Shingles (at least 1 of the 6 super shingles matches) and I-Match achieve extremely low recall. Consequently, the F1 scores are also low. This might be due to the fact that we judge the accuracy using all document pairs, rather than whether each document is put in the same cluster.
Notice that the additional features other than TF and DF significantly boost the performance. Even though the Max F1 scores of ANDD-LSH are lower than ANDD-Raw, they are still much higher than those derived from raw TFIDF vectors (see Table 2).
4.2 Detecting email campaigns
Another application of near-duplicate document detection is capturing email campaigns, which has been shown an effective technique for collaborative spam filtering and identifying abused email accounts [9, 18, 15, 13]. An email campaign is a large volume

of semantically identical mail sent to a large number of recipients within a short period of time. Although some of these campaigns may be legitimate marketing newsletters, a big portion of them are in fact spam. To a large email service provider, such as Hotmail or GMail, as long as there are a few users reporting junkmail messages, all other messages from the same campaign can be quickly classified as spam and removed from the inboxes of their recipients.
Unfortunately, reliably detecting spam campaigns is not trivial. In order to escape from such spam filtering scheme, spammers typically randomize their messages by obfuscating words or phrases in the email subject or body, or by appending random, unrelated paragraphs at the end of the mail. Figure 3 shows an example of two messages from the same campaign. As we can see here, the notion of "near-duplicate" in this domain becomes quite different from the Web domain ­ two dissimilar messages may in fact come from the same campaign because they convey the same payload content.
4.2.1 Experimental setting
To judge the effectiveness of different NDD algorithms when applied to a practical problem such as spam campaign detection, it is best to evaluate them in a realistic setting. We experiment with our approach using a set of 11,108,298 outbound messages served by Hotmail, randomly sampled during the period between Dec 10th, 2008 and Jan 24th, 2009. Detecting outbound email campaigns serves two purposes: reducing outbound spam and capturing abused accounts. Because legitimate marketing campaigns are usually sent from the same sender, duplicate messages sent by multiple unrelated users can therefore be reliably classified as spam and intercepted by an outbound spam filter. In addition, accounts that send these messages are very likely to be abused or hijacked by adversaries. Detecting email campaigns in outbound messages can help find and suspend these compromised accounts.
Given the large number of messages, one difficulty we encountered is how to manually find and label campaign messages in this collection. In order to efficiently construct the "gold" set of email campaigns, we first ran both I-Match and Shingling on this dataset. Two messages were treated near-duplicate and put in the same cluster (i.e., campaign) if either I-Match or Shingling predicted so. As a result, we derived 31,512 initial clusters in total. Not surprisingly, email campaigns detected in this way were not totally correct and contain quite a few false-positive and false-negative cases. To further clean the campaign labels, we randomly selected 875 clusters with at least two messages and corrected their labels manually. Since in reality, a spam detection system can only be trained using historical messages, we mimicked this scenario by splitting the clusters based on the time their messages were sent. 400 clusters (2,256 email messages) sent between Dec 10th, 2008 and Jan 5th, 2009 were used for training, and the remaining 475 clusters (658 email messages) sent between Jan 5th, 2009 and Jan 24th, 2009 were used for testing.
4.2.2 Raw similarity measure
Table 4 shows the results of the similarity measures based on the unigram vectors. Contrary to the experiments on the news data, here we found that the Jaccard coefficient performs better than the cosine function. As for the comparisons against other weighting schemes, when the weights are learned using labeled data, the quality of the raw similarity measure is still better than TFIDF, which is again better than using the binary vector representation.
Interestingly, we also found that the NDD accuracy of SpotSigs drops sharply on this dataset. After carefully examining the unigrams it selects, we realized that although its heuristics of selecting the first non-stop words after some anchor stopwords works great

424

Subject: Outstaindng prcies and huge disuconts
sowftare you've ever watned www.freesoftwarepark.com _____________ sowftare you've ever watned www.freesoftwarepark.com Invite your mail contacts to join your friends list with Windows Live Spaces. It's easy! Try it!

Subject: Dolwnoadable software, 80% discounts
sofwtare you've ever watned www.computercodepark.com _____________ sofwtare you've ever watned www.computercodepark.com Get news, entertainment and everything you care about at Live.com. Check it out!

Figure 3: Two email messages from the same campaign.

ANDD-Raw TFIDF Binary
SpotSigs

Cosine
0.674 0.625 0.622 0.257

Jaccard
0.700 0.626 0.622 0.258

Table 4: MAX F1 of ANDD-Raw k-gram vectors with k = 1 using the cosine and Jaccard similarity functions, compared to other k-gram weighting functions (binary and TFIDF) in the Email domain.

0.7

Method
ANDD-LSH-Cos ANDD-LSH-Jacc
Charikar Shingles 1-Shingles I-Match

IDF
[0,1] [0,1] [0,1] [0,1] [0,1] [0.4,0.75]

Bytes
48 160 48 672 48 24

Prec
0.723 0.821 0.972 0.752 0.827 0.874

Rec
0.599 0.532 0.425 0.502 0.410 0.382

F1
0.656 0.646 0.591 0.602 0.548 0.532

Table 5: Results of signature-based NDD methods for the Email domain. ANDD-LSH-Cos achieves the best Max F1 despite its short signature length.

0.6

0.5

Max F1 Score

0.4

0.3

0.2

0.1
0 0

ANDD TFIDF SpotSigs

10

20

30

40

50

Number of Active Terms

Figure 4: Max F1 vs. the number of active terms in the Email domain for ANDD, TFIDF, and SpotSigs.

on the Web news crawls, it is much less effective on email for two reasons. Email messages are typically shorter than news articles and are written somewhat informally. As a result, stopwords occur less frequently, and each email consists only 5.7 Spot signature in average. In addition, we hypothesize that the SpotSigs heuristic effectively removes the terms in the template blocks such as ads or navigation bar, which typically don't exist in email. It is therefore not clear whether SpotSigs can capture the main content any more.
To validate these conjectures, we conduct an experiment to test the performance degradation when removing some unigrams of the vectors based on their importance, judged by the weighting score. Figure 4 shows the Max F1 scores given by the cosine similarity based on ANDD-Raw and TFIDF vectors with different numbers of active unigrams in them. Although the averaged number of active unigrams is 53.7, the performance of our method does not suffer much even when the number of active unigrams is restricted to 10. When the number of active unigrams is 5, the Max F1 scores of ANDD and TFIDF are 0.621 and 0.525, respectively. Both outperform SpotSigs substantially.

4.2.3 Signature generation
Table 5 reports the prediction accuracy of various NDD methods in terms of Max F1, along with the corresponding precision and recall. Although the Jaccard similarity on the ANDD-Raw unigram vectors performs better than the cosine version, such advantage is not fully carried after dimensionality reduction using the LSH scheme. As a result, ANDD-LSH-Cos achieves slightly

higher Max F1 score than ANDD-LSH-Jacc, and both outperform Charikar's random projection method. Due to the selection bias introduced when sampling email messages for annotation, it is not entirely fair to compare our approach with Shingles or I-Match. We list the results of these algorithms here for reference only.
4.3 Discussion
As shown previously, our ANDD approach not only outperforms existing methods, but can also adapt to a different domain better. Its advantages are mainly due to learning from the labeled training documents and from extracting more information from documents. In what follows we discuss how these two extra needs may impact the deployment of our algorithm in practice.
One natural question for a learning approach such as ours is how many training documents are needed to achieve satisfactory accuracy. To answer this question, we repeat our experiments using the news data set by reducing the number of training documents and record the corresponding Max F1 scores. Although in our regular setting we use 1,005 labeled documents in average for training, surprisingly we found that the algorithm can in fact train a model that achieves the asymptotic result even with a small number of labeled documents. Figure 5 shows the learning curve of the model on unigrams. The initial model of random parameter values gives a 0.856 Max F1 score, which is worse than the cosine similarity based on binary unigram vector (see Table 2). With as few as 60 documents, it already improves the Max F1 score to 0.953, which is very close to 0.956, the Max F1 score of our regular model. Such a sharp learning curve may be due to the fact that we are learning only a few model parameters (23 in these experiments). However, we want to point out that manually tuning these model parameters is still difficult and may not be much better than the initial random model weights (from preliminary experiments not reported here). Also, adding more training documents still improves the performance although the gain is relatively small.
Another practical concern of deploying our approach is the runtime efficiency. Thanks to the effective LSH scheme, our approach can encode documents in short bit-string signatures that preserve the good accuracy result of the raw vectors. The storage requirement of our method remains the same and standard indexing techniques applied to other NDD approaches can be adapted here as well (notice that training the k-gram weighting model is done of-

425

0.96

0.94

0.92

Averaged Max F1 Score

0.9

0.88

0.86

0.84

0.82
0.8 0

ANDD Init Full

50

100

150

200

250

300

Number of Training Documents

Figure 5: Max F1 vs. number of training samples compared to the random initialization (init) and fully learned model (full). Accuracy is only lowered slightly as the number of training examples decreases.

fline). The only place that our algorithm may need slightly more computational time is feature extraction. Fortunately, we can already achieve good performance based on straightforward features listed in Table 1. Among them, the document frequency and query log frequency can be found using a simple table lookup operation. Other features such as term frequency and term position can be determined when the parser or tokenizer finishes scanning the document. Therefore, such additional computation cost should be negligible. Of course, adding more domain-dependent features is usually the most effective way to improve a learning system in practice. Whether more such features should be added will depend on the trade-off between the computational cost of extracting such features and its potential benefits to the final NDD prediction accuracy.

5. CONCLUSIONS
In this paper, we presented a novel adaptive near-duplicate detection (ANDD) method that achieves high accuracy consistently across different target domains. Observing that the raw document representation dictates the prediction accuracy of an NDD algorithm, we extend a recently proposed term-weighting framework [21] to learn general k-gram vectors and to optimize for a different similarity function such as the Jaccard coefficient. Each document is represented by an informative real k-gram vector. Similarity measures computed on these vectors can reliably predict near-duplicate documents. To the best of our knowledge, our method is the first that introduces similarity learning to the NDD problem. As demonstrated by the experiments done on two different problem domains, the Web news dataset and outbound email messages, our method can easily leverage a small number of labeled near-duplicate document clusters, and provide more accurate prediction results without tedious parameter tuning for the target domain. When this approach is applied to large-scale problems, efficiency is preserved by mapping the vector representation to short signatures with the help of effective locality sensitive hashing schemes.
In the future, we would like to explore different possibilities of feature engineering and model improvement to further enhance our approach. For instance, lexical features such as the words contained in each k-gram can be easily added. Information from deeper document analysis modules, if can be efficiently executed during runtime, will be used for feature extraction as well. Applying our NDD method to more applications for Web search and online advertising is also on our agenda.

6. ACKNOWLEDGMENTS
We thank Chris Meek and Susan Dumais for many useful discus-

sions. We are also grateful to Martin Theobald for sharing the data and the SpotSigs package. We also would like to thank anonymous reviewers for their helpful comments.
7. REFERENCES
[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the ACM, 51(1):117­122, 2008.
[2] A. Z. Broder. Identifying and filtering near-duplicate documents. In COM '00, pages 1­10. Springer-Verlag, 2000.
[3] A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher. Min-wise independent permutations. Journal of Computer and System Sciences, 60:630­659, 2000.
[4] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic clustering of the web. Comput. Netw. ISDN Syst., 29(8-13):1157­1166, 1997.
[5] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the 34th Annual ACM Symposium on Theory of Computing, 2002.
[6] A. Chowdhury, O. Frieder, D. Grossman, and M. C. McCabe. Collection statistics for fast duplicate document detection. ACM Trans. Inf. Syst., 20(2):171­191, 2002.
[7] D. Fetterly, M. Manasse, and M. Najork. On the evolution of clusters of near-duplicate web pages. In LA-WEB '03, 2003.
[8] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In VLDB '99, 1999.
[9] R. J. Hall. A countermeasure to duplicate-detecting anti-spam techniques. Technical report, AT&T, 1999.
[10] M. Henzinger. Finding near-duplicate web pages: a large-scale evaluation of algorithms. In SIGIR '06, pages 284­291, New York, NY, USA, 2006. ACM.
[11] T. C. Hoad and J. Zobel. Methods for identifying versioned and plagiarized documents. Journal of the American Society for Information Science and Technology, 54(3):203­215, 2003.
[12] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of 30th STOC, pages 604­613, 1998.
[13] A. Kolcz and A. Chowdhury. Hardening fingerprinting by context. In CEAS '07, 2007.
[14] A. Kolcz and A. Chowdhury. Lexicon randomization for near-duplicate detection with I-match. Journal of Supercomputing, 45(3):255­276, 2008.
[15] A. Kolcz, A. Chowdhury, and J. Alspector. Improved robustness of signature-based near-replica detection via lexicon randomization. In KDD '04, 2004.
[16] D. Lowd and C. Meek. Good word attacks on statistical spam filters. In CEAS'05, 2005.
[17] G. S. Manku, A. Jain, and A. Das Sarma. Detecting near-duplicates for web crawling. In WWW '07, 2007.
[18] V. V. Prakash and A. O'Donnell. Fighting spam with reputation systems. Queue, 3(9):36­41, 2005.
[19] M. Rabin. Fingerprinting by random polynomials. Report TR-1581, Harvard University, 1981.
[20] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs: robust and efficient near duplicate detection in large web collections. In SIGIR '08, pages 563­570, 2008.
[21] W. Yih. Learning term-weighting functions for similarity measures. In Proc. of EMNLP-09, 2009.

426

The Good, the Bad, and the Random: An Eye-Tracking Study of Ad Quality in Web Search

Georg Buscher
DFKI Knowledge Management Dept. Kaiserslautern, 67663, Germany
georg.buscher@dfki.de

Susan Dumais
Microsoft Research One Microsoft Way Redmond, WA 98052 USA
sdumais@microsoft.com

Edward Cutrell
Microsoft Research India 196/36 2nd Main, Sadashivnagar
Bangalore, 560 080, India
cutrell@microsoft.com

ABSTRACT
We investigate how people interact with Web search engine result pages using eye-tracking. While previous research has focused on the visual attention devoted to the 10 organic search results, this paper examines other components of contemporary search engines, such as ads and related searches. We systematically varied the type of task (informational or navigational), the quality of the ads (relevant or irrelevant to the query), and the sequence in which ads of different quality were presented. We measured the effects of these variables on the distribution of visual attention and on task performance. Our results show significant effects of each variable. The amount of visual attention that people devote to organic results depends on both task type and ad quality. The amount of visual attention that people devote to ads depends on their quality, but not the type of task. Interestingly, the sequence and predictability of ad quality is also an important factor in determining how much people attend to ads. When the quality of ads varied randomly from task to task, people paid little attention to the ads, even when they were good. These results further our understanding of how attention devoted to search results is influenced by other page elements, and how previous search experiences influence how people attend to the current page.
Categories and Subject Descriptors
H.1.2 [Models and Principles] User/Machine Systems ­ Human information processing, Human factors .
General Terms
Design, Experimentation, Human Factors, Measurement.
Keywords
Gaze tracking, user study, search engine results pages
1. INTRODUCTION
In designing effective search systems, it is important to understand how people search and interact with the information presented on search engine result pages (SERPs). In this paper we use an eye-tracking study to increase our understanding of the processes that people use in examining result pages, and of variables that influence these processes.
Previous studies have used eye-tracking to understand how people attend to and interact with different elements of SERPs. This work has developed well-known terms to describe typical gaze
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

Figure 1: Gaze heat map on a search engine results page.
distributions on SERPs, such as the "golden triangle" [12] or the "F-shaped pattern" [18]. Figure 1 shows an example of a characteristic heat map for a SERP. These studies tend to be fairly high-level, with qualitative descriptions aggregated across many different pages or tasks. Other researchers have taken a more controlled experimental approach and reported quantitative summaries of eye movements on SERPs, often explicitly controlling users' tasks. These studies characterized how visual attention is distributed on the 10 organic results, e.g., [6], [9], [10], [16]. However, all of today's major commercial search engines include additional elements on a SERP such as sponsored links at the top and on the right rail, related searches, graphical elements such as maps, illustrations, or other content. In this study we seek to understand how the visual attention devoted to organic results is influenced by these other page elements.
Sponsored links are an especially important component of the SERP since they form the main source of income for search engines. Depending on the search intent of the user, ads may provide valuable information and lead searchers directly to their goal. In contrast, if ads are off-topic or simply not relevant to the

42

immediate goal, they run the risk of annoying or distracting users, perhaps even impeding completion of their search.
The main goal of this paper is to study factors that might influence how users distribute their visual attention on different components on a SERP during Web search tasks. While we examine visual attention on most components typically present on a SERP, we are especially interested in sponsored links. Applying eye-tracking techniques, we determine basic differences in gaze distribution due to ad quality and task type. In addition we examine the effects of the sequence in which ads of good or poor quality are presented. Sequence effects reflect how prior search experiences influence behavior on the current search task.
After presenting an overview of related research, we describe the experimental design for our eye-tracking study. We then provide an analysis and a discussion of factors influencing the visual attention to SERP components, with special attention to sponsored links. We conclude with a summary of the implications of the results and some directions for future research.
2. RELATED WORK
2.1 Web Search Behavior in General
Several factors including the quality of the results and their presentation, the type of search task, and individual differences have been shown to influence search behaviors and success.
Search interactions are influenced by the quality of the search results, although the relationships are often weak when measured using total time or overall search success [22]. Similarly, Smith & Kantor [21] find that the search success is the same for both good and degraded systems, but that users alter their strategies depending on the quality of the results.
In the context of Web search, Broder [2] and Rose & Levinson [20] describe three general classes of user goals informational, navigational and resource or transactional. These different search goals lead to different search success, with users being faster and more successful for navigational tasks in general [6], [9]. The influence of query frequency and the popularity of search goals have also been analyzed by Downey et al. [7]. They find that searchers are more successful for common queries and common goals, and also if the frequency of the query matches the frequency of the user's information need. In addition, caption features such as the occurrence of query terms in the title of a result entry significantly influence whether searchers choose to view a result [5].
There is a large body of work on individual differences in search behavior. White and colleagues summarize this work and report findings from large-scale log studies in which they find that search experts [24] and domain experts [23] are more successful and employ different search strategies than novices.
2.2 Eye Tracking on SERPs
Previous research has used eye-tracking studies to understand in detail how searchers examine search results.
Joachims et al. [16] show that the way in which searchers examine a SERP is influenced by the position and relevance of the results. Searchers have a strong bias towards result entries at higher positions on the SERP. Pan et al. [19] and Guan & Cutrell [10] have also reported similar findings.
Cutrell & Guan [6] look in more detail at how eye movements are influenced by the snippets for search results. They find that longer snippets lead to better search performance for informational tasks, but degrade performance for navigational tasks.

Aula et al. [1] find two different types of searchers exhaustive and economic searchers. Exhaustive searchers examine a SERP thoroughly and look up and down the SERP several times before choosing a result entry to click on. In contrast, economic users sequentially look from the top to the bottom and click on the first relevant result entry they notice (see also [9]).
2.3 The Influence of Ads
The previously mentioned eye-tracking studies focus on eye movement behavior on the 10 organic results. However, SERPs typically contain many additional elements including sponsored links, spell suggestions, related queries, rich snippets, etc. Yet there is very little research about the influence of these components on the search behavior and success.
Most of the available research work focuses on sponsored links, which can account for 10% to 23% of all links presented on a SERP [11], depending on the search engine and query. Fallows [8] reports that in 2005 only 38% of searchers were aware of the concept of sponsored links at all, and only 12% could reliably differentiate between sponsored links and organic results.
Jansen et al. [14] analyze factors relating to clicks on sponsored links. They conducted a study in which participants had to interact with SERPs that had 10 organic results and some textual ads on the right side. They find that 51% of the users only look at organic results and completely ignore the ads. (However, this was determined with think-aloud techniques rather than eye tracking.) In addition, users typically examine ads if they do not find an answer to their task on the initially viewed organic results. In general, they report a considerable bias against ads, even when controlling for their quality.
Interestingly, Jansen [13] finds that summary snippets of ads presented by commercial Web search engines are usually as relevant as summaries of organic results. Further, Jansen & Spink [15] report that seamlessly integrating ads with the organic results (i.e., making a differentiation between them impossible) does not increase their click-through rate. Finally, as shown by Yan et al. [25] behavioral targeting of ads can drastically improve clickthrough rates.
In summary, search behavior is influenced by individual user characteristics, the type of search task at hand, and the relevance of the search results. We extend this work by analyzing visual attention to the full range of elements in contemporary SERPs. We also systematically vary and examine the effects of ad quality. Finally, we study the dynamics of user attention and behavior by varying the order in which ads of different quality are presented.
3. METHODS
We use eye-tracking as an instrument to provide detailed information about the user's visual attention. It is common for eye-tracking studies to take gaze position as a proxy for visual attention. Thus, gaze tracking can provide data leading to valuable insights about search strategies and processes.
3.1 Experimental Design and Procedure
We designed an eye-tracking experiment in which participants had to complete a number of given search tasks using a Web search engine. We were interested in effects of:
- task type (i.e., informational or navigational), - elements on search engine results pages (SERPs), most
importantly the quality of the ads, and - the order in which SERPs containing ads of good or bad
quality were presented to a participant.

43

Table 1: Examples of task descriptions and initial queries used for the study.

Task Description
How much optical zoom does the compact digital camera Sony Cyber-Shot W230 have?
Find the special offers page for Southwest Airlines.
Find the official Web site of the Venetian casino in Las Vegas. How many guest rooms does the Bellagio hotel in Las Vegas have? What are some side-effects of Ibuprofen? Go to NikeStore on the official Nike homepage.

Initial Task Query
sony cyber shot W230

Task Type Info

southwest

Nav

special offers

las vegas casino Nav venetian

bellagio las

Info

vegas rooms

ibuprofen side Info effects

nike shoes

Nav

Tasks Every participant had to solve the same set of 32 search tasks. Half of the tasks were navigational (i.e., they had to find a specific Web page) and half were informational (i.e., they had to find factual information). All of the tasks were of a commercial nature so that ads would be a realistic component of the SERPs.
Each task had a description telling the participants what they should look for. In order to make the initial SERP comparable across participants, we provided them with an initial query for each task. Some examples of task descriptions and the corresponding initial task queries are given in Table 1.
We cached results for each initial query. This allowed us to have a consistent initial set of results for each task which we knew included a solution to the task in a fixed position. For 24 (75%) of the tasks, the static first SERP contained a solution within the top 3 organic results, for 6 tasks (19%), a solution could be found in positions 4-6, and for 2 tasks (6%), a solution was after position 6.
After the initial SERP was presented, participants were free to proceed as they wished. They could click links, view the next page of results, or re-query. The combination of an initial fixed SERP and full search functionality provides a good balance between experimental control and search realism for a laboratory study.
Initial task query

Good quality ads

Bad quality ads
Figure 3: Example of good and bad quality ads for the same initial task query.

Upper search b3otxop ads

Related searches
(optional)

5 right rail ads

...

10 organic results

Pagination Lower search box
Figure 2: SERP layout

Ad Quality
In the study, each SERP contained 3 ads at the top and 5 ads on the right rail (see Figure 2). For each SERP, all of the displayed ads were either of good or of bad quality. Figure 3 shows an example of 3 good quality and 3 bad quality ads for the query "Ibuprofen side effects". Across participants, each task was shown equally often with good quality ads or bad quality ads.
The good ads were selected from the ads shown by commercial Web search engines such as Bing, Google, and Yahoo! in response to the initial task queries. The bad ads were selected from the same commercial Web search engines by generating queries using a subset of the terms occurring in the initial task queries. This matching method is responsible for some types of bad matches observed in practice. Since the bad ads were generated from query terms, they contained highlighted terms making them visually similar to the good ads. For the determination of good and bad ads we only consider topicality, but no other factors such as the reputation of the sponsor, etc.
SERP Elements and SERP Generation
The layout of the SERPs was modeled after a commercial Web search engine. As depicted in Figure 2, a SERP contained the following important elements
- an upper and lower search box, - 10 organic results (not containing any special elements like
maps, videos, images, or deep links), - 3 top ads, - 5 right rail ads, and - related searches on the left rail for queries for which they were
available (20 of the 32 initial queries contained related searches).
To generate the SERP for a query, we implemented our own search interface shown in Figure 2. For the initial task query the interface showed a locally cached version of the first SERP for the query. For any other user-generated query, the interface queried a commercial Web search engine in the background, took the

44

1 23 2 26 3 19 4 9 5 6 6 14 7 11 8 5 9 20 10 10 11 12 12 21 13 30 14 27 15 15 16 16 17 24 18 32 19 1 20 7 21 9 22 28 23 4 24 17 25 22 26 8 27 31 28 2 29 18 30 13 31 25 32 2

Task number Trial number

Condition: GB Ad quality Block
Condition: BG Ad quality Block
Condition: RR Ad quality Block

gbgggggg bgbbbbbb gbgggggg bgbbbbbb

G

B

G

B

bgbbbbbb gbgggggg bgbbbbbb gbgggggg

B

G

B

G

gbbgggbb bgbbgbgg gbbggbbg gbgbbgbg

R

R

R

R

Figure 4: Experimental variables. Each sequence of randomly assigned tasks is performed in 1 of 3 conditions (BG, GB, RR). The sequence conditions determine when the SERPs contain good (g) or bad (b) quality ads.

organic results and the related searches (if any), inserted ads, and displayed them using our modified interface layout.
For each task, we had a pool of good quality and of bad quality ads. The static first SERPs for the initial task queries always contained the same ads from the appropriate pool. For subsequent queries, ads from the appropriate pool were randomly selected and integrated into the SERP at runtime.
Trial Sequences To study sequence effects, we controlled the order in which SERPs with good or bad ad quality appeared within the sequence of 32 tasks. In the following, we introduce some terminology for describing how the task sequences were created (see Figure 4).
A trial is one unit of the experiment starting from reading the task description until completing the task. There were 32 trials in an experiment, one trial for each task. For each trial, we specified which task to solve and whether to show only good or only bad quality ads on the SERPs for that task.
The experiment was divided into 4 blocks, of eight consecutive trails. There are three types of blocks - Good (G), Bad (B) or Random (R). A good quality block (G) contains 8 trials with mostly good ad quality, whereas a bad quality block (B) contains 8 trials with mostly bad ad quality. To make the blocking effect less obvious to the participants, the ad quality in the second trial of each G or B block is reversed. In Figure 4, a lower case g or b is used to delineate a trial containing only good (g) or only bad (b) ads. Random blocks (R) consist of half good and half bad ad quality trials (randomly distributed within an R block). The only constraint on the random selection was that, across all participants, each task should be performed using good quality ads around the same number of times as using bad quality ads. For all trials in all conditions, all of the ads displayed on a SERP were either of good (g) or of bad (b) quality.
Each participant was assigned to one of 3 conditions GB, BG, or RR. Each condition contains 4 blocks of trials GBGB, BGBG, and RRRR, respectively (see Figure 4). Thus every participant performed 16 tasks with SERPs showing good quality ads and 16 tasks with SERPs showing bad quality ads.
The order of the tasks in a 32 trials sequence was randomly assigned. Each unique task sequence was performed in all 3 conditions by 3 different participants. It is important to note that the participants saw 32 trials without any special delineation of the block structure or the quality of the ads. The blocks and presentation conditions are for analysis purposes.

Summary of Independent Variables
To summarize, the main independent variables for each trial were
- Task type (informational/navigational) - Quality of the ads (good/bad) shown on the SERPs - Block (G/B/R) the trial belongs to - Condition (GB/BG/RR) the participant was assigned to
Procedure
After a short introduction to the study, the eye tracker was calibrated using a 5 point calibration. Then, the participants started with one practice task to illustrate the procedure and continued in the same way for the remaining 32 tasks.
For each task, we provided the participants with a written task description and the corresponding initial query. After reading the description and the query aloud, the participants pressed a search button to begin searching using the initial query. The first SERP was always the locally stored, static page containing ads of the appropriate quality for that trial. From here on, participants were free to interact with search results. To solve the task, they had to navigate to an appropriate Web page and point out the solution on it to the experimenter. After finding a solution, they had to answer the question "How good was the search engine for this task?" (5point Likert scale).
After completing the example task and all 32 main tasks like this, the participants had to fill in a study questionnaire asking about their Web search experience and practices during the study and in general. The experiment took about one hour per participant.
3.2 Apparatus
The experiment was performed on a 17" LCD monitor (96 dpi) at a screen resolution of 1280x1024 pixels. We used the browser Internet Explorer 7 with a window size of 1040x996 pixels. With this setting, the page fold was usually between the organic results at positions 6 and 7. For gaze tracking, we applied a Tobii x50 eye tracker which has a tracking frequency of 50 Hz and an accuracy of 0.5° of visual angle. Logging of click and gaze data was done by the software Tobii Studio.
3.3 Participants
Thirty-eight participants produced valid eye-tracking data (out of 41). Participants were recruited from a user study pool. They ranged in age between 26 and 60 years (mean = 45.5,  = 8.2), and had a wide variety of backgrounds and professions. 21 participants were female and 17 were male.
For the 38 participants, we generated 13 unique task sequences. 13 participants were assigned to the GB condition, 13 to BG, and 12 to RR. Overall, we got valid eye-tracking data for 1210 trials.
3.4 Measures
For our analysis, we wanted to know how visual attention and clicks are distributed among different elements of the SERP. Therefore, we assigned gaze and click data to areas of interest (AOIs) on the SERPs.
AOIs
Since all SERPs presented during the study had the same kinds of elements, we created common areas of interest. All regions labeled in Figure 2 were defined as AOIs. For the top ads, the right rail ads, and the organic results, we introduced further AOIs matching the different result entries (i.e., separate AOIs for each of the 10 organic results, for the 3 top ads and for the 5 right rail ads). In addition, each of those result entries contained 3 AOIs matching the title, the summary text snippet, and the URL.

45

Figure 5: Mean fixation impact on SERP elements in milliseconds (including standard errors of the mean).
Fixation Impact fi(A)
Fixations were detected using built-in algorithms of Tobii Studio. The algorithms generate a fixation if recorded gaze locations of at least 100ms are close to each other (radius 35 pixels).
We used the measure fixation impact fi(A) to determine the amount of gaze an AOI A received. This measure was introduced by Buscher et al. [4] and is a modified version of simple fixation duration. Fixation duration assigns the entire duration to the AOI(s) that contain the center point of the fixation, but the fixation impact measure spreads the duration to all AOI(s) close to the fixation center using a Gaussian distribution. Thus, fixation impact prorates the duration of a fixation to all AOIs that are projected on the foveal area of the eyes.
Clicks c(A)
We also count the number of clicks on any links on the SERPs (e.g., organic results, top and right rail ads, related searches, etc.). c(A) specifies the number of clicks aggregated for the AOI A. For example, for the AOI top_ads spanning all 3 top ads, c(top_ads) would be the sum of clicks on any of the 3 top ads.
Time on SERP t
Finally, for each participant and task, time on SERP t measures the time the participant spent on the first static SERP for a given task. This time includes all views of the first static SERP, not only the time to first click.
4. RESULTS
For this analysis we focus on several aspects of gaze on SERPs. First, we want to determine differences in visual attention (on AOIs) with respect to task type. Second, we are interested in the difference in visual attention on good and on bad quality ads on SERPs. Third, we focus on sequence effects of presenting good or bad ads in different orders.
We concentrate our gaze-based analysis on the static first SERPs for the initial queries we provided to the participants. Since the same static first SERPs are viewed by each participant (either with good or with bad quality ads displayed), we are confident that all participants are looking at exactly the same information, and comparability is ensured. Gaze on the first SERP represents 88% of the total gaze on all SERPs, and 32% of total task time (with the remainder of the time spent on reading Web pages).
4.1 General Gaze Distribution on SERPs
We start our analysis with a general overview of the distribution of visual attention on SERP components. Figure 1 shows a gaze heat map depicting the distribution of visual attention, averaged

Figure 6: Percentage of visual attention and of clicks attracted by different AOIs.
over all 38 participants and all 32 trials. (It should be noted that the specific SERP in the background of the figure is just an example to show gaze relative to AOIs.) The figure shows the well-known gaze distribution referred to as "golden triangle" [12] or "F-shaped pattern" [18], describing where people allocate their visual attention on SERPs in the aggregate.
Figure 5 shows mean fixation impact on the different SERP components averaged across participants and across trials. Not surprisingly, most visual attention was devoted to the top few organic results. Interestingly, however, the top ads received as much attention as results around the fold (at positions 6-7). In Figure 6 we show the percentage of visual attention AOIs received from the participants along with the percentage of clicks on the respective AOIs. Gaze and click patterns are in general agreement, but there are some interesting differences. The organic result entries at position 1, 2, and 3 together attracted only 53.46% of visual attention on the SERPs, but 63.90% of all clicks. In addition, the top ads received more than 10% of the visual attention but fewer than 5% of the clicks.
Discussion. Overall, our findings are in line with previous research concerning the distribution of visual attention to organic search results on SERPs [12], [18]. We further see that the relative distribution of clicks does not always reflect the relative distribution of visual attention (Figure 6). We find that there are proportionally more clicks that attention on top results, which is consistent with the previously reported bias towards selecting one of the top organic results [16], [17].
Conversely, we find that top and right rail ads receive a higher fraction of visual attention than of clicks. This extends previous subjective reports of a bias against sponsored links [14]. In addition, although each of the top ads got approximately as much visual attention as organic results at the page fold (see Figure 5), the top ads got considerably fewer clicks than organic result entries even below the fold.
4.2 Effects of Task Type
Figure 7 (left side) shows average fixation impact for SERP elements, broken down separately for informational and navigational tasks. There are several large differences of the general gaze distribution with respect to task type.
First, the participants spent significantly more time on SERPs for informational tasks than for navigational ones (mean=16.5 and 12.9s respectively, t(1208)=3.8, p < 0.01).
Second, most of the additional time during informational tasks was spent on the organic results and on the upper search box. Almost every single position in the organic results received more

46

Figure 7: Comparison of mean fixation impact on SERP elements for navigational and informational tasks (left) and for SERPs displaying good or bad ads (right).

visual attention for informational than for navigational tasks. This is especially evident for the organic results at positions 1 and 2 (t(1208)=7.4, p < 0.01).
Interestingly, there was virtually no difference in the distribution of gaze on the top 3 ads. For the right rail ads we observe slightly more visual attention during informational tasks, although the absolute amount of attention is very low (mean informational=189ms, mean navigational=104ms, t(1208)=2.7, p < 0.01).
Discussion. It is striking that none of the extra time for informational tasks was spent on the top ads. Users did not distribute their additional time evenly on the elements of the SERP but seemed to concentrate their attention on the top 2 organic results. This suggests that for informational tasks where users typically focus more on text snippets, the bias for the top organic results is even stronger.
Furthermore, there is a noticeable difference in visual attention on the upper search box which is more than twice as high for informational tasks. This reflects that the fact users requeried more during informational tasks (1.20 queries) than navigational tasks (1.05 queries). Interestingly, even when participants were not able to find the solution on the first static SERP, they did not divert their attention much towards other components of the SERP such as ads or related searches, instead they requeried.
4.3 Effects of Ad Quality
Figure 7 (right side) shows average fixation impact for SERP elements, broken down separately for SERPs containing good and bad ads. There are several large differences of the general gaze distribution with respect to ad quality.
Overall, participants spent somewhat less time on SERPs when good quality ads were displayed (mean time on SERP 14.2s, =16.5s for good quality ads vs. 15.2s, =16.0s for bad quality ads), however, the difference is not statistically significant.
There are, however, interesting differences in the gaze distribution on different components of the SERPs. Participants devoted about twice as much visual attention to top ads when the ads were of good quality (mean=2.1s and 1.1s for good and bad quality, respectively, t(1208)=6.8, p < 0.01). In contrast, participants paid consistently less attention to the organic results when good quality ads were displayed (mean=10.8s and 12.8s for good and bad quality, respectively, t(1208)=2.6, p < 0.01). There were no reliable effects of ad quality on the remaining SERP components such as the search box, right rail ads, and related searches.

We further analyzed the participants' search engine judgments for each trial with respect to ad quality. When good quality ads were displayed, participants rated the search engine slightly better than when bad quality ads were presented (mean of 4.55 vs. 4.49 on a 5-point Likert scale), however this difference is not significant. In addition, the total time to complete a task was about 10% shorter when good quality ads were shown (mean 50.4s, =53.1) than when bad quality ads were shown (mean 54.4s, =62.2), but this difference is not significant.
Discussion. The quality of ads on a SERP directly influenced participants' attention and performance. Top ads of good quality attracted twice as much visual attention as those of bad quality. In addition, the amount of attention devoted to organic results was influenced by the quality of the ads, with less attention to organic results when the ads were good.
The effect of ad quality on visual attention was not evident for right rail ads. Right rail ads seem to be largely ignored, and when participants looked there, they did not do so differentially as a function of ad quality.
4.4 Sequence Effects
Every sequence of 32 tasks was performed in three different conditions, GB, BG, and RR (see Figure 4). The condition determined the order in which good or bad quality ads were displayed on the SERPs for the different tasks. In this section, we concentrate on effects observed in those different conditions. Figures 8 and 9 show results (for fixation impact and clicks) for these three conditions, broken down separately for SERPs containing good and bad ads.
In Figure 8, we see that mean fixation impact on the top ads from participants in either the BG or the GB condition was around 1.8 times larger than from participants in the RR condition (t(1208)=5.3, p < 0.01). Good ads generated higher fixation impact for all conditions than bad ads. In the blocked conditions (BG and GB), good quality top ads received twice as much visual attention as in the random condition (RR). Further, good quality top ads in the random condition received only as much gaze as bad quality top ads in the blocked conditions.
In Figure 9, we see even larger differences for clicks as a function of condition and ad quality. Not surprisingly, the quality of the ads had a large effect on click rate ­ there were no clicks on bad ads and a click rate of about 13% for good ads. Condition also had a large effect on click behavior. Participants in the BG or GB conditions clicked on the top ads 2 to 3 times more often than participants in

47

Figure 8: Mean fixation impact on the top ads fi(top_ads) split by sequence of blocks (GB, BG, RR) and the quality of the displayed ads on the SERP (good / bad).
Figure 9: Mean number of clicks on the top ads split by sequence of blocks for good ads (there were no clicks on bad ads).
the RR condition (average click rate of 16% for BG and GB vs. 6% for RR, t(1208)=3.0, p < 0.01). Discussion. The differences in fixation impact and number of clicks suggest that the order in which the participants see SERPs with either good or bad ad quality strongly affected their search behavior. When SERPs with good or bad ad quality were presented in random order, participants tended to ignore the ads more, even when they were of good quality. On the contrary, when SERPs contained ads of consistently good quality, then participants were more likely to pay attention to and click them. This observation implies that predictability is an important factor influencing how users attend to different regions on the SERP. If the quality of ads is unpredictable, then users seem to get "ad blind" so that even good ads receive less attention. This finding is of direct importance to search engine providers, who might be able to increase revenue from sponsored links by showing few ads of consistently high quality. If ads are of predictably good quality, then general ad blindness might be reversed.
4.5 Blocking Effects
Since we designed the trial sequences to contain four blocks of SERPs with different ad quality, we expected to see behavioral changes within each trial sequence, especially with respect to the way the participants attend to the top ads. As expected, we observed that participants completed tasks more quickly during the course of the experiment. The total time they needed to complete a task dropped from the first half to the second half of the experiment (1st half mean=55.7s, 2nd half mean=49.1s, t(1208)=2.0, p < 0.05). Also, the time the participants needed to evaluate the first SERP for a given task query decreased from the

Figure 10: Mean fixation impact on the top ads fi(top_ads) split by sequence of blocks (GB, BG, RR) and the block type (good, bad or random) of each block of 8 trials (see Figure 4).
first to the second half (1st half mean=15.9s, 2nd half mean=13.5s, t(1208)=2.5, p < 0.05).
Figure 10 shows the average fixation impact on top ads for the four blocks in the experiment, broken down by condition. As expected, during blocks containing SERPs with good ad quality ("G"), more visual attention was directed to the top ads than during bad ad quality blocks ("B"). For example, in the GB condition, users spent more than twice as much time looking at top ads in the first and third blocks which contain good quality ads. Within the random condition, the highest fixation impact on the top ads occurs during the first block, with a more than 30% drop for subsequent blocks (1st block mean=1.5s, =1.7, 2nd-4th block mean=0.9s, =1.2, t(377)=3.8, p < 0.01).
Discussion. These findings show that if ad quality is predictably good, then users do notice and pay more attention to the top ads. Even after having experienced a block containing SERPs with bad ads, participants start to pay more attention to the ads when their quality changes. Interestingly, this was also the case for the second block in the BG condition in which participants started with a block of bad ad quality and then switched to a block of good ad quality. This shows that amount of attention devoted to ads depends on the context of previous experience.
Interestingly, the amount of visual attention on the top ads during the first block of the random condition RR (which consists of 50% SERPs with good quality ads) was the same as during the first block of the BG condition (which consists almost only of SERPS with bad quality ads). This suggests that predictability concerning the quality of the ads is important from the very beginning. If quality is unpredictable, then users quickly start to devote less attention to the ads.
5. CONCLUSION
In this paper, we presented the results of an eye-tracking study to characterize how factors such as task type and ad quality influence how users allocate their visual attention to different components of search engine results pages (SERPs). We found significant effects of task type (informational/navigational), ad quality (good/bad) and the sequence in which ads of different quality were shown.
Consistent with previous research, we found a considerable bias of users' visual attention towards the top few organic result entries which is even stronger for informational than for navigational tasks. Furthermore, we found a strong bias against sponsored links in

48

general. Even for informational tasks, in which participants generally had a harder time finding a solution, the ads did not receive any more attention from the participants.
The quality of ads had a significant influence on the amount of visual attention that participants devoted to both the top ads and the organic results. When good quality ads were displayed, participants paid twice as much attention to these ads and less attention to the organic results. This is strong evidence that how people attend to search results depends on the quality and content of other page elements.
In addition, gaze patterns were strongly related to the order in which good and bad ads were presented. When the quality of ads varied randomly across trials, participants attended to them less than half as long as when the quality varied more predictably by blocks. Strikingly, when ad quality varied randomly, participants attended to the top ads no more than they did for trials in which only bad ads were shown, even though the ads were good on half of the trials. These results are relevant for search engine providers in understanding how ads are perceived under different conditions, and more generally in understanding how prior search experiences influence the current allocation of attention.
This research represents a first step in understanding how task, ad quality and sequence influence search interaction. In our study, we focused on a specific static SERP composition which always consisted of 10 textual organic results with top and right rail ads. One next step for our research is to look at richer variations of SERP composition, e.g., including snippets that contain images, maps or deep links, and using a broader range of queries for which ads may or may not be present. In addition, we would like to explore how the quality of ads interacts with the quality of organic results. For example, how does the presence of a bad ad (or result) affect the perception of the other good quality ads (or results)? Finally, we would like to extend our understanding of temporal dynamics to enable us to develop richer models of search processes and strategies that go beyond individual SERPs to include session behavior as well as longer-term effects.
6. ACKNOWLEDGMENTS
We would like to thank our colleagues in adCenter, Bing and MSR for their valuable comments and great support. We are also grateful for the reviewers' thoughtful comments, and to our participants for their time and insights.
7. REFERENCES
[1] Aula, A., Majaranta, P. & Räihä, K. Eye-tracking reveals the personal styles for search result evaluation. In Proceedings INTERACT 2005, 1058-1061.
[2] Broder, A. A taxonomy of web search. SIGIR Forum, 2002, vol. 36, 3-10.
[3] Beymer, D., Russell, D. & Orton, P. Z. An eye tracking study of how font size, font type, and pictures influence online reading. In Proceedings INTERACT 2007, 456-460.
[4] Buscher, G., Cutrell, E. & Morris, M. R. What do you see when you're surfing? Using eye tracking to predict salient regions of web pages. In Proceedings CHI 2009, 21-30.
[5] Clarke, C. L. A.,, Agichtein, E.., Dumais, S. & White, R. W. The influence of caption features on clickthrough patterns in web search. In Proceedings SIGIR 2007, 135-142.
[6] Cutrell, E. & Guan, Z. What are you looking for? An eyetracking study of information usage in web search. In Proceedings CHI 2007, 407-416.

[7] Downey, D., Dumais, S., Liebling, D. & Horvitz, E. Understanding the relationship between searchers' queries and information goals. In Proceedings CIKM 2008, 449-458.
[8] Fallows, D. Search engine users. Pew Research Center, 2005. Retrieved January 18, 2010 from http//www. pewinternet.org/Reports/2005/Search-Engine-Users.aspx.
[9] Granka, L. A., Joachims, T. & Gay, G. Eye-tracking analysis of user behavior in WWW search. In Proceedings SIGIR 2004, 478-479.
[10] Guan, Z. & Cutrell, E. An eye tracking study of the effect of target rank on web search. In Proceedings CHI 2007, 417-420.
[11] Höchstötter, N. & Lewandowski, D. What users see ­ Structures in search engine results pages. Information Sciences, 2009, vol. 179, 1796-1812.
[12] Hotchkiss, G., Alston, S. & Edwards, G. Eye tracking study, 2006. Retrieved January 18, 2010 from http//www.enquiro.com/eyetrackingreport.asp.
[13] Jansen, B. J. The comparative effectiveness of sponsored and nonsponsored links for Web e-commerce queries. ACM Transactions on the Web, 2007, vol. 1, article 3.
[14] Jansen, B. J., Brown, A. & Resnick, M. Factors relating to the decision to click on a sponsored link. Decision Support Systems, 2007, vol. 44, 46-59.
[15] Jansen, B. J. & Spink, A. Investigating customer click through behaviour with integrated sponsored and nonsponsored results. International Journal of Internet Marketing and Advertising, 2009, vol. 5, 74-94.
[16] Joachims, T., Granka, L., Pan, B., Hembrooke, H. & Gay, G. Accurately interpreting clickthrough data as implicit feedback. In Proceedings SIGIR 2005, 154-161.
[17] Lorigo, L., Haridasan, M., Brynjarsdóttir, H., Xia, L., Joachims, T., Gay, G., Granka, L., Pellacini, F. & Pan, B. Eye tracking and online search Lessons learned and challenges ahead. JASIST, 2008, vol. 59, 1041-1052.
[18] Nielsen, J. F-Shaped pattern for reading Web content, 2006. Retrieved January 18, 2010 from http//www.useit.com/alertbox/reading_pattern.html.
[19] Pan, B., Hembrooke, H., Joachims, T., Lorigo, L., Gay, G. & Granka, L. In Google we trust: Users' decisions on rank, position, and relevance. Journal of Computer-Mediated Communication, 2007, vol. 12, 801-823.
[20] Rose, D. E. & Levinson, D. Understanding user goals in web search. In Proceedings WWW 2004, 13-19.
[21] Smith, C. L. & Kantor, P. B. User adaptation: Good results from poor systems. In Proceedings SIGIR 2008, 147-154.
[22] Turpin, A. & Schöler, F. User performance versus precision measures for simple search tasks. In Proceedings of SIGIR 2006, 11-18.
[23] White, R. W., Dumais, S.T. & Teevan, J. Characterizing the influence of domain expertise on Web search. In Proceedings of WSDM 2009, 132-141.
[24] White, R. W. & Morris, D. Investigating the querying and browsing behavior of advanced search engine users. In Proceedings of SIGIR 2007, 255-262.
[25] Yan, J., Liu, N., Wang, G., Zhang, W., Jiang, Y. & Chen, Z. How much can behavioral targeting help online advertising? In Proceedings WWW 2009, 261-270.

49

Uncovering Social Spammers: Social Honeypots + Machine Learning

Kyumin Lee
Department of Computer Science and Engineering
Texas A&M University College Station, TX, USA
kyumin@cse.tamu.edu

James Caverlee
Department of Computer Science and Engineering
Texas A&M University College Station, TX, USA
caverlee@cse.tamu.edu

Steve Webb
College of Computing Georgia Institute of Technology Atlanta, GA, USA
steve.webb@gmail.com

ABSTRACT
Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.
Categories and Subject Descriptors: H.3.5 [Online Information Services]: Web-based services; J.4 [Computer Applications]: Social and behavioral sciences
General Terms: Design, Experimentation, Security
Keywords: social media, social honeypots, spam
1. INTRODUCTION
The past few years have seen the rapid rise of Web-based systems incorporating social features ­ from Web-based social networks (e.g., Facebook, MySpace) to online social media sites (e.g., YouTube, Flickr) to large-scale information sharing communities (e.g., Digg, Yahoo! Answers). These social systems have attracted a tremendous amount of media and research interest [1, 10, 18].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

One of the key features of these systems is their reliance on users as primary contributors of content and as annotators and raters of other's content. This reliance on users can lead to many positive effects, including large-scale growth in the size and content in the community, bottom-up discovery of "citizen-experts", serendipitous discovery of new resources beyond the scope of the system designers, and new social-based information search and retrieval algorithms. Unfortunately, the relative openness and reliance on users coupled with the widespread interest and growth of these social systems has also made them prime targets of social spammers.
In particular, social spammers are increasingly targeting these systems as part of phishing attacks [14], to disseminate malware [5] and commercial spam messages [7, 26], and to promote affiliate websites [17]. In the past year alone, more than 80% of social networking users have "received unwanted friend requests, messages, or postings on their social or professional network account" (Source: Harris Interactive, June 2008). Unlike traditional emailbased spam, social spam often contains contextual information that can increase the impact of the spam (e.g., by eliciting a user to click on a phishing link sent from a "friend") [7, 12, 14].
Successfully defending against these social spammers is important to improve the quality of experience for community members, to lessen the system load of dealing with unwanted and sometimes dangerous content, and to positively impact the overall value of the social system going forward. However, little is known about these social spammers, their level of sophistication, or their strategies and tactics. Filling this need is challenging, especially in social networks consisting of 100s of millions of user profiles (like Facebook, MySpace, Twitter, YouTube, etc.). Traditional techniques for discovering evidence of spam users often rely on costly human-inthe-loop inspection of training data for building spam classifiers; since spammers constantly adapt their strategies and tactics, the learned spam signatures can go stale quickly. An alternative spam discovery technique relies on community-contributed spam referrals (e.g., Users A, B, and C report that User X is a spam user); of course, these kinds of referral systems can be manipulated themselves to yield spam labels on legitimate users, thereby obscuring the labeling effectiveness. And neither spam discovery approach can effectively handle zero-day social spam attacks for which there is no existing signature or wide evidence.
With these challenges in mind, we propose and evaluate a novel honeypot-based approach for uncovering social spammers in online social systems. Concretely, the proposed approach is designed to (i) automatically harvest spam profiles from social networking communities, avoiding the drawbacks of burdensome human inspection; (ii) develop robust statistical user models for distinguishing between social spammers and legitimate users; and (iii) ac-

435

Community

Legitimate Legitimate

request collect info

insert collected data

Spammer

Social Honeypot

Data analysis Database

Training Set

Update incrementally new evidence

Classifier Test Set

Spammers

Legitimate

Inspectors

collreectq iunefsto

Legitimate Spammer

Legitimate

Figure 1: Overall Framework of Social Honeypot-based Approach

tively filter out unknown (including zero-day) spammers based on these user models. Drawing inspiration from security researchers who have used honeypots to observe and analyze malicious activity (e.g., for characterizing malicious hacker activity [22], generating intrusion detection signatures [16], and observing email address harvesters [20]), we deploy and maintain social honeypots for trapping evidence of spam profile behavior, so that users who are detected by the honeypot have a high likelihood of being a spammer (i.e., low false positive rate). Over two distinct communities (MySpace and Twitter), we find that the proposed approach provides generalizable and effective social spam detection.
2. OVERALL FRAMEWORK
In this section, we present the conceptual framework of the proposed honeypot-based approach and outline the research questions motivating our examination of this framework.
2.1 Problem Statement
In social networking communities like MySpace and Facebook, there are a set of k users U = {u1, u2, . . . , uk}. Each user ui has a profile pi. Profiles are self-describing pages that are created and controlled by a given user. For example, users typically include information such as their name, gender, age, and so on in their profiles. Each community has its own profile format, but most fields in the formats are the same.
The social spam detection problem is to predict whether ui is a spammer through a classifier c when pi is given. A classifier
c : ui  {spammer, legitimate user}
approximates whether ui is a spammer. To build c, we need to extract a set of m features F = {f1, f2, . . . , fm} from U. For example, we can extract Fui from pi of ui.
Whereas traditional email spam detection has focused on identifying spam messages which are of relatively low individual value to the spammer (and whose identification typically doesn't threaten the ongoing ability of a spammer to send new messages), social spam detection is focused on identifying and eliminating spam accounts themselves. This detection is potentially more disruptive to spammers, since these accounts typically represent a more expensive investment by the spammer (through email and social media account registrations).
2.2 Solution Approach
We propose to monitor spammer activity through the creation of social honeypots. We define social honeypots as information

system resources that monitor spammers' behaviors and log their information (e.g., their profiles and other content created by them in social networking communities). Social honeypots and traditional honeypots (e.g., in domains such as network systems and emails [16, 20, 22]) share a similar purpose in that they both monitor and log the behaviors of spammers or attackers. However, traditional honeypots typically target network or systems-level behavior, whereas social honeypots specifically target community-based online activities.
While social honeypots alone are a potentially valuable tool for gathering evidence of social spam attacks and supporting a greater understanding of spam strategies, it is the goal of this research project to support ongoing and active automatic detection of new and emerging spammers (See Figure 1). In practice, we deploy a social honeypot consisting of a legitimate profile and an associated bot to detect social spam behavior. If the social honeypot detects suspicious user activity (e.g., the honeypot's profile receiving an unsolicited friend request) then the social honeypot's bot collects evidence of the spam candidate (e.g., by crawling the profile of the user sending the unsolicited friend request plus hyperlinks from the profile). What entails suspicious user behavior can be optimized for the particular community and updated based on new observations of spammer activity.
As the social honeypots collect spam evidence, we extract observable features from the collected candidate spam profiles (e.g., number of friends, text on the profile, age, etc.). Coupled with a set of known legitimate (non-spam) profiles which are more populous and easy to extract from social networking communities, these spam and legitimate profiles become part of the initial training set of a spam classifier. Through iterative refinement of the features selected and the classifier used (e.g., SVM), the spam classifier can be optimized over the known spam and legitimate profiles.
Based on these developed classifiers, we can then explore the wider space of unknown profiles. On MySpace alone there are 100s of millions of profiles, of which some unknown fraction are spam. Using the classifiers based on the harvested social honeypot data, we iteratively explore these profiles "in-the-wild" to detect new spammers that have yet to be identified by a social honeypot directly. In our design of the overall architecture, we include human inspectors in-the-loop for validating the quality of these extracted spam candidates. Instead of inspecting the entirety of all profiles, these inspectors are guided to validate just the few spam candidates recommended by the learned classifiers. Based on their feedback, the spam classifiers are updated with the new evidence and the process continues. Given the overall architecture, we address three important research challenges in turn in the rest of the paper:

436

· Research Challenge #1 [RC1]: Do social honeypots collect evidence of spam with low false positives? In other words, do honeypots really work in practice? A poorly performing social honeypot will negatively impact the spam classification approach, leading to poor performance.
· Research Challenge #2 [RC2]: Can we build classifiers from the harvested social honeypot profiles and known legitimate profiles to effectively identify spam profiles? Since social honeypots are triggered by suspicious user activity, we must explore if the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). It is our hypothesis that spammers engage in behavior that is correlated with observable features that distinguish them from legitimate users.
· Research Challenge #3 [RC3]: Finally, can the developed classifiers be effectively deployed over large collections of unknown profiles (for which we have no assurances of the degree of spam or legitimate users)? This last question is important for understanding the promise of social honeypots in defending against new and emerging spam as it arises "in-the-wild."
3. RC1: STUDY OF HARVESTED SPAM USERS
Based on the overall social honeypot framework, we selected two social networking communities ­ Myspace and Twitter ­ to evaluate the effectiveness of the proposed spam defense mechanism. Both MySpace and Twitter are large and growing communities and both also support public access to their profiles, so all data collection can rely on purely public data capture.
MySpace Social Honeypot Deployment: In previous research [23], we created 51 generic honeypot profiles within the MySpace community for attracting spammer activity so that we can identify and analyze the characteristics of social spam profiles. To observe any geographic artifacts of spamming behavior, each profile was assigned a specific geographic location (i.e., one honeypot was assigned to each of the U.S. states and Washington, D.C.). Each honeypot profile tracks all unsolicited friend requests. Upon receiving a friend request, we store a local copy of the profile issuing the friend request, extract all hyperlinks in the "About Me" sections (we find that these typically point to an affiliate spam website), and crawl the pages pointed to by these hyperlinks. Based on a four month evaluation period (October 2007 to January 2008), we collected 1,570 profiles whose users sent unsolicited friend requests to these social honeypots.
Twitter Social Honeypot Deployment: Similarly, we created and deployed a mix of honeypots within the Twitter community ­ some of them had personal information such as biography, location and so on, while others did not have this personal information. Some social honeypots posted tweets, while others did not post them. We omit some of the concrete details of the Twitter honeypot deployment due to the space constraint. From August 2009 to September 2009, these social honeypots collected 500 users' data.
3.1 MySpace Observations
After analyzing the harvested spam profiles from MySpace, we find some interesting observations (more fully detailed in [23]): (1) The spamming behaviors of spam profiles follow distinct temporal patterns. (2) The most popular spamming targets are Midwestern states, and the most popular location for spam profiles is California. (3) 57.2% of the spam profiles copy their "About Me" content from another profile. (4) Many of the spam profiles exhibit distinct demographic characteristics (e.g., age, relationship status,

etc.). (5) Spam profiles use thousands of URLs and various redirection techniques to funnel users to a handful of destination Web pages. Through manual inspection, we grouped the harvested spam profiles into five categories:
· Click Traps: Each profile contains a background image that is also a link to another Web page. If users click anywhere on the profile, they are directed to the link's corresponding Web site.
· Friend Infiltrators: These nominally legitimate profiles befriend as many users as possible so that they can infiltrate the users' circles of friends and bypass any communication restrictions imposed on non-friends. Once a user accepts a friend request from one of these profiles, the profile begins spamming the user through existing communication systems (e.g., message spam, comment spam, etc.).
· Pornographic Storytellers: Each of these profiles has an "About Me" section that consists of randomized pornographic stories, which are book-ended by links that lead to pornographic Web pages. The anchor text used in these profiles is extremely similar, even though the rest of the "About Me" text is almost completely randomized.
· Japanese Pill Pushers: These profiles contain a sales pitch for male enhancement pills in their "About Me" sections. According to the pitch, the attractive woman pictured in the profile has a boyfriend who purchased these pills at an incredible discount.
· Winnies: All of these profiles have the same headline: "Hey its winnie." However, despite this headline, none of the profiles are actually named "Winnie." In addition to a shared headline, each of the profiles also includes a link to a Web page where users can see the pictured female's pornographic pictures.
3.2 Twitter Observations
Similarly, we discovered various types of spam users in the harvested data from Twitter. In many cases, spammers inserted malicious or spam links into their tweets. Since most Twitter links use a form of URL-shortening, users clicking on these links have no assurances of the actual destination.
· Duplicate Spammers: These users post a series of nearly identical tweets. In many cases, the only different content from tweet to tweet is the inclusion of different @usernames (or @replies). The insertion of these @usernames essentially delivers the tweet to the username's account even if the spammer has no relationship with the intended target.
· Pornographic Spammers: Their data such as user image, profile URL, and text and links in tweets contains adult content.
· Promoters: These users post tweets about several things such as online business, marketing and so on. Their posting approach is more sophisticated than duplicate spammers since spam tweets are randomly interspersed with seemingly innocuous legitimate tweets.
· Phishers: Similar to promoters, these users use a mix of strategies to deliver phishing URLs to targets on Twitter.
· Friend Infiltrators: Much like their counterparts on MySpace, these users have profiles and tweets that are seemingly legitimate. They follow many people and intend to accumulate many followers; then they begin engaging in spam activities like posting tweets containing pornographic or commercial content.
These observations indicate that social honeypots can successfully attract spammers across fundamentally different communities (MySpace vs. Twitter), and the results suggest that building automatic classifiers may be useful for identifying social spam.

437

4. RC2: EMPIRICAL EVALUATION OF SOCIAL SPAM SIGNALS
We next explore whether there are discernible spam signals in the harvested spam profiles that can be used to automatically distinguish spam profiles from legitimate profiles. Since social honeypots are triggered by spam behaviors only, it is unclear if the corresponding profiles engaging in the spam behavior also exhibit clearly observable spam signals. If there are clear patterns (as our observations in the previous section would seem to indicate), then by training a classifier on the observable signals, we may be able to predict new spam even in the absence of triggering spam behaviors.
4.1 Classification Approach and Metrics
As part of this empirical evaluation of social spam signals, we consider four broad classes of user attributes that are typically observable (unlike, say, private messaging between two users) in the social network: (i) user demographics: including age, gender, location, and other descriptive information about the user; (ii) usercontributed content: including "About Me" text, blog posts, comments posted on other user's profiles, tweets, etc.; (iii) user activity features: including posting rate, tweet frequency; (iv) user connections: including number of friends in the social network, followers, following. For MySpace and Twitter we select a subset of these features to train the classifier.
The classification experiments were performed using 10-fold crossvalidation to improve the reliability of classifier evaluations. When a dataset is not large, it is common to use 10-fold cross-validation to achieve statistically precise results. In 10-fold cross-validation, the original sample is randomly divided into 10 equally-sized subsamples. 9 sub-samples are used as a training set and the remaining one is used as a testing set; the classifier is evaluated, then the process is repeated for a total of 10 times. Each sub-sample is used as a testing set once in each evaluation. The final evaluation result is generated by averaging the results of the 10 evaluations. In practice, we evaluated over 60 different classifiers in the Weka [24] machine learning toolkit using 10-fold cross-validation with default values for all parameters. Classification results are presented in the form of a confusion matrix as in Table 1.
Table 1: Confusion matrix example.

Actual

Spammer Legitimate

Predicted

Spammer Legitimate

a

b

c

d

To measure the effectiveness of classifiers based on our proposed features, we used the standard metrics such as precision, recall, accuracy, the F1 measure, false positive and true positive. Precision is the ratio of correctly predicted users as a class to the total predicted users as the class. For example, the precision (P) of the spammer class in Table 1 is a/(a + c). Recall (R) is the ratio of correctly predicted users as a class to the actual users in the class. The recall of the spammer class in the table is a/(a + b). Accuracy is the proportion of the total number of predictions that were correct. The accuracy in the table is (a + d)/(a + b + c + d). F1 is a measure that trades off precision versus recall. F1 measure of the spammer class is 2P R/(P + R). A false positive is when the actual Y class users are incorrectly predicted as X class users. The false positive of the spammer class is c. A true positive is when actual X class users are correctly predicted as X class users. The true positive of the spammer class is a.
To measure the discrimination power between spammers and le-

True Positive Rate

1

0.8

0.6

0.4

Num of friends AMContent

AMLength

0.2

Age

Martial Status

Sex

00

0.2

0.4

0.6

0.8

1

False Positive Rate

Figure 2: MySpace ­ Feature Comparison

gitimate users of each of the proposed features, we generate a Receiver Operating Characteristics (ROC) curve. ROC curves plot false positive rate on the X axis and true positive rate on the Y axis. The closer the ROC curve is to the upper left corner, the higher the overall accuracy is. The ideal ROC curve includes the coordinate (0, 1), indicating no false positives and a 100% true positive rate.

4.2 MySpace Spam Classification

We randomly sampled 388 legitimate profiles from MySpace

(which were labeled by us) and 627 deceptive spam profiles from

the 1,570 deceptive spam profiles collected by our social honey-

pots. When we sampled the profiles, we considered several condi-

tions. Profiles have to be public, and marital status, gender, age,

and "About Me" content in the profiles have to be valid (i.e., a non-

empty value). In addition, we removed duplicated profiles among

the 1,570 deceptive spam profiles in the case that a spammer sent a

friend request to several social honeypots. The goal of spam clas-

sification over the MySpace data is to predict whether a profile is

either spammer or legitimate.

We considered several representative user features: number of

friends, age, marital status, gender, as well as some text-based fea-

tures modeling user-contributed content in the "About Me" section.

Specifically, we consider a bag-of-words model in which we re-

move punctuation, make all letters lowercase, tokenize each word,

remove stopwords, and do stemming for each word using the Porter

stemmer [19]. We assigned weights to each word based on tf-idf

weighting:

tf-idft,d

=

log(1

+

tft,d

)

×

log(

N dft

),

where

tft,d

means

term frequency of term t in a profile's "About Me", N is the num-

ber of profiles, and dft is the number of profiles which includes

term t. We also measured the length in bytes of the "About Me"

content.

Before evaluating the effectiveness of our classifiers, we investi-

gated the discrimination power of our individual classification fea-

tures. Recognizing that social spam classification is an example

of an adversarial classification problem [11], we wanted to evalu-

ate the robustness of our features against an adversarial attack. To

show the discrimination power and robustness of each feature, we

generated ROC curves for each feature using an AdaboostM1 clas-

sifier. The results are shown in Figure 2. Marital status and sex

are the least discriminative features, which is unsurprising because

they are represented by a small set of predefined values (e.g., "Mar-

ried", "Single", "Male", etc.) that will inevitably appear in both

legitimate and spam profiles. On the other hand, the bag of words

features extracted from "About Me" content (AMContent) are the

most discriminative. This is a very encouraging result because it

means our classifier was able to distinguish between legitimate and

438

Table 2: Performance results of top 10 classifiers

Weka Classifier Decorate SimpleLogistic FT LogitBoost RandomSubSpace Bagging J48 OrdinalClassClassifier ClassBalancedND DataNearBalancedND

Accuracy 99.21% 99.01% 99.01% 99.01% 98.72% 98.62% 98.42% 98.42% 98.42% 98.42%

F1 0.992 0.99 0.99 0.99 0.987 0.986 0.984 0.984 0.984 0.984

FP 0.7% 0.9% 0.9% 1.3% 1.1% 1.2% 1.5% 1.5% 1.5% 1.5%

spam "About Me" content with a high degree of accuracy. Therefore, if spammers begin varying the other features of their profiles (to appear more legitimate), our classifiers will still be effective. Additionally, the "About Me" content is the most difficult feature for a spammer to vary because it contains the actual sales pitch or deceptive content that is meant to target legitimate users.
In Table 2, the performance results for the top 10 classifiers are shown. The table clearly shows that all of the classifiers were successful. Each classifier generated an accuracy greater than 98.4%, an F1 measure over 0.98, and a false positive rate below 1.6%. Overall, meta-classifiers (Decorate, LogitBoost, etc.) performed better than tree classifiers (FT and J48) and a function-based classifier (SimpleLogistic). The best classifier is Decorate, which is a meta-learner for building diverse ensembles of classifiers. It obtained an accuracy of 99.21%, an F1 measure of 0.992, and a 0.7% false positive rate. We additionally considered different training mixtures of spam and legitimate training data (from 10% spam / 90% legitimate to 90% spam / 10% legitimate); we find that the metrics are robust across these changes in training data.

4.3 Twitter Spam Classification

To evaluate the quality of spam classification over Twitter, we

randomly selected 104 legitimate users (labeled by us) from a pre-

viously collected Twitter dataset of 210,000 users. We additionally

considered two classes of spam users: the 61 spammers and the

107 promoters sampled from 500 users' data collected by the social

honeypots. For each user, we collected the user profile, tweets (sta-

tus update messages), following (friend) information and follow-

ers' information. The goal of spam classification over the Twitter

data is to predict whether a profile is either spammer, a promoter, or

legitimate. When we sampled users' data, we considered two con-

ditions: the profiles did not have a verified account badge and the

number of tweets had to be over 0. The verified account badge is

one way Twitter ensures that profiles belong to known people (e.g,

Shaquille O'Neal and not an impersonator).

Unlike MySpace profiles which emphasize on longer-form per-

sonal information sharing (e.g., "About Me" text) and usually have

self-reported user demographics (e.g., age, gender), Twitter accounts

are noted for their short posts, activity-related features, and limited

self-reported user demographics. For user features, we consider the

longevity of the account on Twitter, the average tweets per day, the

ratio of the number of following and number of followers, the per-

centage

of

bidirectional

friends

(

|f ollowingf ollowers| |f ollowing|

),

as

well

as

some features of the tweets sent, including:

· The ratio of the number of URLs in the 20 most recently posted tweets to the number of tweets (|U RLs|/|tweets|).

· The ratio of the number of unique URLs in the 20 most recently posted tweets to the number of tweets (|unique U RLs|/|tweets|).

True Positive Rate

1

0.9

0.8

0.7

0.6

0.5 F-F Ratio

0.4

|@| per tweet |URL| per tweet

0.3

|tweets| per day Account Age

|unique @| per tweet

0.2

|unique URL| per tweet

Bi-friends

0.1

Tweets Similarity

Tweets

00

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

False Positive Rate

Figure 3: Twitter ­ Feature Comparison

· The ratio of the number of @usernames in the 20 most recently posted tweets to the number of tweets (|@username|/|tweets|).
· The ratio of the number of unique @usernames in the 20 most recently posted tweets to the number of tweets (|unique @username|/|tweets|).
Additionally, we measure the average content similarity over all pairs of tweets posted by a user:
similarity(a, b) |set of pairs in tweets|
a,bset of pairs in tweets
where the content similarity is computed using the standard cosine -
similarity over the bag-of-words vector representation V (a) of the tweet content:
- - V (a) · V (b) similarity(a, b) = - - |V (a)|| V (b)|
We finally considered some text-based features to model the content in the tweets. Since tweets are extremely short (140 characters or less), we consider a bag-of-words model and a sparse bigrams model [9]. We remove punctuation, make all letters lowercase, tokenize each word in the bag-of-words model and tokenize a pair of words in the sparse bigrams model. The sparse bigrams model generates a pair of words separated by no more than k words. We assigned k = 3 in our system, while k = 0 yields ordinary bigrams. If a tweet is "check adult page view models", the sparse bigrams will generate the features "check adult", "check page", "check view", "adult page", "adult view", "adult models", "page view", "page models", "view models". We weighted terms and bigrams using tf-idf weighting as in the previous MySpace classification.
In order to know how much discrimination power each feature has for spammers, promoters and legitimate users, we generated ROC curves of the proposed features using Decorate in Figure 3. Average posted tweets per day (|tweets| per day), percentage of bidirectional friends (bi-friends), and ratio of number of following and number of followers (F-F Ratio) have low discrimination powers relatively, while ratio of number of unique URLs in recently posted top 20 tweets and number of the tweets (|unique U RL| per tweet), ratio of number of @username in recently posted top 20 tweets and number of the tweets (|@| per tweet), ratio of number of unique @username in recently posted top 20 tweets and number of the tweets (|unique @| per tweet), and av-

439

Cumulative % Cumulative %

1

0.8

0.6

0.4

Legitimate

0.2

Spammer

Promoter

00

0.2 0.4 0.6 0.8

1

Content Similarity

(a) Content Similarity

1

0.8

0.6

0.4

Legitimate

0.2

Spammer

Promoter

00

0.5

1

1.5

2

|URLs| / |tweets|

(b) Avg URLs per Tweet

Figure 4: Cumulative Distribution of Features Extracted from Users

Table 3: Performance results of top 10 classifiers

Weka Classifier Decorate LogitBoost HyperPipes Bagging RandomSubSpace BFTree FT SimpleLogistic LibSVM (SVM) ClassificationViaRegression

Accuracy 88.98% 87.86% 85.29% 84.56% 84.19% 83.82% 83.46% 83.46% 83.09% 82.72%

F1 0.888 0.877 0.846 0.844 0.837 0.84 0.832 0.832 0.825 0.823

FP 5.7% 6.2% 8.1% 7.5% 8.3% 7.2% 8.3% 8.5% 10.2% 9.1%

erage content similarity between a user's tweets (tweets similarity) have good discrimination powers. Account age, text-based features extracted from tweets, and ratio of number of URLs in recently posted top 20 tweets and number of the tweets (|U RL| per tweet) have the best discrimination power. Overall, the proposed all features have positive discrimination power.
To further illustrate, Figure 4(a) presents the cumulative distributions of content similarity in tweets posted by each user class. It shows clear distinction among legitimate users, spammers and promoters. The content similarity in tweets of each spammer is the largest compared to the other classes because some of them post almost the same content or even duplicate tweets. Promoters have a goal of promoting something like online business, marketing and so on; naturally their tweets include common terms like the name of a product. Therefore, the content similarity in their tweets is larger than legitimate users' one because legitimate users post tweets about their news such as what they are doing, where they are and so on. The content similarity in tweets of legitimate users is the smallest. Figure 4(b) shows the cumulative distributions of the average number of URLs in the tweets of each user. Tweets posted by legitimate users include the smallest number of URLs; not surprisingly, the majority of spammers and promoters post tweets with URLs. The curves of spammers and promoters are overlapped near 1 in the X axis, meaning that promotor and spammer behavior is closely coupled in our dataset.
Table 3 shows the performance results for the top 10 classifiers. Each of the top 10 classifiers achieved an accuracy greater than 82.7%, an F1 measure over 0.82, and a false positive rate less than 10.3%. As in the case with MySpace, the meta classifiers (Decorate, LogitBoost, etc.) produced better performance than tree classifiers (BFTree and FT) and function-based classifiers (SimpleLogistic and LibSVM). The best classifier was Decorate, which obtained an accuracy of 88.98% accuracy, an F1 measure of 0.888, and a 5.7% false positive rate. As in the MySpace analysis, we additionally considered different training mixtures of spam and legitimate training data (from 10% spam / 90% legitimate to 90% spam / 10% legitimate); we find that the classification metrics are robust across these changes in training data.

Table 4: Statistics of MySpace dataset

Public Profiles Private Profiles Total Profiles Size

1,576,684

274,988

1,851,672 150GB

Table 5: Statistics of Twitter dataset
User Profiles Tweets Following Followers Size 215,345 4,040,415 51,650,754 65,904,253 11.3GB

4.4 Summary
Based on our empirical study over both MySpace and Twitter, we find strong evidence that social honeypots can attract spam behaviors that are strongly correlated with observable features of the spammer's profiles and their activity in the network (e.g., tweet frequency). These results hold across two fundamentally different communities and confirm the hypothesis that spammers engage in behavior that is correlated with observable features that distinguish them from legitimate users. In addition, we find that some of these signals may be difficult for spammers to obscure (e.g., content containing a sales pitch or deceptive content), so that the results are encouraging for ongoing effective spam detection.

5. RC3: LARGE-SCALE SOCIAL SPAM CLASSIFICATION IN THE WILD
So far, we have seen that the deployed social honeypots can collect evidence of spam behavior, and that these behaviors are correlated with spam signals which can support automatic spam classification. In this final study, we explore whether these classifiers can be effectively deployed over large collections of unknown profiles (for which we have no assurances of the degree of spam or legitimate users). Concretely, we apply the developed classifiers for both MySpace and Twitter over datasets "in-the-wild" to better understand the promise of social honeypots in defending against new and emerging spam and zero-day spam attacks.

5.1 Data and Setup
For this final study, we considered two large datasets. MySpace Dataset: The first dataset is a crawl over MySpace, including about 1.5 million of public profiles collected in 2006 and 2007. A full description of this dataset and its characteristics is available in [8]. Table 4 summarizes statistics of this dataset. Twitter Dataset: We also collected a large dataset from Twitter for the period September 2 to September 9, 2009. We sampled the public timeline of Twitter (which publishes a random selection of new tweets every minute), collected usernames, and then used the Twitter API to collect each user's recently posted top 20 tweets, plus the user's following (friends) and followers' information. Table 5 presents statistics of this Twitter dataset. It consists of 215,345 user profiles, 4,040,415 tweets.
In both cases, the collected profiles are unseen to our system, meaning that we do not know ground truth as to whether a profile is spam or legitimate. As a result, the traditional classification metrics presented in the previous section would be infeasible to apply in this case. Rather than hand label millions of profiles, we adopted the spam precision metric to evaluate the quality of spam predictions. For spam precision, we evaluate only the predicted spammers (i.e., the profiles that the classifier labels as spam). Spam precision is defined as:

SpamP recision =

true positives

true positives + f alse positives

440

Spam Precision Spam Precision

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Sexual Content

w/o postfilters with postfilters
Advertisement Content

Figure 5: MySpace ­ Spam Precision

5.2 Finding Unknown Spam on MySpace
As in the previous section, we trained a classifier over a training set consisting of 388 legitimate profiles (labeled by us) and 627 deceptive spam profiles collected from social honeypots. In the interests of efficiency, we used the LibSVM classifier ­ an implementation of support vector machines ­ which is a widely popular classifier and classifies a large dataset quickly with high accuracy. Its classification time is faster than meta classifiers that proved successful in the previous experiments. We sampled from the 1.5 million public profiles a smaller test set of 43,000 profiles. We repeated this sampling procedure four times so we had four different test sets.
As we classified each of four test sets, a human inspector verified whether the newly found predicted spam was actually spam, added the new instances to the training set, and the process continued. In each test set, LibSVM classifier predicted about 30 users as spammers. In each subsequent iteration, we found that the spam precision increased.
Figure 5 shows the evaluation results of the fourth test set. The left two bars of the figure present spam precision based on sexual content. If an unseen profile is classified to a deceptive spam profile by LibSVM, and its "About Me" content includes sexual content, it will be considered as a real deceptive spam profile. The right two bars of the figure present spam precision based on advertisement content. If predicted spam profile's "About Me" content includes advertisement content, it will be considered as a real deceptive spam profile. Note that there are two results: with postfilter and without postfilter. We found that LibSVM incorrectly predicted spam labels for profiles containing special profile layout links, e.g., "click here to get a theme for your myspace" or "click here for myspace backgrounds", which are similar to spammer techniques for inserting links into spam profiles. These types of profile layout links are common on MySpace, which allows users to adjust their profile layouts. To correct these errors, we inserted a "postfilter" to check for these standard links and remove their spam labels.
As we can see, using postfilters improved about 40% spam precision in sexual content and about 21% in advertisement content. Detecting spammers whose profiles include advertisement content is easier than detecting spammers whose profile include sexual content. Even with the fairly good results (70% spam precision), the results are significantly worse than what we observed in the previous section over the controlled dataset. We attribute this decline in performance to the time-based mismatch between the harvested social honeypots and the large MySpace dataset. The honeypots were deployed in 2007, but the large MySpace data was crawled in 2006. As a result, the spam signatures developed from the honeypots have difficulty identifying spam in an earlier dataset when

Table 6: Example of "About Me" content in new deceptive spam profiles
"About Me" content I moved to san diego 3 months ago with my boyfriend, well, ex-boyfriend now . . . one thing i did find is this webcam site. it pays pretty decent and the best part is that its really fun, too . . . , click here to visit me.

1

0.8

0.6

0.4

0.2

0 Test Set #1

Test Set #2

Test Set #3

Figure 6: Twitter ­ Spam Precision

those spam signature may have not been in use at all. Even with these challenges, the results are fairly good.
As an example, Table 6 illustrates a legitimate-appearing profile in the first part of the "About Me" content, but then inserts a URL which links to an external site (usually porn or sexual sites) in the middle part or the last part of the "About Me" content.
5.3 Finding Unknown Spam on Twitter
Unlike the MySpace data mismatch, the social honeypots deployed on Twitter pre-date the large Twitter dataset collection. Hence, we are interested in this final experiment to discover if these honeypots can effectively detect new and emerging spam. For Twitter classification, we again relied on the training set consisting of 104 legitimate users' data, 61 spammers' data and 107 promoters' data which were used in Section 4. For prediction, we considered two cases: legitimate or spam+promoter. We randomly selected a test set of 1,000 users' data from the total dataset of about 210,000 users. We repeated this process three times, resulting in three test sets. The feature generator produces the same features used in the previous section. We selected Decorate as a classifier because it showed the best performance in the previous Twitter study. Human inspectors view spam data predicted by the classifier, and then decide whether or not they are real spam data. They will add newly found spam data with labels (spam or non-spam) to the training set in order to iteratively improve the classifier's accuracy.
Figure 6 presents spam precision results obtained from the three test sets. In each test set, the Decorate classifier predicted about 20 users as spammers. We assessed whether the predicted spammers were real spammers. In the first iteration, spam precision was 0.75, nearly matching the performance of the controlled classifier reported in the previous section. By the third iteration, the spam precision was 0.82. We see in this experiment how the social honeypots provide strong ability to discover unknown spam; and as these newly discovered spammers are added to the training set, the classifier becomes more robust (resulting in the improvement from the first to the third iteration).
As an example, Figure 7 shows an example of newly found spammer. The spammer's tweets include URLs which link to sex search tool pages. It is interesting that the spammer has 205 followers, meaning that this spammer has successfully inserted himself into the social network without detection. We additionally found that

441

Figure 7: An example of newly found spammer on Twitter
about 20% of the users predicted to be spammers were bots that post tweets automatically using the Twitter API.
Based on this large-scale evaluation of spam "in-the-wild", we can see how social honeypots can enable effective social spam detection of previously unknown spam instances. Since spammers are constantly adapting, these initial results provide positive evidence of the robustness of the proposed approach.
6. CONCLUSIONS AND NEXT STEPS
In this paper, we have presented the design and real-world evaluation of a novel social honeypot-based approach to social spam detection. Our overall research goal is to investigate techniques and develop effective tools for automatically detecting and filtering spammers who target social systems. By focusing on two different communities, we have seen how the general principles of (i) social honeypot deployment, (ii) robust spam profile generation, and (iii) adaptive and ongoing spam detection can effectively harvest spam profiles and support the automatic generation of spam signatures for detecting new and unknown spam. Our empirical evaluation over both MySpace and Twitter has demonstrated the effectiveness and adaptability of the honeypot-based approach to social spam detection.
In our continuing research, we are interested to explore a number of extensions. First, to what degree can traditional email and web spam approaches be incorporated into the social honeypot framework? For example, email spam and phishing approaches relying on data compression algorithms [6], machine learning [15, 21] and statistics [25] could inform the further refinement of the proposed approach. Similarly, web spam approaches have extensively studied the link structure of the web (e.g., [2, 3]); adapting these link-based approaches to the inherent social connectivity of online communities could further improve social spam effectiveness.
Second, we are interested to explore how social honeypots can be augmented by other recent approaches to deal with spam in social systems, including Heymann et al. [13] and Benevenuto et al. [4]. These prior approaches have focused on particular communities (e.g., social tagging systems, online video sharing sites); in what ways can their domain-specific techniques be incorporated into the social honeypot approach?
Finally, we are interested to expand and refine the social honeypots. For example, it may be worthwhile to both scale up the number of social honeypots (say, to the 1000s) and to consider more variation in the demographics and behaviors of the social honeypot profiles (say, by constructing clique-based social honeypots to measure whether honeypots that are more "connected" induce more spammer activity than "loner" honeypots.). Similarly, we are interested to diversify the domains in which we deploy the honeypots

(while respecting the terms of service of each community). Do we find that spammers engage in similar behaviors across domains? And if so, perhaps we can use this cross-domain spammer correlation to further improve the effectiveness of social spam detection.
7. ACKNOWLEDGMENTS
This work is partially supported by a Google Research Award and by faculty startup funds from Texas A&M University and the Texas Engineering Experiment Station.
8. REFERENCES
[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. Knowledge sharing and yahoo answers: everyone knows something. In WWW, 2008.
[2] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and R. Baeza-Yates. Link-based characterization and detection of web spam. In SIGIR Workshop on Adversarial Information Retrieval on the Web, 2006.
[3] A. A. Benczur, K. Csalogany, and T. Sarlos. Link-based similarity search to fight web spam. In SIGIR Workshop on Adversarial Information Retrieval on the Web, 2006.
[4] F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida, and M. Gonçalves. Detecting spammers and content promoters in online video social networks. In SIGIR, 2009.
[5] D. Boyd and J. Heer. Profiles as conversation: Networked identity performance on friendster. In HICSS, 2006.
[6] A. Bratko, B. Filipic, G. V. Cormack, T. R. Lynam, and B. Zupan. Spam filtering using statistical data compression models. J. Mach. Learn. Res., 7:2673­2698, 2006.
[7] G. Brown, T. Howe, M. Ihbe, A. Prakash, and K. Borders. Social networks and context-aware spam. In CSCW, 2008.
[8] J. Caverlee and S. Webb. A large-scale study of myspace: Observations and implications for online social networks. In ICWSM, 2008.
[9] G. V. Cormack. Email spam filtering: A systematic review. Found. Trends Inf. Retr., 1(4):335­455, 2007.
[10] D. J. Crandall, L. Backstrom, D. Huttenlocher, and J. Kleinberg. Mapping the world's photos. In WWW, 2009.
[11] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial classification. In SIGKDD, 2004.
[12] A. Felt and D. Evans. Privacy protection for social networking platforms. In Workshop on Web 2.0 Security and Privacy, 2008.
[13] P. Heymann, G. Koutrika, and H. Garcia-Molina. Fighting spam on social web sites: A survey of approaches and future challenges. IEEE Internet Computing, 11(6):36­45, 2007.
[14] T. N. Jagatic, N. A. Johnson, M. Jakobsson, and F. Menczer. Social phishing. Commun. ACM, 50(10):94­100, 2007.
[15] D. H. Joshua Goodman and R. Rounthwaite. Stopping spam. Scientific American, 292(4):42­88, April 2005.
[16] C. Kreibich and J. Crowcroft. Honeycomb: creating intrusion detection signatures using honeypots. SIGCOMM Comput. Commun. Rev., 34(1):51­56, 2004.
[17] Y.-R. Lin, H. Sundaram, Y. Chi, J. Tatemura, and B. L. Tseng. Splog detection using self-similarity analysis on blog temporal dynamics. In WWW Workshop on Adversarial Information Retrieval on the Web, 2007.
[18] A. Nazir, S. Raza, and C.-N. Chuah. Unveiling facebook: a measurement study of social network based applications. In SIGCOMM, 2008.
[19] M. F. Porter. An algorithm for suffix stripping. Program, 14(3):130­137, 1980. [20] M. B. Prince, B. M. Dahl, L. Holloway, A. M. Keller, and E. Langheinrich.
Understanding how spammers steal your e-mail address: An analysis of the first six months of data from project honey pot. In the Conference on Email and Anti-Spam (CEAS), 2005. [21] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A bayesian approach to filtering junk E-mail. In ICML Workshop on Learning for Text Categorization, 1998. [22] L. Spitzner. The honeynet project: Trapping the hackers. IEEE Security and Privacy, 1(2):15­23, 2003. [23] S. Webb, J. Caverlee, and C. Pu. Social honeypots: Making friends with a spammer near you. In the Conference on Email and Anti-Spam (CEAS), 2008. [24] I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques (Second Edition). Morgan Kaufmann, June 2005. [25] K. Yoshida, F. Adachi, T. Washio, H. Motoda, T. Homma, A. Nakashima, H. Fujikawa, and K. Yamazaki. Density-based spam detector. In SIGKDD, 2004. [26] A. Zinman and J. S. Donath. Is britney spears spam? In the Conference on Email and Anti-Spam (CEAS), 2007.

442

Studying Trailfinding Algorithms for Enhanced Web Search

Adish Singla
Microsoft Bing Bellevue, WA 98004 USA
adishs@microsoft.com

Ryen W. White
Microsoft Research Redmond, WA 98052 USA
ryenw@microsoft.com

Jeff Huang
University of Washington Seattle, WA 98195 USA
sigir@jeffhuang.com

ABSTRACT
Search engines return ranked lists of Web pages in response to queries. These pages are starting points for post-query navigation, but may be insufficient for search tasks involving multiple steps. Search trails mined from toolbar logs start with a query and contain pages visited by one user during post-query navigation. Implicit endorsements from many trails can enhance result ranking. Rather than using trails solely to improve ranking, it may also be worth providing trail information directly to users. In this paper, we quantify the benefit that users currently obtain from trailfollowing and compare different methods for finding the best trail for a given query and each top-ranked result. We compare the relevance, topic coverage, topic diversity, and utility of trails selected using different methods, and break out findings by factors such as query type and origin relevance. Our findings demonstrate value in trails, highlight interesting differences in the performance of trailfinding algorithms, and show we can find best-trails for a query that outperform the trails most users follow. Findings have implications for enhancing Web information seeking using trails.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process, selection process
General Terms
Algorithms, Experimentation, Human Factors
Keywords
Search trails, trailfinding, best-trail selection
1. INTRODUCTION
Web search engines provide keyword access to Web content. In response to search queries, these engines return lists of Web pages ranked based on estimated relevance. Information retrieval (IR) researchers have worked extensively on algorithms to effectively rank documents (c.f. [20]). However, research in areas such as information foraging [18], berrypicking [2], and orienteering [16], suggests that individual items may be insufficient for vague or complex information needs. In such circumstances, search results represent only the starting points of user exploration [17][21].
Logs containing the search engine interactions of many users have been mined extensively to enhance search-result ranking [1][13]. Richer log data from sources such as browser toolbars offers insight into the behavior of many users beyond search engines. Search trails comprising a query and post-query page views can be mined from these logs [28]. Although trail components--
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

origins (clicked search results) and destinations (trail end points [27]) have been used previously to support search, the typical application of trails is to better rank Web pages [1][3]. In As We May Think [4], Vannevar Bush envisioned using trails marked and willingly shared by trailblazing users to guide others. Joachims et al. [14] suggest that in many cases, only a sequence of pages and the knowledge about how they relate can satisfy a user's information need. This suggests that trails should be a unit of retrieval, or at least shown to users on the search engine result page (SERP). Although others have investigated trail generation for site or hypertext navigation [11][25], the challenge of finding the best trails to show to users directly on the SERP is unaddressed.
In this paper we present a log-based study of trailfinding for Web search. We mine trails from logs and investigate the value that full trails bring to users over the trail origins (i.e., the search results). We then represent trails as graphs and create algorithms to find the best trail for each search result--so-called trailfinding--using graph properties such as breadth, depth, and strength. Since "best" may be task dependent, we use a variety of metrics to evaluate the trails found. Our study answers the following questions: (i) How much benefit do users gather from following trails versus stopping after the origin page? (ii) Which trailfinding algorithms perform best? (iii) Can we extend our algorithms to handle unseen queries? We conduct this study using a log-based methodology since logs contain evidence of real user behaviors at scale and provide coverage of many types of information needs. Information need coverage is important since differences in algorithm performance may not hold for all search tasks. Our findings demonstrate value in trails, interesting differences in the performance of the algorithms, and performance tradeoffs when moving beyond logs to handle unseen queries using term matching.
2. RELATED WORK
A search trail consists of an origin page, intermediate pages, and a destination page. Origin pages are the search results that start a search trail. Query and origin pages from search engine click logs can be used to improve result set relevance [13]. Agichtein et al. [1] and Bilenko and White [3] found that using trails as endorsements for trail pages helped search engines learn to rank search results more effectively. The goal of their research was to improve ranking rather than show trails to users on the SERP. White et al. [27] added trail destination suggestions to the SERP. User study participants found destination suggestions useful. Our research extends that work to consider the suggestion of full trails rather than only destinations on the SERP. Prior to adding trails to result pages, we first study a variety of trailfinding methods to find performant algorithms that are worth further testing in user studies.
Systems such as WebWatcher [14], ScentTrails [15], and Volant [17] highlight candidate pages based on models of information needs or user interests. Studies of these systems show that they can improve search speed and search success. Highlighted pages form a trail over time, but the link-at-a-time approach does not expose the user to much needed initial context [14].

443

Wexelblat and Maes [25] introduced annotations in Web browsers called footprints, which are trails through a Website assembled by the site's designer. Their evaluation found that users required significantly fewer steps to find information using their system. Freyne et al. [10] extend footprints by adding icons to links to offer users visual cues. These cues are gathered from past users and include popularity, recency, and annotations. Wang and Zhai [24] continues the footprint metaphor in a topic map that lets users navigate to related queries, and to queries of varying specificity. Simulation studies revealed potential benefit from topic maps.
Guided tours and trails constructed by domain experts have been proposed, mainly in the hypertext community. Hammond and Allison [12] and Trigg [22] proposed guided tours in hypertext to ease problems of user disorientation. Zellweger [30] introduced scripted documents which are more dynamic than guided tours since they have conditional and programmable paths, automated playback, and active entries. Chalmers et al. [5] propose that human "recommenders" construct and share Web navigation paths.
Rather than requiring human intervention, tours and trails can also be generated automatically. Guinan and Smeaton [11] generate a tour for a given query based on term matching for node selection and inter-node relationships (e.g., "is_a", "precedes") for node ordering. In a user study using a collection of lecture materials, they found that users followed these trails closely; 40% of the time, subjects did not deviate from the proposed trail. Wheeldon and Levene [26] propose an algorithm for generating trails to assist in Web navigation. They define trails as trees and expand them from the root node using the expected information gain as the probability of expansion. This gain is based on the term frequency of the query in the document, with a penalty for duplicate URLs. They presented trails using an interface attached to the browser. User study participants found trails to be useful and noted that seeing the relationship between links helped.
We extend previous work in a number of ways: (i) we recommend full trails rather than only suggesting next steps; (ii) we focus on general Web search, where the content is less constrained than Websites or small hypertext collections, and information such as inter-node relationships is typically unavailable, and (iii) we find best-trails based on real user behaviors evident in logs, avoiding the scalability challenges associated with human intervention.
3. TRAILS
In this section, we describe the log data from which trails are extracted, outline trail mining, introduce some trailfinding algorithms, and describe unseen query handling using term matching.
3.1 Log Data
The primary source of data for this study was the anonymized logs of URLs visited by users who consented to provide interaction data through a widely-distributed browser plugin. Log entries include a unique user identifier, a timestamp for each page view, an identifier for each browser instance, and the URL of the Web page visited. Intranet and secure (https) URL visits were excluded at the source to maintain user privacy. Revisits to pages made through the browser "back" button are also captured in the log data. To remove variability caused by geographic and linguistic variation in search behavior, we only include entries generated in the English speaking United States locale. The results described in this paper are based on URL visits during a nine-month period from February 2009 through December 2009 inclusive, representing billions of URL visits from millions of unique users.

3.2 Trail Mining
From these logs, we mined around a billion search trails, each trail followed by a single user. Trails start with a search engine query (which also includes the SERP) followed by a click on one of the search engine results (trail origin). Search trails are represented as temporally-ordered URL sequences. Trails terminate once they reach 10 steps (to facilitate more controlled analysis later in the study) or a period of user inactivity of 30 or more minutes (also used in [8]), whichever condition is satisfied first. In our logs, there were 1.4 billion search trails followed by 80 million users.
This comprised 314 million unique queries ( ), 226 million
unique origins ( ), 542 million query-origin pairs, and 1.1 billion
unique search trails ( ). Figure 1 illustrates three search trails expressed as Web behavior graphs. Each trail starts with the same query ( 1) and the same origin URL ( ), then proceeds to different pages. The number in brackets on each node represents its sequence order in the trail based on timestamps of user activity.

Figure 1. Web behavior graphs illustrating three trails.

Properties of these behavior graphs, among other things, are used to find the best trails. We now describe the trailfinding algorithms.

3.3 Trailfinding Algorithms
The trailfinding task is defined: given a query and an observed
click to trail origin , find the trail in which has the largest , , . The scoring function can be defined in many ways.
In this study we experiment with a sample of techniques that include graph properties, relevance, and Web domain information.

Trail Length: The

, , for trail length is defined as the

length of in terms of the total number nodes following . This

algorithm prefers long trails which may be most engaging for

some users in terms of browsing activity (or could signify that

users are struggling to find useful information). The limitation is

that long trails could be obscure, especially if frequency is low.

Trails from Figure 1 ordered by length are: 3 (four nodes), 2

(three nodes), and 1 (one node).

Trail Breadth: The

, , for trail breadth is defined as

the number of branches in from the origin . In Figure 1, 2 has

the maximum trail breadth of two and would be the best trail in

the figure according to this algorithm. Broad trails let users ex-

plore various sub-topics while retaining the overall concept, e.g.,

users might look for specific e-cards within an e-card website.

Trail Depth: The

, , for trail depth is defined as the

maximum number of nodes on a single branch from the origin .

Deep trails are usually exploratory and can take users to new con-

cepts or topics. 3 is the "deepest" trail in Figure 1 (depth = 3).

444

Trail Frequency: The

, , is based on the frequency of

for a given query and origin . If we assume that in Figure 1,

1, 1, = 3;

2, 1, = 2 and

3, 1,

= 1, this algorithm would associate scores of 3, 2 and 1 to 1, 2

and 3 respectively. This algorithm favors short trails.

Trail Strength: Scoring trails based on their strength: (i) the en-

gaging potential of the behavior graph in terms of size, and (ii) the

ease of navigation. To estimate the strength of tree starting with

query and origin , we first compute the total frequency of all

navigations of type

with the user navigating to

from in trails starting with query q and origin r. That is:

,,

,,

(1)

where

, , is the frequency of for query and origin .

For 1, in Figure 1, post-SERP navigations over all three trails,

with frequencies as above are:

: 5;

:

2;

: 3;

: 1;

: 1;

: 1. Given this navigation model, trail strength is defined as:

,,

,,

(2)

This helps find long trails that are easy to navigate. Applying this

to trails in Figure 1 results in a trail ranking of 2 (

= 10)

followed by 3 (

= 6) and then 1 (

= 5).

Trail Diversity: The

, , is based on the number of

pages in whose Web domain (extracted automatically from the

URL string for each page) differs from that of the origin . In

Figure 1, if we assume the domain of URLs , and differs

from that of URL , then the trail ordering would be 3 (three

new domains), 2 (two new domains), and 1 (zero new domains).

Best-trails selected using this algorithm are diverse, offering the

user new information relative to the origin page.

Trail Relevance: The

, , for trail relevance for each

page is first calculated using the

%

,

%

then averaging these scores across all

pages in to obtain a final trail score. If in Figure 1, the URL of

contains all query words of 1 and title of and contains

all query terms of 1. The scores assigned to 1, 2 and 3 are 50,

75 and 20 respectively. This algorithm favors trails with query-

relevant titles and URLs, suggesting the trail itself is relevant.

3.4 Trailfinding Using Term Matching

The trailfinding algorithms described in this section so far rely on

an exact match between the user query and the query starting the

trails. The algorithms can be extended to associate trails to unseen

queries using term matching based on a variant of . . This is

important because over half of queries have never been seen by

the search engine [27]. Let { , , ... be terms in and for each

, get all trails in occurring from a prior

. The following

equation generates a score for each trail for query and origin :

,,

1

,

1

,, (3)

where , is the frequency with which appears in a query

leading to result click on ,

is the document frequency of

computed as the number of origins to which w is associated in

logs, and

, , is based on the trailfinding algorithms

above, e.g., breadth algorithm sets

, , as 's breadth.

4. EXPERIMENT
In this section we present the research questions that drive our study, summarize the trail data preparation, present metrics used to compare the algorithms, and describe the experimental variants.
4.1 Research Questions
Our study answers a number of research questions:
 RQ1: Of the trails and origins, which source: (i) provides more relevant information? (ii) provides more coverage and diversity of the query topic? (iii) provides more useful information?
 RQ2: Among trailfinding algorithms: (i) how does the value of best-trails chosen differ? (ii) what are the effects of query characteristics on best-trail value and selection? (iii) what is the impact of origin relevance on best-trail value and selection?
 RQ3: In associating trails to unseen queries: (i) how does the value of trails found through query-term matching compare to trails with exact query matches found in logs? (ii) how robust is term matching for longer queries (which may be noisy)?
4.2 Data Preparation
To help ensure experimental integrity, we did not use all search trails in . Instead, we filtered the data as discussed below.
4.2.1 Human Judged Query-URL Data
In addition to the trail data, we also obtained human relevance judgments for over eighty thousand queries that were randomly sampled by frequency from the query logs of a large search engine. Trained judges assigned relevance labels on a six-point scale --Bad, Poor, Fair, Good, Excellent, and Perfect--to top-ranked pooled Web search results for each query from the Google, Yahoo!, and Bing search engines during a separate search engine assessment activity. This led to relevance judgments for hundreds of pages for each query. These judgments allowed us to estimate the relevance of information encountered at different parts of the trails. We filtered original trail data so that the origins of the trails ( ) have human judgments for at least one query.
4.2.2 ODP Labeling
Two of the four evaluation metrics used in our study--coverage, and diversity--required information about page topicality and query interest. Firstly, we classified trail pages present in into the topical hierarchy from a popular Web directory, the Open Directory Project (ODP) (dmoz.org). Given the large number of pages involved, we used automatic classification. Our classifier assigned one or more labels to the pages based on the ODP using a similar approach to Shen et al. [19]. Classification begins with URLs present in the ODP and incrementally prunes non-present URLs until a match is found or miss declared. Similar to [19], we excluded Web pages labeled with the "Regional" and "World" top-level ODP categories, since they are location-based and are typically uninformative for constructing models of user interests. The coverage of our ODP classifier with URL back-off was approximately 65%. A missing or partial labeling of trail was allowed. Next, we constructed a set of query interest models for each query having human judged data. These models served as the ground truth for our estimates of coverage and diversity. A query's interest model comprises the ODP category labels assigned to the URLs in the union of the top-200 search results for that query from Google, Yahoo!, and Bing. ODP labels are grouped and their frequency values are normalized such that across all labels they sum to one. For example, the most popular labels in the interest model for the query [triathlon training], and their normalized frequencies ( ), are shown in Figure 2.

445

Label

/

/_

/

/

0.58

/

/_

/

/

0.21

/

/

/

0.11

Figure 2. Top ODP categories for [triathlon training].

To improve the reliability of our evaluation metrics, the query interest models had to be based on at least 50 fully-labeled search results (i.e., were not missing a label and did not have a label from an ignored category) and based only on category labels with a frequency of at least three (to reduce label noise).
4.2.3 Data Normalization and Pruning
We applied normalization and pruning to ensure data quality:

 All queries were normalized (involving removing punctuation, lowercasing, etc.) to facilitate comparability among trails and between the trails and other resources.
 Query-origin pairs were required to contain at least five unique trails and at least one trail of length exceeding two to maintain substantial variety for trailfinding.
 Common queries such as [facebook], [myspace], and [yahoo] contained thousands of short trails in the data since the ideal result for such queries presents users with a number of ways to branch into social networks or directory structure. To handle this, we first bucketed each query-origin pair in based on trail length. Then, for all trails of a particular length for each queryorigin pair, pruned the trails for which rank based on frequency was greater than 50 and ratio of frequency to maximum frequency for this bucket was less than 25%. This allowed us to maintain high variability in trail data yet remove many spurious trails for some common queries.
4.2.4 Query and Term-based Trail Data
Filtering and pruning reduced to 209 million trails, roughly 20% of its original size. We created two data sets from : (i) filtered
by queries with query interest models and human judgments, and (ii) created by splitting into terms and filtering by termorigin pairs in . comprises 20 thousand unique queries, 109
thousand unique origins, 139 thousand query-origin pairs and 20 million unique trails. comprises 15 thousand unique query terms, 109 thousand unique origins, 265 thousand term-origin pairs and 78 million unique trails. This filtering created highquality data sets for our log-based investigation.
4.3 Metrics
We used four metrics to compare the best-trails selected using our trailfinding algorithms to compare sources (origins and trails). The metrics were coverage, diversity, relevance, and utility. These metrics were chosen to capture many important elements of information seeking, as highlighted by relevant research (e.g., [6] [7]). The use of multiple metrics allows us to compare the value of the sources in different ways and also understand how the trailfinding algorithms affect different aspects of information gain.
4.3.1 Coverage
Topic coverage is meant to reflect the value of each source in providing access to the central themes of the query topic. To estimate the coverage of each of source, we created a source interest model ( ) comprising ODP labels for each source assigned as described in Section 4.2.2. We then computed the fraction of the query interest model ( ) covered by . That is:

,

(4)



where l is an ODP label and represents the normalized frequen-

cy weight of that label in the query interest model .

4.3.2 Diversity
Topic diversity estimates the fraction of unique query-relevant concepts surfaced by a given source. Exposure to different perspectives and ideas may help users for complex or exploratory search tasks. To estimate the diversity of the information provided by each source we use an approach similar to our coverage estimation, but we only require the fraction of distinct category labels
from that appear in (i.e., frequency is ignored). That is:

,

1 ||



where l is an ODP label and | | is number of distinct

(5) labels.

4.3.3 Relevance
The next metric used to compare the trail sources was relevance to the query that initiated the trail. For each trail, we computed the average relevance judgment score with respect to the query. In this analysis, the missing judgments for a page were labeled Bad since the judged label data was quite exhaustive for each query and hence missing pages may signify irrelevance to the query.

4.3.4 Utility
We also studied the utility of each source, estimated using dwell time (i.e., the amount of time spent on a particular page by a user). Prior research has demonstrated that during search activity, a dwell time of 30 seconds or more on a Web page can be indicative of page utility [9]. We apply this threshold in our analysis to determine if a source contains at least one page of utility

In all metrics used, a higher value is more positive. The metrics are computed for each source, micro-averaged within each query, and macro-averaged across all queries to obtain a single value for each source-metric pair. This ensures that all queries are treated equally and popular queries do not dominate aggregate metrics. More detail on the metrics is provided by White and Huang [29].

4.4 Methodology
In this section so far we have described the research questions, the
trail data preparation procedures, and the metrics used to evaluate the sources. Our methodology comprised the following steps:

1. For each search trail in , assign ODP labels to all pages in . Build source interest models for origin page and full trail. Compute metrics using methods described in Section 4.3.
2. For each query-origin pair, select the best trail using each trailfinding algorithm ( _ ). For each trail in _ , compute metrics. Split findings on query length, query type (informational versus navigational), and origin relevance.
3. For each query-origin pair, find the best trail by applying the term-matching approach to , generate a trail set _ , and compare trails in _ to those in _ using our metrics.
5. FINDINGS
We report findings separately for each of our three research questions. We use parametric statistical testing where appropriate. Given the large sample sizes, all observed differences are significant at < 0.01 unless otherwise stated.

446

Table 1. Comparison of full trails relative to origins. Segments based on trail length to study effect of length on the metrics. Numbers are averages. Underlined numbers represent statistically-significant differences w.r.t the origin ( 0.01) based on paired -tests.

Segment
All 2 3-5
6-10

Metrics (or just  for relevance)are computed relative to origin as baseline)

Trail Source
Origin Full Trail

Coverage (
8.5 9.7 (+14)

Diversity

5.7 6.6 (+15)

Relevance (
2.9 1.0 (-1.9)

Utility

43.7 82.8 (+89)

Origin Full Trail

8.5 9.2 (+8)

5.7 6.2 (+9)

2.9 1.6 (-1.4)

43.9 72.4 (+65)

Origin Full Trail

8.5 9.7 (+14)

5.7 6.6 (+15)

2.9 1.0 (-2.0)

43.7 83.7 (+92)

Origin Full Trail

8.7 10.3 (+19)

5.8 7.1 (+21)

2.9 0.5 (-2.5)

43.6 92.0 (+111)

Trail Statistics

#Queries

#QueryOrigins

#Trails

20,521

139,592 20,429,904

20,143

134,495 1,687,304

20,416

137,172 6,801,382

19,615

122,490 11,941,218

Trail Length

5.1 RQ1: Effectiveness of Trails vs. Origins
Table 1 shows summary statistics and reports on the average performance of trails and origins over all trails in . Significance
testing involved paired -tests with Bonferroni corrections.
5.1.1 Different Metrics
Coverage was computed using Equation 4. The average coverage scores of trails and origins are reported in the "All" row of Table 1. Full trails show a 14% increase in topic coverage over origins.
Diversity was computed using Equation 5. The average diversity scores of trails were 15% higher than origins.
Coverage and diversity increases for trails over origins reflect the extra information that users find during post-origin navigation. Although it seems that most of the value comes from origin pages, users can still derive value from trails, including benefits not captured by our metrics (e.g., topic novelty).
Relevance was computed using human relevance judgments on a six-point scale ranging from Bad (rating=0) to Perfect (rating=5). While the relevance of origins is on average Good, the average relevance of trails is Poor. We attribute this to mapping missing judgments for deep links in trails to the label Bad, perhaps related to dynamism in users' information needs as they search [29].
Utility was estimated using dwell times. Findings show that just under half of origins are useful (43.7%) and over three-quarters of trails have useful pages (82.8%). This shows that the likelihood of finding a useful page via navigation is high, a finding supported by previous work on post-query search behavior [23]. This may also be because origins are search results, typically the starting points for a task, and hence have rapid click-though [17][21].
5.1.2 Effect of Trail Length
To determine the effect of trail length on trail performance, we segmented all search trails into three segments based on length=2, length=3-5, and length=6-10. We did this because: (i) there were insufficient trails for a segment for each length, and (ii) so that we could maintain usable levels of trail variety in each segment. The findings are reported in Table 1 adjacent to "Trail Length." First, even small trails of length 2 added value over origins in terms of coverage, diversity and utility (coverage:+8%, diversity:+9%, utility:+65%). Second, trail length appears to affect trail value. For example, coverage increased from 9.2 to 10.3 (the gain over origin increased from 8% to 19%) in moving between length=2 to length=6-10. This suggests that the longer the trail, the more different topic-related information users are exposed to.

The above findings show that trails can deliver value to users over origins. Even small trails of size 2 can add significant value. Although further study is needed, this analysis suggests that trails may be a useful addition to results on the SERP. Once we know that showing trails may help, the next step is deciding which trails to show. We now report on trailfinding algorithm performance.
5.2 RQ2: Trailfinding Algorithms
We compare the best trails from _ selected by each of the seven algorithms for each query-origin pair. We used origins-only as a baseline for the algorithms. Results are shown in Table 2. Independent-measures analysis of variance (ANOVA) were used among eight sources (seven best full trails + origin) for each metric to measure statistical significance. Also, we carried out posthoc Tukey tests to show if best-trails were significantly better than origins. To select the best algorithm(s), each algorithm is first given votes equal to the number of algorithms it performs significantly better than, using post-hoc Tukey tests ( 0.01). Those algorithms with the most votes performed best for each metric.
5.2.1 Different Metrics
Coverage: Frequency-based trails performed worst among seven algorithms with gain of only 11% over origin (9.4 vs. 8.5). This may be because frequent trails are typically short and may cover less of the topic space. Best-trails based on tree-size and treestrength had average gain of 20% over origins. The trail diversity algorithm performed best with an average gain of 27% (10.7 vs. 8.5), perhaps because different domains discuss different aspects. Even though trails found by the diversity and strength algorithms were shorter than those found by the trail length algorithm, they covered more of the query topic. These and the findings for other metrics show that there are often better criteria than just length.
Diversity: These findings are somewhat similar to the coverage metric. Length-based trails and strength-based trails have an average gain of 22% over the origin. As expected, the diversity algorithm performed best with on average a 30% gain (7.5 vs. 5.7).
Relevance: Trails selected based on relevance scoring have the highest relevance of 1.4 (Poor-Fair). Length and depth based trails performed worst, each having average relevance of 0.5 (Bad-Poor). In long or deep trails, users may get sidetracked or information needs evolve during searching [2].
Utility: Best-trails based on trail length have highest utility with an average increase of 109% over origins (91.2 vs. 43.7). It seems that the longer the trail, the more likely a user finds a useful page.

447

Origin Relevance

Table 2. Average performance of trail selection algorithms for query and term matching approaches. Underlined numbers represent statistically-significant difference relative to origin ( 0.01) based on post-hoc Tukey tests. Bold numbers within each segment represent the trailfinding algorithm(s) that is/are significantly better than the other algorithms most frequently ( 0.01 using post-hoc Tukey tests). The "Origin" rows have the average metric scores across all origins. The "All Trails" rows have the average metric scores across all trails.

Words > 3
(3662, 14320)

Words =1
(4514, 38830)

Worst Origin (6324,
11754)

Best Origin (13614, 25890)

All (20521, 139592)

Segment (#Queries, Algorithm
#Query-Origins)
Origin All Trails Length Diversity Breadth
Depth Relevance Frequency Strength
Origin All Trails Length Diversity Breadth
Depth Relevance Frequency Strength
Origin All Trails Length Diversity Breadth
Depth Relevance Frequency Strength
Origin All Trails Length Diversity Breadth
Depth Relevance Frequency Strength
Origin All Trails Length Diversity Breadth
Depth Relevance Frequency Strength

Coverage (
Full query Term match 8.5
9.7 (+14)

10.2 (+20) 10.2 (+21)

10.7 (+27) 11.1 (+30)

9.9 (+17) 10.1 (+19)

10.1 (+18) 10.1 (+18)

9.5 (+12) 9.4 (+11)

9.4 (+11)

9.3 (+9)

10.2 (+20) 10.2 (+20)

10.0 11.1 (+11)

11.4 (+14) 11.5 (+15)

12.1 (+21) 12.3 (+23)

11.4 (+14) 11.5 (+15)

11.3 (+13) 11.4 (+14)

10.8 (+8) 10.8 (+8) 11.5 (+15)

10.8 (+8) 10.7 (+7) 11.5 (+15)

7.5

9.0 (+20)

9.7 (+29) 9.7 (+29)

10.2 (+36) 10.6 (+41)

9.3 (+24) 9.6 (+27)

9.5 (+27) 9.0 (+20) 8.9 (+18) 9.7 (+29)

9.6 (+28) 8.9 (+18) 8.6 (+15) 9.7 (+29)

8.4

9.6 (+14)

10.0 (+19) 10.8 (+28) 9.8 (+16) 9.9 (+18) 9.5 (+13)

10.1 (+20) 10.9 (+30) 9.9 (+18) 9.9 (+18) 9.5 (+13)

9.3 (+11) 9.2 (+10)

10.1 (+20) 10.1 (+20)

8.4

9.5 (+14)

10.0 (+20) 10.5 (+26)

10.1 (+21) 11.0 (+31)

9.8 (+17) 10.0 (+20)

9.9 (+18) 9.9 (+19)

9.3 (+11)

9.1 (+9)

9.3 (+11)

9.1 (+9)

10.0 (+20) 10.1 (+21)

Metrics

Diversity (
Full query Term match 5.7
6.6 (+15)

Relevance (
Full query Term match 2.9
1.0 (-1.9)

7.0 (+22) 7.1 (+23) 0.5 (-2.4) 0.4 (-2.5)

7.5 (+30) 7.7 (+35) 0.9 (-2.1) 0.7 (-2.2)

6.8 (+19) 6.9 (+21) 0.7 (-2.2) 0.6 (-2.3)

6.9 (+21) 6.9 (+20) 0.5 (-2.4) 0.4 (-2.5)

6.4 (+13) 6.4 (+11) 1.4 (-1.5) 1.5 (-1.4)

6.4 (+12) 6.3 (+10) 1.3 (-1.6) 1.5 (-1.4)

7.0 (+22) 7.0 (+22) 0.6 (-2.3) 0.6 (-2.4)

6.5 7.3 (+12)

4.3 1.3 (-2.9)

7.7 (+17) 7.7 (+18) 0.6 (-3.6) 0.6 (-3.7)

8.2 (+26) 8.4 (+29) 1.0 (-3.2) 0.8 (-3.4)

7.6 (+17) 7.7 (+18) 0.9 (-3.3) 0.8 (-3.5)

7.6 (+16) 7.6 (+16) 0.7 (-3.6) 0.6 (-3.7)

7.1 (+9) 7.1 (+9) 7.7 (+18)

7.1 (+9) 7.1 (+8) 7.7 (+18)

2.1 (-2.2) 2.1 (-2.2) 0.8 (-3.5)

2.2 (-2.0) 2.2 (-2.0) 0.8 (-3.5)

5.1

0.0

6.1 (+20)

0.1 (+0.1)

6.6 (+29) 6.6 (+30) 0.1 (+0.1) 0.1 (+0.1)

7.0 (+37) 7.3 (+43) 0.1 (+0.1) 0.1 (+0.1)

6.3 (+24) 6.6 (+29) 0.1 (+0.1) 0.1 (+0.1)

6.5 (+27) 6.1 (+19) 6.0 (+18) 6.6 (+29)

6.5 (+28) 6.0 (+17) 5.8 (+14) 6.7 (+30)

0.1 (+0.1) 0.2 (+0.2) 0.1 (+0.1) 0.1 (+0.1)

0.1 (+0.1) 0.2 (+0.2) 0.1 (+0.1) 0.1 (+0.1)

5.9

2.7

6.8 (+15)

0.9 (-1.8)

7.1 (+21) 7.8 (+32) 6.9 (+18) 7.1 (+19) 6.7 (+13)

7.2 (+22) 7.9 (+33) 7.0 (+19) 7.1 (+20) 6.6 (+13)

0.4 (-2.3) 0.8 (-2.0) 0.7 (-2.1) 0.5 (-2.3) 1.4 (-1.3)

0.4 (-2.3) 0.7 (-2.1) 0.6 (-2.1) 0.4 (-2.3) 1.4 (-1.3)

6.6 (+11) 6.5 (+10) 1.3 (-1.5) 1.4 (-1.4)

7.2 (+21) 7.2 (+22) 0.5 (-2.2) 0.5 (-2.2)

5.6

3.1

6.4 (+16)

1.1 (-2.0)

6.9 (+23) 7.2 (+30)

6.9 (+25) 7.7 (+38)

0.5 (-2.6) 0.9 (-2.2)

0.4 (-2.7) 0.7 (-2.4)

6.6 (+20) 6.9 (+23) 0.8 (-2.3) 0.6 (-2.5)

6.7 (+21) 6.8 (+22) 0.6 (-2.5) 0.4 (-2.6)

6.2 (+12) 6.1 (+10) 1.4 (-1.7) 1.6 (-1.5)

6.2 (+12)

6.1 (+9)

1.4 (-1.7) 1.6 (-1.5)

6.8 (+23) 6.9 (+24) 0.6 (-2.5) 0.6 (-2.5)

Utility (
Full query Term match 43.7
82.8 (+89)

91.2 (+109) 91.6 (+110)

85.4 (+95) 88.6 (+103)

87.6 (+100) 89.3 (+104)

89.9 (+106) 90.3 (+107)

76.4 (+75) 74.5 (+70)

77.2 (+77) 73.8 (+69)

90.1 (+106) 90.6 (+107)

43.0 83.9 (+95)

90.6 (+111) 90.9 (+111)

86.7 (+102) 89.3 (+108)

88.8 (+107) 90.2 (+110)

89.0 (+107) 89.2 (+107)

75.0 (+74) 74.9 (+74) 90.1 (+110)

73.8 (+72) 72.6 (+69) 90.6 (+111)

39.7

80.2 (+102)

90.4 (+128) 90.6 (+128)

84.0 (+112) 88.3 (+123)

85.3 (+115) 87.6 (+121)

89.1 (+125) 73.8 (+86) 75.3 (+90) 88.8 (+124)

89.2 (+125) 71.5 (+80) 70.2 (+77) 89.2 (+125)

42.7

82.8 (+94)

90.7 (+112) 86.2 (+102) 87.4 (+105) 89.5 (+109) 74.6 (+74)

91.4 (+114) 88.6 (+107) 88.9 (+108) 90.0 (+110) 74.5 (+74)

76.7 (+79) 75.4 (+76)

89.5 (+109) 90.2 (+111)

46.1

84.2 (+83)

92.2 (+100) 92.9 (+101) 86.2 (+87) 90.0 (+95)

88.4 (+92) 90.4 (+96)

91.1 (+97) 91.4 (+98)

79.8 (+73) 76.0 (+65)

78.9 (+71) 74.2 (+61)

91.4 (+98) 91.9 (+99)

Query Length

448

5.2.2 Breakdown Based on Origin Relevance
We also studied the effect of origin relevance on algorithm performance to determine whether best-trails add value to all results or only those with high or low origin quality. We divided the data into two buckets: one with origins having the highest humanjudged label for the query (note that this need not be Excellent) and another with origins judged Poor or Bad.
Best Origins: The coverage of origins increased from 8.5 to 10.0. More relevant origins appear to cover more of the topic space. Also, the coverage values of all best-trail algorithms have increased; for example, trails selected based on diversity increase coverage from 10.7 to 12.1 However, the percentage gain of full trails over origins has decreased to a maximum value of +21% for diverse trails (12.1 vs. 10.0) which is lower than the 27% coverage gain for all origins. This can be explained by the fact that when origins are high quality, the value added by trails drops. Similar results are observed for diversity. Second, trails found using relevance-based scoring performed fairly well: average relevance of Fair as compared to 1.4 (Poor-Fair) for all origins. This suggests that relevant origins may also link to relevant pages.
Worst Origins: While the absolute coverage values of origin and full trails decreased, the percentage gain from trails increased across all trail selection algorithms. Diverse trails again performed best with an average increase of 36% compared to origin (10.2 vs. 7.5). This almost doubles the 21% increase we observed for best origins. Similar trends can be seen for diversity. Second, there was a decrease in utility for origins whereas some trail selection algorithms showed an increase.
5.2.3 Breakdown Based on Query
We studied the effect of query length and query intent on trail performance to determine whether trails were equally useful for all queries. We segmented query length in three ways: length=1, length=2-3, and length > 3 (long queries). For query intent, we segmented the queries into navigational and informational intent based on click frequencies in search engine logs separate from those used in this study. Per our definition, navigational queries led to a click on the same search result 95% of the time; informational queries led to on average two or more different result clicks per query. The results from query intent were somewhat aligned with that of breakdown based on origin quality. Clear intent queries had trends similar to experiments with best origins and informational queries had trends similar to those of worst origins. Due to space constraints, we only discuss results on query length since those are also important for RQ3. The experiments on query length showed no major difference among trailfinding algorithms. We observe similar behavior in terms of relative differences of full trails versus origins. On coverage and diversity metrics, the relevance-based trailfinding algorithm failed to obtain significant differences relative to the origin on long queries. Recall that the relevance-based scoring finds trails based on the match between the query terms and trail titles/URLs. For longer queries, there may be more noise in the queries and the trails found may not cover as much of the query topic space. Another interesting finding was in the absolute values of utility of trails and origins. On long queries, utility increased, suggesting users spent more time on Web pages following those queries.
Interestingly, across all trails and the various segmentations there is at least one trailfinding algorithm (and often many) that outperforms the average over all trails followed by users (shown in the "All Trails" rows). This suggests that trailfinding algorithms may

be helpful to users, at least in cases where the benefit brought by the algorithm (e.g., a boost in diversity) matches the user intent.
5.3 RQ3: Trailfinding Using Term Matching
Next we report the quality of trails found using the term-matching based approach described in Section 3.4. We use this approach to find trails from for all query-origin pairs for which we have best-trails selected from _ . This leads to a new set called
_ . Note that: (i) for comparability the same queries appeared in both sets, and (ii) creating _ could result in associating new trails to query-origin pairs which were not logged.
The average performance numbers for the trails in _ are reported in Table 2 alongside those obtained from best-trails of .
First, the results from all best trail selections from term-based approaches have similar trends as that of best trails selections from the query based approach. This strongly suggests that our trail selection criteria can be effectively applied to unseen queryorigin pairs. Second, the segment based on query length suggests robustness of this technique for longer queries, which posed a challenge because of possible noise. Thirdly, term-based trails have occasionally higher coverage and diversity. For example: for diversity-based best-trails starting with long queries, we have a coverage of 11.0 (i.e., 31% gain over origin) for term-based and 10.5 (i.e., 26% gain over origin) for query-based trails. Fourth, relevance dropped from 0.9 to 0.7. This suggests that despite the coverage and diversity gains, term-based trails are slightly less relevant than query-based trails, perhaps because the term-based technique finds trails that may only be partially query relevant.
6. DISCUSSION AND IMPLICATIONS
We have described a log-based study of various trailfinding algorithms to support post-query search interaction. Trails are selected from the search and browsing logs of many users. Our findings show that users' trails bring them value, best-trails can be chosen that outperform users' own trails, different trailfinding algorithms perform well under different metrics, and a term-matching variant lets algorithms effectively handle unseen queries.
Our first research question compared the value of trails with origin pages. The findings showed a significant increase in value for trails over origins across almost all metrics except relevance when we normalized for trail length. As more information is viewed by users, there is more opportunity for them to gain. Relevance degraded because un-judged pages were labeled Bad. If we ignore un-judged pages, trails have the same relevance as origins.
Since search trails appeared to demonstrate value over origins, the next research question addressed the issue of whether we could find the trail from the available options that maximized coverage, diversity, relevance, and/or utility. Although there was no clear winner, the findings were roughly in line with our intuitions. The diversity algorithm that preferred trails with multiple domains performed best in terms of coverage and diversity and the relevance-based algorithm preferring trails with a high query-totitle/URL match performed best in terms of relevance. Trail length algorithms had the best utility, perhaps because the longer the trail, the more likely that users would encounter a useful page. On average, trailfinding outperforms the trails that users follow themselves. This suggests that there is typically a trail with higher return than that followed by a user and, may improve a user's search effectiveness if shown. It also allows us to exclude underperforming algorithms from further study (e.g., frequency, strength).

449

As part of this analysis we studied the effects of origin quality, query type, and query length on trailfinding algorithm performance. The findings showed differences in the effectiveness of the algorithms depending on origin quality and query characteristics. Trails may not be appropriate for all search results and more work is needed to determine which results or queries deserve trails, to investigate trailfinding algorithms, and to explore ways to effectively select between these algorithms given different user needs. For example, if the user cares about topic coverage, then we should select trails based on the trail diversity algorithm.
The final research question addresses whether our trailfinding approach could be adapted to handle unseen queries. Findings showed that performance was roughly equivalent between the best trails selected from the logs and those generated based on our algorithm. The term-based approach saw an increase in the coverage and diversity and a decrease in relevance. This could be part of a backoff strategy where we search within trails in logs and use those chosen through term matching if no trails are found.
One limitation of this research is the assumption that there is a best trail for each query-result pair. It is conceivable that there will be multiple equivalent or complementary trails for any pairing. Ways to tiebreak between trails (e.g., showing trails that the user has not yet traveled) need to be explored. More work is needed to validate metrics used, in particular measures of coverage, diversity, and utility currently inferred from interactions (e.g., [29]).
The next step in our research is to show trails on SERPs. Trails can be presented as an alternative to result lists, as instant answers above result lists, in pop-ups shown after hovering over a result, below each result in addition to the snippet and URL, or even on the click trail the user is following. Although we are limited by what can be inferred from log data, our approach has provided insight on what algorithms perform best and when. Follow-up user studies and large-scale flights are planned to compare trail presentation methods and further analyze trailfinding techniques.
7. CONCLUSIONS AND FUTURE WORK
In this paper we have presented a study of trailfinding techniques to support Web search. We employed a log-based methodology to afford us control over experimental variables and rapidly test multiple trailfinding algorithms. We showed that trails provided additional value over trail origins, especially for longer trails that may contain more varied information. We experimented with different trailfinding algorithms and showed that they can outperform trails followed by most users; their performance was affected by the relevance of the origins and query characteristics, meaning that trails may need to be tailored to query and result properties. We also tested a term matching variant that alleviated the need for an exact term match between queries and trails, which led to coverage and diversity gains at the cost of a slight decrease in relevance. In future work we will integrate best-trails into search engine result pages and conduct user studies on their effectiveness.
REFERENCES
[1] Agichtein, E., Brill, E. & Dumais, S. (2006). Improving web search ranking by incorporating user behavior information. Proc. SIGIR, 19-26.
[2] Bates, M.J. (1989). The design of browsing and berrypicking techniques for the online search interface. Online Review, 13(5): 407-424.
[3] Bilenko, M. & White, R.W. (2008). Mining the search trails of surfing crowds: identifying relevant websites from user activity. Proc. WWW, 51-60.

[4] Bush, V. (1945). As we may think. Atl. Monthly, 3(2): 37-46. [5] Chalmers, M., Rodden, K. & Brodbeck, D. (1998). The order
of things: activity-centered information access. Proc. WWW. [6] Clarke, C.L.A. et al. (2008). Novelty and diversity in infor-
mation retrieval evaluation. Proc. SIGIR, 659-666. [7] Cole, M. et al. (2009). Usefulness as the criterion for evalua-
tion of interactive information retrieval. Proc. HCIR, 1-4. [8] Downey, D., Dumais, S. & Horvitz, E. (2007). Models of
searching and browsing: languages, studies, and application. Proc. IJCAI, 2740-2747. [9] Fox, S. et al. (2005). Evaluating implicit measures to improve the search experience. TOIS, 23(2): 147-168. [10] Freyne, J. et al. (2007). Collecting community wisdom: integrating social search and social navigation. IUI, 52-61. [11] Guinan, C. & Smeaton, A.F. (1993). Information retrieval from hypertext using dynamically planned guided tours. Proc. ECHT, 122-130. [12] Hammond, N. & Allison, L. (1988). Travels around a learning support environment: rambling, orienteering, or touring? Proc. SIGCHI, 269-273. [13] Joachims, T. (2002). Optimizing search engines using clickthrough data. Proc. SIGKDD, 133-142. [14] Joachims, T., Freitag, D. & Mitchell, T. (1997). WebWatcher: a tour guide for the world wide web. Proc. IJCAI, 770-775. [15] Olston, C. & Chi, E.H. (2003). ScentTrails: integrating browsing and searching on the web. TOCHI, 10(3): 1-21. [16] O'Day, V. & Jeffries, R. (1993). Orienteering in an information landscape: how information seekers get from here to there. Proc. INTERCHI, 438-445. [17] Pandit, S. & Olston, C. (2007). Navigation-aided retrieval. Proc. WWW, 391-400. [18] Pirolli, P. & Card, S.K. (1999). Information foraging. Psychological Review, 106(4): 643-675. [19] Shen, X., Dumais, S. & Horvitz, E. (2005). Analysis of topic dynamics in web search. Proc. WWW, 1102-1103. [20] Singhal, A. (2001). Modern information retrieval: a brief overview. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 24(4): 35-43. [21] Teevan, J. et al. (2004). The perfect search engine is not enough: a study of orienteering behavior in directed search. Proc. SIGCHI, 415-422. [22] Trigg, R.H. (1988). Guided tours and tabletops: tools for communicating in a hypertext environment. TOIS, 6(4). [23] Vakkari, P. & Taneli, M. (2009). Comparing google to ask-alibrarian service for answering factual and topic questions. Proc. EDCL, 352-363. [24] Wang, X. & Zhai, C. (2009). Beyond hyperlinks: organizing information footprints in search logs to support effective browsing. Proc. CIKM, 1237-1246. [25] Wexelblat, A. & Maes, P. (1999). Footprints: history-rich tools for information foraging. Proc. SIGCHI, 270-277. [26] Wheeldon, R. & Levene, M. (2003). The best trail algorithm for assisted navigation of web sites. Proc. LA-WEB, 166. [27] White, R.W., Bilenko, M., & Cucerzan, S. (2007). Studying the use of popular destinations to enhance web search interaction. Proc. SIGIR, 159-166. [28] White, R.W. & Drucker, S.M. (2007). Investigating behavioral variability in web search. Proc. WWW, 21-30. [29] White, R.W. & Huang, J. (2010). Assessing the scenic route: measuring the value of search trails in web logs. Proc. SIGIR [30] Zellweger, P.T. (1989). Scripted documents: a hypermedia path mechanism. Proc. Hypertext, 1-14.

450

Context-Aware Ranking in Web Search

Biao Xiang1 Daxin Jiang2 Jian Pei3 Xiaohui Sun2 Enhong Chen1 Hang Li2
1University of Science and Technology of China 2Microsoft Research Asia 3Simon Fraser University 1{bxiang, cheneh}@ustc.edu.cn 2{djiang, xiaos, hangli}@microsoft.com 3jpei@cs.sfu.ca

ABSTRACT
The context of a search query often provides a search engine meaningful hints for answering the current query better. Previous studies on context-aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. Particularly, about context-aware ranking for Web search, the following two critical problems are largely remained unsolved. First, how can we take advantage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically. We develop different ranking principles for different types of contexts. Moreover, we adopt a learning-to-rank approach and integrate the ranking principles into a state-of-the-art ranking model by encoding the context information as features of the model. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and implicit user click data. The experimental results clearly show that our context-aware ranking approach improves the ranking of a commercial search engine which ignores context information. Furthermore, our method outperforms a baseline method which considers context information in ranking.
Categories and Subject Descriptors
H.3.m [Information Storage and Retrieval]: Search process
General Terms
Algorithms, Experimentation
Keywords
Context-aware ranking, learning-to-rank application
The work was done when Biao Xiang was an intern at Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
In Web search, given a query, a search engine returns the matched documents in a ranked list to meet the user's information need. Ranking models play a central role in search engines. Currently, almost all the existing ranking models consider only the current query and the documents, but do not take into account any context information such as the previous queries in the same session and the answers clicked on or skipped by the user to the previous queries. In other words, almost all the current ranking models are insensitive to context.
Information Retrieval research has well recognized that context information is very helpful in achieving good search results. Context information may provide hints about users' search intent and help to make better matching with documents. For example, if a user raises a query "jaguar" after she searches "BMW", it is very likely that the user is seeking for information about a Jaguar car rather than a jaguar as an animal. The absence of context information in document ranking models is probably partially due to the difficulty of obtaining context information. Only recently have large amounts of search session data become available, which enable large scale empirical studies on context-aware methods for Web search.
Several recent studies explore context-aware search methods from different angles. Shen et al. [15] presented a contextaware ranking method by assuming that context information can better represent search intent. They incorporated the context information to build context-aware language models, which were assumed to give rise to documents not only similar to the current query but also similar to the previous queries and the summaries of the documents clicked on. The study confirmed the effectiveness of the ranking model on TREC data (http://trec.nist.gov). However, the evaluation was based on a small data set consisting of only thirty sessions from three subjects under a controlled laboratory setting. It is unclear whether the assumption in the study holds for Web search engines in the real world.
More recently, Cao et al. [2, 3, 4] extracted context information in Web search sessions by modeling search sessions as sequences of user queries and clicks. They learned sequential prediction models such Hidden Markov Model and Conditional Random Fields from search log data. Different from our study here, their models were designed for predicting search intents based on context information, but not for ranking. Therefore, the models are more suitable for query suggestion, query categorization, and URL recommendation than search results ranking, as will be further analyzed in Section 2.
In spite of the several existing studies on context-aware

451

search methods, the following two critical problems about context-aware ranking for Web search are largely remained unsolved. First, how can we take advantage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically and make the following contributions.
We develop four different ranking principles for different types of contexts. Those principles promote or demote documents according to the context of the current query. We evaluate the four principles using real Web search sessions, and confirm the effectiveness of three principles through the significance test on the data. Interestingly, only one of the three effective principles is consistent with the findings obtained by Shen et al. [15] on TREC data, indicating that Web search is quite different from search on TREC data.
Moreover, we adopt a learning-to-rank approach and integrate the ranking principles into a state-of-the-art ranking model, RankSVM [7], by encoding the context information as features. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and implicit user click data. The experimental results clearly show that our context-aware ranking approach improves the ranking of a commercial search engine which ignores context information. Furthermore, our method outperforms a baseline method which considers context information in ranking.
The rest of the paper is organized as follows. We review related work in Section 2. We discuss the types of contexts, propose ranking principles, and evaluate the effectiveness of the principles in Section 3. We incorporate context information into a learning-to-rank model in Section 4. The experimental results are reported in Section 5. The paper is concluded in Section 6.
2. RELATED WORK
Different users may prefer different results for the same query. Personalized search (e.g., [5, 12, 16, 17, 18]) aims to provide the most relevant search results to individual users based on their interests. Traditional personalization approaches usually build a profile of interests for each user from her/his search or browsing history.
Context information is useful in identifying users' search needs. Context-aware search adapts search results to individual search needs using contexts. While personalized search considers individual users' long and/or short histories, context-aware search focuses on short histories of all users. Research on context-aware search has been concentrated on modeling contexts. For example, Cao et al. [2] mined frequent sequential patterns from search sessions for context-aware query suggestion. Cao et al. [4] modeled contexts containing both queries and clicks within sessions by learning a variable length Hidden Markov Model for query suggestion, URL recommendation, and document re-ranking. Cao et al. [3] incorporated context information into a Conditional Random Field (CRF) model for better query classification. Those models were mainly designed for inferring and predicting user search intents using context information. Therefore, they are more suitable for tasks such as query suggestions and query categorization than ranking. Although the generative HMM model in [4] can be applied to search results ranking, it learns parameters for individual queries and contexts. Consequently, the learned HMM model in [4] can hardly be generalized to handle new queries and contexts not occurring in the training data. In this pa-

per, we mainly focus on ranking principles and models which can be generalized to handle new queries and contexts.
Shen et al. [15] proposed a method for context-aware ranking, which is probably the work most related to this study. They enriched the current query by using context information, and then fitted the enriched query into language models for retrieval. The basic idea is to promote the documents that are more similar to the previous queries and clicked documents within the same session. The authors verified the effectiveness of the method using a small amount of session data created upon TREC data. In reality, user sessions for Web search are more complex. As indicated in previous studies (e.g, [9, 10, 13]), there are multiple possible relations between the current query and the previous queries, such as reformulation, specialization, generalization, and parallel movement. Considering only one situation as in [15] may not be sufficient in complicated cases.
Our work is fundamentally different from the previous studies in the following two aspects. First, different from the previous work on building context models for user intent understanding [2, 3, 4], our study targets at the problem of context-aware ranking in Web search. To the best of our knowledge, we are the first to systematically explore context-aware ranking in real Web search scenarios. Second, compared to the previous work which applies a single ranking strategy to all kinds of contexts [15], our work recognizes different types of contexts, and proposes corresponding principles.
3. RANKING PRINCIPLES
In this section, we propose context-aware ranking principles according to the relations between the current query and the contexts, and evaluate the effectiveness of the principles using real log data extracted from a major commercial search engine.
3.1 Context-Aware Ranking Principles
In general, the context of a query q being asked contains any information that is related to q and available when q is asked. Specifically, in a Web search engine, the context often contains the queries asked before q in the same session as well as the answers (URLs) to those queries that are clicked on or skipped by the user. In the rest of this section, for a query qt in a session, we constrain the context of qt to only the query qt-1 asked right before qt in the session and the answers to qt-1 clicked on or skipped by the user. We will extend our consideration of context to all the queries and answers preceding qt in the session in Section 4.
The relations between queries in sessions have been studied in several previous works [9, 10, 13], which agree on five general types. That is, the current query qt can be unrelated to, reformulating, specializing, generalizing, or generally associated with the preceding query qt-1 in the same session. Obviously, for a query unrelated to its context, the context information cannot help. We discuss the other kinds of relations in this subsection.
3.1.1 Reformulation
A user may reformulate her previous query into a new one because the search results for the previous one do not or only partially fulfill her information need. The user's information need does not change in the case of reformulation.
Example 1 (Reformulation). Table 1 shows two consecutive queries in a session in a real log data set. The user

452

Query 1: "homes for rent in atlanta"

Query 2: "houses for rent in atlanta"

Atlanta homes for rent - home rentals - houses for ren...

Atlanta homes for rent - home rentals - houses for ren...

× Rentlist is directory of Atlanta home rentals featuring links to...

Rentlist is directory of Atlanta home rentals featuring links to...

http://www.rentlist.net

http://www.rentlist.net

Homes For Rent, lease in Atlanta suburbs. Can't sell ...

Homes for Rent in Atlanta, GA

Atlanta homes for rent, homes for lease in Gwinnett and north...

Houses, Apartments and Homes for Rent in Atlanta, GA Find ...

http://atlantahomesforrent.com

http://www.usrentallistings.com/ga/atlanta

Rentals.com - Homes for Rent, Apartments, Houses ...

Atlanta Home Rentals, Homes for Rent in Atlanta ...

Atlanta Home Rentals; Austin Home Rentals; Charlotte Home...

Atlanta Rentals - Homes for Rent in Atlanta, Apartments, Re...

http://www.rentals.com

http://www.rentals.com/Georgia/Atlanta

Atlanta Home Rentals, Homes for Rent in Atlanta ...

Homes For Rent, lease in Atlanta suburbs. Can't sell ...

× Atlanta Rentals - Homes for Rent in Atlanta, Apartments, Re...

Atlanta homes for rent, homes for lease in Gwinnett and north...

http://www.rentals.com/Georgia/Atlanta

http://atlantahomesforrent.com

Homes for Rent in Atlanta, GA

Atlanta Homes for Rent, Rental Properties, Houses for ...

Houses, Apartments and Homes for Rent in Atlanta, GA Find ... × Search for Homes for Rent in Atlanta, Georgia for free. View li...

http://www.usrentallistings.com/ga/atlanta

www.rentalhouses.com/find/GA/AtlantaArea/ATLANTA

Table 1: An example of successive queries with reformulation relation.

Query 1: "time life music"

Query 2: "time life Christian CDs"

Welcome to TimeLife.com | Homepage

Welcome to TimeLife.com | Homepage

× TimeLife.com: The best in music & video from a name you can...

Enjoy 138 romantic classics on 9 CDs from top artists like John...

http://www.timelife.com

http://www.timelife.com

Time-Life - Wikipedia, the free encyclopedia

Time Life Music & Video As Seen On TV

Time-Life is a creator and direct marketer of books, music, vid...

Christian ... Time Life Music & Video CD & DVD Collections ...

http://en.wikipedia.org/wiki/Time-Life Music

http://www.asseenontvmusic.com/timelife.html

Welcome to TimeLife.com | Music

Welcome to TimeLife.com | Music

Shop online for exclusive music CDs, music collections, & musi...

Shop online for exclusive music CDs, music collections, & musi...

http://www.timelife.com/webapp/wcs/stores/servlet/Categor...

http://www.timelife.com/webapp/wcs/stores/servlet/Categor...

Contemporary Country (Time-Life Music) - Wiki...

Songs ... Time Life 10 CD Collection... Christian Music

Contemporary Country was a 22-volume series issued by Time-... × CD/Album review of Songs 4 Ever Time Life 10 CD Collection...

http://en.wikipedia.org/wiki/Contemporary Country (Time-...

http://www.titletrakk.com/album-cd-reviews/songs-4...

Time Life Canada | Homepage

Christian Band - Newsong - More Life - CD Review of ...

The most comprehensive country music collection dedicated to... × Christian Band - Newsong - More Life CD Review ... Three yea...

http://www.timelife.ca

http://christianmusic.about.com/cs/cdreviews/fr/aafpr09080...

Table 2: An example of successive queries with specialization relation.

Query 1: "Free online Tetris"

Query 2: "Tetris game"

Tetris Friends Online Games - Play Free Games Featuri... × Play free online games featuring Tetris. Play single-player and ...

Tetris Friends Online Games - Play Free Games Featuri... Play free online games featuring Tetris. Play single-player and ...

http://tetrisfriends.com

http://tetrisfriends.com

Play Free Tetris Game Online × Play this classic, original, Flash Tetris Game online for free.

Tetris game Free online game: Make lines with falling blocks! Russia's finest...

http://www.gametetris.com

http://www.play.vg/games/6-Tetris.html

Free Tetris Game

Tetris (Game Boy) - Wikipedia, the free encyclopedia

Free tetris game - Play free tetris games online, learn about tet... × Tetris was a pack-in title included with the Game Boy at the ha...

http://www.tetrislive.com

http://en.wikipedia.org/wiki/Tetris (handheld game)

4FreeOnlineGame.com - Free Online Tetris Game

Tetris - non-stop puzzle action

4FreeOnlineGame - Free Online Tetris Game ... This is the all ... × Tetris logo, Tetris theme song and Tetriminos are trademarks of...

http://www.4freeonlinegame.com/Tetris

http://www.tetris.com

Tetris - Play Tetris. Free online games

Free Tetris Game

@ Adoption Media, LLC 1995 - 2010 This site should not subst...

Free tetris game - Play free tetris games online, learn about tetr...

http://games.adoption.com/free-online-games/Tetris

http://www.tetrislive.com

Table 3: An example of successive queries with generalization relation.

Query 1: "Xbox 360"

Query 2: "FIFA 2010"

Xbox.com | Home

FIFA.com - The Official Website of the FIFA World Cup

× Xbox.com is your ultimate source for all things Xbox and Xb...

The Official Website of the 2010 FIFA World Cup South AfricaTM

http://www.xbox.com

http://www.fifa.com/worldcup/index.html

Xbox 360 - Wikipedia, the free encyclopedia

2010 FIFA World Cup - Wikipedia, the free encyclopedia

The Xbox 360 is the second video game console produced by ...

The template below has been deprecated (see discussion), and ...

http://en.wikipedia.org/wiki/Xbox 360

http://en.wikipedia.org/wiki/2010 FIFA World Cup

Xbox.com | Xbox 360

FIFA.com - F´ed´eration Internationale de Football Associa...

× Find out more about Xbox 360, the awesome lineup of games ...

The official site of the international governing body of the sport ...

http://www.xbox.com/en-US/hardware

http://fifa.com

Microsoft Xbox

FIFA 10 Soccer : FIFA 2010 - EA Sports Games

Xbox 360 delivers the most powerful console, the next genera... × Improvement in Management Mode, Flick Passes, Ball Physics, ...

http://www.microsoft.com/xbox

http://www.ea.com/games/fifa-soccer

Xbox 360 - Gizmodo

FIFA 2010 World Cup in South Africa

This No-Name HTPC Remote Has a Keyboard, Can Work W...

A surprise in the 2007 Asian Cup! The Iraqis win it! In spite of ...

http://gizmodo.com/tag/xbox-360

http://southafrica2010.wordpress.com

Table 4: An example of successive queries with general association.

453

first raised query "homes for rent in Atlanta" and clicked on the 1st and 4th search results. The user then issued the second query "houses for rent in Atlanta" and clicked on the 5th search result.
The two queries bear similar meaning. Unsurprisingly, 4 out of the top-5 results returned by the search engine for the second query were also among the top-5 results for the first query. Why did the user skip the top-4 search results for the second query but click on the 5th one?
The 1st and 3rd results for the second query were clicked on by the user for the first query. Obviously, a user was unlikely to click again on pages she just browsed.
Moreover, according to some previous user studies [6, 7, 8], a search result is likely to be viewed by a user if it is 1) among the top two search results; 2) ranked above the lowest clicked result; or 3) ranked one position below the lowest clicked result. If a search result is skipped (i.e., viewed but not clicked on) by a user, it suggests the result may not be interesting to the user. In Example 1, the 2nd and 4th results for the second query were ranked either above or one position below the lowest clicked result for the first query. They were skipped by the user for the first query, and thus can be regarded uninteresting to the user. Therefore, they were unlikely to be clicked on for the second query, either.
Principle 1 (Reformulation). For consecutive queries qt-1qt in a session such that qt reformulates qt-1, if a search result d for qt-1 is clicked on or skipped, d as a result for qt is unlikely to be clicked on and thus should be demoted.
3.1.2 Specialization
When a user issues a specializing query, she likely wants to see results that are more specific about her interests.
Example 2 (Specialization). Table 2 shows two consecutive queries. The user first asked "time life music" and clicked on the homepage of the store. The user further asked "time life Christian CDs" and clicked on the 4th and 5th results.
The information need of the second query consists of two parts: information about "time life" and that about "Christian CDs". If we do not consider the context information, both components should be equally important in ranking search results of the second query. However, given the first query, the user likely wanted to see the search results for the second query specifically about the Christian CDs of the music store. This explains why the user skipped the first three results to the second query where the terms "Christian" and "CDs" do not appear in the titles of the search results.
Principle 2 (Specialization). For consecutive queries qt-1qt in a session such that qt specializes qt-1, the user likely prefers the search results specifically focusing on qt.
The principle is particularly useful in several scenarios. For example, when qt is rare and qt-1 is popular, the answers fully matching qt-1 but partially matching qt may be ranked high for qt by a search engine. The principle can use the context information to demote the answers matching qt-1 given that qt-1 was just asked by the user.
One possible way to implement the principle is as follows. Let qt \ qt-1 be the set of terms appearing in query qt but not in query qt-1. If qt \ qt-1 = , we should promote the results matching qt \ qt-1 in the set of answers to qt.

3.1.3 Generalization
A user may ask a query more general than the previous one. In such a situation, the user may like to see some information not covered by the first query.
Example 3 (Generalization). Table 3 shows a generalization scenario. A user first asked query "free online Tetris game" and clicked on the 1st and 2nd search results. The user then asked query "Tetris game" and clicked on the 3rd and 4th results.
The second query "Tetris game" carries multiple possible information needs. For example, the user may want to download the game or play it online. Alternatively, the user may be interested in the history or news of the game. The user may also look for the basic game rules or advanced cheats of the game. For such a query with ambiguous search needs, search engines often try to diversify search results. In this example, the top-5 results can be divided into two groups. The 1st, 2nd, and 5th results link to some free online Tetris game sites, while the 3rd and 4th results are about the background information of the Tetris game.
With the context that the previous query was "free online Tetris game" and the user clicked on two related sites, we may infer that the user's interest in the second query may likely divert to something about the game but not the game sites. This may explain why the user clicked on the results about the background information of the game.
Principle 3 (Generalization). For consecutive queries qt-1qt in a session such that qt generalizes qt-1, the user would likely not prefer the search results specifically focusing on qt-1.
One possible way to implement the principle is as follows. Let qt-1 \ qt be the set of terms appearing in qt-1 but not in qt. If qt-1 \ qt = , we should demote the results matching qt-1 \ qt among the answers to query qt.
3.1.4 General Association
When a query (especially an ambiguous one) is generally associated with its context, the context may help to narrow down the user's search intent.
Example 4 (General association). In Table 4, a user first raised query "Xbox 360" and clicked on the 1st and 3rd search results. Then, the user raised query "FIFA 2010" and clicked on the 4th result.
The second query "FIFA 2010" bears multiple intents. It may refer to either the FIFA 2010 World Cup at South Africa or a new game of Xbox 360. Therefore, the second query "FIFA 2010" is generally associated with the first query "Xbox 360". Without the context, a search engine may retrieve search results for both intents behind query "FIFA 2010". However, using the first query "Xbox 360" as the context, which indicates that the user was interested in Xbox 360, we may rank the results about the soccer game in Xbox 360 higher than those about the World Cup event.
Principle 4 (General association). For consecutive queries qt-1qt in a session such that qt and qt-1 are generally associated, the user likely prefers the search results related to both qt-1 and qt. Such results should be promoted for qt.
One possible way to implement the principle is the following. First we can choose any topic taxonomy such as the

454

Pid # cases P (c = 1|h = 1) P (c = 1|h = 0)



1 1,628 2 1,378

0.361 0.401

0.217 0.302

0.144 0.099

3

246

0.339

0.315

0.024

4 4,457

0.352

0.296

0.056

 Passes the significance test at the confidence level of 0.01.

Table 5: The effectiveness of ranking principles in

the corresponding types of contexts.

Pid # cases P (c = 1|h = 1) P (c = 1|h = 0)



1 10,186 2 20,200

0.356 0.407

0.234 0.316

0.122 0.091

3 1,539

0.358

0.386

-0.028

4 21,052

0.352

0.318

0.034

 Passes the significance test at the confidence level of 0.01.

Table 6: The effectiveness of ranking principles in

all contexts.

Open Directory Project (http://www.dmoz.org). Let Ct-1 and Ct be the sets of topics of qt-1 and qt, respectively, and C be the set of common topics between Ct-1 and Ct. If C = , we should promote a search result u if the set of topics of u shares at least one topic with C.
3.2 Effectiveness of Principles
We use the search log data from a major commercial search engine to evaluate the effectiveness of the principles. We traced each user's query & click stream by the user-id information in the data. All users were completely anonymous, and no action was taken to reveal the users' identities. We segmented each user's stream into sessions by a commonly applied rule [19]: a boundary between two sessions was set if there was no activity by the user for thirty minutes. From the resulted 37,320 user sessions, we extracted successive query pairs within the same sessions, and manually labeled the relations (i.e., reformulation, specialization, generalization, general association, and unrelated) for 10, 000 randomly selected successive query pairs.

3.2.1 Evaluation in Different Types of Contexts

We first evaluate the effectiveness of each principle in its

corresponding type of contexts, i.e., when the two successive

queries match the relation of the principle. Table 5 shows the

number of successive query pairs for each type of contexts,

where "Pid" indicates the principle-id. In our evaluation,

a search result u is represented by the terms in its title,

snippet, and URL. For Principle 4, we use a classifier in [14]

and classify all the queries and documents into the 16 topics

at the first level of Open Directory Project (http://www.

dmoz.org). For each query or document, we keep the top

three topics returned by the classifier.

According to the previous studies [6, 7, 8], a user views

only a subset of search results and chooses to click on or

skip them individually. Therefore, in each test case for a

principle, we focus on those search results that are likely to

be viewed by the user. Specifically, we adopt the methods

in [6, 7, 8] and consider a search result is viewed if it is

ranked above or one position below the last clicked result.

To evaluate a principle, we aggregate the viewed search re-

sults for all queries in the test cases and obtain a set U . We

call a search result u  U satisfies the principle if u should

be promoted (in cases of Principles 2 and 4) or not demoted

(in cases of Principles 1 and 3) by the principle; otherwise,

u violates the principle. Let Uh1  U be the set of search results that satisfy the principle, and Uh0 = U \ Uh1. Simul-

taneously, U can also be divided into two subsets Uc1 and Uc0, where Uc1  U consists of the search results that were clicked on by the users, and Uc0 = U \ Uc1.

The conditional probability for a search result u to be

clicked on for qt given that it satisfies a principle can be es-

timated

as

P (c

=

1|h

=

1)

=

, |Uc1 Uh1 | |Uh1 |

where

random

vari-

able c denotes whether a search result u was clicked on for

qt or not, and random variable h denotes whether u satisfies

the principle or not. Analogously, the conditional proba-

bility for u to be clicked on for qt given that it violates a

principle

can

be

estimated

as

P (c

=

1|h

=

0)

=

. |Uc1 Uh0 | |Uh0 |

We conduct a t-test on  = P (c = 1|h = 1)-P (c = 1|h =

0), the difference between the two conditional probabilities.

Intuitively, for each principle, this difference indicates how

likely users would choose to click on a search result satisfying

instead of violating the principle. One may wonder whether

user clicks contain position bias. Since  value calculates

the difference between two click probabilities, we may ex-

pect that the position biases are canceled out. Therefore,

if the difference  passes the significance test at confidence

level 0.01, it confirms the effectiveness of the principle. From

Table 5, we can see that Principles 1, 2, and 4 pass the sig-

nificance test, which supports their effectiveness. However,

Principle 3 does not pass the significance test. One reason

is that generalization pairs are relatively rare, only 2.46%

in the manually labeled data. We can hardly draw reliable

conclusions from such a small size of test data.

3.2.2 Evaluation in All Contexts
Given two consecutive queries qt-1qt in a session, a straightforward way is to first determine the relation between qt and qt-1 and then apply the corresponding principle. However, practical cases are often complicated and fuzzy. For example, it is not easy to determine whether query "Geneva food" specializes or is generally associated with query "Geneva travel". It is very challenging to accurately classify the relations between queries and contexts.
To tackle the above problems, we explore how well the principles can adapt to all possible contexts without explicitly distinguishing the types of contexts, i.e., types of query relations. Empirically we evaluate the effectiveness of principles over all the query pairs extracted from user sessions. Each consecutive query pair qt-1qt is used as a test case for a ranking principle if the principle demotes or promotes at least one search result for qt. It is possible for one query pair to be a test case for multiple principles. Table 6 shows the number of test cases for each principle as well as the evaluation results, where "Pid" indicates the principle-id. For Principles 1, 2, and 4, the  values pass the significance tests at the confidence level of 0.01. Since the tests are conducted in all contexts, the results suggest that Principles 1, 2, and 4 adapt well to different types of contexts.
The  value for Principle 3 is negative, and it does not pass the significance test. By finer analysis on the test cases for Principle 3, we observe the following. First, Principle 3 is sensitive to query relations. We manually labeled the 1, 539 test cases for Principle 3, and found that only 55% of them are of generalization relation. As shown in Table 5, the  value on generalization pairs is positive. Although that positive  value does not pass the significance test either, it suggests that Principle 3 may perform differently on different types of relations. Second, the test cases for Principle 3 in all contexts is only about 4% in our data set. This is because the search results for the current query qt are unlikely to contain the terms not in qt but in the previous query qt-1.

455

Name

Description

Pid

OrgPos

The original position of u

-

IsClicked IsSkipped

Whether u  (Utc-1  . . . U1c)

1

Whether u  (Uts-1  . . . U1s)

1

CosBMA(·) Cosine(u, q )

2

JacBMA(·) Jaccard(u, q )

2

CosAMB(·) Cosine(u, q )

3

JacAMB(·) Jaccard(u, q )

3

CosBAA(·) Cosine(u, q)

-

JacBAA(·) Jaccard(u, q)

-

CosTopics Cosine(Cu, C)

4

JacTopics Jaccard(Cu, C)

4

 OrgPos is only used in RankSVM-F.

Table 7: The major features in ranking models.

Finally, since the number of test cases is small, it is unclear whether Principle 3 is effective. We may ignore Principle 3 due to the small amount of applicable cases.

4. CONTEXT-AWARE RANKING
Although we have developed effective context-aware ranking principles, to achieve fully context-aware ranking in Web search practice, there are still several challenges in applying the principles. First, a user session may contain more than two queries, while we only discuss the principles formulated based on two queries. How can we extend the applicability of the principles? Second, given a query as well as its context, there might be multiple principles applicable. How should we jointly execute the principles? Third, user sessions contain rich information. Many factors, such as the positions of the documents returned by the search engine and the terms shared by the current query and the previous ones, may all be useful in ranking documents. How can we incorporate those factors into the ranking model?
To address the above challenges, we employ a learningto-rank approach to build context-aware ranking models. We derive features from the ranking principles developed in Section 3 and incorporate the features into learning-torank models. The ranking features extend the context information from the immediately preceding query and answers to all previous queries and answers within the same session of the current query. Besides the features derived from the previous ranking principles, we also incorporate other factors mentioned above as features of the ranking models. We create training data from search sessions and train the ranking models offline. In online ranking, the trained models can carry out context-aware ranking using the available context information. It is not necessary to explicitly specify which principles to be used. By taking a learning-to-rank approach, we can address all the challenges identified above.
As a concrete example, we use RankSVM [7], a stateof-the-art learning-to-rank model to demonstrate our approach. RankSVM learns an SVM model for classification on the preference between a pair of documents. In the training stage, the RankSVM model takes as instance an ordered pair of documents with respect to a query under a context. Specifically, the i-th training example corresponds to query q(i) and documents d(Ai) & d(Bi) under context c(i), and consists of (u(Ai), u(Bi), y(i)), where u(Ai) and u(Bi) denote the feature vectors corresponding to the two documents, respectively, and y(i) denotes a preference label: if y(i) is 1, then u(Ai) is preferred to u(Bi), otherwise, u(Bi) is preferred to u(Ai).
A feature in our RankSVM model is a function of query, document, and context. Table 7 lists the major features of the context-aware RankSVM model. Column "Pid" indi-

cates from which principle the feature is derived. The model
is flexible to combine features in addition to those from the principles (e.g., feature "OrgPos"). In Table 7, u denotes a document for query qt. Uic and Uis denote the set of clicked and skipped documents for qi (1  i  t - 1), respectively. q = qt \ (qt-1  . . . q1) and q = (qt-1  . . . q1) \ qt are the differences between the current query and previous queries in the session. q = qt  (qt-1  . . . q1) is the set of common terms among queries in the same session. Cosine(·, ·) denotes Cosine similarity, Jaccard(·, ·) denotes Jaccard Index. The common topics C are derived by intersecting the topics of qt with those of previous queries.
One issue is how to combine the original ranking of the
search engine. In general, there are two possible approaches.
We can use the original position of a document returned
by the search engine as a feature in the RankSVM model.
We denote this approach by RankSVM-F. Alternatively, we
can train the RankSVM model without the original position
feature. Given a test case, we combine the original ranking list R0 from the search engine with the list R1 from the RankSVM by Borda's ranking fusion method [1], that is,

score(u) =  · 1 + (1 - ) · 1 ,

(1)

R0(u)

R1(u)

where   [0, 1] is a parameter, and R0(u) and R1(u) are the positions of document u in R0 and R1, respectively. As a special case, when  = 0, the Borda's fusion score completely ignores the search engine ranking. We denote this case by RankSVM-R0. Otherwise, the model is denoted by RankSVM-R1. Both RankSVM-F and RankSVM-R1 are reranking models since they incorporate search engine's ranking, while RankSVM-R0 is a ranking model.

5. EXPERIMENTAL RESULTS
We prepare the experimental data from a search log of a major commercial search engine. Since our ranking models use context features, we extract the search sessions with more than one query. In our search log, the percentage of such sessions is about 50%, which is consistent with the results reported by the previous studies [2, 4]. Among the 37,320 extracted sessions, we use half of them for training and validation, and the remaining half for testing.
In the following, we first describe the training process of the RankSVM models, including RankSVM-F, RankSVMR0, and RankSVM-R1. We then compare the performance of our RankSVM models with a baseline proposed by Shen et al. [15] using both manually labeled data and user click data. We use the BatchUp method in [15] as the baseline since it has the best reported performance in [15]. BatchUp does not incorporate search engine's ranking. Finally, we conduct case studies and discuss the experimental results.
We train the RankSVM models from manually judged document pairs with respect to given queries and their contexts. Given a randomly selected search session with more than one query, we form an example (qt, c, dA, dB), where qt is the last query in the selected session, context c consists of the previous queries and the search results clicked on or skipped by the user before qt in the session, and dA and dB are among the top five documents for qt returned by the search engine. Please note dA is not necessarily ranked higher than dB by the search engine. Each document consists of its title, snippet, and URL.
For each example, a judge is asked to infer the user's search intent based on qt as well as the context c. Then,

456

RS-F RS-R1 RS-R0 Baseline

Among 500 labeled pairs

Num of Correct Pairs

247

Num of Error Pairs

109

Num of Unclear Pairs

144

P(Correct)

49.4%

P(Error) Improvement

21.8% 27.6%

239
100 161
47.8% 20% 27.8%

242
113 145
48.4% 22.6% 25.8%

203 150 147 40.6% 30% 10.6%

Over all test pairs Reverse Ratio on Pairs

37.6%

22.9%

40.1%

42.4%

Table 8: The performance of different methods on human-labeled data.

the judge reviews the titles and snippets of dA and dB and gives the preference between dA and dB as if he or she were the user who issued query qt within the given context c. The judge does not know the original order of dA and dB returned by the search engine. The judge can choose one of the three options: 1) dA is more preferable than dB; 2) dB is more preferable than dA; and 3) unclear. A judge may choose the third option if he or she is not sure about the user's search intent, or dA and dB are equally preferred. In our experimental setting, each example is labeled by three judges, and we take the majority of labeled results as the ground truth. An example is "unclear" if 1) at least two judges label it as "unclear"; or 2) one judge labels "unclear", and the other two judges have inconsistent preferences.
From the judged examples, we pick 1, 500 cases which are not labeled as "unclear". We use 1,000 cases for training and the remaining 500 cases for validation. There are two parameters for our methods: C required by SVM for all the three RankSVM models and  in Equation 1 for the RankSVM-R1 model. We tune the parameters on the validation data and set C = 1, 000 and  = 0.45. We will use this parameter setting for all the following experiments.
Performance on manually labeled data. We first compare on the manually labeled data the perfor-
mance of the three RankSVM models, the baseline, and the search engine. For each of the four context-aware methods (RankSVM models and the baseline), we randomly select 500 examples where the method reverses the original order of dA and dB returned by the search engine.
Table 8 shows the results, where "RS"stands for RankSVM. We consider that a method has a correct case against the search engine if the order given by the method is consistent with the judged preference. Otherwise, the method has an error case. We calculate the percentages of correct and error cases over all the 500 reversed pairs, denoted by P(correct) and P(error) in Table 8, respectively. The row "Improvement" in Table 8 is the difference between P(correct) and P(error), which indicates how much a (re-)ranking method improves over the search engine. We also conduct significance tests on the improvements of different methods. All tests use the t-statistic and set the confidence level to 0.01.
All of the four context-aware (re-)ranking methods (three RankSVM models and the baseline method) make significant improvement over the search engine. Moreover, all of our three models, RankSVM-F, RankSVM-R0, and RankSVMR1, perform significantly better than the baseline. This is because the baseline applies a single ranking strategy and may not adapt well to various types of contexts in Web search. Our models encode multiple principles for contextaware ranking as features and automatically adapt to different types of contexts.
We also compute the reverse ratio, i.e., the percentage

RS-F RS-R1 RS-R0 Baseline

MCP of Methods MCP of Search Engine MCP Improvement

2.922 3.096 0.174

2.916 3.096 0.180

2.923 3.096 0.173

2.966 3.096 0.130

Reverse Ratio on Lists 68.9% 51.0% 66.1%

69.2%

 The MCP improvement of a method over the search engine is in

bold if it passes the significance test, and marked with a star if it

is significantly larger than that of the baseline.

Table 9: The performance of different methods on

user click data.

of document pairs for which a (re-)ranking method reverses the orders by the search engine, over all the document pairs from the test sessions. This measure indicates how likely a method will reverse the order of a random pair of search results returned by the search engine. Table 8 shows the reverse ratio for each method. The two general ranking models, RankSVM-R0 and the baseline, have the highest reverse ratio. One of the re-ranking model, RankSVM-F, has a reverse ratio comparable with those of the two general ranking models, suggesting that the original position feature may not play a critical role in the model. The other re-ranking model, RankSVM-R1, is the most conservative. This is because we set a large  value ( = 0.45) in the Borda's fusion method (Equation 1).

Performance on user click data.

Although manually labeled data is usually of good quality,

there could be two concerns. First, the judging process is

expensive, and thus cannot be scaled up. Second, a search

intent inferred by the judge may not be consistent with that

of the real user. Therefore, we further evaluate the perfor-

mance of the ranking methods by using real click data.

Since we consider users' clicks as their preference on search

results, we only select the sessions in the test data such that

the last query in a selected session must have at least one

click. This results in 13,651 sessions. We follow the previous

studies [6, 7, 8] and assume that 1) a user views and clicks

on search results from top to bottom; and 2) a user keeps

viewing search results until the one that is one position lower

than the last document clicked on. For example, suppose the

last URL clicked on by a user for query impression qt is at

position 5, we consider the user views all search results at

positions from 1 to 6. Then, those six search results form

a test case and will be (re-)ranked by our models and the

baseline.

The performance of the (re-)ranking methods can be eval-

uated by whether they promote the search results which are

clicked on by users to higher positions. Specifically, for the

i-th test case, we derive the set Uc(i) of the clicked URLs for

the last query qt(i). Then, we aggregate all the test cases and

calculate the mean click position MCP=

i

, uUc(i) R(u)
i |Uc(i) |

where R(u) is the rank of u in a ranked list. A smaller

MCP indicates a better ranking method.

Table 9 shows the performance of different methods on the

test data, where "RS" stands for RankSVM. The row "MCP

Improvement" records the differences between the search en-

gine and the (re-)ranking methods. We also conduct signif-

icance test on the improvements. All methods have sig-

nificantly lower MCPs than that of the search engine. In

other words, all methods perform better than the search en-

gine. Again, the improvement of the baseline method passes

the significance test, but it is not as large as those by the

RankSVM models.

We also test for each method the percentage of lists in

457

which the method reverses the order of at least one pair of search results. Similar to Table 8, the RankSVM-R1 method is most conservative, while the other three methods have comparable reverse ratios. For each method, the reverse ratio in Table 9 is much higher than that in Table 8. This is because, in Table 8, we consider the reverse ratio for pairs of search results, while in Table 9, we consider the reverse ratio for lists of search results. Since a list usually contains multiple pairs, the reverse possibility increases substantially.
Summary of performance tests. We consider the human labeled data and user click data
complementary to each other. For example, the human labeled data overcomes the noise and position bias in user clicks, while the user click data is large and truly reflects the preference of users. Interestingly, the experimental results on both data agree with each other on the following aspects. First, all the four context-aware methods, i.e., the three RankSVM models and the baseline, are better than the search engine. This confirms the effectiveness of contextaware ranking. Second, all our three RankSVM methods perform better than the baseline in context-aware ranking. As explained before, this is because our models consider different types of contexts in Web search. Third, RankSVMF, RankSVM-R0, and the baseline have comparable reverse ratios, while the RankSVM-R1 method is relatively conservative due to a large  value ( = 0.45). Finally, on both test data sets, the RankSVM-F and RankSVM-R1 methods show larger improvements than that of the RankSVM-R0 method, suggesting the usefulness of considering the original ranking of the search engine. However, the evidence is not strong enough to pass the significance test.
Case studies and discussions. We conduct case studies on both situations where our
ranking models succeed and fail. Recall the examples in Tables 1-4. For all the four examples, all of our models yield a ranking in which the documents clicked on by the user are ranked higher than those skipped by the user. However, the baseline only works well on the last example. This is because the baseline gives rise to the documents which are similar to both the current query and its context, which consists of the previous queries as well as the summaries of the previously clicked documents. In the last example, the summaries of the clicked documents for qt-1 contain terms about games. Consequently, the language model which incorporates the context information boosts the 4th result for qt to the top position. In this case, the baseline bears a similar spirit with our Principle 4.
In the first three examples, the users reformulate, specialize, and generalize their initial queries in the hope to see some new results. However, in these cases, the baseline does not consider the types of contexts and still applies the single principle which tends to provide information similar to that appeared in the previous queries. Consequently, the ranking results may not meet the users' information needs well. On the contrary, our ranking models incorporate multiple ranking principles and automatically adapt to various types of contexts.
We also investigate some cases for which our models make wrong decisions. We find three major reasons for those cases. First, the ranking principles developed in Section 3.1 do not necessarily hold. For example, in some sessions, people simply click on the documents which have been just clicked before in the same session. As explained in previous studies [18], some users may use queries or keywords

(such as "msn news") to "bookmark" a Web page (such as www.msnbc.msn.com). In some other sessions, the search results do not improve much after the users refine their queries. In such cases, the users may choose to click on some search results they skipped for previous queries.
The second reason for false re-ranking may be our implementation of the ranking principles. In a real search session, the user first raised a query "super bowl" and then submitted query "super bowl nine". Our implementation of Principle 2 promotes the search results containing the term "nine" to higher positions than that of the Wikipedia page for Super Bowl IX, which contains the term "IX" instead of "nine". In fact, the Wikipedia page is the result the user clicked on.
The remaining errors may come from our employment of RankSVM as the ranking model. Although RankSVM is one state-of-the-art ranking models, many other learningto-rank models have been proposed in the literature [11]. It is still an open question which model is the best. Similarly, more studies are needed on what kind of models are most suitable for context-aware ranking.
6. CONCLUSIONS
In this paper, we studied the problem of using context information in ranking documents in Web search. We conducted an empirical study on real search logs and developed four principles for context-aware ranking. We further adopted a learning-to-rank approach and incorporated our principles to ranking models. The experimental results verified the effectiveness of our approach.
7. REFERENCES [1] Borda, J.C. M´emoire sur les ´elections au scrution. Histoire de l'Acad´emie Royal des Sciences, 1781. [2] Cao, H., et al. Context-aware query suggestion by mining click-through and session data. In KDD'08, 2008. [3] Cao, H., et al. Context-aware query classification. In SIGIR'09, 2009. [4] Cao, H., et al. Towards context-aware search by learning a very large variable length hidden markov model from search logs. In WWW'09, 2009. [5] Dou, Z., et al. A large-scale evaluation and analysis of personalized search strategies. In WWW'07, 2007. [6] Guo, F., et al. Efficient multiple-click models in web search. In WSDM'09, 2009. [7] Joachims, T. Optimizing search engines using clickthrough data. In KDD'02, 2002. [8] Joachims, T., et al. Accurately interpreting clickthrough data as implicit feedback. In SIGIR'05, 2005. [9] Jones, R., et al. Generating query substitutions. In WWW'06, 2006. [10] Lau, T. and Horvitz, E. Patterns of search: Analyzing and
modeling web query refinement. In ICUM'99, 1999. [11] Liu, T.Y. Learning to rank for information retrieval.
Foundation and Trends on Information Retrieval, Now Publishers, 2009. [12] Qiu, F. and Cho, J. Automatic identification of user interest for personalized search. In WWW'06, 2006. [13] Rieh, S.Y. and Xie, H. Patterns and sequences of multiple query reformulations in web searching: a preliminary study. In ASIST'01, 2001. [14] Shen, D., et al. Q2C@UST: Our Winning Solution to Query Classification in KDD Cup 2005. SIGKDD Explorations, 7(2):100­110, 2005. [15] Shen, X., et al. Context-sensitive information retrieval using implicit feedback. In SIGIR'05, 2005. [16] Tan, B., et al. Mining long-term search history to improve search accuracy. In KDD'06, 2006. [17] Teevan, J., et al. Personalizing search via automated analysis of interests and activities. In SIGIR'05, 2005. [18] Teevan, J., et al. Information re-retrieval: Repeat queries in yahoo's logs. In SIGIR'07, 2007. [19] White, R.W., et al. Studying the use of popular destinations to enhance web search interaction. In SIGIR'07, 2007.

458

Collecting High Quality Overlapping Labels at Low Cost

Hui Yang*
School of Computer Science Carnegie Mellon University
Pittsburgh, PA, 15213
huiyang@cs.cmu.edu

Anton Mityagin
Microsoft Bing One Microsoft Way Redmond, WA 98052
mityagin@gmail.com

Krysta M. Svore
Microsoft Research One Microsoft Way Redmond, WA 98052
ksvore@microsoft.com

Sergey Markov

Microsoft Bing

One Microsoft Way

Redmond, WA 98052

sergey.markov@microsoft.com

ABSTRACT

depends both on the expertise of the labelers and on the number of labelers. For a given training sample, the more expert the labelers,

This paper studies quality of human labels used to train search engines' rankers. Our specific focus is performance improvements obtained by using overlapping relevance labels, which is by

and the more labelers, the higher the label quality. Therefore, the labels with the best quality should result from obtaining overlapping labels from multiple expert labelers.

collecting multiple human judgments for each training sample. The paper explores whether, when, and for which samples one should obtain overlapping training labels, as well as how many labels per sample are needed. The proposed selective labeling scheme collects additional labels only for a subset of training samples, specifically for those that are labeled relevant by a judge. Our experiments show that this labeling scheme improves the NDCG of two Web search rankers on several real-world test sets, with a low labeling overhead of around 1.4 labels per sample. This labeling scheme also outperforms several methods of using overlapping labels, such as simple k-overlap, majority vote, the highest labels, etc. Finally, the paper presents a study of how many overlapping labels are needed to get the best improvement in retrieval accuracy.
Categories and Subject Descriptors

However, obtaining overlapping labels from multiple experts is expensive. One alternative is to obtain overlapping labels from nonexperts, for example, labelers from Amazon's Mechanical Turk (MTurk), which is an online labor market where workers are paid small amounts of money to complete human intelligence tasks. There is ongoing research [4] on how to effectively use services such as MTurk to obtain more labels. Unfortunately, labels from non-experts are often unreliable; and such unreliable labels decrease the retrieval accuracy of a learned model.
Another alternative to obtaining overlapping labels from experts is collecting one label from one expert for each sample. This is called the single labeling scheme. This single labeling scheme is affordable in general, and is widely used in supervised learning. However, since only one expert is involved in determining the relevance of a sample, the single expert's opinion may contain a

H.3.3 [Information Systems]: Information Storage and Retrieval.

personal bias, which may introduce noise, and thus possibly

General Terms: Algorithms, Experimentation.

interfere with the learning of the ranking model. For example, the Web document at www.svmsolutions.com may be labeled as

Keywords
Relevance Labels, Overlapping Labels, Learning to rank.
1. INTRODUCTION
The retrieval accuracy of a learned model depends both on the quality of the training labels and on the amount of training examples. As expected, the higher the quality of the training labels, and the more the training examples, the better the accuracy of the learned model. A large set of training data is commonly used to improve a model's retrieval accuracy. Recently, however, researchers found that the improvement of the retrieval accuracy

"Good" for the query "SVM" by a given expert, but the same expert may label the document www.svmlight.joachims.org as "Bad" if (s)he is not an expert in machine learning. Hence, the single labeling scheme may create unreliable labels because not every judge is an expert for every query. Figure 1 depicts agreements between aggregated overlapping labels and the ground truth, and agreement between the best judge and the ground truth, for 5 Web queries and 111 urls. Figure 1 shows that when the number of overlapping labels is greater than 5, the aggregated labels achieve better quality than even the "best" single judge does. This motivates the use of overlapping labels instead of a single expert judge.

of a learned model stops after the number of training examples

reaches a certain threshold [5]. When more training examples are

not able to further improve a model's accuracy, improving the

quality of labels is a solution.

Collecting high quality labels is a challenging task. Label quality

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

Figure 1: Aggregated Overlapping Labels are better than Best Judge.

459

In this paper, we present a new scheme of collecting high quality labels at low cost. In particular, the paper focuses on how to cheaply and effectively produce and employ overlapping labels from multiple experts to improve Web search accuracy. The proposed labeling scheme requests additional labels from more experts only when a sample is labeled as relevant by one expert; otherwise, only the single label from that expert is used. Our experiments show that this selective labeling scheme improves Web search accuracy, which is measured in NDCG (Normalized Discounted Cumulative Gain [3]), of both LambdaRank [2] and LambdaMart [11] rankers on several real-world Web test sets, with a low labeling overhead of around 1.4 labels per sample. The proposed new labeling scheme also outperforms several methods of using overlapping labels, such as majority vote, k-overlap labels, the highest labels, etc. Furthermore, our paper also describes how many additional labels are needed to get the best improvement in retrieval accuracy.
Although this paper focuses on the task of Web search, the techniques presented can be generally applied to many research areas, such as computational linguistics, where manual labels are useful for training and evaluation.
Our paper makes two major contributions. First, the paper explores whether, when, and for which samples one should obtain overlapping training labels, as well as how many overlapping labels are needed. Second, the proposed If-good-k scheme creates high quality labels at low cost, which makes the approach promising for application to search engine training or other supervised training applications.
The remainder of the paper is organized as the follows. Section 2 describes the related work. Section 3 introduces the Web search task and the label distributions of multiple experts. Section 4 discusses issues of using overlapping labels. Section 5 details the proposed labeling scheme. Section 6 shows the experimental results and compares our approach to other commonly used overlapping labeling schemes. Section 7 discusses the proposed scheme, and Section 8 concludes the paper.
2. RELATED WORK
Due to the importance of search engine ranking and the advent of MTurk, research on expert labeling has recently become popular. In [9], a high agreement between MTurk non-expert annotations and existing gold-standard labels provided by expert labelers for five natural language processing tasks is demonstrated. Multiple labeling has also been shown to be useful for improving the data quality of annotations of non-experts [5].
Bernstein and Li pointed out in [1] that error-prone labelers often perform worse than a simple supervised learning setting using the initially labeled data. Their study suggested that the key to practical use of active learning with human labelers is to help the human labelers make fewer labeling mistakes.
Earlier work on inferring ground truth from subjective labels includes Smyth et al. [6], Sheng et al. [5], and Snow et al. [9]. In [6], the latent relation between subjective labels and true labels by EM algorithm was studied and it was shown that the posterior conditional probabilities of subjective labels and true labels generally agree with intuition and often (70% of the samples) correspond to a majority vote among the labelers. However, posterior conditional probabilities of subjective labels and true labels of the other 30% samples cannot be derived from the majority vote scheme.

The authors of [8] calculated a simple bound on the average classification accuracy across all labelers given the labels. The true labels are unknown. The bound is obtained by following the fact that the errors from all labelers are bounded by the maximum number of same-value labels from these labelers. This bound can be used to evaluate the quality of the overall labeling process.
The most common source of uncertainty of labels is subjective opinion, either from expert or non-expert labelers. In [7], it is mentioned that labeling of specific items may in itself be inconsistent, whether it is multiple labels from a single labeler at different times or labels from different labelers. In particular, in [7], Smyth et al. evaluated the labels created by two experts, who grouped samples into 5 label probability bins, against the ground truth, which are the consensus labels created by these two experts together. Smyth et al. also found that the labeling of individual experts relative to the consensus is not good.
Sheng et al. [5] pointed out that the improvement due to overlapping labels is more obvious when a single label is of low quality. However, their strategy requests a relatively large number of repeated labels for each sample. This high cost makes this approach impractical if the labeling cost for each labeler is high.
An interesting observation reported in [5] is that directly using multiple overlapping labels for each sample produces better label quality and better classification accuracy than using majority vote of the overlapping labels for each sample. In our experiments, we have similar findings on retrieval accuracy.
Moreover, not all samples need overlapping labels. In [5], Sheng et al. suggested using overlapping labels for samples whose overlapping labels show low agreement, and for samples whose overlapping labels bring high uncertainty to a learned model. Their method requires repeatedly labeling of each sample to determine whether using those overlapping labels for a sample in the training process. In this paper, we propose a more costeffective method to select samples than to repeatedly label them.
The labeling scheme practically used in TREC-9 is perhaps the most similar settings to our proposed method. [10] describes how TREC-9 assessors judged the best page for a retrieval task. In their assessment, three different judges selected the best page(s) for a topic. The original assessor evaluated the entire pool for the topic and selected the best pages; the other two assessors were only given the documents the original assessor judged relevant or highly relevant. Remarkably, they also selected the best pages. The two secondary assessors did not know which were the best pages selected by the original assessor, but knew that all the given pages were judged relevant by the original assessor.
3. BACKGROUND 3.1 The Web Search Task
The Web search task takes in a set of queries and a set of retrieved Web documents for each query. Each Web document is represented by a feature vector consisting of features extracted from the anchor text, body content, url, title, and so on. A ranker learns a model from the training data, and computes a rank order of the urls based on their real-value relevance scores at the query level. In this paper, the two rankers used in the experiments are LambdaRank, a state-of-the-art neural network ranker, and

460

(a)

(b)

Figure 2: Label Distributions of the Featured Judges and Label Distributions of the Corresponding Majority Vote on the Clean set (P=Perfect, E=Excellent, G=Good, F=Fair, B=Bad).

LambdaMART, a state-of-the-art ranker based on boosted regression trees and lambda-gradients.
The training data consists of label(s) and a feature vector for each query-url pair. A label indicates the relevance of a url to a query. Each query-url pair is judged on a 5-level relevance system scaling from highly relevant to not relevant. The 5 levels and respective numeric values are: Perfect (4), Excellent (3), Good (2), Fair (1), and Bad (0).
3.2 Individual Opinion vs. Consensus
One of the label set used in our experiments is called Clean, which consists of 2,093 queries and 39,268 query-url pairs, with on average 19 urls per query. The queries in Clean were selected by hand from a large online query log.
There were 120 judges involved in the labeling process of Clean. Each query-url pair was judged by 11 judges. We found that the judges showed individual differences; some judges are more extreme and tend to label a sample either Perfect or Bad, whereas other judges are more moderate and tend to label a sample Fair.

Figure 2(a) shows label distributions of the "featured" judges, who consistently assigned one label the most frequently among all the judges. For instance, "the most Perfect judge" assigns "Perfect" to samples the most frequently among all the judges. Comparing the label distributions of the five "featured" judges, we found that there exists statistically significant difference among individual judges' opinions.
Moreover, their individual opinions are also statistically significantly different from the consensus of all judges involved. Figure 2(b) shows the corresponding majority vote (consensus) among the judges who judged the same set of query-url pairs as the featured judge. Comparing the label distributions of the featured judges and the label distribution of the corresponding majority vote, we found that the individual opinions again are statistically significantly different from the consensus.
Since individual opinion differs from consensus, which one is better? Can we make use of overlapping labels to remove the individual variance and improve a search engine's retrieval accuracy? The variances among individual judges and the differences between an individual judge and consensus both post challenges to discovering an effective way to use overlapping labels. The following sections provide answers to these questions.
4. USING OVERLAPPING LABELS
As seen in Figure 1, multiple labelers can lead to cleaner higher quality training sets than a single best judge. This section discusses issues of using overlapping relevance labels. Section 4.1 presents methods of aggregating overlapping labels. Section 4.2 discusses the methodology of assigning different weights to different labels.
4.1 Aggregating Overlapping Labels
Suppose there are n samples, each of which needs to be labeled. For each sample, there are k labelers; each of them assigns one label to the sample, which yields k labels per sample.
There are many commonly used methods of aggregating k overlapping labels per sample. We focus on the following three widely used aggregation methods.
4.1.1 K-Overlap (Using All Labels)
This simple method is to train a model using all overlapping labels for each sample. That is, we input k labels (from k labelers) for each query-url pair, rather than one label. We call this method k-overlap since it uses k overlapping labels for each sample. The feature vector of a sample is repeatedly used for the k labels. Therefore for each training sample, there are k training instances with identical feature vectors and k labels each come from a different labeler.
The number of training instances is increased from n to kn. Hence, this method yields a training cost of kn. The labeling cost is k since k labelers are required for this scheme.
 Note that when k=1, we have the single labeling scheme, where each sample has exactly one label. This is the most commonly used labeling scheme in supervised learning, including learning to rank. The single labeling scheme yields a training cost of n, and a labeling cost of 1.
4.1.2 Majority Vote
Another commonly used method of aggregating overlapping labels is majority vote. The majority vote of multiple overlapping labels is the most frequent label among the k labels. If there is a tie (for example, there are 2 labels are Good, 2 Fair and 1 Bad,

461

then there is a tie between Good and Fair), we first sort the most frequent labels in the order of most-relevant to least-relevant, i.e., in the order of Perfect, Excellent, Good, Fair, and Bad. For the above example, the most frequent labels are sorted into (Good, Fair). The majority vote is then picked as the label which is indexed by ceiling(m/2), where m is the number of most frequent labels. In this case, m=2, ceiling(m/2)=1, which corresponds to Good. Here the index starts from 1.
This method yields a training cost of n (as low as the single labeling scheme) since each sample uses one label in the training process; and a labeling cost of k since it required k labels.
4.1.3 The Highest Labels
This aggregating scheme is designed in particular for the Web search task. Out of the k labels for a sample, the highest label is obtained by first sorting the k labels into the order of mostrelevant to least-relevant, i.e., in the order of Perfect, Excellent, Good, Fair, and Bad; then picking the label at the top of the sorted list. For the above example, the highest label is Good.
This method yields a training cost of n (as low as the single labeling scheme) since each sample uses one label in the training process; and a labeling cost of k since it required k labels.
4.2 Weighting the Labels
Web search is a task which emphasizes precision more than recall. It suggests that finding a perfect relevant url to a query is much more important than finding all relevant urls to a query. It is particularly true when using an evaluation metric such as NDCG or Precision at rank position n, which are both common evaluation measures for search ranking models. Therefore, a sample which is labeled as "Perfect" probably deserves more weight during the model training process.
Moreover, relevant labels, such as "Perfect", are rare and should be weighted more during the training process. LambdaRank and LambdaMart optimize directly for NDCG emphasize more on relevant labels and on higher positions in the ranking. In Figure 3, the label distribution of all of the labels in Clean shows that in the order of most-relevant to least-relevant, the amount of labels quadratically increases. The number of relevant samples is much less than the number of non-relevant samples.
In this paper, we assign different weights to labels in a simple way: for samples labeled as Perfect/Excellent/Good, we assign a training weight w1, and for samples labeled as Fair or Bad, we assign a training weight w2, where w1=w2, and >1.
The different weights of labels will neither change the training cost nor the labeling cost of the aggregating methods mentioned in Section 4.1, since both the number of training samples and the number of labels do not change.
5. THE SELECTIVE LABELING SCHEME
This section describes our proposed labeling scheme, if-good-k. In particular, the section studies how to cheaply and effectively use overlapping labels to improve a search engine's retrieval accuracy. The new scheme assigns additional labels to a subset of samples. Section 5.1 describes the intuition behind this scheme. Section 5.2 describes this selective labeling scheme.
5.1 The Intuition
People are difficult to satisfy, in particular in Web search, partly because of the gap between the real information need of a user and the query (s)he issues, and partly because of the limitations of

Figure 3: Label Distribution of the Clean Label Set.

the state-of-the-art retrieval technology. It is rare that a user finds that a url is perfectly relevant to his/her query, in particular for informational queries, rather (s)he often feels that a url is a bad result for his/her query. Therefore, if a labeler thinks a url is relevant to a given query, it is worthwhile to verify others' opinions, If a labeler thinks a url is bad, his/her opinion should be trusted.
Since there are relatively few highly relevant samples, training on highly relevant samples can be unstable. Moreover, it is usually hard for people to agree on some urls being relevant to a query. Labelers often disagree in their selection of the Perfect urls. This disagreement among labelers introduces variance and noise in the training data.
Based on this intuition, we propose to pay more attention to urls labeled as relevant rather than urls labeled as non-relevant. In our case, the relevant labels are "Good and above" (Good+), i.e., Perfect, Excellent, or Good; and the non-relevant labels are "Fair and below" (Fair-), i.e., Fair or Bad.

5.2 Selective Overlapping Labeling

5.2.1 If-Good-k
This scheme collects k-1 additional overlapping labels only on samples previously judged as "Good and above", i.e., Perfect, Excellent, or Good. If a sample is judged as "Fair and below", i.e., Fair or Bad, no additional label are requested and only the original label is used.

For example, when k=3, for a list of url samples, the following labels are given by three labelers under this scheme: (Excellent, Good, Fair), (Bad), (Good, Good, Perfect), (Fair), (Fair), (Perfect, Fair, Good).

The training cost and the labeling cost of this scheme depend on

the Good+:Fair- ratio among the first labelers. If the Good+:Fair-

ratio among the first labelers is r, then both the training and

labeling cost of if-good-k are

r

n 1



nr r 1

k

.

5.2.2 Good-Till-Bad
This scheme continues to collect additional overlapping labels on samples previously judged as "Good and above" until the labels meet the first "Fair and below". If a sample is judged as "Fair and below", i.e., Fair or Bad, no additional label are requested and the original label is used.

This scheme assumes a larger set of labelers are available, but not unlimited. There is still an upper bound of how many labels can be obtained for each sample. Suppose there are k labelers, then this scheme will generate up-to-k overlapping labels for a sample.

462

For example, when k=11, for a list of query-url pairs, the following labels are given by these 11 labelers under this scheme: (Excellent, Good, Fair), (Bad), (Good, Good, Perfect, Excellent, Good, Bad), (Fair).

The pros of good-till-bad over if-good-k is that it encourages collecting more additional labels for samples previously judged relevant, which provides more training data for samples that need more attentions. The cons of good-till-bad, comparing to if-goodk, is that it may result in a more expensive labeling cost since it requests a relatively larger pool of labels.

The training cost and the labeling cost of good-till-bad also

depend on the Good+:Fair- ratio (the ratio of Good or better labels

to Fair or worse labels) among the first labelers. If the

Good+:Fair- ratio among the first labelers is r, and there are k

labelers, then both the training and labeling cost of good-till-bad

are at most

r

n 

1



nr k . r 1

6. EXPERIMENTS

6.1 Datasets
There are two label sets used in the experiments. The first label set is called Clean and consists of 2,093 queries and 39,268 queryurl pairs, with on average 19 urls per query. The queries in Clean were selected by hand from a large online query log. The labeling was done in December 2007. There were 120 judges involved in the labeling process of Clean. Each query-url pair was judged by 11 judges, hence there are 11 labels for each query-url pair. 364 queries do not have judgments from the same set of 11 judges for all urls for that query. In all other cases, all urls for a given query have been judged by the same set of 11 judges.

There are two feature sets used for Clean in our experiments. Note the two feature sets were generated in different periods, and differ in that the click and anchor information may have changed for the urls in the Clean label set, or that the pages themselves may have changed during the five months between the creation of the two feature sets. One set of features were obtained in August 2007. The training data consisting of these feature vectors and the Clean labels, called Clean07, contains 2,071 queries and 31,867 urls. The corresponding standard test set contains 5,207 queries and 930,951 urls. The second feature sets were obtained in January 2008. The training data created from these feature vectors and the Clean set labels, called Clean08, contains 1,563 queries and 287,903 urls. The corresponding standard test set contains 7,260 queries and 1,093,020 urls.
Another label set is called Clean+ and consists of 1,000 queries and 49,785 query-url pairs. Clean+ was created specifically to evaluate our proposed labeling scheme, if-good-k. The query-url pairs were labeled in a way that if the first judge labeled a pair as Perfect, Excellent, or Good, then two additional labels were requested from two additional judges, which yielded 17,800 additional labels; if the first judge labeled a query-url pair as Fair or Bad, then no more labels were requested. The feature vectors were obtained in July 2009 and the overlapping labels were collected during the week of 08/24/2009. The corresponding standard test set contains 11,898 queries, and 1,732,516 urls.

6.2 Evaluation Metrics
NDCG [3] is used as the evaluation metric in our experiments. NDCG is a retrieval measure which recognizes multilevel
relevance labels. It is particularly suitable for Web search applications since it accounts for multilevel relevance and the

truncation level can be set to model user behavior. NDCG for a given query at truncation level L is calculated as follows:

 NDCG@L = 1 L 2l(i) 1 Z i1 log(1  i)

(1)

where l(i) {0,1,2,3,4} is the relevance label of the document at rank position i, and L is the level to which NDCG is computed.

The main evaluation metric used in the experiments is NDCG@3. We also report NDCG@1, NDCG@5, and NDCG@10.

6.3 Experimental Settings
Based on Section 4 and Section 5, there are many different methods of using overlapping labels. Due to space limitations, we only report results for 9 interesting experimental settings. The 9 experimental settings are as follows:
Baseline: This is the single labeling scheme. There is one label for each query-url pair. For the Clean label set, the baseline labels are simulated by randomly drawing 1 label from the 11 labels. For Clean+, the labels from the first labelers are used as the baseline.

3-overlap: This is the k-overlap method with k=3. There are 3 overlapping labels obtained for each query-url pair. The rankers are trained on all overlapping labels and corresponding feature vectors. For the Clean label set, the 3 overlapping labels are created by randomly drawing 3 labels from the 11 labels. This experiment setting is not applicable to Clean+.

11-overlap: This is the k-overlap method with k=11.This experimental setting uses all 11 labels in the Clean label set as the training data. This setting is not applicable to Clean+.

Mv3: This is the majority vote method over 3 overlapping labels. The 3 overlapping labels are drawn randomly from the Clean label set. This experiment setting is not applicable to Clean+.

Mv11: This is the majority vote method over 11 overlapping labels. The 11 overlapping labels are all from the Clean label set. This setting is not applicable to Clean+.

If-good-3: This is the if-good-k labeling scheme, with k=3. The 3 overlapping labels for Clean are randomly drawn from the 11 labels for each query-url pair. This setting is applicable to both Clean and Clean+.

If-good-x3: This experimental setting combines the idea of selective labeling and weighting labels. If a label is "Good or above", the label is assigned a weight which is  times of the weight of other labels. In this setting, =3. This setting is applicable to both Clean and Clean+.

Highest-3: This experimental setting uses the most relevant label of each query-url pair for training. In particular, this setting uses the highest label among k overlapping labels, with k=3. The 3 overlapping labels for Clean are randomly drawn from the 11 labels per sample. This setting is not applicable to Clean+.

Good-till-bad: This is the Good-till-bad labeling scheme. The upper limit of overlapping labels is k, with k=11 for the Clean label set. This setting is not applicable to Clean+.

Note that for the Clean label set, which contains 11 labels for each query-url pair, there is a constraint of k11. We performed a random sampling from these 11 labels if k<11. Due to the randomness of getting k labels when k<11, the averaged results of

463

Table 1: Retrieval Accuracy of Using Overlapping Labels (Clean07, LambdaRank).

Experiment
If-good-3 11-overlap Good-till-bad Highest-3 3-overlap If-good-x3 Mv3 Mv11 Baseline

NDCG@3
49.55%** 49.30% 49.22% 49.16% 49.00% 48.98% 48.87% 48.69% 48.60%

NDCG@1
46.23% 45.77% 45.72% 45.75% 45.52% 45.25% 45.07% 45.25% 45.18%

NDCG@5
51.81% 51.60% 51.73% 51.49% 51.51% 51.26% 51.36% 51.11% 51.02%

NDCG@10
55.38% 55.09% 55.23% 55.01% 54.90% 54.82% 54.93% 54.58% 54.51%

Table 2: Retrieval Accuracy of Using Overlapping Labels (Clean08, LambdaRank).

Experiment
If-good-3 Highest-3 11-overlap Mv11 If-good-x3 3-overlap Mv3
Good-tillbad
Baseline

NDCG@3
45.99%* 45.97%* 45.96%* 45.89% 45.80% 45.78% 45.66%
45.58%
45.53%

NDCG@1
45.03% 44.87% 44.93% 44.97% 44.73% 44.77% 44.83%
44.88%
44.72%

NDCG@5
47.53% 47.48% 47.57% 47.56% 47.40% 47.54% 47.09%
47.05%
46.93%

NDCG@10
50.53% 50.43% 50.58% 50.58% 50.13% 50.50% 49.83%
49.86%
49.69%

Table 3: Retrieval Accuracy of Using Overlapping Labels (Clean07, LambdaMart).

Experiment
If-good-3 3-overlap 11-overlap Mv11 If-good-x3 Highest-3 Mv3 Baseline

NDCG@3
45.93%* 45.91%* 45.48% 45.42% 44.80% 44.77% 44.45% 44.01%

NDCG@1
44.63% 44.70% 44.31% 44.46% 43.78% 43.52% 43.48% 42.96%

NDCG@5
47.65% 47.59% 47.02% 47.16% 46.42% 46.49% 46.11% 45.56%

NDCG@10
50.37% 50.35% 49.97% 50.09% 49.26% 49.44% 49.12% 48.30%

Table 4: Retrieval Accuracy of Using Overlapping Labels (Clean08, LambdaMart).

Experiment
If-good-3 11-overlap Mv11 Highest-3 If-good-x3 3-overlap Baseline Mv3

NDCG@3
45.93%* 45.89%* 45.48% 45.40% 44.79% 44.76% 44.45% 44.02%

NDCG@1
44.64% 44.69% 44.30% 44.49% 43.78% 43.51% 43.47% 42.95%

NDCG@5
47.65% 47.60% 47.10% 47.14% 46.40% 46.48% 46.09% 45.54%

NDCG@10
50.38% 50.33% 49.95% 50.07% 49.25% 49.43% 49.10% 48.28%

Table 5: Retrieval Accuracy of Using Overlapping Labels (Clean+, LambdaRank).

Experiment
If-good-2 If-good-3 Baseline If-good-x3

NDCG@3
48.57%** 48.41% 48.20% 48.16%

NDCG@1
50.53% 50.33% 50.32% 50.04%

NDCG@5
48.56% 48.48% 48.31% 48.18%

NDCG@10
50.02% 49.89% 49.65% 49.61%

Figure 4: NDCG@3 for If-Good-k Runs (Clean07, LambdaRank).
Figure 5: NDCG@3 for If-Good-k Runs (Clean08, LambdaRank).
Figure 6: NDCG@3 for If-Good-k Runs (Clean+, LambdaRank).
5~10 runs are reported for each experimental setting. For the Clean+ label set, there is no such random sampling needed.
6.4 Effect on Retrieval Accuracy
This section reports search engine performance results for two supervised rankers. The two rankers used in the evaluation are LambdaRank and LambdaMart. Statistical significant tests are also performed to compare each experimental setting to the baseline. An NDCG@3 number is marked by "*" in the following tables means it is statistically significant better than the baseline in a t-test. An NDCG@3 number is marked by "**"means it is statistically significant better than all the other settings. Table 1 shows the retrieval accuracy, which is measured in NDCG, for Clean07 using LambdaRank. The results are sorted decreasingly by NDCG@3. The best run is if-good-3, which has 0.95 point gain of NDCG@3 as compared to the baseline. This gain is statistically significant. Table 2 shows the retrieval accuracy for Clean08 using LambdaRank. Similarly, the best run is if-good-3, which introduces 0.46 point gain of NDCG@3 as compared to the baseline. This gain is statistically significant.

464

Table 3 and Table 4 show the retrieval accuracy (measured in NDCG) when using LambdaMart trained on Clean07 and Clean08, respectively. The best runs for both experiments are ifgood-3. As compared to the baseline, if-good-3 introduces 1.92 point gain of NDCG@3 for Clean07 and 1.48 point gain of NDCG@3 for Clean08, respectively. The gains are statistically significant.
Table 5 shows the NDCG numbers of applying several selective overlapping labeling schemes and the baseline. LambdaRank is the ranker used in the evaluation. The results show that if-good-2, which requests only 1 additional overlapping label for samples originally labeled as Good+, yields the best NDCG numbers for NDCG@3, NDCG@1, NDCG@5, and NDCG@10. Based on NDCG@3, if-good-2 introduces a 0.37 point gain as compared to the baseline. This gain is statistically significant.
Note that although if-good-k (k=3 or k=2) runs consistently outperform all other methods and consistently statistically significantly outperform the baseline, the if-good-k runs may not always work statically significant better than some overlapping methods. For example, for Clean07 and LambdaMart, if-good-3 produces similar retrieval accuracy as 3-overlap. However, in Section 6.6 we will show that if-good-k is cheaper than other overlapping methods.
6.5 The Amount of Additional Labels
The experimental results reported in Section 6.4 show that selective overlapping labeling, in most cases, statistically significantly outperforms other methods of using overlapping labels. In the selective overlapping labeling scheme, for example, if-good-k, one may want to know the value of k, i.e., the number of additional labels required for the selected samples impact the ranking accuracy. In this section, we examine how many additional labels are needed for a selected sample to best improve the retrieval accuracy.
For the if-good-k scheme, k-1 additional labels are requested from labelers. For the good-till-bad scheme, additional labels are requested from more labelers with an upper limit of 11 labels. The baseline is the single labeling scheme.
Figure 4 and Figure 5 show the NDCG@3 numbers of the ifgood-k runs for Clean07 and Clean08, respectively. The tested k values range from 2 to 6. In addition, the good-till-bad scheme is also tested. Both schemes are compared with the baseline. The results indicate that when k=3, i.e., two additional labels are requested, the if-good-3 scheme gives the best performance for both Clean07 and Clean08 datasets.
Figure 6 shows the NDCG@3 numbers of the if-good-k runs for Clean+. Since Clean+ only contains at most 3 labels for each query-url sample, the k values tested are 2 and 3. The best run is if-good-2, where only one additional label is requested for a selected sample which was previously labeled as Good+.
6.6 The Costs of Overlapping Labeling
The experimental results in Section 6.4 suggest that the proposed selective overlapping labeling scheme is promising to improve Web search retrieval accuracy. However, one may be concerned about the actual training and labeling costs associated with the proposed labeling scheme. The key reason for using our selective overlapping scheme is that it achieves similar and better accuracy to other methods, and significantly outperforms the baseline, with minimal labeling cost.

Table 6, Table 7, and Table 8 show the costs of various overlapping labeling schemes for Clean07, Clean08, and Clean+, respectively. In particular, these tables illustrate the labeling overhead, the training overhead, as well as the rate between the number of samples labeled as Fair- (Fair or Bad) and the number of samples labeled as Good+ (Good, Excellent, or Perfect).

Table 6: The Costs of Various Overlapping Labeling Schemes (Clean07).

Experiment
Baseline 3-overlap mv3 mv11 If-good-3 If-good-x3 Highest-3 Good-till-bad 11-overlap

Labeling Overhead
1 3 3 11
1.41 1 3
1.87 11

Training Overhead
1 3 1 1
1.41 1.41
1 1.87 11

Fair-: Good+
3.72 3.71 4.49 4.37 2.24 2.24 1.78 1.38 4.37

Table 7: The Costs of Various Overlapping Labeling Schemes (Clean08).

Experiment
Baseline 3-overlap mv3 mv11 If-good-3 If-good-x3 Highest-3 Good-till-bad 11-overlap

Labeling Overhead
1 3 3 11
1.45 1 3
1.98 11

Training Overhead
1 3 1 1
1.45 1.45
1 1.98 11

Fair-: Good+
3.51 3.47 4.16 4.17 2.09 2.09 1.58 1.23 4.17

Table 8: The Costs of Various Overlapping Labeling Schemes (Clean+).

Experiment

Labeling

Training

Fair-: Good+

Overhead

Overhead

Baseline

1

1

3.18

If-good-2

1.23

1.23

2.09

If-good-3

1.48

1.48

2.15

If-good-x3

1

1.48

1.06

The labeling overhead is calculated as the rate between the

number of samples needed to be labeled by a labeling scheme and

the number of sampled needed to be labeled by the baseline. The

training overhead is calculated as the rate between the number of

training samples by a labeling scheme and the number of training

samples used by the baseline. The baselines are the single labeling

scheme, where one label is assigned to each sample by one judge.

The runs which produce the best NDCG results in Section 6.4 are

the if-good-k (k=2 or k=3) runs (in bold font in Tables 6-8). Tables

6-8 show that these if-good-k runs only generate a low labeling

overhead of around 1.4 as compared to the baselines. Such low

labeling overhead suggests that the proposed overlapping labeling

scheme is not only beneficial to improve retrieval accuracy, but

also cost efficient and effective when used in ranking models.

7. DISCUSSIONS
The experiments show that the if-good-k labeling scheme not only improves the retrieval accuracy of both LambdaRank and LambdaMart on real-world Web test sets, but also consistently outperforms other methods of using overlapping labels in cost effectiveness, with NDCG gains significantly better than the baseline, and significantly better than other overlapping methods in most cases.

465

What is the secret behind the newly-proposed labeling scheme? At the beginning, we thought it is because more positive/relevant training samples (samples labeled as Good and above) are given to the ranker; hence the ranker has a more balanced training sample set. We therefore tried to simulate a training dataset with more Good+ samples by repeating a Good+ label for k times, which is equivalent to weighing a sample labeled as Good+ k times more than a sample labeled as Fair-. The if-good-x3 runs reported in Table 1-5 are designed based on this. However, the results show that simply repeating the Good+ labels or simply increasing the weights of the Good+ samples do not work: the if-good-x3 runs are ranked in the middle-to-low range of search performance among all the runs. Table 5 shows that in Clean+, the if-good-x3 run even performs worse than the baseline. This shows that the performance gain of the selective overlapping labeling does not come from more Good+ samples.
We believe that the performance gain of if-good-k comes from generating higher quality labels. The if-good-k scheme correctly captures the worthiness of reconfirming a judgment for a sample. As we have mentioned in Section 5.1, if someone thinks a url is good, it is really worthwhile to double check with more people. On the other hand, if someone thinks a url is bad, we can trust the label. The if-good-k scheme yields higher quality labels by considering more opinions from different judges on those samples that need to be noise-free.
However, the experiments show that it is not true that the more the additional opinions for those selected samples, the better the retrieval accuracy. The experiments shown in Figures 4-6 were designed to find out how many additional labels are needed to most improve retrieval accuracy. It turns out that only a few additional labels are needed to improve the retrieval accuracy. In our experiments, one or two additional labels are good enough to beat other commonly used labeling schemes. It is not surprising since too many opinions from different labelers may create too much noise and too high variance in the training data; and it is hard for the model to learn useful information from such noisy and highlyvariant training data.
In addition, the experiments in Section 6.4 suggest some labeling schemes do not work. It is surprising to see that the majority vote scheme performs significantly worse than just simply using all of the k labels (K-overlap) to train a model, in some cases. Moreover, as we mentioned earlier, simply changing weights for labels is not equivalent to collecting additional labels, if-good-xk does not perform as well as if-good-k.
The run which produces the best NDCG results with the lowest labeling cost for Clean07 and Clean08 is if-good-3, and for Clean+ is if-good-2. Note that these best runs only require a few additional labels. This makes both the labeling cost and the training cost for a good run low. As mentioned in Section 5.2, the costs of a selective overlapping labeling scheme depend on the label distribution of the original labels. Nevertheless, the if-good-3 scheme is able to generate a labeling cost with a low overhead of around 1.4 labels per sample in general, which makes the proposed overlapping labeling an affordable and promising approach to be applied in real search engine training or other supervised training applications.

8. CONCLUSIONS
This paper explores whether, when, and for which samples one should obtain overlapping, expert training labels, as well as what to do with them once they have been obtained. In particular, this paper recommends a new method of effectively and efficiently producing and using overlapping labels to improve data quality and search engine's retrieval accuracy. The proposed selective labeling scheme requests additional labels only when the original labels are relevant. The experiments show that this labeling scheme improves the NDCG of both LambdaRank and LambdaMart ranking models on several real-world Web test sets, with a low labeling overhead of around 1.4 labels per sample. The proposed labeling scheme consistently outperforms several methods of using overlapping labels, such as majority vote, k-overlap labels, the highest labels, etc. Moreover, it is best to choose the labels via the if-good-3 or ifgood-2 method, which achieves statistically significant NDCG@3 gain over using only one label per sample.
9. ACKNOWLEDGEMENT
We would like to thank Paul Bennett, Rich Caruana, and Chris Burges for their helpful discussions and feedback, Rangan Majumder for preparing the label sets, and anonymous reviewers for their useful comments and suggestions.
10. REFERENCES
[1] A. Bernstein and J. Li. From active towards interactive
learning: Using consideration information to improve labeling correctness. University of Zurich, Dynamic and distributed information systems group working paper. www.ifi.uzh.ch/ddis/nc/publications.
[2] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to rank with
nonsmooth cost functions. NIPS'06.
[3] Kalervo Järvelin and Jaana Kekäläinen. Ir evaluation methods
for retrieving highly relevant documents. SIGIR'00.
[4] NAACL-HLT 2010 Workshop on Amazon Mechanical Turk. http://sites.google.com/site/amtworkshop2010/.
[5] V. S. Sheng, F. Provost, and P. G. Lpeirotis. Get another label?
Improving data quality and data mining using multiple noisy labelers. KDD'08.
[6] P. Smyth, U. M. Fayyad, M. C. Burl, P. Perona, and P. Baldi.
Inferring ground truth from subjective labeling of Venus images. NIPS'94.
[7] P. Smyth, M. C. Burl, U. M. Fayyad, P. Perona. Knowledge
discovery in large image databases: Dealing with uncertainties in ground truth. AAAI Knowledge Discovery in Databases Workshop of KDD'94.
[8] P. Smyth. Bounds on the mean classification error rate of
multiple experts. Pattern Recognition Letters 17, 12. 1996.
[9] R. Snow, B. O'Connor, D. Jurafsky and A. Y. Ng. 2008.
Cheap and Fast ­ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. EMNLP'08.
[10] E. M. Voorhees. Evaluating by highly relevant documents.
SIGIR'01.
[11] Q. Wu, C. J. C. Burges, K. M. Svore and J. Gao. Ranking,
Boosting, and Model Adaptation. Microsoft Research Technical Report MSR-TR-2008-109.

* This work was done while the first author was an intern at Microsoft Bing/Microsoft Research.

466

Multi-Style Language Model for Web Scale Information Retrieval

Kuansan Wang
Microsoft Research One Microsoft Way Redmond, WA 98052 USA
kuansan.wang@microsoft.com

Xiaolong Li*
Bing Search Ranking & Intent Group One Microsoft Way
Redmond, WA 98052 USA
xiaolong.li@microsoft.com

Jianfeng Gao
Microsoft Research One Microsoft Way Redmond, WA 98052 USA
jfgao@microsoft.com

ABSTRACT 1
Web documents are typically associated with many text streams, including the body, the title and the URL that are determined by the authors, and the anchor text or search queries used by others to refer to the documents. Through a systematic large scale analysis on their cross entropy, we show that these text streams appear to be composed in different language styles, and hence warrant respective language models to properly describe their properties. We propose a language modeling approach to Web document retrieval in which each document is characterized by a mixture model with components corresponding to the various text streams associated with the document. Immediate issues for such a mixture model arise as all the text streams are not always present for the documents, and they do not share the same lexicon, making it challenging to properly combine the statistics from the mixture components. To address these issues, we introduce an "openvocabulary" smoothing technique so that all the component language models have the same cardinality and their scores can simply be linearly combined. To ensure that the approach can cope with Web scale applications, the model training algorithm is designed to require no labeled data and can be fully automated with few heuristics and no empirical parameter tunings. The evaluation on Web document ranking tasks shows that the component language models indeed have varying degrees of capabilities as predicted by the cross-entropy analysis, and the combined mixture model outperforms the state-of-the-art BM25F based system.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Theory, Experimentation.
Keywords
Information Retrieval, Mixture Language Models, Smoothing, Parameter Estimation, Probabilistic Relevance Model.
*This work was done when Xiaolong Li was with Microsoft Research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

1. INTRODUCTION
Inspired by the success in speech recognition, Ponte and Croft [23] introduced the language modeling (LM) techniques to information retrieval (IR) that have since become an important research area. The motivation is very simple: just as we would like a speech recognition system to transcribe speech into the most likely uttered texts, we would like an IR system to retrieve documents that have high probabilities of meeting the information needs encoded in the query. Over a decade of studies on this topic, it has been now widely understood that LM is a principled realization of the statistical approach envisioned by Maron and Kuhns at the dawn of IR [19], and that its underlying statistical framework provides mathematically sound explanations to why many proven heuristics, such as TF/IDF weightings and document length normalization, have been working so well [1][6][9][18][31]. As is in the case of speech recognition, LM for IR can be formulated as a Bayesian risk minimization problem [16], for which the optimal performance can be achieved by following the maximum a posteriori decision rule that was first shown in [7] and reiterated for IR by Zhai and Lafferty [33]. Specifically, given a query Q, a minimum risk retrieval system should rank the document D based on product of the likelihood of the query under the document language model, PD(Q), and the prior of the document P(D):

s(D,Q)  PD (Q)P(D)

(1)

An enthusing question still in the center of the LM for IR research is what the document language model is and how it could be obtained [6][18][31]. While it is intuitive to use the text body to train the document language model as in the majority of the work [18][31], it has been widely recognized that queries are often composed in a different language style than the document body, and a poor query likelihood can thus occur for relevant documents because of the style mismatch. To this end, Miller et al. [21] has proposed a hidden Markov model in which an additional latent stage is included to model the query generation process. Lafferty et al. have argued for an explicit model of the query language itself [16], and proposed the machine translation techniques to bridge the gap between the body and the query [1]. Jin et al. [14], for example, used the title and the body of a document as the target and the source languages, respectively, and demonstrated that the "translated" title LM can be a viable choice as the PD for IR. The two-stage LM by Zhai and Lafferty [33] proposed yet another idea of using smoothing techniques. There, the document LM is first created by smoothing the document body with a body background model, which is then followed by a second stage of smoothing ideally with a query background model.

In practice, documents often have more fields than just the title and the body. This is particularly true in the Web environment where, in addition to the textual contents created by the document authors, Web documents are also annotated with inbound anchor

467

text by other document authors, as well as the user queries leading to clicks on the documents. Traditional IR has viewed the multiple-field document retrieval as a structured document retrieval problem, and some established retrieval models, such as BM25, have been generalized to multi-field document retrieval [26]. A straightforward generalization for LM is to view the document as being described by multiple text streams. As shown in Sec. 2, a quantitative analysis on the documents indexed by a commercial Web search engine does confirm that all these text streams seem to be written in their own language styles and have varying degrees of mismatch with the query, justifying the idea to model their linguistic characteristics separately. Empirical studies on applying the LMs for different task domains have also confirmed that mixing textual sources with different language styles can significantly degrade the quality of LMs (e.g., [2]).

When a probability space is divided into disjoint partitions, the probability of an event can be evaluated as the sum of the conditional probability of the event occurring in each partition, weighted by the prior of that partition. Apply this principle to the document modeling and let Di denote the ith stream of D and PDi the corresponding component LM for the stream, we have

  PD  P(Di | D)PDi  wDi PDi

(2)

i

i

Such a mixture distribution has been widely used for LM in speech and language processing [11] as well as in IR (e.g., [22]). However, beneath the simple linear interpolation form of (2) lies the serious question of the conditions under which the component LMs can be combined properly. Since the foundation of mixture modeling is derived from the probability space partitioning, all the mixture components should therefore be modeling the same underlying probability space. It is widely known [11] that LMs having different sets of vocabulary should be viewed as modeling different domains and therefore their scores cannot be directly compared, let alone combined into a mixture distribution. When applying LM for IR, for example, it is critical to ensure that all document LMs have the same vocabulary so that the document LMs do not selectively treat different portion of the query as out of vocabulary (OOV) terms. The common approach to smooth the document LMs with a shared background model effectively makes all documents use the same vocabulary of the background model. Still, running into OOVs is quite common. For IR using non-mixture LM, encountering OOVs in a query is not a severe problem because the impact in computing the ranking scores (1) is the same for all the documents. This is not true for mixture LM described by (2) since OOVs of one mixture component are not necessarily OOVs for others, making how to properly compute the combined probability of the query a critical question.

To address this problem, we in this work undertake a so-called "open-vocabulary" LM approach that is prevalent in the language processing community (e.g., [11]) but has not been extensively studied for IR. At the core of the open-vocabulary LM is a formidable challenge to assess the probability mass for OOVs. LMs that can yield non-trivial probabilities for OOVs can be viewed as modeling a language with infinite vocabulary. All openvocabulary LMs thus at least have the same cardinality for their vocabulary, and their probabilistic scores are on a more solid ground to be comparable and combined. As surveyed in Sec. 3, the open vocabulary LM is far from a solved research problem because it inevitably requires one to "guess" the unseen. All the techniques proposed in the past five decades have all involved some kinds of heuristics or parameter tunings that make it challenging to deploy the model outside of research labs. This is be-

cause the application domains usually have different environmental conditions that are either not expected by the heuristics or are incongruent to the properties of the tuning data. The scale of the Web typically amplifies the difficulty of these issues, as demonstrated in the machine learning results reported in [28] that show the retrieval performance can be highly volatile depending on how the parameters in BM25F are acquired.
In this paper, we propose an information theoretically motivated method towards open vocabulary LMs. The emphasis here is to obtain an analytically tractable and fully automated system that alleviates the problems arising from heuristic parameter tunings. Typically, such an approach can only yield "statistically optimal" outcome and cannot guarantee the performance be better than fine-tuned systems in all cases. We apply this spirit to both the smoothing of the mixture component LM PDi and the estimation of the mixture weights in (2). In Sec. 4, we present the detailed mathematical derivation that shows how the smoothing parameters can be obtained by computing how N-gram is predicted by (N-1)-gram. In particular, the OOV probability mass, which is equivalent to unigram discount, can therefore be estimated by inspecting how the unigram is predicted by the zero-gram. In Sec. 5 we describe the methods to compute mixture coefficients, and in Sec. 6 we describe the experimental results.
The contributions of the paper are as follows: First, we provide a large scale quantitative analysis to verify how the query language is different in style from document body. We confirm and generalize the prevalent informal observations that, on the Web scale, various fields associated with the documents do have significantly different properties. From a modeling perspective, the analytical outcomes suggest these text sources are better modeled separately. Based on the analysis, we propose a mixture LM approach to IR. The mixture model has to address two immediate and formidable challenges. First, it requires an open-vocabulary LM that has no known solution without heuristics until this work. We propose a mathematically tractable close form solution to realize openvocabulary LMs. Secondly, the mixture model increases the number of parameters, and we show IR results are very sensitive to tuning. We show that our proposed analytical method can achieve high quality performance without empirical tuning.
2. WEB LANGUAGE STYLE ANALYSIS
The observations that the query language is different in styles from document body and may be closer to titles are intuitive. To formalize the analysis, we conduct a large scale analysis on a June 2009 snapshot of the Web documents in the EN-US market. We examine the language usages in the document text body, the title, the anchor text, as well as the queries against a commercial search engine at the same time. To quantify the language usages in these streams, we first build a statistical N-gram LM for each of them and study the complexity of the language using information theoretic measurements such as entropy or cross-entropy. The LMs used in this section, with the exception of query LMs, are all publicly accessible through [20]. Formally, the cross-entropy between model PA and PB is
 H (PA || PB )   PA (t) log PB (t) t
Since the logarithmic function is convex, it can be easily shown that the cross entropy is smallest when the two models are identical. The cross entropy function can therefore be viewed as a measurement that quantifies how different the two models are. The entropy of a model PA, H (PA) = H (PA || PA). To avoid the confusion on the base of the logarithm, we further convert the

468

entropy into the corresponding linear scale perplexity measurement, namely,
PPL(PA || PB )  e H (PA||PB ) Previously, it has been estimated that the trigram perplexity of general English has an upper bound of 247 words per position based on a 1 million word corpus of American English of varying topics and genres [2]. In contrast, in the Web snapshot the vocabulary size is at least 1.2 billion for the document body, and 60 million, 150 million, and 252 million for the title, anchor text and the user query streams, respectively. As our main objective is to investigate how these language sources can be used to model user queries, we study the cross-entropy between the query LM to others, the results of which are shown in Figure 1.
Figure 1: Cross-stream perplexities against queries for various streams and N-gram (N=1, 2, 3, 4)
As can be seen, when the LM grows more powerful with increasing order, the query language perplexity keeps dropping, from 1754 for unigram down to 180 for trigram and 168 for 4-gram. It thus appears that the query language falls within the previously estimated upper bound for perplexity, 247, for general English. The cross-stream perplexities give hints on the efficacy of using various streams to model the query language. The document body has shown consistently the largest mismatch with the queries, while anchor text seems the best choice among the three to model the queries with powerful enough LM (i.e., N > 1). We note that, starting at bigram, both title and anchor text models have a smaller perplexity than the unigram model of query itself. The study lends some support to the hypothesis that document title is a better source than the body to build a LM for IR. Up to trigram, the heightened modeling power with an increasing order uniformly improves the perplexities of all streams for queries, although this increased capability can also enhance the style mismatch that eventually leads to the perplexity increase at higher order. For the document body and title, the payoff of using more powerful LMs seems to taper off at bigram, whereas trigram may still be worthwhile for the anchor text. As are in many applications of LMs, the perplexity measure is not the ultimate metric for applications, in other words, models with lower perplexities do not necessarily lead to a better performance. However, the perplexity analysis is still informative in that higher perplexity models can seldom outperform the lower perplexity ones.

3. CURRENT STATE OF OPEN
VOCABULARY LANGUAGE MODEL
For any unigram LM PC with vocabulary V, the probabilities of all the in-vocabulary and OOV tokens sum up to 1. An openvocabulary LM is a model that reserves non-zero probability mass for OOVs:

  pUnk  PC (t)  1 PC (t)  0

tV

tV

When an open vocabulary model is used to evaluate a text corpus and encounter additional k distinct OOV tokens, the maximum entropy principle [13] is often applied to evenly distribute pUnk among these newly discovered OOVs, i.e., PC(t) = pUnk / k for t V . The key question is how much mass one should take away
from V and assign it for pUnk. The "discount" strategy, as is often called, remains an unsolved research problem since Shannon invented N-gram as part of the information theory.

Since its publication in 1953, the Good-Turing formula is still a widely used or served as the foundation for many discounting strategies [11]. It states that, if there are nr tokens that appear exactly r times in a corpus, then for the purpose of calculating probability we should "pretend" these tokens appear r* times where

r*



(r

 1)

nr 1 nr

Accordingly, the probability of encountering such a token is given by

 Pr 

r*

 r 0

rnr



r* |T |

where | T | denotes the total number of tokens in the corpus. By applying the Good-Turing formula for r = 0, we have the total probability mass that should be reserved for all the unseen tokens is

pUnk



n0  0 * |T |



n1 |T |

Namely, the probability mass for the unseen is equal to that of the single occurrence tokens. Obviously, how good this discounting strategy is depends heavily on how accurate the Good-Turing formula characterizes the statistical properties of the application in question. Although the Good-Turing has been shown to be useful and superior to many other heuristics for a wide range of applications, the formula is still seen as enigmatic and finding an intuitive explanation to its underlying heuristics remains an active research question [23].

4. COMPONENT MODEL SMOOTHING
USING CALM
In this paper, we adopt a more analytically tractable approach to open-vocabulary discount. The key element in our method is a model adaptation algorithm called CALM first proposed by Wang and Li [29]. A close examination of the original presentation reveals that the adaptation framework in CALM can be explained in an alternative manner using the widely known vector space paradigm. As the original CALM was developed for N-gram, we try to keep the discussion in this section general even though we only report experimental data for unigram (N = 1) in this paper.

469

4.1 Adaptation as Vector Interpolation
First, we note that a LM can be thought of as a vector from an underlying functional space in which all the admissible language models for a given lexicon V form a simplex of the space, namely,
   {P :V  [0,1], vV P(v)  1} . For example, any LM for a
trivial binary lexicon can be represented by a point within the line segment enclosed by (1, 0) and (0, 1) on a two dimensional Euclidean space as illustrated in Figure 2. Let PB denote the background LM and PO the statistics of a set of newly observed data we would like to adapt the background LM to, respectively. The goal of adaptation is to find a target PT  , PT  PB  P, such
that PT and PO are reasonably close and P, the modification on the background LM, is minimized. Because the resultant model PT has to reside on the simplex, one cannot simply use the Euclidean norm to compute distances and determine the adapted LM without constraints. However, it can be easily verified that such a constraint can be met if we choose the adjustment vector P along
the direction of the difference vector of PO  PB , namely,
P  O (PO  PB ) where O is the adaptation coefficient. Natural-
ly, we want to pick a non-negative O so as to point the adjustment towards the right direction, and to choose O < 1 so as to avoid overshoot.

(0, 1)



PO PT

P PB

(1, 0)

Figure 2: A 2-dimesional illustration of model adaptation in the probability space

Putting it together, we have

PT  PB  O (PO  PB )  O PO  (1  O )PB

(3)

LM adaptation can therefore be achieved by linear interpolation, assuming the same mathematical form as smoothing.

A significant contribution of CALM is to derive how the adaptation coefficient can be calculated mathematically when the underlying LM is based on N-gram assuming a multinomial distribu-
tion. Following the work of [29], 1 O can be interpreted as the
prior probability of PB being the correct model and whose closed form formulation can be obtained using Stirling's approximation. In the Appendix we show that the adaptation coefficient for a given set of observations O can be computed as

 log(1O ) 

tO

n(t) log PB (t) LO n(t) / LO

 KL(PO

|| PB )

(4)

where LO and n(t) denote the document length and the term frequency of the term t, respectively. In short, the adaptation coefficient has a closed form relationship to the Kullback-Leibler (KL) divergence between the background model and the ML estimation of the LM of the document PO(t) = n(t) / LO. It can be verified

that, if PB(t) agrees with n(t) / LO for all terms, the adaptation coefficient O is 0 indeed. The more the background model disagrees with the observation, the more negative the right hand side of (4) will become, which leads O to approach 1.
4.2 Open Vocabulary LM through N-gram Discount
The CALM interpolation formula of (3) indicates that in the target LM PT, only O portion of the probability comes from the observation. In other words, the observation is "discounted" because a
probability mass of 1 O in the target LM is set aside for sources
external to the observation. One can therefore use (4) to compute the discount factor of an N-gram by choosing the corresponding (N-1)-gram as the background. For N > 1, (4) coincides with the formulation of the well-known Stolcke heuristics [27] that has been widely used in the N-gram LM pruning: N-grams that can be reasonably predicted by (N-1)-gram can be pruned out of the model. For the purpose of this work, we further extend the idea down to N = 1, where the observation PO and the background PB are the unigram and zero-gram LMs, respectively. Conventionally, the zero-gram LM refers to the least informative LM that treats every token as OOV, namely, its probability mass is exclusively allocated for OOV. Given an observation with a vocabulary size |VO|, a zero-gram LM would just equally distribute its probability mass equally among the vocabulary, leading (4) into

 log(1O ) 

n(t) LO

log

1/ n(t

| VO | ) / LO

 H (PO )  log |VO |

(5)

As in Sec. 2, H(PO) here denotes the (empirical) entropy of the observation LM PO. We can further convert (5) from the logarithmic into the linear scale and express the discount factor in terms of perplexity and vocabulary size:

pUnk  1  O  PPL(PO ) / | VO |

(6)

The interpretation of this outcome is quite intuitive. As well understood, perplexity is the expected number of alternatives when a language model is used to generate a token each time. The ratio of the perplexity to the vocabulary size characterizes how equivocal the language model is. The result of (6) suggests that the higher the ratio, the less certain the language model is and hence the larger the discount should be. At the extreme case when the perplexity equals the vocabulary size, the language model is basically generating tokens in the random pattern as the zero-gram, and hence the discount factor becomes 1.

4.3 Open Vocabulary Component LM
In this paper, we compose the smoothed stream component LM PDi with (3), using an open-vocabulary LM trained from the stream collection as the background model. To be more specific, we first for each document D and each stream i create a closedvocabulary maximum likelihood model as the observation PO,Di. The vocabulary for the stream VO,Ci and the closed-vocabulary stream collection model is thus obtained as

 PO,Ci  P(D)PO,Di D

The discount factor is computed with (6) and is used to attenuate the in-vocabulary probability as

PT ,Ci (t)  O,Ci PO,Ci (t), t VO,Ci
The PT,Ci is the open-vocabulary stream collection model. Finally, the stream collection model is used as the background to obtain the smoothed document stream model through linear interpolation

470

PDi   Di PO,Di  (1   Di )PT ,Ci

(7)

Here, the smoothing with the stream collection model ensures each document LM has the same number of mixture components even though the document does not have some stream observations. This smoothing alleviates the dilemma that some streams are sporadic and sparse for many Web documents.

Although the interpolation coefficient Di in (7) can in practice be kept as a free parameter to be empirically tuned (e.g., [32]), a major objective of this work is to explore alternatives that are tuning-free and thus more desirable when an IR system leaves a lab environment. In addition to the methods described in the next section, we note that the interpolation coefficient in (7) can also be computed using (4) in a document dependent yet query independent fashion. Several observations can be made from this approach. First, the adaptation coefficient of (4) is document dependent as desired. Unlike the Dirichlet smoothing used in [32] that can also yield document dependent estimation of Di, CALM achieves this without having to make a strong assumption that the family of the prior distributions is conjugate to the multinomial distribution. The estimation is fully automatable in that it does not leave us with a free parameter that can vary and has to be empirically determined from applications to applications. CALM can therefore be implemented at the index time not only in a batch mode but also in an online fashion that model adaptation takes place as soon as the document enters the collection (e.g. crawled from the Web). Secondly, since CALM uses a linear interpolation method, the "IDF effect" pointed out by Zhai and Lafferty [32] to explain why LM performs well for IR as the traditional TF/IDF approach also applies to CALM. Third, we note that the computation of (4) is light weight. Its complexity grows only linearly with the unique terms in the observation.

5. MIXTURE LANGUAGE MODEL
The mixture weights for the component LMs play a central role in the multi-style LM approach. As is in the previous section, we
note that we can apply CALM adaptation formula to compute the weights of the multi-component mixture (2) by first re-arranging the distribution as two-component mixture:

 PD  w0PD0  (1 w0 ) w'i PDi

(8)

i0

As (4) can be applied to obtain w0, the process can be recursively repeated to obtain other coefficients. Since the goal of the document LM is to evaluate queries, one would like the model to be close to the query language. Accordingly, it seems appropriate to choose the query stream as D0 in (8) so that the CALM formula functions as adapting other mixture components to the query. This method leads to document-dependent mixture weights, leaves no parameter to tune, and is enticing in terms of engineering Web scale IR because the mixture coefficients can be pre-computed when the documents are being indexed.

The query independent nature of the mixture weights, however, is not as intellectually satisfying as the query dependent ones. While the perplexity studies suggest the average closeness of web document streams to the queries, we observe that the styles of individual queries vary dramatically: As some queries can benefit from large weights on the anchor text or the user query streams, it is not the case for others, especially those whose target documents are new and yet to be widely linked to or sought after with search engines. Indeed, our pilot studies suggest that query dependent weights outperform query independent ones and thus the latter results are omitted in this paper.

The query dependent portion of the ranking function is the query

likelihood

in (2). The objective of choosing the optimal

mixture weights is to maximize this likelihood. As shown in (7),

each mixture component itself is a two-component mixture that

has parameters to be determined. We can obtain the re-estimation

formula under Expectation Maximization (EM) algorithm as

1

|| 

(9)

and

1

,

||

(10)

6. WEB SEARCH EXPERIMENTS
To assess the effectiveness of the proposed tuning-free methods for the web document retrieval, we conduct the experiments on the same query set generating the Web test collection previously described by Svore and Burges [28]. The collection consists of 11,845 distinct queries and a retrieval base of more than 1.2 million documents with 5-scale relevance judgments that can be used to compute NDCG as the metric for the retrieval function. The data set has the following five streams associated with each web document: document text body (B), title (T), URL (U), anchor text (A), and the user queries (C) that have one or more clicks on the document recorded in the search engine logs. The percentages of documents with non-empty streams are as shown in Table 1.

Table 1: Portions of documents in the test collection with nonempty text body (B), title (T), URL (U), anchor text (A), and
user queries (C)

B

T

U

A

C

81.46%

74.21%

74.21%

75.79%

37.76%

Documents that contain only graphic contents, for example, will be regarded as having empty text body. Since the user query stream is sparsely populated, we exclude it from the study in this paper.
The test collection, having been studied by multiple institutions, comes with a few well-established retrieval results. In the following, we report two pertinent experimental data as baselines for comparison. The first is based on Okapi BM25 [25] and its multifield extension [26] (referred to as BM25F below), both of which parameters are taken from the published results in [28]. We note that neither set of the Okapi experiment takes into account the document prior, which we have found to be critical in downplaying the roles of undesirable contents such as spam. Similar observations on the importance of document prior have been made for other applications [15]. As such, we adopt the machine learning technique described in [28] and train a neural network ranker based system that uses NDCG@10 as the objective function to combine BM25/F with the document prior, and its results are reported based on leave-one-out cross validation on the test collection below. The results, shown in the "mean% (standard deviation %)" format, are labeled as "Oracle 1" since the machine learning is conducted on the test collection with all the relevance judgments.
There is no reason to believe the results reported here cannot be reproduced elsewhere, such as the recent TREC Web Track data set [4], provided that the document prior can be computed with methods that effectively confront the prevalent spamming activi-

471

ties on the Web. Specifically, the test collection used in this paper includes a technique described in [30] that identifies spammers based on the HTTP redirection patterns. We have found such crawling time features critical and can augment other link graph analysis and content based methods and lead to an effective prior estimation that makes the IR metrics more meaningful.

6.1 Single Style LM
We first conduct a series of single style LM experiments to understand the merits of the adaptive LM (Sec. 4) against the wellknown Dirichlet smoothing based LM. To be precise, the single style LM here means that the document is represented by a single stream.

The rationale for the experimental design is as follows. Aside from the open-vocabulary, which does not play a role for single style LM in the IR tasks, the "tuning-free" method uses the same form, i.e., linear interpolation, to smooth the LM. The novelty here is in the manner of how the interpolation coefficients, which can be interpreted as the prior of the distributions to be interpolated (Sec. 4.1), are chosen. The Dirichlet approach makes the assumption of conjugate prior, which is only contingent upon the distribution family of the LM and not on the empirical observations. Accordingly, the Dirichlet smoothing leaves a free parameter that has to be empirically tuned based on the application data. In contrast, the adaptive LM makes no assumption on the distribution family of the prior. Rather, it capitalizes on the data observed in the document to derive an analytical yet data-driven estimate of the prior, thereby achieving the objective of no free parameters.

Table 2: NDCG for single stream retrieval experiments

NDCG@1

NDCG@3

NDCG@10

Baseline: BM25

B 26.72 T 26.46

30.19 29.64

37.77 36.24

U 29.77

31.68

37.40

A 33.59

35.90

41.78

CALM

B 28.74

32.02

39.09

T 33.95

36.33

42.19

U 36.81

38.06

43.19

A 35.42

37.50

43.03

EM

B 28.87

32.23

39.30

T 33.85

36.41

42.52

U 36.80

38.03

43.09

A 36.13

38.44

44.20

Oracle 1:

B 27.87 (0.58) 30.98 (0.55) 38.48 (0.40)

BM25 +

T 30.45 (0.18) 33.59 (0.21) 40.44 (0.28)

P(D) w/ML U 34.66 (0.14) 35.98 (0.08) 41.99 (0.18)

A 37.37 (0.25) 38.76 (0.30) 44.14 (0.36)

Oracle 2:

B 29.37 (0.18) 32.47 (0.11) 39.43 (0.17)

Grid-search T 32.05 (0.58) 34.84 (0.54) 41.38 (0.41)

Dirichlet

U

Smoothing A

33.67 (0.96) 37.62 (0.46)

35.54 (0.71) 39.26 (0.30)

41.46 (0.46) 44.56 (0.16)

Table 2 shows the experimental results with an emphasis to understand the parameter tuning effects. The experimental condition labeled "CALM" implements (7) for smoothing, whereas the experiments labeled "EM" utilize the EM algorithm to find the interpolation coefficient that maximizes the query likelihood for

each individual query. As previously described, CALM is an approach where all the parameters can be computed at the document index time, while EM has to be carried out in retrieval time. Even though parameters maximizing query likelihood do not necessarily improve NDCG scores, it appears to be the case between the CALM and the EM cases. We note that, even though the CALM method does not further utilize query specific information for smoothing, its performance has already come close to the "EM" method. Both LM approaches record higher NDCG scores than the baseline (all results are statistically significant based on t-test with significance level of 0.05), and come to the high-end performance of the Oracle 1 that utilizes more data to train the parameters. The closeness to the Oracle 1 result is surprisingly encouraging because all the LM methods optimize only the indirect measures of query likelihood, whereas in all Oracle 1 cases NDCG@10 is directly optimized on the test collection.
To further understand the parameter tuning, we run a grid search on the free parameter in Dirichlet smoothing (from 50 to 500 with a step size 50) and tabulate the corresponding retrieval results in Table 2 labeled as "Oracle 2" in the "mean% (standard deviation%)" format. The results confirm that the choices of free parameters can introduce significant variances in NDCG, and that the EM method can produce reasonable results without tuning.
In all cases, the retrieval experiments lend support to the analysis in Sec. 2 that streams other than the text body tend to be a better choice for IR tasks, and their relative efficacy seems to track the perplexity prediction well. For example, anchor text is consistently outperforms the title and the body streams across all experimental conditions.
6.2 Multi-Style LM
Table 3 summarizes that experiments that test to what extent multiple streams can be combined to improve retrieval performance. The "CALM + EM" condition uses the interpolation coefficients for individual streams determined at the index time in the same manner as described in Sec. 6.1, and uses the EM algorithm to compute the mixture weights at the retrieval time when the query is received. In comparison, the "Joint EM" condition uses the EM algorithm to jointly determine the mixture weights and the interpolation coefficients for all the stream at the retrieval time in the manner described in (10) of Sec. 5. As is in the case for the single style LM, the total retrieval time approach "Joint EM" seems to offer consistent better performance than the partially index time method CALM+EM. Both LM methods produce reasonable performance, even though they do not utilize any judgment data and only are indirectly optimized for query likelihood rather directly on NDCG. Regardless the modeling techniques, all experimental conditions consistently show that better retrieval performance can be achieved when more streams are included in the retrieval model.
The motivation behind the emphasis on "tuning free" is based on our empirical observation that many retrieval methods typically yield dramatically unstable performance, the root cause of which can be traced to their sensitivity to the free parameters in the models. As mixture models increase the number of model parameters, the robustness issue is inevitably exacerbated. We demonstrate the sensitivity issues by including an experimental condition "Oracle 0" in which we retrain the neural net on the test collection to obtain the optimal BM25F parameters. As can be seen, the NDCG metrics change dramatically from the baseline where the BM25F parameters were trained on a separate dataset that is created using the same pooling methodology and judgments guidelines but with the collection harvested from the Web 2 months earlier. More

472

troublingly, such a dramatic swing in performance metric cannot be discovered through cross validation, as the standard deviations in both Oracle 0 and Oracle 1 appear small. Our investigation confirms that single style stream experiments do not exhibit such a big gap in BM25 performance. This leads to our working hypothesis that the combinations of multiple text streams introduce the new performance robustness challenges, a topic that warrants more research in the future.

Table 3: NDCG for mixture LM for retrieval

NDCG@1

NDCG@3

NDCG@10

Baseline: BM25F

BT BTU

26.36 32.15

32.29 35.12

39.36 42.23

BTUA 36.02

38.34

45.05

CALM +

BT

33.90

36.42

42.77

EM

BTU

34.90

37.69

44.16

BTUA 36.46

39.39

45.77

Joint EM

BT

33.92

36.57

42.86

BTU

35.11

37.82

44.29

BTUA 36.98

39.71

46.04

Oracle 0:

BT

30.27 (0.13) 33.48 (0.10) 40.75 (0.16)

BM25F ML BTU

retuned

BTUA

34.85 (0.62) 43.84 (0.17)

37.07 (0.49) 43.47 (0.05)

43.67 (0.42) 48.21 (0.13)

Oracle 1:

BT

33.01 (0.11) 35.91 (0.30) 42.74 (0.37)

BM25F +

BTU

34.85 (0.62) 37.07 (0.49) 44.82 (1.03)

P(D)

BTUA 45.21 (0.28) 44.51 (0.36) 48.92 (0.52)

7. SUMMARY
The key question of using LM for IR is how to create a LM for each document that best models the queries used to retrieve the document. Studying the textual resources with the documents, we first present convincing and quantitative evidence that different language styles are used for composing the document body, title, anchor text, and queries. As such, these different styles are better separately modeled and then combined to form the document language model.
The immediate question is how LMs with different vocabulary sets can be combined in a principled way. Previous attempts to this so-called open-vocabulary LM problem resorts to heuristics many of which are hard to verify. The most famous and widely used, the Good-Turing formula, is recognized as enigmatic and unintuitive. We propose an alternative based on rigorous mathematical derivations with few assumptions. The same mathematical framework, based on LM adaptation, also suggests that once the open-vocabulary issue is resolved the model combination can be achieved by simple linear interpolation. Such a simple form allows us to employ the EM algorithm to dynamically compute the query-document matching scores without tuning free parameters. Our experiments show that the proposed approach can produce retrieval performance close to the high-end oracle results.
8. ACKNOWLEDGMENT
The authors would like to thank Chris Thrasher, Paul Hsu, Evelyne Viegas, Fritz Behr, and Zijian Zheng for the collaboration.
9. APPENDIX
The linear interpolation of (3) indicates the adapted distribution PT is a mixture of the ML estimation of the observation data PO and

the background model PB. Note that the probability of an event E is the mixture sum of the event taking place under various conditions Ci weighted by the respective priors P (Ci):
 P(E)  P(E | Ci )P(Ci ) i
We can view the mixture coefficient in (1) as the prior probability of the respective mixture component being the "real" distribution in describing the probabilistic events whose statistical property is characterized by PT. In the case of adaptation, the probability of the background being the real distribution can be estimated by computing how effective the background model predicts the observation where token t occurs n(t) times among a total of LO tokens, namely,

 LO 

n(t)
tO

With the assumption that the background model PB being a multinomial distribution, the probability of the observation evaluated against PB is

  PB (O) 

LO ! n(t)!
tO

tO PB (t)n(t)

Equivalently,

  log PB (O)  log LO!  log n(t)!  n(t)log PB (t)

tO

tO

The factorial terms in the above equation can be approximated by the well-known Stirling formula

log n!  nlog n  n

Accordingly, we have

 log PB (O)  LO logLO LO  [n(t) log n(t)  n(t)]

tO

  n(t)log PB (t)

tO

 

LO log

LO



tO

n(t ) log

PB (t) n(t)

 

tO

n(t) log

PB (t) n(t) / LO

Note that the mixture weight is the per-token probability whereas PB (O) above is evaluated over a total of LO tokens. With the statistical independent assumptions of the tokens in the LM, we have

log PB (O)  log(1O )LO  LO log(1O ) which leads to (4).

10. REFERENCES
[1] Berger, A. and Lafferty, J. 1999. Information retrieval as
statistical translation. In Proc. SIGIR-99, 222-229.
[2] Brown, P., della Pietra, S. A., della Pietra, V. J., Lai, J.,
Mercer, R. L. 1992. An estimate of an upper bound for the entropy of English. Computational Linguistics, 18(1), 31-40.
[3] Bulyko, I., Ostendorff, M., Siu, M., Ng, T., Stolcke, A., and
Cetin, O. 2007. Web resources for language modeling in conversational speech recognition. ACM Trans. on Speech and Language Processing, 5(1), December, 2005, 1-25.
[4] Clark, C. L. A., and Craswell, N. 2009. Report on the TREC
2009 Web Track. In Proc. TREC 2009.

473

[5] Collins-Thompson, K. and Callan, J. 2005. Query expansion
using random walk models. In Proc. CIKM' 05, Bremen, Germany, 704-711.
[6] Croft, W. B., Metzler, D., and Strohman, T. 2009. Search
Engines: information retrieval in practice, Addison Wesley.
[7] Duda, R. O., Hart, P. E. 1973. Pattern Classification and
Scene Analysis, Wiley, New York.
[8] Fang, H., Tao, T., Zhai, C. 2004. A formal study of informa-
tion retrieval heuristics. In Proc. SIGIR-04, 49-56.
[9] Gao, J., Nie, J., Wu, G., Cao, G. 2004. Dependence language
model for information retrieval. In Proc. SIGIR-04, 170-177.
[10] Hiemstra, D. and Kraaij, W. 2005. 21 language models at
TREC: A language modeling approach to the text retrieval conference. In TREC: Experimental and Evaluation in Information Retrieval, MIT Press, E. M. Voorhees and D. Harman (eds).
[11] Huang, X. D., Acero, A., and Hon, H.-W. 2001. Spoken Lan-
guage Processing, Prentice Hall PTR, New Jersey.
[12] Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and Behr, F.
2010. Exploring web scale language models for search query processing. In Proc. WWW 2010.
[13] Jaynes, E. T. 1957. Information theory and statistical me-
chanics. In Physical Review Series II, American Physical Society, 106(4), 620-630.
[14] Jin, R., Hauptmann, and Zhai, C. 2002. Title language model
for information retrieval. In Proc. SIGIR-02, 42-48.
[15] Kraaij, W., Westerveld, T., and Hiemstra, D., 2002. The
importance of prior probabilities for entry page search. In Proc. SIGIR'02, Tampere, Finland, 27-32.
[16] Lafferty, J. and Zhai, C. 2001. Document language models,
query models, and risk minimization for information retrieval. In Proc. SIGIR'01, New Orleans, LA, 111-119.
[17] Lavrenko, V., and Croft, W. B. 2001. Relevance-based lan-
guage models. In Proc. SIGIR'01, New Orleans, LA, 120127.
[18] Manning, C., Raghavan, P., and Schutze, H. 2008. Introduc-
tion to information retrieval, Cambridge University Press.
[19] Maron, M, and Kuhns, J. 1960. On relevance, probabilistic
indexing and information retrieval. Journal of ACM, 7, 216244.
[20] Microsoft web n-gram services.
http://research.microsoft.com/web-ngram

[21] Miller, D., Leek, T., Schwartz, R. M. 1999. A hidden Mar-
kov model information retrieval system. In Proc. SIGIR-99, 214-222.
[22] Ogilvie, P. and Callan, J. 2003. Combining document repre-
sentations for known item search. In Proc. SIGIR-03, 143151.
[23] Orlitsky, A., Santhanam, N. P., and Zhang, J. 2003. Always
Good Turing: asymptotically optimal probability estimation. Science, 302(5644), 427-431.
[24] Ponte, J., and W. B. Croft. 1998. A language model approach
to information retrieval. In Proc. SIGIR-98, 275-281.
[25] Robertson, S. E., Walker, S., Sparck-Jones, K. S., Hancock-
Beaulieu, M. M., and Gatford, M. 1994. Okapi at TREC-3. In Proc. the third text retrieval conference (TREC-3), D. K. Harman (eds.), NIST special publication 500-225, Gaithersburg, MD, 109-126.
[26] Robertson, S. E., Zaragoza, H., and Taylor, M. 2004. Simple
BM25 extension to multiple weighted fields. In Proc. CIKM2004, 42-49.
[27] Stolcke, A. 1998. Entropy-based pruning of backoff language
models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, 270-274.
[28] Svore, K. M. and Burges, C. J. C. 2009. A machine learning
approach for improved BM25 retrieval. In Proc. CIKM'09, Hong Kong, China.
[29] Wang, K. and Li, X. 2009. Efficacy of a constantly adaptive
language model technique for web-scale applications. In Proc. ICASSP-2009, Taipei, Taiwan, 4733-4736.
[30] Wang, Y.-M., Ma, M., Niu, Y., and Chen, H. 2007. Spam
double-funnel: connecting web spammers with advertisers. In Proc. WWW-2007, 291-300.
[31] Zhai, C. 2008. Statistical language models for information
retrieval: a critical review. Foundations and Trends in Information Retrieval, Vol. 2(3), 137-215.
[32] Zhai, C. and Lafferty, J. 2001. A study of smoothing me-
thods for language models applied to ad hoc information retrieval. In Proc. SIGIR'01, New Orleans, LA, 334-342.
[33] Zhai, C., and Lafferty, J. 2002. Two-stage language models
for information retrieval. In Proc. SIGIR'02, Tampere, Finland, 49-56.

474

Combining Coregularization and Consensus-based Self-Training for Multilingual Text Categorization

Massih-Reza Amini

Cyril Goutte

Nicolas Usunier

National Research Council Canada Institute for Information Technology 283, boulevard Alexandre-Taché
Gatineau, J8X 3X7, Canada First.Last@nrc-cnrc.gc.ca

Université Pierre et Marie Curie Laboratoire d'Informatique de Paris 6 104, avenue du Président Kennedy
75016 Paris, France Nicolas.Usunier@lip6.fr

ABSTRACT
We investigate the problem of learning document classifiers in a multilingual setting, from collections where labels are only partially available. We address this problem in the framework of multiview learning, where different languages correspond to different views of the same document, combined with semi-supervised learning in order to benefit from unlabeled documents. We rely on two techniques, coregularization and consensus-based self-training, that combine multiview and semi-supervised learning in different ways. Our approach trains different monolingual classifiers on each of the views, such that the classifiers' decisions over a set of unlabeled examples are in agreement as much as possible, and iteratively labels new examples from another unlabeled training set based on a consensus across language-specific classifiers. We derive a boosting-based training algorithm for this task, and analyze the impact of the number of views on the semi-supervised learning results on a multilingual extension of the Reuters RCV1/RCV2 corpus using five different languages. Our experiments show that coregularization and consensus-based self-training are complementary and that their combination is especially effective in the interesting and very common situation where there are few views (languages) and few labeled documents available.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information Storage - Record classification; I.2 [Artificial Intelligence]: Learning
General Terms
Algorithms, Experimentation, Theory
Keywords
Multilingual Document Classification, Learning from Multiple Views, Semi-supervised Learning
Copyright 2010 Crown in Right of Canada. This article was authored by employees of the National Research Council of Canada. As such, the Canadian Government retains all interest in the copyright to this work and grants to ACM a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, provided that clear attribution is given both to the NRC and the authors. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

1. INTRODUCTION
In this paper, we address the problem of semi-supervised learning of document classifiers in a multilingual setting where documents are available as a parallel corpus with two or more languages for which labels are only partially available.
Our motivation is that multilingual collections are becoming more and more common in national and supranational contexts. However, the bulk of document classification and organization techniques and research is developed in the monolingual setting, most often for English. In addition, labeling text documents may require cost- and time-intensive human annotation, hence the widespread interest for semisupervised text classification approaches that leverage unlabeled documents to speed-up the learning process.
Our work addresses the two issues of limited annotation and multilingual setting. Using the different languages as different views on a document, we develop a multiview, semisupervised approach that learns from collection of multilingual documents.
We formalize the problem as follows. Given a collection of partially-labeled documents written in different languages and belonging to a set of classes that is fixed across languages, we wish to learn a number of monolingual classifiers for this common set of classes. Note that this problem is different from cross-language text categorization [5], where a document written in one language must be classified in a category system learned in another language.
In our setting, we assume that each document is available in several languages and we are interested in learning improved monolingual classifiers. We also emphasize that we wish to develop inter-dependent monolingual classifiers, rather than a single multilingual classifier, as we wish to be able to classify an incoming document in whatever language it is made available, without having to translate it beforehand.
There have been at least two approaches to multiview semi-supervised learning. One can use coregularization [19] to improve the view-specific classifiers by constraining them to agree on some unlabeled data, leveraging unlabeled data in a multiview learning framework. A more recent proposal [3], by contrast, leverages the multiple views in a semisupervised learning framework by using the consensus between the different views in a self-training framework. Our solution is to combine those two components into a single boosting-based algorithm. View-specific classifiers are trained using coregularization, and a consensus-based self-

475

training process iteratively labels unlabeled examples on which the view-specific classifiers agree.
Using a large publicly available corpus of multilingual documents extracted from the Reuters RCV1 and RCV2 corpora, we show that our approach consistently improves over both coregularization and self-training taken in isolation. We also analyze the conditions in which the combination is most profitable. It turns out that adding coregularization to consensus-based self-training helps most when there are few languages and few documents available. This is a particularly interesting setting when resources are limited, and corresponds in particular to the common situation of bilingual data.
In the next section, we position our work with respect to the state of the art. In Section 3, we then present the problem of multiview semi-supervised learning for multilingual text classification. Section 4 describes the boosting-based algorithm we developed to obtain the language-specific classifiers. In Section 5, we present experimental results obtained with our approach on a subcollection of the Reuters RCV1/RCV2 corpus. Finally, in Section 6 we discuss the outcomes of this study and give some pointers to further research.
2. RELATION TO STATE-OF-THE-ART
Document classification has been a very popular application domain for Machine Learning algorithms, and in particular for multiview [7] and semi-supervised learning [16, 12]. The setting of multilingual document classification, however, has been much less studied so far [1, 2].
Interestingly, the original work on co-training [7] introduced both multiview and semi-supervised learning on a document classification task. Since then, both fields have developed greatly but mostly independently. Semi-supervised learning approaches include generative approaches, densitybased or graph-based approaches (cf. [9] for an overview). Multiview learning techniques include multiple kernel learning [4] and techniques relying on kernel Canonical Correlation Analysis [11].
Some recent work more in line with the original co-training approach have introduced coregularization [19, 8], where classifiers are learnt in each view using a multiview regularizer that constrains predictions made in each view to be as similar as possible.
When this multiview regularizer is computed on unlabeled data, this provides a way to perform semi-supervised learning in a multiview setting. More recently, a semi-supervised multiview approach has been developed [3] where classifiers are learned on each view using standard single view training, but unlabeled examples are iteratively labeled in a selftraining manner using the consensus across the views. The multiview consensus ensures higher confidence in the labeling, which yields improved semi-supervised learning rates.
Our work analyses and illustrates the combination of these two techniques. We use a coregularization component similar to [19, 8], with the key difference that instead of the coregularized least squares, we penalize disagreement using a Kullback-Leibler divergence which has a more natural interpretation in the context of probabilistic classifier outputs. In addition, it allows us to develop a novel boosting-based algorithm for solving the coregularized multilingual classification problem.

We combine this coregularized learning with a consensusbased self-training framework similar to [3] where unlabeled documents are iteratively labeled using the consensus prediction across the multiple views.
As both coregularization and consensus-based self-training use multiview information and unlabeled data for training, the key question we address is to see whether the two techniques can be complementary and improve on each other, as opposed to being completely redundant. We also investigate in which conditions such a complementarity may be exploited. We are particularly interested in the effects of coregularization in the common situation where the number of views is small (eg bilingual documents) and few labeled data are available.

3. FRAMEWORK
We consider V input spaces Xv  Rdv ; v  {1, .., V }, and an output space Y. We take Y = {-1, +1} since we restrict our presentation to binary classification. Each multiview document x  X1 × ... × XV is a sequence
x =def (x1, ..., xV )
where each view xv provides a representation of the same document in a different vector space Xv. In the seminal work on co-training [7], web pages are represented by either their textual content (first view) or anchor text pointing to them (second view). In our setting of multilingual classification, each view is the textual representation in a different language. Although typically one of the views is the original version of the document and the others are its translations, we never rely on this information and treat all views equally. Note that in this framework all views of each document are present simultaneously, hence we deal with multilingual text classification in a parallel corpus.
We further assume that we have a labeled training set Z = {(xi, yi)|i  {1, .., l}} and a possibly much larger set of unlabeled training data that we split into two parts denoted respectively by XU1 = {xl+i|i  {1, .., m1}} and XU2 = {xl+m1+i|i  {1, .., m2}}. Our goal is to obtain V binary classifiers {hv : Xv  {-1, 1}|v  {1, .., V }}, working each on one view, such that the predictive performance as estimated for example from a test set is optimized. Note that by construction, the label for a given document is the same for all views.

4. MODEL
We iteratively learn each classifier hv, v  V, while keeping fixed the classifiers for the other views, hu, u  V u = v, by optimizing the loss

L(hv, Z

, XU1 , )

=

C(hv, Z

)

+

V

 -1

X V

d(hv, hu, XU1 ),

u=1,u=v

(1)

where C(hv, Z ) is the (monolingual) cost of hv on the labeled training set Z , d(hv, hu, XU1 ) measures the divergence

between the two classifiers hv and hu on the unlabelled documents in XU1 , and  is a discount factor which modulates

the influence of the disagreement cost on the optimization.

476

For the monolingual cost, we consider the standard misclassification error:

C(hv, Z

)

=

1 l

Xl [[yihv(xvi )



0]],

i=1

where [[]] is equal to 1 if the predicate  is true, and 0 otherwise. As this cost is non-continuous and non-differentiable,
it is typically replaced by an appropriate convex and dif-
ferentiable proxy. Following standard practice in Machine Learning algorithms, we replace [[z  0]] by the upper bound a log(1+ e-z), with a = (log 2)-1. The monolingual misclassification cost becomes:

C(hv, Z

)

=

1 l

Xl a log(1 + exp(-yihv(xvi ))),
i=1

Assuming that each classifier output may be turned into
a posterior class probability, we measure the disagreement
between the output distributions for each view using the
Kullback-Leibler (KL) divergence. Using the sigmoid function (z) = (1 + e-z)-1 to map the real-valued outputs of the functions hv and hu into a probability, and assuming that the reference distribution is the output of the classifier learned on the other views, hu, u  {1, ..., V }  u = v, the disagreement d(hv, hu, XU1 ) becomes

d(hv, hu, XU1 )

=

1 m1

X m1 kl( (hu (xul+i))|| (hv (xvl+i ))),
i=1

where for two binary probabilities p and q, the KL diver-

gence is defined as:

,,«

,,

«

kl(p||q) = p log

p q

+ (1 - p) log

1-p 1-q

There are two reasons for choosing the KL divergence: first, it is the natural equivalent in the classification context of the l2 norm used for regression in previous work on coregularization [19, 8, 18]; second, it allows the derivation of a boosting approach for minimizing the local objective function (1), as described in the following section.

4.1 A view-specific boosting-like algorithm
In order to learn the classifier hv for view v, we need to minimize

L(hv, Z

, XU1 , )

=

1 l

Xl a log(1 + exp(-yihv(xvi )))

i=1

+ (V

 - 1)m1

X m1
i=1

X V
u=1,u=v

kl(

(hu

(xul+i))||

(hv

(xvl+i

)))

(2)

We show how the loss-minimization of (2) is equivalent to
the minimization of a Bregman distance. This equivalence
will allow us to employ the boosting-like parallel-update op-
timization algorithm proposed by [10] to learn a linear classifier hv : xv  v, xv minimizing (2).
A Bregman distance BF of a convex, continuously differentiable function F :   R on a set of closed convex set  is defined as

p, q  , BF (p||q) =def F (p) - F (q) - F (q), (p - q) .

One optimization problem arising from a Bregman distance is to find a vector p  , closest to a given vector

q0   with respect to BF , under the set of linear constraints {p  |ptMv = p~tMv}, where p~   is a specified vector and Mv is a n × d matrix, with n the number of examples in the training set and d the dimension of the problem.1
Defining the Legendre transform as

Lf (q, Mv v) =def argmin(BF (p||q) + Mvv , p ), p

the dual optimization problem can be stated as finding a vector q in the closure Q¯ of the set Q = {LF (q, Mvv)|  Rp}, for which BF (p~||q) is the lowest, under the set of linear constraints {q  |qtMv = p~tMv}.
It has been shown that both of these optimization prob-
lems have the same unique solution [14]. Moreover, [10] have
proposed a single parallel-update optimization algorithm to
find this solution in the dual form.
They have further shown that their algorithm is a general
procedure for solving problems which aim to minimize the
exponential loss, like in Adaboost, or a log-likelihood loss,
like in logistic regression. Indeed, they showed the equiv-
alence of these two loss minimization problems in terms of
Bregman distance optimization.
In order to apply the boosting algorithm proposed by [10], we have to define a continuously differentiable function F such that by properly setting , p~, q0 and Mv, the Bregman distance BF (0||LF (q0, Mvv)) is equal to Eq. (2). Following [10], we choose:

X n p   = [0, 1]n, F (p) = vi (pi log pi + (1-pi) log(1-pi)) ,
i=1

where vi are non-negative real-valued weights associated to examples xvi .
This definition yields that p, q   × :

BF

(p||q)

=

X n
i=1

vi

,, pi

log

,,

pi qi

«

+

(1-pi)

log

,,

1-pi 1-qi

««

(3)

and,

i, LF (q, z)i

=

qi

-
e

zvii

1

-

qi

+

qi

-
e

zvii

(4)

Using Equations

(3)

and (4),

and

setting

q0

=

1 2

1,

the

vector

with

all

components

set

to

1 2

,

and

Mv

the

matrix

such that i, j, (Mv)ij = vi yixvij,2 the Bregman distance in

Equation (3) writes:

X n BF (0||LF (q0, Mvv )) = vi log(1 + e-yi

v ,xvi

).

(5)

i=1

1We have deliberately set the number of examples to n as in our equivalent rewriting of the minimization problem the
latter is not exactly m1. 2All vectors i  {1, .., n}, iyixvi should be normalized in order to respect the constraint Mv  [-1, 1]n×d.

477

Algorithm 1: Parallel-update optimization algorithm

Input : Matrix v, Mv  [-1, 1]n×d .

Initialize: Let v, v  0

for v = 1, ..., V do

for t = 1, 2, ... do

Let q(t) be the solution of LF (q0, Mvv(t));

for j = 1, ..., d do

Wv(,tj)+ Wv(,tj)-

 

P Pi:sign((Mv )ij
! i:sign((Mv )ij

)=+1 )=-1

qi(t)|(Mv qi(t)|(Mv

)ij )ij

|; |;

v(t,)j



1 2

log

Wv(t,j)+ Wv(t,j)-

;

end

v(t+1)  v(t) + v(t);

end

end Output : v, the sequence v(1), v(2), ... verifying

lim
t

BF

(0||LF

(q0,

Mv v(t) ))

=

inf
v Rd

BF

(0||LF

(q0,

Mvv ))

Algorithm 2: Coregularized semi-supervised Learning
Input : A set of labeled training examples Z ; Two sets of unlabeled training data XU1 and XU2 ; Initialize: Set ZU\  ; v, h(v0) =def argminh C(h, Z ); repeat
t  1;
repeat
for v = 1, .., V do Learn h(vt) = argminh L(h, Z  ZU\, XU1 , );
end
t  t + 1; until Convergence of (Vv=1h(vt), Z  ZU\, ) ; - Let XU\ be the set of unlabeled examples in XU2 on which all classifiers agree over the class label of
examples ; -XU2  XU2 XU\ ; -ZU\  ZU\  XU\ ; until XU2 =  or XU\ =  ; Output : Classifiers hv, v  {1, ..., V }

By developing Eq. (2), we get:

L(hv, Z

, XU1 , )

=

K

+

1 l

Xl a log(1

+

exp(-yihv (xvi )))

+

i=1

(V

 - 1)m1

X m1
i=1

X V
u=1,u=v

(hu(xul+i))

log(1

+

e-hv

) (xvl+i )

+

(V

 - 1)m1

X m1
i=1

X V (1
u=1,u=v

-

(hu(xul+i)))

log(1

+

ehv

) (xvl+i )

(6)

where K is a constant which does not depend on hv.

In order to make Eq. (6) identical to Eq. (5) (up to a constant), we create, for each unlabeled document xvi  XU1 , two examples (xvi , +1) and (xvi , -1) (which makes n = l +
2m1), and set the weights as follows:

8 >><

a l

if xi  Z ,

vi = >>: (V -1)m1u=X 1V,u=[v[yi = -1]]+ yi(hu(xui )) else.

(7)

As a consequence, minimizing Eq. (2) is equivalent to minimizing BF (0||q) over q  Q¯, where

Q = {q  [0, 1]l+2m1 | qi = (yi v , xvi ), v  Rdv }.

optimize each of the hv classifiers while keeping the classifiers for the other views fixed, until the global objective
X V (Vv=1hv, Z  ZU\, ) = L(hv, Z  ZU\, XU1 , ) (8)
v=1
has reached a (possibly local) minimum. This alternating optimization of partial cost functions bears
similarity with the block-coordinate descent technique [6]. At each iteration, block coordinate descent splits variables into different subsets, the set of the active variables and the sets of inactive ones, then minimizes the objective function along active dimensions while inactive variables are fixed at current values.
Once all language-specific classifiers have been trained we assign class labels to unlabeled examples in XU2 for which all mono-lingual classifiers predict the same class label. These newly labeled examples are added to the labeled training set. We then go back to the boosting-based coregularized classifier training using the combined labeled data, and so on until either no remaining unlabeled example can be labeled by consensus, or all unlabeled examples have been labeled. As shown by [3], focusing on functions which agree across several views reduces the complexity of the function class and therefore improves the prediction ability of the resulting model.
Algorithm 2 summarizes this coregularized self-training strategy.

This equivalence allows us to adapt the parallel-update optimization algorithm described in [10] to learn each specificview classifier, as described in Algorithm 1.
4.2 Coregularized semi-supervised algorithm
We embed the boosting-based coregularized classifier learning inside a self-training framework (cf. [22], Section 3) which relies on consensus across views in order to automatically label documents from an unlabeled document pool XU2 .
Each monolingual classifier hv, v  V is first initialized on the supervised monolingual cost alone, then we iteratively

5. EXPERIMENTS
We conducted a number of experiments aimed at evaluating how the combination of coregularization and consensusbased self-training can help to take advantage of multilingual unlabeled documents in order to learn efficient classification functions.
5.1 Data set
We perform experiments on a publicly available multilingual multiview text categorization corpus extracted from

478

Language English French German Italian Spanish Total

# docs 18,758 26,648 29,953 24,039 12,342 111,740

Class C15 CCAT E21 ECAT GCAT M11

# docs 18,816 21,426 13,701 19,198 19,178 19,421

Table 1: Number of documents per language (left) and per class (right) in Reuters RCV1/RCV2 subcollection used in our experiments.

the Reuters RCV1/RCV2 corpus [3].3 This corpus contains more than 110K documents from 5 different languages, (English, German, French, Italian, Spanish), distributed over 6 classes (Table 1). Documents that originally had more than one of these 6 labels were assigned to the smallest class. We reserved a test split containing 25% of the documents, respecting class and language proportions. Within the training set containing the remaining 75% of documents, we randomly sampled labeled documents (Z ), and split the remaining unlabeled data into two subsets: one for evaluating the coregularization term (XU1 ), and one for the self-training process (XU2 ). The motivation for that split is to avoid bias: as coregularization enforces agreement between classifiers, it may yield artificially high consensus for the examples used in the coregularization term.
This corpus of multilingual documents is originally a comparable corpus as it covers the same subset of topics in all languages. In order to produce multiple views for each documents, each original document extracted from the Reuters corpus was translated in all other languages using a phrasebased statistical machine translation system [20]. The indexed translations are part of the corpus distribution.
More precisely, each document is indexed by the text appearing in its title (headline tag) and body (body tag). As preprocessing, all text is lowercased, digits are mapped to a single digit token, and tokens containing non-alphanumeric characters are removed. For each language, words in a stoplist as well as tokens occurring in less than 5 documents were also filtered out. Documents were then represented as a bag of words, using a TFIDF weighting scheme based on BM25 [17].
Results are evaluated over the test set using the accuracy and the standard F1 measure [21], which is the harmonic average of precision and recall. The reported performance is averaged over the resulting five language-specific classifiers. In addition, we also averaged over 10 random (train/unlabeled/test) sets of the initial collection.
5.2 Experimental setup
To validate the coregularized consensus-based self-training approach described in the previous section, we test the following six classification methods. The first method is a purely supervised technique which does not make use of any unlabeled examples in the training stage. The following methods make use of the multiview and semi-supervised learning approaches in different ways, using coregularization and/or consensus-based self-training separately or in
3http://multilingreuters.iit.nrc.ca/

combination, over different subsets of the unlabeled training documents.
- Baseline method [Boost]: This baseline corresponds to a supervised monolingual boosting model optimizing Eq. 2 for  = 0.
- Coregularized boosting [reg-Boost]: Boosting using coregularization on XU1 , optimizing Eq. 2 for  = 0. This constrains the supervised monolingual boosting models to achieve high agreement among their predictions on XU1 .
- Boosting with self-training [Boost-cst]: Boosting using consensus-based self-training, but no coregularization. This is similar in spirit to the iterative co-training algorithm [7]. Given the language-specific classifiers trained on an initial set of labeled examples, we iteratively assign pseudo-labels to the unlabeled examples in XU2 for which all classifier predictions agree.

- SVM with self-training [SVM-cst]: This is simi-

lar to the previous method except that we use the

SVM-Perf package [13] to learn each language-specific

classifiers instead of boosting. For tuning the hyper-

parameter C, we first tried the leave-one-out cross-

validation we found

strategy. out that

tHheowdeevfearu,ltw(it1lhPsmli=a1ll||txria|i|n)-in1g

sets gave

similar, and in some cases, better results. We there-

fore used that default C in all of our experiments.

- Coregularization+self-training [reg-Boost-cst]: Coregularized boosting using the consensus-based selftraining: The coregularization term is computed over XU1 and self-training iteratively labels documents from XU2 .
- Boosting with full self-training [Boost-cst]: In order to determine when the combination of coregularization and self-training is the most useful, we also trained algorithm Boost-cst using all the unlabeled training examples XU = XU1  XU2 rather than just those in XU2 .
Our aim is to show the gradual effect of each of the multiview and semi-supervised learning approaches on the boosting algorithm, progressing from Boost to reg-Boost and Boost-cst, to reg-Boost-cst. Note that the reg-Boost and Boost-cst algorithms use the two separate unlabeled training subsets in different manners. SVM-cst is the same as Boost-cst using a SVM algorithm instead of Boosting. This will allow us to benchmark the boosting-based algorithm against the state of the art SVM model in a similar framework. Note that adding co-regularization in a SVM implementation requires some significant changes to the underlying code, which is why we do not provide reg-SVM variants. Finally, using all the unlabeled training examples in Boost-cst and comparing the results to reg-Boost-cst will allow us to uncover the situations in which it is beneficial to combine coregularization and self-training rather than use the latter alone on the combined unlabeled data. This gives an idea of the true benefit brought by coregularization.

479

Table 2: Test classification accuracy and F1 of different learning algorithms on the six classes, averaged over 10 random sets of 50 labeled examples per training set. For each class, the best result is in bold, and a  indicates a result that is statistically significantly worse than the best, according to a Wilcoxon rank sum test with p < .01.

Strategy
Boost reg-Boost Boost-cst SVM-cst reg-Boost-cst

C15

Acc.

F1

0.771 0.506 0.793 0.532 0.804 0.572

0.815 0.583

0.823 0.595

CCAT
Acc. F1
0.662 0.398 0.689 0.419 0.708 0.421 0.720 0.438
0.748 0.449

E21
Acc. F1
0.765 0.323 0.783 0.342 0.794 0.365 0.800 0.378
0.815 0.394

ECAT
Acc. F1
0.505 0.347 0.513 0.372 0.511 0.384 0.522 0.395
0.542 0.408

GCAT
Acc. F1
0.781 0.587 0.803 0.608 0.866 0.655 0.873 0.662
0.895 0.687

M11
Acc. F1
0.793 0.586 0.815 0.611 0.848 0.668 0.861 0.676
0.883 0.693

5.3 Experimental Results
We start our evaluation by analyzing the gains provided by coregularization, the consensus-based self-training and the combination of both, over the baseline boosting algorithm. We measure the classification accuracy and F1 for a fixed number of labeled and unlabeled examples in the training set. In order to study the role of unlabeled data on the learning behavior we begin our experiments with very few labeled training examples. The size of the labeled training sets in these first experiments is fixed to 50 (an average of 10 per language), with an equal sampling of 25 positive and 25 negative examples in Z . For coregularization, results are reported for the best discount factor  = 1, although as illustrated in Section 5.3.1, results are fairly stable across a wide range of values. We will later investigate the impact on the test performance of the number of labeled examples and the number of views (cf Sections 5.3.3 and 5.3.2).
Table 2 summarizes results obtained by Boost, reg-Boost, Boost-cst, SVM-cst and reg-Boost-cst averaged over five languages and 10 random splits of tests sets for our six main categories. We use bold face to indicate the highest performance rates, and the symbol  indicates that performance is significantly worse than the best result, according to a Wilcoxon rank sum test used at a p-value threshold of 0.01 [15]. From these results it becomes clear that:
1. Using the first part of the unlabeled training examples (XU1 ) to coregularize the boosting algorithm, algorithm reg-Boost always improves over Boost by an average of 2-3 points in F1.
2. The consensus-based self-training framework implemented in Boost-cst and SVM-cst also improves over the baseline. In addition, it always seems to outperform coregularization (reg-Boost) alone. In this self-training framework, the SVM classifiers SVM-cst tend to outperform the boosting-based classifiers Boost-cst.
3. Finally, the combination of coregularization and selftraining (reg-Boost-cst) produces a further improvement of around 1-2 points in F1 over the best semisupervised result (SVM-cst). The improvement is statistically significant in four classes out of six.
Our analysis of these results is that both coregularization and the consensus-based self-training provide consistent improvements over training independent monolingual classifiers. Both are instances of multiview learning, and both

rely in some way on the consensus between classifiers trained on the different views. The question therefore arises as to how redundant these two techniques are? Our experimental results suggest that these techniques are in fact complementary.
The gains provided by adding coregularization to the selftraining boosting-based model is in fact similar to the gain provided by coregularization in the supervised setting, which suggest that the two effects are essentially independent and additive. In order to analyze more finely the situations in which the combination of coregularization and consensusbased self-training is more advantageous, we compared all the algorithms, including Boost-cst, for different numbers of languages and different amounts of labeled documents. These results are reported in Section 5.3.2 and 5.3.3, right after we address the issue of the discount factor .
5.3.1 The effect of the coregularization factor 
We analyze the influence of the discount factor  on the performance of reg-Boost-cst for varying amounts of labeled training data.4 The results obtained on class E21 are presented in Figure 1. Note that  controls the relative importance of the unlabeled data in the coregularization (with  = 0 corresponding to no regularization). Figure 1 shows that unlabeled examples become relatively less important as more labeled data is available: as the amount of labeled training data increases from 50 to 300, the optimal discount factor  moves away from 1.
We recall that for  = 1, unlabeled data plays the same role in the training procedure as labeled data.
Note also that in all cases, the performance of the resulting classifiers seems relatively stable for a wide range of values of . This suggests that the results are not overly sensitive to a precise choice of discount factor .
5.3.2 The value of labeled data
We also analyze the behavior of the various algorithms for growing initial amounts of labeled data in the training set. Figure 2, illustrates this by showing the F1 measures on classes CCAT and ECAT with respect to the number of labeled documents in the initial labeled training set Z . For all labeled data sizes, the proportion of negative/positive examples is maintained at 50%. As expected, all performance curves increase monotonically with respect to the additional
4We always maintain the proportion of positive/negative documents in the labeled training set to 50%/50%.

480

F1 F1

F1

CCAT 0.55

0.5

0.45

0.4

0.35

0.3

Boost-cst* reg-Boost-cst

SVM-cst

0.25

reg-Boost

Boost

10

50 100 200 400 1000

# of labeled documents in the training set

ECAT 0.55

0.5

0.45

0.4

0.35

0.3

Boost-cst* reg-Boost-cst

SVM-cst

0.25

reg-Boost

Boost

10

50 100 200 400 1000

# of labeled documents in the training set

Figure 2: F1 on classes CCAT and ECAT with respect to the number of labeled documents in the initial labeled training set Z .

labeled data. When there are sufficient labeled examples, all algorithms actually converge to the same F1 value, suggesting that the labeled data carries sufficient information and that no additional information could be extracted from unlabeled examples. For a low number of labeled training data, the contribution of each of the algorithms that use unlabeled data is clearly shown. Note that these curves are obtained using five languages, such that the highest performance is achieved by Boost-cst, which is consistent with the findings of the previous section. When fewer views are available, the relative positions of the top algorithms are different, but the effect is similar in that the gains are more important when fewer initial labeled documents are available.
5.3.3 The effect of the number of languages
In our experiments, the unlabeled training set was split in two parts, one for coregularization and one for self-training. Our motivation was to examine the effect of each of the techniques individually without introducing any bias by per-

E21 0.46

0.44

0.42

F1

0.4

0.38

0.36

|||ZZZlll|||===51305000

Maxima

0

0.2

0.4

0.6

0.8

1



Figure 1: F1 with respect to the coregularization factor  for different labeled training sizes on class E21.

forming coregularization and self-training on the same unlabeled data. The previous results suggest that the performance gain is higher when unlabeled examples are iteratively labeled in the self-training framework than when they are used in coregularization to enforce agreement between the language-specific classifiers. The question therefore arises as to what the performance would be if all the unlabeled examples were used in consensus-based self-training rather than being split between coregularization and selftraining? In addition, the consensus is expected to be more reliable when there are many views than when there are few, in which case the language-specific classifiers could agree by chance but erroneously. We therefore investigate the effect of the number of views on the performance of the reg-Boost-cst and Boost-cst algorithms. Figure 3 depicts these results by comparing both algorithms for varying numbers of languages on two classes, E21 and C15. All re-

0.6

0.55

0.5

0.45

0.4

0.35 0.3 2

Boost-cst* reg-Boost-cst
C15 E21

3

4

5

# of languages

Figure 3: F1 with respect to the number of languages used for coregularization and self-training on classes
E21 (solid) and C15 (dash). Comparisons involve
reg-Boost-cst ( ) and the boosting algorithm using the unlabeled examples (XU1  XU2 ) for self-training Boost-cst ( ).

481

sults obtained for less than five languages are averaged over all possible such combinations of languages.
These results show that for five languages, using all the unlabeled data for self-training is slightly more efficient than reserving part of it for coregularization. However, when the number of views is smaller, the combination of both coregularization and consensus-based self-training is more advantageous. Note that this is a common situations, for example when only bilingual documents are available.
This result suggests that in the situation where we have few views, reducing the disagreement between language specific classifiers through coregularization may lead to a more effective use of consensus-based labeling, decreasing the number of noisy examples added to the training set during selftraining. On the other hand, when the number of views is large, the consensus is usually reliable enough without the need for coregularization.
6. CONCLUSION
In this paper we proposed a multiview semi-supervised boosting algorithm for multilingual document classification. We have shown how to embed a disagreement-based coregularization term into a classification objective function using a Bregman distance. This embedding allowed us to adapt an existing boosting algorithm to learn language-specific classifiers while enforcing consistency in prediction across languages. We then proposed a self-training algorithm which assigns class labels to unlabeled data based on the consensus of the classifier predictions across the different views.
Our results show clearly that the consensus based selftraining allows to reach high performance in the situation where few initial labeled training documents are available. We also showed that when there are fewer languages, combining coregularization with the consensus-based self-training approach provides a better leverage of the unlabeled data by improving the quality of the consensus.
Acknowlegdements
This work was supported in part by the IST Program of the European Community, under the PASCAL2 Network of Excellence, IST-2002-506778.
7. REFERENCES
[1] J. J. G. Adeva, R. A. Calvo, and D. L. de Ipin~a. Multilingual Approaches to Text Categorisation. UPGRADE: The European Journal for the Informatics Professional, VI(3):43­51, 2005.
[2] M.-R. Amini and C. Goutte. A Co-classification Approach to Learning from Multilingual Corpora. Machine Learning, 79(1-2):105­121, 2010.
[3] M.-R. Amini, N. Usunier, and C. Goutte. Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization. In Advances in Neural Information Processing Systems 22 (NIPS 2009), pages 28­36, 2009.
[4] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple Kernel Learning, Conic Duality, and the SMO Algorithm. In Proc. 21st International Conference on Machine Learning (ICML 2004), 2004.
[5] N. Bel, C. H. Koster, and M. Villegas. Cross-lingual Text Categorization. In ECDL-2003, pages 126­139, 2003.

[6] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
[7] A. Blum and T. M. Mitchell. Combining Labeled and Unlabeled Data with Co-Training. In Proc. 11th Annual Conference on Learning Theory (COLT 1998), pages 92­100, 1998.
[8] U. Brefeld, T. G¨artner, T. Scheffer, and S. Wrobel. Efficient Co-regularised Least Squares Regression. In Proc. 23rd International Conference on Machine Learning (ICML 2006), pages 137­144, 2006.
[9] O. Chapelle, B. Sch¨olkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006.
[10] M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, Adaboost and Bregman Distances. Machine Learning, 48(1-3):253­285, 2002.
[11] J. D. Farquhar, D. R. Hardoon, H. Meng, J. Shawe-Taylor, and S. Szedmak. Two View Learning: SVM-2k, Theory and Practice. In Advances in Neural Information Processing 18 (NIPS 2005), pages 355­362, 2005.
[12] T. Joachims. Transductive Inference for Text Classification using Support Vector Machines. In Proc. of the Sixteenth International Conference on Machine Learning (ICML 1999), pages 200­209, 1999.
[13] T. Joachims. Training Linear SVMs in Linear Time. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 217­226, 2006.
[14] J. D. Lafferty, S. D. Pietra, and V. D. Pietra. Statistical Learning Algorithms Based on Bregman Distances. In Canadian Workshop on Information Theory, 1997.
[15] E. Lehmann. Nonparametric Statistical Methods Based on Ranks. McGraw-Hill, New York, 1975.
[16] K. Nigam, A. McCallum, S. Thrun, and T. M. Mitchell. Learning to Classify Text from Labeled and Unlabeled Documents. In Proc. of the 15th National Conference on Artificial intelligence (AAAI/IAAI 1998, pages 792­799, 1998.
[17] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proc. 3rd Text Retrieval Conference (TREC), pages 109­126, 1994.
[18] D. S. Rosenberg and P. L. Bartlett. The Rademacher Complexity of Co-regularized Kernel Classes. In Proc. of the 11th International Conference on Artificial Intelligence and Statistics (AISTATS 2007), pages 396­403, 2007.
[19] V. Sindhwani, P. Niyogi, and M. Belkin. A Co-regularization Approach to Semi-supervised Learning with Multiple Views. In Proceedings of the ICML-05 Workshop on Learning with Multiple Views, pages 74­79, 2005.
[20] N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson. NRC's PORTAGE system for WMT 2007. In ACL-2007 Second Workshop on SMT, 2007.
[21] C. J. Van Rijsbergen. Information Retrieval. Butterworth-Heinemann, Newton, MA, 1979.
[22] X. Zhu. Semi-supervised Learning Literature Survey. Technical report, University of Wisconsin Madison, 2008.

482

Towards Subjectifying Text Clustering
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu

ABSTRACT
Although it is common practice to produce only a single clustering of a dataset, in many cases text documents can be clustered along different dimensions. Unfortunately, not only do traditional text clustering algorithms fail to produce multiple clusterings of a dataset, the only clustering they produce may not be the one that the user desires. In this paper, we propose a simple active clustering algorithm that is capable of producing multiple clusterings of the same data according to user interest. In comparison to previous work on feedback-oriented clustering, the amount of user feedback required by our algorithm is minimal. In fact, the feedback turns out to be as simple as a cursory look at a list of words. Experimental results are very promising: our system is able to generate clusterings along the user-specified dimensions with reasonable accuracies on several challenging text classification tasks, thus providing suggestive evidence that our approach is viable.
Categories and Subject Descriptors
I.5.3 [Clustering]: Algorithms
General Terms
Algorithms, Experimentation
Keywords
interactive clustering, active clustering, spectral clustering, multiple clusterings, disparate clusterings
1. INTRODUCTION
Text clustering has been widely researched. The inherent ambiguity of natural language and the fact that many text clustering problems involve a large, complex feature space (usually represented as a bag of words) and a large number of data points make it an exciting clustering task. However, there is an important aspect of text clustering that is often
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

overlooked by text mining researchers: many text datasets can be naturally clustered along multiple dimensions. For instance, news articles can be clustered by topic, the source of the news (e.g., AP, CNN), or the date the article was written; political blog postings can be clustered not only by topic, but also by the author's stance on an issue (e.g., support, oppose) or her political affiliation; and movie reviews can be clustered by genre (e.g., action, documentary), sentiment (e.g, positive, negative), or even the main actors/actresses. In some text mining applications, it is desirable and sometimes important to recover as many clusterings of a dataset along its important clustering dimensions as possible.
Unfortunately, not only do traditional text clustering algorithms fail to produce multiple clusterings of a dataset, the only clustering they produce may not be the one the user desires. The traditional "optimal objective" approach to clustering is overly constrained in the sense that it forces an algorithm to produce a clustering along a single dimension, specifically the dimension along which the objective function is optimized. Although many different objective functions have been used, the basic qualitative criteria employed to evaluate the structure of a clustering (e.g., intra-cluster similarity, inter-cluster dissimilarity, and the size of the clusters) have remained more or less the same over the years. One important notion that is commonly left out of a qualitative measure is the human factor. As mentioned before, different subsets of features might lead to different kinds of clusterings of a dataset. Although a clustering algorithm identifies a particular clustering as the most structured one (i.e., the one that optimizes the objective), it might not be deemed fit by an end user, as she may be interested in a clustering that is different than the optimal clustering.
The question, then, is: can a clustering algorithm produce a clustering along the user-specified dimension (which may be suboptimal with respect to the objective function)? This also leads us to our second question: is it possible for a clustering algorithm to produce multiple clusterings of the same data simultaneously according to user interest?
One may argue that it is possible to design the feature space in a way that helps induce the user-desired clustering. This typically involves having the user identify features that are useful for inducing the desired clusters [14]. However, manually identifying the "right" set of features is both time-consuming and knowledge-intensive, and may require a lot of domain expertise. To overcome this weakness, researchers have attempted constrained clustering [2, 18] and learning a similarity metric from side information [19] such as constraints on which pairs of documents must or must

483

not appear in the same cluster. However, enough side information might not be readily available for many scenarios.
By contrast, recent work has focused on active clustering, where a clustering algorithm can incorporate user feedback during the clustering process to help ensure that the documents are grouped along the user-desired dimension. One way to do this is to have the user incrementally construct a set of relevant features in an interactive fashion [3, 16]. Another way is to have the user correct the mistakes made by the clustering algorithm in each clustering iteration by identifying the set of clusters that need to be merged or split [1]. A major drawback associated with these active clustering algorithms is that they involve a considerable amount of human feedback, which needs to be provided in each iteration of the clustering process.
In this paper, we attack the problem of subjectifying text clustering, or clustering documents according to user interest, from a different angle. We aim to develop a knowledgelean approach to this problem -- an approach that can produce multiple user-desired clusterings without relying on human knowledge for fine-tuning the similarity function or selecting the relevant features, unlike existing approaches. To this end, we propose a novel active clustering algorithm, which assumes as input a simple feature representation (composed of unigrams only) and a simple similarity function (i.e., the dot product), and operates by (1) inducing the important clustering dimensions of a given set of documents, where each clustering dimension is represented by a (small) number of automatically generated words that are representative of the dimension; and (2) have the user select the dimension(s) along which she wants to cluster the documents by examining these automatically generated words. In comparison to previous work on feedback-oriented clustering, the amount of user feedback required by our algorithm is minimal. In fact, the feedback turns out to be as simple as a cursory look at these automatically generated words and is required only once. Experimental results are very promising: our system is able to generate the user-specified clustering with reasonable accuracies on several challenging text classification tasks, thus providing suggestive evidence that our approach is viable.
The rest of the paper is organized as follows. In Section 2, we enumerate the properties that are desirable of an algorithm for producing multiple clusterings and discuss related work on multiple clusterings. In Section 3, we present our active clustering algorithm. We describe our evaluation results in Section 4 and present our conclusions in Section 5.
2. PRODUCING MULTIPLE CLUSTERINGS
In this section, we enumerate the desiderata for an algorithm for producing multiple meaningful clusterings of a dataset. By meaningful, we mean that each of these clusterings should be qualitatively strong. As mentioned in the introduction, there are different ways to evaluate the quality of a clustering (e.g., intra-cluster similarity, inter-cluster dissimilarity), which is typically captured by the objective function employed by the clustering algorithm. A clustering is meaningful if it is not overly suboptimal with respect to the objective function.
Before we formalize the concept, let us introduce some notation. Let X = {x1, . . . , xn} be a set of n documents to be clustered, where each document xi, i = 1 : n, is represented by a bag of d unigrams w1, w2, . . . , wd. We want to

learn m different partitioning functions f i, i = 1 : m, that correspond to m different clusterings Ci, i = 1 : m. Specifically, each partitioning function f i takes data X as input, and outputs a 2-way partition Ci = {C1i , C2i }, i = 1 : m, such that C1i  C2i = X and C1i  C2i = .1 Finally, a clustering algorithm employs an objective function, o : Ci  , which assigns a qualitative score to each clustering.
To produce multiple meaningful clusterings (henceforth multiple clusterings for brevity), we require a clustering algorithm to satisfy three multi-clustering criteria:
Multiplicity: Given data X, a clustering algorithm should be able to produce m (where m > 1) different clusterings of the data, Ci, i = 1 : m, without having to change the feature space and the similarity function.
Distinctivity: Each clustering Ci, i = 1 : m, should be distinctively different. By distinctively different, we mean that two clusterings are highly dissimilar. That is, the similarity of two clusterings should be close to zero.
Quality: Each clustering Ci, i = 1 : m, should be qualitatively strong (i.e., close to optimal) with respect to the objective function o. This condition ensures that none of the clusterings that the algorithm produces is overly suboptimal and thus completely structure-less.
Now, the question is: do existing clustering algorithms produce multiple clusterings of a dataset and satisfy multiplicity, distinctivity and quality? It turns out that a few of them do [5, 7, 9, 11]. Below we first describe each of these algorithms, and then explain how our algorithm differs from them. These existing clustering algorithms can be broadly divided into two categories:
Semi-supervised methods. In these methods, one of the clusterings is provided (by the human) as input, and the goal is to produce the other clustering, assuming that there are only two distinct ways to cluster the data. For instance, Gondek and Hofmann's approach [9] learns a non-redundant clustering that maximizes the conditional mutual information I(C; Y |Z), where C, Y and Z denote the clustering to be learned, the relevant features and the known clustering, respectively. Their approach turns out to be difficult to implement, since it requires modeling the joint distribution of the cluster labels and the relevant features. On the other hand, Davidson and Qi's approach [7] first learns a distance metric DC from the original clustering C, and then reverses the transformation of DC using the Moore-Penrose pseudoinverse to get the new distance metric DC , which is used to produce a distinctively different clustering.
Unsupervised methods. Here, each possible clustering of a dataset is produced in an unsupervised manner (i.e., without using any labeled data). Caruana et al.'s approach [5], also known as meta clustering, produces multiple clusterings of the same data by running k-means multiple times, each time with a random selection of seeds and a random weighting of features. The goal is to present each local minimum of k-means as a possible clustering. It suffers from two weaknesses. First, it does not ensure that the aforementioned distinctivity and quality criteria are satisfied. Second, kmeans tends to produce similar clusterings regardless of the number of times it is run (see our meta clustering results in Section 4). Jain et al.'s approach [11] is more sophisticated, as it tries to learn two clusterings in a "decorrelated" k-means
1While we present our algorithm for 2-way clustering tasks, it can be extended to produce n-way (n > 2) clusterings.

484

framework. Its joint optimization model achieves typical kmeans objectives and at the same time ensures that each of the two induced clusterings are distinctively different. Note that Jain et al. use this framework to produce only two clusterings of the data, as the optimization objective becomes too convoluted to generate more clusterings.
Before providing the details of our active clustering algorithm, we describe the primary differences between our algorithm and the aforementioned approaches. First, our algorithm operates in an unsupervised setting, i.e., it neither uses any labeled data nor assumes the existence of a prior clustering, unlike the semi-supervised methods. Second, it satisfies all three multi-clustering criteria (i.e., multiplicity, distinctivity, and quality). Finally, it is not restricted to producing only two clusterings.
3. OUR APPROACH
In this section, we describe our active clustering algorithm, which can produce multiple clusterings of the same data according to user interest. At the core of our algorithm resides spectral clustering. Even though spectral clustering is widely researched, it has traditionally been used to produce a single clustering of a dataset. To our knowledge, we are the first to exploit spectral clustering to produce multiple clusterings of the same data. Interestingly, a spectral clustering algorithm naturally satisfies all three multiclustering criteria that we desire: multiplicity, distinctivity, and quality. In the rest of this section, we first show that a small extension of a spectral clustering algorithm (namely, Shi and Malik's spectral clustering algorithm [17]) satisfies the three multi-clustering criteria and hence can be used to produce multiple clusterings. We then show how to incorporate human feedback into the spectral clustering algorithm to produce the clusterings that the user desires.
3.1 Spectral Learning and Multi-clustering
Many variants of spectral clustering have been proposed (e.g., [15, 17]). Here, we use Shi and Malik's spectral clustering algorithm [17], as it is widely used. We first show how to extend their algorithm to produce multiple clusterings of a dataset. More specifically, we propose an extension of their algorithm that takes a set of n documents, X  n×d, as input, and outputs m different document clusterings, where d is the number of unigrams, as defined in Section 2.2 In addition, as spectral algorithms work in a matrix space, we introduce another notation. Let s : X × X  be a symmetric similarity function over X (i.e., s(xi, xj) = s(xj, xi)), and S be the similarity matrix that captures pairwise similarities (i.e., Si,j = s(xi, xj)).
Before describing our algorithm, we define two concepts: optimal and suboptimal clusterings. Given a clustering algorithm with a predefined objective function o, the optimal clustering is the clustering that optimizes o. All other clusterings are suboptimal clusterings. Below we show how to learn the optimal clustering and several suboptimal clusterings using Shi and Malik's spectral algorithm.
Learning the optimal clustering. Spectral clustering employs a graph-theoretic notion of grouping. A set of n data points in an arbitrary feature space is represented as an undirected graph, where each node corresponds to a data
2As we only apply spectral clustering to produce 2-way clusterings, we will center our discussion on 2-way clusterings.

point, and the edge weight between two nodes is their similarity as defined by S. The goal is to induce a clustering, or equivalently, a partitioning function f , which is typically represented as a vector of length n such that f (i)  {1, -1} indicates which of the two clusters data point i should be assigned to. Note that the cluster labels are interchangeable and can even be renamed without any loss of generality.
Normalized cut [17] is a widely used objective function in spectral clustering. Shi and Malik show that if we embed the optimization problem in the real domain (i.e, we allow f to be a real-valued vector rather than a binary-valued vector), then the normalized cut partition of X can be derived from the solution to the following constrained optimization problem:

argmin
f n

i,j

Si,j

(

f(i) di

-

f (j) )2 dj

(1)

subject to ||f ||2 = Di,i and D-1/2f  1
i

where D is a diagonal matrix with Di,i = j Si,j and di = Di,i. It can be proved that the closed form solution to this optimization problem is the eigenvector corresponding to the second smallest eigenvalue of the Laplacian matrix L = D-1/2(D - S)D-1/2 [17].3 Clustering using the second eigenvector4, e2, is trivial: we can just apply 2-means to the n data points represented by the second eigenvector [15].
Learning suboptimal clusterings. As mentioned before, suboptimal clusterings are useful if they can capture the user-desired clustering. Our algorithm for producing multiple suboptimal clusterings exploits a useful but rarely utilized fact: if we consider only those vectors that are orthogonal to e2 as candidate solutions to the constrained optimization problem above, then e3 is the solution. More generally, if our candidate solutions are restricted to those vectors that are orthogonal to the first n eigenvectors of L, then en+1 is the solution. In other words, all en where n > 2 are suboptimal solutions to the minimization problem, with en being more suboptimal as n increases. Hence, we can apply 2-means to each of the eigenvectors, en, separately to produce a suboptimal clustering. To our knowledge, employing suboptimal partitioning functions to produce multiple clusterings is an unexplored idea: existing work has focused on using only e2 (or a combination of e2 and other eigenvectors) to derive a single partition of the data; in contrast, we use each of the eis (with i  2) separately to produce multiple clusterings of the data.
So, our algorithm for producing multiple clusterings is simple: given data X and a symmetric similarity function s, we form the Laplacian L, compute its second through (m + 1)-th eigenvectors, and apply 2-means to each of these m eigenvectors separately to produce m different clusterings.
Next, we show that our system satisfies each of the three multi-clustering criteria:
Multiplicity: Our algorithm produces m different clusterings, where m > 1, without changing the feature space and the similarity function. Hence, it satisfies multiplicity.

3Note that the solution to Equation (1) is only an approximation to the (discrete) normalized cut solution. Our definition of optimal and suboptimal clusterings refers to the continuous normalized cut objective.
4We refer to the nth smallest eigenvector of the Laplacian simply as the nth eigenvector, and denote it by en.

485

Distinctivity: With some algebra, one can show that (1) L is symmetric when the similarity matrix S is symmetric, and (2) the eigenvectors of L are orthogonal to each other when L is symmetric. Since we always use a symmetric similarity function, S and L are symmetric. As a result, the eigenvectors of L are orthogonal to each other, and their similarity (obtained via the dot product) is zero. Since we employ the principal eigenvectors of L (starting from e2) as real-valued partitioning functions, the clusterings produced by these functions are distinctively different.5
Quality: As noted before, if we disallow the first n eigenvectors of L to be the solution to our optimization problem, then en+1 is the solution. This implies that clustering using e3 achieves the next optimal objective value after e2, and more generally, clustering using em+1 achieves the next optimal objective value after e2, . . ., em. Hence, e3, . . ., em+1 constitute the (m - 1)-best suboptimal solutions that can be achieved by a spectral system. This gives us direct control over suboptimality: if we do not desire overly suboptimal solutions, we can simply put restrictions on m. In our experiments, we set m to 4, thus producing one optimal and three suboptimal clusterings for each dataset.6

3.2 Incorporating Human Feedback
So far, we have shown how to produce multiple clusterings (Ci, i = 1 : m) of a dataset. To determine which of these m clusterings is the user-desired clustering, one can possibly have the user inspect the clusterings and decide which one corresponds most closely to the desired clustering. The main drawback associated with this kind of user feedback is that the user may have to inspect a large number of documents in order to make a decision. To reduce human effort, we employ an alternative procedure: we (1) identify the most informative unigrams characterizing each cluster, and (2) have the user inspect just these "features" rather than the documents.
To select these informative features, we rank them by their weighted log-likelihood ratio (WLLR):

P

(wi

|

Cj )

·

log

P (wi P (wi |

| Cj) ¬Cj )

,

where wi and Cj denote the ith feature and the jth cluster respectively, and each probability is add-one smoothed. Informally, feature wi will have a high rank with respect to cluster Cj if it appears frequently in Cj and infrequently in ¬Cj . This correlates reasonably well with what we think an informative feature should be. Now, for each of the m partitions, we (1) derive the top 100 features for each cluster according to the WLLR, and then (2) present the ranked lists to the user. The user then selects the feature lists that are most relevant to her interest by inspecting as many features in the ranked lists as needed.
There is a caveat, however. The presence of a large number of ambiguous documents can adversely affect the identification of informative features, owing to the fact that many text documents are ambiguous with respect to the dimen-

5Note that we only compare two partitioning functions (or, more precisely, two clusterings) in the continuous space. Their similarity might be different in the discrete space.
6We assume that the clusterings we are interested in can be captured using four eigenvectors. Note that using only up to e5 is by no means a self-imposed limitation of our algorithm, since we can employ as many eigenvectors as we desire.

sion along which they are clustered. For instance, many product reviews are sentimentally ambiguous (i.e., they contain both positive and negative sentiment-bearing words), as a reviewer often likes certain aspects of the product and dislikes the others. Hence, we remove the ambiguous documents before deriving informative features from a partition.
We employ a simple method for identifying unambiguous documents. In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity. Ambiguous points receive the orthogonal projections from both the positive and negative points, and hence they have near zero values in the pivot eigenvectors. In other words, the points with near zero values in the eigenvectors are more ambiguous than those with large absolute values. We therefore sort the data points according to their corresponding values in the eigenvector, and keep only the top n/8 and the bottom n/8 data points. We induce the informative features only from the resulting 25% of the data points, and present them to the user so that she can select the desired partition.7
In the event that the user identifies more than one eigenvector as relevant to the desired clustering dimension, we apply 2-means to re-cluster the n documents in the space defined by all of the human-selected eigenvectors. Below is the final algorithm.
Algorithm: Active-Spectral-Clustering Input: Data X, Similarity Matrix S Output: Clustering C = {C1, C2}
1. Construct the Laplacian matrix L = D-1/2(D-S)D-1/2, where D is a diagonal matrix with Di,i = j Si,j .
2. Compute E = {e1, e2, . . ., em+1}, the eigenvectors of L that correspond to its (m + 1) smallest eigenvalues.
3. For each ei  E \ {e1}, induce the top feature list. 4. Ask the user to identify E , the subset of E that is rele-
vant to the desired clustering dimension, by inspecting the top feature lists.
5. Create the clustering C = {C1, C2} by using 2-means to cluster the data points in the space defined by E .
Note that this active clustering algorithm produces a single clustering of a dataset along the dimension that the user selects. If we want to use our algorithm to produce multiple clusterings of the same data along different dimensions, we just need to repeat steps 4 and 5 with a different set of eigenvectors chosen by the user for each intended dimension.
4. EVALUATION
4.1 Experimental Setup
Datasets. We employ five evaluation datasets. Blitzer et al.'s book (BOO) and DVD datasets [4] each con-
tains 1000 positive and 1000 negative customer reviews of books or movies, and can therefore be used to evaluate our algorithm's ability to cluster by sentiment. Since we desire that each evaluation dataset possesses at least two clustering dimensions, we also manually annotate each review with a subjectivity label that indicates whether it is "mostly subjective" (where the reviewer mainly expresses her sentiment) or
725% is a somewhat arbitrary choice. Additional experiments revealed that the list of top-ranked features is not particularly sensitive to slight changes to the number of unambiguous documents used in the feature induction process.

486

"mostly objective" (where the reviewer focuses on describing the content of the book or the movie). Details of the annotation process are described later in this subsection.
The MIX dataset is a 4000-document dataset consisting of the 2000 BOO reviews and the 2000 DVD reviews, as described above. We can therefore cluster these reviews by topic (i.e., book or DVD), sentiment or subjectivity.
Schler et al.'s MAN dataset [13] contains 19,320 blog posts. We randomly selected 1000 blog postings, half of which were written by males and half by females. We can therefore cluster these blog posts by the author's gender. Since the author's age information is also available in each blog post, we can also cluster them by age. To do so, we automatically generate a 2-way partitioning of the documents by imposing an age threshold of 25. Specifically, the 932 documents written by bloggers aged below 25 are marked as young, and the remaining 1068 are marked as old.
Our own POA dataset consists of 2000 political articles written by columnists, 1000 of whom identified themselves as Republicans and the remaining 1000 identified themselves as Democrats.8 Hence, we can cluster these articles by the author's political affiliation. We also create a second clustering dimension by annotating each article as either foreign or domestic, depending on the policy that the article discusses. For example, the policy on the Iraq war is foreign, whereas the policy on regulating the job market is domestic.
Table 1 summarizes the dimensions along which the documents are annotated. Note that each of the seven distinct dimensions yields a 2-way partitioning of the documents: (1) Sentiment (positive/negative); (2) Subjectivity (subjective/objective); (3) Topic (book/DVD); (4) Gender (man/woman); (5) Age (young/old); (6) Political affiliation (Democrat/Republican); and (7) Policy (domestic/foreign).
Human annotation. As mentioned above, we need to annotate the BOO, DVD, and MIX datasets with respect to Subjectivity and POA with respect to Policy.9 We had two computer science graduate students independently annotate the documents. For POA, we asked them to use commonsense knowledge to annotate each document with respect to the policy that the article discusses. If both foreign and domestic policies are discussed in the same article, we asked them to assign the label based on the one that is discussed more frequently. On the other hand, given a BOO or DVD review, we asked them to first label each of its sentences as subjective or objective; if a sentence contains both subjective and objective materials, its label should reflect the type of material that appears more frequently. The review is then labeled as subjective (objective) if more than half of its sentences are labeled as subjective (objective).
The inter-annotator agreement rate in terms of Cohen's  is 0.774 (BOO), 0.796 (DVD), and 0.820 (POA), indicating substantial agreement. The annotators examined each case on which they disagreed and decided on the final label together. They reported that essentially all disagreements arose from labeling the ambiguous data points (e.g., articles that discuss both foreign and domestic policies, and sentences that contain both subjective and objective materials). In the end, we obtained 1205 subjective and 795
8These articles were chosen randomly among those written in 2006 from http://www.commondreams.org/archives. 9Note that the subjectivity labels for MIX can be derived from BOO and DVD.

BOO DVD MIX MAN POA

Dimension 1
Sentiment Sentiment
Topic Gender Political Affiliation

Dimension 2
Subjectivity Subjectivity Sentiment
Age Policy

Dimension 3
­ ­ Subjectivity ­ ­

Table 1: Clustering dimensions for the five datasets.

objective documents for BOO, 1124 subjective and 876 objective documents for DVD, and 875 foreign and 1125 domestic documents for POA. To stimulate research, we make these annotations publicly available.10
Document preprocessing. To preprocess a document, we first tokenize and downcase it, and then represent it as a vector of unstemmed unigrams, each of which assumes a value of 1 or 0 that indicates its presence or absence in the document. In addition, we remove from the vector punctuations, numbers, words of length one, and words that occur in only a single document. Following the common practice in the information retrieval (IR) community, we also exclude words with a high document frequency, many of which are stopwords or domain-specific general-purpose words. Details of this preprocessing step can be found in Dasgupta and Ng [6]. We compute the similarity between two documents by taking the dot product of their feature vectors.
Evaluation metrics. We employ two evaluation metrics. First, we report results for each dataset in terms of accuracy, which is the fraction of documents for which the label assigned by our system is the same as the gold-standard label.11 Second, following Kamvar et al. [12], we evaluate the clusters produced by our approach against the gold-standard clusters using the Adjusted Rand Index (ARI) [10]. ARI is the adjusted-for-chance form of the Rand Index, which computes the pairwise accuracy given two partitions. ARI ranges from -1 to 1; better clusterings have higher values.
4.2 Baseline Systems
Clustering using the second eigenvector only. As our first baseline, we adopt the commonly-used approach introduced by Shi and Malik [17] and cluster the reviews using only the second eigenvector, e2, which induces the partitioning that is optimal with respect to spectral clustering's objective function, as described previously. Results of this baseline, reported in terms of accuracy and ARI, are shown in row 1 of Table 2 and Table 3, respectively.12 Since this method can propose only one clustering per dataset but each dataset contains at least two gold-standard clusterings (one for each dimension), the results are obtained by comparing this proposal clustering against each of the gold-standard clusterings. As we can see, accuracy ranges from 52.9 to 77.1, and ARI ranges from 0.003 to 0.291. Note that these and other results involving 2-means are averaged over ten independent runs owing to the randomness in seed selection.
Non-negative Matrix Factorization. As our second base-
10http://www.utdallas.edu/sajib/multi-clusterings.html 11Since a human also labels each cluster by inspecting the induced features, we are able to compute accuracy. 12For each dataset shown in Tables 2 and 3, Dimn refers to the nth dimension listed for the dataset in Table 1. For instance, Dim1 and Dim2 of BOO correspond to Sentiment and Subjectivity, respectively.

487

System Variation 1 2nd eigenvector 2 NMF 3 Meta clustering 4 Feature removal 5 Our approach

BOO Dim1 Dim2 58.9 58.6 52.1 57.8 50.8 51.2 58.9 63.2
69.5 63.8

DVD Dim1 Dim2 55.3 61.0 50.3 60.5 53.9 71.0 51.2 60.5 70.7 60.5

Dim1 77.1 69.2 50.2 77.1
77.1

MIX Dim2 52.9 51.7 50.2 50.0
68.9

Dim3 66.6 58.6 58.6 51.0 59.7

MAN Dim1 Dim2 64.2 62.4 55.6 58.0 51.2 53.6 64.2 57.8
66.4 62.9

POA Dim1 Dim2 54.2 63.1 53.0 62.8 59.4 58.8 57.8 63.1
69.7 76.3

Table 2: Results in terms of accuracy for the five datasets. The best result for each dimension is boldfaced.

System Variation 1 2nd eigenvector 2 NMF 3 Meta clustering 4 Feature removal 5 Our approach

BOO

Dim1 Dim2

0.031 0.027

0.002 0.000 0.031

-0.03 -0.04 0.07

0.152 0.074

DVD Dim1 Dim2 0.011 0.044 0.000 0.005 0.006 0.155 0.000 0.044 0.171 0.044

Dim1 0.291 0.147 0.000 0.291
0.291

MIX Dim2 0.003 0.000 0.000 -0.03
0.142

Dim3 0.109 0.026 0.021 0.010 0.036

MAN Dim1 Dim2 0.080 0.061 0.012 0.022 0.001 0.003 0.080 0.024
0.107 0.065

POA Dim1 Dim2 0.007 0.066 0.003 0.063 0.035 0.023 0.024 0.066
0.155 0.276

Table 3: Results in terms of ARI for the five datasets. The best result for each dimension is boldfaced.

line, we use Non-negative Matrix Factorization (NMF), which has recently been demonstrated to be more effective than Latent Semantic Analysis (LSA) [8] for document clustering [20]. As in the first baseline, we compare the clustering proposed by NMF against each of the gold-standard clusterings for each dataset. Since the algorithm involves choosing seeds at random, the NMF results shown in row 2 of Tables 2 and 3 are averaged over ten independent runs. Despite its algorithmic sophistication, NMF performs consistently worse than the first baseline in terms of both accuracy and ARI.
Meta clustering. Since our algorithm produces multiple clusterings, it is desirable to have a baseline that also produces multiple clusterings. However, many algorithms that produce multiple clusterings operate in a semi-supervised setting [7, 9]. The notable exceptions are Caruana et al.'s meta clustering algorithm [5] and Jain et al.'s approach [11]. Since some of our datasets can be clustered in three different ways but Jain et al.'s approach produces only two clusterings for a given dataset, we evaluate meta clustering only. As mentioned before, meta clustering produces multiple clusterings of the same data by running k-means multiple times, each time with a random selection of seeds and a random weighting of features. We produce multiple clusterings for each dataset by running this algorithm 100 times and report in row 3 of Tables 2 and 3 the best result obtained for each dimension of each dataset.13 Even though the best results are reported, meta clustering underperforms the first two baselines for all but two dimensions (DVD/Dim2 and POA/Dim1). The poor performance can be attributed to the fact that k-means is generally a weaker clustering algorithm than its more recently developed counterparts.
Iterative feature removal. We designed another simple baseline for producing multiple clusterings. Given a dataset, we (1) apply spectral clustering to produce a 2-way clustering using the second eigenvector, and then (2) remove from each cluster the informative features that are identified using WLLR. To produce another clustering, we repeat these two steps, but without the features removed in step 2. Hence, we can generate as many clusterings as we want by repeating
13These results are obtained using a publicly-available implementation of meta clustering, which is available at http://www.cs.cornell.edu/nhnguyen/metaclustering.htm.

these two steps, each time with a smaller number of features. The motivation behind this algorithm is that by repeatedly removing the informative features from a partition and reclustering, we can potentially yield different clusterings. To obtain the results of this algorithm for each dataset in row 4 of Tables 2 and 3, we (1) run it for m iterations to produce m clusterings, where m is the number of dimensions the dataset possesses; and (2) find the bipartite matching between the proposal clusterings and the gold-standard clusterings that has the highest average accuracy/ARI. Since we need to specify the number of features to remove from each cluster in each iteration, we tested values from 100 to 5000 in steps of 100, reporting the best result.
As we can see, except for BOO/Dim2 and POA/Dim1, this algorithm never surpasses the performance of the first baseline. One reason for its poorer performance can be attributed to the fact that the informative features for the different dimensions of a dataset are not disjoint. For example, terrific is likely to be an informative feature when clustering by sentiment and by subjectivity, but since this algorithm removes informative features in each iteration, terrific will only be accessible to one but not both clustering dimensions.
4.3 Our Active Clustering Algorithm
Human experiments. An important step in our algorithm involves having a user identify the dimensions along which she wants to cluster the documents by inspecting the features. To evaluate the feasibility of this step, we performed the experiment independently with ten humans (all of whom are computer science graduate students not involved in data annotation) and computed the agreement rate.
Specifically, for each dataset, we generated four clustering dimensions using the second through fifth eigenvectors of the Laplacian. After that, we showed the human judges the top 100 features for each cluster of each dimension according to WLLR (see Tables 4 and 5 for a snippet, where the dimension and cluster labels are manually assigned by the majority of the judges). To mimic the realistic situation where a user of our algorithm knows a priori the dimension(s) along which she wants to cluster the documents, we informed each judge of the dimensions she was expected to identify: for example, for BOO, she was told to identify the Sentiment and Subjectivity dimensions. To ensure that

488

e2
Sub jective fan
bought money video waste quality reviews series buying worth

DVD
e3
Positive wonderful
music collection excellent
quality cast extras song
special highly

e4
C1 video music workout found videos bought moves doing kids right

e5
C1 saw watched loved series season fan comedy family enjoy whole

Ob jective
young between director played
cast role place performance actors men
Sub jectivity

Negative
money waste thought worst boring actually saw maybe nothing
felt
Sentiment

C2 series
fan cast comedy stars actors episodes original season set

C2 quality money video sound picture version waste found
disk transfer

e2
Book reader information example subject important nature
text provides science human

MIX
e3
Sub jective bought
disappointed information
reviews waste recipes workout video boring easy

e4
Positive music
wonderful excellent
highly collection
special video classic disc version

e5
C1 loved children enjoyed wonderful novel mother bought child parents novels

DVD
music actors watched script films loved
saw video horrible
tv
Topic

Ob jective
men young dirctor cast scene
role actors films plays
war
Sub jectivity

Negative
boring waste novel pages worst
felt person disappointed finish reviews
Sentiment

C2 version quality waste original sound edition
disc features review
video

Table 4: Top ten features induced for each dimension for the DVD and MIX datasets. The shaded columns correspond to the dimensions selected by the judges. e2, . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

e2
Foreign muslim israel islamic islam muslims
jews israeli peace religion saddam

e3
Foreign iran iraqi forces israeli israel
weapons east
nuclear region regime

POA
e4
Foreign oil
growth rate food
poverty living average rates economy god

e5
Republican voters
conservative gop win polls
candidates hilary kerry clinton
conservatives

e2
Woman omg lol haha sooo bye
hahaha hehe soo ppl wanna

MAN

e3

e4

Woman blonde

C1 faith

danced

deeply

wore

happiness

worn

emotions

romantic prayed

dancing strength

dresses existence

porch

actions

dall

soul

lips

courage

e5
Young ur
wmd ashcroft qaeda
haha omg iraqi voters ppl ghraib

Domestic
tax economy spending income corporate
jobs taxes budget rates prices

Domestic
god love kids church im myself person parents folks family
Policy

Domestic
court constituational
supreme judiciary intelligence committee presidential constitution
nsa judicial

Democrat
agency information companies department
justice warrant criminal investigation
legal documents
Political Aff.

Man

Man

policies

issues

voters

linux

democracy

orkut

opposition

xml

coverage developers

networks

software

policy

debian

credibility

rss

governments interface

federal

mozilla

Gender

C2 ebay dvd cable upgrade browser xp users software feature camera

Old
kitchen bedroom laundry
meal dishes delicious grocery cozy wrapped salad
Age

Table 5: Top ten features induced for each dimension for the POA and MAN datasets. The shaded columns correspond to the dimensions selected by the judges. e2, . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

the judges have a consistent understanding of the clustering dimensions, we explained to them the seven clustering dimensions shown in Table 1 with definitions and examples prior to the experiment. (We omitted them here due to space limitations.) Also, we told them that a clustering dimension could be captured by multiple eigenvectors. If they determined that more than one eigenvector was relevant to a dimension, they were instructed to rank the eigenvectors

in terms of their degree of relevance, where the most relevant one would appear first in the list. Finally, they were given the option of not choosing any eigenvector for a given dimension if it was not possible for them to do so. Note that while only the top ten features are shown in Tables 4 and 5, they based their judgment on the top 100 features.
After the judges completed the experiment, we (1) selected for each dimension the largest set of eigenvectors that

489

BOO DVD MIX MAN POA

Dimension 1
4 (100%) 3 (100%) 2 (100%) 2,3 (70%) 5 (40%)

Dimension 2
3 (70%) 2 (90%) 4 (80%) 5 (60%) 2,3,4 (80%)

Dimension 3
­ ­ 3 (100%) ­ ­

Table 6: Human-selected eigenvectors and the agreement rate for the five datasets.

was ranked first by the majority of the judges, and (2) computed the agreement rate as the percentage of judges who assigned the highest rank to the eigenvector set obtained in step 1. Results are shown in Table 6, where the agreement rate is shown in parentheses. As we can see, reasonably high agreement is achieved for all dimensions in all datasets except Political Affiliation (Dimension 1 of POA) and Age (Dimension 2 of MAN). An inspection of the feature lists induced for POA and MAN (see Table 5) reveals that they are fairly noisy, which makes it difficult to identify these two dimensions. In fact, many judges could not find any relevant eigenvector for these dimensions.
Overall, these results, together with the fact that a judge took 8­9 minutes on average to identify each dimension, indicate that asking a human to identify the intended dimension(s) based on the induced features is a viable task.
Clustering results. Next, we cluster the documents in each dataset along each dimension using the eigenvectors selected by the majority of the judges. If more than one eigenvector is selected for a dimension, the documents will be clustered using 2-means in the space defined by all of the selected eigenvectors. The clustering results are shown in row 5 of Tables 2 and 3. As we can see, our clustering algorithm frequently outperforms the four baselines by a large margin. These results substantiate our claim that our algorithm can cluster documents along multiple dimensions according to user interest. In addition, clustering accuracies are generally higher for the topic-related dimensions (e.g., Book vs. DVD and Domestic vs. Foreign) than the other dimensions (e.g., Gender, Age, Sentiment). This should not be surprising: non-topic-based classification tasks can be difficult even for supervised systems that are trained on a large amount of labeled data (e.g., [13]).
5. CONCLUSIONS
Overall, we believe that our work on subjectifying text clustering makes three contributions:
Generation of multiple clusterings. With a few exceptions (e.g., [5, 11]), existing clustering algorithms can only produce a single clustering of a dataset along its most prominent dimension. In contrast, our algorithm can produce multiple clusterings of the same data according to user interest without using any labeled data or considerable feedback.
Interactivity in IR. The active clustering algorithm that we proposed allows for more user interactivity in an easy, low effort manner. Perhaps the implications of our work for interactivity in IR are even more important: we believe this and other interactive algorithms that allow the user to make small, guiding tweaks, and thereby get results better than would otherwise be possible is the future of IR.

Improved understanding of spectral clustering. While spectral clustering has been employed for many years to produce a single clustering of a dataset, to our knowledge we are the first to empirically demonstrate that the top eigenvectors of the Laplacian can be used in isolation or in combination to produce semantic clusterings.
6. ACKNOWLEDGMENTS
We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper. This work was supported in part by NSF Grant IIS-0812261.
7. REFERENCES
[1] M.-F. Balcan and A. Blum. Clustering with interactive feedback. In Proc. of ALT, pages 316­328, 2008.
[2] S. Basu, A. Banerjee, and R. J. Mooney. Active semisupervision for pairwise constrained clustering. In Proc. of SDM, pages 333­344, 2004.
[3] R. Bekkerman, H. Raghavan, J. Allan, and K. Eguchi. Interactive clustering of text collections according to a userspecified criterion. In Proc. of IJCAI, pages 684­689, 2007.
[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. of the ACL, pages 440­447, 2007.
[5] R. Caruana, M. F. Elhawary, N. Nguyen, and C. Smith. Meta clustering. In Proc. of ICDM, pages 107­118, 2006.
[6] S. Dasgupta and V. Ng. Topic-wise, sentiment-wise, or otherwise? Identifying the hidden dimension for unsupervised text classification In Proc. of EMNLP, pages 580­589, 2009.
[7] I. Davidson and Z. Qi. Finding alternative clusterings using constraints. In Proc. of ICDM, pages 240­249, 2007.
[8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of American Society of Information Science, 41(6):391­407, 1990.
[9] D. Gondek and T. Hofmann. Non-redundant data clustering. In Proc. of ICDM, pages 75­82, 2004.
[10] L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2:193­218, 1985.
[11] P. Jain, R. Meka, and I. S. Dhillon. Simultaneous unsupervised learning of disparate clusterings. In Proc. of SDM, pages 858­869, 2008.
[12] S. Kamvar, D. Klein, and C. Manning. Spectral learning. In Proc. of IJCAI, pages 561­566, 2003.
[13] J. Schler, M. Koppel, S. Argamon, and J. Pennebaker. Effects of age and gender on blogging. In AAAI Symposium on Computational Approaches for Analyzing Weblogs, 2006.
[14] B. Liu, X. Li, W. S. Lee, and P. S. Yu. Text classification by labeling words. In Proc. of AAAI, pages 425­430, 2004.
[15] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Advances in NIPS, pages 849­ 856, 2001.
[16] H. Raghavan and J. Allan. An interactive algorithm for asking and incorporating feature feedback into support vector machines. In Proc. of SIGIR, pages 79­86, 2007.
[17] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888­905, 2000.
[18] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr¨odl. Constrained k-means clustering with background knowledge. In Proc. of ICML, pages 577­584, 2001.
[19] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell. Distance metric learning with application to clustering with side-information. In Advances in NIPS, pages 505­512, 2002.
[20] W. Xu, X. Liu, and Y. Gong. Document clustering based on non-negative matrix factorization. In Proc. of SIGIR, pages 267­273, 2003.

490

Visual Summarization of Web Pages
Binxing Jiao Linjun Yang Jizheng Xu Feng Wu
MOE-MS KeyLab of MCC, University of Science and Technology of China, Hefei, 230026, P. R. China Microsoft Research Asia, Beijing 100190, P. R. China
bxjiao@gmail.com, {linjuny, jzxu, fengwu}@microsoft.com

ABSTRACT
Visual summarization is an attractive new scheme to summarize web pages, which can help achieve a more friendly user experience in search and re-finding tasks by allowing users quickly get the idea of what the web page is about and helping users recall the visited web page. In this paper, we perform a careful study on the recently proposed visual summarization approaches, including the thumbnail of the web page snapshot, the internal image in the web page which is representative of the content in the page, and the visual snippet which is a synthesized image based on the internal image, the title, and the logo found in the web page. Moreover, since the internal image based summarization approach hardly works when the representative internal images are unavailable, we propose a new strategy, which retrieves the representative image from the external to summarize the web page. The experimental results suggest that the various summarization approaches have respective advantages on different types of web pages. While internal images and thumbnails can provide a reliable summarization on web pages with dominant images and web pages with simple structure respectively, the external images are regarded as a useful information to complement the internal images and are demonstrated very useful in helping users understanding new web pages . The visual snippet performs well on the re-finding tasks since it incorporates the title and logo which are advantageous on identifying the visited web pages.
Categories and Subject Descriptors
H.5.m [Information Interfaces and Presentation]: Miscellaneous
General Terms
Algorithms, Experimentation
This work was performed at Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
web page summarization, visual summarization
1. INTRODUCTION
Search and re-finding tasks are among the most typical applications on the Internet [4]. In order to help users accomplish these two tasks efficiently, systems usually provide a summarization of web page so that users can quickly judge whether the page is of interest. For example, the search engine usually presents the search result using the summarization of web pages, consisting of the URL, page title and a short textual snippets. When people try to re-find the web pages in the browsing history, they are likely to access the bookmarks in the web browser, where visited web pages are summarized by the url and the title. While most systems summarize web pages by text, due to its compactness and simplicity, it is difficult for users to quickly and precisely judge if one web page is of interest from the textual summarization, since it is often time-consuming for users to read a long text snippet or it usually cannot convey enough information if the text snippet is short.
A picture is worth a thousand words. Information search and re-finding tasks would become more efficient if the web pages are visually summarized, since it is easier for people to get a quick understanding by seeing an image than reading text. Google has recently released the "Images from the pages" feature [7] in its web search system, which summarizes the web page using the images in the page (we refer to it as internal images in this paper). This feature presents two images selected from the web page alongside the textual snippet to users, from which users can quickly identify the desired web pages, even without reading the textual snippets. Visual summarization can also used to enhance the bookmarks in a Web browser to help users easily accomplish the re-finding task. Apple's web browser Safari 4 [15] has introduced a new feature called "Top Sites", which visualizes users' favorite web pages as a gorgeous 3D thumbnail wall generated from the downsized snapshot of the web pages.
Although thumbnails [15, 11] and internal images of web pages [7] are adopted by the mainstream companies due to the simplicity, they suffer from several disadvantages. First, for those web pages which have complex layout or rich content, users are difficult to see clearly anything useful from the small thumbnails. Second, although the internal images are proved to be more informative than the thumbnail [12], they are unavailable for a large amount of web pages which do not contain useful images.
To deal with the above mentioned difficulties of internal

499

image based visual summarization, in this paper we propose to mine images from the external for the visual summarization of web pages without suitable internal images, which is so-called external image based visual summarization. Furthermore, we perform a comparative study to investigate the applicability of the different visual summarization approaches to different kinds of web pages and for different tasks including search and re-finding. The different approaches, including thumbnails, internal images, external images, and a recently proposed new scheme called visual snippet [17], are analyzed based on user study, from which some conclusions are drawn which can provide guidance for the practical application of visual summarization for web pages.
The paper is organized as follows. In section 2, we review related work. Three kinds of technology for generating visual summarizations will be introduced in section 3, 4 and 5. Experimental results and user study are reported in Section 6 and 7. At last, we conclude this paper and point our future work in Section 8.
2. RELATED WORK
Thumbnail [18, 15, 6, 3, 5, 19, 20] and internal image [7, 12, 17] based visual summarizations are widely studied in research communities and commonly adopted in existing products.
Thumbnail is a scaled-down image which displays a snapshot of a web page as rendered in the web browser. Due to its simplicity, it is commonly adopted in existing products [18, 15, 6]. Viewzi [18] presents users the search result using not only the text snippets, but also the thumbnail of the web page, for web search. Firefox "FastDial" [6], like Safari 4 [15], visualizes bookmarks as thumbnails. Alongside the widely adoption of visual summarization of web pages in industry, many research efforts have been invested on this problem in the academia as well. Some studies are focused on the design of thumbnail. Cockburn et al. [3] generate thumbnails that consist of reduced images plus graphical icons that indicate bookmarked and frequently visited pages. Dziadosz and Chandrasekar [5] suggest that thumbnails of web pages used along with text snippets in search engine interface can help users in reducing predicting errors, at little time cost in processing time. Woodruff et al. [19, 20] design textually enhanced thumbnails, which enhance the readability of certain parts of the document within the thumbnail and display highlighted keywords transparently overlaid on the reduced document. As a result, the enhanced thumbnail takes advantages of both textual summaries and raw thumbnails. However, in the above applications, raw thumbnail has disadvantages that for those web pages which have rich content, users are difficult to see clearly anything useful from the small thumbnail. Though enhanced thumbnails [3, 19] address this problem to some extent, they are task specific because they only focus on elements relating to specific tasks (e.g., query term in the case of Woodruff et al. [19, 20] or visitation data in the case of Cockburn et al. [3]).
In order to address the problems of thumbnail based summarization, internal image based summarizations are proposed as an alternative approach since internal images usually can describe the content of the hosting web page. In order to improve the relevance judgment of web search results, Li et al. [12] proposes an internal image based scheme which extracts the dominant images from the web page as

"image excerpt". The dominant images are first detected by a trained model based on three levels of image features. And then the most relevant one is selected based on the relationship between the query and the image's surrounding text. Visual Snippet [17], which is an image generated by composing a dominant image from the web page, a watermarked logo, and salient text (e.g. title) from the web page, is an extension of "image excerpts". These two dominant image based approaches are proved to be more informative for search compared to the above thumbnail based approaches. Internal image based summarizations are also adopted by industrial products, such as Google's "Images from the pages" [7]. However, as aforementioned, for a large amount of web pages, useful internal images are unavailable, which limits the applicability of such approaches.
Some studies have involved how different summarizations perform in search and re-finding tasks. Woodruff et al. [20] conduct a study to examine the use of text summaries, raw thumbnails, as well as the enhanced thumbnails in a web search task and find that for some questions, text summaries outperform raw thumbnails while for other questions, raw thumbnails outperform text summaries. Enhanced thumbnails have better and more consistent performance than either text summaries or raw thumbnails. Teevan et al. [17] explore how text summaries, raw thumbnails and visual snippet perform in both search and re-finding tasks. In this paper, we explore not only how different summarizations perform in search and re-finding tasks, but also how different summarizations perform for different kinds of web pages.
3. INTERNAL IMAGE BASED VISUAL SUMMARIZATION
The images contained in a web page are usually used to describe the content of the page, which are so-called internal images and can be directly used to summarize the web page. However, a typical web page may contain a lot of images, most of which are advertisement images, decoration images and logos. Hence we need to detect the most dominant among all the images in a web page for visual summarization. In this paper, we adopt the learning-based algorithm proposed in [21] for dominant image detection, which will be described as follows.
First, we need to extract features for all the images based on the property of the image itself, the relationship of the image to the hosting web page, and the site information, as summarized in Table 1. The details of the features can be found in [21]. Then an image dominance detection model can be learned from some labeled training samples, which are represented as (xi,j, yi,j), where xi,j is the extracted feature vector of the image i in the page j and yi,j is its labeled dominance. Each image for training is assigned an important level, namely 0 (useless), 1 (important) and 2 (highly important). We regard the dominance detection problem as a ranking problem since there are multiple dominance levels. Then we use linear Ranking-SVM [9] to train the ranking model for dominance detection.1
4. VISUAL SNIPPET
Visual Snippet [17] is a recently proposed web page summarization approach which enriches internal dominant im-
1We directly use the trained model in [21]. Thanks to the authors for providing us the valuable model.

500

Table 1: Features for dominant image detection. Feature kind Feature name Image Level size, width/height ratio, blurriness, contrast, colorfulness, photo vs. graphic Page Level relative pos, relative size, relative width/height ratio Site Level IsInnerSite

(a) Dominant Image

(b) Visual Snippet

ages by synthesizing the salient text (e.g. title), dominant image, and logo into a single image. It's designed to capture the respective advantages of textual snippets and internal images.
The visual snippet generation algorithm consists of four steps, which are described as follows.
1. Internal dominant image selection and processing: The dominant image is detected by the algorithm shown in Section 3. Then the detected dominant image is auto-cropped to extract the salient area using the algorithm proposed in [13].

(c) Thumbnail
Figure 1: Dominant Image, Visual Snippet and Thumbnail Sample.

2. Logo extraction: Logo images are detected by a trained model [14] using features including the image's location in the page, size, name and surrounding text). Then the detected logo is scaled to fit within a 120x45 rectangle while preserving the aspect ratio.
3. Title extraction: The title is extracted from the html source of the web page. Then the first 19 characters of the title are cropped based on the study in [10] that the leftmost 15-20 letters of a page's title yield reasonable recognition of the page's site.
4. Image Composition: The logo is made semi-transparent and overlaid on top of the dominant image, and the cropped title is placed above the image and the logo with a white background for readability.
Figure 1 (a) and (b) show an example2 of the dominant image selected by the algorithm in Section 3 and visual snippet generated by the algorithm as aforementioned.
5. EXTERNAL IMAGE BASED VISUAL SUMMARIZATION
The aforementioned summarization methods including internal image and visual snippet both require that there are dominant images in the web page. However, for a large amount of web pages, the dominant images which can represent the information of the web page don't exist. How to generate a meaningful visual summarization for such web pages is a new challenge to the web page summarization problem.
The intuitive idea to address this problem is to retrieve images which can represent visually the web page from the whole Internet. It is so-called external image based approach compared with the internal images discussed before. The idea can be briefly stated as follows. Firstly the key phrases, which can represent the salient topics in the web page, are
2 http://www.expandore.com/product/sony/Proav/broadcast.htm

Web Page

Key Phrase Extraction

Key phrases
Image Search

Most Relevant External Image

Textual Ranking and Visual Filtering

Images & Correspondi ng Webpages

Figure 2: The work flow of the proposed system. The circle boxes denote operations, and the rectangle boxes denote the results after the operations.

extracted. Then the key phrases are used as queries to query the image search engine for the images relevant to the topics of the web page. Thirdly the returned image results are reranked based on the textual similarity between the web page containing the result image and the target web page to be summarized as well as the visual similarity among the result images. Finally the top ranked image are used to summarize the target web page.
Figure 2 illustrates the work flow of our approach.
5.1 Key Phrase Extraction
In this paper we employ the KEX algorithm proposed in [2] to extract the key phrases from web pages.
In KEX, initial term sequences are first extracted by splitting the texts up according to phrase boundaries such as punctuation marks and dashes. Then candidate key phrases are generated using all subsequences of these initial term sequences up to a given length (4 in [2]). After these candidate key phrases are filtered using query logs, two kinds of features, including 3 structured features which are related

501

to the structure of HTML file and 3 text features which are related to text content, are calculated. Finally given the features and user labeled salience score, a model is trained by logistic regression. More details can be seen in [2].
In our experiment, after we calculate the salience score of each candidate phrase, the top 4 key phrases with the highest scores are used as queries to the image search engine. For each key phrase, 30 results including images and corresponding web pages are retrieved for further processing.
5.2 Image Ranking and Filtering
We develop algorithm to rerank and filter images in the key phrase based search result based on two assumptions. First, good candidate images for visual summarization are likely to appear in web pages textually similar to the target page; Second, an image is more important if more images are visually similar to it. Thereafter, in our algorithm, we firstly rank result images based on the textual similarities between the web pages containing the images and the target web page. Then we propose an algorithm to filter out visually unimportant images.
Textual similarity between two web pages is calculated by cosine similarity based on vector space model(VSM). First, TFIDF (Term Frequency Inverse Document Frequency) score is calculated for each term in the web page. a web page is represented as a vector where each component is a TFIDF score of a particular term in the page. Finally cosine similarity [16] is adopted to calculate textual similarity between each result web page and the target web page.
Visual importance of result images is calculated using VisualRank [8]. For each image, we firstly extract local SIFT features which are a 128-dimensional vector. Then the similarity between two images is defined as the number of local features shared between them divided by the average number of local features in the two images. Finally, a graph is constructed with images as vertices and visual similarities as weights on edges. PageRank is applied on the similarity graph and then the resulted rank score vector is employed to represent the importance of each image. The images whose importance score is below a threshold will be filtered out.
Consequentially the relevance of the image to the target web page can be defined as

Sim(i, T W ) =

T I(CWi, T W ), if VRScore>Thresh

0,

otherwise.

where CWi is the ith candidate web page, T W is the target web page and T I(CWi, T W ) is the TFIDF cosine similarity between CWi and T W . V RScore is the importance computed by VisualRank. T hresh is the threshold which is set to the average of V RScore of all of the candidate images in our experiment.

6. EXPERIMENTAL RESULTS
We have conducted numerous experiments to evaluate the performance of various visual summarization approaches. First we introduce the dataset and our experimental setup. Then we evaluate the usefulness of external images of our system based on the labeled data in our dataset. Finally we discuss how various visual summaries perform for different kinds of web pages.

6.1 Experiment Setup
In order to evaluate our system, firstly we randomly selected 100 queries from the query set released by KDD CUP 2005, covering different queries including "Computers", "Entertainment", "Information", "Living", "Online Community", "Shopping" and "Sports". Then we used the Bing Search API [1] to retrieve the top 100 search results for each of the queries. Then the web pages and the included images were crawled for the top 100 results for each of the 100 queries. Finally, our dataset including 8412 retrieved web pages was obtained.
For each web page in our dataset, we downloaded all of the internal images and retrieved external images using the aforementioned algorithm. After all of these images were obtained, we recruited expert labelers to annotate whether the internal images were dominant and the external images were relevant to summarize the corresponding web pages. In the remaining of this paper, the web pages with at least one internal image annotated as dominant are denoted as "DIP". Those without any dominant internal image annotated are denoted as "NDIP". In our dataset, we notice that about 41.8% of the web pages are NDIPs, which demonstrates that internal dominant images are unavailable for a large amount of web pages.
Moreover, in order to compare the different summarization approaches, we generated the four summaries of nearly the same size so that the result is not affected by the different sizes. The settings to generate the four summaries are listed as follows:
1. Thumbnail (the size is of 160x120). As a tradeoff between the amount of information and the ability to be recognized, a thumbnail is created first by clipping a snapshot area of 640x480 from the web page and then resizing to 160x120.
2. Internal Image (the size is of about 160x120 while keeping the aspect ratio). An internal image with the highest dominance score is selected, no matter if it is a dominant image or not.
3. Visual Snippet (the size is of 160x120).
4. External Image (the size is of about 160x120 while keeping the aspect ratio).
After these summaries were generated, we asked the labelers to judge which summary, including the internal image, the external image, the thumbnail, and the visual snippet, is the best to represent the web page. If the labeler thinks all of them cannot summarize the page, he/she can select "others".
6.2 Usefulness of external images
Table 2 shows the proportion of web pages which have at least one external images annotated as relevant in the entire dataset, DIPs and NDIPs, respectively. External images retrieved by our algorithm are labeled as relevant for 53.3% web pages in the entire dataset. Based on this observation, external images can be considered as a useful source for web page summarization.
Moreover, the external images are labeled as relevant for 59.9% of DIPs and 44.0% of NDIPs, respectively. We can see that surprisingly the proportion of web pages for which the

502

Table 2: Proportions of web pages with annotated

external images.

Total Page with Rel- Proportion

evant External

Image

Entire Dataset 8412 4480

53.3%

DIP

4894 2932

59.9%

NDIP

3518 1548

44.0%

Table 3: Statistics of the best summarization. Best Summarization NDIP DIP

Internal

0 44.01%

External

34.51% 14.69%

Thumbnail

36.24% 27.99%

VisualSnippet

2.67% 13.24%

Others

26.58% 0.06%

external images are useful in DIPs is even larger than that in NDIPs. We argue it is because that web pages containing dominant images can be regarded as more appropriate for visual summarization, which, hence, can also be summarized well by a relevant image retrieved from the external. In addition, the external images are useful for 44.0% web pages of NDIPs, which states that the external images are a valuable complement to internal images for visual summarization of web pages.
6.3 Comparison of various visual summaries
Based on the labelers' judges on which summary is the best for each of web pages, we conduct the statistics on the proportion of web pages for which the various summaries are regarded as the best, in DIPs and NDIPs, respectively, as shown in Table 3.
We assume that web pages which have the same kind of best summarizations are likely to have common characteristics. To explore these characteristics, we looked through our labeled data, and we also had an interview with our labelers to know their thoughts about this problem. Finally some conclusions about the summarization problem were drawn from our observation and the labelers' suggestions.
6.3.1 Dominant Image and Visual Snippet as the best summarization
The dominant image can only be adopted for DIP, and it is voted as the best one for 44.01% of DIP. Besides, visual snippet which is also a dominant image based summarization is voted as the best for 13.24% of DIP. Therefore, dominant image based summarizations are the best summarization for over a half DIPs. Summarizing web pages using dominant images is reasonable because dominant images can present the ideas of the web pages and often they are impressive to attract users' attention. However, thumbnail and external image based summarization are voted as the best summarization for 27.99% and 14.69% of DIPs respectively. We will explain why these two summarizations outperform dominant image based summarization in the next two sections.
We also notice that for most web pages of DIP, the visual snippet is worse than the dominant image. This observation seems anti-intuition because a visual snippet contains more

information (title and logo) than a single internal image. To explain this observation, we discussed with the labelers and found that some weakness slightly let down the approbatory of visual snippet. Specifically, the title information sometimes doesn't describe the content of the dominant image and this would mislead the users; more importantly, visual snippet is composed of three kind of components, which seems a little rough-and-tumble. Therefore, though visual snippet is promising for representing web pages, current implementation limits its applicability.
6.3.2 Thumbnail as the best summarization
We explain some characteristics of those web pages which are best summarized by thumbnails. By observing the web pages which can be best represented by thumbnails, we categorize these pages into the following categories including "pages in small size", "pages with several images in snapshot area", "pages with salient and clear image/text in snapshot area", "pages with logo in snapshot area", "pages with simple page structure", and "pages in well-known web site". Among those pages for which thumbnails are not good summaries, most of them contain a large amount of text. Since texts are usually unable to be seen clearly in a thumbnail with small size, it is difficult for users to recognize or recall these pages containing many text in the body.
For 27.99% of DIP, the thumbnail based summarization outperforms all of the other summarizations. Based on our observations, users tend to choose thumbnail as the best summarization for "pages with salient and clear image/text in snapshot area". Since dominant images are contained and can usually be recognized in the thumbnail, users tend to choose thumbnails as better summarizations because thumbnail can contain more information (e.g. structure of web page) than a single dominant image.
For 36.24% of NDIP, the thumbnail based summarization outperforms all of the other summarizations. This is partly because sometimes external images are not that relevant to the target web page. On the other hand, for NDIP, thumbnail based summarization is preferential for "pages with logo in snapshot area" and "pages in well-known web site".
6.3.3 External Image as the best summarization
For 34.51% of NDIP, the external images are voted as the best summarization. For all of these web pages, external images are annotated as relevant by labelers. However, 50.3% of external images are not relevant to web pages which are best summarized by other summarizations. Therefore, the relevance of external image is the crucial factor to select external image based summarization for NDIPs. However, though useful, external images often suffer from reliability problem, i.e., external images retrieved from the web are not always that relevant to the target web pages, which is demonstrated in Section 6.2. For NDIPs, external images with tremendous relevance scores can be adopted as reliable summarizations, otherwise, thumbnails are better, especially for "pages with logo in snapshot area" and "pages in wellknown web site".
For 14.69% of DIP, the external images outperform the internal images. This is because for some web pages about popular topics, there are relevant images across the web. This suggests that external images, as well as internal images, should be uniquely taken into consideration when we

503

calculate the dominance of image based summarizations in our future work.
6.3.4 Summary
As a conclusion, after the comparison of web pages which are best represented by different visual summarizations, we propose several guidance of selecting summarization type here.
For NDIP, external images are useful summarizations, but they also suffer from reliability problem. Thumbnail based summarizations are also good for NDIP, especially for "pages with logo in snapshot area" and "pages in well-known web site".
For DIP, if the dominant images are contained and can be recognized in the thumbnail, thumbnail is the best choice for summarization; otherwise dominant images are more reliable. Besides, for a web page of DIP, if an exclusive dominant image is found and the title information agrees with the content of the dominant image, visual snippet tends to outperform a single dominant image. Actually, an automatic learning algorithm for selecting the best summarization is our future work.
7. USER STUDY
We have conducted a user study to compare how participants use different summarizations to understand and re-find web pages. The goal of this study is to explore how different summarization approaches (thumbnail, internal image, external image and visual snippet) support a understanding or a re-finding task.
7.1 Set-up
51 participants were invited to conduct our user study, all of whom have at least an Bachelor's degree and range in age from 20 to 35 years old. Among them, 56% participants are male. Besides, the participants are all heavy web users, covering a large range of careers, such as computer engineers, researchers, designers, and students.
In this study, the participants were asked to complete a two-phase study. The first phase involves understanding the contents of web pages that have never been visited by the participants. The second phase involves re-finding the web pages that have been visited during the first phase.
The 11 web pages in this study are chosen from a range of different categories, for example amazon for online sales, wikipedia for information provider. 2 of the 11 web pages are NDIPs. A simple descriptive name extracted from the url for each web page is listed below the x axis in figure 3 and 4.
7.1.1 Phase 1: Understanding
In this phase, the participants were asked to perform 11 tasks about the understanding of web pages. For each task, each participant was given a web page and one of the four visual summaries corresponding to the web page. The participants were asked to strictly perform the following two steps. First they guessed the content of the web page based solely on the provided visual summarization without reading the web page. The second step was to read the web page and to rate a score ranging from -1 to 3 based on whether participants have guessed the content of the web page from the visual summary. Specifically, score -1 means that the summarization misleads the participant; score 0 means that the

participant cannot understand the summarization; score 1 means the summarization can help users grasp a rough idea on what the page is about and score 3 means the summarization can help users exactly understand the content of the web page; score 2 is the between of 1 and 3.
We note that the order in which the different summarization methods are provided to users would affect the judges of the subsequent summaries. For example, if a participant sees the internal image first, she may get more from the thumbnail which may include the internal images, so that she would obtain a better understanding of the page from the thumbnail. In order to avoid such a bias, the participants were divided into 4 groups. Each group was provided a single visual summarization type.
7.1.2 Phase 2: Re-finding
In several hours after participants completed phase 1, they were asked to accomplish the second phase of the user study. In this phase, we explored how the various summarization types would affect the recall of the web pages visited in phase 1. Participants were asked to re-find the web page based solely on one visual summary. In this phase, after we specified a web page to be re-found, the participants were asked to select an image, which can remind them of the web page, from an image collection including a single kind of visual summary for all of the web pages.
The participants were also divided into four groups; each of them was exactly the same as that in phase 1. Since a visual snippet is partly based on internal dominant images, we did not supply these two summarization types at the same time. Therefore, two groups were provided visual snippets; the other two groups were provided internal images.
In order to evaluate the performance of each summarization type for this phase, we define the error rate of a summarization type (e(s)) as

I{GuessedW ebpage = DesiredW ebpage|s}

e(s) =

,

N umberof P articipants

where I{GuessedW ebpage = T argetW ebpage|s} is an indicator which equals to 1 when the web page guessed by a participant based on a summarization type (s) is not the desired web page.

7.2 Results and Discussion

7.2.1 Understanding Task
For each summarization method, we average the scores by different participants for each of web pages, respectively. The result is shown in Figure 3, from which we can draw the following conclusions.
From the figure, we can draw several conclusions about the performance of different summarizations for this understanding task.
First, none of the summarization types always perform the best for any kind of web pages. For example, internal images perform well in DIP. However, they get a score below 0 for NDIP (e.g. coldwar3). Therefore, we analyze the performance of summarizations for DIP and NDIP respectively.
For the 9 web pages which are DIP , internal images achieve an average score of 1.62 while the average scores for the thumbnail, the visual snippet, and external image
3http://www.coldwar.org/articles/90s/BorisYeltsin.asp

504

3 2.5
2 1.5
1 0.5
0 -0.5
-1 ClinicBball

group 1 (thumbnail)

group 2 (internal)

Amazon wikipedia cellphone allinlondon sony

group 3 (snippet)

summit

hotel

group 4 (external)

dolls

xbox

Figure 3: Average scores for all of the web pages in phase 1.

coldwar

Table 4: Error rate for various summaries. Summary Error Rate

Internal Image

0.175

Visual Snippet 0.076

Thumbnail

0.143

External Image 0.152

are 1.27, 1.24 and 1.29, respectively. Moreover, for a large majority of DIPs (6 out of 9), dominant image achieves the best result. For the other three DIPs for which internal images are not the best, based on our interview with the participants we find that it is mainly because the participants do not understand the contents of these images due to the limited expertise of the participants on these topics. In conclusion, for DIP, dominant internal images are regarded as the best summarization.
For the 2 web pages which are NDIP in our study, thumbnail and external image based summarizations achieve relatively high average scores of 1.62 and 1.17 respectively, while the average scores for internal image and visual snippet are -0.65 and 0.46 respectively. The average score for internal image illustrates that the non-dominant internal images selected from NDIP would mislead users (e.g. coldwar in the study).
7.2.2 Re-finding Task
We draw the error rates of various summarizations (e(s)) for all of the web pages in Figure 4. From this figure, the error rates of some summarizations (e.g. internal image, thumbnail) vary among different web pages, i.e. this refinding task is also web page dependent. However, we can draw some common conclusions for this specific task.
We average the error rate on all of the web pages for every summarization type and the result is shown in Table 4.
The visual snippet performs better than, or comparably with other summarizations. Especially, the dominant internal images are worse than the visual snippet although the main informative visual part of the visual snippet is the dominant internal image. Thus, the additional two components of the visual snippet: title and logo information are useful for identifying different visited web pages.
The thumbnail based summarization outperforms visual snippet for NDIPs since the visual snippet is generated from non-dominant images in this case. However, it is worse than

visual snippet for most of DIPs, which indicates that although users re-find the web page via visual information, a short but descriptive text (e.g. cropped title) is also important.
The external image based summarization is not suitable for this task. Intuitively, an image which the users do not see in the target web page before will not recall the users of the page. Besides, external images suffer from reliability problem, i.e., retrieved external images are sometimes not that relevant to the web pages. Therefore, external image based summarization is a useful complementary for NDIPs in understanding task, however, it is not suitable for refinding task, even for NDIPs.
8. CONCLUSION AND FUTURE WORK
In this paper, we present a careful study on comparing the various visual summarization methods for web pages, including thumbnails, internal images, and visual snippets. An extension to internal image based visual summarization is proposed, which selects the representative images not from within the web page, but from the external entire Internet.
The experimental results and user studies suggest that the various summarization approaches have respective advantages on different types of web pages and for different tasks. While thumbnails are simple and effective for web pages with simple structure or clear image and text with large size, internal images can summarize the web page well if it includes dominant images. The external images are a useful source for those web pages without dominant images and can be regarded as a valuable complement to internal images. Visual snippets are extremely effective in helping users identifying the visited web pages in the re-finding task.
The above conclusions suggest two future work. First, since the internal and external images have respective advantages, we will develop an integrated framework which does not select the internal images and external images separately, but selects the representative images from a combined pool of internal and external images. Second, we can develop an algorithm to automatically select the most appropriate visual summaries from the four types of candidates based on the type of the web page and the tasks for which the summaries are used, so that the user satisfaction is improved over each of the individuals.
9. REFERENCES
[1] Bing API. http://www.bing.com/developers.

505

thumbnail (51 participants)

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0 ClinicBball

Amazon

wikipedia

internal (27 participants)

snippet (24 participants)

cellphone allinlondon

sony

summit

hotel

external (39 participants)

dolls

xbox

coldwar

Figure 4: Error rate for all of the web pages in Phase 2.

[2] M. Chen, J.-T. Sun, H.-J. Zeng, and K.-Y. Lam. A practical system of keyphrase extraction for web pages. In CIKM '05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 277­278, New York, NY, USA, 2005. ACM.
[3] A. Cockburn, S. Greenberg, B. McKenzie, M. Jasonsmith, and S. Kaasten. Webview: A graphical aid for revisiting web pages, 1999.
[4] A. Cockburn and B. Mckenzie. What do web users do? an empirical analysis of web use. International Journal of Human-Computer Studies, 54:903­922, 2000.
[5] S. Dziadosz and R. Chandrasekar. Do thumbnail previews help users make better relevance decisions about web search results? In SIGIR '02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 365­366, New York, NY, USA, 2002. ACM.
[6] FastDial. https://addons.mozilla.org/en-us/firefox/addon/5721.
[7] Google. http://www.google.com.
[8] Y. Jing and S. Baluja. Visualrank: Applying pagerank to large-scale image search. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1877­1890, May 2008.
[9] T. Joachims. Optimizing search engines using clickthrough data. In KDD '02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133­142, New York, NY, USA, 2002. ACM.
[10] S. Kaasten, S. Greenberg, and C. Edwards. How people recognize previously seen web pages from titles, urls and thumbnails. In Proceedings of Human Computer Interaction, pages 247­265, 2001.
[11] T. Kopetzky and M. Mu¨hlh¨auser. Visual preview for link traversal on the world wide web. In WWW '99: Proceedings of the eighth international conference on World Wide Web, pages 1525­1532, New York, NY, USA, 1999.

[12] Z. Li and L. Zhang. Improving relevance judgment of web search results with image excerpts. In WWW '08: Proceedings of the 17th international conference on World Wide Web, pages 21­30, April 2008.
[13] Y.-F. Ma and H.-J. Zhang. Contrast-based image attention analysis by using fuzzy growing. In MULTIMEDIA '03: Proceedings of the eleventh ACM international conference on Multimedia, pages 374­381, New York, NY, USA, 2003. ACM.
[14] T. Maekawa, T. Hara, and S. Nishio. Image classification for mobile web browsing. In WWW '06: Proceedings of the 15th international conference on World Wide Web, pages 43­52, New York, NY, USA, 2006. ACM.
[15] Safari4. http://www.apple.com/safari.
[16] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513­523, 1988.
[17] J. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, G. Ramos, P. Andr´e, and C. Hu. Visual snippets: summarizing web pages for search and revisitation. In CHI '09: Proceedings of the 27th international conference on Human factors in computing systems, pages 2023­2032, New York, NY, USA, 2009. ACM.
[18] Viewzi. http://www.viewzi.com/search/webscreenshot.
[19] A. Woodruff, A. Faulring, R. Rosenholtz, J. Morrsion, and P. Pirolli. Using thumbnails to search the web. In CHI '01: Proceedings of the 19th international conference on Human factors in computing systems, pages 198­205, New York, NY, USA, 2001. ACM.
[20] A. Woodruff, R. Rosenholtz, J. B. Morrison, A. Faulring, and P. Pirolli. A comparison of the use of text summaries, plain thumbnails, and enhanced thumbnails for web search tasks. J. Am. Soc. Inf. Sci. Technol., 53(2):172­185, 2002.
[21] Q. Yu, S. Shi, Z. Li, J.-R. Wen, and W.-Y. Ma. Improve ranking by using image information. In ECIR'07: Proceedings of the 29th European conference on IR research, pages 645­652, 2007.

506

Ranking using Multiple Document Types in Desktop Search

Jinyoung Kim and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science
University of Massachusetts Amherst
{jykim,croft}@cs.umass.edu

ABSTRACT
A typical desktop environment contains many document types (email, presentations, web pages, pdfs, etc.) each with different metadata. Predicting which types of documents a user is looking for in the context of a given query is a crucial part of providing effective desktop search. The problem is similar to selecting resources in distributed IR, but there are some important differences.
In this paper, we quantify the impact of type prediction in producing a merged ranking for desktop search and introduce a new prediction method that exploits type-specific metadata. In addition, we show that type prediction performance and search effectiveness can be further enhanced by combining existing methods of type prediction using discriminative learning models. Our experiments employ pseudodesktop collections and a human computation game for acquiring realistic and reusable queries.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: [Information Search and Retrieval]
General Terms
Algorithms
Keywords
Information Retrieval, Desktop Search, Semi-structured Document Retrieval, Type Prediction, Human Computation Game
1. INTRODUCTION
People have many types of documents on their desktop with different sets of metadata for each type. For instance, emails have sender and receiver fields, whereas office documents have filename and author fields. Considering that personal information is now increasingly spread across various places on the web, this diversity of document types is continuing to increase.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10 July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-512-3/09/11 ...$10.00.

Desktop search systems, which is now a standard feature of most platforms, have tried to exploit this type information by presenting search results for each document type separately (e.g., Apple Spotlight) or showing type information distinctively in a single ranked list (e.g., Google Desktop).
In both scenarios, a critical aspect of the system is being able to predict which type(s) of document(s) a user is looking for given a query. If the system displays separate type-specific results, it can rank them by their type scores. Alternatively, the system can incorporate type scores into document ranking as a feature.
The type prediction problem bears some similarity to the vertical or resource selection problem in aggregated or federated search in that the system tries to score the results from each vertical, resource, or collection based on predicted relevance for a given query. In this sense, all these problems can be put in a broad category of collection scoring. There are, however, several notable differences.
First, type-specific sub-collections in the desktop are cooperative in that all the documents are available to a single system. This means that sampling techniques used for federated search may not be necessary for desktop search; second, unlike typical collections used for aggregated search, the sub-collections in the desktop environment are small and have considerable topical overlap. This makes it challenging to apply content-based collection scoring techniques (e.g., CORI [4]) directly; third, each sub-collection in the desktop has unique metadata that has not been exploited in existing collection scoring methods.
The main goal of this paper is to show how the retrieval effectiveness of a desktop search system can be enhanced by improving type prediction performance. We focus on knownitem queries, which are the most frequent type of request in the desktop environment [8], and assume that the system displays a final rank list by merging type-specific results.
Our work makes several contributions: first, we demonstrate the impact of sub-collection retrieval and type prediction performance on the quality of the final rank list; second, we suggest a type prediction method that exploits type-specific metadata and show that the new method has better performance than a state-of-the-art collection scoring method; third, we find that a combination of collection scoring methods can improve the performance further; fourth, we employ a game interface to collect a large quantity of known-item queries in a reasonably realistic setting.
The rest of this paper is organized as follows. We provide an overview of related work in the next section. Then we introduce the retrieval model, type prediction methods,

50

Email Webpage

-Rank List -Type Score
-Rank List -Type Score

Final Rank List

Office File

-Rank List -Type Score

Figure 1: Suggested retrieval model for desktop search.

and test collection generation methods we used. In the experiments, we report retrieval and type prediction performance using pseudo-desktop collections and a computer science (CS) collection where queries are collected using a game interface.
2. RELATED WORK
Related work can be found in several different areas: desktop search, vertical search, and distributed IR.
For desktop search, people have studied user interface issues [7], retrieval models, and evaluation methods [10]. Among these, Thomas et al. [19] views desktop search as a meta-search problem where the results from many servers are merged. They compared several server selection methods using documents collected from various sources, concluding that a selection method based on Kullback-Leibler divergence [18] performed the best. Our work extends this work by suggesting a prediction method that exploits the field structure and a combination method whose performance can be improved by interaction with the user.
To evaluate desktop search, methods for building test collections [5] [10] have been proposed. Among these, the pseudo-desktop method by Kim et al. [10] generated test collections automatically by simulation, based on a technique suggested by Azzopardi et al.[3]. Our work employs these pseudo-desktop collections for the retrieval experiments and improves the procedure of gathering human-generated queries by employing a game interface.
In the context of aggregated search and distributed IR, researchers have proposed many methods of scoring collections against a given query. Approaches such as CORI [4] and KL-Divergence [18] treat collections as large documents and apply document scoring techniques for scoring collections. Other methods, such as ReDDE [17] model the distribution of relevant documents for each collection. Recently, Arguello et al. proposed a classification approach [1] [2] where many sources of evidences can be combined for mapping a user's query into one or more collections. Our combination approach is similar to this work but we use features and evaluation methods more suitable for our problem domain.
3. RETRIEVAL MODEL
In this section we introduce a retrieval model for desktop search. In our retrieval model, as depicted in Figure 1, typespecific results (rank list and type score) are merged into a final rank list. We first explain methods used for the retrieval of sub-collections corresponding to each file type. Then we introduce the type prediction and result merging methods we used.

The following notation will be used throughout this paper. We assume that a query Q = (q1, ..., qm) is composed of m words and each collection C contains documents with n field types (F1, ..., Fn) where n can be different for each collection. Each document d in the collection may include fields (f1, ..., fn), where each field is marked using lowercase letters to distinguish it from the corresponding field type in the schema. Model-specific parameters will be explained as they appear.
3.1 Type-specific Retrieval
The first step in our retrieval model is ranking documents from each sub-collection. Since our focus is on type prediction, we employ retrieval models used in the recent work by Kim et al. [10] on desktop search, which includes document query-likelihood (DLM), the probabilistic retrieval model for semistructured data (PRM-S) and the interpolation of DLM and PRM-S (PRM-D). We explain the PRM-S model in the following section.
3.1.1 Probabilistic Retrieval Model for Semi-structured Data
The probabilistic retrieval model for semistructured data (PRM-S) [11] scores documents by combining field-level querylikelihood scores similarly to other field-based retrieval models [13]. The main feature of the PRM-S model is that weights for combining field-level scores are estimated based on the predicted mapping between query terms and document fields, which can be efficiently computed based on collection term statistics.
More formally, using Bayes' theorem, we can estimate the posterior probability PM (Fj|w) that a given query term w is mapped into document field Fj by combining the prior probability PM (Fj) and the probability of a term occurring in a given field type PM (w|Fj).

PM (Fj |w)

=

PM (w|Fj )PM (Fj )

P
Fk F

PM (w|Fk)PM (Fk)

(1)

Here, PM (w|Fj) is calculated by dividing the number of occurrences for term w by total term counts in the field Fj across the whole collection. Also, PM (Fj) denotes the prior probability of field Fj mapped into any query term before observing collection statistics.
With the mapping probabilities estimated as described above, the probabilistic retrieval model for semistructured data (PRM-S) can use these as weights for combining the scores from each field PQL(w|fj) into a document score, as follows:

mn

YX

P (Q|d) =

PM (Fj |qi)PQL(qi|fj )

(2)

i=1 j=1

This model was shown to have better performance than other field-based retrieval methods, such as the mixture of field language models [13] and BM25F [15], for a semi-structured document retrieval task using the IMDB [11] and TREC email [10] collections.

3.2 Type Prediction
As briefly mentioned in the introduction, an integral part of our retrieval model is the type prediction component, which scores each collection given a user query. The type

51

prediction method will produce scores for each sub-collection and these scores can be used for merging results into the final rank list or ranking sub-collection results. Section 4 deals with type prediction methods in detail.
3.3 Result Merging
With type-specific rank lists from sub-collection retrieval and collection scores from the type prediction component, we can produce the final rank list by rank-list merging algorithms. In this work, we use the well-known CORI algorithm for merging [4].

Ci = (Ci - Cmin)/(Cmax - Cmin)

(3)

D = (D - Dmin)/(Dmax - Dmin)

(4)

D = D + 0.4 · D · Ci

(5)

1.4

Here, Ci and D are normalized collection and document score, computed using the maximum and minimum of col-
lection scores (Cmax / Cmin) and document scores (Dmax / Dmin), respectively. Given Ci and D , the final document score D can be computed by combining these two scores.

4. TYPE PREDICTION METHODS
In this section, we introduce our type prediction methods in detail. We first describe existing methods for type prediction which are adopted from recent works on aggregated and federated search [1] [2]. Then we introduce a new type prediction method that exploits document metadata. Lastly, we explain how type prediction methods can be combined using several learning methods.
4.1 Existing Methods for Type Prediction
4.1.1 Query-likelihood of Collection
Many traditional resource selection methods (e.g. CORI) are computed from collection term statistics. Among these, we use collection query-likelihood (CQL)[18], which is a resource selection method based on the language modeling approach. The approach here is to collapse all documents in each collection into one giant `document' and use the querylikelihood score for the document as the collection score:

Y CQL(Q, C) = (P (q|C) + (1 - )P (q|G)) (6)
qQ
Here, C is the language model of each sub-collection and G is the language model of the whole collection. The smoothing parameter  adjusts the interpolation ratio of P (q|C) and P (q|G). CQL was shown to be the most effective among resource selection methods in a recent evaluation [19].
4.1.2 Query-likelihood of Query Log
Another source of evidence for the type prediction is the aggregated query terms used for finding documents that belong to each sub-collection. As done in previous work [1] [2], we use the query-likelihood score of the language model (QQL) built by queries targeted for sub-collection C as shown below:

Y QQL(Q, C) = (P (q|LC ) + (1 - )P (q|LG)) (7)
qQ

Here, LC is the language model of the query log corresponding to collection C. LG is similarly defined using the query log across all collections.
4.1.3 Geometric Average
Another class of resource selection methods combine the score of top documents to evaluate each collection given the user query. Seo et al. [16] proposed using the geometric mean of the top m documents as the combination method,

Y

1

GAV G(Q, C) = (

P (Q|d)) m

(8)

dDtop

where Dtop is the set of top m documents from the collection and the score P (Q|d) is padded with Pmin(Q|d) if fewer than m documents are retrieved.

4.1.4 ReDDE
ReDDE [17] [2] scores a target collection based on the expected number of documents relevant to the query. Although previous work used a centralized sample index to derive this expectation, we can estimate this directly from the target collection,

X

ReDDE(Q, C) =

P (Q|d)

(9)

dDtop

which is equivalent to using the sum of the top document scores belonging to each collection. Intuitively, this results in a higher score for the collection with more documents in higher positions.

4.1.5 Query Clarity
So far, most of our methods have been derived from resource selection techniques developed in the context of distributed IR. Query performance prediction methods can also be used for type prediction by assigning a higher score for the collection with higher predicted performance. Among such methods, we employ Query Clarity [6], which predicts performance using the KL divergence between a query language model and a collection language model.

Clarity(Q, C)

=

X

P

(w|LQ

)log2

P (w|LQ) P (w|C)

(10)

wV

Here, query language model LQ is estimated from the top m documents from the collection.

4.1.6 Dictionary-based Matching
In some cases, users provide direct clues about which file type they intended to search, by including terms such as `sender'(for email), `pdf'(for office document) or `www'(for webpage). Although these terms may not occur in a majority of queries, they can be a strong indication of type membership for a given query. We built the dictionary for each sub-collection by using the names of the collection and metadata fields.

4.2 Using Document Metadata Fields for Type Prediction
Although some of methods introduced above use the collection term statistics, none use the field structure of documents available for desktop collection. Considering that

52

the retrieval effectiveness of semi-structured document collections have been improved by exploiting this structure [11], we can expect similar benefits for the type prediction problem.
Field-based collection query likelihood (FQL) ­ our new method for type prediction ­ extends the collection query likelihood model for collection scoring by combining the querylikelihood score for each field of the collection instead of using the score for the whole collection. In other words, if we borrow the view of query-term and field mapping described in Section 3.1.1, we try to infer the mapping between a user query and each collection by combining mapping probabilities for the fields of each collection.
More formally, for a collection C that contains documents of n field types (F1, ..., Fn), we can combine the language model score of each field as follows:

F QL(Q, C) = combFiC (P (q|Fi))

(11)

Here, Fi is a smoothed language model of the ith field of the collection and comb can be any function that can combine n numbers into one. We experimented with many variations of comb function and found that arithmetic mean gives the best performance.

4.3 Combining Type Prediction Methods
Considering that the type prediction methods introduced so far are derived from different sources, it is plausible that we can get further performance benefits by combining individual methods in a linear model where weights are found using learning methods. In this section, we describe three learning methods with different objective functions: grid search of parameter values, a multi-class classifier and a rank-learning method.

4.3.1 Grid-search of Parameter Values
Since we have only seven features to be combined, It is feasible to perform a grid search of parameter values that maximize the performance of a training set of queries. Specifically, we can find the optimal value for each parameter in turns while fixing the values of the parameters previously found, and repeating the whole procedure until we reach convergence. In searching for the optimum value of each parameter, we employed Golden Section Search[14].

4.3.2 Multi-class Classification
Given that we want to predict one of k document types for a given query, this is typical multi-class classification scenario where each type corresponds to a class. Among many choices of such methods, we used a one-vs-rest (OVR) support vector machine classifier (MultiSVM) available in Liblinear Toolkit1. Since the output of this classifier is not suitable to be used directly as type scores, we used a simple linear transform to convert the scores into probabilities.

4.3.3 Rank-learning Method
Alternatively, one can cast the type prediction task as a ranking problem where we try to rank the relevant collections higher than non-relevant collections. This approach can be especially beneficial for the case where the user is finding multiple documents with different types, since such
1http://www.csie.ntu.edu.tw/ cjlin/liblinear/

situation is hard to model with typical multi-class classification methods. RankSVM [9] was used as the rank-learning method.
5. TEST COLLECTION GENERATION METHODS
Evaluation of desktop search has been considered a challenging issue due to the private nature of collections. As a solution, methods for building reusable test collections have emerged recently [5] [10]. Here we review the test collection generation methods and introduce our game interface for gathering known-item queries.
5.1 Pseudo-desktop
Kim et al. [10] introduced the pseudo-desktop method of automatically building a reusable test collection for desktop search. They collected documents with similar characteristics to a typical desktop environment and generated queries by statistically taking terms from each of the target documents. The generated queries are validated against a set of manually written queries, which were gathered by showing people documents and asking them to write queries for finding those documents.
Although this pseudo-desktop method provides a costeffective way to generate a large quantity of test data under perfect experimental control, there are several limitations: first, the query generation procedure is too simple in that it independently chooses words from the target document, while real queries contain phrases; second, only terms in target documents are used as queries; third, the procedure of getting manual queries is not realistic in that people were asked to formulate queries for documents they were not familiar with.
It should be possible to mitigate the problems above by refining the query generation method, but we decided instead to improve the procedure of gathering manual queries by developing a human computation game, as explained in the next section.
5.2 DocTrack Game
Human computation games [20] have recently been suggested as method for getting a large amount of human annotations in a way that motivates participants using a gamelike setting. In the context of IR research, Ma et al. [12] introduced Page Hunt, which is a game designed to collect web search log data by showing each participant webpages and asking her to find them with the search interface provided.
By adapting Page Hunt to our problem domain, we developed the DocTrack game for gathering known-item queries in the desktop environment, as shown in Figure 2. In addition to using documents of many types that might be found in a desktop instead of random webpages, we made several modifications to the original Page Hunt game: first, since people generally have good knowledge of their own desktops, we collected documents that participants are familiar with and let each of them browse the collection for some time before starting the game; second, to simulate a typical known-item search scenario, we showed participants multiple documents and asked them to find one of them without specifying which one is the target document; third, we used a document viewer that can show documents of any types

53

Figure 2: The screenshot of the DocTrack game. The user is being shown a document.
(e.g. pdf, doc and ppt) in the same way they are seen on the desktop.
Compared to the method of collecting manual queries in Kim et al.[10], using the DocTrack game, we could gather many more realistic queries together with the whole session log data. This in turn allowed us to perform the training of discriminative learning models which typically requires large amounts of training data.
6. EXPERIMENTS
In this section we describe the experiments for verifying the type prediction and retrieval performance. We used three pseudo-desktop collections with generated queries for the first experiment, where we compared several type prediction methods and showed the impact of type prediction on the final ranking. We then report on experiments using a computer science (CS) collection where queries were collected by the DocTrack game.
In the indexing of both collections, each word was stemmed using the Krovetz stemmer and standard stopwords were eliminated. Indri2 was used as a retrieval engine for all the retrieval experiments. We used prediction accuracy to evaluate type prediction performance, since we have only one correct collection for each query. Mean Reciprocal Rank was used as the measure of retrieval performance for all experiments, since this is a known-item task where each query has only one relevant document.
Four retrieval methods were used for each sub-collection (DLM / PRM-S / PRM-D / Best) and four methods (Uniform / CQL / FQL / Oracle) were used for type prediction. We compared only CQL and FQL for the pseudo-desktop experiment since CQL was shown to be the most effective among collection scoring method [19] and FQL is the extension of CQL for semi-structured document collections. Section 6.2 provides the comparison with other type prediction methods using the CS collection and queries from the DocTrack game.
For the Best retrieval method, we used the retrieval method with the best aggregate performance for each sub-collection,
2http://www.lemurproject.org

Table 1: Number and average length of documents

for pseudo-desktop collections.

Type

Jack

Tom

Kate

#Docs Len. #Docs Len. #Docs Len.

email 6067 555 6930 558 1669 935

html

953 3554

950 3098

957 3995

pdf

1025 8024 1008 8699 1004 10278

doc

938 6394

984 7374

940 7828

ppt

905 1808

911 1801

729 1859

Table 2: Query examples with corresponding target

collections for pseudo-desktop collections.

Query

Target Collection

jose 03 kahan email

presentation 19 ppt

org address

html

making the assumption that the best-performing retrieval method is known in advance. For Uniform and Oracle collection scoring, we considered that each collection has the same chance of containing the relevant document (Uniform) or that we have the perfect knowledge of the collection that contains the relevant document (Oracle).
6.1 Pseudo-desktop Collections
Three pseudo-desktop collections described in Kim et al. [10] were used for these experiments. Each collection contained typical file types such as email, webpage and office documents related to three individuals, as shown in Table 1. For email, the indexed fields were title, content, date, sender and receiver. For the other document types, title, content, date, URL, summary were indexed.
To generate queries, we first chose the target document and took each query word based on the term frequency in a randomly chosen field of the document. For instance, the query `org address' shown in Table 2 was generated by taking the term `org' from the URL field and `address' from the title field, respectively. Kim et al. reported that queries generated using this method have the highest similarity to a set of manual queries.
For each experiment, we generated 50 queries of average length 2 where target documents were taken from each subcollection in proportion to the number of documents it contains. Table 2 shows several queries we used. Lastly, all the experiments were repeated three times since the query generation procedure involves some randomness.
6.1.1 Prediction Accuracy
In Table 3, we compare the accuracy of type prediction in pseudo-desktop collections for the CQL and FQL methods, where FQL shows a clear improvement over CQL method. Although this result should be interpreted with some reservations because we are using simulated queries, the same trend was found in the experiment using manual queries. We also observe that both methods show reasonable performance in the Jack and T om collections, which contain far more email documents than other types. From this, we can conclude that both methods are relatively robust against an imbalance of sub-collection sizes.
6.1.2 Retrieval Performance
We now report the retrieval performance for the same

54

Table 3: Accuracy of type prediction in pseudodesktop collections.
Jack Tom Kate CQL 0.606 0.637 0.38 FQL 0.773 0.807 0.64

Table 5: Number and average length of documents

in a computer science (CS) collection.

Type

#Docs Length

email

851

731

news article

170

352

calendar item

354

306

webpage

4727

539

office document 1887

357

queries in Table 4. The first noticeable trend is that both the choice of type-specific retrieval model and type prediction method has a big impact on the final result. Especially, Oracle type prediction was much better than the FQL method, which in turns outperformed CQL across all collections. On the other hand, the Best retrieval method was not much better than the PRM-D and PRM-S methods.
6.2 CS Collection
Next we report on experiments using a computer science (CS) collection, where the documents of various types are collected from many public sources in the Computer Science department the authors belong to. As shown in Table 5, the CS collection contained emails from the department mailing list, news articles and blog postings on technology, calendar items of department announcements, webpages and office documents crawled from the department and lab websites. The documents in the CS collection are much shorter than in the other pseudo-desktop collections.
For all document types, title and content fields were indexed. Also, there were type-specific fields such as date, sender and receiver for email, tag and author for news articles, starttime and location for calendar items, URL for webpages, filename for office documents.
We used the DocTrack game for collecting queries. We had 20 participants who were students, faculty members and staff in the department, all familiar with the documents in the collection. In total, 66 DocTrack games were played and 984 queries were collected using 882 target documents, some of which are shown in Table 6.
The average length of queries was 3.97, which is longer than the reported length (2) in the other desktop search studies [7]. This may be due to people paying more attention to the task in the competitive game setting compared to typical desktop search. The standard deviation (1.85) of the query length was also quite high, implying that there is a considerable variation among the querying behavior of individuals.
The participants were generally in favor of the game, saying that playing the game was fun and felt reasonably similar to their search experience in the desktop.
Since some of features required data for estimation, we used 528 queries to obtain query-log feature (QQL) values and training parameters for other features. The rest (456) of the queries were used to evaluate the type prediction performance of features and combination methods by 10-fold

Table 6: Query examples with corresponding target

collections for a CS collection.

Query

Target Collection

reminder jeffrey johns email

2010 candidate weekend calendar item

yanlei xml dissemination office document

cs646 homework html html

Table 8: Significance test result for type prediction

accuracy in a CS collection. Each cell shows the p-

value of paired t-test between the accuracy of two

methods. Method CQL FQL Grid RankSVM MultiSVM

CQL

0.03 0.00

0.00

0.00

FQL

0.69

0.27

0.02

Grid

0.41

0.02

RankSVM

0.07

cross-validation. For the retrieval experiments, since many queries did not return any documents, we used only queries where the relevant document was ranked in the Top 50 result set during the game.
6.2.1 Prediction Accuracy
Table 7 summarizes the prediction accuracy result, comparing two of the best-performing single feature runs (CQL / FQL) and combination methods (Grid / RankSVM / MultiSVM). The result shows that all the combination runs improved performance over the best single feature runs given by FQL, which outperformed CQL in this collection as well. MultiSVM was shown to be the most effective among combination methods. This is understandable considering that we had one target collection for each query, which is a natural setting for multi-class classification. RankSVM was slightly better than Grid but the difference was not significant.
The result of a significance test is reported in Table 8, which shows that the performance differences between CQL and all the other methods are significant with a p-value of 0.05 and that MultiSVM outperforms all the other methods significantly with a p-value of 0.1 (using paired t-test). Overall, this means that suggested type prediction method (FQL) improves the performance of the CQL method and that the combination of features improves the performance further.
6.2.2 Retrieval Performance
Table 9 shows the retrieval performance, comparing four retrieval methods (DLM / PRM-S / PRM-D / Best) and the same set of type prediction methods as above in addition to Oracle and Uniform methods.
The result mostly shows the same trends as the pseudodesktop collections despite the big difference in experimental conditions (remember that queries were algorithmically generated for the pseudo-desktop collections). FQL was better than CQL and all the combination methods outperformed CQL and FQL significantly (with a p-value of 0.05 using paired t-test).
The only exception is that the performance of MultiSVM was slightly worse than RankSVM. Given the superior prediction accuracy of MultiSVM, it seems that the procedure of converting the SVM output into the type prediction score caused some problems. We can also see that Oracle type prediction method and the Best retrieval method outperform

55

Table 4: Retrieval performance in three pseudo-desktop collections using different type-specific retrieval

methods and type prediction methods.

Uniform

Jack CQL FQL Oracle Uniform

Tom CQL FQL Oracle Uniform

Kate CQL FQL

Oracle

Average

DLM

0.129 0.159 0.27 0.331

0.104 0.123 0.192 0.224

0.126 0.12 0.237 0.294

0.194

PRM-S

0.152 0.212 0.326 0.403

0.15 0.209 0.289 0.348

0.232 0.239 0.383 0.532

0.346

PRM-D

0.148 0.219 0.335 0.403

0.155 0.204 0.289 0.346

0.25 0.245 0.387 0.538

0.355

Best

0.154 0.225 0.336 0.414

0.157 0.217 0.302 0.361

0.241 0.245 0.388 0.542

0.354

Average

0.146 0.204 0.317 0.388

0.141 0.188 0.268

0.32

0.212 0.212 0.349 0.477

Table 7: Accuracy of type prediction for best-performing single feature runs and combination methods in a

CS collection.

Method CQL FQL Grid RankSVM MultiSVM

Accuracy 0.708 0.743 0.747

0.758

0.808

Table 11: Correlation among the prediction per-

formance of features. Each cell shows the pearson

correlation coefficient between the accuracy of two

methods. Feature FQL Dict QQL Clarity GAVG ReDDE

CQL

0.68 0.10 0.29 -0.03 -0.01

0.04

FQL

0.05 0.32 0.01 -0.01

0.01

Dict

0.28 0.02 -0.00

0.04

QQL

-0.02 0.04

0.05

Clarity

0.17

0.05

GAVG

0.62

other methods, which leaves room for further improvement in both aspects.
6.2.3 Leave-one-out Prediction Accuracy
For further analysis on the effectiveness of individual features, we next report the result of single-feature and leaveone-out experiments in Table 10, where we used only one feature (single-feature) or all but one feature (leave-one-out) to measure the performance.
All in all, features with higher single-feature performance had bigger impacts when they were omitted, as shown in the case of content-based features (CQL / FQL) and query log feature (QQL). On the other hand, features based on the initial retrieval result (GAVG / ReDDE) was shown to be ineffective. Presumably, this is because the documents of different types had different characteristics in this collection, which make the scores almost incomparable.
On the other hand, in some cases, low single-feature prediction accuracy did not necessarily translate into small difference in the leave-one-out experiment. The dictionarybased feature (Dict) and the performance prediction feature (Clarity) were shown to have a high impact on combination performance despite having the lowest single-feature results.
The result above makes more sense when considered together with the correlation among features in Table 11. Methods based on collection term statistics (CQL / FQL) and top result set (GAVG / ReDDE) have high correlations within the group, which explains why performance does not suffer much when one of high-performing features (e.g. CQL and FQL) are omitted. On the other hand, that QQL does not correlate highly with any other features explains its high impact on the leave-one-out experiment.
6.2.4 Personalization of Feature Combination
In the experiments so far we have used the queries from all participants together. Another benefit of the feature combination approach is that it can adjust the result based on the querying behavior of each individual. For instance, if

Table 12: Feature weights trained using queries

written by each user. The last row shows the

weights trained using all queries. All the names are

anonymized.

User ID Jane

WCQL WF QL WDict WQQL WClarity

1

1 0.15

1

0.09

Yao

0.38 0.56 0.15

1

0

Rajiv

0.58

1

0

0

0

Tim

0.62

1

0

1

0.18

David

1

1

0

0

0

All Users

1

1

0

1

0.09

a user frequently includes terms that indicate a particular collection, the learning method can improve type prediction performance by assigning higher weight to the Dict or QQL features.
To explore this potential value of this personalized type prediction, we tried training feature combination methods only with queries written by each user and looked at how much variation it causes for resulting feature weights.
The results in Table 12 compare the weights trained for each user. It shows that content-based features (CQL / FQL) are not very helpful for some users (Yao), whereas the query log does not make a difference for others (Rajiv / David). Although this is a preliminary result, the weights show considerable variation, implying that personalized training data produced different results for each user. We leave further analysis of this benefit of personalization for future work.

7. CONCLUSION & FUTURE WORK
In this paper, we suggest a retrieval model for desktop search where type-specific retrieval results are merged into a final rank list based on type prediction scores. Using the pseudo-desktop and CS collections, we demonstrated that improving the type prediction method can produce significantly better final retrieval performance.
We also introduced field-based collection query likelihood (FQL) ­ a new type prediction method that exploits typespecific metadata ­ which shows superior performance to competitive baseline methods in both collections we tested. Although FQL is used in the context of desktop search, it can be used as a resource selection method for aggregated or federated search, as long as each document has field structure.
Moreover, we developed a human computation game for collecting queries in a more realistic setting and used this data to train discriminative learning models that combines type prediction methods as features. Our results show that

56

Table 9: Retrieval performance in a CS collection using different type-specific retrieval methods and type

prediction methods.

Uniform CQL FQL Grid RankSVM MultiSVM Oracle Average

DLM

0.343 0.507 0.53 0.552

0.563

0.556 0.674 0.526

PRM-S

0.349 0.501 0.518 0.518

0.551

0.547 0.674

0.52

PRM-D

0.36 0.518 0.536 0.536

0.567

0.564 0.694 0.537

Best

0.372 0.548 0.564 0.590

0.596

0.594 0.72 0.563

Average

0.356 0.518 0.537 0.549

0.569

0.565 0.691

Table 10: Accuracy of type prediction for single-feature and leave-one-out experiment. Leave-one-out accu-

racy shows the percentage of decrease for leave-one-out experiment from the experiment using all features.

Method

CQL FQL Dict QQL Clarity GAVG ReDDE

Single-feature Accuracy 0.708 0.743 0.201 0.579 0.240 0.255 0.207

Leave-one-out Accuracy -0.6% -1.7% -0.6% -3.1% -0.6% -0.0% -0.0%

the combination method can improve type prediction performance compared to any of existing methods. We also explored the benefit of personalizing type prediction results, which may potentially improve the performance further.
Our work leaves many interesting challenges. Although this work showed the value of improving type prediction performance, better type-specific retrieval and result merging algorithms should bring further performance gains. For example, we used only textual features for sub-collection retrieval, and it should be possible to incorporate type-specific features (e.g. PageRank score for webpage).
Also, although we focused on the known-item search task in this paper, the retrieval model suggested here will be equally applicable for the ad-hoc search scenario. To gather training data for this scenario, the DocTrack game can be modified to gather ad-hoc queries using topic descriptions and corresponding sets of relevant documents.
8. ACKNOWLEDGEMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0707801. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
9. REFERENCES
[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. In CIKM '09, pages 1277­1286, New York, NY, USA, 2009. ACM.
[2] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. Sources of evidence for vertical selection. In SIGIR '09, pages 315­322, New York, NY, USA, 2009. ACM.
[3] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated queries for known-item topics: an analysis using six european languages. In SIGIR '07, pages 455­462, New York, NY, USA, 2007. ACM.
[4] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In SIGIR '95, pages 21­28, New York, NY, USA, 1995. ACM.
[5] S. Chernov, P. Serdyukov, P.-A. Chirita, G. Demartini, and W. Nejdl. Building a desktop search test-bed. In ECIR' 07, pages 686­690, 2007.
[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR '02, pages 299­306, New York, NY, USA, 2002. ACM.

[7] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, and D. C. Robbins. Stuff i've seen: a system for personal information retrieval and re-use. In SIGIR '03, pages 72­79, New York, NY, USA, 2003. ACM.
[8] D. Elsweiler and I. Ruthven. Towards task-based personal information management evaluations. In SIGIR '07, pages 23­30, New York, NY, USA, 2007. ACM.
[9] T. Joachims. Optimizing search engines using clickthrough data. In KDD '02, pages 133­142, New York, NY, USA, 2002. ACM.
[10] J. Kim and W. B. Croft. Retrieval experiments using pseudo-desktop collections. In CIKM '09, pages 1297­1306. ACM, 2009.
[11] J. Kim, X. Xue, and W. B. Croft. A Probabilistic Retrieval Model for Semi-structured Data. In Proceedings of ECIR '09. Springer, 2009.
[12] H. Ma, R. Chandrasekar, C. Quirk, and A. Gupta. Improving search engines using human computation games. In CIKM' 09, pages 275­284, 2009.
[13] P. Ogilvie and J. Callan. Combining document representations for known-item search. In SIGIR '03, pages 143­150, New York, NY, USA, 2003. ACM.
[14] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery. Numerical Recipes in C. Cambridge University Press, Cambridge, UK, 2nd edition, 1992.
[15] S. Robertson, H. Zaragoza, and M. Taylor. Simple bm25 extension to multiple weighted fields. In CIKM '04, pages 42­49, New York, NY, USA, 2004. ACM.
[16] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08, pages 1053­1062, New York, NY, USA, 2008. ACM.
[17] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In SIGIR '03, pages 298­305, New York, NY, USA, 2003. ACM.
[18] L. Si, R. Jin, J. Callan, and P. Ogilvie. A language modeling framework for resource selection and results merging. In CIKM '02, pages 391­397, New York, NY, USA, 2002. ACM.
[19] P. Thomas and D. Hawking. Server selection methods in personal metasearch: a comparative empirical study. Inf. Retr., 12(5):581­604, 2009.
[20] L. von Ahn and L. Dabbish. Designing games with a purpose. Commun. ACM, 51(8):58­67, 2008.

57

Learning More Powerful Test Statistics for Click-Based Retrieval Evaluation

Yisong Yue
Cornell University
Ithaca, NY, USA
yyue@cs.cornell.edu

Yue Gao
Cornell University
Ithaca, NY, USA
ygao@cs.cornell.edu

Olivier Chapelle
Yahoo! Research
Santa Clara, CA, USA
chap@yahoo-inc.com

Ya Zhang
Shanghai Jiao Tong University Shanghai, China
ya_zhang@sjtu.edu.cn

Thorsten Joachims
Cornell University
Ithaca, NY, USA
tj@cs.cornell.edu

ABSTRACT
Interleaving experiments are an attractive methodology for evaluating retrieval functions through implicit feedback. Designed as a blind and unbiased test for eliciting a preference between two retrieval functions, an interleaved ranking of the results of two retrieval functions is presented to the users. It is then observed whether the users click more on results from one retrieval function or the other. While it was shown that such interleaving experiments reliably identify the better of the two retrieval functions, the naive approach of counting all clicks equally leads to a suboptimal test. We present new methods for learning how to score different types of clicks so that the resulting test statistic optimizes the statistical power of the experiment. This can lead to substantial savings in the amount of data required for reaching a target confidence level. Our methods are evaluated on an operational search engine over a collection of scientific articles.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.
General Terms: Measurement, Human Factors, Experimen-
tation.
Keywords: Implicit feedback, retrieval evaluation, click-
through data.
1. INTRODUCTION
Given the rapidly growing breadth and quantity of information needs and retrieval tasks, the need to develop scalable and reliable evaluation methodologies has likewise been gaining in importance. Towards this end, evaluating retrieval performance based on implicit feedback (e.g., clicks, reformulations and dwell time) is an
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

attractive option for several reasons. First, the evaluation is done on the actual population of users in their natural usage contexts, which is difficult to replicate when using editorial judgments. Second, recording implicit feedback is inexpensive and much faster than obtaining editorial judgments. And, finally, implicit feedback is available even for collections where it would not be economically feasible to hire relevance judges.
One approach for deriving reliable judgments from implicit feedback is to focus on collecting relative as opposed to absolute feedback. For example, while it is difficult to interpret clicks on an absolute scale (e.g., clicked results are relevant, non-clicked results are not relevant), there is clear evidence that clicks provide reliable relative feedback (e.g., clicked results are better than skipped results) [2, 14, 18]. This property is exploited in Interleaving Experiments [13, 18] to compare the relative quality of two ranked retrieval functions h and h . For every incoming query, the rankings of the two retrieval functions are presented to the user as a single interleaved ranking, and the user's clicks are observed. If the user clicks more on results from h than from h in the interleaved ranking, it was shown that one can reliably conclude that h is preferred over h [18, 17]. From an experiment design perspective, interleaving provides a blind paired test where presentation bias is eliminated through randomization under reasonable assumptions.
In this paper, we aim to make interleaving experiments more efficient ­ or scalable ­ by developing a more powerful test statistic. Our motivation comes from the intuition that not every click in the interleaved ranking is equally informative. For example, a click on the result at rank 1 in a query session immediately followed by a "back" (i.e., a quick return to the search results page) is probably less informative than the last click in the session (which satisfies the information need). As such, having more flexible weighting schemes on clicks can reduce the variance of the test statistic1. This improved experiment design will allow us to confidently tease apart the quality of two competing retrieval functions using substantially less data.
We present three learning methods for optimizing test statistics by using training data from pairs of retrieval functions of known relative retrieval quality (e.g., by gathering enough data so that the conventional test statistic is significant). The learned test statistic can then be used to more quickly identify the superior retrieval function in future interleaving experiments. Learning test statistics
1This is also known as the credit assignment problem [17].

507

Algorithm 1 Team-Draft Interleaving
Input: Rankings A = (a1, a2, . . . ) and B = (b1, b2, . . . ) Init: I  (); TeamA  0/ ; TeamB  0/ ; while (i : A[i]  I)  ( j : B[ j]  I) do
if (|TeamA| < |TeamB|)  ((|TeamA| = |TeamB|)  (RandBit() = 1)) then k  mini{i : A[i]  I} . . . . . . . . . . . . . top result in A not yet in I I  I + A[k]; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . append it to I TeamA  TeamA  {A[k]} . . . . . . . . . . . . . clicks credited to A
else k  mini{i : B[i]  I} . . . . . . . . . . . . . top result in B not yet in I I  I + B[k] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . append it to I TeamB  TeamB  {B[k]} . . . . . . . . . . . . . clicks credited to B
end if end while Output: Interleaved ranking I, TeamA, TeamB
can be thought of as solving the inverse problem of conventional hypothesis testing, and we present an empirical evaluation on real data from an operational search engine for research papers.
2. RELATED WORK
The traditional Cranfield methodology [20] relies solely on editorial judgments for evaluation, where human judges assign explicit relevance ratings to results. Though effective at small scales, this approach quickly becomes infeasible as the evaluation tasks grow in size and number. While methods exist that reduce the labeling requirements to some extent (e.g., [6, 3, 4]), or that reduce the cost of collecting explicit feedback [5, 19], leveraging usage logs has been steadily gaining in popularity due to their inexpensive availability.
Our work is closely related to the topic of learning user behavior models, since implicit feedback is essentially a reflection of human behavior. Fox et al. [11] learned an association between implicit feedback gathered by an instrumented browser and explicit judgments of satisfaction. Other existing approaches typically learn user behavior or document relevance models from passively collected usage logs (e.g., [2, 7, 10, 21, 9], often with the goal of aiding the retrieval function in providing more relvant results (e.g., [1, 8]).
Our work is distinguished from the aforementioned work on user modeling in at least two aspects. First, it is set within the framework of a well-controlled paired experiment design. We will be able to exploit the properties of this experiment design when deriving and theoretically justifying the learning methods, as well as when interpreting the results. Second, prior work on user modeling for this purpose focused largely on evaluation at the result (i.e., document) level (e.g., [1, 14, 22]). In contrast, we are interested in performing more holistic evaluations for ranking functions.
Radlinski & Craswell [17] recently conducted a study comparing conventional measures (e.g., NDCG, MAP) to interleaving metrics based on clickthrough data. They found that manually increasing the weight of clicks lower in the ranking improved the statistical power of the interleaving metric. We will provide automatic learning methods for optimizing the power of the test statistic, making use of many attributes beyond the rank of the clicks.
3. INTERLEAVING EVALUATION
In analogy to experiment designs from sensory analysis (see e.g. [15]), interleaving experiments [13, 18] provide paired preference tests between two retrieval (i.e., ranking) functions. Such paired

Rank 1 2 3 4 5 6 ...

Input Ranking AB
ab be ca df gg hh ... ...

Interleaved Rankings

Team-Draft

AAA BAA ABA ...

aA bB aA

bB aA bB

cA cA eB

eB eB cA

dA dA dA

fB fB fB

...

...

...

Figure 1: An example showing how the Team-Draft method interleaves input rankings A and B for different random coin flip outcomes. Superscripts of the interleavings indicates team membership.

experiments are particularly suitable in situations where it is difficult or meaningless to assign an absolute rating (e.g., rate this taste on a scale from 1 to 10), but a relative comparison is easy to make (e.g., do you like taste A better than taste B). To elicit such pairwise preferences, both alternatives have to be presented side-by-side and without presentation bias. For example, the order in which a subject tastes two products must be randomized, and the identity of the products must be "blind" to the user.
For the case of comparing pairs of retrieval functions, interleaving experiments are designed to provide such a blind and unbiased side-by-side comparison of two retrieval functions h and h . When a user issues a query q, the rankings A = h(q) and B = h (q) are computed but kept hidden from the user. Instead, the user is shown a single interleaved ranking I computed from A and B, so that clicks on I provide feedback on the users preference between A and B under reasonable assumptions.
In this paper, we focus on the Team-Draft Interleaving method [18] that is summarized in Algorithm 1. Team-Draft Interleaving creates a fair (i.e. unbiased) interleaved ranking following the analogy of selecting teams for a friendly team-sports match. One common approach is to first select two team captains, who then take turns selecting players in their team. Team-Draft Interleaving uses an adapted version of this approach for creating interleaved rankings. Suppose each document is a player, and rankings A and B are the preference orders of the two team captains. In each round, captains pick the next player by selecting their most preferred player that is still available, add the player to their team and append the player to the interleaved ranking I. We randomize which captain gets to pick first in each round. An illustrative example from [18] is given in Figure 1.
To infer whether the user prefers ranking A or ranking B, one counts the number of clicks on documents from each team. If team A gets more clicks, A wins the side-by-side comparison and vice versa. Denoting the sets of clicks on the respective teams with C and C for query q, the mean or median value of the test statistic

(q,C,C ) = |C| - |C |

(1)

over the distribution P(q) of queries reveals whether one of h and h is consistently preferred over the other. Section 3.1 discusses three possible tests that detect whether the mean or median of (q,C,C ) is significantly different from zero.
Note that the presentation is unbiased in the sense that A and B have equal probability of occupying each rank in I. This means that any user that clicks randomly will not generate a significant preference in either direction.
In this paper, we address one shortcoming of the test statistic

508

in (1): the test statistic scores all clicks equally, which is likely to be suboptimal in practice. For example, a user clicking back immediately after a click is probably an indicator that the page was not good after all. The goal of this work is to learn a more refined function score(q, c) that scores different types of clicks according to their actual information content. This scoring function can then be used in the following rule

(q,C,C ) =  score(q, c) -  score(q, c ) .

cC

c C

Note that this reduces to (1) if score(q, c) is always 1. In the following, we will use a linear model score(q, c) = wT (q, c)
to score clicks, where w is a vector of parameters to be learned and (q, c) returns a feature vector describing each click c in the context of the entire query session q. We can now rewrite (q,C,C ) as
w(q,C,C ) = wT (q,C,C )

where

(q,C,C ) =  (q, c) -  (q, c)

(2)

cC

cC

Feature vectors (q, c) will contain features that describe the click in relation to position in the interleaved ranking, order, and presentation. In Section 5.2, we will decribe the feature construction used in our empirical evaluation.

3.1 Hypothesis Tests for Interleaving

To decide whether an interleaving experiment between h and h

shows a preference in either direction, one needs to test whether

some measure of centrality (e.g. median, mean) of the i.i.d. ran-

dom variables i  (q,C,C ) is significantly different from zero. For conciseness, let (1, ..., n) denote the values of (q,C,C ) on a

random sample. We consider the following three tests, which will

also serve as the baseline methods in our empirical evaluation.

The simplest test, and the one previously used in [12, 13, 18], is

the Binomial Sign Test (see e.g. [16]). It counts how often the sign of i is positive, i.e. S = ni=1[i > 0]. This sum S is a binomial random variable, and the null hypothesis is that the underlying i.i.d.

Bernoulli random variables [i > 0] have p = 0.5.

Unlike the Binomial Sign Test, the z-Test (see e.g. [16]) uses

the magnitudes of the i and tests whether their sum is zero in ex-

pectation.

The

z-Test

assumes

that

S

=

1 n

ni=1 i

is

normal,

which

is approximately satisfied for large n. The ratio of the observed

value

s=

1 n

ni=1

i

and

standard

deviation

st d (S),

called

the

z-

score z = s/std(S), monotonically relates to the p-value of the z-

test. While std(S) has to be known, an approximate z-test results

from

estimating

st d (S)

=

1 n

1 n

 j(s -



j )2

from

the

sample.

The

t-test accounts for the additional variability from the estimate of the

standard deviation, but for large samples z-test and t-test are virtu-

ally identical.

Finally, we consider the Wilcoxon Signed Rank Test (see e.g.

[16]) as a non-parametric test for the median of the i being 0. To compute the test statistic, the observations are ranked by |i|. Let the resulting rank of i be ri. The test statistic is then computed as W =  sign(i)ri, and W is tested for mean 0 using a z-test.

4. LEARNING METHODS
The idea behind learning is to find a scoring function that results in the most sensitive hypothesis test. To illustrate this goal, consider the following hypothetical scenario where the scoring function score(q, c) = wT (q, c) differentiates the last click of a query

session from other clicks within the same session. The corresponding feature vector (q, c) would then have two binary features

(q, c) =

1, if c is last click; 0 else 1, if c is not last click; 0 else

.

Assume for simplicity that every query session has 3 clicks, with
"not last clicks" being completely random while "last clicks" fa-
voring the better retrieval function with 60% probability. Using the weight vector wT = (1, 1) (i.e., the conventional scoring func-
tion), one will eventually identify that the better retrieval function gets more clicks (typically after 280 queries using a t-test with p = 0.95). However, the optimal weight vector wT = (1, 0) will
identify the better retrieval function much faster (typically after 150 queries), since it eliminates noise from the non-informative
clicks.
The learning problem can be thought of as an "inverse" hypothesis test: given data for pairs (h, h ) of retrieval functions where we
know h h , find the weights w that maximizes the power of the
test statistic on new pairs. More concretely, we assume that we are given a set of ranking function pairings {(h1, h1), ..., (hk, hk)} for which we know w.l.o.g. that hi is better than hi, i.e. hi hi. This preference may be known by construction (e.g., hi is a degraded version of hi), by running interleaving until the conventional test statistic that scores each click uniformly becomes significant, or
through some expensive annotation process (e.g., user interviews, manual assessments). For each pair (hi, hi), we assume access to usage logs from Team-Draft Interleaving [18] for ni queries. For each query q j, the clicks Cj and Cj for each "team" are recorded in a triple (q j,Cj,Cj). Eventually, all triples are combined into one training sample

S = ((q1,C1,C1), ..., (qn,Cn,Cn)).
Note that we are essentially treating all interleaving pairs as a single combined example2. After training, the learned w and the resulting test statistic w(q,C,C ) will be applied to new pairs of retrieval functions (htest, htest) of yet unkown relative retrieval quality.
We now propose three learning methods, with each corresponding to opimizing a specific inverse hypothesis test.

4.1 Maximize Mean Difference
In the simplest case, we can optimize the parameters w of scorew(q, c) to maximize the mean difference of scores between the better and the worse retrieval functions,
n
w = argmax  w(q j,Cj,Cj) w j=1 = argmax  wT (q j,Cj,Cj) wj

To abstract from different scalings of w and to make the problem well posed, we impose a normalization constraint ||w|| = 1, leading to the following optimization problem:
w = argmax  wT (q j,Cj,Cj) s.t. ||w|| = 1, wj

which can be written more compactly using  j = (q j,Cj,Cj),

w = argmax  wT  j s.t. ||w|| = 1.

w

j

2A better approach may be to explicitly treat each interleaving pair as a separate example.

509

This has the the following closed-form solution that can be derived via Lagrange multipliers:

w =

j j

 j.

( j  j)T ( j  j) j

While maximizing the mean difference is intuitively appealing, one key shortcoming is that variance is ignored. In fact, one can think of this method as an inverse z-Test, where we assume equal variance for all w. Since the assumption of equal variance will clearly not be true in practice, we now consider the following more refined methods.

4.2 Inverse z-Test
The following learning method removes the assumption of equal variance and optimizes the statistical power of a z-Test in the general case (with the null hypothesis that the mean is zero). Finding the w that maximizes the z-score (and therefore the p-value) on the training set corresponds to the following optimization problem:

w = argmax
w 1 n

1 n



j

w(q

j

,C

j

,

C

j

)

1 n



j

w(q

j

,C

j

,C

j)2

-

1 n



j

w(q

j

,C

j

,C

j)

2

=

argmin
w

 j w(q j,C j,C j)2
2
 j w(q j,C j,C j)

(3)

While (3) has two symmetric solutions, we are interested only in the one where  j w(q j,Cj,Cj) > 0. Using the abbreviated notation  j = (q j,Cj,Cj), this optimization problem can be rewritten as
w = argmax (wT  j  j)2 . w wT  j  jTj w

For any w solving this optimization problem, cw with c > 0 is also a solution. We can thus rewrite the problem as

w = argmax wT   j s.t. wT   jTj w = 1.

w

j

j

Using the Lagrangian

L(w, ) = wT   j -  wT   jTj w - 1 ,

j

j

and solving for zero derivative w.r.t. w and , one arrives at a closed form solution. Denoting  =  j  j and  =  j  jTj the solution can be written as
w =  -1 . T -1
While not used in the experiments for this paper, a regularized version reg of the covariance matrix  can be used to prevent overfitting. One straightforward approach is to add a ridge term reg =  + I, where I is the identity matrix and  is the regularization parameter.

4.3 Inverse Rank Test
Last but not least, we consider a learning method that relates to inverting the Wilcoxon Rank Sign test. A good scoring function w(q,C,C ) for the Wilcoxon test should optimize the Wilcoxon statistic, which can be computed as follows. Assuming h h w.l.o.g., we denote a prediction as "correct" if w(q,C,C ) > 0;

otherwise, we denote it as incorrect. Ranking all observations by |w(q,C,C )| (assuming no ties), the Wilcoxon statistic is isomorphic to the number of observation pairs where an incorrect observation is ranked above a correct observation. One strategy for minimizing the number of such swapped pairs, and therefore optimizing the p-value of the Wilcoxon test, is to choose

w(q,C,C ) = Pr(h h |q,C,C ) - 0.5,

(4)

where Pr(h h |q,C,C ) is the estimated probability that h is better than h given the clicks observed for query q.
We estimate Pr(h h |q,C,C ) from the training data S using a standard logistic regression model

Pr(h ln Pr(h

h |q,C,C h|q,C,C

) )

=

wT (q j,C j,C j).

Using again the convention that h h for the training data and
abbreviating  j = (q j,Cj,Cj), the parameters w are chosen via maximum likelihood,

 w

=

argmax
w

n j=1

1

+

1 e-wT

j

.

w denotes the logistic regression solution on the training data. We used the LR-TRIRLS package3 to solve this optimization problem.
The final ranking function can be simplified to the linear function w(q,C,C ) = wT (q,C,C ), since it produces the same rankings and signs as (4).

5. EMPIRICAL SETUP
5.1 Data Collection
We evaluated our methods empirically using data collected from the Physics E-Print ArXiv4. In particular, we used two datasets of click logs collected while running Team-Draft Interleaving experiments. For both datasets, we recorded information for each query (e.g., the entire session) and click (e.g., rank, timestamp, result information, source ranking function, etc). This information is used to generate features for learning (see Section 5.2 below). One could also collect user-specific information (e.g., user history), but we have not done so in the following experiments.
"Gold standard". Our first dataset is taken from the Team-Draft experiments described in [18]. In these experiments, the incumbent retrieval function was corrupted in multiple ways to provide pairs of retrieval functions with known relative quality. This provides cheap access to a "gold standard" dataset, since one knows by construction which retrieval function is superior within each pair. A total of six pairs was evaluated, with each yielding slightly over 1000 query sessions.
New interleaving experiments. Our second dataset was generated via interleaving pairs of retrieval functions without necessarily having knowledge of which retrieval function is superior within each pair. For example, one retrieval function that we considered modifies the incumbant retrieval function by giving additional weight to query/title similarity. It is a priori unclear whether this would result in improved retrieval quality. Ideally (and intuitively), learning a test statistic on the gold standard dataset should help us more quickly determine the superior retrieval function within these interleaving pairs. We examine this hypothesis further in Section 6.4. A total of six different retrieval functions are considered in this setting. We collected click data from interleaving every possible pairing of the six, resulting in fifteen interleaving pairs with each
3http://komarix.org/ac/lr/ 4http://arxiv.org

510

yielding between 400 and 650 query sessions. We then removed three of the fifteen interleaving pairs from our analysis, since all methods (including the baselines) showed poor performance (pvalue greater than 0.4), making them uninteresting for comparison purposes.

5.2 Feature Generation

The features that are used in the following experiments describe
a diverse set of properties related to clicking behavior, including
the rank and order of clicks, and whether search result clicks led to a PDF download in ArXiv5. Let Cown and Cother denote the clicks from the own team and the other team, respectively, for a
single query session. Recall from (2) that our feature function (q,Cown,Cother) decomposes as

(q,Cown,Cother) =  (q, c) -  (q, c).

cCown

cCot her

We will construct (q, c) for c  Cown in the following way:

1. 1 always

2. 1 if c led to a download

3.

1 |Cown|

if Cown

gets

both

more

clicks

and

downloads

4. If |Cown| == |Cother|:

(a) min

number_o f _bolded_words_in_title number_o f _query_words

,

1

(b) min

number_o f _bolded_words_in_abstract number_o f _query_words

,

2

5. If it is a single-click query: (a) 1 if c is not at rank 1 (b) 1 if c is on first page (top 10)

6. If it is a multi-click query:
(a) 1 if c is first click (b) 1 if c is last click (c) 1 if c is first click and not at rank 1 (d) 1 if c is at rank 1 (e) 1 if c is at ranks 1 to 3 (f) 1 if c is on first page (top 10) (g) 1 if c is followed by click on a higher position (regres-
sion click)

Analogously, we construct (q, c) for c  Cother by swapping Cown

and Cother in the preceding feature definitions. Note that some features are more naturally expressed at the query

level. For example, feature 3 can be equivalently expressed directly

as feature of (q,Cown,Cother) as



 1 if Cown gets both more clicks and downloads



-1 0

if Cother gets both more clicks and downloads . otherwise

For clarity, we focus our formulation on click-level features, since most features we used are more naturally understood at the click level.

5All search results correspond to research papers that are available for download.

Figure 2: Comparing the sample size required versus target t-test p-value in the synthetic experimental setting. Measurements taken from 1000 bootstrapped subsamples for each subsampling size.
6. EMPIRICAL EVALUATION
For ease of presentation, we will only show comparisons against the t-test baseline; our empirical results also hold when comparing against the other baselines. In general, we find the inverse z-test to be the best performing method, with the inverse rank test often being competitive as well.
6.1 Synthetic Experiment
We first conducted a synthetic experiment where all six gold standard interleaving pairs in the training set are mixed together to form a single (virtual) interleaving pair. From this, 70% of the data was used for training and the remaining 30% for testing. Intuitively, this setup satisfies the assumption that the click distribution we train on is the same as the click distribution we test on ­ a core assumption often made when analyzing machine learning approaches.
Figure 2 shows how the required sample size grows with decreasing target t-test p-value. This plot (and all similar plots) was generated by subsampling the test set (with replacement) at varying subset sizes and computing the p-value. Subset sizes increase in increments of 25 and each subset size was sampled 1000 times. Our goal is to reduce the required sample size, so lower curves indicate superior performance.
We observe in Figure 2 that our methods consistently outperform the baseline. For example, for a target p-value of 0.01, the inverse z-test requires only about 800 samples whereas the baseline t-test requires about 1200 ­ a 50% improvement. In all of our subsequent experiments, we find that the max mean difference method consistently performs worse than the inverse z-test. As such, we will focus on the inverse rank test and the inverse z-test in the remaining empirical evaluations.
6.2 Analyzing the Learned Scoring Function
To give some insight into the scoring function w(q,C,C ) = wT (q,C,C ) learned by our methods, Table 1 shows the weights w generated by the inverse rank test on the full gold standard training set. Since the features are highly correlated, it is difficult to gain insight merely through inspection of the weights. As such, we now provide a selection of prototypical example queries for which we will compute the feature vector  = (q,C,C ) and the value of w(q,C,C ).
1. Single click on result from h at rank 1: Feature vector  has

511

Figure 3: Comparing sample size required versus target t-test p-value in leave-one-out testing on the training set. Methods compared are baseline (red), inverse rank test (black dotted) and inverse z-test (black solid). The inverse z-test consistently performs as well as the baseline, and can be much better. Note that the different graphs vary dramatically in scale.

Table 1: Weights learned by the inverse rank test on the full gold standard training set. See Section 5.2 for a full description of the features.

ID

Feature Description (w.r.t. (q, c)) Weight

1

Click 0.056693

2

Download 0.020917

3

More clicks & downloads than other team 0.052410

4a

1[# Clicks equal] × Title bold frac 0.083463

4b

1[# Clicks equal] × Abstract bold frac 0.118568

5a

Single click query AND Rank > 1 0.149682

5b

Single click query AND Rank  10 0.004950

6a

Multi-clicks AND First click 0.063423

6b

Multi-clicks AND Last click 0.000303

6c Multi-clicks AND First click AND Rank > 1 0.015217

6d

Multi-clicks AND Click at rank = 1 0.018800

6e

Multi-clicks AND Click at ranks  3 -0.00419

6f

Multi-clicks AND Click at ranks  10 0.067362

6g

Multi-clicks AND Regression click 0.033067

value 1 for features 1 and 5b, leading to w = 0.062 (we are assuming no downloads in this scenario).
2. Single click on result from h at rank 3: Feature vector  has value 1 for features 1, 5a and 5b, leading to w = 0.211. As expected, this query is judged to be more informative, since a click at rank 3 indicates a more careful selection.
3. Single click on result from h at rank 3 followed by download: Feature vector  has value 1 for features 1, 2, 3, 5a, 5b, leading to w = 0.285. The download adds further evidence, which follows our intuition.
4. One click on result from h at rank 1, followed by another click on result from h at rank 2. Rank 2 has bolded title terms, while rank 1 has not: Feature vector  has value 1 for

features 6a, 6d, and value -1 for 4a and 6b. This leads to w = -0.002, indicating a slight preference for h .
6.3 Cross Validation Experiments
In this setting, we trained our models on five of the gold standard interleaving pairs and tested on the remaining one, repeating this process for all six pairs. This provides a controlled way of evaluating generalization performance. Figure 3 shows how required sample size changes as the target p-value decreases. Again, lower curves indicate superior performance. We observe the inverse z-test performing at least as well as the baseline on all except training pair 3. Note, however, that training pair 3 is an exceptionally easy case where one can achieve confident p-values with very little data. We observe the inverse rank test to also be competitve, but with somewhat worse performance.
6.4 New Interleaving Experiments
To further evaluate the methods in a typical application scenario, we trained our models on all six of the gold standard interleaving pairs, and then tested their predictions on new interleaving pairs. It should be noted that we did not examine the new interleaving dataset when developing the features described in Section 5.2. As such, this evaluation very closely matches how such methods would be used in practice.
Figure 4 shows, for all twelve test cases, how required sample size changes as the target t-test p-value decreases. We observe both learning methods consistently performing at least as well as, and often much better than, the baseline t-test (with the exception of Exp 1). We also verified that all methods and baselines agree on the direction of the preference in all cases (since we are using a two-tailed test).
Table 2 provides numerical comparisons for several standard significance thresholds. For half of the twelve test cases, the inverse z-test reduces the required sample size by at least 10% for a target significance of p = 0.1. For a quarter of the cases, the inverse z-test achieves a significance of p = 0.05 using the available data whereas

512

Figure 4: Comparing sample size required versus target t-test p-value in the twelve new interleaving experiments. Methods compared are baseline (red), inverse rank test (black dotted) and inverse z-test (black solid). Both the inverse rank test and inverse z-test methods outperform baseline in most cases.

the baseline t-test fails to do so. These results imply that substantial savings can be gained from employing optimized test statistics.
7. DISCUSSION AND LIMITATIONS
In this section, we discuss and summarize the core assumptions and limitations of our approach.
While the learned test statistics generally improved the power of the experiments on new retrieval function pairs (h, h ), there is likely a limit to how different the new pair may be from the training pairs. If the retrieval functions to be evaluated move far from the training data (e.g. after several iterations of improving the ranking function), it might be necessary to add appropriate training data and re-optimize the test statistic. Furthermore, we do not believe that test statistics learned on one search engine would necessarily generalize to a different collection or user population.
A key issue in generalizing to new retrieval function pairs (h, h ) lies in the appropriate choice of features (q,C,C ). In particular, if the chosen features allow the learning algorithm to models specific idiosyncracies of the training pairs, this will likely result in poor generalization on new pairs. Furthermore, different systems may record different types of usage behavior (such as maintaining

user IDs for personalization purposes). This dictates the types of features that are available to the learning methods.
Pooling the training examples from multiple training pairs (hi, h j) into one joint training set might lead to unwanted results, since the learning methods optimize an "average" statistic over multiple pairs. In particular, the methods might ignore difficult to discriminate pairs in return for increased discriminative power on easy pairs. It would be more robust to minimize the maximum p-value uniformly over all training pairs.
Finally, the empirical results need to be verified in other retrieval domains. Particularly interesting are domains that include spam. It would be interesting to see whether one can learn scoring functions that recognize (and discount) clicks that were attracted by spam.
8. CONCLUSION
We have presented learning methods for optimizing the statistical power of interleaving experiments for retrieval evaluation. Given clickthrough data from interleaving pairs of retrieval functions of known relative retrieval quality, our proposed methods learn an optimized test statistic. We showed that these learned test statistics generalize to new retrieval functions, often substantially reducing the number of queries needed for evaluation.

513

Table 2: Sample size requirements of three target t-test p-values for the twelve new interleaving experiments.

Baseline

p=0.2 p=0.1 p=0.05

Exp 1 160 288 406

Exp 2 169 310 > 450

Exp 3 247 460 > 500

Exp 4 93 161 228

Exp 5 111 182 259

Exp 6 > 625 > 625 > 625

Exp 7 415 > 475 > 475

Exp 8 59 95 142

Exp 9 70 128 174

Exp 10 352 > 500 > 500

Exp 11 174 328
> 425

Inv. rank test p=0.2 373 149 180 90

64 575 157 < 50 71

353

141

p=0.1 > 500 313 330 160 114 > 625 296 74 129 > 500 260

p=0.05 > 500 > 450 471 230 162 > 625 423 99 184 > 500 365

Inv. z-test p=0.2 491 146 461 104 53 254

76

58 < 50 216

134

p=0.1 > 500 275 > 500 189 97 505 137 95

94

361

222

p=0.05 > 500 416 > 500 251 142 > 625 199 144 138 > 500 339

Exp 12 > 400 > 400 > 400 > 400 > 400 > 400
308 > 400 > 400

The idea of evaluating and learning via pairwise comparisons is attractive due to its simplicity in interpretation. In such cases, it is generally quite intuitive to design meaningful hypothesis tests. As such, the general techniques described in this paper for optimizing these statistical tests (e.g., inverse z-test) can also be applied to other domains beyond traditional information retrieval using domain-appropriate feature representations.
Acknowledgements
The work is funded by NSF Awards IIS-0812091 and IIS-0905467. The first author is also supported in part by a Microsoft Research Graduate Fellowship and a Yahoo! Key Scientific Challenges Award.
9. REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incroporating user behavior. In ACM Conference on Information Retrieval (SIGIR), 2006.
[2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. In ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 3­10, New York, NY, USA, 2006. ACM.
[3] J. A. Aslam, V. Pavlu, and E. Yilmaz. A sampling technique for efficiently estimating measures of query retrieval performance using incomplete judgments. In ICML Workshop on Learning with Partially Classified Training Data, 2005.
[4] C. Buckley and E. M. Vorhees. Retrieval evaluation with incomplete information. In ACM Conference on Information Retrieval (SIGIR), 2004.
[5] C. Callison-Burch. Fast, cheap, and creative: Evaluating translation quality using amazon's mechanical turk. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2009.
[6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2006.
[7] B. Carterette and R. Jones. Evaluating search engines by modeling the relationship between relevance and clicks. In Conference on Neural Information Processing Systems (NIPS), 2007.
[8] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In World Wide Web Conference (WWW), 2009.
[9] G. Dupret and C. Liao. Cumulating relevance: A model to estimate document relevance from the clickthrough logs. In

ACM Conference on Web Search and Data Mining (WSDM), 2010. [10] G. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past observations. In ACM Conference on Information Retrieval (SIGIR), 2008.
[11] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Trans. Inf. Syst., 23(2):147­168, 2005.
[12] T. Joachims. Optimizing search engines using clickthrough data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133­142, 2002.
[13] T. Joachims. Evaluating retrieval performance using clickthrough data. In J. Franke, G. Nakhaeizadeh, and I. Renz, editors, Text Mining, pages 79­96. Physica/Springer Verlag, 2003.
[14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Transactions on Information Systems (TOIS), 25(2), April 2007.
[15] D. Laming. Sensory Analysis. Academic Press, 1986.
[16] A. Mood, F. Graybill, and D. Boes. Introduction to the Theory of Statistics. McGraw-Hill, 3rd edition, 1974.
[17] F. Radlinski and N. Craswell. Comparing the sensitivity of information retrieval metrics. In ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2010.
[18] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In Conference on Information and Knowledge Management (CIKM), 2008.
[19] R. Snow, B. O'Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast--but is it good?: evaluating non-expert annotations for natural language tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 254­263, Morristown, NJ, USA, 2008. Association for Computational Linguistics.
[20] E. M. Vorhees and D. K. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005.
[21] K. Wang, T. Walker, and Z. Zheng. Pskip: Estimating relevance ranking quality from web search clickthrough data. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2009.
[22] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: Evaluating result attractiveness as a source of presentation bias in clickthrough data. In World Wide Web Conference (WWW), 2010.

514

Query Similarity by Projecting the Query-Flow Graph

Ilaria Bordino1 bordino@dis.uniroma.it

Carlos Castillo2 chato@yahoo-inc.com

Debora Donato2

Aristides Gionis2

debora@yahoo-inc.com gionis@yahoo-inc.com

1DIS, Sapienza Università di Roma Roma, Italy

2Yahoo! Research Labs Barcelona, Spain

ABSTRACT
Defining a measure of similarity between queries is an interesting and difficult problem. A reliable query-similarity measure can be used in a variety of applications such as query recommendation, query expansion, and advertising.
In this paper, we exploit the information present in query logs in order to develop a measure of semantic similarity between queries. Our approach relies on the concept of the query-flow graph. The query-flow graph aggregates query reformulations from many users: nodes in the graph represent queries, and two queries are connected if they are likely to appear as part of the same search goal. Our querysimilarity measure is obtained by projecting the graph (or appropriate subgraphs of it) on a low-dimensional Euclidean space. Our experiments show that the measure we obtain captures a notion of semantic similarity between queries and it is useful for diversifying query recommendations.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Finding a measure of similarity between queries can be very useful to improve the services provided by search engines. First, the ability to identify similar queries is in the core of any query-recommendation system. Second, query similarity can be used for performing query expansion. Additionally, a reliable notion of query similarity can be used for broad matching of advertisements to queries, or even for suggesting keywords to advertisers. However, defining a
Part of this work was done while visiting Yahoo! Research Labs, Barcelona
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

query-similarity measure is not an easy task as it strongly depends on user intent: syntactically similar queries may originate from completely different intents. Issues such as polysemy, synonymy, high levels of noise, and the small amount of available information make the problem challenging.
In such a complex setting, information extracted from query logs has shown to be effective. The information on how users interact with search engines has often been used to improve the user search experience. In particular, querylog analysis is used to provide insights on how users refine their queries, and what kind of search strategies they are using to locate the information they need.
In this paper we describe a method of obtaining a querysimilarity measure, based on query-log analysis. Our method relies on an aggregated representation of a query log by the means of a reformulation graph, which is known as the query-flow graph [6]. In this graph, nodes represent queries and two queries are connected if they are likely to appear as part of the same search goal [15].
Our main intuition is that related queries will tend to cluster in local neighborhoods of the graph. Graph-projection methods are known to map graph nodes into geometric spaces so that the distance distortion is minimized. Thus, we suggest projecting the query-flow graph (or appropriately defined subgraphs of it) and then measure query similarity on the resulting geometric space. The technique is general and it can be applied to other graphs obtained from query-logs, for example, the click graph [5, 11, 22].
We use the resulting query-similarity measure for diversifying query recommendations. Query-recommendation systems are provided by all major search engines and they aim at helping users to find more easily what they are searching for. The idea is that a diversification algorithm requires a notion of query similarity, for which we use the proposed measure. We show that the resulting system provides diverse yet relevant recommendations. Our main contributions are summarized as follows:
· we describe a method for measuring similarity between queries by projecting a query reformulation graph;
· we show that our similarity measure captures the human notion of related queries better than other measures on the original graph;
· we apply this method to the task of producing diverse and useful recommendations;
· we show how to improve its efficiency further, by projecting only the neighborhood of the input query.

515

Roadmap. The rest of this paper is organized as follows: Section 2 discusses related work. Sections 3 and 4 describe the framework we use to define our measures of query similarity, and Section 5 explores different variants for optimizing our measures. Section 6 describes our application for diversifying query recommendations and Section 7 offers our concluding remarks.
2. RELATED WORK
Query graphs. Graphs can be used to provide a compact representation of query-logs. Query graphs of different types have been extensively studied in literature [2, 5, 3, 22, 6]. In most of these studies query-logs are represented by graphs in which vertices represent queries and edges capture various types of query relations. Baeza-Yates [2] defines a query-graph taxonomy, introducing five types of graphs where edges connect queries according to different criteria (e.g., common words, common clicked URLs). Tiberi et al. [3] show how to infer semantic relations between queries from the cover graph and describe an application to the task of detecting multi-topical URLs.
Query-document graphs, also known as click graphs [5, 22] are bipartite graphs G = {Q, D, E} where Q is a query set and D is a document set. A query is connected to documents that were clicked in the associated result list.
The query-flow graph [6] aggregates different sources of information in order to capture the latent query behavior of users. Leven and Loizou [19] introduce a concept similar to the query-flow graph, but their work is focused on browsing behavior inside a Web site. Borges and Levene [8] propose an improved method to summarize Web navigation sessions based on a variable-length Markov model.
Query recommendations. Most query-recommendation methods use similarity measures obtained by mining (i) the query terms, (ii) the clicked documents, and/or (iii) the user sessions containing the queries. Typical methods use a combination of these factors.
Query recommendation based on clicked documents. Baeza-Yates et al. [4] propose a measure of query similarity and use it to build methods for query expansion. Their technique is based on a term-weight vector representation of queries, obtained from the aggregation of the term-weight vectors of the URLs clicked after the query. Wen et al. [30] also present a clustering method for query recommendation that is centered around various notions of query distance.
Craswell and Szummer [11] describe a method based on random walks on the query-click graph [5], and they test it for an application of image search. Fuxman et al. [13] use the query-click graph to find related keywords for advertising. Antonellis et al. [1] also use the query-click graph, and exploit the idea of co-citation through its generalization known as SimRank [14]. Mei, Zhou and Church [22] use the notion of hitting time for ranking related queries.
Query recommendation based on query reformulations. Many effective approaches focus on the analysis of user query sessions [12, 33, 16]. Fonseca et al. [12] propose a query recommendation system based on association rules applied to query logs. Zhang and Nasraoui [33] represent each user session by a complete graph where consecutive queries are connected with an edge of a predefined weight d. Not consecutive queries are connected by an edge weighted with the product of the weights on the walk connecting them.

Recent works have shown that not only the previous query, but also the long-term interests of users, are important for understanding their information needs [20, 26].
In [16], the notion of query substitution is introduced: for each query, a set of similar queries is obtained by replacing the whole query or only its sub-phrases. White et al. [31, 32] use the query rewrites observed in a query log to generate query recommendations. Sadikov et al. [27] have recently proposed to cluster the refinements of a user query by performing a random walk on a query-document graph that incorporates both session and click information.
3. PRELIMINARIES
3.1 The query-flow graph
A query-flow graph, as defined by Boldi et al. [6], is a directed graph G = (V, E, w) where:
· V = Q{s, t} is the set of distinct queries Q submitted to the search engine plus two special nodes s and t, representing a starting state and a terminal state of any user search task;
· E  V × V is the set of directed edges;
· w : E  (0..1] is a weighting function that assigns to every pair of queries (q, q)  E a weight w(q, q).
In the query-flow graph, two queries q and q are connected by an edge if there is at least one session of the query log in which q follows q. The weight w may depend on the application; in the following we simply consider the weight to be the frequency of the transition in the query log.
The edge probabilities along with other data associated to each transition, are used to segment physical sessions into missions [15] or chains [25]. Here, physical sessions are defined as sequences of the activities of a single user before a timeout of 30 minutes, while missions/chains are defined as sequences of activities that are topically related.
3.2 Spectral projection
The query-flow graph captures implicit similarity between queries: queries connected by a heavy edge are similar in the sense that they are motivated by the same user information need. Query logs collected over a few months are rich in information, but they also contain a lot of noise. Our approach is motivated by the idea of defining a similarity measure between queries that takes into account the global structure of the query-flow graph, and not only pairs of queries.
Measuring distances of nodes on large graphs is a wellstudied problem, and many approaches have been proposed, including the shortest-path distance and the commute time. A drawback of those measures is that they are very expensive to compute, as their complexity is at least linear to the number of the nodes in the graph, while one would like to have measures whose complexity depends only on the nodes under consideration (and possibly features of the nodes) but not on the whole dataset.
One of the key methods for measuring distances in a graph, which has been used extensively for visualizing graph data, is the idea of graph projection. The idea is to project the original graph into a low-dimensional Euclidean space and then measure distances between graph nodes by considering the distances of the corresponding projected points. There

516

are many techniques that can be used to obtain such projections, including multidimensional scaling [18], spectral projections [10, 17, 24], IsoMap [29], maximum-variance unfolding [28], and many more. In this paper we use the spectral projection, which we briefly describe below, via the concepts of Laplacian matrix and spectral embedding.

Laplacian matrix. Given a graph G with adjacency matrix A, the Laplacian matrix LG = D - A is computed using the diagonal matrix D, whose entry dii equal to the degree of the i-th node of G.
Spectral embedding. An embedding  : V  Rm is computed by finding the (m + 1) eigenvectors of LG that correspond to the smallest eigenvalues. Only m of these are used as the one corresponding to the smallest eigenvalue is the vector of all 1s.

The spectral embedding has the property of preserving the distances in the projected space. For "near-by" nodes u and v in the graph G, the Euclidean distance between the vectors (u) and (v) is small. Details on the properties of spectral embeddings of graphs and spectral algorithms can be found in [10, 17, 24]. The notion of "near-by" nodes in the graph is related to the expected time of coming across node v in a random walk starting from node u. Regarding the property of maintaining distances, the spectral projection optimizes a global objective function that can be interpreted as the overall distortion of the graph projection.
On the projected space, various distance metrics can be used, for example the Euclidean distance or the cosine similarity. In our experiments, we observed that cosine similarity outperforms Euclidean distance, and in the rest of the paper we focus on cosine similarity. Since the cosine between two vectors can be negative we rescale it as follows:

Sim(q, q)

=

1+

cos(q, q) . 2

Recently the notion of directed Laplacian was introduced and analyzed by Fan Chung [9]. Since the query-flow graph is a directed graph, using the directed Laplacian is more appropriate for our application scenarios. However, our evaluation showed that the projection based on the directed Laplacian does not yield any improvement.
In the next two sections we describe our experiments with projections of the query-flow graph. Our objective is to explore systematically the space of possible parameters and alternatives on defining appropriate subgraphs to project, in order to optimize the quality of query similarities.
We note that we have not tried to explore the possibility of improving our empirical results using different graph embedding algorithms. Any graph embedding algorithm can be used as a black-box in our method, instead, our main focus has been to leverage the idea that graph projections can yield meaningful notions of query similarity, and we have also experimented extensively with finding the best subgraphs to project.

Table 1: Example of manually-built clusters

Query Clusters

sun

1. the sun newspaper, mirror, times

2. earth, mars, mercury

3. sun java, sun microsystems

stone 1. stone weight, stone measurement

2. rock, granite

3. stone brick, stone masonry

spoiler 1. movie spoiler, tv show spoiler

2. car spoiler, custom car spoiler

4. FRAMEWORK FOR STUDYING QUERY SIMILARITY
4.1 Dataset
The query-flow graph was built using a set of sessions extracted from a query log from the Yahoo! search engine.
We improve the graph by estimating the probability that both the queries q and q in a transition belong to the same "search mission" [15] (also known as "query chain" [25]). This modification prunes out transitions to frequent navigational queries such as popular web portals.
Motivated by the results of some preliminary assessments, we apply a number of filters on the graph, such as removing the transitions that have frequency less than 5. We also remove s and t and prune all the nodes that remain isolated: these correspond to sessions composed by singleton queries. Altogether, starting from a graph with 58 312 610 nodes and 131 836 560 edges, we obtain a graph with 4 152 773 nodes and 7 788 232 edges. This is the graph G that we consider in the rest of the paper.
4.2 Evaluation method
Our evaluation method is summarized as follows: We select 140 queries from the log, sampling queries that are neither too frequent nor too infrequent (torso queries), as head queries tend to be of a particular type (e.g., navigational), while tail queries give information that is too sparse. We focus on single-term queries that are likely to have more than one interpretation. For each of these queries, we build a test set of related queries by selecting a small set of their most frequent successors in the query-flow graph. Each test set is then clustered into 2 to 5 clusters by human editors; the editors decided the number of clusters for each set.1 The clustering of a test set represents the ground truth for that set. Table 1 shows examples of clustered test sets. We then apply our graph projection method, and we obtain a similarity measure for queries, for which we evaluate its agreement with the human-defined clustering. We compare different variants of graph projections by testing which variant yields similarities that agree better with the golden truth.
Note that to apply our methodology we need to measure agreement between a clustering and a similarity function. We use the following measure: let V be a set and
1The clustering of the query test sets were done by four editors. We tested whether our results depend on the subjective perception of the editors about clustering, by repeating our analysis for the queries labeled by each editor separately. We found that in all cases we obtain similar results. We omit the details for lack of space.

517

C = {C1, . . . , Ck}, with For a similarity function

SVim=(q,Sqi)C, wi,ebinetarocdluucsetetrwinogscoofreVs:.

intra-cluster similarity of cluster Ci:

InSim(Ci)

=

X
qh,qj Ci,h=j

2 · Sim(qh, qj ) |Ci||Ci - 1|

inter-cluster similarity of cluster Ci:

2

3

OutSim(Ci)

=

XX 4
l=1...k,l=i qhCi

X
qj Cl

2

·

Sim(qh, qj ) |Ci||Cl| 5

The intuition is that a similarity measure agrees with clustering C if the InSim score is large compared to the OutSim score. Thus, we capture the quality of a similarity measure, with respect to the clustering C, using the ratio

MC^(Sim)

=

E[InSim(C)]CC^ . E[OutSim(C )]C C^

Given two different similarity measures, the best one is the one that maximizes the measure MC^(Sim).

4.3 Sub-graph construction method
We experiment with two alternatives for extracting subgraphs from the query-flow graph: query-dependent and query-independent methods.
Query-dependent subgraphs. Given a query q we extract a subgraph around q by a breadth-first search from q. For a graph G = (V, E) and two nodes q, q, let d(q, q) be the length of the shortest path from q to q following directed edges in E. Let Vd(q) = {q  V such that d(q, q)  d or d(q, q)  d}. For instance, V0(q) = {q} and V1(q) contains the in-neighbors and out-neighbors of q. We define the sets:

· Ed(q) = {(q1, q2)  E : q1  Vd(q)  q2  Vd(q)}

· Od(q) = {(q1, q2)  E : q1  Vd-1(q)  q2  Vd(q)}

· Id(q) = {(q1, q2)  E : q1  Vd(q)  q2  Vd-1(q)}.

We experimented with the following subgraphs of q:

· Fd(q) = (Vd(q), Ed(q))

· Sd(q) = (Vd(q), Ed-1(q)  Od(q)  Id(q))

Figure 1: Example F1(q) and S1(q).
Figure 1 shows an example. The sizes of the obtained subgraphs vary widely, with |V2(d)| ranging from 31 to 20 702 queries (median: 2 320 queries).
Query-independent subgraphs by partitioning. Querydependent subgraphs may be expensive to compute at query time, so we also experimented with query-independent subgraphs obtained by partitioning the filtered graph. The partitioning was done using metis [23] and varying the number of clusters that metis takes as a parameter.

5. OPTIMIZING THE SIMILARITY MEASURE
The objective of our study is twofold: (i) to demonstrate the effectiveness of the query-flow graph projections in order to define a measure able to capture the human notion of similarity between queries, and (ii) to optimize such a querysimilarity measure.
To address the first objective we define a baseline measure that relies on the query-flow graph but does not use projections. To obtain more refined graphs, and thus similarities of better quality, we apply the projection method locally, on the neighborhood of a given query. This approach is based on constructing query-dependent subgraphs as we discuss in detail in Section 4. Applying projections locally yields a better similarity measure, but unfortunately, the method is computationally more expensive since it requires to build a different subgraph for each query. Thus we propose a "hybrid" approach, which builds query-independent subgraphs by using partitions of the query-flow graph obtained with the metis graph-clustering algorithm.
Overall, we perform a large number of experiments to assess the following parameters: (i) the number of dimensions of the spectral embedding, (ii) the choice of the weighting scheme, (iii) the method to be used in the construction of the query-dependent subgraphs, and (iv) the number of partitions to be considered in the computation of queryindependent subgraphs.
5.1 Similarity without graph projections
Our first similarity measure is purely based on the queryflow graph: the similarity between two queries is given by the cosine similarity of their vectors of neighbors. Given a query q, we denote by N (q) its vector of neighbors in the query-flow graph.
For our test sets, the average value of MC^ for this baseline similarity measure is 1.03 with a variance of 0.03. Figure 4(a) reports the results obtained for the query watch; we observe little separation between queries in different clusters. These results suggest that the metric based on the cosine similarity of the neighborhoods of two queries does not capture well the similarity of queries. We stress here the fact that methods based on random walks [11] or hitting time [22] are not able to capture the notion of semantic similarity we aim to. In the example of Figure 4(a), the probability to end up in frequent queries like "rolex watch" or "watch free movies online", starting from the query "watch", might be quite similar even if such queries are not semantically related.
5.2 Similarity with graph projections
We describe the choices we explored in order to optimize the performance of query projections.
Choice of dimensions. In order to investigate how varying the number of dimensions of the spectral projection affects performance, we build three embeddings of G, which respectively have m = 3, m = 5 and m = 7 dimensions. To get a more complete picture, we do the same for two types of query dependent subgraphs, S2 and S3. For each of these cases, we use the projections to compute the similarity between all the pairs of queries in every test set collected, and then we compute the measure MC^ to evaluate agreement with the clusters created by hand. We also perform t-tests

518

Table 2: Average MC^ for different projections with

different number of dimensions

Dimensions

p-value

Method

3

5

7

(3 vs 7)

S2 1.9 ± 1.5 1.8 ± 1.2 1.3 ± 0.8 0.14

S3

3.3 ± 6.5 2.2 ± 2.5 1.8 ± 1.3 0.12

G

3.2 ± 9.1 1.3 ± 0.9 1.3 ± 0.9 0.19

Table 3: Average MC^ obtained for S2(q), S3(q) using different weighting schemes.

Weighting

Significant differences

bin. log raw 0.1  0.05  0.01 

S2 1.9 1.8 1.3 bin.>raw *** log>raw ***

S3 2.2 1.9 1.7 bin.>raw ** bin.>log *

Table 4: MC^ for query-dependent subgraphs. Average Significant differences

Method ± st.dev 0.1  0.05  0.01 

N

1.02 ± 0.17

G

1.22 ± 0.74 > N ***

F1

1.49 ± 2.07 > N **

S2

1.63 ± 0.88 > N *** > G ***

S3

1.68 ± 1.45 > N *** > G ***

MC 0.5 1.0 1.5 2.0 2.5

to figure out whether the differences among the various cases are statistically significant or not.
Our results, reported in Table 2, show that increasing the number of dimensions does not determine a considerable gain in terms of quality. A slight improvement in terms of less variance is observed, but the differences between the various cases are not statistically significant. For this reason we fix m = 5 in the remainder of our experimentation.
Choice of weighting scheme. In the query-flow graph, every edge connecting two queries is weighted with the frequency of the transition in the original query log, and the transition probabilities of the edges can be used to identify queries that represent similar information needs. We perform a number of tests to investigate whether taking the edge weights into account during the computation of the spectral embeddings improves our method.
We experiment with three different weighting functions:
· binary: w : E  {0, 1} s.t. w(q, q) = 1. This case corresponds to the baseline (unweighted graph).
· raw-count: w : E  N s.t. w(q, q) = c(q, q) ( c(q, q) is the occurrence count of the transition in the log).
· log(count): w : E  R s.t. w(q, q) = log(c(q, q)).
When an edge between two queries exists in both directions, we symmetrize the weights choosing the maximum of the two. This step is needed for the computation of the projection. We also tried with minimum, average or sum of the two weights, but we observed no substantial changes.
We compare the above weighting schemes for two types of query dependent subgraphs: S2(q) and S3(q). Table 3 shows the results. Binary and log weights yield the best results, whereas the usage of raw counts hurts performance, and the difference between this weighting scheme and the other two is statistically significant. For this reason we discard the raw counts, and we focus on the simplest of the two schemes that exhibit the best behavior, which is the one using binary weights. We consider projections of unweighted graphs in the remainder of this section.
Projecting directed or undirected graphs? A natural objection to the use of the spectral embedding of the queryflow graph is that the technique is intended to be applied on undirected graphs. Surprisingly preliminary experiments, which we do not present for lack of space, showed that us-

N

F1

S2

S3

G

Projection Method

Figure 2: Comparison among local projections, projection of the full graph and direct use of the query-flow graph

ing the directed Laplacian does not yield any improvement. Hence, we retained the standard method.
5.3 Query-dependent subgraphs
We now study if projecting the neighborhood of a given query improves performance. We consider three types of query-dependent subgraphs: F1(q), S2(q), S3(q). The first is the subgraph induced by the neighbors of the input query, while the other two graphs are obtained by applying two or three steps of a breadth-first search on the query-flow graph, starting from the input query.
We compare the embeddings of the query-dependent subgraphs against the methods discussed before, i.e., projecting the whole graph or using the query-flow graph directly. The results of this study are presented in Figure 2 and Table 4: N is the baseline, while the other cases represent the methods that project either the full graph G or the corresponding query-dependent subgraphs. Local projections introduce a relative improvement in the average MC^ in the range of the 16%-39%. Even if the variances of local projections are also increasing we can observe that the lower bound for both S2 and S3 match the upper bound of N . Also, the differences between the query-dependent subgraphs and the other two approaches are statistically significant (see Table 4).
Figure 5 reports a qualitative comparison of the above methods for the query "watch".
5.4 Query-independent sub-graphs
Using local projections of small query-dependent subgraphs allows to define a metric that captures better the similarity between queries. However, this method has the clear drawback of requiring query-time computational processing, which may be expensive. For this reason we investigate whether projecting larger, query-independent subgraphs allows to trade-off performance and computational costs.

519

Table 5: MC^ for varying numbers of clusters; none of the pair-wise differences is statistically significant

Expansion Number of Average

Method distance clusters ± st. dev

Full graph

-

-

1.2 ± 0.2

Clustering

1

100

1.0 ± 0.1

Clustering

2

100

1.2 ± 0.5

Clustering

2

200

2.0 ± 10.3

Clustering

2

1 000 4.2 ± 25.8

Clustering

2

5 000 3.9 ± 27.0

Clustering

2

20 000 6.3 ± 46.7

MC 0.5 1.0 1.5 2.0 2.5

N

F1

S2

S3

G

M100 M1K M20K

Projection Method

We generate sets of query-independent subgraphs using the metis algorithm, which partitions the nodes of a graph into balanced clusters minimizing the number of edges in the cut. The number of clusters to be created is chosen by the user. In the following, we briefly describe how we use a partition of the nodes in the query-flow graph created with metis to derive query-independent subgraphs. We then compute the spectral embeddings of these subgraphs.
Cluster expansion. We first partition the graph into 100 and 200 clusters. We choose these values because they yield cluster sizes comparable to the size of query-dependent subgraphs for which our method obtains the best performance.
As first attempt, we directly project the partition created by metis. This solution performs very poorly (results are omitted due to lack of space), because the raw clusters do not typically include a significant fraction of the neighborhood of each node.
To overcome this limitation, we study how to make the partitions include (a significant fraction of) the neighborhood of each node assigned to them. We test two methods. The first strategy consists of enlarging each partition with the in/out-neighbors of every node originally included in it -- so that clusters may overlap. We found that this method does not improve performance.
We then experiment with a more expensive strategy, which consists of adding to each partition the two-step neighborhood of every node originally assigned to it. This solution creates larger clusters, and experimental evaluation shows that it provides results comparable to those obtained by projecting the full graph (see Table 5). Hence, we retain this approach as our cluster expansion method.
Choice of number of clusters. The expansion step is necessary to make the clusters include many neighbors of a given node, i.e., queries that are likely to be related to the input query. However, this operation creates clusters of very large size: we believe that this can be a reason for not having a significant improvement in performance.
Hence, we perform more extensive experiments, using metis to divide the query-flow graph into a larger number of partitions and applying the expansion step to the sets of nodes obtained.
Table 5 shows how the method behaves with 1 000, 5 000 and 20 000 clusters. Starting from 5 000 we get a little improvement in performance, but the differences are not significant at p < 0.1. Although the method performs at least as well as the projection of the full graph, our intuition is that it would be worth exploring other approaches to extract query-independent subgraphs from the query-flow graph.

Figure 3: Summary of performance of some systems
5.5 Summary
Figure 3 summarizes the tested methods, which can be divided into three groups:
· Using query-flow graph without projection (N ): this performs close to random;
· Projecting the whole graph (G) or query-independent clusters (MK): good performance, with a possible advantage to systems based on clustering;
· Projecting query-dependent sub-graphs (Fd or Sd): performs the best, with a small advantage for the systems that expand the neighborhood at 2 or 3 steps over the subgraph induced by the direct neighbors.
With respect to effectiveness, a positive result is that for all the projection-based methods we tested, in at least 75% of the cases the system gave a larger similarity to queries in the same user-defined clusters than to queries in different user-defined clusters.
6. APPLICATION TO QUERY RECOMMENDATIONS
In this section we describe how we apply the proposed method for producing diverse query recommendations. Diversification of search results or query recommendations is a strategy adopted by search engines to improve the user experience and minimize the risk that the information need of the user will not be satisfied.
For our experiment we use a random sample of 100 queries. For each query, we generated a set of baseline query recommendations using a method suggested in [7]. This method (QueryFlow-SP) associates a query q with a set Q of recommendations obtained by selecting the most frequent reformulations from q. Each query q  Q is assigned a ranking score given by the frequency of the transition (q, q). This method was shown to perform as well as more sophisticated recommendation algorithms. In the following, we refer to it as the baseline.
Next, we use the diversification method described by Manca and Pintus [21]. The idea is to apply a greedy search that maximizes diversity while maintaining high relevance. The algorithm takes as input the set Q and builds a diverse set A of queries. First, the query q0  Q with the highest relevance score is selected, removed from Q, and inserted into A. Observe that this ensures that the most popular query related

520

Query: watch

rolex watch citizen watch seiko watch watch free

watch

movie online movies.net

rolex watch

-

0.052

0.044

0.003

0.002

citizen watch

0.052

-

0.087

0.004

0.002

seiko watch

0.044

0.087

-

0.004

0.003

watch free movie online

0.003

0.004

0.004

-

0.04

watch movies.net

0.002

0.002

0.003

0.04

-

(a) Method: N , i.e., cosine similarity between vectors of neighbors in the QFG

Query: watch
rolex watch citizen watch seiko watch watch free movie online watch movies.net

rolex watch citizen watch seiko watch watch free

movie online

-

0.99

0.99

0.12

0.99

-

1.00

0.09

0.99

1.00

-

0.12

0.12

0.09

0.12

-

0.12

0.08

0.11

1.00

(b) Method: spectral embedding of G

watch movies.net
0.12 0.08 0.11 1.00
-

Figure 4: Example: query similarities using cosine similarity of neighbors (no projection) and projection of the full graph for the query watch . Lines separate manually-assigned clusters for these queries.

Query: watch
rolex watch citizen watch seiko watch watch free movie online watch movies.net

rolex watch
1.00 1.00 -0.57 -0.58

citizen watch
1.00 -
1.00 -0.56 -0.57

seiko watch
1.00 1.00
-0.57 -0.57

watch free movie online
-0.57 -0.56 -0.57
1.00

watch movies.net
-0.58 -0.57 -0.57 1.00
-

Figure 5: Example query similarities using projection of the subgraph S2(q) for the query watch

to the input query is always selected for recommendation. Next, the algorithm starts an iterative phase: at each step the query q  Q with maximum score s(q) is removed from Q and inserted into A. The score s(q) is a combination function of the relevance of q with the distance d(q, A) of query q from the set A of queries that have already been selected. The algorithm balances diversity with relevance. Given that the two measures, distance from other queries and relevance, are not comparable, the algorithm tries to maximize the product of the two, picking up queries that have a high ranking score while being not too similar to the queries that have already been selected. We omit the details of the diversity algorithm since it is not the focus of this paper. We derive the distance metric that we use for diversification from our projection method. We experiment with three schemes: N , G and S2(q). These methods measure similarity between queries in terms of cosine similarity between (a) their vectors of neighbors in the query-flow graph; (b) the vectors associated with the queries in the projection of the full graph; (c) the vectors obtained projecting the subgraph S2(q). In the case of N , we define the distance of a query q from the set A as the minimum distance between q and a query in A:
d(q, A) = mintA{d(v(q), v(t))}.
In the case of G and S2(q) the distance of q from A is defined as the distance between q and the centroid c(A) of the set A.
d(q, A) = d(v(q), c(A)).
Perceived diversity. The task is highly subjective, and when measuring the agreement of the assessors on a subset of questions in which they overlap, we observe a moderate level of agreement (Cohen's  = 0.49).
In Table 6 we show the results of this evaluation. The results are expected given the results from previous sections, as

Table 6: Result of user test for assessing diversity

of recommendations Prob. B is more/less diverse than A

N

G

S2

Baseline 0.30/0.13 * 0.39/0.25 * 0.51/0.14 ***

N

-

0.42/0.25 ** 0.49/0.15 ***

G

-

-

0.46/0.25 **

Significance: 0.1  0.05  0.01 

S2(q) is the best method (in 51% of the cases it is perceived as more diverse than the baseline, in 14% of the cases as less diverse), followed by G (projecting the full graph), followed by N (using the graph without applying projection).
Perceived relevance. We examine 100 queries and take the union of the top-3 recommendations from all the systems that are compared. This yields 460 distinct query pairs. The assessment is to measure if the recommendation is relevant to the original query. In this case, the inter-assessor agreement is  = 0.53.
The accuracy of relevant queries that the baseline algorithm recommends is 97%. When using the projection on the full graph, this figure drops to 90%, and to 92% when using the S2 method. Instead, when using N there was basically no drop in relevance, measuring a 97% of recommendations relevant to the original query. These results suggest that (i) N does only a small change in the recommendations, and (ii) S2 and the method that projects the full graph change the recommendations but still keep the fraction of recommended queries that are relevant to the original query at 90% or more.

521

7. CONCLUSIONS
We have shown that projecting the reformulation graphs in a low-dimensional space allowed us to define a similarity measure between queries. To the best of our knowledge, this is the first attempt to apply spectral methods to queryreformulation analysis.
After methodically exploring several design choices, we found two methods that perform well: one for off-line processing and one for on-line processing. The method for offline processing basically works by storing 3-5 spectral coordinates per query and then using them at query time at constant cost. The method for on-line processing requires computing a small subgraph at query time and then projecting this graph to obtain the coordinates. Our experiments suggest that the on-line method is a more effective similarity measure, but of course it has a higher computational cost.
To demonstrate the practical impact of our method, we tested our measure as a component of a system for producing diverse query recommendations. Our experiments show that our method can be used to produce diverse recommendations at small cost of relevance.
As future work, we would like to seek more effective offline methods than the projection on the full graph. We also plan to investigate alternative projection methods, as well as the use of other query graphs, such as the click graph.
Key references: [6, 10].
8. REFERENCES
[1] Antonellis, I., Garcia-Molina, H., and Chang, C.-C. Simrank++: Query rewriting through link analysis of the click graph. In VLDB (2008).
[2] Baeza-Yates, R. Graphs from search engine queries. In Theory and Practice of Computer Science (SOFSEM) (2007).
[3] Baeza-Yates, R., and Tiberi, A. Extracting semantic relations from query logs. In KDD (2007).
[4] Baeza-Yates, R. A., Hurtado, C. A., and Mendoza, M. Query recommendation using query logs in search engines. In EDBT Workshops (2004).
[5] Beeferman, D., and Berger, A. Agglomerative clustering of a search engine query log. In KDD (2000).
[6] Boldi, P., Bonchi, F., Castillo, C., Donato, D., Gionis, A., and Vigna, S. The query-flow graph: Model and applications. In CIKM (2008).
[7] Boldi, P., Bonchi, F., Castillo, C., Donato, D., and Vigna, S. Query suggestions using query-flow graphs. In WSCD (2009).
[8] Borges, J., and Levene, M. Evaluating variable-length markov chain models for analysis of user web navigation sessions. IEEE Trans. Knowl. Data Eng. 19, 4 (2007), 441­452.
[9] Chung, F. Laplacians and the cheeger inequality for directed graphs. Annals of Combinatorics 9, 1 (2005).
[10] Chung, F. R. K. Spectral Graph Theory (CBMS Regional conf. Series in Mathematics, No. 92). American Mathematical Society, February 1997.
[11] Craswell, N., and Szummer, M. Random walks on the click graph. In SIGIR (2007).
[12] Fonseca, B. M., Golgher, P. B., de Moura, E. S., and Ziviani, N. Using association rules to

discover search engines related queries. In LA-WEB (Washington, DC, USA, 2003).
[13] Fuxman, A., Tsaparas, P., Achan, K., and Agrawal, R. Using the wisdom of the crowds for keyword generation. In WWW (2008).
[14] Jeh, G., and Widom, J. Simrank: a measure of structural-context similarity. In KDD (2002).
[15] Jones, R., and Klinkner, K. L. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM (2008).
[16] Jones, R., Rey, B., Madani, O., and Greiner, W. Generating query substitutions. In WWW (2006).
[17] Koren, Y. On spectral graph drawing. In COCOON (2003).
[18] Kruskal, J. Nonmetric multidimensional scaling: A numerical method. Psychometrika 29, 2 (1964).
[19] Levene, M., and Loizou, G. A probabilistic approach to navigation in hypertext. Inf. Sci. 114, 1-4 (1999), 165­186.
[20] Luxenburger, J., Elbassuoni, S., and Weikum, G. Matching task profiles and user needs in personalized web search. In CIKM (2008).
[21] Manca, M., and Pintus, E. Diversity in web search. Master's thesis, University of Cagliari, Italy, 2009.
[22] Mei, Q., Zhou, D., and Church, K. Query suggestion using hitting time. In CIKM (2008).
[23] METIS ­ Family of multilevel partitioning algorithms. http://glaros.dtc.umn.edu/gkhome/views/metis/.
[24] Ng, A. Y., Jordan, M. I., and Weiss, Y. On spectral clustering: Analysis and an algorithm. In NIPS (2001).
[25] Radlinski, F., and Joachims, T. Query chains: learning to rank from implicit feedback. In KDD (2005).
[26] Richardson, M. Learning about the world through long-term query logs. ACM Trans. Web 2, 4 (2008).
[27] Sadikov, E., Madhavan, J., Wang, L., and Halevy, A. Clustering query refinements by user intent. In 19th International World Wide Web Conference, WWW 2010. (2010).
[28] Sun, J., Boyd, S., Xiao, L., and Diaconis, P. The fastest mixing markov process on a graph and a connection to a maximum variance unfolding problem. SIAM Review 48 (2004), 2006.
[29] Tenenbaum, J. B., Silva, V., and Langford, J. C. A global geometric framework for nonlinear dimensionality reduction. Science 290, 5500 (2000).
[30] Wen, J.-R., Nie, J.-Y., and Zhang, H.-J. Clustering user queries of a search engine. In Proc. of the 10th WWW conf. (2001).
[31] White, R. W., Bilenko, M., and Cucerzan, S. Studying the use of popular destinations to enhance web search interaction. In SIGIR (2007).
[32] White, R. W., Bilenko, M., and Cucerzan, S. Leveraging popular destinations to enhance web search interaction. ACM Trans. Web 2, 3 (2008), 1­30.
[33] Zhang, Z., and Nasraoui, O. Mining search engine query logs for query recommendation. In Proc. of the 15th WWW conf. (2006).

522

The Demographics of Web Search

Ingmar Weber
Yahoo! Research Barcelona Diagonal 177, 08018 Barcelona, Spain
ingmar@yahoo-inc.om

Carlos Castillo
Yahoo! Research Barcelona Diagonal 177, 08018 Barcelona, Spain
chato@yahoo-inc.com

ABSTRACT
How does the web search behavior of "rich" and "poor" people differ? Do men and women tend to click on different results for the same query? What are some queries almost exclusively issued by African Americans? These are some of the questions we address in this study.
Our research combines three data sources: the query log of a major US-based web search engine, profile information provided by 28 million of its users (birth year, gender and ZIP code), and US-census information including detailed demographic information aggregated at the level of ZIP code. Through this combination we can annotate each query with, e.g. the average per-capita income in the ZIP code it originated from. Though conceptually simple, this combination immediately creates a powerful user modeling tool.
The main contributions of this work are the following. First, we provide a demographic description of a large sample of search engine users in the US and show that it agrees well with the distribution of the US population. Second, we describe how different segments of the population differ in their search behavior, e.g. with respect to the queries they formulate or the URLs they click. Third, we explore applications of our methodology to improve web search relevance and to provide better query suggestions.
These results enable a wide range of applications including improving web search and advertising where, for instance, targeted advertisements for "family vacations" could be adapted to the (expected) income.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process; H.1.2 [User/Machine Systems]: Human factors
General Terms
Human Factors, Measurement
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
Web search, Demographic factors
1. INTRODUCTION
What kind of web results would you personally want to see for the query "wagner"? Well, if you are a typical female US web user you probably have pages about the composer Richard Wagner in mind.1 However, if you are a male US web user you are more likely to be referring to a company called Wagner which produces paint sprayers.2 Similarly, the term most likely to complete the beginning "hal" is in general "lindsey"3, whereas for people living in areas with an above average education level the most likely completion is "higdon"4 These two examples illustrate that demographic factors have a measurable influence on search behavior.
Even though this user modeling can be an interesting end by itself, the ultimate goal for the search engine is to provide a better service to users. This can take the form of more relevant search results, more helpful query suggestions, more interesting news items on a portal page or, last not least, improved targeting of advertising. Of course, the potential for improvements in these areas is not new and there exists a rich set of literature on the use of personalization [6, 26].
Our approach of grouping users by demographic features, such as age or income, is different from personalization where models are learned for individual users. Given enough data about individual users, personalization can indeed be very powerful. However, our approach is more applicable in practice because (i) it requires less training data, as we use a small number of different user classes, (ii) it has less potential for breaching the user's privacy, as we do not aggregate information on a per-user level, and (iii) it has a simpler interpretation, as we use real-life features with clear semantics instead of latent variables.
As far as sponsored search is concerned, especially this last issue is of relevance as advertisers could, e.g., choose to have their ads displayed only for users in a particular expected income range. This would go well beyond the current "demographic site selection", which uses only age and gender
1For women the most clicked URL was http://en.wikipedia.org/wiki/Richard_Wagner. 2For men the most clicked URL was http://www.wagnerspraytech.com/. 3Hal Lindsey is an American evangelist and Christian writer. 4Hal Higdon is an American writer and runner.

523

and works only for banner ads, but is not widely used for sponsored search.5
The main contributions of our study are:
· we demonstrate how public information for ZIP codes can be used to annotate both web queries and URLs with demographic features;
· we show that the user population of a large, commercial search engine is representative of the whole US population;
· we uncover differences in search behavior across demographic segments, e.g. in the form of "representative" queries; and
· we show that using demographic information has a potential to improve state-of-the-art web search results, especially for difficult queries, and that it leads to improvements in query suggestions.
The rest of this paper is organized as follows. The next section outlines previous work related to ours. Section 3 describes how we obtained and processed the data. Section 4 discusses our methodology. Basic characteristics of the demographics of web searches are shown in Section 5. The impact of using demographic information on three different application scenarios, (i) web search, (ii) automatic labeling of URLs, and (iii) query suggestions, is quantified in Section 6. As we see our work as a first, exploratory study, we discuss possible extensions in Section 7. The final section presents concluding remarks.
2. RELATED WORK
Inferring demographics from behavior. It is well established that the writing style of texts can be used to infer characteristics of their authors, such as gender or age [1, 2]. Koppel et al. [23] apply this method to blog pages.
Search logs kept by search engines have been used to infer demographic characteristics of their users. Hu et al. [12] represent users by the words in the pages they click on, and by the output of a topic classifier on those pages. They are able to determine gender with 80% accuracy and age (discretized in 5 ad-hoc age groups) with 50% accuracy. Jones et al. [15] represent users by the queries written by them. They achieve 84% accuracy on gender and 79% accuracy on age (within ±10 years of error). In the current paper we use an approach that is related to both: we represent users by the queries they write and by the identity (but not the contents) of the documents they click upon.
Gender and internet usage. Jackson et al. [13] interviewed 600 undergraduate students and found that in general females used e-mail more and web less than males ("Women communicating and men searching" is the sub-title of their article). Gavin et al. [14] interviewed over 600 students and found that males used the internet in general more than females, but found no evidence of a different pattern of communication vs search among genders. Specifically in the case of web search, Lorigo et al. [17] use eye-tracking to uncover differences in search strategies across different types of tasks and among genders.
5http://adwords.google.com/support/aw/bin/answer. py?hlrm=en&answer=33743

Income, race, and the digital divide. When demographic dimensions such as income or age are related with internet usage, an unequal access is observed, which is known as the Digital Divide [22]. The digital divide, for instance, establishes a gap between countries with a high gross domestic product and countries with a low gross domestic product in terms of broadband access. The digital divide also can be observed within a country by itself. For instance, in the US it has been observed that more income and more education are correlated with a higher probability of having a computer at home, but that African Americans6 are much less likely to have a computer at home across all levels of income and education [10].
The need for demographic-aware search. Morgan and Trauth [20] advocate for taking into account individual differences in web search. Ford, Miller and Moss [7, 9, 8] have studied this topic extensively, emphasizing the differences among the strategies people with different cognitive styles use to understand information and acquire knowledge. For a survey of works on search behavior, see [11].
Groupization to improve search. Teevan et al. [27, 21] investigated the benefit of using "groupization" rather than personalization to improve web search ranking. In their studies, done in a work environment with a total of roughly 200 participants, they showed that pooling search history information from members in the same "group", ranging from job function to gender, can lead to gains in normalized Discounted Cumulative Gain (DCG), in particular for grouprelated queries. Apart from the setting (work environment with access to desktop information vs. anonymized use of query logs), and the scale (< 1,000 users vs. > 1,000,000 users), one of the main differences to our work is that we are also interested in descriptive results (see Section 5), as well as applications apart from web search (see Section 6).
Conditional entropy of search logs. The question of how much could we improve web search by knowing the identity, or rather the IP address, of a web user, was studied in [19]. Specifically, they measure the conditional entropy H(U RL|Q, IP ) and compare it to H(U RL|Q). Though these are interesting and intuitive measures to look at, there are certain kinds of methodological problems in practice. For example, even when tuples (U RL, Q, R) for a random variable R with H(R) = H(I) were considered, where R is independent of both Q and U RL, then for a fixed pair (Q, R) the number of matching tuples (U RL, Q, R) would be small. This means that the distribution of U RL within a "bucket" of a given value of R will naturally look homogeneous, as the bucket is far to small to represent an adequate sample of the random variable U RL. Therefore, conditioning on any additional variable X, be it IP or R, will always lead to a drop of H(U RL|Q, X) compared to H(U RL|Q), even if X is independent of both U RL and Q. This problem could potentially be avoided by looking at the cross entropy, i.e. by learning the distribution of the triples (U RL, Q, IP ) on one set of training data, and then using these probabilities to estimate H(U RL|Q, IP ) on a different set of test data. However, this approach is also problematic as a single URL in the test set, which was unseen in the training set,
6We use the term "African American" as a short-hand for the term "black or African-American" as used in the official US 2000 census. See http://factfinder.census.gov/ home/en/epss/glossary_r.html#race.

524

would yield an infinite entropy estimate. Ignoring any such unseen URLs will, on the other hand, lead to a biased underestimate of the actual entropy. These issues could be addressed by comparing the the empirical conditional entropy H(U RL|Q, IP ) to H(U RL|Q, R) where H(IP ) = H(R) for an independent random variable R. But even then the results would remain difficult to interpret, as it is not clear, for instance, how much a reduction in conditional entropy of 5% would affect the web search results as the entropy might be reduced without changing the ranking of URLs. Hence, we preferred to look at a more tangible measure, the precision at one. Our methodology is described in detail in Section 4.
3. DATA ACQUISITION AND PROCESSING
For this study, we used the following three sources of information. First, a subset of the query log data for US search traffic of the Yahoo! web search engine. Second, profile information (birth year, gender and ZIP code) provided by registered users. Third, publicly accessible demographic information for US ZIP codes, obtained in the 2000 census7, and joined with the other data sources on the ZIP code (explicitly provided by users).
3.1 Query Log Preprocessing
The collected data went through the following cleaning and preprocessing steps. First, only web searches issued between October 1, 2008 and September 30, 2009 were considered. Second, we used a subset of the queries in this time window, sampled uniformly at random. Third, only web searches by users logged into our services were used, and to protect users' privacy, all user identifiers were hashed using a non-invertible function. Furthermore, during the whole process, nothing was ever aggregated on a per-user level, which is one of the advantages of our proposed approach.
Fourth, only web searches (i) originating from the USversion of the search engine, and (ii) pertaining to users with a valid ZIP code were used.8 Fifth, queries without clicks on URLs were discarded. We denote the pairs of query and clicked result URL by (query, URL) and when multiple URLs where clicked for the same query, multiple such pairs were generated. Sixth, queries were cast to lower case but no stemming was applied and all special characters (such as apostrophes) were kept. Seventh, immediate, repeated duplicates of (query, URL) pairs by a single user were conflated to a single instance. Note that we still kept repeated (query, URL) pairs for a single user as long as there were other pairs in between.
The final preprocessing step depended on the final analysis intended and we considered (input, target) pairs of different types. Concretely, in our first set of experiments we analyzed factors predicting the clicked URL (target) for a given query (input). For our second set of experiment, we looked at differences in how different queries (target) are used to describe the same URL (input). For our third set of experiments, we investigated potential improvements for "suggest as you type" interfaces and we only used queries which started with a sequence of non-white characters of minimum
7This data is freely available at http://factfinder. census.gov/. 8Some of the user-provided zip codes were non existent, e.g. 12345, or they corresponded to an area without any registered inhabitants, e.g. 01244.

length two (input) followed by another such sequence (target). In the vast majority of cases these sequences were actual terms, but tokens including special characters were also kept. In all three settings, we only used (input, target) pairs where the input had a support of at least two users, i.e. there where at least two matching (query, URL) pairs (after the initial screening) originating from different users.
The remaining (input, target) pairs were then labeled with demographic information derived either directly from the user's profile (birth year and gender) or derived from using demographic information pertaining to ZIP codes. Details of this annotation process are described in the following section. The sizes of our final data sets are given in Table 1.

Input

 Target

Distinct Distinct Pairs inputs users

query  URL 479 M URL  query 588 M 1st term  2nd term 509 M

22.7 M 41.7 M 1.6 M

28.3 M 29.7 M 25.3 M

Table 1: Basic statistics for the preprocessed query log used for our study. In all cases, each input was used by at least two distinct users.

3.2 Demographic Feature Extraction
Apart from the the birth year and the gender, which were directly provided by the users, we also used the provided ZIP code to annotate each (input, target) pair with additional information. Concretely, we obtained the average values for the following features for each ZIP code:
· per capita income (in 1999 US dollars) [P-c income k$], · bachelor's degree or higher, for population 25 years
and over [BA degree %], · individuals below poverty level [below poverty %], · race: white [white %], black or African American [African
American %], Asian [Asian %], · speaks a language other than English at home, for pop-
ulation 5 years and over [non-English %]. In all cases, the full name is as in the official census information and, in brackets, is a shorthand used by us. For our current study, we decided to limit ourselves to this subset of features, but in future studies we might also include other available data such as "mean travel time to work in minutes" or "average family size". Note that the demographic features are not independent and, e.g., areas where a large percentage of the population holds a BA degree tend to have a higher per capita income as well.
The labels applied to each (input, target) pair were discretized. For all demographic features we used quintiles: the percentile intervals [0%, 20%], (20%, 40%], ..., (80%, 100]. E.g., a ZIP code with no more than 12.8% of its population 25 years and over holding a bachelor's degree would be placed in the lowest quintile for the corresponding feature and, similarly, users born after 1982 would be in the youngest quintile as described in Table 2. Only for (i) the gender, where only two buckets were used, and (ii) the ZIP code itself, where we used only the two leading digits giving a total of 99 buckets, we did not use quintiles.9 As we were not interested in merely geographical differences, we only
9The ZIP prefix 09 refers solely to locations outside the US, but all other prefixes from 00 to 99 were present. See http: //en.wikipedia.org/wiki/ZIP_code_prefixes.

525

used the shortened ZIP code to filter out localized queries from the lists in Table 3 but did not use them for anything else. All percentiles were derived from (query, URL) pairs and the corresponding buckets were used in all experiments, even if the percentiles for other (input, target) combinations would have differed slightly. Percentiles were computed on a per query instance basis and not on a per-user basis.
The reason for this discretization is that we explicitly wanted to de-personalize and de-localize our analysis. E.g., using the full birth year could potentially isolate a very small user group and the same holds for using the exact average per-capita income, which would correspond to using the full zip code. Given even larger amount of query log data, one could investigate the differences in web search behavior between users born in, say, 1978 and in 1977, or the differences for the zip codes 95967 and 95969. However, one of the strengths of our approach is exactly the fact that we work with more abstract features, such as "very young" (youngest 20%) and "very old" (oldest 20%) or "very rich" or "very poor", thereby allowing a more intuitive interpretation.

Feature

Query-log data

US

20% 40% 60% 80% avg. avg.

P-c income k$ 16.0 18.9 22.4 27.7 22.7 21.6

Bel. poverty % 4.5 7.2 10.9 16.5 11.1 12.4

BA degree % 12.8 18.1 25.6 37.6 25.5 24.4

White % 61.9 78.8 88.1 94.4 76.9 75.1

Afr. Amer. % 0.9 2.4 5.7 15.5 4.0 12.3

Asian % 0.4 1.1 2.3 5.1 4.0

3.6

Non-English % 4.5 7.9 14.0 27.3 17.3 17.9

Year of birth 1956 1966 1974 1982 1968.7 1974

Gender 49.7% female 50.3% male 49.1% vs 50.9%

Table 2: Aggregated per-query demographics from our query-log data, compared to the US average from census data (last column).

Table 2 summarizes the per-query demographics in our dataset, and compares them with averages in the US population, shown in the last column.10 Although the averages agree for most features, ZIP codes with a high percentage of African American population appear to be underrepresented in our data set. This is consistent with findings on the "Racial Divide" [10]. For birth year, 1974 in the table for US population, only the median and not the (lower) mean birth year was available for the US population. As infants are no web users the average for our data set is expected to be below the US average.
3.3 Data Quality
Though it is certain that a fraction of users provided false profile information, sometimes deliberately, and that some of this will have escaped the test for the validity of a ZIP code, our results (Sections 5 and 6) show that the remaining signal is still significant. In settings with poorer data quality or without any profile information, approaches to automatically learn a user's profile from her search history seem promising [15].
Instead of using the user-provided zip code, we could have derived the ZIP code by mapping the user's IP address to
10Details about the US averages and the definitions of the features can be found at http://factfinder.census.gov/

a geographical location.11 We did, however, not experiment with this approach as we expect that (i) the accuracy of the mapping is not high enough (as a "poor" ZIP code can be geographically neighboring a "rich" ZIP code) and that (ii) the mistakes could have a systematic bias (as internet providers could tend to have their routers in neighborhoods of a particular demographic profile). Still, if sufficiently accurate, such mappings could be used to extend the applicability of our work significantly by enabling a "demographic profiling" for requests to arbitrary web servers. Information derived from IP addresses could, when the IP-derived location differs significantly from the actual location, also be used to remove false or outdated ZIP code information. For the present study, we opted not to do this and rather to work with the "raw" data instead.
Note that we do not claim that web users are always representative of their area. Trivially, any individual user could always deviate in an arbitrary way from the "typical" resident of her neighborhood. But even the aggregated averages might not be representative. E.g. in a poor neighborhood the majority of its web users could be made up by people who are, by the local standards, financially better off than their ZIP code would imply. We are only claiming that despite such drawbacks our derived "labels" are still useful for (i) eliciting typical differences (see Section 5) and for (ii) predicting user behavior (see Section 6).

4. METHODOLOGY

The main objective of our experiments is to measure how much demographic information helps to improve the ranking of targets, e.g. clicked URLs, for a given input, e.g. a given query. More formally, let X, Y and D be random variables corresponding to the input, target and demographic information respectively. Similarly, let x, y and d be actual instances of values of these random variables. In this formulation we want to know how often the mode of the distribution changed, i.e. how often argmaxyP (y|x, d) = argmaxyP (y|x). For both conditional distributions we required the two top ranked values to correspond to distinct probabilities, not counting cases of ties as improvements in ranking.
In other words, we looked at the improvement of the "precision at one" (P@1), assuming that (i) the search engine ranks according to the empirical click probability, which is almost always the case12, and that (ii) a click is an indication of relevance so that P@1 is identical to the click-through-rate for the value with the highest (empirical) click probability.
For our evaluation, we only considered cases with sufficient demographic coverage to possibly allow a re-ranking. Concretely, we only considered inputs x with a support, in the probabilistic sense, of at least 100 users for some some combination (x, d) as well as at least 400 users for other values of d. See Table 6 for experimental results for this setup. The alternative possibility of using conditional entropy [3] was discussed in Section 2. Note that we do make use of the conditional entropy for descriptive purposes, in particular in Section 5, but we do not use it for performance evaluation.

11See e.g.

http://www.hostip.info/ for an open

community-based project to map an IP address to geograph-

ical locations.

12Of course, there is also a feedback loop and the highest

ranked results will get clicked more often, independent of

their relevance.

526

5. BASIC DEMOGRAPHIC DIFFERENCES
We begin by presenting partly anecdotal evidence describing how different demographic groups differ in their web search behavior. Table 3 lists the four most "discriminating" queries for different demographic groups, such as very young people, or people living in predominantly "white" neighborhoods. Queries are ranked by the average feature value, where the average is on a per-query basis; adult queries are not included.
If this were the only ranking criterion, then the list would be dominated by localized queries from single neighborhoods with highly skewed demographic distributions. E.g. the "richest" query with a support of at least 16 users would be "paws for life" (145k$, 43 occurrences, 28 users) and the "most educated" "www.diamondbacks.com" (73.3% BA degree, 333 occurrences, 22 users). The first refers to an animal charity in Maryland and the second to a baseball team in Arizona. As such examples fail to illustrate the full potential of our approach, for Table 3 we imposed the additional filter that each query must satisfy H(short ZIP|query)  4.0, where short ZIP refers to the first two digits of a five digit ZIP code. Note that H(short ZIP) = 6.23 so that a query with H(short ZIP|query) = 4.0 covers about 24.0/26.23 = 21% of the US. Note that the entropy constraint also leads to the requirement of at least 16 distinct users for each query. The lists are generated such that "given the query, the demographics is determined", and not the other way around.
Many of the queries in Table 3 agree with stereotypes. For example, queries predominantly issued by young users tend to be related to chat rooms, music and social networking sites. Queries which are issued exclusively by male users in our sample are related to sports, or computer hard- and software. Queries from areas where a language other than English is often spoken at home, turn out to be written in Spanish. The "s2s magazine" query, found in the list for the feature "African American", refers to a magazine "covering the world of black entertainment"13. The query "tvb series", found in the list for the feature "Asian", refers to a series from a TV station based in Hong Kong.
In some cases the entries are less expected. E.g. "www.unitnet.com" is often issued as a web query from ZIP codes where a substantial amount of individuals lives below the poverty line, 26.4% compared to 12.4% US average. This site is the portal page for the "directors" ­salespeople­ of Mary Kay, a brand of skin care and color cosmetics which has been criticized by its opponents as a "product-based pyramid scheme" [5]. Also unexpected is the fact that many queries indicative for older people relate to annual shareholder meetings.
Though not our main focus, we also report on observations relating to differences in the actual search process. Table 4 shows that there is a slight but detectable trend for people with a university degree, or at least living in areas where this is more common, to type longer queries, to click more often on a single result, and to click on "deeper" URLs, meaning that they contain more "/"'s. Query length refers to the number of sequences of non-white-spaces. URL depth refers to the number of parts of a URL separated by a "/" and not including the protocol. E.g. "sigir's homepage" has query length 2 and http://www.sigir.org/
13http://www.dmoz.org/Society/Ethnicity/African/ African-American/News_and_Media/Magazines/

Feature

Query

Value

Per-

chris jordan

81k

capita

electric candle warmer

78k

income

www.popsugar.com

75k

k$

ns4w.org

65k

below

www.unitnet.com

26.4

poverty

slaker

25.8

line

kipasa

24.9

%

www.tokbox.com

24.4

BA spencer stuart executive search 55.5

degree

insight venture partners

54.2

%

federal circuit

53.2

four seasons jackson hole

52.8

White

pulloff.com

97.1

%

central boiler wood furnace

96.2

firewood processors

96.1

midwest super cub

95.5

African

trey songz bio

63.8

Americ.

def jam records address

58.4

%

s2s magazine

58.1

madinaonline

56.0

Asian

sina

25.1

%

big bang lyrics

24.3

tvb series

24.2

jay chou lyrics

23.5

Non-

mis novelas favoritas

60.5

english

sinonimos

59.2

lang.

juegos para baby shower

54.5

%

dichos mexicanos

54.3

birth www.johnshopkinshealthalerts.com

year,

www.envisionreports.com/vz

"old"

yahoo free bridge games

bnymellon.mobular.net/bnymellon/frp

birth

free teen chatrooms

year,

wet seal

"young"

tottaly layouts

photofiltre brushes

1931 1935 1935 1935 1991 1991 1990 1990

gender,

scrapbook myspace layouts

100

female

eyeshadow for brown eyes

100

%

twilight movie screensavers

100

plus size jewelry

100

gender,

2009 nfl team rankings

100

male

football big board

100

%

resharper

100

radeon x600

100

Table 3: Some highly-discriminating queries for our demographic features. Note that these queries, though discriminative in terms of P (D|Q), are not necessarily typical for a demographic group in the sense of P (Q|D). For illustrative purposes we show queries with a nation-wide focus by requiring a minimum entropy of 4.0 over the first two ZIP digits.

events/events-upcoming.html has URL depth 3. As for each of the three groups in Table 4 the numbers are computed over roughly 95.8M = 0.20 × 479M (query,URL) pairs, these differences, though small, are statistically significant at a confidence level well below 0.001, using a t-test for equality of means.

527

% BA degree
Lowest 20% Middle 20%
Top 20%

query length
2.25 2.28 2.32

click entropy
1.74 1.67 1.60

URL depth
1.92 1.95 1.99

Table 4: People that are more likely to have a university degree (based on where they live), (i) type longer queries, (ii) click more often on a single result URL, and (iii) click on "deeper" URLs.

As can already be seen in Table 3, older people tend to be more likely to use URLs as web queries. Out of all 15.7M (query, URL) pairs where the query started with "www." 29% were due to the "oldest" quintile, indicating an overrepresentation of this group for this behavior. Note that such queries are generally more appropriately placed directly in the URL bar of a browser. This can be seen as an indication that older people are on average less experienced with respect to web search.
It is also informative to look at examples of queries q where a demographic group d has an unusually high or low conditional click entropy H(U |q, d) [3]. A high click entropy can be due to a number of reasons. It can be that the presented web results for that query are poor and people have to try many pages, but it can also be seen an expression of high interest on a potentially multi-faceted topic. For example, the query "scrapbooking" has a general click entropy of 5.4. But for the youngest quintile, the click entropy increases to 6.8, despite the fact that smaller buckets usually lead to smaller entropies. The same phenomenon holds for the oldest quintile and the query "civil war".

Gender G H(Q|G) H(U |Q, G) H(U |G) H(Q|U, G)

Male

19.12

1.87

19.85

2.61

Female 19.04

1.83

19.75

2.77

Table 5: Basic differences in how women and men search the web. The first set of columns is computed for the (query,URL) data set and the last two columns for the (URL,query) data set. See Table 1. Both for queries and URLs the uncertainty/diversity for men is about 2.09 = 6.4% higher than for women, even though they show less variability for queries used to describe clicked URLs.

6. APPLICATIONS
The previous section already demonstrates that there is indeed a difference in search and click behavior between different demographic groups. In this section, we investigate if these differences can be exploited to improve web search (Section 6.1), automatic labeling of URLs (Section 6.2) and query suggestions based on query completions (Section 6.3).
6.1 Web Search
As we mentioned in the introduction, for the query "wagner" women predominantly click on the Wikipedia page about the composer and men on the page of a producer of spray brushes. Similarly, most people searching for "esl" click on

the homepage of "ESL Federal Credit Union", but for people in areas with many households where a non-English language is spoken at home, the preferred result is a page with background information about the "English as a Secondary Language" exam. Such examples illustrate that knowing a particular demographic feature such as gender can potentially help to improve web search results. Here we go beyond anecdotal evidence and quantify the attainable improvement gains.
Applied to all queries with sufficient support (see below), the gains in P@1 are small. This is mostly due to the fact that the baseline system, a large commercial web search engine, already has a highly tuned ranking for common queries. Therefore, we also looked at more difficult subsets where the uncertainty of the clicked URL for a given query was large, i.e. where the empirical conditional entropy H(U |Q) was above a certain threshold. For these cases, the demographic information is more useful to explain the diversity in the clicked URL.

Application

Precision @ 1 Instances Baseline Ours Gain

§6.1 Search §6.1 Search H(U |Q)  1.0 §6.1 Search H(U |Q)  2.0 §6.2 URL labeling §6.3 Query completion

207 M 123 M 61 M 246 M 459 M

.703 .713 1.4% .557 .574 3.0% .381 .408 7.1% .461 .483 4.8% .250 .276 10.4%

Table 6: Improvements in P@1 by exploiting demographic information, for the different applications described in Sections 6.1-6.3 . We selected instances (input, target) where the input has a support of at least 100 users for some demographic feature value d, as well as at least another 400 users for other values of the same feature. P@1 was computed for those instances. The baseline system ranks targets according to P (y|x). Our system ranks them by P (y|x, d). The last column shows the relative gain.
Only cases with sufficient support to reliably use demographic data were used for Table 6. In detail, we required at least 100 users for a particular demographic feature value, such as a quintile for the age, as well as at least 400 users for other values of the same demographic feature. Even though only less than half of the (query,URL) pairs in our original data set (see Table 1) satisfy this criterion, we believe that in practice this fraction can be increased by more aggressive pre-processing of query terms, e.g. (i) by removing special characters (e.g. conflating "men's health" and "mens health"), (ii) by stemming (e.g. reducing "cheap flights" and "cheap flight" to singular form) and (iii) by grouping semantically related keys (e.g. combining "ny times" and "new york times" into one query).
6.2 Automatic URL Labeling
In the previous section, we looked at (query,URL) pairs and tried to predict the URL for a given query. Here, we look at the dual setting, i.e. given a clicked URL, can we predict the query it was clicked for?
Looking at frequently used queries leading to a particular URL essentially helps to give a concise description of a web page, very similar in spirit to using tags to label a page [18].

528

These descriptions might not be the same across different demographic groups. For instance, a marijuana-related page is found by most people using the slang "weed", whereas people in the oldest quintile use queries with the term "marijuana" to locate the same page. Apart from being used as labels, the distribution of the queries gives a hint at the function or status of a particular page. E.g., a popular site such as Facebook has a very low entropy in terms of the queries used to find it. As it is almost exclusively "found" by navigational queries, it could be called a "navigational URL", using the terminology from [4]. The page http://www.braces.org is generally found by the query "braces", arguably an informational query. However, people in the highest quintile of per-capita income find it predominantly by the query "braces.org", a navigational query. These are some anecdotal examples illustrating differences in how people search for the same web page. Table 6 shows that using demographic information can lead to an improvement in automatically deriving the query for a given URLs of 4.8% in terms of the precision for the highest ranked query.
6.3 Query Completion
If a user starts typing the word "frontpage" on a search engine, what is the most likely term to follow? For most people, it is "2003", referring to a particular version of the Microsoft software. However, for young people it is "free", for African Americans it is "africa"14, and for people with a high level of education it is "magazine"15. In this set of experiments, we evaluated if demographic information could help in predicting the second query term. We used queries having at least two whitespace-separated terms, and having at least two characters in each of the first two terms.
This application shows the largest gain in Table 6. This make intuitive sense as the full query or URL already absorbs a large amount of demographic information. So the earlier the demographic information is used in the process, the larger its predictive value.
Recall that the percentiles used to discretize the demographic features, as described in Section 3.2, were computed for (query,URL) pairs for the web search setting. So the performance for the other two applications can most likely be improved further by adapting the discretization as this leads to a higher entropy H(D) and hence to a more discriminative predictor.
7. FUTURE EXTENSIONS
Our findings in Section 5 are more descriptive than explanatory. It would be interesting to investigate why, for example, different demographic groups prefer different result URLs for the very frequent query "swine flu symptoms", where people in the lowest quintile concerning a BA degree have http://www.medicinenet.com/swine_flu/article.htm as the single most clicked result as opposed to http:// www.cdc.gov/h1n1flu/qa.htm for the rest of the population, even though both pages appear to be of a similar nature. Apart from doing a questionnaire-based user study, one could look at features of the target page, such as the number of images or the average sentence length, or of the
14http://www.frontpageafrica.com/ is an Africa-centered news and entertainment site. 15http://frontpagemag.com/ is a political, conservative magazine.

result snippet, such as which words were shown. Similarly, features such as the length of a session, the dwell time on a target page or even the percentage of typographical mistakes in queries could help to improve user modeling. We hope that a better understanding of the differences in how certain demographic groups interact with the web leads to more targeted help for people in need. E.g., one could envision a different web search interface for the elderly.
As our query log data seems to be representative of the whole US population (see Table 2), it lends itself for various large-scale sociological studies. For instance, it might be possible to investigate different attitudes of different demographic groups, such as women vs. men, towards certain off-line issues, e.g. child care, or online issues such as digital privacy concerns [24]. This could be done using both the generated search volume and the type of related search queries for the issue of interest. For instance, we observed that men appear to be more worried about deleting their search history while women tend to be more worried about removing their Facebook profiles.16 Any study using query logs would be faster, cheaper and have wider coverage than traditional field studies. As a downside there is, on the other hand, more noise and effects due to spam or a small set of highly biased users have to be taken into account for such studies.
We also deem it interesting to investigate algorithms such as HITS [16] to propagate demographic labels back and forth between, say, users and queries. This way a user who often issues "educated" queries would gradually be labeled as more and more educated, and queries issued by very educated users would similarly become labeled as more educated. Of course, this approach could be applied to any of the demographic features considered.
Finally, we are looking at a possibility to share our data on a per-query basis for high-volume queries if privacy guarantees such as k-anonymity can be given [25]. Releasing query log information aggregated for demographic groups is similar in spirit to releasing census information for a particular ZIP code.
8. CONCLUSIONS
To the best of our knowledge, this is the first study that analyzes the web search behavior of different demographic groups, such as different income ranges or different ethnic groups, for millions of US web users. The simple but important observation that made this possible was the linkage of census information for ZIP codes to user profiles. For most parts, the population of search engine users appears to be a very good approximation of the US population (see Table 2), which highlights the potential of our approach for sociological studies.
It should be emphasized that we could compile lists of queries of people of Asian decent (Table 3) or point at differences in web search behavior for more educated people (Table 4) without knowing the ethnicity or the education level of a single person. We see this as a big advantage of our approach over more traditional questionnaire based field
16For the query "how to delete permanently" the mostclicked URL for women was http://www.ehow.com/how_ 2315204_delete-facebook-account-permanently.html while for men it was http://www.metacafe.com/watch/ 1267808/how_to_permanently_delete_google_search_ history/.

529

studies, where demographic profiles need to be collected and stored for individuals, raising privacy concerns.
With respect to the impact of our approach on the web search results of a major web search engine, we demonstrated that a straightforward application of the demographic information led to a 1.4% increase in P@1 averaged among all searches, and an increase of 7.1% in P@1 for the 30% of queries having the larger entropy in their click distribution. See Section 6 for details. Though this might seem small, one has to take into account that (i) the baseline system is already highly optimized and that (ii) the full query itself already "absorbs" part of the demographic information, as certain user groups are less likely to issue certain queries. Bigger gains are achievable for query completions as here the demographic information is not yet subsumed by the full query. Changes to interfaces of this kind also have the advantage that users are less likely to be confused by the fact that the ranking of results is different between, say, friends.
Although in principle our methodology is applicable to any country, our current study is limited to the US due to the availability of detailed government census information. In cases where this information is not readily available, applying machine learning techniques seems viable.
Our main purpose was to point out opportunities which arise from using demographic information and we believe that we have barely scratched the surface. We hope to do but also to see more work on this topic in the future.
Key references: [27, 19]
9. REFERENCES
[1] S. Argamon, M. Koppel, and G. Avneri. Routing documents according to style. In First International Workshop on Innovative Information Systems, 1998.
[2] S. Argamon, M. Koppel, J. Fine, and A. R. Shimoni. Gender, genre, and writing style in formal written texts. Text, 23, 2003.
[3] C. Arndt. Information Measures: Information and its description in Science and Engineering. Springer, 2001.
[4] A. Broder. A taxonomy of web search. SIGIR Forum, 36(2):3­10, 2002.
[5] T. Coenen. Pink truth. http://www.pinktruth.com. [6] M. Eirinaki and M. Vazirgiannis. Web mining for web
personalization. ACM TOIT, 3(1):1­27, 2003. [7] N. Ford, D. Miller, and N. Moss. The role of
individual differences in Internet searching: An empirical study. JASIST, 52(12):1049­1066, 2001. [8] N. Ford, D. Miller, and N. Moss. Web search strategies and human individual differences: A combined analysis. JASIST, 56(7):757­764, 2005. [9] N. Ford, D. Miller, and N. Moss. Web search strategies and human individual differences: Cognitive and demographic factors, Internet attitudes, and approaches. JASIST, 56(7):741­756, 2005. [10] D. L. Hoffman and T. P. Novak. Bridging the racial divide on the internet. Science, 280:390­391, 1998. [11] I. Hsieh-Yee. Research on Web search behavior. Library and Information Science Research, 23(2):167­185, 2001. [12] J. Hu, H. J. Zeng, H. Li, C. Niu, and Z. Chen.

Demographic prediction based on user's browsing behavior. In WWW, pages 151­160, 2007.
[13] L. A. Jackson, K. S. Ervin, P. D. Gardner, and N. Schmitt. Gender and the internet: Women communicating and men searching. Sex Roles, 44(5):363­379, 2001.
[14] R. Joiner, J. Gavin, J. Duffield, M. Brosnan, C. Crook, A. Durndell, P. Maras, J. Miller, A. J. Scott, and P. Lovatt. Gender, internet identification, and internet anxiety: correlates of internet use. Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society, 8(4):371­378, 2005.
[15] R. Jones, R. Kumar, B. Pang, and A. Tomkins. "I know what you did last summer": query logs and user privacy. In CIKM, pages 909­914, 2007.
[16] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. JACM, 46(5):604­632, 1999.
[17] L. Lorigo, B. Pan, H. Hembrooke, T. Joachims, L. Granka, and G. Gay. The influence of task and gender on search and evaluation behavior using google. Information Processing & Management, 42(4):1123­1131, 2006.
[18] C. Marlow, M. Naaman, D. Boyd, and M. Davis. Position Paper, Tagging, Taxonomy, Flickr, Article, ToRead. In Collaborative Web Tagging Workshop, Edinburgh, Scotland, 2006.
[19] Q. Mei and K. Church. Entropy of search logs: how hard is search? with personalization? with backoff? In WSDM, pages 45­54, 2008.
[20] A. J. Morgan and E. M. Trauth. Impact of Individual Differences on Web Searching Performance: Issues for Design and the Digital Divide, chapter ITB12097, pages 261­282. Idea Group Publishing, 2006.
[21] M. R. Morris, J. Teevan, and S. Bush. Enhancing collaborative web search with personalization: groupization, smart splitting, and group hit-highlighting. In CSCW, pages 481­484, 2008.
[22] P. Norris. Digital Divide: Civic Engagement, Information Poverty, and the Internet Worldwide. Cambridge University Press, 2001.
[23] J. Schler, M. Koppel, S. Argamon, and J. Pennebaker. Effects of age and gender on blogging. In AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs, 2006.
[24] K. Sheehan. An investigation of gender differences in on-line privacy concerns and resultant behaviors. Journal of Direct Marketing, 13(4):24­38, 2000.
[25] L. Sweeney. k-anonymity: a model for protecting privacy. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):557­570, 2002.
[26] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing search via automated analysis of interests and activities. In SIGIR, pages 449­456, 2005.
[27] J. Teevan, M. R. Morris, and S. Bush. Discovering and using groups to improve personalized search. In WSDM, pages 15­24, 2009.

530

A User Behavior Model for Average Precision and its Generalization to Graded Judgments

Georges Dupret
Yahoo! labs
gdupret@yahoo-inc.com

Benjamin Piwowarski
University of Glasgow
benjamin@bpiwowar.net

ABSTRACT
We explore a set of hypothesis on user behavior that are potentially at the origin of the (Mean) Average Precision (AP) metric. This allows us to propose a more realistic version of AP where users click non-deterministically on relevant documents and where the number of relevant documents in the collection needs not be known in advance. We then depart from the assumption that a document is either relevant or irrelevant and we use instead relevance judgment similar to editorial labels used for Discounted Cumulated Gain (DCG). We assume that clicked documents provide users with a certain level of "utility" and that a user ends a search when she gathered enough utility. Based on the query logs of a commercial search engine we show how to evaluate the utility associated with a label from the record of past user interactions with the search engine and we show how the two different user models can be evaluated based on their ability to predict accurately future clicks. Finally, based on these user models, we propose a measure that captures the relative quality of two rankings.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval
General Terms
Theory, Experimentation
Keywords
Click-through Data, User Behavior, Search Engines, Statistical Model, Metrics
Introduction
An accurate method to quantify the quality of a document ranking is a fundamental requisite in the design of search engines. Ranking metrics intervene at different development
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

stages: A prognostic metric is used to train a ranking function and to select the best one among a set of candidates. Once the function has been submitted to users, diagnostic metrics evaluate how users react to the changes brought by the new function.
Problem Description. Suppose reliable editors examined
a set of documents returned in answer to a query and provided us, for each of them, with a label that describes its relevance on a five grades scale (in decreasing order of relevance): "PERFECT", "EXCELLENT", "GOOD", "FAIR" and "BAD" or P, E, G, F and B for short. As long as the user scans the ranking sequentially, i.e. from the top of the list to the bottom, one document at a time, it is clear that the best ranking is obtained by ordering the documents in decreasing order of their labels.
It is nevertheless not enough to know the ideal ranking. In a typical scenario, a new ranking function is designed to operate on a set of documents and query features. To compare this new function to a previous one or to evaluate it with respect to the optimal ranking, a random set of queries is chosen and the documents appearing in the rankings of both functions are manually labeled by editors. This gives rise to two sequences of labels for each query in the evaluation set. To compare these sequences, we have to work at two levels: Individual Query Level: Given the two sequences of ordered labels produced by two search engines in answer to a given query, which is more likely to satisfy user needs? Ranking Function Level: Supposing we know how to compare two rankings for a given query, how do we extend the results on individual queries to a set of queries? The second problem arises because when averaging over the results of several queries, it is not enough to know whether a query ranking is better than the other, it is also necessary to know by how much.
Contribution. We agree with Robertson [5] that "If we can
interpret a measure (. . . ) in terms of an explicit user model (. . . ), this can only improve our understanding of what exactly the measure is measuring". To illustrate the necessity of a user model, consider the case where a first ranking function produces the sequence BBP BB, while another function produces F F F BB. Provided users scan the list sequentially, if users stop their search after the second position in the ranking, then the second ranking is clearly better. This is not obviously true, and may even be false, if most of them scan at least three positions.
Resorting to user modeling also helps us break out the

531

"chicken and egg" problem [1] we face when comparing two different metrics: Deciding which metric is best calls for a third "meta" metric. Because various "meta" metrics are likely to co-exist, a meta metric for the meta metrics is necessary, etc. User models on the other hand can be compared based on their predictive ability. If one model predicts more accurately future user interactions with a search engine, then the metric derived from the first user model is arguably more reliable. This doesn't completely solve the problem though, as different metrics can be derived from a same user model.
This work first concentrates on describing a possible user model for an important and widely used metrics: Average Precision (AP, Section 1). This exercise will help us identify the implicit assumptions behind this metric and relate it quite naturally to other metrics found in the Literature. We will design the user model to be a fully generative statistical model based on explicit assumptions. This way it is possible to evaluate the model parameters based on past data (Section 1.5) and to evaluate the accuracy of the model on a test set. In Section 2, we propose an extension of AP to multi-graded relevance judgments and derive a new metric that compare two rankings based on the proportion of users that are better off with one ranking than with the other.
1. PROBABILISTIC AVERAGE PRECISION
The AP [6] metric can be associated to a particular set of hypothesis on the user behavior. This in turn defines a user model the parameters of which can be estimated from data. We will see that other metrics like pSkip [7], the Average Search Length and the Reciprocal Rank all share the same AP user model.
We first introduce some notations. Because we suppose that all documents are judged, we can understand a ranking as a sequence of labels r, r = 1, . . . , R where r indexes the position in the ranking. In the case of AP documents are either relevant (denoted by +), or not relevant (-).
We often use the notation 1:R to represent the whole ranking up to position R. We also introduce here the binary variable Er, called the examination, that indicates whether a particular rank r is examined by the user. The subscript r is dropped when there is no ambiguity. By examining a position, we mean evaluating the snippet in order to decide whether the corresponding document is promising or not. Finally, the binary variable Cr indicates whether a document was clicked or not. We suppose that if a document is clicked, then its position has been previously examined (There are no "accidental" clicks). On the other hand, if it is not clicked, we ignore if it was is examined or not.
In both our user models, we assume that a user is browsing sequentially a list, and that the user stops when satisfied. We define three different sets of boolean variables: Sr is true when the user is satisfied exactly at rank r, Cr is true when the user clicked the result at rank r, and Er is true when the user examined the document at rank r. Note that the user is satisfied at one rank only, and hence boolean variable Sr can only be true for one rank r, if any: If Sr is true, then Sr is false for any rank r different from r.
In order to keep notations compact, we use the following shorthands. First, for any random variable X, x+ and x- are equivalent to "X is true" and "X is false", respectively. We use lowercase x as a short-hand for X = x, and denote X=x the indicator value which is 1 when the event X = x is true, and 0 otherwise.

Another handy shorthand is used when we deal with a series of variables: A set of variables Xr for r between 1 and R is represented as X1:R. Similarly Xa:b is the set Xa, Xa+1, . . . , Xb. This can be combined with the previous notations: x+a:b is a short-hand for x+a , x+a+1, . . . , x+b .

1.1 Average Precision
The AP metric is defined as the average of the precisions computed at the relevant document positions:

AP

=

1 T


X

precision at r ×

relevance at r

(1)

r=1

where T is the number of documents relevant to the query at hand and "relevance at r" is 1 if the document is relevant and 0 otherwise. In practice, the sum is often truncated to a small number of terms.

1.2 User Model
To relate this measure to a user model, we first observe that the "precision at r" in Eq. 1 can be interpreted as a measure of how "easy" or "fast" r relevant documents are found by a user browsing the result list sequentially for exactly r relevant documents. If we further assume that 1/T users need exactly r relevant documents and that a user always clicks upon examining a relevant document, the expected precision coincide with AP, as discussed by Robertson [5]. In order to formalize these intuitions, we define the following user model:

User Model 1 (Probabilistic AP). 1

1. The user decides the number n of relevant documents she needs to meet her information need.

2. She browses the result list sequentially.

3. She clicks on a document she examines with a probability that depends on the relevance of the document.

4. She ends her search as soon as she clicked on n relevant documents.
Because different users need a different number of documents, n corresponds to discrete random variable that we denote by N . We see that this model assumes that a user ends her search only if she is satisfied and that a search must end on a relevant document.
Several user models can be at the origin of the AP. For example, Moffat & Zobel [4] propose the next interpretation: "Every time a relevant document is encountered, the user pauses, asks "Over the documents I have seen so far, on average how satisfied am I" and writes a number on a piece of paper. Finally, when the user has examined every document in the collection ­because this is the only way to be sure that all of the relevant ones have been seen­ the user computes the average of the values they have written." This scenario stresses how unrealistic is the direct use the total number T of relevant documents as a component of an evaluation measure.
1pAP for short.

532

1.3 Prognostic Metric
Central to the metric is the evaluation of the rank at where the information need is met, which is described by Pr(s+r ).
According to the model assumption, the user browses the
result list sequentially and abandons her search as soon as she meets her information need, so s+r also implies that all documents are examined up to rank r and none is examined after rank r: e+1:r and e-r+1:R. It also implies that the document at rank r was clicked, i.e. c+r .
A prognostic metric attempts to evaluate the quality of a
ranking before it is presented to users. As a consequence, we need to evaluate Pr(s+r ) by marginalizing over all possible user interactions for all values of N (Pr(s+r |n) = 0. If r = - then Pr(s+r |n) = 0). Otherwise:

Pr(s+r |n; +) =1 X X Pr(s+r , e1:R, c1:R|n; 1:R)

| e1:R c1:R

{z
(a)

}

=2 X Pr(s+r , e+1:r, e-r+1:R, c1:r-1, c+r , c-r+1:R|n; 1:R)
c1:r-1

=3 X Pr(e+1:r, e-r+1:R|s+r ) Pr(c+r |e+r ; r) Pr(c-r+1:R|e-r+1:R)

| c1:r-1

{z
=1

} | {z } | {z }

(b)

=1

Pr(c1:r-1|e+1:r-1; 1:r-1) Pr(s+r |c1:r-1, c+r , n; 1:r-1) (2)

|

{z

}|

{z

}

(c)

(d)

The first equality holds by simple marginalization of the joint distribution over all the variables but Sr. Equality 2 holds because (a) is zero unless

1. e+1:r and e-r+1:R because s+r entails that the user examined all positions up to r before ending the search
at r, 2. c+r because a user ends a search only if she clicks on a
relevant document and 3. c-r+1:R because there are no clicks on documents not
examined.

Factor (d) is deterministic in Equality 3: It is zero unless

1. the document at position r is relevant 2. the user clicked at position r, an event that occurs with
probability (b) if the document is relevant. 3. the user clicked on exactly n - 1 relevant documents
prior to position r, a condition that is realized with a probability that obeys to (c)

The model states that the probability to click does not

depend on the rank, provided we know whether the user

examined the position, and the label of the document at

this rank. Hence, Pr(c+r |e+r ; r) is a constant independent of the rank; We denote it µ+. We observe that if there are

tr-1 relevant documents among the first r-1 positions, there

are

"n-1 "
tr-1

possible

configurations2,

each

of

them

having

a

probability

µn+-1(1 - µ+)tr-1-n+1

2if n - 1 > tr-1 then there is no possible configuration, i.e.

the

value

"n-1 "
tr-1

is

defined

as

zero.

We have3:

Pr(s+r |n; )

=

+ r

µ+

"n-1 "
tr-1

µn+-1 (1

-

µ+

)tr-1 -n+1

=

+ r

"n-1 "
tr-1

µn+(1

-

µ+ )tr-1 -n+1

(3)

where the indicator + r is 1 if r = + and 0 otherwise. The first prognostic measure we define is the probabilistic
interpretation of AP, that is the expected precision at the rank where the search is abandoned. In terms of our user model, this is:

pAPpro = E(precision)

=

X

Pr(n)

R
X

n r

Pr(s+r |n)

n

r=1

Using Eq. 3, this can be expressed as:

pAPpro

=

X

Pr(n)

R
X

+ r

n r

"n-1 "
tr-1

µn+(1

-

µ+ )tr-1 -n+1

n

r=1

Finally, if we set µ+ = 1, Pr(n) = T -1 for n = 1, . . . , T and R =  we have:

pAPpro|µ+=1,Pr(n)=T -1

=

X

1 T


X

+ r

tr

=n

n r

n

r=1

This is the original AP as claimed above.
Various other prognostic metrics can be easily constructed based on the pAP user model once Pr(s+r |N = n) defined in Eq. 3 is known. Maybe the most obvious are the Expected
Search Length defined as

ESLpro = X rPr(s+r ) = X Pr(n) X rPr(s+r |n) (4)

r

n

r

or the Expected Reciprocal Rank :

ERRpro

=

X

1 r

Pr(s+r

)

=

X

Pr(n)

X

1 r

Pr(s+r |n)

(5)

r

n

r

A definition closer to the original Expected Search Length from Cooper would consider the expected number of irrelevant documents before retrieving n relevant documents:

X

Pr(n)

X

r

- r

n

Pr(s+r |n)

(6)

n

r

All these metrics are based on the knowledge of one central quantity, namely the probability Pr(s+r |n) of the user being satisfied knowing she was looking for n relevant documents.
This can be understood as different ways of weighting it for
the rank. Unfortunately, although they are correlated these
metrics do not necessarily lead to the same conclusion when
used to compare two ranking functions.

1.4 Diagnostic Metric
Diagnostic metrics are meant to evaluate a ranking after it was presented to users and interactions have been collected. In the case of our user model, they are based on updating the probability Pr(sr; 1:R) with the click information, i.e. to estimate Pr(sr|c1:R; 1:R).
Suppose we observe a sequence of clicks and skips c1:R on a ranking defined by 1:R. Given c1:R, we know the number nb of relevant documents clicked and the position b of
3This is the negative binomial distribution provided document at r is relevant.

533

the last click4. According to the user model, either the user Hwneaeesndcseias,tiPtsofir(eesds-1t:iRamt|ca1rt:aeRn)Pkrisb(ss+b(ism|+bc1p):lRyo)r.1nI-fottPhsrea(tsci+blsific|cke1de:dRa)dtoaacnludlm(aseln-1l:tRwi)es. not relevant, this probability is 0. Otherwise, we know that
if the user was satisfied at rank b, then she was looking for
nb relevant documents, and we have:

Pr(s+b |c1:R) = Pr(N = nb|c1:R)

=

Pr(c1:R|nb)Pr(nb) Pr(c1:R|nb)Pr(nb) + Pr(N > nb)Pr(c1:R|N > nb)

=

Pr(nb)

Pr(nb) +

Pr(N

>

nb

)

QR
r=b+1

Pr(c-r

|e+r

;

r

)

Having defined the probabilities Pr(s+r |c1:R, 1:R), we can compute the diagnostic counterparts of Eqs. 4, 5 and 6 or any

other suitable metric of interest. In particular, if we adopt

the convention that the precision is null if the information need is not met (i.e. s-1:R), we can revise Eq. 4 and compute the expectation of precision knowing the user clicks c1:R:

pAPdia

=

E(precision|c1:R)

=

+ b

Pr(s+b

|c1:R

)

nb b

Moreover, if we suppose that all clicked documents are relevant and disregard the case where the user doesn't meet her information need, the "diagnostic" version of pAP coincide with pSkip [7] model with the pSkip metric being the empirical estimate of µ+. Given the close relation of pSkip with the diagnostic versions of Average Search Length (ASL) and Reciprocal Rank (see [7]), we deduce that the pAP user model also generalizes the underlying user model of those metrics.

1.5 Parameters Estimation
In order to estimate the model parameters, we want to maximize the likelihood of the data. To define the latter, it is necessary to compute the likelihood of a session which is defined by the clicks c1:R, i.e. to compute L = Pr(c1:R; 1:R).
Suppose that the last click of a session is at position b and that the user clicked on nb relevant documents. In that case, we know that the user was either satisfied at rank b or continued his search beyond rank R. The likelihood of the first case is

Pr(s+b , c1:R; 1:R)
b
= Pr(s+b , c1:b; 1:b) = Pr(nb) Y Pr(cr|e+r ; r)
r=1

while in the second case, the user is not satisfied by nb relevant documents and:

R
Pr(s-b , c1:R; 1:R) = Pr(N > nb) Y Pr(cr|e+r ; r)
r=1

Because we don't observe Sb, we marginalize it to obtain

b

R

L = Pr(nb) Y Pr(cr|e+r ; r) + Pr(N > nb) Y Pr(cr|e+r ; r)

r=1

r=1

A session without clicks is never satisfying for the user and its likelihood is obtained from the previous Equation by observing that Pr(N = 0) = 0 and Pr(N > 0) = 1.

4In the case there was no click, the model implies s-1:R.

To evaluate the probabilities Pr(N = n), µ+ = Pr(c+|e+; +) and µ- = Pr(c+|e+; -) we multiply the likelihood of a set of the observed sessions and maximize the resulting product using standard techniques.

1.6 Numerical Experiments
We collected from the logs of the Yahoo! search engine a set of approximately 33,000 sessions with at least one click for which we have a PEGFB editorial judgment for each of the top 10 urls, together with a record of which urls have been clicked. Each record in our data set has the following form: A sequence of 10 labels 1:10 followed by a sequence of 10 True or False tokens that indicates the states of C1:10.
We divided the data in 10 random subsets and used each of these subsets (i.e. 10% of the original set) as the data we maximize the likelihood on. The data is labelled on the PEGFB scale and we need to decide how to adapt these 5 levels to the 2 levels ­relevant and irrelevant­ suitable for pAP. We explore successively all the possible mapping by considering first that only PERFECT documents are relevant (P row in Table 1), then that EXCELLENT and PERFECT documents are relevant (E case in Table 1), etc.
We observe that the estimates based on 10% of the data are fairly stable. If we consider all the documents with label "GOOD" or above as relevant, the probability of a click on a relevant document is 39% as opposed to only 19% on a irrelevant document. We also observe that 83% of users require 1 relevant document to satisfy their information need, while 12% need two and only 5% need more.
We would like to know which of the mappings from PEGFB labels to relevant or irrelevant is best aligned with the actual user behavior. The pAP model is a generative model and can be used to predict user behavior, i.e. which documents are clicked given a specific ranking; We can therefore compare the accuracy of these predictions on the test sets. We use the perplexity ­a common measure of the "surprise" of a model when presented with a new observation. Given a proposed probability model q of the true distribution p, one may evaluate q by asking how well it predicts a separate test sample of size D drawn from p. The perplexity of the model q is defined as

2-

PD i=1

1 D

log2

q(xi )

(7)

Better approximations q of the unknown distribution p will tend to assign higher probabilities to the events observed in the test set. Thus, they have lower perplexity, i.e. they are "less surprised" by the test sample.
In the context of user behaviors, the perplexity is a monotonically increasing function of the joint probability of the sessions in the test set. Analytically, this probability is identical to the likelihood of the test set, but instead of maximizing it with respect to the parameters, the latter are held fixed at the values that maximize the likelihood on the training set.
All sessions in both the training and test sets contains R = 10 results so that by setting D to 10 times the number of sessions in Eq. 7, the perplexity is loosely5 interpretable as the number of trials per correct prediction of a binary event: The click or skip of a document. The lower the perplexity, the better the model: A perplexity of 1 corresponds
5This interpretation is not strictly correct because the clicks and skips in a session are not independent. The evaluation itself continues however to be valid.

534

Table 1: Median Click Probabilities and Required Number of Documents Distribution. The proportion of users requiring more than 4 documents is not significantly larger than zero.

Pr(c+|s+; )

Pr(N = n)

µ- µ+ n = 1 n = 2 n = 3 n = 4

B

- 0.42 0.89 0.11 0.00 0.00

F 0.17 0.44 0.88 0.12 0.00 0.00

G 0.19 0.39 0.83 0.12 0.03 0.02

E 0.14 0.33 0.90 0.07 0.02 0.01

P 0.12 0.68 0.99 0.01 0.00 0.00

to perfect predictions, while a perplexity of 2 corresponds to randomly predicting a click or a skip.
We have plotted the perplexity resulting from the 10 data splits for the 5 possible mappings in Figure 3. Experiments show that considering as relevant the document GOOD or better lead to the best model. To fix ideas, we also plotted the perplexity of a simple CTR (Click-Through Rate) model that predicts a click according to the CTR of the document label. For example, if 100 BAD labels appear in 50 sessions and are clicked 20 times, then the probability of a click on a BAD is estimated as 20/100 = 20%. This model doesn't take into account the document position in the ranking.
2. MULTI-GRADED MAP
The pAP model states that if a user needs n relevant documents, she will stop her search when she finds her nth document and the documents beyond in the ranking have no importance to her. Although the assumption that a user stops her search as soon as her information need is met seems adequate, it is harder to believe that a pre-defined number of relevant documents will satisfy this need. It is also hard to believe that she actually knows this number. In the remaining of this work we propose a model where a certain amount of utility is associated to clicked documents, and a user stops her search when she gathered enough utility to meet her information need6. We relax the assumption that a document can either be relevant or not (  {+, -}) and allow multi-grade labels as for example DCG does. The user model is specified as follows:
User Model 2 (Satisfying Information Need). 7
1. The user examines the page results sequentially,
2. She clicks on a document she examines with a probability Pr(c+|e+; ) that depends on the document label .
3. If she clicks on a document with label , she acquires the quantity U() of utility.
4. When she has gathered enough utility to satisfy her information need, she ends the search.
We assume that utilities are additive: Each clicked document with label  contributes an amount U() of utility to be added to the total utility the user already gathered. This is not completely realistic: If two documents provide the same content, the utility of consulting both should be the same
6This model is reminiscent of [2]. 7SIN for short.

as the utility of consulting one. We ignore this limitation in

this work. As long as document relevances are judged inde-

pendently from one another by editorial judges, there is no

solution to this problem. Note that AP and DCG also suffers

from this shortcoming.

We adopt the same notations as defined for the pAP user

model (Section 1). The total utility associated with a set of

clicks

c1:r

can

be

written

Pr
1

Ur

cr

where

cr

is

1

if

the

user

has clicked at rank r and 0 otherwise.

The larger the total utility the user acquires, the higher

the probability that her information need is met. We capture

the probabilistic relation between the total amount of utility

­ a continuous, positive variable ­ and the binary variable

that states whether the user information need is met using the sigmoid function (u) = (1 + exp(-u0 - u))-1 where

u0 is a suitable intercept. The effect of this function is to

squash any value on the real axis to the interval ]0, 1[ suitable

for probabilities. With these assumptions, we are able to

establish the relation between utility and information need:

r

Pr(s+r |s-1:r-1, c1:r; 1:r) = cr × (X Uscs)

(8)

s=1

where, as in Section 1, Sr is true if the user was satisfied at rank r. As the user clicks on more documents, the total utility increases, increasing the probability that the information need is met. Other parameterization are possible, but the logistic function presents some clear advantages: It is simple and it is monotonically increasing with its argument, the total amount of utility.

2.1 SIN Probabilities of Satisfaction

Prognostic Satisfaction. As for AP, we are interested in
the rank where the user is satisfied Pr(s1:R; 1:R), which can be estimated by marginalizing the joint distribution of the model given by:
Pr(s1:R, e1:R, c1:R; 1:R) =
R
Y Pr(cr|er; r)Pr(er|s1:r-1)Pr(sr|c1:r , s1:r-1; 1:r) (9)
r=1
where the first component is estimated from the training data, the second is deterministic and the third is given by Eq. 8. This marginalization does not present any particular analytical difficulty, but we cannot expect the same kind of simplification as for pAP.
This process is illustrated in Figure 1 for the two first ranks: Assuming that the user always examines rank 1, she either clicks on the first document and follows the left branch ­an event that happens with probability Pr(c+1 |e+1 ; 1)­ or she skips it and follows the right path. If she chooses the first solution, she decides with probability (U1) that the document at position 1 is sufficient to fulfill her information need and she ends her search. Otherwise she continues, an event that happens with probability 1 - (U1). Right before she reaches rank 2 she is in one of three states:
· She clicked on the document 1 and decided that her information need is satisfied. The search ends. (node 5 in Figure 1),
· She clicked on the document but decided her information need was not met (left most branch, node 3),

535

rank 1

1 Click? starting utility: 0

yes: Pr(C1+|E1+)

no: Pr(C1-|E1+)

2 Stop? Utility: U1

no:

1 - (P1t=0 Utct) yes:

(P1t=0 Utct)

rank 2

3 Click?

yes: Pr(C2+|E2+)

no: Pr(C2-|E2+)

5 END

4 Stop? Utility: U1 + U2

6 Stop? Utility: U1

7 Stop? Utility: 0 no: 1
8 Click? yes: Pr(C2+|E2+) 9 Stop? Utility: U2

no: Pr(C2-|E2+) 10 Stop? Utility: 0

Figure 1: SIN decision process.

· She didn't click on the document and her information need is not met (right branch, node 8).
Each end node of rank 1 (nodes 3, 5 and 8) is reached with a probability equal to the product of the probabilities on the path from node 1 (reported in Figure 1 together with the yes / no decisions that determine the path). If the user didn't end at node 5, the process is repeated at node 3 and 88.

Diagnostic Satisfaction. In order to evaluate the diagnos-
tic counterpart of the metrics, we need to estimate the prob-
abilities of satisfaction after the user interactions have been
observed. The development is similar to that of Section 1.5,
where we distinguish two cases (the user was satisfied or not at the rank b of the last click). In the first case (s+b ), we have

Pr(s+b , c1:R; 1:R) = Pr(s+b , s-1:b-1, e+1:b, e-b+1:R, c1:R; 1:R)

b
= Y Pr(cr|e+r ; r)Pr(sr|c1:b, s-1:r-1; 1:r)
r=1

(10)

where we used Eq. 9 and the fact that (a) a user always

examine the rank if she has not been satisfied before, i.e.

Pr(e+r |s-1:r-1) = 1 for any rank less or equal than b, (b) a

user does not examine any rank after being satisfied, i.e.

Pr(e-r |s+b ) = 1 for on non examined

any rank after b and (c) ranks, i.e. Pr(c-r |e-r ) =

she never clicks 1 for any rank

after b. Similarly, if the user is not satisfied, we have:

Pr(s-1:R, c1:R; 1:R) = Pr(s-1:R, e+1:R, c1:R; 1:R)
R
= Y Pr(cr|e+r ; r)Pr(s-r |c1:r , s-1:r-1; 1:r)
r=1

(11)

and finally, combining Eqs. 10 and 11 :

Pr(s+b |c1:R; 1:R)

=

Pr(s+b , c1:R; 1:R) Pr(s-b , c1:R; 1:R) + Pr(s+b , c1:R; 1:R)

=

cb × (P1:R Urcr)



(P1:R

Ur

cr

)

+

(1

-

(P1:R

Ur

cr

))

QR
r=b+1

Pr(c-r |e+r

;

r)

where we used Eq. 8.

8 A python script to compute p(s+r ; 1:r) is publicly available at: http://sinmetric.sourceforge.net/

Pr(c+|e+; ) 1.0 0.8 0.6 0.4 0.2 0.0
BFGEP

U() 6 4 2 0 -2
u0 B F G E P

Figure 2: Left boxplot: Probability of click given the document label. Right boxplot: Utility of a document with the given label. The value of the intercept u0 is also reported.

2.2 Model Estimation and Evaluation
The likelihood L of a session correspond to the probability Pr(c1:R; 1:R) of a sequence of clicks, which can be obtained by adding Eqs. 10 and 11:
b
Pr(c1:R; 1:R) = Y Pr(cr|e+r ; r)Pr(sr|c1:r , s-1:r-1; 1:r)
r=1
R
+ Y Pr(cr|e+r ; r)Pr(s-r |c1:r, s-1:r-1; 1:r) (12)
r=1
As before, we maximize the likelihood of 10 subsets of our dataset. The results are reported in Figure 2 and Table 2. In our opinion these are very interesting results. First, click probabilities ­with the exception of the label BAD­ and utilities increase according to the label ordering, which corresponds to intuition but is not enforced by the model. The BAD documents have, when compared to FAIR documents, a higher probability of click but a lower utility. This is in agreement with what we expect of spam documents. Perfect documents ­identified as the target of a navigational query­ have a particularly high probability of being clicked and, if they are the first click, the user is predicted to stop with a probability (U(PERFECT)) = 95%.
We computed the perplexity of the SIN model to compare

536

1.25 1.30 1.35 1.40
nDCG
0.6 0.7 0.8 0.9 1.0

Table 2: Probability of click and utility according to the document label. The third column reports the probability of ending the search after clicking on one document with the corresponding label. The median of the intercept u0 is -2.71.

Label 

Pr(c+|e+; ) U() (U ())

BAD

0.36 2.32

0.40

FAIR

0.30 2.81

0.52

GOOD

0.38 3.54

0.70

EXCELLENT

0.42 3.66

0.72

PERFECT

0.76 5.68

0.95

CTR

car rentals

pAP

pAP

pAP

pAP

pAP

SIN

B

F

G

E

P

Figure 3: Perplexity of the different user models: Boxplots of 10 × cross-validation.

it with the performance of the pAP user model. The results are shown in Figure 3 where we see that the SIN significantly outperforms pAP.

2.3 Loss & Benefit Metric
We can define the same set of prognostic and diagnostic metrics for the SIN as for pAP (Eqs. 4, 5 and 6), but this time the probability Pr(s+r ) is estimated based on the SIN user model.
Which of these metrics best reflects user satisfaction is not
clear and might even be system dependent. These metrics can be understood as different ways of averaging the values of Pr(s+r ) over the positions r and are out of necessity to a certain extend arbitrary. The new approach we propose here doesn't avoid completely this caveat, but it attempts to remain as neutral as possible by avoiding the averaging over positions. As a basis for comparison between two rankings A and B, we propose to estimate the expected number of users who meet their information need earlier in one ranking than in the other. We denote SA (resp. SB) the set of satisfaction variables for ranking A (resp. B). The fact that a user meets her information need sooner in ranking A than in ranking B is denoted A  B and happens with probability:

R

R

Pr(A  B) = X Pr(s+A;r, s-B;1:r) = X Pr(s+A;r)Pr(s-B;1:r)

r=1

r=1

where we supposed that the user behavior on the two rank-
ings were independent. The benefit B of ranking A ­or the
loss if negative­ is defined as the proportion of users that are better off with ranking A than ranking B9:

B(A, B) = Pr(A  B) - Pr(B  A)
9The same script of footnote 8 also computes the prognostic and the diagnostic benefit.

-0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0
loss
Figure 4: The loss vs. the nDCG for the ranking in the entire dataset. The histograms reflect the loss and nDCG values distribution (Top and Right, respectively).
To compare two ranking functions, the benefit is averaged over queries.
We argue that this new metric is more "neutral" because no weighted average of Pr(s+r ) is computed for A or B. Instead the distributions of SA and SB are compared pointwise. The benefit also has the advantage of being straightforward to interpret.
2.4 Numerical Experiments
When comparing more than two systems, it is convenient to have an absolute value characterizing each of them separately. This is easily achieved by computing the benefit with respect to the ideal ranking obtained by ordering the documents according to their utilities. In Figure 4 we report the benefit with respect to the ideal ranking (actually a loss) as a function of the normalized DCG (nDCG) with logarithmic discounting factors. As expected, the larger the nDCG, the lower the loss. Although correlated, these two values are sufficiently different to lead to a different choice of ranking function.
The same Figure 4 distinguishes a set of ranking on the left for which the loss and nDCG lead to opposite conclusions: These rankings are very far from ideal in terms of the loss (values range from -.3 to -.5), while the nDCG is moderately below average with values in the .7 to .8 range. We isolate the ranking identified by a filled circle in Figure 4, extreme left to get more insight. It corresponds to the query "car rentals" with ranking GGEGGGPEGP. We report the different statistics associated with this ranking in Table 3. We first observe that the loss is stable from rank 2 because the vast majority of users (72.3 + 20.2 = 92.5% of users) presented with the ideal ranking meet their information need before

537

Table 3: Absolute gain for the query "car rentals". DCG discounts are logarithmic and scores are 10, 5, 3, 0.5, 0. The second column reports the document labels of the actual A and the ideal B rankings. The next two columns report the proportion of users meeting their information need at the different ranks for of rankings A and B, respectively.

rank
1 2 3 4 5 6 7 8 9 10

label
G/P G/P E/E G/E G/G G/G P/G E/G G/G P/G

Pr(SA = r)
0.265 0.207 0.176 0.107 0.076 0.054 0.085 0.011 0.006 0.009

Pr(SB = r)
0.723 0.202 0.025 0.017 0.010 0.007 0.005 0.003 0.002 0.002

benefit
-0.458 -0.549 -0.549 -0.550 -0.550 -0.550 -0.549 -0.549 -0.549 -0.549

DCG
3.000 4.893 7.393 8.685 9.845 10.914 14.247 15.825 16.728 19.618

nDCG
0.300 0.300 0.393 0.414 0.445 0.471 0.589 0.630 0.642 0.729

rank 3 according to the SIN model. The loss is then essentially determined by the proportion of users who meet their information need on the actual ranking on the first two positions. In our data collection, a PERFECT document is the target page of a navigational query. The SIN model is consistent with this definition: It predicts with a high probability that the user will stop her search after seeing the target document. The nDCG on the other hand keeps increasing steadily up to rank 10 because the contribution of a given position to the final DCG value is independent of the documents presented at other positions.
Discussion
We have shown that a reasonable reconstruction of the user decision process can be deduced from the definition of AP. This is important because this help us question the implicit hypothesis behind this metric and propose the improvements at the origin of the pAP user model: We supposed first that users click on a document with a probability that depends on whether it is relevant or not, as opposed to AP where, at least implicitly, users always click on relevant documents. Like [4], we also reject the idea that the total number of relevant documents in the collection needs to be known to evaluate the system. Instead we suppose that the number of documents required by the user follows a distribution that can be estimated from past user interactions.
In the SIN model we further question the pAP hypothesis: Rather than supposing that users need a pre-defined number of relevant documents, we argued that they search as long as their information need is not satisfied. Making the assumption that documents with a higher level of relevance provide more "utility" to the user and contribute more to her satisfaction, we designed the SIN user model to predict user stops based on the total amount of utility she gathered. This hypothesis is more appealing intuitively and is able to handle naturally multi-graded relevance levels.
Unlike metrics, user models can be compared quantitatively because they have the ability to predict user interactions, i.e. which documents a user will click or skip when presented with a new ranking. By evaluating the prediction accuracy, we can determine which model is more adapted, i.e. represents better the user behavior, to a given search engine, a given market or a given set of users. In particular,

we have shown that for our dataset the pAP model based on considering GOOD and better documents as relevant leads to the best prediction accuracy. We also showed that the SIN model outperforms the pAP model. This matched intuition because it is able to handle multi-graded levels of relevance.
Most metrics are based on the knowledge of a probability distribution on the rank at which the user meet her information need. This distribution can be evaluated prior to exposing the ranking to users by marginalizing over all the possible interactions with the result list. Metrics based on this prior distribution are qualified as prognostic metrics and can be used to train a ranking function or to chose among different candidates. The same probability distribution can be estimated after the new ranking function has been exposed to users and enough interactions have been recorded, giving rise the diagnostic counterpart of the metrics.
We have seen that a same user model can give rise to different metrics. Choosing one in particular is to a certain extent arbitrary and in this context it is important to make the weaker assumptions possible. This led us to propose that out of two rankings, the best is the one that leads the user to fulfill her information need at an earlier rank. Based on this definition, it is possible to estimate the benefit of a new ranking as the number of users it will favor.
The models we have proposed are still rather crude and many important aspects have been ignored. Both the pAP and the SIN models make the assumption that the user examines the ranking until she meets her information need. This is clearly unrealistic: Users do abandon search out of despair, reformulate their query, etc. The correction of this assumption is the topic of future work. Other aspects like document diversity, user diversity, query classes have also been ignored, etc. In this respect, the field of Interactive Information Retrieval [3] is certainly an important source of inspiration.
3. REFERENCES
[1] G. Dupret. User models to compare and evaluate web IR metrics. In Proceedings of SIGIR 2009 Workshop on The Future of IR Evaluation, 2009.
[2] G. Dupret and C. Liao. Estimating intrinsic document relevance from clicks. In Proceedings of the 3rd WSDM conference, 2010.
[3] D. Kelly. Methods for Evaluating Interactive Information Retrieval Systems with Users, volume 3 of Foundations and Trends in Information Retrieval. 2009.
[4] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):1­27, 2008.
[5] S. Robertson. A new interpretation of average precision. In Proceedings of SIGIR'08, pages 689­690, New York, NY, USA, 2008. ACM.
[6] E. M. Voorhees and D. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT press, 2005.
[7] K. Wang, T. Walker, and Z. Zheng. Pskip: estimating relevance ranking quality from web search clickthrough data. In Proceedings of the 15th ACM SIGKDD, pages 1355­1364, New York, NY, USA, 2009. ACM.
More complete references for this work can be found in [1].

538

Estimating Advertisability of Tail Queries for Sponsored Search

Sandeep Pandey Kunal Punera Marcus Fontoura Vanja Josifovski
Yahoo! Research 701 First Ave.
Sunnyvale, CA 94089
{spandey, kpunera, marcusf, vanjaj}@yahoo-inc.com

ABSTRACT
Sponsored search is one of the major sources of revenue for search engines on the World Wide Web. It has been observed that while showing ads for every query maximizes shortterm revenue, irrelevant ads lead to poor user experience and less revenue in the long-term. Hence, it is in search engines' interest to place ads only for queries that are likely to attract ad-clicks. Many algorithms for estimating query advertisability exist in literature, but most of these methods have been proposed for and tested on the frequent or "head" queries. Since query frequencies on search engine are known to be distributed as a power-law, this leaves a huge fraction of the queries uncovered.
In this paper we focus on the more challenging problem of estimating query advertisability for infrequent or "tail" queries. These require fundamentally different methods than head queries: for e.g., tail queries are almost all unique and require the estimation method to be online and inexpensive. We show that previously proposed methods do not apply to tail queries, and when modified for our scenario they do not work well. Further, we give a simple, yet effective, approach, which estimates query advertisability using only the words present in the queries. We evaluate our approach on a realworld dataset consisting of search engine queries and user clicks. Our results show that our simple approach outperforms a more complex one based on regularized regression.
Categories and Subject Descriptors
H.3.m [Information Storage and Retrieval]: Miscellaneous
General Terms
Algorithms, Design, Experimentation
Keywords
sponsored search, click estimation, tail queries
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Sponsored search is the dominant form of textual advertising on the Web in terms of revenue. It involves displaying advertisements (ads) alongside the results returned by search engines. Under the pay-per-click mechanism, search engines get paid every time a user clicks on a displayed ad. Clearly, sponsored search is useful for search engines since it is a source of revenue for them. Moreover, it is beneficial for users as well since it helps them in finding relevant products/services, especially for queries with commercial intent. It also aids advertisers in reaching the right set of users.
The success of sponsored search heavily relies on displaying relevant ads for appropriate queries. Previous studies [6], have shown that irrelevant or unwanted ads are useless to search engines since they do not attract clicks. They may even be harmful since they degrade the quality of search experience driving users away. Even users who continue using the search engine despite seeing irrelevant ads might get "trained" to ignore the sponsored sections of the search result page, impacting the long term revenue of the search engine. Hence, estimating the following two properties are of key importance for an advertising engine:
· Query advertisability: Certain queries are more suitable for advertising than others. For instance, queries such as "digital camera" and "car insurance" are more likely to be satisfied by sponsored search results than queries like "hotmail". Gauging the query advertisability correctly helps the advertising engine decide whether to show ads or not (or how many ads to display), and thus only showing ads to which the user will react. Moreover, this reduces the computation cost of ad selection for queries that should not display ads.
· Ad relevance and clickability : Once the advertising engine has determined that the query is advertisable, it attempts to retrieve the ads which are most likely to satisfy the user's information need. This typically involves ranking the ads in the system based on various factors like their syntactic match with the query, their estimated CTRs (click-through rates) from historical data, a user's past behavior etc. Several methods have been proposed for doing this ad selection and CTR estimation [4, 9, 14, 15, 18, 23, 24].
Estimating Advertisability for Tail Queries. In this paper we focus on the first task above, that of es-
timating Query Advertisability. There is past work on identifying whether user queries have an underlying commercial

563

intent, the intention to purchase a product or a service [2, 6, 10]. However, in addition to the underlying intent, advertisability should also capture other factors that influence the likelihood of engaging the user; the suitability of the current ad supply, the ability of the ad selection algorithms to select good ads, and even business rules. Therefore, we consider the ad clicks obtained in response to a query as a proxy for its advertisability. Specifically, in this paper we define advertisability of a query as the probability of seeing a click on any sponsored search ads displayed on the result page of the query.
Past work on modeling advertisability of queries have focused on using features derived from a plethora of information about them; the set of all retrieved ads in [6] and the set of all retrieved search results in [10]. Moreover, these approaches determine advertisability from an offline analysis of the historical click data. These methods have only been tested and shown to work well on frequently occurring queries. However, in this paper we focus on the more challenging problem of estimating advertisability of infrequent or tail queries, so called because they form the "heavy tail" of the power-law distribution of query frequencies on a search engine. The above mentioned approaches are not applicable to these tail queries as they are too rare to have significant historical data. Furthermore, tail queries are almost always unique, thus requiring an online estimation procedure (i.e., estimation is performed when users issue the queries). Since search users are very sensitive to any latency in the results presentation, under any reasonable system infrastructure the online procedure must be inexpensive and cannot employ complex query expansion methods. Hence we study the problem of estimating query advertisability using the query keywords only, similar to [2, 22].
Technical Challenges and Solutions. Most of the queries in the datasets used in this study con-
sist of 3-4 words and have occurred 1-2 times. This results in the two principal challenges of the problem: noisy ground truth due to the rarity of the queries, and sparseness of features due to the short query lengths.
The noise in the ground truth, i.e. the estimates of query advertisability, results from the low number of impressions for each query. This makes not only the learning difficult, but also affects the testing methodology. For instance, an oracle is also unlikely to match the advertisability estimates of individual queries derived using our data. One of our key insights is that though the advertisability of each individual query is noisy, when many queries are put together they provide a fairly stable advertisability estimate. Hence, given an estimation policy we evaluate it by looking at the aggregate advertisability of the top-ranked queries (instead of their individual advertisability).
In order to learn in the presence of noisy ground truth, we propose a word-based advertisability model that employs the above "grouping" insight (in Section 2). We estimate the parameters of this model via a maximum likelihood based method. As a competitive baseline we also present a regression based methodology that combines state-of-art elements from machine learning literature (in Section 3). Finally, orthogonal to these two methodologies we study an additional way of dealing with noise: learning from the head queries (which have reliable advertisability estimates) and then applying the model on the tail queries (see Section 2.2.1). From our experiments we found that this does not work well, since

tail queries exhibit fairly different vocabulary and characteristics than the head queries.
We handle the sparsity of features in different ways for the two learning methodologies. For the maximum likelihood estimation method, we show that simplifying assumptions to remove interactions among the features result in improved accuracy. In the regression methodology, we handle sparsity using two methods. One way is to perform regression under a L1-regularization (also known as Lasso). A second way is to make the features denser by clustering them [8, 22, 27]. In Section 3.3, we give an LDA-based method of clustering tail queries and a learning method that maps queries into latent clusters and learns a model using these cluster-based features.
Contributions. We make the following contributions in the paper:
1) We investigate and formalize the problem of estimating advertisability of tail queries using its keywords only. The problem is challenging due to the inherent noise and sparsity present in the data.
2) We propose a simple, yet effective, word-based model to estimate query advertisability. Our estimation method is robust to noisy ground truth as well as sparse features.
3) We put together a competitive baseline regression-based approach that deals with sparsity by: (a) incorporating regularization in the model and (b) using LDA-derived latent topics.
4) We give an evaluation framework for the problem using a large scale dataset from a real-world search engine. Our extensive empirical results show that our word-based model outperforms the more expensive and complex regressionbased approach.
2. WORD-BASED QUERY ADVERTISABILITY MODEL
Our goal in this work is to make predictions for Query Advertisability, which is the probability of the event when one or more ads displayed for a query are clicked. In this section, we give a model to accomplish this.
2.1 Model Formulation
In Section 1 we discussed the challenges of learning and evaluation in the presence of noisy ground truth and sparse features. Further, we used these to motivate our use of a word-based model. In this section, we start with a discussion of the additional properties that we want our word-based query advertisability model to have.
A basic desired property would determine the influence each word exerts on the advertisability of the query it is part of. Some words indicate that the user is looking for a certain product, e.g., "download", "buy" and "compare", while other words like "insurance", "flight" and "hotel", are associated with products/services that are known to be amenable to advertising. When we look at the queries that contain these words and their corresponding ad-clicks, we observe that these words have heavy positive influence on the query advertisability. Similar observations have been made in [2]. Hence, a useful basic property to have is, P1: a single "advertisable word" should be capable of ensuring high advertisability for a query containing it. On the flip side, consider

564

some words from [2] that lead to low ad-clicks on queries, such as "weather", "free", "university" etc. From our data, we find that queries containing these words could potentially still be highly advertisable; for example, "weather in fiji", "free download", and "university admissions". Hence, a useful second property to have is, P2: while some words do not contribute to a query's advertisability, no one word's presence should reduce the advertisability. Finally, the effect of a word on a query advertisability might depend on other words present, like effect of "music" in the queries "music ringtones" and "music lyrics". However, because the word occurrences in tail queries are extremely sparse we do not incorporate such dependencies into the model.
Conforming to these desirable characteristics we give the following query advertisability model. In the model we say that each word in a query has a certain propensity of attracting a click on an ad, say c(w). Let us denote the advertisability of query q by c(q). Say, the query q consists of the words w1, w2 . . . wn. Thus, under the independence assumption, each word in the query independently attracts an ad-click for the query (with probability c(w)). Hence, the advertisability (i.e., the probability of the ads displayed for query q to be clicked) can be written as:

n

c(q) = 1 - Y `1 - c(wi)´

(1)

i=1

A key property of this formulation is that, all things being equal, it favors longer queries (e.g., if all c(wi)'s were the same, c(q) gets larger as n gets larger). While this makes sense in most cases, it can score longer queries containing words with mediocre click propensities higher than shorter ones with few good terms. To avoid this shortcoming, we introduce parameter k where k denotes the maximum number of words from the query that can take part in the clicking process. Under this constraint:

!

Y

c(q) = max 1 - (1 - c(w))

(2)

S

wS

where S  q and |S|  k.

2.2 Parameter Estimation
In Equation 1 we presented the model to combine each query word's contribution to the query advertisability.1 We can estimate the parameters of this model by computing the maximum likelihood estimate of the training data. The training data consists of queries and their associated click or impression events. Say, s(q) denotes the number of instances when query q received an ad-click, while n(q) denotes the number of instances when it did not. Given a dataset of queries Q and click events, its likelihood can be written as:

Y" Y

"s(q)

L(s(q), n(q); c(w)) =

1 - (1 - c(w))

qQ

wq

"Y

"n(q)

×

(1 - c(w))

wq

1In Equation 2 we use the k "best" terms to compute query advertisability, but we will use Equation 1 to estimate the model parameters.

On taking the logarithm of the both sides:

logL(s(q), n(q); c(w)) = X s(q) · log`1 - Y (1 - c(w))´

qQ

wq

+ X n(q) · log` Y (1 - c(w))´

qQ

wq

Taking derivatives with respect to c(w) results in:

P
q

w n(q)

(1 - c(w))

=

Q w q (1 - c(w ))

X" s(q)
qw

·

1

w !=w

-

Q
w

q (1

-

c(w

" ))

Here q w is the set of queries that contain the keyword w. Solving this complex set of equations is difficult, especially since the feature combinations used in queries are sparse and the ground truth is unreliable. Hence, we approximate this solution by assuming that each instance of a click or not click is a referendum on the advertisability of each keyword in the query independently. This has the same effect as replicating each query once for each term contained in it, with each such replication having just one of the keywords. Under this assumption we obtain:

c(w)

=

P
q

P
q

w s(q)

w(s(q) + n(q))

(3)

In other words, the contribution of a word to advertisability is the fraction of times it is present in a query instance which attracted an ad-click.

2.2.1 Choice of Training Set
As mentioned earlier, tail queries have very few impressions, the words combinations are extremely sparse, and thus their advertisability estimates tend to be very noisy. Clearly, learning from such a dataset is difficult and may lead to a poor model estimation.
An alternative approach is to learn the model from the head queries, which tend to have significant number of occurrences in the historical data. For these queries we can compute advertisability estimate with confidence. The disadvantage of learning from such a dataset is that it is very different from the test set of tail queries in terms of vocabulary, word combinations, and maybe even word advertisability scores. This difference could counteract the advantages gained from reliable ground truth when learning on the head queries.
Each training set has its own advantages and disadvantages. While the training set of tail queries is noisy, the training set of head queries may exhibit different behavior and not generalize well on the test set. In Section 4.5 we evaluate our model while training on both the head and tail set of queries.

3. REGRESSION-BASED QUERY ADVERTISABILITY MODEL
In Section 2, we proposed a simple word-based model for predicting query advertisability. In this section, we present alternative approach that combines state of the art elements from machine learning literature. First, we will formulate the task of predicting query advertisability as a regression problem. Then we will present a few ideas to help the regression model handle sparsity: regularization and clustering.

565

3.1 Linear Regression Model

Just as in the word-based model in Section 2, we say that

each word in a query has a certain propensity of attracting

a click on an ad; we denote this with c(w). The advertis-

ability of query q, denoted by c(q), can then be computed

quite naturally by summing of the individual word adverti-

sability values [19, 20]. Under this model we consider words

in queries as binary features and the weight of each word is

given by its advertisability. Say, the query q consists of the

words

w1,

w2

. . . wn,

then

c(q)



P
wq

c(w).

Hence, we can write the following set of linear equations:

X q, c(w) = c(q) + q
wq

where q's are the error terms. Under the squared loss function this problem can be for-
mulated as:

X"

X "2

arg min

c(q) - cw

c(w)

qQ

wq

This set of linear equations can be solved using existing methods. However, we face another challenge here which is that due to the sparsity in word occurrences. A tail query consists of 3-4 words on average, which is a very little amount of text. Moreover, tail queries significantly differ from each other, thus resulting in a large vocabulary of words. For instance, in our experiments we noted that the number of unique words is more than half of the number of unique queries. In other words, if we represent a query in this feature space, it will consist of a couple of non-zero entries for the words present in the query, while the rest of the thousands of dimensions will be all zero. This is likely to make this set of equations under-determined. Next we consider a couple of ways to handle this issue of sparsity.

3.2 Regularized Regression
When the number of parameters is large, the estimates of linear regression exhibit high variance which is undesirable. One way of controlling this by incorporating regularization while training. Under L1 regularization (also known as Lasso [12, 26]) this can be written as:

X"

X "2

arg min

c(q) - cw

c(w)

qQ

wq

subject to:

X c(w)  t
w
where  is a given constant. This is also written as:

X"

X "2 X

arg min

c(q) - cw +  c(w)

c(w)

qQ

wq

w

where  is the shrinkage parameter. When  is close to 0, this behaves like regular linear regression, while as  goes to infinity it forces many c(w)'s to be zero. Hence, this performs feature selection for us, though unlike traditional feature selection methods this is not limited to completely picking or dropping a feature.

3.3 Inferring Topics from Queries
An alternative method of dealing with sparsity is by mapping the sparse high-dimensional feature space to a dense

low-dimensional space. Principal component analysis is often used in doing so while maximizing the variance of the data captured in the low-dimensional space [17]. Latent semantic analysis is also used for dimension reduction [11, 16]. It transforms the sparse word-document matrix to a more dense topic-document matrix, where each topic is a latent concept that is derived using the co-occurrence information.
In this paper, we use Latent Dirichlet Allocation to obtain topics from queries. This is a generative model for the documents where the topic distribution is assumed to have a Dirichlet prior [5]. We describe it in more details next.

3.3.1 Latent Dirichlet Allocation
In this model a document is assumed to be generated from a mixture of topics, where each topic has its own word distribution. In particular, we can write the probability of the ith word in the document as (given in [13]):

T
X P (wi) = P (wi|zi = j)P (zi = j)
j=1

where T is the number of topics. Variable zi denotes the

latent topic from which word wi is generated. It is equal to

topic j with prior probability P (zi = j), in which case the

word has P (wi|zi = j) of being generated given the word distribution of the jth topic.

Let T denote the number of topics and D denote the num-

ber of documents. In the LDA model P (w|z) is modeled

using a set of T multinomial distributions  over the vocab-

ulary W (one multinomial  per topic). The  distribution

of a topic gives the distribution of words under the topic.

P (z) is modeled using a set of D multinomial distributions, denoted by d, over T topics (one multinomial  per document). The multinomial distribution d gives the mixture

of T topics present in the document d, i.e., j(d) = P (z = j). Both  and  distributions have Dirichlet priors [13]. In

brief, the model can be written as:

wi|zi, (zi)  Discrete((zi))



 Dirichlet()

zi | (di )

 Discrete((di))



 Dirichlet()

where  and  are the hyperparameters.

3.3.2 Deriving LDA Topic-based Features
We use LDA to find a low dimensional representation of a query. In particular, we run LDA over a training set of queries for T number of topics. After the end of run, we have a topic distribution of each query q, P (z|q). Each topic makes a latent concept and the topic distribution of query P (z|q) gives a representation of the query in this lowdimensional concept space. For each query, these posterior topic-membership values can be used in two ways: they can be added to the query words as additional binary features, or used as a sole representation of a query. In this paper we experiment with both methods. We can use these query representations in our model from Section 3.1. In particular, we find the advertisability of each topic using regularized regression and use them to compute the advertisability of a query.
The advantage of this method is that it is able to relate query keywords with each other in an intelligent manner. For example, we found that in our experiments a topic consisted of words like "ipod", "iphone", "samsung" etc. So,

566

using just the co-occurrence information of data, LDA was able to connect these words together which are clearly very related. This helps significantly since new queries that fall into this cluster will be able to use the advertisability information estimated for all queries that fall into the cluster, hopefully leading to robust results. In Section 4.3 we empirically evaluate this approach.
4. EXPERIMENTS
In this section we evaluate the word-based model we proposed for query advertisability with the state-of-art regression based methodology.
4.1 Empirical Setup
Dataset. We train and evaluate our approach on a real-life search
engine data. We collected a sample of queries issued to a major search engine over a period of 7 days. The dataset consists of more than 5 million query impressions. We also recorded the ad clicks for these queries during this period. Of these queries we put those queries into the tail set which have less than 2 impressions per day on average. The goal of this study is to predict the advertisability on these tail queries. The aforementioned tail set consists of more than 2 million unique queries. For our experiments we placed half of the queries in the training set and the rest in the test set.
The average query length is 3.3 which shows the amount of sparsity in the data. The average number of impressions per query is 1.55. As mentioned earlier, given such a few impressions for a query it is difficult to estimate its advertisability with any certainty. This presents challenges while learning as well as evaluation. In view of this, we propose our evaluation metric next.
Evaluation Metric. A conventional way of evaluating our advertisability esti-
mation methods would be to take the L1-error or L2-error of the predicted (c^(q)) and "true" advertisability of queries (c(q)). However, since our focus is on tail queries which have very few impressions (as shown above), it is not possible to estimate their true advertisability. In other words, given the noise in "true" advertisability estimated from our data an oracle is also unlikely to match them.
To deal with this problem, we opt to evaluate the prediction of a method with respect to a group of queries instead of the individual queries. In particular, given method  we rank the queries in order of their decreasing predicted advertisability values c^(q)'s. Starting from the top, let S(r, )) and F (r, ) be the cumulative total of number of ad-clicks and the total number of all impressions till rank r. Due to aggregation over a group of queries, S(r, ) and F (r, ) are fairly stable for large values of r and amenable for conducting analysis. Clearly, the best method is the one which maximizes S(r, ) for all values of F (r, ). In case there is no clear best method, we can plot S(r, ) on the y-axis and F (r, ) on the x-axis and compute area under the curve (AUC) to succinctly summarize the performance of a method. The best policy for a given impression threshold, say  , is the one which maximizes S(r, ) for F (r, ) =  .
Another reason this evaluation method is suitable is that launch criteria in real-world systems are likely to be framed w.r.t. to the relationship between S(r, ) and F (r, ). In order to preserve the user's search experience, an advertising

fraction of clicks

0.7

word-based model (k=1)

word-based model (k=2) 0.6 word-based model (k=3)

word-based model (k=100)

0.5

random

0.4

0.3

0.2

0.1

0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 fraction of impressions

Figure 1: Performance of the word-based advertisability model for different values of k.

system controller will likely limit the number of instances of queries in which ads are shown. The controller would then be interested in determining the algorithm that yields the most clicks in the fixed number of impressions. On the flip side, the number of clicks might be fixed in order to make revenue numbers.
4.2 Evaluation of Word-based Advertisability Model
In this experiment we evaluate the performance of our word-based advertisability model presented in Section 2. We tokenize each query into words by treating whitespace for word boundaries. Since many of our queries are URLs, we tokenize these queries at punctuation characters. We remove the stop-words and stem the remaining words.
For this experiment we learn word advertisability scores (c(w)'s) from the training set using the approximate method (Equation 3) of Section 2.2. Then we evaluate the method on the test set. Figure 1 shows the performance of our method for different values of k. Recall that k is a parameter in our method which limits the number of words from a query that can contribute towards its advertisability. The x-axis in the figure is the cumulative fraction number of impressions till a given rank (F (r, )), while the y-axis is the cumulative fraction of clicks (S(r, )).
Note that all of our model variants are significantly above the straight line which denotes the random method (i.e., predict the advertisability at random). This is encouraging since it shows that the advertisability can be estimated quite well using the query keywords only. As expected the curves in the figure are convex. This happens because when the number of impressions is small (x-axis), the clicked impressions (y-axis) are aggregated over queries present on the top of the ranked list. These queries are predicted to have high advertisability, thus resulting in a high average click per impression (i.e., the slope of the curve). However, as the number of impressions increases, queries with low advertisability start getting accounted for and hence, they bring down the average click per impression.
From the figure it is clear that k = 2 performs the best. Intuitively, this makes sense because when k is too small, the method does not give due credit to queries with more advertisable words. On the other hand, when k is too large,

567

long queries get an unfair advantage. Still, the model looks fairly stable when k is in a reasonable range (i.e., 1 to 3).
4.3 Evaluation of Regression-based Approach
In Section 3 we gave a regression-based approach to predict query advertisability. Due to sparsity the simple linear regression is unlikely to work, hence we experiment with the L1-regularized regression. We perform L1-regularization using the SMIDAS software [25] where SMIDAS stands for "Stochastic Mirror Descent made Sparse".
Furthermore, as discussed in Section 3.3.2, we use LDA to derive dense topic-based features. In particular, we represent each query as a mixture of latent topics derived using LDA and the query words. We then learn the model over this hybrid low-dimensional query representations using the SMIDAS software as above. In Table 2 we show some topics and the top words in them as derived using LDA. Clearly, the topics are fairly coherent and meaningful. We experimented with constructing a 100 and a 1000 topics; the results were very similar and we present the ones with 100 topics. We also experimented with using just the low-dimensional representations of a query in the regression formulation, but the results were much worse; we do not present those results here.
In Figure 2 we plot the performance of the regressionbased approach with and without the topic-based features. As we can see adding the topic-based features has so significant effect on the accuracy of the prediction task. This is in contrast to the results obtained in [22], where keyword cluster based features were shown to have significant impact on accuracy. We posit that this is because our focus is on tail queries; while the topics constructed by LDA are themselves meaningful, the uniqueness of tail queries means that inferring the topics for each tail query is extremely difficult. Hence, the knowledge contained in sparse word-based features subsumes the contribution of dense topic-based features. We leave further investigation of this phenomena for future work.
4.4 Comparison of Word-based and Regression-based Models
Above we evaluated the two different methods for estimating query advertisability. In Table 1 we show the popular words with top advertisability under the two methods. Note that both methods are doing a reasonably good job of finding highly advertisable words such as rental, vacation, travel etc. Even though the top features from the two estimation methods look fairly similar, the weights of many other features show differences.
In particular, we plot the performance of the two approaches, word-based approach and L1-regularized regression, in Figure 3. It is clear that the word-based method performs better in comparison to its state-of-art counterpart. This shows that careful modeling of the properties of the problem can result in a simple, yet effective, approach that can outperform a method based on much more complex machine learning primitives.
4.5 Choice of Training Set
So far we have been learning the model from the training set of tail queries. Another data set that can be employed for training is the head set which consists of all the head queries. The advantage of this set is that it consists of fre-

0.6

0.5

without clustering with clustering

0.4

fraction of clicks

0.3

0.2

0.1

0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 fraction of impressions

Figure 2: Performance of the regression-based approach with and without LDA-derived topic features.

fraction of clicks

0.7

0.6

word-based model

L1-regularized regression

0.5

0.4

0.3

0.2

0.1

0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 fraction of impressions

Figure 3: Performance comparison of the wordbased model and regression-based approach.

quent queries and has relatively stable query advertisability values (c(q)). The disadvantage is that it is not similar to the test set which consists of tail queries only.
We perform learning from these training sets using modelbased method for k = 2. Figure 4 shows the performance for different training sets. Note that the training set of head queries performs worse. This shows that the naive approach of learning from head queries and applying the learned model on tail queries does not work well since they exhibit different characteristics than the head queries. This does not imply that head queries are useless for our task; instead, it means that head queries are not sufficient by themselves and must be used in conjunction with tail queries to aid the learning.
5. RELATED WORK
Sponsored search is an active area of research. Several studies have been published recently that focus on the sponsored search advertising [1, 6, 7, 21, 22, 24]. We classify the related work along the following aspects and distinguish our work from them.
Modeling Ad-specific CTR. Most prior art [7, 21, 22, 24] in sponsored search deals
with estimating the CTR of a given query-ad pair; this es-

568

fraction of clicks

0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

tail set head-set
0.05 0.1 0.15 0.2 0.25 0.3 0.35 fraction of impressions

Figure 4: Performance of word-based model under different training sets.

Method Word-based Model
L1regularized Regression

Top Words
cheap, boot, ticket, discount, bag, bed, laptop, wholesale, chevrolet, airline, rent, flight, coupon, shower, rental, nike, vacation, loan, hotel, furniture, diet, cruise, outlet, dress, job, phone, printer, hp, truck, car, price, dvd chair, costume, store, hotel, wholesale, diet, dress, rental, boot, camera, part, sale, cheap, price, inn, bed, ticket, travel, rent, shower, cruise, bathroom, batteries, ring, vegas, furniture, car, free, shoes, curtain, loan, discount

Table 1: Words with high advertisability estimates under the two prediction methods.

Topic 1 2 3 4 5 6 7 8 9 10 11 12
13
14 15 16
17
18
19
20

Words
free, game, online, download, video honda, toyota, bmw, yamaha, tire ipod, iphone, samsung, sim, touch truck, chevy, chevrolet, engine, parts diet, product, pill, lose, fat, weight coffee, table, top, bed, chair, antique lyrics, love, song, victoria, secret coupon, pizza, code, family, restaurant county, court, sheriff, public, office, clerk job, office, apply, description, salary phone, number, service, verizon, sprint loan, money, student, mortgage, finance, grant, aid california, sacramento, pittsburgh, riverside, buffalo army, base, navy, military, force, nation digital, camera, review, batteries, samsung lose, weight, blood, pressure, sugar, diabetes, skin, pill sony, camera, digital, photo, memory, screen, film, batteries hawaii, beach, resort, ski, spa, car, rental, hotel, vacation software, printer, dell, monitor, screen, driver, laptop, computer phone, service, motorola, verizon, sprint, cell, mobile, wireless, call

Table 2: Some LDA-derived topics and their top words.

timate is often used to display the ads with the highest predicted CTR for a given query. In this work, we are inter-

ested in predicting the advertisability of queries, which we define as the probability of seeing an event in which one of the ads displayed for the query gets clicked. Gauging the query advertisability correctly helps the advertising engine decide whether to show ads or not (or how many ads to display), and thus only showing ads to which the user will react. Moreover, this reduces the computation cost of ad selection for queries that should not display ads.
Modeling Commercial Intent of User Queries. Another line of related work has focused on identifying
queries with commercial intent. An approach for detecting the commercial intent is proposed in [10]. They define the term OCI (Online Commercial Intention) and present a framework of building machine learning models to learn OCI based on the Web page content. They use that framework to detect the commercial intent of queries, which is related to the problem we solve in this paper. In [3] the authors analyze the click-through behavior of ads to characterize and predict query intent. An analysis of the contributions of the different query terms and their corresponding click rates on commercial intent queries is presented in [2]. In a follow-up work [1] the authors examine detecting commercial intent by building a classifier based on editorial judgments of the commercial intent of the queries. They show that those queries that are characterized as commercial have higher CTR than the others.
Our work differs form this set of works in a few ways. First, we focus on modeling query advertisability, which in addition to the underlying intent, also captures all the factors that influence the likelihood of engaging the user; the suitability of the current ad supply, the ability of the ad selection algorithms to select good ads, and even business rules. Because of this we consider the ad clicks obtained in response to a query as a proxy for its advertisability. Hence, unlike past work that has relied on using human judgments for learning, in our approach we use the click data directly, without using a human understandable definition of the property of interest. Thus we identify directly the queries for which the users would be inclined to click on ads. Last, we focus specifically on the more challenging problem of modeling for tail queries. These queries, due to their rarity and the sparseness of their term combinations present very different problems than considered in past work. In this paper we give some solutions to these problems.
Modeling using Rich Query Features. In most past work, modeling of commercial intent behind
queries has focused on using features derived from a plethora of information about them; the set of all retrieved ads in [6] and the set of all retrieved search results in [10]. Moreover, these approaches determine advertisability from an offline analysis of the historical click data. These methods have only been tested and shown to work well on frequently occurring queries. However, in this paper we focus on the more challenging problem of estimating advertisability of tail queries. The above mentioned approaches are not applicable to these tail queries since they are too rare to have significant historical data. Furthermore, tail queries are almost always unique, thus requiring an online estimation procedure (i.e., perform the estimation when users issue the queries). Since search users are very sensitive to any latency in the results presentation, under a reasonable system infrastructure the online procedure must be inexpensive and cannot employ complex query expansion methods. Hence we study the

569

problem of estimating query advertisability using the query keywords only.
Regression with Cluster-based Features. Our work is most close to the work in [22], where the au-
thors propose to the clustering of the bid phrases of ads in estimating CTRs. Both top-down and bottom up hierarchical clustering are applied. The CTR of a bid phrase is then calculated as a linear combination of the predicted CTR and the CTR of its cluster. The results show that the smoothing helps the estimates for rare bid phrases and it slightly decreases the precision for common bid phrases. In this paper we update this approach by performing a more sophisticated topic modeling using LDA, and feeding the results of it into a state of the art learner based on regularized regression. We show that our our simple, yet effective, word-based model outperforms this regression/clustering based approach via empirical results in Section 4.
6. SUMMARY
In this paper we focused on the problem of estimating the advertisability of tail queries. Furthermore, due to some exogenous practical constraints, we performed the estimation using the query keywords only. We discussed how noisy ground truth and sparsity in the data make this problem difficult and techniques from past work do not apply well to our scenario. We showed how to deal with problems associated with tail queries by proposing a words-based query advertisability model. We gave a maximum likelihood method of learning the model. We also gave a competitive baseline methodology based on a regression formulation of the problem that used state-of-art machine learning approaches to deal with sparsity of data: (a) incorporating L1-regularization in the model training and (b) finding latent topics using LDA. We conducted extensive experiments on real data to evaluate our model. Our results are encouraging and show that the advertisability of queries can be estimated pretty accurately using their keywords only. We also compared different model estimation methods and study the effect of regularization, clustering, and training set selection.
7. REFERENCES
[1] A. Ashkan and C. Clarke. Characterizing commercial intent. In CIKM, 2009.
[2] A. Ashkan and C. Clarke. Term-based commercial intent analysis. In SIGIR, 2009.
[3] A. Ashkan, C. Clarke, E. Agichtein, and Q. Guo. Characterizing query intent from sponsored search clickthrough data. In SIGIR Workshop, 2008.
[4] A. Ashkan, C. Clarke, E. Agichtein, and Q. Guo. Estimating ad clickthrough rate through query intent analysis. Web Intelligence and Intelligent Agent Technology, IEEE/WIC/ACM International Conference on Web Intelligence, 1:222­229, 2009.
[5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993­1022, 2003.
[6] A. Broder, M. Ciaramita, M. Fontoura, E. Gabrilovich, V. Josifovski, D. Metzler, V. Murdock, and V. Plachouras. To swing or not to swing: Learning when (not) to advertise. In CIKM, 2008.
[7] A. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski,

D. Metzler, L. Riedel, and J. Yuan. Online expansion of rare queries for sponsored search. In WWW, 2009. [8] J. Carrasco, D. Fain, K. Lang, and L. Zhukov. Clustering of bipartite advertiser-keyword graph. In ICDM, 2003. [9] D. Chakrabarti, D. Agarwal, and V. Josifovski. Contextual advertising by combining relevance with click feedback. In WWW, 2008. [10] H. Dai, L. Zhao, Z. Nie, J.-R. Wen, L. Wang, and Y. Li. Detecting online commercial intention (OCI). In WWW, 2006. [11] S. Deerwester. Improving information retrieval with latent semantic indexing. In Proceedings of the 51st Annual Meeting of the American Society for Information Science, pages 36­40, 1988. [12] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32:407­499, 1996. [13] T. Griffiths and M. Steyvers. Finding scientific topics. PNAS, 101, 2004. [14] W. Guo and G. Li. Predicting click rates by consistent bipartite spectral graph model. In Proceedings of the 5th International Conference on Advanced Data Mining and Applications, 2009. [15] M. Gupta. Predicting click through rate for job listings. In WWW, 2009. [16] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR, 1999. [17] I. Jolliffe. Principal Component Analysis. Springer-Verlag New York Inc, 1986. [18] A. Lacerda, M. Cristo, M. Goncalves, W. Fan, N. Ziviani, and B. Ribeiro-Neto. Learning to advertise. In SIGIR, 2006. [19] W. Mendenhall and T. Sincich. A Second Course in Statistics: Regression Analysis. Pearson Education, 2003. [20] D. Montgomery, E. Peck, and G. Vining. Introduction to Linear Regression Analysis. New York: Wiley, 2001. [21] F. Radlinski, A. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski, and L. Riedel. Optimizing relevance and revenue in ad search: A query substitution approach. In SIGIR'08, 2008. [22] M. Regelson and D. Fain. Predicting click-through rate using keyword clusters. In Proceedings of the Second Workshop on Sponsored Search Auctions, 2006. [23] B. Ribeiro-Neto, M. Cristo, P. Golgher, and E. de Moura. Impedance coupling in content-targeted advertising. In SIGIR, 2005. [24] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: Estimating the click-through rate for new ads. In WWW, 2007. [25] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1 regularized loss minimization. In ICML, 2009. [26] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267­288, 1996. [27] J. Yi and F. Maghoul. Query clustering using click-through graph. In WWW, 2009.

570

Exploring Reductions for Long Web Queries

Niranjan Balasubramanian
University of Massachusetts Amherst 140 Governors Drive, Amherst, MA 01003
niranjan@cs.umass.edu

Giridhar Kumaran and Vitor R. Carvalho
Microsoft Corporation One Microsoft Way, Redmond, WA
{giridhar,vitor}@microsoft.com

ABSTRACT
Long queries form a difficult, but increasingly important segment for web search engines. Query reduction, a technique for dropping unnecessary query terms from long queries, improves performance of ad-hoc retrieval on TREC collections. Also, it has great potential for improving long web queries (upto 25% improvement in NDCG@5). However, query reduction on the web is hampered by the lack of accurate query performance predictors and the constraints imposed by search engine architectures and ranking algorithms.
In this paper, we present query reduction techniques for long web queries that leverage effective and efficient query performance predictors. We propose three learning formulations that combine these predictors to perform automatic query reduction. These formulations enable trading off average improvements for the number of queries impacted, and enable easy integration into the search engine's architecture for rank-time query reduction. Experiments on a large collection of long queries issued to a commercial search engine show that the proposed techniques significantly outperform baselines, with more than 12% improvement in NDCG@5 in the impacted set of queries. Extension to the formulations such as result interleaving further improves results. We find that the proposed techniques deliver consistent retrieval gains where it matters most: poorly performing long web queries.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation, Theory
Keywords
Query reformulation, Learning to Rank, Combining Searches This work was done while the author was at Microsoft Corporation.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Long queries form a sizable fraction of the queries that are submitted to web search engines. Queries of length five words or more have increased at a year over year rate of 10%, while single word queries dropped 3% [1]. Unfortunately, web search engines perform poorly on longer queries when compared to shorter ones [4].
Several past works have focused on improving long query performance [3, 15, 14, 16, 13, 7] (see Section 2). They can be broadly classified into query re-weighting and query reduction approaches. In query re-weighting, the original query terms are assigned different weights before the query is submitted to the search engine. In query reduction, instead of the original query, a reduced version of it (obtained by removing one or more terms from the original query) is selected and submitted to the search engine.
Both approaches are motivated by the observation that long queries usually contain superfluous terms, which if downweighted or completely removed, result in improved performance. For example, consider the query easter egg hunts in northeast columbus parks and recreation centers, which performs moderately well on most popular web search engines. If we remove (or down-weight in some fashion) the terms and recreation centers, we can observe a perceptible improvement in the quality of results.
While query term re-weighting and reduction techniques have shown significant improvements in performance on TREC1 data [15, 13], their utility in the web environment is not well understood. Further, query reduction/re-weighting on the web poses unique challenges due to the architecture of web search engines, and the extremely low latencies tolerated. Query re-weighting requires the use of a ranking algorithm that permits the online assignment of weights to query terms -- a difficult thing to implement given that most web retrieval algorithms use a learning to rank framework that relies on boolean match as well as several additional querydependent and query-independent features. Query reduction requires analyzing a potentially exponential number of reduced versions of the original query, and relies heavily on query quality prediction measures like Clarity that are expensive to compute on the web.
In this paper, we focus on query reduction, and develop techniques that are especially suited for the web. We utilize an efficient, rank-time query quality prediction technique [2], and consider only reduced versions of the original query that are obtained by dropping a single term at a time. As we will show in Section 3.2, dropping just a single (and correct!)
1http://trec.nist.gov

571

term from the original long query can result in a 26% improvement in NDCG@5. After formally defining the query reduction problem in Section 3.1, we propose three different formulations of the problem in Section 3.3. We demonstrate their utility (Section 5) through experiments on a set of approximately 6400 long queries sampled from the query logs of a major web search engine. Additionally, we develop simple thresholding and interleaving extensions to these formulations that enable balancing the trade-off between the queries impacted and the improvements attained.
The main contributions of this paper are (1) a rank-time query reduction technique that is well suited for incorporation in a web search engine and works reliably in improving hard long queries (2) formal treatment of the query reduction technique that allows control over when and how to do query reduction, and (3) the first large scale evaluation of query reduction for web queries. Analysis of the experimental results show that query reduction achieves consistent gains whenever there is high potential for improvement (Section 6). More importantly, we find that query reduction mostly benefits poorly performing queries. In other words, it provides benefits where they are most needed.
2. RELATED WORK
Retrieval effectiveness for long queries is often lower than retrieval effectiveness for shorter keyword queries. Kumaran et al. [12] showed that shorter queries extracted from longer user generated queries are more effective for ad-hoc retrieval. Using click positions as surrogates for effectivenessm Bendersky and Croft [4] show that long query effectiveness is lower than short keyword queries for web queries. Several works have focused on improving the retrieval performance of long queries [3], [15], [14], [16],[13], and [7]. They can be broadly categorized as automatic term weighting and query reduction approaches.
Weighting - Bendersky and Croft [3] use a key concepts classifier to identify important query terms in long queries, and to assign weights to query terms resulting in improved retrieval effectiveness. Also, Bendersky et al. [5] successfully extend this term weighting approach to concepts and build a weighted dependence model that performs well for web queries. Lease et al. [15] use Regression Rank, a regression framework to directly learn weights on query terms using query dependent secondary features. An extension to this approach achieves significant improvements over Markov Random Field dependency models on TREC collections [14].
However, as mentioned earlier it is difficult to directly incorporate term weights in existing web search engine ranking algorithms. In contrast, our goal is develop an approach that can be seamlessly incorporated into existing web search engines architectures without requiring modifications to the underlying search algorithms.
Reduction - Kumaran and Carvalho [13] develop an automatic method for reducing long TREC description queries. Using content-based query quality predictors such as Clarity and Mutual Information Gain, they convert the query reduction task into a problem of ranking (reduced) queries based on their predicted effectiveness. Their results on TREC Robust 2004 show the viability of automatic query reduction. In this work, we investigate the extension of this approach to web search engines.
Lee et al. [16] use statistical and linguistic features of

query terms to greedily select query terms from the original long query to achieve the effect of query reduction. Experiments on NTCIR collections demonstrate that this approach, and its extension which considers pairs of terms [17] improves long queries' effectiveness.
Chen et al. [7] use personal query history to select short queries related to the original long query, cluster the short queries based on similarity of their contexts, and select representatives from each cluster as a substitution for the original long query. Unfortunately, this approach requires ranktime processing of texts of retrieved documents, extracting noun phrases and other features from sentences that contain the query words. These heavy rank-time computations make this approach infeasible for use in web search engines.
In summary, the term weighting, query reduction, and substitution approaches show good potential for improving long query effectiveness. However, term weighting approaches cannot be readily incorporated into web search engines, while the features used for the various query reduction approaches are not suitable for efficiency reasons. Furthermore, there has been no large scale experiments on actual long web queries that demonstrate the utility of automatic query reduction. In this paper, we investigate the utility of query reduction for long web queries, and develop formulations that are well-suited for Web search engines.
3. QUERY REDUCTION
Retrieval effectiveness is typically lower for longer queries than for shorter keyword queries, partly because users often utilize extraneous terms in long queries [12]. Query reduction - the technique of automatically identifying and removing extraneous terms from long queries - has proved to be an effective technique for improving performance on long queries [13]. In this section, we present a formal description the query reduction problem.
3.1 Formal Description
Let f : P × D  R denote a ranking function that scores documents (D) with respect to a query P , represented as a set of query terms. Also, let Tf (P ) denote a target measure of the effectiveness of the ranking produced by f for the query P .
Given an arbitrary query Q = {q1, · · · , qn}, we use PQ to denote the power set of Q, i.e., the set containing all subsets of terms from query Q (including the original query Q). Then, the query reduction problem is to find a reduced version P  that achieves the highest value for the target measure as shown in Equation 1. Note that this problem statement allows the original query Q to be selected as well.

P  = arg max Tf (P )

(1)

P PQ

Obviously, the target measures cannot be completely spec-

ified for inferences over all possible queries, and hence we

need to estimate Tf (P ). The query reduction task is then

expressed as:

P  = arg max Tcf (P )

(2)

P PQ

Query performance predictors, such as Clarity [8] or Query

Scope [9], can be used to obtain estimates of the target mea-

sure, Tcf in order to select a reduced version P  of the original

query Q.

572

(a) Average Gains

(b) Max Gains

(c) Original versus Gains

Figure 1: Distribution of Potential Gains in NDCG@5.

3.2 Approximation

niques that can reliably improve the performance of hard

Efficiency is a key challenge for query reduction. Because the number of possible reduced queries in PQ is exponen-
tially large, enumerating and evaluating all possibilities is
not feasible. This challenge is even more important for web
search engines, where response times are in the order of mil-
liseconds.
To address this issue, we propose a simpler version of the
query reduction problem. In particular, instead of consider-
ing all possible reduced versions, we only consider those that
differ from the original query Q by only one term. That is, instead of using the entire power-set PQ, we use a restricted version P1Q = {P |P  PQ  |P |  |Q| - 1}. Thus, if the original query had n query words, we only need to consider
the n reduced versions and original query Q.

long web queries through query reduction.
3.3 Learning Formulations
We use three formulations for choosing between the original query and its reduced versions: 1) Independent performance prediction, 2) Difference prediction, and 3) Ranking queries. All three formulations use the same performance predictors to generate features but differ in their target learning functions. For the remainder of this paper, we assume that the same ranking algorithm, f , is used to retrieve results for both the original query and its reduced versions and hence drop it from our notations.
Let Q be the set of training queries and let T (Q) be the effectiveness of their retrieved results.

Despite the obvious limitation of ignoring a large number of potentially better reduced versions, this simple approach can yield dramatic performance improvements. On a large collection of more than 6400 Web queries (see Section 4 for details), we find that an oracle that chooses between an original long query and its reduced versions achieves more than 10 points gain in NDCG@5.
However, in order to achieve this gain, we need to reliably identify reduced versions whose performances are better than that of the corresponding original queries. To illustrate the potential impact of this technique, we analyze the distribution of maximum and average gains for reduced ver-

3.3.1 Independent Prediction
Given an original long query and its reduced versions, we predict the performance of each query independently. Then, we select the query that has the highest predicted performance. Thus, the query selection problem is transformed into a query performance prediction task: Given a query, and the retrieved results, the task is to predict the effectiveness of the retrieved results.
Formally, given a set of functions h : PQ  R, we learn a non-linear regressor h that minimizes the mean squared error as given by:

sions using Figures 1(a), (b), and (c). Figures 1(a) and (b)

show the distribution of gains when compared to the original query. Figure 1(c) shows distribution of gains (as box plots) for original queries with different NDCG@5 values.
On average, the reduced versions' effectiveness are worse compared to the original query's effectiveness, as shown by the negative gains dominant in Figure 1(a). Also, the maxi-

v

h

=

arg

min

u u

X

(h(P ) - T (P ))2

h

t
QQ,P P1Q

For a given test query Qt, we select the query P  with the largest predicted performance, i.e.:

mum gains, the gains that can be achieved if we always identify the best reduced version, are mostly positive as shown in Figure 1(b). However, for some queries the maximum

P  = arg max h(P )

(3)

P P1Qt

gains are negative i.e., choosing any reduced version will re-

3.3.2 Difference Prediction

sult in decreased performance. Finally, Figure 1(c) shows that if the original query has poor performance, then it is more likely for some reduced version to be better than the original query. Conversely we are unlikely to find reduced versions of well-performing queries that provide substantial performance gains.
Based on these observations, we develop learning tech-

While the Independent formulation is relatively simple, it does not encode the relationship between the original query and its reduced versions. Furthermore, based on the observations from Figure 1(c), it may be more important to predict the difference in performance between the original query and its reduced versions, than to accurately predict the effectiveness of the individual queries.

573

In the Difference formulation, we predict the difference in performance between each reduced version and its original query, and then select the query that has the highest positive difference. If there is no reduced version with a predicted positive difference, then we choose the original query.
Let D(Q, P ) = T (P ) - T (Q), denote the target measure difference between a reduced version P and its original query Q. Given a set of functions hd : Q×Q  R, we learn a leastsquared-errors regressor hd given by:

v

hd

=

arg

min

uX u

X

(hd (Q, P ) - D (Q, P ))2

hd

t
QQ P P1QP =Q

For a given test query, Qt, we choose a reduced representation P  as:

P  = arg max h(Q, P )

(4)

P P1Qt

3.3.3 Ranking Queries
In this formulation, the goal is to rank the original query and its reduced versions in order to select the top ranking query. The ranking model is learned by training on pairwise preferences between the queries.
For each reduced version P  P1Q, P is preferred over Q if T (P )  T (Q). The pairwise preferences induce a partial ordering and the query at the top of the ordering is selected. This formulation fully encodes dependencies between the original query, and all the reduced versions. Kumaran and Carvalho [13] successfully use this learning to rank approach to select only amongst reduced versions of the query on TREC collections. In this work, we also include the original query in addition to the reduced versions.
Let  denote the error function for incorrect pairwise ordering defined as follows:

( 1 if sign (h(P ) - h(Q)) = sign (T (P ) - T (Q))
h(Q, P ) = 0 otherwise

We want to learn a function hr from the set of ranking functions hr : Q  R such that it minimizes the overall ranking errors, i.e.,:

hr = arg min X X h(Q, P )

hr

QQ P P1Q

For a given test query, Qt, we choose a reduced representation P  as:

P  = arg max hr (P )

(5)

P P1Qt

3.4 Thresholding
As an extension to these formulations, we also learn a threshold on the assigned scores in order to control the number of queries for which reduced versions are selected. In Independent, a reduced version is selected if and only if, there exists a reduced version whose predicted performance is greater than that of the original query by a specified threshold. For Difference, the positive difference has to exceed a threshold in order to choose a reduced version. Finally, for Ranking, the predicted performance of the topranking reduced version must exceed the original query's predicted performance by the specified threshold.

For all three formulations, we also learn the best threshold values by selecting the values that achieve the best improvements over the training set of queries.
3.5 Performance Prediction
Accurate prediction of performance of the original query and reduced versions is critical to the success of the query reduction formulations we have described in this Section. We leverage a rank-time technique for query performance prediction [2]. The key idea behind this technique is to utilize retrieval scores, and the features that a ranker uses for ranking documents to estimate the quality of the results. Table 1 lists the two broad types of features used for ranking documents as well as predicting query quality ­ those that characterize the prior probability of query effectiveness, and those that characterize the quality of the retrieved results. In Independent the features are used as is, while in Difference and Ranking the difference in feature values between the original and the reduced versions are used.
Query Features : We use several query-specific features to provide a richer representation of the query. Lexical features flagging the presence of URL, stop words, and numbers as well as location features that denote the presence of town, city, or state names are part of the feature set. Additionally, we use query length as a feature. Query length has a strong negative correlation with performance i.e., retrieval effectiveness degrades as query length increases.
Query-Document Features: The retrieval scores assigned by the ranking algorithm are indicative of the relevance of individual documents, and aggregates of these scores are useful predictors of the relevance of the retrieved results. Web search engines often combine multiple ranktime features to score documents. These rank-time features provide different types of evidence for the relevance of the documents. Some useful features include query-independent features similar to page-rank, query-log based features such as variations of click-through counts, and term match-based features such as BM25F.
Using these query and document-related features, we train a regressor to directly predict the performance (NDCG@5) of queries.
4. EXPERIMENTAL SETUP
We conduct experiments to evaluate query reduction and the utility of the different learning formulations.
We use LambdaRank [6] as our web ranking algorithm. LambdaRank has been shown to be an effective learning to rank technique for the web and can handle a large number of features. We target NDCG@5 [10] as the metric to maximize. We use the top 5 results to create the querydocument features, and select the top 100 features that are most correlated with NDCG@5 in the training data. We evaluate the query reduction approach and the utility of the different formulations on a large collection of Web queries. To create the query reduction collection, we first obtain a frequency-weighted random sample2 of more than 6400 long queries issued to a major Web search engine. For each query in this collection, we also create reduced versions by dropping a single word each time. We use the LambdaRank retrieval algorithm to rank documents for both the original
2The frequency-weighted sampling ensures the chosen queries are a representative mix of the types of long queries.

574

Table 1: Features used for performance prediction. Pos. indicates that the value for the particular row is computed for each top-k document and used as an independent feature. Agg. indicates that statistical aggregates (mean, max, standard deviation, variance and coefficient of dispersion) of the values for the top-k documents are also used as independent features. Additionally, we perform a min-max normalization to rescale all features between [0, 1].

Type

Feature Name Description

Variants

Query

URL

Binary: Does query contain an URL?

-

Stop-words

# of stop-words in query

-

Location

Does query contain a town, city name or a state name ?

-

Query Length # of words in query

-

Query-Document

LR BM25 Click-based Static Scores

LambdaRank score of top-k documents Okapi-BM25 score of top-k documents Click-through counts and other variants Page-rank like scores of the top-k documents

Pos. and Agg. Pos. and Agg. Agg. Agg.

long query and all its reduced versions. Then, we obtain relevance judgments for both the results for the original query and the reduced versions from a large pool of independent annotators. The reduced versions results are also judged with respect to the original long query, and not with respect to the reduced one. We refer to this collection as Long Web henceforth.
Learning Algorithms. For Independent and Difference formulation, the goal of learning is to find real-valued functions that predicts the target metric, and the differences in the target metric, respectively. For both formulations, we use non-linear regression with the Random Forests [18] algorithm3, to predict performance of queries and performance differences respectively. For the Ranking formulation, we use RankSVM [11] 4 to learn pair-wise preferences. For all three problem formulations we perform five-fold cross validation for training and evaluating the learning models.
5. RESULTS
For each formulation, we report results for two types of experiments. In Query Replacement experiments, if a reduced version is selected, it is used to replace the original query. In Results Interleaving if a reduced version is selected, the results of the selected reduced version and the original query are interleaved. The interleave order depended on the sign of the difference between their predicted NDCG@5.

does not capture such relationships. Second, the Independent formulation appears to solve a harder learning problem. The regression in Independent attempts to minimize meansquared errors of the predicted and actual NDCG@5 values. The accuracy of the predicted values matter only in terms of the ordering they induce, the difference between the absolute values are not important. In fact, we find that the root-mean squared errors for Independent's regression was 10% worse compared to that of Difference's regression, even though both regressions are learned using the same learning algorithm, the same set of base features, and same training data.
With thresholding however, Independent improves dramatically and its overall performance is comparable to Difference. On the other hand, Difference and Ranking do not achieve any additional improvements to the overall measures due to thresholding. Independent benefits from thresholding as it selects fewer reduced versions which are more likely to yield improvements. Despite the similar average performance (over the entire set of queries), the three formulations provide different types of improvements. First, Independent and Ranking select reduced versions for less than 10% of the queries, whereas Difference selects reduced versions for more than 27% of the queries. The number of selected reduced versions depends upon the accuracy of the predicted scores, and the thresholds learnt during training.

5.1 Query Replacement
Table 2 shows the performance of the different problem formulations (top half) along with the thresholding extension (bottom half) for the Query Replacement task. We compare the formulations using two measures: 1) Overall NDCG@5, which is the macro-averaged NDCG@5 over all queries, and 2) Subset NDCG gain, the average improvements on the subset for which reduced versions were chosen.
With no thresholding, Difference achieves the best overall gain. Ranking's overall gain is lower but the subset gain is substantially higher. While, Difference and Ranking both achieve small but significant overall gains, Independent is actually worse than the original. We believe that this performance difference is due to two reasons. First, Difference and Ranking encode the relationship between the original query and its reduced versions, whereas Independent
3We used the randomForest package available from R with default parameters 4We used the SVMLight implementation with default parameters and a linear kernel

Figure 2: Number of queries affected versus improvements in subset gains.
To better understand the relationship between the number of queries affected versus the average improvement in performance, we explore the behavior of the different formulations at various thresholds. In general, we expect increasing thresholds to cause fewer reduced versions to be chosen (lower recall), but to also increase the likelihood of

575

Table 2: Effectiveness of Query Replacement. The baseline i.e., using the original query, has an overall NDCG@5 of 38.19. Bold face indicates the highest value, and * indicates statistically significant improvement over the baseline (p < 0.05 on a two-tailed paired t-test).

No Thresholding Independent Difference Ranking Thresholding Independent Difference Ranking

Overall NDCG@5
35.18 38.63 38.50
38.64 38.63 38.50

Affected Queries
4567 (70%) 1761 (27%)
612 (9%)
457 (7%) 1761 (27%)
612 (9%)

Improved Queries
1583 513 245
219 513 245

Hurt Queries
2346 427 212
149 427 212

Subset NDCG Gain
- 4.26 (-12%) + 1.61 (+4.2%) + 4.64(+12.1%)
+ 6.33 (+16.5%) + 1.61 (+4.2%) + 4.64(+12.1%)

improving over the original query (higher precision). Figure 2 shows this precision-recall trade-off in terms of the gains achieved on the subset of affected queries against the percentage of queries affected. As expected for all three formulations, the average performance on the subset is drastically high for a small percentage of queries, and performance decreases as more queries are affected. Independent and Difference achieve more than 10 points absolute improvement over the original queries on a subset of 5% of the queries. Compared to Independent and Difference, Ranking achieves smaller gains but retains its performance over a large fraction of queries. This suggests that Ranking is able to choose reduced versions effectively for more number of queries but it may not always choose the best reduced version.
Feature Importance. The contribution of the different feature groups for Difference are shown in Table 3. Most of the gains come from the query-document features. Adding the query features provides small improvements. As expected features that characterize the result set directly are more useful than the query-features. Also, adding estimates of original query's effectiveness further improves performance. This is because reduced versions are more likely to improve poorly performing original queries and adding estimates helps Difference to encode this relationship.
Table 3: Feature importance for Difference: Orig -- using original query, with no query replacement. QueryDoc -- using query-document features alone. +Query -- adding query features. +Estim -- adding estimate of the performance of the original query.

Orig QueryDoc + Query + Estim

Overal Gain 38.19

38.52

38.63

38.73

Subset Gain 0

1.30

1.61

2.05

5.2 Results Interleaving
To reduce the risk involved in choosing between queries, we conduct interleaving experiments. In all three formulations, if a reduced version is selected, we interleave its results with the results of the original query. Furthermore, we decide the order of interleaving based on the predicted performance. If the original query's predicted performance was higher than that of the reduced version then interleaving begins with the original query, and vice-versa otherwise5
Table 4 shows the gains achieved by the interleaving results. Difference achieves the best overall gains, whereas
5Because interleaving combines results from the original query and the top-ranked reduced version, it can yield gains even in cases where the top-ranked reduced version's predicted performance is lower than that of the original query.

Table 4: Results Interleaving at the best thresholds for the three formulations. For the NDCG Gain row, bold face indicates the highest value, and * indicates statistically significant improvement over original NDCG@5 (p < 0.05 on a paired t-test).

Overall Gain Subset Gain Best Threshold Affected Queries Improved Queries Hurt Queries

Indep. 0.7 9.89 0.2 457
228 (4%) 139 (2%)

Diff. 1.3 1.3 -0.2 6435 2052 (31%) 2063 (31%)

Rank. 0.97 1.19 -0.8 5258 1620 (25%) 1612 (25%)

Independent achieves the best subset gains. Difference and Ranking both have a positive impact on a large number of queries, 31% and 25% respectively, whereas Independent provides positive gains for only 4%. We hypothesize that one of the reasons that Difference and Ranking achieve higher performance compared to Independent is because Difference and Ranking achieve better ranking of reduced versions compared to Independent, thus allowing more queries to benefit from interleaving.
Figures 3(a), (b), and (c), show Results Interleaving and Query Replacement performance at different thresholds for all formulations. For all formulations, Results Interleaving performs better compared to Query Replacement at all thresholds. Since interleaving ensures that at least some of the original query's results are mixed in with the chosen reduced versions results, we reduce the risk of hurting performance in case of erroneous choices. However, interleaving also benefits from the fusion of results from the original long query, and the reduced version in the case of good choices. Although not shown here, an analysis of the gains obtained by Query Replacement, and Results Interleaving for Independent formulation shows that there are fewer queries with large positive gains for Results Interleaving compared to Query Replacement, but there are also fewer negative gains. For example, nearly 10% of positive gains in Query Replacement are above 25 points, whereas only 5% of positive gains in Results Interleaving are above 25 points. On the other hand, nearly 20% of the negative gains in Query Replacement are below 20 points, whereas less than 5% of the negative gains in Results Interleaving are below 20 points.

6. ANALYSIS
We further analyze the results to better understand the distribution of gains and the nature of the improvements.

576

Average Gains -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
Average Gains -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
Average Gains -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5

Results Interleaving Query Replacement

Results Interleaving Query Replacement

Results Interleaving Query Replacement

-0.4 -0.2 0.0 0.2 0.4 Threshold
(a) Independent: NDCG@5

-0.4 -0.2 0.0 0.2 0.4 Threshold
(b) Difference: NDCG@5

-0.4 -0.2 0.0 0.2 0.4 Threshold
(c) Ranking: NDCG@5

Figure 3: Interleaving Results.

Query reduction results in dramatic gains on some subset of queries but also incurs losses on some queries. For Query Replacement using Independent formulation nearly 60% of this impacted queries were improved. Further, for 20% of these queries, the gain was more than 50 points in absolute NDCG. This indicates the dramatic improvements attainable through query reduction. For the remaining 40%, a large proportion of their losses are small, which explains the overall improvements. We observe similar trends for Difference and Ranking.
Potential versus Achieved Gains. All three formulations provide improvements when there is a large potential for improvement. Figure 4 shows the distribution of the gains achieved by Independent in relation to the best gains that an oracle can achieve. In most cases when the potential for improvement is large, Independent formulation achieves larger improvements. Also, When the potential is greater than 0.8, Independent always results in some positive improvement. We believe that the large gains achieved by the formulations are primarily due to two factors.

given the range of prediction errors, while larger differences can be captured more reliably.
Second, for queries with large gains, the problem of choosing a reduced version becomes easier. Figure 5(b) shows the distribution of number of reduced versions that are better than the original query. The number of better reduced versions correlates with the maximum achievable gain (shown in the x-axis). For queries with high potential for improvement (i.e., queries with high maximum gain), the number of better reduced versions is higher, which makes the problem of finding better reduced versions easier.

(a) NDCG@5 Prediction Errors: Frequency histogram of difference between predicted and actual NDCG@5 values.

Figure 4: Oracle versus Achieved Gains: Boxplot showing the distribution of gains achieved by Query Replacement using Independent in relation to the best possible gains that can be achieved by an oracle.
First, the formulations are able to detect large differences in NDCG@5 more reliably. The distribution of absolute prediction errors for the regression used by Independent is shown in Figure 5(a). The histogram shows the frequencies of absolute difference between the predicted NDCG@5 and the acutal NDCG@5 values for all queries (including reduced versions). Most prediction errors are smaller, and very few errors are larger than 50 points (less than 3%). This suggests that smaller differences in NDCG@5 are harder to capture

(b) Boxplot showing distribution of reduced versions that are better than the original query versus potential gains.
Figure 5: Prediction accuracy and difficulty of choosing reduced versions.
Improving Poorly Performing Queries. As illustrated in Figure 1, poorly performing original queries often have large potential for improvements. Since all formulations deliver improvements when there is large potential, most gains are achieved for poorly performing original queries. Figure 6(a) shows the histograms for the number of queries that achieve positive gains against the effectiveness

577

of the original query. Clearly, most of the gains are achieved for queries whose original NDCG@5 low. Nearly 75% of the queries that benefit from Independent are queries with NDCG@5  40. Further, the magnitude of the gains for the poorly performing queries are higher than for well performing queries as shown in Figure 6(b). Thus, unlike traditional techniques such as pseudo-relevance feedback, query reduction delivers improvements where it matters most.
(a) Number of queries at each effectiveness level which achieved positive gains.
(b) Boxplot showing magnitude of achieved gains for original queries of different effectiveness levels. Figure 6: Performance of Independent for original queries of different effectiveness levels
7. CONCLUSIONS
As the average length of queries users submit to search engines increases, long query retrieval becomes an increasingly important problem. In this paper we have addressed some of the key challenges in adapting query reduction for long web queries -- a particularly difficult task due to the inherent constraints imposed by modern search engine architectures and operational requirements.
We presented three learning formulations that naturally provide different trade-offs in terms of number of queries affected versus the overall average gains achieved by query reduction. Such flexibility is valuable when designing large scale systems, where memory and processing limitations may significantly impact when and whether a query should be altered.
We also provided the first comprehensive evaluation on a large collection of real long web queries. Our experiments showed that directly predicting performance differences generally outperforms independent performance predictions. Also, performance of the proposed formulations can be improved even further by interleaving results from the original and reduced versions. A careful analysis of the results clearly showed that, unlike traditional techniques such as pseudo-relevance feedback, our query reduction techniques achieved most NDCG gains on difficult long queries.

The na¨ive approximation to the full scale (exponential) query reduction problem substantially improves efficiency (exponential to linear), while still providing significant effectiveness gains. However, even evaluating a linear number of additional queries can be burdensome for search engines. Also, despite improved average performance, we find that there is high variance in performance. As part of future work, we aim to further improve efficiency and reduce variance using a two-staged approach that first predicts the effectiveness of the original long query to decide when to evaluate the reduced versions.
8. ACKNOWLEDGMENT
This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsor. The authors also thank the anonymous reviewers for their helpful comments and suggestions.
9. REFERENCES
[1] Searches getting longer: A weblog by alan long, hitwise intelligence. http://weblogs.hitwise.com/alan-long/ 2009/11/searches_getting_longer.html.
[2] N. Balasubramanian, G. Kumaran, and V. Carvalho. Predicting query performance on the web. In SIGIR 2010.
[3] M. Bendersky and W. Croft. Discovering key concepts in verbose queries. In SIGIR, pages 491­498, 2008.
[4] M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. In WSCD, pages 8­14, 2009.
[5] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM '10, pages 31­40, 2010.
[6] C. Burges, R. Ragno, and Q. Le. Learning to rank with nonsmooth cost functions. NIPS, 19:193, 2007.
[7] Y. Chen and Y.-Q. Zhang. A query substitution - search result refinement approach for long query web searches. In WI-IAT, pages 245­251, 2009.
[8] C. Hauff, V. Murdock, and R. Baeza-Yates. Improved query difficulty prediction for the web. In CIKM, pages 439­448, 2008.
[9] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In SPIRE, pages 43­54, 2004.
[10] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems, 20(4):422­446, 2002.
[11] T. Joachims. Optimizing search engines using clickthrough data. In SIGKDD, pages 133­142, 2002.
[12] G. Kumaran and J. Allan. A case for shorter queries, and helping users create them. In HLT/NAACL, pages 220­227, 2007.
[13] G. Kumaran and V. Carvalho. Reducing long queries using query quality predictors. In SIGIR, pages 564­571, 2009.
[14] M. Lease. An improved markov random field model for supporting verbose queries. In SIGIR, pages 476­483, 2009.
[15] M. Lease, J. Allan, and W. B. Croft. Regression rank: Learning to meet the opportunity of descriptive queries. In ECIR, pages 90­101, 2009.
[16] C. Lee, Y. Lin, R. Chen, and P. Cheng. Selecting Effective Terms for Query Formulation. In AIRS 2009, pages 168­180, 2009.
[17] C.-J. Lee, R.-C. Chen, S.-H. Kao, and P.-J. Cheng. A term dependency-based approach for query terms ranking. In CIKM '09, pages 1267­1276, 2009.
[18] A. Liaw and M. Wiener. Classification and regression by randomforest. R News, 2(3):18­22, 2002.

578

Acquisition of Instance Attributes via Labeled and Related Instances

Enrique Alfonseca
Google Inc. 110 Brandschenkestrasse
Zurich, Switzerland
ealfonseca@google.com

Marius Pas¸ca
Google Inc. 1600 Amphitheatre Parkway
Mountain View, California
mars@google.com

Enrique Robledo-Arnuncio
Google Inc. 110 Brandschenkestrasse
Zurich, Switzerland
era@google.com

ABSTRACT
This paper presents a method for increasing the quality of automatically extracted instance attributes by exploiting weakly-supervised and unsupervised instance relatedness data. This data consist of (a) class labels for instances and (b) distributional similarity scores. The method organizes the text-derived data into a graph, and automatically propagates attributes among related instances, through random walks over the graph. Experiments on various graph topologies illustrate the advantage of the method over both the original attribute lists and a per-class attribute extractor, both in terms of the number of attributes extracted per instance and the accuracy of top ranked attributes.
Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Information extraction, instance attributes, unstructured text, distributional similarities, labeled instances
1. INTRODUCTION
Motivation: Due in part to the quantitative limitations of the then-available textual data sources, early work on information extraction focuses on training supervised systems on small to medium-sized document collections, requiring relatively expensive manual annotations of data [5]. Some authors investigate the possibility of obtaining manual annotations more easily, through the creation of manual or semi-automatic annotations [6]. But as larger amounts of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

textual data sources have become available at lower computational costs, either directly as document collections or indirectly through the search interfaces of the larger Web search engines, information extraction has seen a shift towards large-scale acquisition of open-domain information [8]. In this framework, information at mainly three levels of granularity is extracted from text, with weak or no supervision: classes (having a label, e.g., painkillers), class elements or instances (e.g., vicodin, oxycontin); and relations among instances (e.g., france-capital-paris) or classes (e.g., countries-capital-cities).
Among other types of relations targeted by various extraction methods, attributes (e.g., side effects and maximum dose) have emerged as one of the more popular types, as they capture properties of their respective classes (e.g., painkillers), and thus serve as building blocks in many knowledge bases. Consequently, a variety of attribute extraction methods mine textual data sources ranging from unstructured [23] or structured [26, 4] text within Web documents, to human-compiled encyclopedia [25, 7] and Web search query logs [17, 16], attempting to extract, for a given class, a ranked list of attributes that is as comprehensive and accurate as possible. Contributions: This paper introduces a method that acquires instance relatedness information, and applies it for attribute extraction, in order to produce ranked lists of attributes of higher coverage and accuracy than current state of the art. The algorithm proposed is general enough that can be plugged to any generic attribute acquisition algorithm to increase precision and coverage. We explore this method, based on the identification of attributes of individual instances (vicodin), in contrast to most previous work on attribute extraction [23, 16], why acquire attributes of classes (e.g., painkillers). Thus, since a large majority of popular search queries are precisely instances of various kinds, the method operates over an input vocabulary (i.e., instances rather than classes) that better approximates the overall composition of the sets of millions of search queries submitted daily to Web search engines.
Instance relatedness is approximated through two types of data: pairwise distributional similarities [13], quantifying the extent to which any two instances occur in similar contexts in text; and labeled classes of instances, capturing the class labels (e.g., painkillers, prescription drugs, medications and substances) applicable to various instances (vicodin). It produces a significant improvement over either learning the attributes for class labels, and over learning the attributes for instances without propagation.

58

Applications: The special role played by attributes, among other types of relations, is documented in earlier work on language and knowledge representation [20, 9]. It inspired the subsequent development of text mining methods [14] aiming at constructing knowledge bases automatically. In Web search, the availability of instance attributes is useful for applications such as search result ranking and suggestion of related queries [2], and has also been identified to be a useful resource in generating product recommendations [19].
2. PREVIOUS WORK
Previous work on attribute extraction uses a variety of textual data sources for mining attributes. Taking advantage of structured and semi-structured text, the method presented in [26] submits list-seeking queries to general-purpose Web search engines, and analyzes retrieved documents to identify common structural (HTML) patterns around class labels given as input, and potential attributes. Similarly, layout and other HTML tags serve as clues to acquire attributes from either domain-specific documents such as those from product and auction Web sites [24] or from arbitrary documents, optionally relying on the presence of explicit itemized lists or tables [4]. As an alternative to Web documents, articles within online encyclopedia can also be exploited as sources of structured text for attribute extraction, as illustrated by previous work using infoboxes and category labels [22, 15, 25, 7] associated with Wikipedia articles.
Working with unstructured text within Web documents, the method described in [23] applies manually-created lexicosyntactic patterns to document sentences in order to extract candidate attributes, given various class labels as input. The candidate attributes are ranked using several frequency statistics. If the documents are domain-specific, such as documents containing product reviews, additional heuristicallymotivated filters and scoring metrics can be used to extract and rank the attributes [21]. In [2], the extraction is guided by a small set of seed instances and attributes rather than manually-created patterns, with the purpose of generating training data and extract new pairs of instances and attributes from text. The set of seeds is acquired automatically in [19], thus further reducing the overhead associated to the preparation of the input data, while exploiting words and part-of-speech labels as features during extraction. Web search queries have also been considered as a textual data source for attribute extraction, using lexico-syntactic patterns [17] or seed attributes [16] to guide the extraction.
Virtually all existing methods for attribute extraction produce ranked list of attributes for each input item (i.e., class label or, more rarely, class instance). Therefore, the method presented in this paper is generally applicable. It acts as a wrapper that takes as input the output from an existing method, propagating attributes across related instances to improve recall, while re-ranking top attributes to improve precision.
3. RELATED INSTANCES IN ATTRIBUTE EXTRACTION
3.1 Instance Attribute Extraction
Given an instance (e.g., cloxacillin as element of the set antibiotics), the goal of instance attribute extraction is the automatic acquisition of the set of relevant attributes (e.g.,

side effects, mechanism of action, cost, therapeutic range) capturing the most prominent properties of the instance. Ideally, the resulting set would be complete (perfect coverage), and contain only relevant attributes (perfect precision). In practice, the task of attribute extraction can be approximated by the acquisition of a ranked list of attributes [a1, a2, a3, ..., aN ], such that as many relevant attributes as possible are among the top items in the ranked list.
The task of instance attribute extraction is related to, and more difficult than, the task of class attribute extraction. Indeed, although both tasks generate ranked lists of attributes, the former does so for an individual instance (e.g., cloxacillin), whereas the latter works for a class of instances (e.g., antibiotics or penicillins). Since a class of instances is often available as input in the form of a set of instances (e.g., {ampicillin, oxacillin, cloxacillin, benzylpenicillin}) associated with a corresponding class label (e.g., penicillins), class attribute extraction methods [17] enjoy access to significantly more input data (i.e., a set of instances instead of a single instance), which allows them to mitigate the negative impact of data sparseness on the extraction outcome, by acquiring attributes of a class (e.g., car manufacturers), based on associations with some of the more popular instances of the class (e.g., jaguar, audi, toyota, chevrolet), even if a few of the input instances (e.g., tesla}) of the class may be absent from the underlying source of textual data from which attributes must be extracted;
Since instance attribute extraction is equivalent to class attribute extraction where the class contains only one element (the instance itself), none of the above positive properties hold in the case of instance attribute extraction. In particular, data sparseness may strongly affect the quality and coverage of the ranked lists of attributes extracted for long-tail instances, that is, instances that occur relatively infrequently in the underlying textual data source. Concretely, a previous method operating over query logs [17] extracts the following ranked lists as attributes for the respective instances: [solubility, analytical methods, inventor, dossier development, last patent, adverse effects] for bicalutamide; [map, opening] for agua caliente casino; and [] (i.e., the empty list) for ct scan. Clearly, the extracted lists suffer from occasional less-than-optimal extractions (e.g., dossier development for bicalutamide), but more importantly from occasionally too few or no elements in the lists, which suggests a low coverage.
3.2 Role of Related Instances
The operational advantage of class attribute extraction over instance attribute extraction stems from its ability to aggregate attributes of the class, from the attributes extracted for individual instances of the class. In order to emulate a similar behavior in instance attribute extraction, one would need a set of instances as input, which are simply not available. However, given the input instance, a rough approximation of a set of instances could be generated dynamically, based on access to a large resource of instance relatedness data. In that case, instance attribute extraction becomes equivalent to aggregating attributes of the instance, from the attributes extracted for other instances that are strongly related to it, based on the following observation:
Hypothesis 1: Let i1 and i2 be two instances. The more strongly related i1 and i2 are semantically, the more likely it is for them to share common attributes.

59

3.3 Sources of Instance Relatedness Data
Two types of relatedness data are explored in this paper for the purpose of attribute extraction: IsA pairs and distributional similarities.
Distributional similarities are a technique for calculating similarities among words or phrases [12]. They capture the extent to which the textual contexts in which phrases occur are similar, with the intuition that phrases that occur in similar contexts tend to have similar meanings. Distributional similarities scale well to large text collections, since their acquisition can be done through parallel implementations, yet they perform well against more expensive, knowledgebased similarity metrics [1]. Distributional similarities are assumed to be available, as pairs of similar phrases with an associated similarity score (e.g., cloxacillin and erythromycin with a score of 0.36; or cloxacillin and alcohol with a score of 0.05).
In addition to distributional similarities, IsA pairs are assumed to be available as weighted pairs of a class instance and an associated class label (e.g., audi and car manufacturers; or cloxacillin and antibiotics). IsA pairs indirectly capture instance relatedness, since any two instances involved in IsA pairs with the same class (e.g., audi and nissan, as distinct instances of the class car manufacturers) are related to each other.
4. ATTRIBUTE PROPAGATION METHOD
Graph Representation: Given an instance ij and some extraction method, let
Aj = [(a0, wj0), (a1, wj1), (a2, wj2), ..., (a|A|, wj|A|)]
be the weighted list of attributes extracted by the method for the instance ij , where A is the set of all attributes of all instances. One can define a probability of transitioning from instance ij to attribute ak by normalizing the weights to sum up to 1:
pjk = P|lwA=j0| kwjl
and represent the instances and attributes as a bipartite graph, with weighted edges from instance nodes to attribute nodes. Injecting Class Labels: The availability of IsA pairs allows for the extension of the instance-attribute graph, by adding a layer for the class labels. Given an instance ij , let
Cj = [(c0, wj0), (c1, wj1), (c2, wj2), ..., (c|C|, wj|C|)]
be the weighted list of the class labels of the instance, where C is the set of all class labels. The normalization of the weights produces a class-label probability distribution pcjm, where cm is a class label of the instance ij . In the scenario of a random walk across the graph representation, this probability distribution would provide, for each instance ij , the probability of transitioning to a class label cm. Conversely, the aggregation and normalization of the weights by class label rather than instance produces a probability distribution pimj , indicating the probability of transitioning from a class label cm to an instance ij within the graph, as illustrated in Figure 1. The resulting graph can be used to propagate attributes across instances of the same class, using a three-step random walk process:
1. From an instance (e.g., i0), execute a random step to one of its class labels (e.g., c0). The probability of each step

G@GG@@G@FAFFAAFAaaaaEBEEBBEB0123 PÕ Ò`ÑDCDDCCDCPÓqwfxÒ`Po PqwÒ`PÒ`qwPÒ`PqwPÒ`PqwppppÒ`PÒ`pp1010qwPÒ`P122301qwpPÒ`13Pqw0ppÒ`P0Ò`01qwPÒ`P40qwPÒ`PqwÒ`PÒ`qwPPÒ`qwPÒ`PqwÒ`PÒ`qwPPÒ`?8P>9i0=:9 @U<;B ~

G@FAaEB4 DCo

p14

?8>9i1=:<; p

b C E
F
~ 

p

pc00
wpi00



H P Rpi1p0c01

T

V

Y

 a pc1p0i01



Ø

Ö

Ô Ñ f

pc11
kpi11q

x


  d
H Ð

& 8?a 9>c# 0:=;< "
f 8?9>c1:=;< Ø

Figure 1: Graph for the transitions from instances to instances via class labels (i, a and c are instance, attribute, and class label nodes, respectively)

is governed by the distribution pc(·), following the dotted edges in Figure 1.
2. From a class label (e.g., c0), execute a random step to move to an instance of that class (e.g., i0). The probability is given by pi(·), following the dashed edges.
3. From an instance (e.g., i0), execute a random step to an attribute (e.g., a0), using the probability distribution p(·), following the solid edges.
This random walk process defines, for each original instance ij , a set of reachable attributes, and the probability of reaching each of them from ij . These attributes can be ranked by this probability.
Note that, if the input data does not contain any class label at all for any instance, no ranked list of attributes can be returned for that instance. This can be avoided by adding, for each instance i, a pseudo-class label ci representing a class that only contains that instance. In this way, for instances without any class label at all no propagation will be done, but they will at least retain their original ranked list of attributes. Injecting Distributional Similarities: The second type of information used to propagate attributes across instances are distributional similarities, used to identify related instances. Given an instance ij and its list of similar instances:
Ij = [(i0, wj0), (i1, wj1), (i2, wj2), ..., (i|I|, wj|I|)]
where I is the set of all instances, the graph can be extended by again normalizing the weights, and adding edges to the graph corresponding to transitions from each instance ij to similar instances. The resulting graph is depicted in Figure 2. In this case, the propagation would be defined by a two-step random walk, first transitioning from an instance to similar instances and then transitioning to attributes. Using distributional similarities the similarity of any instance with itself will always be 1.0, value which will be normalized together with other the similary scores. Injecting Class Labels and Distributional Similarities: Figure 3 shows an example of a graph topology that

60

ps00



G@GG@@G@FAFFAAFAaaaaEBEEBBEB0123 PÕ Ò`ÑDCDDCCDCPÓqwfx Ò`Po PqwÒ`PÒ`qwPÒ`PqwPÒ`PqwppppÒ`PÒ`pp1010qwPÒ`P122301qwpPÒ`13Pqw0ppÒ`P0Ò`qw01PÒ`P40qwPÒ`PqwÒ`PÒ`qwPPÒ`qwPÒ`PqwÒ`PÒ`qwPPÒ`8?P9>i0p:=s0 ;<1

ps10

G@FAaEB4 DCo

p14

8?t 9>i1:=;<

ps11

Figure 2: Graph for the transitions from instances to distributionally similar instances (i and a are instance and attribute nodes respectively)

includes both class labels and distributionally similar instances. The outgoing probabilities for every node are normalized so their sum is 1.
Using this graph topology, the propagation is a two-step algorithm. In the first step, the algorithm calculates, for each instance, the probability of transitioning to any instance in the dataset either by following the self-loop, by randomly walking in two steps through the class labels, or by stepping randomly to a distributionally similar instance. Next, the algorithm calculates the probability of transitioning to any of the attributes. These probabilities are used to rank the attributes, and thus generate a ranked list of attributes as output, for each instance.
5. EXPERIMENTAL SETTING
Textual Data Sources: The acquisition of instance attributes relies on unstructured text available within Web documents and search queries. The collection of queries is a random sample of fully-anonymized queries in English submitted by Web users in 2006. The sample contains about 50 million unique queries. Each query is accompanied by its frequency of occurrence in the logs. The document collection consists of around 200 million documents in English, as available on the Web in 2009. The textual portion of the documents is cleaned of html, tokenized, split into sentences and part-of-speech tagged using the TnT tagger [3]. Parameters for IsA Pairs: Pairs of a class label (e.g., antibiotics) and a class instance (e.g., cloxacillin), along with associated frequency-based scores, are extracted from the collection of Web documents by applying a few IsA extraction patterns selected from [11], as described in [18]. The pairs are organized as ranked lists of class labels per class instance, e.g., [antibiotics, resistant penicillins, lactams, peni-

ps00
 G@GGG@@@G@FAFFFAAAFAaaaaaEBEEEBBBEB01234 ÖIÓYÑDCDDDCCCDCIÓrvfx ÓYIoo IÓYrvIÓYrvIÓYIrvIÓYppppIÓYrvIpp1010ÓYIrv122301ÓYIpprvI13ÓY01ppIÓYrvI0401ÓYIrv40IÓYrvIÓYIÓYrvIÓYIrvIÓYIrvÓYIÓYrvIIÓY88??It 99>>pii01::==s0 1 Q;;<< qdxo StTspvs1V0xtXv`exbfdzhppppppppi0c1c1i1f|c0i0i1c00010i0111h~kpÐmÒrnÔtpÖruØtÙwG V8?8?69>9>% cc01:=:=;<;<
ps11
Figure 3: Graph including all possible transitions (i, a and c are instance, attribute, and class label nodes, respectively)
cillins, penicillinase-resistant penicillins, spectrum antibiotics, semisynthetic penicillins,..] for cloxacillin. Parameters for Distributional Similarities: When applied to the collection of Web documents, the pipeline for extracting distributional similarities described in [1], which scales to millions of instances and billions of Web documents, and identifies pairwise similarity scores among all instances involved in any IsA pairs. Following standard settings [12], the similarity score between two instances is the cosine between their vectors of context windows, where a window consists of three words to the left and three words to the right of each occurrence, within Web documents, of the two instances respectively. Parameters for Initial Instance Attributes: The extraction method introduced in [17] applies a few patterns (e.g., the A of I, or I's A, or A of I) to queries within query logs, where I is an instance from the IsA pairs, and A is a candidate attribute. For each instance, the method extracts ranked lists containing zero, one or more attributes, along with frequency-based scores. Parameters for Graph Representation: As described in the previous section, the complete version of the graph representation of the instances, attributes and class labels contains three types of nodes: instance, attribute and class label nodes. An instance node is created for each instance from the IsA pairs. For efficiency, at most 250 of the attributes extracted initially for each instance are used to populate attribute nodes and the associated instance-attribute edges in the graph; and distributional similarities below 0.001 are discarded. Even if they were kept, due to the low weights associated to the attributes and the similar queries below those thresholds, it would not have a notable impact on the relative ranking of attributes.

61

aaa, ac compressors, acheron, acrocyanosis, adelaide cbd, african population, agua caliente casino, al hirschfeld, alessandro nesta, american fascism, american society for horticultural science, ancient babylonia, angioplasty, annapolis harbor, antarctic region, arlene martel, arrabiata sauce, artificial intelligence, bangla music, baquba, bb gun, berkshire hathaway, bicalutamide, blue jay, boulder colorado, brittle star, capsicum, carbonate, carotid arteries, chester arthur, christian songs, cloxacillin, cobol, communicable diseases, contemporary art, cortex, ct scan, digital fortress, eartha kitt, eating disorders, file sharing, final fantasy vii, forensics, habbo hotel, halogens, halophytes, ho chi minh trail, icici prudential, jane fonda, juan carlos, karlsruhe, kidney stones, lipoma, loss of appetite, lucky ali, majorca, martin frobisher, mexico city, pancho villa, phosphorus, playing cards, prednisone, right to vote, robotics, rouen, scientific revolution, self-esteem, spandex, strattera, u.s., vida guerra, visual basic, web hosting, windsurfing, wlan
Table 1: Set of 75 target instances, used in the evaluation of instance attribute extraction
Target Instances: In order to assemble a set of instances whose attributes are evaluated for accuracy and relative coverage, the set of all instances from the graph is partitioned according to the number of attributes extracted initially for each instance. The partitions contain all instances with: 0 attributes; 1 to 5 attributes; 6 to 20 attributes; 21 to 50 attributes; and more than 51 attributes, respectively. From each partition, a random sample of 60 instances is automatically selected. The sample is further inspected manually, in order to eliminate instances for which human annotators would likely need a long time to become familiar with the instance and its meanings, before they can assign correctness labels to the attributes extracted for the instance. The only purpose of the manual selection step is to keep the costs associated with the subsequent, manual evaluation of attributes within reasonable limits. To remove any possible bias towards instances with more or better attributes, the extracted attributes, if any, remain invisible during the selection of instances. For example, the instance it (which, among other meanings, is as an acronym for information technology) is discarded due to extreme ambiguity. Conversely, agua caliente casino is retained, since it is relatively less difficult to notice that it refers to a particular casino. The manual selection of 15 instances, from the random sample of each of the 5 partitions, results in an evaluation set containing 75 target instances, as shown in Table 1.
While the comprehensiveness of any attribute extraction experiments increases with the the number of target instances used for evaluation, the time intensive nature of manual accuracy judgments often required in the evaluation of information extraction systems [8] sets a practical limit to the size of the test set. With this in mind, we choose what we feel to be a large enough size for our test set (75 instances with 4,833 attributes in total) to ensure varied experimentation on several dimensions. Experimental Runs: The experiments consist of four different runs: SnCn, SyCn, SnCy and SyCy, where S stands for transitions from instances to distributionally similar instances, C stands for transitions from instances to class labels, and y/n indicate whether the respective transitions are included in the graph topology (y) or not (n). The first run, SnCn, corresponds to using the ranked lists of attributes initially extracted from text, without any propagation.

Run
Sn Cy Sy Cn Sy Cy

Node Type

A

IC

0.49 2.12 3.50

0.49 2.12

0

0.49 2.12 3.50

Edge Type

I-C

I-I I-A

19

03

0 16.50 3

19 16.50 3

Table 2: Number of graph nodes and number of edges of various types, in millions (A=attribute; I=instance; C=class label)

Label vital
okay
wrong

Value 1.0
0.5
0.0

Examples of Attributes
capsicum: calorie count cloxacillin: side effects lucky ali: album songs jane fonda: musical theatre contributions mexico city: cathedral robotics: three laws acheron: kingdom berkshire hathaway: tax exclusion contemporary art: urban institute

Table 3: Correctness labels for the manual assessment of attributes

6. EVALUATION RESULTS
6.1 Quantitative Results
IsA Pairs: The IsA pairs, extracted according to the patterns from [18] from the document collection, cover a total of 2.12 million of the instances that each have two or more class labels, with an average of 19.72 class labels per instance. This set of 2.12 million instances was used for propagation via class labels. Distributional Similarities: The distributional similarities, for the set of 2.12 million instances that have two more class labels, capture one or more similar instances for 13% of them. For the ones that have at least one similar instance, an average of 9 similar instances are returned. Initial Instance Attributes: The initial set of instance attributes, extracted according to [17] from the collection of search queries, is available in the form of ranked lists containing zero, one or more attributes. There is one (possibly empty) ranked list for each of the 2.12 million instances. The extracted ranked lists contain 0 attributes for 1.63 million instances; 1 to 5 attributes for 315,052 instances; 6 to 20 attributes for 56,721 instances; 21 to 50 attributes for 13,902 instances; and more than 50 attributes for 9,249 instances. Graph Representation: Table 2 shows the number of nodes and edges of various types, for the three runs that apply propagation for extracting attributes.
6.2 Qualitative Results
Evaluation Procedure: The measurement of recall requires knowledge of the complete set of items (in our case, attributes) to be extracted. Unfortunately, this number is often unavailable in information extraction tasks in general [10], and attribute extraction in particular. Indeed, the manual enumeration of all attributes of each target instance, to measure recall, is unfeasible. Therefore, the evaluation focuses on the assessment of attribute accuracy.
To remove any bias towards higher-ranked attributes during the assessment of instance attributes, the top 50 attributes within the ranked lists of attributes produced by each run to be evaluated are sorted alphabetically into a

62

Instance: acheron
0.8 SnCn SyCn SnCy SyCy
0.6

Instance: habbo hotel
0.8 SnCn SyCn SnCy SyCy
0.6

Instance: spandex
0.8
0.6

Precision Precision Precision

0.4

0.4

0.4

0.2
0 0

0.2

0

10

20

30

40

50

0

Rank

0.2

0

10

20

30

40

50

0

Rank

SnCn SyCn SnCy SyCy

10

20

30

40

50

Rank

Instance: Average-Instance
0.8 SnCn SyCn SnCy SyCy
0.6

Instance: Average-Instance
0.8 SnCn SyCn SnCy SyCy
0.6

Precision Precision

0.4

0.4

0.2

0.2

0

0

0

10

20

30

40

50

0

10

20

30

40

50

Rank

Rank

Figure 4: Accuracy of the ranked lists of attributes extracted by various runs for a few target instances (top graphs); as an average over all instances (bottom left graph); and as average over every instance that has at least one distributionally similar instance (bottom right graph)

merged list. Each attribute of the merged list is manually assigned a correctness label within its respective instance. In accordance with previously introduced methodology, an attribute is vital if it must be present in an ideal list of attributes of the instance (e.g., side effects for cloxacillin); okay if it provides useful but non-essential information; and wrong if it is incorrect [16]. Thus, a correctness label is manually assigned to a total of 4,833 attributes extracted for the 75 target instances, in a process that confirms that evaluation of information extraction methods can be quite time consuming. Two computational linguists performed the evaluation, with each of the attributes rated by the two of them. The inter-annotator agreement of 88.79%, resulting in a Kappa score of 0.85, indicating substantial agreement.
To compute the precision score over a ranked list of attributes, the correctness labels are converted to numeric values (vital to 1, okay to 0.5 and wrong to 0), as shown in Table 3. Precision at some rank N in the list is thus measured as the sum of the assigned values of the first N attributes, divided by N . Accuracy of Instance Attributes: Figure 4 plots precision values for ranks 1 through 50, for each of the four experimental runs. The first three graphs in the figure show the precision over three individual target instances. Several conclusions can be drawn from these. First, the quality of the attributes extracted by a given run varies among instances. For instance, the attributes extracted for the instance spandex are better than for habbo hotel. Second, the experimental runs have variable levels of accuracy. The bottom left graph in Figure 4 shows the average precision over all target instances. Although none of the runs outperforms the others on each and every target instance, on average, SyCy performs the best and SnCn (i.e., the baseline) the worst, with SyCn placed in-between and SnCy almost as

accurate as SyCy. In other words, propagation based on distributionally similar instances (SyCn) gives a significant improvement over the baseline SnCn, and propagation based on class labels rather than distributionally similar instances (SnCy) gives an even larger improvement. In order to discover whether these results are due to the fact that class labels are more reliable or whether it is because class labels are available for more instances, The bottom right graph in Figure 4 shows the precision of the experimental runs over the subset of instances that have at least one distributionally similar instance. This subset contains exactly 50 of the 75 target instances. The results indicate that class labels and distributional similarities (as well as their combination), when available, lead to similar improvements when both types of relatedness data are available.
Table 4 provides a more detailed view on the accuracy of the extracted attributes, for various subsets of target instances obtained according to the number of initial attributes (i.e., extracted in run SnCn) for the respective instances. For example, rows with an attribute count "[1,5]" refer to instances that have one through five attributes in run SnCn. For this range, the baseline SnCn is outperformed by the three propagation-based runs at all ranks, with precision scores of 0.22 at rank 5 and 0.02 at rank 50 for SnCn, vs. 0.60 at rank 5 and 0.38 at rank 50 for SyCy, (fifth and eigth rows of the table). The table indicates that: 1) with few exceptions, SyCn, SnCy and SyCy outperform SnCn for all ranges, with SnCy and SyCy giving the best results; 2) SyCn, SnCy and SyCy produce more accurate attributes than SnCn even for the range [51,), which indicates that the attribute reranking caused by the propagation is better than the initial attribute ranking given by SnCn; and 3) attribute propagation is quite resistant to sparseness of initial attributes, as illustrated by precision scores of, e.g., SyCy that are com-

63

Instance angioplasty
bicalutamide

Run
Sn Cn Sy Cn Sn Cy
Sy Cy Per-class
Sn Cn Sy Cn Sn Cy
Sy Cy
Per-class

Precision @5 @10 0.00 0.00 1.00 0.75 1.00 0.95
1.00 0.75 1.00 0.90
0.60 0.40 0.60 0.40 0.60 0.80
0.60 0.80
0.40 0.40

Top Ten Attributes

Attributes

none extracted [history, types, complications, risks, definition, cost, pictures, symptoms, center, side effects] [complications, cost, history, types, risks, purpose, definition, pictures, side effects, advantages] [history, complications, types, cost, risks, definition, pictures, side effects, symptoms, center] [types, history, pictures, cost, principles, pros and cons, map, definition, different types, methods]
[solubility, analytical methods, inventor, dossier development, last patent, adverse effects] [solubility, analytical methods, inventor, dossier development, last patent, adverse effects] [solubility, analytical methods, inventor, last patent, dossier development, adverse effects, side effects, pharmacology, effects, pharmacokinetics] [solubility, analytical methods, inventor, last patent, dossier development, adverse effects, side effects, pharmacology, effects, pharmacokinetics] [symptoms, causes, side effects, types, effects, pathophysiology, treatment, american journal, definition, signs and symptoms]

Table 5: Ranked lists of attributes extracted by various runs for a sample of target instances

Att. Count
[0,0]
[1,5]
[6,20]
[21,50]
[51,)
[0,) (entire
set)

Run
SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy SnCn Sy Cn SnCy Sy Cy
SnCn (Rel) (Err) Sy Cn (Rel) (Err) SnCy (Rel) (Err) Sy Cy (Rel) (Err) Perclass

@5
0.00 0.61 0.80 0.77 0.22 0.24 0.60 0.60
0.61 0.63 0.64 0.64
0.67 0.71 0.69 0.71
0.68 0.72 0.75 0.77
0.44 0% -0%
0.58 31% -25% 0.69 57% -45% 0.69 57% -45% 0.52

@10
0.00 0.57 0.76 0.74 0.11 0.13 0.54 0.53
0.54 0.60 0.66 0.63
0.71 0.73 0.71 0.72
0.66 0.71 0.70 0.71
0.41 0% -0%
0.54 32% -22% 0.67 63% -44% 0.66 61% -42% 0.50

Precision @20 @30

0.00 0.52 0.65 0.66 0.05 0.09 0.43 0.43
0.33 0.49 0.55 0.58
0.68 0.70 0.68 0.68
0.65 0.66 0.68 0.67

0.00 0.50 0.64 0.65 0.04 0.08 0.40 0.40
0.22 0.44 0.53 0.54
0.60 0.66 0.64 0.64
0.64 0.63 0.65 0.66

0.34 0% -0%
0.49 44% -23% 0.59 74% -38% 0.60 76% -39% 0.47

0.30 0% -0%
0.46 53% -23% 0.57 90% -39% 0.58 93% -40% 0.44

@40
0.00 0.46 0.62 0.61 0.03 0.07 0.38 0.39
0.16 0.40 0.51 0.52
0.51 0.60 0.61 0.62
0.63 0.62 0.62 0.63
0.27 0% -0%
0.43 59% -22% 0.55 104% -38% 0.55 104% -38% 0.43

@50
0.00 0.46 0.60 0.59 0.02 0.06 0.37 0.38
0.13 0.38 0.50 0.50
0.42 0.57 0.60 0.60
0.61 0.61 0.62 0.62
0.24 0% -0%
0.41 71% -22% 0.53 121% -38% 0.54 125% -39% 0.42

Table 4: Precision scores at various ranks, as an average over subsets of the evaluation set of instances, where a subset contains the instances whose number of attributes extracted by the experimental run SnCn falls into a particular count range (Rel=increase relative to SnCn; Err=error reduction relative to SnCn). In each solid-line block in the table, the highest value and those that are indistinguishable at 95% confidence are bolded

petitive for the range [0,0], where sparseness is extreme (no initial attributes) vs. the range [51,), where sparseness over the top 50 attributes is non-existent.
In the lower part of Table 4, for the range [0,), the precision scores are computed over the entire set of target

instances, and therefore they correspond to points on the curves from the last graph of Figure 4. Also shown in the table are the relative increases (Rel) and the reduction in the error rates (Err) at various ranks, for each of SyCn, SnCy or SyCy, on one hand, relative to SnCn, on the other hand. Per-class attributes: For a comparison, we have used the procedure in [17] as another baseline. This is a per-class extraction procedure that works, from the same input data as the per-instance attribute extraction (SnCn) in the following way: first, from the IsA data, each instance is associated with its highest-ranking class label. For every class label, it looks for attributeness-denoting lexicosemantic patterns in the query logs (e.g. X of Y ) involving any element in the class. The extracted attributes are ranked by frequency in the whole class, and assigned, equally, to all the class elements. There are more than one million class labels, to ensure that they are fine-grained enough that only very closely related instances belong to the same class.
The last row in Table 4 shows the results using this method. The scores consistently improve over the per-instance attribute extraction (SnCn). This is mainly due to the sparse data affecting SnCn: the fact that the instances are grouped together and all their attributes are shared allows the perclass extraction to collect at least 50 attributes for 98% of the instances. On the other hand, since all the instances in the same class have the same relative weight, attributes that are vital only to some members of the class may receive a high standing for all of them. Some examples are king for u.s., or senator for countries that do not have a Senate. Using the graph structure this is partially avoided because (a) each instance has the highest distributional similarity with itself, and (b) every instance share all its class labels with itself. Therefore, the per-instance original attributes tend to end up higher in the final lists after the propagation. Examples: Table 5 shows the top items in the ranked lists of attributes extracted for a few of the 75 target instances. For angioplasty and bicalutamide, SnCn extracts 0 and 6 attributes respectively. The other three runs extract more instances via propagation, with the exception of SyCn for the instance bicalutamide. The per-class procedure does well for angioplasty, but bicalutamide shows some drawbacks of this procedure: although its class label is correctly identified in the IsA data (antiandrogens), this class mistakenly contains a couple of sicknesses, which pollute the list of attributes.

64

7. CONCLUSIONS
Data sparseness is a problem affecting the precision and coverage of open-domain information extraction tasks. Instance attribute extraction is not an exception. Data capturing the degree to which various instances may be related to one another is useful in re-ranking and expanding attributes extracted from text with standard techniques. The injection of instance relatedness data into a graph representing the initially extracted instance attributes, allows for relevant attributes to be propagated across related instances. The resulting ranked lists of attributes have higher accuracy levels than previous results. When both class labels and distributionally similar instances are available, the improvements using the two methods are comparable. Current work investigates the utility of distributional similarities among class labels, as signals during propagation.
8. REFERENCES
[1] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pa¸sca, and A. Soroa. A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches. In Proceedings of NAACL-2009, pages 19­27, 2009.
[2] K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liberman, A. McCallum, and M. Dredze. Lightly-Supervised Attribute Extraction. In NIPS Workshop on Machine Learning for Web Search, 2007.
[3] T. Brants. TnT - a statistical part of speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00), pages 224­231, Seattle, Washington, 2000.
[4] M. Cafarella, A. Halevy, D. Wang, and Y. Zhang. Webtables: Exploring the Power of Tables on the Eeb. Proceedings of the VLDB Endowment archive, 1(1):538­549, 2008.
[5] N. Chinchor. Overview of MUC-7/MET-2. In Proceedings of the Seventh Message Understanding Conference (MUC-7), volume 1, 1998.
[6] T. Chklovski and Y. Gil. An Analysis of Knowledge Collected from Volunteer Contributors. In Proceedings of the National Conference on Artificial Intelligence, page 564, 2005.
[7] G. Cui, Q. Lu, W. Li, and Y. Chen. Automatic Acquisition of Attributes for Ontology Construction. In Proceedings of the 22nd International Conference on Computer Processing of Oriental Languages. Language Technology for the Knowledge-based Economy, pages 248­259, 2009.
[8] O. Etzioni, M. Banko, S. Soderland, and S. Weld. Open Information Extraction from the Web. Communications of the ACM, 51(12), December 2008.
[9] N. Guarino. Concepts, Attributes and Arbitrary Relations. Data and Knowledge Engineering, 8:249­261, 1992.
[10] T. Hasegawa, S. Sekine, and R. Grishman. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 415­422, Barcelona, Spain, 2004.
[11] M. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th

International Conference on Computational Linguistics, pages 539­545, Nantes, France, 1992. [12] L. Lee. Measures of Distributional Similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pages 25­32, 1999. [13] D. Lin and P. Pantel. Concept Discovery from Text. In Proceedings of COLING, volume 2, pages 577­583, 2002.
[14] R. Mooney and R. Bunescu. Mining knowledge from text using information extraction. SIGKDD Explorations, 7(1):3­10, 2005.
[15] V. Nastase and M. Strube. Decoding wikipedia categories for knowledge acquisition. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1219­1224, Chicago, Illinois, 2008.
[16] M. Pa¸sca. Organizing and searching the World Wide Web of facts - step two: Harnessing the wisdom of the crowds. In Proceedings of the 16th World Wide Web Conference (WWW-07), pages 101­110, 2007.
[17] M. Pa¸sca and B. Van Durme. What you seek is what you get: Extraction of class attributes from query logs. In Proceedings of IJCAI-07, pages 2832­2837, 2007.
[18] M. Pa¸sca and B. Van Durme. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 19­27, Columbus, Ohio, 2008.
[19] K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu. Semi-Supervised Learning of Attribute-Value Pairs from Product Descriptions. IJCAI-07, 2007.
[20] J. Pustejovsky. The Generative Lexicon: a Theory of Computational Lexical Semantics, 1991.
[21] S. Raju, P. Pingali, and V. Varma. An Unsupervised Approach to Product Attribute Extraction. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, pages 796­800, 2009.
[22] F. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge unifying WordNet and Wikipedia. In Proceedings of WWW-2007, pages 697­706, 2007.
[23] K. Tokunaga, J. Kazama, and K. Torisawa. Automatic discovery of attribute words from Web documents. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05), pages 106­118, 2005.
[24] T. Wong and W. Lam. An Unsupervised Method for Joint Information Extraction and Feature Mining Across Different Web Sites. Data & Knowledge Engineering, 68(1):107­125, 2009.
[25] F. Wu, R. Hoffmann, and D. Weld. Information extraction from Wikipedia: Moving down the long tail. In Proceedings of the 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-08), pages 731­739, 2008.
[26] N. Yoshinaga and K. Torisawa. Open-Domain Attribute-Value Acquisition from Semi-Structured Texts. In Proceedings of the Workshop on Ontolex, pages 55­66, 2007.

65

Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs

Ryen W. White
Microsoft Research Redmond, WA 98052 USA
ryenw@microsoft.com

Jeff Huang
University of Washington Seattle, WA 98195 USA
sigir@jeffhuang.com

ABSTRACT
Search trails mined from browser or toolbar logs comprise queries and the post-query pages that users visit. Implicit endorsements from many trails can be useful for search result ranking, where the presence of a page on a trail increases its query relevance. Following a search trail requires user effort, yet little is known about the benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the destination page at the end of the trail. In this paper, we present a logbased study estimating the user value of trail following. We compare the relevance, topic coverage, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destinations (pages where trails end). Our findings demonstrate significant value to users in following trails, especially for certain query types. The findings have implications for the design of search systems, including trail recommendation systems that display trails on search result pages.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process, selection process
General Terms
Experimentation, Human Factors, Measurement
Keywords
Search trails, trail following, log analysis
1. INTRODUCTION
Web search engines afford keyword access to Web content. In response to search queries, these engines return lists of Web pages ranked based on their predicted relevance. For decades, the information retrieval (IR) research community has worked extensively on algorithmic techniques to effectively rank documents (c.f. [22]). However, research in areas such as information foraging [18], berrypicking [2], and orienteering [17], suggests that individual items may be insufficient for vague or complex information needs. In such circumstances, search results may only serve as the starting points for exploration [24].
Search trails are a series of Web pages starting with a search query and terminating with an event such as session inactivity [33]. Although the traversal of trails following a query is common, little is known about how much value users derive from following the trail versus sticking with the origin (the clicked search result) or jumping to the destination page at the end of the trail [32]. In this
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

paper we present a log-based study estimating the value to users of traversing multi-page search trails. Our primary aim is to estimate the benefit that trail following brings to users under different metrics versus viewing only the origin and/or destination pages. Significant differences in the performance of trails over origins and destinations would suggest that users benefit from the journey as well as the origin and the destination. Knowing if and when this is the case could help us build more effective search systems centered around trails, e.g., full trails could be shown to users directly on the results page. We estimate the value of trails, subtrails, origins, and destinations (collectively called trail sources) based on the relevance, completeness, diversity, novelty, and utility of the information they contain. We conduct this study using a log-based methodology since logs contain evidence of real user behaviors at scale and provide coverage of many types of information needs. Information need coverage is important since differences in source performance may not hold for all search tasks.
The remainder of this paper is structured as follows. Section 2 presents related work on trails. Section 3 describes the primary data source used in our study, as well as the extraction and labeling of search trails, and trail statistics. Section 4 describes the experiment performed to estimate the value of trails or sub-trails, including a comparison with trail origins and trail destinations. Section 5 describes the findings of our study for all queries and different query types. Findings are discussed along with their implications in Section 6. We conclude in Section 7.
2. RELATED WORK
Vannevar Bush first introduced the concept of trails when he envisioned the memex, a theoretical proto-hypertext system to extend human memory [4]. Bush foresaw "a new profession of trail blazers, those who find delight in the task of establishing useful trails through the enormous mass of the common record." Associative trails explicitly created by trailblazing users form links between stored materials that can help others navigate. Interaction logging via browsers and toolbars has made us all (implicit) trail blazers.
A search trail consists of an origin page, intermediate pages, and a destination page. Origin pages are the search results that start a trail, and may be referred to as landing pages in other literature. The use of query and origin pages from search engine click logs has been shown to be useful for improving search result relevance [1][14]. Teevan et al. [24] studied users jumping directly to destination pages and introduced the concept of teleportation when they observed users issuing sophisticated queries in an attempt to navigate to a page they knew existed deep in a Web site. White et al. [32] incorporated destination pages corresponding to Web search queries into search interface prototypes and presented them to user study participants. Most users found destination pages useful when shown on the search results page after the query was submitted. Bilenko and White [3] studied full trails, including the origin, intermediate, and destination pages. They found that treating the pages in these trails as endorsements improved ranking in

587

search engines. Individual pages in full trails have been shown to improve search results, destination pages have been shown to benefit users, and origin pages have been studied extensively in search relevance. We are the first to study the value of trails to users and directly compare trails to origins and destinations.
Trails have been studied in domains outside of IR. Wexelblat and Maes [29] introduced annotations in Web browsers called "footprints," which are trails through a Website assembled by the Website designer. Their evaluation found that users required significantly less steps to find information using their system. Freyne et al. [12] add a second dimension to footprints by displaying icons with links to offer visual cues to the user. These cues are gathered from past users and include popularity, recency, and usergenerated annotations. More recent work by Wang and Zhai [28] continues the footprint metaphor in a topic map. This topic map allows the user to navigate horizontally to related queries, and vertically to queries of different specificity. Simulated users with a predefined strategy benefited from such maps. Pirolli and Card [18] developed a sophisticated model of user behavior called information foraging derived from how animals forage for food in the wild. They use a foraging metaphor to discuss how information foragers could use cues left by previous visitors to find "patches" of information in a collection and consume patch information to satisfy information needs. Fu and Pirolli [13] developed and validated computational cognitive models of Web navigation behavior based on information foraging theory.
ScentTrails [16] combines browsing and searching into a single interface by highlighting potentially valuable hyperlinks. Olston and Chi perform user studies with different interfaces incorporating "scents" of trails in the search results. Users could find information faster and more successfully using ScentTrails than by either searching or browsing alone. O'Day and Jeffries [17] propose the orienteering analogy for understanding users' information-seeking strategies. Their qualitative study relates to ours in describing the benefits of building a system that considers the entirety of users' paths. Similarly, Bates's berrypicking [2] discusses users moving between information sources due to dynamic information needs. Search trails are extensions of these ideas into Web search, showing the routes with information to harvest, and orienting them towards the winding paths others have taken. As with orienteering and berrypicking, the origin and destination are important but the route taken in-between is also important; in this study we estimate how much benefit users gain from this journey.
Trigg [26] introduced the concept of guided tours, whereby authors could construct sequences of pages that may be useful to others. Reich et al. [19] discuss tours and trails as tools for helping hypertext users by showing where others have gone. Tours and trails in hypertext differ; trails are marked by users at each step while tours are typically authored beforehand and may have a hierarchical structure. Reich et al. also propose following users with similar interests as they move around the collection. Beyond hypertext, Chalmers et al. [6] present a system where people who are "recommenders" manually construct Web navigation paths. These recommenders share their paths with others. Wheeldon and Levene [30] propose an algorithm for generating trails to assist in Web navigation. Trails are presented in a tree interface attached to the browser. User study participants expressed satisfaction with the trails, noting that seeing the relationship between links helped, and found trails to be useful as a navigational aid.
The study described in this paper differs from previous work in that we are focused on estimating the value that trail following

brings to users, rather than describing existing trail traversal behavior, modeling user behavior, or using trails or computational models to recommend future actions. If findings show that users benefit from trail following, likely post-query trails could be considered in search system design and even as units of retrieval [23].
3. SEARCH TRAILS
In this section we describe the logs, trail mining from the logs, automatic classification of trail pages, and summary trail statistics.
3.1 Log Data
The primary source of data for this study was the anonymized logs of URLs visited by users who opted in to provide data through a widely-distributed browser toolbar. These log entries include a unique identifier for the user, a timestamp for each page view, a unique browser window identifier (to resolve ambiguities in determining which browser a page was viewed), and the URL of the Web page visited. Intranet and secure (https) URL visits were excluded at the source to maintain user privacy. In order to remove variability caused by geographic and linguistic variation in search behavior, we only include entries generated in the English speaking United States locale. The results described in this paper are based on a sample of URL visits during a three-month period from March 2009 through May 2009, representing millions of URL visits from 100,000 unique users. The user sample was selected at random from a larger set of twelve million users after we had pre-filtered the data to remove several thousand extremely-active outlier users, all of whom issued over one thousand queries per day on average across the three-month period. These highvolume users were likely automated traffic. For each user, we required an adequate number of Web page visits to create their long-term search history that was used to evaluate source novelty (described in more detail later). Therefore, in addition to removing outliers, we also only selected users who issued at least 30 queries per month from March 2009 to May 2009 inclusive.
3.2 Trail Mining
We mined tens of millions of search trails from the May 2009 logs, referred to hereafter as . As defined by White and Drucker [33], search trails consist of a temporally-ordered sequence of URLs beginning with a search engine query and terminating with either: (i) another query, (ii) a period of user inactivity of 30 or more minutes, or (iii) the termination of the browser instance or tab. The 30-minute inactivity timeout is commonly used to demarcate sessions in Web log analyses (e.g., [9]). We chose to use search trails rather than session trails (which comprise multiple queries) to lessen the likelihood of query skew, where user intent shifts over the course of the session, making it challenging to associate visited pages to the original query. Figure 1 illustrates a search trail, expressed as a Web behavior graph [5]. The trail starts with a search engine query ( 1) (which also includes the searchengine result page (SERP)) and comprises a set of pages visited until the trail terminates with a new query or an inactivity timeout. The nodes of the graph represent Web pages that the user has visited: rectangles represent page views and rounded rectangles represent search engine result pages. Vertical lines represent backtracking to an earlier state (e.g., returning to a page of results in a search engine after following an unproductive link). A "back" arrow, such as that below 4, indicates that the user has requested to visit a page seen earlier in the search trail. Time runs left to right and then from top to bottom. In addition to the complete trail, also marked on Figure 1 are the origin (the search result,

588

2), the destination (the trail's terminal page, 5), and the pages between origin and destination (in this case 3, 4, 3, 2 ).

Origin page

Q1

P2

P3

P4

P3

P2

P5

SERP page
Non-SERP page Revisit Nav (fwd)

Destination page

Nav (back)

Figure 1. Web behavior graph illustrating a search trail.

3.3 Trail Labeling
Three of the five evaluation metrics used in our study--coverage, diversity, and novelty--use information about page topicality. Millions of unique URLs were present in the set of all trails mined from the toolbar logs. This made the evaluation of coverage, novelty, and diversity challenging as it was impractical to download all pages and comparisons based on URLs would be severely limited. To address this challenge, we classified the Web pages sourced from each context into the topical hierarchy from a popular Web directory, the Open Directory Project (ODP) (dmoz.org). Given the large number of pages involved, we used automatic classification. Our classifier assigned labels to pages based on the ODP using a similar approach to Shen et al. [21]. Classification began with URLs present in the ODP and incrementally pruned non-present URLs one path level at a time until a match was found or miss declared. Similar to [21], we excluded the "Regional" and "World" top-level ODP categories since they are typically uninformative for building interest models.
3.4 Trail Statistics
There were 15 million search trails followed by the 100,000 toolbar users in our sample during May 2009. The median (Med) number of trails followed per user was 91 (mean (M) was 160, standard deviation (SD) was 228). The median number of steps in the trails was two (M=5.3, SD=12.2), (i.e., the search engine result page and a single result click), but around one third of the trails were abandoned following the query, and around one third of trails contained three or more pages. The median time spent on trails was 81 seconds (M=308s, SD=615s), and around 20% of trails contained backtracking to a site already visited in the trail.

Interestingly, around 19.3% of trails with three steps or more (i.e., had pages between the origin and destination) had at least one site with a different ODP label to the origin and destination pages. Analysis of the queries on the remaining 80.7% of trails revealed that their original queries were generally navigational (e.g., [delta airlines]) or directed informational (e.g., [what is daylight savings time?]). For other types of informational query, such as undirected, advice, locate or list [20], intermediate pages may be valuable to users. The extent of this value is estimated in our study.

4. STUDY
We devised an experiment to determine the value of search trails compared to search results and destinations. In this section we outline the research questions that drove our study, describe the experimental variants, summarize the trail data preparation, and present the metrics used to compare sources.

4.1 Research Questions
Our study answers a number of research questions. Specifically, of the four sources (origin, destination, sub-trail, and full-trail), which: (i) provide more relevant information? ( 1); (ii) provide more topic coverage? ( 2); (iii) provide more topic diversity? ( 3); (iv) provide more novel information? ( 4), and; (v) provide more useful information? ( 5). Answers to these questions help us understand the value of trail (or sub-trail) traversal compared to viewing only the origin and/or destination pages.
4.2 Trail Sources
To determine the value of trail traversal we experiment with a number of trail sources. They are as follows:
Origin: The first page in the trail after the SERP, visited by clicking on a search result hyperlink. This is regarded as a baseline in this study since current search engines show this source alone in search results. 2 is the origin in Figure 1.
Destination: The last page in the trail, visited prior to trail termination through a follow-up query or inactivity timeout. Destinations are defined similarly to the popular destinations from White et al. [32]. We include them here for comparison with that earlier work. 5 is the destination in Figure 1.
Sub-trail: All pages in the trail except for destination, including all post-SERP pages. 2, 3, 4, 3, 2 is the sub-trail in Figure 1.
Full-trail: The complete trail, including all post-SERP pages.
We mine these sources from each trail in and compute the value of each source in terms of relevance, coverage, diversity, novelty, and utility across all queries and divided by query type. We elected not to study intermediate pages directly (i.e., pages in the trail that lie between the origin and destination) since a trail must contain an origin page in our current definition. The value of the intermediate pages over the origin can be estimated by comparing the performance differences between origins and sub-trails.
4.3 Trail Data Preparation
To help ensure experimental integrity, we did not use all search trails in . Instead, we filtered based on the following criteria:
 Queries originating the trails were normalized to facilitate comparability between trails, and between the trails and other resources (as described in the next section). Normalization involved the removal of punctuation, lowercasing, trimming extraneous whitespace, and ordering terms alphabetically.
 Trails were required to contain at least three pages: an origin page, a destination page, and at least one intermediate page. It was important to have these sources in all trails used since we wanted to compare their value.
 To ensure that origin pages were reached through a SERP click, we required that the first non-SERP page in the trail be connected to the SERP with a hyperlink click (i.e., the referrer of the origin page must be a SERP). Trail pages thereafter were not required to be joined via a hyperlink click.
 The coverage of our ODP classifier with URL back-off was approximately 65%. A missing label may have skewed the distribution of labels for or against a particular source. We therefore required that all selected trails be fully labeled.
 To prevent sample bias from highly-active users, we selected at most 10 search trails that met the above criteria from each user.
The application of these criteria reduced to one quarter of its original size, but yielded a high-quality data set for our study.

589

4.4 Metrics
We used five metrics to compare the different trail sources: relevance, coverage, diversity, novelty and utility. These metrics were chosen to capture many important elements of information seeking, as highlighted by the wealth of relevant research in the IR community (e.g., [7][8]). The use of multiple metrics allowed us to compare the value of the different sources in different ways. For example, a trail destination page may be less relevant than sub-trail, but may provide additional information not in the subtrail. We now describe each metric and its implementation.

4.4.1 Relevance
The first metric used to compare the sources was relevance to the query that initiated the trail. In addition to the trail data used during the course of this study, we also obtained human relevance judgments for over twenty thousand queries that were randomly sampled by frequency from the query logs of the Bing search engine; they were normalized per the description in Section 4.3,
and were present in . Trained judges assigned relevance labels on a six-point scale--Bad, Poor, Fair, Good, Excellent and Perfect--to top-ranked pooled Web search results for each query from the Google, Yahoo!, and Bing search engines as part of a separate search engine assessment activity. This provided hundreds of relevance judgments for each query. These judgments allowed us to estimate the relevance of information encountered at
different parts of the trails. For each trail in , we computed the average relevance judgment score for each source. Each page in the trail was used at most once in relevance score calculations, even if it appeared multiple times in the trail. This discounted revisitation, since diminishing returns from each repeat visit to a page in the same trail were likely. In this analysis we only used trails for which we had a relevance judgment for the origin page, the destination page, and at least one intermediate page. Trails for
8,712 queries, comprising a query set and initiating around two million trails, afforded a detailed comparison of source relevance.

4.4.2 Coverage
Another aspect that we studied was topic coverage, meant to reflect the value of each trail source in providing access to the central themes of the query topic. To estimate the coverage of each trail source, we first constructed a set of query interest models
representing the dominant intents associated with each query in . These models served as the ground truth for our estimates of coverage (in this subsection) and diversity (in the next subsection). Each constructed query interest model is assumed to contain most of the significant themes for the query. A query's interest model comprises the ODP category labels assigned to the URLs in the union of the top-200 search results for that query from Google, Yahoo! and Bing. ODP category labels are grouped and their frequency values are normalized such that across all labels they sum to one. For example, the highest-weighted labels in the query interest model for [solar system discoveries], and their associated
normalized frequencies ( ), are shown in Figure 2.

Label

/

/

/

/

0.64

/

/

/

/

__

0.18

/

/

0.16

Figure 2. Top ODP categories for [solar system discoveries].

To improve the reliability of our coverage estimates, we selected a set of query interest models, , that were required to be based on

at least 100 fully-labeled search results (i.e., were not missing a label and did not have a label from an ignored ODP category) and were based only on labels with a frequency count of at least five
(to reduce label noise). was modified to include only trails
originating from queries with interest models in . For each trail in , we created a source interest model comprising ODP cate-
gory labels and associated frequencies for origin, destination, subtrail, or full-trail. We then compute the coverage of each source in (denoted ) using:

(1)


Where l is ODP category label and represents the normalized frequency weight of that label in the corresponding interest model for the current query, denoted as .

4.4.3 Diversity
Another aspect studied was topic diversity, which estimates the fraction of unique query-relevant concepts surfaced by a given trail source. Exposure to different perspectives and ideas may help users with complex or exploratory search tasks. Indeed, existing search engines already consider diversity in the search results they present to satisfy more users with the first few results.

To estimate the diversity of information provided by each trail source we use an approach similar to our coverage estimation. We generate trail interest models for each trail source and compare those with the relevant query interest model to estimate diversity. The main difference between how the estimates of coverage and diversity lies in whether normalized label frequency is considered. When estimating coverage we want to establish the fraction of
appearing in (i.e., label frequency is used). In contrast, when we estimate diversity, we only count the number of unique category labels from that appear in (i.e., frequency is ignored).

For each trail in originating with one of the queries in , we created a source interest model comprising ODP category labels and associated frequencies for origin, destination, sub-trail, and
full-trail. We computed diversity for each using:

Where l is ODP label and |

1 ||

| is the number of unique

(2) labels.

4.4.4 Novelty
Another aspect that we studied was the amount of new queryrelevant information from each trail source. Novel information may help users learn about a new subject area or broaden their understanding of an area with which they are already familiar.

Trails with novelty contain information that users have not encountered for a query. Unlike coverage and diversity, the novelty provided by a trail source may depend on both the query and the user. For example, what is new topic-related information for one individual may not be new information for another. Therefore, to estimate the novelty of the information provided by each trail source, we first had to construct a model of each user's general interest in the query topic based on historic data. To do this, we leveraged users' search trails for the two-month period from March to April 2009 inclusive (referred to hereafter as ), and
constructed historic interest models , for all user-query pairs.
Each interest model , whose query was present in , comprised a distribution of ODP category labels (and associated nor-

590

malized frequencies) similar to those used in earlier coverage and diversity estimates. Only labels appearing in the query interest
model are included in . The historic interest model is there-
fore a subset of focused on a given user's history with that query. White et al. [31] used a similar approach to depict longterm user interests. We estimate the novelty of each trail source relative to the historic interest model for the user and the query.

For each trail in , we built source interest models to estimate the source novelty based on whether it contained topic-related information not in . The novelty of each is estimated using:

1

||

(3)





Where l represents an ODP category label present in and but not in , and | | represents the number of unique labels.
4.4.5 Utility
The final aspect that we studied was the utility of each of the trail sources, estimated for the purposes of this study using page dwell time (i.e., the amount of time spent on a particular page by a user). Dwelling on a page for a significant amount of time implies that a user may be deriving utility from it. Indeed, prior research has shown that during search activity, a dwell time of 30 seconds or more on a Web page can be indicative of page utility [11]. We apply this threshold in our analysis and across all trails in , we estimate the fraction of page views from the origin, destination, sub-trail, and full-trail that exceed this dwell time threshold.
In all metrics used in this study, a higher value is regarded as a more positive outcome. The metrics are computed for each trail, then micro-averaged within each query, and then macro-averaged across all queries to obtain a single value for each source-metric pair. This procedure ensures that all queries are treated equally in the analysis and popular queries are not allowed to dominate the aggregated metric values for each source. Although we might expect sub-trails and full-trails to have higher metric scores than origins or destinations (simply because they have more pages), it is the extent that the metrics' values increase from these sources that lets us estimate the additional value of trails and sub-trails. This is reasonable since we plan to show full-trails and sub-trails directly to users on the search engine result page.
4.5 Methodology
In this section so far we have described the research questions, the four trail sources evaluated, trail data preparation procedures, and the metrics used to evaluate the sources. The methodology employed during our experiments comprised the following steps:
1. Construct the set of query interest models based on the set of queries for which we have human relevance judgments ( ).
2. Construct historic interest models ( ) for each user-query pair in , filtered to only include queries appearing in .
The data sets created during the first two steps are used to evaluate each of the four trail sources.
3. For each search trail in :
a. Assign ODP labels to pages all pages in .
b. Build source interest models for the origin, destination, subtrail and full-trail sources.
c. Compute relevance, coverage, diversity, novelty and utility using the methods described in Section 4.4.

4. Compute the average values for each metric per query, and then average across all queries (to treat all queries equally), breaking out the findings by query type as appropriate.
In the next section we report on the findings from our study.

5. FINDINGS
We first present findings over all queries; then divided by query
type, varying query popularity and query re-finding behavior, both
of which have been shown to influence search interaction in pre-
vious work [8][10]. Since our data were shown to be normally distributed, we use parametric statistical testing, with .05.

5.1 All Queries
We computed the five metrics across all trails in port on source performance.

and now re-

Relevance: We begin our analysis by reporting on the relevance of the information encountered at the origin, destination, sub-trail and full-trail, determined using human relevance judgments. As noted in the previous section, the judgments were captured for query-URL pairs on a six-point scale, ranging from 0 (Bad) to 5 (Perfect). Sources that provide more relevant information would be expected to have a higher average relevance score. In the "All" column of Table 1 (shaded) we report on the mean average relevance score obtained from each of the sources across all trails in
. Also reported are the percentage differences between the relevance score obtained for each of the non-origin sources and
origins () to estimate the additional value obtained from full or partial trail traversal, or from teleporting directly to destinations. We do not show standard deviations to avoid crowding findings.

The findings show that the relevance scores for all sources were

generally positive (around three or Good). An independent-

measures analysis of variance (ANOVA) computed between the

relevance scores obtained from all four sources revealed no signif-

icant differences in the relevance of the origin page versus infor-

mation encountered on the trail ( (3,8708) = 1.5,

0.21).

However, as is apparent in the table, trends in the findings suggest

that the relevance scores for non-origin sources were slightly low-

er than those of the origin pages (e.g., 3.3 versus 2.9-3.0). This

may be related to a combination of the distance between non-

origin sources and the original queries, and the effect of dynam-

ism in information needs as users traverse search trails [33]. Since

non-origin sources are further from the query than origin pages,

they may be less query relevant as user needs evolve.

Coverage: We also studied the extent that each trail source covered the query interest models representing the dominant themes for each query. The coverage estimate of each source for each trail was computed using Equation 1. The average coverage scores for each metric are reported in the "All" column of Table 1, along with the percentage difference between each of the sources and origin. The findings show that on average, around 40% of the total mass of the query interest models can be covered by origins and destinations, and around 50% are covered by sub-trails and fulltrails (coverage gains of 20-30% from traversing trails). Analysis of the findings using a one-way independent measures ANOVA revealed statistically significant differences between the sources
( (3,8708) = 5.5, .001). Post-hoc testing, performed using Tukey tests, revealed that on average across all queries, sub-trails
and full-trails covered more of the query interest models in
than the origins or destinations alone (all < 0.01).

591

Table 1. Metric scores across all queries and broken down by query popularity and query history. Statistically-significant differences between non-origin trail sources and the origin within each metric are shown in bold ( .05) and bold-italic ( .01).

Relevance

Coverage

Source
Origin Destination Sub-trail Full trail Origin Destination Sub-trail Full trail Origin Destination Sub-trail Full trail Origin Destination Sub-trail Full trail Origin Destination Sub-trail Full trail

All

N=8,712

M
3.3 2.9 3.0 3.0 0.377 0.372 0.455 0.489 0.291 0.307 0.384 0.412 0.034 0.045 0.127 0.159 0.473 0.498 0.624 0.653


12 9 9
1 +21 +30
+5 +32 +42
+32 +273 +367
+5 +32 +38

Query breakdown

Query Popularity (per query)

Query History (per user-query pair)

Low

Medium

High

None

Some

Lots

N=211

N=6,421

N=2,080

N=1,022,874 N=1,081,895 N=18,220

M  M  M  M  M  M 

2.9

3.2

3.4

3.1

3.3

3.4

2.6 10 2.9

9

3.1

9

2.7 13 3.0

9

3.0 12

2.8

3

3.0

6

3.1

9

2.8 10 2.9 12 3.0 12

2.8

3

3.1

3

3.2

5

2.8 10 3.0

9

3.1

9

0.355

0.374

0.389

0.382

0.374

0.373

0.349 2 0.369 +1 0.385 1 0.385 +1 0.371 1 0.367 2

0.454 +28 0.455 +22 0.456 +17 0.472 +24 0.439 +17 0.410 +10

0.485 +37 0.488 +30 0.492 +26 0.502 +31 0.476 +27 0.457 +23

0.287

0.290

0.293

0.293

0.290

0.290

0.296 +3 0.307 +6 0.308 +5 0.311 +6 0.305 +5 0.304 +5

0.369 +29 0.384 +32 0.385 +31 0.398 +36 0.370 +28 0.339 +17

0.407 +42 0.413 +42 0.413 +41 0.433 +48 0.394 +36 0.361 +24

0.031

0.034

0.036

n/a n/a 0.034

0.010

0.043 +39 0.044 +29 0.046 +28 n/a n/a 0.046 +35 0.012 +20

0.125 +303 0.126 +271 0.129 +258 n/a

n/a 0.129 +279 0.040 +300

0.156 +403 0.159 +368 0.162 +350 n/a

n/a 0.161 +374 0.066 +560

0.473

0.473

0.473

0.440

0.468

0.492

0.493 +4 0.497 +5 0.502 +6 0.461 +4 0.489 +4 0.523 +6

0.617 +30 0.623 +32 0.629 +33 0.599 +36 0.645 +38 0.664 +35

0.649 +37 0.653 +38 0.656 +39 0.626 +42 0.676 +44 0.689 +40

Diversity

Novelty

Utility

Diversity: To estimate the extent that each trail source covers different aspects of the query interest model, we calculated their diversity using Equation 2. Increased diversity may be useful to users engaged in search tasks with multiple sub-tasks, such as planning a vacation. The average coverage scores for each source across all trails in are reported in the "All" column of Table 1. The findings show that approximately one-third of the central themes for a query can be captured by each trail source, with more topic diversity coming from the trail-based sources (diversity gains of 30-40% from traversing trails). Statistical analysis of the findings reveals significant differences between the levels of topic diversity provided by each source ( (3,8708) = 7.0, 0.001). Post-hoc testing revealed that sub-trails and full-trails provide more diversity than origins (all < 0.01). The increase in diversity for destinations over origins was not statistically significant.
Novelty: Novelty calculations estimate the amount of new queryrelevant information provided to users by each of the trail sources. Unlike the other metrics in this study, novelty is specific to both user and query; one user's experience with a query may differ from another's. As described previously, novelty is computed based on the number of new query-relevant ODP category labels

added to a user's query interest models compared with historic data. In Table 1 ("All" column) we report on the average novelty score and the percentage differences between all non-origin sources and the origin only. The findings show modest increases in the amount of new information obtained from all sources, but seemingly larger gains from the non-origin trail sources (0.130.16 versus 0.03). Statistical analysis of our findings revealed differences among the sources ( (3,8708) = 3.0, .01). Posthoc testing revealed significant differences between sub-trails / full-trails and origins / destinations (all < 0.01). On average, trails provide more novel information than origins or destinations. In turn, destinations provide slightly more novel information than origins, but differences were not significant ( 0.12).
Utility: We also studied the utility of each trail source. To estimate utility for a given Web page from the logs, we used a 30second page dwell time threshold selected based on previous work [11]. For each of the sources across all trails in , we computed the fraction of trails for which each source contained a useful page (i.e., a page with a dwell time equaled or exceeded the 30-second threshold). These values are shown in the "All" column of Table 1. Also shown are the percentage differences between non-origin

592

sources and origins. The findings show that just under half of

origins and destinations are useful, around 60% of sub-trails have

useful pages, and almost two-thirds of full-trails contain useful

pages. Statistical analysis of the findings revealed significant dif-

ferences between the sources in terms of their estimated utility

( (3,8708) = 3.3,

.01). Post-hoc testing revealed that all

sources differed from trail origins (destinations: = .03; sub-

trails: .01; full-trails: .01). It seems that users find non-

origin pages more useful than origin pages. This may be because

origin pages are search results and may only be the starting points

for a search task or sub-task [24].

One important factor that may cause variation in the effectiveness of search trails is the nature of the search query. Downey et al. [10] showed that user behavior following a query varied significantly with query popularity. Teevan et al. showed that the frequency with which a query is reissued by a given user over a period of time (so-called "re-finding" behavior) affects that user's search interactions for that particular query [25]. To test whether such factors influenced the source value we varied query popularity and history as part of our experimental design. In the remainder of this section we report on the findings of this analysis.

5.2 Effect of Query Popularity
To study the effect of query popularity on source value, we created a tripartite division of queries in , grouping them into low, medium, and high, based on user frequency in . Low popularity queries were issued by at most one user in , medium popularity queries were issued by between 1 and 100 users in , and high popularity queries were issued by over 100 users in .

Table 1 presents findings on the effect of query popularity on source performance for each of the five metrics we study. On all metrics, we observe a trend that as query popularity increases, each of the metric values also increases. The relative ordering and percentage gains from the trail sources remain consistent across all five metrics. However, within each metric, differences in the values obtained for the three query popularity groupings are not significant using a two-way independent measures ANOVA with source and query popularity group as the factors (source (rows):
all .02; popularity (columns): all .13). -statistics for all performed ANOVA are not reported to avoid crowding the paper. Small increases in coverage as query popularity increases may be attributable to the dominance of the intent associated with the query. More popular queries are more likely to have a single dominant intent, giving the category label for that intent a high
weight ( from Equation 1). Since coverage derives from , we are likely to observe increases in coverage as a dominant intent
with a high . Improvements in search engine performance as query frequency increases (already noted in [10]) may account for some of the slight increases in relevance and utility with popularity (Table 1).

5.3 Effect of Query History
We also studied the effect of query history on the value of each of the four trail sources. We divided queries into three groups--none, some, and lots--based on the number of times they were issued by a particular user in . Queries in none appeared in but did not appear in , queries in some appeared in and were issued by a particular user 30 times or less in (i.e., on average less than once every two days), and queries in lots appeared in and were those issued by a particular user more than 30 times in (i.e., on average more than once per two days).

Table 1 presents findings on the effect of query history on source

performance for each of the five metrics. From the findings, it

seems that as query history increases, there is a mixed effect on

the five metrics. However, within each metric all differences be-

tween sources and between query history groupings are signifi-

cant, as shown by a two-way independent measures ANOVA with

source and query history grouping as the factors (source (rows):

all

.001; history (columns): all

.001). We found that

relevance and utility rise across all sources given increased re-

finding behavior. This is perhaps because users are more familiar

with the query topic and are more able to identify relevant infor-

mation. Similar findings have been reported in previous work on

topic familiarity (e.g., [15]). In contrast, coverage, diversity, and

novelty decrease, perhaps as a result of a reduced variance in the

pages visited. Such consistency in interaction behavior for queries

with high re-finding rates has been reported previously [27].

6. DISCUSSION AND IMPLICATIONS
We have demonstrated that following search trails provides users with significant additional benefit in terms of coverage, diversity, novelty, and utility over origins and destinations. Although more work is required to supplement the methodology used in our study and further understand the impact of experimental decisions such as only studying search trails that could be fully-labeled using ODP lookup, our log analysis helps establish the value of trails to users and inform search system design.

We showed that full-trails and sub-trails provided significantly more coverage, diversity, novelty, and utility, versus trail origins and destinations. The one metric for which we did not obtain significant differences between origin and non-origin sources was relevance. Trends in the findings suggest that trails were less relevant than origins. This may be related to the definition of relevance in this study. Our relevance judgments are assigned to pairs of queries and search results. However, during the session, user intent may shift and the relevance to the initial query is dynamic [2]. Pages encountered on the trails may be relevant but not appear so due to these shifts. More work is required on how relevance changes during browsing and to understand the relevance benefit from trails. Enhancements include studying the cumulative relevance of trail information, considering relevance changes, and devising proxies for relevance similar to that used for utility.

Destinations were more useful and led to a slight novelty increase over origins. This confirms some of the findings of White et al. [32], who showed in a user study that destinations were a useful addition to the results interface. In retrospect, this agrees with expectations that users will give more attention, and hence dwell on the destination page. In information foraging theory [18] where this corresponds to a food patch, users satisfy some or all of their need and do not pursue the information scent further.

While destinations were useful in one metric, adding intermediate pages contributed to gains in several metrics, notably in novelty, diversity, and utility, where the differences between origins and sub-trails are substantial. The success of sub-trails suggests that users may not need to traverse full-trails to derive significant value from post-query navigation. As expected, full-trails provide even more benefit than sub-trails; full-trails are sub-trails plus destinations. Although the findings of our study appear to support trail recommendation, they also suggest that the nature of the query is important. For some queries, the trails might be useful in supporting exploration, but for other queries, especially for focused tasks, presenting trail information might be a hindrance.

593

Questions remain about how to select trails and how to integrate trails into the SERP. Popular search trails are typically short and obvious, so we need to consider diverse and unexpected trails, perhaps leveraging popular sub-trails as well as full trails in trail selection algorithms. Trail selection methods could discount trails with numerous cases of rapid backtracking or maximize relevance, coverage, diversity, novelty, and utility with the shortest path. Alternatively, we can personalize trail recommendation by weighting trails based on the extent of the current user's refinding behavior or perform a priori trail analysis to recommend trails when the destination is unclear (i.e., users end up on many pages), and present trail destinations when the destination is clear (i.e., many users end up at the same page). Trails can be presented as an alternative to result lists, as instant answers above result lists, in pop-ups shown after hovering over a result, below each result along with the snippet and URL, or even on the click trail a user is following. Follow-up user studies and large-scale flights will further analyze trail appropriateness for different queries and compare trail selection algorithms and trail presentation methods.
7. CONCLUSIONS
In this paper we have presented a study estimating the value of search trails to users. Our log-based methodology has allowed us to systematically compare the estimated value of trails to other trail components: trail origins (clicked search results), the trail destinations (terminal trail pages), and sub-trails comprising the origin plus intermediate pages. We studied the relevance, coverage, diversity, novelty, and utility of each of the four sources using metrics devised for this purpose, human relevance judgments, historic log data, and URL classification where appropriate. When we varied the query by overall popularity, the values of each metric increased with query frequency. The evaluation showed that full-trails and sub-trails provide users with significantly more topic coverage, topic diversity, and novelty than trail origins, and slightly more useful but slightly less relevant information than the origins. Our findings show that there is value in the trail (the scenic route), as well as the origin and the destination. These findings vary slightly by query popularity over all users and significantly by the level of re-finding performed by a user for a given query. The next steps are to investigate best-trail selection for queryorigin pairs and add trails to search engine result pages.
REFERENCES
[1] Agichtein, E., Brill, E. & Dumais, S. (2006). Improving web search ranking by incorporating user behavior information. Proc. SIGIR, 19-26.
[2] Bates, M.J. (1989). The design of browsing and berrypicking techniques for the online search interface. Online Review, 13(5): 407-424.
[3] Bilenko, M. & White, R.W. (2008). Mining the search trails of surfing crowds: identifying relevant websites from user activity. Proc. WWW, 51-60.
[4] Bush, V. (1945) As we may think. Atlantic Monthly, 3(2): 37-46.
[5] Card, S.K. et al. (2001). Information scent as a driver of web behavior graphs: results of a protocol analysis method for web usability. Proc. SIGCHI, 498-505.
[6] Chalmers, M., Rodden, K. & Brodbeck, D. (1998). The order of things: activity-centered information access. Proc. WWW, 359-367.
[7] Clarke, C.L.A. et al. (2008). Novelty and diversity in information retrieval evaluation. Proc. SIGIR, 659-666.

[8] Cole, M. et al. (2009) Usefulness as the criterion for evaluation of interactive information retrieval. Proc. HCIR, 1-4.
[9] Downey, D., Dumais, S. & Horvitz, E. (2007). Models of searching and browsing: languages, studies, and application. Proc. IJCAI, 2740-2747.
[10] Downey, D. et al. (2008). Understanding the relationship between searchers' queries and information goals. Proc. CIKM, 449-458.
[11] Fox, S. et al. (2005). Evaluating implicit measures to improve the search experience. ACM TOIS, 23(2): 147-168.
[12] Freyne, J. et al. (2007). Collecting community wisdom: integrating social search and social navigation. Proc. IUI, 52-61.
[13] Fu, W.-T. & Pirolli, P. (2007). SNIF-ACT: A cognitive model of user navigation on the world wide web. Human-Computer Interaction, 22(4): 355-412.
[14] Joachims, T. (2002). Optimizing search engines using clickthrough data. Proc. SIGKDD, 133-142.
[15] Kelly, D. & Cool, C. (2002). The effects of topic familiarity on information search behavior. Proc. JCDL, 74-75.
[16] Olston, C. & Chi, E.H. (2003). ScentTrails: integrating browsing and searching on the web. ACM TOCHI, 10(3).
[17] O'Day, V. & Jeffries, R. (1993). Orienteering in an information landscape: how information seekers get from here to there. Proc. INTERCHI, 438-445.
[18] Pirolli, P. & Card, S.K. (1999). Information foraging. Psychological Review, 106(4): 643-675.
[19] Reich, S. et al. (1999). Where have you been from here? Trails in hypertext systems. ACM Computing Surveys, 31(4).
[20] Rose, D.E. & Levinson, D. (2004). Understanding user goals in web search. Proc. WWW, 13-19.
[21] Shen, X., Dumais, S. & Horvitz, E. (2005). Analysis of topic dynamics in web search. Proc. WWW, 1102-1103.
[22] Singhal, A. (2001). Modern information retrieval: a brief overview. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 24(4): 35-43.
[23] Singla, A., White, R.W. & Huang, J. (2010). Studying trailfinding algorithms for enhanced web search. Proc. SIGIR.
[24] Teevan, J. et al. (2004). The perfect search engine is not enough: a study of orienteering behavior in directed search. Proc. SIGCHI, 415-422.
[25] Teevan, J. et al. (2007) Information re-retrieval: repeat queries in yahoo's logs. Proc. SIGIR, 151-158.
[26] Trigg, R.H. (1988). Guided tours and tabletops: tools for communicating in a hypertext environment. ACM TOIS, 6(4).
[27] Tyler, S.K. & Teevan, J. (2010). Large scale query log analysis of re-finding. Proc. WSDM, 191-200.
[28] Wang, X. & Zhai, C. (2009). Beyond hyperlinks: organizing information footprints in search logs to support effective browsing. Proc. CIKM, 1237-1246.
[29] Wexelblat, A. & Maes, P. (1999). Footprints: history-rich tools for information foraging. Proc. SIGCHI, 270-277.
[30] Wheeldon, R. & Levene, M. (2003). The best trail algorithm for assisted navigation of web sites. Proc. LA-WEB, 166.
[31] White, R.W., Bailey, P. & Chen, L. (2009). Predicting user interests from contextual information. Proc. SIGIR, 363-370.
[32] White, R.W., Bilenko, M. & Cucerzan, S. (2007). Studying the use of popular destinations to enhance web search interaction. Proc. SIGIR, 159-166.
[33] White, R.W. & Drucker, S.M. (2007). Investigating behavioral variability in web search. Proc. WWW, 21-30.

594

PRES: A Score Metric for Evaluating Recall-Oriented

Information Retrieval Applications

Walid Magdy
Centre for Next Generation Localization School of Computing Dublin City University Dublin 9, Ireland

Gareth J.F. Jones
Centre for Next Generation Localization School of Computing Dublin City University Dublin 9, Ireland

wmagdy@computing.dcu.ie

gjones@computing.dcu.ie

ABSTRACT
Information retrieval (IR) evaluation scores are generally designed to measure the effectiveness with which relevant documents are identified and retrieved. Many scores have been proposed for this purpose over the years. These have primarily focused on aspects of precision and recall, and while these are often discussed with equal importance, in practice most attention has been given to precision focused metrics. Even for recalloriented IR tasks of growing importance, such as patent retrieval, these precision based scores remain the primary evaluation measures. Our study examines different evaluation measures for a recall-oriented patent retrieval task and demonstrates the limitations of the current scores in comparing different IR systems for this task. We introduce PRES, a novel evaluation metric for this type of application taking account of recall and the user's search effort. The behaviour of PRES is demonstrated on 48 runs from the CLEF-IP 2009 patent retrieval track. A full analysis of the performance of PRES shows its suitability for measuring the retrieval effectiveness of systems from a recall focused perspective taking into account the user's expected search effort.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; H.3.4 Systems and software ­ performance evaluation.
General Terms
Measurement, Performance, Experimentation.
Keywords
PRES; Recall-Oriented Information Retrieval; Patent Retrieval; Evaluation Metric
1. INTRODUCTION
The objective of an information retrieval (IR) system is to retrieve relevant documents to satisfy user information needs. The evaluation of IR systems should thus test their ability to achieve this objective. Evaluation of IR systems has been the focus of
much research in recent years [18, 29]. A number of evaluation
methods and metrics have been proposed and explored for the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

wide range of IR tasks now under investigation, e.g. web search, question answering and structured document retrieval.
Laboratory IR tests generally adopt the Cranfield evaluation framework paradigm [11]. Metrics used in these experiments generally measure how early relevant documents are retrieved with less focus on the system recall. While this situation is reasonable for precision-oriented applications, where a small number of relevant documents are sufficient to satisfy the user information need, they are less informative of system behaviour for recall-oriented tasks, where all relevant documents are required to be retrieved. However, while metrics such as, mean average precision (MAP) are not sufficient, they have been used as the central evaluation measures in applications such as patent retrieval [13, 25]. Viewing recall-oriented tasks purely in terms of measuring recall is actually rather simplistic. In practice the user's effort expended in the search is often also a key consideration. Thus it can be important for an evaluation metric to take account not only of the recall, but also of the user's effort as reflected in the ranks at which relevant items are retrieved.
This paper describes a study analyzing the behaviour of current evaluation metrics when applied to recall-oriented IR tasks. The results of this analysis are used to motivate the proposal of a novel evaluation metric which combines recall with the quality of ranking of the retrieved relevant results. This allows us to distinguish between systems of similar recall giving higher scores to systems with better ranking of relevant documents. A study performed on the CLEF-IP 2009 patent retrieval task [25] shows the advantage of the new score over existing recall and precision metrics. The new score showed a 0.87 correlation to recall and 0.66 correlation to precision, which demonstrates how it reflects both recall and precision with more emphasis on recall. Additional analysis shows that the new score also works well for other recalloriented IR applications such as legal search when the number of relevant documents is typically very large.
The remainder of the paper is organized as follows; Section 2 surveys background on IR evaluation scores; Section 3 explores the effectiveness of the current IR evaluation scores for measuring system performance for recall-oriented IR applications; Section 4 explains normalized recall, which is one of the classic IR evaluation scores used later to develop our new PRES evaluation metric, Section 5 formally introduces PRES; Section 6 explores the behaviour of PRES by use of illustrative examples and by testing it on the 48 CLEF-IP 2009 runs, in addition, it reports the behaviour of PRES for other tasks; Section 7 discusses the theoretical meaning of the score and compares it to the normalized recall; and finally, Section 8 concludes the paper with suggestions for possible future research directions.

611

2. BACKGROUND
While many evaluation metrics have been proposed for ad hoc type IR tasks, by far the most popular in general used is MAP [5]. The standard scenario for use of MAP in IR evaluation is to assume the presence of a collection of documents representative of a search task and a set of test topics (user queries) for the task along with associated manual relevance data for each topic. The relevance data for each topic is assumed to be a sufficient proportion of the documents from the collection that are actually relevant to that topic. "Sufficient" here relates to the fact that the actual number of relevant documents each topic is unknown without manual assessment of the complete document collection for each topic. Several techniques are available for determining sufficient relevant documents for each topic [8, 15, 26]. As its name implies, MAP is a precision metric, which emphasizes returning a greater number of relevant documents earlier. The impact on MAP of locating relevant documents later in the search of a ranked list is very weak, even if very many such documents have been retrieved. Thus while MAP gives a good and intuitive means of comparing systems for IR tasks emphasising precision, it will often not give a meaningful interpretation for recall focused tasks. A detailed analysis of the behaviour of MAP is described in [19]. Some other IR evaluation metrics are found to be more representative than MAP for other types of IR task. For example, Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) are used for IR applications such as question answering and web search respectively [10, 28]. MRR measures performance when looking for one specific "known item" in a document collection [3]. Mean reciprocal rank is simply the inverse of the rank of the relevant document in the retrieved list. NDCG treats the relevant documents differently where the relevant documents are classified into classes according to the degree of relevance to the query. The objective is to find highly relevant documents earlier in the ranked list than less relevant ones. Additional IR evaluation scores have been introduced with the advent of new IR applications such as mean average generalized precision (MAgP) for structured document retrieval [1, 16] and GMAP which is the same as MAP but using geometric mean instead of the arithmetic mean, GMAP was used in the Robust Track at TREC [30]. Recently some scores have been introduced as alternatives to the MAP in order to overcome its shortcomings. Bpref, inferred average precision (infAP), and rank-biased precision (RBP) are examples of these scores. Bpref is designed to overcome the problem of incomplete relevance judgements [9]. infAP is designed for a similar purpose, where it collapses to MAP when judgements are complete [2]. RBP is designed to reflect a better modelling of user behaviour in terms of how deep they are willing to go down in the results list [19].
Similar to MAP, these IR evaluation metrics focus on measuring effectiveness at retrieving relevant documents earlier rather than on the system recall. While this is sufficient and reasonable for precision focused tasks, it is not suitable for tasks where the objective is to find "all" relevant documents, and in particular if the objective is to find all relevant documents with minimum effort for the user. In this kind of application, the user is willing to exert much effort to go deeper in the list in order to find relevant documents. Additionally, for recall-oriented IR applications the maximum number of documents to be checked by the user (the cut-off of the retrieved results) is also very important, since it has a direct impact on the cost of user effort and on recall. This concern was the reason behind using recall along with MAP in

evaluating similar IR tasks [25, 31]. The maximum number to be checked by the user is completely overlooked by most of the metrics considered so far, and is variable in measures such as the f-score [21]. The f-score combines recall with precision, and has been used for legal IR [20]; although this score includes recall, it has the problem that the number of documents to be retrieved is not fixed, which is often a practical concern of real users.
Other measurements such as retrievability and findability have been used for analyzing query formulation on the retrieval effectiveness [4, 6]. Although these scores give some analysis for the effect of query formulation on system performance, they fail to compare performance of different systems on a set of topics.

3. IR EVALUATION SCORES FOR RECALL-ORIENTED IR TASKS
The simplest solution to measuring performance in a recall focused IR task is of course simply to evaluate the recall. However, as noted in the previous section, the problem of doing this is that it fails to reflect how early a system retrieves the relevant documents and thus the user effort involved. Although recall is the objective for such applications, the score should be able to distinguish between systems that retrieve relevant documents earlier than those that retrieve them later. To overcome this problem the f-score can be used, but at a fixed number of retrieved documents. However the same problem will arise, since applying it after retrieving N-documents for two systems that retrieved the same number of relevant documents, the f-score will be the same. This situation arises since the f-score is designed for classification tasks, but for recall-oriented IR applications, the problem is viewed as a ranking problem with a cut-off for a maximum number of documents to be checked Nmax.
One modification for using the f-score is to calculate it as a combination between the recall and the average precision (AP) instead of using the absolute precision (equation 1). Such a modified f-score will reflect the system recall in addition to its average precision. However, while this captures the recall, it will have the same disadvantages for recall focused tasks with respect to AP which were noted earlier.

F '

=

(1 +  2 )  ( AP  R )  2  AP + R

(1)

where, AP: Average precision of a topic R: recall at a given number of retrieved documents : weight of recall to precision

Table 1 shows an illustrative example of how different metrics perform with four different IR systems when searching a collection for a single query. In this case it is known that there are four relevant documents, and it is assumed that the user is willing to check the top 100 documents retrieved by each system.

Table1. Performance of different scores with different IR systems

Ranks of rel. docs AP Recall F1 F'1 F'4

System 1

{1}

0.25 0.25 0.0192 0.25 0.25

System 2 {50, 51, 53, 54} 0.0481 1 0.0769 0.0917 0.462

System 3 {1, 2, 3, 4}

1

1 0.0769 1

1

System 4 {1, 98, 99, 100} 0.2727 1 0.0769 0.429 0.864

612

In Table 1, system 3 is the prefect result with all relevant documents retrieved at the top ranks. System 1 has the lowest recall, while system 2 has moderate performance retrieving all relevant documents in the middle of the ranked list, System 4 has fair performance since it ranks one relevant document at rank 1, but achieves 100% recall only after checking the full list of 100 top results.
From the table it can be seen that AP for system 1 is much higher than for system 2, which is unfair, since system 2 has been able to retrieve all relevant documents in the middle of the list, but system 1 has failed to retrieve more than one relevant document in the full list. The same situation arises when comparing system 4 to system 2, even though both systems have been able to retrieve the full list of relevant documents, system 2 has done so at much higher ranks than system 4.
Recall and F1 score fail to differentiate between systems 2, 3, and 4, even though these systems have very different behaviour.
F'1 does not focus on the recall, which is the objective of recalloriented applications. To emphasize recall a modified f-score, F'4 was tried giving recall four times the weight of the average precision ( = 4 in Equation 1). Initial inspection suggests that F'4 looks to be a good representation of the system performance, however on deeper analysis, it can be seen that system 4 is evaluated to be nearly twice as good as system 2, even though while it retrieves a relevant document at rank 1 no further relevant documents are found until the end of the list and that while system 2 failed to return any relevant documents among the first half of the list, all relevant documents are retrieved by rank 54. For two systems such as 2 and 4 for a recall-oriented task with users willing to check the first 100 documents, system 2 will give more confidence to the user that there is little chance of finding further relevant documents after rank 100; since the presence of low ranked relevant documents in system 4 may suggest that further ones are more to be present. Hence, F'4 fails to evaluate system 2 and system 4 fairly from the perspective of a recall-oriented
application in practical usage.
4. NORMALIZED RECALL (RNORM)
One of the proposed IR evaluation metrics that has never found its way into wide usage is normalized recall (Rnorm) [21, 24], shown in Equation 2. This measures the effectiveness in ranking documents relative to the best and worst ranking cases, where the best ranking case is retrieval of all relevant documents at the top of the list, and the worst is retrieving them only after retrieving the full collection. Figure 1 shows an illustrative graph of how to calculate Rnorm, where Rnorm is the area between the actual and worst cases divided by the area between the best and worst cases.
A1
A2
Figure 1. Illustration of how Rnorm curve is bounded by the best and worst cases [21]

  Rnorm

=

A2 A1 + A2

= 1-

ri - i n(N - n)

(2)

where: ri: the rank at which the ith relevant document is retrieved, N: collection size, and n: number of relevant docs

Normalized recall can be seen as a good representative measure for recall-oriented IR applications. This measure is greater when all relevant documents are retrieved earlier. However it requires ranking of the full collection. Applying Rnorm on collections of very large numbers of documents is infeasible, since it is nearly impossible to rank a collection of potentially many millions of documents. In addition, some relevant documents may have no match to the query leading to them not being retrieved at all.

One approximation to address this problem is to consider any
relevant documents not retrieved in the top Nmax to be ranked at the end of the collection. Using this approximation to enable the
calculation of Rnorm leads to its value being nearly equal to the system recall at a cutoff of Nmax. For example, for a collection of tens of thousands of documents and when retrieving the top 1000 documents; if recall at 1000 equals 50%, Rnorm with the previous approximation will equal 49.99% (Figure 2).

Figure 2. Illustration of how Rnorm curve behaves with large document collections
5. PATENT RETRIEVAL EVALUATION SCORE (PRES)
In the previous sections we demonstrated that current evaluation metrics do not represent system performance well in recalloriented IR applications. In this section, a novel score is presented based on modifications to the normalized recall measure. As outlined in the previous section, Rnorm can be seen as a good score for evaluating recall-oriented applications but only for small collections. Our new score "Patent Retrieval Evaluation Score" (PRES) is based on the same idea as the Rnorm but with a different definition for the worst case. The new assumption for the worst case is to retrieve all the relevant documents just after the maximum number of documents to be checked by user (Nmax). The idea behind this assumption is that getting any relevant document after Nmax leads to it being missed by the user, and getting all relevant documents after Nmax leads to zero recall, which is the theoretical worst case scenario. Applying this assumption in equation 2, N is replaced with Nmax+n, where n is the number of relevant documents. Any relevant document not retrieved in the top Nmax is assumed to be the worst case (Figure 3). For example, for a retrieved ranked list for a topic with 10 relevant documents (n = 10) and for which the user is willing to check the top 100 documents (Nmax = 100); the best case will be finding the 10 relevant documents at ranks {1, 2, ... 10}, and the worst case will be finding them in the ranks {101, 102, ... 110}, which means the user missing all the relevant documents. Assuming retrieval of only 7 relevant documents in the top 100,

613

then the missing 3 relevant documents will be assumed to be found at ranks {108, 109, 110}.

Figure 3. PRES curve is bounded between the best case and the new defined worst case

Equation 3 shows the calculation of PRES. Equation 4 shows the direct calculation of the summation of ranks of relevant documents in the general case, when some relevant documents are missing in the top Nmax documents.

 ri - n + 1

PRES = 1 - n

2

(3)

N max

  ri =

nR
ri + nR ( N max
i =1

+ n) -

nR ( nR 2

- 1)

(4)

where, R: Recall (number of relevant retrieved docs in the 1st Nmax docs)

From equation 3, it can be inferred that PRES is a function of the recall of the system, the ranks of the retrieved documents, and the maximum number of results to be checked by user. For a given Nmax, PRES behaves as shown in Figure 4(a). For recall = R, the PRES value ranges from R, when retrieving all relevant document on the top of the list, to nR2/Nmax when retrieving them at the bottom of the list. For the special case where the number of relevant documents for a topic is one (n=1), PRES will have a linear characteristic. Figure 4(b) shows the difference between PRES and MRR performance with different ranks for the case where n=1. In this case PRES could be used as an alternative measure for evaluating question answering instead of MRR. For example, if the user is willing to check the first 10 answers for a question before reformulating it [10], PRES with Nmax = 10 could be used instead as it will assign a low penalty to systems that retrieve the relevant document within the first 10 ranks, and a full penalty to systems that retrieve the document afterward.

Figure 4(a). PRES performance with various recalls and rank

Figure 4(b). PRES vs MRR for different rank when n=1
6. ANALYSIS OF PRES PERFORMANCE
In this section, PRES is tested on the same sample examples as Table 1, with additional illustrative real samples from one run in the CLEF-IP 2009 patent retrieval task. In addition, the average performance is tested on real examples of 48 participants' runs from CLEF-IP 2009. The aim of the CLEF-IP track is to automatically find prior art citations for patents. The topics for this task are patents filed in the period after 2000, and the searched collection contains about one million patents filed in the period from 1985 to 2000 [25]. The objective is to use some text from each patent topic to automatically retrieve all cited patents found in the collection. The design of the patent test collection assumes that filed patents examined by the patent office for novelty, are the training and test collections, and that the patent citations, which are mostly added by the patent office, are considered as the relevant document set [13, 14, 25].
6.1. Performance with Sample Examples

Table 2. Performance of PRES with different IR systems

System1 System2 System3 System4

Ranks of rel. docs {1}
{50, 51, 53, 54} {1, 2, 3, 4}
{1, 98, 99, 100}

AP 0.25 0.0481
1 0.2727

Recall 0.25
1 1 1

PRES 0.25 0.51
1 0.28

Table 2 shows how PRES performs with the sample examples presented in Table 1. From Table 2, it can be seen that PRES is a better representative measure for the system performance as a combination between system recall and average ranking of relevant documents. Some real samples of topics from one run of the CLEF-IP 2009 track are presented in Table 3 with maximum number of results to be checked by user Nmax = 1000. In Tables 2 and 3, PRES is always less than or equal to recall, i.e. PRES is a portion of the recall depending on the quality of ranking of the relevant documents relative to Nmax. For example, getting a relevant document at rank 10 will be very good when Nmax=1000, good when Nmax=100, but bad when Nmax = 15, and very bad when Nmax=10. Systems with higher recall can achieve a lower PRES value when compared to systems with lower recall but better average ranking. This is clear in Table 3, where one topic with 67% recall has 63.6% PRES because of good ranking (41 and 54 among 1000), and one topic with 100% recall got 52.5% for PRES because of the moderate ranking where 60% of them are below rank 500 out of 1000.

614

Comparing PRES to average precision (AP) for the samples in Table 3, it can be seen that AP is more sensitive to how early the first relevant document is found regardless of the number of documents to be checked by user. However, PRES is more sensitive to the average ranking of the relevant retrieved documents as a whole relative to the maximum number of documents the user is willing to check. The last sample topic in the table has a PRES of 96.43% even though relevant documents are not ranked in the top 10 or even 20 results. The reason is that Nmax=1000, and the ranks {32, 35, 46} are considered relatively good compared to this number. Nevertheless, when calculating PRES with Nmax=100, the PRES value will be 64.33% which represents the average ranking of the relevant documents relative to the maximum number of documents to be checked.

Table 3. AP/R/PRES performance with real samples of topics

Ranks of rel. docs

N

{98,296}

41

{23,272,345}

6

{2,517,761}

6

{660,741}

3

{41,54}

3

{1,781}

3

{1,33,354,548,733,840,841} 7

{32,35,46}

3

R 0.05 0.5 0.5 0.667 0.667 0.667
1 1

AP ~ 0 0.01 0.085 0.001 0.021 0.334 0.157 0.051

PRES 0.039 0.394 0.288 0.201 0.636 0.407 0.525 0.964

6.2. PRES Average Performance
PRES was tested on 48 different submissions from 15 participants to the CLEF-IP 2009 Patent Track [25]. Table 4 shows the score for each submission in MAP, recall, and PRES. Participant IDs are anonymous and the number of topics for each participant used was 400 instead of the official 500 in order to further mask participant identities and to avoid violating the privacy of any of the participants. For all topics, Nmax = 1000 was used. The average number of relevant documents per topic is 6 (navg = 6). From the results, it can be seen that PRES reflects the recall with the average quality of the ranking, which is mainly reflected in the MAP. Run 21 (R21) which achieved the highest MAP and recall also achieved the highest PRES, with the same behaviour being observed for the lowest scoring runs. However, some submissions which achieved high precision but low recall were punished and received only a moderate PRES score. For systems which achieved high recall but low precision (which reflects bad ranking such as system R18), the PRES score was moderate too. Figure 5 plots the three scores of the same 48 submissions sorted by PRES from low to high values. From Figure 5, it can be noted that PRES is a good single score that can represent both the precision and recall of each run. Figure 6 shows the change in ranking of the submissions with the three scores. It can be seen that ranking using PRES is more biased towards recall, than MAP. However, this is not always the case, for example R12 has moderate ranking in both recall and MAP, but lower ranking in PRES, which is due to the fact that MAP is more sensitive to the high ranking of some of the relevant documents, but PRES is dependent on relative average ranking of "All" relevant documents to Nmax. Figure 6 shows that the scores have high agreement on the ranking of systems with very high or very low performances.
In order to check the agreement of the three scores, pair wise comparison of submissions was carried out with each two runs being compared: 1) the first run is statistically significantly better

than second run, 2) the second run is statistically significant better than 1st run, and 3) Both runs are statistically indistinguishable [7]. Wilcoxon significance test with confidence level of 0.95 was used for comparing each of the two runs [12]. Comparing 48 runs in a pair wise manner led to 1,128 comparisons. The agreement of scores for each comparison is plotted in Figure 7.From Figure 7, it is clear that PRES is an intermediate score between recall and MAP. In addition, in a small number of cases (1%) PRES disagrees when recall and MAP agree. These situations are mainly for examples where recall and MAP agree that system 1 (1st run) is better than system 2 (2nd run), but PRES shows that both systems have the same performance, or when recall and MAP agree that two systems are statistically indistinguishable, but PRES prefers one over the other.
Calculating the Kendall's tau correlation between the ranking of runs according to the three scores [17], it is found that the correlations are as follows: MAP and recall = 0.56, PRES and recall = 0.87, and PRES and MAP = 0.66. This emphasizes that PRES lies between MAP and recall with a bias towards recall.

Table 4. MAP/Recall/PRES for 48 submissions in CLEF-IP

Run ID

MAP

Recall

PRES

Run ID

MAP

Recall

PRES

R01 0.077 0.530 0.434 R25 0.064 0.492 0.392

R02 0.087 0.617 0.499 R26 0.084 0.511 0.431

R03 0.084 0.609 0.497 R27 0.097 0.514 0.447

R04 0.053 0.219 0.213 R28 0.091 0.514 0.442

R05 0.000 0.020 0.011 R29 0.082 0.436 0.373

R06 0.000 0.016 0.009 R30 0.092 0.559 0.469

R07 0.000 0.012 0.007 R31 0.081 0.568 0.460

R08 0.000 0.016 0.009 R32 0.078 0.476 0.391

R09 0.071 0.454 0.369 R33 0.085 0.457 0.379

R10 0.088 0.533 0.430 R34 0.082 0.427 0.354

R11 0.087 0.489 0.404 R35 0.114 0.572 0.496

R12 0.088 0.534 0.430 R36 0.108 0.553 0.480

R13 0.065 0.508 0.406 R37 0.114 0.572 0.494

R14 0.068 0.467 0.363 R38 0.107 0.553 0.479

R15 0.064 0.434 0.348 R39 0.113 0.575 0.498

R16 0.020 0.197 0.148 R40 0.107 0.560 0.483

R17 0.067 0.584 0.463 R41 0.079 0.547 0.447

R18 0.033 0.656 0.490 R42 0.103 0.555 0.466

R19 0.105 0.600 0.529 R43 0.091 0.575 0.475

R20 0.003 0.051 0.040 R44 0.091 0.574 0.474

R21 0.266 0.760 0.691 R45 0.106 0.616 0.507

R22 0.028 0.256 0.200 R46 0.102 0.611 0.504

R23 0.087 0.728 0.603 R47 0.104 0.589 0.484

R24 0.011 0.069 0.054 R48 0.102 0.587 0.484

0.8 MAP

0.7

Recall

PRES

0.6

0.5

0.4

0.3

0.2

0.1

0

0

8

16

24

32

40

48

Figure 5. MAP/Recall/PRES for 48 submissions in CLEF-IP 2009 sorted by PRES

615

Ranked by MAP

Ranked by PRES1

Ranked by Recall

Figure 6. Ranking change of 48 submissions according to MAP/PRES/Recall
Figure 7. Agreement chart of MAP/Recall/PRES on pair wise comparison of 48 submissions
1 This information is from a personal communication with patent examiners in the European Patent Office (EPO)

6.3. Performance versus Different Cut-off
Values (Nmax)
Cut-off value of documents to be checked is considered one of the key variables that affect the value of PRES. It is the same case for recall, as the more documents that are retrieved the more possibility there is to find further relevant documents, hence the higher the system recall. Additionally, for PRES Nmax affects its value even if no more relevant documents are found, since for different cut-offs, the relative ranking of relevant documents is different. This effect has been shown earlier in one of the examples (section 6.1).
For recall-oriented applications, the actual number of documents to be checked by the user is typically higher than other IR applications. This number can exceed a hundred documents in the case of a patent examiner before he/she thinks of reformulating the query1. Different factors can affect the decision to stop checking for relevant documents; one of these can be the failure to find a relevant document for some while in the list, or the user can decide to check a fixed number of documents, but when less relevant documents are found while checking the list the user will generally move more quickly through the list leading to more rapid task completion. For both scenarios the effort the user exerts to find a relevant document will be greater as long as he/she continues to find relevant documents deep in the list. This is the reason of why PRES penalizes finding documents deeper in the list of the Nmax ranked results.

MAP

MAP @ Nmax = {10, 20, ... 100}
0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

R12

0.01

R18

R23 0

10

20

30

40

50

60

70

80

90

100

Nmax

Recall @ Nmax = {10, 20, ... 100}
0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

R12

R18 0.05
R23 0

10

20

30

40

50

60

70

80

90

100

Nmax

Recall

PRES

PRES @ Nmax = {10, 20, ... 100}
0.35

0.3

0.25

0.2

0.15

0.1

R12

0.05

R18

R23 0

10

20

30

40

50

60

70

80

90

100

Nmax

Figure 8. MAP/Recall/PRES performance for different values of Nmax applied on three sample runs

616

Figure 8 shows the effect of changing the value of Nmax on MAP, recall, and PRES. Three sample runs from CLEF-IP 2009 (R12, R18, and R21) were selected to examine the variation of the three scores at different values of Nmax.
In figure 8, the effect of finding more relevant documents on MAP is very poor regardless of the number of documents to be checked by the user and regardless of the number of relevant documents found deeper in the list. PRES and recall performances look similar in general, however, for the example, when Nmax = 10, PRES judges R12 to be better than R23, but recall is judged to be the opposite. Furthermore, for R18 the recall curve with Nmax has a higher slope than the PRES curve. This returns us to the issue of recall neglecting the ranking of documents by recall, which is taken into account by PRES.

6.4. PRES when n > Nmax
Usually for recall-oriented applications, when all or at least a significant portion of the relevant documents are required to be retrieved, the user will check a number of retrieved results higher than the expected number of relevant documents. However, this scenario can be neglected in some applications where the number of relevant documents is very high and the task is to evaluate different IR systems for the ability to find the largest number of relevant documents. This is the exact scenario in recall-oriented IR applications such as legal search. The legal track at TREC seeks to evaluate the ability of different systems to retrieve relevant legal documents [27]. The number of relevant documents for a topic can reach tens of thousands. Several scores and methods have been proposed to overcome this problem by estimating the number of relevant documents and the actual system precision and recall.

In this subsection, the behaviour of PRES is studied for cases like this where the number of relevant documents (n) is higher than the maximum number of documents to be checked by the user (Nmax).
As shown in Figure 9, the best case will never be applicable as retrieving all relevant documents at the top ranks will exceed the cut-off value, and the user will never be able to achieve 100% recall. However, the calculation of PRES in this case can still be applied without any modification. As mentioned before, for a recall = R, PRES will range from nR2/Nmax, to R. The only difference here is that the maximum applicable R will be Nmax/n, which is the case when all the retrieved documents are relevant.

Although the PRES calculation is still applied, the PRES value will have some limitation in expressing the general system performance. Hence, estimated an PRES can be calculated to approximate the full performance of the system as shown in Equation 5.

PRES est

=

PRES Rmax

(5)

R max

=

N max n

, ( N max  n )

(6)

where, PRESest: estimated PRES, Rmax: maximum possible recall (Rmax = 1 when Nmax  n)
While this provides an estimate of system performance, it is advisable only to use PRESest in evaluation campaigns where there are a large number of runs with a very large number of relevant documents and it is impractical to evaluate the very long

submitted lists of many systems. For an accurate evaluation using PRES, Nmax should be carefully selected according to the user and application models, and for a recall-oriented application, Nmax should be higher than n
Figure 9. PRES curve for situations when n > Nmax
7. THEORETICAL DISCUSSION
In the previous sections, it was shown how PRES was derived from normalized recall (Rnorm) after changing the worst case scenario definition. Although both scores are very similar in characteristics and calculations, this small modification led to a significant change in the performance and the theoretical meaning of the PRES score.
Normalized recall was first proposed by Rocchio in 1964 [24] as an IR evaluation score that is independent of the cut-off value of the retrieved documents, as it requires (as was shown in section 3) returning all documents of the collection ranked by relevance. In 1969, Robertson showed that Rnorm is the same area under the recall-fallout curve (operating characteristic curve), which makes Rnorm equal to the probability of pairwise error in ranking, and which leads to Rnorm = 0.5 for random ranking of documents in the collection [22]. This is not the case for PRES, where the PRES value is directly dependent on the cut-off value. Furthermore, random ranking of documents will eventually lead to PRES = 0 for the current common collection sizes, as the probability of finding a relevant document = n/N, where N is the collection size which is typically millions or billions of documents in case of web search.
Normalized recall was a suitable evaluation measure at the time it was introduced, but with the current collection sizes and type of applications, Rnorm is found to be an impractical measure for operational use. This is the reason why it has never found its way into wide spread usage. PRES can be considered as an IR evaluation measure that has the characteristics of the classic Rnorm, but with a different meaning. PRES is designed specifically for recall-oriented applications to emphasize the system quality in retrieving the most significant number of relevant document as early as possible within a specific number of results in a ranked list.
8. CONCLUSION & FUTURE WORK
In this paper, a study of recall-oriented applications has been described and a novel score "PRES" has been presented that is designed for these applications. The score is a refinement of the normalized recall score. It has been tested and compared to the most widely used IR scores on a patent retrieval task. Illustrative samples and real data examples demonstrated the effectiveness of the new score. The score reflects the system recall combined with the quality of relative ranking of retrieved relevant documents within the maximum numbers of documents to be checked by a user. The PRES value varies from R to nR2/Nmax according to the average quality of ranking of relevant documents; hence it can be

617

seen as a function of system recall, ranking of relevant documents, and the maximum number of documents to be checked by a user (which directly affects the recall and relative ranking).
In future work, the utility of PRES as a measure for the patent retrieval could be investigated further by direct consultations with professional patent experts. Such a study should have a practical and theoretical analysis of the user model represented by PRES (similar to the study in [23]). Additionally, PRES could be applied to other recall-oriented IR applications such as chemical IR and legal IR [32], which can be characterized by different experimental environments, different users, and different numbers of relevant documents. Although the performance of PRES has been analyzed for legal search in this paper, real sets of runs are needed in order to explore its behaviour on this type of data.
9. ACKNOWLEDGMENTS
This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (CNGL) project.
10. REFERENCES
[1] Ali M. S., Consens, M. P., Kazai, G., and Lalmas, M. Structural relevance: A common basis for the evaluation of structured document retrieval. In Proceedings of CIKM '08, pages 1153-1162, 2008.
[2] Aslam J. A., and E. Yilmaz. Estimating average precision with incomplete and imperfect judgments. In Proceedings of CIKM' 06, page102-111, 2006.
[3] Azzopardi L., de Rijke, M., and K. Balog. Building simulated queries for known-item topics: an analysis using six european languages. In Proceedings of SIGIR '07, pages 455-462, 2007.
[4] Azzopardi, L. and Vinay, V. Retrievability. An evaluation measure for higher order information access tasks. In Proccedings of CIKM '08, pages 1425-1426, 2008.
[5] Baeza-Yates, J., and Ribeiro-Neto, B. Modern Information Retrieval. Addison Wesley, 1999.
[6] Bashir, S., and Rauber A. Analyzing Document Retrievability in Patent Retrieval Settings. In Proceedings of Database and Expert Systems Applications (DEXA 2009), pages 753-760, 2009.
[7] Buckley, C., and Voorhees, E. M. Evaluating Evaluation Measure Stability. In Proceedings of SIGIR 2000, pages 3340, 2000.
[8] Buckley, C., Dimmick, D., Soboroff, I., and E. Voorhees. Bias and the limits of pooling. In Proceedings of SIGIR '06, pages 619-620, 2006.
[9] Buckley, C., and Voorhees, E. M. Retrieval evaluation with incomplete information. In Proceedings of SIGIR '04, pages 25-32, 2004.
[10] Carterette, B., Bennett, P. N. Chickering, D. M., and Dumais, S. T. Here or There: Preference Judgments for Relevance. In Proceedings of ECIR '08, pages 16- 27, 2008.
[11] Cleverdon, C. The Cranfield Tests on Index Language Devices. In: Sparck Jones, K. and Willett, P. (eds.). Readings in Information Retrieval, pages 47-59, Morgan Kaufmann, 1997.
[12] Hull, D. Using statistical testing in the evaluation of retrieval experiments. In Proceedings of SIGIR '93, pages 329-338, 1993.

[13] Fujii, A., Iwayama, M., and Kando, N. Overview of Patent Retrieval Task at NTCIR-4. In Proceedings of the 4th NTCIR Workshop, 2004.
[14] Graf, E., and Azzopardi, L. A methodology for building a patent test collection for prior art search. In Proceedings of the 2nd EVIA Workshop, pages 60-71, 2008.
[15] Jordan, C., Watters, C., and Gao, Q. Using controlled query generation to evaluate blind relevance feedback algorithms. In Proceedings of JCDL '06, pages 286-295, 2006.
[16] Kamps, J., Pehcevski, J., Kazai, G., Lalmas, M., and Robertson, S. INEX 2007 evaluation measures. In Proceedings of INEX '07, pages 24-33, 2007.
[17] Kendall, M. A new measure of rank correlation. Biometrika, 30(1/2):81-93, 1938.
[18] Mandl, T. Recent developments in the evaluation of information retrieval systems: moving toward diversity and practical applications. Informatica, 32:27-38, 2008.
[19] Moffat, A., and Zobel, J. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst. 27(1):1-27, 2008.
[20] Oard, D. W., Hedin, B., Tomlinson, S., and Baron, J. R. Overview of the TREC 2008 legal track. In Proceedings of TREC 2008, 2008.
[21] van Rijsbergen, C. J. Information Retrieval, 2nd edition. Butterworths, 1979.
[22] Robertson S. E. The parametric description of the retrieval tests. Part 2: Overall measures. Journal of Documentation, 25(2):93-107, 1969.
[23] Robertson, S. A new interpretation of average precision. In Proceedings of SIGIR '08, pages 689-690, 2008.
[24] Rocchio J. Performance indices for document retrieval systems. In Information storage and retrieval, Computation Laboratory of Harvard University, Cambridge, MA, 1964.
[25] Roda G., Tait J., Piroi F., and Zenz V. CLEF-IP 2009: retrieval experiments in the Intellectual Property domain. In Proceedings of CLEF '09, 2009.
[26] Tague J., Nelson, M., and Wu, H. Problems in the simulation of bibliographic retrieval systems. In Proceeding of SIGIR '81, pages 66-71, 1981.
[27] Tomlinson S., Oard, D. W., Baron, J. R., and Thompson, P. Overview of the TREC 2007 Legal Track. In Proceedings of TREC 2007, 2007.
[28] Voorhees, E. M., and Tice, D. M. The TREC-8 Question Answering Track Evaluation. In Proceedings of TREC 1999, pages 77-82, 1999.
[29] Voorhees, E. M. The Philosophy of Information Retrieval Evaluation. In Evaluation of Cross-Language Information Retrieval System, Proceedings of CLEF '02, pages 355-370, 2002.
[30] Voorhees, E. M. The TREC robust retrieval track. In SIGIR Forum 39(1):11-20, 2005.
[31] Xue, X., and Croft W. B. Automatic Query Generation for Patent Search. In Proceedings of CIKM'09, pages 20372040, 2009.
[32] Zhu, J., and Tait, J. A proposal for chemical information retrieval evaluation. In In Proceedings of the 1st ACM Workshop on Patent Information Retrieval at CIKM '08, pages 15-18, 2008.

618

Content-enriched Classifier for Web Video Classification

Bin Cui1

Ce Zhang1

Gao Cong2

1Department of Computer Science & Key Lab of High Confidence Software Technologies (Ministry of Education), Peking University
{bin.cui,zhangce}@pke.edu.cn

2School of Computer Engineering, Nanyang Technological University, Singapore
gaocong@ntu.edu.sg

ABSTRACT
With the explosive growth of online videos, automatic real-time categorization of Web videos plays a key role for organizing, browsing and retrieving the huge amount of videos on the Web. Previous work shows that, in addition to text features, content features of videos are also useful for Web video classification. Unfortunately, extracting content features is computationally prohibitive for realtime video classification. In this paper we propose a novel video classification framework that is able to exploit both content and text features for video classification while avoiding the expensive computation of extracting content features at classification time. The main idea of our approach is to utilize the content features extracted from training data to enrich the text based semantic kernels, yielding content-enriched semantic kernels. The content-enriched semantic kernels enable to utilize both content and text features for classifying new videos without extracting their content features. The experimental results show that our approach significantly outperforms the state-of-the-art video classification methods.
Categories and Subject Descriptors
H.3 [Information Systems]: Information Storage and Retrieval
General Terms
Algorithms, Experimentation, Performance
Keywords
Web, Video, Content, text, Classification
1. INTRODUCTION
Recent years have witnessed an explosive growth of online video sharing and many Web sites provide video sharing services such as YouTube, GoogleVideo, YahooVideo, MySpace, and ClipShack. These video sharing Web sites often organize online videos into categories so that users can browse videos by categories and search within a certain category.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

The category labels in the video sharing Web sites are usually provided by users when they upload videos to the Web sites. However, the manual annotation of category label for the Web videos suffers from some problems. For example, it can be burdensome for users to choose an appropriate category for a video; and users generally have different understandings on video categories, and thus the user labeled categories are often inconsistent. Hence, it will be highly desirable to be able to automatically suggest categories based on the text description and content of video even if the suggestion may not be perfect. Additionally, some Web sites, e.g., Google Video, collect not only the Web videos uploaded by users in video sharing Web sites, but also the videos embedded in news websites, blogs, etc. For the latter, the videos usually do not have category information. Hence, automatic video categorization is essential to determine categories of such videos so that the videos without user labeled categories can also be organized in the same way as the videos having category labels.
The semantics of videos are described by both content features and textual descriptions. On one hand, most Web videos are associated with text descriptions: almost every video sharing Web site (e.g., YouTube) requires users to attach tags and descriptions; most videos from other circumstances, e.g., blogsphere and news Web sites, are also surrounded by text. On the other hand, Web videos are visual content-rich, e.g., color, texture, shape, etc. The previous work on automatic video classification [4, 26, 16] exploits both text features and content features for video categorization, and the results reported in the existing work show that text features consistently outperform content features for video classification; however, the performance of using both text features and content features is better than the performance of using either of them alone.
Although content features can contribute to video classification, extracting content features is computationally expensive. The time cost of extracting content features is generally in the same order of magnitude with the video length. For example, it is reported [9] that the speed of extracting some visual features, e.g., SIFT, is less than 10 frames per second. The expensive computational cost renders the previous work utilizing content features impractical in two typical application scenarios: 1) to automatically categorize huge volumes of Web videos, and 2) to provide real-time category suggestions for a new uploaded video in a video sharing Web site.
The existing work on video classification [4, 26, 16] simply treats the text description of a video as a bag of words to build classifier as in text classification for general documents. However, the text associated with Web video has special characteristics compared with normal text documents. First, the text descriptions of videos are usually quite short (tens of words). Additionally the descriptions of different videos usually share very few identical words. That is, the feature space is extreme sparse. Second, the text descriptions

619

(often generated from users) of Web videos often contain special words, such as names of persons or organizations, acronyms (e.g. "TNA" for "Total Nonstop Action") and Web language.
The extreme sparsity limits the performance of most existing text classification methods. One would be tempted to employ the classification methods based on semantic kernels using WordNet or Word co-occurrence for Web video classification to alleviate the sparsity problem. However, the large number of special words are not covered by Wordnet and this will limit the performance of classification method [3] using WordNet based semantic kernels; the method [22] based on co-occurrences of words in a collection itself is limited by the feature sparsity problem.
To this end, in this paper we propose a new video classification framework that exploits features from both text and visual content in a novel way. Specifically, we integrate visual content features extracted from the training data into the computation of the semantic similarity between words to enrich the text-based semantic similarity. Based on this novel similarity measure, we construct contentenriched semantic kernel, and employ it to build a text-based classifier. At classification time, we do not need to extract the content features of videos to be classified, which is computationally expensive, while we are still able to implicitly utilize the content features to promote classification performance.
Compared with existing work on video classification, the proposed framework has a salient feature: it incorporates content features into building text-based classifier without sacrificing efficiency at the classification stage. To further improve the classification performance, we also employ Multi-Kernel SVM techniques to combine multiple kernels including the proposed content-enriched semantic kernel and text-based semantic kernels. In a summary, this paper makes the following contributions:
1. We present a novel framework that is able to exploit both content features, e.g. visual features, and text features extracted from training data without requiring to extract content features at the classification stage.
2. We implement this framework by introducing Content-Enriched Similarity (CES) between words, which integrates the visual features to enhance the semantic similarity between words. We also theoretically justify that the CES can effectively capture the similarity between words by establishing its connection with Pointwise Mutual Information that is developed to measure similarity in information theory and has solid foundation in statistics.
3. We conduct an extensive performance study on a large reallife dataset containing more than 10,000 videos (more than 500 hours) with text descriptions downloaded from YouTube. The experimental results demonstrate the superiority of our method over state-of-the-art approaches for online video classification.
The rest of this paper is organized as follows. Section 2 discusses related work. In Section 3, we present the proposed video classification framework and Content-Enriched Similarity (CES). The proposed approach is evaluated in Section 4. We conclude this paper in Section 5.
2. PRELIMINARIES
2.1 Related Work on Video Classification
With the increasing availability of online videos, automatic video categorization has attracted increasing attention recently [4, 26, 15, 16, 19, 23]. We summarize the existing work into three video classification frameworks.

(a) Text-based
(b) Content-based
(c) Fusion of text and content Figure 1: Existing frameworks for video classification
Text-based Framework: The framework is given in Figure 1(a). Existing approaches in this framework cast video classification as a text classification task. Each video is represented with "a bag of word" features and text classification models are used to build classifiers. Approaches in this framework avoid the expensive computation on extracting content features and can be efficient in classification time. However, the video classification methods based on text feature alone have two disadvantages. First, they cannot leverage the rich information contained in video content. Second, the sparsity of text features for online videos limits the performance of these approaches as discussed in Introduction.
Content-based Framework: Figure 1(b) shows the framework. The approaches in this framework use visual content features, e.g., color, texture and edge, to construct the classifiers [15, 26, 19, 23]. Different classification models are employed, such as rule based model[15], decision tree [23] and SVM [19]. Several categorization methods are used in [4, 26], such as Naive Bayes, Maximum Entropy, SVM, etc., and it is shown that SVM obtains the best performance. The content features are less effective in general than text features, although they perform better on some video categories. In addition, extracting content features is very time consuming, which limits their application on a very large video database.
Fusion Framework using Both Text and Content Features: The framework is shown in Figure 1(c), and is adopted in the recent work [4, 26]. These approaches build separate classifiers using text features or different content features. At the classification time, these approaches extract the content features for a new video, and the results from separate classifiers are fused to determine the category of the video. As for the fusion methods, [4] uses a voting scheme, the classification is based on a linear combination of results of separate classifiers in [26], and [16] uses the judgment for each class from each classifier as features, and then builds a meta-classifier to make the final decision. These approaches [4, 26] perform better than the approaches using text features or content features alone. However, this framework also suffers from the expensive computational cost of extracting content features at the classification time. Moreover, they build separate classifiers independently and then employ fusion methods to combine the results. This misses the correlation of different features for classification.
2.2 Related Work on Text Classification
Since text-based classifiers play significant roles in video classification [26, 4], we also introduce related work on text classifica-

620

tion. Many techniques have been proposed in text categorization field [8, 21, 27]. According to the surveys in [21, 27], SVM often outperforms other methods. There is a lot of work on semantic kernels using SVM for categorization to explore the semantic similarity between words. Bloehdorn et al. [2] summarized previous work on semantic kernels and proposed a general framework for semantic kernels. The framework [2] divides the methods of semantic kernels into two categories. First, syntactic structures of sentences are extracted and Tree Kernels [18] have been proposed to exploit the parse trees of sentences. Second, ontology and term co-occurrence have been employed to compute the semantic similarity between words and this semantic similarity is used in kernel functions. WordNet [10] is used in [3] and term co-occurrence [6] is used in [22] to derive a Latent Semantic Kernel. The idea behind them is to incorporate a similarity matrix that records the similarity between tokens (words or latent semantic words), and then use this matrix for semantic expansion of original feature space.
To our knowledge, none of previous work on video classification employs semantic kernels for video classification. Even if the semantic kernels are used, it can only partly address the feature sparsity problem for Web video classification: The text descriptions of online videos contain a large number of words that are not in WordNet and this limits the classification performance of semantic kernels based on WordNet. Additionally, the video descriptions are usually short and generated by users. Users may not often use two words with similar senses in a short description. For example, consider two synonyms bunny and rabbit. Most users will not use both words to describe animal rabbit in a short video description. Therefore, this limits the effectiveness of Semantic Kernels using co-occurrence Similarity for Web video classification.
We briefly introduce SVM, kernel function and Multi-Kernel SVM[7, 17] to provide background of the proposed approach. SVM trains a hyper-plane to separate data of two categories while minimizing the empirical risk. Various non-linear kernels are proposed for classification problems where the data in different categories is not linearly separable. The idea is to apply a non-linear mapping of data to a feature space, in which the linear SVM method can be used. This mapping is defined as  : X  F , where X is the original space, and F is the feature space. Because it is difficult to build a feature space directly, instead kernel functions are used to implicitly define the feature space.
DEFINITION 1 (KERNEL FUNCTION). A kernel is a function K, such that for all x, z  X, K(x, z) = (x)(z), where  is a mapping from X to an (inner product) feature space F .
The works of semantic kernel aim at defining the different function (.) satisfying the above definition using knowledge from WordNet or cooccurence statistics.
Two word-similarity matrices are popularly used in defining semantic kernels. First, several semantic similarity measure [25, 14] between words are defined based on WordNet . Second, semantic similarity between words is also computed by Word Co-occurrence. Intuitively, two words are similar when they frequently appear together in some documents. The relationship between words occurring in the same document is called co-occurrence. Term COOccurrence (COO) is a popular similarity measure of words. If we represent each word as a vector of term frequency (tf ) in each document, then COO of two words can be computed by the cosine similarity of their vectors.
3. CONTENT ENRICHED CLASSIFIER
In this section, we first present the proposed framework for video classification, then introduce the core component of the proposed

framework, namely content-enriched semantic kernels, and finally present the algorithm for Web video classification.
3.1 The Proposed Framework
Our proposed framework for online video classification is shown in Figure 2. The framework consists of the following steps: given a training video data, we extract both text and visual features from it. After that, we obtain Content-Enriched Similarity (CES) between words (to be explained in the next subsection), and extend the semantic kernel technique to the CES to build a video classifier. At the classification stage, this classifier classifies a new video using its text features (but not its content features).
The idea is that we incorporate visual content information extracted from training data into the semantic similarity between words. That is, classifier is built using the kernels based on the semantic similarity that considers both text and content information. The design mechanism enables us to utilize visual clues of Web videos to obtain more reasonable semantic similarity among words.
Figure 2: The proposed content-enriched framework
Although the proposed framework makes use of both text features and content features as the fusion framework in Figure 1(c), it is very different from the fusion framework in the following two aspects: First, in the fusion methods [4, 16] content features are employed as features to build classifier, and thus the content features of video to be classified need to be extracted at classification time. In contrast, in the proposed framework content features of training data are used to calculate Content-Enriched Similarity (CES) between words and the similarity is integrated into semantic kernels to build classifier. We avoid extracting content features from the video to be classified at the classification time. Second, the text classifiers built using fusion methods[4, 16] for video categorization do not consider the semantic information of words.
Compared with existing video classification frameworks, the proposed framework has two salient advantages. First, our framework is comparable to the text-based framework in Figure 1(a) in terms of classification efficiency. Both frameworks do not need to extract content features at the classification stage. This is essential for a framework to be applied to real-time online video classification and classifying online videos of a large scale. Second, our framework is able to achieve better classification accuracy than existing approaches, because our approach can not only take advantage of both text and content features, but also effectively address the problem of sparse features in Web video categorization.
3.2 Content-enriched Semantic Kernel
In the existing classification work, only the text information is taken into consideration for kernel construction, which is insufficient for Web video classification as we discussed previously.

621

The motivation of proposing Content-Enriched Similarity is based on two key observations. First, the effects of text feature and content feature are typically complementary: for some categories (e.g., news video, music video), text-based classifiers have better accuracy than content-based classifiers; while for other categories, like films, content-based classifiers work better [4, 26]. The combination of two types of features is more effective for video classification as shown in [4, 26], although text-based approaches generally perform better than content-based approaches. Second, utilizing content feature in video classification stage is computationally expensive, as content feature extraction is time consuming.
Ideally we can have a way to leverage content features without extracting content features at the classification time. This might sound impossible at first glance. Our idea is to use content features extracted from training data to enhance the text features to obtain content-enriched semantic kernels for classification. This idea is inspired by the way that semantic similarity is used to enhance the text features in semantic kernel. However, we will see that we compute the content-enriched semantic kernels in a very different fashion from existing work on semantic kernels.
We proceed to introduce the key idea of semantic kernels, and present the idea of integrating content features into kernels for classification. The key component of semantic kernels is a word similarity matrix. We denote the matrix by P, where each matrix entry Pij represents the semantic similarity between words i and j. The semantic similarity between two words can be computed using WordNet or term co-occurrence as presented in Section 2.2. Following the work on text classification using semantic kernel, e.g., [3], we can compute the semantic kernels using similarity matrix P as follows:

K = X × P × PT × XT,

(1)

where X is the document-word matrix and Xij gives the term frequency (tf ) of word j in document i.
Although WordNet or co-occurrence based similarity measures can capture some semantic similarities between words, they are still not sufficient for online video classification and they ignore the content information of videos.
Our idea is to exploit the content features to enhance the similarity matrix P, so that we are able to utilize the content features. If we have the Content-Enriched Similarity matrix, we can define the content-enriched semantic kernel similarly as it is in Equation 1. We next present how to compute Content-Enriched Similarity between words and how to derive matrix P using Content-Enriched Similarity.
We observe that similar words often appear in the text descriptions of similar videos although they may not appear in text description of single video. Here, we use the visual content information of videos to decide whether two videos are similar. If two videos are similar in terms of their content information, the words in their text description would be somehow similar. It is expected that we can capture the similarity between words even if they do not appear in the text description of a single video, e.g., bunny and rabbit, since they may appear in the text descriptions of different videos that all contain visual features of animal rabbit. We notice that the similarity between bunny and rabbit can also be captured by WordNet (but not term co-occurrence). However the coverage of WordNet is only about 50% in our video collection downloaded from YouTube. As another example, Content-Enriched Similarity can also capture the word similarity between or favorite and favourite. Note that, for the example of bunny and rabbit, content features like shape and color can associate these two words. For the example of favorite and favourite, there are no specified features;

however, as these two words are often used alternatively, the visual characters of their associated videos are similar statistically. These relations cannot be extracted by WordNet and term co-occurrence (more examples will be given in Section 4.2).
We proceed to present the method of computing Content-Enriched Similarity. We first extract content features of training data, and then cluster videos based on visual content features to obtain k clusters, C1,...Ck, where k is a parameter to be determined on training data. Generally, we expect that two words are similar if they appear in the same cluster, within which the videos are similar in terms of content. This statistical co-occurrence information implies the semantic tightness between two words, i.e., the higher frequency of co-occurrence represents closer similarity. The parameter k, the number of clusters, will affect the similarity measure between words. With a small k we will get large clusters, and thus two dissimilar videos may be included in a cluster. This will degrade the similarity implicated by co-occurrence. On the contrary, if too many clusters are generated with a large k, some words may be only related with weak connections indicated by infrequent co-occurrences. The weak connections are not always informative: they may introduce noise in the Content-Enriched Similarity, however we will miss similarity relationships if we prune all weak connections. Hence, we need to use an appropriate value for parameter k, which can be set empirically using a development set, if any, or using cross-validation on training set.
After we have k clusters, we next compute the word similarity in the space of k clusters. For each word w, it is associated with a vector in document (video) space, i.e., w = <tfw,1, tfw,2, ..., tfw,|D| >, where tfw,i is the frequency of term w in the ith text description, and |D| is the number of text descriptions (videos) in the collection. In order to compute the frequencies of word w in each cluster, we project the vector w from document space to cluster space. To do this, we need to define a video-cluster relation matrix VS, where each entry VSij = 1, if video i is in cluster Cj; otherwise VSij = 0. Then we can project w to wc as follows.

wc = w × VS

(2)

wc = <tfwc,1, tfwc,2, ..., tfwc,k >, where tfwc,i is the frequency of word w in cluster i.
Given two words x and y, together with their term frequency vectors in document space, x = <tfx,1, tfx,2, ..., tfx,|D| > and y = <tfy,1, tfy,2, ..., tfy,|D| >, we compute their term frequency vectors in cluster space according to Equation 2. Then we can com-
pute their semantic similarity using cosine similarity as follows.

Sim(x, y) = cos(xc, yc) = cos(x × VS, y × VS) (3)

where VS is the video-cluster relation matrix presented earlier.
In practice, we can use matrix calculation to compute the content-
enriched semantic similarity for all word pairs. Given the documentword matrix X and video-cluster relation matrix VS, we first compute the product of the two matrices Y = XT × VS. Matrix Y is a word-cluster matrix and we need normalize the matrix to compute cosine similarity. For each row vector y of Y, we normalize it by
yi2, yi  y. Then we get normalized matrix Y. The ContentEnriched Similarity matrix CES can be calculated as follows:

CES = Y × YT

(4)

It is natural to derive explanation for Equation 4. Y =XT × VS gives the distribution of words in the cluster space, and according to Equation 4, word pairs occurring frequently in the same clusters will have larger similarity score. The visual content information of videos is incorporated in the matrix VS without introducing expensive computation in matrix processing.

622

The Content-Enriched Similarity matrix CES is used to replace the matrix P in Equation 1 to compute the Content-Enriched Similarity kernel K.

3.3 Theoretical Justification of CES

In the previous section, we proposed a novel Content-Enriched

Similarity measurement to evaluate the similarity between words.

To theoretically justify our approach can effectively capture the

similarity information, we start from Pointwise Mutual Informa-

tion (PMI) in information theory [5], which is a measure of as-

sociation/similarity between two objects (words) and defined as

SI(x, y)

=

log

p(x,y) p(x)p(y)

.

We start from PMI because it is built

on solid statistical theory.

Let x, y be two words in vocabulary set V , d a video in video

dataset D. Then P (x, y) can be derived by:

P (x, y) = P (x  d, y  d)

=P (x  d1, y  d2, d1 = d2)

(5)

 P (di = dj)P (x  di, y  dj|di = dj)

i,j

where  denotes the values are proportional, P (di = dj) is the probability that video di and dj are equal. Then, we can compute P (x) using this P (x, y), i.e.,

P (x) = P (x, x)

 P (di = dj)P (x  di, x  dj|di = dj).

(6)

i,j

Substituting Formulas 5 and 6 into P M I, we can get

S I (x)



P (x, y) P (x)P (y)



i,j P (di = dj)P (x  di, y  dj |di = dj) i,j P (di = dj)P (x  di, x  dj|di = dj)

(7)

×

1 i,j P (di = dj)P (y  di, y  dj|di = dj)

Now, the remaining task is to estimate the value of P (di = dj) and P (x  di, y  dj |di = dj).
In the traditional co-occurrence similarity calculation, only the
words appearing in the same video text description are considered
to be similar, i.e., i must be equal to j. In our proposed CES mechanism, P (di = dj) can be seen as the probability that videos di and dj are similar, e.g., di and dj are in the same cluster. A straightforward method can be used to estimate P (di = dj) which is denoted by eij as:

P (di = dj) 

1, di and dj are similar 0, di and dj are not similar

(8)

To compute the value of P (x  di, y  dj|di = dj ), we can adopt the model based on term frequency (tf ). There exist many reasonable estimation methods, e.g., the popularly used two models as follows:

P (x  di, y  dj|di = dj)  tf (x, di) × tf (y, dj) (9)

or

P (x  di, y  dj |di = dj )  tf (x, di) × tf (y, dj) (10) Integrating Formula 9 into SI(x, y), we have:

SI(x, y)



i,j eijtf (x, di)tf (y, dj)

(11)

i,j eij tf (x, di)tf (x, dj) i,j eijtf (y, di)tf (y, dj )

The above theory is a natural extension of state-of-the-art approaches by introducing a new parameter of eij , i.e., P (di = dj). There may exist the case that i = j but P (di = dj) = 0; in other words, the similar videos can contribute to word similarity computation.
Recall the aforementioned co-occurrence similarity computation which uses inner product as semantic similarity measurement. The inner product of co-occurrence measurement is based on the assumption that eij is not equal to 0 if and only if i=j. Our mechanism relaxes this restriction that we apply the inner production in an un-orthogonal space, where eij is not equal to 0 if videos i and j are similar, e.g., videos are in same cluster.
Let x and y represent the tf vectors of words x and y respectively. We can redefine the dot function to compute the similarity:

Sim

(x, y) =

cos(x, y) =

< x, y > |x| × |y|

.

(12)

where < x, y > is i,j tf (x, di)tf (y, dj)eij , |x| is equal to

i,j eij tf (x, di)tf (x, dj), |y| is

i,j eijtf (y, di)tf (y, dj ).

This equation can be easily proved in a 2-norm Euclid space. Ob-

viously, Sim (x, y) = 0 does not require words x and y to appear

in the same video. However, it relies on eij, which represents the

similarity between the videos i and j.

Revisit Formula 11, let x and y in SI(x, y) be also represented

by two tf vectors, we have

SI(x, y)



< x, y > |x|2|y|2

(13)

After normalizing x and y to unit norm [12], i.e., |x| = |y| = 1, we can get

SI(x, y)  cos(x, y) = Sim (x, y).

(14)

The above formula shows that our similarity measure is consistent with PMI model, i.e, if there is a tighter "semantic" association between words x and y, the similarity value will be higher. In other words, the Content-Enriched Similarity is capable of capturing the semantic relation between words, and hence can be applied to build semantic kernel for video classifier. Moreover, our discussion is not limited to 2-norm Euclid space, and one can derive a formula in another space, e.g., 1-norm space, by making a different estimation of p(x, y) and p(x), using Equation 10 instead of 9.

3.4 The Classification Process
In this subsection, we summarize the procedure of building classifier using the content-enriched mechanism and applying classifier at the classification stage.
The process for building classifiers is presented in Algorithm 1. To build classifiers using the Content-enriched semantic kernels, three steps are processed for the training dataset. Feature extraction (line 1):We extract the text feature from the text description, and visual content feature from video such as color and texture. CES computation (lines 3-10): We first employ k-means clustering method to cluster the training data into k clusters using content features extracted from training data. And then, we construct a video-cluster relation matrix V S to compute the Content-enriched Similarity between the words in the word set by Formula 4. Building Classifiers (lines 11-12): For each category in the training dataset, we build SVM classifier employing the Content-enriched kernel. As Web video sets are multi-class sets, we use One-AgainstAll [11, 17] method, one of the most widely used methods for multi-class classification, to train classifiers. Thus, each category will have a classifier.

623

Algorithm 1: Algorithm for building CES classifiers

Input: Labeled training video data, and parameter k

Result: A set of classifiers

1 Extract text and content features;

2 Cluster the dataset into k groups;

3 foreach Video vi do

4 if vi in clusterj then

5

VSij = 1; /*VS:video-cluster matrix */

6 else

7

VSij = 0

8 foreach word-pair in Vocabulary Set do 9 Compute the content-enriched similarity of the word-pair
by Formula 4; /*build word similarity matrix*/
10 Construct a CES kernel by Formula 1; 11 foreach category do 12 Build a One-Against-All SVM classifier using the CES
kernel;
13 return Classif iers.

At the classification stage, we use only text features of new video, but not its content features, as the input to the classifiers. Each classifier will return a score to indicate the possibility of the video belonging to the category; the class label of the classifier which returns the highest score is assigned to the video.
3.5 Multi-Kernel Enhancement
We proceed to extend the proposed content-enriched framework using Multi-Kernel SVM techniques [13, 20]. When various kernel functions exist for a classification job, Multi-Kernel learning optimization can take the advantage of individual kernel function and converge towards a more reasonable solution. For example, [13] proposed the Multi-Kernel technique that defines a new optimization function and solves it as Semi-definite Programming and Quadratically Constrained Quadratic Programming.
Multi-Kernel SVM techniques can be applied to the proposed CES scheme that only uses text features at the classification stage. Therefore, we can generate different types of kernels for multiple kernel optimization, e.g., the content-enriched semantic kernels and text-based semantic kernels using WordNet or term co-occurrence.
In order to assign appropriate weights to different types of kernels, we adopt a standard Multi-Kernel scheme [20]. Given several kernels created using different word pair-wise similarity matrices as we discussed above, we first train a linear combination between them, and then use this fusion result as the final kernel. The MultiKernel optimization can effectively explore the advantages of various similarity measures, and will not have obvious effect on the classification efficiency as only the text features are considered at the classification stage of our framework.
4. PERFORMANCE STUDY
In this section, we evaluate our proposed method for online video classification by comparing with state-of-the-art classification techniques. All experiments were conducted on a PC running Linux with 2.4 GHz CPU and 3G memory.
4.1 Experimental Setup
4.1.1 Data preparation
In order to evaluate the performance of our approach, we collect the real-life video data from YouTube (http://www.youtube.

com). During the process of crawling the video data, we collect the videos uploaded to YouTube every 5 minutes by YouTube API on Sep 23 & 24, 2009, which are denoted as "YT923" and "YT924" respectively. This makes our collected two data sets be representative of the distribution of YouTube videos. Finally, 5149 videos from "YT923" and 4447 videos from "YT924" are collected. For our studies, the datasets of "YT923" and "YT924" are taken as our training set and testing set, respectively. The Youtube videos are organized with 15 categories, and the videos belonging to "comedy", "music" and "entertainment" attract more interests relatively.
4.1.2 Feature Extraction
Both the text descriptions and visual content features of videos are extracted for Web video classification task. In our study, the text features extracted from videos include "video title" and the descriptions provided by users who uploaded the videos. We do stemming on these text features by using a standard WordNet stemmer and remove stop words. Employing a similar filtering mechanism in [3], we eliminate words with frequency less than 5 to remove the noisy text features in our experiments. For the visual content features of our videos, we extract the followings features: color (24D), texture (8D) and edge (180D) features.
4.1.3 Performance Metrics
We evaluate both effectiveness and efficiency in our performance study. For efficiency, we concern more on the time cost at the classification stage, as the classifiers are generated offline. The effectiveness performance in terms of accuracy of our video classification problem is measured as an F-score for classification results. It is defined as (2pr)/(p +r), where p is the precision (the number of correct results divided by the number of all returned results) and r represents the recall value (the number of correct results divided by the number of results that should have been returned). F-score can be calculated for each category and then averaged, or can be calculated over all decisions and then averaged. The former is called Macro-F, and the latter is called micro-F.
4.1.4 Parameter Tuning
In the proposed content-enriched Classifier, we first generate the clusters based on the visual content feature of video as introduced in Section 3. The number of clusters k is the key parameter to compute the matrix, which may affect the similarity between words and hence the classification performance. We use k-means clustering algorithm and compute the enriched similarity matrixes using the training data with 5149 videos. We have observed the performance of classification is poor when the number of cluster is too large or small. If k is small, i.e., the cluster has more videos averagely, two dissimilar videos may be included in a cluster, which degrades the similarity implicated by word co-occurrence in an identical cluster. Additionally, more word relationships are generated, which makes the kernel space denser and difficult to be separated by SVM classifiers. On the contrary, if the number of clusters is large, the clustering based mechanism cannot extract enough similarity relationships. Based on the experimental results, we set the number of clusters for generating enriched semantic kernels as 100, which will be used as default parameter in the rest of our experiments. The detailed tuning results are omitted due to the space constraint.
4.2 Comparison on word similarity
The content-enriched word similarity is the core component in our CES framework. The CES based on content feature can capture some interesting relationships between words through clustering of visual information, which cannot be discovered by other text-based

624

methods. Some examples are illustrated in Table 1. We also show the results of WordNet and co-occurrence (COO) based methods which are generally used for text classifier.

Table 1: Example of Word Similarity by Different Approaches

word1

word2 CES COO WordNet

bird

goose





wombat animal





rose

fragrance 

jasmine fragrance 

francesco francisco 

favorite favourite

We observe that the word pairs in Table 1 are quite "contentenriched" in several meaningful ways. First, we can extract WordNetlike type-of relations, e.g., (goose, bird) and (wombat, animal), because they often point to similar objects, which can be analyzed easily using visual features instead of text features. Second, we can extract Visual Similarities between words, which can be represented by own-properties-of relations, e.g. (jasmine, fragrance) and (rose, fragrance). These relations may not be simply extracted by only using text features. For example, visual similarity between videos captures the relationship for flower jasmine and rose, and this relationship can be utilized to further predict their relation with fragrance. Finally, it is interesting that we find our methods can even discover some typos and synonyms in different languages. For example, (francesco, francisco) is mined by CES, which seems like a common typo; (favorite, favourite) is also detected: one of them is American English, and the other is British English. It is difficult for text-based approaches to discover such relationships, because it seems unlikely that a user uses both words to describe the video.
Note that, there exist some noises in the extracted word relationships, and some discovered relationships may not precisely represent the similarity between words. However, it is difficult, if not impossible, to evaluate the performance quantitatively, due to the absences of golden-standard word similarity/relationship and evaluation methods, as the text descriptions for Web video are generally free text and with diverse content. The results demonstrated in Table 1 show that these relationships discovered by CES are meaningful and agree with our common sense, and the classification results reflect the superiority of our proposed methods.
4.3 Comparison on Web video Classification
In this subsection, we report our experimental results on the comparisons among four frameworks including our proposal and three existing frameworks introduced in Section 2.1. We compare our proposed CES method with other competitors, i.e., T ext [3] that represents the classifier based on the text information of Web videos, Content that represents the classifier based on the visual content of videos, and F usion that explores multi-modal information of videos by fusing the results from T ext and Content classifiers [4, 26]. Note that, the T ext classifier can expand word similarity to form SVM kernels based on the WordNet and word co-occurrence as introduced in Section 2.2, which performs better than linear SVM based on text feature. The WordNet based kernel yields similar performance with the co-occurrence based kernel in our experiment, and we integrate word co-occurrence based kernel in the T ext classifier using the approach [3] in our study.
4.3.1 Effect on effectiveness
Table 2 shows the performance on classification effectiveness of different frameworks. We find that the performance considering only content features is almost 50% lower than that of utilizing text features in general. There exists semantic gap between visual

content feature and video information, thus direct application of visual feature cannot achieve satisfactory results.

Table 2: Performance Comparison with Different Frameworks

Text Content

Fusion

CES

Macro-F .21 .11(-47%) .22 (+5%) *.25 (+20%)

Micro-F .28 .13 (-53%) .30 (+7%) *.32 (+15%)

F usion framework using fused classifiers outperforms the frameworks using text or content features, and it achieves 5% increase compared with T ext in terms of Macro-F score. As the Content classifier is much weaker than the T ext classifier, the fusion may also import noise, and hence degenerates the impact of fusion.
The proposed CES classifier outperforms the existing methods apparently by 20% than the T ext approach. In CES approach, we incorporate visual information into the semantic similarity measure between words rather than into the classifier directly. This design mechanism is expected to make use of visual clues of Web videos to obtain more reasonable semantic relations among words, which can better bridge the semantic gap between visual feature and video content. CES exploits the video similarity based on visual content and integrates both text and visual content feature for video classification. The effects of text and content features are typically complementary, and the combination can improve overall effectiveness. Additionally, the clustering of videos can help extract relations between words from different videos, and thus effectively addresses the problem of over-sparse kernel matrix in text categorization.

Table 3: F-Score (%) on Various Categories

Text

Fusion

CES

howto

15.5

*17.6

15.4

news

*45.4

39.5

40.7

comedy

10.4

15.9

*28.1

music

29.6

29

*30.2

travel

14.2

*31.3

24.6

animals

3.6

7.2

*7.4

education

15.7

20.7

*21

sports

20.7

18.4

*20.8

tech

18.6

23.6

*23.9

film

21.7

*22.9

20

entertainment 24.7

26.8

*28.4

games

3.7

3

*13.2

people

17.3

19.3

*19.7

nonprofit

23.4

18.5

*27.8

autos

49.1

47.5

*53.1

Macro-F

20.9

22.2

*25.1

Micro-F

27.7

29.5

*31.7

Table 3 shows the detailed F-score performance on all the categories of video dataset. Since the Content classifier performs much worse than other methods, we exclude it for comparison. We can see that CES performs the best in 11 out of 15 categories. The detailed performance comparison on F-score demonstrates the ad-

vantage of our methods.

4.3.2 Effect on Efficiency
Compared with classification accuracy, time cost is usually considered to be less important [21]. However, in the scenario of online video classification, it is essential for us to take the efficiency into consideration. This is because only with high efficiency can a realtime categorization approach provide satisfactory user experience. In this section, we only evaluate the time cost of the classification stage, as the building classifiers is done offline.
Text-based classifiers and CES classifier yield comparable efficiency and average classification time cost for an incoming video is less than 0.01 second. There exist some marginal computational cost differences due to the kernel density in SVM classifier. The

625

approaches based on content features, i.e., Content and F usion, perform significantly worse due to the expensive content feature extraction. The time cost of content feature extraction from video is typically of the same order of magnitude as video length, which makes it unacceptable to utilize the content-based video classification methods for Web video classification. Take Youtube as an example, more than 20 hours videos are uploaded per minute on average.
In a summary, compared with the state of the art approaches for video classification, our CES-based video classification method performs superiorly in terms of both effectiveness and efficiency . It utilizes the content features at the training stage, but it does not need to extract content features when classifying an new video in the Web. The proposed content-enriched kernel provides a promising solution for online video classification.
4.4 Performance on Multi-kernel
In the last set of experiments, we evaluate the classification performance on multi-kernel enhancement. Since the content based methods, e.g., Content classifier and F usion, perform much worse in terms of time efficiency, and thus are not applicable in online Web video classification scenario, we here only present the results on the combination of two approaches, i.e., T ext and CES.

Table 4: Performance Comparison with Multi-Kernel Solution

Text CES

Macro-F

Micro-F

Single



.21

.28

Kernel

  *.25 (+20%)* *.32 (+15%)

Multi-Kernel

*.28 (+33%)* *.34 (+21%)

The results on classification effectiveness are shown in Table 4. As been discovered earlier, the Multi-Kernel method that takes all of the similarity measures into consideration performs best for video classification. We can see that Multi-Kernel SVM can obtain the best performance, which is 33% better than Text-based classifier. This improvement not only demonstrates the great success of MultiKernel SVM in fusing different semantic kernels, but also indicates the different similarity measures may convey and capture different complementary features and they together can provide highperformance video classification. Note that, the Multi-kernel optimization will not introduce extra time cost at the classification stage, as only one joint kernel will be generated after Multi-kernel learning in the training stage.

5. CONCLUSION
In this paper, we presented a novel framework that efficiently exploits both visual content and text features to facilitate online video categorization. Within this framework, we proposed effective content-enriched semantic kernel which can extract word relationship by clustering the videos with visual content features. Extensive experimental results based on a large dataset demonstrate the superior performance of the proposed method in terms of both effectiveness and efficiency for video classification.
This work opens to several interesting directions for future work. Notably, it is interesting to investigate the performance issues of the current framework by using other content features of video, such as motion and audio features, to generate CES. Moreover, it will be interesting to adapt and apply CES for query expansion/suggestion in video retrieval.

6. ACKNOWLEDGMENTS
This research was supported by the National Natural Science Foundation of China under Grant No. 60933004 and 60811120098.

7. REFERENCES
[1] F. Bach, G. Lanckriet and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proc. of ICML Conference, 2004.
[2] S. Bloehdorn and A. Moschitti. Structure and semantics for expressive text kernels. In Proc. of CIKM conference, 2007.
[3] M. Cammisa, S. Bloehdorn, R. Basili and A. Moschitti. Semantic kernels for text classification based on topological measures of feature similarity. In Proc. of IEEE ICDM Conference, 2006.
[4] J. H. Chow, W. Dai, R. F. Zhang, R. Sarukkai and Z. F. Zhang. Joint categorization of queries and clips for Web-based video search. In Proc. of ACM MM Workshop on MIR, 2006.
[5] K. W. Church and P. Hanks. Word association norms, mutual information, and lexicography. In Computational Linguistics 16, 1990.
[6] C. Ciro, B. Dominik, H. Andreas and S. Gerd. Semantic Grounding of Tag Relatedness in Social Bookmarking Systems. In Proc. of ISWC Conference, 2008.
[7] N. Cristianini and J. S. Taylor. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000.
[8] F. J. Damerau, C. Apte and S. M. Weiss. Automated learning of decision rules for text categorization. In ACM Trans. Information Systems, vol. 12, no. 3, pp. 233-251, 1994.
[9] Z. Dong, G. Zhang, J. Jia and H. Bao. Keyframe-Based Real-Time Camera Tracking. In Proc. of IEEE ICCV Conference, 2009.
[10] C. D. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press, 1998.
[11] C.W. Hsu and C. J. Lin A comparison of methods for multiclass support vector machines. In IEEE Trans. on Neural Networks, vol. 13, no. 2, pp. 415-425, 2002.
[12] A. B. A. Graf and S. Borer. Normalization in Support Vector Machines. In Proc. of DAGM-Symposium on Pattern Recognition, 2001.
[13] G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui and M. Jordan. Learning the Kernel Matrix with Semidefinite Programming. In Journal of Machine Learning Research, Vol. 5, pp 27-72, 2004.
[14] C. Leacock and M. Chodorow. Combining local context and wordnet similarity for word sense identification. MITPress, 1998.
[15] R. Lienhart, S. Fischer and W. Effelsberg. Automatic recogition of film genres. In Proc. of ACM MM Conference, 1995.
[16] W. H. Lin and A. Hauptmann. News video classification using svm-based multimodal classifiers and combination strategies. In Proc. of ACM MM Conference, 2002.
[17] Y. Liu and Y. F. Zheng. One-against-all multi-class svm classification using reliability measures. In Proc. of IJCNN Conference, 2005.
[18] A. Moschitti. Efficient convolution kernels for dependency and constituent syntactic trees. In Proc. of ECML Conference , 2006.
[19] T. Mei, X. S. Hua, X. Yuan, W. Lai and X. Q. Wu. Automatic video genre categorization using hierarchical svm. In Proc. of ICIP Confernece, 2006.
[20] A. Rakotomamonjy, F. Bach, Y. Grandvalet and S. Canu. SimpleMKL. In Journal of Machine Learning Research, Vol. 9, pp 2491-2521, 2008
[21] F. Sebastiani. Machine learning in automated text categorization. In ACM Computing Surveys, 2002.
[22] J. Shawe-Taylor, N. Cristianini and H. Lodhi. Latent semantic kernels. In Journal of Intelligent Information Systems, 18(2-3):127-152, 2002.
[23] B. T. Truong, S. Venkatesh and C. Dorai. Automatic Genre Indentification for Content-based Video Categorizaion. In Proc. of ICPR Conference, 2000.
[24] P. Wang and C. Domeniconi. Building Semantic Kernels for Text Classification using Wikipedia. InProc. of SIGKDD conference, 2008.
[25] Z. Wu and M. Palmer. Verb semantic and lexical selection. In Proc. of ACL Conference, 1994.
[26] X. Yang, X.-S. Hua, L. Yang and J. Liu. Multi-modality Web video categorization. In Proc. of ACM MM Workshop on MIR, 2007.
[27] Y. Yang and X. Liu. A re-examination of text categorization methods. In Proc. of SIGIR Conference, 1998.

626

Robust Audio Identification for MP3 Popular Music
Wei Li, Yaduo Liu, Xiangyang Xue
School of Computer Science and Technology, Fudan University 825 Zhangheng Road, Shanghai 201203, P.R.China
weili-fudan@fudan.edu.cn duoyal@gmail.com xyxue@fudan.edu.cn

ABSTRACT
Audio identification via fingerprint has been an active research field with wide applications for years. Many technical papers were published and commercial software systems were also employed. However, most of these previously reported methods work on the raw audio format in spite of the fact that nowadays compressed format audio, especially MP3 music, has grown into the dominant way to store on personal computers and transmit on the Internet. It would be interesting if a compressed unknown audio fragment is able to be directly recognized from the database without the fussy and time-consuming decompression-identification-recompression procedure. So far, very few algorithms run directly in the compressed domain for music information retrieval, and most of them take advantage of MDCT coefficients or derived energy type of features. As a first attempt, we propose in this paper utilizing compressed-domain spectral entropy as the audio feature to implement a novel audio fingerprinting algorithm. The compressed songs stored in a music database and the possibly distorted compressed query excerpts are first partially decompressed to obtain the MDCT coefficients as the intermediate result. Then by grouping granules into longer blocks, remapping the MDCT coefficients into 192 new frequency lines to unify the frequency distribution of long and short windows, and defining 9 new subbands which cover the main frequency bandwidth of popular songs in accordance with the scale-factor bands of short windows, we calculate the spectral entropy of all consecutive blocks and come to the final fingerprint sequence by means of magnitude relationship modeling. Experiments show that such fingerprints exhibit strong robustness against various audio signal distortions like recompression, noise interference, echo addition, equalization, band-pass filtering, pitch shifting, and slight time-scale modification etc. For 5s-long query examples which might be severely degraded, an average top-five retrieval precision rate of more than 90% can be obtained in our test data set composed of 1822 popular songs.
Categories and Subject Descriptors
I.5.4 [Pattern Recognition]: Applications ­ signal processing
General Terms
Algorithms, Experimentation
Keywords
Audio identification, compressed domain, MDCT spectral entropy robustness, fragment retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

1. INTRODUCTION
Music identification is the most typical application of audio fingerprinting technique. By comparing the fingerprint of an unknown music query fragment, usually transmitted from mobile phones on the wireless telecom network or from personal computers on the Internet, with those previously calculated and stored in a fingerprint database, matching results and related metadata are returned. The fingerprint must characterize the nature of music content to differentiate from each other, and possess strong robustness against various kinds of audio signal degradations. Also, the query music fragment is usually required to be only a few seconds long, namely fit the demand of fragment retrieval. To date, a number of algorithms such as [1, 2, 3, 4, 5, 6] have been published with rather high retrieval precision. Most of them run on the raw wave format, and commercially deployed software systems also appeared [7].
However, with the mature of CD quality audio compression techniques at low bit rate and the fast growing of the Internet, compressed audio signals are increasingly ubiquitous and have become the dominant fashion of storing in personal electronic equipments and transmitting on the Internet. It would be interesting and meaningful in practice if audio features are directly extracted from the compressed domain and used for music identification in the database.
So far, only a few algorithms are designed to perform music information retrieval (MIR) directly in the compressed domain, research in this field is still in its infancy. Liu et al. [8] calculate the compressed-domain energy distribution from the output of the poly-phase filters as features to index songs. For each song in the data set, they use its refrain as the query example to retrieve all similar repeating phases, obtaining an average 78% recall and 32% precision rate. They claim that, to their knowledge, this is the first compressed-domain MIR algorithm. Lie et al. [9] directly use selected modified discrete cosine transform (MDCT) spectral coefficients and derived sub-band energy as well as its variation to represent the tonic characteristic of a short-term sound and to match between two audio segments. The retrieving probability achieves up to 76% among the top 5 matched. Tsai et al. [10] describe a query-by-example algorithm using 176 MP3 songs of a same singer as the database. They calculate spectrum energy from sub-band coefficients (SBC) to simulate the melody contour and use it to measure similarity between the query example and those database items. By summing up the sub-band coefficients in every 12 frames (about one tone duration) to obtain tone energy lines, the melody contour is represented by a string sequence with two letters (U, D), where `D' means the current tone energy is smaller than its preceding one and `U' means greater. With 40 frames assembled as a query example, the accuracy achieves 74% within top 4 and 90% within top 5. In Tsai's another paper [11], they use scale factors (SCF) and sub-band coefficients in a MP3 bit stream frame as features to characterize and index the object. All SCF and SBC values are divided into 26 bins using a tree-structured quantizer, values in the same bin are accumulated to form a

627

histogram as the final indexing patterns. Due to its statistical nature, this approach can tolerate certain length variance between the query example and database items. When length variance is between [0%, 10%), [10%, 15%), [15%, ), the query item can be obtained in top 5, 10, 15 results, respectively. Pye et al. [12] design a new compressed-domain audio feature referred to as MP3 cepstrum (MP3CEP) based on partial decompression of MPEG layer-3 audio signals to facilitate the management of a typical digital music library. It is approximately six times faster than conventional MFCC coefficient for music retrieval while the average precision is only 59%. Jiao et al. [13] design a robust compressed-domain audio fingerprinting algorithm, taking the ratio between the sub-band energy and the full-band energy of a segment as intra-segment feature and difference between continuous intra-segment features as inter-segment feature. Experiments show that such fingerprints are robust against transcoding, down sampling, echo addition and equalization. However, the authors didn't show any results on the retrieval precision rate.
The above methods acquired certain retrieval achievements, whereas they have two critical limitations. The first is that none of them works in the way of fragment retrieval, which is a basic requirement of audio fingerprinting systems; the second is that robustness is not considered in the above methods except reference [13] which shows some basic robustness results without taking account of the two most challenging distortions i.e. temporeserved pitching shifting and pitch-reserved time-scale modification (TSM). Moreover, previously used features principally follow the line of MDCT coefficient and its derived spectral energy, then can we develop a new type of compresseddomain feature to achieve high robustness in music identification? It's well known that entropy plays an important role in information theory as a measure of information, choice and uncertainty, it has been widely used in many fields like automatic speech recognition (ASR) [14], uncompressed-domain robust audio identification [15] and speech/music classification [16]. So far, various compressed-domain audio features including scale factors [17, 18], MP3 window-switching pattern (WSP) [19, 20], basic MDCT coefficients and derived spectral energy, energy variation, duration of energy peaks, amplitude envelope, spectrum centroid, spectrum spread, spectrum flux, roll-off, RMS, rhythmic content like beat histogram etc [21, 22, 23, 24, 25, 26] have been used in different applications such as retrieval, segmentation, genre classification, speech/music discrimination, summarization, singer identification, watermarking and beat tracing/tempo induction. However, in spite of the extensive use in various research fields for years, to the authors' knowledge, we are not aware of the usage of entropy for music identification in the compressed domain. This motivated our initial idea of developing MDCT spectral entropy as a novel audio feature for robust music identification.
In this paper, we first group 22 MP3 bit stream granules, the basic processing unit in decoding, into a longer window which is called `block' for the statistical purpose. Then we remap the 576 MDCT coefficients extracted from a granule of partially decoded MP3 songs into 192 new frequency lines to unify the frequency distribution of long and short windows. After dividing these new MDCT values into 9 new subbands in terms of the scale-factor bands of short windows, we calculate the spectral entropy of every block and acquire the final fingerprint sequence via magnitude relationship modeling. Experimental results show that this MDCT spectral entropy based fingerprint achieves high

robustness against common audio signal distortions like recompression, noise contamination, echo addition, equalization, band-pass filtering, pitch shifting and slight time-scale modification etc. A query example of 5s-long music, even if severely degraded by various time-frequency distortions, is sufficient to retrieve its original recording with an average topfive precision rate of more than 90% in our test data set composed of 1822 popular songs.
The remaining paper is organized as follows. Section 2 introduces the basic principles of MPEG Layer-3, bit stream data format, and the concept of compressed-domain spectral entropy. Section 3 details the steps of deriving MDCT entropy based audio fingerprint and the searching strategy. Experimental results on retrieval precision under audio signal distortions are given in section 4. Finally, section 5 concludes this paper and points out possible ways for future research.
2. COMPRESSED-DOMAIN SPECTRAL ENTROPY 2.1 Principles of MP3 Compression and Decoding
A simplified illustration of MPEG Layer-3 encoder is shown in Figure 1. The input PCM signal is mapped into 32 equalbandwidth subbands through a polyphase filterbank, which simulate the critical bands in the human auditory system (HAS). The sub-band outputs are further subdivided by MDCT transform using long or short window to provide better spectral resolution. The long window allows greater frequency resolution for audio signals with stationary characteristics, while the short window provides better time resolution for transients. Combined with other adjuvant techniques including psychoacoustic model, scalefactor, Huffman coding, quantization etc, the final compressed bit stream is generated [27]. Figure 2 displays the frame format of MP3 bit stream, and each frame has two granules to exploit further redundancies.
Figure 1. Simplified frequency transformations of MPEG Layer-3 encoder

628

Figure 2. Frame format of MPEG Layer-3 bit stream

In MP3 decoder, the basic unit of input bit stream is a granule of 576 samples, approximately 13 ms at the sampling rate of 44.1 kHz. One granule of compressed data is first unpacked and dequantized into 576 MDCT coefficients, then mapped to the polyphase filter coefficients in 32 subbands by IMDCT. Finally, these sub-band polyphase filter coefficients are inverse transformed and synthesized into PCM audio. Therefore, access of transformation coefficients in Layer 3 can be either at the MDCT or the filterbank level, the latter is obviously more time consuming.

2.2 Compressed-domain Spectral Entropy
Entropy is a fundamental concept in information theory to measure the uncertainty associated with a random variable [28]. If a random variable X is discrete with possible values {xi: i=1, ... , n}, the Shannon information entropy of X can be explicitly defined as,

n

 H ( X )   p( xi ) logb p( xi )

(1)

i 1

where p(x) denotes the probability mass function (PMF) of X, b is the base of the logarithm used and typically adopted as 2. With respect to a random signal, its entropy is a measure of how unpredictable it is. The entropy will be minimum when the signal is constant since it is most predictable and on the contrary maximum if the signal has a uniform distribution so that sample values are most unpredictable. For example, voiced sounds with clear formants have lower entropy; oppositely, flatter spectrum corresponding to non-speech or noisy regions has higher entropy [14]. Entropy is therefore expected to be different from those usual features derived from spectral energies and to capture the nature of audio content with a distinctive capability.
Follow this line of thought, we propose to calculate the MDCT spectral entropy as follows. Without loss of generality, we only take a set of MDCT coefficients X={x1, ..., xn} as an illustration, detailed formulation compliant with MP3 bit stream format will be described in the next section. In light of formula (1), the biggest problem to compute the entropy of a spectrum distribution lies in that spectrum itself does not possess the necessary property of a PMF, namely all the spectrum elements don't sum up to 1. In order to convert the original MDCT spectral coefficients into a PMF-like function, we divide every individual MDCT coefficient by the sum of all components, i.e. by sum normalization, to approximate its probability as shown in formula (2). Then formula (1) can be used to compute the MDCT spectral entropy.

p(xi ) 

xi
n

{xi : i  1,..., n}

(2)

 xi

i 1

As stated in the introduction, the goal we calculate MDCT spectral entropy is to use it as an audio feature for compresseddomain music identification, then is it steady enough under various audio signal distortions? We did some experiments to check it. Figure 3 shows the MDCT entropy sequence calculated as above from a 6s clip of compressed song after granule grouping and sub-band division as detailed in the next section. It can be clearly seen that the entropy curve is rather stable under volume modulation, echo addition, band-pass filtering, noise interference and MP3 recompression at 32kbps. For pitch shifting up to ± 10% and 10-band equalization, though relatively bigger entropy variation is introduced compared with the above, the basic profiles are maintained. When the example excerpt is slightly time-scale modified, ±2% in experiment, the entropy curve only translates a small distance along the granule/time axis with little change to the spectral entropy contour. These observed phenomena confirm our initial motivation, and herein MDCT spectral entropy displays great potential to become a powerful audio fingerprint.
Figure 3. MDCT spectral entropy curve under various audio signal degradations. From the first to the second row, the curve of a clip of 6s original audio and distorted versions under volume modulation, noise addition, band-pass filtering, lossy recompression, echo addition, pitch shifting, equalization and time-scale modification are drawn for comparison
3. PROPOSED METHOD
The whole algorithm includes five steps stated as follows.
3.1 Granule Grouping
Compared with conventional content-based music information retrieval, fragment input and robustness are known to be two crucial constraints on audio fingerprinting schemes to retrieve in a music database. If modeling with audio signal operations, the input fragmented query example can be obtained from the original music via random cropping plus other types of audio processing. Random cropping causes serious desynchronization problem between the input fingerprint sequence and its stored original version, posing a serious threat on the retrieval accuracy. In general, there are two effective mechanisms to resist time-domain misalignment: one is invariant feature, the other is implicit synchronization which might be more powerful than the former [29]. However, in the MPEG compressed domain, due to the inherent data nature of compressed bit stream and the fixed frame structure, it is almost impossible to extract meaningful salient points serving as anchors as in the uncompressed domain [30]. Therefore, designing statistically stable audio features and adopting heavy overlap between adjacent time-domain processing

629

units become the only two ways to counter desynchronization and fulfill the task of fragment retrieval in audio fingerprinting.
To achieve this end, we first need to calculate the audio feature in longer time duration to alleviate its fluctuation over time. In the proposed method, we group 22 granules, the basic processing unit in the MP3 compressed bit stream, into a so-called `block' as the basic time unit to compute audio features. Next, a hop size of 1 granule between two consecutive blocks is adopted, i.e. approximately 95% overlapped. Heavy overlapping is expected to alleviate the problem of time-domain misalignment. In the above case, the displacement between the query fragment and those original items will be no more than half the hop size, i.e. 1/2 granule will be the worst case. Figure 4 shows an illustration of the mechanism: for an original compressed bit stream, H(i) represents the start section of the ith block, H(i+1) represents that of the (i+1)th block (the whole block length is unable to be depicted due to space limitation). The hop size is 1 granule (95% overlap), e.g. A+B (A=B here). By reason of the time-domain desynchronization caused by random cropping etc, the input query example is scarcely possible to be exactly aligned to H(i) or H(i+1). When the query fragment lies on the left of the dashed line, for example QH(j), it resembles H(i) more; while when it lies on the right, for example QH(j'), it looks more like H(i+1). Only when the query fragment happens to lies in the middle, i.e. half of the hop size, it comes to the worst synchronization situation. We hope that a suitably designed audio fingerprint with statistical characteristics will only be slightly or even not changed under this scope of misalignment.
Figure 4. Desynchronization alleviation by overlapping between contiguous blocks
3.2 Frequency Alignment between Long and Short Windows
MP3 encoded bit stream is divided into many frames, which are the basic processing unit in decoding. Each frame is further subdivided into two independent granules, each with 576 values. If a granule is encoded using a long window, these values represent 576 frequency lines which are equally assigned into 32 subbands. That is, each subband includes 18 frequency lines. If a granule is compressed via a short window, these values only stand for 192 frequency lines and each line includes 3 values that belong to three consecutive windows respectively, see Figure 5.

Figure 5. Distribution of MDCT coefficients in long- and short-window type of granules

Granule grouping makes any block a mixture of long-window type of and short-window type of granules, the former are usually in the majority and the latter in the minority. In order to calculate the MDCT spectral entropy, the original frequency distribution of long and short windows must be rearranged to achieve approximately the same frequency resolution. For long-window cases, we group every three consecutive MDCT coefficients of one granule into a new value, which is equal to the mean of the absolute value of the original three MDCT coefficients considering that MDCT coefficients may be positive or negative, see formula (3). For short-window cases, we substitute the original three MDCT values belonging to different windows at the same frequency line with the mean of their absolute value, see formula (4). In this way, all MDCT values in a granule are uniformly divided into 192 new frequency lines for both long- and short-window cases, this forms the basis for further processing.

 sn(i,

j)



    

sn sn s

l (i, (i,

j)  j) 

1

3

j2
|

s(i,

n)

|

3 n3 j

1 2 | sm (i, j) 3 m0

|

j  0,1,2,...191 (3) j  0,1,2,...191 (4)

where s(i, n) is the original MDCT coefficient in the ith granule,

nth frequency line for the long-window cases; sm(i, j) is the

original MDCT coefficient in the ith granule, jth frequency line, mth

window for the short-window cases; snl(i, j) and sns(i, j) are the

new MDCT value in the ith granule, jth frequency line for the long-

and short-window cases, respectively.

3.3 New Subband Division
After time-domain granule grouping and the definition of new frequency lines, MDCT coefficients between the process of reordering and alias reduction of decoding are extracted for further processing. In MPEG Layer-3 compressed bit stream, most of the 576 MDCT coefficients in a granule are fed into a set of scale-factor bands, each band covers several coefficients and approximates certain critical band. The number of scale-factor bands depends on the sampling rate and the window type used in encoding. Granules using short window usually correspond to music edges such as transients or percussive instruments that are most crucial to auditory perception, therefore we divide the above newly defined 192 frequency lines into 9 subbands which cover the main frequency spectrum of popular music in terms of the scale-factor bands defined for short windows, as shown in Table 1.

630

Table 1. Division of new subbands

New Index of new Long window Short window

subband

MDCT

coefficients

Index of MDCT

Index of frequency

coefficients

lines

0

0-3

0-11

0-3

1

4-7

12-23

4-7

2

8-11

24-35

8-11

3

12-15

36-47

12-15

4

16-21

48-65

16-21

5

22-29

66-89

22-29

6

30-39

90-119

30-39

7

40-51

120-155

40-51

8

52-65

156-197

52-65

3.4 MDCT Spectral Entropy Calculation and

Fingerprint Modeling

With the above preparation, we first calculate the sub-band energy of the ith block jth new subband, each block includes 22 granules in this research. Then the PMF like ratio between the jth subband energy and the sum of all subbands in the same block is

approximately computed as shown in formula (5) and (6),

2i20 MDCTT j
SBE (i, j)    sn 2 (m, n)

m2i1 nMDCTB j

i  1,2,..., Nblock , j  0,1,...,8

(5)

P(i, j) 

SBE(i, j)
8

 SBE(i, j)

j 0

i  1,2,..., Nblock , j  0,1,...,8

(6)

where sn(m, n) denotes the nth new MDCT coefficient in the mth granule, MDCTTj and MDCTBj represent the top and bottom index of MDCT coefficients which belong to the jth subband respectively, and Nblock is used to indicate the maximum granule number of the input query example or original recordings stored in the database.
Finally, the entropy of the ith block is calculated in terms of formula (7) and the fingerprint sequence is obtained by comparing the magnitude of entropy between two adjacent blocks as shown in formula (8). This method is equivalent to a two-level quantization, maintaining the relative relationship regardless of the detailed magnitude. In this way, the final fingerprint sequence of ones and zeros will be not only more robust but also more convenient for fingerprint matching.

8

 H (i)   P(i, j)log2 P(i, j) i  1,2,..., Nblock

(7)

j 0

S

(i)



0 1

H (i)  H (i  1) H (i)  H (i  1)

i  1,2,..., Nblock 1

(8)

In section 2, we have demonstrated the invariance of this proposed compressed-domain spectral entropy under various time-frequency audio distortions through some experiments. Herein we will derive from a simplified theoretical way that H(i), the MDCT spectral entropy of P(i, j) in a block, has approximately the same stability with the variance of P(i, j). Based on the intuitive experience of human auditory perception,

there are always more and stronger notes distributed in the midfrequency range than in the high and low frequency regions. Therefore, energy of MDCT coefficients sn(i, j) and derived P(i, j) in each block should reflect this phenomenon and can be approximately modeled with Gaussian distribution. Let the mean

and

variance

of

P(i  j)

be

8
m(i)  P(i, j)/9

and

j0

(i)

8
 (P(i,

j) m(i))2

/9



after

rearranging

j

into

[-4,

+4]

and

j0

extending to ± (P(i, j)=0, j[0, 8])we have

 H(i)    

1

e ln[ 

(um(i))2 2 (i)2

1

e ]du 

(um(i))2 2 (i)2

2(i)2

2(i)2

 Eu{ln[

1

e ] } 

(um(i))2 2 (i)2

1

2(i)2



Eu[12

ln(2(i)2

)



(u  m(i))2 2 (i)2

]



1 2

ln(2(i)2)



Eu

(u  m(i))2 2 (i)2



1 2

ln(2(i)2)



 (i)2 2 (i)2



1 2

ln(2e (i)2 )

 ln( 2e)  ln((i)))

 ln( 2e) (i)

Obviously, for P(i, j) of the i-th block, its entropy and variance are in the same magnitude of order. It is known that variance is a typical statistic reflecting the overall fluctuation of random data, it will keep almost invariant unless undergoes strong distortions. From this point of view, the stability of the entropy based compressed-domain fingerprint is also demonstrated.

3.5 Fingerprint Matching

The emphasis of this paper is taking compressed-domain

MDCT spectral entropy as the key feature for audio fingerprint

deriving. As demonstrated above and in section 2, such kind of

feature is rather stable under common audio signal distortions and

slight time-domain misalignment like ±2% time-scale

modifications. By further modeling with the fault tolerant big and

small relationship between entropy of successive blocks, the

steadiness of derived fingerprints is further reinforced. Therefore,

by right of the power of the stable fingerprint, we can adopt a

relatively straightforward yet effective measure, i.e. Hamming

distance, to perform exhaustive matching between the query

example and those stored recordings. An illustration of the

matching procedure is shown in Figure 6. To explain more

clearly, let {x1, x2, ... , xn} be the input query fingerprint sequence

which

belongs

to

the

k-th

song,

{x1i

,

x2i

,...,

x

i N

}

be

the

stored

fingerprint sequence of the i-th song (

), Nsong be the number

of songs stored in the database, then the minimum bit error rate

(BER) of matching within a song is denoted as formula (9). The

total number of comparison within the database is

1

.

631

BER

(i

)



min((

x1

,

x

2

,...,

x

n

)



(

x

i j

,

x

i j

1

,...,

x

i j

n

1

))

/

n

i  1,2,..., N song j  1,2,..., N  n  1

(9)

bits are random i.i.d. (independent and identically distributed) and error bits can be further modeled by normal distribution. This is unfortunately not the case in reality due to the relevance incurred by large overlap between contiguous blocks. Therefore, we go through an experimental way to determine the BER threshold. For example, in experiment we exhaustively calculate the bit error rate of all possible pairs between 11 input distorted fingerprint sequence and those different fingerprints stored in database, obtaining 7951408 results as shown in Figure 7. Given a specific bit error rate as the threshold, we can count the number of falsely matched queries and then calculate the false positive rate. Some results are listed in Table 2, where we can see that FPRs corresponding to most thresholds are acceptable in practice, then which threshold is most appropriate?

Figure. 6 An illustration of the fingerprint matching procedure

Given a reasonable false positive rate (FPR), the threshold T of bit error rate can be derived from both theoretical and experimental ways to indicate under what condition a match can be viewed as true. Let BER(i') be the ascending reordered form of BER(i), namely BER(1')<BER(2')<BER(3')<BER(4')<BER(5')<...

<BER(Nsong') in which 1'  arg min(BER(i)  T ) , thus

is

i

deemed as the song that is most similar to the input query

fragment, associated metadata like lyric is often returned to users

in real software systems. Remember that k means the number of

the original song from which the query is excerpted, the overall

retrieval result is summarized in formula (10).

 top1

if k 1'

result 

  

top5 top10

elseif k 2',3',4',5' elseif k 6',7',8,9',10'

(10)

 failed

else

4. EXPERIMENTAL RESULTS
To test the proposed algorithm, we first set up a music database composed of 1822 distinct MP3 popular songs and a corresponding fingerprint database based on the procedures in section 3. Each song is mono, 30 seconds long, sampled at 44.1 kHz and compressed at 64 kbps, with a fingerprint sequence of 2280 bits long. To achieve a good tradeoff among fingerprint granularity, robustness and efficiency, we experimentally use 100 pieces of 5s-long audio signals as the query examples, which are composed of excerpts cut from selected database songs and their distorted copies. Each query example has a fingerprint of 366 bits.

4.1 BER Threshold Determination
Since we use BER as the metric to test fingerprint similarity (discrimination) and robustness, we have to first determine a reasonable threshold T based on the desired false positive rate (FPR) in real applications. It is insignificant to claim the robustness without taking FPR into consideration. For a query fingerprint and an equal-length part of a stored fingerprint, they are judged as similar in an auditory perceptual sense if the bit error rate is below the threshold T. Theoretical analysis on bit error rate has been studied in the literature, for example [1, 13]. However, this approach relies on the assumption that fingerprint

Bit Error Rate Figure 7. BER distribution of fingerprint pairs

BER threshold
0.3000 0.3100 0.3200 0.3300

Table 2. FPR vs. BER thresholds

False positive BER False positive

rate

threshold

rate

1.2576e-007 0.3400 3.5968e-005

3.7729e-007 0.3500 1.0350e-004

2.7668e-006 0.3600 2.5970e-004

1.0187e-005 0.3700 6.8969e-004

To help this selection, we did some experiments from another point of view to investigate the relationship between top-1 identification precision and the BER threshold T as shown in Figure 8. It can be seen that the change of T (0.30~0.37) doesn't affect the identification performance under common audio signal distortions, while it shows obvious influence on the identification results under time-scale modifications. The precision lines under TSM (the lowest three lines) go upwards slowly and monotonously when T increases from 0.30 to 0.33 and hereafter keep horizontal. That is, thresholds bigger than 0.33 doesn't contribute to the identification precision any more.
Generally speaking, bigger BER threshold will give better robustness results and meanwhile make the performance of false positive rate worse. To balance these two aspects, we adopt 0.33 as the BER threshold in this paper. Its corresponding FPR equals 1.0187e-005 that is fairish for practical audio identification applications

632

Top-1 Retrieval precision vs. BER threshold

100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0%

0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37

echo addition pitch shift(-10%) noise addition MP3 volume change TSM(-2%) TSM(-3%)

equalization pitch shift(+10%) band pass volume change TSM(+2%) TSM(+3%)

Figure 8. Identification results vs. BER thresholds
4.2 Retrieval Results under Distortions
To simulate the real world interference, we apply various audio signal operations on the compressed input query example using audio editing tools Cool edit and Gold wave. By their processing procedures to MP3 audio, the simulation is actually equivalent to decoding plus random cut plus audio signal processing plus recompression.
Given 100 randomly chosen query excerpts and 0.33 as the BER threshold, the top-1, 5 and 10 retrieval performance of this proposed algorithm under various audio distortions are averaged and shown in Figure 9. It can be seen that this MDCT entropybased fingerprint shows very good identification precision, even under severe audio signal distortions like lossy compression of MP3@32kbps-64kbps, volume modulation, obvious echo addition (100ms, 50%), noise interference with the signal noise ratio (SNR) as low as 15dB, equalization with a 10-band equalizer, band-pass filtering from 200 to 6000 Hz, tempo-reserved pitch shifting up to ±10% and slight pitch-reserved time-scale modifications at ±2%. In the above cases, the averaged top-1 precision results are all over 80% and top-5 over 90%. Overall, the robustness under various audio signal distortions is pretty good. The only blemish is that the top-1 result under TSM@-3% is not very satisfying with only 42% averaged precision, inferior by comparison with those state-of-the-art uncompressed-domain algorithms such as [4] and [5] which can resist TSM up to ±15% and -21% - +26% respectively. The weakness of compresseddomain algorithms is essentially caused by the fixed data structure of the MP3 compressed bit stream. In this case, implicit synchronization methods based on salient local regions like in [30] can't be applied. The only way to resist serious time-domain desynchronization is to increase the overlap between contagious blocks and design more steady fingerprints, whereas the overlap has an upper limit of 100% (95% has been used in this paper) and discovering more powerful features is neither an easy work.
Compared with other related compressed-domain algorithms introduced in the introduction [8, 9, 10, 11] whose best top-5 precision rate is 90% [10] under a clean environment (no experimental results under any time-frequency distortions are reported in the above papers), our algorithm obviously

outperforms those methods with the top-5 precision rates bigger than 90% even under severe audio signal distortions such as MP3@32kbps-64kbps, 10-band equalization, pitch shifting up to ±10% etc and the challenging time-scale modifications up to ±2% as shown in Figure 8. With respect to reference [13], to the authors' knowledge it is the only one published compresseddomain algorithm with some robustness reports including MP3@64Kbps, reverberation, echo, down sampling and equalization. However, it doesn't show any results against the two most challenging audio distortions, i.e. tempo-reserved pitch shifting and pitch-reserved time-scale modification, as we do in this paper. What is more important, the crucial parameter in audio identification, i.e. granularity, is not considered in any of the above algorithms. That is, they are not actually working in the way of fragment retrieval, this is an intrinsic deficiency compared with our method. Overall, this proposed method not only outperforms other existing compressed-domain audio retrieval systems, but also comes close to many state-of-the-art uncompressed domain algorithms for example [1, 4, 5, 6] in terms of robustness under common audio signal distortions except that results under serious time-scale modifications are worse due to the inability to use time-domain implicit synchronization method.
Retrieval results under various distortions
100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0%
top-1 top-5 top-10
Figure 9. Retrieval results under various time-frequency distortions
5. CONCLUSION
In this paper, we propose a novel compressed-domain audio fingerprinting algorithm for MP3 popular music identification. In virtue of the short-time steady property of MDCT spectral entropy and large overlap, a 5s query excerpt is shown being able to achieve promising results on robustness and retrieval precision rates under various time-frequency audio signal distortions including the challenging pitch shifting and time-scale modification. For future work, designing more statistically stable compressed-domain features and combining with this MDCT entropy-based feature using information fusion will be our main

633

approaches to improve the retrieval precision and robustness against large time-domain misalignment. Cover song identification performed right on the compressed-domain is our final aim to be accomplished.
6. ACKNOWLEDGMENTS
This work is jointly supported by NSFC(60873255), 973 Program(2010CB327906), Shanghai Leading Academic Discipline Project (B114).
7. REFERENCES
[1] J. Haitsma and T. Kalker, "A highly robust audio fingerprinting system, " proceeding of the international conference on music information retrieval, 2002, 107­115.
[2] S. Baluja and M. Covell, "Waveprint: efficient wavelet-based audio fingerprinting, " Pattern recognition, vol. 41, no. 11, 2008, 3467-3480.
[3] H. M. Yu, W. H. Tsai and H. M. Wang, "A query-by-singing system for retrieving karaoke music, " IEEE Transactions on multimedia, vol. 10, no. 8, 2008, 1626-1637.
[4] R. Bardeli and F. Kurth, "Robust identification of time-scaled audio, " proceeding of audio engineering society convention (AES 2004).
[5] F. Kurth, T. Gehrmann and M. Müller, "The cyclic beat spectrum: tempo related audio features for time-scale invariant audio identification, " proceeding of the international conference on music information retrieval (ISMIR 2006).
[6] J. Haitsma and T. Kalker, "Speed change resistant audio fingerprinting using autocorrelation, " proceeding of the international conference on acoustics, speech and signal processing (ICASSP 2003), 728-731.
[7] P. Cano, E. Batlle, T. Kalker and J. Haitsma, "A review of audio fingerprinting, " Journal of VLSI signal processing, vol. 41(3), 2005, 271-284.
[8] C. C. Liu and P. J. Tsai, "Content-based retrieval of MP3 music objects, " proceeding of the ACM international conference on information and knowledge management 2001, 506-511.
[9] W. N. Lie and C. K. Su, "Content-based retrieval of MP3 songs based on query by singing, " proceeding of the IEEE international conference on acoustics, speech, and signal processing (ICASSP 2004), 929-932.
[10] T. H. Tsai and J. H. Hung, "Content-based retrieval of MP3 songs for one singer using quantization tree indexing and melody-line tracking method, " proceeding of the IEEE international conference on acoustics, speech, and signal processing (ICASSP 2006), 505-508.
[11] T. H. Tsai and Y. T. Wang, "Content-based retrieval of audio example on MP3 compression domain, " proceeding of the IEEE workshop on multimedia signal processing (MSP 2004), 123- 126.
[12] D. Pye, "Content-based methods for the management of digital music," proceeding of the IEEE international conference on acoustics, speech and signal processing (ICASSP 2000), 24-27.
[13] Y. H. Jiao, B. Yang, M. Y. Li and X. M. Niu, "MDCT-based perceptual hashing for compressed audio content identification, " proceeding of the IEEE workshop on multimedia signal processing (MMSP 2007), 381-384.
[14] H. Misra, S. Ikbal, H. Bourlard and H. Hermansky, "Spectral entropy based feature for robust ASR, " proceeding of the

IEEE international conference on acoustics, speech, and signal processing, 2004, 193-196. [15] A. C. Ibarrola and E. Chavez, "A robust entropy-based audio fingerprint, " proceeding of the IEEE international conference on multimedia and expo,2006, 1729-1732. [16] J. Pinquier and R. André-Obrecht, "Audio indexing: primary components retrieval and robust classification in audio documents, " Multimedia tools and applications, vol. 30, 2006, 313­330. [17] R. Jarina, N. O'Connor, S. Marlow and N. Murphy, "Rhythm detection for speech-music discrimination in compressed domain," proceeding of the IEEE international conference on digital signal processing (DSP 2002), 129- 132. [18] K. Takagi and S. Sakazawa, "Light weight MP3 watermarking method for mobile terminals," proceeding of the ACM international conference on multimedia (ACM Multimedia 2005), 443-446. [19] Y. Wang and M. Vilermo, "A compressed domain beat detector using MP3 audio bit streams," proceeding of the ACM international conference on multimedia (ACM Multimedia 2001), 194-202. [20] A. D'Aguanno and G. Vercellesim, "Tempo induction algorithm in MP3 compressed domain," proceeding of the ACM international conference on multimedia information retrieval (ACM MIR 2007), 153-158 [21] G. Tzanetakis and P. Cook, "Sound analysis using MPEG compressed audio, " proceeding of the IEEE international conference on acoustics, speech, and signal processing (ICASSP 2000), 761-764. [22] C. C. Liu and C. S. Huang, "A singer identification technique for content-based classification of MP3 music objects, " proceeding of the ACM international conference on information and knowledge management 2002, 438 ­ 445. [23] C. C. Liu and P. C. Yao, "Automatic summarization of MP3 music objects, " proceeding of the international conference on speech, acoustics, and signal Processing (ICASSP 2004), 921924. [24] X. Shao, C. S. Xu, Y. Wang and M. Kankanhalli, "Automatic music summarization in compressed-domain, " proceeding of the international conference on speech, acoustics, and signal Processing (ICASSP 2004), 261-264. [25] R. Jarina, N. O'Connor, N. Murphy and S. Marlow, "An experiment in audio classification from compressed data, " proceeding of the international workshop on systems, signals and image processing (IWSSIP 2004). [26] A. Rizzi, N. M. Buccino, M. Panella and A. Uncini, "Genre classification of compressed audio data," proceeding of the IEEE workshop on multimedia signal processing (MMSP 2008), 654-659. [27] S. Pfeiffer and T. Vincent, "Formalisation of MPEG-1 compressed-domain audio features, " technical report number 01/196, CSIRO mathematical and information, sciences, Australia, 2001. [28] http://en.wikipedia.org/wiki/Information_entropy. [29] I. Cox, M. Miller, J. Bloom, J. Fridrich and T. Kalker, "Digital watermarking and steganography, " 2nd Edition, published by Morgan Kaufmann, 2007. [30] C. W. Tang and H. M. Hang, "A feature-based robust digital image watermarking scheme, " IEEE Transactions on signal processing, vol. 51, no. 4, 2003, 950­959

634

Effective Music Tagging through Advanced Statistical Modeling

Jialie Shen Meng Wang Shuicheng Yan HweeHwa Pang Xiansheng Hua
 School of Information Systems, Singapore Management University, Singapore {jlshen, hhpang}@smu.edu.sg
 Microsoft Research Asia, Beijing, China {mengwang, xhua}@microsoft.com
Department of ECE, National University of Singapore, Singapore eleyans@nus.edu.sg

ABSTRACT
Music information retrieval (MIR) holds great promise as a technology for managing large music archives. One of the key components of MIR that has been actively researched into is music tagging. While significant progress has been achieved, most of the existing systems still adopt a simple classification approach, and apply machine learning classifiers directly on low level acoustic features. Consequently, they suffer the shortcomings of (1) poor accuracy, (2) lack of comprehensive evaluation results and the associated analysis based on large scale datasets, and (3) incomplete content representation, arising from the lack of multimodal and temporal information integration.
In this paper, we introduce a novel system called MMTagger that effectively integrates both multimodal and temporal information in the representation of music signal. The carefully designed multilayer architecture of the proposed classification framework seamlessly combines Multiple Gaussian Mixture Models (GMMs) and Support Vector Machine (SVM) into a single framework. The structure preserves more discriminative information, leading to more accurate and robust tagging. Experiment results obtained with two large music collections highlight the various advantages of our multilayer framework over state of the art techniques.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval, Search process; I.2.m [Computing Methodologies]: Artificial Intelligence; H.5.5 [Sound and Music Computing]: Systems
General Terms
Algorithms, Design, Experimentation, Human Factors
Keywords
Music Information Retrieval, Tagging, Browsing, Search
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Music is an unique art form created by human to represent emotion, cultural background, social context and time. To facilitate music information retrieval (MIR) from large music collections, it is necessary to annotate the music documents with comprehensive textual information [2, 16, 24]. Existing music tagging approaches generally perform musical feature extraction, followed by applying machine learning methods to model the relationship between text labels and music features. The approaches hinge on two interrelated issues: (1) the extraction of high quality acoustic features to represent multiple music characteristics, and (2) the judicious application of statistical model(s) for classification and tagging. The effectiveness of the approaches should be demonstrated through a proper evaluation process involving large test collections and appropriate benchmarking metrics.
There has been a long history of using low level acoustic features extracted from audio objects as content descriptors [13, 3, 10, 9]. Unfortunately, how to effectively derive high-level semantic concepts (such as genre and mood) from the physical features still remains an extremely difficult problem. There are several reasons for this. First, there is a gulf between high level concepts and low level acoustic characteristics, as evident by the mismatch in semantic similarity in the search results produced by systems that rely solely on low level features [5]. Second, the content of music is rich and complex, spanning a wide range of features like timbral texture, harmony, rhythm structure and pitch [21, 27, 12, 19, 20, 30]. It is thus imperative to employ a content representation that captures these features comprehensively, and to determine which features to use for what purpose. In view of the challenges, it is not surprising that existing music tagging systems that adopt a simple approach of applying machine learning classifiers directly on low level acoustic features do not deliver good performance.
In this work, we propose a framework called MMTagger (Multifeature based Music Tagger) that combines advanced feature extraction techniques and high level semantic concept modeling for effective annotation of music documents. The basic idea for the proposed scheme is to model music information (text based description) with hierarchical structure and relationship between tags and concepts. It tries to map sound documents to a representation in the so-called latent musical concept space, where relevance between documents and tags can be more accurately modeled than in the

635

acoustic feature space. The MMTagger's architecture comprises three interconnected functionality layers. The technical design of the first layer aims at not only providing high quality feature combination but also to incorporate temporal information. The latter is motivated by the observation that music documents belonging to the same category generally share certain temporal patterns. The second layer of the proposed system is for discriminative musical concept modeling, and is intended to bridge the 'semantic gap' between the low level music features in the first layer, and the music tags in the third layer. Here, we utilize multiple Gaussian Mixture Models (GMMs) to represent different concepts [15, 7]. Since a semantic concept could be relevant to many different keywords, the third layer contains multiple support vector machines (SVM), each trained to derive the likelihood score of a tag from its association strength with the various music concepts. We have conducted a comprehensive experiment study with two large test collections. The results indicate that our solution achieves substantial performance improvement in accuracy and robustness in annotating music documents.
The rest of the article is structured as below: Section 2 gives a brief overview of related work in the area of music tagging, including their assumptions and limitations. In Section 3, we provide details on our proposed architecture and introduce the structure of each system component module and its learning algorithms. Section 4 reports on our experiment configuration while Sections 5 presents empirical evaluation results. Finally, our conclusions and directions for future research are summarized in Section 6.
2. RELATED WORK
Automated music tagging is an important research problem with numerous applications such as music search and music recommendation. This area has received considerable attention and many related techniques have been developed in recent years. Among the earliest of such systems, Whitman and Rifkin [29, 28] proposed a novel Regularized LeastSquares Classification (RLSC) based approach. The goal is to derive non-linear relationship between text captions and acoustic features in an efficient way. For performance evaluation, 255 songs from 51 performers are separated into training and testing sets with roughly equal size. Using the SVM classifier, accuracy achieved ranges from 0.0% to 38.9% depending on the terms used for evaluation process. In [23], Turnbull et al. applied a supervised multiclass na¨ive Bayes model to estimate relationship between musical sound and words. The features considered by this system can be classified into two categories - textual features and audio features (e.g., dMFCC and auditory filter-bank temporal envelope features). The test collection contains totally 2,131 songs and their song reviews. Using dMFCC feature, precision and recall rates achieved by the system is 0.072 and 0.119 for the annotation task. To facilitate effective music retrieval with semantic description, Turnbull et al. developed a music labeling scheme based on the supervised multi-class labeling model (SML) [26, 25]1. In this approach, sound documents are modeled as a GMM distribution over a set of predefined terms (corpus). The distance between the multinomial distributions of keyword query and a music feature can be estimated with the Kullback-Leibler (KL) divergence
1In this paper, we use MSML to denote this system.

Symbols
C s f F t T Gc wk k k K V |V | A M mss tss tes pt  r

Definitions
Total number of high level music concepts Notation of music segment s Notation of feature f Total number of acoustic features extracted Notation of tag t Total number of tags GMMs for music concept c Weight of the kth Gaussian component Mean of the kth Gaussian component Covariance matrix of the kth Gaussian component Number of mixture components in GMMs Vocabulary of test collection Size of vocabulary Annotation length Transformation matrix Music segment s Starting time of music segment s End time of music segment s Probability for music tag t Likelihood vector generated by DCML Tag relevance vector generated by TRL

Table 1: Summary of symbols and definitions

for the purpose of ranking search results. The acoustic feature considered in this system is MFCC. Using the CAL500 dataset, they achieved a nice performance improvement in retrieval and annotation accuracy. More recently, Duan et al. designed an interesting approach for collective annotation of music data [6]. It assumes that there are certain levels of correlation between different tags. Studying the relationship is useful for improving annotation performance. They employed two different statistical models - GMMs and Conditional Random Field to exploit the label correlation. Experiment results demonstrate a small but consistent performance gain. In addition, Bertin-Mahieux et al. proposed Autotagger system using advanced ensemble learning schemes to combine discriminative power of different classifiers [8, 4]. Those schemes include AdaBoost and FilterBoost. Acoustic features considered by the scheme include 20 MFCC Coefficients, 176 autocorrelation coefficients, and 85 spectrogram coefficients. Experiment results based on the CAL500 dataset and another large test collection demonstrate that Autotagger performs better than MSML. It is currently the most advanced technique for music tagging.
3. A TAGGING FRAMEWORK WITH MULTILAYER STRUCTURE
This section presents a novel scheme to facilitate effective automated tagging over large music collections. As illustrated in Figure 1, the architecture of our system consists of three functionality layers: music preprocessing layer (MPL) for music sequence segmentation and acoustic feature extraction, discriminative concept modeling layer (DCML) and SVM based tag refinement layer (TRL). Similar to an Artificial Neural Network [7], each layer is fully connected with each other. The first layer MPL aims to extract four different features including timbral feature, spectral feature, rhythm feature and melody feature. Using those features, a set of statistical models based on GMMs are constructed to characterize high level musical concepts in database, one GMMs per concept (e.g., genre, singer, mood). Those concepts can be treated as the most fundamental component of

636

Music Document 1 Music Document 2 Music Document i
Raw Music Signal Input

Music Segmentation

Timbral Feature Extraction
Spectral Feature Exraction
Rhythmic Feature Extraction
Melody Feature Extraction

GMMs for Concept 1
GMMs for Concept 2
GMMs for Concept 3
GMMs for Concept C

SVM for Tag 1 SVM for Tag 2 SVM for Tag 3 SVM for Tag 4

Relevance Score for Tag 1
Relevance Score for Tag 2
Relevance Score for Tag 3
Relevance Score for Tag 4

SVM for Tag T

Relevance Score for Tag T

Music Feature Extraction

Music Concept Modeling with Multiple GMMs

Music Tag Modeling with SVM

Figure 1: Architecture of MMTagger scheme.

latent musical concept space. The output of this layer is a set of likelihood scores, which serve as input to TRL. The TRL contains a collection of SVM based tag classifier, each is trained to generate relevance score for a tag. Based on the relevance scores, we can finally rank each tag and pick the top N tags to annotate the input music. The following sections elaborate on each of the layers and give a full description of the associated algorithms.
3.1 Music Preprocessing Layer
The function of this layer is to preprocess raw audio signal and compute music features. The related process comprises two steps: music segmentation and feature extraction. When an audio signal is received, it is first partitioned into several short fixed length time-frames. For this study, we set the length of each frame to be 0.5 second. Distinguished from previous tagging schemes, we introduce a temporal descriptor in the music content representation. Its main advantage is better content description capability through combining both acoustic information and temporal information. For a segment with starting time tss and end time tes, the corresponding temporal musical descriptor is defined as,

tdf (mi, mss) = extraf (mi, mss)

(1)

where tdf (mi, mss) denotes the feature f calculated from the segment mss = (tss, tes) of music file mi and extraf is an extraction function for feature f . Each music document is treated as a composite of different kinds of acoustic feature vectors with temporal information. The motivation derives from the observation that discriminative characteristics are often embodied within local temporal acoustic features. Thus the proposed temporal based feature enjoys greatest potential to provide more comprehensive summarization for the purpose of classification. The MMTagger system considers four different kinds of music features:

· Timbral features (TF) characterize the timbral property of music objects. We apply short time Fourier transform in the calculation. The timbral features

computed include Mel-Frequency Cepstral Coefficients (MFCCs) [13], Spectral Centroid, Rolloff, Flux, LowEnergy feature [27], and Spectral Contrast [14]. The total dimensionality of timbral features calculated is 20.
· Spectral features (SF) characterize the spectral composition of music signal. In our implementation. each spectral feature vector contains Auto-regressive (AR) features; Spectral Asymmetry, Kurtosis, Flatness, Crest Factors, Slope, Decrease, Variation; Frequency Derivative of Constant-Q Coefficients; and Octave Band Signal Intensities [14]. The total dimensionality of these feature vectors is 20.
· Rhythmic features (RF) summarize the patterns of a music object over a certain duration. The rhythmic features calculated in this study include: Beat Histogram [27]; Rhythm Strength, Regularity and Average Tempo [14]. The total dimensionality is 12.
· Melody features (MF) describe the pitch content and its duration in a music document. The Pitch Histogram proposed in [27] is used as melody features in our proposed framework. The total dimensionality of this group of features is 48.
Accordingly, the final content representation includes four different groups of musical features (local content information) and time information (temporal information). Total dimensionality of the feature set considered in this study is 100.
3.2 Discriminative Concept Modeling Layer
For the second layer of the proposed MMTagger system, multiple GMMs are trained to statistically model the relationship between each high level concept and various acoustic features. Those high level concepts constitute the latent musical concept space. Each high level music concept corresponds to one GMMs. GMMs is among the most widely applied statistical analysis methods due to its flexibility of

637

representing different kinds of distributions. However, gaining an accurate estimation of the distribution of the music features associated with to a high level concept goes beyond a straightforward application of the GMM method. While the distance between two music concepts can be estimated via the KL divergence between their GMMs, accurate result cannot be expected in general. There are two main reasons:

· Due to the limited number of learning examples (music clips) for a certain music concept, it is very hard to estimate the parameters of a GMM robustly and accurately.
· The KL divergence between GMMs does not take the concept label information into account and consequently can result in poor discriminative ability.
To solve those problems, we develop a two-step adaptation approach to construct the GMMs based on adaptive learning [1]. It includes generative adaptation and discriminative music concept adaptation. Generative adaptation has been widely explored in many tasks such as speaker identification and image categorization [11, 18]. It tries to use all learning samples in the training process and then obtain the Universal Background Model (UBM), which is the GMMs optimized by the principle of Maximum a Posteriori (MAP). In the second step, discriminative music concept adaptation is designed to adjust the mean vectors of each GMMs to achieve the targets of 1) keeping the music documents belonging the same semantic concept closer, and 2) separating out those with different labels. In this way, the obtained GMMs exhibit better classification capability.
3.3 Generative Adaptation
The UBM obtained in the initial phase of training the GMMs is denoted as,

X K

G = P (x|) = wkN (x; k, k)

(2)

k=1

where wk, k and k are the weight, mean and covariance matrix of the kth Gaussian component, respectively. x is input feature vector. K is the total number of Gaussian

components and the probabilistic density is calculated as a weighted combination of K Gaussian densities,

e-

1 2

(z-k

)T

- k 1

(z-k

)

N (x; k, k) =

(2)

d 2

|k

|

1 2

.

(3)

The parameters of UBM are estimated using the traditional EM algorithm. In the E-step, the posterior probability is calculated via

P r(k|xi) =

wkN (x; k, k) PK

,

(4)

wkN (x; k, k)

k=1

PK where nk = P r(k|xi) ,and the M-step updates the mean
k=1
vectors via

^k

=

1 nk +

r

X n P r(k|xi)xi
i=1

+

r nk +

r k

(5)

When EM iteration stops, the resulting GMMs is the Universal Background Model (UBM).
3.4 Discriminative Concept Adaptation
The purpose of discriminative concept adaptation is to estimate the GMMs parameter belonging to a certain music concept from a UBM. After it, a series of GMMs {G1, ..., GC } = {P 1(x|1), P 2(x|2), ..., P C(x|C )} are constructed with each GMMs approximating distribution over the audio feature space of a high level concept. A special transformation matrix M on the mean vectors of the GMMs is designed to enhance classification capability further. Since each high level music concept is represented by a GMMs, the distance between two concepts can be measured using the KL divergence between their GMMs. However, since the KL divergence of GMMs is not analytically tractable, we use the upper bound of the divergence for calculating distance. It can be proved that

X K

D(Ga||Gb) 

wkD(N (x; ak, k)||N (x; bk, k))

k=1



1 2

X K

wk (ak

-

bk )T

-k 1(ak

-

bk )

(6)

k=1

where ak and bk respectively denote the mean of the kth component from music concept a and b. The current model cannot yet achieve optimal classification performance because inter-class and intra-class distances are not taken into account. To improve its effectiveness, Neighborhood Component Analysis (NCA) is performed to derive transformation matrix M and apply it on D(Ga||Gb). NCA is a learning method, which constructs a distance metric optimizing leave-one-out (LOO) performance based on training data. An infinitesimal change in A may change the neighbor graph and thus lift LOO discrimination power by a finite amount. NCA adopts a more well behaved measure of nearest neighbor performance by introducing a differentiable cost function based on stochastic neighbor assignment in the transformed space. Each point i in multidimensional feature space selects another point j as its neighbor with certain probability pij, and inherits its class label from the selected point. pij is defined as

pij

=

Pe(-Dij ) k=j Djk

(7)

where Dij = ||M xi - M xj ||2. The objective is to maximize the expected number of samples correctly classified. Thus, we have,

X f (M ) =

X

ePxp(-Dij )

(8)

i jCi

k=j Djk

where Ci denotes the set of samples in the same class as i-th sample. After carrying out differentiation with respect to the transformation matrix M , we can obtain,

f M

X =-

X

X pij (qij - pikqik)

(9)

i jCi

k

where

638

qik = PK wk-k 1M (ak - bk)(ak - bk) k=1

(10)

The optimization problem above can be easily solved with a gradient descent process. After the training process, the GMMs for each high level music concept estimates the likelihood score c and this score is used to quantify the distance between raw feature input and concept label. At the same time, the output of DCML is a vector  that models probabilistic relationship between different music concepts and audio input, where  = [1, 2, ...., C]. It serves as input for tag refinement layer, the last layer in the MMTagger framework.

3.5 Tag Refinement Layer
The SVM based computational nodes constitute the Tag Refinement Layer (TRL) of the MMTagger framework. Each of them is designed to estimate the probability of a particular tag based on input from DCML. In the current implementation of MMTagger system, SVM is selected as a tag classifier due to its effectiveness [22].
Since traditional SVM can be used only for binary classification, the method proposed by Hastie and Tibshirani [17] is employed to derive numeric value for the probability that an unknown sample belongs to a certain tag. Its key idea is to adopt Gaussian distribution to model tag-conditional densities pt(|y = 1) and pt(|y = -1). Using Bayes' rule, the relevance score (posterior probability) rt for each given tag t can be computed via:

rt

=

Pt(y

=

1|)

=

P pt(|y = 1)Pt(y = 1) i=-1,1 pt(|y = i)Pt(y =

i)

(11)

where  = [1, 2, ...., C] is a vector generated by DCML. It contains a set of likelihood values describing the probability that an input music belongs to music concept c, where c = 1, ..., C. Using equation 11, a set of relevance scores r = [r1, r2, ..., rT ] are obtained for ranking tags. Eventually the top k tags are selected as annotation for input music, with value of k being predefined by the user.

4. EXPERIMENTAL CONFIGURATION
This section introduces the experiment configuration for our performance evaluation. We report details on two test datasets, performance metrics, competitors and evaluation methodology. All tagging methods evaluated in this study have been fully implemented and tested on a Pentium (R) D, 3.20GHz, 1.98 GB RAM PC running the Windows XP operating system.
4.1 Test Collections
Test collections play a very important role in empirical study. Two test collections are used in this evaluation. The first one (TS1) is the Computer Audition Lab 500-Song (CAL 500) data set developed by the CAL group [25, 26]. This collection contains 500 modern western music documents performed by 500 different artists. Altogether, there are 174 tags categorized into six different semantic groups including instrumentation, vocal characteristics, genre, emotions, solo and usage terms. For this dataset, we use those six groups as high level musical concepts to train our statistical model.

Since the size of the CAL500 data set is relatively small, we developed the second test collection called TS2. It contains 4000 popular music items downloaded from Youtube. They are performed by 110 different singers including 55 females and 55 males. The music documents are converted to 22050Hz, 16-bit, mono audio files. 12 amateur musicians, who are familiar with various music taxonomy and concepts, were hired to create ground truth about this collection. The ground truth information was generated by attaching a tag to a music item if at least three people assigned the tag to the song. In the case that an agreement on tag assignment between different respondents can not be reached, a similar resolution used in generating CAL500 is applied. At the end of the process, we obtain totally 250 tags, belonging to 8 different categories. They are instrumentation, emotions, country, time, genre, vocal characteristics, solo and usage terms. Consequently, our evaluation with TS2, involves 8 high level concepts. The size of the vocabulary |V | is 174.
4.2 Evaluation Metrics and Methodology
Textual information generated by music tagging systems can be used for many different MIR applications. To validate the performance of different tagging schemes, we select two MIR tasks:
· Task 1 - music annotation: for a given song track issued by the user, determine a proper set of tags. In this study, the size of tag set is 10.
· Task 2 - music search: for a given tag selected from the vocabulary, search for relevant song tracks in the test collection.
Here, three different evaluation metrics are used for music annotation. They are mean per-tag precision and recall, and the F-score. Based on the methodology used by Turnbull et al. [26], the top 10 tags generated by the models are used for comparison and thus annotation length A is 10. Per-tag recall and per-tag precision is formally defined as

P recision

=

|tT P | |tGT |

Recall

=

|tT P | |tA|

(12)

where |tGT | is the number of songs annotated with the tags in the human-generated "ground truth" annotation and tT P is the number of songs annotated correctly with the tags. Based on per-tag precision and per-tag recall, the F-score is defined as

F - score = 2 × P recision × Recall

(13)

P recision + Recall

To measure the performance of different approaches for music search, the mean average precision (MeanAP) and the area under the receiver operating characteristic curve (AROC) are adopted as assessment metrics. Given a query tag, the focus of MeanAP is on finding the most relevant songs, while AROC emphasizes whether relevant songs are ranked higher than irrelevant ones. We also apply -fold cross validation to ensure the stability and robustness of the empirical results.  is predefined to be 5.
In this study, we compare the performance of our system against two state-of-the-art approaches including Autotagger [8, 4] and MSML [26, 25]. Acoustic feature considered

639

by MSML is Mel-frequency cepstral coefficient (MFCC). Au-

Model

Precision Recall F-Score

totagger is evaluated based on three feature sets including MFCC delta, afeats and bfeats 2 For MMTagger, we consider

MSML Autotagger(MFCC delta)

0.121 0.257

0.043 0.072 0.102 0.101

five low level feature configurations (timber features denoted

Autotagger(afeats)

0.239 0.073 0.117

by TF, rhythm features denoted by RF, spectral features de-

Autotagger(bfeats)

0.268 0.139 0.186

noted by SF, melody features denoted by MF and timber fea-

MMTagger(ALL)

0.327 0.241 0.284

tures+rhythm features+spectral features+melody features

MMTagger(TF)

0.231 0.117 0.144

denoted by ALL.). Autotagger(MFCC delta), Autotagger(afeats)

MMTagger(SF)

0.220 0.116 0.139

and Autotagger(bfeats) denote Autotagger with MFCC delta,

MMTagger(MF)

0.207 0.103 0.125

afeats and bfeats respectively. MMTagger(TF), MMTag-

MMTagger(RF)

0.262 0.125 0.156

ger(SF), MMTagger(MF), MMTagger(RF), MMTagger(ALL)

denote our proposed model with timbral features, spectral

Table 3: Tagging accuracy on test collection TS2.

features, rhythmic features, melody features and the combi-

nation of all four musical features.

5.2 Result Analysis for Music Retrieval

5. EXPERIMENT RESULTS
This section presents an experiment study to evaluate the competing techniques on the task of music annotation, retrieval as well as music annotation in noisy environment.
5.1 Result Analysis for Music Annotation
We report a comparative study of the various tagging systems on music annotation processing. Table 2 and 3 summarize the empirical results for three systems with various configurations on the two test collections. The size of tag set is set to 10. Here, MMTagger is tested with five different feature settings is tested. The bottom four rows of both tables present the accuracy of our proposed system with just one acoustic feature. In comparison to MMTagger(ALL), they suffer from lower accuracy. In fact, the multiple feature combination achieves significant effectiveness gain ranging from 5% to 15%. The empirical results points clearly to the importance of combining features intelligently to tagging effectiveness. The experimental results also demonstrate that the MMTagger significantly outperforms the existing approaches. For example, in Table 2, comparing to Autotagger(bfeats), MMTagger(ALL) improves the precision from 0.291 to 0.351 for the CAL500 dataset, and from 0.268 to 0.327 for the TS2 collection. Similar observations can be made on the other two evaluation metrics over different test collections. We thus conclude that MMTagger emerges as the most effective music tagging scheme.

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

Precision 0.144 0.281 0.266 0.291 0.351 0.256 0.241 0.226 0.289

Recall 0.064 0.131 0.094 0.153 0.291 0.141 0.137 0.131 0.150

F-Score 0.089 0.179 0.139 0.205 0.314 0.176 0.165 0.149 0.171

Table 2: Tagging accuracy on test collection CAL500(TS1).

With the wide availability of large music collections, accurate music search is mandatory to achieve usability. This section presents empirical results to compare the accuracy of music retrieval facilitated by our proposed scheme and the two competitors. Experimental methodology is that given a keyword query kwq in vocabulary V , a test set of songs are ranked. The metrics MeanAP and MeanAROC of each ranking are calculated for performance comparison. Tables 4 and 5 summarize the experiment results with CAL500(TS1) and TS2. Clearly, the proposed MMTagger(ALL) significantly outperforms the other approaches.
In particular, the results shows that relative to Autotagger, MMTagger enjoys at least 10% MeanAP increase on both test collections. Although a nice improvement over Autotagger can be observed, we find that there is more significant gain over MSML. Averagely around 21% lift in term of accuracy can be found for the two datasets. In addition, we can summarize that for our proposed system, a proper integration of multiple music features can bring substantial improvement for search and annotation effectiveness. This observation corroborates other researchers' finding that accurate MIR can not be achieved with a single type of music feature and development of effective acoustic feature combination scheme plays important role for tagging system's performance enhancement.

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

MeanAP 0.231 0.305 0.323 0.340 0.410 0.282 0.275 0.286 0.288

MeanAROC 0.503 0.678 0.622 0.662 0.782 0.496 0.489 0.501 0.508

Table 4: Music retrieval accuracy on test collection CAL500(TS1).

2Detail information about those feature sets can be found in [26].
640

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

MAP 0.204 0.267 0.289 0.301 0.385 0.251 0.257 0.249 0.242

MeanAROC 0.461 0.613 0.597 0.626 0.753 0.449 0.452 0.467 0.472

Table 5: Music retrieval accuracy on test collection TS2.

5.3 Result Analysis for Noise Robustness
Modern MIR systems often need to work robustly in presence of ambient noise (e.g. raw music signal recorded from live concerts or other outdoor environments). However, existing schemes might not perform effectively when handling noisy audio input. Thus, it is important to evaluate the robustness of different music annotation schemes against music sources containing different kinds of audio distortion. In this work, we study how different types of noise in the query music affect annotation accuracy. We use the TS1 (CAL500) as evaluation data and apply the same set of test music as those used in the music annotation experiment. Before the test, various kinds of audio distortion are injected into each query music item. The distortion cases include 50% volume amplification, 50% volume deamplification, 10 second cropping, 35dB SNR mean background noise and 35db SNR white background noise3.
Tables 6-10 illustrate the noise robustness performance of MMTagger and its competitors for the different distortion cases. In general, the results show that when the music input is polluted by a certain kind of noise, annotation accuracy of all the systems suffers. Comparing to the other approaches, MMTagger demonstrates high resilience and stable performance. Specifically, MMTagger using single kind of acoustic feature suffers greater performance degradation than MMTagger with all four acoustic features. Moreover, crosschecking the results from this section and Section 5.1 reveals that MMTagger demonstrates more stable performance and enjoys less accuracy degradation than Autotagger and MSML under noisy circumstances. For example, MMTagger's precision decreases about 7% when annotating inputs with 50% volume amplification. Whereas, Autotagger and MSML suffer about 15% and 17% drop on average. We thus conclude that MMTagger is robust to different kinds of noise.

6. CONCLUSION

As a key enabling technology for music information retrieval, tagging has received a lot of research attentions in recent years. However, the performance of existing systems is far from satisfactory. In this paper, we describe a new music annotation scheme based on advanced feature extraction and multilayer structure. We have applied our method to two large test collections. Theoretical analysis and empirical results indicate that our approach achieves substantially

3SN RdB

=

10log10

S N

is the

equation

used to calculate

the

signal-to-noise ratio, where S denotes the signal power, and

N denotes the noise power in dB

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

Precision 0.121 0.241 0.226 0.252 0.325 0.206 0.201 0.216 0.253

Recall 0.053 0.112 0.084 0.130 0.271 0.121 0.107 0.101 0.117

F-Score 0.075 0.154 0.113 0.167 0.291 0.133 0.127 0.119 0.143

Table 6: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type - 50% volume amplification.

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

Precision 0.132 0.239 0.231 0.248 0.321 0.212 0.209 0.214 0.249

Recall 0.059 0.117 0.088 0.125 0.281 0.128 0.111 0.108 0.118

F-Score 0.071 0.167 0.119 0.171 0.295 0.131 0.119 0.121 0.145

Table 7: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type - 50% volume deamplification.

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

Precision 0.127 0.237 0.230 0.251 0.330 0.211 0.211 0.209 0.249

Recall 0.051 0.120 0.085 0.131 0.281 0.128 0.116 0.111 0.113

F-Score 0.080 0.150 0.121 0.162 0.211 0.135 0.120 0.121 0.145

Table 8: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type - 10 second cropping.

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

Precision 0.117 0.240 0.221 0.259 0.332 0.201 0.208 0.212 0.252

Recall 0.064 0.110 0.081 0.133 0.271 0.124 0.106 0.105 0.112

F-Score 0.081 0.160 0.109 0.171 0.291 0.130 0.129 0.111 0.146

Table 9: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type - 35dB SNR mean background noise

641

Model MSML Autotagger(MFCC delta) Autotagger(afeats) Autotagger(bfeats) MMTagger(ALL) MMTagger(TF) MMTagger(SF) MMTagger(MF) MMTagger(RF)

Precision 0.115 0.242 0.219 0.256 0.328 0.204 0.207 0.209 0.245

Recall 0.061 0.108 0.082 0.130 0.268 0.127 0.111 0.112 0.109

F-Score 0.083 0.156 0.107 0.176 0.287 0.127 0.125 0.113 0.140

Table 10: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type - 35dB SNR white background noise

higher accuracy in tagging music documents comparing to existing techniques. Moreover, our method demonstrates superior robustness against different kinds of audio distortion.
This work can be extended in several directions: At this stage, we have only tested our method on audio data. It would be very interesting to apply the method to data from other application domains (e.g. image and video retrieval) and investigate its performance characteristics. In addition, we plan to integrate more acoustic features into our framework. A natural question arises as to what kinds of feature combination is best in terms of effectiveness and robustness enhancement. Finally, designing a robust and effective evaluation methodology is also very important for further investigation and fair performance comparison.
7. ACKNOWLEDGEMENTS
Shuichang Yan is partially supported by AcRF Tier-1 Grant of R-263-000-464-112, Singapore.
8. REFERENCES [1] E. Alpaydin. Introduction to Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2004. [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley, 1999. [3] M. Bartsch and G. Wakefield. To catch a chorus: Using chroma-based representations for audio thumbnailing. In Proc. of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2001. [4] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere. Autotagger: A model for predicting social tags from acoustic features on large music databases. Journal of New Music Research, 37(2), 2008. [5] C. Dorai and S. Venkatesh. Bridging the semantic gap with computational media aesthetics. IEEE Multimedia, 10(2), 2003. [6] Z. Duan, L. Lu, and C. Zhang. Collective annotation of music from multiple semantic categories. In Proc. of ISMIR, 2008. [7] R. Duda, P. Hart, and D. Stork. Pattern Classification. John Wiley and Sons, 2001. [8] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. Automatic generation of social tags for music recommendation. In Proc. of NIPS, 2007. [9] H. Hermansky and N. Morgan. Rasta processing of speech. IEEE Transaction on Speech and Audio Processing, 2:578­589, 1994. [10] N. Hu, R. Dannenberg, and G. Tzanetakis. Polyphonic
audio matching and alignment for music retrieval. In Proc. of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pages 185­188, 2003.

[11] C. Lee, C. Lin, and B. Juang. A study on speaker adaptation of the parameters of continuous density hidden markov models. IEEE Transactions on Signal Processing, 39(4), 1991.
[12] T. Li, M. Ogihara, and Q. Li. A comparative study on content-based music genre classification. In Proc. of ACM SIGIR Conference, 2003.
[13] B. Logan. Mel frequency cepstral coefficients for music modeling. In Proc. of the ISMIR, 2000.
[14] L. Lu, D. Liu, and H. Zhang. Automatic mood detection and tracking of music audio signals. IEEE Trans. Acoust., Speech, Signal, 2006.
[15] G. McLachlan and D. Peel. Finite Mixture Models. John Wiley & Sons, 2000.
[16] N. Orio. Music retrieval: A tutorial and review. Foundations and Trends in Information Retrieval, 1(1), 2006.
[17] J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 2000.
[18] G.-J. Qi, X.-S. Hua, Y. Rui, J. Tang, Z.-J. Zha, and H.-J. Zhang. A joint appearance-spatial distance for kernel-based image categorization. In Proc. of CVPR, 2008.
[19] J. Shen, B. Cui, J. Shepherd, and K.-L. Tan. Towards efficient automated singer identification in large music databases. In Proc. of ACM SIGIR Conference, pages 59­66, 2006.
[20] J. Shen, J. Shepherd, B. Cui, and K.-L. Tan. A novel framework for efficient automated singer identification in large music databases. ACM Trans. Inf. Syst., 27(3), 2009.
[21] J. Shen, J. Shepherd, and A. H. H. Ngu. Towards effective content-based music retrieval with multiple acoustic feature combination. IEEE Transactions on Multimedia, 8(6):1179­1189, 2006.
[22] S. Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size. In Proc. of ICML, 2008.
[23] D. Turnbull, L. Barrington, and G. Lanckriet. Modeling music and words using a multi-class na¨ive bayes approach. In Proc. of ISMIR, 2006.
[24] D. Turnbull, L. Barrington, G. R. G. Lanckriet, and M. Yazdani. Combining audio content and social context for semantic music discovery. In Proc. of ACM SIGIR Conference, pages 387­394, 2009.
[25] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Towards musical query-by-semantic-description using the CAL500 data set. In Proc. of ACM SIGIR Conference, 2007.
[26] D. Turnbull, L. Barrington, D. Torres, and G. R. G. Lanckriet. Semantic annotation and retrieval of music and sound effects. IEEE Transactions on Audio, Speech & Language Processing, 16(2), 2008.
[27] G. Tzanetakis and P. Cook. Musical genre classification of audio signals. IEEE Trans. on Speech and Audio Processing, 2002.
[28] B. Whitman. Learning the meaning of music. PhD thesis, Massachusetts Institute of Technology, 2005.
[29] B. Whitman and R. M. Rifkin. Musical query-by-description as a multiclass learning problem. In Proc. of IEEE Workshop on Multimedia Signal Processing, 2002.
[30] B. Zhang, J. Shen, Q. Xiang, and Y. Wang. Compositemap: a novel framework for music similarity measure. In Proc. of ACM SIGIR, pages 403­410, 2009.

642

To Translate or Not to Translate?
Chia-Jung Lee, Chin-Hui Chen, Shao-Hang Kao and Pu-Jen Cheng
Department of Computer Science and Information Engineering National Taiwan University, Taiwan
{cjlee1010, chchen.johnson, denehs}@gmail.com, pjcheng@csie.ntu.edu.tw

ABSTRACT
Query translation is an important task in cross-language information retrieval (CLIR) aiming to translate queries into languages used in documents. The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another. Some untranslated terms cause irreparable performance drop while others do not. We propose an approach to estimate the translation probability of a query term, which helps decide if it should be translated or not. The approach learns regression and classification models based on a rich set of linguistic and statistical properties of the term. Experiments on NTCIR4 and NTCIR-5 English-Chinese CLIR tasks demonstrate that the proposed approach can significantly improve CLIR performance. An in-depth analysis is also provided for discussing the impact of untranslated out-of-vocabulary (OOV) query terms and translation quality of non-OOV query terms on CLIR performance.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithm, Experimentation, Performance
Keywords
Query Translation, Translation Quality, Query Term Performance, Cross-language Information Retrieval
1. INTRODUCTION
Query translation, which aims to translate queries in one language into another used in documents, has been widely adopted in CLIR. Conventional approaches to query translation have focused mainly on correctly translating as many query terms as possible, including translation disambiguation [3, 8, 9], phrasal translation [17, 11], and unknown
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

AP 0.65
0.6
0.55

0.5

Peru Fujimori bribery the 2000 exile abroad impeach Congress of

President

scandal election

Peru

Figure 1: AP for each untranslated query term.

words translation [7]. Such approaches pursue the reduction of erroneous or irrelevant translations in hope that the CLIR performance could approach to that of monolingual information retrieval (MIR). However, the accuracy of query translation is not always perfect. Each query term has a risk of being translated incorrectly. Some incorrect translations can be remedied in the process of MIR but others may cause irreparable retrieval performance drop. In other words, query translation may cause deterioration of CLIR performance. This phenomenon motivates us to explore whether a query term should be translated or not.
Consider the query: "Peru President, Fujimori, bribery scandal, the 2000 election, exile abroad, impeach, Congress of Peru", which is obtained based on the description field from a NTCIR-5 English-Chinese CLIR topic (after stop words removal). Its correct Chinese translations result in average precision (AP) of 0.5914 for CLIR. Figure 1 shows that if one of the query terms is not translated (x-axis), how the corresponding AP (y-axis) changes using the correct translations of the rest of terms as a query. It is observed that without correct translation of "Fujimor" or "bribery scandal", we are far from satisfying retrieval performance, compared to AP of 0.5914 (dash line). However, on the other hand, we find interestingly that if the correct translation of "Peru President" or "Congress of Peru" is ignored, a better AP can be even achieved. Still the missing of the translation such as "the 2000 election", "exile abroad", or "impeach" seems to be tolerable. This observation reveals that some untranslated terms cause irreparable performance drop while others do not. That is to say, the query terms are not equally important for translation, and it is not always the case that all translations are required.
In the above example, term "Fujimori" seems to bear more important semantics and thus should be translated. It might appear OOV terms always need perfect translations. Take into account the query from another NTCIR-5 English-Chinese topic (after stop words removal): "Chinese-American, scien-

651

tist, Wen-Ho Lee, suspect, steal, classified information, nuclear weapon, US's Los Alamos National Laboratory". It could be found that the AP decreases 45.9% when "WenHo Lee" is not translated, whereas untranslated "US's Los Alamos National Laboratory" conversely helps improve 39.6% of AP. Although missing the translation of "US's Los Alamos National Laboratory" loses some information about the query, we notice that term "" (laboratory) luckily emerges in its (post-translation) feedback documents, which alleviates the problem. Moreover, there are many possible transliterations of "Los Alamos" in Chinese such as "" and " ", which introduce a further mismatching problem in MIR and are harmful to the retrieval. This example illustrates that sometimes leaving an OOV term untranslated would probably be a reasonable choice.
Conventional approaches to query translation mostly put efforts in finding accurate translations [15] or examining how translation resources affect CLIR performance [12, 16]. Few did pose the problem of predicting CLIR performance or whether to translate a query term or not. Our most relevant work [10] presented a method to predict the performance of CLIR according to translation quality and ease of queries. Yet [10] focused merely on evaluating the performance of a whole query and did not give insight into the effect of translation for each query term. Also, [10] did not propose the issue of translation necessity which potentially helps improve retrieval performance.
The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another. We are interested in realizing (1) the possibility of predicting a query term to be translated or not; (2) whether the prediction can effectively improve CLIR performance; and (3) how untranslated OOV and various translations of non-OOV terms affect CLIR performance.
We propose an approach to estimate the translation probability of a query term according to its effect on CLIR. The translation probability serves as a basis for the decision to translate the query term or not. The proposed approach learns classification and regression models, where comprehensive factors that are essential in determination of CLIR performance are considered, inclusive of linguistic, statistical, and CLIR features in source and target language corpora. Experiments on NTCIR-4 and NTCIR-5 EnglishChinese CLIR tasks show that CLIR performance can be significantly improved based on our approach. Such effectiveness is consistent across different translation approaches as well as benchmarks. An in-depth analysis of how untranslated OOV terms and translation quality of non-OOV terms influence CLIR performance is also provided. We highlight that query terms needing no translation may result from intrinsical ineffectiveness in CLIR, semantic recovery by query expansion, or poor translation quality.
2. RELATED WORK
Improving translation accuracy is important for query translation. Phrasal translation approach [17, 11] was inspected for improving CLIR performance. Disambiguation of multiplesense terms by estimating co-occurrence for each chandidate[3] has also shown evident accuracy enhancement. Some others utilized statistical properties in parallel corpus [5, 13] as well as query expansion techniques [2] in search of better translation accuracy. Machine translation techniques [18, 14] are effective for long sentences, but they are not suit-

Source Query
Qs = {s1, s2, ... , sn} si

p(t j | si , Qs )

Translated Query
tj Qt = {t1, t2, ... , tm}

Figure 2: Basic query translation model.

able for short, context-inadequate queries. Though these works have brought significant improvement in translation accuracy, they eventually tried to translate as many terms as possible, which we believe is not always an effective approach in CLIR.
Our work is also related to term selection from a query. The focus of previous works[1, 4] did key-term selection in the mono-lingual environment; however, our discovery of various causes such as pre- and post-translation query expansion would influence the preference of translation in CLIR.
Our most relevant work [10] has developed regression models for predicting the performance of CLIR. The translation quality and ease of query were taken into account. Their concern was evaluated on a whole query, whereas we think every single term has its own impact on CLIR performance. Moreover, their method focused on the goodness of their regression models, while we aim to learn the need of translation for each term and bring up CLIR performance.
3. TRANSLATE QUERY TERMS OR NOT?
3.1 Estimation of Translation Probability
Given a query topic Qs = {s1, s2, ..., sn} in source language, conventional query translation methods endeavor to find a set of translated terms Qt = {t1, t2, ..., tm} in target language. Various translation methodologies such as phrasal translation or sense disambiguation have brought significant improvements in CLIR. Particularly, they incorporate dictionaries, bilingual corpora, or the Web to estimate the probability of translation p(tj|si, Qs). This probability shows how good it will be to translate si to tj given topic Qs, as shown in Fig. 2. p(tj|si, Qs) also means the translation depends on not only si and tj but the rest of terms in Qs. For simplicity, some previous works focused on p(tj|si) under assumption that the probability is irrelevant from Qs.
As illustrated in Fig. 1, the effect of query term translation may differ from one to another. Noticing this point, the goal of this paper does not focus on seeking high translation accuracy. Rather, we are interested in realizing, given a source term si, whether it should be translated or not. This can be casted as a classification problem by introducing a binary variable T  {0, 1}. T = 1 and T = 0 represent should-betranslated and should-not-be-translated, respectively, w.r.t. a given source term. When bringing variable T in estimation of p(tj|si, Qs), we get the following:
p(tj |si, Qs) = T p(T |si, Qs)p(tj |si, Qs, T ) = p(T = 0|si, Qs)p(tj |si, Qs, T = 0) +p(T = 1|si, Qs)p(tj |si, Qs, T = 1) = p(T = 1|si, Qs)p(tj |si, Qs, T = 1)
p(tj|si, Qs, T = 0) equals 0 because the probability of translation from si to tj is 0 given that si should not be translated. The final probability is composed of two parts. p(T = 1|si, Qs) estimates if it is suggested that si should

652

p(T = 1| si ,Qs )
si
Qs = {s1, s2, ... , sn}

p(t j | si ,Qs ,T = 1)

T

tj

Qt = {t1, t2, ... , tm}

Figure 3: Extended query translation model.

be translated, whereas p(tj|si, Qs, T = 1) shows how proper it is to translate si to some particular tj. This point is illustrated in Fig. 3 with the newly introduced variable T , where si is mapped to tj only if it is worth being translated (T = 1). The focus of previous work lies in generating translation equivalences based on p(tj|si, Qs, T = 1) or p(tj|si, Qs), since every si is required to be translated by default. Yet the goal of this paper is to predict the probability p(T = 1|si, Qs), which concerns whether to translate si or not.
Given Qs = {s1, s2, ..., sn}, we formulate our problem by seeking a classifier c : Qs  T , and the classification gives probabilities of 0 or 1 in the prediction process. To estimate real numbered probabilities, we resort to finding regression function r : Qs  R which predicts a value indicating the necessity of translating each si. With the regressed value, say r(si), as input of the Sigmoid function, we can obtain the probability of translating or not-translating ranged within [0, 1]. Mathematically,

p(T = 1|si, Qs) = 1/(1 + e-r(si))

With the probabilities (either binary or real numbered) in hand, we could use them in CLIR retrieval models by integrating p(T = 1|si, Qs) into p(tj|si, Qs). In this paper, we simply use p(T = 1|si, Qs) to translate worthily-translated terms from Qs. This approach enjoys the flexibility and extendability across various frameworks, because translating some portion of query terms is independent of what retrieval models are adopted. The probabilities based on Sigmoid function rank the source terms with a permutation  : s(1) > s(2) > ... > s(n) such that
p(T = 1|s(1), Qs) > p(T = 1|s(2), Qs) > ... > p(T = 1|s(n), Qs)

Based on classifier c, query terms are easily classified according to their needs of translation. Similarly, based on regression r, top k query terms {s(1), s(2), ..., s(k)} will be selected to be translated. Four different translation strategies and various threshold k's have been examined in Sections 4 and 5. Here, we apply support vector machine (SVM) and support vector regression (SVR) [6] to do classification and regression; other alternatives can also be adopted.
Finally, we define that the regressed value r(si) equals to LRCLIR(si) with the following formula:

r(si)

=

LRCLIR(si)

=

CLIR(Qs) - CLIR(Qs C LI R (Qs )

- {si})

where CLIR(q) is the AP measure for query q in CLIR. The larger the loss ratio LRCLIR(si) is, the more importantly we translate si due to its better effectiveness in CLIR.
We develop the regression function r : Qs  R by learning examples in the form of <f (si), LRCLIR(si)>, where f (si) is the set of features for si and will be described in Section 3.2. For classifier c : Qs  T , the form would be

<f (si), sign(LRCLIR(si))>, where sign(x) converts LRCLIR into non-negative and negative classes based on x.
3.2 Feature Set
We utilize linguistic (Ling), statistical (Stat), and CLIR features f (si) of query term si to capture its characteristics from different aspects. We denote tj as the corresponding translation of si in target language.
Table 1 shows all features adopted. Linguistic features include parts of speech (POS), named entities (NE), acronym, phrase, and size (number of words in a term). More precisely, the POS features contain noun, verb, adjective, and adverb, while the NE features comprise person names, locations, organizations, event, and time. POS and NE in our experiments are binary and are labeled manually.
Statistical features are good predictors from the viewpoints of document corpora. In our experiment, both source and target language corpora are used. Here, we consider cooccurrence, context, and TFIDF features. Co-occurrence features reveal the degree of how often a term co-exists with others, and hence the degree of semantic substitutions by them. The more a term can be replaced by others, the less likely it needs to be exactly translated. Pointwise mutual information (PMI) is adopted for calculation in pre- and post-translation corpora. Given two sets of terms x and y, we measure their co-existence level by
pmi(x, y) = log{p(x, y)/p(x)p(y)}.
In addition, context features are helpful for low frequency query terms that yet share common contexts in search results. The context vector v(t) takes term(s) t as input, and is composed of a list of search-result pairs <document ID, relevance score> returned by retrieval systems. Given two vectors x and y, we measure their cohesive level by
cos(x, y) = (x · y)/( x y ).
For those pairwisely computed sets in co-occurrence or context features, we extract their maximal, minimal, and average values as the features for the corresponding term.
TFIDF features show a term's capability of distinguishing relevant documents from irrelevant ones. We compute TFIDF in both source and target language corpora for each term.
CLIR features are the key to learning what characteristics make a term favorable or adverse for translation. We define translation, expansion, and replacement features.
Translation features such as the number of translations a term encompasses measure the degree of ambiguity according to dictionary knowledge. Also, we use binary feature isOOV to indicate if a term exists within the coverage of dictionary.
Expansion features express if the losing information from an untranslated term can be recovered by the semantics from the rest of terms with query expansion. Query expansion in source language reserves the room for untranslated terms by including relevant terms in advance. Also, query expansion in target language recovers the semantics loss by inspecting the rest well-translated terms. Here we denote QE(q) as the set of expanded terms obtained from the search results of query q by query expansion, and  can either be pmi() or cos() functions. From formulas in Table 1, the measurements estimate the similarity between expanded terms derived with or without term si. The same calculation is re-

653

Table 1: All features used in experiments

Type Feature Description

POS

noun, verb, adjective, adverb

Ling NE

person name, location, organization, event, time

Other

acronym, phrase, size

pmi(si, Qs - {si})

Co-

pmi(tj , Qt - {tj })

Stat

occurrence pmi(si, sp), (p = i , sp  Qs) pmi(tj , tq), (q = j , tq  Qt)

cos(v(si), v(Qs - {si}))

Context

cos(v(tj), v(Qt - {tj})) cos(v(si), v(sp)), (p = i , sp  Qs)

cos(v(tj), v(tq)), (q = j , tq  Qt)

TFIDF

Term frequency Inverse document frequency

CLIR

Translation Expansion

isOOV (binary OOV indicator) trans-size (# of translations) (QE(Qs - {si}), QE(Qs)) (QE(Qt - {tj}), QE(Qt))

Replacement

(Qs, (Qt,

QE(Qs QE(Qt

- -

{si}) {tj })

 

(Qs (Qt

- -

{si})) {tj }))

peated in target language for each tj. It is inferred that the more the two expanded sets of terms resemble each other, the more likely the loss information from untranslated si can be made up.
Lastly, replacement features estimate if the rest of terms, (Qs-{si}), within the same topic together with its expanded terms set, QE(Qs - {si}), can take the place of si. If the replacement intension is strong, it implies translation of only the rest of terms is sufficient for retrieval. In other words, QE(Qs - {si}) replaces the position of si in original query Qs, while QE(Qt - {tj}) substitutes the semantics of tj in query Qt.
4. EXPERIMENTS
4.1 Experimental Data
The data used in the experiments includes NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks, whose statistics in the title and description fields of English topics can be found in Table 2 (after data clean). The poorly-performing queries whose AP is below 0.02 are filtered to ensure the quality of our training data for classification and regression models. Table 3 shows the numbers of OOV and non-OOV terms in detail for each task. Note "term" refers to manual segmentation on original topic words after stop words removal, which forms a set of semantic-rich building blocks. We construct the vector space model (TFIDF), the language model (Indri), and the probabilistic model (Okapi) using the Lemur Toolkit1. Both queries and documents are stemmed with Porter stemmer and filtered with standard stop words lists. We use mean average precision (MAP) as performance metric evaluating over top 1000 documents retrieved. To avoid inside test, 5-fold cross validation is used through the entire experiments.
We use correct translations in the benchmarks to train the regression and classification models. The correct translations are available since NTCIR-4 and NTCIR-5 CLIR tasks
1Lemur Project: http://www.lemurproject.org

provide both English and Chinese topics at the same time. Usage of correct translations shall help reveal the necessity of translation. NTCIR-4 and NTCIR-5 CLIR tasks also provide English and Chinese documents, which are used as the source and target language corpora, respectively. Note that the English and Chinese documents are not parallel texts.

Table 2: Data set of English topics(after data clean)

Setting

# query # distinct # avg words

topics

words

per topic

NTCIR4

title desc

44 58

216 865

4.90 14.90

NTCIR5

title desc

35 47

198 623

5.65 13.20

Table 3: Numbers of OOV and non-OOV terms

Setting

# terms # OOV # non-OOV

NTCIR4

title desc

154 298

15 (9.8%) 139 (90.2%) 15 (5.0%) 283 (95.0%)

NTCIR5

title desc

131 277

27 (20.6%) 104 (79.4%) 36 (13.0%) 241 (87.0%)

4.2 Regression and Classification Performance
The coefficient of determination R2 measures how well future outcomes are likely to be predicted by the statistical models. A higher R2 gives us more confidence in prediction. We train and test the regression models under a variety of features and document collections. Table 4 demonstrates the R2 results.
Averagely speaking, the best regression performance can be achieved when both pre- and post-translation corpora is used, as query expansion is often helpful in CLIR. Also, it shows that a higher R2 can be found in post-translation corpus than in pre-translation one. This is because posttranslation corpus can provide more effective expanded terms for MIR in the set of target documents. Moreover, within each corpus setting, we go into details to inspect the effectiveness using different features. Statistical features consistently achieve better R2 than CLIR features, which are followed by linguistic features (R2 of linguistic features is the same across different corpora since such properties remain still despite change of languages). It is caused by that statistical features reflect the underlying distribution of translated terms in the document collection, and also that CLIR features reveal the degree of translation necessity. Finally, a larger R2 can be achieved by including more features for training. We also review the classification accuracy under the same settings, where similar results can be found. Roughly speaking, overall classification accuracy climbs up to 80.15% when all features are adopted. As linguistic, statistical and CLIR features are complementary, we use all of the features in the following experiments.
4.3 Feature Analysis
By inspecting correlation between the features and MAP, we may have better understanding of the effectiveness of our features. Three standard measurements inclusive of Pearson's product-moment, Kendall's tau and Spearman's rho are adopted.

654

Table 4: Regression performance under various feature sets and document collections

Pre-translation Corpus

Post-translation Corpus

Pre- and Post-translation Corpora

Model Topic Lin Stat CLIR All

Lin Stat CLIR All

Lin Stat CLIR All

Indri

Title 0.0657 0.5215 0.1720 0.8848 0.0657 0.3442 0.1726 0.8623 0.0657 0.9183 0.5773 0.9878 Desc 0.0472 0.1322 0.0417 0.5274 0.0472 0.1454 0.0887 0.5990 0.0472 0.4542 0.1793 0.9260

TFIDF

Title Desc

0.1780 0.0879

0.6872 0.2284

0.2767 0.0410

0.7718 0.8268

0.1780 0.0879

0.3379 0.3235

0.3023 0.2328

0.8555 0.8458

0.1780 0.0879

0.9611 0.8062

0.4611 0.2796

0.9712 0.9688

Okapi

Title 0.1163 0.6092 0.2154 0.7146 0.1163 0.4046 0.2650 0.8709 0.1163 0.8386 0.3948 0.9820 Desc 0.0406 0.0423 0.0083 0.3193 0.0406 0.0794 0.0455 0.4604 0.0406 0.3126 0.0766 0.9100

Avg. Title 0.1200 0.6060 0.2214 0.7904 0.1200 0.3622 0.2466 0.8629 0.1200 0.9060 0.4777 0.9803

Avg. Desc 0.0586 0.1343 0.0303 0.5578 0.0586 0.1828 0.1223 0.6351 0.0586 0.5243 0.1785 0.9349

Table 5: CLIR performance under various translation resources, document collections, query topics, and

prediction methods. T-test with p < 0.01 (**) and p< 0.05 (*) against baseline method

Okapi

Correct Trans

Google Dict Top1 Google Dict All Google Trans

Average

Title BL 0.2366

0.0902

0.0659

0.1692

0.1405

Title UB 0.2774

0.1088

0.0874

0.1966

0.1676

Title C 0.2475 (+4.60%) 0.1019 (+13.0%)

0.0785* (+19.2%) 0.1875 (+10.8%) 0.1539 (+9.52%)

Ntcir4

Title R Desc BL

0.2602** (+9.98%) 0.2121

0.1062* (+17.8%) 0.0876

0.0775 (+14.6%) 0.0671

0.1884* (+11.4%) 0.1576 (+12.2%)

0.1601

0.1317

Desc UB 0.3025

0.1347

0.1319

0.2168

0.1965

Desc C 0.2448* (+15.4%) 0.1003* (+14.5%) 0.0998** (+48.7%) 0.1803** (+12.6%) 0.1563 (+18.7%)

Desc R 0.2493** (+17.5%) 0.1073** (+22.5%) 0.0847** (+26.2%) 0.1856** (+15.9%) 0.1567 (+19.0%)

Title BL 0.3541

0.1376

0.1065

0.3089

0.2267

Title UB 0.4253

0.1552

0.1252

0.3496

0.2638

Title C 0.3945 (+11.4%) 0.1437 (+4.46%)

0.1136 (+6.68%) 0.3299* (+6.79%) 0.2454 (+8.22%)

Ntcir5

Title R Desc BL

0.4059** (+14.6%) 0.357

0.1546* (+12.3%) 0.1841

0.1235* (+16.0%) 0.3348* (+8.39%) 0.2547 (+12.3%)

0.0835

0.2728

0.2243

Desc UB 0.4788

0.2464

0.1893

0.3904

0.3262

Desc C 0.4349* (+21.8%) 0.2073** (+12.6%) 0.1484** (+77.7%) 0.3267** (+19.8%) 0.2793 (+24.5%)

Desc R 0.4363** (+22.2%) 0.2102** (+14.2%) 0.1348** (+65.7%) 0.3394** (+24.4%) 0.2810 (+25.3%)

Figure 4 depicts a wholesome picture of all features, where the absolute value of correlation using Okapi on NTCIR-4 data is shown. Clearly, classic TFIDF features show its discriminative power in identifying terms that need translation. Context features are effective through inspecting retrieval results, but such features meantime suffer from higher cost of computation. Another group of useful features are CLIR features. As mentioned previously, CLIR features are crucial for estimation of semantic recovery, which is captured by expansion and replacement features. It is worth noticing that the "isOOV" feature is evidently correlated to retrieval performance. It again assures that efforts in translating OOV terms are significant for CLIR, as indicated by previous work [7]. Lastly, "trans-size", which records the number of translations for each term, is negatively correlated to MAP (positive in Fig. 4 because of absolute value). The more senses (or translations) a term contains, the more challenging correct translation can be detected.
Linguistic features such as NE are relatively important for search. It is not always the case yet is usually true. A named entity often has unshirkable responsibility in describing the information needs especially for short queries. And this is why NE is much more correlated to MAP than POS is. Overall, statistical features are more powerful than linguistic ones. Specifically, context features are more effective than co-occurrence features. CLIR features also contribute substantially in translation estimation.
4.4 CLIR Performance
In this section, we show the effectiveness of our approach

for CLIR. We use NTCIR-4 and NTCIR-5 English-Chinese tasks for evaluation and consider both <title> and <desc> fields as queries. We use 5-fold cross-validation and ensure that a test instance would not appear in the training set.
Table 5 shows the MAP results using translated queries for search. Based on the pre-trained model, we'd like to test if we can improve the CLIR performance with 4 different translation strategies. Each strategy generates its own tj given source term si. "Correct Trans" gives the standard translations in the benchmark, which is also recognized as MIR; "Google Dict top1" extracts the first translation from Google Dictionary2; "Google Dict all" combines all possible translations from Google Dictionary for a given term; finally "Google Trans" returns the translations from Google Translation3. Moreover, for each setting, we show its baseline and upper bound performance. The baseline methods (BL) suppose entire terms in Qs need to be translated, and simply combine all the translated terms in Qt as one query string. For each topic in <title> or <desc>, there are in total 2m - 1 possible subclasses of Qt, considering that each si in Qs can be translated to tj or not. We construct the upper bounds (UB) by discovering the subclass (sub-query as well) with the highest AP. We also run the two-sample pairwise significance test against BL.
From Table 5, our classification (C) and regression (R) models consistently outperform the baseline methods using different translation strategies. The retrieval result proves
2Google Dictionary: http://www.google.com.tw/dictionary 3Google Translation: http://translate.google.com/?hl=en#

655

0.4

0.35

Pearson Kendall Spearman

0.3

0.25

0.2

0.15

0.1

0.05

0

noun verb
adj acronym
person org geo
event phrase
size tf_pre idf_pre tfidf_pre tf_post idf_post tfidf_post pmiinc_pre pmi_pre pmi_max_pre pmi_min_pre pmi_avg_pre pmi_max_pre_r pmi_min_pre_r pmi_avg_pre_r pmiinc_post pmi_post pmi_max_post pmi_min_post pmi_avg_post pmi_max_post_r pmi_min_post_r pmi_avg_post_r cosine_pre cosineinc_pre cosine_min_pre cosine_max_pre cosine_avg_pre cosine_min_pre_r cosine_max_pre_r cosine_avg_pre_r cosine_post cosineinc_post cosine_min_post cosine_max_post cosine_avg_post cosine_min_post_r cosine_max_post_r cosine_avg_post_r qe_cosine_pre qe_pmi_pre qe_cosine_post qe_pmi_post replace_cosine_pre replace_pmi_pre replace_cosine_post replace_pmi_post
oov trans_size

Figure 4: Absolute values of correlation using Okapi retrieval model on NTCIR-4 data set.

0.5 0.4 0.3 0.2 0.1
0 1 2 3 4 5 6 7 8 9 10

n4-title n4-desc n5-title n5-desc

Figure 5: MAP with various k (number of top terms translated) on different dataset.

our observation that some terms are "meant to be" translated while others are not. It is also worth noticing that the improvement rate of description queries is larger than title queries. As longer queries have more chances to encompass noisy terms, we can thereby improve retrieval performance by not translating them. Short queries such as Web queries, on the other hand, lose a great amount of information if a term cannot be well translated. Further, comparing the improvement rate between different translation strategies, we find that "Google Dict all" leaves the most room for improvement. We attribute this to the ambiguity it involves in due to inclusion all translations in dictionary. Fig. 5 illustrates the impact of the variable k.

5. TRANSLATION ANALYSIS
In this section, we discuss the effect of translating OOV and non-OOV query terms on CLIR. We will explore what factors make a query term favorable for translation (T=1), adverse for translation (T=0), or just somewhere in between.
Given a query topic Qs = {s1, s2, ..., sn}, we denote its correct translation as Qt = {t1, t2, ..., tn} where tj is the correct translation of sj. In the following, we will stick to LRMIR(sj) which indicates the necessity of translation based on sj, and is defined as

LRMIR(sj )

=

MIR(Qt) - MIR(Qt M I R (Qt )

- {tj}) ,

where MIR(q) is the AP measure for query q in MIR. LRMIR(sj) tells the influence of translating sj to tj. Terms

Table 6: Average raking percentages (ARP)(x100%)

and proportion of effective and ineffective OOV

terms (N4: NTCIR-4, N5: NTCIR-5)

Title

Desc

LRM I R

0

<0

0

<0

N4 ARP 0.6794 0.8750 0.2996 0.3095

N5 ARP 0.6507 0.6667 0.2705 0.5067

Prop.

83.3% 16.7% 76.5% 23.5%

Table 7: Classification accuracy with selected features (N4: NTCIR-4, N5: NTCIR-5)
N4<title> N4<desc> N5<title> N5<desc> OOV 90.66% 91.05% 93.82% 89.78% Non-OOV 77.33% 69.21% 78.13% 72.20%

with positive LRMIR(sj) are thought intrinsically-effective in target language and had better be translated.
5.1 OOV Terms Analysis
We discuss how untranslated OOV terms affect CLIR performance, and why some OOV terms are not required to be perfectly translated. All of the OOV terms appearing in <title> and <desc> from both NTCIR-4 and NTCIR-5 are collected. Table 3 shows the numbers in detail.
Firstly, based on the term ranking lists generated by regression function r, we calculate the ranking percentage for each OOV term. For example, if an OOV term is ranked at top 2 in a list of size 5, its ranking percentage equals (2/5)*100%. Following this manner, it is expected that effective terms (LRMIR > 0), i.e., the terms need to be translated, are usually ranked in front of ineffective ones (LRMIR  0) and thus have smaller average ranking percentage. Table 6 reveals the reliability of our ranking lists. In addition, it is worth noting that for longer queries such as <desc>, we can earlier discover should-be-translated terms, as the ranking percentage in <desc> is often smaller. The result is somehow not surprising since longer queries usually contain more noises.
By calculating the proportions of effective terms and ineffective terms, Table 6 tells that the less number of terms a query such as <title> includes, the more effective each query term is for retrieval (83.3%:16.7% vs 76.5%:23.5%).

656

0.6

0.4

0.2

0

-1.5

-1

-0.5 -0.2 0

-0.4

-0.6

-0.8

-1

-1.2

r1 LinearReg(r1)

0.5

1

1.5

y = -0.5993x - 0.2575

Figure 6: LRMIR (x-axis) versus r1 (y-axis).

4

3

2 y = 1.4163x - 0.4283
1

0

-1.5

-1

-0.5 -1 0

0.5

1

1.5

r2 -2
LinearReg(r2)

-3

Figure 7: LRMIR (x-axis) versus r2 (y-axis).

-3

-2

y = 0.455x - 0.0117

2

1

0

-1

0

-1

-2

-3

1

2

-1L.R5*2T9D033116 LinearReg(LR*TD )

Figure 8: LRMIR (x-axis) versus LRT D (y-axis).

cosine LinearReg(cosine)

-1.2

-1

-0.8

y = 0.8199x + 1.0252

-0.6

-0.4

-0.2

1.2 1
0.8 0.6 0.4 0.2
0 0

Figure 9: LRT D (x-axis) versus cosine (y-axis).

The result is consistent with [7]. The untranslated OOV terms in short queries, especially for title queries or Web queries, crucially influence retrieval performance.
Moreover, we show what factors (features) distinguish effective and ineffective OOV terms. We collect all OOV instances from the entire dataset to train a classifier. The upper part of Table 7 shows the cross-validation classification accuracy for OOV terms. When applying the greedy-hillclimbing-based method to decide the best feature set, we get top-ranked features, including replacement, expansion and TFIDF features. We propose the following formula to reveal the capability of these features in predicting the need of translation for OOV terms,
r1 = MIR(QE(Qt - {tj })  Qt - {tj }) - MIR(Qt) M I R (Qt )
r2 = MIR(tj ) - MIR(Qt - {tj }) M I R (Qt )
Measure r1 estimates how possible the loss semantics caused by untranslated sj can be recovered by other terms together with its post-translation expanded terms (expansion and replacement features). Figure 6 shows the relations between LRMIR and r1 for each sj. Interestingly, a negative correlation exists between the two variables. If OOV term sj is slightly effective (LRMIR is positive but close to 0) and cannot be translated, the semantics it carries may be rescued by expansion of the rest terms. An extremely-effective OOV term sj (LRMIR 0) is the term whose semantics cannot be recovered well (r1 0). For those ineffective OOV terms (LRMIR < 0), not-translating such terms is beneficial to CLIR performance.
Measure r2 captures the relevance of OOV term sj to the rest of the terms. Figure 7 shows a positive correlation between LRMIR and r2. Reasonably, an effective OOV term often has higher distinguishing power (TFIDF feature) in locating relevant documents compared to others. Consequently, our features explain why some OOV terms need to be translated while others do not. Especially, a difficult query term with low distinguishing power had better not

be translated even if its correct translation can be obtained. Also, for those which are effective for search, some terms are not necessary to appear in the query.

5.2 Non-OOV Terms Analysis
To understand the impact of different translations for nonOOV term sj, we define a translation loss ratio as follows:

LRT D(sj )

=

M I R (Qt

 {tkD(sj )} - {tj }) - MIR(Qt) M I R (Qt )

where tkD(sj) denotes the k-th translation given in translation resource D (Google Dictionary in our case). LRT D(sj) tells the influence of translating sj to tkD(sj) in CLIR. Translations with non-negative LRT D are regarded having good
translation quality, as they perform as well as or better than
correct translation in the benchmarks. We first focus on the most effective translation tD(sj) that
has the highest LRT D (i.e., LRT D) for each sj. From Fig. 8, LRMIR and LRT D exhibit a positive relation for each sj. We can see that an effective term (LRMIR > 0) usually has good translation quality (LRT D > 0) if one is able to find the best translation tD(sj) in D, and this is why these terms need to be translated. It also means that we will gain more performance boost (LRT D > 0) if we translate the shouldbe-translated terms (LRMIR > 0).
In addition, we apply feature selection to non-OOV terms
as what we do in the OOV analysis, and we find that statis-
tical features are the most important, including context and
co-occurrence features. The lower part of Table 7 demon-
strates the classification accuracy for all non-OOV terms.
Figure 9 shows the relations between LRT D and cos(Qt  {tkD(j)} - {tj}, Qt) for each sj (context feature). Assuming that translation tkD(sj) is extremely ineffective (LRT D 0) in CLIR, it can be inferred that tkD(sj) would be irrelevant to Qt - {tj }. Hence, tkD(sj) and (Qt - {tj }) together are dissimilar from Qt. We can see that a worse translation usually comes along with a weaker similarity to the original topic.

657

Context features indeed help distinguish diverse translation qualities and help in prediction of T .
6. CONCLUSIONS AND DISCUSSIONS
In this paper, we propose an approach to estimate the translation probability of query term si, p(T = 1|si, Qs), indicating if si should be translated or not. One of our merits is that we consider comprehensive factors including linguistic, statistical, and CLIR aspects to predict T . It shows that T is influenced by intrinsic ineffectiveness, semantic recovery by query expansion, or poor translation quality. We also verify that translating should-be-translated terms indeed helps improve CLIR performance across various translation methods, retrieval models, and benchmarks.
Realizing what factors determine translation necessity is important. For an OOV term, we discover that it does not always need translation, as sometimes translations in target corpus are ineffective or are irrelevant to original query. Specifically, leaving si untranslated could be a wise choice if its semantics could be recovered by pre- or post-translation expansion. For a non-OOV term, we show that if there exists an effective translation in dictionaries, it is suggested that translating si would help CLIR performance. Context features are useful for predicting translation quality. If the translations tend to deviate original intention, we'd better leave si untranslated. It is not worth taking a risk to translate a term if the term probably perform poorly in CLIR.
In brief sum, "to-translate-or-not-to-translate" is influenced by various and complicated causes. Some should-not-betranslated terms inherently suffer from their ineffectiveness in CLIR. Still others are affected by the translation quality obtained. These findings are consistent with [10]. Our approach could minimize the efforts of translation by selecting terms that really need it. This is especially helpful under condition that lexicon coverage or time constraint is limited, we can translate terms according to the ranking lists. Further, our approach can be easily extended to predict the effect of translating whole queries on CLIR as in [10].
We plan to train different models for OOV and non-OOV terms instead of a universal one, as they are intrinsically different from each other. We also want to explore how to automatically choose the best value for parameter k, which is anticipated to optimize the retrieval performance for each query topic. Despite the difficulty of automatic determination of k, it turns out that a fixed value 2 in <title> and 4 in <desc> work acceptably in our experiments. Finally, we need the Web corpus for calculating statistical features before applying our method to Web applications. We leave these limitations as our future work.
7. ACKNOWLEDGMENTS
This work was supported by the National Science Council, Taiwan, under contract NSC97-2221-E-002-222-MY2.
8. REFERENCES
[1] J. Allan, J. Callan, W. B. Croft, L. Ballesteros, J. Broglio, J. Xu, and H. Shu. Inquery at trec-5. In Proc. of the Fifth Text Retrieval Conference TREC-5, pages 119­132, 1997.
[2] L. Ballesteros and W. B. Croft. Dictionary methods for cross-lingual information retrieval. In Database and Expert Systems Applications, pages 791­801, 1996.

[3] L. Ballesteros and W. B. Croft. Resolving ambiguity for cross-language retrieval. In Proc. of ACM-SIGIR '98, pages 64­71, 1998.
[4] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. of ACM-SIGIR '08, 2008.
[5] J. Carbonell, Y. Yang, R. Frederking, R. Brown, Y. Geng, and D. Lee. Translingual information retrieval: A comparative evaluation. In Proc. of IJCAI, pages 708­715, 1997.
[6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[7] P.-J. Cheng, J.-W. Teng, R.-C. Chen, J.-H. Wang, W.-H. Lu, and L.-F. Chien. Translating unknown queries with web corpora for cross-language information retrieval. In Proc. of ACM-SIGIR '04, pages 146­153, 2004.
[8] M. Federico and N. Bertoldi. Statistical cross-language information retrieval using n-best query translations. In Proc. of ACM-SIGIR '02, pages 167­174, 2002.
[9] J. Gao, J.-Y. Nie, E. Xun, J. Zhang, M. Zhou, and C. Huang. Improving query translation for cross language information retrieval using statistical models. In Proc. of ACM-SIGIR '01, pages 96­104, 2001.
[10] K. Kishida. Prediction of performance of cross-language information retrieval using automatic evaluation of translation. Library & Information Science Research, 30(2):138­144, 2008.
[11] J. Kupiec. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proc. of ACL, pages 17­22. Association for Computational Linguistics, 1993.
[12] P. McNamee and J. Mayfield. Comparing cross-language query expansion techniques by degrading translation resources. In Proc. of ACM-SIGIR '02, pages 159­166, 2002.
[13] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In Proc. of ACM-SIGIR '99, pages 74­81, 1999.
[14] D. Oard. A comparative study of query and document translation for cross language information retrieval. Machine Translation and the Information Soup, pages 472­483, 1998.
[15] D. Oard and A. Diekema. Cross-language information retrieval. Anne Diekema, page 5, 1998.
[16] A. Pirkola. The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval. In Proc. of ACM-SIGIR '98, pages 55­63, 1998.
[17] F. Smadja, K. McKeown, and V. Hatzivassiloglou. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1­38, 1996.
[18] J. Zhu and H. Wang. The effect of translation quality in mt-based cross-language information retrieval. In Proc. of ACL, pages 593­600. Association for Computational Linguistics, 2006.

658

Multilingual PRF: English Lends a Helping Hand

Manoj K. Chinnakotla manoj@cse.iitb.ac.in

Karthik Raman

Pushpak Bhattacharyya

karthikr@cse.iitb.ac.in

pb@cse.iitb.ac.in

Department of Computer Science and Engineering Indian Institute of Technology, Bombay Mumbai, India

ABSTRACT
In this paper, we present a novel approach to Pseudo-Relevance Feedback (PRF) called Multilingual PRF (MultiPRF). The key idea is to harness multilinguality. Given a query in a language, we take the help of another language to ameliorate the well known problems of PRF, viz. (a) The expansion terms from PRF are primarily based on co-occurrence relationships with query terms, and thus other terms which are lexically and semantically related, such as morphological variants and synonyms, are not explicitly captured, and (b) PRF is quite sensitive to the quality of the initially retrieved top k documents and is thus not robust. In MultiPRF, given a query in language L1, it is translated into language L2 and PRF is performed on a collection in language L2 and the resultant feedback model is translated from L2 back into L1. The final feedback model is obtained by combining the translated model with the original feedback model of the query in L1.
Experiments were performed on standard CLEF collections in languages with widely differing characteristics, viz., French, German, Finnish and Hungarian with English as the assisting language. We observe that MultiPRF outperforms PRF and is more robust with consistent and significant improvements in the above widely differing languages. A thorough analysis of the results reveal that the second language helps in obtaining both co-occurrence based conceptual terms as well as lexically and semantically related terms. Additionally, the use of the second language collection reduces the sensitivity to performance of initial retrieval, thereby making it more robust.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval, Retrieval Models, Search Process
General Terms
Algorithms, Performance, Design, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
Multilingual, Pseudo-Relevance Feedback, Language Models, Query Expansion
1. INTRODUCTION
The central problem of Information Retrieval (IR) is to satisfy the user's information need, which is typically expressed through a short (approximately 2-3 words) and often ambiguous query. The problem of matching the user's query with the documents is rendered difficult by natural language phenomena like morphological variants, polysemy and synonymy. Relevance Feedback (RF) tries to overcome these problems by eliciting user feedback on the relevance of documents obtained from the initial ranking and then using it to automatically refine the query. Since user input is hard to obtain, Pseudo-Relevance Feedback (PRF) [4, 30, 19] is used as an alternative, where the RF is performed by assuming the top k documents from initial retrieval as being relevant to the query. Based on the above assumption, the terms in the feedback document set are analyzed to choose the most distinguishing set of terms that characterize the feedback documents and as a result the relevance of a document. The query refinement is done by adding the terms obtained through PRF, along with their weights, to the actual query.
Although PRF has been shown to improve retrieval effectiveness, it suffers from the following drawbacks: (a) due to the assumption inherent in the PRF process, i.e., relevance of top k documents, it is sensitive to the performance of the initial retrieval algorithm and as a result is not robust, and (b) the type of term associations obtained for query expansion is restricted to co-occurrence based relationships in the feedback documents, and thus other types of term associations such as lexical and semantic relations (morphological variants, synonymy), which are relevant in the context of the query, are not explicitly captured.
In this paper, we propose a novel approach called Multilingual Pseudo-Relevance Feedback (MultiPRF) to overcome both of the above limitations of PRF. We take help of a different language called herein the assisting language.
In MultiPRF, given a query in a source language L1, the query is automatically translated into the assisting language L2 and PRF performed in the assisting language. The resultant terms are translated back into L1 using a probabilistic bi-lingual dictionary. At the same time, a feedback model is also computed in L1 and finally combined with the feedback model obtained through the assisting language. The resultant model is finally used to re-rank the corpus and

659

fetch a new ranked list of documents. Experiments on standard CLEF [3] collections in languages with widely divergent characteristics such as French, German, Finnish and Hungarian with English as the assisting language show that MultiPRF achieves significant performance improvement over monolingual PRF. A point about why English is used as the assisting language is in order here. English shares about 72% of the web content. Larger coverage typically ensures higher proportion of relevant documents in the top k retrieval [12]. This in turn ensures better PRF. Assisting the fact is the other fact that query processing in English is a simpler proposition than in most other languages due to English's simpler morphology and wider availability of NLP tools for English.
A thorough qualitative analysis of the results reveal that MultiPRF indeed overcomes the fundamental limitations of PRF. Firstly, since it relies on the PRF in two collections of different languages, it is more robust. Secondly, the assisting language helps in obtaining both co-occurrence based conceptual terms as well as lexically and semantically related terms. The proposed approach is especially attractive in the case of languages where the original retrieval is bad due to poor coverage of the collection and/or inherent complexity of query processing (for example term conflation) in those languages. For example, Hungarian has only 0.2% share of web content1 with a rich morphology. Experiments also show that MultiPRF improves over monolingual PRF even when the query translation accuracy is sub-optimal.
The organization of the paper is as follows: In section 2, we discuss the related work in the area. Section 3 explains the Language Modeling (LM) based PRF approach which is used for performing monolingual PRF and which forms our baseline. We present the MultiPRF approach: our proposed model in Section 4. Section 5 presents the experimental set up and results followed by a discussion of these results in section 6. Finally, section 7 concludes the paper by summarizing observations and outlining possible directions for future work.
2. RELATED WORK
PRF has been effectively applied in various IR frameworks like vector space models, probabilistic IR and language modeling [4, 15, 17, 33]. Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) to refine the feedback document set [19, 24], (ii) refining the terms obtained through PRF by selecting good expansion terms [5] and (iii) using selective query expansion [1, 7] and varying the importance of documents in the feedback set [25]. Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms [32, 27]. The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift.
Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees et al. [28] use Wordnet for query expansion and report negative results. Recently, ran-
1http://www.netz-tipp.de/languages.html

dom walk models [16, 6] have been used to learn a rich set of term level associations by combining evidence from various kinds of information sources mentioned so far like WordNet, co-occurrence relationships, web, morphological variants etc.,. Metzler et al. [18] propose a feature based approach called latent concept expansion to model term dependencies.
All the above mentioned approaches use the resources available within the language to improve the performance of PRF. However, we make use of a second language (English) to improve the performance of PRF. As mentioned earlier, this is an attractive proposition for languages where the original retrieval is bad due to poor coverage and inherent complexity of query processing due to rich morphology, word compounding etc.
The idea of using one language to improve the accuracy of another language in a specific task has been successfully tried for the problem of Word Sense Disambiguation (WSD) [8].
A recent work by Gao et al. [11] uses English to improve the performance over a subset of Chinese queries whose translations in English are unambiguous. They use interdocument similarities across languages to improve the ranking performance. The computation of cross language document level similarities between English and Chinese documents is done using a bi-lingual dictionary. However, cross language document similarity measurement is in itself known to be an equally hard problem especially without using parallel or comparable corpora [10]. Moreover, the scale of their experimentation is quite small and they demonstrate their approach only on a small class of queries in a single language.

3. PRF IN LANGUAGE MODELING FRAMEWORK
The Language Modeling (LM) Framework for IR offers a principled approach to model PRF. In the LM approach, the document and query are modeled using multinomial distribution over words called document language model P (w|D) and query language model P (w|Q) respectively. For a given query, the document language models are ranked based on their proximity to the query language model, measured using KL-Divergence.

Rank(D, Q) = KL(Q||D)

=

X

P

(w|Q

)

·

log

P (w|Q) P (w|D)

w

Since the query length is short, it is difficult to estimate the query language model accurately using the query alone. In PRF, the top k documents obtained through the initial ranking algorithm are assumed to be relevant and used as feedback for improving the estimation of Q. The feedback documents contain a mix of both relevant and noisy terms. The actual relevant terms modeled using the feedback language model F is inferred from DF based on a Generative Mixture Model [33] formulation.

3.1 Mixture Model for Estimating Feedback Model
Let DF = {d1, d2, . . . , dk} be the top k documents retrieved using the initial ranking algorithm. Zhai and Laf-

660

Query in L1
Initial Retrieval Algorithm
(LM Based Query Likelihood)

Translated Query to L2 Initial Retrieval
Algorithm
(LM Based Query Likelihood)

L1 Index

Top `k' Results

Top `k' Results

L2 Index

Query Model
Q

PRF
(Model Based Feedback)

PRF
(Model Based Feedback)

Feedback Model L1

Feedback Model L2

Feedback Model Interpolation

Translated Feedback
Model

KL-Divergence Ranking Function

Relevance Model Translation
Probabilistic Dictionary
L2  L1

Final Ranked List Of Documents in L1

Symbol
Q FL1 FL2 TL1rans PL2 L1 , 

Description
Query Language Model Feedback Language Model obtained from PRF in L1 Feedback Language Model obtained from PRF in L2 Feedback Model Translated from L2 to L1 Probabilistic Bi-Lingual Dictionary from L2 to L1 Interpolation coefficients coefficients used in MultiPRF

Table 1: Glossary of Mathematical Symbols used in explaining MultiPRF

Source Term
French am´ericain nation e´tude German flugzeug spiele verh¨altnis

Top Aligned Terms in Target
English american, us, united, state, america nation, un, united, state, country study, research, assess, investigate, survey English aircraft, plane, aeroplane, air, flight play, game, stake, role, player relationship, relate, balance, proportion

Table 2: Top Translation Alternatives for some sample words in Probabilistic Bi-Lingual Dictionary

Figure 1: Schematic of the Multilingual PseudoRelevance Feedback Approach

ferty [33] model the feedback document set DF as a mixture of two distributions: (a) the feedback language model and (b) the collection model P (w|C). Assuming a fixed mixture proportion  in the feedback document set, the feedback language model is inferred using the EM Algorithm [9]. In the EM algorithm, the feedback model is iteratively refined by accumulating probability mass on most distinguishing terms which are more frequent in the feedback document set and less frequent across the entire collection. Let F be the final converged feedback model. Later, in order to keep the query focus, F is interpolated with the initial query model Q to obtain the final query model F inal.

F inal = (1 - ) · Q +  · F

(1)

F inal is used to re-rank the corpus using the KL-Divergence ranking function to obtain the final ranked list of documents. Henceforth, we refer to the above PRF technique by as Model Based Feedback (MBF).

4. MULTILINGUAL PSEUDO-RELEVANCE FEEDBACK (MULTIPRF)
In this section, we describe our main contribution - the Multilingual PRF approach. The schematic of the approach is shown in Figure 1.
Given a query Q in the source language L1, we automatically translate the query using a query translation system into the assisting language L2. We then rank the documents in the L2 collection using the query likelihood ranking function [14]. Using the top k documents, we estimate the feedback model using MBF described in the previous section. Similarly, we also estimate a feedback model using the original query and the top k documents retrieved from

the initial ranking in L1. Let the resultant feedback models be FL2 and FL1 respectively.
The feedback model estimated in the assisting language FL2 is translated back into language L1 using a probabilistic bi-lingual dictionary PL2L1 (f |e) from L2  L1 as follows:
P (f |TL1rans) = X PL2L1 (f |e) · P (e|FL2 ) (2)
 e in L2
The probabilistic bi-lingual dictionary PL2L1 (f |e) is learned from a parallel sentence-aligned corpora in L1 - L2 based on word level alignments. Tiedemann [26] has shown that the translation alternatives found using word alignments could be used to infer various morphological and semantic relations between terms. For example, in Table 2, we show the top translation alternatives for some sample words. For example, the French word am´ericain (american) brings different variants of the translation like american, america, us, united, state, america which are lexically and semantically related. Hence, the probabilistic bi-lingual dictionary acts as a rich source of morphologically and semantically related feedback terms. During the step for translating the feedback model given in Equation 2, the translation model adds related terms in L1 which have their source as the term from feedback model FL2 .
The final MultiPRF model is obtained by interpolating the above translated feedback model with the original query model and the feedback model of language L1 as given below:
M L1ulti = (1 -  - ) · Q +  · FL1 +  · TL1rans (3)
Since we want to retain the query focus during back translation the feedback model in L2 is interpolated with the translated query before translation. The parameters  and  control the relative importance of the original query model, feedback model of L1 and the translated feedback model obtained from L1 and are tuned based on the choice of collection in L1 and L2.

661

Language English
French

CLEF Collection Identifier EN-00+01+02 EN-03+05+06 FR-00 FR-01+02 FR-03+05 FR-06
DE-00

Description
LA Times 94 LA Times 94 + Glasgow Herald 95 Le Monde 94 Le Monde 94, French SDA 94 Le Monde 94, French SDA 94, 95 Le Monde 94, 95, French SDA 94, 95 Frankfurter Rundschau 94 Der Spiegel 94/95

Assisting Collection Used EN-00+01+02 EN-00+01+02 EN-03+05+06 EN-03+05+06
EN-00+01+02

German DE-01+02

Frankfurter Rundschau 94, Der Spiegel 94, 95, German SDA 94

EN-00+01+02

DE-03

Frankfurter Rundschau 94, Der Spiegel 94, 95, German SDA 94, 95

EN-03+05+06

Finnish FI-02+03+04

Aamulehti 94-95

EN-03+05+06

Hungarian HU-05

Magyr Hirlap 2002

EN-03+05+06

No. of Documents 113005 169477 44013 87191 129806 177452

No. of Unique Terms 174669 234083 127065 159809 182214 231429

CLEF Topics (No. of Topics) 1-40 (29) 41-140 (88) 141-200 & 251-300 (99) 301-350 (48)

153694

791093

1-40 (33)

225371

782304

41-140 (85)

294809 55344 49530

867072 531160 256154

141-200 (51) 91-250 (119) 251-300 (48)

Table 3: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final column CLEF Topics indicate the actual number of topics used during evaluation.

5. EXPERIMENTAL SETUP
We evaluate the performance of our system using the standard CLEF evaluation data [3] in four widely differing languages - French, German, Finnish and Hungarian using more than 600 topics. We use English as the assisting language. The details of the collections, their corresponding topics and the assisting collections used for MultiPRF are given in Table 3. Note that we choose the English assisting collection such that the coverage of topics is similar to that of the original corpus so as to get meaningful feedback terms. In all the topics, we only use the title field. We ignore the topics which have no relevant documents as the true performance on those topics cannot be evaluated.
We use the Terrier IR platform [21] for indexing the documents. We perform standard tokenization, stop word removal and stemming. We use the Porter Stemmer for English and the stemmers available through the Snowball2 package for French, German, Finnish and Hungarian. Other than these, we do not perform any other processing on German, Finnish and Hungarian. However, in French, since some function words like l', d' etc., occur as prefixes to a word, we strip them off during indexing and query processing, since that caused the baseline performance to decrease. We use standard evaluation measures like MAP, P@5 and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric [23] which is also used in the TREC Robust Track [27].
The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++ - a word alignment tool [20] on a parallel sentence aligned corpora. For French-English, German-English and Finnish-English language pairs, we used the Europarl Corpus [22] and in case of Hungarian-English, we used the Hunglish Corpus3.
We make use of off-the-shelf translation systems available in the above language pairs. We use Google Translate4 as
2http://snowball.tartarus.org/index.php 3http://mokk.bme.hu/resources/hunglishcorpus 4http://translate.google.com

the query translation system as it has been shown to perform well for query translation [29]. Later, we show that our approach is not dependent on Google Translate, and report results using a basic SMT system for query translation. For this, we evaluate the quality of the above Query Translation systems and analyze their impact on the quality of our results. In the pathological case of term not being found in English after query translation, we only perform MBF on the source language L1.
We use the MBF approach explained in Section 3.1 as a baseline for all our comparisons. We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection [34]. We tune the parameters of MBF, specifically  and , and choose the values which give the optimal performance on a given collection. We uniformly set the number of feedback documents, i.e., k as 10 i.e. top ten documents. The overall results are shown in Table 4. We observe that the optimal values of interpolation coefficients ,  in MultiPRF are almost uniform across collections and vary in the range 0.4-0.48.
6. RESULTS AND DISCUSSION
As in Table 4, the results show that the MultiPRF approach with English as the assisting language significantly outperforms the MBF approach across all datasets of all the chosen languages. We consistently observe significant improvements in MAP (between 4% to 8%), P@5 (between 4% to 39%) and P@10 (around 4% to 22%). The MultiPRF approach is also more robust than plain MBF as reflected in the improvements obtained in GMAP scores (between 15% to 730%). This could be attributed in part to the reduced sensitivity of our approach to the number of relevant documents in the feedback set of the source language. An analysis of the overall results reveal that MultiPRF leverages the performance in English language and adds relevant terms like morphological variants and synonyms in addition to co-occurrence based term relations. Besides this, it also improves the performance of some queries where the PRF performance was poor to start with, by bringing in related terms through PRF in L2 and back translation.

662

Collection FR-00 FR-01+02 FR-03+05 FR-06 DE-00 DE-01+02 DE-03 FI-02+03+04
HU-05

MAP

MBF MultiPRF % Improv.

0.4220 0.4393

4.10

0.4342 0.4535

4.43

0.3529 0.3837 0.2158 0.4229 0.4274

0.3694 0.4104 0.2273 0.4576 0.4355

4.67 6.97 5.31 8.2 1.91

0.3966 0.4246

7.06

0.3066 0.3269

6.61

P@5 MBF MultiPRF % Improv. 0.4690 0.5241 11.76

0.4636 0.4818

3.92

0.4545 0.4917 0.2303 0.5341 0.5098

0.4768 0.5083 0.3212 0.6000 0.5412

4.89 3.39 39.47 12.34 6.15

0.3782 0.4034

6.67

0.3542 0.4167 17.65

P@10

MBF MultiPRF % Improv.

0.4000 0.4000

0.00

0.4068 0.4386

7.82

0.4040 0.4625 0.2394 0.4864 0.4784

0.4202 0.4729 0.2939 0.5318 0.4980

4 2.25 22.78 9.35 4.10

0.3059 0.3319

8.52

0.3083 0.3292

6.76

GMAP

MBF MultiPRF % Improv.

0.2961 0.3413

15.27

0.2395 0.2721

13.61

0.1324 0.2174 0.0023 0.1765 0.1243

0.1411 0.2810 0.0191 0.2721 0.1771

6.57 29.25 730.43
9.19 42.48

0.1344 0.2272

69.05

0.1326 0.1643

23.91

Table 4: Results comparing the performance of MultiPRF approach over the baseline MBF approach on CLEF collections. Results marked as  indicate that the improvement was found to be statistically significant
over the baseline at 90% confidence level ( = 0.01) when tested using a paired two-tailed t-test.

TOPIC NO. ORIGINAL QUERY

TRANSLATED MBF ENGLISH QUERY MAP

MPRF MAP

MBF - Top Representative Terms (With meaning)

MultiPRF - Top Representative Terms (With meaning)

FRENCH '00. TOPIC 33

Tumeurs

et

génétique

Tumors and Genetics

0.0414

0.2722

malad (ill), tumeur (tumor), recherch (research), canc (cancer), yokoham, hussein

tumor (tumor), génet, canc, gen, malad, cellul (cellular), recherch

FRENCH '03. TOPIC 198

Oscar honorifique pour des réalisateurs italiens

Honorary Oscar for Italian filmmakers

0.1238

0.4324

italien, président (president), oscar , gouvern (governer) , scalfaro , spadolin

film, italien, oscar, honorair (honorary) , cinem (film), cinéast (filmmaker), réalis (achieve), produit(product)

FRENCH '06. Les Drogues AntiTOPIC 317 cancer

The Anti-Cancer Drugs

0.001

0.1286

drogu (drugs), anti , trafic (trafficking), entre (between), légalis (legalise), canc , malad , cocaïn , afghanistan , iran

canc, drogu, recherch, malad, trait, taxol, glaxo, cancer

GERMAN

prozent (percent), unterstutz (supporters), frau statist, scheidung , zahl (number), elt (parent),

'02. TOPIC Scheidungsstatistiken Divorce Statistics 0.2206 0.4463 (woman), minderjahr (underage), scheidung

kind (child), famili, geschied (divorced),

115

(divorce)

getrennt (separated), ehescheid (divorce)

GERMAN '03. TOPIC Ölunfälle und Vögel 147

Birds and Oil Spills 0.0128

0.1184

rhein (rhine), olunfall (oil spill), fluss (river), ol (oil) heizol (fuel/oil), tank (tanker)

,

ol, olverschmutz (oil pollution), vogel (bird), erdol (petroleum), olp (oil slick), olunfall , gallon, vogelart (bird species)

FRENCH '05. TOPIC 274

Bombes actives de Seconde Guerre Mondiale

la

Active bombs of the Second World 0.6182 War

0.3206

bomb, guerr(war), mondial (world), vill(city), découvert(discovery), second, explos, alemagn (germany), allemand (german)

guerr, mond(world), deuxiem (second), activ, bombard, japon(japan), hiroshim, nagasak, atom, nucléair (nuclear)

GERMAN '03. TOPIC 188

Deutsche Rechtschreibreform

German spelling reform

deutsch, reform, spiegel (reflect),

0.8278

0.6776

rechtschreibreform (spelling reform), sprach (language), osterreich (austria), rechtschreib

(spelling), wien (vienna), schweiz (switzerland)

deutsch, reform, deutschland (Germany), clinton, deutlich (clearly), president , berlin, europa, gipfel(summit), bedeut(important)

Table 5: Qualitative comparison of feedback terms given by MultiPRF and MBF on representative queries where positive and negative improvements were observed in French and German collections.

To illustrate the qualitative improvement in feedback terms, a detailed analysis of a few representative queries is presented in Table 5. Based on the above analysis, the improvements obtained by MultiPRF approach could be mainly attributed to one of the following three reasons:- (a) Retrieval Performance in L2 is good and the resultant feedback model contains a lot of relevant terms, which when brought back to L1 via back-translation leads to improvement. (b) During the back-translation process, important synonyms and popular morphological variants (inflectional forms) of key terms are found, which otherwise were missing from the ModelBased feedback model. and (c) A combination of both the above factors.
For example, consider the French Query "Oscar honorifique pour des r´ealisateurs italiens", meaning "Honorary Oscar for Italian Filmmakers". Model Based Feedback on French expands the query using the top retrieved documents of the initial retrieval. However, here it introduces significant topic drift towards Oscar Scalfaro (a former Italian President) and Italian politics thus causing words such as

{scalfaro, spadolin, gouvern}. However, feedback in English produces relevant terms, which on translation back into French, introduces terms such as {cinem, cin´east, r´ealis}. This wrenches back the focus of the query from the political domain to the intended film domain, thus leading to performance increase. Another example of this phenomenon is the query "Les Drogues Anti-Cancer" (Anti-Cancer Drugs). Here too MBF causes drift away from the intended meaning and instead to Drug-Trafficking, by introducing terms such as {traffic, entre, afghanistan}, which causes very poor performance on the query. MultiPRF however utilizes the good feedback performance of English on this query, to generate a set of very relevant French terms such as {recerch, taxol, glaxo}. Hence the drift from the intended meaning towards drug-trafficking is corrected, by the introduction of the above mentioned terms, which help in bringing up the performance on this query. These examples demonstrate the robustness of the MultiPRF approach and the reduced sensitivity to the relevance of the top documents from the initial retrieval.

663

MAP

P@5

P@10

GMAP

MPRF MPRF MPRF

MPRF MPRF MPRF

MPRF MPRF MPRF

MPRF MPRF MPRF

MBF SMT GT Ideal MBF SMT GT Ideal MBF SMT GT Ideal MBF SMT GT Ideal

FR-01+02 0.4342 0.4494 0.4535

FR-03+05 0.3529 0.3576 0.3694

DE-01+02 0.4229 0.4275 0.4576

DE-03

0.4274 0.4236 0.4355

0.4633 0.4636 0.4818 0.4818 0.4864 0.4068 0.4239 0.4386

0.3762 0.4545 0.4707 0.4768 0.4889 0.404 0.4141 0.4202

0.4639 0.5341 0.5523

0.6

0.6 0.4864 0.5125 0.5271

0.4388 0.5098 0.5294 0.5412 0.5451 0.4784 0.4863 0.498

0.4477 0.2395 0.245 0.2721 0.4323 0.1324 0.1329 0.1411 0.5386 0.2492 0.2032 0.2721 0.4922 0.1243 0.1225 0.1771

0.2965 0.1636 0.2816 0.1981

Table 7: Results comparing the performance of MultiPRF approach over the baseline MBF approach with Google Translate and another SMT system trained using Europarl corpus.

Corpus
FR-01+02 FR-03+05 DE-01+02 DE-03

Google Translate
0.93 0.88 0.93 0.81

SMT
0.67 0.77 0.64 0.58

Table 6: Comparison of Query Translation Quality using Google Translate and SMT system trained on Europarl Corpus on a scale of 0-1.

Apart from this we also see improvements on queries due to introduction of synonyms and other semantically related terms. For example, on the German query "O¨lunf¨alle und V¨ogel" meaning "Birds and Oil Spills", MBF performs poorly with many irrelevant terms introduced in the feedback model. However English finds some relevant terms, and additionally adds many terms to the feedback model, which are synonyms/semantically related to oil spills and birds, such as {olverschmutz, ol, olp, vogelart}. This helps in bringing up more relevant documents while reducing drift.
6.1 Effect of Query Translation Quality
Accurate Query Translation is fundamental to MultiPRF. As explained earlier, we chose Google Translate mainly due to its ease of availability. In this section, we study the impact of varying translation quality on the performance of our approach. We train a Statistical Machine Translation (SMT) system, on the French-English and German-English language pairs, by running an off-the-shelf publicly available tools like Moses [13] on Europarl corpora. The above SMT system is quite simple because we do not perform any language-specific processing or any parameter tuning to improve the performance of the system and also it is limited by the domain of the parallel corpora which is parliamentary proceedings. To correlate the translation quality with the performance of MultiPRF, we evaluated the query translations produced by Google Translate and SMT system on a three-point scale between 0 and 1 (0 - Completely Wrong Translation, 0.5 - Translation not optimal but query intent partially conveyed and 1 - Query intent completely conveyed). The results are shown in Table 6. We compare the performance of MultiPRF using Google Translate, Basic SMT system, and ideal query translations. The ideal translations were obtained by manually fixing some of the errors in the above two systems. The performance on ideal

Source Assisting Language Collection Collection

No. of Docs. No. of Docs. in Source in Assisting Collection Collection MAP GMAP

German DE-01+02 DE-03

225371

DE-01+02 EN-00+01+02 225371

294809 113005

0.4445 0.2328 0.4576 0.2721

French

FR-01+02 FR-06

87191

FR-01+02 EN-00+01+02 87191

177452 113005

0.4394 0.2507 0.4535 0.2721

Table 8: Comparison of MultiPRF performance with MBF using an assisting collection in same language. Coverage of source and assisting collections given for comparison.

query translations gives an idea of the upper bound on the performance of MultiPRF. The results of our evaluation are shown in Table 7.
As expected, the performance of MultiPRF on ideal translations is the best followed by Google Translate and the Basic SMT system. The results demonstrate that translation using the basic SMT system improves over monolingual MBF, especially P@5 and P@10. This shows that the performance of MultiPRF improves performance with any reasonably good query translation system.
6.2 Comparison with Assisting Collection in Same Language
One of the prime reasons for improvement in MultiPRF performance is good monolingual performance of assisting collection. The natural question which may then arise is whether the assisting collection needs to be in a different language. In this section, we study the performance of MultiPRF when the assisting collection is in the same language. Given a query, we use MBF on both source and assisting collections and interpolate the resultant feedback models. The final interpolated model is used to rerank the corpus and produce the final results. For the experiments, we use the French and German collections (FR-01+02, DE-01+02) since they have additional collections (FR-06, DE-03) with larger coverage in their own language. The results of comparison are shown in Table 8.
From the results, we notice that although the coverage of assisting collections in the source language is more than that of English, MBF still performs poorly when compared to MultiPRF. This can be attributed to the following reasons

664

a) the MBF performance of a query, which is ambiguous or hard in the source language collection, will be bad due to the poor quality of top k documents retrieved during initial retrieval. The quality of the top k documents will not change if the same ambiguous query is given to assisting collection in the source language. However, if source and assisting languages differ, the ambiguity may get resolved during translation causing an improvement in MBF performance. The above intuition is confirmed by the decrease in robustness, as reflected in the GMAP scores, when the source and target languages are same. b) it still suffers from the fundamental limitation of monolingual PRF i.e. the expansion terms included are only based on co-occurrence relations and does not include lexically and semantically related terms.

6.3 Comparison with Thesaurus Based Expansion in Source Language
As discussed earlier, another major source of improvement in MultiPRF is due to the inclusion of lexically and semantically related terms. However, this alone does not justify the use of an assisting collection in a different language since the same effect could be achieved by using thesaurus based expansion in the source language. In this section, we show that augmenting MBF with both thesaurus based expansion and assisting collection in the same language is not effective when compared to MultiPRF.
Since there is no publicly available thesauri for the above mentioned European languages, as proposed in Xu et al. [31], we learn a probabilistic thesaurus PLL, in source language L, from the probabilistic bi-lingual dictionaries in LEnglish PLE and English-L PEL. Given two words s1 and s2 in source language L and e is a word in English (E), PLL is given by:

X

PLL(s2|s1) =

PLL(s2, e|s1)

eE

X

=

PEL(s2|e) · PLE (e|s1)

eE

(Assuming s2, s1 are independent given e)

Lexically and semantically related words like morphological variants and synonyms have a high probability score in PLL since they usually map to the same word in the target language. Given a query, we initially run MBF in the source language and let FL be the resultant feedback model. Later, we use the probabilistic thesauri to expand the feedback model as follows:
P (f |TLhesaurus) = X PLL(f |s) · P (s|FL )
sS
The above step includes morphological variants and synonyms for the terms in the feedback model. The final model is obtained by interpolating the TLhesaurus with the MBF model FL as shown in Equation 3.
For the above experiments, we use the FR-01+02 and DE-01+02 French and German collections. The results of comparison is shown in Figure 2. It shows that MBF with both thesaurus based expansion and assisting collection in the source language does not perform as well as MultiPRF. MultiPRF automatically combines the advantage of PRF in two different collections and thesaurus based expansion. This addresses the fundamental limitations of MBF and re-

0.5

0.4576

0.4067
0.4

0.4535 0.4263

0.3

0.2

0.1

0 German (DE-01+02) French (FR-01+02)

MBF + Assisting Collection + Thesaurus Based Expansion
MultiPRF

Figure 2: MAP score comparison of MultiPRF and MBF with assisting collection in same language and Thesaurus Based Expansion. In MBF experiments, FR-06 and DE-03 were used as assisting collections for French and German respectively.
sults in an improvement of both retrieval performance and robustness.
7. CONCLUSION AND FUTURE WORK
We presented a novel approach to PRF called Multilingual PRF in which the performance of PRF in a language is improved by taking the help of another language collection. We also showed that MultiPRF addresses the fundamental limitations of monolingual PRF, viz., (i) the inability to include term associations based on lexical and semantic relationships and (ii) sensitivity to the performance of the initial retrieval algorithm. Experiments on standard CLEF collections across a wide range of language pairs with varied degree of familial relationships show that MultiPRF consistently and significantly outperforms monolingual PRF both in terms of robustness and retrieval accuracy. Our error analysis pointed to the following contributing factors: (i) inaccuracies in query translation including the presence of out-of-vocabulary terms, (ii) poor retrieval on English query, and in a few rare cases, (iii) inaccuracy in the back translation. We feel we have taken only the first step towards a direction of work with rich potential, viz. how a language can help another with respect to pseudo-relevance feedback.
As part of future work, we plan to vary the assisting language and study its effect on MultiPRF performance. Also, we would like to remove the dependence of MultiPRF approach on availability of parallel corpora in the assisting language.
Acknowledgements
We would like to thank ACM SIGIR and Amit Singhal (Donald B. Crouch Travel Grant) for the student travel grant to the first and second authors. The first author was also supported by by a fellowship award from Infosys Technologies Ltd., India.
8. REFERENCES
[1] G. Amati, C. Carpineto, and G. Romano. Query Difficulty, Robustness, and Selective Application of Query Expansion. In ECIR '04, Sunderland, UK, pages 127­137, 2004.

665

[2] A. Berger and J. D. Lafferty. Information Retrieval as Statistical Translation. In SIGIR `99, pages 222­229, Berkeley, USA, 1999. ACM.
[3] M. Braschler and C. Peters. Cross-Language Evaluation Forum: Objectives, Results, Achievements. Information Retrieval, 7(1-2):7­31, 2004.
[4] C. Buckley, G. Salton, J. Allan, and A. Singhal. Automatic Query Expansion Using SMART : TREC 3. In TREC-3, pages 69­80, 1994.
[5] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting Good Expansion Terms for Pseudo-Relevance Feedback. In SIGIR '08, pages 243­250, NY, USA, 2008. ACM.
[6] K. Collins-Thompson and J. Callan. Query Expansion Using Random Walk Models. In CIKM '05, pages 704­711, NY, USA, 2005. ACM.
[7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A Framework for Selective Query Expansion. In CIKM '04, pages 236­237, NY, USA, 2004. ACM.
[8] I. Dagan, A. Itai, and U. Schwall. Two Languages Are More Informative Than One. In ACL '91, pages 130­137, Morristown, NJ, USA, 1991. ACL.
[9] A. Dempster, N. Laird, and D. Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, 39:1­38, 1977.
[10] T. S. Dumais, A. T. Letsche, L. M. Littman, and K. T. Landauer. Automatic Cross-Language Retrieval Using Latent Semantic Indexing. In AAAI Technical Report SS-97-05, pages 18­24, 1997.
[11] W. Gao, J. Blitzer, and M. Zhou. Using English Information in Non-English Web Search. In iNEWS '08: ACM Workshop on Improving Non English Web Searching, pages 17­24, NY, USA, 2008. ACM.
[12] D. Hawking, P. Thistlewaite, and D. Harman. Scaling Up The TREC Collection. Information Retrieval, 1(1-2):115­137, 1999.
[13] H. Hoang, A. Birch, C. Callison-Burch, R. Zens, R. Aachen, A. Constantin, M. Federico, N. Bertoldi, C. Dyer, B. Cowan, W. Shen, C. Moran, and O. Bojar. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL '07, Prague, Czech Republic, pages 177­180, 2007. ACL.
[14] John Lafferty and Chengxiang Zhai. Probabilistic Relevance Models Based on Document and Query Generation. In Language Modeling for Information Retrieval, volume 13, pages 1­10. Kluwer International Series on IR, 2003.
[15] K. S. Jones, S. Walker, and S. E. Robertson. A Probabilistic Model of Information Retrieval: Development and Comparative Experiments. Information Processing and Management, 36(6):779­808, 2000.
[16] J. Lafferty and C. Zhai. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In SIGIR '01, pages 111­119, NY, USA, 2001. ACM.
[17] V. Lavrenko and W. B. Croft. Relevance Based Language Models. In SIGIR '01, pages 120­127, NY, USA, 2001. ACM.
[18] D. Metzler and W. B. Croft. Latent Concept

Expansion Using Markov Random Fields. In SIGIR '07, pages 311­318, NY, USA, 2007. ACM.
[19] M. Mitra, A. Singhal, and C. Buckley. Improving Automatic Query Expansion. In SIGIR '98, pages 206­214, NY, USA, 1998. ACM.
[20] F. J. Och and H. Ney. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19­51, 2003.
[21] I. Ounis, G. Amati, P. V., B. He, C. Macdonald, and Johnson. Terrier Information Retrieval Platform. In ECIR '05, Volume 3408 of Lecture Notes in Computer Science, pages 517­519. Springer, 2005.
[22] K. Philipp. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT Summit, 2005.
[23] S. Robertson. On GMAP: and Other Transformations. In CIKM '06, pages 78­83, NY, USA, 2006. ACM.
[24] T. Sakai, T. Manabe, and M. Koyama. Flexible Pseudo-Relevance Feedback via Selective Sampling. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111­135, 2005.
[25] T. Tao and C. Zhai. Regularized Estimation of Mixture Models for Robust Pseudo-Relevance Feedback. In SIGIR '06, pages 162­169, NY, USA, 2006. ACM.
[26] J. Tiedemann. The Use of Parallel Corpora in Monolingual Lexicography - How Word Alignment Can Identify Morphological and Semantic Relations. In Proceedings of the 6th Conference on Computational Lexicography and Corpus Research (COMPLEX), pages 143­151, UK, July 2001.
[27] E. Voorhees. Overview of The TREC 2005 Robust Retrieval Track. In E. M. Voorhees and L. P. Buckland, Editors, The Fourteenth Text REtrieval Conference, TREC 2005, Gaithersburg, MD, 2006. NIST.
[28] E. M. Voorhees. Query Expansion Using Lexical-Semantic Relations. In SIGIR '94, pages 61­69, NY, USA, 1994. Springer-Verlag.
[29] D. Wu, D. He, H. Ji, and R. Grishman. A Study of Using an Out-Of-Box Commercial MT System for Query Translation in CLIR. In iNEWS '08: ACM Workshop on Improving Non English Web Searching, pages 71­76, New York, NY, USA, 2008. ACM.
[30] J. Xu and W. B. Croft. Improving the Effectiveness of Information Retrieval with Local Context Analysis. ACM Transactions on Information Systems, 18(1):79­112, 2000.
[31] J. Xu, A. Fraser, and R. Weischedel. Empirical Studies in Strategies for Arabic Retrieval. In SIGIR '02, pages 269­274, NY, USA, 2002. ACM.
[32] Y. Xu, G. J. Jones, and B. Wang. Query Dependent Pseudo-Relevance Feedback Based on Wikipedia. In SIGIR '09, pages 59­66, NY, USA, 2009. ACM.
[33] C. Zhai and J. Lafferty. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01, pages 403­410, NY, USA, 2001. ACM Press.
[34] C. Zhai and J. Lafferty. A Study of Smoothing Methods for Language Models applied to Information Retrieval. ACM Transactions on Information Systems, 22(2):179­214, 2004.

666

Relevance and Ranking in Online Dating Systems

Fernando Diaz
Yahoo! Labs 4401 Great America Parkway
Santa Clara, CA 95054
diazf@yahoo-inc.com

Donald Metzler
Yahoo! Labs 4401 Great America Parkway
Santa Clara, CA 95054
metzler@yahoo-inc.com

Sihem Amer-Yahia
Yahoo! Labs 111 West 40th Street
17th Floor New York, NY
sihem@yahoo-inc.com

ABSTRACT
Match-making systems refer to systems where users want to meet other individuals to satisfy some underlying need. Examples of match-making systems include dating services, resume/job bulletin boards, community based question answering, and consumer-to-consumer marketplaces. One fundamental component of a match-making system is the retrieval and ranking of candidate matches for a given user. We present the first in-depth study of information retrieval approaches applied to match-making systems. Specifically, we focus on retrieval for a dating service. This domain offers several unique problems not found in traditional information retrieval tasks. These include two-sided relevance, very subjective relevance, extremely few relevant matches, and structured queries. We propose a machine learned ranking function that makes use of features extracted from the uniquely rich user profiles that consist of both structured and unstructured attributes. An extensive evaluation carried out using data gathered from a real online dating service shows the benefits of our proposed methodology with respect to traditional match-making baseline systems. Our analysis also provides deep insights into the aspects of match-making that are particularly important for producing highly relevant matches.
Categories and Subject Descriptors
H.3.5 [Online Information Services]: Web-based services
General Terms
Algorithms,Experimentation
Keywords
dating systems, relevance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
A match-making system is a bulletin board where people seek to meet other individuals in order to satisfy a particular need. Many match-making systems exist today including dating services, resume/job bulletin boards, communitybased question answering systems, and consumer-to-consumer marketplaces. Despite the popularity of such systems, relatively little research has been conducted on the design of information retrieval models particularly suited for the matchmaking task.
Typically, in a match-making system, each user is associated with a profile that includes general information about the user. For example, in an online dating service, the profile will include the user's location, physical attributes (e.g., hair color), and political affiliation. On a job seeking site, the profile may contain the job seeker's education, years of experience, and desired salary range. It is also common for users of these systems to be able to define the attributes they would like matches to satisfy. In match-making systems, these are often called target profiles. In information retrieval terminology, the target profile can be considered the user's information need, or query. A key aspect of matchmaking systems is the ranking of candidate matches for a given user. Different ranking functions are used to compute the relevance of a match to a user. In this paper, we study information retrieval-based ranking functions in this context. To the best of our knowledge, this is the first in-depth study of retrieval and ranking in a match-making system.
We use an online dating service as our main application. One key observation is that in such systems, two-sided relevance is a natural way of ranking matches. Intuitively, matches that satisfy a given user's target profile and whose target profile is also satisfied by the given user own profile, are preferred to matches whose target profile is not satisfied by the given user. Consider the situation where user u is interested in someone with attributes similar to those of user v but v is not interested in someone with attributes similar to those of user u. In this case, we argue that it is undesirable for a retrieval system to rank v highly for u. There are two reasons for this. First, we would like to avoid v being contacted by undesirable candidates. Second, we would like to maximize the likelihood that u receives a reply. Another interesting aspect of the dating domain is subjective relevance. Understanding the relevance of a pair of individuals often requires a complex understanding of the intents of both users. This makes a good match more difficult to detect than document relevance and as a result is particularly interesting from an information retrieval perspective.

66

This paper has three primary contributions. First, we formulate the match-making task as an information retrieval problem, whereupon user profiles are ranked with respect to a given target profile using a machine learned ranking function. Second, we exploit the uniquely rich nature of user profiles by extracting novel features based on their structured (e.g., age, income) and unstructured (e.g., description) attributes. Finally, we undertake an extensive evaluation using the data from a real-life online data site in order to determine the level of interest between two users. Our experiments also provide interesting insights into the types of features and models that are the most useful for these types of systems.
The rest of this paper is laid out as follows. First, Section 2 provides an overview of previous work related to match-making systems. Then, in Section 3 we formally define the match-making problem from an information retrieval perspective. Section 4 discusses our proposed machine learned ranking framework. Sections 5 and 6 detail our experimental methodology and present our experimental evaluation, respectively. Finally, Sections 7 and 8 conclude the paper with some discussions and directions for future work.
2. RELATED WORK
There is a large body of research related to match-making systems. Previous research has looked at match-making from algorithmic, empirical, and even psychological perspectives. We now highlight the previous research that is most related to our study.
First, in mathematics there is the well-known stable marriage problem. This problem aims to find a matching between all pairs of men and women such that it cannot be the case that two potential partners both prefer each other to their current partner [11]. When there is no distinction between sets of users, this is the stable roommate problem [13]. In combinatorics, the assignment problem refers to matching tasks and agents so as to minimize some cost. In our situation, we are not required to provide a hard matching between users, but rather to rank possible partners for a given user. This distinction is necessitated by the interactive nature of online match-making systems.
There have also been a number of studies that have specifically looked at dating systems. Hitsch et al. provide a very detailed exploration into factors influencing match-making in online dating systems [12]. Ranganath et al. explore how language used during speed dating sessions can be used to predict potential matches [24].
The combination of information retrieval and database approaches has been a fertile area of research recently. Such research is related to our work because of the structured nature of user profiles in match-making systems. Several previous studies have investigated the use of free text retrieval for a structured database [22, 16]. An important side effect of this is a ranking of records based on predicted relevance. Lavrenko et al. proposed structured relevance models for retrieving records from semi-structured data sets that have missing field information[18]. Follow-up work by Yi et al. used the structured relevance model for matching resumes to recruiter queries [26]. Several other methods have also been proposed for ranking database results [7, 6, 19] and for using explicit relevance feedback within database systems [21, 25, 3]. Our approach is unique in that we take

a feature-oriented, machine learning-based approach to the problem that provides a great deal of flexibility and a strong level of effectiveness.
Finally, the task of finding users (rather than documents) that satisfy an information need has been addressed in a variety of contexts. For example, in the TREC Enterprise Track, the Expert Search task requires participants to rank human experts with respect to specific topic [1]. Expert Search situations occur when a user seeks an expert. However, the expert has no constraint over who contacts her. One other difference is that users, both queriers and experts, usually do not have semistructured profiles. Completely decentralized search algorithms for individuals have also been studied in the context of social network analysis [17]. Furthermore, the reviewer assignment problem also involves a search over a set of users [14, 15]. While related, none of these tasks exhibit the unique properties exhibited by the online dating match-making task.
3. PROBLEM DEFINITION
We consider a retrieval scenario consisting of a set of users, U, each of whom maintains a self-description and a query. For a user u  U , a description, du, consists of a set of descriptive attributes. These attributes may be scalar, categorical, or free text. In this paper we consider the attributes presented in Table 1. A query, qu consists of a set of constraints on candidate attribute values. In this paper, we consider binary and scalar preferences. Binary constraints indicate that a certain attribute be present in a candidate record. Scalar constraints indicate that a scalar attribute be a certain value or in a certain range.
For each user, u, we would like to rank all other users, v  U - {u}, such that relevant matches occur above nonrelevant candidates.
4. RANKING FOR MATCH-MAKING SYSTEMS
We adopt a machine learning approach to learning our ranking model [20]. Machine learned ranking models consist of three parts: ranking features, relevance, and a ranking function. Ranking features include all signals we observe which may influence the scoring of a candidate match. In our work, relevance refers to two users being an appropriate match (e.g. they want to meet). Finally, the ranking function is a model of relevance given observable ranking features. In this section, we be explain in some detail each of these components as they relate to the match-making task.
4.1 Ranking Features
Given a user's query, we are interested in ranking all other users in decreasing order of two-way relevance. We consider three types of features that we believe are predictive of relevance. All of the features that we consider are based on the match-making attributes listed in Table 1.
First, we extract a number of features from user profiles. The user profiles represent information about a given user. Profiles typically only specify a single value for a given attribute rather than multiple values. This is due to the fact that users have a single age, a single body type, a single astrological sign, and so on. When ranking for a particular user, u, the profile features of a candidate, v, can be thought of as independent of user u and her query. We represent a

67

scalar age
height income num children num photos

body type city
country desires more children
drinking

education employment
ethnicity eye color featured profile
gender

categorical

hair color

marital status

humor style

new user

interests

occupation

languages

personality type

living situation political bent

religion

religious activity romantic style sexuality smoking social style

star sign state
subscription status television viewer zip code

Table 1: Match-making Attributes. Users define a set of scalar and categorical attributes when building a profile. In addition, users write a textual description of who they are and what they are looking for. User queries define their ideal match in terms of constraints on scalar and categorical attributes.

- candidate's profile features with the notation d (330 fea-
- tures), a querier's profile features with d (330 features),
 and the concatenation of both sets as d (660 features).

Our second set of features compares pairs of user pro-

files. We expect that match relevance will be influenced

when some profile features are very different (e.g. age). We

compare two profiles in a match by comparing the individual

-

-

profile features, d i and d i, as,

- - i = | d i - d i|

scalar features

- - i = d i  d i

binary features

We also implemented a simple score comparing the similarity of pairs of users' text self-description. For each user u, we compute an 2-normalized tf.idf-based term vector, tu, based on the self-description. Given a pair of users, the text comparison is,

text = tu, tv

Notice that, unlike our non-text feature comparison, the text comparison is a similarity as opposed to a difference. This is a minor issue since our modeling should be able to support both flavors of features. We present a pair's comparison features with  (331 features).
The final set of features represent attribute matches with respect to a user's query. These are the attributes that the querier desires a potential match to have. For the scalar attributes, users can specify a range of values of interest, such as age between 18 and 35. These ranges are transformed into two scalar features, one representing the minimum allowable value and the other representing the maximum. In our age example, this would correspond to the features age min = 18 and age max = 35. With categorical attributes, users specify one or more desirable values for each attribute. These preferences are encoded as binary features, one for each possible attribute value. For example, if a user is interested in matches with red or blonde hair, then the features hair red and hair blonde would be set to true (1) and all other hair color features (e.g., hair black) would be set to false (0). Finally, users can specify the importance of each attribute. The possible options are "must match", "nice to match", and "any match". Here, "must match" attributes are those that the querier requires to be satisfied for a match to be relevant, "nice to match" are those attributes that the user would like to matches, but does not require, and "any match" means the user would be satisfied with any match for the given attribute. These attribute importances are encoded as binary features (e.g., hair must match, hair nice to match, and hair any match). Our set of match features represent how well each attribute matches between a user's query and a candidate profile as well as the attribute preferences of the querier. For exam-

querier

candidate

query

qq

profile



d

d

Figure 1: Graphical representation of the feature sets we consider in our experiments.
ple, if a querier is interested in matches with a given hair color, a match feature would encode whether a candidate's profile attribute satisfied that hair color as well as how important that attribute match was. Match features of this form are extracted for all query attributes. We represent a candidate's match features with respect to a querier's query with the notation -q (156 features), a querier's match features with respect to the candidate's query with -q (156 features), and the concatenation of both sets as q (312 features).
Figure 1 provides a graphical depiction of the symbols used to represent the different feature sets.
4.2 Relevance
Relevance in classic text retrieval tasks (e.g. TREC ad hoc retrieval tasks) is often interpreted as topical relevance. When interpreted this way, some degree of editorial assessment of document relevance can be performed. In a matchmaking system, queries are often constrained to a small set of structured attributes. Therefore, asking an editor to detect two-sided relevance can be a more time-consuming task; the editor would need to determine the intent of a querier given a structured query as well as a potentially rich profile in both directions. Furthermore, in many cases, the relevance of matches in a match-making system is more subjective than topical relevance. For example, there may be many subtle attributes in the profile or query which are important to the users but difficult to detect, even with query attribute preferences. Because of this, accurately determining intent and relevance may be impossible.
In order to address the difficulty with editorial relevance assessment, a retrieval system can use behavioral information of a running system to detect when users have found rel-

68

evant information. This approach is practiced in web search when relevance engineers monitor user clickthrough patterns [5, 23, 2]. Because salient behavioral information normally occurs after a query is issued, we refer to these behavioral signals as post-presentation signals. We are interested in

phone exchange email exchange regexp match
num exchanges message orphan

users both exchanged phone numbers users both exchanged email addresses users both exchanged information about meeting number of exchanges between users one user sent a message without a reply

tuning ranking function parameters using queries with post-

message disparity

presentation signals and generalizing to queries with no postpresentation signals.

exchange timespan

Match-making systems provide a unique set of post-presentation message density relevance signals which can be used both for training and

evaluation. For example, if two users exchange phone num-

skip

difference in number of messages exchanged between users duration of the message exchanges between users the density of exchanges between the first and last messages exchanged one user saw the other's profile and did

bers, they are probably a good match. On the other hand, if one user's message never receives a reply, then they were probably a poor match. We present a complete list of our post-presentation features in Table 2. Instead of committing to a single feature to define relevance, we engineer a set of high precision rules to define relevance and non-relevance for

num view exchanges view orphan view disparity

not send a message number profile views exchanged between users one user viewed another's profile and was not also viewed difference in number of views exchanged between users

a subset of matches. In our work, we consider as relevant any matches where users exchanged contact information; we consider as non-relevant any matches where at least one user

view timespan view density

duration of views between users the density of views between the first and last messages exchanged

inspected the other's profile but did not send a message as

well as any matches where one user sent a message but the other did not reply. We refer to matches satisfying these rules as `labeled' matches.
We often have post-presentation features likely to correlate with relevance but whose exact relationship we are unsure of. In this situation, because we have a small set of

Table 2: Post-Presentation Features. These features measure the interactions between users after they have seen a profile. Labeled features are bolded. Regular expressions used to detect meeting were taken from [12].

(automatically) labeled matches, we can train a model to

predict the labels of these unlabeled matches. Specifically, we train a logistic regression model using the labeled set and the unlabeled features in Table 2 (i.e. we only use those features not used to infer relevance). We then use this model to label the unlabeled set with predicted relevance. For a match of users u and v, we use the notation P (R|u, v) to refer to the predicted relevance. We force labeled users to

ture vector associated with the pair (u, v). A regression tree defines a function T (u, v) by partitioning the space of feature values into disjoint regions Rj, j = 1, 2, . . . , J, which are associated with the terminal nodes of the tree. Each region is assigned a value j such that T (u, v) = j if fu,v  Rj. Thus, the output of the regression tree is computed as:

have P (R|u, v)  {0, 1} based on the automatic labeling. We want to be clear that our predicted relevance is a noisy
signal. For example, a successful interaction will not be detected if messages are exchanged through an external proto-

J

T (u, v; ) = j I(fu,v  Rj ),

(1)

j=1

col. Furthermore, even when messages are exchanged within a system, it may be that the match is inappropriate. Messaging is also subject to false negatives if contacts are not initiated due a searcher's perceived probability of response. Nevertheless, we believe that basing relevance for a dating system on behavioral information is both more reliable and more efficient than editorial labeling.

where  = {Rj, j}J1 , are parameters and I is the indicator function. Given a pair of users, a single regression tree will return a single real-valued score for that pair of users. Precisely how the score is computed depends on the model parameters Rj and j.
For a given loss function L these model parameters are estimated by minimizing the the total loss:

4.3 Ranking Function
There are many different ways to define a ranking function for the match-making task. In this work, we make use of a machine learned ranking function based on gradient boosted decision trees (GBDTs) [10]. We chose to use GBDTs for several reasons. First, GBDTs can handle both numerical and categorical features, which is a good fit for the types of attributes found in match-making systems. Second, GBDTbased ranking functions can be trained using a wide variety of relevance sources, including manual labels and clickthrough data. Finally, these models have been shown to be highly effective for learning ranking functions [27].
We now provide a basic overview of GBDTs. Given a querier u and a candidate match v, we use GBDTs to compute a relevance score for the pair. As the name implies, GBDTs are boosted regression trees. Let fu,v be the fea-

J

^ = arg min

L(yu,v, j ).

(2)



j=1 fu,v Rj

where yu,v = P (R|u, v) is the actual or predicted relevance label for pair (u, v). Numerous heuristics exist for solving this optimization problem, the details of which are beyond the scope of this paper. In all of our experiments, we use mean squared error as our loss.
A boosted regression tree is an aggregate of such trees, each of which is computed in a sequence of stages. That is,

M

sM (u, v) = T (u, v; m),

(3)

m=1

where at each stage m, m is estimated to fit the residuals

69

from stage m - 1 as follows:

^ m = arg min
m

L(yu,v, sm-1(u, v) + jm ). (4)

(u,v)

where  is a free parameter known as the shrinkage rate. Another important free parameter is the depth of the individual regression trees T . If the depth of the trees is greater than 1, then interactions between features are taken into account.
Thus, given a training set consisting of pairs of users and their associated relevance labels, we can learn a GBDT model model {i}M i=1, which is an ensemble of regression trees. At test time, we use the learned model to score, and subsequently rank, candidate matches v with respect to queries u using sM (u, v).
Finally, the GBDT algorithm also provides what is called feature importance [10]. The importance is computed by keeping track of the reduction in the loss function at each feature variable split and then computing the total reduction of loss function along each feature variable. The feature importance can be useful for analyzing which features contribute the most to the learned model.

5. METHODS AND MATERIALS
5.1 Data
We collected the profiles and queries for users of a personals system during the fall of 2009. We collected user interaction data during this time period. After processing, we had 6,420,563 matches with features from Table 2 defined. We bootstrapped from labeled features to label unlabeled matches. We kept only users who had at least one relevant match with P (R) > 0. After this processing, our data set consisted of 33,233 users (18,457 men and 14,776 women). We split our data into training, testing, and validation sets by user. This prevents us from evaluating on features observed in the training set. However, as a result, some matches cannot be observed because users are in different splits. Our data sets consisted of a training (7716 users, 17538 matches), validation (3697 users, 7543 matches), and testing set (4836 users, 10636 matches). Users whose only matches were broke by partitioning were removed from the experiments.
Text comparison features used idf statistics from the collection of user self-descriptions and the set of SMART stopwords.
5.2 Evaluation
We can compute standard information retrieval metrics, macro-averaged over users. That is, we iterate through all users, considering each user a `querier' and all other users with labels or predictions `candidates' to be ranked. Because we use both relevance predictions, we evaluate using metrics supporting estimates of the probability of relevance. In particular, we use the probabilistic versions of the following metrics: average precision (AP), precision at k documents (P@k), normalized discounted cumulative gain at rank k (NDCGk), and expected reciprocal rank (ERR). These are

defined by,

1

APu =

Prec(v)P (R|u, v) v P (R|u, v) v

1

P@k =

P (R|u, v)

k

vRk

k 2P (R|u,vi) - 1 NDCGk =
log(i + 1)
i=1

ERR =

P (R|u, r

vi)

i-1

(1

-

P

(R|u,

vj ))

i

j=1

The expected reciprocal rank metric was recently proposed by Chapelle et al. [4] and is designed for tasks with probabilistic relevance grades. The metric overcomes some of the position bias issues associated NDCG by incorporating a simple cascade-style user browsing model [8].
We define two sets of evaluation users as,
U1R = {u  U : v  U , P (R|u, v) = 1}
U0R = {u  U : v  U , P (R|u, v) > 0}
U1R would only include those users who have at least one labeled relevant match. On the other hand, U0R would only include those users who have at least one match with non-zero predicted relevance. Because the reliability and robustness of rank metrics is influenced by the number of candidates being ranked, we only evaluate over users with five or more matches with P (R|u, v) defined.
We will present results under two evaluation regimes: labeled and predicted. For the labeled evaluation, we use the set of users in U1R as queriers and only the matches labeled by our rules (i.e. P (R|u, v)  {0, 1}). In this case, our metrics reduce to their standard, non-probabilistic form. For the predicted evaluation, we use the set of users in U0R as queries and all matches with P (R|u, v) defined.
To identify statistically significant differences between two ranking functions with respect to a given retrieval metric we use a paired, one-tailed non-parametric bootstrap test [9]. We adopt this significance test because it allows us to sample users non-uniformly. We sample a user u by maxv P (R|u, v). For U1R, this results in uniform sampling. For U1R, this results in sampling proportional to the most relevant match. All significance tests are reported at the p < 0.05 level.

5.3 Training

We estimated GBDT models using different subsets and

combinations of features regressing against P (R|u, v). These

runs

are

labeled

-q

(match

only),

- d

(candidate

profile

only),

 (profile similarity only), and q (two-way match only). We

also experiment with a runs which combine sets of features.

Because of unbalanced class distributions, we weighed in-

stances according to,



1

 |UP (R|u,v)=1|

1

w = u,v

|UP (R|u,v)=0|

 

1



|U0<P (R|u,v)<1|

P (R|u, v) = 1 P (R|u, v) = 0 0 < P (R|u, v) < 1

so that we give a match weight according the source of its label.
Gradient-boosted decision trees have several free parameters: number of trees, number of nodes, and shrinkage. We

70

trained our decision trees using the training partition and selected free model parameters using the validation set, exploring ranges of free parameter values,
number of nodes {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50} number of trees {5, 10, 50, 75, 100, 250, 500, 1000}
shrinkage {0.01, 0.025, 0.05, 0.075, 0.10, 0.15, 0.25, 0.50}
Although we train on the full set of matches, we validate using the pool depth restriction we use in evaluation.

6. RESULTS

The results of our experimental evaluation are shown in

Table 3. We can make several observations about our runs

from these results. First, and perhaps most strikingly, we see -
that using d features alone tends to perform very well across -
metrics. Recall that the d features are query-independent,

and thus we are essentially learning nothing more than a

static ranking of the profiles. In fact, there are no statistical -
differences between the strongest runs and d .

Second, features comparing both users, , although effec-

tive, do not yield significant improvements over the query

independent ranking (and actually results in a statistically

significant drop for P@5 under both evaluation sets).

Using only match features, both one-way and two-way, re-

sults in the worst performance across metrics. These drops

in performance are statistically significant across metrics

compared to the next best run, , except P@5 and P@10

for the labeled relevance set and P@5 for the predicted relevance set. When we compare -q and q runs, we observe

statistically significant gains for four of the eight metrics for

the labeled relevance set (AP, ERR, NDCG5, NDCG10) and

four of the eight metrics for the predicted relevance set (AP,

ERR, NDCG5, NDCG10). We note that, when combining  and q features, we see
improvements compared to only using  or q . Compared to

using only  features, statistically significant improvements

are isolated to one metric for labeled relevance set (P@5)

and three metrics for the predicted relevance set (NDCG5,

NDCG10, P@10). All improvements are statistically signif-

icant compared to using the q alone.

Finally,

combining

 d

and

q

provides

strong

performance,

although not as strong as our other combination (q ). Fur-

thermore, this run degrades performance compared to using -
only d features, although these degradations are never sta-

tistically significant.

In Table 4, we show the top fifteen features, as measure

by their feature importance as described in Section 4.3, for

each run we evaluated. As the results indicate, the most

important features across runs tend to be those associated

with distance, age, height, the profile having been featured,

and the user's subscription status. When considering match features (-q , q ), we notice the selection of both match val-

ues as well as constraints, suggesting that the algorithm is

indeed taking advantage of the users' preferences, not just

the match value. When we consider two-way match fea-

tures, match features in both directions are selected, indi-

cating that both users' match preferences are important for

ranking. Finally, we note that the text similarity measure,

when available, often ranks highly in terms of feature im-

portance, suggesting that non-relational attributes also play

an important role.

7. DISCUSSION
- We were surprised that that model trained only using d

features resulted in such strong performance. Does this im-

ply that attributes specific to the querier are irrelevant?

Most likely not. There could be several other reasons for

this. For example, the expressiveness of the query repre-

sentation may be limited. Even though users are provided

with the ability to indicate the importance of specific traits

and their values, the importance value is still discrete; these

importances are unlikely to be as expressive as real-valued

weights applied in the machine learned model. Although it

may seem that the attributes in Table 1 are not informa-

tive enough to rank candidates, this hypothesis is unlikely -
to be supported since the d features are constrained to the

same representation. Finally, it might be that the user is

just not agile enough with the query interface to produce

effective queries. This suggests that alternative interfaces or

more intuitive features might be better at eliciting effective

queries from the users for this particular task. -
The strength of the d features may also be the result of

our data gathering process. We only experiment with pairs

of users who have interaction features defined, in the form

of profile views or message exchanges. Users may become

cognizant of how their own attributes compare to the pref-

erences associated with the profiles they view. This is espe-

cially true since profile displays include information about

the user's query. There is likely a self-censoring process in-

volved whereby a user only sends a message if they notice

that they match the profile's preferences.

The conjunctions of features noticeably did not provide

statistically significant improvements for many metrics. This

result likely follows from the number of features considered

in the aggregated set compared to the training set size. For

example,

The

combination

of

 d

and

q

features

results

in a vector of 1614 features. With only 17,538 training in-

stances, it is unlikely that the model had enough data to

learn an accurate predictor, thereby degrading ranking ef-

fectiveness. We believe that, with more training data, our

combined runs would significantly outperform the simpler

baselines.

In addition to physical attributes related to age and height,

those attributes with the strongest predictive power were

distance, subscription status, and whether a user's profile

had been featured. While the distance constraint is under-

standable to many (e.g. individuals are looking for matches

within their city or state), the others require some explana-

tion. The subscription status of a user indicates an invest-

ment in the search process. If a user has paid money for

extra site features, they may be more engaged in the system

and willing to look for matches. On the other hand, when

the site decides to feature a profile, this raises the credibil-

ity of the user, making them more attractive to other users.

As a result, we expect these users to be more successful at

finding matches.

The importance of the text similarity feature when in

conjunction with the match features, suggests that there

is information present in the text description that can be

exploited for ranking. Text provides user an unconstrained

field to discuss arbitrary interests. The ability to detect sim-

ilarity in these interests using a very simple text similarity

measure means that eliciting text from users and exploiting

it for ranking is a good avenue for future work.

71

labeled relevance

predicted relevance

- d



-q

q

q d q

- d



-q

q q d q

AP

0.453 0.439 0.368 0.398 0.456 0.445 0.485 0.484 0.428 0.454 0.497 0.494

NDCG1 0.248 0.269 0.186 0.198 0.271 0.267 0.346 0.366 0.287 0.317 0.380 0.367 NDCG5 0.513 0.478 0.409 0.437 0.505 0.500 0.576 0.556 0.501 0.527 0.575 0.580 NDCG10 0.573 0.555 0.497 0.520 0.571 0.565 0.649 0.643 0.598 0.619 0.659 0.656

P@1 P@5 P@10

0.248 0.269 0.186 0.198 0.271 0.267 0.360 0.380 0.304 0.334 0.395 0.381 0.207 0.188 0.176 0.178 0.200 0.201 0.326 0.311 0.298 0.303 0.318 0.326 0.129 0.127 0.124 0.123 0.129 0.129 0.226 0.223 0.221 0.219 0.227 0.226

ERR

0.481 0.464 0.392 0.420 0.482 0.475 0.582 0.577 0.517 0.552 0.595 0.589

Table 3: Results for match-making ranking using metrics defined with the high precision subset of labels -
as well as predicted relevance. Subsets of features include query-independent profile ranking ( d ), profile similarity (), one-way match (-q ), and two-way match (q ). Statistical significance between pairs of runs for a metric are described in the text.

- d featured age height living arrangement subscription status religious activity employment num photos religion interests new user smoking activity more kids occupation

 subscription status
distance height living arrangement featured religious activity gender eye color humor activity
text new user

-q distance max age
max height
more kids min height max height
ethnicity ethnicity
languages
has kids
body type max age education+ education
zip

q distance (-q ) distance (-q ) max age (-q ) max age (-q ) max age (-q ) ethnicity (-q ) max height (-q ) min height (-q ) ethnicity (-q ) body type (-q ) max height (-q ) max height (-q ) max height (-q ) more kids (-q ) body type+ (-q )

q text () distance (-q )
age () distance (-q ) subscription status ()
height ()
featured ()
new user ()
gender ()
num photos ()
eye color () max age (-q ) religious activity () max age (-q ) featured ()

d q distance (-q ) distance (-q )
- age ( d )
- age ( d ) max age (-q ) max age (-q )
- featured ( d )
- featured ( d )
- subscription status ( d )
- subscription status ( d )
- height ( d )
- height ( d ) max height
- new user ( d ) max height (-q )

Table 4: Top ten features for each of our feature combinations. Superscripts indicate query constraint features (: `any'; +: `nice to have'; : `must have'). Symbols in parentheses indicate the source of the feature. In all runs except , more than fifteen features were selected by the algorithm.

72

8. CONCLUSIONS
We have presented the problem of ranking for matchmaking systems. We motivated this problem by its ubiquity as well as its compelling differences with many standard retrieval scenarios. Specifically, we believe that the notions of two-way relevance, its detection, and usefulness for ranking are all very different from many search problems.
We found that, for our problem setting, query-independent ranking provided a simple and effective method for ranking candidate matches. This greatly simplifies system design since ranking can be done globally, with some simple filtering for important match constraints (e.g. distance, age). At the same time, we believe that users' queries, as they are often revealed to all users in a system may provide a selfcensoring which in turn results in a higher quality list of candidates. Furthermore, we believe that such systems may benefit from different query interfaces more appropriate for the task. This might include the option of users to issue free text queries, as text similarity appeared to be a strong predictor of relevance when combined with match features.
There is a wide range of research problems associated with retrieval in match-making systems. In the area of relevance, future work includes discovering additional implicit relevance signals. More interestingly, we can design system modules which, as a side effect, detect relevance with high accuracy. In the area of features, future work includes the refinement of features we propose and development of new attributes found to be highly correlated with relevance. For example, new structured attributes might be detected by inspecting text description term matches which are correlated with relevance. Finally, in the area of ranking, it may be possible to develop improved loss functions that take into account the unique notion of two-way relevance.
9. ACKNOWLEDGMENTS
We would like to thank Sergiy Matusevych, Seena Cherangara, Ramesh Gollapudi, and Ramana Lokanathan for helpful discussions and support.
10. REFERENCES
[1] K. Balog. People Search in the Enterprise. PhD thesis, University of Amsterdam, June 2008.
[2] B. Carterette and R. Jones. Evaluating search engines by modeling the relationship between relevance and clicks. In NIPS, 2007.
[3] K. Chakrabarti, K. Porkaew, and S. Mehrotra. Efficient query refinement in multimedia databases. In ICDE '00: Proceedings of the 16th International Conference on Data Engineering, page 196. IEEE Computer Society, 2000.
[4] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM 2009, 2009.
[5] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW 2009, pages 1­10. ACM, 2009.
[6] S. Chaudhuri, G. Das, V. Hristidis, and G. Weikum. Probabilistic ranking of database query results. In VLDB '04: Proceedings of the Thirtieth international conference on Very large data bases, pages 888­899. VLDB Endowment, 2004.
[7] S. Chaudhuri, G. Das, V. Hristidis, and G. Weikum. Probabilistic information retrieval approach for ranking of database query results. ACM Trans. Database Syst., 31(3):1134­1168, 2006.

[8] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM '08: Proceedings of the international conference on Web search and web data mining, pages 87­94. ACM, 2008.
[9] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall, 1993.
[10] J. H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):1189­1232, 2001.
[11] D. Gale and L. S. Shapley. College admissions and the stability of marriage. The American Mathematical Monthly, 69(1):9­15, January 1962.
[12] G. Hitsch, A. Hortac¸su, and D. Ariely. What makes you click? an empirical analysis of online dating. 2005 Meeting Papers 207, Society for Economic Dynamics, 2005.
[13] R. Irving. An efficient algorithm for the stable roommates problem. Journal of Algorithms, 6:577­595, 1985.
[14] M. Karimzadehgan and C. Zhai. Constrained multi-aspect expertise matching for committee review assignment. In CIKM 2009, 2009.
[15] M. Karimzadehgan, C. Zhai, and G. Belford. Multi-aspect expertise matching for review assignment. In CIKM 2008, pages 1113­1122. ACM, 2008.
[16] J. Kim, X. Xue, and W. B. Croft. A probabilistic retrieval model for semistructured data. In ECIR 2009, pages 228­239. Springer-Verlag, 2009.
[17] J. Kleinberg. The small-world phenomenon: An algorithmic perspective. In in Proceedings of the 32nd ACM Symposium on Theory of Computing, pages 163­170, 2000.
[18] V. Lavrenko, X. Yi, and J. Allan. Information retrieval on empty fields. In HLT 2007, pages 89­96. Association for Computational Linguistics, April 2007.
[19] J. Li, B. Saha, and A. Deshpande. A unified approach to ranking in probabilistic databases. PVLDB, 2(1):502­513, 2009.
[20] T.-Y. Liu. Learning to Rank for Information Retrieval. Now Publishers, 2009.
[21] M. Ortega-Binderberger, K. Chakrabarti, and S. Mehrotra. An approach to integrating query refinement in sql. In EDBT '02: Proceedings of the 8th International Conference on Extending Database Technology, pages 15­33. Springer-Verlag, 2002.
[22] D. Petkova, W. B. Croft, and Y. Diao. Refining keyword queries for xml retrieval by combining content and structure. In ECIR 2009, pages 662­669. Springer-Verlag, 2009.
[23] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In CIKM 2008, pages 43­52. ACM, 2008.
[24] R. Ranganath, D. Jurafsky, and D. McFarland. It's not you, it's me: Detecting flirting and its misperception in speed-dates. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334­342. Association for Computational Linguistics, August 2009.
[25] L. Wu, C. Faloutsos, K. P. Sycara, and T. R. Payne. Falcon: Feedback adaptive loop for content-based retrieval. In VLDB '00: Proceedings of the 26th International Conference on Very Large Data Bases, pages 297­306. Morgan Kaufmann Publishers Inc., 2000.
[26] X. Yi, J. Allan, and W. B. Croft. Matching resumes and jobs based on relevance models. In SIGIR 2007, pages 809­810. ACM, 2007.
[27] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen, and G. Sun. A general boosting method and its application to learning ranking functions for web search. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1697­1704. MIT Press, 2008.

73

Comparing the Sensitivity of Information Retrieval Metrics

Filip Radlinski
Microsoft Cambridge, UK
filiprad@microsoft.com

Nick Craswell
Microsoft
nicRkecdr@momndic, rWoAs,oUftS.cAom

ABSTRACT
Information retrieval effectiveness is usually evaluated using measures such as Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP) and Precision at some cutoff (Precision@k) on a set of judged queries. Recent research has suggested an alternative, evaluating information retrieval systems based on user behavior. Particularly promising are experiments that interleave two rankings and track user clicks. According to a recent study, interleaving experiments can identify large differences in retrieval effectiveness with much better reliability than other click-based methods.
We study interleaving in more detail, comparing it with traditional measures in terms of reliability, sensitivity and agreement. To detect very small differences in retrieval effectiveness, a reliable outcome with standard metrics requires about 5,000 judged queries, and this is about as reliable as interleaving with 50,000 user impressions. Amongst the traditional measures, NDCG has the strongest correlation with interleaving. Finally, we present some new forms of analysis, including an approach to enhance interleaving sensitivity.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Measurement Keywords: Interleaving, Evaluation, Search
1. INTRODUCTION
A tremendous amount of research has improved information retrieval systems over the last few decades. As effective approaches mature and relative improvements become smaller, the sensitivity of evaluation metrics and their fidelity to actual user experience becomes increasingly critical. Without sensitive measurement we might reject a small but significant improvement. This becomes a problem if we reject a large number of independent small improvements, because we have forgone an overall large improvement. Without fidelity in measurement, a small change in a retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

model might be taking into account some bias of relevance judges, rather than the preferences of real users.
The predominant form of evaluation in information retrieval is based on test collections (e.g. [19]) comprising query topics, a document corpus and human relevance judgments of topic-document pairs. This allows the application of standard metrics such as NDCG, MAP and Precision@k. Sensitivity depends on the number of topics and judgments. Fidelity depends on whether the test collection reflects realworld search behavior. For example, the TREC Web Track found that changing from informational to navigational [4] assumptions when judging can change the outcome of an evaluation [19, chapter 9]. Experiment outcomes can also be affected by an assessor's level of background knowledge [14].
An alternate evaluation approach is based on user behavior, estimating user success by measuring click, re-querying and general browsing patterns on search results. This can be motivated on grounds of fidelity and cost. On fidelity, judges are usually far removed from the search process, so may generate unrealistic query topics from observed queries, and have a hard time assessing documents in a way that reflects a user's actual information need. Additionally, traditional measures combine document judgments to obtain a score per query, for example based on discount and gain, but these may not match real user experience. Finally, judgments are slow and expensive to collect. For a system with real users, usage-based evaluation is far cheaper, despite the fact that the click data collected may not be reusable in the way that most test collections are.
This paper considers the reliability, sensitivity and agreement of these competing evaluation approaches. On the Cranfield/TREC side, we consider relevance judgments for up to 10,000 queries. On the user metric side, we perform click-based tests involving the interleaving of two retrieval functions over 200,000 user impressions, which we define as events where a user runs a query and clicks a result. Using a large commercial dataset, we establish results that we believe would also hold true in an academic setting.
We test sensitivity by measuring outcomes with varying numbers of queries/impressions. This is done on pairs of retrieval functions with varying degrees of difference, including one pair with a very small difference in effectiveness. Our results show that both approaches can be very sensitive, but judged evaluation may require thousands of judged queries to obtain the required sensitivity.
We test agreement in overall outcomes between traditional measures and interleaving. We tend to find agreement, which is an indication of the fidelity of the judgment-

667

based metric, since it is agreeing with an experiment involving real users. We then study various new ways of aggregating and analyzing the interleaving data, showing how to improve agreement with traditional metrics and also attain reliability with fewer impressions. We also show that, in contrast to judgment-based metrics, interleaving can measure the fraction of users for whom a ranking change was meaningful. This allows assessment to move beyond an assumption that relevance for all users is identical, and that relevance of individual documents should be aggregated identically for all queries.
2. RELATED WORK
A small number of previous studies have evaluated the sensitivity of MAP and Precision@10 in the TREC setting [18, 19]. Voorhees and Buckley [8] concluded that an absolute difference in MAP of five to six percent was needed between two retrieval functions before the direction of the difference between them as measured on fifty TREC topics is reliable. Sanderson and Zobel [13] found that an even larger difference is necessary. This paper compares traditional measures (with larger query sets) against interleaving, which was found by Joachims and collaborators to be particularly sensitive to ranking changes [17, 10].
A few previous papers studied the agreement between TREC-style evaluation and user studies. Hersh, Turpin and their collaborators found that MAP does not correlate with the time it takes users to find relevant documents [11, 2]. Allan et al. found that bpref correlated with user search effectiveness only for some quality differences [12].
On the other hand, Al-Maskari et al. [1] found that various metrics including search time, number of relevant documents found and users' perceived satisfaction differ significantly when comparing behavior between the best and worst of performing of three common information retrieval systems for TREC topic queries (as measured by MAP). Additionally, large differences in precision have been found to correlate with how long it takes users to find relevant documents [9], and user perception of result relevance [7]. However none of these evaluations detected small changes in ranking quality, which are of interest when developing retrieval algorithms.
Simply observing user clicking behavior on a real search system, Carterette and Jones [3] found a correlation between clicks and DCG on advertisements. In addition, Huffman and Hochster show that satisfaction (as estimate by judges) correlates with a DCG performance metric based on judgments of the top three retrieved documents [15]. Finally, Radlinski et al. found that a number of commonly measured usage-based ranking metrics, such as time to first click, rank of click and fraction of abandoned queries, do not reliably correlate with ranking quality on an academic article collection given large differences in ranking quality [10] . However, they found that an interleaved evaluation did allow clicks to identify the better of two rankings quickly and reliably. Despite this, the lack of relevance judgments on their collection left open the question as to whether metrics such as MAP, NDCG and precision correlate (or even agree) with interleaving.
Finally, a less common evaluation approach asks users or judges to select the better of two rankings shown side-byside [16]. When done by judges, the same challenges exist as with judging topic-document pairs. If done by users, this requires a different search interface, meaning the evaluation cannot be done with users in a natural setting.

3. EXPERIMENT DESIGN
In this section we detail the retrieval systems evaluated, and the metrics we use.
3.1 Retrieval Systems
We evaluate the differences between five pairs of rankers (retrieval functions) produced during normal development by a large commercial search engine. We treat the rankers as black boxes: for a query, each ranker produces an ordered set of results. We split our experiments by the magnitude of changes they measure.
Major experiments: The first three experiments we present involve major revisions of the web search ranker, which we refer to as rankerA, rankerB and rankerC. Experiment majorAB compares rankers A and B, with experiments majorBC and majorAC named equivalently. The differences between these rankers involve changes of over half a percentage point of MAP and NDCG. These were chosen because the changes in retrieval quality are of similar magnitude to those commonly seen in recent research publications.
Minor experiments: The remaining two experiments involve minor modifications to the ranking system ­ we term these minorD and minorE. The overall differences involve changes in retrieval performance of under 0.2 points (out of 100) of MAP and NDCG, chosen as they are typical of incremental changes made during algorithm development. Experiment minorD involves a change in the processing of rare queries, with a large effect on the performance of a small fraction of queries. Experiment minorE involves a small change in search engine parameters, with a small effect on the performance of many queries.
3.2 Evaluation with Judgement-Based Metrics
Each ranker was evaluated using both standard information retrieval metrics and based on user traffic. The standard metrics were evaluated using approximately 12,000 queries uniformly sampled from a real workload as part of previous work (allowing frequent queries to appear multiple times, and omitting queries classified as adult by human annotators). The relevance of the top ten results returned by each ranker were assessed by trained judges on a five-point scale ranging from "bad" to "perfect". As precision and MAP both require binary relevance judgments, we binarized the ratings by taking the top two levels as relevant, and bottom three as non-relevant. This is consistent with the recent observation by Scholer and Turpin that precision and user metrics are better correlated when slightly relevant documents are grouped with non-relevant documents rather than with highly relevant documents [9].
Given Q queries, we compute precision at cutoff 51 for retrieval algorithm R as follows

1Q P recision@5(R) =
Q

1 5

5

relb(dij )

i=1

i=j

where dij is the jth-ranked document returned by R in response to query qi, and relb(dij) is the binarized relevance assessment of this document. We compute Mean Average
Precision (MAP) similarly, except that instead of measur-
ing MAP down to a deep rank (such as 1,000 in TREC), we

1Chosen because few users look at results below the top 5.

668

Algorithm 1 Team-Draft Interleaving

1: Input: Rankings A = (a1, a2, . . . ) and B = (b1, b2, . . . )

2: Init: I  (); T eamA  ; T eamB  ;

3: while (i : A[i]  I)  (j : B[j]  I) do

4: if (|T eamA| < |T eamB|) 

((|T eamA| = |T eamB|)  (RandBit() = 1)) then

5:

k  mini{i : A[i]  I} . . . top result in A not yet in I

6:

I  I + A[k]; . . . . . . . . . . . . . . . . . . . . . . . . append it to I

7:

T eamA  T eamA  {A[k]} . . . . . clicks credited to A

8: else

9:

k  mini{i : B[i]  I} . . . top result in B not yet in I

10:

I  I + B[k] . . . . . . . . . . . . . . . . . . . . . . . . append it to I

11:

T eamB  T eamB  {B[k]} . . . . clicks credited to B

12: end if

13: end while

14: Output: Interleaved ranking I, T eamA, T eamB

limit ourselves to only the top ten documents2:

1Q M AP @10(R) =
Q
i=1

1 ni

10
relb(dij ) ·
j=1

P recision@j(R, qi)

where for query qi there are ni known relevant documents. We measure NDCG using an exponential gain and logarithmic decay based on the graded relevance judgments:

1 Q 1 5 2rel(dij ) - 1

N DCG@5(R) =

Q
i=1

Ni j=1 log(j + 1)

where Ni is the maximum possible DCG given the known relevant documents for qi. Due to space constraints, we refer the reader to [5] for more details about these metrics.

3.3 Evaluation with Interleaving
Interleaved evaluation, originally proposed by Joachims [17], combines the results of two retrieval functions and presents this combination to users (essentially, alternating between the results from the two rankings while omitting duplicates). The users' clicks indicate a relative preference comparing the quality of two retrieval functions: the ranking that contributed the most clicked results is considered to be better. Radlinski et al. [10] showed that Joachims' interleaving approach, as well as a modified approach they introduced, detects changes in ranking quality much more reliably than other click-based metrics.
Our evaluation on real user traffic using interleaving involved showing the rankings produced for each experiment to a small fraction of users of a commercial search system until 220,000 impressions of non-adult queries with clicks had been observed. The experiments were performed in succession over two months, with each experiment run on the same days of the week (Tuesday through Friday) to avoid any weekday/weekend effects.

Producing Interleaved Rankings
We now describe our specific interleaving algorithm, the Team-Draft approach introduced by Radlinski et al. [10]. Let A and B be retrieval functions. Given the results for a query q, A(q) = (a1, a2, . . . , an) and B(q) = (b1, b2, . . . , bm),
2Deeper judging of documents was impractical due to the large number of queries assessed. However, since ni is the number of known relevant documents, we are essentially assuming that anything not in the top 10 is unranked.

Team-Draft interleaving combines these results into a single ranking. This algorithm is motivated by how sports teams are often assigned in friendly games: Given a pool of available players, two captains take turns picking the next preferred available player for their team. This approach treats A(q) and B(q) as team captains' preference orders. Subject to a coin toss after every pick, the rankings take turns "picking" the next available result for their "team", with the ranking shown to the user who issued the query being the pick order. An example ranking produced by Team-Draft interleaving, along with team assignments, is shown in Figure 5. The full algorithm is presented in Algorithm 1. For further details, we refer the reader to [10].
In addition, our implementation involves a minor modification to this algorithm due to the many near duplicate documents commonly found on the web. While each ranker avoided returning near-duplicates, each may return a different near-duplicate of the same result. Hence steps 5 and 9 were modified: when verifying the next result in the preference order of A(q) and B(q) was not already selected, we also skip over a result if it is very similar to one already selected, using the similarity measure described in [6].
Credit assignment
Given an interleaved ranking I produced by Algorithm 1 with team assignments T eamA and T eamB, and clicks on these results, we must determine which ranking is considered better. To do this, we simply count how many distinct results were clicked on for each team. If one of the teams received clicks on more documents, this impression counts as a preference for that team. Otherwise it is a tie, and the impression is ignored. Note that the actual number of clicks is ignored, as is the order of clicking and the rank at which the clicked documents were presented. We will explore alternative credit assignment approaches in Section 7.4.
3.4 Research Questions
In the rest of this paper, we ask the following questions: (1) How many queries must be judged to obtain significant results for each metric given realistic ranking quality differences? (2) Does interleaving produce correlated results with judgment-based metrics? (3) How many impressions are needed to obtain comparable results? (4) How do interleaving algorithm design choices affect the outcome of the evaluations, and how can we extend interleaving analysis?
4. STABILITY OF JUDGMENT METRICS
In this section, we evaluate how the outcome of a comparison between ranking functions depends on the number of queries assessed when evaluated according to standard information retrieval metrics. We start with the previously described set of about 12,000 queries. From this set, we subsample n queries (with replacement) and measure the difference in the score of each input ranking according to NDCG@5, MAP@10 and Precision@5, repeating the sampling 1,000 times for each n. We then count the fraction of sampled sets of queries where each of the input rankings scored higher, ignoring cases when the scores were identical. The outcome of this evaluation is presented in Figure 1.
The top plot in Figure 1 shows the fraction of query samples for which the ranker hypothesized to be better (by the ranker developers) obtains a higher average NDCG@5 score than the other ranker, versus the number of queries in the query set evaluated. For very small query set sizes, each

669

Freq. of better ranking winning by NDCG@5

1

Experiment

majorAC

0.9

majorBC majorAB

minorD

0.8

minorE

0.7

0.6

0.5

0.4 1

10 Number of qu1e0r2ies in sample 103

104

1

Experiment

0.9

majorAC majorBC

0.8

majorAB minorD

minorE

0.7

0.6

0.5

0.4

0.3

0.2 1

10 Number of qu1e0r2ies in sample 103

104

1

Freq. of better ranking winning by MAP

Freq. of better ranking winning by Prec@5

0.8

0.6

Experiment

0.4

majorAC

majorBC

majorAB

0.2

minorD minorE

0 1

10 Number of qu1e0r2ies in sample 103

104

Figure 1: Query set size vs. the frequency with which "better" ranking scores higher.

100%

Probability of outcome (MAP)

80%

better (not significant)

60%

exactly tied 40% 20%

worse (not significant)

0%1

worse (90% significant) worse (95% significant)

10 Number of qu1e0r2ies in sample 103

104

Figure 2: Preferred ranking drilling down by outcome for experiment minorE measured with MAP.

ranker has higher NDCG@5 roughly half the time. Once the query set size is comparable to typical TREC evaluation sets of 50 to 200 queries, there is a preference for the better ranker on between 50 and 90 percent of samples. Once the query set size approaches about 1,000 queries, one of the rankers tends to be consistently identified as better, with the exception of the minorE experiment. As would be expected, larger changes in ranking quality can be detected with smaller query set sizes. It is worth noting, however, that even with 10,000 queries in the sample, the outcome for experiment minorE is still uncertain using NDCG@5.
The middle plot in the figure shows the same results using MAP@10 to evaluate performance. Although the results are similar to those obtained with NDCG for the major experiments, this is not the case for the minor experiments, which measure smaller ranking changes. MinorD involves a change where we expect to significantly improve performance on a small fraction of queries. For small numbers of queries, the "better" ranker in fact performs worse according to MAP, while the opposite is true with large numbers of queries. This happens for two reasons. First, the binarization of the relevance judgments makes MAP scores behave differently than NDCG@5 scores. Second, according to the binarized scores, the improved ranker actually reduces performance slightly for many frequent queries, while improving some rare queries dramatically. If only one query is picked at random, it is usually a frequent query, and hence the unimproved ranker scores higher for most query sets. However, once the set of queries becomes large enough that at least one rare query is usually selected, the average change in MAP on the entire set of queries becomes positive. Experiment minorE sees consistent improvements with large query set sizes, unlike with NDCG. Also note that the relative differences in majorAB and majorBC are different when using MAP than when using NDCG. This can be explained by noting that perhaps majorBC involved more improvements in finding medium relevance documents, while majorAB involved more improvements in finding highly relevant documents (with MAP only sensitive to the latter).
The lower plot in Figure 1 shows the outcome as measured by Precision@5. Interestingly, the outcome for experiment minorD disagrees with both MAP and NDCG for large query set sizes. We hypothesize this difference to happen because improvements to rare queries often occur at lower ranks, with MAP and NDCG both less sensitive to such changes than Precision. A relevant document for a frequent query dropped out of the top 5 more often than a relevant document was added to the top 5 for a rare query.
We also tried taking the top three levels as relevant when binarizing our five levels of judgments (rather than the top two levels), in which case the plots for MAP and Precision@5 become more similar to those for NDCG@5. This suggests that the changes made in the minor experiments happen precisely to documents near this relevance threshold, and the choice of threshold is critical when evaluating ranking quality using metrics based on binary relevance. One could argue that this is evidence that the correct threshold for "relevant" is lower (so that all three metrics agree), yet perhaps one of the metrics better agrees with user behavior: we will study this in the next section. Our results also suggest that if only highly relevant documents are considered relevant (as found by [9], although based on judgments collected with very different judging guidelines), NDCG and MAP may disagree on the relative ordering of some ranking functions.

670

Frequency of ties using different metrics Freq. of better ranking winning by interleaving

1

Metric

NDCG

MAP

0.8

Prec5

0.6

0.4

0.2

0 1

10 Number of qu1e0r2ies in sample 103

104

Figure 3: Frequency of ties vs. query set size

To further analyze the effect of queryset size on evaluation outcome, Figure 2 shows more detail for one metric (MAP) for one experiment (minorE ). It shows the fraction of query samples for which a two-sided paired t-test indicates that one ranking is significantly better or worse (of 1,000 samples for each query set size). For small query set sizes, the performance on the set is usually exactly tied. As the query set size increases, more often than not the hypothesized better rankings is preferred (for up to about 85% of query sets of size 10,000). However, for small query sets from five to 30 queries, the worse ranking is sometimes statistically significantly better. At this significance level, this is not unusual (this incorrect conclusion with 95% confidence is drawn less than 5% of the time), but it is interesting that the difference is never significant the other way: For no query set, even consisting of 10,000 queries, does a t-test indicate that the hypothesized better ranking is better, although it is preferred by 85% of the selected query sets.
Finally, Figure 1 ignores queries where the scores are tied. As we saw that ties are very frequent at small sample sizes, Figure 3 shows how often each of the metrics were tied depending on the number of queries judged, averaged across the five experiments. As expected, ties are more common for small numbers of queries and for Precision@5 and MAP, which can take fewer values than NDCG@5.
5. SENSITIVITY OF INTERLEAVING
In this section, we perform a similar analysis to the previous section but with interleaving. From the 220,000 impressions observed for each experiment (except for the majorAC, where due to a misconfiguration only 190,000 impressions were collected) we sample impressions at random, obtaining from 1,000 to 200,000 sampled impressions. For each number of impressions, we evaluate which ranking was preferred by interleaving. This is repeated 1,000 times for each sample size, and the results are plotted in Figure 4. The figure shows the fraction of impression samples for which the ranking hypothesized to be better was indeed preferred by interleaving. The errors bars are too small to be visible.
We see a similar result as when sampling judged queries, with the major experiments agreeing with the hypothesized direction for even small numbers of impressions, and slower convergence for the minor experiments. However, even with just 1,000 impressions one ranking was consistently preferred 60% to 80% of the time. Moreover, as the number of impressions grows, the preferred ranking is always preferred for a larger fraction of samples, without the flipping behavior seen

1

0.8

Experiment

0.6

majorAC majorBC

majorAB

0.4

minorD minorE

0.2

0

1k

2k

N5ukmber of1i0mkpress2io0nks sample5d0k

100k 200k

Figure 4: Number of query impressions during evaluation vs. the frequency with which hypothesized better ranking wins.

for MAP earlier. From the plot, interleaving results are 95% reliable after about 50,000 impressions, which corresponds to the standard IR metrics with about 5,000 judged queries for the major experiments, and over 10,000 queries with the minor experiments. Also, note that the outcome of minorE disagrees with the binarized judgment-based metrics (MAP, Precision@5) when the top two relevance levels are taken as relevant, and provides a statisticaly significant outcome whereas NDCG@5 does not.
5.1 Query level sensitivity
Given the consistency of interleaving with even a relatively small number of impressions, we now investigate whether it can be used to drill down further to analyze ranking performance. Figure 5 shows one example impression for the query shaun cassidy ruby and the rockits during experiment majorAC (the URLs shown are shortened to fit). This query appears 27 times with a non-draw outcome (at least one click, and not an equal number on each "team") in the 190,000 total impressions. In the example, we see that the rankings differed in where they returned the Wikipedia page for Shaun Cassidy. In fact, for 70% of clicked impressions, this result was clicked and determined the winner according to interleaving ­ and always preferred Ranking C. This suggests that this particular web result is most relevant for users who issued this query. Such an analysis can provide a detailed view not only of which ranker was preferred, but which results contributed to this preference for each frequent query.
Note that in addition to being useful as an evaluation tool, identifying particularly important differences in the rankings that affect user behavior could be used to generate training data for learning to rank. We leave this as future work.
6. CORRELATION BETWEEN METRICS
We can now address our second question: Does interleaved evaluation agree with standard information retrieval metrics in direction as well as in magnitude? We answer it by combining the results from the previous two sections. For each experiment, Figure 6 shows the relative NDCG@5, MAP@10 and P@5 difference versus the deviation from 50% observed with interleaving. For example if rankerA is preferred to rankerB for 52% of impressions in some experiment, we plot this as a 2% interleaving signal. The error bars on the judgment-based metrics indicate the 95% confidence intervals using 1000 samples of 10,000 queries as in Section 4.

671

team

Presented

(A) facebook/ShaunCassidy wall

X(C) wikipedia/Shaun Cassidy

(A) facebook/ShaunCassidy

(C) usatoday.com/life/television/...

(C) wikipedia/Ruby & The Rockits

(A) prime-time...suite101.com/...

Ranking A (@1) facebook/ShaunCassidy wall ­(@2) wikipedia/Shaun Cassidy (@3) facebook/ShaunCassidy (@6) prime-time-...suite101.com/... (@7) thedeadbold.com/news/... (@8) buzzdash.com/polls/ruby-the..

Ranking C X (@2) wikipedia/Shaun Cassidy
(@4) usatoday.com/life/television/... (@5) wikipedia/Ruby & The Rockits (@7) facebook/ShaunCassidy wall (@6) prime-time...suite101.com/... (@10) puckettsprojects.com/2009/07/...

Figure 5: Example impression for query "shaun cassidy ruby and the rockits" from experiment majorAC. "X" indicates a click that counts, "­" the clicked URL on the other ranking, "@n" is the rank at which results from each input were shown. Of 27 impressions with a non-draw outcome, wikipedia/ShaunCassidy determined the outcome 19 times (70% of the time).

Table 1: Correlation between IR metrics and inter-

leaving experiments.

Inter'l Scoring IR Metric Correlation p-value

NDCG@5 0.882

0.048

Per impression MAP@10

0.689

0.198

P@5

0.662

0.223

NDCG@5 0.910

0.032

Per query

MAP@10

0.776

0.122

P@5

0.733

0.159

The error bars on interleaving are 95% binomial confidence intervals given all the impressions for each experiment.
The figure shows that NDCG@5 is highly correlated with interleaving, with the other metrics being somewhat less correlated (although the difference is not statistically significant due to the small number of experiments). This suggests that interleaving is a reliable way to estimate the NDCG@5, MAP@10 and Precision@5 difference between pairs of rankers. Note that with the numbers of queries and impressions considered, the differences in all the interleaving experiments, and most of the judgment based evaluations, are statistically significant ­ despite the disagreements between metrics. The correlations corresponding to these plots are shown in the first three rows of Table 1.
7. INTERLEAVING DESIGN CHOICES
Thus far, we have used the team-draft interleaving method exactly as described by Radlinski et al. [10]. We now explore a number of possible variations of the analysis of interleaving.
7.1 Impression Aggregation
Interleaving credit assignment provides one "vote" to each impression, in effect allowing more frequent queries to contribute more to the outcome of an interleaving experiment. The alternative to is aggregate the preference by query: For each query, count how often each ranker is preferred, then aggregate per query and measure the fraction of queries for which each input ranker is preferred.
As shown in Table 1, this method provides a higher (although not statistically significantly so) correlation with all the judgment based metrics. This effect is surprising because the queries for evaluating the judgment-based metrics were sampled from a real workload, so we would expect interleaving to correlate more highly with the NDCG measured on this workload sample. We hypothesize this happens because the set of queries used for evaluating with standard IR metrics was sampled from the search workload a few years ago, thus has a different distribution than the current workload.

NDCG difference

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

minorE

-0.2 -0.5

0

1

majorAC

majorBC majorAB

minorD

Interlea0v.5ing signal

1

1.5

0.8

majorAC

MAP difference

0.6

majorAB

Precision@5 difference

0.4

0.2
minorE 0

-0.5

0

1.2

1

0.8

0.6

0.4

0.2

0

minorE

-0.2

-0.5

0

majorBC

minorD

Interlea0v.5ing signal

1

1.5

majorAC majorAB
majorBC

minorD

Interlea0v.5ing signal

1

1.5

Figure 6: Correlation between IR metrics and interleaving experiments (corresponding to "per impression" row in Table 1).

672

Table 2: Summary of interleaving and NDCG@5

evaluation for each experiment.

Experi-

NDCG@5

Interleaving

ment

All % qry  All % imp 

majorAC 1.41 82.3% 1.70 1.4% 40.3% 3.5%

majorBC 0.83 80.5% 1.03 1.2% 36.8% 3.2%

majorAB 0.58 78.8% 0.73 0.9% 38.5% 2.1%

minorD 0.20 16.0% 1.21 0.6% 7.1% 6.9%

minorE 0.01 63.5% 0.02 -0.4% 28.6% -1.1%

7.2 Change frequency versus magnitude
In addition to providing a summary difference per experiment, both judgment based metrics and interleaving allow us to measure the fraction of queries for which the relevance of the two rankers differs, and the changes on just those queries. We present this analysis in Table 2. The left three columns show the mean NDCG@5 difference for each experiment (matching the NDCG@5 signal in Figure 6), as well as the average fraction of queries where NDCG@5 differs, and the mean NDCG@5 difference on just those queries.
To perform a similar analysis for interleaving, we must modify the credit assignment process. In Team Draft interleaving, each result is assigned to exactly one team, even if the rankers agree about the result order. If the input rankings are identical down to rank k, then the interleaved ranking will also share that top-k, and credit assignment is purely according to the coin toss. This is fair on average, over many queries, but is not informative.
In the modified credit assignment process, no credit is assigned to clicks in any such shared top-k. Lower clicks are treated as before. This does not change the mean interleaving signal (the shared results belong to each team equally often), but reduces the fraction of impressions that contribute to the outcome of the interleaving experiment. The last three columns of Table 2 show the mean interleaving signal, fraction of impressions where the click is on a non-shared result, and the mean signal from just these impressions.
We see that the fraction of queries where NDCG changes is much higher than the fraction of interleaving impressions where a click happens on a non-shared result. This is because changes in the relevance of any of the top 5 results (whether the user clicks on them or not) count as changes in NDCG@5, but do not count as changes in interleaving if the user only clicks on higher results. In fact, much of this difference is explained by navigational queries: When both rankers return the same top result, and users only click on that top result, any changes lower down are not considered meaningful by interleaving.
Second, note that the effect of experiment minorD becomes much clearer: a small fraction of queries/impressions changed, but the performance difference on these queries is large. The disagreement between NDCG@5 and interleaving on minorE persists, but whereas NDCG@5 seems to have changed only a very small amount on average, the signal on the impressions with changes in interleaving is now stronger.
7.3 Detecting Multiple Intents
Taking the analysis of interleaving impressions with changes further, we can look for queries with a particularly high or particularly low fraction of affected impressions and a signal far from 0% on those queries. Table 3 shows a sample of such queries from the majorAC experiment. Predominantly navigational queries that are answered well by

Table 3: Sample queries from majorAC experiment.

Inter'l Affected impr.

Query

Impressions Signal Fraction Signal

facebook

5461 0.2%

4% 5.0%

myspace

1778 5.0%

12% 42%

usps

55 8.2%

16% 50%

cash for clunkers

58 36%

94% 39%

oprah denim makeovers 331 10%

97% 10%

Table 4: Effect of different credit assignment ap-

proaches on the consistency of interleaving outcome.

Impre-

Credit Assignment

ssions constant log(rank) 1/rank top bottom

1,000 70.2% 72.2% 66.5% 67.5% 71.1%

5,000 86.0% 89.2% 82.2% 82.5% 86.2%

10,000 91.3% 93.5% 88.6% 89.5% 92.3%

50,000 98.8% 99.3% 97.3% 97.9% 98.8%

100,000 99.8% 99.9% 99.0% 99.4% 99.7%

200,000 100%

100% 99.9% 100% 100%

both rankerA and rankerC have a low fraction of affected impressions. For example, for "facebook", 96% of impressions are followed by a click on the top result for both rankers, http://facebook.com/. The remaining impressions are followed by clicks on various results, the most commonly clicked one (usually presented around rank 53) was a direct link to the facebook.com login page. For "usps", most users clicked on the top result, http://usps.com/. Of the 16% that did not, most clicked on the US Postal Service package tracking page. However, other queries saw big changes in ranking quality between rankerA and rankerC, resulting in almost all clicks being on non-shared results (although the rankers did sometimes share at least one top result, as the fraction of affected impressions is not 100%)4.
7.4 Credit Assignment Alternatives
As a final analysis, we consider a credit assignment alternative where, unlike [17, 10], all clicks are not given an equal (constant) weight. This is motivated by the particularly strong bias web users have to click on top ranked search results. It may be the case that users who click on the top result are more likely to be clicking randomly than users who click further down the list. Alternatively, presenting the best result at the top of rankings is most important, so perhaps clicks at top positions should be weighted higher.
Table 4 shows the fraction of impression sample sets for which the ranking that was preferred overall was preferred on the sample (averaged across all five experiments). We compare the standard credit assignment (constant) with providing a score of log(rank) or 1/rank to each click before determining which input ranking is preferred. We also compare this to only considering the highest ranked click (top) or the lowest ranked click (bottom).
As in Figure 4, with more impressions interleaving is more consistent, and the choice of credit assignment has little effect. However, giving logarithmically more weight to lower
3The ranks changed as the web changed, and due to other instabilities inherent in web search result ranking. 4Note that if 100% of clicks on an interleaved ranking are on results from one of the input rankings, this translates to a signal of 50% in Table 3.

673

clicks improves consistency. In contrast, giving higher weight to higher clicks makes interleaving less consistent. The differences in bold are statistically significant improvements (with 95% confidence) over constant credit assignment.
Recently, in a completely independent study, Yue et al. [20] found that by learning a combination of related scoring alternatives, even larger improvements in sensitivity are possible.
8. CONCLUSION
In this paper, we have presented a detailed comparison between performance as measured by judgments-based information retrieval metrics and performance as measured by usage-based interleaving on five real pairs of web search ranking functions. We saw that performance measured by these methods is in agreement and, particularly in the case of NDCG@5 and interleaving, is highly correlated.
Using judgment-based metrics, we saw that realistic differences in ranking quality of about 1% by these metrics often were not reliably detected with queryset sizes below thousands of queries. We also saw that for some ranking improvements, particularly involving large changes to rare queries, it is possible for MAP measured on small query sets to disagree with MAP measured on large query sets. This suggests that small query set sizes may be impractical for measuring certain types of improvements in information retrieval research, or may even provide misleading results. NDCG appeared more reliable in this regard.
Evaluation with interleaving metrics was seen to require tens of thousands of user impressions to detect changes of this magnitude, with approximately 5,000 judged queries appearing to be similarly reliable to 50,000 user searches with clicks. Additionally, our results demonstrated that measuring the fraction of impressions where a click was made on non-shared results provides a better view of the changes in ranking quality than by identifying queries where NDCG changes: This separates changes to results which matter less to users from those that affect users more.
While 50,000 impressions per pair of rankers to evaluate may appear impractical for comparing tens of rankers, as are often evaluated during research, our results are consistent with interleaving outcomes being transitive (as was also seen by [10]), which we intend to investigate further in future work. In particular, if different rankers were interleaved with one or more standard baseline rankers, this would likely allow direct comparison between different ranking algorithms that were never compared directly.
Finally, we explored a number of alternative credit assignment modifications to interleaving. Our results suggested that placing more weight on lower clicks improves the consistency of the experimental outcome, thus making it more reliable with a small number of impressions.
Overall, we found a strong agreement between judgmentbased and click-based evaluation, bolstering our confidence in both types of performance assessment. Moreover, our results show that the query volumes necessary to detect realistic changes in retrieval quality using interleaving require just tens to hundreds of regular search users, making them attainable in an academic environment.
9. ACKNOWLEDGMENTS
We would like to thank the developers of the Bing search engine, and in particular Rishi Agarwal, Nikunj Bhagat, Eric Hecht, Manish Malik and Sambavi Muthukrishnan for making this research possible.

10. REFERENCES
[1] A. Al Maskari, M. Sanderson, P. Clough, and E. Airio. The good and the bad system: does the test collection predict users' effectiveness? In Proc. of SIGIR, 2008.
[2] Andrew Turpin and Falk Scholer. User Performance versus Precision Measures for Simple Search Tasks. In Proc. of SIGIR, 2006.
[3] Ben Carterette and Rosie Jones. Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks. In Proc. of NIPS, 2007.
[4] A. Broder. A taxonomy of web search. SIGIR Forum, 26(2):3­10, 2002.
[5] W. B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison Wesley, 2009.
[6] Dennis Fetterly, Mark Manasse, and Marc Najork. On The Evolution of Clusters of Near-Duplicate Web Pages. In LA-WEB, pages 37­45, 2003.
[7] Diane Kelly, Xin Fu, and Chirag Shah. Effects of rank and precision of search results on users' evaluations of system performance. Technical Report TR-2007-02., UNC SILS, 2007.
[8] Ellen M. Voorhees and Chris Buckley. The effect of topic set size on retrieval experiment error. In Proc. of SIGIR, 2002.
[9] Falk Scholer and Andrew Turpin. Metric and relevance mismatch in retrieval evaluation. In Proc. of the Asia Information Retrieval Symposium, 2009.
[10] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. How Does Clickthrough Data Reflect Retrieval Quality. In Proc. of CIKM, 2008.
[11] W. Hersh, A. Turpin, S. Price, B. Chan, D. Kraemer, L. Sacherek, and D. Olson. Do Batch and User Evaluations Give the Same Results? In Proc. of SIGIR, 2000.
[12] James Allan, Ben Carterette, and J. Lewis. When Will Information Retrieval be "Good Enough"? In Proc. of SIGIR, 2005.
[13] Mark Sanderson and Justin Zobel. Information Retrieval System Evaluation: Effort, Sensitivity and Reliability. In Proc. of SIGIR, 2005.
[14] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In Proc. of SIGIR, 2008.
[15] Scott B. Huffman and Michael Hochster. How Well does Result Relevance Predict Session Satisfaction? In Proc. of SIGIR, 2007.
[16] P. Thomas and D. Hawking. Evaluation by comparing result sets in context. In Proc. of CIKM, 2006.
[17] Thorsten Joachims. Optimizing Search Engines Using Clickthrough Data. In Proc. of KDD, 2002.
[18] Text Retrieval Conference. http://trec.nist.gov/. [19] E. M. Voorhees and D. K. Harman, editors. TREC:
Experiments in Information Retrieval Evaluation. MIT Press, 2005. [20] Y. Yue, Y. Gao, O. Chapelle, Y. Zhang, and T. Joachims. Learning More Powerful Test Statistics for Click-Based Retrieval Evaluation. In Proc. of SIGIR, 2010.

674

Vertical Selection in the Presence of Unlabeled Verticals


Jaime Arguello
Language Technologies Institute
Carnegie Mellon University 5000 Forbes Ave.
Pittsburgh, PA, USA
jaime@cs.cmu.edu

Fernando Diaz
Yahoo! Labs 4301 Great America Pkwy
Santa Clara, CA, USA
diazf@yahoo-inc.com

Jean-François Paiement
Yahoo! Labs 1000 Rue de la Gauchetiere
Suite 2400 Montreal, QC, Canada
paiement@yahooinc.com

ABSTRACT
Vertical aggregation is the task of incorporating results from specialized search engines or verticals (e.g., images, video, news) into Web search results. Vertical selection is the subtask of deciding, given a query, which verticals, if any, are relevant. State of the art approaches use machine learned models to predict which verticals are relevant to a query. When trained using a large set of labeled data, a machine learned vertical selection model outperforms baselines which require no training data. Unfortunately, whenever a new vertical is introduced, a costly new set of editorial data must be gathered. In this paper, we propose methods for reusing training data from a set of existing (source) verticals to learn a predictive model for a new (target) vertical. We study methods for learning robust, portable, and adaptive crossvertical models. Experiments show the need to focus on different types of features when maximizing portability (the ability for a single model to make accurate predictions across multiple verticals) than when maximizing adaptability (the ability for a single model to make accurate predictions for a specific vertical ). We demonstrate the efficacy of our methods through extensive experimentation for 11 verticals.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms
Keywords
vertical search, distributed information retrieval, query intent, domain adaptation
work done while at Yahoo! Labs Montreal
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Many web search engines provide their users with access to specialized search services, or verticals, that focus on a specific type of media (e.g., blogs, images, video) or domain (e.g., health, music, travel). In some cases, if a user is aware of a relevant vertical, the query can be issued directly to a vertical-specific search engine. However, to improve userexperience, portal search engines have started embedding relevant vertical content in Web search results. This has been referred to as aggregated search. A necessary part of an aggregated search system is vertical selection, the task of deciding, given a query, which vertical back-ends, if any, should be presented alongside Web search results.
Vertical selection can be viewed as a type of resource selection, a well-studied task in distributed information retrieval. Recent work shows that a supervised machine learning approach to vertical selection outperforms traditional resource selection methods [2, 1]. Most of these traditional methods derive evidence exclusively from vertical content and do not require extensive training data [4, 15, 18, 17, 16, 19].
While a supervised approach outperforms state-of-the-art methods, one of its drawbacks is that it requires training data. As is shown in Arguello et al. [2], suitable training data (e.g., vertical relevance judgments for a set of queries) can originate from human annotations. Human annotation is resource intensive in terms of time and money. An annotation effort may be sensible as a one-time investment. However, in practice, the aggregated search environment is often dynamic. Verticals can be added to or removed from the set of candidate verticals. Documents can be added to or removed from a vertical. The interests of the user population may drift, in effect, changing the set of vertical documents most likely to be requested by users. It may not be feasible to annotate a new set of queries every time the environment undergoes a significant change.
In this paper, our goal is to improve a system's return on editorial investment. We are interested in the following scenario. Suppose we have a set of verticals for which we have collected training data in the form of human vertical relevance judgements. We refer to these verticals as the source verticals. Then, suppose we have a new vertical associated with no training data. We refer to this vertical as the target vertical. Our objective is to learn a predictive model for the target vertical using only source-vertical training data.
Our solution to this problem focuses on two model properties: portability and adaptability. A portable model is one that can be effectively applied to any target vertical with no additional training data. If a model is portable, it can

691

be used as a `black box' for any new vertical. An adaptable model is one that can be tailored to a specific new vertical with no additional training data. If a model is adaptable, its parameters can be automatically adjusted to suit the new vertical at no editorial cost. We will present models which exhibit these properties and evaluate their performance across a set of 11 target verticals.
2. RELATED WORK
In distributed IR, most traditional approaches to resource selection cast the problem as resource ranking and score collections using functions tuned manually on a few training queries. Most of these approaches focus exclusively on the similarity between the query and the collection content, possibly using sampled documents [4, 15, 18, 17, 16, 19]. Of these, so-called large document models treat each collection (or its document samples) as a single monolithic document and adapt document ranking functions to rank collections [4, 18]. In contrast, small document models focus on those (sampled) collection documents most relevant to the query [15, 17, 16, 19]. ReDDE [17], for example, scores collections by their expected number of relevant documents. This expectation is based on the number of collection samples predicted relevant, scaled by a factor proportional to the collection size.
Other approaches cast resource selection as a supervised machine learned classification [2, 1] or rank-learning task [20]. In contrast to the single-evidence resource-scoring methods described above, these approaches have the advantage of easily incorporating multiple types of evidence as input features, for example, resource-specific query-logs [2] or clickthrough data [1]. This type of evidence integration is critical when dealing with text-impoverished verticals (e.g., images, video), which are problematic for methods that focus exclusively on content-based evidence.
In machine learning, domain adaptation is the task of using training data from one or more source domains to learn a predictive model for a target domain, typically associated with little or no training data. While domain adaptation has not been studied in the context of supervised vertical selection, it has been widely studied in other applications. The domain adaptation problem arises from the fact that the source and target data originate from different distributions.
One line of work performs instance weighting to downweight the influence of "misleading" source-domain training instances. Jiang and Zhai [11] do this by discarding sourcedomain training instances that are misclassified by a model trained on whatever target-domain training data is available. This is slightly different than our problem setting since we assume that we have no training data in the target domain.
A different approach is to perform feature weighing to learn a model that generalizes better to the target domain. Saptal and Sarawagi [14] perform feature subset selection by minimizing a distance function between a single source domain and target domain data distribution. The target data distribution is estimated using predicted labels from a source-trained model. This is different from our work since we want to learn from multiple source domains. Jiang [10] proposes an approach that uses training data from multiple domains. First, a generalizable subset of features is identified based on their predictiveness across source domains. Then, a model that focuses heavily on these features

is used to produce predictions on the target domain. Finally, a target-domain classifier with access to all available features is trained on these predictions. This approach assumes binary features and requires using linear classifiers. The framework we propose can handle real-valued features and models that can explore complex features interactions.
Feature subset selection can be viewed as a type of representation change aimed to make a model more generalizable to the target domain. A similar technique is feature augmentation. Given access to some target-domain training data, Daum´e III [6] augments the feature space by making three versions of each feature: a source-domain, target-domain, and general version. A model is then trained on the union of the source and target training data. This allows the model to effectively weight a source of evidence differently when it is predictive of the target class in only the source domain, only the target domain, or both. Blitzer et al. [3] propose an approach that does this without requiring target-domain training data. The goal is to identify the correspondence of non-pivot features, which may have a different correlation with the target class across domains, based on their predictiveness of pivot features, manually identified as similarly correlated with the target class across domains. The assumption is that the cross-domain correspondence between pairs of non-pivot features is encoded in the weights assigned to each when training linear classifiers to predict the presence of each pivot feature using non-pivot features.
A different approach is to train a model on the source domain and then to only fine-tune it using a few target-domain training examples. In the context of web search, Chen et al. [5] propose several ways to fine-tune a model trained using Gradient Boosting Decision Trees, which combines weak models into a more complex model. The approach of appending trees (based on target-domain training data) to a source-trained model has the advantage of accommodating features only present in the target domain.
3. PROBLEM FORMULATION
Let yv(q) denote the true relevance label of vertical v with respect to query q. In the general vertical selection setting, the goal is to learn a function f that approximates y. In this work, we focus on the following scenario. Assume we have a set, S, of source verticals each with labeled queries. Then, suppose we are given a new (target) vertical t with no labeled data. Our objective is to learn a function f that approximates yt using only source-vertical training data. The quality of an approximation will be measured by some metric that compares the predicted and true query labels. We use notation,
µ(f, yt, Q)
to refer to the evaluation of function f on query set Q. This metric could be classification based (e.g. accuracy) or rankbased (e.g. average precision).
4. FEATURE-BASED MODEL
In our work, the domain of f is not the universe of possible query strings. Instead, we generate features of the query string which we believe correlate with vertical relevance and are generalizable across queries. In this section, we review the basic feature-based vertical selection model. A more

692

detailed description of our features can be found in our references [2, 1].
4.1 Features
Given query q and vertical v, let v(q) be a vector of features. Whatever the vertical, the semantics of a particular feature are the same. For example, if v(q)i refers to the number of times the query was issued by users directly to v, then v (q)i refers to the number of times the query was issued to v . As a result, all feature vectors are of the same length.
We generate two types of features,
1. query-vertical features: specific to the query-vertical pair, for example, the number of times the query was previously issued directly to the vertical by users.
2. query features: specific to the query and independent of the vertical, for example, whether the query is related to the travel domain.1
Our approach is to use signals shown to be useful for supervised vertical selection in previous work. We describe them briefly below.
4.1.1 Query-Vertical Features
Query-vertical features are generated from the vertical. In our case, they are derived from the similarity between the query and sampled vertical content and from the similarity between the query and queries issued previously to the vertical by users.2 We use five query-vertical features.
The first four query-vertical features are generated using a retrieval from a centralized sample index, an index that combines documents sampled from every vertical. ReDDE.top [2, 1] is a variation of ReDDE [17]. GAVG measures the geometric average document score of the vertical's top sampled documents [15]. Soft.ReDDE [2] is a variation of ReDDE.top and uses a soft document-to-vertical membership. We generated two Soft.ReDDE features. One version uses a documentto-vertical membership based on the similarity between the document and a language model constructed from verticalsampled documents. A second version uses the similarity between the document and a language model constructed from the vertical query-log. Finally, we use the query likelihood given the vertical's query-log language model. Each query-vertical feature was mass normalized across verticals.
4.1.2 Query Features
Query features are generated from the query, independent of the vertical. These features are used in previous work and are described more completely in Arguello et al. [2]. Boolean features include regular expressions and dictionary look-ups likely to correlate with vertical intent (e.g., does query contain the keyword "news"?). Geographic features correspond to the output of a geographic named-entity tagger (e.g., does the query contain a city name?). Categorical features correspond to the output of a query-domain categorization algorithm (e.g., is the query related to the travel domain?). In total, we use about 120 query features.
1We can imagine additionally having query-independent vertical features, for example, whether the vertical has observed a sudden increase in query traffic. We do not make use of vertical features. 2Vertical-specific query-logs are available to the system under the assumption of a cooperative environment.

4.2 Learning Algorithm
We adopt a machine learning approach to vertical selection. In all experiments, we used the Gradient Boosted Decision Trees (GBDT) algorithm [9]. The main component of a GBDT model is a regression tree. A regression tree is a simple binary tree. Each internal node corresponds to a feature and a splitting condition which partitions the data. Each terminal node corresponds to a response value, the predicted output value. GBDT combines regression trees in a boosting framework to form a more complex model. During training each additional regression tree is trained on the residuals of the current prediction. In our case, we chose to minimize logistic loss, which has demonstrated effective performance for vertical selection in prior work [1, 2, 8, 7]. That is, the model maximizes,

X

µlog(fv, yv, Q) = -

log(fv(v(q)) × yv(q)) (1)

qQ

where log is the logistic loss function,

log(z) = log(1 + exp(-z))

(2)

We adopt GBDT because it is able to model complex feature interactions and has been effective for other tasks such as text categorization [13] and rank-learning [22].

5. PORTABILITY
A portable vertical selection model is defined as one that can make vertical relevance predictions with respect to any arbitrary vertical. In other words, a portable model is not specific to a particular vertical, but rather agnostic of the candidate vertical being questioned for relevance.
Let us examine the distinction between a portable and non-portable vertical selection model with an example. Consider a single-evidence model that predicts a vertical relevant based on the number of times the query was issued previously to the vertical by users. This type of evidence is likely to be positively correlated with the relevance of the vertical in question. In fact, it is likely to be positively correlated with vertical relevance irrespective of the particular candidate vertical. On the other hand, consider a model that predicts a vertical relevant if the query is classified as related to the travel domain. This model may be effective at predicting the relevance of a vertical that serves travelrelated content. However, it is probably not effective on a vertical that focuses on a different domain. This model is less portable.
Most existing single-evidence resource selection models can be considered portable [18, 17, 16, 15, 19]. For example, ReDDE [17] prioritizes resources for selection based on the estimated number of relevant documents in the collection. This expectation is a function of the number of documents sampled from the collection that are predicted relevant and the estimated size of the original collection. The greater the expectation the greater the relevance irrespective of the particular resource.
5.1 Basic Model
The objective of a portable vertical selection model, f , is to maximize the average performance across source verticals. Our assumption is that if f performs consistently well across S, then f will perform well on a new (target) vertical t. In general, the portability of a model is defined

693

by a metric that quantifies performance for a vertical s  S and a function that aggregates performance across verticals in S.
For example, the portability, , which uses the arithmetic mean of the logistic loss metric is defined by,

laovgg (f

, yS , QS ) =

1X

|S |

µlog(f

, ys, Qs)

(3)

sS

where Qs is the set of training queries for source s and QS is the set of those sets; similarly, ys provides labels for vertical s and yS is the set of these functions. We refer to the model which optimizes laovgg as the basic model. Notice that,

laovgg (f

, yS , QS ) =

1 -
|S |

X

X

log(f (s(q)) × ys(q))

sS qQs

As a result, the solution which maximizes laovgg is equivalent to the solution which minimizes the logistic loss across all feature-vector/relevance-label pairs from all source verticals. That is, we can perform standard GBDT training on a pooling of each source vertical's training set.

5.2 Vertical Balancing
In the basic model's training set, positive instances correspond to relevant query-vertical pairs from all source verticals. For this reason, we expect the basic model to focus on evidence that is consistently predictive of relevance across source verticals, and hence predictive of the target vertical. In other words, vertical-specific evidence that is conflicting with respect to the positive class should be ignored. The challenge, however, is that the positive instances in the basic model's training pool may be skewed towards the more popular source verticals. This is problematic if these verticals are reliably predicted relevant using vertical-specific evidence, not likely to be predictive of the new target vertical. To compensate for this, we consider a weighted average of metrics across verticals. Specifically,

lwoagvg (f

, yS , QS ) =

1 Z

X wsµlog(f

, ys, Qs)

(4)

sS

where

Z

=

P
sS

ws.

We

use

the

simple heuristic

of

weight-

ing a vertical with the inverse of its prior,

1 ws = ps

where ps is the prior probability of observing a query with relevant vertical s. This value is approximated with the training data,

ps



P
qQs

ys(q)

|Qs|

The goal is to make training instances from minority verticals more influential and those from majority verticals less.
It is easy to see that Equation 4 is a generalization of Equation 3. Because we use logistic loss, this technique reduces to training with an instance weighted logistic loss where the instances are weighted by ws, the weight of the vertical,

lwoagvg (f

1 , yS , QS ) = - Z

X

X

ws

log(f

(s(q)) × ys(q))

sS qQs

As with the basic model, we can use standard GBDT training to optimize for this metric.

5.3 Feature Weighting
An alternative to optimizing for a portable model is to find portable features and to train a model using only those. A portable feature is defined as a feature which is highly correlated with relevance across all verticals. Recall that, across verticals, all features are identically indexed. Let i be a predictor based only on the value of feature i. In previous work, the effectiveness of features across verticals was shown to be very dependent on the vertical being considered. In order to address the expected instability of feature predictiveness across verticals, we adopt a harmonic average for our aggregation method.

havg(i, yS , QS ) = P

|S |
1

(5)

sS µ(is,ys,Qs)

Additionally, features, on their own, are not scaled to the

label range, making the use of logistic loss difficult. Instead

of constructing a mapping from a feature value to the ap-

propriate range, we adopt a rank-based metric. Let f (Q) be the ranking of Q by f . We use average precision as our

rank-based metric,

|Q|
X µAP(f, y, Q) = y(f (Q)r) × Pr(y, f (Q)) (6)
r=1
where f (Q)k denotes the query at rank k and Pk is the precision at rank k,

1

k
X

Pk(y, ) = k y(k)

r=1

In other words, for each feature, we rank queries by feature value and compute the harmonic mean average precision across verticals.3 Having computed the portability of each feature, we build a portable model by restricting our training to the most portable features.
The most portable features were selected by inspecting the distribution of portability values. Because portability values are in the unit range, we model our data with the Beta distribution. We fit the Beta distribution using the method of moments and then select features whose portability is in the top quartile of this distribution.

6. ADAPTABILITY
Above, we focus on ways of improving the portability of a model by influencing the model to ignore evidence that is vertical-specific. The argument is that a model that focuses heavily on vertical-specific evidence will not generalize well to a new target vertical.
Given access to target-vertical training data, previous work reveals two meaningful trends [2]. First, given a wide-range of input features, most features contribute significantly to performance. In Arguello et al. [2], no small subset of features was solely responsible for effective vertical prediction. Second, the features that contributed the most to performance, which characterize the domain of the query, seem to be vertical-specific (assuming that verticals focus on different domains). Based on these observations, while ignoring vertical-specific evidence seems necessary to improve a
3Because we do not know whether the feature value has a positive or negative relationship with the label, we compute AhaPvg(f, yS , QS ) using  induced in both directions and use the max.

694

model's portability, a model customized to a particular vertical is likely to benefit from it.
In the context of adaptation for web search, Chen et al. [5] propose several ways to adapt an already-tuned GBDT model given data in a new domain. Their approach, Tree-based Domain Adaptation (TRADA), essentially consists of continuing the GBDT training process on labeled data from the target domain. More specifically, a set of new regression trees are appended to the existing model while minimizing a loss function (logistic loss, in our case) on the target data.
In our case, the challenge of using TRADA to adapt a model to a specific target is that we lack target-vertical training data. In the context of semi-supervised learning, self-training or bootstrapping [21] is the process of re-training a model using previous model predictions on unlabeled data. We combine self-training and model adaptation in the following way. First, we use a portable model to label a set of queries with respect to the target vertical. Then, we use TRADA to adapt the portable model to its own targetvertical predictions.
Tree adaptation provides a method for adjusting the modeling of all features. Just as we can select portable features for a portable model, we can select vertical-specific, nonportable features for adapting a model to a specific target vertical. That is, the base model may focus on portable features while the additional trees--added in the context of pseudo-training data--may focus on non-portable features. In this case, we use the same feature portability measure (Equation 5) but select the least portable features for tree augmentation.
Pseudo-labels for the target vertical were produced using the portable model's prediction confidence value with respect to the target vertical. We used the simple heuristic of considering the top N % most confident predictions as positive examples and the botton (100 - N )% predictions as negative examples.
7. METHODS AND MATERIALS
The objective is to predict the relevance of a target vertical given a query. For this reason, we evaluate on a per-vertical basis. Given a set of verticals V with query-vertical relevance labels, each vertical was artificially treated as the new target vertical and all remaining verticals as the source verticals. That is, for each t  V, we set S = V - t.
Given a set of queries with vertical-relevance judgements, evaluation numbers were averaged across 10 cross-validation test folds, using the remaining 90% of data for training. GBDTs require tuning several parameters: number of trees, maximum number of nodes per tree, and shrinkage factor (see Friedman [9] for details). These were tuned using a grid search and 10-fold cross-validation on each training fold. In all cases, cross-validation folds were split randomly. Significance is tested using a 2-tailed unpaired t-test.
TRADA was self-trained using predictions made on the test set. More specifically, at each cross-validation step, a basic model was tuned on the training fold (90% of all queries) and applied to the test fold (10% of all queries). Then, a TRADA model was pseudo-trained using predictions on the test fold. In other words, for a given vertical, we trained 10 basic and 10 TRADA models. Rather than tune pseudo-training parameter N , we present results for N = 2.5, 5, 10%.

vertical finance games health images
jobs local movies music news travel video

Table 1: Vertical descriptions. retrievable items financial data and corporate information hosted online games health-related articles online images job listings business listings movie show times musician profiles news articles travel and accommodation reviews and listings online videos

7.1 Data
We focus on 11 verticals, described in Table 1. These verticals differ in terms of number of documents, domain (e.g., health, finance, travel), document type (e.g., news stories, embedded video clips, images), and level of query traffic.4 Our evaluation data consists of 25, 195 unique queries, randomly sampled from Web traffic. Given a query, human editors were instructed to assign verticals to one of four relevance grades (`most relevant', `highly relevant', `relevant', `not relevant') based on their best guess of the user's vertical intent. It is possible for a query to have multiple verticals tied for a particular relevance grade. For about 25% of queries all verticals were labeled `not relevant'. These are queries for which a user would prefer to see only Web search results.

7.2 Metrics
We are interested in the accuracy of a model's targetvertical predictions. Given a set of predictions for a target vertical, precision and recall can be computed by setting a threshold on the prediction confidence value. Rather than report a single precision-recall operating point, we evaluate using ranking metrics. These metrics are computed from a ranking of test queries by descending order of prediction confidence value, the probability that the target vertical is relevant to the query.
We adopt two rank-based metrics: average precision (AP) and normalized discounted cumulative gain (NDCG). In computing AP, the labels `most relevant', `highly relevant', and `relevant' were collapsed into a single grade: relevant. We then compute average precision according to Equation 6.
NDCG differs from AP in two respects. First, it differentiates between relevance grades. Second, given a target vertical, t, it favors a model that is more confident on queries for which t is more relevant. Put differently, compared to AP, it punishes high-confidence errors more severely than lowconfidence errors. Following J¨arvelin and Kek¨al¨ainen [12], NDCG for a target vertical t, evaluated over queries Qt, is computed as,

1

|Qt |
X

2yt(f (Qt)r ) - 1

µNDCG(f, yt, Qt) = Z

log(max(r, 2))

(7)

r=1

where y maps the relevance grade to a scalar (`most relevant': 3, `highly relevant': 2, `relevant': 1, `not relevant': 0). The normalizer Z is the DCG of an optimal ranking of queries with respect to the relevance.

4Each vertical is associated with its own search interface.

695

Table 2: Target-trained ("cheating") results: AP and NDCG.
vertical AP NDCG finance 0.556 0.861 games 0.741 0.919 health 0.800 0.945 hotjobs 0.532 0.814 images 0.513 0.855
local 0.684 0.926 movies 0.575 0.851 music 0.791 0.934
news 0.339 0.748 travel 0.797 0.947 video 0.290 0.701
Recall that our objective is to achieve the best performance possible without target-vertical training data. A major motivation is to alleviate the need for a model trained on target vertical data. Therefore, it is useful to measure our performance as a fraction of the performance achievable by a model with access to target training data. We show AP and NDCG results given human-annotated target-vertical training data in Table 2. A target-specific model (using all available features) was trained and tested for each vertical using 10-fold cross-validation. As in all results, we present performance averaged across test folds. Given our objective, this can be considered a "cheating" experiment. However, these numbers present a kind of upper bound for our methods. Our goal is to approximate these numbers using no targetvertical training data. For this reason, all results beyond this section are normalized by these numbers (i.e., results are reported as percentage of target-trained performance). Also, this normalization facilitates an understanding for the cost-benefit of labeling the new vertical given source-vertical labels and our proposed methods.
7.3 Unsupervised Baselines
Section 4.1.1 describes several query-vertical features that are used as input signals to our models. Prior work shows that each of these can be used as an unsupervised singleevidence vertical predictor [2, 1]. In other words, these methods can make target vertical predictions without training data. To justify the added complexity of our models, we compare against these single-evidence approaches. To conserve space, we present results for that which performed best in this evaluation: Soft.ReDDE [2]. Soft.ReDDE (the version for which the vertical language model was derived from vertical samples) performed equal to or better than the next best single-evidence method for all but 3/11 verticals based on both AP and NDCG.
8. RESULTS
We present portability results in Table 3. Across metrics, both vertical balancing (VB) and feature weighting (FW), that is, using only the most portable features, improves the performance of the basic model. Performance across verticals was either statistically indistinguishable or better. Compared to each other, feature weighting significantly improves the basic model across more verticals (8/11 for both metrics). Compared to Soft.ReDDE, the only method that does noticeably better is the basic model with feature

weighting. Performance was significantly better for 4 verticals based on AP and 5 based on NDCG. Performance was significantly worse for only one vertical based on AP and no vertical based on NDCG.

Table 3: Portability results: normalized AP and NDCG. A ( ) denotes significantly better(worse) performance compared to Soft.ReDDE (SR). A ( ) denotes significantly better(worse) performance compared to the unbalanced basic model with all features. Significance was tested at the p < 0.05 level.

vertical finance games health hotjobs images
local movies
music news travel video

SR 0.446 0.720 0.823 0.155 0.283 0.696 0.477 0.757 0.559 0.487 0.525

(a) AP

basic (all feats.) 0.209 0.636 0.797 0.193 0.365 0.543 0.294 0.673 0.293 0.571 0.449

basic+VB (all feats.) 0.199 0.724 0.793 0.226 0.404 0.614 0.388 0.700 0.434 0.618 0.539

basic+FW (only portable feats.) 0.392 0.683 0.839 0.321 0.390 0.628 0.478 0.780 0.548 0.639 0.691

vertical finance games health hotjobs images
local movies
music news travel video

SR 0.776 0.910 0.953 0.563 0.712 0.905 0.775 0.937 0.852 0.846 0.817

(b) NDCG

basic (all feats.) 0.663 0.884 0.950 0.583 0.745 0.875 0.685 0.922 0.703 0.881 0.816

basic+VB (all feats.) 0.651 0.918 0.946 0.607 0.776 0.897 0.745 0.922 0.781 0.908 0.828

basic+FW (only portable feats.) 0.775 0.903 0.960 0.671 0.768 0.910 0.798 0.957 0.875 0.911 0.902

We present adaptability results in Table 4. TRADA adapts a basic model using its target-vertical predictions as pseudotraining data. Given its superior performance (Table 3), we used the unbalanced basic model with only portable features (basic+FW). We refer to this as the base model. TRADA was tested under two conditions. In the first condition, TRADA is given access to all features for adaptation. In the second condition, it is restricted access to only the nonportable features (those purposely ignored to improve the portability of the base model).
Table 4 presents several meaningful results. First, TRADA performs poorly when the adapted model is given access to all features (columns 4-6). In contrast, when restricted access to only the non-portable features (TRADA+FW), results improve (columns 7-9).
TRADA+FW performs either equal to or better than the base model in all but one case for AP and in all cases for NDCG. Similarly, across metrics, TRADA+FW significantly outperforms Soft.ReDDE across most verticals for all values of N . It performs significantly worse than Soft.ReDDE in three cases in terms of AP and none in terms of NDCG. Overall, we interpret this as a positive result in favor of adaptability. TRADA succeeds at adapting a portable model to a specific target vertical at no additional editorial cost.

696

Table 4: Trada results: normalized AP and NDCG. A ( ) denotes significantly better (worse) performance compared to soft.redde (SR). A ( ) denotes a significantly better(worse) performance compared to the base model. Significance was tested at the p < 0.05 level.

vertical finance games health hotjobs images
local movies
music news travel video

SR 0.446 0.720 0.823 0.155 0.283 0.696 0.477 0.757 0.559 0.487 0.525

basic+FW (only portable feats.) 0.392 0.683 0.839 0.321 0.390 0.628 0.478 0.780 0.548 0.639 0.691

(a) AP

trada

(all features)

(N=2.5%) (N=5%) (N=10%)

0.364

0.328

0.226

0.735

0.660

0.491

0.814

0.813

0.592

0.360

0.384

0.345

0.320

0.370

0.405

0.523

0.601

0.609

0.493

0.462

0.411

0.751

0.778

0.760

0.509

0.556

0.523

0.531

0.573

0.597

0.633

0.648

0.586

trada+FW

(only non-portable feats.)

(N=2.5%) (N=5%) (N=10%)

0.476

0.407

0.339

0.819

0.817

0.787

0.907

0.868

0.818

0.390

0.348

0.323

0.410

0.499

0.523

0.562

0.614

0.663

0.640

0.587

0.578

0.868

0.866

0.838

0.607

0.665

0.615

0.744

0.709

0.710

0.735

0.722

0.688

vertical finance games health hotjobs images
local movies
music news travel video

SR 0.776 0.910 0.953 0.563 0.712 0.905 0.775 0.937 0.852 0.846 0.817

basic+FW (only portable feats.) 0.775 0.903 0.960 0.671 0.768 0.910 0.798 0.957 0.875 0.911 0.902

(b) NDCG

trada

(all features)

(N=2.5%) (N=5%) (N=10%)

0.760

0.718

0.632

0.920

0.885

0.778

0.947

0.939

0.827

0.720

0.736

0.710

0.745

0.775

0.790

0.885

0.907

0.898

0.808

0.790

0.732

0.939

0.939

0.931

0.850

0.868

0.828

0.875

0.890

0.889

0.869

0.865

0.827

trada+FW

(only non-portable feats.)

(N=2.5%) (N=5%) (N=10%)

0.826

0.765

0.734

0.947

0.951

0.938

0.974

0.960

0.953

0.747

0.698

0.676

0.801

0.850

0.869

0.901

0.911

0.930

0.856

0.842

0.840

0.977

0.974

0.965

0.898

0.908

0.879

0.944

0.933

0.926

0.930

0.896

0.880

9. DISCUSSION
In the previous section, we demonstrated that the performance improvement for both portable and adaptive models requires measuring individual feature portability. In order to investigate precisely which features were being selected, we plotted the value of AhaPvg in Figure 1. As it turns out, the same five features were consistently chosen as the most portable for all target verticals (i.e., for all sets of source verticals). Interestingly, these correspond to our five queryvertical features (Section 4.1.1). Conversely, those features which were the least portable were consistently our remaining query features (Section 4.1.2). In hindsight, this observation makes sense. Many existing resource selection methods score resources using a single metric and focus exclusively on query-vertical evidence [4, 15, 18, 17, 16, 19]. Despite this observation, we recommend future experiments continue to measure feature portability since this behavior may not generalize to different sets of verticals and different tasks.
Vertical balancing significantly improved the basic model only across 4/11 verticals based on AP and 3/11 based on NDCG. Recall that we introduced balancing in order to discourage examples from source verticals with high priors dominating the training set. However, this does not address cases where several sources have the same non-portable features correlated with relevance. For example, video and images tend to be relevant to queries that mention a celebrity name; travel and local tend to be relevant to queries that mention a geographic entity; and finance and jobs tend to be relevant to queries that contain a company name.

havg

query-vertical features

0.20

query features

0.15

0.10

0.05

Figure 1: Feature portability values (hmap) across sets of source verticals.
Even though vertical balancing addresses a single vertical's dominance in the training set, it does not address a small coalition of related verticals causing the model to use nonportable features. We believe that a more robust averaging technique--for example, the harmonic average used for feature portability--may result in superior performance in the presence of similar source verticals.
In the previous section, TRADA improves considerably when restricted access to only the non-portable features for

697

adaptation. We believe this is due to the following. TRADA was pseudo-trained using predictions from the base model. This model (our best portable model) used only the most portable features. The set of all features is a superset of these. When TRADA is given access to the same features used by the base model, the adapted model tends to focus on these features in order to better fit the original base-model predictions. Therefore, the non-portable features (purposely inaccessible to the base model) were ignored. As it turns out, our set of non-portable features are highly effective given target-vertical training data. We compared a set of targettrained models using only the portable features (excluding the non-portable ones) with our target-trained models using all features (Table 2). When given access to non-portable features performance improved significantly across all verticals for AP and all but one vertical for NDCG (results suppressed due to space limitations). When restricted access to only the non-portable features for adaption, TRADA is forced to focus on these highly effective features. This mechanism can be seen as a sort of regularization ensuring that both portable and non-portable features are used for prediction.
With respect to pseudo-training data parameter N , we observe that the optimal value of N seems to be verticaldependent. There may be two reasons for this. First, the base model performance (column 5 in Table 3) is also verticaldependent. Given a fixed value of N across verticals, pseudolabels from some verticals may be noisier than others. Second, the optimal value of N for a given vertical may correlate with the vertical's prior.
10. CONCLUSION
Maximizing the return on editorial investment is an important aspect of any system requiring training data. We presented an ensemble of approaches which significantly improve prediction of a new target vertical using only sourcevertical training data. Our results demonstrate that model portability, the ability of a model to generalize across different target verticals, requires careful attention to feature portability, the ability of a feature to correlate with vertical relevance across different target verticals. We found that those features which seemed to be the most portable--and hence most important for a portable model--were queryvertical features as opposed those that are independent of the candidate vertical. Conversely, when we tried to adapt a model for a specific target, the least portable features appeared to be those most important for the adapted model to consider.
Furthermore, we showed that a portable solution can be used to build a target-specific one at no additional editorial cost. Our best approach adapted a portable model using its own target-vertical predictions. This approach consistently outperformed both the base model and a competitive alternative which does not require adaptation. Results also showed that, given available resources, human-annotation on the new target vertical remains the best alternative.
This work could be extended in several directions. In terms of portability, vertical balancing may be improved by modeling the similarity (in terms of predictive evidence) between source verticals. In terms of adaptability, further improvements may be achieved by modeling the similarity between each source vertical and the target vertical.

11. ACKNOWLEDGMENTS
We would like to thank Jing Bai, Daniel Boies, Hugues Bouchard, Jean-Fran¸cois Crespo, and Alexandre Rochette for helpful discussions and feedback. This work was supported in part by the NSF grants IIS-0916553 and IIS-0841275 and a generous gift from Yahoo! through its Key Scientific Challenges program. Any opinions, findings, conclusions, and recommendations expressed in this paper are the authors' and do not necessarily reflect those of the sponsors.
12. REFERENCES
[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. In CIKM 2009, pages 1277­1286. ACM, 2009.
[2] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. Sources of evidence for vertical selection. In SIGIR 2009, pages 315­322. ACM, 2009.
[3] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In EMNLP 2006, pages 120­128. ACL, 2006.
[4] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In SIGIR 1995, pages 21­28. ACM, 1995.
[5] K. Chen, R. Lu, C. K. Wong, G. Sun, L. Heck, and B. Tseng. Trada: tree based ranking function adaptation. In CIKM 2008, pages 1143­1152. ACM, 2008.
[6] H. Daum´e III. Frustratingly easy domain adaptation. In ACL 2007, pages 256­263. ACL, 2007.
[7] F. Diaz. Integration of news content into web results. In WSDM 2009, pages 182­191. ACM, 2009.
[8] F. Diaz and J. Arguello. Adaptation of offline vertical selection predictions in the presence of user feedback. In SIGIR 2009, pages 323­330. ACM, 2009.
[9] J. H. Friedman. Gradient function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 1999.
[10] J. Jiang. Domain Adaptation in Natural Language Processing. PhD thesis, University of Illinois at Urbana-Champaign, 2008.
[11] J. Jiang and C. Zhai. Instance weighting for domain adaptation in nlp. In ACL 2007, pages 264­271. ACL, 2007.
[12] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. TOIS, 20:2002, 2002.
[13] S.-M. Kim, P. Pantel, L. Duan, and S. Gaffney. Improving web page classification by label-propagation over click graphs. In CIKM 2009, pages 1077­1086. ACM, 2009.
[14] S. Satpal and S. Sarawagi. Domain adaptation of conditional probability models via feature subsetting. In PKDD 2007, pages 224­235. Springer-Verlag, 2007.
[15] J. Seo and B. W. Croft. Blog site search using resource selection. In CIKM 2008, pages 1053­1062. ACM, 2008.
[16] M. Shokouhi. Central rank based collection selection in uncooperative distributed information retrieval. In ECIR 2007, pages 160­172, 2007.
[17] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In SIGIR 2003, pages 298­305. ACM, 2003.
[18] L. Si, R. Jin, J. Callan, and P. Ogilvie. A language modeling framework for resource selection and results merging. In CIKM 2002, pages 391­397. ACM, 2002.
[19] P. Thomas and M. Shokouhi. Sushi: Scoring scaled samples for server selection. In SIGIR 2009. ACM, 2009.
[20] J. Xu and X. Li. Learning to rank collections. In SIGIR 2007, pages 765­766. ACM, 2007.
[21] D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In ACL 1995, pages 189­196. ACL, 1995.
[22] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression framework for learning ranking functions using relative relevance judgments. In SIGIR 2007, pages 287­294. ACM, 2007.

698

iCollaborate : Harvesting Value from Enterprise Web Usage

Ajinkya Kale, Thomas Burris¶, Bhavesh Shah, T L Prasanna Venkatesan,
Lakshmanan Velusamy, Manish Gupta§, and Melania Degerattu
India Software Lab, IBM India Pvt. Ltd., ¶CIO Innovation Initiatives, IBM India Pvt. Ltd., §India Research Lab, IBM India Pvt. Ltd., GWWPE, CIO Office, IBM Corp.
{ajinkyakale, thomburris, bhaveshshah, prasannav, lakshmanan.v,
gmanish}@in.ibm.com, mdegera@us.ibm.com

Figure 1: A screenshot of iCollaborate in action
Categories and Subject Descriptors : H.3.3 [Information storage and retrieval]Information filtering
General Terms : Algorithms Keywords : Enterprise Social Data, Social browsing
1. INTRODUCTION
We are in a phase of `Participatory Web' in which users `add value' to the information on the web by publishing, tagging and sharing. The Participatory Web has enormous potential for an enterprise because unlike the users of the internet an enterprise is a community that shares common goals, assumptions, vocabulary and interest and has reliable user identification and mutual trust along with a central governance and incentives to collaborate. Everyday, the employees of an organization locate content relevant to their work on the web. Finding this information takes time, expertise and creativity, which costs an organization money. That is, the web pages employees find are knowledge assets owned by the enterprise. This investment in web-based knowledge assets is lost every time the enterprise fails to capture and reuse them. iCollaborate is tooled to capture user's web interaction, persist and analyze it, and feed that interaction back into the community - the enterprise.
Pain points : Current web applications that explicitly monitor web browsing activity, such as eyebrowse[2], are limited by a number of factors. First, they exist out on the
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

public web, where user identity, trust, and governance are issues. Second, the extent to which they are aimed at providing recommended content and views into browsing trends are challenged by the lack of tightly organized, closed communities. Other utilities like Delicious, StumbleUpon usually need an explicit user actions to mark the page on the user profile and to rate the page.
Crowd Intelligence in an Enterprise : The ideal community for web data mining and services is an enterprise. In an enterprise, user identity and trust can be assured. The sub-net of web pages that employees use is, by definition, focused on the mission and business of the enterprise. These user groups can be utilized to enhance the web browsing knowledge of the enterprise as a whole.
Solution outline : iCollaborate specifically targets a community of enterprise employees. We track user's web interaction via a browser plugin using a page usage score algorithm to determine the extent to which an employee actually used the page. This data, along with data derived from the content of the pages themselves, is used to create clusters of `similar' users and content. We map users into two different cluster groups based on their long term interest ie browsing history and their short term interests ie their recent web browsing sessions. These clusters are used to provide real-time people and content recommendations. Using a combination of Locality Sensitive Hashing[1], Min-hash and Co-Visitation techniques we bind content-based and collaborative filtering for common interest clustering of users and content. These services derived from this analysis is a direct function of the coherence of the community from which it is derived. To an enterprise, seeking competitive advantage by maximizing the use of the information at its disposal, a system like iCollaborate can provide much value.
2. REFERENCES
[1] P. Indyk, R. Motwani. Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality. In Proc. of the 30th Annual ACM Symposium on Theory of Computing, 1998, pp. 604-613.
[2] B. Moore, M. V. Kleek, D. Karger, eyebrowse : http://eyebrowse.csail.mit.edu/

699

Exploring Desktop Resources Based on User Activity Analysis

Yukun Li, Xiangyu Zhang and Xiaofeng Meng
School of information, Renmin University of China Beijing, China
liyukun@ruc.edu.cn, zhangxy@live.com, xfmeng@ruc.edu.cn

ABSTRACT
Relocation in personal desktop resources is an interesting and promising research topic. This demonstration illustrates a new perspective in exploring desktop resources to help users re-find expected data resources more effectively. Different from existing works, our prototype OrientSpace has two features: automatically extract and maintain user tasks to support task-based exploration, and support vague search by exploiting associations between desktop resources.
Categories and Subject Descriptors: H.5.2 [User Interfaces]:Prototyping.
General Terms: Design, Human Factors, Management.
Keywords: Desktop resources, Task exploration, Association exploration.
1. INTRODUCTION
Nowadays, the most widely used approaches to explore desktop resources is by Windows Resource Explorer(WRE) and Desktop Search Tools(DST). WRE demands users to recall precise path information, and DST demands users to remember exact key words. However, there're many occasions when users can not remember the promising keywords or pathes. In fact, users often expect to relocate desktop resources based on user tasks( [1],etc). Some existing works make good efforts to tackle this problem, like prototype Haystack [2] and Phlat [3]. But these works paid little attention to the role of user activities for personal data relocation. This demonstration is try to overcome the disadvantages of the extisting works, and help users to explorer personal desktop resources based user activities and associations between desktop resources.
2. SYSTEM OVERVIEW
Figure 1 shows the interface of OrientSpace system. It has two major features: task-based resources exploration and association-based resources exploration.
Task-based Resource Exploration. In this work we define each task as a set of desktop files related to generating a special personal document, and identify each task based on analyzing user access sequential list on desktop resources. The left area of figure 1 shows a list of user tasks ranked by time, which are extracted automatically through detecting
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: System Interface
and analyzing user operations. By clicking one of the tasks, user will get the documents related to this task. This would be especially useful for those people who don't spend enough time in organizing their documents.
Association-based Resource Exploration. The right area of figure 1 represents an association graph of documents and tasks. This is very useful when a user can not remember the right keyword and directory for the desired file, but can remember some information about other files with relation to it. As shown in figure 1, the user expects to find file A, and can not remember its keywords and directory, but can remember a keyword "extraction" of another document B associated to a same task "SIGIR 2010" with the desired file A. She can first find B by keyword "extraction", then relocate file A by this association-based explorer. Currently supported associations by OrientSpace include: have common keywords, belong to the same task, attached to email and so on.
3. ACKNOWLEDGMENTS
This research was partially supported by the grants from the National High-Tech Research and Development Plan of China (No:2007AA01Z155).
4. REFERENCES
[1] P. Vakkari. Task based information searching. In: Cronin, B. (Ed.) [ARIST 37]: 413-464, 2003.
[2] D.R. Karger et al. Haystack: A General-Purpose Information Management Tool for End Users Based on Semistructured Data, CIDR 2005:13-26.
[3] E. Cutrell et al. Fast, flexible filtering with phlat ­Personal Search and Organization Made Easy. CHI 2006:261-270.

700

A Data-Parallel Toolkit for Information Retrieval

Dennis Fetterly
Microsoft Research Silicon Valley Mountain View, CA USA
fetterly@microsoft.com

Frank McSherry
Microsoft Research Silicon Valley Mountain View, CA USA
mcsherry@microsoft.com

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software
General Terms
Experimentation
1. EXTENDED ABSTRACT
Due to the explosive growth of the web that has occurred throughout its history, many researchers working on web corpora have begun to move toward distributed, data parallel computing. The size of the ClueWeb09 [2] corpus, at approximately one billion documents, is an indication of this. Even limiting the collection to only documents in the English language only halves the size of the collection.
In this work, we describe the collection of information retrieval algorithms we have implemented using DryadLINQ [8]. DryadLINQ is a data parallel processing system that allows programmers to write distributed programs without worrying about the implementation of a distributed system. DryadLINQ executes programs containing SQL-like Language Integrated Query statements (LINQ) by shipping the computation to nodes in the cluster for parallel execution. The ability to break a computation into many pieces that can be processed on individual machines means that even a small number of computers can be leveraged to reduce the time necessary to process large collections.
When researchers first obtain a collection of web documents, there is a substantial amount of preprocessing before analysis can commence. The toolkit assists with parsing, link extraction, associating discovered anchor text with the referenced document. Once the document content and links are in a standard format, then further processing can be performed. The toolkit provides implementations of textbased retrieval methods (BM25 [7] and BM25F [9]), queryindependent link based scoring functions (PageRank, indegree, and trans-domain indegree), query-dependent linkbased scoring functions (SALSA-SETR [6]). Additionally, the toolkit provides an implementation of shingle based duplicate document detection [1], n-gram extraction, and a mechanism to build an inverted index.
The algorithms included in this toolkit include both traditional algorithms as well as recent research results. Elements

of this toolkit formed the basis of the Microsoft Research entry in the TREC 2009 conference [3]. Given the implementation in a declarative, high-level language, these algorithms are easy to modify and extend making them a good basis for research into new algorithms.
In addition to discussing the use and implementation of this toolkit during the demonstration, we intend to release it [5] in source and binary form to others in the community to aid in large-scale information retrieval research. This, coupled with the public availability of the ClueWeb [2] dataset and the Dryad/DryadLINQ system [4] makes large-scale web information retrieval research substantially more accessible.
2. ACKNOWLEDGMENTS
We are very grateful to Nick Craswell, Marc Najork, and Emine Yilmaz for their assistance in writing the DryadLINQ implementations of the algorithms in this toolkit.
3. REFERENCES
[1] A. Broder, S. Glassman, M. Manasse, and G. Zweig. Syntactic Clustering of the Web. In Proc. of WWW6, 1997.
[2] http://boston.lti.cs.cmu.edu/Data/clueweb09/ [3] N. Craswell, D. Fetterly, M. Najork, S. Robertson and
E. Yilmaz. Microsoft Research at TREC 2009: Web and Relevance Feedback Tracks. In Proc. of the 18th Text Retrieval Conference, 2009. [4] http://research.microsoft.com/collaboration/tools/ dryad.aspx [5] http://research.microsoft.com/dryadlinqir [6] M. Najork, S. Gollapudi, and R. Panigrahy. Less is More: Sampling the neighborhood graph makes SALSA better and faster. In Proc. of the 2nd ACM International Conference on Web Search and Data Mining, pages 242­251, 2009. [7] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proc. of the 3rd Text REtrieval Conference, 1994. [8] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U´ . Erlingsson, P. K. Gunda, J. Currey. DryadLINQ: a system for general-purpose distributed data-parallel computing using a high-level language. In Proc. of the 8th USENIX Symposium on Operating Systems Design and Implementation, pages 1­14, 2008. [9] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson. Microsoft Cambridge at TREC­13: Web and HARD tracks. In Proc. of the 13th Text Retrieval Conference, 2004.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

701

Finding and Filtering Information for Children
Desmond Elliott, Richard Glassey, Tamara Polajnar, Leif Azzopardi
Information Retrieval Group, Department of Computing Science University of Glasgow, Glasgow, G12 8QQ
{delliott, rjg, tamara, leif}@dcs.gla.ac.uk

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Information Filtering
General Terms: Design, Human Factors
Keywords: Information Filtering, Children
Extended Abstract
Children face several challenges when using information access systems. These include formulating queries, judging the relevance of documents, and focusing attention on interface cues, such as query suggestions, while typing queries [3]. It has also been shown that children want a personalised Web experience and prefer content presented to them that matches their long-term entertainment and education needs [2]. To this end, we have developed an interaction-based information filtering system to address these challenges.
The system, a prototype developed within the PuppyIR project [1], is initialised by supplying a list of manually selected and vetted syndication feeds. We use sources such as the BBC and other reputable providers to overcome the problem of content moderation1. Interaction with the system facilitates topic discovery over time; however, a default set of topics are defined to deal with the cold-start problem. Feeds are periodically fetched and split into individual feed entries, which are checked against existing content to ensure they are unique. Finally, documents are filtered for a topic using the Okapi BM25 scoring function with the topic definition acting as a query. Filtered documents are presented in reverse chronological order and the document-query score is used to create visual cues in the presentation of results.
The system interface is shown in Figure 1. The set of default topics are shown above the set of discovered topics (1a). In this example, the Science topic has been selected (1b) and the list of documents titles filtered for this topic are presented to the child. The amount of space used in presenting a title indicates the relevance of a document, as shown by comparing a somewhat relevant document (2a) and a strongly relevant document (2b). Children are also able to manually define a new topic (3), if needed, and personalise the interface (4), such as the customizing the title, colour scheme and style.
1This decision allows us to focus on filtering documents from the syndication feeds according to a set of topics.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Maddie's Favourite Stuff 4

General
Latest News

Geese tagged and tracked to assess wind 2a farm impacts

Sport
Entertainment New human-like

1a Science ® 1b

2b

species' revealed

Maddie's

Twilight

Venus 'still volcanically active'

Figure Skating Scientists find the first

animals able to live

without oxygen

3
Add Topic

In pictures: Arctic foxes and their long distance journeys
'First image' of star's eclipse

Figure 1: Interface showing default and personalised filters, variable title size and user personalisation
When a child clicks on the an item in the list, for example (2b), the remainder of the document is presented (not shown). This interaction is used to update the system's knowledge about the child's interests. The history of interaction actions is subsequently used to learn the set of discovered topics (1a), to provide a personalised experience.
The interaction-based information filtering system presented offers a novel approach to the challenges faced by children. By inferring the topics of interest over time, instead of relying on explicit queries, query formulation becomes optional. The subsequent reduction in queries allows the child to fully focus their attention upon the information presented. Finally, by varying the amount of space used for each document title, relevance cues are clearly expressed in a simple and intuitive manner.
Acknowledgements: PuppyIR is funded by the EC's FP7 2007-2013 under grant agreement no. 231507.

1. REFERENCES
[1] L. Azzopardi, R. Glassey, M. Lalmas, T. Polajnar, and I. Ruthven. PuppyIR: Designing an OS Framework for Interactive Information Services for Children. In Proc. of the 3rd HCIR Workshop, pages 26­30, 2009.
[2] D. Bilal. Draw and tell: Children as designers of web interfaces. American Soc for Info Sci and Tech, 40(1):135­141, 2003.
[3] H. E. Jochmann-Mannak, T. W. C. Huibers, and T. J. M. Sanders. Children's IR. In Procs. of FDIA, pages 64­72, 2008.

702

Automatic Content Linking: Speech-based Just-in-time Retrieval for Multimedia Archives

Andrei Popescu-Belis
Idiap Research Institute Rue Marconi 19, BP 592 1920 Martigny, Switzerland
apbelis@idiap.ch
Alexandre Nanchen
Idiap Research Institute Rue Marconi 19, BP 592 1920 Martigny, Switzerland
ananchen@idiap.ch

Jonathan Kilgour
HCRC, Univ. of Edinburgh 10 Crichton Street
Edinburgh EH89AB, Scotland
jonathan@inf.ed.ac.uk
Erik Boertjes
TNO ICT Brassersplein 2 2612 Delft, The Netherlands
erik.boertjes@tno.nl

Peter Poller
DFKI GmbH Stuhlsatzenhausweg 3 66123 Saarbrücken, Germany
peter.poller@dfki.de
Joost de Wit
TNO ICT Brassersplein 2 2612 Delft, The Netherlands
joost.dewit@tno.nl

ABSTRACT
The Automatic Content Linking Device monitors a conversation and uses automatically recognized words to retrieve documents that are of potential use to the participants. The document set includes project related reports or emails, transcribed snippets of past meetings, and websites. Retrieval results are displayed at regular intervals.
Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
General Terms: Design, Human factors
1. INTRODUCTION
The availability of recording devices facilitates the acquisition of multimedia data repositories for use in personal or corporate contexts. Meeting participants may talk about items contained in such repositories, but searching the data requires more time than they can afford to spend during their discussion. We have designed the Automatic Content Linking Device (ACLD), a retrieval system inspired by the `query-free' [1] and `just-in-time' [2] approaches, using automatic speech recognition (ASR). Its primary use is during live meetings, but it can also be demonstrated with a single speaker using a laptop computer, or over previously recorded material.
2. ARCHITECTURE
The main components of the ACLD are the following ones. The Document Bank Creator is run before a meeting to create or update the repository that will be searched during the meeting. Text versions of documents are generated for indexing, and meaningful metadata is added. One-minute snippets of previous meetings, represented by their ASR transcript, are also included. All "documents" are indexed using the Apache Lucene open-source software. The Query Aggregator processes the words spoken by the participants, found by the ASR component, in batches of fixed duration or length. The stopwords are filtered out and pre-specified
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

keywords are spotted. Finally, queries are constructed, with boosted keywords. The search results represent the content linked to current conversations. These are obtained for each time frame, and smoothed in time using a persistence mechanism. Web queries are handed to the Google API, possibly restricted to a single website such as Wikipedia.
The modular User Interface can display its widgets sideby-side (in full screen mode) or as superposed tabs (in minimized mode). The widgets show, respectively, the list of recognized words with highlighted keywords, a separate tag cloud reflecting the importance of keywords, the five most relevant document names retrieved during the latest time interval, as well as the titles of the five most relevant web pages. Hovering over result names provides a brief overview of the respective documents, showing metadata and excerpts with highlighted retrieval terms. Clicking on a result name opens the corresponding document with an adapted viewer, such as a meeting browser for snippets of past meetings.
3. FEEDBACK AND EVALUATION
Performance evaluation was attempted by enacting meetings and measuring the ACLD effect on the outcome, but the method was too costly. The usability of the GUI was tested with ten users, reaching ca. 70% satisfaction. Two focus groups discussed the ACLD, and found the general concept useful. The ACLD was also demonstrated to about 30 potential industrial partners, with positive verbal evaluation and suggestions for future improvement. Many suggestions were already implemented, such as linking content on demand, or highlighting keywords found in documents.
4. ACKNOWLEDGMENTS
The ACLD was supported by the EU AMIDA Integrated Project FP6-0033812, and by the Swiss IM2 NCCR.
5. REFERENCES
[1] P. E. Hart and J. Graham. Query-free information retrieval. IEEE Expert: Intelligent Systems and Their Applications, 12(5):32­37, 1997.
[2] B. J. Rhodes and P. Maes. Just-in-time information retrieval agents. IBM Systems Journal, 39(3-4):685­704, 2000.

703

Si-Fi: Interactive Similar Item Finder
Inbeom Hwang1, Minsuk Kahng1, Sung Eun Park1, Jinwook Seo2, and Sang-goo Lee1
School of Computer Science and Engineering Seoul National University
Seoul 151-742, Republic of Korea
1{inbeom,minsuk,separk1031,sglee}@europa.snu.ac.kr, 2jwseo@cse.snu.ac.kr

Categories and Subject Descriptors: H.3 Information Storage and Retrieval: On-line Information Services
General Terms: Design, Human Factors
Keywords: Interactive Search, Hierarchical Clustering
1. EXTENDED ABSTRACT
Many recommender systems retrieve items similar to the given one. Similar items can be numerous items in the wide range of their properties, since, in users' point of view, similarity can be any relationship that the given item has with other ones. Despite of this fact, top-k retrieval scheme that most information retrieval (IR) systems employ returns k items whose similarity score computed by the system is high regardless of users' wide range of current interest.
Let us say there is a user wants to find out artists similar to her favorite British Pop artist. Most IR systems adopting traditional top-k retrieval scheme would use the artist as a query and return a list of similar items. The result list may include almost identical British Pop artists on the top, with hundreds of related items below them. It would not be problematic when she likes the top-ranked items, but users are not always satisfied with them. She may know almost all of top-ranked artists, or she may want to try another genre of music related with her favorite one, by filtering out British Pop artists. Her needs could not be satisfied with the plain approach. As some previous work points out, plain top-k list should be refined to satisfy more.
The biggest difficulty lies in that single measurement of similarity does not reveal the taste of users in most cases, and even it is hard to know what the users' taste is, which is one of the goals for this kind of retrieval. Content-based methods can be used to find users' preferred properties for items. But suggestions from content-based systems often annoy users with their huge size. Some work tried to overcome this by adding interactive exploration methods to IR systems. Google image swirl1 and [1] use iterative clustering of returned set of items to make structure of them and to give diversified choices to users. But their approach is not well-suited for similar item finding, because it runs based on users' clear interest. We focus on situations where users' interest is hard to be determined.
To overcome the stated problem, we present Si-Fi (Similar
1Google image swirl, http://image-swirl.googlelabs.com/
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Si-Fi interface overview
Item Finder, http://vega.snu.ac.kr/sifi/). As a visual aid, it helps users to find items they like more intuitively. It exploits explorative querying method and visualizes its exploration in interactive manner. Si-Fi is flexible enough to be applied to any set of items having weighted links describing similarity between all pairs of them. Using the similarity scores, it builds clusters in hierarchy to show structured relationship among items in the domain and lets users to browse inside the hierarchy. The problem of matching similarity scores and users' taste can be solved in users' hand, because users implicitly adjust their desired level of similarity during their browsing.
Si-Fi tries to maximize the browsing capability with several visualization techniques. Dynamic filtering method enables users to filter out uninteresting items. Along with clusters overview, it helps users to determine their direction of navigation. In addition, history browsing feature guides them not to be lost in the graph.
With visual assistances, Si-Fi becomes a useful tool for similar item retrieving. Making use of this tool, users are able to find their desired items in more clear way.
2. ACKNOWLEDGEMENTS
This research was supported by the MKE(The Ministry of Knowledge Economy), Korea, under the ITRC(Information Technology Research Center) support program supervised by the NIPA(National IT Industry Promotion Agency). (grant number NIPA-2010-C1090-1031-0002)
3. REFERENCES
[1] B. Liu and H. V. Jagadish. Using trees to depict a forest. PVLDB, 2(1):133­144, 2009.

704

Suggesting Related Topics in Web Search

Santosh Raju
Microsoft Research India, Bangalore 560080, India
t-sanv@microsoft.com

Shaishav Kumar
Microsoft Research India, Bangalore 560080, India
v-shaisk@microsoft.com

Raghavendra Udupa
Microsoft Research India, Bangalore 560080, India
raghavu@microsoft.com

ABSTRACT
Suggesting topics that are related to user's goal or interest is very important in web search. However, search engines today focus on suggesting mainly reformulations and lexical variants of the query mined from query logs. In this demonstration, we show a system that can suggest related topics for a query based on the top search results for the query. It can help users in exploring the topics related to their information need. The topic suggestion system can be integrated with any search engine or it can be easily installed on the client machine as a browser plugin.
Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing
General Terms: Algorithms, Experimentation
1. SYSTEM OVERVIEW
Web search engine users often seek information on a topic and issue queries to explore the topic. Web directories such as Open Directory allow the user to explore topics in the directory and locate pages that answers his/her information need. However, current web search engines such as Google, Bing and Yahoo ignore topic exploration and focus on directing the user to the most relevant page for his information need. There have been several research efforts dedicated to suggesting Related Searches for a query which are primarily reformulations of the query or lexical variants [1, 2]. However, there is no system which gives topic suggestions for a query and allows the user to explore the topics related to his information need.
This demonstration introduces MSRI Topic Explorer , a system that suggests topics related to the query from the point of view of topic exploration. Specifically, the system provides the following functionalities for topic exploration:
· Contextual Topic Suggestions: For a user query, it leverages the content of the top results of the search engine for the query and dynamically generates a set of topics related to the query for exploration.
· Topic Groups: If the query is ambiguous or multifaceted like "Pathology", the system identifies the multiple senses of the query (first, a medical specialty and second, a movie) and suggests a separate set of topics for each sense. This is different from the existing
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Two Groups of Topics for the query Pathology suggested by "MSRI Topic Explorer".
search results clustering systems like Clusty 1 where result pages for a query are clustered. Our system finds groups of topic suggestions.
Figure 1 illustrates the exemplary results highlighting the functionalities of the system.
2. REFERENCES
[1] Z. Zhang and O. Nasraoui, Mining search engine query logs for query recommendation. In Proceedings of the Thirteenth International World Wide Web Conference (WWW-2006)
[2] Jones, Rosie and Rey, Benjamin and Madani, Omid and Greiner, Wiley, Generating query substitutions. In Proceedings of the Thirteenth International World Wide Web Conference (WWW-2006)
1http://clusty.com/

705

Agro-gator: Digesting Experts, Logs, and N-grams

Michael Huggett
iLab @ Dalhousie University 6100 University Avenue Halifax, Canada 1 902 494 8392
mhuggett@dal.ca

ABSTRACT
As research includes more and larger user studies, a significant problem lies in combining the many types of data files into a single table suitable for analysis by common statistical tools.
We have developed a data-aggregation tool that combines user logs, expert scoring, and task/session attributes. The tool also integrates the n-grams derived from a given sequence of actions in the user tasks. The tool provides a GUI for quick and easy configuration.

...

User Logs

... Expert Scores

... Task Attributes

Agro-gator Configuration

Data Table

Categories and Subject Descriptors
H.5.2 [Information Interfaces and Presentation]: User Interfaces ­ evaluation/methodology, prototyping
General Terms
Measurement, Experimentation, Human Factors, Standardization
Keywords
User studies, data analysis, data aggregation, n-gram analysis.
1. USER DATA AGGREGATION
Combining myriad data sources into a coherent, analyzable set is a non-trivial feat. User logs record multiple simultaneous events, expert scoring defines a "golden set" of correct answers for task problems, and the assigned tasks need to be differentiated by their attributes. We see the goal of data aggregation as combining these factors into a single table prior to correlational analysis.
By contrast, the best-known aggregation tools are of a different scope and character. The qualitative Nvivo package [1] allows users to tag, annotate and link text, video, and sound data in a drag-and-drop interface; the data can thereafter be explored using search and query engines. Similarly, the ATLAS.ti package [2] lets users label and search through complex phenomena hidden in text and multimedia.
2. N-GRAM PROCESSING
N-grams of user activity streams may contain interesting behaviour patterns that can be tied to successful and unsuccessful task performance. To date, studies using n-grams have limited the n-gram lengths: both [3] and [4] limited n-grams to 5 or 6 symbols in length. One goal of our tool is to allow researchers to generate easily all n-grams in a range of desired lengths, for finding correlations of user-action patterns to task attributes.
3. OVERVIEW OF THE PROTOTYPE
The tool currently supports input and output file types in commaand tab-separated value (CSV, TSV), and Excel formats.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1. Multiple input files are configured into a single ouput file suitable for statistical analysis.
Researchers configure the Agro-gator by selecting desired columns and data worksheets from the input files. A GUI allows them (1) to link user responses to an expert's "correct" answers, (2) to link the user's tasks to the task's attributes (e.g. its type and structure), and (3) to choose the stream of user actions that will be fragmented into n-grams.
The n-grams can be filtered to accept or ignore specific actions, and n-gram lengths can be limited to minimum and maximum values, up to the total length of the activity stream. By default, all possible n-grams of length 1..n are generated.
Output is in table format. The default format displays in each row the task and user IDs, the task attributes, the user score for the task, the count per action type, user demographic data, pre- and post-questionnaire data, and one unique n-gram per line along with its attributes (such as length and type). Thus per user task, there will be as many output lines as there are n-grams. Researchers can configure the tool to include or omit any of these facets in the output.
The resulting table can be used as input to large-data statistical analysis packages (e.g. R, SAP, SPSS).This is not a revolutionary tool, but we expect it to be useful.
4. REFERENCES
[1] Nvivo, http://www.qsrinternational.com/products_nvivo.aspx, 2010.
[2] ATLAS.ti, http://www.atlasti.com/ . 2010.
[3] Lin, J. and Wilbur, J. 2009. Modeling actions of PubMed users with n-gram language models. Information Retrieval (12), 487-503.
[4] Tague-Sutcliffe, J. and Toms, E. 1995. Information system design via the quantitative analysis of user transaction logs. 5th International Conference on Scientometrics and Infometrics.

706

Medical Search and Classification Tools for Recommendation

Jimmy Xiangji Huang1, Aijun An2, Qinmin Hu1,2
1Information Retrieval and Knowledge Management Lab, York University, Toronto, Canada 2Department of Computer Science & Engineering, York University, Toronto, Canada
jhuang@yorku.ca, {aan, vhu}@cse.yorku.ca

Categories and Subject Descriptors: H.3.3 Information SystemsInformation Retrieval
General Terms: Design
Keywords: Medical Search, Classification, EMR, Recommendation
1. EXTENDED ABSTRACT
As an increasing number of medical professionals move their patients' records from paper to computer, enormous amounts of electronic medical records (EMR) have become available for medical research. Some of the EMR data are well-structured, for which traditional database management systems can provide effective retrieval and management functions. However, most of the EMR data (such as progress notes and consultation letters) are in free text formats. How to effectively and efficiently retrieve and discover useful information from the vast amount of such semi-structured data is a challenge faced by medical professionals. Without proper tools, the rich information and knowledge buried in the medical health records are unavailable for clinical research and decision-making.
The objective of our research is to develop text analytics tools that are capable of parsing clinical medical data so that predefined search subjects that correspond to a list of medical diagnoses can be extracted. In addition to this particular core functionality, it is also desired that several important assets should be present within the text-analytics tools in order to improve its overall ability to be used as recommendation tools.
In this research, we work with research scientists at the Institute for Clinical Evaluative Sciences (ICES) in Toronto and examine a number of techniques for structuring and processing free text documents in order to effectively and efficiently search and analyze vast amount of medical records. We implement several powerful medical text analytics tools for clinical data searching and classification. For data classification, our tools sort through a great amount of patient records to identify the likelihood of a patient having myocardial infarction (MI) or hypertension (HTN), and classify the patients accordingly. Our tools can also identify the likelihood of a patient being a smoker, previous smoker or non-smoker based on the text data of medical records. All the algorithms and domain knowledge implemented in these tools were provided by medical doctors and domain experts
Copyright is held by the author/owner. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

working in the fields. System evaluations have been conducted and the performance results have been shown to be promising.
2. AN INTEGRATED GUI-BASED TOOL
A GUI-based interface is designed and implemented for the text-analytics tool under a point-and-click environment. Clicking the button of ICES search tool, we can get the starting to the free-text search as shown in Figure 1. The free-text search tool is designed to retrieve records based on keywords such as "MI". The search results display the key sentences in the medical records containing the keywords. Clicking the button of ICES classification tool, we can use three classification tools summarize the records of each patient in Figure 2. Three classification tools classify patients based on their likelihoods of having MI or hypertension, or on their smoking status.
Figure 1: ICES Search Tool
Figure 2: ICES Classification Tool
3. ACKNOWLEDGEMENTS
This research is supported in part by research grants from the Institute for Clinical Evaluative Sciences, Canadian Institutes of Health Research (CIHR) and Ontario Ministry of Research & Innovation (MRI).

707

Multilingual People Search

Shaishav Kumar
Microsoft Research India Bangalore - 560080, India
v-shaisk@microsoft.com

Raghavendra Udupa
Microsoft Research India Bangalore - 560080, India
raghavu@microsoft.com

ABSTRACT
People Search is an important search service with multiple applications (eg. looking up a friend on Facebook, finding colleagues in corporate email directories etc). With the proportion of non-English users on a steady rise, people search services are being used by users from diverse language demographics. Users may issue name search queries against these directories in languages other than the language of the directory, in which case the present monolingual name search approaches will not work. In this demo, we present a Multilingual People Search system capable of performing fast name lookups on large user directories, independent of the directory language. Our system has applications in areas like social networking, enterprise search and email address book search.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation, Languages
1. SYSTEM OVERVIEW
People Search is a well studied problem, but not in a multilingual setting. One could adapt existing monolingual sytems to the multilingual environment by transliterating user queries to the language of the directory. However, such a system would have low accuracy and high response time because of the intermediate transliteration step making it impractical.
We tackle the problem by representing names using a language/script independent representation learned by employing Machine Learning techniques [1],[2]. The key idea is to treat names written in two languages/scripts as parallel views of the same semantic object. Given a training data set consisting of parallel name tokens in two languages, we employ Cannonical Correlation Analysis to learn hash functions for each language. These hash functions are used for indexing the directories. Subsequently, when performing a name search, the query is hashed and matching results are retrieved from the directory. The compact and computer friendly hash codes make indexing large directories possible and offer efficient retrieval performance even on simple commodity hardware.
Some key advantages of our system are:
· Data driven: Requires small training data. Cur-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Multilingual Email Address Book Search and Multilingual Wikipedia Search showing results for the user query Albert Einstein (typed in English)
rently supported languages are English, Russian, Hebrew, Hindi, Tamil, Telugu, Kannada and Bengali. Other languages can be easily supported.
· Fast Response Time and Scalability : The use of hash codes offers fast response time with minimal resource requirement making it highly scalable.
We have evaluated our system against a state-of-the-art transliteration based baseline and achieved substantially better accuracy and response time figures [1].
The demo will showcase Multilingual Email Address Book Search and Multilingual Wikipedia People Search. Figure 1 shows snapshots of both applications. Results are shown for the query Albert Einstein in English, fetched from a Russian directory in the first case and from a list of Russian wikipedia titles in the second.
2. REFERENCES
[1] R. Udupa and M. Khapra. Improving the multilingual user experience of wikipedia using cross-language name search. In Proceedings of NAACL-HLT 2010.
[2] R. Udupa and M. Khapra. Transliteration equivalence using canonical correlation analysis. In Proceedings of ECIR 2010.

708

Closed Form Solutions of Similarity Algorithms

Yuanzhe Cai, Miao Zhang, Chris Ding, Sharma Chakravarthy
CSE Department The University of Texas at Arlington
{yuanzhe.cai, miao.zhang}@mavs.uta.edu, chqding@uta.edu, sharma@cse.uta.edu

ABSTRACT
Algorithms defining similarities between objects of an information network are important to many IR tasks. SimRank algorithm and its variations are popularly used in many applications. Many fast algorithms are also developed. In this note, we first reformulate them as random walks on the network and express them using forward and backward transition probability in a matrix form. Second, we show that P-Rank (SimRank is just the special case of PRank) has a unique solution of eeT when decay factor c is equal to 1. We also show that SimFusion algorithm is a special case of P-Rank algorithm and prove that the similarity matrix of SimFusion is the product of PageRank vector. Our experiments on the web datasets show that for P-Rank the decay factor c doesn't seriously affect the similarity accuracy and accuracy of P-Rank is also higher than SimFusion and SimRank.

Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms Keywords: Linkage Mining, Similarity Calculation

1. SIMRANK AND P-RANK ALGORITHMS
SimRank[1] is a method of measuring linkage-based similarity between objects in a graph that models the object-to-object relationships for a particular domain. The intuition behind SimRank similarity is that two objects are similar if they are linked by the similar objects. However, SimRank only consider in-link information on the information network but in fact out-link information is also useful for the similarity calculation on the real network. Thus, P-Rank[2] extends SimRank intuition and consider both out-link and in-link information. The intuition of P-Rank is that "two objects are similar if (1) they are linked by the similar objects; and (2) they link the similar objects."

We proceed to present the formula to compute P-Rank. Given a

graph G(V, E) consisting of a set of nodes V and a set of links E, the

P-Rank similarity between objects a and b, denoted as S(a,b), is

computed recursively as follows:

1

  

S (a, b)

=

 



|

c I (a) ||

|I ( a )| |I (b )|

I (b) | i=1

S (Ii (a), I j (b))
j =1

+

  
(1- 

)

|

O

(

a

)

c ||

O

(b)

|

|O ( a )| i =1

|O (b )| j =1

S

(Oi

(a

),

O

j

(b

))

(a = b) (a  b) (1)

, where c is a constant decay factor, 0 c 1 and [0, 1] is used to

adjust the weight of in- and out-link. I(a) is the set of in-neighbor nodes of a and Ii(a) is the ith in-neighbor node of a. |I(a)| is the
number of in-neighbors of node a. O(a) is the set of out-neighbor nodes of a and Oi(a) is the ith out-neighbor of node a. |O(a)| is the
number of out-neighbor of node a. In addition, if  is equal to 1, P-

Rank boils down to SimRank.

A solution to P-Rank equation (1) can be reached by iteration to a

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

fixed-point. For each iteration k, let Sk(.,.) be an iteration similarity function and Sk(a, b) be the iterative similarity score of pair (a, b) on kth iteration. The iteration process is started with S0 =I and iterate with

   Sk+1(a,b)

=

|

c I(a) ||

I(b)

|

|I (a)| i=1

|I (b)| j=1

Sk

(Ii

(a),

I

j (b))+(1-

)

|

c O(a) || O(b)

|

|O(a)| |O(b)| i=1 j=1

Sk

(Oi

(a),Oj

(b))

(2)

Equation(2) is the sum of two items: the former item uses

backward random walk to calculate the similarity score (i,e, walk

in opposite direction of the hyperlinks) and the other item uses

forward random walk to compute the similarity score(i,e, walk in

direction of the hyperlinks). Theoretically, let L be the link

matrix, i.e., Lij = 1 if there exists an edge pointing from node i to node j. Then, the backward transition probability Bij is equal to (1/|I(i)|)LijT and the forward transition probability Tij is equal to (1/|O(i)|)Lij. Thus, we give the matrix expression of P-Rank.

Lemma 1. Let T be the forward transition probability, B be the
backward transition probability, I be identify matrix, S be the similarity matrix, and Sk be the kth-iteration similarity result. We have S0 = I, Sk+1 = I + (cBSkBT)offdiag +(1-)(cTSkTT)offdiag, where
Aoffdiag = A ­ diag(A).

2. CLOSED FORM SOLUTION OF P-RANK
We discuss the closed form solution of P-Rank, when c =1.
Theorem 1. Let e =[1,1,...,1]nT and S the similarity matrix . If c = 1, S = eeT is the unique result of P-Rank equation. Proof: When the iteration converges, Sk= Sk-1=S. For c=1, We have S = I + (BSBT)offdiag +(1-)(TSTT)offdiag. Thus, Soffdiag = (BSBT)offdiag +(1-)(TSTT)offdiag. T is a transition matrix. Be definition Te=e. Thus eTTT = eT, and Te eTTT = eeT . Similarly, because B is a transition probability, Be eTBT = eeT . Substituting S=eeT verifies that it is the solution. Also, diag(eeT ) = I satisfy the diagonal condition. Because of the
diagonal dominance of the iteration, there is unique solution to the problem. Thus, if c = 1, S = eeT is unique solution of P-Rank. 
According to theorem 1, if c = 1, similarity matrix will converged into eeT. Therefore, for the real world applications, we should
choose the decay factor c less than 1.

3. SIMFUSION ALGORITHM
SimFusion[3] has been proposed as an effective in calculating the similarity score of objects in graphs. The motivation of SimFusion is similar to that of SimRank: "the similarity between two objects can be reinforced by the similarity of related data".
SimFusion algorithm employs a non-negative, irreducible and row-stochastic matrix, denoted by P, to capture all the relationships between nodes. Each element Pij represents the relationship between objects i and j. In order to prevent similarity sinks, in SimFusion, for a node with no links to the other nodes in the graph, we set the elements in the corresponding row of relationship matrix P to 1/n.(n is the number of nodes in the graph) SimFusion reinforcement equation is represented as follows:

709

The iteration process starts with S0:

S0 = I

(3)

To calculate Sk+1 from Sk, we have the following equation:

Sk+1= PS1PT

(4)

where the matrix P = [T +(1-)eeT/n], e is the column vector

which contains all of ones , T is a forward transition matrix and 

is a constant between 0 and 1 and set  = 0.85 in our experiment.

This P is also identical to PageRank transition probability.

4. CLOSED-FORM SOLUTION OF
SIMFUSION
If we don't consider the elements on the diagonals of the similarity matrix (in real world application we don't care about that), SimFusion is the special case of P-Rank. When c = 1 and  = 0, P-Rank is reduced to SimFusion. According to the theorem 1, SimFusion algorithm will converge into eeT and that can't be used for the real world application. That means SimFusion has some problems for its expression. Thus, we modify the SimFusion algorithm.

The SimFusion score is computed with iterations as follow:

The iteration process starts with S0: S0 = I To calculate Sk+1 from Sk, we iterate using the equation: Sk = PTSk-1P.

(5) (6)

One of our main results of this paper is the following the closed-

form solution of SimFusion:

Theorem 2. Let  be the stationary distribution of P. SimFusion scores S =  T.

Proof: According to the definition of matrix P, P is a non-
negative, irreducible and row-stochastic matrix. Thus,  TP =  T and PT  =  . Therefore, PT  TP =  T
Thus, S =  T is the final similarity score. 

According to theorem 2, we are easy to see that similarity matrix of SimFusion is the product of PageRank vector. PageRank score of one node describes the possibility of a surfer staying at each node on the graph and SimFusion score of two nodes describes the possibility of two surfers meeting with each other starting respectively from these two nodes. Therefore, we can calculate PageRank vector first and then product these two PageRank vector together to get the finial similarity matrix. In this way, the time complex of this method is O(Kn2), but original SimFusion's time complex is O(Kn3). K is the iteration time and n is the number of objects.

5. EXPERIMENTAL RESULTS
For evaluating our methods, we used WebKB datasets[4] and these web pages crawled from these four universities' website are manually divided into seven classes, such as students, faculty, staff, department, course, project and others. Considering that for a webpage w1 in the graph, these algorithms will return a rank list of relative pages. For each web page in the list, if this web page's class is the same as page w1, these two pages are much related and grade 2 at that position; otherwise grade 0. Then, we use the NDCG[5] to evaluate the performance of similarity ranking list. We calculate NDCG with in 10 related web pages for each object in each dataset and get the average score to evaluate the performance in the experiments. The detail information about datasets shows at tab. 1.

Table 1. Statistic of Datasets

Dataset Cornell Texas Wisconsin Washington

Vertices#(n) 867 827

1263

1205

Edges#(e) 1496 1428 2969

1805

We test the decay factors c for SimRank and P-Rank( = 0.5). In

this experiment, we vary c from 0.05 to 0.95. In fact, the effect of

different decay factor c is not very obvious and the scope of

accuracy for different c values is just 0.02 for SimRank and 0.01

for P-Rank. Fig.1 also shows that when c is between 0.25 to 0.45,

SimRank will receive the highest scores and when c is between

0.55 to 0.9, P-Rank will get the highest scores for these datasets.

Figure 1. decay factor c

We also test the accuracy of SimFusion algorithm. In tab. 2, it shows that the accuracy of SimFusion is lower than SimRank and P-Rank.

Table 2. Accuracy of Algorithm(NDCG@10)

Alg.

Data.

SimRank

SimFusion

P-Rank

Cornell 0.7106 0.3556 0.7822

Texas 0.8197 0.3643 0.8610

Wisconsin 0.6860 0.4063 0.7274

Washington 0.5916 0.5903 0.6564

6. CONCLUSIONS
In this paper, we presented the closed form solution of P-Rank (c = 1) and SimFusion. Firstly, when c = 1, S = eeT is the unique result of P-Rank equation. In addition, in our experiment, the effect of different decay factor c is not very obvious. Secondly, when c = 1 and  = 0, P-Rank is reduced to SimFusion. That means S = eeT is the final solution of SimFusion. Thus, we modify the SimFusion and give the closed form solution of SimFusion algorithm.

Acknowledgements. Work supported by NSF-DMS-0915228,
NSF-CCF-0939187, University of Texas Regent STARS Award.

7. REFERENCES
[1] J. Glen and J. Widom, SimRank: a measure of structural-context
similarity, pp:538-543, SIGKDD, 2002 .
[2] P.X. Zhao, J.W. Han and Y.Z. Sun, P-Rank: a comprehensive structural
similarity measure over information networks, pp:553-562 CIKM, 2009.
[3] W.S. Xi, E.A. Fox, W.G. Fan, B.Y. Zhang, Z. Chen, J. Yan, D. Zhuang,
SimFusion: measuring similarity using unified relationship matrix, pp:130-137, SIGIR, 2005.
[4] CMU Four University Dataset,
http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/
[5] K. Jvelin and J. Kenen, Cumulated gain-based evaluation of IR
techniques, ACM Trans. on Inform. Systems, pp:422 ­ 446, 2002.

710

SIGIR: Scholar vs. Scholars' Interpretation
James Lanagan and Alan F. Smeaton
CLARITY: Centre for Sensor Web Technologies Dublin City University Dublin, Ireland
{jlanagan, asmeaton}@computing.dcu.ie

ABSTRACT
Google Scholar allows researchers to search through a free and extensive source of information on scientific publications. In this paper we show that within the limited context of SIGIR proceedings, the rankings created by Google Scholar are both significantly different and very negatively correlated with those of domain experts.
Categories and Subject Descriptors
H.1.2 [MODELS AND PRINCIPLES]: User/Machine Systems--Human information processing
General Terms
Algorithms, Experimentation, Human Factors
1. INTRODUCTION
The launch of Google Scholar1 (GS) in late 2004 meant that scholars were suddenly provided with a free and extensive source of scientific information for searching and citing. Even though this resource is free, it has been shown to compare well with the performance of paid indexes such as the Web of Science2 [1]. But one of the largest criticisms levelled against GS is its lack of transparency in its ranking methods: "Google Scholar aims to sort articles the way researchers do, weighing the full text of each article, the author, the publication in which the article appears, and how often the piece has been cited in other scholarly literature. The most relevant results will always appear on the first page."
In this paper we use 10 years of SIGIR conference papers as a testbed for comparing expert rankings vs. GS rankings, and we reach some surprising conclusions.
2. IMPLEMENTATION
The first study of SIGIR conference proceedings was performed as part of the 25th anniversary celebrations of the SIGIR conference [4], and later extended for the 30th year of the conference [2]. These studies focused on the closed collection of SIGIR papers and have not, to our knowledge, taken into account any citations of SIGIR papers by papers
1http://scholar.google.com/intl/en/scholar/about.html 2http://isiknowledge.com/
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Table 1: Number of documents ranked by our ex-

perts for each of the topics chosen.

Topic

Query

Documents Ranked

Collaborative Filtering (CF) "collaborative filtering"

10

Distributed IR (DR)

"distributed retrieval"

8

Document Clustering (DC) "document clustering"

10

Image Retrieval (IR)

"image retrieval"

11

Language Modeling (LM)

"language model"

12

Latent Semantic Indexing/Analysis (LS)

"latent semantic"

12

Linkage Analysis (LA)

"link analysis"

10

Question Answering (QA) "question answer"

9

Relevance Feedback (RF)

"relevance feedback"

10

Spam (S)

"spam"

6

Text Summarisation (TS)

"text summarization"

9

Topic Distillation (TD)

"topic distillation"

8

either external or internal to SIGIR. That previous work [4] also clustered the first 25 years of SIGIR proceedings into several distinct and reoccurring topics. We have now identified a new larger set of topics which cover the years (1997-2007). Using the session names from within each SIGIR conferences together with the cluster names from [4], we identified 12 topics (Table 1) covering long-standing interests of the IR community, as well as new interests such as spam, and these are used in our experiments.
To create a dataset for searching we built an extended SIGIR citation graph from a 10 year window (1997-2007) of full papers in SIGIR proceedings3. Within this there are over 4,000 authors, 770 SIGIR publications and an additional 2,100 non-SIGIR publications which cite these SIGIR articles. We have calculated PageRank scores for every document, allowing us to generate ranked lists.
We used our 12 topics to generate a list of documents to present to experts by combining the top 30 documents returned from a query against GS (a restricted query returning papers from the SIGIR proceedings published between 19972007) with a ranked list returned for the same query against our extended SIGIR citation network, and then using the top-ranked papers that appeared in both lists.
We then asked 14 expert users from 3 different university information retrieval research groups4 to provide rankings for each topic's list of documents. For each topic, experts were given the first page only from each paper and asked to provide a ranking suitable for a novice research student
3Our time-window was defined as a result of the limited availability of machine-readable documents prior to 1997. 4Dublin City University, University College Dublin, and Glasgow University.

713

interested in the topic. Broad queries against Google Scholar had specifically been used to simulate a novice user searching for relevant papers about a topic. An explanation of the rankings was requested, as well as a topic expertise rating of 1 ("I have had no real experience of this topic" ) to 5 ("I am knowledgeable in this topic" ). The main reasons given for ranking papers highly were author, institution, scope, content, and year of publication5.
While the reasons that experts gave for highly ranked documents overlapped greatly, the rankings themselves were not uniform. Overall we collected 1,082 document judgements with an average of 7 judgements per paper, and 6 of the 14 experts ranked all 12 topics.
We used the Kendall coefficient of concordance (W ) to measure inter-rater agreement [3], showing significant agreement within each topic's expert rankings6. This enables us to use the median expert rank of each paper within a topic to create a new combined ranking for that topic. In cases where the median of two papers' ranks are equal, the mean ranks are used to decide the ordering.
3. COMPARING SCHOLAR TO EXPERTS
Correlations between the combined experts' rankings and those created by GS lead us to believe that the rankings that GS is modelling are far from expert: GS's algorithm seems to use features available through direct analysis of the papers, quite the opposite to expert assessors who may call upon past experience and prior knowledge -- prior knowledge that increases with the level of self-determined expertise of the ranking expert.
Figure 1: The per-topic correlation of expert and scholar rankings.
This can be seen in the decrease in correlation between pertopic expert rankings and GS rankings as the average (mean) expertise of that topic's experts increases (Figure 1). The reason for issuing broad and non-specific queries to GS as shown in Table 1 is to simulate the inexperienced user who comes to our experts with a selection of papers and no clear idea of their relative values. We expected the GS vs expert rankings to correlate well with each other regardless of expertise. Instead, there is a -0.7922 correlation between rankings as expertise increases. This leads us to the following conclusion: The rankings provided by Google Scholar are 5No expert was asked to rank papers that they had authored, nor any from their own institution. 6This measure was used due to the ordinal nature of our data. Although the experts created the rankings independent of each other, the ranking a document receives is not independent of the other documents.

Figure 2: The correlation of per-expert and scholar rankings, divided into differing levels of expertise.
most similar to those provided by experts who have little expertise in the area and can bring no prior knowledge to bear on their ranking.
If we now look at the per-expert correlations with the GS ranking as shown in Figure 2, we see that whilst the correlation is not as strongly negative as on a per-topic basis it remains negatively correlated. The graph does not use within-topic agreements of rankings amongst the experts, looking only at the level of agreement between each selfassigned expertise level's ranking and that of GS. It is interesting nonetheless that the divergence of expertise and GS is repeated at this level also.
4. CONCLUSIONS
While it may be argued that the ranking Google Scholar provides is designed to best fit user expectation and need, we do not feel that this ranking is optimal for broad topics. It appears from our results that the expectation being met is that of someone unfamiliar with the area being queried, and this is less desirable than having an expert rank output. This is an interesting observation given the continued rise of Google Scholar as a source for researchers, and one we feel is worthy of further investigation.
Acknowledgments
This work is supported by Science Foundation Ireland under grant number 07/CE/I1147.
5. REFERENCES
[1] A. Harzing and R. van der Wal. Google Scholar: The Democratization of Citation Analysis? Ethics in Science and Environmental Politics, 8(1):61­73, Jan 2007.
[2] D. Hiemstra, C. Hauff, F. Jong, and W. Kraaij. SIGIR's 30th Anniversary: An Analysis of Trends in IR Research and The Topology of Its Community. ACM SIGIR Forum, 41(2), Dec 2007.
[3] M. Kendall and B. Smith. The Problem of m Rankings. Annals of Mathematical Statistics, 10(3):275­287, 1939.
[4] A. Smeaton, G. Keogh, C. Gurrin, K. McDonald, and T. Sødring. Analysis of Papers From Twenty-Five Years of SIGIR Conferences: What Have We Been Doing For The Last Quarter of a Century? ACM SIGIR Forum, 37(1), Apr 2003.

714

A Method to Automatically Construct a User Knowledge

Model in a Forum Environment

Ahmad Kardan
Advanced E-Learning Technology Lab
Amirkabir University of Technology Tehran, Iran
aakardan@aut.ac.ir

Mehdi Garakani
Advanced E-Learning Technology Lab
Amirkabir University of Technology Tehran, Iran
garakani@aut.ac.ir

Bamdad Bahrani
Advanced E-Learning Technology Lab
Amirkabir University of Technology Tehran, Iran
bamdad.bahrani@aut.ac.ir

ABSTRACT
Having a mechanism to validate the opinions and to identify experts in a forum could help people to favor one opinion against another. To achieve this, some solutions have already been introduced, including social network analysis techniques and reputation modeling. However, neither of these solutions considers the users' knowledge to identify an expert. In this paper, a novel method is proposed which estimates users' knowledge based on the forum itself, and identifies the possible areas of expertise associated with each user.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process; H.3.4 [Information Storage and Retrieval]: Systems and Software user profiles and alert services.
General Terms
Algorithms, Measurement, Performance, Experimentation
Keywords
Expert finding, Knowledge model, Forum, Information retrieval
1. INTRODUCTION
Nowadays, forums are known as a great source of raw information and having them analyzed could benefit both educational and commercial sectors. One problem with forums is how someone could be convinced during the discussion and how the convergence is made. Another problem arises when the discussion is over, and someone needs to make a decision to favor one opinion against another. These issues have been partially dealt with using expert finding systems and reputation models.
User knowledge appears to be the most important feature of a user that should be considered to solve the mentioned issues in forums. Therefore, a knowledge model of a participant is needed. A knowledge model could be designed and created in several ways which are discussed in detail in [1]. In this paper we propose a novel method to estimate user knowledge in forums. By using the proposed method, the knowledge level of every participant in each fragment of the domain knowledge would be estimated. This method uses a concept map or related ontology as the structural knowledge model of a domain.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. RELATED WORK
Expert finding roles as the motivation and objective of this paper, while concept map scoring is a related research area which serves as the general idea for assessing one's knowledge. Expert finding refers to studies aiming at finding people with relevant expertise. There are two main approaches to finding experts. The first approach has focused on social network analysis. Another approach to expert finding is to rank people within an organization based on their level of expertise about a given topic.
Concept map is a graphical knowledge representation method which includes some nodes indicating concepts and labeled links describing the relationship between two concepts [2]. Concept mapping can also be used as an assessment method for eliciting the conceptual knowledge that a student holds in a subject domain [4]. In order to score a concept map, a criterion map (also called a reference map) is required. Comparing specific features of a given map with the same features in the criterion map, a score would be computed. There are different concept map scoring methods including measuring structural similarity, exact node matching and exact propositional matching [4]. In [3] a method is proposed to automatically generate students' conceptual models from answers in plain text based on the frequency of a term used by a learner. In [5] three attributes are presented in order to score a concept map. 1) The volume of a concept map that refers to the total number of relations used in the learners' concept map. 2) The `ruggedness', which is the division of a concept map into unconnected sub-maps. 3) The amount of accurate propositions in relation to the volume. A low ruggedness indicates that the knowledge structure is consistent.
3. SOLUTION
The assumption of the proposed method is that the conceptual network for the domain of a forum should exist to serve as the structural knowledge model of the domain. A fragment of the domain knowledge or simply a specific subject matter usually consists of a subset of interrelated concepts in a conceptual network. The fragments could be considered as fixed subsets or defined dynamically when needed based on a risen topic.
Considering reviewed related works, the following three parameters would be leveraged to estimate the knowledge of a user in each fragment of the domain model resulting in an estimation of the user knowledge model extracted from a particular forum:
1. The number of times a concept in the conceptual network is used in the posts associated to the user.

717

2. The number of connected components in the specific fragments.
3. The number of concepts mentioned by the user in the posts associated to the user.

Figure 1. Connected components in a fragment

As shown in Figure 1, there are three types of links connecting

the concepts. Thick black links connect two concepts that are both

mentioned at least once in the posts associated to the user. The

thin grey links connect two concepts, at least one of which is

never mentioned in the posts associated to the user. Dotted links

connect one concept inside the fragment area to an outer concept

which is in the other fragments. Furthermore, the numbers near

each concept represent the number of times a concept is used by

the user. The proposed method will estimate the knowledge of a

user in a specific fragment of the domain knowledge based on

Formula 1:

(

l N

)

Score(k) =

n mi

×(

cij )

i=1 j=1

(1)

n

In Formula 1, k is the fragment which is scored, n is the number of connected components, mi is the number of concepts in the connected component i, cij is the number of times a specific concept is repeated in the posts associated to the user, l is the number of concepts mentioned by the user, N is the total number of concepts, and , and are weight variables that could be assigned to each parameter for achieving better results.

4. DISCUSSION AND EXPERIMENTS
Firstly, it should be noted that concepts counting for a user should be performed in all of the posts created by the user and not just the posts in a specific thread. Secondly, a user knowledge model should be updated every time he/she posts a message in a thread. Having the participants of a forum ranked based on their estimated level of knowledge in the specific fragment, the discussion could be conducted in a more reasonable manner and an outer observer can acquire the conclusion more easily.
The granularity level of fragments may affect the result of Formula 1. Hence, the more domain knowledge is represented in

detail including many nodes, the more accurate the result of Formula 1 is. Furthermore, Formula 1 could be applied on the whole knowledge structure if the domain knowledge is considered as a total fragment. In this case, connected components will present the areas where a user is likely to be an expert in. Those areas could then be reported as expertise areas pertaining to the user.
Java Forum (http://forums.sun.com/index.jspa) is a well-known forum where a wide range of users share and discuss issues related to Java. We chose Java forum to evaluate our proposed method because of the following reasons: 1) Java technology concept map is provided by Sun. We used this concept map with a little modification as the domain knowledge structure. 2) A program called Duke Stars is being run by Java Forum. To evaluate the proposed method, we chose one hundred members whose earned Duke Stars were close enough from Java Forum. We applied the proposed method to the posts associated to each selected users. We analyzed their level of expertise using Formula 1 considering the whole Java concept map.
The Spearman rank correlation between the ranking based on Duke Stars and the ranking made using the calculated scores was 0.88 after adjusting the weights.
5. CONCLUSION AND FUTURE WORK
In this paper we presented a method to determine the level of expertise of each user participating in a forum. The proposed method can estimate a user's knowledge, leveraging the forum itself. This method could be used in every forum where introduced mechanisms such as reputation models and social network analysis cannot distinguish users with specific areas of expertise. Currently, we are working on a more precise evaluation of the proposed method by implementing it completely for Java Forum. Calculating the efficient values for weight variables in the proposed Formula, as well as defining other parameters to make the proposed method more effective, is under investigation.
6. REFERENCES
[1] Brusilovsky, P. and Millan, E. 2007. User Models for Adaptive Hypermedia and Adaptive Educational Systems. Lecture Notes in Computer Science, Springer.
[2] Novak, J. D. and Cañas, A. J. 2008. The Theory Underlying Concept Maps and How to Construct and Use Them, Technical Report IHMC CmapTools 2006-01 Rev 01-2008.
[3] D. Pérez-Marín , E. Alfonseca , P. Rodríguez , I. PascualNieto. 2007. Automatic Generation of Students' Conceptual Models from Answers in Plain Text, Proceedings of the 11th international conference on User Modeling.
[4] Park, U., Calvo, RA. 2008. Automatic Concept Map Scoring Framework Using the Semantic Web Technologies. Proceedings of Eighth IEEE International Conference on Advanced Learning Technologies, Santander, Cantabria.
[5] Schaal, S. 2008. Concept Mapping in Science Education Assessment: An Approach to Computer-Supported Achievement Tests in an Interdisciplinary Hypermedia Learning Environment. Proceeding of the Third Int. Conference on Concept Mapping, Finland.

718

Learning to Rank Audience for Behavioral Targeting

Ning Liu1 Jun Yan1 Dou Shen2 Depin Chen3 Zheng Chen1 Ying Li2

1Microsoft Research Asia
Sigma Center, No.49, Zhichun Road
Beijing, 100190, China

2Audience Intelligence Microsoft Corporation One Microsoft Way Redmond, WA 98052, USA

3Department of Computer Science University of Science and Technology
of China Hefei, P.R. China

{ningl, junyan, zhengc}@microsoft.com

{doushen, yingli}@microsoft.com

depin.chen@gmail.com

ABSTRACT
Behavioral Targeting (BT) is a recent trend of online advertising market. However, some classical BT solutions, which predefine the user segments for BT ads delivery, are sometimes too large to numerous long-tail advertisers, who cannot afford to buy any large user segments due to budget consideration. In this extend abstract, we propose to rank users according to their probability of interest in an advertisement in a learning to rank framework. We propose to extract three types of features between user behaviors such as search queries, ad click history etc and the ad content provided by advertisers. Through this way, a long-tail advertiser can select a certain number of top ranked users as needed from the user segments for ads delivery. In the experiments, we use a 30days' ad click-through log from a commercial search engine. The results show that using our proposed features under a learning to rank framework, we can well rank users who potentially interest in an advertisement.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ retrieval models; H.4.m [Information Systems Applications]: Miscellaneous.
General Terms: Economics, Experimentation
Keywords Online Advertising, Behavioral Targeting, Learning to Rank
1. INTRODUCTION
In recent years, user-targeted advertising is booming in industrial circles by collecting and analyzing user data in many aspects such as online behavior, demographic and geographic information. Among various sources of user data, the richest user behavioral data exhibit the biggest potential for further exploitation. Many major search engines are providing Behavioral Targeting (BT) [1] solutions to advertisers through acquiring BT companies or bringing behavioral targeting technique into their existing products. Generally speaking, the output of many BT solutions is hundreds of user segments, within each of which users are believed to have similar interests. Then, each segment is given a snippet or name like "sports enthusiasts" for advertisers to judge which segments they should buy for ads delivery. However, in this way, the relatedness between the user segment and the advertiser is assessed by human experience rather than from data mining directly. Moreover, since a single user segment can have millions of users, long-tail advertisers may not have enough budgets to buy a complete segment. They are supposed to know which part of users in a segment is the best audience desired by them, which results in a user ranking problem.
Copyright is held by the author/owner(s).
SIGIR10, July 19-23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

In this paper, we propose to rank users according to their potential interests on a given advertisement. Similar to the learning to rank problem in the field of information retrieval for document ranking, we consider our problem in the same framework by treating "ads" as "queries" and "users" as "documents". We propose three types of features based on the user behavioral data and the ad content. The behavioral data include users' query history and ad click history. The first type of features reflects the direct content matching between users' behavioral data and the ad content. For the second type of features, we map both user behaviors and ads contents into high-level topics and then compute their topic similarity. Finally, the third type of features gives some priori user behavioral statistics, e.g. global ad clickthrough rate (CTR), a high value of which indicates that the user may have habit to view ads and perform clicks. Using these user behavior oriented features, we use a subset of ads together with users who ever viewed or clicked these ads as training data to learn a user ranking model. The experimental results show that using our proposed features under a learning to rank framework, we can well rank users who potentially interest in an advertisement.
2. LEARNING TO RANK AUDIENCE
Suppose an advertiser from a certain industry wants to run his ad campaigns online and comes to an online ad publisher (e.g. a search engine or an ad network). He knows much about his products which are going to be advertised, but has no idea about the online audience. There are billions of people on the Internet, and hence it is impossible and unaffordable to deliver his ads to all of them. Although he may buy some predefined user segments with descriptions from the BT companies, the number of user could be large for many long-tail advertisers. The cost of buying user segments will exceed his budget. To make efficient use of his budget, he wants to find a smaller part of best audience within a user segment, and only displays ads to them during online activities. To address this problem, we propose a user ranking mechanism according to the advertiser's ad campaigns.
Analogous to learning to rank documents in document retrieval [2], in the user ranking problem, there should be queries, document repository, relevance label definition and feature set. In terms of learning to rank audience, the "query" is the advertisement a. An online user u is treated as a "document", with the label l representing whether this user has viewed or clicked the given ad, where l=-1 stands for the user u has never viewed a, l=0 stands for u has viewed a but did not click it, and l=1 stands for u has viewed a and clicked it. In this work, we make use of two types of behavioral data: user search query history and user ad click history. Features are extracted from the ad and these behavioral data simultaneously. We list the description of all three types of features we propose to use in Table 1. Using this set of

719

features, which are represented by a feature vector x, for each user-ad pair with label l, i.e. <u, a, l>, we can train a ranking model under the learning to rank framework. And then, for any new ad and all candidate users, we can rank the audience according to their probability to click the given ad.
Table 1. Feature description

No. feature name

description

Relevance features

#1

`query history to ad BM25 score computed between a user's

title' similarity

query history and the title of the given ad.

`query history to ad BM25 score computed between a user's

#2 description

query history and the description of the

similarity

given ad.

#3

`query history to ad page similarity

BM25 score computed between a user's query history and the landing page content of the given ad.

`clicked

query BM25 score computed between a user's

#4 history to ad page query history (queries with ad click) and

similarity

the landing page content of the given ad.

`non-click query BM25 score computed between a user's

#5 history to ad page query history (queries without ad click)

similarity

and the landing page of the ad.

#6

`clicked ad history to ad page similarity

BM25 score computed between a user's ad click history and the whole content of the given ad.

Topic-based relevance features

#7

`query history to ad Cosine similarity of topics between query page topic similarity history and the ad landing page content.

`clicked ad history Cosine similarity of topics between ad

#8 to ad page topic click history and the ad landing page

similarity

content.

Priori behavioral statistics features

#9 user global CTR

User's global ad click CTR in the user behavioral history log.

#10 topic-specific CTR

User's ad click CTR when inputting queries which have the same topic with the given ad.

#11

topic-specific %query

The percentage of queries in user query history that have the same topic with the given ad.

#12

topic-specific %adclick

The percentage of user clicked ads having the same topic with the given ad.

#13 # user query topic

The number of topics covered by the user submitted queries in history.

#14

# user ad-click topic

The number of topics covered by the ads that have been clicked by a user.

After surveying existing learning to rank methods, we choose Ranking SVM [2] as our main method due to its proven efficacy in prior research. Running Ranking SVM is equivalent to solving the following optimization problem.

 min ,ij

1 2

 2 C

ij

s.t. , xi  xj  1ij ,xi  xj ,ij  0

where the subscripts i and j are used to distinguish different feature vectors extracted from behaviors of different users.

Suppose the solution of this optimization problem is * and a

series of ij , the obtained ranking model is,

f (x)  *, x .

3. VALIDATION
For experiments, we use a dataset comes from a commonly used commercial search engine which lasts for a full month. It contains the fields such as user ID, query text of user, the displayed ad to user, the clicked ad by user etc. We select 100 frequently clicked ads from the ad repository for demonstration. We only consider the users who have viewed and/or clicked these 100 ads for ranking. To label users for experiments, if a user clicked an ad, he is labeled as relevant to the ad; the users who have been displayed the ad but did not click are labeled as irrelevant. For all users correlate to the 100 ads, we create the profile for each user, including the user query history and ad click history. Then, for each ad, we extract the 14 features defined in Table 1to train the ranking model. We randomly divide the whole user dataset into 5 folds and we run the experiments for 5 times. In each round, 1 fold is used as testing dataset and others are used for training.
Figure 1 shows the normalized distributions of relevant users in the ranking list. The x-axis indicates positions in the ranked list. The y-axis indicates the proportion of relevant users in a specific position over the 100 ads. From the figure we can see that through using our audience ranking solution, more relevant users are ranked to higher positions.
0.05
0.045
0.04
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
Figure 1. The distribution of relevant users Over the 100 ads and all related users in our dataset, the averaged ad CTR is about 2.02%. After ranking, at the first position of each ad, CTR of ad is as high as 16.7% and 11.2% at the second position. This means we can well rank users who potentially interest in an advertisement.
4. CONCLUSION
In this paper, we propose a novel user ranking problem to answer "whom to deliver ads" for online advertisers. We utilize the user behavioral data including user search query history and ad click history to create the user profile. 14 features are defined and extracted between user profile and the content of a given ad. We then embed the user ranking problem into the learning to rank framework and employ Ranking SVM to obtain the ranking model. Experimental results show that our approach can effectively rank the relevant users on top.
5. REFERENCES
[1] J. Yan, N. Liu, G. Wang, W. Zhang, Y. Jiang, and Z. Chen. How much behavioral targeting help online advertising? In Proceedings of the 18th International World Wide Web Conference (WWW), 2009.
[2] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers, MA, MIT Press, 2000.

720

Multi-Modal Query Expansion for Web Video Search*
Bailan Feng1,2, Juan Cao1, Zhineng Chen1,2, Yongdong Zhang1, Shouxun Lin1
1Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China 2Graduate School of the Chinese Academy of Sciences, Beijing 100039, China
{fengbailan, caojuan, chenzhineng, zhyd, sxlin}@ict.ac.cn

ABSTRACT
Query expansion is an effective method to improve the usability of multimedia search. Most existing multimedia search engines are able to automatically expand a list of textual query terms based on text search techniques, which can be called textual query expansion (TQE). However, the annotations (title and tag) around web videos are generally noisier for text-only query expansion and search matching. In this paper, we propose a novel multimodal query expansion (MMQE) framework for web video search to solve the issue. Compared with traditional methods, MMQE provides a more intuitive query suggestion by transforming textual query to visual presentation based on visual clustering. Parallel to this, MMQE can enhance the process of search matching with strong pertinence of intent-specific query by joining textual, visual and social cues from both metadata and content of videos. Experimental results on real web videos from YouTube demonstrate the effectiveness of the proposed method.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process
General Terms: Design, Performance, Experimentation

1. INTRODUCTION
Most of existing popular multimedia search engines (such as Google, Yahoo! and Bing) allow users to represent their search intents by issuing the query as a list of keywords, and provide textual query expansion (TQE) strategy for disambiguation. Alternatively, the authors in [5] formulate a visual query suggestion (VQS) framework by providing image presentations to help users express their search intent for image search; nevertheless it is still mainly depending on textual clustering to overcome query ambiguity. As we all know, compared with images and audios, the annotations around web videos are usually much noisier, which could result in the unsatisfactory performance in video search application when query expansion is only based on textual cues.
Motivated by these analyses, actually, the noisiness of annotations around web videos is the main challenge which limits the effectiveness of web video search. To address this challenge, we propose a novel query expansion framework (see Figure 1), named multi-modal query expansion (MMQE) for web video search application.
The main contributions of this paper are twofold:
*This work was supported by the National Basic Research Program of China (973Program, 2007CB311100), National High Technology and Research Development Program of China (863Program, 2007AA01Z416), National Nature Science Foundation of China (60873165, 60902090), Beijing New Star Project on Science & Technology (2007B071), Co-building Program of Beijing Municipal Education Commission.
Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure1. Framework of Multi-Modal Query Expansion for Web Video Search
We propose a novel query expansion framework named MMQE for web video search. MMQE can assist users to formulate an intentspecific query by transforming ambiguous textual query to intuitive visual presentation based on visual clustering. Meanwhile, MMQE facilitates the process of human-computer interaction with a simply one-time operation.
We investigate different strategies based on textual, visual and social cues to enhance the process of search matching with strong pertinence of intent-specific query, which is able to help users specify and deliver their search intents in a more precise and efficient way, and can contribute to a significant search improvement.
2. MMQE FOR WEB VIDEO SEARCH
Given a textual query, MMQE for web video search is a two-stage process as illustrated in Figure 1. The first stage named user intent visualization is designed to formulate an intent-specific visual query for users. The second stage utilizes multi-modal cues of videos to provide a more effective expansion scheme for web video search. The challenge of noisy annotations around videos can be well addressed by both stages mentioned above.
2.1 User Intent Visualization
User intent visualization aims to avoid the interference of noisy annotations in search intent presentation. As introduced in [5], textual information is mainly used to suggest search intent by key-

721

words clustering. Different from image annotations, video annotations are much noisier. Therefore, we depend on the more reliable and objective visual cues of videos to suggest search intent for users.
We first use annotations around videos to transform textual query to visual query set by word co-occurrence statistics. This process may bring lots of noisy videos due to the ambiguous textual query and the noisy annotations. For example, as illustrated in Figure 1, some noisy videos about somebody talking about "Bush attacked by shoe" are expanded into the visual query set. After that, we resort to affinity propagation [1] to automatically cluster the videos in the visual query set into different search intent categories based on SIFT feature. Then users can choose their intent-specific category intuitively to perform the following search process.
2.2 Web Video Search via MMQE
With the selected intent-specific category, three expansion strategies are designed to boost the web search quality.
Firstly, a visual classifier is trained with the visual query videos as positive samples, and negative samples are randomly extracted from testing dataset. Actually, any classifier model is extensible in this framework. Considering the fact that the positive samples are of high quality and limited quantity, in this work, we choose SVM as the visual classifier model, and leverage the time efficiency and the performance effects acceptably.
Secondly, a textual ranker is designed to expand the original textual query with more relevant keywords. We calculate the tag frequencies in the intent-specific category and empirically extract the tags whose frequencies are greater than half of the maximum frequency as the expansion keywords. Both expansion strategies motioned above result in the improvement of search recall.
Thirdly, a social re-ranker is utilized to refine the average fusion list of visual classifier and textual ranker according to the social relationships among videos. Considering the fact that relevant videos are generally with similar social relationships, such as communities and categories, we employ a manifold ranking algorithm [2] to utilize these social relationships for search improvement. On the basic assumption that users only care about the top order relation of search results, we firstly construct a correlation graph on the top n fusion list. Each vertex denotes a video entity, and each edge denotes the social correlation between two video entities. Recommendation and category information are utilized to assign the weight of edges. If the corresponding two videos both have the same two, one or none of above cues, the weight of edges will be given high, middle or low values respectively. Then top k (k<<n) videos of fusion list are utilized as pseudo query points and ranking cues are reserved to trigger the propagation of social relationships on the graph. After convergence, the resultant ranking score of each video is in proportion to the probability that it is relevant to the intent-specific query. The process of social re-ranker mentioned above can adjust a number of relevant videos to a closer top position, and therefore contribute to the improvement of search precision. For example, as illustrated in Figure 1, the visual effect of the final search list is to some extent better than the fusion list after social re-ranking.
3. EXPERIMENTS
We conduct the MMQE framework on MCG-WEBV [4], a web video dataset containing 80,031 representative YouTube videos. The parameters n and k are empirically set to be 1000 and 10 to meet the requirement of near real-time search. We select ten topics with the most popular view from the existing studies [4] and extract no more than two keywords as the corresponding initial queries

based on the statistical conclusion in [3]. The details of the hot topics are shown in Table 1.
Table1. Hot Topics
We adopt NDCG@k as the evaluation metric and evaluate the performance of three search strategies: Google_E: searching videos using the official textual expansion proposed by Google engine. VQE+TQE: searching videos using the combined query expansion consisting of the visual classifier and the textual ranker. VQE+TQE+SQE (MMQE): re-ranking the returned videos of VQE+TQE based on the social re-ranker. The average performance over the ten queries is shown in Figure 2. From the figure, we can see that by specifying the search intent using visual and textual cues in the proposed expansion framework, VQE+TQE can overcome the noisiness issue and thus outperform the Google_E strategy. By further appending social cues to the expansion, the VQE+TQE+SQE (MMQE) strategy gets the best performance. Both observations above demonstrate the effectiveness of MMQE for web video search application.
Figure2. Comparison of NDCG@k for MMQE and Google_E over Ten Queries
4. CONCLUSION
This paper has proposed a novel multi-modal query expansion (MMQE) framework for web video search. By formulating an intent-specific visual query and leveraging textual, visual and social cues of web videos, MMQE successfully addresses the noisiness challenge of annotations around web videos. Experimental results on web video search demonstrate the superiority of MMQE than the traditional web search engine.
5. REFERENCES
[1] B. Frey, et al. Clustering by passing messages between data points, Science, 319(5814):726, 2007.
[2] D.Y Zhou, et al. Ranking on data manifold, Proc. NIPS, 169176, 2003.
[3] D. Tjondronegoro, et al. Multimedia web searching on a metasearch engine, Proc. ADCS, 80-83, 2007.
[4] J. Cao, et al. MCG-WEBV: A benchmark dataset for web video analysis, Technical report, ICT-MCG-09-001, 2009.
[5] Z.J. Zha, et al. Visual query suggestion, Proc. MM, 15-24, 2008.

722

Context Aware Query Classification Using Dynamic Query Window and Relationship Net

Nazli Goharian
Computer Science Department Georgetown University, Washington, DC
nazli@ir.cs.georgetown.edu
ABSTRACT
The context of the user queries, preceding a given query, is utilized to improve the effectiveness of query classification. Earlier efforts utilize fixed number of preceding queries to derive such context information. We propose and evaluate an approach (DQW) that identifies a set of unambiguous preceding queries in a dynamically determined window to utilize in classifying an ambiguous query. Furthermore, utilizing a relationship-net (Rnet) that represents relationships among known categories, we improve the classification effectiveness for those ambiguous queries whose predicted category in this relationship-net is related to the category of a query within the window. Our results indicate that the hybrid approach (DQW+R-net) statistically significantly improves the Conditional Random Field (CRF) query classification approach when static query windowing and hierarchical taxonomy are used (SQW+Tax), in terms of precision (10.8%), recall (13.2%), and F1 measure (11.9%).
Categories and Subject Descriptors
[Pattern Recognition]: Design Methodology- Classifier design and evaluation
General Terms
Algorithms, Experimentations
Keywords
Query Classification
1. INTRODUCTION
Query classification is the process of assigning predefined categories to queries. Such category information is useful in various domains such as vertical search and advertisement search. The information about the context of the queries is utilized to improve the effectiveness of query classification. We apply two methods, each of which individually or combined (hybrid) increase the effectiveness of existing query classification approaches. Our first method, Dynamic Query Window (DQW), calculates the query window size based on the unambiguous queries in the query stream. An earlier effort, called Conditional Random Field (CRF) approach [1], uses a static query window (fixed number of preceding queries) as the context of a given query. This leads to the loss of information from queries that may indicate the context of the current query but lay outside of the fixed window. We propose an approach that expands the query window dynamically based on the unambiguous preceding queries. Unambiguous queries point to only one category. For example, a query that is looking for virus is ambiguous (belongs

Saket S. R. Mengle
Dataxu Inc. Boston, Massachusetts smengle@dataxu.com
to multiple categories such as Computer Security and Biomedicine); considering the preceding queries of the query virus that are Flu, Fever and H1N1, there is an indication that the user is looking for information on biomedical articles, and subsequently the query is classified as such. In the same query stream, however, the query H1N1 is unambiguous as it only points to the category Biomedicine, and hence, does not require context information for query classification. Preceding queries to query H1N1 such as query Football, World Cup and Soccer, on other hand, mislead the classification of the query H1N1. Thus, our approach utilizes the query stream only for handling the ambiguous queries.
Our second method utilizes the relationships among categories represented in a relationship-net, R-net, [4] for query classification. Relationship-net is a network structure where the categories are represented by nodes and relationships among such categories are represented using edges. Relationship-net is automatically generated using text classification [4]. Earlier efforts utilized hierarchical taxonomies to represent the relationships among categories. It was shown in [4] that R-net represents more relationships among categories than a hierarchical taxonomy, thus, improving the effectiveness of query classification algorithms. Finally, a hybrid approach that utilizes the DQW approach and R-net to improve the existing query classification algorithms is presented and evaluated.
Our results indicate that using our approaches, individually or combined, statistically significantly (95% confidence) improve the results of CRF approach, which utilizes static query window and hierarchical taxonomy, in terms of precision, recall and F1 measures.
2. METHODOLOGY
Our four-step methodology follows.
Step 1: Determining if context information is needed: In this step, we identify if a query is ambiguous, so that in the next steps based on the context information of the query, we classify the ambiguous query. We utilize a feature selection algorithm called Ambiguity Measure (AM) [3] to assign weight to every query term. AM(ti) of a term ti, is defined as the maximum AM value that a term ti has in respect to all categories. AM of a term with respect to a given category is defined as the ratio of the term frequency of the term in that category to the term frequency of the term in the entire collection. Thus, AM assigns higher weight to terms that appear only in one category. The query weight Wqj

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

723

Figure 1: Evaluation of Precision

Figure 2: Evaluation of Recall

of query qj is calculated as the summation of ambiguity measures of all terms ti in the query (Formula 1), where T is the number of terms in query qj.

T

 Wq j = AM (ti )

.. 1

i=0

We only use the context information for queries, whose query weight Wqj is below an empirically determined threshold (0.7), indicating the query is ambiguous.

Step 2: Forming Dynamic Query Window (DQW): To build context information for an ambiguous query, we need to identify a dynamic query window. Initially, a small static window (three preceding queries) is selected to calculate context information. As unambiguous queries are of our interest in forming context information, we only utilize the information from those preceding queries that their weight (Wqj) is above an empirically determined threshold (0.7). We recursively expand the query window by including three preceding queries of every unambiguous query in the query window. We discontinue expanding the query window when all the three preceding queries of an unambiguous query have weights lower than an empirically determined threshold.

Step 3: Identifying category relationships using R-net: In this step, the weights of the queries in the query window are adjusted. The premise is that users search on related categories in the same query stream. Hence, any of the queries in the window whose category is not related to the ambiguous query in hand are penalized by reducing the weights of those queries. Unlike earlier efforts that utilized hierarchical taxonomy (Tax) to detect category relationships, we utilize R-net, which represents more relationships, to detect relationships between the category of a given query and categories of its preceding queries in the query window. A lower weight (Wqj * 0.5) is assigned to queries in the query window that are not related to the query to be classified. The related queries, however, are assigned the original weight (Wqj).

Step 4: Classifying ambiguous queries: CRF is a discriminative probabilistic model used in the classification of sequential data. CRF is represented using undirected graph where the vertex represents a random category and edge represents the dependency between two random categories [2]. We utilize CRF to classify an ambiguous query into a category based on the categories of the preceding unambiguous queries in a window, based on weights calculated in step 3. We implement the CRF approach using an open source utility called CRF++. As the earlier effort [1], using CRF, utilizes static query window (SQW) and hierarchical taxonomy (Tax), we refer to CRF approach as CRF+SQW+Tax.

Figure 3: Evaluation of F1 Measure
3. EXPERIMENTAL FRAMEWORK
We utilized 67 categories from KDD Cup 2005 as the set of predefined categories. To build the classifier, we used 500 documents from ODP dataset. The Excite query log is used to extract 500 query streams. Each query stream is at least 5-query long (Query length: avg: 2.7, median: 3). We manually labeled the queries with the KDD Cup 2005 categories. Standard evaluation metrics of precision, recall and F1-measure are used. Statistical significance of our results is measured using paired ttests.
4. Results
Figures 1, 2 and 3 indicate that our hybrid approach (CRF+DQW+R-net) statistically significantly (95% confidence) outperforms the existing state of the art (CRF+SQW+Tax) approach with respect to precision (10.8%), recall (13.2%) and F1-measure (11.9%). CRF+DQW approach, even without using R-net, still improves over CRF+SQW by 3.5%. Moreover, as CRF+DQW expands the window size to allow more unambiguous queries to provide context information, the improvements over CRF+SQW in recall (3%) and F1-measure (3.2%) of query classification are also statistically significant. Using R-net along with CRF approach (CRF+R-net) also statistically significantly improves the precision (6.7%), recall (7.1%) and F1 measure (6.9%) over CRF+Tax approach.
5. REFERENCES
[1] Cao, H., Hu, D. H., Shen, D., Jiang, D., Sun, J., Chen, E., and Yang, Q., Context-aware query classification. SIGIR, 2009
[2] Lafferty, J. D., McCallum, A., and Pereira, F. C., Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. ICML, 2001
[3] Mengle, S., Goharian, N., Ambiguity measure featureselection algorithm. JASIST, 60(5), 2009
[4] Mengle, S., Goharian, N., Detecting relationships among categories using text classification. JASIST, 61(5), 2010

724

Predicting Query Potential for Personalization,

Classification or Regression?

Chen Chen*, Muyun Yang*, Sheng Li*, Tiejun Zhao*, Haoliang Qi+

Harbin Institute of Technology*

Heilongjiang Institute of Technology+

Harbin, 150001, P.R.China

Harbin, 150050, P.R.China

{chenchen, ymy, lisheng, tjzhao}@mtlab.hit.edu.cn

haoliang.qi@gmail.com

ABSTRACT
The goal of predicting query potential for personalization is to determine which queries can benefit from personalization. In this paper, we investigate which kind of strategy is better for this task: classification or regression. We quantify the potential benefits of personalizing search results using two implicit click-based measures: Click entropy and Potential@N. Meanwhile, queries are characterized by query features and history features. Then we build C-SVM classification model and epsilon-SVM regression model respectively according to these two measures. The experimental results show that the classification model is a better choice for predicting query potential for personalization.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process
General Terms
Measurement, Experimentation, Performance
Keywords
Query potential for personalization, Classification, Regression
1. INTRODUCTION
In contrast to the traditional retrieval models usually focusing on the topical relevance, a strong motivation of personalized Web search is further to take the user relevance into account. It is a subtle issue to implement a fully personalized Information Retrieval (IR) model because of different requirements on the same queries by users. A number of research groups have built models personalizing search results by individual interests [1, 2, 3]. Regardless of the details, they typically apply the same algorithm and parameter settings to all queries. However, Dou et al. [4] reveal that current personalization models can improve the results for some queries while actually harming others. This is somewhat reasonable since personalization models can be expected beneficial to queries with a substantial gap between the personal requirement and the aggregate group preference.
To measure this kind of gap, Teevan et al. [5] examined the variability among user intents by both explicit relevance judgments and large-scale log analysis of user behavior patterns. They found that the click-based implicit measures correspond well with the explicit measures and suggested two implicit measures assessing query potential for personalization. The first is Click entropy, which measures the variability in clicked results across
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

individuals [4]. The second is the NDCG-based Potential@N, which describes the gap between the optimal rating for an individual and the optimal rating for the group [6]. Both of them can be easily calculated from the query logs.

The value for Click entropy and Potential@N are generally continuous, and the regression may be a good choice since the goal of regression is to predict the value of continuous variables. However, the previous studies [5] treat query potential prediction as a classification problem which assigns an input query to one of multiple discrete classes without any justification. The classification approach, as a loose and approximate measure, is robust and stable in prediction, while the regression model can greatly help practical applications due to its nearly accurate prediction. In this paper, we tentatively investigate which kind of model is better for predicting query potential for personalization on AOL (American Online) query logs. Results show that the classification model is more effective than the regression model.

2. METHODS
To compare the classification and regression model for predicting query potential for personalization, we choose Support Vector Machine (SVM), a state-of-the-art machine learning algorithm, as a fair framework of a case study. Though SVM is a classification model in essence, it has been successfully extended into a regression version. We apply C-SVM as the classification model and epsilon-SVM as the regression model because they have similar structural loss function and optimization method. After that we evaluate and compare the performance of these two models in a unified way.

Click entropy and Potential@N are two implicit measures which quantify query potential for personalization by user clicks. Assuming the user click as an indication for the relevance, these measures regard that queries with great variations in the click also have great variations in what people consider relevance. Click entropy is calculated as:

 Click Entropy(q) =- P(cu | q)*log2(P(cu | q))

(1)

URLu

where p(cu|q) is the probability that URL u was clicked on a query q. In contrast, the Potential@N describes the potential gain that can be achieved by personalizing rankings in terms of the NDCG difference between the ideal individualized ranking and the best group ranking. Note that N is the size of a group.

Both Click entropy and Potential@N are generally continuous measures. Naturally, we can learn epsilon-SVM regression model straightforward with the training data. For the classification task, we follow the approach described in [5] and bin the data into four equal-sized bins according to the above two measures. With such

725

obtained gold standard for classification, the C-SVM is applied to identify which bin a sample belongs to. To compare the two models, we again divide the regression results into four equalsized bins in the same way, enabling a direct comparison with the gold standard. Therefore, the precision of classification can be chosen as the final evaluation metric for both models.

We employ similar query features and history features as in [5] to predict Click entropy and Potential@N. Table 1 presents the detailed features, which are extracted from the AOL query log.

Table 1. Query and history features used to predict query potential for personalization

Feature Class
Feature Name

Query Features Query Length(char)
Query Length(word) Location Mentioned Person Mentioned Organization Mentioned Contain URL Fragment

History Features % issued during work hours # of times issued # of distinct users # of distinct URLs Avg. click position Avg. clicks per user

3. EXPERIMENTS AND DISCUSSION
To verify which kind of model fits better for query potential prediction, we look at a large sample of queries issued to AOL (American Online) search engine from March 1 to May 31, 2006. We extract queries which are issued by at least ten people to ensure sufficient data to understand the variability across users on the same query. Table 2 shows the statistics of the whole collection and data for experiments.

Table 2. The statistics of collection and experiment data

# of query instances # of unique queries # of unique users

AOL 21,011,340 10,154,742
657,426

In Experiment 3,390,310 57,103 414,034

We learn C-SVM and epsilon-SVM that best explain the training data by libSVM toolkits [7]. The learning targets are Click entropy and Potential@N. The group size is ten. We predict these two variables using query features and history features. All results are reported by using five-fold cross validation, as shown in Table 3 and 4. In these two tables, "q" denotes query features, "h" means history features, "y" represents the features employed and "n" indicates features removed. Following the work [5], we have the data divided into four equal-sized bins, and the baseline of a random guess is 25%.

According to Table 3 and 4, we can find that the results are consistent in both the Click entropy and the Potential@10. Compared with the baseline, we can somewhat identify queries which benefit from personalization using query features alone, although the overall level of prediction precision is moderate. Meanwhile, we can find that the history features extracted from query logs can strongly improve the effectiveness of both models and produce a sizeable jump in precision. It should also be noticed that in the case of using both query features and history features, classification method is only slightly better, while about 5% improvement using the query features only. Therefore, when

predicting query's potential for personalization, it is reasonable to believe that the classification is more suitable than the regression.

Table 3. The model performance using different features to

predict Click Entropy.

Feature Baseline

C-SVM

Epsilon-SVM

q h Prec(%)

Prec(%)

Prec(%)

y n

25

34.44

29.39

y y

25

80.69

80.26

Table 4. The model performance using different features to predict Potential@10.

Feature q h y n y y

Baseline Prec(%)
25 25

C-SVM Prec(%)
33.67 58.45

Epsilon-SVM Prec(%) 28.42 42.26

4. CONCLUSION AND FUTURE WORK
This paper tentatively shows that query potential prediction is more promising to be treated as a classification problem. We follow the previous work and divide data into four equal-sized bins. The comparison is made between C-SVM as a classification model and epsilon-SVM as a regression model with query features and history features. In the future, more substantial comparisons between the classification and regression model under other framework are scheduled. And we will enrich the model with more features mined from query logs and other resource to better resolve query potential prediction. Finally, the way of classifying data into equal-sized groups deserves further examination.

5. ACKNOWLEDGMENTS
This work is supported by the Key Project of National Science Foundation of China (Grant No. 60736044) and the National High Technology Research and Development Program of China (Grant No. 2006AA010108).

6. REFERENCES
[1] Chirita, P. A., Nejdl, W., Paiu, R., and Kohlschutter, R,C. 2005. Using ODP metadata to personalize search. In Proc. of SIGIR'05, 178­185.
[2] Shen, X., Tan, B., and Zhai, C. X. 2005. Implicit user modeling for personalized search. In Proc. of CIKM '05, 824­831.
[3] Teevan, J., Dumais, S.T., and Horvitz, E. 2005. Personalizing search via automated analysis of interests and activities. In Proc. of SIGIR '05, 449-456.
[4] Dou, Z., Song, R., and Wen, J.R. 2007. A large-scale evaluation and analysis of personalized search strategies. In Proc. of WWW '07, 581-590.
[5] Teevan, J., Dumais, S. T., and Horvitz, E. 2008. To personalize or not to personalize: modeling queries with variation in user intent. In Proc. of SIGIR '08, 163-170.
[6] Teevan, J., Dumais, S. T., and Horvitz, E. 2010. Potential for personalization. To appear in ACM Transaction on Computer Human Interaction.
[7] libSVM. http:// www.csie.ntu.edu.tw/~cjlin/libsvm/

726

Spatial Relationships in Visual Graph Modeling for Image Categorization

Trong-Ton Pham
Grenoble INP-LIG 385 Av. de la Bibliothèque
Grenoble, France
ttpham@imag.fr

Philippe Mulhem
CNRS-LIG 385 Av. de la Bibliothèque
Grenoble, France
mulhem@imag.fr

Loïc Maisonnasse
TecKnowMetrix 4 rue Léon Béridot
Voiron, France
lm@tkm.fr

ABSTRACT
In this paper, a language model adapted to graph-based representation of image content is proposed and assessed. The full indexing and retrieval processes are evaluated on two different image corpora. We show that using the spatial relationships with graph model has a positive impact on the results of standard Language Model (LM) and outperforms the baseline built upon the current state-of-the-art Support Vector Machine (SVM) classification method.
Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous
General Terms
Algorithms, Experimentation
Keywords
Graph theory, language model, image categorization
1. VISUAL GRAPH MODELING
Our goal here is to automatically induce, from a given image, a graph that represents the image content. This graph will contain concepts directly associated with the visual elements in the image, as well as relations which express how concepts are related spatially in the image. To do so, our procedure is based on four main steps: (1) Identify regions within the image that will form the basic blocks for concept identification; (2) Index each region with a predefined set of features; (3) Cluster all the regions found in the collection in k classes, where each class represents one concept. Each region in the image is then associated to one concept. We obtain for each type of region a set of concepts C; (4) Finally, extract spatial relations between visual concepts.
Based on this representation, our contributions are twofold. First, a directed graph model is constructed to represent the image content based on concepts. Second, we apply a simple and effective method for the graph matching based on the language model [2]. Unlike the previous approach [3] based on a sequence of n-gram concepts, our framework embeds smoothly different types of spatial relations. The experiments carried out on two image collections confirm the significative impact of our method.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

1.1 Graph definition
We assume that each image i is represented by a set of weighted concept sets SW i C = {WCi } and a set of weighted relation sets SW i E = {WEi }, forming a graph:
Gi =< SW i C , SW i E >
Each concept of one set WCi corresponds to a visual concept used to represent the image according to the feature associated with. Denoting C a set of concepts for one feature over the whole collection, WCi is a set of pairs (c, #(c, i)), where c is an element of C and #(c, i) is the number of times c occurs in the document image i:
WCi = {(c, #(c, i))|c  C}
Any labeled relation between any pair of concepts (c, c )  C × C is represented by a triple ((c, c ), l, #(c, c , l, i)), where l is an element of L, the set of possible labels for the relation, and #(c, c , l, i) is the number of times c and c are related with label l in image i. WEi is then defined as:
WEi = {((c, c ), l, #(c, c , l, i))|(c, c )  C × C , l  L}

If a pair of concepts (c, c ) come from the same concept set, we refer this relation as intra-relation set. Otherwise, we refer it as inter-relation set.

1.2 Language model for graph matching
Inspired by [1], the probability for a graph query Gq =< SW q C , SW q E > to be generated from one graph document Gd is computed as:

P (Gq|Gd) = P (SW q C |Gd) × P (SW q E |SW q C , Gd) (1)

This probability composed of two parts: a probability to generate the concept sets P (SW q C |Gd) and a probability to generate the relation sets P (SW q E|SW q C , Gd). The proba-
bility of generating query concept sets from the document model P (SW q C |Gd) uses a concept set independence hypoth-
esis:

P (SW q C |Gd) = Y P (WCq |Gd)

(2)

WCq SW q C

Assuming concept independence, standard in information
retrieval and the number of occurrences of the concepts (i.e.,
the weights considered previously) are integrated through the use of a multinomial model, we compute P (WCq |Gd) as:

P (WCq |Gd)  Y P (c|Gd)#(c,q)

(3)

cC

729

where #(c, q) denotes the number of times concept c occurs in the graph representation of the query. This contribution corresponds to the concept probability. The quantity P (c|Gd) can be estimated through maximum likelihood using Jelinek-Mercer smoothing:

P (c|Gd)

=

(1

-

C

)

#(c, #(,

dC dC

) )

+

C

#(c, DC ) #(, DC )

(4)

where #(c, d) represents the number of occurrences of c in

the graph representation of the image d, and where #(, d)

is

equal

to

P
c

#(c, d).

The quantities #(c, D) are similar,

but defined over the whole collection (i.e., over the union

of all images in the collection). Based on the relation set

independence hypothesis, we follow a similar process for the

relation sets, leading to:

P (SW q E |SW q C , Gd) = Y P (WEq |SW q C , Gd)

(5)

WEq SW q E

For the probability of generating query relation from the document, we assume that a relation depends only on the two linked sets. Assuming that the relations are independent and following a multinomial model, we compute:

P (WEq |SW q C , Gd) 

(6)

Y

P (L(c, c ) = l|WCq , WCq , Gd)#(c,c ,l,q)

(c,c ,l)C×C ×L

where c  C, c  C and L(c, c ) is a variable with values
in L reflects the possible relation labels between c and c , in
this relation set. The parameters of the model P (L(c, c ) = l|WCq , WCq , Gd) are estimated by the maximum likelihood with Jelinek-Mercer smoothing. Images are ranked based
on their relevance status value.

2. EXPERIMENTAL RESULTS
In order to assess the validity of our methods, we have experimented on 2 image collections and compared the results with other state-of-the-art approach in image categorization such as SVM classification method (implemented thanks to the libsvm1). We applied the same visual features used for our experiment. Each class was trained with a corresponding SVM classifier using RBF kernel. STOIC-101 collection The STOIC-101 collection contains 3849 photos of 101 tourist landmarks in Singapore. For experimental purposes, the collection has been divided into a training set of 3189 images and a test set of 660 images. We extracted from each block of 10×10 pixels a center pixel as a representative for the region. From this pixel, a vector of HSV color (8 bins for each channel) was extracted and clustered into 500 concepts. Based on this representation, we built 2 graph models: (1) with only concept set, referred as simple Language Model (LM); (2) with integrating of intra-relation set {lef t of, top of } to concept set, referred as Visual Graph Model (VGM). As a comparison, we present in table 1 the best results obtained using SVM classifiers. RobotVision'09 collection The RobotVision'09 collection was used for ImageCLEF competition aiming to address the problem of localization of a robot using only the visual information. This collection contains a sequence of 1034 images for training and a sequence
1http://www.csie.ntu.edu.tw/cjlin/libsvm/

Table 1: Results on categorizing STOIC-101 and

RobotVision'09 collections

#class LM VGM

SVM

STOIC-101 101 0.789 0.809 (+2.5%) 0.744

RobotVision

Validation

5 0.579 0.675 (+16.6%) 0.535

Test

6 0.416 0.449 (+7.9%) 0.439

of 909 images for validation. The official test is carried out on a set of 1690 images. Image sequences were captured within an indoor laboratory environment consisting of 5 rooms. For this collection, we have applied 2 types of image representations: (1) regular division of 5 × 5 patches; (2) extraction of SIFT (Scale Invariant Feature Transform) features from local key-points. From these image representations, we defined an inter-relation set {inside} between patches and key-points representation if one key-point is located inside the region of one patch. Similar to above, we referred the model without relation as LM (simply the production of probability generated by different concept sets) and graph model with the spatial relation as VGM (with the contributing of relation probability to graph model). The SVM model was trained based on the fusion of concepts come from both patches and SIFT key-point features.
Table 1 summarizes the results obtained from both collection STOIC-101 and RobotVision'09. We can see that in all cases our VGMs outperformed other methods. More precisely, with the integration of spatial relation into VGM helped improving the accuracy of classical approaches of LM by at least 2.5%. Especially with the RobotVision collection, VGMs have increased roughly the accuracies of 7.9% to 16.6% comparing to LMs respectively for both test and validation set. Lastly, the VGMs have retained medium to large improvements over the state-of-the-art SVM classifiers in both image collections.
3. CONCLUSION
In this work, we have presented a novel graph-based framework for integrating smoothly the spatial relationships of visual concepts. Our contributions are two-folds: (1) a wellfoundeness graph model for representation of image content (2) a simpler and more effective graph matching process based on the language model. Our experimental results confirmed the stability of our visual graph models, as well as, enhanced the results obtained with other approaches such as standard LM and SVM classification method.
Acknowledgments
This work was supported by AVEIR (ANR-06-MDCA-002) and Merlion PhD. programme from Singapore.
4. REFERENCES
[1] L. Maisonnasse, E. Gaussier, and J. Chevalet. Model fusion in conceptual language modeling. In ECIR'09, pages 240­251, 2009.
[2] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR'98, 1998.
[3] P. Tirilly, V. Claveau, and P. Gros. Language modeling for bag-of-visual words image categorization. In CIVR'08, pages 249­258, 2008.

730

A Picture is Worth a Thousand Search Results: Finding Child-Oriented Multimedia Results with collAge

Karl Gyllstrom karl.gyllstrom@cs.kuleuven.be

Marie-Francine Moens sien.moens@cs.kuleuven.be

Department of Computer Science Katholieke Universiteit Leuven Leuven, Belgium

ABSTRACT
We present a simple and effective approach to complement search results for children's web queries with child-oriented multimedia results, such as coloring pages and music sheets. Our approach determines appropriate media types for a query by searching Google's database of frequent queries for cooccurrences of a query's terms (e.g., "dinosaurs") with preselected multimedia terms (e.g., "coloring pages"). We show the effectiveness of this approach through an online user evaluation.
Categories and Subject Descriptors
H.5.0 [Information Interfaces and Presentation]: General
General Terms
Experimentation, Human Factors
Keywords
children, Google, query suggestion, Mechanical Turk
1. INTRODUCTION
Web search engines are not highly usable by children and young audiences [4], in part because results are presented to users in the form of a list of web page summaries, with a limited integration of mixed media (e.g., images). We believe that children's experiences with search engines would benefit from a greater emphasis on multimedia results, particularly of media that are not typically presented as firstclass entities. For example, while Google search results commonly present images, there is no specialized presentation of interactive and learning media such as mazes, puzzles, tracing/coloring pages, music sheets, and games. We refer to these specialized media results as TotBytes, and we are building a search interface called collAge in which they can play a strong role in children's search experiences. Our search engine web page currently acts as a thin wrapper between the user and Google search, where, for the user's query, TotBytes are presented alongside fewer traditional web page search results. Figure 1 depicts a mockup of our system, showing a potential interface displaying TotBytes in response to a user's query about dinosaurs.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

One challenge in integrating TotBytes into collAge is in determining appropriate types of media results to return for a particular query. For example, while a music sheet image would be an appropriate TotByte for a query about piano music, it would probably not be appropriate for a query about biology. To address this problem, our work leverages a "crowd-sourcing" approach in which frequently issued queries are used to determine whether or not a query and media type are coherent. For example, the query "dinosaur coloring pages" is issued quite frequently (e.g., 577,000 times in Google's Suggestion database [3]), while, presumably, the nonsensical query "calculus coloring pages" is not (at least, it does not appear in Google's database).

http://co.lla.ge/search?q=dinosaurs

collAGE dinosaurs
Results for "dinosaurs"
Dinosaurs (Greek: , deinosauros) were the dominant terrestrial vertebrate animals for over 160 million years, from the late Triassic period (about 230 million years ago) until the end of the Cretaceous period
summary (about 65 million years ago), [...]

dinosaurs Search mazes

dinosaurs Search

anatomy dinosaurs music

diagram

connect the dots

Figure 1: collAge mockup (can be zoomed).
2. APPROACH
Initially, we preselected a set of media types which we felt would be fun and useful for information-seeking children (see the left column of Table 1), although our approach can easily be expanded to new types. For a given user's query q, we generate a set of TotBytes TB q as follows. First, for each media type, we create a media query mi  Mq by appending to q the media type's terms (e.g., "crossword puzzles"). To filter out media types which are not meaningful complements to q, we dispatch each media query mi  Mq ­ without its

731

last letter1 ­ to Google Suggest [3], which generates a set of suggested queries G(mi). If mi  G(mi), it is considered to be a valid query (as mi is frequently issued by Google users), and it is dispatched to Google's image search2, from which the URL of the first result is added to the set of TotBytes for the user's query. The TotBytes can then be presented ­ with image links to the containing pages ­ to the user alongside traditional search results, as in Figure 1, although we leave the problems of presentation and ranking for future work. The combined processes of query suggestion lookup and image search take between 100-200 milliseconds on average, and the process for each media type can be executed in parallel.
3. EVALUATION AND RESULTS
We conducted an online evaluation with adults to determine if the TotBytes produced by collAge reflected the media type for which they were constructed; for example, that the TotBytes for the "music sheet" media type actually portrayed music sheets. Our hypothesis was that collAge would produce more accurate results than an approach that simply appended media terms without checking them against the database of popular queries. We first generated a set of queries Qeval with which to evaluate our approach, as, to our knowledge, there is no existing corpus of children's web search queries. We constructed this set by collecting the titles of leaf subdirectories under the top-level topic "Kids and Teens" from the Open Directory Project [2], which included titles such as "dinosaurs" and "Egypt". Though not necessarily reflective of queries that children would naturally generate, these queries suffice in this stage of evaluation as we are merely determining the extent to which collAge can determine coherent TotBytes for arbitrary queries.
Next, we generated a set of TotBytes TB for a random sample of queries from Qeval. For comparison, we also generated a set of baseline TotBytes, where the query database did not include their corresponding media queries, and added this set to TB . For each bi  TB , we constructed an HTML page displaying the media term(s) (e.g., "maps") and image result from bi, a question asking if the image depicted an instance of the term (e.g., a map), and a "Yes/No" input form for users to record answers. These pages were uploaded to Amazon's Mechanical Turk [1] service, which presented the pages to human users, who answered their questions for a small payment. This produced a binary validity assessment for each bi. We then compared the number of valid assessments between the collAge and baseline TotBytes across each media type, and measured the significance of their differences using the two-tailed Fisher's exact test.
Note that we assumed that a valid depiction of the media type partially validated that the media type was appropriate for the query. The theory was that less coherent combinations of queries and media terms are less likely to produce a result that depicts either query or media type. A limitation in this evaluation is that we do not ask for assessments for whether the TotBytes reflect the topic of the original query.
1The last letter is removed from the query because Google Suggest will not offer a suggestion that is an exact match to the query provided. 2The results may be filtered for certain media types; for example, given a media query for the maze media type, we filter the result list to include only images that are grayscale and feature line art.

This is due to the wide topical range of Qeval; we felt users would often not recognize the query topic and use Google to learn about the topic, at which point they would be using the source of information (valid or not) to verify itself.
The results are depicted in Table 1. Our results included assessments by 156 unique Mechanical Turk worker IDs. Due to the anonymity policy, we could not collect demographic data on the users, but we assume from the site's policy that they are all at least 18 years of age. We filtered eligible candidates to those who have received favorable scores for at least 95% of their completed tasks.

Media type

collAge V T V /T

baseline V T V /T

P

music sheet

21 21 1.00 26 42 0.62 0.00

connect-the-dots 13 13 1.00 20 40 0.50 0.00

painting

103 112 0.92 25 41 0.61 0.00

coloring page 146 162 0.90 32 41 0.78 0.06

map

139 159 0.87 22 41 0.54 0.00

flag

82 98 0.84 16 41 0.39 0.00

anatomy

43 54 0.80 25 52 0.48 0.00

interactive game 16 21 0.76 26 46 0.57 0.17

maze

8 12 0.67 21 37 0.57 0.74

tracing page

2 3 0.67 29 51 0.57 1.00

word puzzle

9 15 0.60 15 36 0.42 0.36

crossword puzzle 7 23 0.30 14 49 0.29 1.00

Total

589 693 0.85 271 517 0.52 x

Table 1: Results. V indicates number of positive assessments, T indicates total assessments, and V /T indicates the ratio of valid to invalid assessments. P indicates P-value from the significance test between collAge and baseline.

4. CONCLUSIONS AND FUTURE WORK
collAge showed a consistent improvement over the baseline that was significant in cases where collAge was performing the strongest. The media types we chose had a range of effectiveness when used to generate TotBytes, with many performing quite well, and an overall strong performance. Even types at the low ranges could be useful (e.g., for a media type with 50% accuracy, simply displaying two TotBytes for the media type will create a 75% chance that at least one is meaningful.) Further, the collAge approach and evaluation can be easily repeated for new potential media types, making it generalizable to domains beyond children's interests. We plan to continue this research by implementing a usable prototype and evaluating it with children in a natural scenario, as well as determining further methods to find and rank media results.
Acknowledgements The research leading to these results has received funding from the European Community's Seventh Framework Programme FP7/2007-2013 under grant agreement No. 231507.
References
[1] Amazon Mechanical Turk. http://www.mturk.com/.
[2] ODP ­ Open Directory Project. http://www.dmoz.org/.
[3] Query Suggest FAQ. http://labs.google.com/intl/en/ suggestfaq.html.
[4] D. Bilal and J. Kirby. Differences and similarities in information seeking: children and adults as web users. Information Processing & Management, 38(5):649 ­ 670, 2002.

732

Query Recovery of Short User Queries: On Query Expansion with Stopwords

Johannes Leveling and Gareth J. F. Jones
School of Computing, CNGL Dublin City University Dublin, Ireland
{jleveling, gjones}@computing.dcu.ie

ABSTRACT
User queries to search engines are observed to predominantly contain inflected content words but lack stopwords and capitalization. Thus, they often resemble natural language queries after case folding and stopword removal. Query recovery aims to generate a linguistically well-formed query from a given user query as input to provide natural language processing tasks and cross-language information retrieval (CLIR). The evaluation of query translation shows that translation scores (NIST and BLEU) decrease after case folding, stopword removal, and stemming. A baseline method for query recovery reconstructs capitalization and stopwords, which considerably increases translation scores and significantly increases mean average precision for a standard CLIR task.
Categories and Subject Descriptors: H.3.3 [INFORMATION STORAGE AND RETRIEVAL] Information Search and Retrieval--Query formulation, Search process
General Terms: Experimentation, Performance, Measurement
Keywords: Query Reformulation, Query Expansion, CLIR
1. INTRODUCTION
Query processing for experimental information retrieval (IR) systems typically involves transforming the original user query (OQ) by successively applying case folding (CF), stopword removal (SR), and stemming (ST). However, real user queries to search engines usually consist of 2-3 words [5, 6] and seldom take the form of full sentences or questions [4]. Thus, they already resemble results from query preprocessing (as shown in Table 1) in that they typically lack capitalization and stopwords, but still contain full word forms. This paper proposes query recovery (QR), a method which seeks to restore a fully capitalized query with syntactic structure from its input. For example, the query embargo iraq (topic C046) is transformed into The embargo against Iraq, which results in a better query translation for CLIR.
Reconstructing punctuation and capitalization has been applied to automatic speech recognition and machine translation (MT) [1, 2], but focuses on processing full text instead of short queries. Query modification for IR has been concerned with query expansion by adding content terms to the
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

query (see, for example [7]). In contrast, QR aims to expand a query by adding stopwords and capitalization.
2. CORPORA AND QUERY ANALYSIS
Our experiments and analyses are performed on the following corpora and data sets: The Excite query log (ENEx) of user queries, as distributed in the Pig query log analysis tool1; the 1M sentence English (EN1M) and 3M sentence German corpus from the Leipzig Corpora Collection2; the English and German Wikipedia article names (ENWi)3; and the titles of 160 English and German topics which have been used in ad-hoc retrieval experiments at CLEF from 2003-2006 (see, for example [3]).
Results of an analysis of this data are shown in Table 1, confirming that the average length of user queries (in column ENEx) is 2-3 words. In addition, the following observations can be made: user queries rarely contain stopwords, punctuation symbols (e.g. "!", "?"), or numeric terms; special characters (e.g. quotation marks or "-") often indicate queries with special syntax, e.g. a phrase search or exclusion of terms. Topic titles contain capitalization in expected places, e.g. at the beginning of sentences. Thus, the proportion of capitalized words is actually much higher in comparison with corpora containing full sentences. Compared to the original and processed CLEF topic titles (OQ-ST), user queries are most similar to topic titles after CF and SR with respect to their average length in tokens, the number of lowercase words, stopwords, and stems (base forms). Users often enter full word forms as query terms (52.9% stems, 47.1% non-stems for ENEx), assuming that the search engine will handle morphological variation or exact matching of query terms. In contrast, topic titles after stemming mostly consist of stems only (94.9% stems).
3. BASELINE QUERY RECOVERY
A user query can be represented as a sequence of content words Wi. Between any two tokens Wi and Wi+1, a stopword sequence Si occurs (a special case is the empty sequence). Thus, a query is a sequence of content words and stopword sequences (S0, W1, S1, . . ., Sn-1, Wn, Sn).
The baseline QR method uses the 1M sentence English and 3M sentence German corpora and the Wikipedia article names as training data. The method consists of replacing
1http://hadoop.apache.org/pig/ 2http://corpora.uni-leipzig.de/ 3http://dumps.wikimedia.org/

733

Table 1: Analysis of English corpora and topics.

Corpus

CLEF topic titles

ENEx EN1M ENWi OQ CF SR ST

entries tokens avg. length

0.94M 1M 5.24M 160 160 160 160 2.45M 25.1M 16.8M 577 577 458 458
2.6 25.1 3.2 3.6 3.6 2.9 2.9

uppercase [%] 0.7 13.8 66.6 45.8 0.00 0.0 0.0

lowercase [%] 81.8 70.6 17.7 50.6 96.4 99.2 99.2

numeric [%]

4.9 2.1 2.4 0.5 0.5 0.8 0.8

punct. [%]

6.8 11.2 5.3 2.3 2.3 0.0 0.0

special [%]

5.8 2.3 7.9 0.8 0.8 0.0 0.0

stopwords [%] 7.8 49.0 11.7 18.3 18.3 0.0 1.3 non-stopw. [%] 92.2 51.0 88.3 81.7 81.7 100.0 98.7

stem [%]

52.9 28.5 8.3 13.6 47.7 47.7 94.9

non-stem [%] 47.1 71.5 91.7 86.4 52.3 52.3 5.1

Table 2: NIST/BLEU scores for CLEF topics. Processing EN DEEN DE ENDE

OQ

9.45/0.97 5.80/0.37 9.66/1.00 5.30/0.39

CF

4.65/0.22 4.26/0.21 2.47/0.07 5.07/0.37

SR

3.34/0.10 2.98/0.10 0.49/0.00 4.00/0.14

ST

1.15/0.00 2.09/0.06 0.20/0.00 1.67/0.00

QR

6.79/0.32 4.84/0.20 5.72/0.24 4.31/0.21

lowercase words Wi in the input with the most frequent capitalized variant found in the training corpus and inserting the most frequent stopword sequence Si occurring between two words Wi and Wi+1. If Wi is unknown, its initial character is capitalized, according to the observation by [1] that most out-of-vocabulary words are proper nouns (which are capitalized in English and German). If Wi or Wi+1 is unknown, the empty stopword sequence is selected for Si.
4. QUERY TRANSLATION EXPERIMENTS
The effect of QR for NLP is investigated by evaluating the baseline method for query translation, which is a typical task for CLIR. Translation experiments and CLIR experiments are based on the CLEF topic titles (C041-C200), which are capitalized, contain stopwords and full word forms. For comparison with real user queries, the original topics are preprocessed by applying case folding (CF), stopword removal (SR), and stemming (ST). Table 2 shows NIST and BLEU scores for CLEF topics after query processing and translation by the Google translate web service4. The original parallel English (EN) and German (DE) topics and variants with corrected orthography were used as reference translations. Query translation scores decrease after each processing step, i.e. translating queries lacking stopwords, case information, or full word forms adversely affects MT quality. As expected, QR for short queries reverses the effects of CF and SR and increases the the quality of translations. For monolingual QR, the English (German) queries achieve 71.8% (59.2%) of the score for the original queries. For a translation of queries after QR to English (German), the baseline QR yields 83.4% (81.3%) of the NIST score for translating the original query. The QR scores are considerably higher than scores for processed queries.
4http://translate.google.com/

Table 3: MAP for CLIR on 160 CLEF topics.

EN

DEEN DE ENDE

SR

0.241 0.221

0.350 0.335

QR

0.241 0.235 (+6.5%) 0.350 0.353 (+5.2%)

OQ

0.241 0.237 (+7.5%) 0.350 0.342 (+1.9%)

5. CLIR EXPERIMENTS
Results for IR experiments on the English and German CLEF ad hoc document collections are shown in Table 3. Significance using the Wilcoxon test with p < 5% is indicated by ''. For comparison with an upper baseline, performance for the unprocessed original queries (OQ) is also shown. MAP does not change at all for monolingual IR experiments, i.e. applying processing steps to the query twice (e.g. stemming) will not affect IR results. For bilingual IR, a slight but significant increase in MAP is observed for query translation after QR (more specifically: after CF, SR, and QR have been applied) compared to query translation after SR (SR and CF). Stemming was not included as a preprocessing step because users do not typically enter stems in queries. For DEEN, QR achieves almost the same MAP compared to using OQ, which demonstrates the usefulness of QR for CLIR. For ENDE, MAP is even slightly higher, due to hyphenated compounds in the German translation of recovered topics, i.e. compound splitting.
6. CONCLUSIONS AND FUTURE WORK
The major findings are: User queries to search engines lack capitalization and stopwords, and are most similar to topic titles after CF and SR (e.g. in average length). Restoring capitalization and adding stopwords to user queries benefits MT and CLIR which was shown by calculating translation scores for various processing stages and after QR. Translation scores for the baseline QR are considerably higher than for preprocessed queries. The proposed baseline QR method serves as a proof of concept for different approaches at QR (e.g. with language models). Future work will include recovery of questions for question answering.
Acknowledgments
This material is based upon works supported by the Science Foundation Ireland under Grant No. 07/CE/I1142.
7. REFERENCES
[1] E. Brown and A. Coden. Capitalization recovery for text. In Information Retrieval Techniques for Speech Applications, volume 2273 of LNCS, pages 11­22. Springer, 2002.
[2] A. Gravano, M. Jansche, and M. Bacchiani. Restoring punctuation and capitalization in transcribed speech. In ICASSP 2009, pages 4741­4744. IEEE, 2009.
[3] G. M. D. Nunzio, N. Ferro, T. Mandl, and C. Peters. CLEF 2006: Ad hoc track overview. In Evaluation of Multilingual and Multi-modal Information Retrieval, CLEF 2006, volume 4730 of LNCS. Springer, 2007.
[4] S. Ozmutlu, H. Ozmutlu, and A. Spink. Are people asking questions of general web search engines? Online Information Review, 27(6):396­406, 2003.
[5] A. Spink, D. Wolfram, M. Jansen, and T. Saracevic. Searching the web: The public and their queries. JASIST, 52(3), 2001.
[6] J. Teevan, E. Adar, R. Jones, and M. Potts. History repeats itself: Repeat queries in Yahoo's query logs. In SIGIR 2006, pages 703­704. ACM, 2006.
[7] J. Xu and W. Croft. Query expansion using local and global document analysis. In SIGIR '96, pages 4­11. ACM, 1996.

734

Flickr Group Recommendation based on Tensor Decomposition
Nan Zheng1, Qiudan Li1, Shengcai Liao2, Leiming Zhang1
Institute of Automation, Chinese Academy of Sciences, Beijing, China1,2
{nan.zheng, qiudan.li, leiming.zhang}@ia.ac.cn1, scliao@nlpr.ia.ac.cn2

ABSTRACT
Over the last few years, Flickr has gained massive popularity and groups in Flickr are one of the main ways for photo diffusion. However, the huge volume of groups brings troubles for users to decide which group to choose. In this paper, we propose a tensor decomposition-based group recommendation model to suggest groups to users which can help tackle this problem. The proposed model measures the latent associations between users and groups by considering both semantic tags and social relations. Experimental results show the usefulness of the proposed model.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications ­ Data mining, H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ Information filtering
General Terms: Algorithms, Experimentation.
Keywords
Flickr Group, Tensor Decomposition, Group Recommendation.
1. INTRODUCTION
Flickr offers a number of ways for users to browse and find photos, such as exploring or searching through members, tags, groups or interesting photos. Groups in Flickr are self-organized communities to share photos and conversations with common interests. As groups are self-organized, plenty of groups fall into a similar topic, i.e. 22,183 groups are returned by searching with the keyword cat. Indeed, people need an easy-to-use tool to guide their selection. In this paper, we provide a group recommendation model to help people more easily engage in group activities.
Typically, only group members can contribute to a group pool. A user attaches tags to a photo and shares it with one or several suitable groups. Semantic relations among tags could be detected by their overlaps in groups or users. The joint patterns of two users can be denoted by their semantic tag usage and similar groups involved. Similarly, the joint patterns of two groups can be expressed by semantic tags co-occurrence and the number of users who participated in both groups. However, the multiple facets introduce challenges of modeling, as a result, most prior work has focused on specific points, such as applying two-mode relations: group-tag, user-tag or group-user [1,2] or incorporating data into two dimensions [3] to construct topic models. Although these models are effective by considering semantic data like tags or social data like group membership respectively, it may bring better performance if
Copyright is held by the author/owner(s). SIGIR '10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

integrating all these information together. As tensor model offers a natural representation for multiple facets, we propose a tensor decomposition-based group recommendation model in this paper to combine semantic tags with social relations. In the following of this article, we will illustrate the details of this model, along with the experimental results.

2. METHOD
2.1 Data Representation
The idea of group recommendation is to suggest groups which may have latent associations with the active user. We provide a small example shown in Figure 1 to illustrate the benefits of three modes compared with two modes. In the two-mode relations (on the left), we cannot find any associations between user U1 and group G3, while in the three-mode relations (on the right), tags perform as a bridge between users and groups. We can see that U1 has latent association with G3 through tag T1.

User s

U1

1

2

U2 3

Gr oups G1 G2

U3

4

G3

User s

U1

1

2 U2
43 U3

Tags

T1

1

4

2 T2

3 T3

Gr oups G1 G2 G3

Figure 1. Usage data of an example.
From Figure 1, a three-mode tensor Z R I×J×N can be constructed from the usage data, where I, J, N are the numbers of users, tags and groups respectively. Each element zijn in the tensor denotes user i's corresponding photo count with tag j in group n. The advantage of a three-way tensor is that we can explicitly model the relations among users, tags and groups in a unified way.

2.2 Tensor Decomposition-based Group
Recommendation Model
We employ CANDECOMP/PARAFAC (CP) tensor decomposition [4] to capture the underlying patterns in the user-tag-group tensor. The basic idea of the CP decomposition is to fit Z with a sum of component rank-one tensors, such that

R

 Z  rar o br o cr

(1)

r =1

where the symbol o denotes the outer product, R is the decomposed

rank of the tensor, r is a weight that indicates how important the r-th component is, and vectors ar  R I , br  R J and cr  R N .

Combining the vectors from the rank-one components, we get three

factor matrices: user matrix A, tag matrix B and group matrix C of

737

size I×R, J×R, N×R respectively. The advantages of the CP model lie in: (i) the model transforms rich relations among users, tags and groups into R components, which present the principal axes of entities across the three modes and (ii) the results project users, tags and groups to the same R components, which reduces dimensions and brings conveniences for further applications based on such a model. Specially, we compute the non-negative CP decomposition by multiplicative updates like non-negative matrix factorization (NMF) adopted [5] by using the Tensor Toolbox. Such an approach could retain the non-negative characteristics of the original data which facilitates easier interpretation.

Three factor matrices discover the latent topics that govern the associations among users, tags and groups. The i-th row of user

matrix A provides an additive linear combination of components which indicate the topics of user i. The higher weight user i is assigned to a component, the more interested user i is in the relevant

topic. The n-th row of group matrix C provides an additive linear combination of components that indicate the topics of group n. The higher weight a group is assigned to a component, the more related

the group is with the relevant topic. Thus, groups can be
recommended according to the captured associations. We define a score matrix S of size I×N as follows:

R

 S = rarcTr .

(2)

r =1

S measures the latent associations between users and groups in

which Sin represents the likeliness user i will participate in group n.

3. EXPERIMENTS
3.1 Dataset and Parameter Settings
We use Flickr API to gather data about users, tags and groups. After stop-word removal and stemming, we end up with a size of 197×5328×4064 tensor and over 0.9 million nonzero entries. The ideal value of R should be large enough to capture all the important characterizations yet small enough to gain time efficiency. We range R from 10 to 100 by an interval of 10 and find that 50 is a suitable tradeoff. Thus, R is empirically set to be 50 in our experiments.

3.2 Results and Discussions
Before performing group recommendation, we first discuss if the proposed model could reasonably identify the latent associations among users, tags and groups. We examine the decomposition results to present the leading entities in the 5th component on each mode shown in Table 1. As shown, all the tags and groups express the topic of component 5 to be self portrait, and the first column of Table 1 lists 5 users who are most relevant to self portrait. In this way, our model can successfully discover the latent associations among users, tags and groups.

Table 1. Top 5 entities in the fifth component on each mode.

Users Nic Temby Lee Bader antonkawasaki Steven Dempsey Nikki P.

Tags portrait self woman girl selfportrait

Groups Self-Portraits!!! Artistic Self-Portraits 365 Days Self Portraits Autoportrait

We use the top-k recommendations metric [6] to evaluate group

recommendation results. That is, we suggest the top k groups for each user. We randomly select one joined group and k-1 unjoined groups for each user to form the test set and the remaining joined groups form the training set. We repeat this procedure 50 times for different random samplings. The objective is to find the place of the joined group in the recommendation list. There are k possible ranks for the joined group and the best result is that no unjoined groups appear before it. Figure 2 shows the cumulative distributions of ranks for the joined groups in the test set (k=201). 0% means that the joined group is at the first place in the ordered list, while 100% means that it turns up in the last position. To demonstrate the usefulness and effectiveness of our model, we offer comparison tests on user-based model [7] and NMF-based model [8] on usergroup matrix.

1

Cumulative Distribution

0.8

0.6

0.4 0.2
0 0%

our model NMF-based model user-based model
10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Recommendation Rank

Figure 2. Top-201 recommendation performances.

Comparing the results, our model provides better group recommendations than user-based model and NMF-based model on our dataset. All these differences are statistically significant (p<0.01). It may be because the proposed model could capture more latent and complex associations among user-tag-group than two-mode relations and provide more reasonable results.

4. CONCLUSIONS AND FUTURE WORK
In this paper, we provide a tensor decomposition-based Flickr group recommendation model by combining semantic data with social data. Preliminary experiments support the validity and effectiveness of the approach. We plan to offer a deep analysis on the latent associations mined by such a model. This research is supported by projects 863 (No.2006AA010106), 973 (No.2007CB311007) and NSFC (No.60703085).

5. REFERENCES
[1] Negoescu, R. and Gatica-Perez, D. Analyzing flickr groups.
CIVR'08 pp. 417-426.
[2] Negoescu, R., Adams, B., Phung, D., Venkatesh, S. and
Gatica-Perez, D. Flickr hypergroups. MM'09 pp. 813-816.
[3] Negoescu, R. and Gatica-Perez, D. Topickr: flickr groups and
users reloaded. MM'08 pp. 857-860.
[4] Carroll, J. and Chang, J. (1970) Analysis of individual
differences in multidimensional scaling via an N-way generalization of "Eckart-Young" decomposition. Psychometrika. 35(3) 283-319.
[5] Lee, D. and Seung, H. Learning the parts of objects by non-
negative matrix factorization. Nature'99. 401(6755) 788-791.
[6] Koren, Y. Factorization meets the neighborhood: a multi-
faceted collaborative filtering model. KDD'08 pp. 426-434.
[7] Sarwar, B., Karypis, G., Konstan, J. and Riedl, J. Analysis of
recommendation algorithms for e-commerce. EC'00.
[8] Zhang, S., Wang, W., Ford, J. and Makedon, F. Learning from
incomplete ratings using non-negative matrix factorization. SDM'06 pp. 548­552.

738

Robust Music Identification Based on Low-Order Zernike Moment in the Compressed Domain

Wei Li, Yaduo Liu, Xiangyang Xue
School of Computer Science and Technology, Fudan University 825 Zhangheng Road, Shanghai 201203, P.R. China
weili-fudan@fudan.edu.cn, duoyal@gmail.com , xyxue@fudan.edu.cn

ABSTRACT
In this paper, we devise a novel robust music identification algorithm utilizing compressed-domain audio Zernike moment adapted from image processing techniques as the pivotal feature. Audio fingerprint derived from this feature exhibits strong robustness against various audio signal distortions including the challenging pitch shifting and time-scale modification. Experiments show that in our test dataset composed of 1822 popular songs, a 5s music query example which might have been severely corrupted is still sufficient to identify its original near-duplicate copy, with more than 90% top five precision rate.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Music identification; Zernike moment; Robustness
1. INTRODUCTION
Music identification is a technique that uses audio fingerprint to recognize the near-duplicate original copy from a music database given an unknown input query example that might have been severely corrupted by various audio signal distortions. To date, a number of algorithms have been published with rather high retrieval performance and most of them operate on the PCM wave format. However, with the mature of CD quality audio compression techniques and the fast growing of the Internet, it will be interesting and meaningful in practice if audio features are directly extracted from the compressed domain and used for music identification in the database.
So far, only a few algorithms that perform music information retrieval (MIR) directly on the compressed domain have been proposed [1-5], research in this field is still in its infancy. The above methods achieved certain retrieval achievements, while they didn't consider or obtain compellent results to the most central problem of audio identification, i.e. robustness. Moreover, previously used features principally follow the line of MDCT coefficient and its derived spectral energy.
In this paper, we develop a new compressed-domain feature to achieve high robustness in audio fingerprinting based on Zernike moment which has been widely used in image related research fields. To the authors' knowledge, this is the first attempt that Zernike moment is used for robust audio identification in the compressed domain. Experiments show that the fingerprint derived from low-order audio Zernike moments
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland ACM 978-1-60558-896-4/10/07.

is very robust and a 5s unknown query music fragment which might have been contaminated by various severe audio signal distortions is still sufficient to retrieve its original near-duplicate copy from our test dataset which is composed of 1822 popular songs, with an average top-5 precision rate of more than 90%.
2. ALGORITHM DESCRIPTION 2.1 MDCT-Granule Auditory Image
To calculate Zernike moment in the compressed domain, we have to first construct 2-D auditory images from 1-D MP3 encoded bit stream. Two preprocessing steps i.e. frequency alignment and granule grouping are needed. Frequency alignment incorporates the 576 original MDCT coefficients that are differently distributed in long- and short-window types of granules into 192 unified new frequency lines to obtain approximately the same frequency resolution, and forms the Y axis of an auditory image. Granule grouping puts N continuous granules into a slot to reinforce the statistical steadiness and constitute the X axis of an auditory image.
For the kth auditory image as shown in Fig.1, its pixels fk(x, y)
forms an M×N matrix, where the y axis represent N new MDCT
coefficients and the x axis mean M time-domain granules, i.e. a slot. It is known that sounds located in the low-middle frequency range cover the main content most vital to the human auditory system and are more robust than high frequency components. Therefore, we pick out the 2nd to the 51st new MDCT values to act as the y axis, which roughly corresponds to 300 ­ 5840 Hz of real frequency. M is set to 50 granules to form the x axis.

Figure. 1 An illustration of constructed auditory image

2.2 MDCT Zernike Moments and Fingerprints

With the above preparations, the Zernike moment of the kth

auditory image is calculated as below

A

n1 

f x, y V , x, y

1

where n is the moment order, and m must be subject to the
condition that n |m| is nonnegative and even.

739

Generally speaking, low-order moments characterize the basic
component of a signal, while higher order ones depict the high
frequency details. Therefore we take the sum of Zernike
moments of order 2 as the final feature and further derive
the audio fingerprint sequence, see formula (2) and (3).

2

||

| |%

S (k )



0 1

if if

Z

k mn



Z k 1 mn

Z

k mn



Z k 1 mn

k  0,1,2,..., N slot  1 (3)

2.3 Fingerprint Matching
The emphasis of this paper is to invest the effectiveness of this Zernike moment based compressed-domain audio feature. Therefore we perform straightforward exhaustive matching between the query example and those stored recordings using bit error rate (BER) as the similarity measure.

3. EXPERIMENTS
We first set up a music database composed of 1822 distinct MP3 format popular songs (30s, 64kbps) and a corresponding fingerprint database. Query examples include 5s-long excerpts cut from selected database songs and their distorted copies.
Next we need to determine a reasonable BER threshold T under a specific false positive rate (FPR). Fingerprint bits are assumed to be random i.i.d. (independent and identically distributed) and error bits are modeled by normal distribution. In accordance to the method of reference [5], FPR equals 4.3907e005 when 0.32 is set as the threshold, this is acceptable for practical applications, accordingly we use 0.32 as the boundary to distinguish whether two fingerprints are matched or not
Given 100 randomly chosen query examples, the top-1, 5, 10 retrieval precision rates with 0.32 as the BER threshold are averaged and shown in Figure 2.
It can be seen that this proposed MDCT Zernike moment based fingerprint shows very good identification precision, even under severe audio signal processing like heavy lossy recompression, volume modulation, echo addition, noise interference and various frequency wrapping such as band-pass filtering, equalization, pitching shifting (±10%) etc. The only deficiency is that under pitch reserved time-scale modifications (TSM), only ±3% TSM can be resisted. This weakness is essentially caused by the fixed data structure of the MP3 compressed bit stream. In this case, implicit synchronization methods based on salient local regions can't be applied. The only way to resist serious time-domain desynchronization is to increase the overlap between contagious slots and design more steady fingerprints, whereas the overlap has an upper limit of 100% (96% has been used in this paper) and discovering more powerful features is neither an easy work.
Compared with other related compressed-domain algorithms in the introduction whose best top-5 precision rate is 90% [4] under a relatively clean environment (no robustness results are reported in [1-4], and pitching shifting and TSM are not considered in [5] ), our algorithm obviously outperforms those methods with the top-5 precision rates bigger than 90% even under severe audio signal processing such as MP3 compression@32kbps, equalization, ±10% pitch shifting and ±2% time-scale modification etc. What is more important, previous arts are not actually working in the way of fragment

retrieval, this is an intrinsic deficiency compared with our method.
Retrieval precision rate under various distortions
100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0%
top-1 top-5 top-10
Figure 2. Retrieval results under various distortions
4. CONCLUSION
We propose a novel and practical music identification algorithm which works directly on the MP3 encoded bit stream by constructing the MDCT-granule auditory images and then calculating the audio Zernike moments. This algorithm achieves promising retrieval precision even if the 5s query examples are severely degraded by various audio distortions.
5. ACKNOWLEDGMENTS
This work is jointly supported by NSFC (60873255), 973 Program (2010CB327906), Shanghai Leading Academic Discipline Project (B114).
6. REFERENCES
[1] C. C. Liu and P. J. Tsai, "Content-based retrieval of MP3 music objects," proceeding of the ACM international conference on information and knowledge management 2001, pp. 506-511.
[2] W. N. Lie and C. K. Su, "Content-based retrieval of MP3 songs based on query by singing," proceeding of the IEEE international conference on acoustics, speech, and signal processing (ICASSP 2004), pp. 929-932.
[3] T. H. Tsai and J. H. Hung, "Content-based retrieval of MP3 songs for one singer using quantization tree indexing and melody-line tracking method," proceeding of the IEEE international conference on acoustics, speech, and signal processing (ICASSP 2006), pp. 505-508.
[4] T. H. Tsai and Y. T. Wang, "Content-based retrieval of audio example on MP3 compression domain," proceeding of the IEEE workshop on multimedia signal processing (MMSP 2004), pp. 123- 126.
[5] Y. H. Jiao, B. Yang, M. Y. Li and X. M. Niu, "MDCTbased perceptual hashing for compressed audio content identification," proceeding of the IEEE workshop on multimedia signal processing (MMSP 2007), pp. 381-384.

740

Language-Model-based Pro/Con Classification of Political Text

Rawia Awadallah, Maya Ramanath, and Gerhard Weikum
Max-Planck Institute for Informatics Saarbrücken, Germany
rawadall@mpi-inf.mpg.de, ramanath@mpi-inf.mpg.de, weikum@mpi-sb.mpg.de

ABSTRACT
Given a controversial political topic, our aim is to classify documents debating the topic into pro or con. Our approach extracts topic related terms, pro/con related terms, and pairs of topic related and pro/con related terms and uses them as the basis for constructing a pro query and a con query. Following standard LM techniques, a document is classified as pro or con depending on which of the query likelihoods is higher for the document. Our experiments show that our approach is promising.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--Linguistic Processing
General Terms
Algorithms, Experimentation
Keywords
Language Models, Sentiment Analysis, Text Classification
1. INTRODUCTION
The popularity of online forums such as film and book review sites, online political fora, personal blogs, the comments section on newspaper articles, etc., allow people to post their views and opinions on a wide range of topics. The proliferation of such opinion oriented content has led to renewed interest in sentiment analysis and opinion mining techniques to facilitate the automatic analysis and classification of opinions. Automated analysis of opinions has a wide range of applications, including, advertising, political policy formulation, business intelligence applications, etc.
In this paper, we focus on the problem of classifying political opinions (expressed for example, in online debate forums) on controversial questions such as, "Should felons be given voting rights?" and "Should the death penalty remain a legal option in America?" into "pro" opinions and "con" opinions. Figure 1 shows an example of the kind of documents we would like to classify.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Topic: "Should the death penalty remain a legal option?" Pro: "The death penalty for heinous crimes in which the circumstances warrant capital punishment should not be altered. The laws that expand the number of federal crimes punishable by death, including terrorism and narcotics trafficking by drug kingpins should be retained ." Con: "Since I was a law student at Harvard, I have been against the continuation of death penalty. It does not deter. It is severely discriminatory against minorities, especially since they're given no competent legal counsel defense in many cases."
Figure 1: A discussion topic with pro and con documents.
Many previous works on sentiment analysis address the problem of query independent opinion mining. For example, classifying a movie review into positive or negative. However, our setting is query dependent and the classification of a document into pro or con can change depending on the topic (see [1]). For example, in Figure 1, if the wording of the topic were changed to "Should the death penalty become illegal in America?", then the first document is actually a con document while the second document is pro. Moreover, most previous methods rely on training classifiers with annotated training data (see [2] for an overview and [3] for an example of classifying political text), which often has to be annotated manually. In this work, we follow an alternative approach of using language models (LMs) to classify opinions, thus reducing the dependence on annotated training data.
1.1 Our Approach
Our approach follows the query likelihood method [4], where a query is regarded as a sample of the document and its likelihood gives the measure of relevance of the document to the query. Clearly, in our setting, given a discussion topic and a document which debates it, the document will always be highly relevant to the topic. However, our goal is not to quantify relevance, but to classify the document based on its opinion. If a document contains expressions which are in agreement to similar expressions in the topic statement, then it is likely that the document is a pro document. Otherwise, it is likely to be a con document. For example, in the pro document in Figure 1, the expression "capital punishment should not be altered" is in agreement with the discussion topic expression "death penalty remain a legal option", while, in contrast, the con document contains the expression, "against the continuation of death penalty". Finding such expressions and using them for pro/con classification is one of the key challenges that we address in this work.
In brief, our technique is based on the following steps. First, we map both the discussion topic and documents to a set of "interesting patterns". Second, we make use of the interesting patterns from

747

Table 1: Synonyms and Antonyms of terms

Term Synonyms

Antonyms

penalty punishment, retribution award, pardon

legal

sound, lawful

illegal, unlawful, not legal

remain stay, continue

change, alter, not remain

the discussion topic to construct two queries: a "pro" query and a "con" query. Third, for a given test document, an LM is estimated based on the interesting patterns in the document and in the background corpus. And finally, both the pro query and the con query likelihoods are computed and the document is classified as pro or con based on the likelihood values. In the rest of this paper, we formalize our technique and present results of our experiments.
2. PRO/CON CLASSIFICATION MODEL

Interesting patterns. Given a controversial discussion topic
and the corpus of documents debating it, we first identify the "discussion vocabulary", consisting of two kinds of terms: topical terms describing the topic, and pro/con terms describing opinions on the topic. In the discussion topic, nouns are assumed to be topic-related while verbs, adjectives, and adverbs are assumed to be pro/con related. For both types of terms, we look up WordNet for synonyms and antonyms. Table 1 shows examples of synonyms and antonyms for both topical as well as pro/con terms of the discussion topic in Figure 1. Note that the term itself can be negated and is indicated with a "not" and negations of terms are detected during parsing. In Figure 1, topic's topical terms and their synonyms and antonyms in the pro/con documents are italicized, while topic's pro/con terms and their synonyms and antonyms are in bold. Negations are italicized and bold.
Let Ts denote a topic term or its synonym and Ta denote an antonym of a topic term. Similarly, PCs and PCa denote a pro/con term or its synonym and an antonym of a pro/con term. We construct two types of interesting patterns: (1) binary patterns (lexical pairs): Ts, PCs , Ta, PCa , Ta, PCs , Ts, PCa (e.g. penalty, remain , penalty, against continuation , etc.), and (2) unary patterns: Ts, Ta, PCs, PCa. Let B and U denote the set of binary and unary patterns respectively.

Constructing queries. Similar to general ontology based
query expansion, the interesting patterns described above are used to construct two queries: a pro query Q+ and a con query Q-,
Q+ = { Ts, PCs }  { Ta, PCa }  {Ts}  {PCs}
Q- = { Ts, PCa }  { Ta, PCs }  {Ta}  {PCa}

Estimating the LM of a document. An LM MD of a test
document D is estimated as an interpolation of a binary pattern and a unary pattern LMs over all interesting patterns, thus benefiting from both LMs and overcoming the sparseness problem [4]:

PMD (pati|D) = (1 - )PB (pati|D) + PU (pati|D) where PB(pati|D): LM of D over binary patterns, PU (pati|D): LM of D over unary patterns, pati: a pattern and : a weight parameter. A Unary pattern LM PU (pati|D) is estimated as:

PU (pati|D) = (1 - )P (pati|D) + P (pati|C) where pati  U , C: a background corpus and : a smoothing parameter. P (pati|D) and P (pati|C) are estimated as:

P (pati|D) =

P c(pati; D) patj D c(patj ; D)

P (pati|C)

=

P c(pati; C) patjC c(patj ; C)

where c(pati; D) and c(pati; C) denote the frequency of pati

in document D and corpus C, respectively. The binary LM

PB(pati|D) is estimated in an analogous manner.

Classifying the document. Given MD, the estimated LM of
test document D, we estimate the query likelihoods of both the pro

and con queries, assuming independence between patterns. That is,

Y

P (Q+|MD) =

P (pati|D)

pati Q+

Y

P (Q-|MD) =

P (pati|D)

pati Q-

The document is classified as pro if P (Q+|MD) > P (Q-|MD)

and con otherwise.

3. EXPERIMENTAL EVALUATION

Table 2: Precision and Recall of all Techniques

OWN

LM-term SVM

D1 (prec.,recall) 0.66,0.68 0.64,0.66 0.64,0.65

D2 (prec.,recall) 0.67,0.67 0.63,0.62 0.65,0.63

We used two datasets to evaluate our method: one from http://www.procon.org (dataset D1) and another from http://www.opposingviews.com (dataset D2). Both websites contain controversial political questions. Each question has a clearly marked (pro or con) set of documents debating it and thus serving as the ground truth for evaluation. We chose around 350 questions and their corresponding documents from each dataset.
We evaluated our method against two methods: a trained SVM classifier with our patterns as features, and an LM-based method which considers only unary patterns (denoted LM-term). The results in Table 2 show the differences (which are statistically significant) in both precision and recall between our method and the other two methods on both datasets.
4. CONCLUSIONS AND FUTURE WORK
We proposed an LM-based method for classifying political texts into pro or con, based on a controversial discussion topic. We evaluated our proposal and showed that it is promising. In this work, we considered topical and pro/con unigrams. A natural extension is to extend this to n-grams. Our datasets were known to contain formal political opinions expressed in non-emotive language and so, we were able to ignore many other tricky issues, such as, for example, dealing with noisy informal text, and identifying opinion-bearing sentences relevant to the topic. In the future, we plan to extend our techniques to work on other datasets (e.g. newspaper articles, blogs).
5. REFERENCES
[1] K. Eguchi and V. Lavrenko. Sentiment retrieval using generative models. In EMNLP, 2006.
[2] B. Pang and L. Lee. Opinion mining and sentiment analysis. Found. and Trends in IR, 2(1-2):1­135, 2008.
[3] M. Thomas, B. Pang, and L. Lee. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In EMNLP, 2006.
[4] C. Zhai. Statistical language models for information retrieval. Found. and Trends in IR, 2(3):137­215, 2008.

748

Intent Boundary Detection in Search Query Logs

Chieh-Jen Wang

Kevin Hsin-Yih Lin

Hsin-Hsi Chen

Department of Computer Science and Information Engineering

National Taiwan University

Taipei 106, Taiwan

cjwang@nlg.csie.ntu.edu.tw

f93141@csie.ntu.edu.tw

hhchen@csie.ntu.edu.tw

ABSTRACT
Identifying intent boundary in search query logs is important for learning users' behaviors and applying their experiences. Timebased, query-based, and cluster-based approaches are proposed. Experiments show that the integration of intent clusters and dynamic time model performs the best.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation.
General Terms
Algorithms, Experimentation, Measurement
Keywords
Intent clustering, intent boundary detection, query log analysis.
1. INTRODUCTION
Search query logs record users' queries along with their clicked URLs. Many researches [1] try to mine common behaviors from logs, and apply the experience to information access. One issue in query log mining is to determine the intent boundaries in a sequence of queries. The static time-based approach considers the average time per intent and employs a time threshold to partition query logs into intent-coherent sessions. The major problem of this approach is that it is easy to leave out information when using a small threshold, and to introduce noise when using a large threshold. The query-based approach computes the average number of queries needed to fulfill an information need, and uses it to recognize intent boundaries. The query-based approach suffers from the same problem. A very small or a very large threshold will result in too little or too much information.
This paper proposes a dynamic time model which considers user comprehension time in querying and clicking URL. First, the model determines a potential intent boundary. Then, intent clusters learned from a Live search query log [2] are adopted to adjust the boundary. For each query, its query terms, query time, and the associated session are recorded in the log. For each click, the clicked URL, the click time, and the associated query are recorded in the log. In total, there are 7,468,628 sessions. Each session may contain one or more intents. To purify the dataset, we propose the following criteria to select 14,242 sessions for intent clustering: 1) session duration is no longer than 60 minutes; 2) at least 3 distinct queries; 3) at least 3 clicked URLs which can be found in the Open Directory Project (ODP).
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. COMPREHENSION TIME MODEL
A session is a sequence of queries and URLs. The possible transitions between two continuous actions are: qq, qu, uq and uu, where q and q are queries, and u and u are URLs, respectively. Transition q  q means submitting two queries continuously; transition q  u means submitting q and then clicking u reported by this query; transition uq means clicking
u reported by one query and then issuing another query q; transition uu means clicking two URLs reported by one query continuously. |q  q|, |q  u|, |u  q| and |u  u| denote the
duration time for the corresponding transitions. In addition to the explicit submitting and clicking actions, there are implicit actions in a time interval. For example, clicking u, retrieving u, reading u, deciding to initiate a new query q, and submitting it are actions embedded in transition uq.

The duration time |uq| depends on several factors, e.g., Internet

bandwidth, users' comprehension ability, and so on. Here we

neglect the bandwidth issue, and focus on comprehension. We

define the comprehension time h of a specific session as follows, where n is the number of uu transitions in a session for the user:

h

=

1 n

in=1| ui



ui+1

|

Given search query logs, we compute all duration time |qq|, |q

u|, |uq|, and |uu|. Then the following average duration time is derived: 1) |QQ|: the average duration time between

submitting one query and another one, i.e., 98.75 sec in Live search query logs; 2) |UU|: the average duration time between

clicking one URL and another one, i.e., 116.32 sec; and 3) |UQ|:

the average duration time between clicking a URL and then

issuing a query, i.e., 236.74 sec.

We define static and dynamic comprehension time models as follows to capture users' comprehension behaviors:
(1) StaticCTime Model: if |uq| > |UQ|, we propose a
potential boundary before q. (2) DynamicCTime Model: if |uq|-h > |UQ|-|UU|+S,
we propose a potential boundary before q, where S is a standard deviation of UU duration time, i.e., 92.39 sec.

3. INTENT CLUSTERING
Intent clustering aims to group sessions of similar intents into a cluster. Query and clicked URLs are clustering features which can be considered. An individual query may be ambiguous in meaning, but queries in the same session can be disambiguated with one another. Clicked URLs are not always relevant to the queries, but this issue is not dealt with in this paper. Similar to a query, a URL may have more than one ODP category

749

(abbreviated as path hereafter). We resolve the ambiguity by using contextual information. The features of query, URL, path, query+URL, and query+path are explored on complete link and average link clustering algorithms to derive intent clusters.
Given a session of n queries, there may be n possible intent boundaries (defining the last boundary to be right after the last query) and thus n possible segments. We have to compute the similarity of these n segments with all the m intent clusters. The segment of the highest similarity is proposed. We need m*n computations to find boundaries. To reduce time cost, we use time model to propose a potential boundary first. Then we compute the similarity of the segment with all intent clusters, select the cluster with the highest similarity with the segment, and adjust the potential intent boundary (say qi) using the intent cluster. We try to move the boundary to the right to include one more query qi+1 and compute similarity. If the similarity becomes lower, we try to move the boundary to the left to query qi-1 and compute the similarity. Otherwise, we try to move the boundary to the right again. The right/left movement procedure is repeated until the similarity becomes lower.

4. EXPERIMENTS AND DISCUSSIONS

We randomly select 1,000 sessions from Live search query log

data set [2], and manually label the intent boundaries in each

session. Total 1,456 boundaries are identified. Assume ground

truth G and system report S are composed of m and n intent

boundaries, respectively. Let {c1, c2, ..., ck} be k common boundaries between G and S. Precision (P), recall (R), and F-

Score (F) are defined as follows, where bi=1 if there does not

exist any boundary c in G and S such that c is located between ci-1

and ci, otherwise bi=0.

 P =

b k
i=1 i

n

 R =

b k
i=1 i

m

F = 2×P×R P+R

Table 1 shows the performance of four baseline approaches using

time or query thresholds. AvgTime which considers a segment

spanning 20 minutes as an individual intent is the best baseline.

Avg#Queries which regards a segment consisting of 7 queries

performs the next. The performance of considering

comprehension time, i.e., StaticCTime and DynamicCTime, is

behind that of AvgTime and Avg#Queries.

Next we introduce intent cluster to adjust the intent boundary proposed by the DynamicCTime approach. Table 2 shows the results. Intent clusters generated by complete link and average link clustering algorithms on different features are compared. Using URL features is better than using query features. It may be due to that clicked URLs express some sort of users' intents, and users' queries may be ambiguous. Using path feature, denoting ODP category, performs better than using URL features in the complete link algorithm. That meets our expectation because the ODP category is a conceptual representation of URL. Using a Query together with URL/Path is better than using the URL/Path only in these two clustering algorithms. Integrating features of query and path in complete the link algorithm generates the best intent clusters, which are the most helpful to intent boundary identification. The resulting model achieving an F-score 0.6543 is significantly better than the four baselines in Table 1 (ChiSquare test, p-value<0.001).

We also introduce intent cluster to the best baseline, AvgTime. Table 3 shows the results. The tendency is similar to Table 2. Complete link clusters with Query+Path performs better than the

AvgTime only approach. However, it still cannot compete with the approach of integrating intent cluster and DynamicCTime. The results reflect again considering individual comprehension time is quite useful for boundary detection.

Table 1. Using Time/Query Constraints

AvgTime Avg#Queries StaticCTime DynamicCTime

Precision 0.6368 0.6203 0.5171 0.5229

Recall 0.6348 0.6195 0.5175 0.5235

F-Score 0.6355 0.6196 0.5157 0.5217

Table 2. Introducing Intent Clusters to DynamicCTime

CompleteLink Query URL Path
Query+URL Query+Path
AverageLink Query URL Path
Query+URL Query+Path

Precision 0.6305 0.6316 0.6323 0.6334 0.6409 Precision 0.6292 0.6341 0.6324 0.6358 0.6355

Recall 0.6581 0.6598 0.6666 0.6660 0.6681 Recall 0.6609 0.6635 0.6670 0.6667 0.6733

F-Score 0.6441 0.6452 0.6492 0.6495 0.6543 F-Score 0.6447 0.6491 0.6488 0.6509 0.6539

Table 3. Introducing Intent Clusters to AvgTime

CompleteLink Query URL Path
Query+URL Query+Path
AverageLink Query URL Path
Query+URL Query+Path

Precision 0.6313 0.6351 0.6393 0.6349 0.6413 Precision 0.6258 0.6293 0.6311 0.6367 0.6382

Recall 0.6284 0.6319 0.6370 0.6325 0.6384 Recall 0.6235 0.6270 0.6285 0.6344 0.6357

F-Score 0.6294 0.6331 0.6376 0.6334 0.6394 F-Score 0.6242 0.6278 0.6293 0.6352 0.6364

5. CONCLUSIONS
This paper detects intent boundaries in search query logs. The intent clusters generated by using queries and ODP categories of clicked URLs are proved to be useful.

6. ACKNOWLEDGEMENTS
This work was supported in part by Microsoft Research Asia and by the Excellent Research Projects of National Taiwan University.

7. REFERENCES
[1] Jones, R. and Klinkner, K.L. Beyond the Session Timeout: Automatic Hierarchical Segmentation of Search Topics in Query Logs, In Proceeding of the 17th ACM CIKM, 2008, 699-708.
[2] Craswell, N., Jones, R., Dupret, G. and Viegas, E. Workshop on Web Search Click Data, held in conjunction with WSDM 2009.

750

Entropy Descriptor for Image Classification

Hongyu Li, Junyu Niu, and Jiachen Chen
School of Computer Science Fudan University Shanghai,China
{hongyuli,jyniu,0361040}@fudan.edu.cn

Huibo Liu
School of Software Engineering Tongji University Shanghai,China
7huiboliu@tongji.edu.cn

ABSTRACT
This paper presents a novel entropy descriptor in the sense of geometric manifolds. With this descriptor, entropy cycles can be easily designed for image classification. Minimizing this entropy leads to an optimal entropy cycle where images are connected in the semantic order. During classification, the training step is to find an optimal entropy cycle in each class. In the test step, an unknown image is grouped into a class if the entropy increase as the result of inserting the image into the cycle of this class is relatively least. The proposed approach can generalize well on difficult image classification problems where images with same objects are taken in multiple views. Experimental results show that this entropy descriptor performs well in image classification and has potential in the image-based modeling retrieval.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; I.4.8 [Image Processing and Computer Vision]: Scene-Analysis--Object recognition
General Terms
Algorithms, Performance, Experimentation
Keywords
image classification, entropy minimization, cycle
1. INTRODUCTION
Image classification is the task of classifying images according to their object category, which is significant for both effective image organization and retrieval. This problem has been the subject of many recent papers [2, 3] using classifiers based on support vector machine (SVM). Achieving high image classification accuracy, however, is quite challenging. This is partly because semantically related images may reside in an embedded manifold and not a linear hyperplane in the feature space. In this paper, we focus our attention on solving this problem and propose a novel entropy descriptor for image classification.
The proposed entropy is used to describe an embedded manifold with its geometrical features. Image classification is implemented through organizing images as a semantically continuous cycle with entropy minimization. The optimal cycles are actually the extracted
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

model of training images, which could be used to determine the class label of test images. The classification strategy here is to find the best position of a test image in all cycles. The test image is grouped into the class providing the best position, i.e., the class with the least entropy increase resulting from inserting the image to this position. In this study, the optimization problem is solved using tabu search. To expediate the procedure of finding the optimal cycle, parallel computing on GPU is adopted to simultaneously calculate the entropy of each element.

2. ENTROPY DESCRIPTOR
The semantic representation of images is the key to the success of image classification. In this study, we propose GEOmetric Manifold ENtropy (GEOMEN) to describe the semantic similarity of images in the feature space.
Specifically, given a set of image feature vectors X = {xi|xi  Rm, i = 1, 2, ..., n}. We first define an entropy cycle of length n as a closed path without self-intersections. Each vector in this cycle is connected with two neighbors and the corresponding connection order O is symbolized as {o1, o2, . . . , on, o1}, where the entry corresponds to the index of vectors. Then, the GEOMEN of the set X with the order O is represented as the average of the entropy on every point in the cycle as,

1 n

S(X, O) =

s(X, O, i).

(1)

n

i=1

and each s(X, O, i) is the sum of two components: the spatial position p(X, O, i) and geometric g(X, O, i) as,

s(X, O, i) = p(X, O, i) + g(X, O, i).

(2)

Here,  is used to adjust the contribution of p to GEOMEN. GE-
OMEN essentially represents the smoothness and sharpness of the cycle with the connection order O. In addition, it is also a metric
of disorder and similarity of the data in the embedded manifolds.
Since image ranking can be thought as the problem of extracting
a 1-dimensional manifold, actually a curve, we only consider the
representation of GEOMEN on 1-dimensional curves.
The spatial component of GEOMEN is measured by the Euclidean distance, p(X, O, i) = xoi - xo(i+1) 2, where xoi and xo(i+1) are continuously connected in the cycle order O. The geometric component of GEMOEN is composed of two terms: the curvature  of curves and a regularization term . That is,

g(X, O, i) = 2(xo(i-1) , xoi , xo(i+1) ) + 2(xo(i-1) , xoi , xo(i+1) , xo(i+2) ).

(3)

The reason of introducing the regularization term is that the dis-

753

Table 1: Image classification on different datasets.

datasets UMIST car gesture aircraft

class 13 15 11 20

image 360 450 220 2000

ours(%) 97.52(1.93) 93.54(1.64) 91.36(4.19) 81.33(2.21)

SVM(%) 85.84(3.59) 71.04(5.33) 78.64(7.20) 65.34(3.22)

crete curvature is sensitive to noise. The regularization term can significantly improve the robustness of our algorithm.
3. IMAGE CLASSIFICATION
Since there is a clear distinction between two classes, the entropy will greatly increase at boundary points. This enlightens us that misclassifying a point into a class must lead the sharp increase of the entropy of this class with an optimal cycle. In reverse, if a point is correctly grouped into a class, the entropy of this class with an optimal cycle will only change a little, where the principal idea of the proposed classification framework comes from. In our image classification framework, each class is first respectively trained to obtain an optimal cycle (model). Then an unknown image is assigned a class label through comparing the entropy increase after inserting this image into each optimal cycle.
In order to find the optimal cycle, we need to minimize the GEOMEN, O = arg min S(X, O). In this study, we approximate the global minimum of the entropy through a simplified tabu search method. The details about the tabu search method can be found in reference [4]. Since the calculation of entropy of each element is independent, parallel computing is a good way to speed up such an optimization problem. This study makes full use of the ability of graphic processing unit (GPU) in parallel computing and implements the tabu search procedure with the CUDA programming technology.
The optimal cycle of each class is actually the extracted model of training images, which could be used to determine the class label of unknown (test) images. The strategy of classification here is to find the best position of a test image Q in all cycles. We means by "best" that the increase of entropy S due to adding this test image to the current position is the least among all positions. The test image is grouped into the class providing the best position. In this case, however, the decision strategy appears too simple to provide correct class information due to the disturbance of acquisition noise and computation error. Therefore, we bring in the idea of k-NN and make it suitable for our framework. Specifically, we pick first k best positions as possible candidates, and then rank all classes according to the following criterion, r(Ci, Q) = (count(Ci) - )/avg(Ci), where parameter  is introduced to prevent over-fitting. count(Ci) denotes the number of candidates belonging to class Ci. avg(Ci) is the average entropy increase that candidates in class Ci result in and can be formulated as, avg(Ci) = ( Ci S)/count(Ci), where S represents the entropy increase if image Q is added. The rank r(Ci, Q) implies the confidence of image Q belonging to class Ci. Obviously, high confidence requires more count and less avg when  is fixed. Therefore, image Q is considered as a member of the class with highest rank.
4. EXPERIMENTS
The image feature adopted in the following experiments is the Pyramid of Histograms Orientation Gradients (PHOG) [1]. Four

datasets are tested: UMIST face1, car, gesture, and aircraft. For the convenience of 10-fold cross-validation, we discarded those classes with the sample number less than 20 in the UMIST face database. The car images were taken with natural background clutters. Each gesture class includes 20 images taken from multiple views. The aircraft images were produced through projecting threedimensional aircraft models into different planes, which is like changing the viewpoint by rotating the models in some sense.
To compare with state-of-the-art methods, the classifier based on SVM was also tested on the datasets stated above. All test results are presented in Table 1, where the second and third columns respectively represents the number of classes and images in each dataset. The last two columns are the average classification accuracy respectively obtained using our approach and SVM. The value in parentheses denotes the confidence interval. Although it is well known that SVM has the strong ability of generalization in image classification, the proposed framework obviously has the better performance than SVM, higher accuracy and smaller confidence interval. In addition, we employed the NVIDIA GTX 260 GPU to speed up computation. There is distinct improvement of almost 100 times in the computation speed when the size of a class in the aircraft dataset is only 100. Moreover, the speedup ability is proportional to the size of a dataset, which means that our approach has advantages in handling large-scale datasets.
5. CONCLUSIONS
The good classification performance on the UMIST face and gesture datasets demonstrates that the proposed framework is quite promising in the application of face and gesture analysis. In spite of the existence of background clutter in the car images, our method can still work well with the average accuracy 93.54% and confidence interval 1.64, much better than SVM with the average accuracy 71.04% and confidence interval 5.33. The success in classifying the projection images of aircraft models makes possible the future image-based model retrieval. Anyway, no matter how different the viewpoint of images is or how disordered the image background is, the proposed classification method can always perform well, which demonstrates its feasibility and robustness.
6. ACKNOWLEDGMENTS
This research was partially supported by National High Technology Research and Development Program 2009AA01Z429, Shanghai Leading Academic Discipline Project B114, and Natural Science Foundation of China Grant 60903120.
7. REFERENCES
[1] A. Bosch, A. Zisserman, and X. Munoz. Representing shape with a spatial pyramid kernel. In CIVR '07, pages 401­408.
[2] K.-S. Goh, E. Chang, and K.-T. Cheng. SVM binary classifier ensembles for image classification. In CIKM '01, pages 395­402, 2001.
[3] C. Wang, D. M. Blei, and F.-F. Li. Simultaneous image classification and annotation. In CVPR 2009, pages 1903­1910.
[4] C. Zhang, H. Li, Q. Guo, J. Jia, and I.-F. Shen. Fast active tabu search and its application to image retrieval. In IJCAI'09, pages 1333­1338, 2009.
1http://images.ee.umist.ac.uk/danny/database.html

754

Has Portfolio Theory Got Any Principles?
Guido Zuccon, Leif Azzopardi, and C. J. "Keith" van Rijsbergen
Dept. of Computing Science, University of Glasgow Glasgow, Scotland (UK)
{guido, leif, keith}@dcs.gla.ac.uk

ABSTRACT
Recently, Portfolio Theory (PT) has been proposed for Information Retrieval. However, under non-trivial conditions PT violates the original Probability Ranking Principle (PRP). In this poster, we shall explore whether PT upholds a different ranking principle based on Quantum Theory, i.e. the Quantum Probability Ranking Principle (QPRP), and examine the relationship between this new model and the new ranking principle. We make a significant contribution to the theoretical development of PT and show that under certain circumstances PT upholds the QPRP, and thus guarantees an optimal ranking according to the QPRP. A practical implication of this finding is that the parameters of PT can be automatically estimated via the QPRP, instead of resorting to extensive parameter tuning.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval - Retrieval Models General Terms: Theory, Experimentation Keywords: Portfolio Theory for IR, Quantum Probability Ranking Principle, interdependent document relevance
1. INTRODUCTION
Inspired by financial models used in economics, a new model for retrieval has been developed: Portfolio Theory [2, 3]. The intuition behind the model is as follows: estimates of the document's relevance can be improved by accounting for the variance and risk of the estimate in relation to the other documents. Key to the approach is the assumption that document relevance is dependent upon other documents and this must be taken into consideration; violating the PRP. Thus, PT deviates from traditional retrieval models which assume independence between documents. In this poster, we analytically examine PT in the context of the PRP and its Quantum counterpart, explaining the relationships that exist between the different principles and PT.
2. RANKING WITH PORTFOLIO THEORY
Portfolio Theory ranks documents by balancing the probability estimates returned by a retrieval model with the variance of their estimates. This accounts for the risk associated with ranking documents under uncertainty. Specifically, the resulting ranking criteria combines the estimated document
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

relevance with: (i) an additive term which synthesises the risk inclination of the user, (ii) the uncertainty (variance) associated with the probability estimation, and (iii) the sum of correlations between the candidate document and documents ranked in previous positions. In PT, for each rank position i, documents are selected according to:

0

additive term

1

z

}|

{

di = arg max BBP (d) - b(wdd2 - 2 X wd dd d,d )CC

@

A

d RA

(1)

where P (d) is the estimated probability of relevance of docu-
ment d, parameter b encodes the risk propensity of the user, d2 is the variance associated to the probability estimation of document d, wd is a weight inversely proportional to the rank position, which expresses the importance of the rank
position itself, d,d is the correlation between document d and document d , and RA is the list of documents already
ranked. Given this model we now compare it analytically to
the two ranking principles.

Probability Ranking Principle: Under the PRP, the optimal ranking would be obtained by taking, at each rank position i, the document d that maximises P (d). In relation to PT then, when the user parameter b is zero, or documents' variance is null, the additive component of Eq. 1 is zero. In this case, PT upholds the PRP. This guarantees the optimality of the ranking in tasks such as ad-hoc retrieval. But this is a trivial case. As soon as |b| increases, the influence of the additive term will perturb the ranking, and PT will begin to violate the PRP (the greater the |b| the further PT departs from the PRP)1. This is because documents will not be strictly ordered according to their decreasing probability of (independent) relevance as prescribed by the PRP.

Quantum Probability Ranking Principle: While, we have shown that PT does not uphold the PRP in non-trivial cases, here we explore whether PT upholds a different ranking principle: the QPRP [6]. This ranking principle stems from the use of quantum probability theory [1] within IR through an analogy between the ranking scenario and the double slit experiment. In [6], the idea is that the interference between particles is analogous to the interference between document relevance. Essentially, the interference can be thought to represent interdependent document relevance

1Assuming that the other PT parameters are non zero.

755

and it is estimated from documents features and relationships. The resultant ranking principle, the QPRP, retrieves at rank i a document such that:

!

X

di = arg max P (d) +

Id,d

(2)

d RA

!

Xp p

= arg max P (d) +

P (d) P (d ) cos d,d

d RA

where Id,d is the interference between documents d and d and is equivalent to pP (d)pP (d ) cos d,d . The interference arises because in quantum probability theory, the total probability obtained from the composition of the probabilities associated to two events is the sum of the probabilities of the events and their "interference" (i.e. pAB = pA + pB + IAB)2 [1]. The angle d,d is the phase difference between the probability amplitudes associated to documents d and d (see [6] for further details and [5] for way to estimate this component).
Like PT, the QPRP reduces to the PRP when the interference between documents is null, i.e. documents are not interdependently related. And also like PT, the QPRP is characterised by an additive ranking formula, which interpolates relevancy and document dependencies. Previously, we have shown that PT violates the PRP in non trivial circumstances. This is actually desirable, since PT aims to overcome PRP's assumption of independent document relevance.
But, does PT uphold the QPRP? To answer this, we consider a particular situation. We instantiate QPRP approximating the interference term with a function of the Pearson's correlation  between documents term vectors, i.e. cos d,d = -d,d , as suggested in [4]. The QPRP's ranking formula can be written as:

!

Xp p

di = arg max P (d) -

P (d) P (d )d,d

(3)

d RA

Similarly, the Pearson's correlation  can be employed to
measure the correlation in Eq. 1, as proposed in [3]. More-
over, since d is assumed to be a constant for each document in the collection3, Eq. 1 can be re-stated as

!

di = arg max P (d) - bd2(wd + 2 X wd d,d )

d RA

!

= arg max P (d) - X 2bd2wd d,d

(4)

d RA

where wd is dropped for rank equivalence reasons, i.e. whatever the d under consideration, wd is constant and so is bd2wd. When instantiating PT in these particular circumstances, b and d2 can be treated as parameters to be tuned. In particular, PT delivers the same ranking of QPRP, i.e.
theoretical optimal performances under QPRP's assump-
tions, when:

X

pp P (d) P (d )d,d

=

X

2bd2wd d,d

(5)

d RA

d RA

2As opposed to what happens in Kolmogorovian probability theory,
i.e. pAB = pA + pB , when A and B are mutually exclusive events. 3This assumption is realistic in the case relevance probabilities are es-
timated using the Okapi BM25 scoring schema, and has been already
introduced in [3].

or when the two quantities are proportional (this is justified
by rank equivalences). This relation can be exploited to
estimate PT's parameters and thus guaranteeing optimality
under the QPRP. In fact, from Eq. 5 and focusing on a particular d , these will be characterised by the pairs (b, d2) and the function wd such that:

bd2

=

pP (d)pP (d 2wd

)

(6)

While the parameterization of Portfolio Theory means that the ranking strategy is more general and configurable than the QPRP, this introduces the complexity and burden of having to estimate these parameters. By using this relationship with the QPRP it is possible to directly estimate the parameters of PT without requiring training data and parameter estimation problems.

3. DISCUSSION AND CONCLUSION
In this poster, we have shown that Portfolio Theory for IR upholds the different ranking principles under particular conditions. While the fact that PT upholds the PRP in only a trivial case is not very useful, the fact that PT can uphold the QPRP in certain non-trivial settings is potentially very useful. This is an important contribution because it shows that PT, under particular circumstances, upholds the Quantum Probability Ranking Principle. This implies that the parameters of PT can be automatically estimated via the relationship with the QPRP and, by doing so, guarantees theoretically optimal performance under the QPRP. This has the added benefit that no expensive parameter tuning is required. It may also be the case that developments within the QPRP, specifically how the angle between documents is approximated, could also be transferred to PT, further improving the method.
Future work will be directed towards empirically investigating whether estimating the parameters of PT given this relationship with the QPRP lead to effective approximations that validate these findings.

Acknowledgments. The authors would like to thank Alvaro Huertas-Rosero. This work is partially funded by EPSRC EP/F014384/.

4. REFERENCES
[1] R. P. Feynman. The Concept of Probability in Quantum Mechanics. In Proc. 2nd Berk. Symp. on Math. Stat. and Prob., pages 533­541, 1951.
[2] J. Wang. Mean-variance analysis: A new document ranking theory in information retrieval. In ECIR '09, pages 4­16, 2009.
[3] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR '09, pages 115­122, 2009.
[4] G. Zuccon and L. Azzopardi. Using the Quantum Probability Ranking Principle to Rank Interdependent Documents. In ECIR '10, pages 357­369, 2010.
[5] G. Zuccon, L. Azzopardi, C. Hauff, and C. J. van Rijsbergen. Estimating interference in the QPRP for subtopic retrieval. In SIGIR '10, 2010. to appear.
[6] G. Zuccon, L. Azzopardi, and C. J. van Rijsbergen. The quantum probability ranking principle for information retrieval. In ICTIR '09, pages 232­240, 2009.

756

Comparing Click-through Data to Purchase Decisions for Retrieval Evaluation
Katja Hofmann, Bouke Huurnink, Marc Bron and Maarten de Rijke
ISLA, University of Amsterdam, The Netherlands
{k.hofmann, bhuurnink, m.m.bron, derijke}@uva.nl

ABSTRACT
Traditional retrieval evaluation uses explicit relevance judgments which are expensive to collect. Relevance assessments inferred from implicit feedback such as click-through data can be collected inexpensively, but may be less reliable. We compare assessments derived from click-through data to another source of implicit feedback that we assume to be highly indicative of relevance: purchase decisions. Evaluating retrieval runs based on a log of an audiovisual archive, we find agreement between system rankings and purchase decisions to be surprisingly high.
Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval
General Terms: Experimentation, Human Factors
Keywords: Query log analysis, Evaluation
1. INTRODUCTION
Traditionally, information retrieval experiments use explicit relevance judgements. Annotators examine queries and candidate documents, explicitly judging which documents are relevant to a query. The use of explicit judgments is problematic: the judging process takes a lot of time, there can be wide interannotator variation [1], and explicit judging may not result in the same assessments that a user would make in a real search situation [6].
As an alternative to explicit relevance judgments, researchers have started to use click data from search engine transaction logs to infer relevance judgments for user searches [3, 5]. Click data can be collected unobtrusively, resulting in large numbers of judgments, and it reflects the search behavior of the original user. However, clicks are qualitatively different from explicit judgments, and agreement between the two is typically low. Kamps et al. [4] found large differences between system rankings based on explicit relevance assessments and those based on click data in web search.
We compare the use of click data for system evaluation to a related form of implicit relevance judgment -- purchase decisions. One goal in commercial environments can be to rank items a user will buy as highly as possible. Here, purchase decisions would be a logical source of relevance judgments for evaluating system performance. Furthermore, if system evaluation based on clicks gives similar results to system evaluation based on purchases, then it would be possible to evaluate systems on a larger amount of data, as clicks are more plentiful than purchase decisions (in our data, the ratio between clicked items and purchased items is over 10 to 1), and commercially less sensitive.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

We address the following question: do system rankings based on relevance judgments inferred from clicks agree with system rankings based on judgments inferred from purchase decisions? We investigate this question by comparing the system rankings resulting from three sets of relevance assessments extracted from a large log of an audiovisual archive. The first two sets are based on queries that resulted in purchases -- the first considers purchased items as relevant, the second clicked items. The third set is based on queries that did not result in purchases and considers clicked items as relevant. We simulate comparison of retrieval systems by generating 22 retrieval runs with different query weighting and term normalization schemes. Our goal here is to obtain a diverse set of runs, so that the sensitivity of evaluations using the different types of relevance assessments can be investigated. The runs are evaluated against the three sets of relevance assessments using standard retrieval measures and then ranked by performance.
We find that, in our setting, evaluation based on purchase decisions results in system rankings that are highly correlated with those based on click data. Rankings based on click data for queries that resulted in a purchase are close to identical to system rankings based on purchase decisions, while agreement with clicks on queries that did not result in a purchase is lower but still significant.
2. DATA AND METHODS
We first detail the collection and log data used in our study. Then we outline how queries and relevance assessments were derived, and how the retrieval runs were generated and evaluated.
We obtained click and purchase data from the Netherlands Institute for Sound and Vision, a large national audiovisual broadcast archive. To enable search, the archive indexes catalog entries describing the audiovisual documents in the collection. The archive primarily serves media professionals, who can search for and purchase audiovisual material for reuse in new productions. User interactions, including searches, result clicks, and purchases, are recorded in transaction logs; in this paper we use the data set described in detail in [2].
We use two sets of queries: (1) purchase queries that resulted in an order from the archive, and (2) non-purchase queries where results were clicked but nothing was ordered. Purchase queries are associated with both click data and purchase decisions. As an item must be clicked before it can be ordered, purchase decisions form a subset of the clicked results (in our data set overlap is 61%). Nonpurchase queries are associated only with click data.
Result clicks and purchase decisions are mapped to queries in the archive transaction logs as follows: (1) identify the documents that were clicked for each query; (2) identify the documents that were purchased for each query; (3) collapse identical queries, and their associated clicked and purchased documents. Using this method

761

MAP 0.0 0.1 0.2 0.3 0.4 0.5 tagsonly 2 tagsonly 1 content_nofilters 2 content_nofilters 1 tags_free 2 tags_free 1 freetext 1 freetext 2 title 2 tags_content 2 title 1 tags_content 1 meta 2 free_content 2 free_content 1 meta 1 all_content 2 meta_content 2 title_content 2 meta_content 1 title_content 1 all_content 1

purchase click (queries with purchase) click (queries without purchase)
Figure 1: MAP per run and type of relevance assessment.
we extracted 13, 506 unique purchase queries from the logs, which were associated with 28, 761 clicks and 17, 629 purchases. These queries were randomly split in half to obtain two non-overlapping sets of relevance assessments: one for evaluation using purchases, one using clicks on purchase queries. The third set of assessments was created using the 83, 898 unique non-purchase queries contained in our data set. These were associated with 33, 379 clicks.
Retrieval systems were created using the catalog entries maintained by the archive. We built two indexes using Lucene (http: //lucene.apache.org/): for the first we preprocessed the collection using a standard tokenizer (index 1), for the second we also removed stop words and applied stemming (index 2). With the two indexes we generated 22 retrieval runs based on one or more of the following fields: content (all text associated with a document), free(text) (summary and description), meta (title and technical metadata), and tags (named entities, genre). By default we included the date filter and advanced querying options provided in the original search interface.
To assess retrieval performance we use mean average precision (MAP), mean reciprocal rank (MRR), and precision at 10 (P@10) using each set of relevance assessments. Systems are ranked according to each measure; agreement between rankings is measured using Kendall's  rank correlation, and the number of pair-wise switches that would be required to turn one ranking into the other.
3. RESULTS AND DISCUSSION
We first summarize the retrieval scores obtained by our retrieval systems. Then we compare the system rankings obtained using the three sets of relevance judgments inferred from purchase and click data. Fig. 1 shows the performance in terms of MAP for all generated runs. System performance varies widely by run and assessment, ranging from 0.0382 (tagonly 2, clicks -- no purchase) to 0.511 (all_content 1, clicks -- with purchase). System rankings are similar for MRR (omitted due to limited space, absolute scores range from 0.049 to 0.538) and P@10 (0.008 to 0.097).
In terms of absolute scores, system performance is very similar when using purchase decisions and clicks from purchase data, even though these are obtained on different sets of queries. Differences are greater when looking at clicks from queries that did not result in a purchase. For the two runs content_nofilter, scores are substantially higher than when evaluating with queries that resulted in a purchase. For the remaining runs, scores are lower, with some systems changing ranks when evaluated on the different sets of queries. Despite differences in terms of absolute scores, a clear trend is visible: systems that score highly on one set tend to perform well on the other set too. Differences between evaluation scores based on purchases and clicks on non-purchase queries are system-

Table 1: Agreement between system rankings generated by click vs. purchase data according to standard evaluation measures. Agreement is calculated using Kendall's  , and the number of pair-wise switches between ranked systems. All correlations are statistically significant with p 0.001.

purchases vs clicks

purchases vs clicks

from purchase queries from non-purchase queries

measure 

switches 

switches

MAP 0.974

6 0.766

54

MRR 0.948

12 0.766

54

P@10 0.991

2 0.775

52

atic and indicate a qualitative difference between the two sets of queries. For example, queries that did not result in a purchase use date filters a lot less often (22% vs. 46%), which explains the performance jump on the nofilter runs. We think that queries that did not result in purchases are more exploratory in nature, while queries that did result in purchases include many known-item searches.
For purchase queries (Table 1) system rankings using purchase decisions are highly correlated to those using clicks, with a rank correlation of 0.974 for MAP and similar values for MRR and P@10. In contrast, the correlation between system rankings using purchase decisions and those using clicks from non-purchase queries is lower at 0.77, but still statistically significant.

4. CONCLUSION
We investigated the use of purchase and click data for evaluating retrieval systems in a commercial setting. We found system rankings based on clicks to be close to identical to those based on purchase decisions when considering queries that resulted in a purchase. The high agreement between system rankings based on purchase decisions and those based on clicks is somewhat surprising as there is a marked difference in the size of the recall bases. While system ranking agreement is lower when evaluating systems with click data from non-purchase queries, it is still surprisingly high -- especially in view of the low agreement that has been found between click data and explicit relevance assessments [4]. This may be due to the size of our data set and the professional nature of our users and their tasks; moreover, purchase decisions may be a better indicator for contextual relevance than explicit feedback.
Acknowledgements This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802.

5. REFERENCES
[1] S. P. Harter. Variations in relevance assessments and the measurement of retrieval effectiveness. JASIS&T, 47(1):37­49, 1996.
[2] B. Huurnink, L. Hollink, W. van den Heuvel, and M. de Rijke. The search behavior of media professionals at an audiovisual archive: A transaction log analysis. JASIS&T, 2010. DOI: 10.1002/asi.21327.
[3] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05, pages 154­161, 2005. ACM.
[4] J. Kamps, M. Koolen, and A. Trotman. Comparative analysis of clicks and judgments for IR evaluation. In WSCD '09, pages 80­87, 2009. ACM.
[5] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In CIKM '08, pages 43­52, 2008. ACM.
[6] I. Ruthven. Integrating approaches to relevance. New directions in cognitive information retrieval, pages 61­80, 2005. Springer.

762

Personalize Web Search Results with User's Location
Yumao Lu Fuchun Peng Xing Wei Benoit Dumoulin
Yahoo Inc. 701 First Avenue Sunnyvale, CA, 94089
{yumaol,fuchun,xing,benoitd}@yahoo-inc.com

ABSTRACT
We build a probabilistic model to identify implicit local intent queries, and leverage user's physical location to improve Web search results for these queries. Evaluation on commercial search engine shows significant improvement on search relevance and user experience.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
query log analysis, personalized search
1. INTRODUCTION
There is a huge amount of searches on the Web has local intent, meaning that they are searching for things in a particular area, like restaurant, job listings, shopping center, etc. Some queries have explicit location information in the query like "pizza hut palo alto", while many others do not but still expect search engines to return localized search results, like "laundry service". This kind of queries is considered as implicit local intent queries. For such queries, users expect personalized search results [2] that are customized to their locations. Thus, identifying implicit local intent queries and finding out the location information for the users are particularly useful to improve user search experience.
Yi et al. [3] uses language modeling approach to identify implicit local intent queries. In our work we also use language modeling approach, however instead of building a language model for each city, we only build one single language model for all locations to avoid sparsity. We then obtain user's location directly from IP address mapping. What further makes our work unique is that we integrate the identified implicit location information into ranking directly to improve search relevance and prove the practical impact of this work. As far as we know, there is little work has been published in this front.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. IMPLICIT LOCAL INTENT DISCOVERY
Let Q denotes the general query set. A conditional random field (CRF) based named entity tagger is used to tag all the queries in Q. We select all queries that contain a location from Q to form a new query set QL. We then remove all location components from queries in QL and form an artificial query set QLC . We define the probability that a query q has implicit local intent by

P (implicit

local

intent|q)

=

PQLC (q) , PQ(q)

(1)

where PQLC (q) and PQ(q) can be estimated using n-gram language model from corpus QLC and Q respectively.
Equation (1) however does not always give an accurate estimation on P (implicit local intent|q) since locations are often used as constraints for queries that do not have local intent. For example, query "John San Jose mypace" implies the user is looking for some one named John in San Jose from myspace. A popular domain name "myspace" is a strong indicator for such case. We create a feature called highest domain rank for each query

Rdm(q)

=

min
sq

Rd(s)

(2)

where s is a substring of query q and Rds is the rank of domain s, which is ranked by the accumulated number of clicks on the documents that belong to domain s. The lower value Rd(s) is, the more popular domain s is. In implementation, we only keep tracking top 1000 domains for simplicity.
A implicit local query may not contain general local intent which should return same results even when queries are from different locations. For example, "Google headquarter office" contains implicit local query but the intent is restricted to a specific location as there is only one Google headquarter office. To capture the general local intent, the entropy of the conditional probability distribution P (l|q), where l is a specific location associated with query q that is identified from user IP address (described later). To calculate P (l|q), we first normalize set QL to set Q¯L such as all locations are disambiguated and canonicalized. For example, "CA" is canonicalized to "California", "la" is canonicalized to "Los Angeles" or "Louisiana", "New York" is canonicalized to "New York City" or "New York State" based on the context. The conditional probability distribution P (l|q) can be then estimated through n-gram language model estimated in corpus Q¯L:

P (l|q)

=

PQ¯L (l, q) , PQ¯L (q)

(3)

763

where PQ¯L (l, q) is the probability of location l and a query q co-occur estimated in corpus Q¯L. The entropy E(q) is defined as

E(q) = - P (l|q) log p(l|q).

(4)

l

If E(q) is high, the query q is more likely to be associated with many locations with similar possibilities; otherwise, q is biased towards certain locations.
Gaussian kernel support vector machine is used as the classifier. We first train a weak classifier with 5000 editorially labeled random query set. For each query, we generate features based on (1) (2) and (4). The resulting weak classifier based on the 5000 training data is used to label 100, 000 queries. Queries that lie between the margin (with non-zero  values are selected for the next batch of editorial test. The classifier is then retrained with combined labeled data.
After a query is classified as having implicit general local intent, we use user's IP address to obtain the city name and zip code where the query is sent from. For ambiguous city names that exist in multiple states (such as Oakland) or that have different meanings (such as Mountain View), we add use user location's state name for disambiguation.

3. PERSONALIZE WEB SEARCH RESULTS

3.1 Personalization by Query Rewriting
To personalize search results given user's location, one intuitive way is to expand the original query by user's location. For example, if the original query is "Italian restaurants" and the user's location has been determined to be "San Francisco", a new search query may be formed as "Italian restaurants San Francisco" by appending the determined location to the original search query. The new search query is then issued to the search engine to obtain the search result for the user. Because the location "San Francisco" is now included in the new search query, based on which the search result is identified, the search engine is more likely to find Italian restaurants located in San Francisco. This approach, however, suffers from two sources of errors: (1) the local intent might not be the only intent of the query or even may not exist due to limited precision of the general implicit local intent classifier; (2) the user's location may be determined wrongly.
3.2 Personalization by Re-ranking
As a more conservative approach, document re-ranking is proposed to leverage user's location. We first extract top K documents with the original query and then adjust the search results by increasing the ranks of those documents that match the user location. Consequently, the Web documents that match the user's location are ranked higher than those documents that do not match the user's location. We also differentiate and weight user location matches for different sections in top documents. We re-rank those document based on their current rank score and text matching features that tell if user location and its variation exists in certain document sections. The re-rank score sr(q, d, l) for query q, Web document d and user location l can be expressed by

sr(q, d, l) = s(q, d) + wiIi(d, l),

(5)

i

where s(q, d) is the original rank score before personaliza-

tion, Ii(d, l) is an indicator function that tells if user location l exist in document section i and wi is the weighting parameter for document section i. Supervised learning is used to estimate the parameter wi to maximize the relevance after personalization.
4. EXPERIMENTS
We evaluate our personalization system by both editorial relevance test based on Discounted Cumulative Gain (DCG) and online bucket test for user experience evaluation based on click-through rate (CTR), two commonly used metrics to evaluate search engine relevance and user experience [1].
We apply both query expansion and document re-ranking to the queries that are classified as general implicit local queries. We sample 1300 random personalized queries and submit the top 5 Web search results for each approaches together with test queries and the user location for each query to trained editors for relevance judgment. If a test query does not contain general implicit local intent, the user location information will be ignored in the judgment; otherwise, the user location will be considered in the relevance judgment. The baseline is one of the top commercial search engines. We can see from Table 1 personalized Web search results dramatically improved search relevance. In online test, we also observe that user experience measured by CTR has been improved significantly by 1.4% with p - value < 0.05 for affected queries.

Table 1: Relevance Impact with Personalization

Search Engine

DCG Improv. p-value

baseline

2.71

-

-

Query Expansion 2.96 9.2% 0.000

Re-rank

3.29 21.4% 0.000

5. CONCLUSION AND FUTURE RESEARCH
We successfully improved relevance the user experience over one of the top commercial search engine by personalizing search results using user's physical location. The current model covers 2.6% search traffic. We plan to increase the coverage substantially by incorporating more salient features and better language models. The current re-ranking algorithm is rather simple and can be substantially improved by jointly considering location matching features and other ranking features together in the phase of re-rank.
6. REFERENCES
[1] B. Carterette and R. Jones. Evaluating search engines by modeling the relationship between relevance and clicks. In NIPS, 2007.
[2] A. Micarelli, F. Gasparetti, F. Sciarrone, and S. Gauch. Personalized search on the world wide web. The Adaptive Web, 4321 of Lecture Notes in Computer Science:195 ­ 230, 2007.
[3] X. Yi, H. Raghavan, and C. Leggetter. Discovering Users' Specific Geo Intention in Web Search. In WWW, 2009.

764

Using Search Session Context for Named Entity

Recognition in Query

Junwu Du1, Zhimin Zhang2, Jun Yan2, Yan Cui1, Zheng Chen2

1School of Computer Science Beijing Institute of Technology
Beijing, 100081, P.R. China

2Microsoft Research Asia Sigma Center, 49 Zhichun Road
Beijing, 100080, P.R. China

du.junwu@gmail.com; cui.yan@live.cn

{zhzha, junyan, zhengc}@microsoft.com

ABSTRACT
Recently, the problem of Named Entity Recognition in Query (NERQ) is attracting increasingly attention in the field of information retrieval. However, the lack of context information in short queries makes some classical named entity recognition (NER) algorithms fail. In this paper, we propose to utilize the search session information before a query as its context to address this limitation. We propose to improve two classical NER solutions by utilizing the search session context, which are known as Conditional Random Field (CRF) based solution and Topic Model based solution respectively. In both approaches, the relationship between current focused query and previous queries in the same session are used to extract novel context aware features. Experimental results on real user search session data show that the NERQ algorithms using search session context performs significantly better than the algorithms using only information of the short queries.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process
General Terms
Algorithms, Experimentation
Keywords
Search Session, Named Entity Recognition, CRF, Topic Model
1. INTRODUCTION
Nowadays, the Named Entity Recognition problem in Query (NERQ) has attracted increasingly attention from information retrieval community since it can be used to fix a number of failure cases in the relevance based search engines. Figure 1 gives an example of failure case in a commonly used commercial search engine. If we can identify "Eagle Vision" as a named entity in query, this irrelevant result will not be returned to end users with high rank. To our best knowledge, there are only a few previous studies that have tried to recognize named entities in queries. As one of the earliest related works, Guo et al [3] has tried to use a topic model to identify named entities in queries and they showed that around 70% of the real search queries contain named entities. However, since the queries are always very short (i.e., 2-3 words on average) and many of them do not satisfy the natural language grammar [3], both the classical Named Entity Recognition (NER)
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

algorithms [2] and the algorithms particularly designed for query [3] may fail when the queries are short with limited context.

Figure 1: A failure case of a commercial search engine

In this paper we propose to address the NERQ problem by utilizing the context information in search sessions, once used to perform QC [1]. We extract two new types of features from the search sessions, namely class feature and overlap feature to help performing NERQ.

2. CONTEXT AWARE NAMED ENTITY RECOGNITION

In this Section, we propose two novel search session features for

NERQ. Suppose the predefined classes are

, , ... and the

queries, which are in the same search session, are

, ....

Let the words, which make up of a query be

, ,... .

2.1 Class Feature and Overlap Feature
In a search session, a sequence of queries can more accurately
identify the class than just the current focused query.

Definition-1: the class feature extracted from queries before the

current focused query

1 in the same search session is

defined as

, , ... , ... | | , and the is defined as:



Pr

0

,1 ,1

(1)

Definition-2: the overlap feature extracted from the current focused query q i 1 and its previous queries in the same search session is based on single word. This feature is defined as
, , ... , ... | | , and is defined as:

 0

,, ,

1

, 1

(2)

In Eqn. (2), , , is defined as:

1 ,

,,

0 ,

(3)

765

2.2 Applications of New Features
In the CRF [4] based method, two new types of search session features are used by the classifier. Following are the details of the
new features:

 Class feature: using every element of a CRF feature.

in Definition-1 as

 Overlap feature: using every element of

in

Definition-2 as a CRF feature.
In the Topic Model based method, a single-named-entity query is represented as triples , , , where denotes named

entity, denotes the context of in , and denotes the class of [3]. In our experiment, for simplicity, only one query before the current focused query is considered. In online prediction, we try to segment both the current focused query and the previous query in all possible ways together, and the triples with the

highest

value are output results for NERQ.

is defined as:

Pr , , Pr , , (4)

where Pr , , and Pr , , are the joint probabilities

with respect to previous query and current query in the same

search session.

represents the class distribution similarity

between the two sequential queries, which is defined as :

| | | | Pr

Pr |

(5)

In Eqn. (5), | |is the number of predefined classes. represents the effectiveness of the overlap feature, which
is defined as:



(6)

In Eqn. (6), is number of words in , is the entity's first

word's index in the current focused query, and is defined in

Eqn. (2). is the weight parameter of overlap feature, and in our

experiment we define

as P

,, ||

.

3. EXPERIMENTS
In the experiments of this work, we only use the car model domain queries for demonstration. We have two sets of experiments by CRF and topic model respectively to compare the NERQ results with and without context information in search sessions.

In the experiment of using CRF model for NERQ, we start with a small car name dictionary which contains 1,663 standard car names to build a dataset for evaluation. Using the car name dictionary, we firstly scan one month's click-through log data of a commercial web search engine. If a query which contains car names in the car name dictionary is found by exact match, then all the queries in the same search session are added to our dataset with the query order in session unchanged. In this experiment, we defined 5 named entity (NE) tags, namely 0 (Not entity), 1 (Single word entity), 2 (Beginning of the entity), 3 (Middle of the entity), 4 (Ending of the entity). We have asked 10 human labelers to manually label car domain named entities in queries of our dataset. From the labeled data, 5000 queries and their corresponding search sessions are used as training set and 1000 queries in remaining search sessions are used as testing set. In this

experiment, the CRF classifier without considering the search session context is used as baseline, which is represented by CRF. On the other hand, our approach is the CRF algorithm that uses our proposed session level context features. We name it as the Context aware CRF (CCRF). The evaluation metrics used for the two classifiers are precision, recall and F-measure. Results of evaluation comparison of the two classifiers are shown in Table 1.
Table 1: With/without session context by CRF classifiers

NE

Precision

Recall

F1

tags

CRF CCRF CRF CCRF CRF CCRF

0

45.45% 49.35% 57.38% 69.09% 50.72

57.58

1

62.69% 79.10% 66.67% 71.62% 64.62

75.18

2

85.71% 83.93% 63.16% 67.14% 72.73

74.60

3

47.37% 63.16% 47.37% 50.00% 47.37

55.81

4

69.64% 71.43% 69.64% 76.92% 69.64

74.07

Overall 62.91% 69.09% 62.91% 69.09% 62.91

69.09

In the experiment by using topic model, 160 car names are selected as seed named entities. We constructed the topic model in the same way with the work [3] using the seeds and search engine log. 1,000 query pairs in the same search session were randomly selected from the dataset, generated in the experiment of using CRF, as test set. In this experiment, the Topic Model without considering search session context is used as baseline. In contrast, our approach is the Topic Model algorithm that has used the search session context features. We name it as the Context aware Topic Model (Ca Topic Model). We use two word indexes of the query to label the beginning and the ending of the named entity in the query. The evaluation metrics used for the two topic models are precision. The experiment results are shown in Table 2.

Table 2: With/without session context by Topic Models

Topic Model Ca Topic Model

NE Beginning 65.9% 74.8%

Precision
NE Ending 59.3% 65.4%

Total NE 54.2% 60.6%

4. CONCLUSION
In this extended abstract, we proposed two context aware features extracted from user search sessions to tackle the problem of named entity recognition in query. We applied the two features to improve two classical NER algorithms, which are known as Conditional Random Field (CRF) and Topic Model. The experimental results on real user search session data show that the NERQ algorithms using search session context through our proposed features can perform significantly better than the classical NER algorithms using the short query information only.

5. REFERENCES
[1] Cao, H., Hu, D., Shen, D., Jiang, D., Sun , J., Chen, E., Yang, Q. 2009. Context-Aware Query Classification. In SIGIR '09
[2] Ekbal, A., Haque, R., Das, A., Poka, V., Bandyopadhyay, S. 2008. Language Independent Named Entity Recognition in Indian Languages. In Proc.IJCNLP-08.
[3] Guo, J., Xu, G., Cheng, X., Li, H. 2009. Named Entity Recognition in Query. In SIGIR '09, pages 268.
[4] Lafferty, J., McCallum, A., Pereira, F. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. ICML.

766

Evaluating Whole-Page Relevance
Peter Bailey, Nick Craswell, Ryen W. White, Liwei Chen, Ashwin Satyanarayana, and S.M.M. Tahaghoghi {pbailey, nickcr, ryenw, liweich, assatya, stahagh}@microsoft.com
Microsoft Corporation, One Microsoft Way, Redmond, WA 98052 USA

ABSTRACT
Whole page relevance defines how well the surface-level representation of all elements on a search result page and the corresponding holistic attributes of the presentation respond to users' information needs. We introduce a method for evaluating the whole-page relevance of Web search engine results pages. Our key contribution is that the method allows us to investigate aspects of component relevance that are difficult or impossible to judge in isolation. Such aspects include component-level information redundancy and cross-component coherence. The method we describe complements traditional document relevance measurement, affords comparative relevance assessment across multiple search engines, and facilitates the study of important factors such as brand presentation effects and component-level quality.
Categories and Subject Descriptors
H.3.4 [Systems and Software]: Performance evaluation (efficiency and effectiveness)
General Terms
Design, Experimentation, Measurement
Keywords
Web search relevance, measurement, evaluation
1. INTRODUCTION
Traditional information retrieval (IR) evaluation methodologies (e.g., [2][6]) judge the relevance of the individual documents from a ranked list returned for a query, and compute a single performance score that is averaged across many queries. This method of assessing a search engine's result relevance manifests a high level of abstraction over the retrieval task by eliminating several sources of variability [5], enabling important experimentation, measurement, and evaluation efforts. However, this method ignores page elements other than the main document ranking, the surface-level representation of these elements, and holistic resultspage characteristics such as coherence, diversity, and redundancy.
Search engine result pages (SERPs) play a critical role in the Web search process. Web searchers first interact with the SERP returned for their query and then with the retrieved results. On the SERP, each result has a summary and may include multimedia or links to additional documents. Around and interspersed within the ranked list are other page elements such as suggested spelling corrections, suggestions of follow-on queries, results from alternate queries, advertising links, and mini-rankings from other sources such as news, image, and video search.
Whole-page relevance (WPR) defines how well SERP components and the corresponding holistic attributes of the result page
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

presentation respond to searchers' information needs. Despite its importance, whole-page relevance is seldom considered in IR evaluation. User studies (e.g., [4]), log analysis of user interaction with SERP components (e.g., [1]), and parallel A/B testing (e.g., [3]) can capture aspects of WPR but are limited in terms of factors such as scalability (user studies are costly and time consuming) or their ability to capture qualitative feedback (log analysis and A/B tests study only behaviors rather than users' rationales for them).
This poster presents an evaluation method for whole-page relevance. Our evaluation metaphor draws on teaching assessment and has judges consider the SERP responses to a query as though they were teachers grading school assignments from multiple students. While each student may have different styles and layout, overall they can be graded with respect to how well they address and satisfy the information needs represented in the assignment. Assignments can be graded both on component elements of their response (e.g., did they mention an important fact?) and on holistic aspects such as coherence, comprehension, and use of authoritative sources. This metaphor gives rise to our method's name: the School Assignment Satisfaction Index (SASI).
2. OVERVIEW OF THE SASI METHOD
The SASI method is a framework for judging aspects of SERP component and holistic relevance. It differs from traditional Cranfield-style evaluations [2] in three important ways: (i) judging the surface-level presentation on the SERP rather than the full content of documents, (ii) judging all components of the SERP rather than only the top-ranked algorithmic search results, and (iii) judging the SERP components in context rather than judging each document in isolation. Since SASI focuses on assessing the user's experience when interacting with the search engine, SERPs are captured and presented to judges in their entirety. However, there is no interaction with the interface components or any hyperlink clicking which may take the judge away from the SERP and potentially introduce bias from landing pages encountered. SASI focuses on judging only the surface-level representation of the page components, but an alternative WPR judging system could also judge the landing pages of all linked items.
3. EXAMPLE SASI JUDGE INTERFACE
Figure 1 shows our implementation of a SASI judging interface, with a judging pane and a pane showing a Microsoft Bing SERP for the query [generic drugs]. During judging, the interface can highlight the element currently being judged, which in Figure 1 is page middle answer ­ a universal search result in the middle of the page. In the figure, the first of the top-10 results (top algo) has already been rated as good. Note that this is just one example of a judging interface, and others could be used. For example, although we adopted an emoticon-based judging scale; numeric or label scales could also be used. The selection of a three-point scale is arbitrary; a two, four, or five point scale may be equally or more effective for providing SASI judgments.

767

Figure 1. SASI judging interface example. Page middle answer is currently being judged.

4. CONCLUSIONS
We have introduced SASI, a new evaluation method that deliberately considers only surface-level information on a search results page, enabling evaluation of whole-page relevance. While sharing some similarities with traditional search evaluation, the SASI method does not generate a reusable test collection because the judgments are not independent of other components shown on the page. Instead it gives visibility into different search system characteristics, that are normally evaluated using user studies or A/B testing. SASI is an efficient form of experiment; initial testing with the judge interface described herein revealed that a whole SERP can be judged in the time it takes to judge two documents in Cranfield-style experiments. We have also performed further investigations of the utility of SASI for evaluating whole-page Web search relevance, revealing that it provides insights on likely user perceptions of relevance that are unavailable via traditional IR evaluation methods (e.g., component-level quality and search engine branding effects). In targeting whole-page relevance, SASI provides a useful complement to document relevance approaches for assessing search performance. In future work we will refine SASI to further our understanding of whole-page relevance.

REFERENCES
[1] Clarke, C.L.A., Agichtein, E., Dumais, S., and White, R.W. (2007). The influence of caption features on clickthrough patterns in web search. Proc. SIGIR, 135-142.
[2] Cleverdon, C.W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.
[3] Kohavi, R., Henne, R., and Sommerfield, D. (2007). Practical guide to controlled experiments on the web: listen to your customers not to the HiPPO. Proc. SIGKDD, 959-967.
[4] Turpin, A., Scholer, F., Järvelin, K., Wu, M., and Culpepper, J.S. (2009). Including summaries in system evaluation. Proc. SIGIR, 508-515.
[5] Voorhees, E.M. (2008). On test collections for adaptive information retrieval. Information Processing and Management, 44(6): 1879-1885.
[6] Voorhees, E.M. and Harman, D. (2005). TREC: Experiment and Evaluation in Information Retrieval. MIT Press.

768

Predicting Escalations of Medical Queries Based on Web Page Structure and Content

Ryen W. White and Eric Horvitz
Microsoft Research One Microsoft Way, Redmond, WA 98052 USA
{ryenw, horvitz}@microsoft.com

ABSTRACT
Logs of users' searches on Web health topics can exhibit signs of escalation of medical concerns, where initial queries about common symptoms are followed by queries about serious, rare illnesses. We present an effort to predict such escalations based on the structure and content of pages encountered during medical search sessions. We construct and then characterize the performance of classifiers that predict whether an escalation will occur after the access of a page. Our findings have implications for ranking algorithms and the design of search interfaces.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process, selection process
General Terms
Experimentation, Human Factors
Keywords
Medical search, cyberchondria
1. INTRODUCTION
Web search engines are used frequently by consumers to access health information [4]. Although such retrieved information can be invaluable, problems may arise when Web search is relied upon for diagnosis, where queries describing symptoms are input and rank and results are interpreted as diagnostic conclusions. Prior research identified search and browsing sessions where queries on symptoms associated largely with common, benign explanations can lead to searches on more serious, rare ailments [7]. Such escalations may be based in several factors, including the relatively large quantity of Web content describing serious illnesses versus benign explanations, and the potential use of ranking algorithms based on clickthrough data. Presenting people with troubling health scenarios without information on the probabilities of these outcomes, or on the typically greater likelihoods of more common explanations, can heighten concerns inappropriately [6].
Previous studies have explored the effects of factors such as page design and content on the perceptions of health information seekers [3][5] and on actions such as self-diagnosis and self-treatment [1], and have showed that, while 80% of American adults have searched for healthcare information online, 75% refrain from verifying key quality indicators such as source validity and source creation date [4]. We focus here on predicting escalations in queries based on the structure and content of Web pages. We specifically build classifiers to predict whether an escalation will occur immediately following a page visit. Accurate predictions of escalations in medical concerns given page features allows search systems to flag specific pages as potential sources of inappropriate anxiety or down-weight such pages in their result ranking.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. IDENTIFYING QUERY ESCALATIONS
We studied anonymized logs of URLs visited by users who opted in to provide data through a widely distributed browser toolbar. Log entries include a user identifier, a timestamp for each page view, and the URL of the page visited. Intranet and secure (https) URL visits were excluded at source. Only entries generated in the English speaking United States locale were included. From these logs, we mined many thousands of search sessions. Sessions start with a search engine query followed by a click on a search engine result. We consider sessions to be a sequence of time-ordered URLs terminating after 30 minutes of user inactivity (as in [7]).
We define query escalations as observed increases in the medical severity of search terms used within single health-related search sessions. For the purpose of the study, we examined the query terminology for sessions containing queries on six common symptoms: headache, chest pain, muscle twitches, abdominal pain, nausea, and dizziness. We identified cases where users transition from a query about one of these basic symptoms to queries on a related, but serious illness within a search session. We focus on two outcomes following a query about a basic symptom:
1. Next-query escalation: Following visits to pages after a query, an escalation occurs on the next follow-up query. 2. Any-query escalation: Following visits to pages after a query, an escalation occurs on any follow-up query within a session.
Lists of terms for expressing the six basic symptoms and terms for expressing serious illnesses for each of the symptoms were constructed through review of the medical literature as well as the browsing of portions of the search logs by the authors, one of whom received medical training during a PhD/MD program (EH). Lookup into these lists was used to identify candidate sessions and escalations in the queries present in session logs.
Predicting escalations requires identifying non-escalations as well as escalations. Non-escalations are defined in several ways:
1. Follow-up query is benign as explicitly defined. 2. Follow-up query not associated with defined escalation or non-
escalation (hereafter called an undefined non-escalation). 3. Session terminates without escalation.
Figure 1 illustrates these definitions graphically, with  as queries, boxes as pages, and  as the page from which features would be generated in each case. In our study, we experiment with the prediction accuracy for each of these definitions of non-escalations.
3. PREDICTING ESCALATIONS
We constructed classifiers to predict whether a user's next query following a page visit contains an escalation or a non-escalation based on features of that page. We built the models using logistic regression, considering the outcome for each identified session and a set of over 40 features representing attributes of the structure and content of pages. Space limitations preclude detailing the entire feature list. However, in summary the features used include:

769

Table 1. Percentage of escalations versus non-escalations given order of serious and benign conditions on the preceding page.

Order of presentation

Query outcome Escalation
Non-escalation

Serious first 68.6% 31.4%

Benign first 33.4% 66.6%

Figure 1. Escalations and non-escalations in search sessions.
structural features (e.g., serious illness precedes benign explanation, number of words between serious illness and benign explanation, modifiers such as "unlikely to be" appear near serious illnesses), title and URL features (e.g., title has serious illness, is forum URL), first-person testimonials (e.g., page has phrases such as "told me", "I felt", and "I am worried"), page reliability features (e.g., page is externally verified by healthonnet.org, recommends consulting physician), commercial intent features (e.g., page has advertisements, number of advertisement blocks), and general page features (e.g., length in words, total kilobytes).
We used a set of six thousand Web pages sampled randomly from our data. In initial analyses, we separately probed the predictive value of single features. For example, we considered the influence on escalation of the order in which serious illnesses versus benign explanations appeared on the pages viewed (for pages containing both serious illnesses and benign explanations). Table 1 shows the boost in escalations associated with viewing pages that relay information about serious disorders before discussing benign explanations as compared with viewing pages with the reverse ordering. One explanation for this phenomenon is that users may not read the full page (as supported by eye-tracking research [2]).
Each page is associated with an escalation or non-escalation in the query immediately following it. We selected an equal number of escalations and non-escalations (such that the accuracy of a marginal model that predicted escalation was 50%), and used fivefold cross-validation. Prediction accuracy varied with definitions of escalation and non-escalation. The highest accuracy was obtained for next-query escalations and defined non-escalations (73.4%), followed by next query escalation where non-escalation is session end (70.7%), and next-query escalation where the escalation is undefined (68.8%), and finally any-query escalation (65.5%). Performance differences were statistically significant (at  < .01) between all methods across 100 runs. Figure 2 shows the receiver-operator characteristic curves for one of the runs.
Our findings show that we can generate potentially valuable predictions about escalations using page features, and that accuracy varies with the definition of escalations and non-escalations. Best performance is obtained when non-escalations come from a defined set of benign explanations with no queries between concern and escalation. The most predictive features for all models are:

Figure 2. Receiver-operator characteristic curves.
serious illness precedes benign explanation in page, serious illness vs. benign explanation appears in page title or near beginning of page, page from Web forum, and page has external verification.
4. SUMMARY
We studied the prediction of escalations of queries for health information on the Web based on features of accessed pages. The results have implications for the design of search engines that may use escalation likelihood as a ranking feature or present this information to users. Future directions include the leveraging of the richer rhetorical structure of page content to introduce additional structural features. We also hope to investigate the predisposition to escalate based on features of users, examine cumulative effects of multiple session pages, and study the influence of contents displayed on search results pages.
REFERENCES
[1] Bengeri M. & Pluye P. (2003). Shortcomings of health-related information on the internet. Health Prom Int., 18(4): 381-387.
[2] Buscher, G., Cutrell, E. & Morris, M.R. (2009). What do you see when you're surfing? Using eye tracking to predict salient regions of web pages. SIGCHI, 21-30.
[3] Eysenbach, G. & Köhler, C. (2002). How do consumers search for and appraise health information on the world wide web? British Medical Journal 324: 573-577.
[4] Fox, S. (2006). Online health search 2006. Pew Internet and American Life Project. Accessed January 2009.
[5] Sillence, E., Briggs, P., Fishwick, L. & Harris, P. (2004). Trust and mistrust of online health sites. SIGCHI, 663-670.
[6] Tversky, A. & Kahneman, D. (1974). Judgment under uncertainty: heuristics and biases. Science, 185(4157): 1124-1131.
[7] White, R.W. & Horvitz, E. (2009). Cyberchondria: Studies of the escalation of medical concerns in web search. TOIS, 23(4).

770

Contextual Video Advertising System Using Scene Information Inferred from Video Scripts

Bong-Jun Yi, Jung-Tae Lee, Hyun-Wook Woo, and Hae-Chang Rim
Dept. of Computer & Radio Comms. Engineering, Korea University Seoul, 136-713, South Korea
{bjyi,jtlee,hwwoo,rim}@nlp.korea.ac.kr

ABSTRACT
With the rise of digital video consumptions, contextual video advertising demands have been increasing in recent years. This paper presents a novel video advertising system that selects relevant text ads for a given video scene by automatically identifying the situation of the scene. The situation information of video scenes is inferred from available video scripts. Experimental results show that the use of the situation information enhances the accuracy of ad retrieval for video scenes. The proposed system represents one of the pioneer video advertising systems using contextual information obtained from video scripts.
Categories and Subject Descriptors
H.3.5 [Information Storage and Retrieval]: Online Information Services--Commercial services
General Terms
Algorithms, Experimentation
Keywords
contextual video advertising, scene, script, situation
1. INTRODUCTION
Over the past years, demands to serve ads on digital videos based on the content displayed to the user have been consistently increasing with the growing number of various digital devices and services. However, it is difficult to get direct access to the content of the video with existing visual analysis and speech recognition techniques, because of not only the enormous computational cost they require but their low performance. Other video metadata, such as title, summary, or genre, provides very limited information for dynamically analyzing individual scenes within the video. A reasonable approach would be to utilize available video scripts that contain descriptions of scenes as well as dialogues. This approach is also practically applicable today, because it is easy to obtain such data on the Web especially for popular TV shows or movie contents.
In this paper, we demonstrate the implementation of an intelligent contextual video advertising system that is able to select ads for individual scenes in video contents with the
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Table 1: Situation categories (in no specific order). Work, Shopping, Meal, Interaction, Leisure, Travel, Accident, Health, Infant care, Beauty care, Finance, Love, Marriage, Conflict, Religious activity, Home living, Telephone conversation, Transportation, Study, Miscellaneous
use of their scripts. A main difference of our system against a typical contextual advertising system is that, instead of scanning a given text (a scene script in this case) for bid advertising keywords right away, the system examines the situation of individual scenes using descriptions and dialogues in scripts to ensure contextually relevant advertising. For example, if the user is viewing a sports game scene, the user may expect to see ads for sports related topics, such as sportswear or sports equipments. We adopt a learning-based approach to classify individual scenes into (pre-defined) situation categories. The ads are also automatically located in given categories in order to ensure that they would only appear in the appropriate situation context. To the best of our knowledge, this work is one of the first attempts to match video scenes to text ads based on the situation information of the scenes inferred from video scripts.
2. SITUATION CATEGORIES
As in [1], we have pre-defined a set of situation categories as follows. Category names of products and services were collected from various sources, such as shopping websites. For each category name, four volunteers were asked to suggest general scenes that the category reminds of. For example, a category name "Health" may infer exercise scenes and hospital scenes. All suggested situations were manually flat clustered into 20 different clusters (categories) as shown in Table 1. Note that these categories do not cover all possible situations that could happen in the real world, but they represent the ones that may often provoke advertising.
3. SYSTEM IMPLEMENTATION
Figure 1 illustrates the architecture of the proposed system. A video is given along with its script as the input. The video is first decomposed into a series of individual scenes according to the video script. For each scene, the system extracts advertising keywords and analyzes the situation information from the corresponding scene text in the script. A candidate list of ads is retrieved from the ad index with regard to the extracted keywords using a standard informa-

771

Figure 1: System architecture.
tion retrieval model. Given the candidate list of ads and the situation information inferred from the script, the system finally re-ranks the list and outputs the top n ads that are determined to be most appropriate for each scene.
Preprocessor
The duty of the preprocessor module includes decomposing a given script input as a series of individual scenes and converting each scene text into machine readable format. For the scene decomposition, it uses a collection of hand-crafted rules with several cue expressions that reflect the beginning of a new scene. It also distinguishes dialogues from instructions using pre-defined regular expressions. Some shallow linguistic analyses, such as part-of-speech (POS) tagging, are applied to the text for later use.
Keyword Extractor
The role of this module is to automatically extract advertising keywords from each scene. Given a scene text, the module detects noun words and phrases as keyword candidates. For each keyword candidate, several features suggested in earlier advertising keyword extraction studies [1, 4], such as its frequency features, position features, and external advertising keyword log features, are extracted. The module computes the score of each candidate using a logistic regression model trained in advance with a set of correct and incorrect keyword samples. It finally outputs candidates that receive probability scores higher than a pre-defined threshold value as keyword queries for ad retrieval.
Scene Situation Analyzer
Given a preprocessed scene text, this module takes a learning based text categorization approach to classify the scene into one or more pre-defined situation categories. Because multiple situations can occur in a scene in general, we built a multi-class logistic regression model in advance with a training set of scene-situation pair samples. All situation categories in which the regression model outputs probability scores higher than a manually-tuned threshold value are selected. For features, we use the association measures (calculated using Pointwise Mutual Information) between all words in a scene text and individual situation categories instead of the traditional tf-idf features. This approach is similar to the one used in [3].

Table 2: Retrieval performance.

NDCG Baseline

Proposed

at 1 0.4467 0.4733 ( 5.95%)

at 3 0.4562 0.4787 ( 4.93%)

at 5 0.4566 0.5002 ( 9.55%)

Initial Retriever
Given a set of extracted keywords for each scene, the initial retriever module finds a list of top n candidate ads from the ad index using the Indri retrieval model [2] as the basis of textual relevance.
Situation based Reranker
Given a ranked list of candidate ads and the situation information for each scene, this module re-ranks the initial list so that ads relevant to the situation context of the scene are ranked higher. Because ads are generally not located in pre-defined situation categories, we built a multi-class logistic regression model for ads as for scenes. The regression model uses the same PMI features but is trained with different training data, consisting of ad-situation pair samples. Ad candidates are ranked according to their probability of being classified as the situation category of the given scene.
4. EXPERIMENTS
We compare the quality of the final re-ranked list of the proposed system with the initial retrieval list, which represents the result of conventional advertising that do not consider the situation context of individual scenes. We indexed 414,419 ad texts collected from various web search engines. For scene data, we randomly chose 75 scene texts from the data set used in [1], which consists of 3406 scene texts from the scripts of popular TV drama shows broadcasted in Korea. From each scene text, the system extracted average 3.25 keywords, which were used as queries for that particular scene. The relevance of the top n retrieved ads were manually judged by undergraduate students in 3-scale (2=relevant, 1=somewhat relevant, 0=non-relevant). Since video advertising systems can display only a few ads due to the limited screen size, we measure the Normalized Discounted Cumulative Gain (NDCG) at top k ranks. Table 2 shows the results. It is observed that the proposed system achieves better retrieval performance, which implies a better user experience than conventional advertising.
5. ACKNOWLEDGMENTS
This work was supported by Samsung Electronics.
6. REFERENCES
[1] J.-T. Lee, H. Lee, H.-S. Park, Y.-I. Song, and H.-C. Rim. Finding advertising keywords on video scripts. In Proc. SIGIR '09, pages 686­687, 2009.
[2] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40(5):735­750, 2004.
[3] G. Mishne. Experiments with mood classification in blog posts. In Proc. SIGIR '05 Style Workshop, 2005.
[4] W.-t. Yih, J. Goodman, and V. R. Carvalho. Finding advertising keywords on web pages. In Proc. WWW '06, pages 213­222, 2006.

772

Cross-Language Retrieval Using Link-Based Language Models

Benjamin Roth beroth@coli.uni-saarland.de

Dietrich Klakow dietrich.klakow@lsv.uni-saarland.de

Spoken Language Systems, Saarland University D-66125 Saarbrücken, Germany

ABSTRACT
We propose a cross-language retrieval model that is solely based on Wikipedia as a training corpus. The main contributions of our work are: 1. A translation model based on linked text in Wikipedia and a term weighting method associated with it. 2. A combination scheme to interpolate the link translation model with retrieval based on Latent Dirichlet Allocation. On the CLEF 2000 data we achieve improvement with respect to the best German-English system at the bilingual track (non-significant) and improvement against a baseline based on machine translation (significant).
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--retrieval models
General Terms
Algorithms, Experimentation
Keywords
CLIR, Wikipedia, LDA, language modeling
1. INTRODUCTION
Translation lexica and parallel corpora are often only accessible for some European language pairs. And even if they exist their vocabulary is inherently limited in contrast to an ever growing wide-coverage ressource like Wikipedia, where corresponding articles are connected across languages. Attempts have therefore be made to extract information for CLIR from article-level co-occurrence statistcs, with moderate succes [6]: Coarse thematical relationships alone can arguably not capture the specific meaning contained in a query, a word-to-word translation is necessary. The question therefore arises how word-specific mappings can be obtained from freely available large-scale knowledge sources such as Wikipedia.
Several approaches in this direction have been undertaken. Often, a Wikipedia title in the source language is associated with a word and the corresponding title in the target language is used as a translation [4]. However, [5] note that the vocabulary distribution of titles has a skew to certain words.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Because the titles of Wikipedia articles are specifically tailored to be unique identifiers rather than representative text samples, translations of large classes of words might be problematic when a variation is used. In [2] a bilingual dictionary is extracted from Wikipedia by supervised classification.
The method we use is unsupervised and based on the anchor text of links. In Wikipedia, any text can be linked to any page. For example, the German texts "in W¨aldern gelegte Br¨ande" and "Buschbrand" may be valid contexts to be linked to the article with the English counterpart "Wildfire". Three items of information are necessary to build a probabilistic translation model based on linked text:
· How likely is a text to be linked? · What is a probable target article, given a linked text? · What is a probable anchor text, given a link to a cer-
tain article?

2. LINK TRANSLATION MODEL
If, for the sake of simplicity, a unigram model is used for translation this amounts to the probablities P (l|w), P (a|w, l) and P (w|a, l). In a bilingual setting l is a variable indicating whether the source word is linked, a is the bilingual article the source word is linked to, and wE and wF are words in the source and the target languages respectively. The probability of translating a source word into a target word is:

P (wF |wE ) = P (wF |ltrue, wE )P (ltrue|wE ) +P (wF |lfalse, wE)P (lfalse|wE)
We focus on the linked case. Assuming that the translation of linked source words does only depend on the articles they are linked to, one gets:

P (wF |ltrue, wE) = X P (wF |a, ltrue)P (a|ltrue, wE)
a
We note that the probability P (ltrue|wE) can function as a term weighting in the source documents, assuming that the importance of terms is correlated with their probability of being linked. Translation and linking are assumed to be independent of the document, given a source word. In the following we write D for a source document in language E, l for ltrue and n(w, Q) for the count of w in a query Q.

2.1 The Pure Link Model

In a query likelihood model a document provides a prob-

abilistic model for a query. The ranking is usually done by

log

P

(Q|D)

=

P
wQ

n(w,

Q)

log

P

(w|D).

773

In principle, all components are provided by the link model to perform retrieval in such a setup. The probability that Wikipedia article a is the link target when a linked word is picked from document D (making the same independence assumptions as before) is

P (a|D, l)

=

P
wE

P

(a|l,

wE )P

(l|wE )P

(wE |D)

P
a ,wE

P

(a|l,

wE )P

(l|wE )P

(wE |D)

and P (wF |a, l) is the probability that, given a source word is linked to article a, it is translated to word wF . Together the elements of the link model provide us with the distribution:

X Plink(wF |D, l) = P (wF |a, l)P (a|D, l)
a
It is practical to think of this distribution as combined in that way, because it separates the per-document estimates from the vocabulary estimates. The atomic probabilities are estimated from relative frequencies: P (a|l, wE), P (l|wE) and P (wF |a, l) from Wikipedia, P (wE|D) from the current document.
This formulation poses three problems: The zero-probability problem: Because the components of the model are estimated from relative frequencies, to many events zero probability is assigned. The summation problem: If the probability distributions are smoothed and are never zero, summation might for every word go over all Wikipedia articles, which would be prohibitively expensive to compute. The training basis problem: The model only considers words likely to be linked. Especially high frequency or function words could have skewed distributions.
The model is hence not immediately applicable. We tackle these problems by interpolating the link model with a language model based on LDA and by considering the probability of being linked for the query term weights.

2.2 Model Combination on Word Level
One possible combination scheme of LDA and link model is to interpolate word distributions given a document. We use Wikipedia as a bilingual training corpus for LDA by cutting articles at 100 words, discarding shorter ones and concatenating both language sides. We trained models with 125, 250, 500 and 1000 topics (parameterized as suggested in [3]) and interpolated them with equal weight to avoid local maxima. After inference on the retrieval collection, one has:

PLDA(w|D) = X P (w|z)P (z|D)
z
Having the word probabilities, the question arises how to weight the query word counts, as a weighting according to their probability of being linked seems reasonable for the link model, but is not justified for LDA. Hence, we use two model parameters  and  to interpolate weightings and distributions respectively. The query log-likelihood becomes:

log P,(Q|D) = X n(w, Q) [P (l|w) + (1 - )]
wQ
· log [Plink(w|D, l) + (1 - )PLDA(w|D)]
The LDA-distributions are smooth by defininition, so for 0   < 1 there is no zero-probability problem. For efficiency reasons, we did not smooth the link component and summed only over the 1000 most probable articles per document. In an ad-hoc parametrization we let both weightings and both

models contribute equally strongly. The probability of a word being linked is very low with p(l) = .06, to get equal influence of both weightings we require ·p(l) = 1-, which results in  = .94,  is set to .5.
We evaluated on the German-English CLEF 2000 bilingual track1 (title+description) and achieve map= .291 which is better than any of those reported for the same language pair [1], this difference is however statistically not significant. The model is significantly (p < 0.05, paired t-test) better than a base-line using Moses machine translation trained on Europarl with tf.idf vector retrieval.

Table 1: Results on German-English CLEF2000

method

map gmap

LDA only

.074 .002

Moses + tf.idf

.203 .061

Best system CLEF2000 .267

­

LDA + links

.291 .107

3. CONCLUSION
We have introduced a CLIR method that is based solely on information extracted from Wikipedia. It combines documentlevel information (captured by LDA) and word-specific information (captured by a link model) in a clear language modeling setup. There is much room for the exploration of different smoothing and combination schemes. We tried a simple one on word level. This model did not only outperform a base-line obtained with Moses machine translation, it also produced results that compare favorably against values reported for the CLEF 2000 bilingual track.
4. REFERENCES
[1] M. Braschler. CLEF 2000-Overview of results. In Cross-language information retrieval and evaluation: workshop of the Cross-Language Evaluation Forum. Springer Verlag, 2001.
[2] M. Erdmann, K. Nakayama, T. Hara, and S. Nishio. Improving the extraction of bilingual terminology from Wikipedia. ACM Transactions on Multimedia Computing, Communications, and Applications, 2009.
[3] T. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(90001):5228­5235, 2004.
[4] D. Nguyen, A. Overwijk, C. Hauff, R. Trieschnigg, D. Hiemstra, and F. de Jong. WikiTranslate: Query translation for cross-lingual information retrieval using only Wikipedia. In 9th Workshop of the Cross-Language Evaluation Forum, 2008.
[5] J. Sjobergh, O. Sjobergh, and K. Araki. What types of translations hide in wikipedia? Lecture Notes in Computer Science, 4938:59, 2008.
[6] P. Sorg and P. Cimiano. Cross-lingual information retrieval with explicit semantic analysis. In Working Notes of the Annual CLEF Meeting, 2008.

1ELRA catalogue (http://catalog.elra.info), The CLEF Test Suite for the CLEF 2000-2003 Campaigns, catalogue reference: ELRA-E0008; All data was processed using the Snowball stemmer and stopword list.

774

Search System Requirements of Patent Analysts

Leif Azzopardi, Wim Vanderbauwhede
Department of Computing Science University of Glasgow, United Kingdom
{leif,wim}@dcs.gla.ac.uk

Hideo Joho
Graduate School of Library, Information and Media Studies University of Tsukuba, Japan
hideo@slis.tsukuba.ac.jp

ABSTRACT
Patent search tasks are difficult and challenging, often requiring expert patent analysts to spend hours, even days, sourcing relevant information. To aid them in this process, analysts use Information Retrieval systems and tools to cope with their retrieval tasks. With the growing interest in patent search, it is important to determine their requirements and expectations of the tools and systems that they employ. In this poster, we report a subset of the findings of a survey of patent analysts conducted to elicit their search requirements.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval
General Terms: Human Factors
Keywords: User Study, Patent Engineers, Patent Analysts
1. INTRODUCTION
With new initiatives providing patent test collections and a spate of workshops and symposiums on patent retrieval1, there has been renewed interest in researching and developing Information Retrieval (IR) tools, techniques and theory for patent search. Patent analysts perform a number of difficult and challenging search tasks (such as Novelty search or Infringement search) [2] and rely upon sophisticated search functionality, tools, and specialised products [1]. These search tasks are often performed under stringent conditions (esp. regulatory and legal requirements) [2], and they also require different search strategies to achieve the end goal (which in some cases means not actually finding documents, i.e. no "kill" document, for instance) [3]. Whilst there has been substantial research on patent search and the tasks and tools involved, little work has been performed investigating the requirements of patent searchers, and what they want. It is vital that users are consulted and their needs understood. This is to ensure that the ongoing research and development meets their current requirements, identifies new requirements, highlights areas of potential opportunity (i.e. what advancements are needed to improve patent search techniques) and identifies constraints. To this aim, we surveyed over eighty patent analysts in order to obtain a better picture of their search habits, preferences, and the types of
1New initiatives include CLEF-IP, TREC-CHEM, MAREC, PaIR, etc. while past initiatives have been run at NTCIR and TREC.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

functionality that they want/need, along with how they go about accomplishing their search tasks. In this poster paper, we present a subset of the findings from this survey, where we focus on reporting the results about the search tasks and the functionality that patent searchers desire and require.
2. METHOD AND RESULTS
The survey instrument2 that we used consisted of an online questionnaire with 86 questions consisting of four parts: (i) demographics and expertise (ii) search tasks (iii) search functionality and (iv) open ended questions on search system requirements. The survey was designed to be completed in approximately 30 minutes.
To obtain a large and representative sample we sent the survey instrument out to two patent user group mailing lists: (i) the Confederacy of European Patent Information User Groups (CEPIUG) and (ii) the international Patent Information Users Group (PIUG). In total, these lists have over 700 members from over 27 different countries, and of these members, approximately 300 are patent information professional who regularly perform patent searching. We received 81 responses in total to the survey ­ a response rate of just over 10% overall. Of the 81 respondents, 58% were male, while 42% were female. Respondents were from 14 different countries including France, Netherlands, UK, US, other european countries and China. The majority were aged between 39-52 (44%) and had about 10 years experience in patent searching. Almost all respondents primarily searched in English (98%), with a few also using German, French and Dutch (less than 15% in total).
In this poster, we report only on a subset of questions regarding search task frequency and querying functionality requirements. These represent the current demands and requirements that analysts have when using an IR system. It is important to appreciate these needs to ensure that research is relevant and targeted to meet them. Firstly, it should be noted that respondents spend approximately 17 hours searching (i.e. searching takes place over several days), and they submit about 27 queries per search task on average. The amount of time they spent formulating queries was about 18 minutes per query. This shows that analysts are willing to spend a lot of time and effort searching and formulating their needs. Table 1 presents the results from a series of questions regarding how often an analyst performs the different types of search tasks (a description of the different search tasks is provided in the table, see [2] for
2Ethics approval from the University of Glasgow was obtained to conduct this survey (Reference code: ETHICS-FIMS00638).

775

Search Task State of the Art
Novelty
Patentability Infringement
Opposition
Freedom to Operate Due Diligence Other

Description identify patents for the purposes of a general review (aka landscaping) identify patents and non-patents which may affect the patentability of an idea/invention (performed before writing a patent application) given a patent application, ensure novelty identify patents or applications which cover the proposed product or process and are still in force identify literature available to the public to show lack of novelty or inventive step of a granted patent like infringement, but also includes non-patent literature analyze strengths, weaknesses and scope of IP rights.

Often 22.2
37.0
34.6 21.0
13.6
46.9 16.1 27.2

Sometimes 33.3
40.7
39.5 23.5
17.3
25.7 19.8 27.2

Rarely 44.5
22.3
25.9 55.5
69.1
28.4 64.2 45.7

Table 1: How often search tasks are undertaken by respondents (values shown are percentages).

more details). The category headings are: often (daily or weekly), sometimes (monthly), and rarely (once or twice a year, or never) and the value is the percentage of respondents. The results indicate that Novelty, Patentability and Freedom To Operate searches are the most frequent types of search tasks which are routinely performed, given our respondents. It is notable that these tasks require the analyst to search both patents and non-patents collections. By contrast, Infringement, Opposition and Due Diligence search tasks were rarely or never performed by 55-69% of the respondents. These findings suggest that a lot of effort is focused on ensuring that infringements or challenges are not made by ensuring that patent applications are novel and patentable while products are appropriately licensed.
For any given search task they perform, analysts were asked about the importance of a variety of operations they use when querying the retrieval system. Table 2 presents a summary of these results. Participants were asked to rate statements such as "Boolean operators are important to formulate effective queries" on a five point scale (Strongly Agree to Strongly Disagree). Here, we have compressed the scale to Disagree, Neutral and Agree. The results show that Boolean operators were very important to almost all respondents. Proximity, Truncation, Wildcards and Field Operators were important to most respondents (80-91%), while Expansion and Translation were seen as important by around half the respondents with a large proportion of respondents impartial to these features. This may be because they have not used such functionality frequently enough to form an opinion, or because such functionality is not required as most respondents searched primarily in English.
Finally, the Weighting of terms in the query obtained a very mixed response; where most respondents are impartial, while the rest were split between important and not important. This may be because of the difficulties associated with weighting query terms manually. It appears that the features that introduce some uncertainty in the process (i.e. Weighting, Expansion and Translation) are not considered to be as important as the other very precise operators, which can be controlled and have a clear interpretation. This is perhaps due to the fact that analysts are often required to fulfil strict and stringent practices given the legal and regulatory requirements [2].
These findings suggest that research and development of models, methods and systems for patent search need to consider the user requirements identified as important. It

would appear that patent searchers prefer search functionality which provides a high degree of control and precision for accomplishing their search tasks, and they are willing to spend a lot of time and effort in constructing requests and examining documents.

Question Boolean operators Proximity, Adjacency or Distance operators Weighting Truncation (left/right) Wildcards Field Operators Query Expansion Query Translation

Disagr. 2.5 4.9
22.2 1.2 6.2 3.7 2.5 11.1

Neut. 1.2 11.1
48.2 7.4 13.6 11.1 42.0 43.2

Agree 96.3 84.0
29.6 91.4 80.3 85.2 55.6 45.7

Table 2: Survey Questions: is the feature important to formulate effective queries, and responses.
3. SUMMARY AND FUTURE WORK
In this poster, we have provided empirical evidence which suggests that the requirements of patent analysts are quite different from those assumed in standard retrieval tasks (like web search, etc). In particular, our results indicate a strong preference towards functionality that gives fine grained control over the search process using operators with a clear semantic and precise interpretation. These findings motivate further research into understanding more deeply the importance of each type of functionality given the different patent search tasks along with ascertaining a better understanding of the context in which patent search is undertaken, in order to meet searcher needs and requirements.
Acknowledgments: We would like to thank the CEPIUG and PIUG user groups and the participants who took part in the survey. This work was supported by the Information Retrieval Facility (http://www.ir-facility.org).
4. REFERENCES
[1] K. H. Atkinson. Toward a more rational patent search paradigm. In Proceeding of PaIR '08:, pages 37­40. ACM CIKM, 2008.
[2] D. Hunt, L. Nguyen, and M. Rodgers. Patent Searching: Tools and Techniques. John Wiley and Sons, 2007.
[3] Y.-H. Tseng and Y.-J. Wu. A study of search tactics for patentability search: a case study on patent engineers. In Proceeding of PaIR '08:, pages 33­36. ACM CIKM, 2008.

776

Focused Access to Sparsely and Densely Relevant Documents

Paavo Arvola
Dept. of Information Studies and Interactive Media
University of Tampere, Finland paavo.arvola@uta.fi

Jaana Kekäläinen
Dept. of Information Studies and Interactive Media
University of Tampere, Finland jaana.kekalainen@uta.fi

Marko Junkkari
Dept. Of Computer Science University of Tampere, Finland
marko.junkkari@cs.uta.fi

ABSTRACT
XML retrieval provides a focused access to the relevant content of documents. However, in evaluation, full document retrieval has appeared competitive to focused XML retrieval. We analyze the density of relevance in documents, and show that in sparsely relevant documents focused retrieval performs better, whereas in densely relevant documents the performance of focused and document retrieval is equal.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process
General Terms
Measurement, Performance, Experimentation.
Keywords
XML retrieval, tolerance to irrelevance, focused retrieval.
1. FOCUSED ACCESS TO A DOCUMENT
Ideal information retrieval (IR) systems would return only relevant information to the user. In traditional document retrieval, returned documents typically include both relevant and nonrelevant content. Approaches like passage and XML retrieval aim at returning the relevant content more accurately: the user should be guided directly to the relevant content inside the document instead of having to browse through the whole document. Surprisingly, in recent studies document retrieval has been found a competitive approach to focused XML retrieval according to retrieval effectiveness [e.g. 7]. However, some essential features in focused access to a document have been overlooked: the order of browsing, the user's reluctance to browse non-relevant information and the proportion of relevant text in documents. In the present study this proportion is referred to as the density of relevance of the document.
An XML retrieval system provides retrieved and assumedly relevant passages first to the user. If a returned passage turns out to be non-relevant, the user will not necessarily browse it through but rather continues with the next result. This user behavior is combined to effectiveness evaluation in the tolerance to irrelevance (T2I) metric [4], which models the user interrupting to browse after a given amount of non-relevant information is encountered. The sooner T2I is reached, the less the document benefits the effectiveness in evaluation.
In this study, we follow a browsing model with the given

assumptions: A focused retrieval system guides a user to the relevant content, and the user starts browsing the document from the passages indicated by the system [1,2]. Returned passages are browsed first and the browsing continues until T2I is reached. With this model, effectiveness measures like precision and recall can be calculated for the document. These measures are calculated based on the proportion of relevant text browsed at the point where T2I is reached.
We compare focused XML retrieval with document retrieval by taking into account the access point to the document, browsing order and T2I. We analyze the effectiveness of retrieval at the level of a retrieved relevant document. More specifically, we examine the effectiveness by the density of relevance in documents. Our hypothesis is that focused retrieval provides a more effective access to the relevant content of a relevant document than full document retrieval, especially when it comes to sparsely relevant documents.
2. DENSITY AND DISTRIBUTION OF RELEVANCE
As the test collection we use the frozen Wikipedia collection [3] of more than 650,000 documents covering various subjects. The collection is used with topics and relevance assessments of the INEX 2008 initiative [6], where relevant passages (in 4,887 relevant documents) for each topic (totally 70) are assessed. All the relevant documents are sorted according to their ratio of relevant text to all text, i.e. how many percent of the document's text is relevant. Then the sorted list of documents is split into deciles, each covering 10% of the documents. The (rounded) lower boundaries of the density (relevance ratio) for the deciles are 0.005%, 2.4%, 6.6%, 12.1%, 24.4%, 58.4%, 94.9%, 99.3%, 99.7% and 99.9% (dec 1, dec 2,..., dec 10 respectively). That is, the last 4 deciles i.e. 40% of the relevant documents have a very high relevance density. Obviously, focused access to those documents does not bring any improvements.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07

Figure 1: Average distribution of document's relevant text on five smallest deciles

781

recall precision browsed text %

elem, 500 doc, 500

a

1

elem, 2000

0.9

doc, 2000

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

decile

elem, 500

b

1

doc, 500

0.9

elem, 2000

0.8

doc, 2000

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

decile

elem, 500

c

100

doc, 500

90

elem, 2000

doc, 2000

80

70

60

50

40

30

20

10

0

1

2

3

4

5

6

7

8

9

10

decile

Figure 2: Average Recall (a), Precision (b), % browsed content (c) of relevant documents on each decile

Figure 1 shows the average precision of the five smallest deciles at percentages of the relevant documents' running text. The relevant content in the lowest deciles is somewhat steady across the average document, which means the relevant content may be at any location, whereas the fourth and fifth decile shows a slight bias towards the beginning of a document. The remaining deciles especially from 7 upwards draw a high, relatively straight line and are left out for the readability of the lowest curves.
3. PRECISION AND RECALL WITHIN A DOCUMENT
To study the benefit of focused retrieval strategy for the retrieval within a document on each decile, we selected the retrieved passages of each relevant document. These passages were provided by the best performing run at INEX 2008 (RiCBest, University of Waterloo [5]). Then we compared these focused results with a document retrieval baseline, where each relevant document is browsed sequentially. Figure 2 shows the average recall, precision and the percentage of browsed content in the relevant documents for each decile. The elem column refers to the focused retrieval strategy (i.e. RiCBest) while the doc column refers to the document retrieval baseline. We report figures on two T2I points: 500 and 2000 characters. In other words the browsing is expected to end when the user has bypassed 500 or 2000 nonrelevant characters. The amount of 500 characters corresponds approximately to the next paragraph.
Figure 2c shows that the amount of browsed content is about the same for both of the strategies when assuming the same T2I. That is, the focused retrieval strategy does not reduce the amount of browsed content. However, with that amount the precision (Figure 2b) and especially the recall (Figure 2a) of the browsed content are notably higher with the focused strategy for half of the relevant deciles (dec1-5). The documents after sixth decile are uninteresting since they are densely relevant and neither browsing order matters nor T2I is reached.
4. DISCUSSION AND CONCLUSIONS
Focused retrieval is beneficial in locating the relevant content in between non-relevant material. Therefore documents with high relevance density are not interesting in the scope of focused retrieval. The results show that the less relevant content, the better the focused retrieval performs. In plain document retrieval, the user is responsible for finding the relevant content within a sparsely relevant document. This leads into poor performance

with documents having only some relevant content, when T2I is assumed. Namely, in many cases the browsing ends before the relevant content is met. This leads to zero recall. On the other hand, if the relevant content is pointed out accurately, the recall for the document is typically 1 (100%). However, due to the nature of T2I, where the browsing goes on with the non-relevant material until the T2I is met the precision is always less than 1.
While we assume the T2I and browsing order in focused retrieval, our findings differ from the previous studies, where the full document retrieval has been a competitive approach [7]. This is due to the densely relevant documents, where the focused retrieval systems tend to retrieve only parts for why the recall per document remains low. This has led into overemphasizing precision, which is taken into account four times more than recall in the official metrics (i.e. the F-Measure).
5. ACKNOWLEDGEMENTS
The study was supported by the Academy of Finland under grants #115480 and #130482.
6. REFERENCES
[1] Arvola, P. 2008. Passage Retrieval Evaluation Based on Intended Reading Order. LWA 2008, 91-94.
[2] Arvola, P., Kekäläinen, J., Junkkari, M. 2010. Expected Reading Effort in Focused Retrieval Evaluation. To appear in Information Retrieval.
[3] Denoyer, L., and Gallinari, P. 2006. The Wikipedia XML Corpus. Sigir Forum, 40:64-69.
[4] de Vries, A.P., Kazai, G., and Lalmas, M. 2004. Tolerance to irrelevance: A user-effort oriented evaluation of retrieval systems without predefined retrieval unit. In Proceedings of RIAO 2004, 463-473.
[5] Itakura, K.Y., and Clarke C.L.A. 2009. University of Waterloo at INEX 2008: Adhoc, Book, and Link-the-Wiki Tracks, In Advances in Focused Retrieval, 132-139.
[6] Kamps, J., Geva, S., Trotman, A., Woodley, A., and Koolen, M. 2009. Overview of the INEX 2008 ad hoc track. In Advances in Focused Retrieval, 1-28.
[7] Kamps, J., Koolen, M., and Lalmas, M. 2008. Locating relevant text within XML documents. In Proceedings SIGIR '08. ACM, New York, NY, 847-848.

782

Text Document Clustering with Metric Learning

Jinlong Wang, Shunyao Wu
School of Computer Engineering Qingdao Technological University
Qingdao, 266033, China
{wangjinlong, shunyaowu}@gmail.com

Huy Quan Vu, Gang Li
School of Information Technology Deakin University
Victoria 3125, Australia
{hqv, gang.li}@deakin.edu.au

ABSTRACT
One reason for semi-supervised clustering fail to deliver satisfactory performance in document clustering is that the transformed optimization problem could have many candidate solutions, but existing methods provide no mechanism to select a suitable one from all those candidates. This paper alleviates this problem by posing the same task as a soft-constrained optimization problem, and introduces the salient degree measure as an information guide to control the searching of an optimal solution. Experimental results show the effectiveness of the proposed method in the improvement of the performance, especially when the amount of priori domain knowledge is limited.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering
General Terms
Algorithm, Experimentation
Keywords
Document Clustering, Metric Learning
1. INTRODUCTION
As one of the most fundamental data mining tasks, clustering is a subjective process in nature: different users may want different clusterings when exploring the same data set. However, specifying an appropriate similarity measure in advance is usually difficult for general users. Recently, semisupervised clustering which can utilize priori pair-wise constraints has attracted a lot of research interest [5].
For the topic of document clustering, there have been some pioneer work in applying semi-supervised clustering to increase clustering quality [3]. However, their performance is still not as good as expected, especially when the number of priori constraints is limited [2, 3]. One of the possible reasons is that: existing work transforms the clustering into an optimization problem, with priori constraints as hard constraints. When the number of constraints is inadequate, there could be many candidate solutions to this optimization problem, and existing methods fail to provide a mechanism to select a suitable one from these possible solutions.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Moreover, when the priori knowledge contains some inconsistent constraints, existing methods might even not be able to generate a feasible result because no solution can satisfy all constraints.
In order to alleviate this problem so that the document clustering can work more effectively with inadequate priori constraints, we propose a novel soft-constraint algorithm for document clustering: instead of satisfying ALL constraints, the method aims to satisfy constraints as much as possible. The proportion of satisfied constraints is adopted as a heuristic to inform the search of the optimal solution. Experimental results show the effectiveness of the proposed method, especially when the number of priori constraints are limited.

2. CLUSTERING DOCUMENT WITH MET-

RIC LEARNING

Let    stands for the data space which contains 

data points {x1,    , xn}, x = [1,    , ] . Let w = [1,    , ] , and  are the weights for attributes ( =

1, ..., ). For document clustering, we use the weighted 

similarity and set


=1

  

and

w(x, xw

y)=

1-

x,yw xw yw

,

= x, xw. With

where x, yw = a must-link pair-

wise constraints set  and a cannot-link pair-wise constraints

set , the document clustering problem can be transformed

into an optimization problem with the objective as [5]:



w

w(x, y)

(x,y)

 subject to (x,y) w(x, y)   and w  0.
Considering the fact that any  similarity is within the range [0, 1], set (x,y) w(x, y)   ( stands

for the number of elements in the cannot-link set ) in the

algorithm which enforces that every pair in the cannot-link

set  is exactly 1, with an aim to make sure distances of

instances in the cannot-link set  as large as possible.

During the search for an optimal set of weights, we in-

troduce a new measure, the salient degree, to evaluate how

the current clustering result respects the priori constraints.

We utilize k -means method with metric w(., .) to parti-

tion data, and get salient degree of clustering result. The

centroids in clustering process of k -means are estimated as

{ }=1 [1].



=



 x x x  x w

,

where,



represents points

assigned to the cluster . The salient degree is defined as the

proportion of satisifed constraints by the clustering result

783

with

parameter

w,

namely

(w)

=

()+(  + 

)

,

here () means the satisfied constraints in the set .

Accordingly, the document clustering can be carried out

as an optimization problem, but with the salient degree as

a heuristic to decide whether it is necessary to carry out

further gradient descending search or not. The pseudo-code

of this algorithm is provided in Algorithm 1.

Algorithm 1: Document clustering with metric learning

Input: Dataset  , number of output clusters , mustlink constraints , cannot-link constraints .

Output: Clusters obtained with metric learning.

 = 1;

w

=

[

1 

,

...,

1 

];

while not convergent do

 = 1;

w = w;

Step 1: Select initial cluster centroids;

Randomly select cluster centroids, and make sure

(w+1) - (w) > 0,

where, w+1 = w +  (w).

Step 2: Iteration for optimization;

while the value of objective function decreases and

(w+1) - (w) > 0 do

w+1 = w +   (w);

 =  + 1;

end  =  + 1; w = w;

end

3. EXPERIMENTS AND RESULTS
Here we compare the performance of the proposed method with k -means and the hard-constraint algorithm COP-Kmeans implemented according to [4]. The 20Newsgroup dataset is used in our experiment.
From original 20Newsgroup dataset, we randomly select 100 documents for each category, and create 2 datasets: the     3 data set (alt.athei-sm, rec.sport.baseball, sci.space) consisting of 3 clusters on 3 distinct topics, and the    3 data set (comp.graphics, comp.os.ms-windows, comp.windows.x) contains 3 clusters with large overlaps between them. All the datasets have been pre-processed by removing stop-words, and words with too high or too low frequency, and each document is then represented by TFIDF.
We run 10 trials of 2-fold cross-validation for each dataset: 50% of the dataset is used as training set to obtain pairwise constraints, and the other half is used as input of compared algorithms after peering off its class/clustering information. The clustering results are then compared with the "ground truth" clustering using Normalized Mutual Information (  ) and   measures.
The results are shown as Figure 1 and Figure 2. On both data sets, we can see that the proposed method outperforms the other methods in both the    and the   measures. Additionally, the results in our method are more stable. Another important observation is that: When the amount of priori knowledge is adequate, our method performs similarly with the compared methods; but when the amount of priori knowledge is limited, our method can still achieve satisfactory clustering results, while the performance of other methods deterioriate significantly.

(a) NMI

(b) Purity

Figure 1: Clustering result on     3

(a) NMI

(b) Purity

Figure 2: Clustering result on    3

4. CONCLUSIONS
This paper proposes an efficient soft-constraint algorithm by obtaining a satisfactory clustering result so that the constraints will be respected as many as possible. Experiments show the advantage of the proposed algorithm especially when provided with little priori domain knowledge, the proposed method is more robust and accurate than the existing methods.
5. ACKNOWLEDGMENTS
This paper was partially supported by the National Natural Science Foundation of P.R.China (No.60802066), the Excellent Young Scientist Foundation of Shandong Province of China under Grant (No.2008BS01009) and the Science and Technology Planning Project of Shandong Provincial Education Department (No.J08LJ22).
6. REFERENCES
[1] S. Basu, M. Bilenko, and R. J. Mooney. A probabilistic framework for semi-supervised clustering. In KDD '04, pages 59­68, 2004.
[2] I. Davidson, K. Wagstaff, and S. Basu. Measuring constraint-set utility for partitional clustering algorithms. In PKDD '06, pages 115­126, 2006.
[3] A. Huang, D. Milne, E. Frank, and I. H. Witten. Clustering documents with active learning using wikipedia. In ICDM '08, pages 839­844, 2008.
[4] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. Constrained k-means clustering with background knowledge. In ICML '01, pages 577­584, 2001.
[5] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell. Distance metric learning with application to clustering with side-information. In NIPS '02, pages 505­512, 2002.

784

Predicting Query Performance on the Web

Niranjan Balasubramanian
University of Massachusetts Amherst 140 Governors Drive, Amherst MA 01003
niranjan@cs.umass.edu

Giridhar Kumaran and Vitor R. Carvalho
Microsoft Corporation One Microsoft Way, Redmond, WA
{giridhar,vitor}@microsoft.com

Predicting the performance of web queries is useful for several applications such as automatic query reformulation and automatic spell correction. In the web environment, accurate performance prediction is challenging because measures such as clarity that work well on homogeneous TREC-like collections, are not as effective and are often expensive to compute. We present Rank-time Performance Prediction (RAPP), an effective and efficient approach for online performance prediction on the web. RAPP uses retrieval scores, and aggregates of the rank-time features used by the documentranking algorithm to train regressors for query performance prediction. On a set of over 12,000 queries sampled from the query logs of a major search engine, RAPP achieves a linear correlation of 0.78 with DCG@5, and 0.52 with NDCG@5. Analysis of prediction accuracy shows that hard queries are easier to identify while easy queries are harder to identify.
Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation, Theory Keywords: Performance prediction, Query difficulty, Web search
1. RANK-TIME PREDICTION
Query performance prediction is the task of estimating the quality of the results retrieved for a query, using effectiveness measures such as normalized discounted cumulative gain (NDCG). Performance prediction is useful for various applications such as detecting queries with no relevant content, performing selective query expansion, and to merge results in a distributed information retrieval system [5]. However, most post-retrieval query performance predictors are expensive to compute, and not well-suited for the web.
The key idea behind RAPP is to use retrieval scores and features that are available to the retrieval algorithm during ranking. Our choice of features is based on two observations. First, the retrieval scores of the top-ranked documents are good indicators of document relevance and therefore are good estimators of retrieval effectiveness. Retrieval score-based features have previously been shown to be effective for classifying TREC queries as easy or hard [4]. Second, Web search engines use retrieval algorithms that combine query dependent and query-independent document features that are designed to capture relevance. The performance of a query is intimately related to these feature values. Since retrieval scores for different queries may not be directly comparable, we use statistical aggregates of the scores. Moreover, statistical aggregates such as maximum, mean, and standard deviation cap-
This work was done while the author was at Microsoft Corporation.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Top K Results

Doc 1 f1,f2,...,fn
...

Score 1
...

Doc k f1,f2,...,fn

Score k

Aggregator

Aggregated Features
a11,a12,...,a1n a21,a22,...,a2n
... al1,al2,...,aln

Regressor

Retrieval Algorithm

User Query
Figure 1: RAPP Overview.

Predicted Performance

ture different aspects of the quality of search results. For example, our initial analysis showed that retrieval scores for low-performing queries tend to have low mean and high variance.
Figure 1 illustrates RAPP. First, we retrieve the top k documents using the retrieval algorithm.We use the retrieval scores as well as the query-dependent and query-independent features of these topranking documents. We then compute statistical aggregates such as mean, maximum, standard deviation, variance, and coefficient of dispersion of these features. Finally, we use these aggregated features and the individual retrieval scores to train a regressor to predict a target performance measure.

2. EXPERIMENTS
To evaluate RAPP, we target the prediction of two performance measures commonly used in web search: DCG@51 and NDCG@5 (referred as DCG and NDCG henceforth). We use a set of 12,185 queries, which were obtained as a frequency-weighted random sample from the query logs of a major web search engine. For retrieval we use LambdaRank [2], an effective learning to rank algorithm for the web. For each query in our collection, we create feature vectors as follows. First, we use LambdaRank to assign scores and rank documents on the Web2. Our implementation uses several retrieval features such as BM25F-based features, click-based features, query length, and other query-independent features such as variants of PageRank. For each of these retrieval features we create statistical aggregates as listed in Section 1. Next, we select the top 100 aggregates (referred to as regression features henceforth) that have the highest linear correlation with the target metric on a set of training queries. Some example features include clickboost_max (maximum value of a click-based feature) and score_stdev (standard deviation of LambdaRank scores). Finally, we create a query performance prediction dataset by associating with each query, the performance metric, DCG@5 or NDCG@5 and the regression features. On this dataset, we conduct 3-fold cross-validation experiments to train linear as well as non-linear regressors based on the Random Forest algorithm [6]3.
Clarity comparison. We use Clarity [3], a competitive performance prediction technique, as an experimental baseline. To compute Clarity for a query, we use a query model built from the top
1Normalized by perfect DCG@5 to scale values to (0,1). 2LambdaRank was trained on an entirely different data set. 3We use the R package implementation with default parameters.

785

50 results returned by the search engine. Because Clarity computation is expensive, we calculated Clarity only for a random subset of 600 queries drawn from our original query set. Table 1 shows the results of performance prediction for DCG and NDCG using Clarity as well as selected features used in RAPP. Clarity achieves very low linear correlation with both DCG and NDCG. When compared to the performance of features used in RAPP, even the lowest performing individual feature outperforms Clarity. This suggests that while Clarity is a competitive measure in smaller TREC collections, it is not a well-suited for the Web.

Table 1: Clarity comparison: Average ­ the average correlation of RAPP features. Best and Worst ­ the highest and the lowest individual correlation of RAPP features on the entire set of 12,185 queries. Clarity correlation is measured on a subset of 600 queries.

Predicted Measure Clarity Average Best Worst

DCG

0.11 0.57 0.70 0.20

NDCG

0.10 0.27 0.50 0.17

Table 2 shows the prediction accuracy for RAPP in terms of linear correlation and root mean squared error (RMSE). Both predicted DCG and NDCG values achieve a high linear correlation and low RMSE. Also, NDCG prediction is much worse as indicated by the low correlation and the higher RMSE values. This is mainly because NDCG is a non-linear metric that is calculated based on the actual number of relevant documents that exist in the collection. Thus NDCG cannot be estimated based on the features of the top-ranked documents alone. Finally, in terms of correlation and RMSE, there is little difference in prediction effectiveness between simple linear regression and the non-linear random forest based regression.

Table 2: RAPP Effectiveness: Corr. ­ Linear correlation measure.

RMSE ­ root mean squared error.

Method

DCG

NDCG

Corr. RMSE Corr. RMSE

Linear

0.78 0.13 0.50 0.23

Random Forest 0.79 0.13 0.52 0.22

The scatter plot in Figure 2(a) illustrate a strong correlation between the predicted and actual DCG values for one fold of the data. Figure 2(b) shows predicted NDCG values which are not as strongly correlated with the actual values. For DCG, when the actual values are less than 0.2, the predicted values are also less than 0.2 in most cases. On the other hand, when the actual values are greater than 0.4 the predicted values are more spread out. This suggests, DCG prediction is more precise for hard queries than for average, and easy queries. Our preliminary analysis suggests that feature values for hard queries are most consistent (lower values) compared to easy queries.
Similarly, NDCG prediction is highly precise when predicted values are below 0.3. However, prediction effectiveness degrades quickly when predicted values are above 0.4. Thus, for both measures, the high linear correlation and low RMSE values mask the rather poor effectiveness at the extremes.

(a) DCG

(b) NDCG

Figure 2: Prediction versus Actual Metrics for Test fold 1.

Feature Importance. Next, we inspect the features used for regression. We consider three subsets: features based on 1) LambdaRank scores, 2) Click-based features, and 3) BM25F-based features. Table 3 shows the prediction effectiveness of the different feature groups for linear regression. For DCG, all feature groups achieve high correlation while for NDCG, click and BM25F features are substantially lower compared to the combined features. Also, relative feature importance differs for DCG and NDCG. For instance, click features are more important for predicting DCG than LambdaRank score features. The order is reversed for NDCG. Click-based features are strong predictors of user preference [1], and it is no surprise that they correlate well with DCG. However, NDCG being a non-linear metric, is harder to predict with clickbased features alone. Also, we hypothesize that since LambdaRank combines several features including click features and is trained to optimize for NDCG, the LambdaRank-based features are better predictors than click-based features. Interestingly, we find that the click features for DCG and LambdaRank features for NDCG are as effective as all the features combined. This suggests that more careful feature selection can reduce run-time computations while retaining prediction effectiveness.
Table 3: Feature Groups Effectiveness: Corr. ­ Linear correlation measure. RMSE ­ root mean squared error.

Group
LambdaRank Click BM25F All

DCG Corr. RMSE 0.75 0.14 0.78 0.13 0.71 0.14 0.78 0.13

NDCG Corr. RMSE 0.50 0.22 0.41 0.24 0.38 0.24 0.50 0.23

3. CONCLUSIONS
In this paper, we describe RAPP, an effective and efficient Web query performance prediction technique that uses retrieval scores and retrieval features. Large scale evaluation using actual web queries shows that RAPP is effective, and outperforms the stateof-the-art Clarity baseline. Moreover, experimental results suggest that Clarity is not well-suited for the web. While RAPP is a general approach that can be used for different ranking algorithms and to target different measures, the results in this paper are based only on DCG@5 and NDCG@5 prediction for LambdaRank. We leave investigation of RAPP's utility for other ranking algorithms and performance measures such as MAP as part of future work.

4. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsor.
5. REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR 2006, 19-26.
[2] C. Burges, R. Ragno, and Q. Le. Learning to rank with nonsmooth cost functions. Advances in NIPS, 19:193, 2007.
[3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR 2002, pages 299-306.
[4] J. Grivolla, P. Jourlin, and R. de Mori. Automatic classification of queries by expected retrieval performance. In SIGIR 2005 Workshop on Predicting Query Difficulty.
[5] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In SIGIR 2005, pages 512­519.
[6] A. Liaw and M. Wiener. Classification and regression by randomforest. R News, 2(3):18­22, 2002.

786

Hashtag Retrieval in a Microblogging Environment
Miles Efron
Graduate School of Library and Information Science University of Illinois at Urbana-Champaign
501 E. Daniel St., Champaign, IL, 61820, USA
mefron@illinois.edu

ABSTRACT
Microblog services let users broadcast brief textual messages to people who "follow" their activity. Often these posts contain terms called hashtags, markers of a post's meaning, audience, etc. This poster treats the following problem: given a user's stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x. This poster proposes a language modeling approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Relevance Feedback
General Terms
Experimentation, Performance, Theory
Keywords
microblog, twitter, hashtag, relevance feedback
1. INTRODUCTION
Microblogging services allow users to post brief textual messages that are broadcasted to the user's "followers." Today, the most visible microblogging service is twitter.com where users post so-called tweets of no more than 140 characters. While many tweets are inconsequential, others contain information of broad interest, as well as links to external resources (e.g. photos or websites). This poster proposes an approach to one aspect of microblog IR: retrieving hashtags on a topic of interest to a searcher.
Many tweets are marked with so-called hashtags. A hashtag is a character string preceded by a # sign. Hashtags often signal aspects of a tweet's meaning such as its topic or its intended audience. Thus #sigir2010 ostensibly marks tweets related to the 2010 SIGIR conference. A person who is interested in a topic, say vegetarian recipes, might want to find hashtags that are often applied to posts about vegetarian recipes, veganism, healthy eating, etc.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

For a user's topical query q, we wish to find a list of k tags that are relevant to the information need represented by q. The task involves accepting a keyword query and returning a ranked list of hashtags. We approach hashtag retrieval as a type of entity search [1, 2].
Finding useful hashtags offers benefits that are particular to the microblog setting:
· Tags to follow : A user may wish to find hashtags that he or she can track on an ongoing basis.
· Result display: If users are searching for tweets, people, or posted URLs, returned units of retrieval could be grouped into clusters by their associated hashtags.
· Query expansion: Hashtags provide leverage for query expansion during relevance feedback.
The task we address aims to support these actions. This poster explicitly treats only the last item: query expansion. More specifically, we are concerned with query expansion in service to hashtag retrieval.

2. MODEL

Let C be a corpus containing n tweets. Among these n tweets we have m distinct hashtag types. We induce m language models, one per hashtag. To fit a tag ti's language model we analyze the set of tweets containing ti, fitting a multinomial over the vocabulary words, with probability vector i. The maximum likelihood estimator for wi, the probability of a word w given i, is the number of times w occurs in the set of tweets containing ti divided by this set's total word count. We smooth estimated models ^ by Bayesian updating with Dirichlet priors (µ = 2000). Given a query q generated by the query model q, we rank hashtags in decreasing order of the negative KL divergence between their models and q [3]:

r(ti, q) = -D(q||i)

(1)

where unless otherwise noted the calculation uses ^ M q L, the maximum likelihood estimator for q.

2.1 Hashtag Query Expansion

We propose restricting the added query terms to those candidates that are hashtags, stripping candidates of their leading #. The topical nature of hashtags motivates this operation.
Let rk be the set of the k top-ranked hashtags (by Eq. 1). Here we set k = 25. We define r, a multinomial parameter vector that has non-zero probability over the tags in rk and zero probability for all other terms. We derive a feedback

787

query model: ^ fb = (1 - )^ M q L + ^ r where  is a tunable parameter on (0, 1) that we fix at 0.2 (a value chosen empirically). As for ^ r, we propose two variants:

· HFB1 : Non-zero elements of ^ r are uniformly dis-

tributed.

·

HFB2 :

each

non-zero

^ri

is

proportional

to

IDF (ti) max IDF

where IDF (ti) is the inverse document frequency for tag i
and max IDF is the IDF for a tag with document frequency 1. Feedback information enters retrieval by using ^ fb for
the query model in Eq. 1.

2.2 Hashtag Association
Given the previous definition of rk, let X be the k × k matrix where for two tags ti and tj in rk, xij gives the number of times ti occurs (across the corpus) in a tweet that also contains tj. Also let xii = 1. We normalize X so that its columns (rows) are of unit length. Let a tag's association with the retrieved tags rk be:

k

X

a(ti, rk) = xij .

(2)

j

Eq. 2 is large if a tag co-occurs with many other tags in rk. Large values for Eq. 2 suggest that a tag has a strong presence in the "neighborhood" of the query. We combine Eq. 1 with Eq. 2, leading to the ranking score, ra(ti, q) = r(ti, q) + log a(ti, rk). Runs using ra are designated HFB1a or HFB2a (depending on the HFB used).

3. EVALUATION
We gathered data over a 24-hour period using Twitter's streaming API1 (cf. Table 1). 29 topical queries were created based on the author's interaction with Twitter. Relevance judgments were obtained using the Amazon Mechanical Turk service2. For each query, we created a pool of tags to be judged using runs from three systems: simple KL divergence, KL divergence on a Porter-stemmed corpus, and Rocchio relevance feedback using a TF-IDF model. Each judger was shown a keyword query, a candidate tag, a paragraph-long description of what would make a tag "useful," and a sample of recent tweets using the tag. Judgers ranked each query-tag pair on a four-point scale from 0 (not useful) to 3 (definitely useful). Each query-tag pair was judged by 5 judgers. Usefulness scores were obtained by taking both the mean and median of these 5 scores (we report results based only on means). We take tags with usefulness > 1 to be relevant (graded relevance is used for NDCG).

Table 1: Test collection summary statistics

Number of tweets

3,414,330

Number of hashtag types

50,097

Number of hashtag tokens 571,861

Number of users

874,892

Number of queries

39

Median number relevant tags 28.5

1http://api.twitter.com 2http://www.mturk.com

We tested four experimental conditions:
1. Baseline, no feedback: simple KL divergence 2. Baseline, with feedback: KLD retrieval with diver-
gence minimization feedback (divergence minimization gave stronger results than mixture model feedback) [3] 3. Hashtag-based query expansion (HFB1 and HFB2) 4. Hashtag query expansion with association measure (HFB2a).
For all feedback, five terms were added to the initial query, with a weight of 0.2. The baseline feedback model used the top 10 documents. No stemming or stoplists were applied. Runs returned the top 25 tags for each query.
We report three statistics (Table 2): Mean average precision (MAP), normalized discounted cumulative gain (NDCG) at 15, and precision at 10 (P10). All runs using hashtagbased feedback gave results that were statistically significantly better than the baseline run using standard term-based feedback.

Table 2: Retrieval effectiveness. All HFB runs show statistically significant improvement over the baseline feedback run on all three measures (p < 0.05 on a randomization test).  indicates p < 0.01.

Method Baseline, no FB Baseline, FB HFB1 HFB2 HFB2a

MAP 0.4268 0.4381 0.4605 0.4617 0.4684

NDCG 0.6110 0.6209 0.6431 0.6388 0.6488

P10 0.7034 0.7138 0.7483 0.7414 0.7483

Table 2 suggests that hashtags provide useful information for relevance feedback. While the baseline (term-based) feedback run was only slightly more effective than the baseline run without feedback, all tag-based feedback runs performed better than the term-based baseline feedback model. HFB2 (using IDF weighting for feedback tags) gives marginal improvement over uniformly weighted expansion, while our association measure gives a bit more of an edge. However, the differences among the three test conditions are slight.
4. CONCLUSIONS
In future work we will identify additional features of hashtags (e.g. from author-based statistics) for use in IR. We will also undertake a more thorough empirical evaluation. The main work, however, lies in defining the relevant problems, applications, and user needs in IR from microblogs.
5. REFERENCES
[1] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling framework for expert finding. Inf. Process. Manage., 45(1):1­19, 2009.
[2] C. Macdonald and I. Ounis. Using relevance feedback in expert search. Proc. of 29th European Conf. on Information Retrieval, Springer LNCS, 4425:431­443, 2007.
[3] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM01: Proc. of the 10th Intl Conference on Information and Knowledge Management, pages 403­410, 2001.

788

Crowdsourcing a Wikipedia Vandalism Corpus
Martin Potthast
Bauhaus-Universität Weimar 99421 Weimar, Germany
martin.potthast@uni-weimar.de

ABSTRACT
We report on the construction of the PAN Wikipedia vandalism corpus, PAN-WVC-10, using Amazon's Mechanical Turk. The corpus compiles 32 452 edits on 28 468 Wikipedia articles, among which 2 391 vandalism edits have been identified. 753 human annotators cast a total of 193 022 votes on the edits, so that each edit was reviewed by at least 3 annotators, whereas the achieved level of agreement was analyzed in order to label an edit as "regular" or "vandalism." The corpus is available free of charge.1
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation
General Terms: Experimentation
Keywords: Wikipedia, Vandalism Detection, Evaluation, Corpus
1. INTRODUCTION
Wikipedia is an encyclopedia written by the crowd. The key to Wikipedia's success is a collaborative writing process, where everybody can edit every article. Ideally, the reader of an article also revises it to the best of her abilities, e.g. by correcting errors, by improving the writing style, by adding missing information, or by removing redundancy. In this way Wikipedia's articles get continuously improved and updated. This "freedom of editing" gave the lie to those who suggested that the resulting articles would be characterized by poor quality and instability. Wikipedia thrives. There is no free lunch, however, and Wikipedia faces problems that limit its growth, such as vandalism, edit wars, and lobbyism. Our concern is the automatic detection of vandalism in Wikipedia, i.e., the detection of edits that were made with bad intentions. We contribute to this research field by developing a large corpus of human-annotated edits, which is a prerequisite for the meaningful evaluation of vandalism detection algorithms. In particular, we report on our efforts to use Amazon's Mechanical Turk as a possibility to drive the corpus size to the necessary order of magnitude without compromising the corpus quality.
Related Work. Although vandalism has been observed in Wikipedia right from the start, and, although vandalism is often deemed one of Wikipedia's biggest problems, research has addressed automatic vandalism detection only recently--for the first time in [3, 5, 7]. Vandalized articles often get restored rather quickly by other editors, but still, the authors of [6] find that the number of times vandalized articles get viewed amounts up to hundreds of millions,
1Download the corpus from http://www.webis.de/research/corpora
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

and that the probability of encountering vandalism grew exponentially between 2003 and 2006. In reaction to this development, the Wikipedia community has developed a number of rule-based robots that are capable of restoring the most obvious cases of vandalism, or that aid editors to do so [2]. However, the performance of the robots is surpassed, for instance, by an approach based on machine learning [5]. Other reactions include the temporary suspension of the freedom of editing for articles that are often vandalized, which threatens the very idea of Wikipedia.
The first vandalism corpus was the Webis-WVC-07, which consists of 940 human-annotated edits of which 301 are vandalism [4]. The PAN-WVC-10 is two orders of magnitude larger and has been annotated by many different people; it thus forms a more representative sample of vandalism and allows for better estimates of whether a vandalism retrieval model will actually work in practice. In this respect, the Mechanical Turk provides an exciting new way to scale up corpus construction, which has also been applied successfully, e.g., to recreate TREC assessments [1].
2. CORPUS DESIGN
Corpus Layout. An edit marks the transition from one article revision to another. On Wikipedia, each revision of every article is accessible by means of a permanent identifier, so that an edit is described uniquely by a pair of revision IDs referencing the old article revision and the new revision.2 Basically, our corpus is a list of revision ID pairs along with labels whether or not the respective edit is vandalism. Moreover, for each edit meta information is given as well as the plain texts of both the old and the new article revision.
Corpus Acquisition. Our sample of edits is drawn from the revision histories of Wikipedia articles by means of probability proportional to size sampling, where in our case, the "size" of an article is the average number of times it gets edited in a given time frame. We hypothesize that the average edit ratio of an article correlates with the number of times it gets viewed. In that case, our edit sample resembles well the distribution of article importance at the time of sampling, which presumably also influences the articles chosen by vandals. By contrast, the edits of the Webis-WVC-07 were chosen in search for vandalism from articles whose topics, per se, have a high conflict potential, which reveals a sample bias of that corpus.
Corpus Annotation. Amazon's Mechanical Turk is a platform for paid crowdsourcing. It acts as an intermediary between workers and so-called requesters who offer tasks and a reward for each task being solved. Typically, task assignment and result submission is handled double-blind. This sense of anonymity and the fact that real money can be earned tempts some workers to fake results in
2Here is an example for a vandalism edit, shown as Wikipedia Diff page: http://en.wikipedia.org/w/index.php?diff=327907617&oldid=327774745

789

Table 1: Re-annotation of the Webis-WVC-07 corpus.

3 Annotators / Edit

Agreement with Webis-WVC-07 (Gold Standard)

3 agree 3 disagree 2 agree 2 disagree

56 % 2% 36 % 6%

Accuracy

if 3 agree

Baseline (all edits regular)

96 % 68 %

16 Annotators / Edit

more than 2/3 agree 93 %

more than 2/3 disagree 1 %

tie majority agrees

0%

tie majority disagrees 6 %

if more than 2/3 agree 99 % 68 %

order to get paid without working. Requesters therefore may approve or reject results while workers are paid only if their results are approved--which in turn of course tempts requesters to reject acceptable results to save the money. From this it becomes clear that requesters need to analyze the results obtained via the Mechanical Turk to sort out bad workers, while the type, design, and reward of a task may influence their amount significantly. Hence, a task should be designed so as to make faithful work more worthwhile than deception. In our case we first presented workers with a list of links to edits, and along each link, a form to select whether the linked edit is regular or vandalism. This simple and straightforward design led 80 % of the workers to quickly select options at random without clicking on the associated link. We therefore redesigned our task as a dialog that shows the worker one edit at a time, along the aforementioned form. This lowers the bar for faithful work since no additional interactions are necessary, and at the same, faking results requires the same amount of interaction.
3. PILOT EVALUATION
Before compiling our own corpus, we have first evaluated the quality of the annotations obtained via the Mechanical Turk by re-annotating the Webis-WVC-07 edits. This allows to determine whether and how scaling up vandalism annotation works. Additionally, we surveyed how often the workers use Wikipedia.
Corpus Annotation Accuracy. Table 1 shows the results of two rounds of re-annotating the Webis-WVC-07, each with a different number of annotators per edit. When considering three annotators per edit, two things can happen: all annotators agree with each other, or it is two against one. Moreover, in each of these cases the annotators either agree or disagree with the gold standard. In 56 % of the cases the annotators achieve perfect agreement with the gold standard, while in 2 % of the cases 3 annotators disagree completely. We have analyzed the latter edits and found that in half of these cases the annotations of the Webis-WVC-07 are wrong. In the other half of the disagreement cases we found that, with a ratio of about 3:1, more vandalism edits were considered regular than the other way around. We have conducted the same analysis for a round of 16 annotators per edit, only this time we consider more than 2/3 agreement among annotators as sufficient, while less agreement is considered a tie. Again, the majority of annotators either agree or disagree with the gold standard. We find that 93 % of the edits are annotated in accordance with the Webis-WVC-07. The remainder of the edits either correspond to the erroneous cases mentioned above or they are truly tough calls, even for an expert. Altogether, when considering only edits on which more than 2/3 of the annotators agree, the classification accuracies are 96 % and 99 %, which increase significantly over the baseline.
Worker Survey. The results of the survey are summarized in Table 2: while the majority of workers read Wikipedia daily to weekly a much smaller proportion also edit articles. 2 % of the workers admittedly vandalized Wikipedia. Interestingly, most of the work-

Table 2: Wikipedia usage of 753 Mechanical Turk workers.

Reading

Wikipedia Usage

Editing

Vandalizing

daily 27 % daily 2 % no

weekly 23 % weekly 3 % yes

monthly 4 % monthly 6 %

less

2 % less 16 %

never 0 % never 29 %

54 % 2%

n/a 44 % n/a 44 % n/a 44 %

Noticing Vandalism (if editing daily-monthly)

daily 3 % weekly 7 % monthly 15 % less 26 % never 5 %

(22 %) (34 %) (33 %) (10 %) ( 1 %)

n/a 44 % ­

ers do not often notice vandalism, however, when considering only workers who edit daily to monthly, the picture is turned upside down: Wikipedia's editors often have to restore vandalism. In any case, these numbers have to be taken with a grain of salt: they are not representative of all Wikipedia users, and there is no way of knowing whether the workers answered truthfully. In an attempt to minimize false answers, filling out the questionnaire was optional.

4. CORPUS CONSTRUCTION
In sum, we pursued the following strategy to generate our corpus: 33 000 edits were sampled from Wikipedia and annotated by 3 annotators each. All edits on which no more than 2/3 of its annotators agreed were re-annotated by another 3 annotators, and again, until ties were resolved or their number was small enough to be reviewed manually. We observe that the number of tie edits decreases exponentially with each iteration:

Iteration Tie Edits

0

1 2 3 4 5 6 78

33 000 22 834 9 776 3 880 2 138 1 315 815 288 70

In order to check up on the worker's success in annotating edits, every 5th edit to be classified was in fact a vandalism edit chosen at random from the Webis-WVC-07. From iteration 3 onwards, however, the check edits were chosen from the vandalism edits already identified. This way, these edits received more votes than necessary, but in the long run, false positives may have been retracted. The 70 edits that were still tied after the 8th iteration have been reviewed by two experts who made a decision about them to the best of their knowledge. A handful of edits turned out to be undecidable, and those were given the benefit of the doubt. Finally, some of the edits became inaccessible along the way due to errors or administrative removal on the side of Wikipedia. A total of 32 452 edits were successfully annotated of which 2 391 are vandalism.

5. REFERENCES
[1] O. Alonso and S. Mizzaro. Can We Get Rid of TREC Assessors? Using Mechanical Turk for Relevance Assessment. In Proc. of SIGIR'09.
[2] R. S. Geiger and D. Ribes. The Work of Sustaining Order in Wikipedia: The Banning of a Vandal. In Proc. of CSCW'10.
[3] K. Y. Itakura and C. L. A. Clarke. Using Dynamic Markov Compression to Detect Vandalism in the Wikipedia. In Proc. of SIGIR'09.
[4] M. Potthast and R. Gerling. Webis Wikipedia Vandalism Corpus Webis-WVC-07. http://www.webis.de/research/corpora, 2007.
[5] M. Potthast, B. Stein, and R. Gerling. Automatic Vandalism Detection in Wikipedia. In Proc. of ECIR'08.
[6] R. Priedhorsky, J. Chen, S. Lam, K. Panciera, L. Terveen, and J. Riedl. Creating, Destroying, and Restoring Value in Wikipedia. In Proc. of Group'07.
[7] K. Smets, B. Goethals, and B. Verdonk. Automatic Vandalism Detection in Wikipedia: Towards a Machine Learning Approach. In Proc. of WikiAI at AAAI'08.

790

MEMOSE ­ Search Engine for Emotions

in Multimedia Documents

Kathrin Knautz
Heinrich-Heine-University
Düsseldorf Universitätsstrasse 1 D-40225 Düsseldorf +49 (0)211-81-12334
kathrin.knautz@uniduesseldorf.de

Tobias Siebenlist
Heinrich-Heine-University
Düsseldorf Universitätsstrasse 1 D-40225 Düsseldorf +49 (0)211-81-12913
tsiebenlist@acm.org

Wolfgang G. Stock
Heinrich-Heine-University
Düsseldorf Universitätsstrasse 1 D-40225 Düsseldorf +49 (0)211-81-12913
stock@phil-fak.uniduesseldorf.de

ABSTRACT
The MEMOSE (Media Emotion Search) system is a specialized search engine for fundamental emotions in all kinds of emotionalladen documents. We apply a controlled vocabulary for basic emotions, a slide control to adjust the intensities of the emotions and the approach of broad folksonomies. The paper describes the indexing and the retrieval tool of MEMOSE and results from its evaluation.
Categories and Subject Descriptors
H.3.1 [Information storage and retrieval]: Content Analysis and Indexing - indexing methods
H.3.3 [Information storage and retrieval]: Information Search and Retrieval ­ Search process
General Terms
Design, Human Factors
Keywords
Emotion, Multimedia Resources, Collaborative Indexing, Slide control tagging, Emotional Information Retrieval (EmIR)
1. INTRODUCTION
Some content in multimedia resources is able to display feelings or to provoke certain emotions in the users. The aim of our research is to identify these emotions and to make them searchable so that they can be used for information retrieval. Emotions are hidden in different document types. We can find emotional-laden documents in textual objects like lyrics, poems or novels, music [3], images [5], videos [1] and blogs. Lee and Neal [3], Schmidt and Stock [5] and Knautz et al. [1] pointed out that users are able to index fundamental emotions consistently by means of tagging (in the context of a broad folksonomy). In our studies we worked with a controlled vocabulary for basic emotions (love, happiness, fun, surprise, desire, sadness, anger, disgust and fear), a slide control to adjust the emotions' intensities, and the approach of broad folksonomies [4].
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

We found two forms of distribution: the power law and the inverse logistic distribution [6]. In the first one users mostly assign only one emotion with a high intensity to a resource. An inverse logistic distribution is available if several emotions with a high average value are assigned to the multimedia documents. Knautz et al. [1] showed that a relative stability of the distributions' shapes can be reached after some dozens of tagging users.
2. METHOD
Our next-generation retrieval system for emotions in multimedia documents is designed as a web application and consists of the following components: a tool for emotional tagging for adding weighted emotional tags to the resources, some processing scripts that interact with the APIs of Web 2.0 services and our API and finally a retrieval tool needed to access the indexed data.
2.1 Dataset & Technology
For our experimental evaluation we selected about 500 pictures with animals on it from Flickr. The pictures were collected by a group of students studying information science. We reduced the scope of this broad field by limiting the available animals to pets and farm animals in order to get several results for every type of animal. There were no restrictions regarding the selection of images, the students were encouraged to choose pictures by random and not by means of felt emotions. The only demand was that these pictures were taken from Flickr. After the links were collected, they were checked for validity. Additional data (e.g, information about the author, associated tags, license etc.) were collected by using the Flickr API. All the tools that will be described here were developed using PHP. The data were held in a MySQL database with an InnoDB engine. In order to get fast response times the emotional intensity was saved in a materialized view. The user interface was designed using strict XHTML and the jQuery library for dynamic elements like the slide controls.
2.2 Emotional tagging
For the purpose of emotional tagging we created a user interface consisting of one picture and 18 slide controls on one web page (figure 1). The 18 slide controls where split into two groups of 9 slide controls each.

791

usability test. In order to get a first idea of what people think of a search engine for emotions we took a number of people who were not involved in any part of development. They had to do several tasks (e.g., to search for pictures with dogs and happiness) including filling out questionnaires and doing task-based thinking-aloud tests that were recorded. We concluded from the results of the evaluation that the search for emotions in multimedia documents is an exciting new task that people need to adapt to. Especially the separated display of shown and felt emotions in a two-column raster was at first hard to cope with. And ­ not unimportant for Web 2.0 services ­ our test persons found MEMOSE an enjoyable system.

Figure 1. MEMOSE's tagging interface
One group contained the emotions shown, i.e. the emotions that are displayed on the picture. The other group covered the emotions felt by the viewer when looking at the picture. The range of every scroll bar contains values from 0 to 10, so the viewers could differentiate the intensity of every emotion. A zero value means that the emotion is not shown on the image or not felt by the observer. As we had this large number of pictures for every user to tag it was not practicable to force the students to tag all the pictures in one pass one after the other. We applied a user management by which the already tagged pictures were logged and with its help every user could make breaks whenever needed and go on later. Another benefit of this user management was that no user could tag a picture twice.
2.3 Emotional retrieval
The retrieval tool was designed to access the tagged pictures in a manner of a search engine. For this purpose we designed a clean user interface consisting of one input field and nine checkboxes where the emotions could be selected. The user first had to choose the emotions to search for and then got suggestions by typing in the search terms through an auto completion feature based on the tags we got from Flickr according to the pictures. After sending the request the results are shown in two distinct columns, differencing between shown and felt emotions (figure 2). Both columns are sorted in descending order regarding the selected emotions. For every hit a thumbnail of the corresponding picture and bars that show the intensity of the chosen emotions are displayed. The retrieval status value (RSV) of the documents (which satisfy both topic and emotion) was calculated through the arithmetic mean of the intensity of the emotion. If there were more than one emotional search argument, we took the sum of the means as RSV. By clicking on the thumbnail the picture is opened in an overlay window filled with further information like associated tags, information about the author etc.
3. Evaluation
From the tool box of evaluation methods [2] we took the SERVQUAL, the customer value discovery and the critical incident approach to evaluate the IT service dimension. To evaluate the IT system dimension we worked with success factors such as ease of use, perceived usefulness, trust and fun, and with a

Figure 2. MEMOSE's search results interface
4. REFERENCES
[1] Knautz, K. et al. 2010. Indexieren von Emotionen bei Videos. Information ­ Wissenschaft und Praxis 61(4), in press
[2] Knautz, K., Soubusta, S., and Stock, W.G. 2010. Tag Clusters as Information Retrieval Interfaces. In Proceedings of the 43th Annual Hawaii International Conference on System Sciences (HICSS-43), January 5-8, 2010. IEEE Computer Society Press, 10 pages.
[3] Lee, H.J. and Neal, D. 2007. Toward Web 2.0 Music Information Retrieval. Utilizing Emotion-based, Userassigned Descriptors. In Proceedings of the 70th ASIS&T Annual Meeting. Joining Research and Practice: Social Computing and Information Science, Milwaukee, October 19-24, 2007.
[4] Peters, I. 2009. Folksonomies. Indexing and Retrieval in Web 2.0. Berlin, Germany: De Gruyter Saur.
[5] Schmidt, S. and Stock, W.G. 2009. Collective Indexing of Emotions in Images. A Study in Emotional Information Retrieval. J. Am. Soc. Inf. Sci. Tec, 60(5), 863-876.
[6] Stock, W.G. 2006. On Relevance Distributions. J. Am. Soc. Inf. Sci. Tec., 57(8),

792

The Power of Naïve Query Segmentation

Matthias Hagen

Martin Potthast

Benno Stein

Christof Bräutigam

Bauhaus-Universität Weimar 99421 Weimar, Germany
<first name>.<last name>@uni-weimar.de

ABSTRACT
We address the problem of query segmentation: given a keyword query submitted to a search engine, the task is to group the keywords into phrases, if possible. Previous approaches to the problem achieve good segmentation performance on a gold standard but are fairly intricate. Our method is easy to implement and comes with a comparable accuracy.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Query formulation
General Terms: Algorithms, Experimentation
Keywords: Query Segmentation
1. INTRODUCTION
Most Web search queries are sequences of keywords that may comprise complete phrases or compound concepts, e.g., new york yankees. Such phrases and concepts can be exploited by search engines as indivisible units in order to improve retrieval precision, or to allow for query reformulation on the level of phrases instead of keywords. Ideally, Web searchers would assist the engines by enclosing their phrases in quotes, but experience shows that many searchers aren't even aware of this option. Hence, search engines apply pre-retrieval algorithms that automatically segment queries in order to second-guess the user's intended phrases and to improve the overall user experience. Our contribution in this respect is a new and simple approach to the task of query segmentation that achieves a segmentation accuracy comparable to stateof-the-art algorithms.
Related Work. Recent research suggests a variety of approaches to query segmentation. For instance, Bendersky et al. [1] use a two-stage procedure. Guo et al. [4] and Yu and Shi [9] use methods based on conditional random fields. However, Yu and Shi explicitly focus on query segmentation in the context of text stored in relational databases and use database-specific features that are not applicable in the Web setting. One of the earliest approaches to Web query segmentation is by Risvik et al. [6]. They segment queries by computing so-called connexity scores that measure mutual information within a segment and the segment frequency in a query log. Jones et al. [5] also use a mutual-information-based scoring that finds segments in which adjacent terms have high mutual information. However, neither Risvik et al. nor Jones et al. experimentally evaluate the segmentation accuracy of their approaches. In more recent papers, methods based on mutual information are used as baselines and they are shown not to perform as good as
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

more involved methods, such as the supervised learning method by Bergsma and Wang [2]. Bergsma and Wang use statistical features obtained by Web queries and from query logs, as well as dependency features that are focused on noun phrases. They also established a gold standard corpus of 500 queries, each segmented by three human annotators. Subsequent work [8, 10] has adopted the corpus, as do we in our evaluations. Bergsma and Wang's supervised learning method is trained on queries segmented by the same annotator who also segmented the gold standard queries. This leaves some doubts with regard to their otherwise remarkably good accuracy. Instead of the supervised approach that requires training data, Tan and Peng [8] and Zhang et al. [10] suggest unsupervised methods based on expectation maximization and eigenspace similarity. Tan and Peng's method, like ours, uses n-gram frequency counts from a large Web corpus, but unlike our method, it tries to establish segment scores via expectation maximization. In a second step they also check whether segments are prominently used in Wikipedia. Instead, Zhang et al. suggest to compute segment scores from the eigenvalues of a correlation matrix corresponding to the query. Hence, all three approaches, Bergsma and Wang's, Tan and Peng's, and Zhang et al.'s, rely on intricate models and techniques whose optimization involves several hyperparameters. By contrast, our approach to query segmentation implements a straightforward usage of n-gram frequency counts, which performs just as well.
2. N-GRAM QUERY SEGMENTATION
The basic and major assumption of our approach is that phrases contained in queries actually exist on the Web. The idea then is to use the Web itself as a corpus of potential query phrases. The largest obtainable collection of Web phrases is the Google n-gram corpus [3]; it contains n-grams of length 1 to 5 from the 2006 Google index along with occurrence frequencies. Based on these Web occurrence frequencies our approach scores a query's possible segmentations and outputs the "best" choice.
We regard a query q as a sequence (w1, w2, . . . , wn) of n keywords. A valid segmentation S for q is a sequence of disjunct segments s, each a contiguous subsequence of q, whose concatenation equals q. There are 2n-1 valid segmentations for q, and (n2 -n)/2 potential segments that contain at least two keywords from q. Our algorithm derives a score for a valid segmentation as follows. First, the n-gram frequency count (s) of each potential segment s is retrieved. For n-grams up to n = 5 the frequencies can be obtained directly from the corpus; for longer n-grams up to n = 9 estimations are made analogously to the set-based method described in [8]. Having the frequencies at hand, all valid segmentations are enumerated systematically, and for each segmentation S a score is

797

Table 1: Segmentation performance on the gold standard.

Anno- Accuracy tator Measure MI

Algorithm [2] [8] [10] Naïve

query

0.274 0.638 0.526

0.536

break

0.693 0.863 0.810

0.807

A

seg prec 0.469

0.657 0.652 0.665

seg rec 0.534

0.657 0.699 0.708

seg F

0.499

0.657 0.675 0.686

query

0.244

break

0.634

B

seg prec 0.408

seg rec 0.472

seg F

0.438

0.494 0.802 0.623 0.640 0.631

0.632 0.659 0.645

0.380 0.752 0.519 0.626 0.568

query

0.264

break

0.666

C

seg prec 0.451

seg rec 0.519

seg F

0.483

0.494 0.796 0.634 0.642 0.638

0.614 0.649 0.631

0.454 0.772 0.581 0.653 0.615

Agree

query break seg prec seg rec seg F

0.343 0.728 0.510 0.550 0.530

0.717 0.892

0.671 0.871 0.767 0.782 0.774

0.772 0.826 0.798

0.627 0.851 0.718 0.778 0.746

computed according to the following function:

X

score (S) =

|s||s| · count (s).

sS,|s|2

The factor |s||s| gives significant weight to long segments compared to shorter ones in order to compensate the power law distribution of occurrence frequencies on the Web. For example, "new york" has a much larger count than "new york yankees", so that the exponential scoring function helps us to avoid segmentations like "new york" "yankees". For a query q we choose from all valid segmentations the segmentation S that maximizes score (S). This naïve approach is competitive with the more involved methods, as our evaluation shows.

3. EVALUATION
We have indexed the Google n-gram corpus in an inverted file in a way similar to [7]. As a query corpus we use the gold standard for query segmentation established by Bergsma and Wang [2], which was also used in subsequent evaluations [8, 10]. Table 1 contains the results reported on that corpus for the mutual information (MI) baseline of [8], the results of the best performing methods from [2, 8, 10], and the results of our naïve approach. The table should be read as follows. Three annotators--A, B, and C--independently segmented the 500 queries of the gold standard, which were originally drawn from the AOL 2006 query log. The annotators segmented 220 of the 500 queries in the same way, denoted in the row named "Agree." As for the segmentation accuracy measures, we report performances on the following three levels: the query accuracy is the ratio of segmented queries that match the gold standard, the break accuracy is the ratio of decisions between two consecutive words (different/same segment) that match the gold standard, and, at the segment level we measure how well the segments found match the gold standard by means of segment precision, segment recall, and segment F -Measure.
The results in Table 1 show that the approach of Bergsma and Wang [2] is slightly better than the other approaches on annotator A as well as on the queries all annotators agree upon. However,

it should be noted that their approach is based on a supervised learning algorithm that was explicitly trained on queries segmented by annotator A (the agreement queries also match A's segmentation), and that it requires a lot of additional Web search queries in order to segment one query. Bergsma and Wang did not report exact performance values for the annotators B and C and they did not measure segment level performance. Zhang et al. [10] did not report performances on the query or the break level. As for the two approaches of Tan and Peng [8] and Zhang et al. [10], note that our method is marginally better on annotator A, approximately 0.1 worse on annotator B, and in a 0.05 range on annotator C as well as the agreement queries. Our performance on annotator B can be improved in a postprocessing step which looks for single keyword segments in the segmentation with the highest score, and which adds them to the respective left or right segment if the occurrence frequency count (s) of the resulting segment s is above a threshold of 1000. This tweak then also achieves a 0.05 range on annotator B (query: 0.442; break: 0.774; seg F: 0.579). However, we do not wish to add this tweak to our method for reasons of simplicity, and more importantly, because it appears to be overfitted to annotator B.
As for runtime performance, we are able to segment more than 3000 queries per second on a single machine with sufficient main memory to store all the n-gram counts (about 12 GB).
4. CONCLUSION AND OUTLOOK
In this paper, we presented an approach to query segmentation that is competitive with the best algorithms developed so far, while being less intricate at the same time. As for future work, the evaluation of the state-of-the-art-approaches with respect to gains in retrieval performance is an interesting open problem. To measure segmentation accuracy on a larger scale we are also working on a gold standard that includes significantly more queries.
5. REFERENCES
[1] M. Bendersky, W. B. Croft, and D. Smith. Two-stage query segmentation for information retrieval. In Proc. of SIGIR 2009, pages 810­811.
[2] S. Bergsma and Q. I. Wang. Learning noun phrase query segmentation. In Proc. of EMNLP-CoNLL 2007, pages 819­826.
[3] T. Brants and A. Franz. Web 1T 5-gram Version 1. Linguistic Data Consortium LDC2006T13, 2006.
[4] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In Proc. of SIGIR 2008, pages 379­386.
[5] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In Proc. of WWW 2006, pages 387­396.
[6] K. M. Risvik, T. Mikolajewski, and P. Boros. Query segmentation for Web search. In Proc. of WWW 2003 (Posters).
[7] B. Stein, M. Potthast, and M. Trenkmann. Retrieving customary Web language to assist writers. In Proc. of ECIR 2010, pages 631­635.
[8] B. Tan and F. Peng. Unsupervised query segmentation using generative language models and Wikipedia. In Proc. of WWW 2008, pages 347­356.
[9] X. Yu and H. Shi. Query segmentation using conditional random fields. In Proc. of KEYS 2009, pages 21­26.
[10] C. Zhang, N. Sun, X. Hu, T. Huang, and T.-S. Chua. Query segmentation based on eigenspace similarity. In Proc. of ACL-IJCNLP 2009, pages 185­188.

798

Clicked Phrase Document Expansion for Sponsored Search Ad Retrieval

Dustin Hillard
Yahoo! Inc Great America Parkway Santa Clara, CA, 95054
dhillard@yahoo-inc.com
ABSTRACT
We present a document expansion approach that uses Conditional Random Field (CRF) segmentation to automatically extract salient phrases from ad titles. We then supplement the ad document with query segments that are probable translations of the document phrases, as learned from a large commercial search engine's click logs. Our approach provides a significant improvement in DCG and interpolated precision and recall on a large set of human labeled query-ad pairs.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval--General
General Terms
Algorithms, Experimentation
1. INTRODUCTION
The primary source of revenue for major search engines is textual advertising. A search engine typically displays sponsored listings along with organic web-search results in response to user queries. The revenue model for these listings is "pay-per-click" where the advertiser pays the search engine only if the advertisement is clicked. A search engine typically decides which ads to show (and in what order) by optimizing revenue based on the probability that an ad will be clicked, combined with the cost of the ad [5].
The advertiser "targets" specific keyword markets by bidding on search queries. An advertising campaign consists of many ad groups where each ad group in turn consists of a set of bidded phrases or keywords that the advertiser bids on. A creative is associated with an ad group and is composed of a title, a description and a display URL. The title is typically 3-4 words in length and the description has about 10-15 words.
An advertiser can choose to use standard or advanced match for the keywords in an ad group. For example, enabling only standard match for the keyword "sports shoes", will result in the corresponding creative being shown only for that exact query. Whereas, if the keyword is enabled for advance match, the search engine can show the same ad for the related queries "running shoes" or "track shoes". A bid
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Chris Leggetter
Yahoo! Inc Great America Parkway Santa Clara, CA, 95054
cjl@yahoo-inc.com

is associated with each keyword and a second price auction model determines how much the advertiser pays the search engine for the click [3].
Most search engines typically take a two pronged approach to the problem: (1) finding relevant ads for a query, and (2) estimating CTR for the retrieved ads and appropriately ranking those ads for display on the search page. In this work we are largely concerned with the first point of discovering ads for a query.
Finding ads relevant to a query is an information retrieval problem and the nature of the queries makes the problem very similar to web-search, with some key differences. The collection of web documents is significantly larger than the advertiser database and retrieving candidate ads for infrequent queries is a very important area of research for sponsored search. A relevant sponsored search result may also match the query in a broader sense than when compared to web search: an ad for "limo rentals" could be a good match to a query for "prom tux", although the match would be too coarse for a typical web search.
In this paper we present an approach for document expansion that improves retrieval of sponsored results by mining user click logs for high click-rate phrase translations. We expand ad documents by automatically extracting title phrases and then including query segments that have historically high click rates in the context of an ad's title phrases.

2. PHRASE-BASED

DOCUMENT EXPANSION

Our document expansion approach consists of three main

components: CRF segmentation of the user queries and ad

titles, collection of click-rate statistics for query-title seg-

ment pairs, and expanding ad documents by inserting high

click-rate query phrases.

Linear chain CRFs have been used successfully for many

sequential labeling tasks like segmentation and part-of-speech

tagging, as well as in web search[4]. CRFs are particularly

attractive because we can use arbitrary feature functions on

the observations. Let Q = q1, q2...q3 denote an input query

and seg = y1, y2...yn denote the corresponding state se-

quence. Each yi falls into one of two categories begin-segment

and end-segment. The conditional probability P (seg|Q) is

given as:

1

n
XX

p(seg|Q; ) =

exp{

Z(Q; )

k

fk(yi-1, yi, Q, i)}

k

i

(1)

where fk(yi-1, yi, Q, i) is a feature function and  = {i} are

799

Figure 1: Interpolated Precision Recall

Figure 2: Precision Recall

the learned feature weights. We have several features based on dictionaries of people names, celebrity names, stopword lists and the top phrases in the query logs. We use the CRF++ toolkit 1 to train the model. The model is trained in a supervised fashion on 6000 queries annotated with phrases by humans.
We then apply this segmentation model to two months of sponsored search web logs, obtaining the top three hypotheses for every query and title. For each unique query-title segment pair we count the number of user clicks and expected clicks, where expected clicks is a position normalized count of impressions [6]. The result is a translation table of click-rates for all observed query-title segment pairs.
Finally, for each title segment in an ad we augment our ad document by adding all query segments that occurred in the context of that title segment as confident translations (we threshold on a click-rate of 1.5 and translation probability of 0.05, optimized on a held-out data set). In order to weight the query segments according to their click-rate we multiply their TF contribution by their click-rate in that context, so that additional weight is given to segments with high click rates. The final index contains ad documents expanded with weighted query segments that either reinforce current document phrases or add additional related phrases. Our approach is related to much previous work in using search logs as implicit relevance feedback (such as [2]), but our novel contribution is to focus on CRF extracted phrase terms and normalize clicks by expected clicks.
3. RESULTS
Our baseline retrieval system is a TF-IDF based ad retrieval system similar to [1]. We index our advertiser database, where each ad document consists of a title, description, URL, and bid terms. Our experiments compare the standard baseline system to the document expansion approach described in Section 2.
The test data contains 1k unique queries, which were selected based on a stratified sample of search engine traffic that represents all ten search frequency deciles. An average of 20 results per query were retrieved for the baseline and experimental system (the retrieved documents from the two
1http://crfpp.sourceforge.net/

systems overlapped by about 60%). A total of about 30k unique query-ad pairs were judged by human editors on a standard five point relevance scale. The document expansion experiment provides a 8% relative gain for DCG@1, and 3% relative gain for DCG@3.
Figure 1 shows a 2% absolute gain in recall for the region of higher recall performance. Figure 2 additionally shows the document expansion has improved the classification accuracy in terms of detecting Bad ad documents. Classification is particularly important for sponsored search because the retrieval stage typically presents an unordered candidate set that is later re-ranked by a click model.
4. CONCLUSION
We develop an approach to document expansion that utilizes automatic CRF phrase extraction, leverages normalized click-rates from web logs, and augments ad documents with weighted query phrases. The approach provides gains in both relevance ranking and classification of relevant ads.
5. REFERENCES
[1] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In CIKM. ACM, 2003.
[2] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic query expansion using query logs. In WWW, 2002.
[3] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords. American Economic Review, 97(1), March 2007.
[4] X. Li, Y.-Y. Wang, and A. Acero. Extracting structured information from user queries with semi-supervised conditional random fields. In SIGIR, pages 572­579, 2009.
[5] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In WWW, 2007.
[6] W. V. Zhang and R. Jones. Comparing click logs and editorial labels for training query rewriting. In WWW 2007 Workshop on Query Log Analysis: Social And Technological Challenges, 2007.

800

Three Web-based Heuristics to Determine a Person's or Institution's Country of Origin

Markus Schedl 1

Klaus Seyerlehner 1

Dominik Schnitzer 1,2

markus.schedl@jku.at klaus.seyerlehner@jku.at dominik.schnitzer@ofai.at

Gerhard Widmer 1,2 gerhard.widmer@jku.at

Cornelia Schiketanz 1 music@jku.at

1 Dept. of Computational Perception 2 Austrian Research Institute for

Johannes Kepler University (JKU)

Artificial Intelligence (OFAI)

Linz, Austria

Vienna, Austria

ABSTRACT
We propose three heuristics to determine the country of origin of a person or institution via text-based IE from the Web. We evaluate all methods on a collection of music artists and bands, and show that some heuristics outperform earlier work on the topic by terms of coverage, while retaining similar precision levels. We further investigate an extension using country-specific synonym lists.
Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing I.7.m [Document and Text Processing]: Web Mining
General Terms: Algorithms, Measurement
Keywords: information extraction, country of origin detection, term weighting, music information research, evaluation
1. INTRODUCTION AND CONTEXT
The country of origin of a person or institution represents an interesting aspect of his/her/its life/existence. It plays a vital role in understanding a person's background and context. As a semantic descriptor, the country of origin of a music artist or band, both of which we will refer to as "artist" in the following, can be used to query music collections based on learned associations between acoustic features and textual features ­ cf. [2, 6]
The "country of origin" of an artist is defined as the country where he or she was born, or the band was founded. What makes this task a challenge is foremost that the origin is neither always unambiguous, nor well-known. Consider, for example, Farrokh Bulsara, later known as Freddie Mercury. He was born in Zanzibar, Tanzania. However, he relocated to the UK at the age of 17, where he later became world famous as co-founder of the band Queen. Mercury's country of origin is nevertheless Tanzania, whereas Queen's is the UK. This example highlights the problem of determining the origin in cases where the main country of musical activity differs from the place of birth.
Earlier work on predicting the origin of a music artist mainly consists of [3, 4]. In contrast to our approaches that may ­ at least in theory ­ use the whole Web as data source, Govaerts and Duval focus on specific Web sites, such as last.fm, Wikipedia, and Freebase, and apply heuristic func-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

tions on their textual content. For evaluation they use a commercial, non publicly available set of artists, which has been manually annotated by music experts.

2. HEURISTICS
The first and simplest heuristic investigates estimates of search engine's page counts for queries containing the artist to be classified and the country name. We use Google's index since this engine has already proven to yield respectable results for artist-to-genre classification tasks based on weighted term features ­ cf. [5]. To mitigate the distortions arising from artist names that equal common speech words1, we employ the query scheme "artist " "country " music. We finally predict the country whose page count is highest for a given artist, formally maxc pc(artist, c) c  C, where C is the set of country names.
The second approach applies term weighting functions [1] to the textual content of the 100 top-ranked Web pages retrieved from Google's index for the person under consideration. We use the following term weighting measures since they are well founded in IR research: document frequency (df ), term frequency (tf ), and tf · idf of the country terms in the set of artist-related Web pages. We conducted experiments with various tf · idf variants and found that the following seems to be suited best for this particular task:

tf · idft,a = ln (1 + tft,a) · ln

1

+

n dft

In this formulation tft,a denotes the number of occurrences of term t in the 100-page-set retrieved for artist a, dft represents the number of pages where t occurs among the com-

plete set of all Web pages retrieved, and n is the total num-

ber of pages retrieved. The origin of an artist a is then deter-

mined by predicting the country whose name ranks highest

with respect to the employed term weighting function.

The third approach uses text distance measures between

country names and origin-related key terms, such as "born",

"founded", "origin", or "country", on the set of top-ranked

Web pages. Based on the offset at character-level between

the country terms and the origin-related key terms in a's

pages, we build a model of a's most likely country of ori-

gin. The core part of this model integrates two different

functions: a distance measure on the intra-page-level (ipl)

to determine the distances within a Web page of a, and an

aggregation function (af) to combine the ipl-distances for all

pages retrieved for a. The choice of these two functions is

1Examples of such artists are "Bush", "Prince", and "Kiss".

801

vital to the quality of the prediction. For the evaluation experiments described next, we use the following scheme to describe a setting: {key1, · · · , keyn}, ipl, af .
Using Country Synonyms.
We further looked into using synonyms for countries and nationalities, extracted from Thesaurus.com. A complete list of the used mappings country  syn1, · · · , synn is available.2 This step is motivated by the fact that certain countries, such as the "United States" (of America), are often wrongly predicted due to their ambiguity, and abundant presence on the Web.
3. EVALUATION AND DISCUSSION
Since there exists no standardized data set for this kind of task and we did not have access to the one used in [3], we manually extracted 578 artists and their country of origin from sources such as allmusic.com, last.fm, and Wikipedia.3 We included artists from 69 distinct countries of the world.
Table 1 shows the evaluation results in terms of coverage (or recall ), precision, and F-measure [7]. Coverage denotes the share of artists for which a country could be determined, precision is the share of artists whose origin is correctly predicted among the number of artists for which a prediction was made, and the F-measure aggregates precision and recall via the weighted harmonic mean. The best-performing approaches within each category are printed in bold.
The page counts approach seems to be too simple to capture the country of origin correctly. The term weighting approaches yield overall the best results. Interestingly, tf and df measures outperform tf · idf . Even though tf · idf is the standard approach in text-based IR, it underperforms in this specific IE task. This is likely a result of tf · idf 's penalization of terms that occur in a large number of documents. Suppressing such terms does make sense in most IR tasks. In our IE task, however, general and popular terms should not be given less weight. The text distance approach performs worse than expected. The reason for this bad performance may be an unfavorable set of key terms. We will investigate this as part of future work.
Table 2 shows the best evaluation results from Govaerts and Duval [3]. Comparing Tables 1 and 2, our approaches perform, in general, better with respect to coverage and Fmeasure. In terms of precision, the picture is more diversified. While Govaerts and Duval's combined method reaches about 77% precision (at a 59%-recall-level), our best method in terms of precision achieves about 71% (but at a 100%recall-level).
Synonyms significantly impact the obtained results.
Employing the Wilcoxon signed-rank test on each pair of approaches (with and without synonyms) revealed significant difference for tf · idf -based approaches. Furthermore, three of the approaches based on text distances perform significantly worse if synonyms are used. This may be explained by ambiguous synonyms, such as "US" or "Sam".
Significant differences between the three heuristics.
Friedman's two-way analysis of variance revealed highly significant differences between all categories of approaches. We further employed the post-hoc Wilcoxon signed-rank test to analyze which settings differ within their category. In Table 1 the settings that significantly differ from the best per-
2http://www.cp.jku.at/people/schedl/music/countries syn.txt 3http://www.cp.jku.at/people/schedl/music/C578a country.txt

Approach

C (%) P(%)

Google

Page counts 100.00 23.18

df tf tf · idf

Term weighting (without synonyms)
100.00 65.57 100.00 68.86 100.00 63.49

df
tf tf · idf

Term weighting (with synonyms)
100.00 66.09 100.00 70.76 100.00 59.34

Text distance (without synonyms)

{born}, min, min

100.00 34.08

{born, founded}, min, min 100.00 37.20

{born}, avg, min

100.00 14.19

{born, founded}, avg, min

100.00 14.19

Text distance (with synonyms)

{born}, min, min

100.00 29.41

{born, founded}, min, min 100.00 32.53

{born}, avg, min

100.00 12.11

{born, founded}, avg, min

100.00 12.46

F
37.64
79.21 81.56 77.67
79.58 82.88 74.48
50.84 54.22 24.85 24.85
45.45 49.09 21.60 22.15

Table 1: Evaluation results of our approaches.

Approach
last.fm origin freebase origin freebase most freq wikipedia most freq combined method

C (%)
7.19 21.37 26.20 55.76 59.12

P(%)
89.58 90.85 91.60 64.63 77.09

F
13.13 34.60 40.75 59.87 66.92

Table 2: Best evaluation results from [3].

forming setting in each group are marked in italics. Except for the term weighting group without synonyms, where no significant difference between df and tf could be determined, the performance of the best setting is always significantly different from all others.
4. CONCLUSIONS
We presented three parameterizable heuristics to determine the origin of a person or institution and applied these heuristics with different settings to a set of music artists and bands. We were able to outperform earlier work in terms of coverage and F-measure, while retaining precision levels. Future work will include refining our methods by combining them with NLP techniques or estimates of Web page reputation.
Acknowledgments
This research is supported by the Austrian Fonds zur F¨orderung der Wissenschaftlichen Forschung (FWF) under project numbers L511-N15 and Z159.
5. REFERENCES
[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley, 1999.
[2] D. Turnbull et al. Towards Musical Query-by-Semantic Description using the CAL500 Data Set. In Proc of 30th ACM SIGIR, Jul 2007.
[3] S. Govaerts and E. Duval. A Web-based Approach to Determine the Origin of an Artist. In Proc of 10th ISMIR, Oct 2009.
[4] M. Schedl et al. Country of Origin Determination via Web Mining Technique. In Proc of 2nd AdMIRe, Jul 2010.
[5] P. Knees et al. Artist Classification with Web-based Data. In Proc of 5th ISMIR, Oct 2004.
[6] P. Knees et al. A Music Search Engine Built upon Audio-based and Web-based Similarity Measures. In Proc of 30th ACM SIGIR, Jul 2007.
[7] C. J. van Rijsbergen. Information Retrieval. Butterworth-Heinemann, London, UK, 2nd ed., 1979.

802

Exploiting Click-Through Data for Entity Retrieval
Bodo Billerbeck, Gianluca Demartini, Claudiu S. Firan, Tereza Iofciu, Ralf Krestel
 Microsoft Research, Cambridge, United Kingdom  L3S Research Center, Hannover, Germany
bodob@microsoft.com,{demartini,firan,iofciu,krestel}@L3S.de

ABSTRACT
We present an approach for answering Entity Retrieval queries using click-through information in query log data from a commercial Web search engine. We compare results using click graphs and session graphs and present an evaluation test set making use of Wikipedia "List of" pages.
Categories and Subject Descriptors: H.3.3 [Infor-
mation Search and Retrieval]: Retrieval Models. General
Terms: Algorithms, Experimentation. Keywords: Entity
Retrieval, Evaluation, User Session, Query Log Analysis, Wikipedia.
1. INTRODUCTION
Current Web search engines retrieve textually relevant Web pages for a given keyword query even if the information need targets entities. The idea behind Entity Retrieval (ER) is to find entities directly. Instead of the user browsing through all Web pages retrieved by the search engine, a list of relevant entities can be presented to the user. This not only saves the user's time but also improves search experience. Consider queries that should return a list of entities and the difficulty of compiling such a list based on keyword search results. A user looking for a list of "films shot in Venice" or "Spanish fish dishes" will have difficulties to find suitable answers. They have to manually compile the result list by extracting entities from the retrieved documents.
The idea of specifically developing a system for ER has been explored before (e.g., [2], [4]). In [1] the authors describe how to model user search behaviour exploring session data and in [5] methods are presented for named entity mining from query logs using Latent Dirichlet Allocation.
We apply the results of query log analysis to ER by performing random walks on click and session graphs. In [3] random walks are described on click graphs, containing information about clicked URLs but not about user sessions. The authors show how click graphs can be used to improve ranking of image search results. Our approach for ER extends this idea by also taking into account session data mined from the search query log. Thus we make use of the hidden semantic value of user session data to find relevant entities for a given ER query. We compare the usefulness of session data and click-through data for the ER task.

2. ENTITY SEARCH ON SESSION AND CLICK GRAPHS
Given an ER query we want to find all relevant entities and display them as a ranked list. Our hypothesis is that users posing an ER query which does not yield satisfying results will reformulate the query to find useful information. A reformulated query often consists of an instance of the group of entities the user is looking for, e.g. "Spanish fish dishes" and "Paella". This is not necessarily an ordered process but these kinds of co-occurrences can be found in user session logs nonetheless. We collect session data from a Web search engine query log and we use it to build a session graph containing each user query as a node. Two queries posed in the same 10 minute user session are connected. The direction of the edges goes from the earlier query to the more recent ones. Each of these edges is then weighted depending on the frequency of co-occurrence within different users sessions. In the second step we perform a random walk over the graph starting from a given ER query up to n steps away.
We also build a click graph, where a link between a query and a URL is established if the URL is ranked in response to the query and clicked by at least one user. Our click graph is the result of applying a Markov random walk to a large click log, as described in [3]. Similarly to the session graph, the random walk is performed for n steps away from the starting node: At search time, the given ER query is matched in the graph and set as starting node. We then perform a random walk over the graph, using query-URLquery transitions associated with weights on the edges (i.e. click frequencies) as shown in Figure 1.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: An example Click Graph connecting an ER query q with other queries (i.e. entities) ql,i via URLs ul.
803

We compare our approaches for ranking entities using a click graph, a session graph and their intersection:
(1) RWL5 - session. Starting from an ER query, walk to all queries (interpreted as entities) reachable in 5 steps and rank all the reached queries (interpreted as entities) in the session graph by their random walk probability ([3]), but keep only queries one step away from the original query in the session graph (level 1);
(2) RWL2 - session. Similarly to RW L5-session, with the random walk performed only on the first 2 levels on the session graph. That is, this approach does not explore the session graph further away than queries at level 2.
(3) RWL2 - click. Rank all the reached queries in the click graph by their random walk probability 2 steps (a step is marked for each query) away, but keep only queries closest to the original query in the click graph (i.e., one URL away from the original query);
(4) RWL2 loop - click. Similar to RW L2 - click, but keep only queries which can be reached by multiple paths starting from the given ER query (i.e. those that are reinforced by URLs at deeper levels) ­ this would keep only q1:2 and q1:3 in Figure 1. A level 1 query q1:j is considered to be reinforced as long as the path from the origin going through a different level 1 query comes back to the query q1:j.
(5) RWL2 - intersection. After computing the intersection of the click graph and the session graph, rank all the reached queries (interpreted as entities) by their random walk probability; the random walk is performed only on the first 2 levels on each of the two graphs and only queries closest to the original one are kept.
The choice of path lengths of walks can have a large impact on the results. For this poster we experimented with path lengths of 2 and 5 steps. We plan to evaluate the effect of different path lengths more exhaustively in future.
3. EXPERIMENTS
In our experiments we use two different sets of data: (1) Query log data from the Bing search engine, and (2) queries and answers collected from Wikipedia for evaluating our approach. Both the session and click graph were built using query log data which consists of US American English language user sessions and was collected over a period of 10 months. The session graph is made up of 18 million unique queries and 65 million edges, while the click graph contains 35 million queries, 44 million URLs and 121 million edges between them.
As gold standard for the evaluation we use the "List of" pages from Wikipedia. The title of such a page, after removing the first two words, is used as an ER query (e.g., "-- Lis-t o--f Presidents of the United States"). The titles of the Wikipedia pages that are linked to from such a "List of" page are considered to be relevant results (e.g., "Barack Obama", "George W. Bush", etc.). In order to use only queries that are more similar to typical Web queries in terms of length, we keep only those queries that consist of 2 or 3 words apart from "List of". Thus we have 17,110 pages out of the total of 46,867 non-redirect "List of" pages. We match these titles to queries in the log (exact string match) and keep only the 82 queries which were posed at least 100 times and attracted at least 5 clicks on results1. In order to compare the different ranking approaches, we compute MAP and R-Precision
1The test set of Wikipedia titles and relevant entities is available from http://www.l3s.de/~iofciu/wikipediaER/

Method RW L5 - session RW L2 - session RW L2 - click RW L2 loop - click RW L2 - intersection

MAP 0.2372 0.2450 0.1862+ 0.1911+ 0.3462+

P10
0.1175 0.1173 0.0483+ 0.0545+ 0.1918+

Retrieved 60 61
531 399
21

Table 1: Results for finding entities using click and session graphs. * indicates statistical significant difference with RW L2 - click, + with RW L2 - session (paired t-test, p < 0.05).

of the produced rankings. For the purpose of evaluation we stem both the retrieved queries and the relevant results. Furthermore, we consider the first ER query in each ranked list as relevant if it contains (as a substring) any entry in the respective "List of" page. Because of this, the paper at hand should be viewed as a head-room experiment; in future work we plan to extract entities from queries before matching these to the target set for the purpose of evaluating our approach. As a side note, according to the definitions above, the session and click graphs used for this experiment cover roughly 60% and 75% of relevant entities, respectively.

4. DISCUSSION AND CONCLUSIONS
We can see in Table 1 that the approach based on reinforcement improves over a standard random walk in the click graph. Performing a random walk and ranking the queries by their respective probabilities works better using the session graph than the click graph. This can be explained because users typically start a search session by posting a generic query such as "Spanish fish dish" and refining it with "Spanish fish dish paella". Additionally, we can see that walking only two levels yields even better performances (as most of the relevant results are one step away from the original ER query) which is also computationally more efficient. Interestingly, using the session graph retrieves overall less entities but more relevant ones per query. This shows how using the session graph is a more suited approach as the average Web user would not browse hundreds of results. Finally, we can see that when computing the intersection of the two graphs we obtain best effectiveness. This means that results contained in both graphs are mainly relevant ones. Moreover, the number of retrieved results is reduced to a minimum which is realistic for the average Web user. This proves that large query logs from Web search engines can be successfully used for the emerging task of Entity Retrieval. A combination of the methods presented here with traditional IR is definitely worth investigating.

5. REFERENCES
[1] Ricardo Baeza-Yates and Alessandro Tiberi. Extracting semantic relations from query logs. In KDD, 2007.
[2] Tao Cheng, Xifeng Yan, and Kevin Chen-Chuan Chang. Entityrank: searching entities directly and holistically. In VLDB, 2007.
[3] Nick Craswell and Martin Szummer. Random walks on the click graph. In SIGIR, 2007.
[4] Desislava Petkova and W. Bruce Croft. Proximity-based document representation for named entity retrieval. In CIKM, 2007.
[5] Gu Xu, Shuang-Hong Yang, and Hang Li. Named entity mining from click-through data using weakly supervised latent dirichlet allocation. In KDD, 2009.

804

Exploring the Use of Labels to Shortcut Search Trails

Ryen W. White and Raman Chandrasekar
Microsoft Research One Microsoft Way, Redmond, WA 98052 USA
{ryenw, ramanc}@microsoft.com

ABSTRACT
Search trails comprising queries and Web page views are created as searchers engage in information-seeking activity online. During known-item search (where the objective may be to locate a target Web page), searchers may waste valuable time repeatedly reformulating queries as they attempt to locate an elusive page. Trail shortcuts help users bypass unnecessary queries and get them to their desired destination faster. In this poster we present a comparative oracle study of techniques to shortcut sub-optimal search trails using labels derived from social bookmarking, anchor text, query logs, and a human-computation game. We show that labels can help users reach target pages efficiently, that the label sources perform differently, and that shortcuts are potentially most useful when the target is challenging to find.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process, selection process
General Terms
Measurement, Design, Experimentation, Human Factors
Keywords
Trails, labels, click graph, anchor text, social bookmarks
1. INTRODUCTION
In recent years, there has been significant interest in using social media and community behavior to improve Web search. Annotations from social bookmarking sites [2], anchor text garnered from Web crawls [3], and query logs from search engines [7] (collectively referred to here as labels) have been included as additional content for result ranking. However, labels may also be useful for other purposes such as query recommendations [1,5] to help users experiencing difficulty in formulating queries required to find specific pages, perhaps due to vocabulary mismatches [4].
In this poster, we present a log-based study on the effectiveness of labels as navigational shortcuts to reduce the average number of query refinements in search trails and to help users get to a destination page in fewer queries. Previous work has shown that popular destinations can help users search more effectively [8]. We extend that research to support searchers in situations where they may struggle to find a destination page and perform a large number of query refinements. A log-based methodology allowed us to carefully compare methods for shortcutting such sub-optimal trails using large numbers of real searching episodes providing evidence of users targeting specific Web pages. Our study answers two questions: (i) can labels reduce the average number of query refinements to reach a desired destination page? and (ii) if labels do help, how do the label sources compare?
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. LABEL SOURCES
Trail shortcuts use labels assigned to target Web pages to help users bypass unnecessary intermediate queries appearing between the first and the last queries in the trail. There are many sources from which we could derive page labels. We selected four representative sources for comparison in this study: (i) query data from a human computation game, (ii) queries from a commercial Web search engine click-graph, (iii) anchor text extracted from a commercial search engine index, and (iv) social bookmarks. We now describe each of the label sources in more detail.
Page Hunt Queries: Page Hunt [6] is a human computation game operating like search in reverse. In the game, the player is shown a random Web page, and challenged to construct a query that would return this page as one of the top results on a search engine. In effect, the player is identifying labels for the Web page shown to her. The Page Hunt query dataset has information about 577 URLs (henceforth referred to as Page Hunt URLs) and 23,807 queries related to these URLs, where the user successfully found the target page. As part of that work, the authors defined a metric called findability, which measures how easy it is for users to find a given page ­ if everyone can hunt down a page, it has 100% findability and if no one is able to find it, it has 0% findability.
Click-graph Queries: A click graph is a bipartite graph of search engine users' queries and the result URLs they clicked on, represented as a set of triples < , , >, where URL was clicked times by users when they issued a query . From a click graph collected from 18 months of Web search engine logs, where each URL had at least five clicks, we extracted all click data available for the Page Hunt URLs. This allowed us to create the Click-graph dataset consisting of 546 URLs and 491,689 queries and counts associated with these URLs.
Anchor Text: Anchor text refers to the visible, clickable text (often underlined) in a hyperlink on a Web page. Anchor text is used by Web site authors to provide a contextually relevant description or label for the Web page (landing page) it is linked to. Anchor text is used by search engines as additional metadata to rank landing pages. For our experiment, we identified the anchor text for the Page Hunt URLs from crawls performed by a major web search engine. We found label and count data for 512 of these URLs giving us 77,663 rows of anchor text data.
Social Bookmarks: Social bookmarking services allow users to store bookmarks for Web sites, and share and discover other bookmarks. One example of such a service is delicious.com. Users of the service can bookmark any site with tags and get to the site using those tags. Users can send bookmarks to others, keep track of users and tags, and view popular tags and sites. In this experiment, we downloaded all the tags available from delicious.com for the Page Hunt URLs using their programmatic interface; this gave us 6,787 labels corresponding to 364 URLs.

811

3. EXPERIMENT
We performed an oracle study to determine if query shortcuts created from label sources help users reach their goal faster.
In our study we use search trails comprising queries and pages defined as in [9]. Trails were mined from six months of log data from consenting users of a widely-distributed browser toolbar. The information in these log entries includes a unique user identifier, a timestamp for each page view, and the URL of the web page visited. Intranet and secure (https) URL visits were excluded at source. Only entries generated in the English speaking United States locale were included. From these data, we extract millions of search trails where the Page Hunt URLs were visited.
To test each of the label sources, we first selected trails: (i) that had at least three queries, offering some scope for shortcuts; (ii) where all consecutive queries shared at least one term (signifying information need consistency in a similar way to query chains [7]); (iii) where queries did not contain spelling errors (to avoid situations where shortcuts may not help); (iv) that had no page visit until the last query (signaling potential dissatisfaction with all but the last query), and (v) where the last query led immediately to a visit to a Page Hunt URL. This gave us a test set of trails around 5% the size of the original sample. The average number of queries in these trails was 3.61 (median=3). Thus there was an opportunity to save users at least one query on average and perhaps more queries for pages that were more challenging to find. From the filtered trail set we created ten samples of ten thousand randomly-selected trails. Within each set we do the following:
For each label source , for a destination URL :
a. Select the top-20 most frequent labels for from . b. Identify , the set of trails that have as destination URL. c. Extract , the set of queries on each trail in that is not the
last query in the trail. d. For each query in (starting with the first query and
proceeding in temporal order), check if a query obtained by appending a top-20 label from or one of the top-20 labels from by itself matches the last query in the trail. e. If there is a match, compute the distance saved in terms of the number of unsuccessful queries skipped.
The number of steps saved is averaged across all trails and runs.
4. FINDINGS
We present findings on the number of trail queries saved over all label sources and then broken down by source and by the findability of the destination page. Table 1 shows performance metrics for each labeling source and a combination (Best) that picks the most performant source for each trail based on the number of queries each source saves (randomly selecting a source in the case of ties). We report the average number of queries saved over trails where at least one label source helps (which is 27.3% of all trails sampled), as well as source coverage over those sampled trails. Also shown is the fraction of the ideal number of queries that could be saved in each trail (e.g., if a trail has four queries, then the ideal number saved is two, jumping from first to last).
The findings suggest that if presented as shortcuts, labels could shorten sub-optimal search trails by around two queries for almost 20% of such trails (bottom row of Table 1). The findings also show that anchor text saved a greater fraction of possible queries and that the click-graph covered more of the sampled trails. Similar trends in the results were observed across all trails (not just

those where at least one source helped) and for those trails where all sources offered a shortcut. Additional analysis was conducted for destinations with high (> 40%) and low findability scores. The results, summarized in Table 1, show that for Best, on average, 2.65 steps were saved for pages with low findability, versus 1.48 steps for pages with high findability; a trend mirrored by the sources individually. Thus trail shortcuts may help users more when they seek hard-to-find Web pages. Given the large sample sizes, all differences between sources were significant at < .01 with ANOVA and Tukey post-hoc testing where appropriate.
Table 1. Number of queries saved and percentage of ideal for all saved trails and trails with high/low destination findability.

Source
Page Hunt Click graph Anchor text Bookmarks Best (of all)

All destinations

Avg. num. queries saved
(% ideal)

% cov.

1.58 (90.9) 10.7 1.83 (89.9) 15.8 1.65 (93.1) 12.0 1.54 (81.4) 5.6 1.97 (95.2) 19.6

High find. Low find.
Avg. num. Avg. num. queries saved queries saved
(% ideal) (% ideal) 1.27 (90.5) 2.01 (91.6) 1.43 (89.3) 2.39 (90.7) 1.20 (93.0) 2.26 (93.2) 1.19 (81.2) 2.02 (81.6)
1.48 (94.8) 2.65 (95.4)

5. CONCLUSIONS AND FUTURE WORK
We have presented a study of using labels to shortcut search trails, in particular sub-optimal trails typified by multiple query reformulations. Findings show that labels could help users search more efficiently, especially when the target page is hard to find. When a search engine receives a query, the most frequent label assigned to pages in the result set could be shown as a shortcut on the result page. Future work will perform a more extensive analysis of the reported differences, study when adding a new term or substituting the query is more appropriate, use frequently-followed query chains extracted from log data for shortcut generation, and investigate the use of shortcuts for tasks beyond known-item search.
REFERENCES
[1] Anick, P.G. & Tipirneni, S. (1999). The paraphrase search assistant: terminological feedback for iterative information seeking. SIGIR, 153-159.
[2] Bao, S., Xue, G., Wu, X., Yu, Y., Fei, B. & Su, Z (2007). Optimizing web search using social annotations. WWW, 501-510.
[3] Craswell, N., Hawking, D. & Robertson, S.E. (2001). Effective site finding using link information. SIGIR, 250-257.
[4] Furnas, G.W., Landauer, T.K., Gomez, L.M. & Dumais, S.T. (1987). The vocabulary problem in human-system communication. CACM, 30(11): 964-971.
[5] Kraft, R. & Zien, J.Y. (2004). Mining anchor text for query refinement. WWW, 666-674.
[6] Ma, H., Chandrasekar, R., Quirk, C. & Gupta, A. (2009). Improving search engines using human computation games. CIKM, 275-284.
[7] Radlinski, F. & Joachims, T. (2005). Query chains: learning to rank from implicit feedback. SIGKDD, 239-248.
[8] White, R.W., Cucerzan, S. & Bilenko, M. (2007). Studying the use of popular destinations to enhance web search. SIGIR, 159-166.
[9] White, R.W. & Drucker, S. (2007). Investigating behavioral variability in web search. WWW, 21-30.

812

From Fusion to Re-ranking: a Semantic Approach

Annalina Caputo acaputo@di.uniba.it

Pierpaolo Basile basilepp@di.uniba.it
Department of Computer Science University of Bari "Aldo Moro" 70125 Bari, Italy

Giovanni Semeraro semeraro@di.uniba.it

ABSTRACT
A number of works have shown that the aggregation of several Information Retrieval (IR) systems works better than each system working individually. Nevertheless, early investigation in the context of CLEF Robust-WSD task, in which semantics is involved, showed that aggregation strategies achieve only slight improvements. This paper proposes a re-ranking approach which relies on inter-document similarities. The novelty of our idea is twofold: the output of a semantic based IR system is exploited to re-weigh documents and a new strategy based on Semantic Vectors is used to compute inter-document similarities.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: Re-ranking, Semantics, WordNet
1. BACKGROUND AND MOTIVATION
To date, many semantic approaches to IR have been developed. These approaches tackle the word ambiguity problem by shifting from a lexical space towards a semantic one. Among the most investigated techniques are those that rely on WordNet1 synsets through which groups of synonym words are uniquely identified and linked to each other by semantic relations. Ranging from query expansion to concept representation of documents by synset indexing, these attempts have not shown a turning point with respect to classical techniques. Further investigations were performed in the context of Robust-WSD task at Cross Language Evaluation Forum (CLEF). Systems which achieved the best performance in the last two campaigns [6, 3, 14] adopted strategies based on ranking aggregation. Ranking aggregation methods [7, 9] are founded on the idea that different retrieval methods find different sets of documents with small overlap in both relevant and non-relevant documents sets. Thus, fusing all these sets in a single list of ranked documents should result in the best performance. Although the usage of this kind of strategy showed at times slight improvements, in most cases they are not significant. This paper presents a different approach to document aggregation based on a variation of the "inter-document similarities" [8, 10] idea. We combine two retrieval strategies that work at two different
1A semantic lexicon for the English language.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

representation levels: keyword and synset. The ranked list of documents retrieved using the synset-based representation (synset list) is exploited to re-rank the list of documents retrieved using the keyword-based one (keyword list). The insight of this method is that documents in the keyword list with the highest number of similar documents in the synset list should climb in the result set. To compute interdocument similarities we use Semantic Vectors [13] which rely on the WordSpace model. In that model, documents and words are represented by points in a vector space in such a way that words/documents related to each other are close in that space.

2. METHODOLOGY
Following the cluster hypothesis [12] in which "closely associated documents tend to be relevant to the same requests", our approach tries to re-weigh documents in response to a query promoting those documents with the highest number of supporters. In this context, a supporter is a document with content similar to the target one.
Let us denote by Lk and Ls the ranked lists of documents retrieved using keywords and synsets representation, respectively. The idea behind our method is to give more evidence to the documents in Lk that are widely supported by similar documents occurring in both lists. The method requires the following steps:

1. For each document di  Lk we compute the supporters (di, ), which is the set of  documents {d1, ...d}  Lk with the highest inter-document similarity to di.

2. We get the overlap supporters = {dj  Ls : dj  supporters(di, )} which is the set of documents occurring in both Ls and supporters.

3. We assign to di a new score S(di) taking into account supporting documents computed in the step 2. Formally:

S(di) =   Ssupporters + (1 - )  Sk(di) (1)

where

Ssupporters =

X

Sk(dj)  Ss(dj) (2)

dj overlap supporters

and Sk(dj) is the score of dj in Lk, while Ss(dj) is the score of dj in Ls, and  is a free parameter used to smooth Ssupporters, which denotes the scores combination of supporting documents.

815

To compute the inter-document similarities we build a vector space (DocSpace) where similar documents are represented by close vectors by means of the Semantic Vectors package [13]. To build the DocSpace, Semantic Vectors rely on a technique called Random Indexing [4], which performs a matrix reduction of the term-document matrix. Hence, in the DocSpace the similarity between documents is computed by the traditional cosine similarity.
3. EVALUATION AND REMARKS
Our evaluation aims to establish if a synset-based retrieval system brings to significative improvements in IR when it is exploited by a re-ranking approach based on inter-document similarities.
The evaluation is carried out on the CLEF 2009 Ad-Hoc Robust WSD dataset [2]. The document collection is made up of LA Times 94 and Glasgow Herald 95 newspaper documents. Each user's information need is expressed by a topic, a structured sentence consisting of a title, a description and a narrative field. The benchmark supplies 150 topics for the training step and 160 for the test step. Documents and topics are automatically annotated with WordNet synsets using two state-of-the art systems [1, 5]. We built a retrieval system based on the Okapi BM25 model for both levels of representation: keyword and synset. Stemming and stop word removal were applied to keyword-based representation of documents and topics, but a different list of stop words was used for topics in order to remove frequent words which are poorly discriminating. Queries were built exploiting all topic fields and using different boosting factors to adjust their impacts on the result set (title=8, narrative=2, description=2). Hence, we retrieved the Lk and Ls lists of ranked documents. The evaluation was performed using the MAP and GMAP measures. Table 1 summarizes the main results. Foremost, we evaluated each system alone (Keyword and Synset). Keyword was used as baseline of the evaluation. Then, we evaluated two aggregation strategies, CombM N Z and CombSU M [7]. In particular we adopted a modified version of those strategies to assign different weights to each list during the aggregation. Finally, the result of the proposed method has been denoted by ReRank. After a tuning step, we set the weights for Lk and Ls to 0.8 and 0.2, respectively. Moreover, we tested several values of   {0.2, 0.3, . . . , 0.9} and   {10, 20, 30}. The number of supporters  was set to 20 and the smoothing parameter  was set to 0.3. Tuning was performed using training topics provided by the CLEF organizers.
The ReRank method achieves the best results in term of MAP and GMAP. These improvements are significant with respect to both baseline Keyword and aggregation methods. We validated our experiments using both the parametric Student paired t-test and the non parametric Randomization test as suggested in [11] ( = 5%). Results confirm our hypothesis: the ranking provided by synsets (Ls) contributes significantly to the final document score. In fact, to point up our result, we proposed another experiment in which only the keyword list was exploited. In this experiment the supporters score was computed using only Lk list and we obtained a MAP of 0.3677.
4. REFERENCES
[1] E. Agirre and O. L. de Lacalle. UBC-ALM: combining k-NN with SVD for WSD. In SemEval '07: Proc. of the 4th

Table 1: Experimental Results (figures in boldface

are statistically significant)

Exp

MAP GMAP

Keyword Synset

0.4205 0.1900 0.3201 0.1242

CombSU M 0.4252 0.1972 CombM N Z 0.4238 0.1969

ReRank

0.4332 0.1989

Int. Workshop on Semantic Evaluations, pages 342­345. ACL, 2007.
[2] E. Agirre, G. M. D. Nunzio, N. Ferro, T. Mandl, and C. Peters. CLEF 2008: Ad Hoc Track Overview. In Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17-19, 2008, Revised Selected Papers, volume 5706 of LNCS, pages 15­37. Springer, 2009.
[3] P. Basile, A. Caputo, and G. Semeraro. UNIBA-SENSE @ CLEF 2009: Robust WSD task. In Working Notes for the CLEF 2009 Workshop, 2009.
[4] E. Bingham and H. Mannila. Random projection in dimensionality reduction: Applications to image and text data. In KDD '01: Proc. of the 7th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data mining, pages 245­250. ACM, 2001.
[5] Y. S. Chan, H. T. Ng, and Z. Zhong. NUS-PT: exploiting parallel texts for word sense disambiguation in the English all-words tasks. In SemEval '07: Proc. of the 4th Int. Workshop on Semantic Evaluations, pages 253­256. ACL, 2007.
[6] L. Dolamic, C. Fautsch, and J. Savoy. UniNE at CLEF 2008: TEL, Persian and Robust IR. In Working Notes for the CLEF 2008 Workshop, 2008.
[7] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of the 2nd Text REtrieval Conference (TREC-2), pages 243­252, 1994.
[8] A. K. Kozorovitzky and O. Kurland. From "identical" to "similar": Fusing retrieved lists based on inter-document similarities. In Advances in Information Retrieval Theory, 2nd Int. Conf. on the Theory of Information Retrieval, ICTIR 2009, Cambridge, UK, September 10-12, 2009, Proceedings, volume 5766 of LNCS, pages 212­223. Springer, 2009.
[9] J. H. Lee. Analyses of multiple evidence combination. In SIGIR '97: Proc. of the 20th annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 267­276. ACM, 1997.
[10] L. Meister, O. Kurland, and I. Kalmanovich. Two are better than one! Re-ranking search results using an additional retrieved list. Technical report IE/IS-2009-01, Technion, 2009.
[11] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In CIKM '07: Proc. of the 16th ACM Conf. on Information and Knowledge Management, pages 623­632. ACM, 2007.
[12] C. J. van Rijsbergen. Information Retrieval. Butterworth, 1979.
[13] D. Widdows and K. Ferraro. Semantic Vectors: a scalable open source package and online technology management application. In Proc. of the 6th Int. Language Resources and Evaluation (LREC'08). ELRA, 2008.
[14] E. Wolf, D. Bernhard, and I. Gurevych. Combining probabilistic and translation-based models for information retrieval based on word sense annotations. In Working Notes for the CLEF 2009 Workshop, 2009.

816

Ontology-enriched Multi-Document Summarization in Disaster Management

Lei Li, Dingding Wang, Chao Shen, Tao Li
School of Computing and Information Sciences Florida International University Miami, FL 33199
{lli003, dwang003, cshen001, taoli}@cs.fiu.edu

ABSTRACT
In this poster, we propose a novel document summarization approach named Ontology-enriched M ulti-Document S ummarization(OMS ) for utilizing background knowledge to improve summarization results. OMS first maps the sentences of input documents onto an ontology, then links the given query to a specific node in the ontology, and finally extracts the summary from the sentences in the subtree rooted at the query node. By using the domain-related ontology, OMS can better capture the semantic relevance between the query and the sentences, and thus lead to better summarization results. As a byproduct, the final summary generated by OMS can be represented as a tree showing the hierarchical relationships of the extracted sentences. Evaluation results on the collection of press releases by Miami-Dade County Department of Emergency Management during Hurricane Wilma in 2005 demonstrate the efficacy of OMS.
Categories and Subject Descriptors: H.3.3[Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation, Performance
Keywords: Ontology, Multi-Document Summarization, Disaster Management
1. INTRODUCTION
Ontology is a philosophy concept, dealing with questions about what entities exist or can be said to exist, and how such entities can be grouped within a hierarchy, and subdivided according to their similarities and differences. Ontology has been applied in many research areas in information retrieval, particularly, in text mining. For example, D. S´nchez et al use the ontology to compute semantic similarity [1], and I. Yoo et al utilize the ontology to improve document clustering [2]. However, relatively few research efforts have been reported on using the ontology for improving document summarization.
Generally, given a query, multi-document summarization is the process of generating a query-focused/relevant condensation (i.e., a generated summary) of the content of the entire input set. Existing summarization methods usually rank the sentences in the documents according to their scores calculated by a set of predefined features, such as term frequency-inverse sentence frequency (TF-ISF), sentence or term position, and number of keywords [3]. The above anal-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

yses are difficult to capture the hidden semantic relationships between the sentences and queries. Ontology, with abundant concise concepts and rich domain-related information, can capture the hidden semantic information.
In this poster, we develop a novel method, OMS, to generate query-relevant summary from a collection of documents by making use of the ontology. In particular, OMS first links the sentences of documents being considered onto a domain-related ontology, then maps the given query to a specific node in the ontology, and finally extracts the summary from the sentences in the subtree rooted at the corresponding query node, by using FGB, a text summarization approach described in [4]. As a byproduct, the summaries we finally acquire can be represented as a tree showing the hierarchical relationships of the extracted sentences.
We apply OMS to disaster management for evaluation. For natural calamities, such as hurricanes and earthquakes, vast amount of related news and reports are generated through time for broadcasting and recording events. Experimental results on such disaster management demonstrate the efficacy of OMS.
2. FRAMEWORK OVERVIEW
Figure 1 shows the framework of OMS. First of all, a domain specific ontology hierarchy was created by domain experts to describe concepts appearing in disaster related document sets. Given a collection of documents related to disasters, we disassemble them into a set of sentences. Then we map these sentences onto the ontology hierarchy based on their semantic correlations, discarding some sentences not relevant to any concept in the ontology, and omitting from the ontology some concepts with less importance.
Up to this point, we obtain an ontology-sentence tree representation in which each node is linked by a set of relevant sentences. Given a query q, OMS links it to a specific node i in the ontology-sentence tree according to the semantic relationship between the query and the nodes, and extracts a sub-hierarchy rooted at i. Then the FGB model [4] is applied to summarize sentences linked to each node in this subtree. Finally, a summary tree that satisfies the query q is achieved.
Ontology Refinement: In the ontology hierarchy, a myriad of concepts relevant to disaster management documents are specified by domain experts. Given a subset of documents, some concepts may not be predominant or even not appear in this documents set. We need to refine the ontology hierarchy so that the ontology-sentence tree representation can better reflect the subject of the document set.

819

To do so, we first rank nodes at the same level in order of sentence counts, then ignore the nodes and their subtrees with sentence counts less than the average. By iteratively running the above procedure in a top-down manner, a thematic ontology-sentence tree representation is generated.

set of documents

Hurricane

block

impair

Public Service People

Transit School Resident Visitor

set of sentences

ontology hierarchy

Sentences

Hurricane

Sentences Public Service People

Sentences

Transit

School Resident Visitor

... Sentences

Ontology-sentence tree representation

Sentences

Transit

Bus

Rail

Airline

Sentences

...

sub-tree representation

query

Summary

Transit

summarize

Bus Rail Airline
... Summary
summary tree representation

Figure 1: OMS Framework

3. EXPERIMENTS
3.1 Real World Data
The document set used in our experiments is a collection of press releases from Miami-Dade County Department of Emergency Management and Homeland Security during Hurricane Wilma from Oct. 19, 2005 to Nov. 4, 2005. It contains approximately 1,700 documents, about half of which contain similar contents. We randomly select 100 documents from this document set as our experiment data.
Data Preprocessing: For the sake of the theme embodiment of ontology-sentence tree representation, some sentences are removed from the whole sentences set in the procedure of sentence mapping, since they have no semantic relationship with any ontology concept node. For example, "For those outside of Miami Dade County can call (305) 4685900 to reach the Answer Center." describes the phone number of the Answer Center; however, this kind of sentences repeatedly appears in most disaster related documents we are considering, and they have no specific meaning for summarization.
3.2 An Illustrative Case Study
In order to illustrate the interpretability of our proposed method, we provide an example of queries and the corresponding result generated by OMS. Figure 2 demonstrates this case study.
Given the query "get all the information related to transit in Miami-Dade County after Hurricane Wilma passed", the result is represented as a summary tree in Figure 2, in which

Transit

Maimi-dade county transit continues to recover from hurricane wilma
along with the rest of the county.

Bus

Rail

Airline

Regular bus service will be implemented as streets are cleared.

Airlines are not expected to resume operations out of miami international airport until late afternoon monday.

Regular weekday service of metrorail will begin Monday.

Figure 2: Summary tree related to transit

the topics in eclipses are domain concepts, and the sentences linked to them are summaries correlating to concepts. We observe that (1) every summary in the summary tree concretely reflects the status of relevant concept; (2) summaries generated by OMS exhibit apparent semantic hierarchy.

3.3 Performance Evaluation
For performance evaluation, 42 queries related to some specific concepts in the ontology are designed by domain experts for experimentation. In order to evaluate the quality of the generated summaries by OMS and other methods, we use human generated summaries as references. For hurricane data, we hire 5 human labelers to manually create summaries based on the selected document set and the given queries. Then we run OMS on such document set; meanwhile, we apply FGB [4] to automatically summarize query-relevant documents without using ontology. The summarization results are evaluated by ROUGE, a document summarization evaluation tool described in [5]. The experiment results are shown in Table 1. The results indicate that summarization efficacy is significantly improved by adopting ontology.

Measure
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-S ROUGE-SU

Using Ontology AVG-R AVG-P AVG-F 0.83236 0.75748 0.78326 0.77324 0.72221 0.74223 0.82032 0.75032 0.77512 0.76362 0.68136 0.70562 0.77299 0.68715 0.71202

Without Ontology AVG-R AVG-P AVG-F 0.56601 0.47562 0.50340 0.43841 0.40197 0.41570 0.53661 0.46032 0.48460 0.43231 0.37311 0.39071 0.44875 0.38025 0.39913

Table 1: Summarization results comparison between OMS and FBG without using ontology. Remark: We use ROUGE-N, ROUGE-L, ROUGE-S and ROUGE-SU, and compare F-scores of the two different methods.

Acknowledgements
The work is partially supported by an FIU Presidential Fellowship and NSF grants IIS-0546280 and HRD-0833093.
4. REFERENCES
[1] D. S´chez, M. Batet, A. Valls, and K. Gibert. Ontology-driven web-based semantic similarity. Intelligent Information Systems, October 2009.
[2] I. Yoo and X. Hu. Clustering large collection of biomedical literature based on ontology-enriched bipartite graph representation and mutual refinement strategy. PAKDD, 2006.
[3] D. Jurafsky and J.H. Martin. Speech and Language Processing. Pearson, second edition, 2008.
[4] D. Wang, S. Zhu, T. Li, Y. Chi, Y. Gong. Integrating Clustering and Multi-Document Summarization to Improve Document Understanding. CIKM, 2008
[5] C. Lin. Rouge: A package for automatic evaluation of summaries. Post-Conference Workshop of ACL, 2004.

820

Caching Search Engine Results over Incremental Indices

Roi Blanco
Yahoo! Research Barcelona, Spain
roi@yahoo-inc.com
Ronny Lempel
Yahoo! Labs Haifa, Israel
rlempel@yahoo-inc.com

Edward Bortnikov
Yahoo! Labs Haifa, Israel
ebortnik@yahoo-inc.com
Luca Telloli
Barcelona Supercomputing Center
Barcelona, Spain
telloli.luca@bsc.es

Flavio P. Junqueira
Yahoo! Research Barcelona, Spain
fpj@yahoo-inc.com
Hugo Zaragoza
Yahoo! Research Barcelona, Spain
hugoz@yahoo-inc.com

ABSTRACT
A Web search engine must update its index periodically to incorporate changes to the Web. We argue in this paper that index updates fundamentally impact the design of search engine result caches, a performance-critical component of modern search engines. Index updates lead to the problem of cache invalidation: invalidating cached entries of queries whose results have changed. Na¨ive approaches, such as flushing the entire cache upon every index update, lead to poor performance and in fact, render caching futile when the frequency of updates is high. Solving the invalidation problem efficiently corresponds to predicting accurately which queries will produce different results if re-evaluated, given the actual changes to the index.
To obtain this property, we propose a framework for developing invalidation predictors and define metrics to evaluate invalidation schemes. We describe concrete predictors using this framework and compare them against a baseline that uses a cache invalidation scheme based on time-to-live (TTL). Evaluation over Wikipedia documents using a query log from the Yahoo! search engine shows that selective invalidation of cached search results can lower the number of unnecessary query evaluations by as much as 30% compared to a baseline scheme, while returning results of similar freshness. In general, our predictors enable fewer unnecessary invalidations and fewer stale results compared to a TTL-only scheme for similar freshness of results.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Performance, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
Search engine caching, Real-time indexing
1. INTRODUCTION
Search engines are often described in the literature as building indices in batch mode. This means that the phases of crawling, indexing and serving queries occur in generations, with generation n + 1 being prepared in a staging area while generation n is live. When generation n + 1 is ready, it replaces generation n. The length of each crawl cycle is measured in weeks, implying that the index may represent data that is several weeks stale [8, 9].
In reality, modern search engines try to keep at least some portions of their index relatively up to date, with latency measured in hours. News search engines, e-commerce sites and enterprise search systems all strive to surface documents in search results within minutes of acquiring those documents (by crawling or ingesting feeds). This is realized by modifying the live index (mostly by append operations) rather than replacing it with the next generation. Such engines are said to have incremental indices.
Caching of search results has long been recognized as an important optimization step in search engines. Its setting is as follows. The engine dedicates some fixed-size fast memory cache that can store up to k search result pages. For each query in the stream of user-submitted search queries, the engine first looks it up in the cache, and if results for that query are stored in the cache - a cache hit - it quickly returns the cached results to the user. Upon a cache miss - when the query's results are not cached - the engine evaluates the query and computes its results. The results are returned to the user, and are also forwarded to the cache. When the cache is not full, it caches the newly computed results. Otherwise, the cache's replacement policy may decide to evict some currently cached set of results to make room for the newly computed set.
An underlying assumption of caching applications is that the same request, when repeated, will result in the same response that was previously computed. Hence returning the cached entry does not degrade the application. This does not hold in incremental indexing situations, where the searchable corpus is constantly being updated and thus the results of any query can potentially change at any time. In such cases, the engine must decide whether to re-evaluate repeated queries, thereby reducing the effectiveness of caching

82

their results, or to save computational resources at the risk of returning stale (outdated) cached entries. Existing search applications apply simple solutions to this dilemma, ranging from performing no caching of search results at all to applying time-to-live (TTL) policies on cached entries so as to ensure worst-case bounds on staleness of results.
Contributions. This paper studies the problem of search results caching over incremental indices. Our goal is to selectively invalidate the cached results only of those queries whose results are actually affected by the updates to the underlying index. Cached results of queries that are unaffected by the index changes will continue to be served. We formulate this as a prediction problem, in which a component that is aware of both the new content being indexed and the contents of the cache, invalidates cached entries it estimates that have become stale. We define metrics by which to measure the performance of these predictions, propose a realizing architecture for incorporating such predictors into search engines, and measure the performance of several prediction policies. Our results indicate that selective invalidation of cached search results can lower the number of queries invalidated unnecessarily by roughly 30% compared to a baseline scheme, while returning results of equal freshness.
Roadmap. The remainder of this paper is organized as follows. Section 2 surveys related work on search results caching and incremental indexing. Section 3 defines the reference architecture on which this work is based. Section 4 presents schemes for selectively invalidating cached search results as the search index ingests new content. We also discuss in this section the metrics we use to evaluate cache invalidation schemes. Section 5 describes the experimental setup and reports our results. We conclude in Section 6.
2. RELATED WORK
Caching of search results was noted as an optimization technique of search engines in the late 1990s by Brin and Page [5]. The first to publish an in-depth study of search results caching was Markatos, in 2001 [17]. He applied classical cache replacement policies (e.g. LRU and variants) on a log of queries submitted to the Excite search engine, and compared the resulting hit-ratios, which peaked around 30%. PDC (Probability Driven Caching) [14] and SDC (Static Dynamic Caching) [10] are caching algorithms specifically tailored to the locality of reference present in search engine query streams, both proposed originally in 2003. PDC divides the cache between an SLRU segment that caches top-n queries, and a priority queue that caches deeper result pages (e.g., results 11-20 of queries). The priority queue estimates the probability of each deep result page to be queried in the near future, and evicts the page least likely to be queried. SDC also divides its cache into two areas, where the first is a read-only (static) cache of results for "head" (perpetually popular) queries, while the second area dynamically caches results for other queries using any replacement policy (e.g. LRU or PDC).
The AC scheme was proposed by Baeza-Yates et al. in 2007 [3]. It applies a predictor that estimates the "repeatability" of each query. Several predictors and the features they rely on were evaluated, showing that this technique is able to outperform SDC.
Gan and Suel [12] study a weighted version of search results caching that optimizes the work involved in evaluating

the cache misses rather than the hit ratios. They argue that different queries incur different computational costs.
Lempel and Moran studied the problem of caching search engine results in the theoretical framework of competitive analysis [15]. For a certain stochastic model of search engine query streams, they showed an online caching algorithm whose expected number of cache misses is no worse than four times that of any online algorithm.
Search results are not the only data cached in search engines. Saraiva et al. [19] proposed a two-level caching scheme that combines caching of search results with the caching of frequently accessed postings lists. Long and Suel extend this idea to also caching intersections of postings lists of pairs of terms that are often co-used in queries[16]. BaezaYates et al. investigate trade-offs between result and posting list caches, and propose a new algorithm for statically caching posting lists that outperform previous ones [2]. It should be noted, however, that in the massively distributed systems that comprise Web search engines, caching of postings lists and caching of search results may not necessarily compete on the RAM resources of the same machine. The work of Skobeltsyn et al. describes the ResIn architecture, which lines up a cache of results and a pruned index [20]. They show that the cache of results shapes the query traffic in ways that impact the performance of previous techniques for index pruning, so assessing such mechanisms in isolation may lead to poor performance for search engines.
The above works do not address what happens to the cached results when the underlying index, over which queries are evaluated, is updated. To this effect, one should distinguish between incremental indexing techniques, that incorporate updates into the "live" index as it is serving queries, and non-incremental settings. Starting with the latter case, we note that large scale systems may choose to not incrementally update their indices due to the large cost of update operations and the interference of incremental updates with the capability to keep serving queries at high rates [18, 7]. Rather, they manage content updates at a higher level.
Shadowing is a common index replacement scheme [1, 8]: while one immutable index is serving queries, a second index is built in the background from newly crawled content. Once the new index is ready, the engine shifts its service from the older index to the newly built one. In this approach, indexed content is fully updated upon a new index generation, and the results cache is often flushed at that time.
Another approach, that performs updates at a finer level of granularity than shadowing, uses stop-press or delta indices [7, 11, 21]. Here, the engine maintains a large main index, which is rebuilt at relatively large intervals, along with a smaller delta index which is rebuilt at a higher rate and reflects the new content that arrived since the main index was built. When building the next main index, the existing main index and the latest corresponding delta index are merged. Query evaluation in this approach is a federated task, requiring the merging of the results returned by both indices. The main index can keep its own cache, as its results remain stable over long periods of time.
We note that the vast literature on incremental indexing is beyond the scope of this paper. However, we are not aware of any work that addressed the maintenance of the search results cache in such settings. In incremental settings, systems typically either invalidate results whose age exceeds some threshold, or forego caching altogether.

83

3. SYSTEM MODEL
At a high level, Web search engines have three major components: a crawler, an indexer, and a runtime component that is dominated by the query processor (Figure 1). The crawler continuously updates the engine's document collection by fetching new or modified documents from the Web, and deleting documents that are no longer available. The indexer periodically processes the document collection and generates a new inverted file and auxiliary data structures. Finally, query processors evaluate user queries using the inverted file produced by the indexer [1, 5].
The runtime component of a Web search engine typically also includes a cache of search results, located between the engine's front-end and its query processor, as depicted in Figure 1. The cache provides two desirable benefits: (1) it reduces the average latency perceived by a user, and (2) it reduces the load on back-end query processors. Such a cache may run in the same machines as query processors or in separate machines. To simplify our discussion, we assume that caches of results reside in separate machines, and that most resources of those machines are available to the cache.

Documents from the Web
Crawler
- Add new documents - Remove old documents

Inverted File

Use new inverted file

New inverted file
Indexer

Runtime system

Query Processor

Query processor returns results

Cache sends query to processor in case of a miss

Document Collection

Periodically extracts documents from collection to generate a new inverted file

Cache User queries

Figure 1: Overview of system model.

However, as the index evolves, the cached results of certain queries no longer reflect the latest content and become stale. By stale queries, we precisely mean queries for which the top-k results change because of an index update. In order to keep serving fresh search results, the engine must invalidate those cached entries. One trivial invalidation mechanism is to have the indexers indicate whenever the inverted index changes, thereby prompting the cache to invalidate all queries. When the index is updated often, the frequent flushing of the cache severely impacts its hit rate, perhaps to the point of rendering caching worthless.
To efficiently invalidate cache entries, we assume that the indexer is able to propagate information to the runtime component upon changes to the index. More concretely, we assume that even though the crawler continuously updates the document corpus, the indexer only generates a new version every t time. Upon a new version, we assume that a set of documents D have each been either inserted to or deleted from the index. Note that this simple model subsumes incremental (real-time) indexing, in the sense that the indexer can index every new or removed document by setting t to a very small value and having D be a singleton set.
We embody the above idea by introducing a new component to the search engine architecture ­ the Cache Invalidation Predictor (CIP).

4. CACHE INVALIDATION PREDICTORS
Cache invalidation predictors bridge the indexing and runtime processes of a search engine, which typically do not interact in search engines operating in batch mode, or limit their interaction to synchronization and locking.

User Queries

Cache

Query Processor

Runtime system

Invalidator

Index pipeline

Crawled Documents

Parser/ Tokenizer

Synopsis Generator

Index

Figure 2: CIP Architecture.

When introducing cache invalidation prediction into a system, the very front end of the runtime system ­ the cache ­ needs to become aware of documents coming into the indexing pipeline. We thus envision building a CIP in two major pieces, as depicted in Figure 2:
The synopsis generator: resides in the ingestion pipeline, e.g., right after the tokenizer, and is responsible for preparing synopses of the new documents coming in. The synopses may be as robust as the full token stream and other ranking features of each and every incoming document, or as lean as nothing at all (in which case the generator is trivial).
The invalidator: implements an invalidation policy. It receives synopses of documents prepared by the synopsis generator, and through interaction with the runtime system, decides which cached entries to invalidate. The interaction may be complex, such as evaluating each synopsisquery pair, or simplistic (ignoring the synopses altogether).
Section 4.1 describes various pairings of synopsis generators and invalidators, which together constitute a CIP. In each case we note the computational complexities of both components, as well as the communication between them.
4.1 CIP Policies
Our architecture allows composing different synopsis generators with different invalidators, yielding a large variety of behaviors. Below we show how the traditional age-based time-to-live policy (TTL) fits within the framework, and proceed to describe several policies of synopsis generators and invalidators, which we later compose in our experiments.
4.1.1 TTL: Age-based invalidation
Age-based policies consider each cached entry to be valid for a certain amount of time  after evaluation. Each entry is expired, or invalidated, once its age reaches  . At the two extremes,  = 1 implies no caching as results must be recomputed for each and every query. With  =  no

84

invalidation ever happens, and results are considered fresh as long as they are in the cache. As the value of  increases from 1 to , the number of unnecessary invalidations decreases, whereas the number of missed invalidations increases.
TTL-based policies ignore incoming content. In terms of our architecture, the synopsis generator is null in TTL policies, and no communication is required. The invalidator can be realized with a complexity of O(1) per query.
4.1.2 Synopsis Generation and Invalidation Policies
To improve over TTL, we exploit the fact that the cached results for a given query are its top-k scoring documents. By approximating the score of an incoming document to a query we can try to predict whether it affects its top-k results.
Synopsis generation.
The synopsis generator attempts to send compact representations of a document's score attributes, albeit to unknown queries. Its main output is a vector of the document's top-scoring TF-IDF terms [4] ­ these are the terms for which the document might score highly for. To control the length of the synopsis, the generator sends a fraction  of each document's top terms in the vector.  can range from zero (empty synopsis) to 1 (all terms, full synopsis). Intuitively, selective (short) synopses will lower the communication complexity of the CIP but will increase its error rate, as less information is available to the invalidator.
Another observation, applicable to document revisions, is that insignificant revisions typically do not affect the rankings achieved by the document. Consequently, cached entries should not be invalidated on account of minor revisions of documents. Hence, we estimate the difference between each document revision and its previously encountered version, and only produce a synopsis if the difference is above a modification threshold . Concretely, we use the weighted Jaccard similarity [13] as a similarity measure, where the weight of term t in document D is the number of occurrences of t in D. This measure can be efficiently and accurately estimated by using shingles [6]. Increasing  will result in fewer synopses being produced, thereby lowering the communication complexity of the CIP, at the cost of failing to invalidate cached entries that have become stale.
Invalidation policies.
Once a synopsis is generated, the CIP invalidators make a simplifying assumption that a document (and hence, a synopsis) only affects the results of queries that it matches. While this is true for most synopses and queries, it does not always hold. For example, a document that does not match a query may still change term statistics that affect the scores of documents that do. With this assumption, an invalidator first identifies all queries (and only those) matched by the synopsis. A synopsis matches query q if it contains all of q's terms in conjunctive query models, or any term in disjunctive models. Then, the invalidator may invalidate all queries matched by a synopsis (note that match computation can be efficiently implemented with an inverted index over the cached query set). Alternatively, it can apply score thresholding ­ namely, using the same ranking function as the underlying search engine, it computes the score of the synopsis with respect to cached query q, and only invalidates q if the computed score exceeds that of q's last cached result. This score projection procedure, which tries to de-

 fraction of top-terms included in synopsis  revision modification threshold for producing a synopsis 1s boolean indicating whether score thresholding is applied  time-to-live of a cached entry
Table 1: Summary of parameters.
termine whether a new document is in the top-k results of a cached query, is feasible for many ranking functions, e.g. TF-IDF, probabilistic ranking, etc. However, it is inherently imperfect for an incremental index where cached scores cannot be compared with newly computed ones as the index's term statistics drift. We denote by the indicator variable 1s whether score thresholding is applied.
Similarly to TTL, CIP applies age-based invalidation ­ they invalidate all queries whose age exceeds a certain timeto-live threshold, denoted by  . This bounds the maximum staleness of the cached results.
Finally, all CIPs invalidate any cached results that include documents that have been deleted. Clearly, all invalidation due to deleted documents are correct.
Table 1 summarizes the parameters of our CIP policies.
4.2 Metrics of Cache Invalidation Predictors
Upon processing a new document set D, a Cache Invalidation Predictor (CIP) makes a decision whether to invalidate or not each cached query. We say CIP is positive (p) about query q when CIP estimates that the ingestion of D by the corpus will change q's results, and so q's entry should be invalidated as it is now stale. CIP is negative (n) about q when it estimates that q's cached results do not change with the ingestion of document set D.
For each query, we can compare CIP's decision with an oracle that knows exactly if the ingestion of D by the corpus will change q's results or not - as if it had re-run every cached query upon indexing D. This leads to four possible cases (depending on whether CIP or the oracle decide positive or negative for the query). Let us call them {pp, pn, np, nn}, where the first letter indicates the decision of the CIP and the second the oracle's.
There are two types of errors CIP might make. In a false positive (pn), CIP wrongly invalidates q's results, leading to an unnecessary evaluation of q if it is submitted again. In a false negative (np), CIP wrongly keeps q's results, causing the cache to return stale results whenever q is subsequently submitted until its eventual invalidation. If we have a set of cached queries Q of size Q, we can compute the total number of queries falling in each one of these categories. Let us call these totals P N and N P respectively.
These two types of errors have very different consequences. The cost of a false positive is essentially computational, whereas false negatives hurt quality of results. Conservative policies, aiming to reduce the probability of users receiving stale results, will focus on lowering false negatives. More aggressive policies will focus on system performance and will tolerate some staleness by lowering false positives. This implies that CIPs should be evaluated along both dimensions - each application will determine the most suitable compromise between false positive and false negatives. We note that modern search engines are conservative, and are willing to devote computational resources to keep their results as fresh as possible ("keeping up with the Web").

85

False Positive Ratio (FP) False Negative Ratio (FN) Stale Traffic Ratio (ST)

P N/Q

N P/Q

P
qS

fq

/F

Table 2: CIP performance metrics

We use the ratio of false positives and false negatives, de-

noted FP and FN respectively, as our performance metrics

(see Table 2 for definitions). High FP implies many wasteful

computation cycles due to unnecessary invalidations. High

FN implies many stale results in the cache, leading to po-

tentially many of them being returned to the users.

The metrics above were defined with respect to the con-

tents of the cache given a single document set D. In an

incremental setting, a CIP would receive a sequence of doc-

ument sets, D1, D2, . . .. It is important to note that a false

positive made by CIP when processing Dt can propagate er-

rors (from the users' standpoint) into the future. Consider a

query q, upon which CIP incurs a false negative (np) when

processing Dt, thereby leaving q's stale results in the cache.

Assume that when processing Dt+1, CIP correctly labels q

as negative (nn) and does not invalidate its results, as the

documents in Dt+1 indeed do not affect q's results. While

the predictor made a correct point-in-time decision at time

t + 1, q's cached results remain stale, and any user submit-

ting q until such time when CIP invalidates q will receive

stale results. Let S be the set of cached queries whose re-

sults are stale. Note that after processing any document set,

|S|  N P since stale queries may have persisted in the cache

from false negatives made on earlier document sets.

False positives and false negatives are asymmetrical also

in another aspect: a false positive on query q will incur a

single (redundant) re-evaluation of q, so the cost for the

engine is irrespective of the query stream. In contrast, the

cost of a false negative on q (and any stale query q  S in

general) depends on the frequency of q in the query stream,

as the cache returns stale results for each request of q. We

therefore define a Stale Traffic ratio metric ST (see Table 2),

in which the cost of each stale query q  S is weighted by

its frequency, denoted fq. The quantity F in the formula of

ST

is

the

sum

of

all

query

frequencies

F

=

P
qQ

fq

.

Note that the metrics above are defined irrespective of the

cache replacement policy that may be used. In particular, a

CIP false negative on q is harmless if the cache replacement

policy evicts q before the next request of q. The interaction

between cache invalidation due to the dynamics of the un-

derlying corpus and cache replacement due to the dynamics

of the query stream is subject of future work.

5. EXPERIMENTS
This section presents our evaluation framework. We use a large Web corpus and a real query log from the Yahoo! search engine to evaluate our CIP policies. Note that our setup makes several simplifying assumptions to make tractable the problem of simulating a crawler, an indexer, a cache, and a realistic query load interacting in a dynamic fashion.
5.1 Experimental Setup
As a Web-representative dynamic corpus, we use the history log of the (English) Wikipedia1, the largest time-varying
1http://www.wikipedia.org/

dataset publicly available on the Web. This log contains all revisions of 3, 466, 475 unique pages between Jan 1, 2006 and Jan 1, 2008. It was constructed from two sources: the latest public dump from the Internet Archive2, with the information about page creations and updates, and the deletion statistics available from Wikimedia3.
The initial snapshot on Jan 1, 2006 contained 904, 056 individual pages. We processed Wikipedia revisions in singleday batches called epochs, each containing the revisions that correspond to one day of Wikipedia history. The average number of revisions per day is 41, 851 (i.e., about 4% of the initial corpus), consisting mostly of page modifications (95.22%) and new page creations (4.16%). The (uncompressed) size of the corpus, with all revisions, is 2.8 TB.
We focus on conjunctive queries (the de facto standard for Web search) ­ i.e., documents match a query only when containing all query terms. Our experiments use the opensource Lucene search library as the underlying index and runtime engine4. Lucene uses TF-IDF for scoring.
We assess the performance of predictors on a fixed representative set of queries Q, which represents a fixed set of cached queries. The synopsis generator consumes each epoch in turns, sends synopses of its documents to the invalidator, and the invalidator makes a decision on each query q  Q. We compute the "ground truth" oracle by indexing the epoch in Lucene and running all queries, retrieving the top-10 documents per query. The ground truth oracle is conservative and declares a query as invalid upon any change to the ranking of its top-10 results. We record the performance of each CIP relative to the ground truth, and track its set of stale queries. The performance numbers reported in the next section are all averaged, per CIP policy, over a history of 120 consecutive epochs (days) of Wikipedia revisions.
To generate the set of cached queries Q, we performed a uniform sample, with repetitions, of 10, 000 queries from the Yahoo! Web search log, sampled from a query log recorded on May 4 and May 5, 2008, which resulted in a user clicking on a page from the en.wikipedia.org domain. Q consists of the 9,234 unique queries in the sample. The multiset of queries was used to derive the frequency fq of each q  Q, for computing the stale traffic ratio (ST ).
Our choice of working with a fixed query set stems from our desire to isolate the performance of the CIP policies from the effects of a dynamic cache and its parameters (e.g., cache size and replacement policies). The dynamic study, which is plausible and interesting, is left for future research.
5.2 Numerical Results
We start by analyzing the results obtained for three standard policies: no caching, no invalidation (static cache), and TTL caching (invalidating all queries after a fixed period of time). Table 3 reports their performance. Not invalidating entries causes the cache to return stale results. Not caching guarantees that no results are stale, but it also forces the engine to process queries unnecessarily as previous work on caching has shown. Using a TTL value improves the overall situation, since it reduces the amount of stale traffic compared to not invalidating entries, but it still generates a significant number of false positives and negatives. Finally, a basic CIP policy with the following parameters is able to re-
2http://www.archive.org/details/enwiki-20080103 3http://stats.wikimedia.org 4http://lucene.apache.org/

86

Policy No Invalidation No Cache TTL  = 2 TTL  = 5 Basic CIP

FP 0.000 0.892 0.446 0.179 0.679

FN 0.108 0.000 0.054 0.086 0.001

ST 0.768 0.000 0.055 0.175 0.008

Table 3: Baseline CIP comparison.

duce the amount of stale traffic significantly, with very few false negatives - similarly to the "no cache" case - at the cost of many false positives:
Basic CIP:  = ,  = 0,  = 1, and 1s = f alse
In words, our Basic CIP does not expire queries ( = ), does not exclude documents based on similarity ( = 0), does not exclude terms ( = 1), and does not use score thresholding. The synopsis generator of the Basic CIP essentially sends each document in its entirety to the predictor, which then invalidates each query whose terms appear in conjunction in any synopsis.
Ruling out a cache is ideal with respect to freshness of results, but it is undesirable from a performance perspective. The Basic CIP is able to achieve a similar degree of freshness, while benefiting from cache hits. We next assess how changing the CIP parameters affects both freshness and performance.
Dynamics of stale traffic: Over time, errors due to false negatives accumulate, and imply an increasingly high stale traffic ratio (ST). The impact is most severe for frequent queries. A false negative can be fixed by either (1) a CIP positive, either true or false; or (2) an age threshold expiration. CIP positives depend on the arrival rate of matching documents: if a match never happens after a false negative, then the latter will persist forever. Consequently, it is critical to augment the CIP with a finite age threshold  , not only to bound the maximum result set age, but also to guarantee that ST converges.
Figure 3 shows how stale traffic evolves over time with three CIP instances. The CIP instances in the figure use a synopsis of the top 20% terms ( = 0.2), employ score thresholding (1s = true), and have different  values. For  = , ST grows, albeit in a declining pace, and eventually exceeds 30% without stabilizing. For  = 5 and  = 10, ST stabilizes within a few epochs after the first expiration. Infinite  is practical only when the predictor's FN ratio is negligible, e.g., with the Basic CIP.
Varying  and  : Figure 4 depicts the behavior of CIP for different values of synopsis size  and time-to-live  , also employing score thresholding (1s = true). In this experiment, we create synopses for all document revisions ( = 0). In addition to plotting the TTL baseline, we show 5 CIP plots, each having a fixed value of  . The rightmost CIP plot (circle marks) does not apply score thresholding (1s=false) while the other 4 plots do. The six points in each CIP plot correspond to increments of 0.1 in , from  = 0.5 at the top point of each plot to  = 1.0 at the bottom. The Basic CIP is the bottom point in the rightmost CIP plot.
Score thresholding reduces false positives but increases the false negatives ratio (FN). The  parameter only affects the positive predictions, hence it has no impact on FN. How-

Stale traffic ratio

Stale Traffic Ratio Dynamics

0.5 !=0.2, "=#, 1s

0.45

!=0.2, "=10, 1s

0.4

!=0.2,

"=5,

1
s

!=1, "=# (Basic) 0.35

0.3

0.25

0.2

0.15

0.1

0.05

0 0 10 20 30 40 50 60 70 80 90 100 110 120
Epoch

Figure 3: Convergence of stale traffic metric for CIP instantiations. For finite age threshold  , stale traffic stabilizes shortly after  . For infinite  , stale traffic grows throughout the evaluation.
 = 0  = 0.005  = 0.01  = 0.05  = 0.1 100% 69.03% 57.25% 29.25% 20.38%
Table 4: Percentage of transmitted synopses as the modification threshold  increases.
ever, lowering  reduces stale traffic, as frequent age-based invalidation rectifies false negatives from previous epochs and limits their adverse effect on stale traffic. For example, although the Basic CIP ( = , 1s = f alse) achieves the smallest possible FN (0.08%), there are instances (e.g.,  = 2, 1s = true) which improve upon it by reducing both stale traffic and false positives (0.35% vs 0.89%, and 59.1% vs 67.8%, respectively). In such configurations, false negatives are fixed quickly, causing little cumulative effect.
Finally, shorter synopses (smaller  values) reduce false positives and communication, at the expense of more false negatives, and consequently, higher stale traffic.
Varying  and : Figure 5 evaluates the effect of varying the modification threshold . These experiments use complete synopses ( = 1) and score thresholding (1s = true). Each plot fixes a value of  , and varies .
Increasing the value of  yields a reduction of FP's at the cost of higher FN's and ST. Additionally, eliminating synopses due to minor revisions reduces the communication overhead between the synopsis generator and the invalidator. This is particularly useful when the two CIP components reside on separate nodes. Table 4 shows how the percentage of generated (and transmitted) synopses drops as the value of  increases. Note that we compute the communication overhead here by counting the number of synopses.
Best cases: Here we contrast the best individual instances of CIP classes studied in the previous sections against the baseline TTL heuristic. Figure 6 depicts the policy instances that formed the bottom-left envelope of Figure 4 and Figure 5. Our results show that for every point of TTL, there is at least one point of CIP that obtains a significantly lower stale traffic for the same value of false positives. For example, tolerating 6% of stale traffic requires below 20% of false

87

False Negatives ratio

False Positives vs False Negatives

0.2 0.18 0.16 0.14

!=", 0.5 # $ # 1

!=2, 0.5 # $ # 1, 1s

!=3,

0.5

#

$

#

1,

1
s

!=5, 0.5 # $ # 1, 1s

!=10, 0.5 # $ # 1, 1s

0.12

TTL (1 # ! # 5)

0.1

!=5!=4

0.08

!=3

0.06

!=2

0.04

0.02

0

Basic

!=1 (no caching)

0

0.2

0.4

0.6

0.8

1

False Positives ratio (unnecessary invalidations)

Stale Traffic ratio

False Positives vs Stale Traffic

0.2

0.18

!=5

0.16

0.14

!=4

!=", 0.5 # $ # 1

!=2, 0.5 # $ # 1, 1s

!=3,

0.5

#

$

#

1,

1
s

!=5, 0.5 # $ # 1, 1s

!=10, 0.5 # $ # 1, 1s

0.12

TTL (1 # ! # 5)

0.1

!=3

0.08

0.06

!=2

0.04

0.02 Basic

0

!=1 (no caching)

0

0.2

0.4

0.6

0.8

1

False Positives ratio (unnecessary invalidations)

Figure 4: False Negatives (FN, left) and Stale traffic (ST, right) vs. False Positives (FP) curves, for varying 1s (f alse/true),  (2, 3, 5, 10) and  (50%, 60%, 70%, 80%, 90%, 100%). The Basic CIP achieves the optimal FN but a suboptimal ST, due to  = . Score thresholding (1s), longer timeouts ( ), and smaller synopses () lead to more aggressive policies.

False Negatives ratio

False Positives vs False Negatives

0.2

1,
s

!=2,

0

"

#

"

0.1

0.18

1,
s

!=3,

0

"

#

"

0.1

0.16

1s, !=5, 0 " # " 0.1

1s, !=10, 0 " # " 0.1

0.14

TTL (1 " ! " 5)

0.12

0.1

!=5!=4

0.08

!=3

0.06

!=2

0.04

0.02

0

!=1 (no caching)

0

0.2

0.4

0.6

0.8

1

False Positives ratio (unnecessary invalidations)

Stale Traffic ratio

False Positives vs Stale Traffic

0.2

1,
s

!=2,

0

"

#

"

0.1

0.18

!=5

1,
s

!=3,

0

"

#

"

0.1

0.16

0.14

!=4

1s, !=5, 0 " # " 0.1 1s, !=10, 0 " # " 0.1
TTL (1 " ! " 5)

0.12

0.1

!=3

0.08

0.06

!=2

0.04

0.02

0

!=1 (no caching)

0

0.2

0.4

0.6

0.8

1

False Positives ratio (unnecessary invalidations)

Figure 5: False Negatives (FN, left) and Stale traffic (ST, right) vs. False Positives (FP) curves, for varying  (2, 3, 5, 10) and  (0%, 0.5%, 1%, 5%, 10%). Higher modification thresholds (increasing , from bottom to top of each plot) lead to more aggressive policies.

positives, in contrast with TTL's 44.6%. When high precision is required (low ST), CIP performs particularly well ­ the number of query evaluations is 30% below the baseline.
6. CONCLUSIONS
Cache invalidation is critical for caching query results over incremental indices. Traditional approaches apply very simple invalidation policies such as flushing the cache upon updates, which induces a significant penalty to cache performance. We presented a cache invalidation predictor (CIP) framework, which invalidates cached queries selectively by using information about incoming documents. Our evaluation results using Wikipedia documents and queries from a real search engine shows that our policies enable a significant reduction to the amount of redundant invalidations (false positives, or FP) required to sustain the desired precision (stale traffic, or ST). More concretely, for every target ST,

the reduction of FP compared to the baseline TTL scheme is between 25% and 30%.
The implication of our results to the design of caching systems is the following. False positives impact negatively the cache hit rate as they lead to unnecessary misses in our setting. Consequently, selecting a policy that enables a low ratio of false positives is important for performance. With our CIP policies, it is possible to select a desired ratio of false positives as low as 0.2. Lowering the ratio of false positives, however, causes the ratio of false negatives (and stale traffic) to increase, which is undesirable when the degree of freshness expected for results is high. When designing a caching system, a system architect must confront such a trade-off and choose parameters according to the specific requirements of precision and performance. Our CIP policies enable such choices and improve over previous solutions.

88

Stale Traffic ratio

False Positives vs Stale Traffic

0.2

1,
s

2

"

!

"

10,

#

=

0

0.18

!=5

1s, 3" ! " 10, # = 0.005

0.16

1s, 5 " ! " 20, # = 0.01

0.14

!=4

TTL (1 " ! " 5)

0.12

0.1

!=3

0.08

0.06

!=2

0.04

0.02

0

!=1 (no caching)

0

0.2

0.4

0.6

0.8

1

False Positives ratio (unnecessary invalidations)

Figure 6: Stale traffic (ST) vs False Positives (FP) for the best cases. We use:  = 1 ­ complete synopses, 1s = true ­ score thresholding,  = (0%, 0.5%, 1%) ­ small modification threshold, and 2    20 ­ a variety of age thresholds.
Acknowledgements
This work has been partially supported by the COAST (ICT248036) and Living Knowledge (ICT-231126) projects, funded by the European Community.
7. REFERENCES
[1] A. Arasu, J. Cho, H. Garcia-Molina, A. Paepcke, and S. Raghavan. Searching the Web. ACM Transactions on Internet Technology, 1(1):2­43, 2001.
[2] Ricardo Baeza-Yates, Aristides Gionis, Flavio P. Junqueira, Vanessa Murdock, Vassilis Plachouras, and Fabrizio Silvestri. Design trade-offs for search engine caching. ACM Transactions on the Web, 2(4):1­28, 2008.
[3] Ricardo Baeza-Yates, Flavio Junqueira, Vassilis Plachouras, and Hans F. Witschel. Admission Policies for Caches of Search Engine Results. In SPIRE, 2007.
[4] Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. Modern Information Retrieval. ACM Press / Addison Wesley, New York, NY, 1999.
[5] Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. In WWW'98: Proceedings of the 7th International Conference on the World Wide Web, pages 107­117, 1998.
[6] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering of the Web. Computer Networks and ISDN Systems, 29(8-13):1157­1166, 1997.
[7] Soumen Chakrabarti. Mining the Web - Discovering Knowledge from Hypertext Data. Morgan Kaufmann Publishers, San Francisco, CA, 2003.
[8] Junghoo Cho and Hector Garc´ia-Molina. The evolution of the Web and implications for an incremental crawler. In Proc. 26th International Conference on Very Large Data Bases (VLDB2000), pages 200­209, 2000.

[9] Anirban Dasgupta, Arpita Ghosh, Ravi Kumar, Christopher Olston, Sandeep Pandey, and Andrew Tomkins. The discoverability of the Web. In WWW '07: Proceedings of the 16th International Conference on the World Wide Web, pages 421­430. ACM, 2007.
[10] Tiziano Fagni, Raffaele Perego, Fabrizio Silvestri, and Salvatore Orlando. Boosting the Performance of Web Search Engines: Caching and Prefetching Query Results by Exploiting Historical Usage Data. ACM Transactions on Information Systems, 24(1):51­78, 2006.
[11] Marcus Fontoura, Jason Zien, Eugene Shekita, Sridhar Rajagopalan, and Andreas Neumann. High performance index build algorithms for intranet search engines. In Proc. 30th International Conference on Very Large Data Bases (VLDB 2004), pages 1158­1169. Morgan Kaufmann, August 2004.
[12] Qingqing Gan and Torsten Suel. Improved techniques for result caching in Web search engines. In WWW'09: Proceedings of the 18th International Conference on the World Wide Web, pages 431­440, April 2009.
[13] Paul Jaccard. E´tude comparative de la distribution florale dans une portion des alpes et des jura. Bulletin de la Soci´et´e Vaudoise des Sciences Naturelles, 37:547­579, 1901.
[14] Ronny Lempel and Shlomo Moran. Predictive Caching and Prefetching of Query Results in Search Engines. In WWW'03: Proceedings of the 12th International Conference on the World Wide Web, pages 19­28. ACM Press, 2003.
[15] Ronny Lempel and Shlomo Moran. Competitive caching of query results in search engines. Theoretical Computer Science, 324(2):253­271, September 2004.
[16] Xiaohui Long and Torsten Suel. Three-level caching for efficient query processing in large Web search engines. In WWW'05: Proceedings of the 14th International Conference on the World Wide Web, pages 257­266, May 2005.
[17] Evangelos P. Markatos. On Caching Search Engine Query Results. Computer Communications, 24(2):137­143, 2001.
[18] Sergey Melnik, Sriram Raghavan, Beverly Yang, and Hector Garcia-Molina. Building a distributed full-text index for the Web. In WWW'01: Proceedings of the 10th International Conference on the World Wide Web, pages 396­406, May 2001.
[19] P. Saraiva, E. Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Ribeiro-Neto. Rank-preserving two-level caching for scalable search engines. In Proc. 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 51­58, 2001.
[20] Gleb Skobeltsyn, Flavio Junqueira, Vassilis Plachouras, and Ricardo Baeza-Yates. ResIn: a combination of results caching and index pruning for high-performance Web search engines. In Proceedings of the 31st ACM SIGIR conference, pages 131­138, 2008.
[21] Ian Witten, Alistair Moffat, and Timoty Bell. Managing Gigabytes. Morgan Kaufmann Publishers, Inc., San Francisco, CA, second edition, 1999.

89

Multi-view Clustering of Multilingual Documents

Young-Min Kim Massih-Reza Amini
Université Pierre et Marie Curie Laboratoire d'Informatique de Paris 6 104, avenue du Président Kennedy
75016 Paris, France
First.Last@lip6.fr

Cyril Goutte

Patrick Gallinari

National Research Council Canada Institute for Information Technology 283, boulevard Alexandre-Taché
Gatineau, J8X 3X7, Canada
First.Last@nrc-cnrc.gc.ca

ABSTRACT
We propose a new multi-view clustering method which uses clustering results obtained on each view as a voting pattern in order to construct a new set of multi-view clusters. Our experiments on a multilingual corpus of documents show that performance increases significantly over simple concatenation and another multi-view clustering technique.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering; I.5.3 [Clustering]: Algorithms
General Terms
Algorithms, Experimentation
Keywords
Multilingual document clustering, Multi-view learning, PLSA
1. INTRODUCTION
Much data is now available in multiple representations, or views, for example multimedia content or web pages translated into several languages. Multi-view learning is a principled approach to handle these kinds of documents using the relations between the multiple views. The key is to leverage each view's characteristics in order to do better than simply concatenating views. Recently, multi-view clustering methods have been proposed that address this situation and have been shown to improve over traditional single-view clustering. [2] proposed an extension of k-means and EM for a dataset with two views, and [5] presented a late fusion approach which re-estimates the relationship between documents from single-view clustering results. [3] and [6] show that dimensionality reduction via canonical correlation between views gives better results for document clustering than via principal components analysis or random projections.
This paper introduces a novel multi-view clustering method. Our approach consists of two steps. First we find robust topics for each view using the PLSA approach. The topic pattern
Copyright 2010 Crown in Right of Canada. This article was authored by employees of the National Research Council of Canada. As such, the Canadian Government retains all interest in the copyright to this work and grants to ACM a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, provided that clear attribution is given both to the NRC and the authors. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

over the multiple views defines cluster signatures for each document. We use those to prime a second-stage clustering process over all the views. Experiments carried out on a large five language corpus of Reuters documents show that we consistently improve over competing techniques.
2. MUTLI-VIEW CLUSTERING
We consider a multilingual document as d =def (d1, ..., dV ) where each version or view dv, v  {1, ..., V } provides a representation of document d in a different language, with feature space Xv. Our algorithm operates in two steps.
2.1 Stage I - Single-view clustering
At the first stage of our multilingual clustering, we apply Probabilistic Latent Semantic Analysis (PLSA) [7] independently over each of the V languages, constraining each model to have the same number of unobserved topics. For every view v, the probability that document dv arises from topic z  Z is given by p(z|dv), estimated by PLSA. Documents are then assigned to each topic using the maximum posterior probability. We hence obtain a set of V estimated topics (zd1 , ..., zdV ) for each document d, which we call the voting pattern in the following. Each zdv indicates the estimated topic index of d on the vth view according to the view-specific PLSA model.
2.2 Stage II - Voting & Multi-view clustering
Once a voting pattern is obtained for each multilingual document, we attempt to group documents such that in each group, documents share similar voting patterns. As documents belonging to each of these groups received by definition similar votes from the view-specific PLSA models, the voting pattern representing each of these groups is called the cluster signature. We keep the C largest groups with the most documents as initial clusters. Documents that have voting patterns with at least V - 1 in common with a cluster signature are pre-assigned to that cluster. The remaining documents have voting patterns different from any of the selected cluster signatures. They are matched to one of these C groups by applying a PLSA model on the concatenated document features.
The parameters of the final PLSA model are first initialized using the documents that have been pre-assigned to the selected cluster signatures. For these documents p(c | d) has a binary value equal to 1 if d belongs to cluster c and 0 otherwise. For the remaining documents, posteriors are estimated at each iteration as in the traditional E-step. In the M-step, after updating model parameters, we keep the val-

821

ues of p(c | d) fixed for the pre-assigned documents. After convergence, documents are assigned to the clusters using the posteriors p(c | d). Note that any generative model giving p(c | d) may be employed instead of PLSA, such as Latent Dirichlet Allocation [4].
3. EXPERIMENTS
We perform experiments on a publicly available multilingual multi-view text categorization corpus extracted from the Reuters RCV1/RCV2 corpus [1].1 This corpus contains more than 110K documents from 5 different languages, (English, German, French, Italian, Spanish), distributed over 6 classes. The multilingual collection is originally a comparable corpus as it covers the same subset of topics in all languages. In order to produce multiple views for documents, each original document extracted from the Reuters corpus was translated in all other languages using a phrasebased statistical machine translation system. The indexed translations are part of the corpus distribution.
Experiments are repeated 10 times on the whole dataset, using different random initializations of the PLSA models. The number of topics in each single-view PLSA model as well as the number of clusters C are fixed to 6, the number of classes in the collection. We used the micro-averaged precision (micro-AvgPre) as well as the Normalized Mutual Information (NMI) to measure clustering results [8]. In order to use these evaluation measures, the predicted label for each cluster is the label of the most dominant class in that cluster. The reported performance is averaged over the 10 different runs. To validate our approach we compare our algorithm (denoted by voted-PLSA in the following) with a PLSA model operating over the concatenated feature representations of documents (conc-PLSA) and the late fusion approach (Fusion-LM) for multi-view clustering [5].
First, we are interested in the clustering results after the first step of our algorithm on the C = 6 largest clusters containing each the same voting pattern documents. Table 1 shows the micro-AvgPre performance of the clustering results per language as well as the percentage of documents being grouped with our voting strategy. We observe that partitions formed using the votes of single-view models contain more than half of the documents in the collection and that these groups are highly homogeneous with an average precision of 0.76.

Table 1: Proportion of pre-assigned documents and average precision on those, obtained from the first stage single-view PLSA models.

Language English French German Italian Spanish Average

% of documents
51.18 63.85 67.44 58.03 73.73 62.84

micro-AvgPre 0.79 0.78 0.80 0.60 0.81 0.76

Table 2 summarizes results obtained by conc-PLSA, FusionLM and voted-PLSA averaged over five languages and 10 dif-
1http://multilingreuters.iit.nrc.ca/

Table 2: micro-AvgPre and NMI of different clustering techniques averaged over 10 initialization sets and 5 languages.

Strategy conc-PLSA Fusion-LM voted-PLSA

micro-AvgPre 0.63 0.61
0.65

NMI 0.41 0.41
0.44

ferent initializations. We use bold face to indicate the highest performance rates, and the symbol  indicates that performance is significantly worse than the best result, according to a Wilcoxon rank sum test used at a p-value threshold of 0.05. Note that in our approach, the second stage multiview clustering model relies on a PLSA on the concatenated views, just as in conc-PLSA. This suggests that the difference of 2 to 3 points in micro-AvgPre and NMI (respectively) between voted-PLSA and conc-PLSA shows the real impact of the first stage voting process. In addition, both voted-PLSA and conc-PLSA perform at least as well as Fusion-LM.
4. CONCLUSIONS
We presented a multi-view clustering approach for multilingual document clustering. The proposed approach is an incremental algorithm which first groups documents having the same voting patterns assigned by view-specific PLSA models. Working in the concatenated feature spaces the remaining unclustered documents are then assigned to the groups using a constrained PLSA model. Our results have brought to light the positive impact of the first stage of our approach which can be viewed as a voting mechanism over different views. The effect of the length of these voting patterns and the number of latent variables in view-specific PLSA models are interesting avenues for future research.
Acknowledgments
This work was supported in part by the IST Program of the EC, under the PASCAL2 Network of Excellence.
References
[1] M.-R. Amini, N. Usunier, and C. Goutte. Learning from multiple partially observed views - an application to multilingual text categorization. In NIPS 22, pages 28­36, 2009.
[2] S. Bickel and T. Scheffer. Multi-view clustering. In ICDM-04, pages 19­26, 2004.
[3] M. B. Blaschko and C. H. Lampert. Correlational spectral clustering. In CVPR-08, 2008.
[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, pages 993­1022, 2003.
[5] E. Bruno and S. Marchand-Maillet. Multiview clustering: A late fusion approach using latent models. In SIGIR-09, pages 736­737, 2009.
[6] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correlation analysis. In ICML-09, pages 129­136, 2009.
[7] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1-2):177­196, January 2001.
[8] N. Slonim, N. Friedman, and N. Tishby. Unsupervised document classification using sequential information maximization. In SIGIR-02, pages 129­136, 2002.

822

A Stack Decoder Approach to Approximate String Matching
Juan M. Huerta
IBM T. J. Watson Research Center 1101 Kitchawan Road,
Yorktown Heights, NY, 10598 (914) 945 3378
huerta@us.ibm.com

ABSTRACT
We present a new efficient algorithm for top-N match retrieval of sequential patterns. Our approach is based on an incremental approximation of the string edit distance using index information and a stack based search. Our approach produces hypotheses with average edit error of about 0.29 edits from the optimal SED result while using only about 5% of the CPU computation.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ retrieval models, search process.
General Terms
Algorithms, Performance, Experimentation
Keywords
A* search, Stack decoder, String Edit Distance, String Matching
1.INTRODUCTION
We address the problem of efficiently identifying the top scoring set of sentences from a large database of sentences using an incremental approximation of the string edit distance (SED) given a query sentence. Our motivation is the translation memory domain (e.g. [6]) where the goal is to identify the closest matching sentence from a large database of translation pairs given a query sentence in a source language in order to reduce the load of the translation engine, if the match is close enough. Sentences in this domain are typically short (consisting of less than 20 words).
The key idea in our approach is to approximate the SED which consists of the sum of edits costs (insertions, deletions and substitutions) under an optimal alignment using instead position adjusted similitude counts coming from an index. We also propose a way to carry out the search using a stack but without computing string alignments explicitly. This is desirable because, in general, index-based approaches are computationally advantageous when the search is carried out over large databases.
Our search strategy is similar to Jelinek [1] and Paul [4] with these differences: (1) we rely on an inverted index with position information rather than HMM state observations. (2) We approximate the SED instead of the observation likelihoods. (3) Our approach approximates string alignment scores. (4) The evidence is considered in order of decreasing rarity. Compared to Navarro and Baeza-Yates [2] our approach is based on a simpler structure,
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

we do not partition the query pattern and we search terms in decreasing rarity order.

2.INCREMENTAL APPROXIMATION

The SED between sentences A and B consists of the sum of the

cost of the edit operations given the optimal alignment  (where
the costs of the kth insertion, deletion or substitution operations are

denoted byk ,  k and  k , respectively):

   SED( A, B)

=

min 

(
k

k

+ k
k 

+ k
k 

)

Using dynamic programming (DP) one can simultaneously obtain

the optimal alignment and the associated minimum total distance

between A and B. The computational cost of naïve DP is O(nm),

where n is the length of string A and m is the length of string B

(cfr. [3]). When the comparison is performed between string A

and each sentence in the database  = {B1 , B2 ...Bk } the cost of

exhaustively computing SED using naïve DP is O(  nm) . In this

case m is the average sentence length in  , and |  | is the set cardinality. In translation memory domains |  | is typically large

(millions of sentences) while n and m are relatively small.
In order to approximate the SED using index-derived counts we manipulate the SED from a distance into a similitude score. For this we define the non-negative complemented SED (denoted as
SED*) between A and the jth sentence B j in  :
SED * ( A, B j ) = 2n - SED( A, B j )
When A=Bj then the SED* = 2n . This is two times the total number of observed evidence counts that are obtainable using the index (if we increment the score by one per observation). We can express the SED* in terms of two components: one provided by the counts directly identified using the index (which are equal to n minus deletions). We call this the observable evidence. The other term arises from the total number of spurious terms (n minus insertions). This is the unobservable evidence (the index returns no counts for these word-sentence combinations). Then:

SED * ( A, Bj) = (n -   k ) + (n -  k -  k )

k i

ki

ki

When estimating SED* for a collection of strings and a query we
keep track and update the SED* for each sentence in  as the

sum of the observable and unobservable evidence as each of the

terms ai in the query {a1 , a2 ...an }is incrementally considered.

We denote the observable evidence (n minus deletions) up to term

ai as g(i). While h(i) is the estimate up to term ai of unobservable

823

terms (amortized deletions). The SED*i up to the ith term is:

SEDi * ( A, B j ) = g(i) + h(i)

We want g(n) to approximate the actual insertion related score:
g(n)  n -  k ki
As the evidence is incrementally introduced we update g(i): g(i) = g(i -1) +  gi

The incremental contribution of term ai to g(i) is computed
based on the linear distances between the position of that term and

the previously considered term ai-1 in the query and hypothesis:

 gi = 1 - dist A (ai , ai-1 ) - dist Bj (ai , ai-1 )

The incremental contribution of a term should only be introduced if its location in the sentence is consistent with the hypothesis in question and the previously observed evidence. For example, if A="fast run" and B="run fast"; terms a1="fast" and a2="run" should not both contribute towards the observable score of B. In the case of h(i) the increment depends on the amortized expected deletions is:

 hi

=1 -

m

- Bj  Bj  A

A

Where Bj  A denotes the cardinality of the set of terms occur-

ring both in the hypothesis and in the query, and m is B j .

3.A* STACK DECODER ALGORITHM
To understand the process of searching for the top match from a large corpus using SED* estimates, we depart from a conceptual formulation in which alignments between the words in the query A and words in each of the sentences Bj are represented as unique paths in a tree with edges corresponding to alignment associations between ai and bj. Each root-to-leaf path represents an implicit alignment between A and Bj. Partial paths with common prefixes represent hypotheses with common observable evidence sets. Our goal is to find the root-to-leaf path that maximizes the total SED* score as evidence is incrementally introduced. This conceptualization is useful to motivate the use of A* search [5] to conduct bestfirst tree search using our incremental SED* approximation as described below:
Algorithm: StackSED* Input: query string A={a1 , a2 ...an } and inverted index Y
Output: ranked list of hypotheses R for each word ai in {a1 , a2 ...an }
obtain the set of sentences X recalled by ai from for each sentence Bj in set X
if Bj exists in S
compute  hi ,  gi and update hypoth. score and positions
else append Bj sentence id to S with score=1 and initial positions,
eliminate hypotheses with score smaller than P% of top score sort stack S by score return R list of top-n sentence id's.
Figure 1. StackSED* Algorithm
During search we do not explicitly build the whole search tree, but rather dynamically extend hypotheses representing top partial paths in a hypothesis stack (similar to [1, 2]) and prune low performing ones. Our approach uses these heuristics: (1) Terms in A

are introduced in increasing frequency order (decreasing rarity). (2) We consider only the top n terms in terms of rarity. (3) The lowest scoring hypotheses are pruned in each iteration.

4.EXPERIMENTS
We performed experiments based on a translation memory database. The task consisted of identifying the closest match from a database consisting of 1,000,000 sentences given a query. The test set consisted of 5,745 query sentences. The baseline computation of the SED using DP (without backtrack step) for the test set consumed a median of 0.56 (x10^6) CPU cycles/sentence. Table 1 shows the average edit error distance between the closest sentence and the top 1 stack hypothesis (and in parenthesis, the median CPU cycles per sentence) for several stack and rarity cutoff configurations. With a stack with maximum depth of 400 and considering only the top 4 terms per query, we speed up the computation approximately twenty-fold while still obtaining results that have edit scores that are 0.29 edit points higher per sentence on average from the top match.

Table 1. Avg. edit error and median CPU /sentence (in parenthesis) for rarity cutoff vs. stack depth

3

4

5

6

250 0.36(0.01) 0.30(0.03) 0.34(0.07) 0.43(0.18)

400 0.36(0.01) 0.29(0.03) 0.34(0.08) 0.43(0.19)

600 0.36(0.01) 0.30(0.04) 0.36(0.09) 0.47(0.19)

900 0.36(0.01) 0.32(0.04) 0.38(0.10) 0.51(0.19)

5.CONCLUSION
We introduced a new approach for efficient approximate string match using an inverted index using order information. Through a stack search and heuristics our approach reduces the search computation while producing near-optimal hypotheses. It can be shown that the average complexity of our approach (as implemented) is approximately O(log S nl) , where l is the average number of sentence occurrences for the rarest terms in the query and |S| is the size of the stack.
6.REFERENCES
[1] F. Jelinek (1969), Fast sequential decoding algorithm using a stack, IBM Journal of Res.and Devel., vol. 13, Nov. 1969.
[2] G. Navarro and R. Baeza-Yates (2000). A hybrid indexing method for approximate string matching. Journal of Discrete Algorithms, 1(1):205­239, 2000.
[3] G. Navarro (2001), A guided tour to approximate string matching, ACM Computing Surveys v.33 No. 1 2001.
[4] D. Paul, (1992), An efficient A* stack decoder algorithm for continuous speech recognition with a stochastic language model. Proc. of the Workshop on Speech and N. L. 1992.
[5] S. Russel and P. Norvig, (1995) Artificial Intelligence: A Modern Approach. Prentice Hall
[6] E. Sumita, (2001). Example-based machine translation using DP-matching between word sequences. Proc. of the Workshop on Data-Driven Methods in M.T. Vol. 14

824

Late Fusion of Compact Composite Descriptors for Retrieval from Heterogeneous Image Databases

Savvas A. Chatzichristofis Avi Arampatzis
Department of Electrical and Computer Engineering Democritus University of Thrace Xanthi, Greece
schatzic@ee.duth.gr, avi@ee.duth.gr

ABSTRACT
Compact composite descriptors (CCDs) are global image features, capturing more than one types of information at the same time in a very compact representation. Their quality has so far been evaluated in retrieval from several homogeneous databases containing images of only the type that each CCD is intended for, and has been found better than other descriptors in the literature such as the MPEG-7 descriptors. In this study, we consider heterogeneous databases and investigate query-time fusion techniques for CCDs. The results show that fusion is beneficial, even with simple score normalization and combination methods due to the compatibility of the score distributions produced by the CCDs considered.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-- retrieval models, search process; H.2.5 [Database Management]: Heterogeneous Databases General Terms: Measurement, Experimentation, Theory Keywords: Image Retrieval, CCD, Fusion, Normalization, Combination

perform better than the MPEG-7 descriptors and other descriptors in the related literature.
The Brightness and Texture Directionality Histogram, or BTDH, is proposed for grayscale and medical radiology images [7], and has been found to perform better than other descriptors in several benchmarking databases. The recently proposed Spatial Color Distribution (SpCD) combines color information and its spatial distribution in a quantized histogram [9]. The SpCD is considered suitable for colored graphics, since they contain a relatively small number of colors and less texture regions than natural color images.
The quality of the aforementioned CCDs has so far been evaluated in retrieval from homogeneous benchmarking databases, containing images of only the type that each CCD is intended for. For example, the JCD is tested on NISTER [6] and Wang databases which contain natural color images, the BTDH on the IRMA database consisting of grayscale medical radiology images, and the SpCD on two benchmarking databases with artificially generated images.
In this study, we evaluate the retrieval effectiveness of late fusion techniques which enable the combined use of the JCD, BTDH, and SpCD, on heterogeneous databases.

1. INTRODUCTION
Fusion in image retrieval goes hand-in-hand with practical, viable system development, which is critical for the future of image retrieval research [3]. Two main approaches to fusion have been taken: early fusion, where multiple image descriptors are composed to form a new one before index time [8], and late fusion, where result-lists from individual descriptors are fused during query time [4, 5], as in text meta-search. While early fusion has been common, late fusion still remains an under-explored possibility.
Compact Composite Descriptors (CCDs) are global image features for content-based image retrieval. CCDs capture more than one types of information at the same time in a very compact representation. An example of early fusion is the Joint Composite Descriptor (JCD), created by combining two CCDs [8]: the Color and Edge Directivity Descriptor (CEDD), and the Fuzzy Color and Texture Histogram (FCTH). The CEDD and FCTH can be seen as products of early fusion themselves, since they both combine color and texture information. Both descriptors are developed for color natural images, and experiments have shown that they
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. A CCD LATE FUSION EXPERIMENT
We created a heterogeneous database with 20230 images by joining the following: 9000 grayscale images from the IRMA 2005 database; 10200 natural color images from the NISTER database and 1030 artificially generated images from the Flags database [9].
We used 40 queries: The first 20 natural color image queries from the NISTER database and the first 20 grayscale queries of the IRMA 2005 database.
Fusion methods consist of a score normalization and a score combination component. We mainly focus on the normalization, using each time the combination method more natural to the normalization at hand, although we investigated other possibilities in initial experiments not reported here. Five fusion methods were tested:
· CombSUM: It is the addition of all scores per image, without normalization.
· BC+CombSUM: Borda Count originates from social theory in voting. The image with the highest rank on each ranked-list gets n votes, where n is the collection size. Each subsequent rank gets one vote less than the previous. Votes across ranked-lists are naturally combined with CombSUM.

825

· Z-score+CombSUM: Z-score is a linear normalization per query which maps each score to its number of standard deviations above or below the mean score. It is more suitable for non-skewed score distributions (SDs) where the mean would make more sense. In this respect, we also tried it with the median score instead of the mean. We present results with CombSUM; we also tried multiplication, but it gave inferior performance.

· IRP: The Inverse Rank Position merges ranked lists in the decreasing order of the inverse of the sum of inverses of individual ranks.

· HIS+multiplication: HIS is a non-linear normalization which maps each score to the probability of a historical query scoring a collection image below that score. It is recently proposed by [1] and found to be robust and effective in a distributed retrieval testbed. As historical queries we used 50 images drawn randomly from the database. Since HIS returns probabilities, the natural combination would be multiplication; addition gave inferior results in initial experiments.
We evaluate with two measures: the Average Normalized Modified Retrieval Rank (ANMRR), and the Mean Average Precision (MAP). ANMRR is the evaluation measure used in all the MPEG-7 color core experiments; it ranges between 0 and 1, with 0 being the maximum effectiveness.
Since the goal of fusion is to achieve better results than those achieved by any of the CCDs in isolation, we use the performance of SpCD as baseline, as shown in Table 1.

Descriptor / Fused
JCD BTDH SpCD
CombSUM BC + CombSUM Z-score with Mean + CombSUM Z-score with Median + CombSUM IRP HIS + multiplication

40 Queries

ANMRR 0.3554 0.4015 0.3081

MAP 0.5899 0.5555 0.6311

0.2491 0.2678 0.2400 0.2420 0.2729 0.2664

0.7121 0.6848 0.7194 0.7193 0.6674 0.6846

Table 1: Experimental Results.

All fusion methods beat the baseline. Best effectiveness overall is achieved by Z-score which beats the baselines of the individual CCDs by wide margins. Both versions (with the mean or median) perform similarly.
While HIS performs better than IRP and close to BC, it lacks behind Z-score and the bare CombSUM. We tried using more than 50 historical queries (up to 1000) in order to deduce smoother normalization functions, but the effectiveness of HIS did not improve. This is in line with [1], where 50 queries were deemed sufficient to achieve a performance plateau.
The performance of the bare CombSUM is remarkable. Although it is considered a naive method [2], it is found effective and robust. On a further investigation it turned out that the reason for this is the similarity of the SDs, in both shape and range, across the CCDs (Fig. 1).

Figure 1: SDs of the three CCDs for 2 queries.
3. CONCLUSIONS
We investigated methods for fusing retrieval results obtained from an heterogeneous image database using multiple descriptors individually. This type of fusion, known as late fusion, is found to be a viable method for retrieving from heterogeneous databases, which improves effectiveness over single descriptor baselines even with simple score normalization and combination methods.
While [2] postulates that effective normalization methods should be non-linear taking into account the shape of SDs-- especially for non-text descriptors where a wilder variety of SDs is assumed--we found these claims to be not necessarily true. By using compatible descriptors from the family of CCDs, combining by adding bare or linearly normalized scores with Z-score works best.
4. REFERENCES
[1] A. Arampatzis and J. Kamps. A signal-to-noise approach to score normalization. In CIKM, pages 797­806. ACM, 2009.
[2] A. Arampatzis, S. Robertson, and J. Kamps. Score distributions in information retrieval. In ICTIR, volume 5766 of LNCS, pages 139­151. Springer, 2009.
[3] R. Datta, D. Joshi, J. Li, and J. Wang. Image retrieval: Ideas, influences, and trends of the new age. ACM Computing Surveys, 40(2):1­60, 2008.
[4] M. Jovic, Y. Hatakeyama, F. Dong, and K. Hirota. Image retrieval based on similarity score fusion from feature similarity ranking lists. In FSKD, volume 4223 of LNCS, pages 461­470. Springer, 2006.
[5] M. Bleschke , R. Madonski and R. Rudnicki. Image retrieval system based on combined mpeg-7 texture and colour descriptors. In MIXDES, pages 635 ­ 639, 2009.
[6] D. Nister and H. Stewenius. Scalable recognition with a vocabulary tree. In Proc. CVPR, volume 5, pages 2161­2168. Citeseer, 2006.
[7] S. A. Chatzichristofis and Y. S. Boutalis. Content based radiology image retrieval using a fuzzy rule based scalable composite descriptor. Multimedia Tools and Applications, 46:493­519, 2010.
[8] S. A. Chatzichristofis, Y. S. Boutalis, and M. Lux. Selection of the proper compact composite descriptor for improving content based image retrieval. In SPPRA, pages 134­140. ACTA Press, 2009.
[9] S. A. Chatzichristofis, Y. S. Boutalis and M. Lux. SpCD--spatial color distribution descriptor. A fuzzy rule based compact composite descriptor appropriate for hand drawn color sketches retrieval. In ICAART, pages 58­63, 2010.

826

Inferring User Intent in Web Search by Exploiting Social Annotations
Jose M. Conde, David Vallet, Pablo Castells
Universidad Autónoma de Madrid Cantoblanco, 28049 Madrid, Spain
jose.conde@estudiante.uam.es,{david.vallet, pablo.castells}@uam.es

ABSTRACT
In this paper, we present a folksonomy-based approach for implicit user intent extraction during a Web search process. We present a number of result re-ranking techniques based on this representation that can be applied to any Web search engine. We perform a user experiment the results of which indicate that this type of representation is better at context extraction than using the actual textual content of the document.
Categories and Subject Descriptors H.3.3 Information Search and Retrieval ­ retrieval models, information filtering.
General Terms Algorithms, Experimentation.
Keywords Web search, folksonomy, context.
1. INTRODUCTION
With the advent of the Web 2.0, social tagging systems have exponentially grown both in terms of users and contents. These systems encourage users to tag different types of content items in a way that enhances their organization and sharing. The domains of these systems are varied, including music (e.g., Last.fm), photo streams (e.g. Flickr), or Web pages (e.g. Delicious). However, even with the aid of currently available tools, users still have problems accessing the ever increasing information available on the Web, which is known as the information overload problem.
Personalized and contextualized content access has aimed in recent years to alleviate the information overload problem on the Web by taking both long and short term interests of users into account. Additionally, the underlying semantic information generated by users in social tagging systems, known as folksonomies, has enabled the research and development of new effective retrieval and personalized methods. Folksonomies have been recently exploited in order to, for instance, improve [1] or personalize [4] Web search, by exploiting the implicit user and document profiles that can be extracted by mining the social tagging actions of users.
In this paper, we investigate whether social tagging systems can be a new information source for the runtime construction and representation of users' search intent, i.e. if this information can be interpreted as a new source of (implicit) search context. We focus our research on Web search and the use of Delicious1, a social bookmarking service. We hypothesize that social tagging systems such as Delicious can be a valuable source of information in order to elucidate the semantics involved in the search process of the user. Context-aware models based on implicit feedback rely on processing the content of the executed queries and accessed documents by users during search activities [5]. This information is used to build a representation of the current search context of
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

the user that can be leveraged in subsequent queries of a search session. Here we investigate the application of such techniques to a folksonomy-based representation of Web documents.
This use of social tagging information for user intent extraction has not been investigated in depth before. An initial approach to the exploitation of Delicious as a context source has been presented by Schmidt et al. [3], who extract the tags related to the documents opened by the user, and apply them in query expansion, using an ad-hoc decay factor that gives more importance to recently opened documents. However, no evaluation is provided in this work, so the feasibility of folksonomy-based context representation lacks a formal empirical study, to the best of our knowledge. Rather than on query expansion [3],[5], our approach is based on result re-ranking, as we focus on raising the precision of the top results presented to the user rather than improving recall, as users are prone to only inspect the top results returned by Web search engines. We also investigate more formal context construction strategies [5], such as the ostensive model [2].
Similar to [1],[4], we exploit Delicious as the social annotation corpus to improve Web search, but we focus on providing a context-aware search to the user, rather than implementing a popularity measure [1] or a personalized search [4]. Differently from [4], our approach does not require the user to have a valid user profile in the system. Instead, we build a short term profile at runtime, based on the implicit information provided by the user during the search process and the available tagging information provided by users of the social tagging service.1
2. A Folksonomy-based Context Model
We define a folksonomy F as a tuple F = T, U, D, A, where T = , ... ,  is the set of tags that comprise the vocabulary expressed by the folksonomy, U = , ... ,  and D = , ... ,  are respectively the set of users and the set of documents that annotate and are annotated with the tags of T, and A = U × T × D is the set of annotations of the form , ,  where  is a tag assigned by a user  to a document  by a user . The profile of document  is defined as a vector  = ,, ... , ,, where , = |, ,    A|  U| is the number of times the document has been annotated with tag . In our Web search scenario, the set of documents D represents the resources present on the Web, and are identified by a URL.
In order to represent the search process of the user, we define a query trail,  = , , ... , ,   D, as a query  and an ordered set of documents in the result set that the user has accessed after issuing the query. We can then define a session trail,  = , ... ,  as an ordered set of query trails that are related
1 Delicious - Social bookmarking, http://delicious.com/

827

to the current search session of the user. When the user executes a query, the current session trail can be analyzed in order to infer a folksonomy-based representation of the user's search context.
We define a context representation C, t as a weighting function that scores the importance of a given tag t based on its degree of relation to the user's current session trail . We based our context representation techniques on the ostensive model [2]:
, t =     ,,
where  is a weight factor that takes into consideration the order of the query trails, i.e. how recently within the current session the query trails occurred. , is a weighting function that indicates how well tag  represents document . By varying the  parameter we can obtain different implicit context representation models [5]. For instance, for  < 1 the context function represents the standard ostensive model [2], where documents that the user opened recently are taken more into consideration for the context representation. In this case  is regarded as a reduction factor. With  = 1 we represent an accumulative model where all opened documents have the same importance when representing the user's context. With an  > 1 documents opened at the beginning of the search session have more importance when representing the user's context. We analyze two different weighting functions: 1) a weighting function -, , that uses the - value of a tag using , as the frequency value; and 2) a weighting function that takes also into consideration the viewing time of the document, by multiplying the previous function by a factor , given by the time in seconds that the user spent viewing the document.
We then use this context representation to re-rank the results of the Web search system. Result documents are re-ordered by the similarity value between the document  and the current context of the user, taking into account the original ranking. We define this similarity value as a scalar product: ,  =  ,  , t. Finally, in order to take into account the actual query of the user, the new ordered result list is aggregated with the original result list using CombSUM with rank based normalization.
3. EXPERIMENTS
In order to test different context-aware adaptation techniques, we defined three search tasks and monitored 14 users while performing each task for 15 minutes. The tasks were performed on a common Web browser. All user interactions were collected using a query and action log toolbar2. In order to act as a baseline, we instructed users to use Microsoft's Bing3 Web search engine when performing a Web search. As a way to obtain relevance judgments, users were instructed to bookmark those results they found interesting for their tasks, but they were not asked to find as many relevant documents as they could. Our intention was that users deviate as little as possible from their normal search methodology. The three search tasks were designed to emulate common search situations. These were: 1) planning a holiday trip; 2) searching for a new electronic product you are interested in buying; and 3) an open task were users could search on any topic of their interest. When each task was finished users had 5 additional minutes to judge an aggregated list of results that appeared during their search session, in order for us to obtain more relevance judgements. The above information constituted our evaluation topics. It is worth noting that the coverage of Delicious
2 Lemur toolbar: http://www.lemurproject.org/querylogtoolbar/
3 Microsoft Bing: http://www.bing.com

was 38% for accessed documents and 51% for accessed documents that users marked as relevant.

Figure 1 presents the results of the evaluation. In order to test our hypothesis, we compare the ranking results of the approaches presented in the previous section (tag source) with the same approaches when using the actual textual content of the documents as its representation (text source). The combination of both sources is not shown as it did not produce better results. As a baseline, we use the original ranking provided by Bing (baseline). Some of the evaluated approaches were our representation techniques with varying values of :  = 0.5 (ostensive),  = 1.0 (accumulated), and  = 1.5 (begin). These three techniques used the - weighing function. We also show the performance of the accumulated technique with the time weighting factor (acum + time). In addition, we adapted the technique presented by Schmidt et al. [3], although their work was originally designed for query expansion. As a metric we show MAP at a cut-off point of the top 50 results. Other metrics such as P@10 and NDCG give similar results.

Table 1: MAP@50 performance of context-aware techniques

Sour baseline osten [3] acum begin acum+ti

tcaeg 0.0969 0.1005 0.0998 0.0998 0.1005 0.1m0e13

text

0.0881 0.0880 0.0880 0.0887 0.0870

The results show that the context-aware approaches had significantly better results when using the folksonomy-based representation of documents than their actual content. These differences were statistically significant for all approaches (Wilcoxon, p< 0.05). Additionally, only the folksonomy-based approaches resulted in an improvement over the baseline, also with statistical significance. From the analysis of the results we can conclude that in our experiment the variations of the  parameter produced no significant effect. The best performing approach was the combination of the accumulated approach and the time sensitive weighting function, which achieved a 15% improvement over the best technique based on textual content and a 4.5% increase over the baseline.

To conclude, we have presented a number of approaches in order to evaluate folksonomy-based context representation techniques. Our results validate our hypothesis that the folksonomy-based representation and exploitation of the user context is more effective than traditional approaches based on the content of the document.

4. ACKNOWLEDGEMENTS
This research was partially supported by the Spanish Ministry of Science and Education (TIN2008-06566-C04-02) and the Regional Government of Madrid (S2009TIC-1542).

5. REFERENCES
[1] Bao, S., Xue, G., Wu, X., Yu, Y., Fei, B., and Su, Z. Optimizing web search using social annotations. In WWW '07, pages 501-510, 2007.

[2] Campbell, I. and van Rijsbergen, C. J.. The ostensive model of developing information needs. In COLIS'96, pages 251268, 1996.

[3] Schmidt, K. U., Sarnow, T., and Stojanovic, L. Socially filtered web search: an approach using social bookmarking tags to personalize web search. In SAC'09, pages 670-674, 2009.

[4] Xu, S., Bao, S., Fei, B., Su, Z., Yu, Y. Exploring folksonomy for personalized search. In SIGIR'08, pages 155-162, 2008.

[5] White, R. W., Ruthven, I., Jose, J. M., and Van Rijsbergen, C. J. Evaluating implicit feedback models using searcher simulations. ACM TOIS., 23(3), pages 325-361, 2005.

828

A Ranking Approach to Target Detection for Automatic Link Generation
Jiyin He and Maarten de Rijke
ISLA, University of Amsterdam, The Netherlands
{j.he, derijke}@uva.nl

ABSTRACT
We focus on the task of target detection in automatic link generation with Wikipedia, i.e., given an N-gram in a snippet of text, find the relevant Wikipedia concepts that explain or provide background knowledge for it. We formulate the task as a ranking problem and investigate the effectiveness of learning to rank approaches and of the features that we use to rank the target concepts for a given Ngram. Our experiments show that learning to rank approaches outperform traditional binary classification approaches. Also, our proposed features are effective both in binary classification and learning to rank settings.
Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software
General Terms: Algorithms, Experimentation
Keywords: Link generation, disambiguation, learning to rank, Wikipedia
1. INTRODUCTION
Annotating text with human defined concepts from ontologies or knowledge bases is useful in providing background knowledge that helps users to understand difficult concepts as well as to capture the meaning of ambiguous words or phrases. Recently, several approaches have been proposed for annotating free text with human defined concepts from Wikipedia or similar knowledge bases in the context of automatic link generation. Here we focus on a sub-task of the automatic link generation problem, namely, the target detection task. That is, given a N-gram in a piece of text, find the related concepts in Wikipedia, i.e., Wikipedia pages. Mihalcea and Csomai [6] formulate the target detection task as a classification problem and propose several linguistic features for learning to link the phrases in free texts to Wikipedia pages. Milne and Witten [5] improve the classification performance by utilizing contextual features, i.e., the surrounding "non-ambiguous" words or phrases, of a given N-gram. Meij et al. [4] study the problem of semantic query suggestion, where each query is linked to a list of concepts from DBpedia, ranked by their relevance to the query. Although presented as a ranking problem, they use binary classification to rank the related concepts. INEX (the INitiative for the Evaluation of XML retrieval) has launched the Link-the-Wiki task, which defines target detection as a ranking problem, as at most 5 target concepts can be returned for an anchor text; various heuristics as well as retrieval-based methods have been proposed [7].
We investigate the effectiveness of learning to rank approaches
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

for target detection. By its very nature, the target detection task is a ranking problem. First, the recommended concepts from the knowledge base may give background knowledge at different granularity levels. For example, given the N-gram "dublin core," while the Wikipedia page on "dublin core" is an obvious choice, the concept "ontology" may also be interesting for a reader unfamiliar with the topic. Second, when context is short, it is usually difficult to be certain about the meaning of an ambiguous word or phrase. Listing possible concepts ordered by decreasing relevance thus effectively avoids missing correct answers while trying to provide the correct answers as early as possible in a ranked list. We explore features that do not rely heavily on the "non-ambiguous" context of a given N-gram (as proposed in [5]), which makes our method easily applicable to short texts such as queries submitted to a search engine.
We experiment with the INEX 2008 Wikipedia collection. The performance achieved by the proposed features is comparable to the state-of-the-art when evaluated with binary classification metrics. However, when measured with ranking-based metrics, our learning to rank approaches using these features significantly outperform binary classification.
2. METHOD
Features We briefly introduce our notation and proposed features. Given a N-gram, we refer to the text snippet (e.g., an article, a paragraph, etc) in which it is contained as a topic and a Wikipedia page to which it links (or should link) as a target. We consider three types of feature for our learning experiments, namely, N-gramtarget features, target features and topic-target features.
N-gram-target features. The N-gram-target features describe how well an N-gram ng and a candidate target ctar are related. Three features are explored in this category: (i) TitleMatch: in Wikipedia, titles usually denote the key concept on which a page focuses and therefore the match between a ng and the title of a ctar indicates the relatedness of the two; we use three values for this feature: 0 (no match), 1 (partial match) and 2 (exact match). (ii) Link Evidence: existing links among Wikipedia pages are effective indicators of how likely ng and ctar are linked, which we measure with the following two scores: RatioLink and RatioAnchor, calculated as RatioLink(ng, ctar) = |link(ng, ctar)| · |inlink(ctar)|-1 and RatioAnchor(ng, ctar) = |link(ng, ctar)| · |ng  A|-1, where |link(ng, ctar)| is the number of times ng and ctar are linked, |inlink(ctar)| is the number of times ctar is linked by some Ngrams and |ng  A| is the number of times ng is annotated as an anchor text and linked to some concepts in Wikipedia. (iii) Retrieval scores: we use ng as query and compute retrieval scores against ctar as a measure of relatedness of the two. BM25 and the Markov Random Field model (MRF) [1] are used.
Target features. The target features are indicators of how likely

831

ctar alone would be linked with some N-grams in Wikipedia. Four features are used, namely, (i) number of inlinks, (ii) outlinks and (iii) Wikipedia categories associated with ctar, which presumably indicate the "popularity" of ctar, and (iv) generality as proposed in [5], i.e., the level of ctar in the Wikipedia category hierarchy.
Topic-Target features. Features of this type describe the relatedness between the context of ng, i.e., the topic t and ctar. Two features are considered: (i) cosine similarity between t and ctar, and (ii) retrieval score using title of ctar as query and t as target document, where BM25 is used as the retrieval model.
Learning to Rank the Target Concepts We employ two learning to rank approaches, namely Ranking SVM [2] and AdaRank [3]. To construct the training set, we use anchor texts in the Wikipedia collection, which ensures that there exists a manually linked target concept. Each instance is an (ng, ctar) pair. Relevance judgements are derived from the manual annotations, i.e., for a ng, the manually linked target concept is judged as relevant, while other candidate target concepts are judged as non-relevant. Although this is a binary judgement, the learning algorithms learn the preference relation between the positive and negative examples. There exist various ways to collect candidate target pages for a given ng, e.g., using ng as a query and retrieving a list of potentially relevant Wikipedia pages. Here we use all the pages that have been linked to the ng at least once in Wikipedia as candidate target concepts. Also, we use various binary classifiers as baseline approaches. Since the binary classifiers output binary decisions, a logistic regression model is fit to the output so as to generate probability distributions for the binary decisions such that candidate target concepts are ranked according to their probabilities of being a positive example.
3. EXPERIMENTS AND RESULTS
For our experimental evaluation, we construct the training, validation and test set from the INEX 2008 Wikipedia collection. We randomly sample 500 pages for training, 100 pages for validation, i.e., to tune model parameters, and 50 pages for testing. The training set contains 11,112 anchor texts, which result in 170,102 instances; the validation set contains 9,365 anchors texts and 106,051 instances; the test set contains 3,452 anchor texts and 33,259 instances. We use existing Wikipedia links as ground truth. Following Meij et al. [4], three binary classifiers are used as baselines: NaiveBayes, SVM with linear kernel and J48, a decision tree type classifier. We use WEKA1 for binary classification and SVMLight for RankingSVM.2 We use MAP, P@1 and P@5 to measure the ranking performance of the binary classifiers as well as the learning to rank approaches.
Table 1 shows the results. P@1 measures how good the first result in the ranked list is, which is important for the target detection task, as ideally we would expect that the first answer is a correct one while the remaining concepts in the ranked list supply complementary material. P@5 measures the unranked early precision. We see that all methods except NaiveBayes have a similar performance in terms of P@5. This suggests that the difference of the rankings generated by different algorithms are limited to the very top of the ranked list, e.g., within the top 5. In general, learning to rank approaches outperform binary classification approaches. AdaRank significantly outperforms all binary classifiers, on all measures.
In addition, Table 2 shows the binary classification results with our proposed features. J48 achieves best performance, which is comparable to the performance of binary classification approches in the literature. In [6] the best performance (F-measure 0.88)
1http://www.cs.waikato.ac.nz/ml/weka/ 2http://svmlight.joachims.org/

was achieved by applying a NaiveBayes classifier with linguistic features. Milne and Witten [5] report an F-measure of 0.96, achieved using a C4.5 classifier with a set of context-based features. The main difference between our features and those proposed in [5] is that our features do not rely heavily on the context "nonambiguous" concepts, which allows our method to be successfully applied to short texts with limited context.

Method Ranking SVM AdaRank SVM-class J48 NaiveBayes

MAP 0.9502 0.9629 0.9476 0.9496 0.9200

p@1 0.9235 0.9395 0.9180 0.9218 0.8699

p@5 0.1970 0.1980 0.1970 0.1968 0.1962

Table 1: Performance of learning to rank approaches compared to binary classification approaches. ( ) denotes significant difference as determined using a paired t-test at level 0.05 between binary classification and RankingSVM (AdaRank).

Method SVM-class J48 NaiveBayes

Precision 0.86 0.93 0.63

Recall 0.82 0.91 0.67

F-measure 0.84 0.92 0.65

Table 2: Performance of binary classifications on the test set.

4. CONCLUSION
We investigated the effectiveness of learning to rank approaches and a set of underlying features for target detection in automatic link generation. Learning to rank approaches outperform binary classifiers in terms of various ranking metrics with the same set of features. Our features are shown to be useful in both binary classification and learning to rank settings. We leave the analysis of the importance of the features for future work. A natural next step is to extend the binary judgements to multiple relevance levels. Also, our approach to target detection can be naturally applied to many real-world problems such as word sense disambiguations as well as semantic query suggestion with Wikipedia.
Acknowledgements This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802.
5. REFERENCES
[1] D. Metzler and W. B. Croft. A Markov Random Field Model for Term dependencies. In SIGIR'05, 2005.
[2] R. Herbrich, T. Graepel and K. Obermayer. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers, 2000.
[3] J. Xu and H. Li. AdaRank: a boosting algorithm for information retrieval. In SIGIR'07, 2007.
[4] E. Meij, M. Bron, L. Hollink, B. Huurnink and M. de Rijke. Learning Semantic Query Suggestions. In ISWC'09, 2009.
[5] D. Milne and I. H. Witten. Learning to link with Wikipedia. In CIKM '08, 2008.
[6] R. Mihalcea and A. Csomai. Wikify!: linking documents to encyclopedic knowledge. In CIKM '07, 2007.
[7] G. Shlomo, J. Kamps and A. Trotman. Advances in Focused Retrieval. In INEX '08, 2008.

832

Using Local Precision to Compare Search Engines in Consumer Health Information Retrieval

Carla Teixeira Lopes
Departamento de Engenharia Informática Faculdade de Engenharia da Universidade do
Porto Rua Dr. Roberto Frias s/n 4200-465 Porto, Portugal
ctl@fe.up.pt
ABSTRACT
We have conducted a user study to evaluate several generalist and health-specific search engines on health information retrieval. Users evaluated the relevance of the top 30 documents of 4 search engines in two different health information needs. We introduce the concepts of local and global precision and analyze how they affect the evaluation. Results show that Google surpasses the precision of all other engines, including the health-specific ones, and that precision differs with the type of clinical question and its medical specialty.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; J.3 [Computer Applications]: Life and Medical Sciences
General Terms: Experimentation, Performance, Human Factors.
Keywords: Health information retrieval, evaluation, precision, user study.
1. INTRODUCTION
Patients, their family and friends are increasingly using the Web to search for health information [2]. The last Pew Internet report on health information [4] reveals that 61% of the american adults look online for health information. In the Internet users, this proportion rises to 83%. A previous study reported that 66% of health information sessions start at generalist search engines (SE) and 27% start at healthspecific websites [3]. Large companies in information retrieval have been developing efforts in the health area (e.g. Google Health and Bing Health) and several health-specific services are also appearing.
This study evaluates the performance of 4 generalist SE (Google, Bing, Yahoo! and Sapo) and 3 specific SE (MedlinePlus, WebMD and Sapo Sau´de) in health information retrieval. The evaluation is based on the data collected in a user study with undergraduate students and work tasks defined according to the framework proposed by Borlund [1]. Besides an overall comparison, the SE are also compared according to the type of clinical question and medical specialty.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Cristina Ribeiro
Departamento de Engenharia Informática Faculdade de Engenharia da Universidade do
Porto/INESC-Porto Rua Dr. Roberto Frias s/n 4200-465 Porto, Portugal
mcr@fe.up.pt
2. METHOD
We conducted a user study with 5 work tasks that were defined based on popular questions submitted to web health support groups. Each work task acts as the context of 4 information needs (IN) that are linked to it. The defined work tasks are available at http://www.carlalopes.com/ research/userstudy2.html and are associated with the following medical specialties: gynecology, dermatology, psychiatry and urology. Moreover, each IN is associated with one of the following types of clinical questions: overview, diagnosis/symptoms, treatment, prevention/screening, disease management, prognosis/outcome.
Each user chose 2 IN that, after being transformed into queries, were submitted to 4 SE selected by the user. Following the pooling approach, each user assessed the relevance of the top 30 documents returned by each SE. Totally there were 82 sets of judged documents, one for each pair of user and IN, from which were excluded duplicates. Forty-one undergraduate users participated in this study (27 females; 14 males) with a mean age of 27.2 years (SD=9.8). These users evaluated 9,572 documents, less than 41 × 2 × 4 × 30 because some queries returned less than 30 documents.
To evaluate and compare the SE we used the 11-point interpolated average precision and the Mean Average Precision (MAP). In these measures we used two types of precision, a local precision that is calculated based on the actual relevance judgments of the users and a global precision, based on the set of relevant documents assembled for the IN in the pool. As human relevance judgments are idiosyncratic, these two types of precision will allow us to compare the real precision - as judged by the users - and the estimated precision - calculated in a test-collection fashion way.
The first type of precision is calculated using a local set of relevant documents that is defined by LRel(u, in) = {doc : doc  P (u, in)  RJ(doc) = 1}. The second uses a global set of reSlevant documents that is defined by GRel(in) = unique( uU LRel(u, in)). In these formulas, u is an user; U represents the set of all users, in is an information need; doc is a document; P (u, in) is the pool of judged documents for user u and information need in and RJ(doc) is the relevance judgment for document doc that can be 0 or 1.
3. RESULTS
With the exception of one IN, all information needs were associated with at least one user. All users chose Google as one of the four SE. The other SE with more selections were

835

Table 1: Local MAP, global MAP and maximum

recall in GRel(in) by search engine

Engine Local MAP Global MAP Recall

Bing

0.56

0.65

0.25

Google

0.74

0.82

0.56

MedlinePlus

0.61

0.69

0.27

Sapo

0.58

0.67

0.22

Sapo Sau´de

0.59

0.71

0.21

WebMD

0.56

0.63

0.29

Yahoo!

0.59

0.63

0.18

the Sapo Sau´de (27 users), Bing (25 users) and MedlinePlus (23 users).
Table 1 presents the Local MAP, calculated with local precision, the Global MAP, calculated with global precision, and the maximum recall of each SE. As global map is more comprehensive, it was expected to have a Local MAP lower than Global MAP, and the difference is in fact significative (p<0.01). From this table we can see that Google is the engine with larger MAP and maximum recall. Although the literature mentions that the variation of individual assessors' judgements have little impact on the the relative effectiveness ranking of different systems [5], we found that the use of these two types of precision slightly changes this ranking. MedlinePlus and Sapo Sau´de, two health-specific SE, appear in both rankings but in reverse order, after Google. Yahoo! is the SE that rises more positions, from 6th to 3rd, in the ranking with local MAP.
Recall is not an uniform measure because each SE has its own collection. Moreover, while generalist SE index contents from several sources, the 3 analyzed health-specific SE only index their own contents. Therefore, the maximum recall in GRel(in), presented in Table 1, is not supposed to be used as a measure to rank the 7 SE. In the generalist SE, we can see that in terms of recall, Bing, Sapo and Yahoo, in this specific order, follow Google. Considering the unfair situation of health-specific SE, an individual analysis of their recall make us predict that, in a pool composed by only their documents, they would have a good recall. The recall issues described above and space constraints made us decide to omit the 11-point interpolated average precision analysis.
We found significative differences in local and global MAP between SE, both with p<0.01. As local MAP is more strict on users' relevance judgements, it is more realistic and therefore will be used in the following analysis. We further studied the differences between every pair of SE and found that Google has a significant larger MAP than the other SE (p<0.05 with Sapo and p<0.01 in the other comparisons). The differences between other SE are not significant.
Our analysis has also focused on the type of clinical question and on the medical specialty of the query. In both cases, we have compared the MAP of each category of the query type and query specialty, the MAP of each SE in each category and the MAP of each category in all SE.
Each IN is associated with one type of clinical question. The small number of submitted queries (< 5) of the Disease Management and Prognosis/Outcome categories, made us omit them from our analysis. In the overall analysis, we found significant differences at =0.1 (p=0.06) between types of queries. The Overview query type has the largest MAP median and the smallest dispersion. A similar analysis

at an engine perspective, show us that only MedlinePlus and Yahoo have significant differences in MAP at query types at =0.1 (p=0.08 and p=0.07, respectively). In MedlinePlus, the Overview and Treatment query types have significant larger MAP at different  (p=0.01 and p=0.07, respectively) than the Prevention/Screening ones. In Yahoo, the Overview category is significantly better in MAP than Diagnosis/Symptoms and Treatment (p=0.07 and p=0.05, respectively). In the comparative analysis of the engine's precision in each query type, in the Prevention/Screening query type, the engines have significant different MAP (p=0.06). In a deeper analysis, we found that in this type of question, Google is significantly better (p<0.05) than MedlinePlus, Sapo and WebMD and also that WebMD is significantly worse than Bing (p= 0.03) and MedlinePlus (p= 0.03).
In the overall analysis on the medical specialty, we found significant differences at =0.1 (p=0.06). At different significance levels, dermatology has lower MAP than gynecology (p=0.01), psychiatry (p=0.00) and urology (p=0.05). In an engine analysis, only Sapo Saude has significant differences between specialties (p=0.09). A deeper analysis show us that in this engine, the psychiatry questions have higher MAP than urology (p=0.09), gynecology (p=0.05) and dermatology (p=0.01). In an specialty analysis, we could verify that there are significant differences between engines in the psychiatry (p=0.04) and dermatology (p=0.08) specialties. In psychiatry, we found that Google has a significant better MAP than Bing (p=0.00), MedlinePlus (p<0.05), Sapo (p=0.04) and Yahoo (p=0.02). On the other hand, Bing has also a worse MAP than MedlinePlus (p=0.02), Sapo Saude (p=0.00) and WebMD (p=0.04). In dermatology, Google has a significant better MAP than Bing (p=0.05), MedlinePlus (p=0.05) and Sapo Saude (p=0.05).
4. CONCLUSION
We have introduced two types of precision, a local and a global one. The former is closer to each user relevance judgments and the latter is similar to the notion of precision used in TREC evaluations. We found that there are significant differences between them and that different ranking of search engines appear with these two type of precisions. Future work will be done on the influence of context features in relevance evaluations by human assessors and on ways to deal with different judgements. We also found that generalist search engines don't have lower precision than healthspecific ones. In fact, Google has shown a precision significantly higher than all the others search engines. It would be interesting to complement this study with an evaluation of the documents' contents by health experts and to analyze its correlation with user's judgements.
5. REFERENCES
[1] P. Borlund. The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), 2003.
[2] R. J. W. Cline and K. M. Haynes. Consumer health information seeking on the Internet: the state of the art. Health Educ. Res., 16(6):671­692, December 2001.
[3] S. Fox. Online health search 2006. Technical report, Pew Internet & American Life Project, 2006.
[4] S. Fox and S. Jones. The social life of health information. Technical report, Pew Internet & American Life Project, June 2009.
[5] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, first edition, July 2008.

836

multi Searcher: Can We Support People to Get Information from Text They Can't Read or Understand?

Farag Ahmed
Otto-von-Guericke University Magdeburg Data & Knowledge Engineering Group 39106 Magdeburg, Germany
farag.ahmed@ovgu.de

Andreas Nürnberger
Otto-von-Guericke University Magdeburg Data & Knowledge Engineering Group 39106 Magdeburg, Germany
andreas.nuernberger@ovgu.de

ABSTRACT
The goal of the proposed tool multi Searcher is to answer this research question: can we expect people to be able to get information from text in languages they can not read or understand? The proposed tool multi Searcher provides users with interactive contextual information that describes the translation in the user's own language so that the user has a certain degree of confidence about the translation. Therefore, the user is considered as an integral part of the retrieval process. The tool provides possibilities to interactively select relevant terms from contextual information in order to improve the translation and thus improve the cross lingual information retrieval (CLIR) process.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Query formulation, Search process
General Terms
Algorithms, Performance, Experimentation, Human Factors
Keywords
cross lingual information retrieval, word sense disambiguation, Arabic
1. CLIR INTERACTION TOOLS
The increasing diversity of internet web sites has created millions of multilingual resources in the World Wide Web. Therefore, there is an urgent need to bridge barriers between languages in order to access this flood of multilingual information. In the past little attention was paid to develop multilingual interaction tools where users are really considered as an integral part of the retrieval process. However, the involvement of the user in CLIR systems by reviewing and amending the query had been studied, e.g., using Keizai [4], Mulinex [1] and recently MIRACLE [3]. These interfaces provide query translation from the source language into the target languages using bilingual dictionaries. Furthermore, they use a sort of "query assistant", which enables interactive disambiguation of the query translation process: supporting the user in selecting the correct translations out of a list of possible translations. In Mulinex the
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

"query assistant" shows how the translated query term translates back into the source language in order to support those users who do not understand the target language. In Keizai, users have to select appropriate translations after examining source language definitions of each possible translation before the search is conducted. An important property of MIRACLE is the immediate feedback in response to any action (selecting/deselecting proposed translations), which gives the user an opportunity to refine the search. The MultiLexExplorer [2] follows a different strategy: it allows users to explore combinations of query term translations by visualizing EuroWordNet relations together with search results and search statistics obtained from web search engines.

2. THE PROPOSED TOOL
multi Searcher deals with several CLIR issues. Firstly, there is translation ambiguity, i.e. one word in one language can have several meanings in another language. Secondly, the user's lack of knowledge in the target language. Here, the tool supports the user by providing interactive contextual information that describes the translation in the user's language. Due to the availability of the language resources needed for Arabic (dictionary and parallel corpora aligned at sentence level1) English was selected as test languages.

2.1 Translation Disambiguation
The translation process starts by translating the query terms; a set of possible translations of each of the query terms are obtained from the dictionary. Based on the translation sets of each term, sets of all possible combinations between terms in the translation sets are generated. Using co-occurrence data extracted from monolingual corpora1, the translations are then ranked based on a cohesion score computed using Mutual Information (MI): Given a query q = {q1, q1, ..., qn}, and its translation set Sqk = {qk, ti}, where 1  k  n, 1  i  mk and mk is the number of translations for query term k. The MI score of each translation combination can be computed as follows:

M I(qt1 ,

qt2 ,

...,

qtn )

=

log2

P (qt1 , qt2 , ..., qtn ) p(qt1 )p(qt2 )...p(qtn )

(1)

with P (qt1 , qt2 , ..., qtn ) being the joint probability of all translated query terms to occur together, which is estimated by
counting how many times qt1 , qt2 , ..., qtn occur together in the corpora. The probabilities p(qt1 )p(qt2 )...p(qtn ) are esti-

1see www.nongnu.org/aramorph and www.ldc.upenn.edu

837

mated by counting the number of individual occurrences of each possible translated query term in the corpora.

2.2 Interactive Contextual Information (ICI)

When the user query is translated, it is looked up in the target language documents index in order to obtain the relevant documents (contextual information) for the translation. In order to get the equivalent documents in the source language the parallel corpora is queried. Since it is possible that some retrieved documents will be very similar ­ which would result in duplicate contextual information ­ the documents retrieved from the source language are automatically grouped and contextual information is selected only once from each cluster. As shown in Fig. 1, the finally selected contextual information is not provided to the user as raw text, but instead a classified representation of each contextual information term will be presented: each term of the contextual information is colored according to its related type and can be selected as disambiguating term (the user's query terms green, suggested terms by the tool based on high frequent co-occurrences in the context of the query bold blue and underlined, all remaining terms blue except stop words that are not selectable and black). For example, consider the following case where the user submitted

the Arabic query "

dyen alh. kwmh ". The query

term "

alh. kwmh " has two translations (the govern-

ment or the administration), while the other term " dyen " has several possible translations .e.g. (Religion) or (Debt). Based on the M I score translation alternatives are displayed in ranked order together with their contextual information. Thus the user has the possibility to select the suitable translation. Here, the translations provided by the system (the government religion) and (the government debt) are correct even though they are used in a different context. This is due to the fact that (government) appears frequently in the context of "religion" or "debt". As shown in Fig. 1, the user is interested in the second ranked translation (debt government). Using the contextual information, the user can select one or more terms to improve the translation. To simplify the user's task, the tool automatically proposed relevant terms (highlighted in bold blue and underlined), .e.g., ("payment", "financial", "lending", "loan"). Once the user

selects, for example, the interactive term " ¯aqr¯ad. " (loan or lending), the tool re-translates the modified query and displays the new translations ("debt government loan", "debt government lending" and "debt administration loan"), to the user. Using search engine integrated web services, the user can, with a simple mouse click, confirm the translation which will then be sent to his favorite search engine, retrieving the results and displaying them.

2.3 Evaluation
We selected randomly 20 Arabic queries from the corpora that included at least one ambiguous word having multiple translations. The number of senses per test word ranged from 1 to 14, and the average was 4.3. The number of query translation combinations ranged from 4 to 200 with the average being 29.1. In order to evaluate the performance of the tool, we used two measurements: applicability and precision. The applicability is the proportion of the ambiguous words that the algorithm could disambiguate. The precision is the proportion of the corrected disambiguated senses of

Figure 1: The translation alternatives with their contexual information in the source language.
the ambiguous word. The evaluation has been performed using monolingual corpora over the 20 test queries. The applicability and precision were 80% and 70%, respectively. The tool has been initially tested by 5 users who have (no knowledge or little knowledge) about the target language. The results were very encouraging, in that the tool could give the users a certain degree of confidence about the translation. Furthermore, the possibility to interactively select term/terms from the contextual information in order to improve the translation was praised.
3. CONCLUSIONS AND FUTURE WORK
We proposed a context-based CLIR tool, to support the user, in having a certain degree of confidence about the translation. It provides the user with interactive contextual information in order to involve her/him in the translation process. The translation ambiguity was taken into account by the use of a MI score based approach. Experiments about the accuracy of the tool proved that the tool has a certain degree of translation accuracy. In addition, a small pilot user study (5 participants) was conducted. A larger user study has already been designed and is underway.
4. REFERENCES
[1] J. Capstick, A. K. Diagne, G. Erbach, H. Uszkoreit, A. Leisenberg, and M. Leisenberg. A system for supporting cross-lingual information retrieval. Inform. Proc. and Management, 36(2):275­289, 2000.
[2] E. W. D. Luca, S. Hauke, A. Nu¨rnberger, and S. Schlechtweg. MultiLexExplorer - combining multilingual web search with multilingual lexical resources. In Combined Works. on Language-enhanced Educat. Techn. and Devel. and Eval. of Robust Spoken Dialogue Sys., pages 71­21, 2006.
[3] D. W. Oard, D. He, and J. Wang. User-assisted query translation for interactive cross-language information retrieval. Inform. Proc. and Management, 44(1):181­211, 2008.
[4] W. C. Ogden and M. W. Davis. Improving cross-language text retrieval with human interactions. In 33rd Hawaii Intl. Conf. on System Sciences, volume 3, page 3044, 2000.

838

Linking Wikipedia to the Web
Rianne Kaptein1 Pavel Serdyukov2 Jaap Kamps1
1 University of Amsterdam 2 Delft University of Technology The Netherlands
kaptein@uva.nl p.serdyukov@tudelft.nl kamps@uva.nl

ABSTRACT
We investigate the task of finding links from Wikipedia pages to external web pages. Such external links significantly extend the information in Wikipedia with information from the Web at large, while retaining the encyclopedic organization of Wikipedia. We use a language modeling approach to create a full-text and anchor text runs, and experiment with different document priors. In addition we explore whether social bookmarking site Delicious can be exploited to further improve our performance. We have constructed a test collection of 53 topics, which are Wikipedia pages on different entities. Our findings are that the anchor text index is a very effective method to retrieve home pages. Url class and anchor text length priors and their combination leads to the best results. Using Delicious on its own does not lead to very good results, but it does contain valuable information. Combining the best anchor text run and the Delicious run leads to further improvements.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]
General Terms: Experimentation, Measurement, Performance.
Keywords: Link Detection, Entity Search, Wikipedia.
1. INTRODUCTION
Wikipedia is a natural starting point for information on almost any topic. As a result, Wikipedia is one of the top ranked results for all queries matching an article's title. But where to go if you want to know more? Can we point searchers directly to other relevant web pages? For this purpose, many Wikipedia pages contain `External Links' to web pages. According to the guidelines,1 the links in the External Links section should link to sites that contain neutral and accurate material that is relevant to an encyclopedic understanding of the subject. For example, pages about entities should link to its official home page, and pages about media to a site hosting a copy of the work.
However, only some 45% of all Wikipedia pages have an `External links' section. Hence, our research question is:
Can we automatically find external links for Wikipedia pages?
To evaluate how well we can find external links for Wikipedia pages, we construct a test collection by removing the currently existing links in Wikipedia, and using these links as our ground truth. This is similar to the INEX Link-the-Wiki task [2] where the task consists of finding links between Wikipedia pages. Our task is to
1http://en.wikipedia.org/wiki/Wikipedia:External_links
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

find links from Wikipedia pages to external web pages. We use the Clueweb category B, consisting of 50 million English web pages as our test collection to find the external web pages.
To validate that Wikipedia's external links indeed correspond to official home pages, we use the assessments of the 2009 TREC entity ranking task. These assessments contain 60 relevant Wikipedia pages with at least one linked website in the Clueweb collection. When we consider the entity as a query, and urls found in `External links' as ranked pages a Mean Reciprocal Rank of 0.768 is attained for finding the home pages. That is, there is a high level of agreement between the External Links in Wikipedia and the independent judgment of a TREC assessor on what constitutes the home page for an entity.
2. EXTERNAL LINK DETECTION
2.1 Task and Test Collection
Our task is defined as follows: Given a topic, i.e. a Wikipedia page, return the external web pages which should be linked in the `External Links' section. We have created a topic set by reusing relevant entities found in the TREC Entity Ranking task. The topic set contains 53 topics with 84 relevant home pages. A topic can have more than one relevant home page, because the Clueweb collection contains duplicate pages, i.e. pages with the same normalized url. We match the urls of the existing external links on the Wikipedia pages with the urls in the Clueweb collection. External links on entity pages are split into two parts, the first external link is a home page, the other links are usually informational pages. In our experiments we only use the home pages.
2.2 Link Detection Approaches
We experiment with three approaches. First, our baseline approach is a language model with a full-text index. Secondly, we make an anchor text index, which has proved to work well for home page finding [1]. We experiment with different document priors for both indexes. We construct priors for the document length, anchor text length, and the url class [3]. To determine the url class, we first apply a number of url normalization rules, such as removing trailing slashes, and removing suffixes like `index.html'. Since we have no training data, we cannot estimate prior probabilities of url classes based on the distribution of home pages in the training collection. Instead we use only two url classes: root pages (a domain name not followed by any directories) receive a prior probability a 100 times larger than non-root pages, which is a conservative prior compared to the previous work [3]. Our third approach exploits information of social bookmarking site Delicious. Delicious ranks search results by relevance, taking into account bookmark titles, notes, and tags, among other things. We send a search re-

839

Table 1: Language Modeling Results

Prior None Doc. length Anchor length Url class Anch.length + Url

Full-text

M RR Suc@5

0.0385
0.0085
0.0853 0.2348· 0.2555·

0.0364
0.0000
0.1636 0.2727· 0.2909·

Anchor M RR Suc@5
0.5865 0.7091 0.4178· 0.5455·
0.6131 0.6909
0.6545 0.7273 0.6774 0.7636

Significance of increase or decrease over "None" according to t-test, one-tailed, at significance levels 0.05(), 0.01(·), and 0.001(·).

quest to the site and match the first 250 results with the urls in the Clueweb collection to create a ranking. As retrieval score we use the (inverted) ranks. To make combinations with our language model runs we normalize all scores using the Z-score and make a linear combination of the normalized scores.
For our experiments we use the Indri toolkit. We build two indexes: an anchor text and a full text index. Both indexes are stemmed with the Krovetz stemmer. We have created document priors for document length, anchor text length, and url class. For all our runs we apply Dirichlet document smoothing. To construct the query we always use the title of the Wikipedia page. We use Mean Reciprocal Rank (M RR) and Success at 5 (Suc@5) to evaluate our runs.
2.3 Link Detection Results
Results of our experiments using the language modeling approach are shown in Table 1. The anchor text index leads to much better results than the full-text index. Home pages often contain a lot of links, pictures, and animations, and not so much actual text, so it was to be expected that the anchor text index is more effective. For the same reason, applying a document length prior deteriorates the results: longer documents are not more likely to be a relevant home page.
The two other document priors do lead to improvements. The full-text index run has much more room for improvement, and indeed the priors lead to a major increase in performance, e.g. using the url class prior increases the MRR from 0.0385 to 0.2348. The improvements on the anchor text runs are smaller. The anchor text length prior does not affect the results much. A reason for this can be that the Dirichlet smoothing also takes into account the document length, which equals the anchor text length for the anchor text run. Despite its simplicity, the url class prior leads to significant improvements for both the full-text and the anchor text runs. Since we did not have training data available, we did not optimize the url class prior probabilities, but used a conservative prior on only two classes. Combining the full-text runs and the anchor text runs does not lead to improvements over the anchor text run. We experimented also with including different parts of the Wikipedia page in the query, such as the first sentence and the page categories, but none of these runs improved over using only the title of the page. By analyzing the failure cases, we identify three causes for not finding a relevant page: the external link on the Wikipedia page is not a home page, the identified home page is redirected or varies per country, and the Wikipedia title contains ambiguous words or acronyms.
Besides the internal evidence, we also looked for external evidence to find home pages. The results of the run using Delicious, and a combination with the best anchor text run can be found in Table 2. The Delicious run performs better than the full-text run, but not as good as the anchor text run. One disadvantage of the Deli-

Table 2: Delicious Results

Run

M RR Suc@5

Delicious 0.3597 Comb 0.7119 Anchor 0.6774

0.4000 0.7818 0.7636

cious run is that it does not return results for all topics. Some topics with long queries do not return any results, other topics do return results, but none of the results exists in the Clueweb collection. For 49 topics Delicious returns at least one result, for 41 topics at least one Clueweb page is returned. Around half of all returned results are part of the Clueweb collection. When we combine the Delicious run with the best anchor text run we do get better results, so Delicious is a useful source of evidence. Most of the weight (0.9) in the combination is on the anchor text run though. The Delicious run retrieves 68 relevant home pages, which is more than the 58 pages the anchor text run retrieves. The Delicious run however contains more duplicate pages, because it searches for all pages matching the normalized url retrieved by searching Delicious. In the combination of runs, pages found both by Delicious and by the anchor text run, end up high in the ranking.
When we compare our results to previous home page finding work, we can make the following remarks. Most differences can be attributed to the test collections. Clueweb is crawled in 2009, and in comparison to older test collections the full-text index performs much worse. Modern home pages contain less relevant text and more pictures, photos and animations, making the full-text index less informative. The anchor text index on the other hand, performs better than ever before. The Clueweb collection is larger than previous collections, and has a higher link density, so there is more anchor text available for more pages.

3. CONCLUSION
In this paper, we investigate the task of finding external links for Wikipedia pages. We have constructed a test collection of topics about different entities, with their corresponding relevant home pages. Two language modeling approaches, one based on a fulltext index, and one based on an anchor text index have been investigated. In addition a run based on the Delicious bookmarking site is made. All anchor text runs perform much better than the full-text index runs. Useful document priors are the anchor text length and the url class. Delicious on itself does not perform so well, but it is a useful addition when it is combined with an anchor text run. We can conclude our system is effective at predicting the external links for Wikipedia pages.
Acknowledgments The test collection for Wikipedia external link detection is available at http://staff.science.uva.nl/~kamps/effort/data. This research was supported by the Netherlands Organization for Scientific Research (NWO, under project # 612.066.513). We thank Arjen de Vries for useful discussions on related issues.

REFERENCES
[1] N. Craswell, D. Hawking, and S. Robertson. Effective site finding using link anchor information. In SIGIR '01, pages 250­ 257, 2001.
[2] D. W. Huang, Y. Xu, A. Trotman, and S. Geva. Overview of INEX 2007 link the wiki track. In Focused Access to XML Documents, pages 373­387, 2008.
[3] W. Kraaij, T. Westerveld, and D. Hiemstra. The importance of prior probabilities for entry page search. In SIGIR '02, pages 27­34, 2002.

840

Short Text Classification in Twitter to Improve Information Filtering

Bharath Sriram, David Fuhry, Engin Demir, Hakan Ferhatosmanoglu
Computer Science and Engineering Department, Ohio State University, Columbus, OH 43210, USA {sriram,fuhry,demir,hakan@cse.ohio-state.edu}

Murat Demirbas
Computer Science and Engineering Department, University at Buffalo, SUNY, NY 14260, USA demirbas@cse.buffalo.edu

ABSTRACT
In microblogging services such as Twitter, the users may become overwhelmed by the raw data. One solution to this problem is the classification of short text messages. As short texts do not provide sufficient word occurrences, traditional classification methods such as "Bag-Of-Words" have limitations. To address this problem, we propose to use a small set of domain-specific features extracted from the author's profile and text. The proposed approach effectively classifies the text to a predefined set of generic classes such as News, Events, Opinions, Deals, and Private Messages.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information filtering.
General Terms
Algorithms, Performance, Experimentation.
Keywords
Short text, classification, Twitter, feature selection.
1. INTRODUCTION
Twitter1 is a social networking application which allows people to micro-blog about a broad range of topics. It helps users to connect with their followers. The tweets from users are referred to as microblogs because there is a 140 character limit imposed by Twitter for every tweet. This lets the users present any information with only a few words, optionally followed with a link to a more detailed source of information. The goal of our work is to automatically classify incoming tweets into different categories so that users are not overwhelmed by the raw data. This is particularly useful when Twitter is accessed via hand held devices like smart phones.
Existing works on classification of short text messages integrate messages with meta-information from other information sources such as Wikipedia and WordNet [2,3]. Sankaranarayanan et al [6] introduce TweetStand to classify tweets as news and non-news. Automatic text classification and hidden topic extraction [5] approaches perform well when there is meta-information or the context of the short text is extended with knowledge extracted using large collections.
We propose an intuitive approach to determine the class labels and the set of features with a focus on user intentions on Twitter [4] such as daily chatter, conversations, sharing information/URLs, and
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

reporting news. Our approach is more general when compared with the TweetStand. It classifies incoming tweets into categories such as News (N), Events (E), Opinions (O), Deals (D), and Private Messages (PM) based on the author information and features within the tweets. Experimental results show that classification accuracy is high even without meta-information and the proposed approach outperforms the traditional "Bag-Of-Words" strategy.
Empirical results show that the authorship plays a crucial role in classification. Authors generally adhere to a specific tweeting pattern i.e., a majority of tweets from the same author tend to be within a limited set of categories.
2. FEATURE SELECTION
Selecting a subset of relevant features for building robust learning models2 is another research problem. Hence we used a greedy strategy to select the feature set, which generally follows the definitions of classes. We extracted 8 features (8F) which consist of one nominal (author) and seven binary features (presence of shortening of words and slangs, time-event phrases, opinioned words, emphasis on words, currency and percentage signs, "@username" at the beginning of the tweet, and "@username" within the tweet). In the classification step, the learning model trains itself using these features. Here we discuss how these features may represent certain classes.
Categorization of tweets into the selected classes requires the knowledge of the source of information. Hence, we selected the authorship information as our primary feature. Corporate tweeters generally have different motivations than personal tweeters. While the former generally publish news in a clear form, the latter instead frequently express themselves by using slang words, shortenings and emotions. Thus, a feature for discriminating news may be the absence of shortenings, emotions, and slang words. This feature can be further used to differentiate the personal tweeters from corporate tweeters.
If we define an event as "something that happens at a given place and time", the presence of participant, place, and time information could determine the existence of an event in the text. Hence, we extracted the date/time information and time-event phrases which are collected from a set of tweets based on general observation of users and set the presence of them as a feature. Participant information is also captured via the presence of the `@' character followed by a username within tweets.
Presence of opinions is determined by a lookup in a wordlist which consist of about 3000 opinionated words obtained from the Web.
1 http://www.twitter.com 2 http://en.wikipedia.org/wiki/Feature_selection

841

Figure 1. Overall accuracies.
We also capture the emphasis on words based on the usage with uppercase letters. Another way to detect the emphasis is the usage of repeating characters in a word (e.g., "veeery").
The keyword "deal" and special characters within the text such as currency and percentage signs are good features to capture the context of deals.
Twitter lets the users send private messages to other users by using the `@' character followed by a username at beginning of the tweet. Hence, private messages are captured by the usage "@username" at the beginning of tweets.
3. EXPERIMENTAL RESULTS 3.1 Experimental Setup
We downloaded a collection of recent tweets from random users and eliminated the ones not in English, with too few words (threshold set as three), with too few words apart from greeting words, with just a URL, and with too few words apart from URL. Our final collection is composed of 5407 tweets from 684 authors. These tweets were manually labeled with the best matching category (i.e., 2107 N, 625 O, 1100 D, 1057 E, and 518 PM). After removing the stop words, there are 6747 unique words.
Experiments are conducted with the available implementation of Naïve Bayes classifier in WEKA3 using 5-fold cross validation.
3.2 Performance Evaluation
In Figs 1 and 2, BOW, BOW-A, and 8F refer to Bag-Of-Words, BOW with the author feature, and our approach, respectively. As shown in Fig 1, 8F achieves 32.1% improvement over BOW on the overall accuracy. The author feature is found to be very discriminative in our dataset. BOW-A achieves 18.3% improvement over BOW, and even 3.7% over 7F+BOW (without authorship) on the overall accuracy.
As shown in Fig 2, 8F performs consistently better for all classes. It may be used with BOW to have a better accuracy with an additional time cost of initial training. 8F achieves 35.2%, 103.4%, 12.2%, 9.9%, and 87.0% improvements over BOW for N, O, D, E, and PM, respectively. In BOW, misclassified tweets are mainly between N and PM (383), N and O (407), whereas in 8F, they are mainly between N and O (104). We attribute this to the fact that tweets in N may also be opinionated. We believe that multi-label classification would resolve this issue to a certain extent.
3 http://www.cs.waikato.ac.nz/ml/weka/

Figure 2. Accuracies for individual classes.
The times taken to build the training models are 37.2 and 0.8 sec for BOW and 8F, respectively. The ratio between these timings will be larger with larger collections as the number of words (features in BOW) will increase, while the feature count in 8F stays fixed.
4. CONCLUSION
We have proposed an approach to classify tweets into general but important categories by using the author information and features within the tweets. With such a system, users can subscribe to or view only certain types of tweets based on their interest.
Experimental results show that BOW approach performs decently but 8F performs significantly better with this set of generic classes. With the usage of a small set of discriminative features, our approach provides a baseline to classify new tweets online with a better accuracy. However, noisier data may degrade the performance of the proposed approach; hence noise removal techniques are necessary in such cases.
We are currently working on incremental classification models to update the set of categories and features dynamically using user's feedback. As a future work, we are planning to support similarity search within our classes supplemented with semantic information gathered from URL information [1] in the tweets. We believe that this will result in higher precision and be especially useful when Twitter is accessed on hand-held devices where performance and accuracy are the major concerns.
5. REFERENCES
[1] Altingovde, I.S., Demir, E., Can, F., and Ulusoy, O. Site-based
dynamic pruning for query processing in search engines. In Proc. SIGIR (Singapore, July 2008), 861-862.
[2] Banerjee, S., Ramanthan, K., and Gupta, A. Clustering short text using
Wikipedia. In Proc. SIGIR (Amsterdam, The Netherlands, July 2007), 787-788.
[3] Hu, X., Sun, N., Zhang, C., and Chua, T.-S. Exploiting internal and
external semantics for the clustering of short texts using world knowledge. In Proc. CIKM (Hong Kong, China, Nov. 2009), 919-928.
[4] Java, A., Song, X., Finin, T., and Tseng, B. 2007. Why we twitter:
understanding microblogging usage and communities. In Procs WebKDD/SNA-KDD '07 (San Jose, California, August, 2007), 56-65.
[5] Phan, X.-H., Nguyen, L.-M., and Horiguchi, S. Learning to classify
short and sparse text & web with hidden topics from large-scale data collections. In Proc. WWW (Beijing, China, Apr. 2008), 91-100.
[6] Sankaranarayanan, J., Samet, H., Teitler, B. E., Lieberman, and M. D.,
Sperling, J. TwitterStand: news in tweets. In Proc. ACM GIS'09 (Seattle, Washington, Nov. 2009), 42-51.

842

Can Search Systems Detect Users' Task Difficulty?
Some Behavioral Signals
Jingjing Liu, Chang Liu, Jacek Gwizdka, Nicholas J. Belkin
School of Communication and Information, Rutgers University 4 Huntington Street, New Brunswick, NJ 08901, USA
{jingjing, changl}@eden.rutgers.edu, {jacekg, belkin}@rutgers.edu

ABSTRACT
In this paper, we report findings on how user behaviors vary in tasks with different difficulty levels as well as of different types. Two behavioral signals: document dwell time and number of content pages viewed per query, were found to be able to help the system detect when users are working with difficult tasks.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ relevance feedback, search process.
General Terms
Performance, Experimentation, Human Factors.
Keywords
Dwell time, First dwell time, Queries, Task type, Task difficulty.
1. INTRODUCTION
Despite the fact that search engines have done a good job with some easy tasks, for example, "when and where will SIGIR 2010 be held?", people still have difficulties in finding information for difficult tasks, for example, "which food additives pose a risk to your physical health?"[2]. Better search systems are needed that can help people more easily locate desired information in difficult tasks, meanwhile, it is also important for the system to be able to detect when the users are working with difficult tasks.
Task difficulty has been attracting much research attention and has been found to be a significant factor influencing users' search behaviors and search performance. In difficult tasks, users are more likely to visit more web pages ([2], [3]), issue more queries ([1], [3]), and spend longer time on search result pages ([1]), and so on. However, the behavioral and performance aspects addressed in most previous work have focused mainly on the overall task level, so that the variable parameters cannot be obtained until the end of the whole task. Therefore, it is not easy or practical for systems to detect task difficulty in real time based on the previously examined behavioral aspects. More work is needed to explore behavioral signals for systems to learn dynamically if users are dealing with a difficult task so that they can adapt search towards the user's specific situation.
Task type is another factor that has attracted much attention in studying searchers' behavioral differences. Among the many
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

existing classification methods, one classifies tasks as closed (specified factual answer(s)) or open-ended (unspecified answer(s)), and with single or multiple answer(s). It was found that for open-ended tasks, users spent more overall time and performed more moves than for closed tasks [4]. Like task difficulty, there is also a need to look at the behavioral factors which are real-time detectable to differentiate task types.
One such behavioral signal that is good for real-time detection is document dwell time, which measures how long a user reads the retrieved document. Dwell time has been studied as a potential factor for predicting document usefulness [6] but not in conjunction with task difficulty yet. In addition, previous studies tended to look at mean dwell time on all behaviors including time reading both search result list pages and content pages, as well as time (re)formulating queries. We think that mean dwell time on the content pages only would be more informative since the content pages are what users really focus on in finding documents. In addition, the first dwell time of a content page, i.e., the dwell time each page was first shown to users, could be easily tracked by the system in earlier phases of an information-seeking session; thus it would be helpful to see if it can help predict task difficulty.
Another behavioral signal which could be good for real time tracking is the number of viewed documents per query. These signals are named "section-level" factors in this study, different from task-level ones that cannot be captured until the end of a task. We wanted to see if section-level behaviors can suggest task difficulty levels and task types, and if so, how.
2. METHOD
Data came from a lab experiment that was designed to explore users' behavioral differences in different types of tasks. The study had 12 search tasks: 8 of them were created by Toms et al. [5], 4 of which were open-ended tasks and the other 4 were multipleitem closed tasks; the remaining 4 were created by us, and were single-item closed tasks. Participants were asked to conduct 6 tasks of different types on the English Wikipedia. For each task, each participant was allowed to choose between two tasks of the same type but on different topics. Each experiment session was 1.5 to 2 hours long, and user interaction with the computer was logged. Participants were 48 students (17 females and 31 males), with an average age of 27 years. After completing each task, users were asked to rate the difficulty level of the task based on a 5point scale. For the purpose of the current analysis, we chose 8 tasks out of 12: 4 were open-ended tasks and the other 4 were single-item closed tasks.

845

Table 1. User behavioral differences in different tasks

Factor Level

Factors

Tasks of different difficulty levels

Mean (Standard Deviation)

Easy

Difficult

t/Z (p value)

Tasks of different types

Mean (Standard Deviation)

t/Z (p value)

Closed

Open-ended

Section- # of content pages per query

3.29 (4.34)

3.75 (3.27)

1.63(.103)

2.67 (2.84)

4.22 (4.82)

3.98(.000*)

level

First dwell time (sec.)

23.20 (25.91) 29.41 (31.17) 3.29(.001*)

29.81 (29.47)

24.64 (28.28) 2.99(.003*)

factors

Average dwell time (sec.) 20.52 (26.01) 24.33 (28.42) 3.29(.001*)

25.43 (29.75)

21.15 (26.12) 2.40(.016*)

Task-

Task completion time (sec.)

342 (271)

684 (382)

7.01(.000*)

305 (227)

580 (382)

6.94(.000*)

level

# of queries

2.29 (1.91)

4.78 (2.83)

6.66(.000*)

2.27 (2.05)

3.94 (2.70)

5.40(.000*)

factors

# of content pages viewed

6.82 (8.07)

13.43 (9.67)

5.89(.000*)

5.18 (5.76)

12.80 (10.27) 6.80(.000*)

* means significant differences were detected.

3. RESULTS & DISCUSSION 3.1 Behavior Differences with Task Difficulty
We first looked at users' behavioral differences in tasks of different difficulty levels (Table 1). The original rating scale was 5-point, which was appropriate for users to evaluate. In our analysis, the difficulty scores were collapsed into 2 groups based on the distribution (scores 1-3 into a "difficult" group, and scores 4-5 into an "easy" group), which was more appropriate for the system to differentiate difficulty levels.
Examining the task-level factors, i.e., task completion time, number of queries, and number of content pages viewed, we found that more difficult tasks were associated with longer task completion time, more queries, and more content pages. These patterns are consistent with findings of previous studies.
Considering the three section-level factors, i.e., first dwell time, average dwell time, and number of content pages viewed per query, our results show that more difficult tasks were associated with longer first dwell time and longer average dwell time than easier tasks. Dwell times thus seem to be indicators of difficulty levels of the tasks. However, the numbers of content pages viewed by users per query did not differ between easy and difficult tasks. This suggests that although users issued more queries in difficult tasks, they did not necessarily view more content pages per query. Users viewed roughly the same numbers of documents per query in tasks of different difficulty levels. In difficult tasks, they just reformulated queries more often.
3.2 Behavior Differences with Task Type
We also looked at users' behavioral differences between two types of tasks (Table 1). With respect to the task-level factors, open-ended tasks were associated with longer completion time, more queries, and more content pages viewed. These patterns are consistent with findings in previous studies.
For the section-level factors, it was found that closed tasks were associated with fewer content pages viewed per query, longer first dwell time, and longer average dwell time. It was reasonable to see that users spent longer dwell time on documents in the closed tasks because these tasks required users to look for a specific piece of information in a document and to judge its relevance according to its correctness with respect to the task question. By contrast, in the open-ended tasks, users could make a judgment of the overall relevance of the document based on any piece of information on the page. That closed tasks elicited fewer content pages per query was also reasonable considering that the users only needed to look for one fact or piece of information, unlike in the open-ended tasks, where users had to collect as much information as possible.

3.3 Predicting Task Difficulty and Task Type
Results show that longer first dwell time (about 29 seconds) or longer average dwell time (about 24-25 seconds) was associated with both difficult tasks and closed tasks. It is hard to say if the longer first dwell time on a document and the longer average dwell time were due to users dealing with a difficult task or a closed task. Hence, it would be difficult to make a prediction that users are having difficulty based solely on dwell time. However, the number of content pages viewed per query can help differentiate task type, and can be used to help determine if the longer dwell time suggests that users are facing more difficult tasks.
4. CONCLUSIONS
First dwell time and average dwell time can be useful for systems to instantly assess user behaviors. We found that dwell time measures alone cannot reliably predict if users are facing difficult tasks. However, taking into account the number of content pages per query can help make a more correct prediction of task difficulty. Future studies will make use of these findings in system design.
5. ACKNOWLEDGMENTS
This research is sponsored by IMLS grant LG#06-07-0105.
6. REFERENCES
[1] Aula, A., Khan, R. & Guan, Z. (2010). How does search behavior change as search becomes more difficult? Proc. CHI, 35-44.
[2] Gwizdka, J., Spence, I. (2006). What can searching behavior tell us about the difficulty of information tasks? A study of Web navigation. Proc. of Annual Meeting of the American Society for Information Science and Technology '06.
[3] Kim, J. (2006). Task difficulty as a predictor and indicator of web searching interaction. Proc. CHI, 959-964.
[4] Marchionini, G. (1989). Information-seeking strategies of novices using a full-text electronic encyclopedia. Journal of the American Society for Information Science, 40(1), 54-66.
[5] Toms, E., MacKenzie, T., Jordan, C., O'Brien, H., Freund, L., Toze, S. et al. (2007). How task affects information search. Workshop Pre-proceedings in Initiative for the Evaluation of XML Retrieval (INEX), 337-341.
[6] White, R., & Kelly, D. (2006). A study of the effects of personalization and task information on implicit feedback performance. Proc. CIKM , 297-306.

846

Query Log Analysis in the Context of Information Retrieval for Children

Sergio Duarte Torres
University of Twente The Netherlands
duartes@cs.utwente.nl

Djoerd Hiemstra
University of Twente
The Netherlands
hiemstra@cs.utwente.nl

Pavel Serdyukov
Delft University
The Netherlands
p.serdyukov@tudelft.nl

ABSTRACT
In this paper we analyze queries and sessions intended to satisfy children's information needs using a large-scale query log. The aim of this analysis is twofold: i) To identify differences between such queries and sessions, and general queries and sessions; ii) To enhance the query log by including annotations of queries, sessions, and actions for future research on information retrieval for children. We found statistically significant differences between the set of general purpose and queries seeking for content intended for children. We show that our findings are consistent with previous studies on the physical behavior of children using Web search engines.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation
General Terms
Experimentation, Measurement
Keywords
query log analysis, query intent, query representation
1. INTRODUCTION
The Internet today is widely used by children for information and communication purposes. Unfortunately, most of the current Information Retrieval (IR) systems are designed for adults who have shown to have different search approaches and cognitive skills than children [1]. Thus, there is an increasing need for research aimed at understanding children's search characteristics and to provide them suitable IR systems. Query logs are valuable resources to explore the search behavior of users and have been found highly useful to improve their search experience. In this study, we employed the AOL query log and the DMOZ kids&teens directory to identify differences in the queries and sessions employed by users to retrieve children and general-purpose information. This comparison also allows us to confirm on a large-scale environment previous findings in children physical search on Web search engines [1]. It is important to mention that we are aware of the controversy of the usage of this query log in the research community. For this reason we clarify that no actual user identification is carried out in any of the experiments performed in our study.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Table 1: Most frequent children queries

1) nickjr.com 4) coloring pages 7) the wiggles

2) elmo

5) postopia

8) starfall.com

3) nick jr

6) candystand 9) dora the explorer

2. CHILDREN QUERIES & SESSIONS
The AOL query log [5] contains approximately 36 million entries. We represent this query log as the set:

Log = ui, qi, ti, di, ri | 1  i  n

(1)

where u, q, t, d, r refers to the user ID, query, time of submission, domain clicked and its rank position respectively, n defines the size of the query log.
The identification of queries employed to retrieve content for children was performed by matching the DMOZ kids&teens entries tagged for kids (which point to appropriate Web content for children up to 12 years old) with the clicked domains of the query log. Given that the query log does not include the entire URL visited, matches were restricted to the cases in which only the domain is listed as DMOZ entry. The set of these queries is represented by Equation 2.

Kids = ui, qi, ti, di, ri | di  DM OZkids

(2)

Sessions were constructed by grouping contiguous queries from the same user that are submitted with a time difference smaller than . A formal definition of session is shown in Equation 3.

S = qi1 , ui1 , ti1 , ..., qik , uik , tik

(3)

where ui1 = ... = uik , ti1  ...  tik and tij+1 - tij  t for all j = 1, 2..., k - 1 The parameter t was set to 30 minutes because it is the most common value employed in the literature [4]. This time window is also suitable for sessions expressing children's information needs since it has been shown that on average children spend up to 16 minutes to fulfill an information need [2]. We define a children session as a session that contains at least one children query entry.
The data collected contains 485,561 query entries (10,252 unique queries) and 21,009 sessions. The most frequent queries are shown in Table 1. Note that although it is not possible to establish if these queries were submitted by children, we are still able to study the characteristics of the queries and sessions for which the underlying information need is related to children content. We consider that this assumption is reasonable since the DMOZ kids directory employed to identify these queries is realistic and of high quality.

847

Cumulative Frequency 0.0 0.2 0.4 0.6 0.8 1.0

Frequency
0.0 0.1 0.2 0.3 0.4 0.5

Frequency
0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.4

All

All

All

Kids

Kids

Kids

0.3

0.2

Frequency

0.1

All Kids

0.0

1 2 3 4 5 6 7 8 9 10 11
Query length
Figure 1: Length frequency

1 2 3 4 5 6 7 8 9 10 11
Rank position
Figure 2: Rank frequency

1 2 3 4 5 6 7 8 9 10 12 14
Session length (# entries)
Figure 3: Session length

0

10

20 30 40 60 80 1000

Sessions duration (minutes)

Figure 4: Session duration

3. FINDINGS SUMMARY
The analysis was carried out at the query and session level. Query length and domain rank data of the clicked domains were considered for the former and session length, duration and query reformulations for the latter. All the results found for the kid queries are significantly different from the queries of the whole log using the Wilcoxon signed-rank test and the t-test at the 95% confidence level.
3.1 Query length analysis
Query length is an indicator of the complexity of the query and the difficulty of the user to express information needs using keywords. Queries that were used to retrieve information for children were on average (3.23 words per query) significantly longer than the average of the queries in the whole query log (2.5 words per query) as illustrated Figure 1. Interestingly, this finding is in-line with Druin et al.[3] studies in which children aged 8 to 12 are found to formulate longer queries. A more frequent formulation of queries using natural language constructs have also been found in children search behavior [1]. We verified this observation by extracting adjectival and verbal phrases from the queries. We found that 65% of the children retrieval queries contain either of these phrases compared to 56% of the queries in the whole query log. We also found a greater use of questions in the former queries (3.92% vs 2.71%).
3.2 Click analysis
The rank distribution of clicks was collected to compare the retrieval performance between children and general purpose queries. Queries in which highly ranked domains are more often clicked indicate that information needs are more efficiently satisfied by the IR system. Figure 2 shows the rank frequency distribution of clicks. This figure demonstrates that on average the retrieval performance of the children queries is poorer than the queries used to retrieve generalpurpose content since clicks on lower ranked results are more frequent in the children query set (5.77 vs 3.58 average rank). We also observed an important drop for the children queries on the clicks ranked as 10. This can be explained by the fact that children refuse to go beyond the first page more often than older users [3].
3.3 Sessions length analysis
Figure 3 shows that sessions used to retrieve information for children are longer than general-purpose sessions. The longer average length found for the children sessions (8.76 vs 2.8 query entries per session) suggests that these users were not certain of the relevance of the information found since they had to perform more queries and explore more documents. This result is consistent with Bilal's findings [2] in which children showed nonlinear navigation style when solving search tasks. This search style is characterized by the exploration of several choices before a final relevance

judgment is made [2]. This result can also indicate that the documents retrieved by the search engine are not sufficient to satisfy the user's information need.
3.4 Session duration analysis
The duration distribution in minutes of children and generalpurpose sessions are shown in Figure 4. This figure shows that users require more time to explore and complete information needs associated to children content, which suggests more difficulty to solve the information tasks associated to these sessions. This results is consistent with the greater amount of queries and clicks on lower ranked pages found in the children queries. Interestingly, the mean duration found for the children's sessions (20.38 minutes) is in line with the average time reported by Bilal et al.[2] for children that were unsuccessful completing fact-based information tasks (19.69 minutes).
4. ACKNOWLEDGEMENTS
This research is funded in by PuppyIR, a project in the European Union's 7th Framework ICT Programme under grant agreement no. 231507.
5. CONCLUSIONS AND FUTURE WORK
This work represents a valuable methodology to study the search behavior of user's pursuing children information needs given that our findings are in-line with previous studies of children information-seeking behavior on web search engines [1, 2, 3]. This method has also been proved adequate to corroborate on a large-scale setting the results drawn by case-studies on small group of users and can be applied to characterize the search behavior of different user groups. We have enriched the AOL query log by annotating the children queries and session information based on this study and as future work we will employ this resource to study query assistance methods to improve the search experience of children.
6. REFERENCES
[1] D. Bilal. Children's use of the yahooligans! web search engine: Ii. cognitive and physical behaviors on research tasks. J. Am. Soc. Inf. Sci. Technol., 52(2):118­136, 2001.
[2] D. Bilal. Children's use of the yahooligans! web search engine. iii. cognitive and physical behaviors on fully self-generated search tasks. J. Am. Soc. Inf. Sci. Technol., 53(13):1170­1183, 2002.
[3] A. Druin, E. Foss, L. Hatley, E. Golub, M. L. Guha, J. Fails, and H. Hutchinson. How children search the internet with keyword interfaces. In IDC '09: Proceedings of the 8th International Conference on Interaction Design and Children, pages 89­96, New York, NY, USA, 2009. ACM.
[4] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08, pages 699­708. ACM, 2008.
[5] G. Pass, A. Chowdhury, and C. Torgeson. A picture of search. In InfoScale '06: Proceedings of the 1st international conference on Scalable information systems, page 1, New York, NY, USA, 2006. ACM.

848

Transitive History-based Query Disambiguation for Query Reformulation

Karim Filali filali@yahoo-inc.com

Anish Nair anishn@yahoo-inc.com
Yahoo! Labs Santa Clara, CA 95054

Chris Leggetter cjl@yahoo-inc.com

ABSTRACT
We present a probabilistic model of a user's search history and a target query reformulation. We derive a simple transitive similarity algorithm for disambiguating queries and improving history-based query reformulation accuracy. We compare the merits of this approach to other methods and present results on both examples assessed by human editors and on automatically-labeled click data.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: Personalization, Reformulation, Graphical Models
1. INTRODUCTION
The importance of using web search history to personalize and improve search results has long been recognized and several approaches have been proposed in the literature to leverage short or long-term user events to better predict relevant results ([1, 2, 3, 5, 6]). In this paper, we focus on improving query reformulation (a query rewriting technique for better search result selection) by using search history for disambiguating a given user query.
Specifically, we address the problem of adjusting query similarity scores in the presence of additional history context. Given candidate query reformulations, we build a query-history-reformulation model and update reformulation scores that can be used to rerank the candidate set or, as we do, to remove reformulations which become less relevant in the context of the search history.
Our model is general and can be used to derive various scoring algorithms depending on modeling choices. In this work, we describe a simple algorithm whose main feature is the use of the transitive nature of query similarity: given two candidate reformulations (possibly with very different senses) of a query q, the better rewrite is likely the one most related to past search queries relevant to q. By using rewrite-to-history similarity and history-to-currentquery similarity, our model strives to account for the search intent that can be implicit in relevant search history. The algorithm builds on (but is not tied to) an independently-trained general discriminative pairwise query similarity model which extends the machine learned model in [4]. The pairwise similarity model can be treated as a black box and is not the focus of this paper. We just note that we use several types of features (e.g., lexical, semantic, search log-derived) and train the model on both web click log data and examples judged by human editors.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. MODEL AND ALGORITHMS 2.1 History-reformulation Model

Let q be a search query, A the set of alternative reformulations

of q, and H the vector of queries preceding q in the search his-

tory. As an example (from Yahoo's search logs), a user searches for q="bec" and we map q to alternative rewrites, A = {a1 = "bose einstein condensates", a2 = "bahamas electric company", a3 = "basic ecclesial community"}. All three reformulations are rea-

sonable in the absence of other information. Knowing the previous queries H = {h1 = "bahamas", h2 = "facebook", h3 = "bahamas electric"}, however, strongly suggests a2 might the more

appropriate alternative of the three. In this section, we wish to build

a model that encodes the intuition that the subset of the history rel-

evant to the current query should affect query-to-rewrite similarity

through history-to-reformulation similarity.

We introduce a latent relevance indicator set, G (of size m),

whose setting determines the subset of the history related to the

current query, q (search history can consist of many unrelated intents).For a given a  A, we write the joint probability, P (q, a, H)

as

P (q, a, H) = G.m P (q, a, H, G, m) = G,m P (q, a, H|G, m)P (G|m)P (m)

(1)

where P (m) represents a prior on the size of the relevant history and P (G|m), a prior on the distribution of relevant history (e.g., more recent history tends to be more relevant than distant history).
Let RG,m (R when unambiguous) be the subset of H relevant to q given a setting of (G, m), and R¯ = H \ R.

P (q, a, H|G, m) = P (q, a|R, R¯, G, m)P (H) = P (q, a|R)P (R, R¯) = P (q, a, R)P (R¯|R)

(2)

where we assume (q, a) is conditionally independent from (G, m, R¯ ) given R. P (R¯|R) represents the degree to which it is likely to observe the remaining history R¯ given the history relevant to q.
Finally, we decompose P (q, a, R) as

P (q, a, R) = P (R|q, a)P (q|a)P (a) = rR P (r|q, a)P (q|a)P (a)

(3)

where we assume independence of queries in the history conditioned on q and a. Putting together eqn. 1, 2, and 3, we obtain

P (q, a, H) = G,m

P (r|q, a)P (q|a)P (a) (4)

G,m

rRG,m

where G,m is the prior P (G|m)P (m)P (R¯G,m|RG,m). A graphical model (GM) corresponding to the history-reformulation
model is shown in Fig. 1. The idea of reformulation transitivity is embodied in the dependence of q and a through R, even if we removed the direct edge from a to q.

849

a

m

G

q

r1

. . .

rm

Figure 1: GM representation of the history-reformulation model.

2.2 Transitive Disambiguation Algorithm
To implement eq. 4 into an algorithm that takes advantage of an optimized pairwise similarity model, we approximate P (r|q, a) by P (r|a)P (r|q). This also addresses the data sparsity problem in estimating P (r|q, a). Finally, we use a uniform prior P (a) and approximate the summation in eq.4 by a maximization (we pick a minimum query-history similarity threshold, t, so that R = {h  H|s(q, h) > t}, where s(q, h) is the pairwise similarity score for (q, h)) and leave the implementation of the full Bayesian formulation for future work. With a log-space transformation and introduction of a history weight, wh, and history cutoff,  (if no history query passes the  bar of relevance to q, we drop the history contribution altogether), we obtain alg. 1.

Algorithm 1 Transitive disambiguation algorithm

Require: Query q; prev relevant queries R = {r1, . . . , rm}; rewrites A =

{a1, . . . , an}; history weight, wh; history contribution cutoff, 

Ensure: Output rewrites {ai1 , . . . , ain } with new similarity scores, where {i1, . . . , in }  {1, . . . , n}
1: Calculate similarity scores, s(r, q), for each r  R

2: For each rewrite a in A

3: Compute similarity scores s(r, a) for each r  R

4:

Calculate new score snew(q, a)

m
snew(q, a) = w0s(q, a) + (1 - w0) s(ri, a)
i=1

s(q, ri) j s(q, rj )

where w0 = 1 - wh and  =

0.0 max s(q, ri)
i

if max s(q, ri) < 
i
otherwise

2.3 Vector Space Similarity and Random Walks
Another simple approach we propose to leverage history for query reformulation is based on cosine similarity in the vector space whose axes are the degrees of similarity to each query in the search history: a pair (q, a) has high cosine similarity if q and a tend to match the same history H similarly.1
For comparison, we have also implemented the Markov chain random walk method proposed in [1]. A random walk is performed on a query similarity graph, where the nodes are the current query, the user's search history, and candidate rewrites. (q, a) pairs strongly connected to the same history queries have their similarity scores boosted and vice-versa for pairs with weak connections.
3. EVALUATION
We evaluate our algorithms on a sample of 600 history-reformulation examples assessed by editors as good or bad, and on 2M automaticallylabeled examples. The examples are randomly sampled across all Yahoo search users over a period of one month. Automatic labeling is done by calculating reformulation click-through rates (CTR) on a separate large commercial search engine log, and labeling examples with higher than average (normalized by position on the results page) CTR as good reformulations. We summarize the history into a most relevant bag of words to obtain reliable CTR estimates. Search history can go back as far as one month.

1We also extend this approach to the history-rewrite vector space to encourage rewrite set cohesiveness by favoring rewrites with high similarity to each other.

0.65 0.0

0.64

0.25 0.75

0.63

0.62

accuracy

0.61

0.6

0.59

0.58

0.57

0.56

0

0.2

0.4

0.6

0.8

1

w_h

Figure 2: Results on hand-labeled example set with varying history

weight (x-axis) and different  values. Somewhat surprising, putting

all the weight on history (wh = 1) achieves better performance than using direct query-reformulation similarity (wh = 0). This validates the idea of transitive query-history-reformulation similarity in that the

a - r - q path in fig 1 is at least as important as the direct a - q link.

Fig. 2 shows improvements on editorial data when history is used to varying degrees (the no-history baseline, wh = 0, has accuracy 56%). The cosine similarity (§2.3) method has 58% accuracy on the same data, lower than the all-history extreme (wh = 1) but higher than the no-history baseline. This reinforces the importance of indirect query-reformulation similarity. Similar results were observed using the Markov chain method (results did not improve with increasing random walk iterations).
Testing on the much larger automatically labeled data, the disambiguation algorithm shows similar statistically significant gains (60% to 64%). An early version of the disambiguation algorithm was also run on live traffic over a week resulting in a 1% CTR lift--a significant result for a heavily optimized system.
4. CONCLUSIONS AND RELATED WORK
We introduce a general history-reformulation probabilistic model and build a simple disambiguation approach on top of an optimized query-pair similarity function. Compared to the recent vlHMM model in [3], our approach presents the advantages that (i) reformulations are not restricted to history-based rewrites but can be derived from arbitrary sources (in our case, from search sessions, bipartite query-url graphs, and query segment substitutions); (ii) the model can handle long mixed-goal sessions and put a prior on its subsets importance; no history segmentation is imposed (iii) the algorithm is less prone to sparsity problems an applies to rare queries/sessions.
Our algorithm does not require retraining. This is useful to easily apply it to a different setting, such as query-document matching, but can be a limitation in terms of obtaining the best accuracy.
5. ACKNOWLEDGMENTS
We thank the reviewers for their extensive feedback.
6. REFERENCES
[1] P. Boldi et al. The query-flow graph: model and applications. In CIKM '08, pages 609­618, New York, NY, USA, 2008.
[2] H. Cao et al. Context-aware query suggestion by mining click-through and session data. In KDD '08, pages 875­883.
[3] H. Cao et al. Towards context-aware search by learning a very large variable length hidden markov model from search logs. In WWW '09.
[4] R. Jones et al. Generating query substitutions. In WWW'06. [5] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized
search. In CIKM'05, pages 824­831. [6] B. Tan, X. Shen, and C. Zhai. Mining long-term search history to
improve search accuracy. In KDD'06, pages 718­723.

850

Using Flickr Geotags to Predict User Travel Behaviour
Maarten Clements, Pavel Serdyukov, Arjen P. de Vries and Marcel J.T. Reinders
Delft University of Technology, The Netherlands. m.clements,p.serdyukov,m.j.t.reinders}@tudelft.nl CWI, The Netherlands. Arjen@acm.org

ABSTRACT
We propose a method to predict a user's favourite locations in a city, based on his Flickr geotags in other cities. We define a similarity between the geotag distributions of two users based on a Gaussian kernel convolution. The geotags of the most similar users are then combined to rerank the popular locations in the target city personalised for this user.
We show that this method can give personalised travel recommendations for users with a clear preference for a specific type of landmark.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
1. PERSONALISED TRAVEL GUIDES
Before visiting a city, many people consult a travel guide or website that lists the most interesting locations. These travel guides are commonly based on the opinions of all other users. However, people have different preferences and therefore are not equally satisfied by these popularity rankings.
We propose to predict a user's favourite locations in a city based on his travel behaviour in previously visited cities. On social photo sharing websites like www.flickr.com people can annotate their photos, including the geographical location where the photo was made. Also, increasingly more cameras and smartphones are automatically storing the GPS coordinates when a photo is made. These geotags give an accurate indication of the user's preferred landmarks. Based on a set of collected geotags, we define a measure to identify similar users in previously visited cities. Then we aggregate these users' opinions in a different city to obtain a personalized travel recommendation for the target user.
The exploitation of geotags has shown to be effective for various tasks, like global event detection [3] and mapping textual tags to geographical locations [1]. Based on users' GPS tracks, location recommenders have been proposed that attempt to predict popular places and activities near the current location of the user [5, 4].
In this work we predict relevant locations based on users' geotags in a geographically remote location. We show statistical improvements over all users that visited the 10 largest cities and give an effective recommendation example based on an artificial user profile.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Mean(MAP@50)

Optimal sigma for baseline prediction 0.5

0.4

0.3

0.2
Optimal
0.1

0

101

102

V

PC: 25 PC: 50 PC: 100 PC: 200
103

Figure 1: Mean MAP@50 of the baseline prediction in the top-10 cities for several positive cutoff values (P C) and increasing values of the kernel size ().

2. FLICKR GEOTAGS
Using the public Flickr API we have collected the geotags of 36,264 users, who actively use the geotag functionality. Together these users have uploaded 52,425,279 photos of which 22,710,496 have been geotagged.
We keep the data points that lie within the bounding boxes of the ten most visited cities. Based on our data, these cities in order of visitors are: London, New York, Paris, San Fransisco, Los Angeles, Rome, Chicago, Washington, Barcelona, Berlin. We only keep users who have made at least 5 photos in at least two cities. After this constraint the number of geotags of a single user in a city ranges from 5 to 5073 photos, and in total the 4750 remaining users have made 12,669 city visits. Together the users made 526,827 photos on the qualifying trips.
3. METHODOLOGY & RESULTS
Baseline ranking As a baseline prediction we create a scale space representation of all the geotags in each city using a mean shift algorithm, similar to Crandall et al. [1], but using a Gaussian kernel instead of a uniform disc: K(z) = e-z2/22 , where the standard deviation  is used as a scaling parameter. This method finds the maximum values of a kernel convolution of the distribution of all users' geotags with a Gaussian kernel (All). To ensure we reach all local maxima, we initiate the mean shift algorithm with all individual geotags. For each subsequent scale we use the peaks found in the previous scale to initiate the optimalisation procedure. The ranking based on the resulting peak

851

MAP NDCG

0.382

0.267

0.381 0.38

0.266

0.379 0.378

0.265

0.3770 0.2 0.4 0.6 0.8 1 0.264 0 0.2 0.4 0.6 0.8 1

T

T

Figure 2: MAP@50 and NDCG@50 for increasing personalisation weight .

weights gives us the top landmarks for each city, based on
the general popularity.
To evaluate the ranking we judge a recommended location lj as correct if the target user ut has a geotag i within the positive cutoff value (P C) of that location, i : |lj - ut(i)| < P C. Figure 1 gives the mean MAP@50 over the 10 cities, which computes the mean over the precision after each cor-
rect prediction in the top-50. Figure 1 shows that the optimal  is strongly dependent
on the choice of P C. The predictions in this paper will be evaluated at P C = 100 meter, which is roughly the radius of a landmark (e.g. the Colosseum is 189m. long). Based on the baseline results at P C = 100 we select  = 68m. for all further experiments.
Personalised reranking To personalise the landmark ranking for ut in the target city (Ct), we compute the similarity between ut and all other users uc in the similarity city (Cs), where Ct and Cs are any two cities from the top10, both visited by ut. Using the mean shift algorithm we compute the peaks of ut at  = 68m in Cs. For each peak k of ut we now compute the value of the kernel convolution (uc (k)) on the geotags of uc in Cs. The similarity between the two users is now derived by computing the
sum over thePminimum value in the two resulting profiles S(ut, uc) = k min(ut (k), uc (k)). As both profiles are normalised, this will give a similarity score in the range 0-1.
Based on all similar users we now rerank the top-50 popular locations lj, predicted by the baseline method in Ct. This is done by recomputing the kernel convolution at these
locations while weighing each user's gPeotags with his similarity to the target user: Sim(lj ) = uc S(ut, uc)uc (lj ). The top-50 locations are now reranked by a linear combination of the baseline and the personalised score: R(lj) = (1 - )All(lj ) + Sim(lj ).
Figure 2 gives the mean results over all users in the 90 pos-
sible combinations of two cities. The baseline is represented by the score at  = 0, where all user similarities are set to 1. Compared to the baseline, the optimal result on MAP improves 0.3%. At  = 0.2 there are 10,081 trips where we present an improved ranking to the user, against 8,440 trips
where the baseline ranking would have been better. We also
show the NDCG (refer to [2] for details) where the gain of
each correct prediction is assigned as the inverse popularity of that location (1/All(lj)). The increase in NDCG shows that our recommender suggests less popular and therefore
more serendipitous locations.
The improvement on MAP@50 is statistically significant
in 22 out of 90 city pairs (based on a paired t-test with

Table 1: Query: MACBA + Miro

City
London NY NY Paris SF Chicago Washington Berlin Berlin

Rank
1 1 3 3 3 2 1 7 14

Rank
+3 +10 +5 +4 +7 +29 +20 +40 +16

Landmark name
Tate Modern Guggenheim Museum Museum of modern art
Centre Pompidou SF Museum of Modern Art Museum of Contemporary Art
Hirshhorn Museum Hamburger Bahnhof Museum
Neue Nationalgalerie

p < 0.05). For most users the improvement will however not make a big practical difference in the recommended locations. Compared to traditional collaborative filtering data sets, we find that many more people conform to the global popularity ranking if landmarks are concerned. For example, almost all people who visit Paris will make a photo of the Eiffel tower, while people who do not like Sci-Fi movies will never watch Star Wars even though it is one of the most highly ranked movies all times. This makes improving over the baseline a challenging task. Also, we observe many mixed preferences in user profiles (e.g. there are no users who only make photos at zoos), this makes it hard to match similar users.
As an example of the potential benefit of personalized travel recommendations, we created an artificial user profile with 10 geotags scattered around two modern/contemporary art landmarks in Barcelona (MACBA and Miro foundation). Table 1 shows a completely personalised ranking (with  = 1) and the rank difference between the baseline and the personalised ranking for modern art museums in other cities. It is clear that in all other cities where a modern art museum was in the top-50 we obtain a big rank improvement between the baseline and the predicted ranking.
Conclusions A user's favourite landmarks in a previously unvisited city can be predicted by reranking the most popular locations based on users with similar travel preference. Our results indicate that statistical improvement over all users is hard to achieve, but for users with a clear travel preference very accurate predictions can be made.
4. REFERENCES
[1] D. Crandall, L. Backstrom, D. Huttenlocher, and J. Kleinberg. Mapping the world's photos. In WWW '09: Proceeding of the 18th international conference on World Wide Web, pages 761­770, 2009.
[2] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, October 2002.
[3] T. Rattenbury, N. Good, and M. Naaman. Towards automatic extraction of event and place semantics from flickr tags. In SIRIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 103­110, New York, NY, USA, 2007. ACM Press.
[4] Y. Takeuchi and M. Sugimoto. Cityvoyager: An outdoor recommendation system based on user location history. In J. Ma, H. Jin, L. T. Yang, and J. J. P. Tsai, editors, Ubiquitous Intelligence and Computing, volume 4159, chapter 64, pages 625­636. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006.
[5] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yang. Collaborative location and activity recommendations with gps history data. In WWW '10: Proceeding of the 19th international conference on World Wide Web, page 10, New York, NY, USA, April 2010. ACM.

852

Metrics for Assessing Sets of Subtopics

Filip Radlinski
Microsoft Research Cambridge, UK
filiprad@microsoft.com

Martin Szummer
Microsoft Research Cambridge, UK
szummer@microsoft.com

Nick Craswell
Microsoft
Redmond, WA, USA
nickcr@microsoft.com

ABSTRACT
To evaluate the diversity of search results, test collections have been developed that identify multiple intents for each query. Intents are the different meanings or facets that should be covered in a search results list. This means that topic development involves proposing a set of intents for each query. We propose four measurable properties of query-tointent mappings, allowing for more principled topic development for such test collections.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Measurement Keywords: Diversity, Novelty, Subtopic
1. INTRODUCTION
There is increasing interest in producing search results that avoid redundancy, and where results for ambiguous queries cover more of the likely intents for the query (e.g. [3, 4, 5, 6]). A number of researchers have proposed evaluation approaches that take diversity and redundancy into account, including subtopic coverage [1], nugget recall [2] and coverage of Wikipedia disambiguation pages.
However, such evaluation relies on lists of different meanings, aspects or intents for queries. These lists are often created ad-hoc, e.g. by asking human judges to guess possible intents for a query. Given a set of intents generated by some method, there are no well accepted criteria of how the quality of such a set should be measured. We propose four measurable properties for this purpose. We argue that they are desirable for any mapping from queries to intents. We illustrate the properties with a number of examples.
2. FORMALISM
Let q be a query, and i be a textual description of an intent for this query. For example the query "harry potter " may have been issued by a user wishing to satisfy the intent "find the homepage of the official Harry Potter fan club" or "buy the first Harry Potter book ". Given a fixed document collection D, let Ri  D denote the documents relevant to i in D. Similarly, assume that when a user u issues query q they have a well-defined intent in mind. Let Ru(q)  D denote the set of documents relevant to that user.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Given a query q, suppose some algorithm provides a set

I(q) of possible intents for q. Ranking and evaluation is

usually done at the per-document level, hence if two intents

i1, i2  I(q) have different textual descriptions but have the

same relevant documents, for information retrieval purposes

the intents can be considered identical. Thus, we start by

defining the similarity between two intents using the Jaccard

similarity between their relevant documents:

sim(i1, i2)

=

J (Ri1 , Ri2 )

=

|Ri1 |Ri1

 

Ri2 | Ri2 |

(1)

An alternative similarity measure is Cohen's kappa statistic measured on all judgments for i1 and i2. Using the similarity function, we propose four metrics to assess the quality of the mapping q  I(q). These properties are that the intents be Coherent, Distinct, Plausible and Complete.

Coherence. Given a query q and intent i, a relevance judge
should be able to reliably evaluate the relevance of a document with respect to i. This is desirable because it measures the extent to which the intents produced by the method are themselves unambiguous. This is simply standard interjudge agreement. More formally:

Definition 1. An intent i for query q is -coherent if the documents Rij1 and Rij2 judged relevant by two independent judges j1 and j2 for intent i satisfy sim(Rij1 , Rij2 ) > .
Distinctness. For queries with multiple intents, it is impor-
tant that the intents be substantially different in terms of the documents judged relevant. Otherwise, judgment effort may be wasted to measure equivalent intents. Formally:

Definition 2. Intents i1, i2 for query q are -distinct (for a given document collection D) if sim(i1, i2)  1 - .
Plausibility. To avoid too many intents being provided for
a query, all intents provided should be plausible. Intuitively, plausibility means that a sufficient fraction of users who issued q are satisfied with documents that are relevant to i.

Definition 3. An intent i for query q is -plausible if at least fraction  of users who issued the query q satisfy sim(Ru, Ri)  .

Note that plausibility could also be defined as: Given an intent i, is q a plausible way of expressing it? This matches how users select queries. However, defining plausibility as the probability of q given Ri would mean that if q is rare, all intents satisfied by popular documents are implausible, as we are not conditioning on the fact that q was issued. Alternatively, estimating it as the probability of q given i makes the

853

precise wording of i critical, with tiny changes (even ones that don't affect Ri) potentially affecting the plausibility drastically. Our formulation avoids these problems.
Completeness. In generating intents, the frequent mean-
ings of a query should all be present. Otherwise, a diverse ranking for the query may not be useful to many users.
Definition 4. Given a query q, a set of intents I(q) is considered -complete if at least fraction  of users satisfy maxiI(q) sim(Ru, Ri)  .
Note that the tempting definition, that I(q) is complete if no coherent, plausible and distinct intents can be added, is inappropriate: It would consider I(q) = {} complete if q is issued with small probability for many different intents. Instead, our definition requires at least fraction  of users to be satisfied by some intent in I(q).
Also, while plausibility and completeness appear very similar, they are opposites: The former requires that each intent agrees well with the relevance judgments of at least some fraction of users. The later, that for most users there exists at least one intent that agrees well with them.
Since coherence, distinctness and plausibility are defined on individual (or pairs of) intents, we need some way to aggregate given a set I(q). We propose to define each in terms of the minimum of each measure, so that e.g. any implausible intent makes a set of intents implausible.
Finally, while all four metrics are measured on a scale ,   [0, 1], for any one mapping q  I(q) each metric could have different values. Moreover, a "good" value for one metric may be poor for others. We plan to investigate appropriate values for each property in the future.
3. EXAMPLES AND DISCUSSION
We now present four examples of mappings from query to intents (both toy and taken from real datasets). Each illustrates one of the metrics, and how it can be measured.
Toy Example 1. Query: harry potter Intents: (1) Find documents about the movies;
(2) Find documents about the books.
While the intents in this example probably score highly on coherence and distinctness, they would score poorly on completeness. Given that many users who issued this query likely have more specific intents, only a small fraction would have their relevance assessment strongly agree with the judgments for these intents. Now consdier adding an intent (3) Find documents relating to the fictional character. This new intent is very general, much like the first two. Thus completeness would likely not change. On the other hand, adding (4) Find showtimes of the latest movie at my local cinema would improve completeness since some users would likely associate closely with this intent.
Measurability: Completeness of this set could be estimated by asking a some users who issued the query if they agree with document judgments for one of the intents, allowing similarity of Ru with the Ris to be measured.
Toy Example 2. Query: bruce croft Intents: (1) Find Bruce Croft's homepage; (2) Find a list of Bruce Croft's recent publications; (3) Find a biography about Bruce Croft.
This example illustrates distinctness: While the intents are clearly different from an information need perspective, the set of relevant documents would likely overlap substantially.

Measurability: Given judged documents for each intent, distinctness could easily be measured using Equation (1).
Ambiguous nouns often have disambiguation pages on Wikipedia, which list possible meanings. For example:
Real Example 1. Query: meteor Intents: (1) A "shooting star", the visible trace of a meteoroid as it enters the atmosphere; (2) The town of Meteor, Wisconsin, USA; (3) METeOR, Australian information repository; (4) Meteor goldfish, a rare variety of goldfish; ... (8) Ireland's third mobile phone operator; (9) Meteor Vineyard in Napa Valley, USA; ... (11) A series of weather satellites of the Soviet Union; ... (27) A 1979 science fiction film; ... (39) BSA Meteor Air Rifle.
While the structure of Wikipedia means the intents are likely coherent, distinct and complete, we now consider plausibility. Intuitively, the list of intents is long, thus perhaps some are implausible ­ but this is not necessarily the case.
Measurability: By asking a sample of users who issued a particular query (or asking study participants to imagine issuing it) to specify if they agree with particular intents, plausibility of each of a set of intents could be assessed.
Real Example 2. Query: appraisals Intents: (1) What companies can give an appraisal of my home's value? (2) I'm looking for companies that appraise jewelry; (3) Find examples of employee performance appraisals; (4) I'm looking for web sites that do antique appraisals.
This example is TREC topic number 8 from the 2009 Web Track, and illustrates coherence.
Measurability: Inter-judge agreement can be measured by obtaining document judgments for each intent from two judges. Note that the intents may differ in agreement rate.
4. CONCLUSION
We have proposed four properties of a mapping from query to a set of intents. They can be used as guiding principles in developing evaluation sets that take account of query ambiguity and diversity, and are measurable given a collection judged for different intents ­ although also working with actual users appears essential to measure some of the metrics.
5. REFERENCES
[1] C. Zhai, W. Cohen, and J. Lafferty. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR, 2003.
[2] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and Diversity in Information Retrieval Evaluation. In SIGIR, 2008.
[3] H. Chen and D. Karger. Less is More: Probabilistic Models for Retrieving Fewer Relevant Documents. In SIGIR, 2006.
[4] J. Carbonell and J. Goldstein. The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In SIGIR, 1998.
[5] F. Radlinski, P. Bennett, B. Carterette, and T. Joachims. SIGIR Workshop Report: Redundancy, Diversity and Interdependent Document Relevance. SIGIR Forum, 43(2):46­52, 2009.
[6] Y. Yue and T. Joachims. Predicting Diverse Subsets using Structural SVMs. In ICML, 2008.

854

Web Page Publication Time Detection and its Application for Page Rank

Zhumin Chen1,2, Jun Ma1, Chaoran Cui1, Hongxing Rui2, Shaomang Huang1,
1School of Computer Science & Technology, Shandong University, Jinan, 250101, China 2School of Mathematics, Shandong University, Jinan, 250100, China
{chenzhumin,majun,hxrui}@sdu.edu.cn

ABSTRACT
Publication Time (P-time for short) of Web pages is often required in many application areas. In this paper, we address the issue of P-time detection and its application for page rank. We first propose an approach to extract P-time for a page with explicit P-time displayed on its body. We then present a method to infer P-time for a page without P-time. We further introduce a temporal sensitive page rank model using P-time. Experiments demonstrate that our methods outperform the baseline methods significantly.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Search process
General Terms: Algorithms, Experimentation, Performance
Keywords: temporal information detection, publication time extraction, publication time inference, page rank
1. INTRODUCTION
Publication time (P-time) is the inception (creation) time when a Web page was published. In most cases, P-time is a precise temporal expression with a limited number of fixed formats. P-time is very useful temporal information in many applications [1].
As P-time detection, a method to extract P-time of a news story was discussed in [2]. However, the news story is not a page but a search result record returned from a search engine. Web document was dated based on its own Last-Modified value, the average LastModified value of its incoming links, outgoing links and assets [3]. A document corpus was first grouped into time partitions, and then a document's timestamp is determined by the partition with maximum similarity score to it [4]. However, almost all literatures do not use the format and link information of Web pages. This leads to performance decreasing for P-time detection.
As P-time application, a model was presented to measure the distribution of documents retrieved in response to a query over the time domain in order to create a temporal profile for a query [5]. Implicitly year qualified queries were investigated in [6]. This kind of query does not actually contain a year, but yet a user may have implicitly formulated the query with a specific year in mind.
In this paper, we propose an approach to automatically detect Ptime of Web pages and a temporal sensitive page rank model using P-time. A page may have explicit P-time in its HTML body or not. For the former, we present a domain and language independent machine learning method to extract the P-time. For the latter, we infer the P-time based on the link and text similarity relations with its neighbors. As an application, we introduce a model for page
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

rank, which considers the relevance between the text of a page and a query, the P-time, as well as the important score of the page.

2. P-TIME DETECTION

For a given page, we first try to extract P-time from its body. If the P-time can not be found, we then infer it.

P-Time Extraction (PTE) We present a machine learning approach to extract P-time unit
from the body of a page which has explicit P-time. We incorporate only general linguistic and format information as the features for the machine learning model. We mainly use following information.
(1) Linguistic information: Number of numerical characters, Number of alphabetic characters, Number of all characters.
(2) Position information: Position of Unit before the page title, Position of Unit after the page title, Position of Unit from bottom of the page, Width and Height of Unit in the page.
(3) Format information: Temporal expression format, Font size, Font weight, Font family, Alignment: center, left, right, and justify.
(4) Tag information: H1, H2, ..., H6, DIR, A, U, BR, Class name (`time', `date'), etc.
With above information, we create 88 binary features used by the model of Support Vector Machine (SVM) to identify the P-time.

P-time Inference (PTI)

We infer P-time based on link and text information of a page and

its neighbors. For a page, we get the span of its P-time according

to the link relation with its neighbors. A page's P-time is later than

its outlink pages' P-time and earlier than its inlink pages' P-time.

For a page pi with P-time pti, its outlink pages are represented as {pO1 , pO2 , ..., pOe } with P-time {pt1O, pt2O, ..., pteO} respectively, and its inlink pages are denoted by {pI1, pI2, ..., pIw} with P-time {pt1I , pt2I , ..., ptwI } respectively. So, pti belongs to the time span T S i as follows:
pti  T S i = [MAX{pt1O, pt2O, ..., pteO}, MIN{pt1I , pt2I , ..., ptwI }] (1)

Then, we infer its exact P-time in terms of the text similarity

between its content and those neighbors content whose P-time be-

longs to the span. Many pages are published by some sites to report

an event when it happens. The reverse case, pages describing the

same event may be published at the same time. Thus, the intuition

is that highly relevant pages may share the same P-time. For two

pages: pi with unknown P-time pti, and pj with known P-time ptj, we infer pti in terms of ptj and the text similarity of pi and pj:

p(pt j|pi)  simscore(pi, pj) if pt j  T S i

(2)

P-time of the page with maximum simscore is estimated as pti.

3. P-TIME DETECTION EXPERIMENTS

P-time Extraction Experiments We annotated P-time of 2500 pages in English and 2500 pages
in Chinese. We compared six approaches. The first fives methods,

859

Precision

1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 FLT LLT LaT FfT BhT PTESVM PTE Methods
Figure 1: Precision of P-time extraction methods.

The total number of pages

330

300 270

PTIB PTI1 PTI2

240

210

180

150

120

90

60

30

0 0 1 - 3 4 - 6 7 - 9 10 - 12 13 - 15 > 16
time distance range (days)

Figure 2: Results of P-time inference methods.

represented as FLT, LLT, LaT, FfT and BhT, are baseline methods and utilize the first time, the last time, the latest time, the first time before title, the first time behind title in a page as its P-time respectively. The last method, PTESVM, is ours. Precision, the fraction of extracted P-time that agrees with the annotated P-time, is used to evaluate PTE results.
Figure 1 shows the results. Our approach significantly outperforms these baseline methods. It seems that use of only linguistic or position information is not enough for extracting P-time effectively. Our approach is effective since it makes full use of not only linguistic information but also various format information. As expected, the performance of FLT is much better than that of LLT, and BhT achieves the best performance among these baseline methods. This is because P-time often locates in the front part of pages, especially following titles. A small number of English authors write P-time before the page tile, while none of Chinese authors write P-time before the page tile. Thus, FfT gets the lowest performance.
P-time Inference Experiment We collected 11,000 pages with a span of 40 days and annotated
the P-time. We then randomly selected 500 pages and inferred their P-time. The rest of pages were grouped into 40 partitions according to the P-time. We define an evaluation measure time_distance, which is the time gap between the inferred P-time and the annotated P-time in days. The first method, PTIB, uses the date of the partition with maximum similarity score as the inferred P-time [4]. The second, PTI1, is ours, except that the similarity is computed between a page and each partition. The third method, PTI2, is ours.
Experiment result is shown in Figure 2. Both our approaches surpass the baseline method significantly. This is because our approaches use both link and text information to improve the effectiveness and efficiency of PTI. The performance of PTI1 is better than that of PTI2. The reason may be that a partition, which is merged using all titles of pages which belong to a certain date, contains more noisy information than a page title.
4. PAGE RANK WITH P-TIME
Many queries imply users' intention associated with time. We propose an approach to rank pages considering their text content, temporal information (i.e. P-time in this paper), and page importance. Our hypothesis is that the text similarity of a page to a query does not change over time, while its importance changes over time.
For a query q, the rank score of the page i is computed as follow:

Table 1: NDCG at Position n for GCorpus Methods N@1 N@2 N@3 N@4 N@5
PRE 0.752 0.741 0.706 0.692 0.695 PRT 0.772 0.761 0.762 0.771 0.772 PRP 0.789 0.813 0.792 0.798 0.802
Table 2: NDCG at Position n for YCorpus Methods N@1 N@2 N@3 N@4 N@5
PRE 0.478 0.472 0.501 0.508 0.524 PRT 0.571 0.621 0.673 0.669 0.691 PRP 0.599 0.673 0.734 0.731 0.740

rank(i) =   sim(i, q) +   f (i)  pagerank(i) (3)

where sim(i, q) is the cosine similarity between i and q, pagerank(i)

is the importance score of i, and we set  =  = 0.5. f (i) is a time

based weight function and its value depends on the P-time of i.

f

(i)

=

D(C)-D(i)
DR xu

(4)

where DR denotes the decay rate of a page's importance over time. We set DR = 0.5. D(C) is current time, D(i) is the P-time of i, and D(C) - D(i) is the time gap in days. xu, which denotes the sensitive degree of a query to time, can be dynamically tuned in terms of the
distribution of q over time from query log of a search engine.

5. RANK EXPERIMENTS WITH P-TIME
We issued 20 queries to Google and Yahoo! Search, and collected top 20 results for each query, denoted by GCorpus and YCorpus. And, we recruited four experts to judge the result rating. Normalized Discount Cumulative Gain (NDCG) was used to evaluate the results [7]. The first method, PRE, ranks results in the original order returned by a search engine. The second, PRT, ranks results taking the linear combination of text similarity, temporal information and page importance [8]. The last method, PRP, is ours.
We list the results in Table 1 and Table 2. We can see that PRP achieves the best performance for both GCorpus and YCorpus. PRP and PRT both outperform PRE. It is clear that methods using Ptime are better than that does not. In addition, PRE on GCorpus is superior to that on YCorpus. This may reflect the fact that the performance of Google is better than that of Yahoo! Search. Note that the NDCG values of all methods are very high because we only collected top 20 relevant results returned by the search engine.

6. ACKNOWLEDGMENTS
This work is supported by the Natural Science Foundation of China (60970047), the Key Science-Technology Project of Shandong Province (2007GG10001002, 2008GG10001026) and Independent Innovation Foundation of Shandong University.

7. REFERENCES
[1] Omar Alonso, Michael Gertz, and Ricardo Baeza-Yates. On the value of temporal information in information retrieval. SIGIR Forum, 41(2):35­41, 2007.
[2] Yiyao Lu, Weiyi Meng, and et. al. Automatic extraction of publication time from news search results. In ICDEW'06, page 50, 2006.
[3] Sérgio Nunes, Cristina Ribeiro, and Gabriel David. Using neighbors to date web documents. In Proceedings of the WIDM'07, pages 129­136. ACM, 2007.
[4] Nattiya Kanhabua and Kjetil Nørvåg. Using temporal language models for document dating. In ECML PKDD'09, pages 738­741. Springer-Verlag, 2009.
[5] Rosie Jones and Fernando Diaz. Temporal profiles of queries. ACM Transactions on Information Systems, 25(3):14, 2007.
[6] Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang Zhang. Improving search relevance for implicitly temporal queries. In SIGIR'09, pages 700­701. ACM, 2009.
[7] M.A. Najork, H. Zaragoza, and M.J. Taylor. Hits on the web: How does it compare? In SIGIR'07, pages 471­478, 2007.
[8] Peiquan Jin, Jianlong Lian, Xujian Zhao, and Shouhong Wan. Tise: A temporal search engine for web contents. In IITA'08, volume 3, pages 220­224, 2008.

860

HCC: A Hierarchical Co-Clustering Algorithm 
Jingxuan Li Tao Li
School of Computer Science Florida International University
Miami, FL, 33199
{jli003,taoli}@cs.fiu.edu

ABSTRACT
In this poster, we develop a novel method, called HCC, for hierarchical co-clustering. HCC brings together two interrelated but distinct themes from clustering: hierarchical clustering and co-clustering. The goal of the former theme is to organize clusters into a hierarchy that facilitates browsing and navigation, while the goal of the latter theme is to cluster different types of data simultaneously by making use of the relationship information. Our initial empirical results are promising and they demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering
General Terms: Algorithms, Experimentation
Keywords: Hierarchical, Co-Clustering
1. INTRODUCTION
Hierarchical clustering generates tree-like structures (e.g., dendrograms and hierarchies) which can be utilized to facilitate data navigation and browsing and has many applications in Information Retrieval(IR) [1]. Most existing hierarchical clustering algorithms aim at clustering homogeneous single type of data.
In many real world applications, however, a typical task often involves more than one type of data points. For example, in document analysis, there are terms and documents. Recently, many co-clustering algorithms have been proposed to simultaneously clustering different types of data [2]. However, few algorithms have been developed on simultaneously building hierarchies for different types of data.
In this poster, we develop a novel method, called HCC, for hierarchical co-clustering, which aims at generating dendrograms for different types of data simultaneously by making use of their relationship information. In this regard, HCC brings together two interrelated but distinct themes from clustering: hierarchical clustering and co-clustering. HCC utilizes the agglomerative hierarchical clustering algorithms as the frame, starting with singleton clusters, successively merges the two nearest clusters until only one cluster remains. Different from traditional agglomerative hierarchical
The work is partially supported by NSF grants IIS-0546280 and CCF-0939179.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

ZZ  & Z   &  Z   Z 
    d    Z
: Z  /Z
d  ,
< tZ < & < , < ' < d < /Z

dZ ,

























  

  
 

 






,
 














<

<

<

<

<

Figure 1: An Illustrative Example. Blue rectangles denote books and red rectangles denote keywords. The nodes containing both blue and red rectangles denote the clusters having some books and keywords.
clustering algorithms, HCC applies on the union of different types of data instead of on the points of a single data type. At each step of the merging process, HCC can merge a subset of one type of data objects with a subset of another type of data objects based on a measure of cluster internal heterogeneity. Figure 1 shows the hierarchy generated by traditional single-link hierarchical clustering algorithm and the hierarchy generated by HCC on an example dataset of book titles with two data types (books and keywords). It can be easily observed that HCC builds two coupled hierarchies (book hierarchy and keyword hierarchy) simultaneously. Books in a cluster can be well explained using the keywords in the same cluster. In addition, the relationships between book clusters can be described using their shared keywords.
Our initial empirical results are promising and they demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal. In the rest of the poster, we introduce our HCC algorithm and also present some experimental results.

2. HCC METHOD
In the typical setting of a dyadic co-clustering problem, there are two types of data objects, A={a1, a2, a3 . . . am} and B={b1, b2, b3 . . . bn} with size m and n, and we are given a relationship matrix X = (xij)  Rm×n, such that xij represents the relationship between i-th point in A and the j-th point in B. The goal of HCC is to generate hierarchical clustering solutions for A and B simultaneously by making

861

use of X. In the example of Figure 1, A is the set of book titles, B is the set of keywords, and X is the (normalized) book-term matrix indicating the relationship between the book titles and keywords.
HCC has its base in the approach developed in [4]. Similar to agglomerative hierarchical clustering algorithms, HCC starts with singleton clusters and then successively merges the two nearest clusters until only one cluster remains. Different from traditional agglomerative hierarchical clustering algorithms, HCC applies on the union of different types of data instead of on the points of a single data type. The output of HCC is a tree: each leaf of the tree is a data point of A or B and each internal node is a cluster of data points in A and B. The root of this tree is the largest cluster which contains all data points of A and B. The general procedure of HCC is described in Algorithm 1.

Algorithm 1 HCC Algorithm Description
Create an empty hierarchy H List  Objects in A + Objects in B N  size[A] + size[B] Add List to H as the bottom layer

for i = 0 to N - 1 do p, q = PickUpTwoNodes(List) o = Merge(p, q) Remove p, q from List Add o to List Add List to H at a higher layer
end for

The core issue in Algorithm 1 is the process of PickUpT-
woNodes: picking up two nodes (corresponding to two clusters) from the current layer to merge. Different from traditional hierarchical clustering where a node only contains objects of a single data type, a node here can have objects from different data types. Given a cluster C with m1 objects from A (denoted as A1) and n1 objects from B (denoted as B1), the cluster heterogeneity CH(C) can be defined as

C H (C )

=

1 m1 n1

(xij
sA1 ,tB1

- µ)2

(1)

where µ = Avgs,txst is the average value of the corresponding entries in X. Once the cluster heterogeneity is defined, we can select two nodes which would result in the least increase in cluster heterogeneity to merge [4]. Therefore, at each step of the merging process, HCC can merge two subsets of different types of data objects.

3. EXPERIMENTAL RESULTS
We perform experiments on DBLP dataset [5] to evaluate our HCC method. DBLP dataset contains the paper titles published by 552 relatively productive researchers over the last 20 years (from 1988 to 2007, inclusive) from 9 categories.
The input to our HCC method is a normalized authorterm matrix where each entry indicating the relationship between the corresponding author's paper titles and the corresponding keyword. As we discussed before, HCC is able to generate a coupled dendrogram for both authors and terms. Figure 2 shows the dendrogram generated by HCC. In the dendrogram, each leaf represents one author or one term and each internal node contain subsets of authors and terms.

Z   Z ,Z   Z  : ,
d d  d ' d D d D d K d ^

 

  
 









d

d

d

d

d

d

Figure 2: The dendrogram generated by HCC on the DBLP dataset
Clustering NMI CPCC HCC 0.19 0.67 SLHC 0.06 0.61
K-means 0.10 N/A TNMF 0.17 N/A
Table 1: Clustering performance comparison
Note that authors' research interests can be clearly described by the associated terms in a hierarchical manner. The more representative are the words for certain authors, the larger possibility for them to be clustered together. For example, at the sixth layer, `data' and `mining' have been merged with `Jiawei Han' who is renowned computer scientist in data mining research area.
We also evaluate the clustering performance of our HCC method. Normalized mutual information(NMI) and CoPhenetic Correlation Coefficient(CPCC) are used as evaluation measures. NMI is the normalized version of mutual information which measures how much information the two clusterings shares. In general, larger NMI indicates better clustering results. CPCC measures how faithfully a dendrogram preserves the pairwise distances between the original data points. Since DBLP dataset contains 9 classes, we make a cut on certain layer of the generated dendrogram, such that we can generate 9 clusters from the tree to compare with the original classes. We compare HCC with K-means, Tri-Factor Nonnegative Matrix Factorization(TNMF) [3], and Single Linkage Hierarchical Clustering(SLHC). Note that TNMF is an effective matrix-based framework for co-clustering. The experimental results presented in Table 1 clearly show that HCC outperforms other rivals. In summary, HCC combines the strengths of hierarchical clustering and co-clustering, simultaneously builds hierarchies for different types of data utilizing their relationships, and thus leads to better clustering performance.

4. REFERENCES
[1] D.R. Cutting, D.R. Karger, J.O. Pedersen, and J.W. Tukey. Scatter/gather: a cluster-based approach to browsing large document collections. In SIGIR '92, 1992.
[2] M. Deodhar and J. Ghosh. A framework for simultaneous co-clustering and learning from complex data. In KDD '07, 2007.
[3] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix tri-factorizations for clustering. In KDD '06, 2006.
[4] T. Eckes and P. Orlik. An error variance approach to two-mode hierarchical clustering. Journal of Classification, 10(1):51­74, January 1993.
[5] T. Li, C. Ding, Y. Zhang, and B. Shao. Knowledge transformation from word space to document space. In SIGIR '08, 2008.

862

The Value of Visual Elements in Web Search

Marilyn Ostergren, Seung-yon Yu, Efthimis N. Efthimiadis
The Information School University of Washington, Box 352840
Seattle, WA 98195-2840 {ostergrn, syyu, efthimis}@uw.edu

ABSTRACT
We used eye-tracking equipment to observe 36 participants as they performed three search tasks using three graphicallyenhanced web search interfaces (Kartoo, SearchMe and Viewzi). In this poster we describe findings of the study focusing on how the presentation of SERP results influences how the user scans and attends to the results, and the user satisfaction with these search engines.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process H.5.2 [Information interfaces and presentation]: User Interfaces. - Graphical user interfaces.
General Terms
Design, Human Factors.
Keywords
Search Engine Results Page Display (SERP), Eye-tracking study, Search Engine Evaluation, User Study
1. INTRODUCTION
The results of a web search are generally presented as a collection of web-page surrogates. Each surrogate conveys information that

can be used to support the decision `should I follow this link?' These surrogates, along with other information on the page may also support the decision to reformulate the search query ­ either because the results don't seem relevant, or because they trigger ideas that alter the searcher's target or conceptualization of the information need [4].
Our study investigates how searchers interact with graphical, nontextual search engine results page user interfaces (SERP UIs) to reveal the potential value of these alternative display strategies. We study whether the unique characteristics of these displays facilitate the work of scanning the page for the clues that support the decision to follow a link or reformulate a query.
2. REVIEW OF RESEARCH
Others have also used eye-tracking to gain an understanding of what happens during search. A major finding of this work is that, when results are presented as a ranked list, users direct most of their attention to results near the top of the list [2,5]. This is true even if the list is manipulated so that more relevant results appear lower on the list [5], and even if the eye-tracking data shows that the viewer looked at those more relevant results [2]. Cutrell & Guan [1] found that this bias toward earlier results can also be affected by the content of the surrogates ­ in this case, by the length of the text snippet.

Figure 1: Screenshots of ViewZi, SearchMe, and KartOO

Figure 2: Hot spots for ViewZi, SearchMe, and KartOO

Copyright is held by the author/owner(s).
SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Some other observations revealed by eye-tracking that are relevant to our work are that placement and proximity of image search results affects a viewers gaze path [6], and that an organized display with an explicit hierarchy reinforced with

867

headings and indentations facilitates a more efficient visual search [3].
3. METHODOLOGY
To gather evidence for how graphical qualities of a SERP support or hinder a user, we collected and analyzed two sources of data: 1. self-report (both audio recordings of verbalizations during search and responses to a questionnaire), and 2. observation (cursor and gaze behavior). We collected this data while users completed a series of search tasks using three graphically-enhanced search interfaces. The interfaces were Kartoo (kartoo.com), SearchMe (searchme.com) and Viewzi (viewzi.com). We chose these three because they display the results using graphical representation rather than the typical text-based ranked list. They also have enough similarities to each other to allow for meaningful comparisons.
3.1 Participants
We recruited 36 participants, all undergraduate students at a major research university. They were divided into two groups: 18 were trained to search the three SE, 18 were untrained.
3.2 Search Tasks
We chose search tasks which require different types of cognitive processing and navigation to elicit a range of search behaviors. Task 1 asked the user to find the schedule of events at a local performance theater. Task 2 asked to find two sites where a specific camera model could be purchased. Task 3 asked the user to find two credible sites describing the side-effects of aspirin.
3.3 Data Collection
Data was collected using questionnaires, the think-aloud process, eye-tracking, transaction logging, and participant observation. We used the Tobii eye-tracking system and ClearView 2.7.1 software to record and analyze the eye-tracking data. After each search, we asked the users to reflect upon how the interface affected their searching. After the entire set of searches, we asked them to compare the interfaces with each other.
4. DISCUSSION
Each of these three SERP interfaces uses an arrangement of page surrogates which differs from the usual top-to-bottom linear display. Viewzi (upper left in Figure ) uses a grid. SearchMe (central image in Figure ) uses a horizontal display in which only one surrogate (the central one) is clearly displayed at a time, while the previous and subsequent surrogates are smaller and displayed as if in a stack set at an angle to the central surrogate. Kartoo (right in Figure ) spreads the surrogates across the page organized by major topic area and resembles a map. The three displays also differ in the content of each surrogate. All three include a thumbnail image of the site though they vary substantially in size (Figure ). All three include some text ­ a partial URL and keywords in Kartoo, a slightly-abbreviated Google-derived snippet in Viewzi, and a snippet plus access to the full page (using a magnifying window) in SearchMe. We can easily see differences in scanning behavior across the three interfaces. This is evident in the heat-map images in Figure which show the combined gaze data from 18 participants. Our analysis indicates that the Kartoo display elicited the most scanning. Participants generally spent time looking at all 10 surrogates before clicking to go to a page. The SearchMe Display elicited careful analysis of individual surrogates, but inhibited scanning beyond the first few results. Though it is possible to

scan sequentially through the entire result set using the slider control that is located at the bottom of the screen, none of our participants took advantage of this feature choosing instead to analyze each carefully in sequence. The Viewzi display facilitates scanning in a way that is similar to the typical ranked list. The results are displayed from left to right and continue in rows from top to bottom. The most attention is given to the surrogate in the upper-left hand corner with correspondingly less attention to the surrogates to the right and bottom. However we did find that this strong preference for the surrogates near the beginning of the sequence is less pronounced than that reported by Guan & Cutrell with the typical ranked list display [2]. The question remains how much of this scanning behavior was influenced by the arrangement of the surrogates and how much was influenced by the content of the surrogates. The fact that users viewed more surrogates using the Kartoo display appears to be mostly because the surrogates provide fewer textual cues as to the content of the target website. The results of our questionnaires indicate that Kartoo was the least popular interface while SearchMe, which heavily favored the first site in the list and elicited very little browsing, was the most popular. In terms of ease of interaction on a scale of 1-9, where 1 is most difficult, 5 is average, and 9 is least difficult, 30% found Kartoo most difficult (1-2), 70% found SearchMe least difficult (7-9), and 42% found Viewzi average.
5. CONCLUDING REMARKS
The major insight from our initial analysis is that the visual, nonlinear qualities of these SERP displays strongly influence the user interaction, in particular, the number and sequence in which the surrogates are explored. Satisfaction with the SERP displays is correlated more closely with the textual content of the surrogates. Our more detailed analysis will look at what elements of the surrogate were examined (e.g. the URL, the text snippet, the screenshot) and compare these results with a similar data for a standard text-based search result display. Familiarity to SE and knowledge about the functionality of the SE affected user satisfaction. Training how to search using the visual search engines enhanced the user's search effectiveness, e.g., less number of query reformulations, more efficient search by using search features, and better user satisfaction. When asked if they intend to use the visual search engine again, more trained users answered positively than non-trained users. The most popular search engine that people want to use again was SearchMe (72%), followed by Viewzi (36%), and then Kartoo (33%).
6. REFERENCES
[1] E. Cutrell & Z. Guan. What are you looking for?: An eyetracking study of information usage in Web search. CHI '07. ACM, New York, NY, (2007), 407-416.
[2] Guan, Z., & Cutrell, E. An eye tracking study of the effect of target rank on Web search. In CHI 2007, ACM (2007).
[3] Hornof, A.J., & Halverson, T. Cognitive strategies and eye movements for searching hierarchical computer displays. In Proceedings of CHI 2003, ACM Press (2003), 249-256.
[4] Johnson, F C. User interactions with results summaries. In Proceedings of the ACM SIGIR 2007 workshop on web information seeking and interaction (2007), 131-134.
[5] Lorigo, L., et al. 2008. Eye tracking and online search: Lessons learned and challenges ahead. J. Am. Soc. Inf. Sci. Technol. 59, 7 (May. 2008), 1041-1052.
[6] Tseng, Y.C., Howes, A. The Adaptation of Visual Search Strategy to Expected Information Gain. CHI 2008, 1075-84.

868

Capturing Page Freshness for Web Search
Na Dai and Brian D. Davison
Department of Computer Science & Engineering Lehigh University
Bethlehem, PA 18015 USA
{nad207,davison}@cse.lehigh.edu

ABSTRACT
Freshness has been increasingly realized by commercial search engines as an important criteria for measuring the quality of search results. However, most information retrieval methods focus on the relevance of page content to given queries without considering the recency issue. In this work, we mine page freshness from web user maintenance activities and incorporate this feature into web search. We first quantify how fresh the web is over time from two distinct perspectives--the page itself and its in-linked pages--and then exploit a temporal correlation between two types of freshness measures to quantify the confidence of page freshness. Results demonstrate page freshness can be better quantified when combining with temporal freshness correlation. Experiments on a realworld archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance
Keywords: temporal correlation, web freshness, web search
1 Introduction
Web search engines exploit a variety of evidence in ranking web pages to satisfy users' information needs as expressed by the submitted queries. These information needs may contain distinct implicit demands, such as relevance and diversity. Recency is another such need, and so is utilized as an important criteria in the measurement of search quality. However, most information retrieval methods only match queries based on lexical similarity. Link-based ranking algorithms such as PageRank [1] typically favor old pages since the authority scores are estimated based on a static web structure and old pages have more time to attract in-links.
To overcome this problem, we quantify page freshness from web activities over time. We observe that pages and links may have diverse update activity distributions from inception to deletion time points. We infer that pages having similar activity distributions with their in-links suggest that such page activities have stronger influence on their parents' activities.
Motivated by the above analysis, in this work we incorporate a temporal freshness correlation (TFC) component in quantifying page freshness, and show that by using TFC, we can achieve a good estimate of how up-to-date the page tends to be, which is helpful to improve search quality in terms of both result freshness and rel-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Link activity
1 creation of link l : q  p 2 update on link l : q  p (changed anchor) 3 update on link l : q  p (unchanged anch.) 4 removal of link l : q  p
Page activity
1 creation of page q 2 update on page q 3 removal of page q

Infl. on p's InF
   
Infl. on q's PF
  

Gain of p's InF
3 2 1.5 -0.5 Gain of q's PF 3 1.5 -0.5

Table 1: Activities on pages and links and their influence on web freshness. (The link l points from page q to page p. : positive influence on web freshness. : negative influence on web freshness. The number of  or 
indicates the magnitude.)

evance. We consider the effects of other aspects of freshness on retrieval quality elsewhere [4].

2 Page Freshness Estimation

We start by quantifying web freshness over time. We assign every
page two types of freshness: (1) page freshness (PF) inferred from
the activities on the page itself; and (2) in-link freshness (InF) in-
ferred from the activities of in-links. Table 1 lists the detailed web activities and their contributions1 to page and in-link freshness. To
simplify analysis, we break the time axis into discrete time points (t0, t1, . . . , ti, . . .) with a unit time interval t = ti - ti-1, where i > 0. It is reasonable to assume that any activities that occur in [ti-1, ti] can be considered as occurring at ti, especially when t is small. We assume that the influence of activity decays exponentially over time. Therefore, we estimate PF and InF at ti by aggregating the web activities with such a decay, written as:

X ti

X

P Fti (p) =

e(i-j)t

wk Ctj ,k (p)

tj =1

kP A

X ti

XX

InFti (p) =

e(i-j)t

wk Ctj ,k (l)

tj =1

l:qp kLA

where wk and wk are contributions associated with each type of page and link activities, and Ctj,k(p) is the number of the kth type of page activity on page p at tj , and Ctj,k(l) is the number of the kth type of page activity on link l at tj, and P A and LA are
the page and link activity sets. In this way, we estimate web page
freshness at multiple predefined time points from web activities.

1The sensitivity of activity weights with respect to freshness estimation is omitted due to space limitation.

871

We next quantify the temporal freshness correlation between
pages and their in-links. We exploit the method by Chien and Im-
morlica [3], in which the authors measure query semantic similarity
by using temporal correlation. Given a page p, its page and in-link freshness are denoted as (P Ftc (p), P Ftc+1 (p), . . . , P Ftr (p)) and (InFtc (p), InFtc+1 (p), . . . , InFtr (p)) covering p's life span. The temporal freshness correlation (TFC) between page p and its
in-links is given by:

T F C(p)

=

1 n

X tr " P Ft(p) - P F (p) "" InFt(p) - InF (p) "

t=tc

P F (p)

InF (p)

where P F (p) and InF (p) are the standard deviations of P F (p) and InF (p), respectively.
Once we calculate the temporal freshness correlation for every page (tr - tc  2t), we next combine it with page freshness score by ranks. Given a time point of interest ti, the combined page freshness rank of document d is written as:

Rankcombined(d) = (1 - )RankP Fti (d) + RankT F C (d)

where 

=

a-1 n-1+a-1

,

and

n is the total number

of time points, and

a is the number of time points on which p exists. As a increases,

T F C(d) is more stable, and therefore we emphasize its contribu-

tion in the combined page freshness estimation.

3 Experimental Results and Discussion

Our goal is to improve web search quality on both relevance and freshness. To test the effect of combined page freshness on web search, we use an archival corpus of the .ie domain provided by the Internet Archive [5], covering from Jan. 2000 to Dec. 2007, and extract page and link activities. To minimize the influence of transient pages, we remove pages with fewer than 5 archival snapshots. The remaining sub-collection (with 3.8M unique URLs and 908M temporal links) is used for ranking evaluation.
We choose April 2007 as our time point of interest. 90 queries are selected from popular queries in Google Trends2 for evaluation. For each query, we have an average of 84.6 URLs labeled by at least one worker of Amazon Mechanical Turk3. Editors give judgments on each document with respect to a given query for both relevance and freshness. Relevance is judged from "highly relevant" (4) to "not related" (0). Freshness is judged from "very fresh" (4) to "very stale" (0). The document with an average score above 2.5 is marked as relevant/fresh.
To evaluate the effectiveness of the combined page freshness, we compare with PageRank, running on a single web snapshot of April 2007. The global ranking lists generated by the combined page freshness and PageRank scores are linearly combined with Okapi BM2500 [6] (baseline) by ranks individually. The parameters are the same as Cai et al. [2]. Precision@k and NDCG@k are used as metrics for ranking evaluation on both relevance and freshness. All methods are compared based on their best rank combination of query-specific scores and global scores on metric Precision@10 of relevance. The decay parameter  is set to 1 in this work.
Table 2 lists the ranking performance comparison varying the time span involved in the combined page freshness computation. For relevance, except for NDCG@3, the correlation between ranking performance and the time span is not consistent. Unlike relevance, freshness performance consistently improves with the increase of time span used in the combined page freshness computation. This suggests temporal freshness correlation calculated from

2http://www.google.com/trends 3http://www.mturk.com

NDCG@3 NDCG@3

Method Okapi BM2500
PageRank 200601-200704 200401-200704 200201-200704 200001-200704
Method Okapi BM2500
PageRank 200601-200704 200401-200704 200201-200704 200001-200704

P@10 0.4695 0.4894 0.5021 0.4893 0.5002 0.4986
P@10 0.3138 0.3325 0.3288 0.3342 0.3361 0.3374

Relevance NDCG@3
0.2478 0.2589
0.2917 0.3027 0.3081 0.3115 Freshness NDCG@3 0.2137 0.1946
0.2315 0.2329 0.2416 0.2477

NDCG@5 0.2740 0.2840 0.3152 0.3201 0.3157 0.3211
NDCG@5 0.2379 0.2345 0.2490 0.2552 0.2565 0.2617

NDCG@10 0.3344 0.3457 0.3675 0.3657 0.3642 0.3647
NDCG@10 0.2805 0.2838 0.2979 0.2988 0.3027 0.3028

Table 2: Ranking performance comparison. A  means the performance improvement is statistically significant (p-value<0.1) over Okapi BM2500. Performance improvement with p-value<0.05 is marked as .

0.32

combined page freshness

0.315

page freshness

temporal correlation

0.31

0.305

0.3

0.295

0.29

0.285

0.28

0.275 200601-

200501- 200401- 200301- 200201-
time span
(a) relevance

200101-

200001-

0.25

combined page freshness

0.245

page freshness

temporal correlation

0.24

0.235

0.23

0.225

0.22

0.215 200601-

200501- 200401- 200301- 200201-
time span
(b) freshness

200101-

200001-

Figure 1: Ranking performance on metric NDCG@3 while varying the time span involved in page freshness calculation.

long-term web freshness measures can benefit more on accurate page freshness estimation. Figure 1 shows the performance on NDCG@3 with the variance of the time span for both relevance and freshness. We observe that (1) the ranking performance of page freshness first decreases, and then keeps nearly constant with the increase of time span, indicating the page activities within the past 1-2 years influence page freshness estimation the most; (2) the ranking performance of temporal freshness correlation shows unstable trends with variance of time span; and (3) the combined page freshness shows promising performance, and demonstrates its superiority over either page freshness or TFC.
Acknowledgments
This work was supported in part by a grant from the National Science Foundation under award IIS-0803605 and an equipment grant from Sun Microsystems. We also thank Anlei Dong for helpful comments on the ranking evaluation criteria issue.
4 References
[1] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. In Proc. of 7th Int'l World Wide Web Conf., pages 107­117, Apr. 1998.
[2] D. Cai, X. He, J. Wen and W. Ma. Block-level link analysis. In Proc. 27th Annual Int'l ACM SIGIR Conf., pages 440­447, Jul, 2004.
[3] S. Chien and N. Immorlica. Semantic similarity between search engine queries using temporal correlation. In Proc. 14th Int'l World Wide Web Conf., pages 2­11, 2005.
[4] N. Dai and B. D. Davison. Freshness Matters: In Flowers, Food, and Web Authority. In Proc. of 33rd Annual Int'l ACM SIGIR Conf., Jul, 2010.
[5] The Internet Archive, 2010. http://www.archive.org/. [6] S. E. Robertson. Overview of the OKAPI projects. Journal of Documentation,
53:3­7, 1997.

872

S-PLSA+: Adaptive Sentiment Analysis with Application to Sales Performance Prediction

Yang Liu§,, Xiaohui Yu,§, Xiangji Huang, and Aijun An
§School of Computer Science and Technology, Shandong University Jinan, Shandong, China, 250101
York University, Toronto, ON, Canada, M3J 1P3
yliu@sdu.edu.cn,xhyu@yorku.ca,jhuang@yorku.ca,aan@cse.yorku.ca

ABSTRACT
Analyzing the large volume of online reviews would produce useful knowledge that could be of economic values to vendors and other interested parties. In particular, the sentiments expressed in the online reviews have been shown to be strongly correlated with the sales performance of products. In this paper, we present an adaptive sentiment analysis model called S-PLSA+, which aims to capture the hidden sentiment factors in the reviews with the capability to be incrementally updated as more data become available. We show how S-PLSA+can be applied to sales performance prediction using an ARSA model developed in previous literature. A case study is conducted in the movie domain, and results from preliminary experiments confirm the effectiveness of the proposed model.
Categories and Subject Descriptors
H.4.0 [Information Systems Applications]: General
General Terms
Algorithm, Experiment
Keywords
sentiment analysis, review mining, prediction
1. INTRODUCTION
Online reviews present a wealth of information on products and services, and if properly utilized, can provide vendors highly valuable intelligence to facilitate the improvement of their business. As such, a growing number of recent studies have focused on the economic values of reviews, exploring the relationship between the sales performance of products and their reviews [3, 2, 4]. Gruhl et al. [3] show that the volume of relevant postings can help predict the sales rank of books on Amazon, especially the spikes in sales ranks. Ghose et al. [2] also demonstrate that subjectivity of reviews can have an impact on sales performance.
Liu et al. [4] propose a probability model called Sentiment PLSA (S-PLSA for short) based on the assumption that sentiment consists of multiple hidden aspects. They develop a model called ARSA (which stands for Auto-Regressive Sentiment-Aware) to quantitatively measure the relationship between sentiment aspects and reviews. Our experience with running ARSA on several online review datasets reveals that the model is highly sensitive to the sentiment factors, which are constantly changing over time as new reviews become available. It is therefore essential to allow the SPLSA model to adapt to newly available review data.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

To this end, we take a Bayesian approach, and propose an adaptive version of the S-PLSA model that is equipped with the incremental learning capability for continuously updating the model using newly observed reviews. The proposed model is motivated by the principle of quasi-Bayesian (QB) estimation, which has found successful applications in various domains such as adaptive speech recognition and text retrieval [1]. We call the proposed model the S-PLSA+model, in which the parameters are estimated by maximizing an approximate posterior distribution. One salient feature of our modeling is the judicious use of hyperparameters, which can be recursively updated in order to obtain up-to-date posterior distribution and to estimate new model parameters. This modeling approach makes it possible to efficiently update the model parameters in an incremental manner without the need to re-train the model from scratch each time as new reviews become available.

2. S-PLSA

In the S-PLSA model [4], a review can be considered as being

generated under the influence of a number of hidden sentiment fac-

tors. The use of hidden factors provides the model the ability to

accommodate the intricate nature of sentiments, with each hidden

factor focusing on one specific aspect. What differentiates S-PLSA

from conventional PLSA is its use of a set of appraisal words [4]

as the basis for feature representation. The rationale is that those

appraisal words, such as "good" or "terrible", are more indicative

of the review's sentiments than other words.

For a given set of N reviews D = {d1, . . . , dN }, and the set

of M appraisal words W = {w1, . . . , wM }, the S-PLSA model

dictates that the joint probability of observed pair (di, wj) is gen-

erated by P (di, wj) = P (di)

K k=1

P (wj |zk)P (zk|di),

where

zk  Z = {z1, . . . , zK } corresponds to the latent sentiment fac-

tor, and where we assume that di and wj are independent condi-

tioned on the mixture of associated sentiment factor zk. The set of

parameters  of this model consist of {P (wj|zk), P (zk|di)}, the

maximum likelihood estimates of which can be obtained through

an expectation-maximization (EM) algorithm [4].

3. ADAPTIVE S-PLSA
The S-PLSA model can be trained in a batch manner on a collection of reviews, and then be applied to analyze others. In many cases, however, the reviews are continuously becoming available, with the sentiment factors constantly changing. We thus hope to adapt the model to the newly obtained reviews, in order to make it more suitable to the changing contexts. A naïve way to perform the adaptation is to re-train the model from scratch using all data available including the newly obtained data, which is clearly highly inefficient. Here, we propose a model called S-PLSA+, which performs incremental learning based on the principle of quasi-Bayesian

873

(QB) estimation. The basic idea is to perform updating and down-

dating at the same time by (i) incrementally accumulating statistics

on the training data, and (ii) fading out the out-of-date data. Let Dn be the set of reviews made available at epoch n (e.g.,
the reviews published on a certain day, but the time unit used can be set to be finer or coarser based on the need), and denote by n = {D1, . . . , Dn} the set of reviews obtained up to epoch n. QB S-PLSA estimates at epoch n are determined by maximizing the posterior probability using n:

(n) = arg max P (|n) = arg max P (Dn|)P (|n-1)





In order to allow closed-form recursive update of , we use the

closest tractable prior density g(|(n-1)) with sufficient statistics

to approximate the posterior density P (|n-1), where n-1 is

evolved from review sets n-1. This leads to (n)  arg max P (Dn|)g(|(n-1)). Note that at epoch n, only the new reviews

Dn and the current statistics (n-1) are used to update the S-PLSA+

parameters, and the set of reviews Dn are discarded after new parameter values (n) are obtained, which results in significant sav-

ings in computational resources.

The particular choice of the prior g(|) in our model is the

Dirichlet density, which can be expressed by


KM


N

g(|) =

 P (wj |zk)j,k-1

P (zk|di)k,i-1

k=1 j=1

i=1

where  = {j,k, k,i} are the hyperparameters of the Dirichlet
distribution. Assuming for the moment that (n-1) is known, we can show that (n) can be obtained through an EM algorithm [1].
A major benefit of S-PLSA+ lies in its ability to continuously update the hyperparameters. We can show that the new hyperparameters are given by

|Dn |

(jn,k) =

c(d(in), wj(n))P (n)(zk |d(in), wj(n)) + (jn,k-1)

(1)

i=1

M

k(n,i) =

c(d(in), wj(n))P (n)(zk |d(in), wj(n)) + k(n,i-1).

(2)

j=1

where the posterior P (n)(zk|di(n), wj(n)) is computed using Dn and the current parameters (n), and c(di(n), wj(n)) denotes the number of (d(in), wj(n)) pairs.
To summarize, S-PLSA+works as follows. In the startup phase, initial estimates of the hyperparameters (0) are obtained. Then, at each learning epoch n, (i) new estimates of the parameters (n)
are computed based on the newly available data Dn and hyperpa-
rameters obtained from epoch n - 1; and (ii) new estimates of the hyperparameters (n) are obtained using (1) and (2). This way, the
model is continuously updated when new reviews (Dn) become available, and at the same time fades out historical data n-1, with the information contained in n-1 already captured by (n-1).

4. APPLICATION TO SALES PREDICTION

The proposed S-PLSA+model can be employed in a variety of

tasks, e.g., sentiment clustering, sentiment classification, etc. As a

sample application, we plug it into the ARSA model proposed in

[4], which is used to predict sales performance based on reviews

and past sales data. The original ARSA model uses S-PLSA as the

component for capturing sentiment information. With S-PLSA+,

the ARSA model can be formulated as follows:

p

qR

yt =

iyt-1 +

i,j t-i,j + t,

i=1

i=1 j=1

where (i) yt denotes the sales figure at time t after proper preprocessing such as de-seasoning, (ii) p, q, and R are user-chosen

parameters, (iii) i and i,j are coefficents to be estimated using

training data, and (iv) t,j

=

1 |Rt |

dRt p(zj |d), where Rt is

the set of reviews available at time t and p(zj|d) is computed based

on S-PLSA+. It reflects the sentiment "mass" that can be attributed
to factor zj. The ARSA model can be trained using linear least
squares regression. Note that the notion of time (t) in the ARSA model is different from the epoch (n) in S-PLSA+. For example,
sales prediction can be made for each day using ARSA, whereas the model adaptation of S-PLSA+can happen every other day.

5. EXPERIMENTS

Experiments were conducted on an IMDB dataset to evaluate

the effectiveness of the proposed approach by comparing the prediction accuracy of ARSA using S-PLSA+and that of the original

ARSA. The dataset was obtained from the IMDB Website by col-

lecting 28,353 reviews for 20 drama films released in the US from

May 1, 2006 to September 1, 2006, along with their daily gross

box office revenues. Half of the movies are used for batch training.

For the original ARSA, the trained model is then used to make pre-

dictions in the testing data consisting of the other half the movies. For the proposal model, adaptation of the S-PLSA+component is

performed for each movie in the testing set, in four epochs on four

different days v (v = 2, 4, 6, 8) using the review data available up

to day v. The up-to-date model at day v is then used for subsequent

prediction tasks.

We use the mean absolute percentage error (MAPE) to measure

the prediction accuracy:

MAPE

=

1 T

Ti=1(|Predi-Truei|/Truei),

where T is the number of instances in the testing set, and Predi and

Truei are the predicted value and the true value respectively. The

results on the accuracy of the original ARSA and that of the ARSA

using S-PLSA+updated at Epochs 1-4 (v = 2, 4, 6, 8) respectively

are shown in the table below.

Original Epoch 1 Epoch 2 Epoch 3 Epoch 4

0.352

0.295

0.241

0.247

0.240

The accuracy improves as the model is getting updated in the first two epochs, which demonstrates the benefits of having an incremental model to absorb new information; especially in our case, S-PLSA+allows the models to be adapted to the individual movies. The accuracy stays stable from Epoch 2 through Epoch 4, indicating that no significant new information is available from Epoch 2 to Epoch 4.

6. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented an adaptive S-PLSA model that is capable of incrementally updating its parameters and automatically downdating old information when new review data become available. This model has been used in conjunction with the ARSA model for predicting sales performance. Preliminary experimental results show that by allowing the model to be adaptive, we can capture new sentiment factors arising from newly available reviews, which can greatly improve the prediction accuracy. For future work, we plan to study the performance of S-PLSA+in other information retrieval and data mining tasks.

Acknowledgements
This work is supported by NSERC Discovery Grants, an Early Researcher Award of Ontario and an NSFC Grant (No. 60903108).
7. REFERENCES
[1] Jen-Tzung Chien and Meng-Sung Wu. Adaptive bayesian latent semantic analysis. IEEE TASLP, 16(1):198­207, 2008.
[2] Anindya Ghose and Panagiotis G. Ipeirotis. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In ICEC, pages 303­310, 2007.
[3] Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak, and Andrew Tomkins. The predictive power of online chatter. In KDD '05, pages 78­87, 2005.
[4] Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. ARSA: a sentiment-aware model for predicting sales performance using blogs. In SIGIR, pages 607­614, 2007.

874

Machine Learned Ranking of Entity Facets

Roelof van Zwol
Yahoo! Research
roelof@yahoo-inc.com

Lluis Garcia Pueyo
Yahoo! Research
lluis@yahoo-inc.com

Mridul Muralidharan

Börkur Sigurbjörnsson

Yahoo! Research

Yahoo! Research

mridulm@yahoo-inc.com borkur@yahoo-inc.com

ABSTRACT
The research described in this paper forms the backbone of a service that enables the faceted search experience of the Yahoo! search engine. We introduce an approach for a machine learned ranking of entity facets based on user click feedback and features extracted from three different ranking sources. The objective of the learned model is to predict the click-through rate on an entity facet. In an empirical evaluation we compare the performance of gradient boosted decision trees (GBDT) against a linear combination of features on two different click feedback models using the raw click-through rate (CTR), and click over expected clicks (COEC). The results show a significant improvement in ranking performance, in terms of discounted cumulated gain, when ranking entity facets with GBDT trained on the COEC model. Most notably this is true when evaluated against the CTR test set.
Categories and Subject Descriptors
H.3.3 [Information Retrieval]: Information Search and Retrieval; H.3.5 [Information Retrieval]: On-line Information Services
General Terms
Experimentation, Measurement, Performance
Keywords
ranking entity facets, click feedback, GBDT
1. ABOUT RANKING ENTITY FACETS
The major Web search engines are gradually changing the search experience. Most notably this is visible through the introduction of semantic search assistants, the enrichment of the search results shown to the user and other components that try to predict the user intent. Key to enriching the search experience is the wide-scale availability of user-generated content and other knowledge bases such as Wikipedia, the Internet Movie Database (IMDB), GeoPlanetTM, or Freebase to name a few.
The research presented here is part of the faceted search experience of the Yahoo! Web and Image search engines [4].
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Screen captions of entity facets show in the search engine interface.
Figure 1 shows a fragment of the search engine interface depicting the facets bar for the celebrity "Daniel Day-Lewis" and the location "Geneva, Switzerland".
To support the entity facet ranking application of Figure 1, we propose a machine learned ranking of entity facets based on user click feedback. Given an entity of interest, we have collected a large pool of related candidate facets, e.g. related entities. These entity facet pairs have been extracted from various knowledge bases such as Wikipedia, GeoPlanetTMand other sources. Typically this provides us with a few hundred candidate facets for an entity. The task is then to rank the candidate facets related to each entity in our pool based on its relevance.
Facets are ranked using statistical features extracted from three different sources that contain entity information in the context of images: query terms entered by users in query logs, query session information from users in query logs, and tags provided by users annotating their photos in Flickr. Query term information captures entities that co-occur in a single user query, while query sessions provide information about entities that frequently co-occur in a user session. Flickr tags have a good coverage of travel and location related entities, as well as topics of a more general nature, but

879

tend to be less focussed on, for instance, celebrity entities. For every source, we extract different unary, symmetric and asymmetric features such as query frequency, conditional probability, KL divergence, etc. [4]. For the initial launch of the faceted search experience, we constructed a ranking function that is a linear combination of the conditional probabilities extracted from the three ranking sources .
The main contribution of this paper is a machined learned approach for ranking entity facets based on user click feedback. We propose to learn a ranking using the full set of features extracted from the ranking sources that will predict the click-through rate (CTR) on an entity facet [1]. For that purpose we introduce two click models: raw clickthrough rate on the facets, and the click over expected click (COEC), which is claimed to be more robust towards the position-bias on a click as users tend to click more on those results shown high in the ranking [3].
The click-feedback is used as the ground truth for our training, development and test sets. We have experimented with various learners, but for the experiment reported here we limit ourselves to the discussion of the performance using stochastic gradient boosted decision trees (GBDT) [2]. We used least squares regression as our loss function.
Next we collected the user click feedback on the facets over a period of three months, based on which we compute the click-through rate and click over expected click for each entity facet pair that was shown at least 25 times to a user. The latter constraint is to ensure that the CTR and COEC values are stable enough to be used as the labels for our training, development, and test sets. We join the feature set with the CTR and COEC sets, using the entity facet pair as the key. Next we split the collection into training, development and test sets. When splitting, we ensure that an entity can only occur in one of the three collections.
2. EVALUATION
The objective of the experiment is to measure the prediction accuracy based on the click-through rate of an entity facet pair. We first present the setup of the experiment, followed by a discussion of the results of the evaluation.
Ranking strategies. Central in the experiment are the three
ranking strategies: (1) Baseline. A linear combination of the conditional probabilities. (2) GBDTctr. GBDT trained on the CTR click model and (3) GBDTcoec. GBDT trained on the COEC click model.
Test sets. For the experiment we use two test sets both
containing the same 100 entities and their 10+ facets that have not been used for training or parameter tuning. For a fair comparison of the performance between queries we have normalized the CTR and COEC values for each of the facets of the 100 selected entities to be in the range of [0, 1].
Evaluation metrics. To evaluate the performance on the
test collection, we adopt Discounted Cumulative Gain (DCG) as our metric. DCG is an effectiveness measure that is used frequently for information retrieval tasks, and allows for the use of a graded relevance scale.
Results. The overall performance of the baseline and the
two GBDT models is reported in Table 1. For each of the two test sets, CTR and COEC, the mDCG and mnDCG is included. The performance of all strategies, independent of

Table 1: Overall performance.

CTR

COEC

Run

mDCG mnDCG mDCG mnDCG

Ideal

2.375 -

2.594 -

Baseline 1.728 0.709

1.812 0.677

GBDTctr 2.090

0.874

-

-

GBDTcoec 2.343

0.986

2.436 0.930

Figure 2: nDCG comparison of performance of baseline and GBDT on CTR vs COEC test sets.
the test set is good (mnDCG > 0.67). It can be clearly seen that on both the CTR and COEC test sets, the GBDT models outperform the baseline strategy. Using the normalized metric (mnDCG) we see that can better estimate the actual COEC using the GBDTcoec model than predicting the raw CTR with our GBDTctr model. This gives us a first indication that the COEC click model is more effective than the CTR click model when learning to rank entity facets.
Figure 2 plots the nDCG scores at different points in the ranking. This allows for a direct comparison of the different strategies across the two test sets. In addition to the strategies already discussed, we introduce a new variant where we evaluate the performance of the GBDTcoec strategy, e.g. the GBDT model that was trained to optimized the click prediction on the COEC click model, against the CTR test set. As can be seen this gives a near optimal performance over the first ten positions in the ranking, and a perfect prediction of the most important facet for each of the 100 entities in our test set.
3. REFERENCES
[1] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM '08: Proceedings of the international conference on Web search and web data mining, pages 87­94, New York, NY, USA, 2008. ACM.
[2] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2001.
[3] Y. Zhang and R. Jones. Comparing click logs and editorial labels for query rewriting. In Query Log Analysis: Social And Technological Challenges, 2007.
[4] R. van Zwol, B. Sigurbj¨ornsson,et al. Faceted Exploration of Image Search Results. In WWW2010. Raleigh, NC, USA. 2010.

880

User Comments for News Recommendation in Social Media

Jia Wang


Qing Li

Southwestern Univ. of Finance Southwestern Univ. of Finance

and Economics

and Economics

55 Guanghua Cun Road

55 Guanghua Cun Road

Chengdu, China

Chengdu, China

wangjia@2008.swufe.edu.cn liq_t@swufe.edu.cn

Yuanzhu Peter Chen
Memorial Univ. of Newfoundland
St. John's, A1B 3X5 NL, Canada
yzchen@mun.ca

ABSTRACT
Reading and Commenting online news is becoming a common user behavior in social media. Discussion in the form of comments following news postings can be effectively facilitated if the service provider can recommend articles based on not only the original news itself but also the thread of changing comments. This turns the traditional news recommendation to a "discussion moderator" that can intelligently assist online forums. In this work, we present a framework to recommend relevant information in the forum-based social media using user comments. When incorporating user comments, we consider structural and semantic information carried by them. Experiments indicate that our proposed solutions provide an effective recommendation service.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Filtering
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Web is one of the most important vehicles for "social media", e.g. Internet forums, blogs, wikis, and twitters. One form of social media of particular interest here is self-publishing. In selfpublishing, a user can publish an article or post news to share with other users. Other users can read and comment on the posting and these comments can, in turn, be read and commented on. Digg (digg.com) and Yahoo!Buzz (buzz.yahoo.com) are commercial examples of self-publishing. A useful extension of this self-publishing application is to add a recommendation feature to the current discussion thread. That is, based on the original posting and various levels of comments, the system can provide a set of relevant articles, which are expected to be of interest of the active users of the thread.
Here, we explore the problem of news recommendation for dynamic discussion threads. A fundamental challenge in adaptive news recommendation is to account for topic divergence, i.e. the change of gist during the process of discussion. In a forum, the original news is typically followed by other readers' opinions, in
This research is supported by National Natural Science Foundation of China Grant No.60803106.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Multi-relation graph of comments
the form of comments. Concerns and intention of active users may change as the discussion continues. Therefore, news recommendation, if it were only based on the original posting, can not benefit the potentially changing interests of the users. Apparently, there is a need to consider topic evolution in adaptive news recommendation and this requires novel techniques that can help to capture topic evolution precisely to prevent wild topic shifting which returns completely irrelevant news to users. A related problem is content-based information filtering (or recommendation). Most information recommender systems select articles based solely on the contents of the original postings [1] [3] [4].
In this work, we propose a framework of adaptive news recommendation in social media. It has the following contributions. (1) It is the first attempt of incorporating reader comments for adaptive news recommendation. (2) We model the relationship among comments and that relative to the original posting in order to evaluate their overall impact on recommendations.
2. SYSTEM DESIGN
The proposed news recommender first constructs a topic profile for each news posting along with the comments from readers, and uses this profile to retrieve relevant news.
We first model the relationship among comments and that relative to the original posting in order to evaluate their overall impact. In our model, we treat the original posting and the comments each as a text node. This model both considers the content similarity between text nodes and the logic relationship among them. On one hand, the content similarity between two nodes can be measured by any commonly adopted metric, such as cosine similarity and Jaccard coefficient. This metric is taken over every node pair in the discussion thread. On the other hand, the logic relation between nodes takes two forms. First, a comment is always made in response to the original posting or an earlier comment. In graph theoretic terms, the hierarchy can be represented as a tree  = (,  ), where  is the set of all text nodes and  is the edge set. In particular, the original posting is the root and all the comments are ordinary nodes.

881

There is a directed edge    from node  to node , denoted (, ), if the corresponding comment  is made in response to comment (or original posting) . Second, a comment can quote from one or more earlier comments. From this perspective, the hierar-
chy can be modeled using a directed acyclic graph (DAG), denoted  = (, ). There is a directed edge    from node  to node , denoted (, ), if the corresponding comment  quotes from comment (or original posting) . As shown in Figure 1, for either graph  or , we can use a   ×   adjacency matrix, denoted  and , respectively, to record them. Inline with the adjacency matrices, we can also use a   ×   matrix defined on [0, 1] to record the content similarity between nodes and denote it by  . Thus, we can combine these three aspects linearly.
Intuitively, the important comments are those whose topics are
discussed by a large number of other important comments. There-
fore, we propose to apply the PageRank algorithm [2] to rank the
comments as 
 = /  + (1 - ) × (, ) × ,


where  is the damping factor as in PageRank and this value is recommended to be 0.85,  and  are node indices, and   denotes the number of text nodes in the thread. In addition, (, ) is the normalized weight of comment  referring to  defined as

(,  )

=



, ,

+



,



where , is an entry in the graph adjacency matrix and  is a constant to avoid division by zero.
Once the importance of comments on one news posting is quan-
tified by our model, this information along with the news itself are
fed into a synthesizer to construct a topic profile of this news dis-
cussion thread. The profile is a weight vector of terms to model the language used in the thread. Consider a news posting 0 and its comment sequence {1, 2,    , }. For each term , a compound weight  () is calculated. It is a linear combination of the contribution by the news posting itself, 1(), and that by the comments, 2(). The weight contributed by the news itself, 0, is:

1() = (, 0)/ma x (, 0)

The weight contribution from the comments {1, 2,    , } incorporates not only the language features of these documents but also their importance of leading a discussion in related topics. That is, the contribution of comment score is incorporated into weight calculation of the words in a text node.

 

2() =

(,



)/max 

(

,



)

×



/max 



=1

Such a treatment of compounded weight  () is essentially to recognize that readers' impact on selecting relevant news and the difference of their influence strength.
With the topic profile constructed as above, we can use it to select relevant news for recommendations. That is, the retriever returns an order list of news with decreasing relevance to the topic. Our model to differentiate the importance of each comment can be easily incorporated into any good retrieval model. In this work, our retrieval model is derived from [4].

3. EXPERIMENTAL EVALUATION
To gauge how well the proposed recommendation approach performs, we carry out a series of experiments on a synthetic data set

 @10  

Table 1: Overall performance The Proposed CF Okapi

0.94

0.789 0.827

0.932

0.8 0.833

LM
0.804 0.833

collected from Digg and Reuters news website. We randomly select 20 news articles with corresponding reader comments from Digg website. These news articles with different topics are treated as the original news postings, recommended news are selected from a corpus of articles collected from Reuters news website. This simulates the scenario of recommending relevant news from traditional media to social media readers for their further reading. We compared the proposed approach to three other retrieval approaches as the baseline: one is a simple content filter (CF) which treats news and comments as a single topic profile, the other two are well-known news recommendation methods [1], Okapi and LM.
To observe the impact of readers' concerns on original news posting in social media, we investigate the effect of the three forms of relationship among comments, i.e. content similarity, reply, and quotation. We carry out a series of experiments for this purpose. we find that replies are slightly more effective than quotations and both of these outperform pure content similarity. In other words, the importance of comments can be well evaluated by the logic organization of these comments. We also notice that the incorporation of content similarity decreases the system effectiveness. This may seem to contradict our intuition that the textual information should complement the logic-based models. By further investigating our results, we find that content similarity sometimes misleads the decision on the importance of the comments. Besides, the computation cost of calculating the content similarity matrix  is very high. Therefore, we only apply the structural information to determine the importance of each comment.
We have -tests using  @10 and MAP as performance measures, respectively, and the  values of these tests are all less than 0.05, which means that the results of experiments are statistically significant. We conduct a series of preliminary experiments to find the optimal performance obtained when the topic file word number is 60 and combination coefficient  is 0.7. As shown in Table 1, the overall performance of the proposed approach performed significantly better than the best baseline methods.

4. CONCLUSION
In this work, we present a framework for adaptive news recommendation that incorporates information from the entire discussion thread. This study can be extended in a few interesting ways. For example, we can use this technique to process personal Web blogs and email archives. The technique itself can also be extended by incorporating such information as reader scores on comments, chronological information of comments, and reputation of users. Indeed, its power is yet to be further improved and investigated.

5. REFERENCES
[1] T. Bogers and A. Bosch. Comparing and evaluating information retrieval algorithms for news recommendation. In Proc. of ACM Recommender systems, 2007.
[2] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107­117, 1998.
[3] J.-H. Chiang and Y.-C. Chen. An intelligent news recommender agent for filtering and categorizing large volumes of text corpus. International Journal of Intelligent Systems, 19(3):201­216, 2004.
[4] V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie, D. Jensen, and J. Allan. Language models for financial news recommendation. In Proc. of CIKM, 2000.

882

Incorporating Global Information into Named Entity Recognition Systems using Relational Context

Yuval Merhav

Filipe Mesquita

Denilson Barbosa

Illinois Institute of Technology University of Alberta, Canada University of Alberta, Canada

yuval@ir.iit.edu

mesquita@cs.ualberta.ca denilson@cs.ualberta.ca

Wai Gen Yee

Ophir Frieder

Illinois Institute of Technology

Georgetown University

yee@iit.edu

ophir@cs.georgetown.edu

ABSTRACT
The state-of-the-art in Named Entity Recognition relies on a combination of local features of the text and global knowledge to determine the types of the recognized entities. This is problematic in some cases, resulting in entities being classified as belonging to the wrong type. We show that using global information about the corpus improves the accuracy of type identification. We explore the notion of a global domain frequency that relates relationidentifying terms with pairs of entity types which are used in that relation. We use this to identify entities whose types are not compatible with the terms they co-occur in the text. Our results on a large corpus of social media content allows the identification of mistyped entities with 70% accuracy.
Categories and Subject Descriptors
I.2.7 [Natural Language Processing]: Text analysis
General Terms
Experimentation, Performance
1. INTRODUCTION
Named Entity Recognition (NER) is an important task for many Information Retrieval applications. One sub-task of particular importance is type identification: assigning meaningful types (e.g., Person, Organization, Location, etc.) to the extracted entities. The state-of-the-art NER systems rely on a mix of local information (statistics and results of lexical analysis) about small portions of the corpora and external knowledge (usually obtained through learning on training data) to perform type identification. For example, LBJ [3] analyzes the corpus in fixed-sized text windows ignoring document boundaries and relies on two sources of external information: high-precision lists of named entities (gazetteers) and clusters of commonly used words in different contexts. While the use of external information has been shown to improve accuracy over purely local methods, they are limited to the knowledge contained in a small collection of documents or tokens for every entity type assignment decision they make. Inevitably, these methods eventually assign incorrect types to the entities they extract. As an example, consider the snippet "[MISC Jewish] by birth, [ORG Alamo] married [PER Edith Opal] who was also [MISC Jewish]" which is tagged with LBJ. As one can see, Alamo is correctly identified
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

as an entity, but assigned type ORG (for Organization) instead of PER (for Person). As Alamo appears in LBJ's gazetteers as an organization, it is likely that this is the reason LBJ labeled Alamo incorrectly.
To overcome the limitation, we propose a scoring feature based on global information (extracted from the entire corpus) for improving the assignment accuracy of entity types. The feature is designed to be used as a supplement in different NER systems and other tasks. This work is motivated by our social network extractor system SONEX [2], that extracts latent social networks from social media text (in [2] we report results on the ICWSM'09 Spinn3r Blog dataset with 44 millions posts [1]). SONEX works by extracting named entities with LBJ, and individual sentences with LingPipe1. Then it identifies relations that associate pairs of entities (e.g., Alamo and Edith Opal) by clustering those sentences using a variety of features. In the example above, SONEX identifies that Alamo and Edith are married to each other, and thus assigns this term to the pair. In this work, we show how to automatically use the results of this relation extraction in SONEX to identify entities which are incorrectly typed (Alamo in the example).

2. DOMAIN FREQUENCY
It is natural to expect that the relations extracted in SONEX are strongly correlated with a given context. For instance, marriage is a relation between Persons, and thus, belongs to the domain PER-PER. We exploit this observation to identify mistyped entities. Starting from the social network extracted by SONEX, which we call the dataset in the sequel, we proceed as follows. For each relation-identifying term in the dataset (excluding stop words), we compute its relative frequency in every possible domain. We refer to the frequency of a term t in a given domain i as the term's Domain Frequency (DF) score, and refer to it as dfi (t).
We normalize the domain frequency of every term across all domains associated with the term. More precisely, let t be a term and let i1, . . . , in be all possible domains of pairs of entity types; let fi(t), . . . , fn(t) be the frequencies in which that term identifies a relation of a pair of entities in each of the domains. Then,

dfi (t) =

fi(t)

.

1jn fj (t)

The number of possible domains is the square of the number of types the NER system identifies. LBJ offers the following types: PER (Person), ORG (Organization), LOC (Location), and MISC

1http://alias-i.com/lingpipe

883

PER - PER ORG - ORG ORG - PER

0.8409 0.0681 0.0227

Table 1: Top-3 Domain Frequencies for "married" in the Spinn3r social network extracted with SONEX.

(miscellaneous)2. Table 1 shows the most significant relative DF scores for the term married across different domains. As expected, the DF score for the PER to PER domain is significantly larger than all other domains. If a term does not appear in a certain domain, its DF score for the domain is zero (e.g., "married" does not appear in the LOC to LOC domain, and hence, it does not appear in Table 1). The entire list of terms associated with their DF scores is available by request. This list can be used as an external knowledge source in different NER systems, in various dataset domains.
2.1 Detecting Incorrectly Typed Entities
Our premise is that a pair of entities (E1, E2) from a given domain d1 = T1 × T2 contains at least one mistyped entity if there is a relationship-identifying term t that connects them in the dataset, and the term's domain frequency for d1 is "significantly" lower than that of another different domain d2. Our hypothesis is that we can detect mistyped entities by comparing such domain frequencies. We validate it as follows.
Setup. Since the ICWSM'09 dataset does not include labels for
named entities, we identify all type errors through manual evaluation. The complete evaluation we performed is available by request. We compute the list of terms in the dataset and their associated DF scores. Then, we obtain two random entity sets for our evaluation. The first, RANDOM contains 70 entity pairs (thus 140 typed entities) randomly chosen from the dataset. The second, SUSPICIOUS, consists of 326 entity pairs for which the gap in domain frequencies (highest to lowest) is higher a threshold.
More precisely, SUSPICIOUS is obtained as follows. For each term t in the dataset, we compute the difference between its highest and the lowest DF scores. We keep those terms for which this difference is larger than 0.5 (empirically tested to produce high precision with reasonable recall), resulting in 482 unique terms (i.e., relations). From these, we randomly chose 30 to obtain the entity pairs in our tests. For each term, we gathered up to 15 entity pairs, resulting in a total of 326 unique entity pairs (some terms were associated with less than 15 pairs), that generated 375 entities for the SUSPICIOUS evaluation (for most of the entity pairs, only one entity out of the two in a domain is identified as a mistake).
Hypothesis. Our hypothesis is that pairs in the SUSPICIOUS
set are more likely to contain at least one mistyped entity. This is justified by the following observation: out of the 375 entities, 269 (or 72%) are of type ORG, 83 (22%) are of type LOC, and 23 (6%) are of type PER. This distribution is very different than the one that takes into account all entities in the dataset: 12% for ORG, 43% for LOC, and 45% for PER. The type ORG, which appears in only 12% of all entities, appears in 72% of the entities in the SUSPICIOUS set, implying a higher error rate for organizations compared to other entity types.
Table 2 validates this hypothesis. In the table, precision is the number of entities with correct types (we did not consider the cor-
2We do not consider the MISC type in our experiments. They are too generic, making it hard to evaluate whether the entities assigned to this type are misclassified by LBJ.

PER LOC ORG Weighted

RANDOM 0.79 0.70 0.43 0.62

SUSPICIOUS 0.43 0.44 0.25 0.30

Table 2: Correctness of the entity types in the 2 evaluation sets

rectness of the entity boundary) divided by the total number of entities in each set. Weighted is the weighted average of the precision for the three types. Observe that in the RANDOM set entities are correctly typed 62% of the times, whereas in the SUSPICIOUS set this happens only 30% of the time. This 30% reflects the entities we incorrectly identified as mis-typed. Also, observe that LBJ is much more accurate in correctly identifying persons and locations compared to organizations.
Method. The significant observation from our experiment is that
the difference in domain frequency scores may be an effective way of identifying mis-typed entities. It effectively yields a very simple and automatic procedure for detecting incorrect type assignments which has 70% precision. Given the much lower rate mistyping rate of just 38% for the RANDOM set, these results are promising.
3. CONCLUSION
We proposed the use of Domain Frequency scores to predict entities which are erroneously typed by NER systems. This measure can be readily incorporated into existing NER systems with ease. DF exploits terms between pairs of entities to estimate the likelihood of a term to appear between given entity types. DF relies on global (corpus-wide) information as is thus sensitive to the domain at hand. We showed experimentally that the difference in DF scores for a given term serves as a good indicator that the entities associated through that term are incorrectly typed, and that this simple rule was able to detect an entity with incorrect type in 70% of the cases, and that this rate is much higher than that of a random sample of the dataset.
We are investigating ways in which to use the DF scores to further improve not only entity type identification but also the extraction of the relations among the entities. We envision a mutual refinement scheme in which both tasks go hand-in-hand.
4. ACKNOWLEDGEMENTS
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada and the Alberta Ingenuity Fund.
5. REFERENCES
[1] K. Burton, A. Java, and I. Soboroff. The icwsm 2009 spinn3r dataset. In ICWSM '09: Proceedings of the 3rd Int'l AAAI Conference on Weblogs and Social Media, 2009.
[2] F. Mesquita, Y. Merhav, and D. Barbosa. Extracting information networks from the blogosphere: State-of-the-art and challenges. In ICWSM '10: Proceedings of the 4th Int'l AAAI Conference on Weblogs and Social Media, 2010.
[3] L. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition. In CoNLL '09: Proceedings of the 13th Conference on Computational Natural Language Learning, pages 147­155, Morristown, NJ, USA, 2009. Association for Computational Linguistics.

884

Author Interest Topic Model
Noriaki Kawamae
NTT Comware 1-6 Nakase Mihama-ku Chiba-shi, Chiba 261-0023 Japan
kawamae@gmail.com

ABSTRACT
This paper presents a hierarchical topic model that simultaneously captures topics and author's interests. Our proposed model, the Author Interest Topic model (AIT), introduces a latent variable with a probability distribution over topics into each document. Experiments on a research paper corpus show that AIT is very useful as a generative model.
Categories and Subject Descriptors
H.3.1 [Content Analysis and Indexing]:
General Terms
Algorithms, experimentation
Keywords
Topic Modeling, Latent Variable Modeling
1. INTRODUCTION
Attention is being focused on how to model users' interests in several fields. A model of interest allows us to infer which topics each user prefers and to measure the similarity between them in terms of their interests. For example, the Author-Topic(AT) [3] groups all papers associated with a given author by using a single topic distribution associated with this author. Author-Persona-Topic(APT) [2] introduces a persona, which is also a latent variable, under a single given author. Thus, these models allow each author's documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona
This paper presents the Author Interest Topic(AIT) model; it is a generalization of known author interest models such as AT and APT. AIT allows a number of possible latent variables to be associated with author's interest, while previous models limit this number. Therefore, AIT can describe a wider variety of authors' interests than other models, which reduces the perplexity. Moreover, AIT can infer the overall interest in the training data and so can assign probabilities to previously unseen documents.
2. AUTHOR INTEREST TOPIC MODEL
This section details our model. Table 1 shows the notations used in this paper. Figure 1 shows graphical models to
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Graphical models: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indicates a conditional dependency between variables and the plates indicate a repeated sampling with the iteration number shown. This figure shows that each author produces words from a set of topics that are preferred by the author in (a), persona associated with the author in (b), each document class in (c). In learning a document written by multiple authors, AIT makes copies of the document and associates one copy with each author.
describe the generative process. For modeling each author's interest, our proposal, AIT, incorporates document class cd; it provides an indicator variable that describes which mixture of topics each document d takes, into d. Accordingly, AIT represents documents of similar topics as the same document class in the same way that topic models represent cooccurrence words as the same topic variable. Therefore, the difference between AIT and AT, APT is that rather than representing author's interest as a mixture of topic variables a(AT) or Pa (APT) in each document layer, AIT represents each author's interest as a mixture of document classes a in each author layer. Although both a(AT) and Pa (APT) are associated with only authors, the document class can be shared among authors. This class allows AIT to represent documents having similar topics as the same document class by merging parameters; this reduces the number of possible parameters without losing generality. Accordingly, as the size of training data is increased, relatively fewer parameters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data. Moreover we decide the number of latent variables following CRP [1]. Consequently, AIT increases the number of possible latent variables for explaining all authors' interests.
AIT employs Gibbs sampling to perform inference approximation. In the Gibbs sampling procedure, we need to cal-

887

Table 1: Notations used in this paper

SYMBOL DESCRIPTION

A

number of authors

J

number of document classes

T

number of topics

D

number of documents

V

number of unique words

Ad

authors associated with document d

Da

number of documents written by author a

Nd

number of word tokens in document d

ai

author associated with ith token in document d

pd

persona associated with document d

cd

document class associated with document d

zdi

topic associated with the ith token in document

d

wdi

ith token in document d

a

multinomial distribution of document classes

specific to author a (a|  Dirichlet() )

j

multinomial distribution of topics specific to in-

terest j (j |  Dirichlet() )

t

multinomial distribution of words specific to

topic t (t|  Dirichlet() )

culate the conditional distributions. The predictive distribution of adding interest class cd in documents written by author a to topic cd = j is given by

8 (Pt

njt\d +t)

Q t

(njt +t )

>>< n , aj\d

Q t

(njt\d +t)

(Pt

njt +t )

P (j|c\d, a, z, , )  if j is an existing class class

> > :

j

(Pt njt\d +t)

Q t

(njt\d +t)

, Q t

(njt +t

)

(Pt njt+t)

otherwise

(1)

where naj\d represents the number of documents assigned to j in all documents written by author a, except d, and njt\di represents the total number of tokens assigned to topic t

in the documents associated with document class j, except

token di.

The predictive distribution of adding word wdi in docu-

ment d written by a to topic zd = t is given by

8
> > <

n , ntw\di+w

jt\di

PV v

(ntv\di +v )

P (t|j, z\di, w, , )  if t is an existing class

> > :

 , ntw\di+w

t

PV v

(ntv\di +v

)

otherwise

(2)

where ntw\di represents the total number of tokens assigned to word w in topic t, except token di, and njt\di represents the total number of tokens assigned to topic t in

all tokens assigned to j, except token di.

3. EXPERIMENTS
We focus here on the extraction of interests from given documents, and demonstrate AIT's performance as a generative model. The dataset used in our experiments consisted of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that appeared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this dataset for training and comparison.
In our evaluation, the smoothing parameters ,  and

Table 2: Perplexity of AT, APT and AIT: This difference between AIT and APT is significant according to one-tailed t-test with the number of samples G = 100. For fair comparison, the number of topic variables T was fixed at 200, the number of document classes J was fixed at 40(AIT). Results that differ significantly by t-test p < 0.01, p < 0.05 from APT are marked with '**', '*' respectively. The value of Avg means the average computing time for each iteration in gibbs sampling.

Iteration AT APT AIT

2000 1529 1454 1321

4000 1488 1304 1217

6000 1343 1180 1103

8000 1339 1059 988

10000 1333 1027 964

Avg 3.2s 10.4s 11.7s

 were set at 0.1, 10(APT),1(AIT) and 1, respectively. We ran single Gibbs sampling chains for 10000 iterations on machines with Dual Core 2.66 GHz Xeon processors.
To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated parameters and compared the resulting values.
Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are better). Table 2 shows the results of the perplexity comparison. This table shows that AIT yielded significantly lower perplexity on the test set than AT or APT, which shows that AIT is better as a topic model. This is due to the ability of AIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather than grouping documents by a given author or persona under a few topic distributions. This implies that clustered documents contain less noise than otherwise. If the number of document classes is overly restricted, the difference between the observed data and the data generated by the model under test increases, which raises the perplexity.
4. CONCLUSION
Our proposed model, AIT, supports the expression of topics in text documents and can identify the interests of authors in these documents. Future work includes extending AIT by taking other metadata such as time, references and link structure into account, for tracking the dynamics of interests and topics.
5. REFERENCES
[1] D. J. D. Aldous. Exchangeability and related topics, volume 1117 of Lecture Notes in Math. Springer, Berlin, 1985.
[2] D. Mimno and A. McCallum. Expertise modeling for matching papers with reviewers. In KDD, pages 500­509, 2007.
[3] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Griffiths. Probabilistic author-topic models for information discovery. In KDD, pages 306­315, 2004.

888

Mining Adjacent Markets from a Large-scale Ads Video Collection for Image Advertising

Guwen Feng
Nanjing University Nanjing, Jiangsu, P.R.China
linvondepp@gmail.com

Xin-Jing Wang, Lei Zhang, Wei-Ying Ma
Microsoft Research Asia Beijing, P.R.China
{xjwang,leizhang,wyma}@microsoft.com

ABSTRACT
The research on image advertising is still in its infancy. Most previous approaches suggest ads by directly matching an ad to a query image, which lacks the power to identify ads from adjacent market. In this paper, we tackle the problem by mining knowledge on adjacent markets from ads videos with a novel Multi-Modal Dirichlet Process Mixture Sets model, which is a unified model of (video frames) clustering and (ads) ranking. Our approach is not only capable of discovering relevant ads (e.g. car ads for a query car image), but also suggesting ads from adjacent markets (e.g. tyre ads). Experimental results show that our proposed approach is fairly effective.
Categories and Subject Descriptors
I.5.4 [Pattern Recognition]: Applications ­ computer vision. G.3 [Probability and Statistics]: Nonparametric statistics.
General Terms
Algorithms, Performance.
Keywords
Image advertising, adjacent marketing, video retrieval.
1. INTRODUCTION
Though image has become an important media in the Web, how to monetize web images is a seldom touched problem. Few research works have been published in the literature. Among them, most of the works suggest directly mapping an ad to an image [2]. They suffer from the vocabulary impedance problem so that if a term does not appear in both an image and an ad' features, no connections will be built between them. The approach of Wang et al. [4] improves this by leveraging the ODP ontology to bridge the vocabulary gap, but it is still limited by the available texts.
Adjacent marketing means to develop additional items which compliment a customer's needs in some manner, e.g. suggest insurance when one buys a car. The insurance package thus makes up of qualified adjacent markets of cars. A key challenge is to discover the potential adjacent market (e.g. insurance) of a certain
 This work was performed in Microsoft Research Asia.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Retrieved And Expanded Frames

... ...

Query

Ranking Results

Advertise

Benz

Insurance

Tires

Figure 1. Key idea: the story frames (in red blocks) and ads frames (cyan blocks) of video ads suggest adjacent markets.

product/object (e.g. car).
In this paper we propose a solution of adjacent marketing for image advertising based on a large-scale ads video collection. It is motivated by the fact that generally a video ad contains two types of frames ­ story frames and ads frames, as shown in Figure 1. Story frames in general provide the main concepts (e.g. cars) that are related to the corresponding ads (e.g. insurance, tires). They imply certain human knowledge on the adjacent markets, e.g. showing tire ads on car images. A novel Multi-Modal Dirichlet Process Mixture Sets (MoM-DPM Sets) model is proposed as the key technique behind.
2. SYSTEM OVERVIEW
Figure 2 shows the system overview. In the offline stage, we extract video keyframes and perform auto-tagging on them. The online stage contains three steps: 1) multimodal search on keyframes of ads videos given a query image (or query images). Both visual and textual similarities are taken into consideration. The texts come from three resources: image auto-tagging, OCR and surrounding texts; 2) search result expansion. Since the search step tends to find video frames of similar content, to incorporate the adjacent information between objects, we expand the retrieved frames with the rest frames in corresponding videos. For example, given the pizza image in Figure 2, the retrieved frames are generally about food, but by expanding them with the rest frames from the same ads videos, we are able to retrieve those soft drink and restaurant video frames which suggest the adjacent markets; and 3) ads clustering and ranking. The disadvantage brought by frame expansion is that the search results become noisy and with scattered topics. We propose the MoM-DPM Sets model to automatically learn the key topics from the expanded search results, and rank an ads database with the learnt topics. The top-

893

Query Image
Pizza 1. Search

Advertise

Annotate Using Arista
Visutal Features Detection Using ColorDescriptor

Large-Scale Video Frames DB

2. Cosine Measurement

Expanded Video Frames 3. Expand

Suggested Ads

Pizza

Pepsi Restaurant

6. Sorting Approach

Ads DB

MoM-DPM Sets

Clusters

5. Ranking

4. Clustering

...

...

Figure 2. System Overview.

ranked ads will be output. Therefore we find ads of Pepsi Cola and restaurants for the pizza image.

3. THE MoM-DPM SETS MODAL
The MoM-DPM Sets model addresses four challenges: 1) discover the latent topics shared among the frames, 2) automatically determine the number of topics, 3) leverage both the visual and textual features to ensure a better performance of topic discovery, and 4) unify the approaches of topic mining and ads ranking.

Let be the th latent topic and , be the visual and textual features of a query image respectively. Let , , denote the
concentration parameter and base distributions of visual and textual features respectively. Let , be model parameters and
, be the visual and textual features of the observed video
frames labeled by the topic respectively. The general form of is MoM-DPM Sets is given in Eq(1).

| , ,, ,

,

|

,



|

|,

(1)

if

for some ;



|

|

|

|

if

for all .

where and are the observed video frames corresponding to topic . is the normalization factor. is the number of observed video frames and , is the number of observed video frames (except the th) which belong to topic . We use Gibbs sampling to solve the model, which generally converges in 30 iterations.
MoM-DPM Sets has two key features which make it different from previous multimodal DP Mixture processes [3]. Firstly, rather than to learn an optimal parameter set of , , it intends to figure out the membership of each video frame given the observed video frames , . In our approach, , , and
are known ( and are learnt from the clustering step), while
the model parameters are going to be integrated out analytically. Such a set-based reasoning strategy [1] is more powerful in discovering analogical objects, e.g. given a frame set of Pepsi-cola and Coca-cola, this model is able to discover soda because they share the same concept of soft drinks. Secondly, since the model does not rely on certain parameter set, the clustering (topic mining) step and ranking step shares the same model formulation. The ranking process is as Eq.(2).

Average Precision

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

3

5

Our Approach

7

10

15

20

top N

Argo [4] DM [2]

Figure 3. Average precision performance @ top N.

| .. , ,

max



|

,

(2)



|

,

where ..

, , ... , defines the latent topic space.

4. EXPERIMENTS

We crawled about 32k videos from Youtube.com initiated by 30 popular concepts for advertising. In total 327,889 key frames were extracted, which make up of the ads videos collection for frame search. We randomly selected 450 ads as a separate ads DB for the ranking purpose. 100 Flickr images were used as queries.

Figure 3 illustrates the average precision at top 20 ads of our approach compared with those of the baselines Argo [4] and direct match [2]. It can be seen that our approach consistently outperforms the baselines. The gap between the blue curve and the green one indicates that our approach is able to identify the relevant ads from potential adjacent market, which have little overlap with the query image in both visual and textual features. And the gap between the red curve and the green one indicates that Argo [4] also tackles the adjacent marketing problem to a certain extent but it is not effective enough.
There are big gaps between our methods and the baselines in top 3 results, while the gap narrows down from top 5 to top 20. This may due to the limited size of our ads DB. Considering that generally a publisher such as Google shows less than five targeted ads, our method suggests a promising research direction for adjacent marketing.
5. CONCLUSION
Web image is an uncovered gold mine. Our method is the first work to tackle the adjacent marketing problem for image advertising. It leverages the human intelligence embedded in video ads to build the connections among ads objects based on a novel Multi-Modal Dirichlet Process Mixture Sets model.

6. REFERENCES
[1] Z. Ghahramani, and K. A. Heller. Bayesian Sets. Neural Information Processing Systems (NIPS). 2005.
[2] T. Mei, X.-S. Hua, and S.-P. Li. Contextual In-Image Advertising. 2008.
[3] A. Velivelli, and T.S. Huang. Automatic Video Annotation Using Multimodal Dirichlet Process Mixture Model. ICNSC 2008.
[4] X.-J. Wang, M. Yu, et al. Argo: Intelligent Advertising by Mining a User's Interest from His Photo Collections, in conjunction with SIGKDD (ADKDD), Paris, 2009.

894

A Co-learning Framework for Learning User Search Intents from Rule-Generated Training Data

Jun Yan1 Zeyu Zheng1
1Microsoft Research Asia Sigma Center, No.49, Zhichun Road Beijing, 100190, China
{junyan, v-zeyu, zhengc}@microsoft.com

Li Jiang2 Yan Li2 Shuicheng Yan3 Zheng Chen1

2Microsoft Corporation

3Department of Electrical and

One Microsoft Way

Computer Engineering

Redmond, WA 98004

National University of Singapore

{lij, roli}@microsoft.com

117576, Singapore
eleyans@nus.edu.sg

ABSTRACT
Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as "compare products", "plan a travel", etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data for learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated training data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different training datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently classified data by one classifier are added to other training datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process.

Though various popular machine learning techniques could be applied to learn the underlying search intents of the users, it is generally laborious or even impossible to collect sufficient and label high quality training data for such learning task [1]. Despite of the laborious human labeling efforts, many intuitive insights, which could be formulated as rules, can help generate small scale possibly biased and noisy training data. For example, to identify whether the users have intents to compare different products, several assumptions may help make the judgment. Generally, we may assume that if a user submits a query with explicit intent expression, such as "Canon 5d compare with Nikon D300", he/she may want to compare products. Though the rules satisfy the human common sense, there are two major limitations if we directly use them to infer ground truth. First, the coverage of each rule is often small and thus the training data may be seriously biased and insufficient. Second, the training data are usually not clean since no matter which rule we use, there may exist exceptions. In this paper, we propose a co-learning framework (CLF) for learning user intent from the rule-generated training data, which are possibly biased and noisy. The problem is,

Without laborious human labeling work, is it possible to train

user search intent classifier using the rule-generated training data,

which are generally noisy and biased? Given sets of rule-

generated training datasets , 1,2, ... , how to train the

classifier :

on top of these biased and noisy training data

sets with good performance?

General Terms
Algorithms, Experimentation
Keywords
User intent, search engine, classification.
1. INTRODUCTION
The classical relevance based search strategies may fail in satisfying the end users due to the lack of consideration on the real search intents of users. For example, when different users search with the same query "Canon 5D" under different contexts, they may have distinct intents such as to buy Canon 5D, to repair Canon 5D, etc. The search results about Canon 5D repairing obviously cannot satisfy the users who want to buy a Canon 5D camera. Learning to understand the true user intents behind the users' search queries is becoming a crucial problem for both Web search and behavior targeted online advertising.
Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. THE CO-LEARNING FRAMEWORK

Suppose we have sets of rule-generated training data ,

1,2, ... , which are possibly noisy and biased, and a set of

unlabeled user behavioral data . Each data sample in the

training datasets is represented by a triple , ,

1,

1,2, ... | |, where stands for the feature vector of the data

sample in the training data , is its class label and | | is the

total number of training data in . On the other hand, each

unlabeled data sample, i.e. the user search session that could not

be covered by our rules, is represented as , ,

0,

1,2, ... | |. Suppose for any

, all the features constituting

the feature space are represented as a set

| 1,2, ... .

Suppose among all the features F, some have direct correlation to

the rules, that is they are used to generate the training dataset .

These features are denoted by

, which constitute a subset

of F. Let =

be the subset of features having no direct

correlation to the rules used for generating training dataset .

Given a classifier :

, where

is any subset of F, we

use to represent an untrained classifier and use to represent

the classifier trained by the training data . Suppose

|

895

means to train the classifier by training dataset using the

features

, we have

trained classifier , let

| , 1,2, ... . For the | stands for classifying

using features F. We assume for each output result of trained

classifier , it can output a confidence score. Let

|

,

where is the class label of assigned by and the is the corresponding confidence score.

After generating a set of training data , 1,2, ... , based on rules, we first train the classifier by , 1,2, ... ,
independently. Then we can get a set of K classifiers,

| , 1,2, ... .

Note that the reason why we use to train classifier on top of

instead of using the full set of features F is that is generated

from some rules correlated to , which may overfit the classifier

if we do not exclude them. After each classifier is trained

by , we use to classify the training dataset itself. A basic assumption of CLF is that the confidently classified instances by

classifier , 1,2, ... , have high probability to be correctly

classified. Based on this assumption, for any

, if the

confidence score of the classification is larger than a threshold, i.e. > and the class label assigned by the classifier is different

from the class label assigned by the rule, i.e.

, then

is considered as noise in the training data . Note that here

is the label of assigned by classifier, is its observed

class label in training data, and is the true class label, which is

not observed. We exclude it from and put it into the unlabeled dataset . Thus we update the training data by

,

.

Then we use the classifier ,

1,2, ... , to classify the

unlabeled data independently. Based on the same assumption

that the confidently classified instances by classifier have high probability to be correctly classified, for any data belonging to ,

if the confidence score of the classification is larger than a

threshold, i.e. > , where

|

, we

include into the training dataset. In other words,

,

, 1,2 ... , .

Through this way, we can gradually reduce the bias of the rulegenerated training data.

On the other hand, some unlabeled data are added into the training

datasets. Suppose the ,

1| is the probability of a

data sample to be involved in the training data at the iteration n

conditioned on this data sample is represented as a feature vector

and

1 is the probability of any data sample in D is

considered as a training data sample. It can be proved that after n

iterations using CLF, for each training dataset, we have

,

1|

1.

The remaining questions are when to stop the iteration and how to

train the classifier after iteration stops. In this work, we define the

iteration stopping criteria as "if |{ |

,

}| < n

or the number of iterations reaches N, then we stop the iteration".

After the iterations stop, we obtain K updated training datasets

with both noise and bias reduced. Finally, we merge all these

training datasets into one. Thus we can train the final classifier as

.

3. EXPERIMENTS
In this short paper, we utilize the real user search behavioral dataset, which comes from the search click-through log of a commonly used commercial search engine. It contains 3,420 user search sessions, in each of which, the user queries and clicked Web pages are all logged. Six labelers are asked to label the user intents according to the user behaviors as ground truth for results validation. We name this dataset as the "Real User Behavioral Data". The n-gram features are used for classification in the Bag of Words (BOW) model. One of the most classical evaluation metrics for classification problems, F1, which is a tradeoff between Precision (Pre) and recall (Rec) is used as the evaluation metric. For comparison purpose, we utilize several baselines to show the effectiveness of the proposed CLF. Firstly, since we can use different rules to initialize several sets of training data, directly utilizing one training dataset or the combination of all rule-generated training datasets to train the same classification model can give us a set of classifiers. Among them, we take the classifier with the best performance as the first baseline, referred to as "Baseline" in the remaining parts of this section. The second baseline is the DL-CoTrain algorithm, which is a variant of cotraining algorithm. It also starts from the rule-generated training data for classification and thus has the same experiments configuration as CLF. The classification method selected in CLF is the classical Support Vector Machine (SVM). In Table 3, we show the experimental results of CLF after 25 rounds of iterations compared with the baseline algorithms. From the results we can see that, in terms of F1, the CLF can improve the classification performance as high as 47% compared with the baseline.
Table 3. Results of CLF after 25 iterations

Baseline

DL-CoTrain

Pre

0.78

0.78

Rec

0.24

0.12

F1

0.36

0.21

CLF 0.81 0.39 0.53

4. CONCLUSION
One bottleneck of user search intent learning for Web search and online advertising is the laborious training data collection. In this paper, we proposed a co-learning framework (CLF), which aims to classify the users' search intents without laborious human labeling efforts. We firstly utilize a set of rules coming from the common sense of humans to automatically generate some initial training datasets. Since the rule-generated training data are generally noisy and biased, we propose to iteratively reduce the bias of the training data and control the noises in the training data. Experimental results on both real user search click data and public dataset show the good performance of the co-learning framework.

5. REFERENCE
[1] Russell, D.M., Tang, D., Kellar, M. and Jeffries, R. 2009. Task behaviors during web search: the difficulty of assigning labels. Proceedings of the 42nd Hawaii International Conference on System Sciences (Hawaii, United States, January 05 - 08, 2009). HICSS '09. IEEE Press, 1-5. DOI= 10.1109/HICSS.2009.417.

896

Learning the Click-Through Rate for Rare/New Ads from Similar Ads

Kushal Dave
Language Technologies Research Centre International Institute of Information Technology
Hyderabad, India
kushal.dave@research.iiit.ac.in

Vasudeva Varma
Language Technologies Research Centre International Institute of Information Technology
Hyderabad, India
vv@iiit.ac.in

ABSTRACT
Ads on the search engine (SE) are generally ranked based on their Click-through rates (CTR). Hence, accurately predicting the CTR of an ad is of paramount importance for maximizing the SE's revenue. We present a model that inherits the click information of rare/new ads from other semantically related ads. The semantic features are derived from the query ad click-through graphs and advertisers account information. We show that the model learned using these features give a very good prediction for the CTR values.
Categories and Subject Descriptors
I.2.6 [Computing Methodologies]: Artificial Intelligence-- Learning; I.6.5 [Computing Methodologies]: Simulation and Modeling--model development; H.3.3 [Information Systems]: Information Storage and Retrieval
General Terms
Algorithms, Economics, Experimentation.
Keywords
Sponsored Search, Click-Through Rate Prediction, Ranking
1. INTRODUCTION
Sponsored search can be seen as an interaction between three parties - SE, User and the Advertiser. The user issues a query to a SE related to the topic on which he/she seeks information. Advertisers and SEs try to exploit the immediate interest of user in the topic by displaying ads relevant to the query topic. Advertisers bid on certain keywords known as bid terms and their ads may get displayed based on the match between bid term and the user query. SEs try to rank the ads in a way that maximizes its revenue.
Search engines typically rank ads based on the expected revenue ( ad(Rev)). Expected revenue from an ad is a function of both bid and relevance: ad(Rev) = BidRelevancead. The relevace of an ad is measured using its CTR. The CTR of an ad for a query is the no. of clicks normalized by no. of impressions for that query. CTR of an ad is a function of both ad and the query, i.e. an ad can have a different CTR for different queries. The CTR value for an ad-query pair
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

is calculated form past click logs. For new/rare ads, we do not have any/sufficient past click data. Hence CTR for such ads need to be predicted so that they can be ranked along with other frequent ads. Richardson et. al. [5] predict the CTR based on ad text, ad quality etc. Fain et. al. [4] predict the CTR based on term clusters. We propose similarity features derived from click logs and advertisers hierarchy to accurately predict the CTR for new ads.

2. DATASET

The dataset used in our experiments comprised 12 days search log from Yahoo! search engine's US market. After removing redundant fields, each record in the dataset contained following fields: 1.Query 2.Term Id 3. Creative ID 4.Adgroup ID 5.Campaign ID 6.Account ID 8.CTR. Fields 2-6 point to a unique ad. Creative id points to the ad text. An ad text comprises bid term, title, abstract & display URL. The CTR values are normalized by removing the position & presentation bias. After some preprocessing, we got 1,447,543 unique query-ad pairs from the click through logs. It contained 1,97,080 unique queries and 9,43,431 unique ads. We randomly divide this dataset into 65-25-10 ratio for training, testing and validation respectively. We use Gradient boosted decision trees (GBDT) as a regression model [3]. Using validation set, the number of trees and no. of nodes parameters of GBDT were set to 600 and 150 respectively.

3. FEATURES

Features from Query-ad click graph: These features

are based on the semantic relations of the queries and ads

with other similar queries and ads. Regelson [4] have shown

that similar ads (bid terms in their case) follow similar CTR

distribution. The idea here is to learn the CTR values of

query-ad pair from semantically similar queries and ads.

We derive the semantic similarity from the query ad click-

through graph. The click graph is built from 12 days query

log (same period from which we generated our dataset).

Queries are represented as vectors and these query vectors

are compared to find similarity amongst the queries. A

query q is represented as a vector of transition probability

from q to all the ads in the graph. Edges are weighted us-

ing click frequency-inverse query frequency (CF-IQF) model:

cf iqf (qi, aj ) = cij  iqf (aj ).

The transition probability from a query to an ad, P (aj|qi) =

cf iqf (qi, aj)/cf iqf (qi). Each query is represented as q =

(P (a1|qi), P (a2|qi), · · · , P (an|qi)). The similarity between

two queries qi and qj is the cosine similarity between the

two query vectors. Sim(qi, qj) = Cosine

qi  qj qi · qj

. This

897

Table 1: Improvement for various features (p-value  0.01)

Feature
Baseline Sim-Q Sim-A Sim-QA Term Creative Adgroup

RMSE (1e-3)
7.20 5.86 6.31 5.68 6.24 6.51 5.87

KL Diver-gence (1e-1)
1.72 1.42 1.53 1.38 1.45 1.50 1.35

% Improvement
18.61% 12.36% 21.11% 13.34%
9.6% 18.48%

Feature
Campaign Account AdH
SimQA+Camp QADL
SimQA+Camp +QADL

RMSE (1e-3) 5.67 5.94 6.20 5.28 6.50
5.14

KL Diver-gence (1e-1)
1.32 1.39 1.46 1.24 1.56
1.21

% Improvement
21.25% 17.50% 13.9% 26.67% 9.72%
28.61%

similarity is used to predict the CTR for new query-ad pair by retrieving top k queries similar to q' and calculating the weighted average of the CTR values for all the ads over query q' as in [1]. Using query similarity,the CTR is estimated as:
X CT R(qk)  Sim(qi, qk)
QCT R(ai) = k X Sim(qi, qk)
k
The similarity between ads is also calculated in a similar fashion, with each ad being represented by the transition probability from ad to query P (qj|ai) and similarity between two ads is reffered as Sim(ai, ak). Using ad similarity, The CTR of is estimated as follows:
X CT R(ak)  Sim(ai, ak)
ACT R(ai) = k X Sim(ai, ak)
k
Along with QCTR/ACTR We also consider the number of similar queries/ads retrieved (Nq/Na). The Query and ad similarity featuers are called Sim-Q & Sim-A.
Figure 1: A typical Ad hierarchy
Features from Ad Hierarchy: Advertisements on an ad engine are typically maintained in some kind of a hierarchy. One such hierarchy is shown in Fig. 1. There are numerous reasons for maintaining ads in a hierarchy: (1)Advertiser's business may span various business units (BU). Ads from the same advertiser but from different BUs are maintained in different accounts. (2)For each BU, the advertisers can have ads on a range of products. Advertisements from the same account on similar products fall under the same Campaign. (3)Adgroups do further granular classification of ads. (4) Finally, an ad comprises a bid term and ad text (creative). Combination of these two makes an ad. We aggregate ads at each level viz. Term, Creative, Adgroup, Campaign and Account, compute the average within each group and use them as features in our model. In addition, number of featuers in each group are also taken as

features. We call these features as AdH features. Detailed explanation of all the features is available in [2].
Features from Query-ad lexical match: In an attempt to capture how relevant an ad is to the query, we compute the lexical overlap between the query and these ad units. We compute various text matching features such as cosine similarity, word overlap, character overlap, and string edit distance for each combination of unigrams and bi-grams. We refer to this category of features as QADL. For all the set of features we also consider log of each feature as a feature. In all we have 50 features.
As shown in Table 1, Sim-Q & Sim-A give good improvements and when combined (Sim-QA) give an improvement of 21.11%. In the AdH category, Campaign (Camp) gave the best result and when Sim-QA was clubbed with Camp the improvement over baseline reached 26.67%. Finally, lexical feature did not yeild much improvement alone, but (SimQA+Camp+QADL) give the best performance with a good 28.61% improvement over the baseline. All these improvements are staistically significant at 99% significance level.
When all the features were ranked according to the feature importance [3]. Features like Campaign, ACTR, log(ACTR), No. of ads in campaign were amongst the top few.
4. CONCLUSIONS
We have proposed an approach to predict the CTR for new ads based on the similarity with other ads/queries. The similarity of ads is derived from sources like query ad clickthrough graph and advertisement hierarchies maintained by the ad engine. The model gives good prediction on the CTR values of new ads. Analysis of the feature's contribution shows that the features derived from the ad hierarchy and from the click-through graphs contribute the most to the model followed by some of the word overlap features.
5. ACKNOWLEDGMENTS
We are grateful to Yahoo! labs Bangalore for granting access to the ad click-through logs.
6. REFERENCES
[1] T. Anastasakos, D. Hillard, S. Kshetramade, and H. Raghavan. A collaborative filtering approach to ad recommendation using the query-ad click graph. In CIKM '09, pages 1927­1930, 2009.
[2] K. Dave and V. Varma. Predicting the click-through rate for rare/new ads. Technical report IIIT/TR/2010/15, IIIT-H, 2010.
[3] J. H. Friedman. Stochastic gradient boosting. Comput. Stat. Data Anal., 38(4):367­378, 2002.
[4] M. Regelson and D. C. Fain. Predicting click-through rate using keyword clusters. In Electronic Commerce (EC). ACM, 2006.
[5] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In WWW '07, pages 521­530, 2007.

898

Graphical Models for Text: A New Paradigm for Text Representation and Processing

Charu C. Aggarwal
IBM T. J. Watson Research Center Hawthorne, New York, USA
charu@us.ibm.com

Peixiang Zhao
University of Illinois at Urbana-Champaign Urbana, Illinois, USA
pzhao4@uiuc.edu

ABSTRACT
Almost all text applications use the well known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve distance and ordering information between the words, and provide a much richer representation of the underlying text. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms: Algorithms
1. INTRODUCTION
The most common representation for text is the vector-space representation. The vector-space representation treats each document as an unordered "bag-of-words". While the vector-space representation is very efficient because of its simplicity, it loses information about the structural ordering of the words in the document. For many applications, such an approach can lose key analytical insights. This is especially the case for applications in which the structure of the document plays a key role in the underlying semantics. The efficiency of the vector-space representation has been a key reason that it has remained the technique of choice for a variety of text processing applications. On the other hand, the vectorspace representation is very lossy because it contains absolutely no information about the ordering of the words in the document. One of the goals of this paper is to design a representation which retains at least some of the ordering information among the words in the document without losing its flexibility and efficiency for data processing.
While the processing-efficiency constraint has remained a strait-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

jacket on the development of richer representations of text, this constraint has become easier to overcome in recent years. This is because of advances in the computational power of hardware, and the increased sophistication of algorithms in other fields such as graph mining, which can be leveraged with representations such as those discussed in this paper. In this paper, we will design graphical models for representing and processing text data. In particular, we will define the concept of distance graphs, which represents the document in terms of the distances between the distinct words. We will show that such a representation can retain much richer information about the underlying data. It also allows for the use of many current text and graph mining algorithms without the need for new algorithmic efforts for this new representation. In fact, we will see that the only additional work required is a change in the underlying representation, and all existing text applications can be directly used with a vector-space representation of the structured data. In some cases, it also enables distance-based applications which are not possible with the vector-space representations.
2. DISTANCE GRAPHS
While the vector-space representation maintains no information about the ordering of the words, the string representation is at the other end of spectrum in maintaining complete ordering information. Distance graphs are a natural intermediate representation which preserve a high level of information about the ordering and distance between the words in the document. At the same time, the structural representation of distance graphs make it an effective representation for easy processing. Distance graphs can be defined to be of a variety of orders depending upon the level of distance information which is retained. Specifically, distance graphs of order k retain information about word pairs which are at a distance of at most k in the underlying document. We define a distance graph as follows:
DEFINITION 1. A distance graph of order k for a document D in corpus C is defined as graph G(C, D, k) = (N (C), A(D, k)), where N (C) is the set of nodes defined specific to the corpus C, and A(D, k) is the set of edges in the document. The sets N (C) and A(D, k) are defined as follows:
(a) The set N (C) contains one node for each distinct word in the entire document corpus C. Therefore, we will use the term "node i" and "word i" interchangeably to represent the index of the corresponding word in the corpus. Note that the corpus C may contain a large number of documents, and the index of the corresponding word (node) remains unchanged over the representation of the different documents in C. Therefore, the set of nodes is denoted by N (C), and is a function of the corpus C. (b) The set A(D, k) contains a directed edge from node i to node j

899

MARY HAD A LITTLE LAMB, LITTLE LAMB, LITTLE LAMB, MARY HAD A LITTLE LAMB, ITS FLEECE WAS WHITE AS SNOW

REMOVE STOPWORDS
CREATE

MARY LITTLE LAMB, LITTLE LAMB, LITTLE LAMB, MARY LITTLE LAMB, FLEECE WHITE SNOW

Order 0:

DISTANCE GRAPH

2

4

4

1

1

1

MARY

LITTLE

LAMB

FLEECE

WHITE

SNOW

Order 1:

2 2
MARY
Order 2:
2 2
MARY

4

4

4 1

LITTLE

LAMB

2

1

6

4

6 1

LITTLE

LAMB

3

2

1 1
FLEECE
1 1
FLEECE 1

1 1
WHITE
1 1
1 WHITE

1 SNOW
1 SNOW

Figure 1: Illustration of Distance Graph Representation

if the word i precedes word j by at most k positions. For example, for successive words, the value of k is 1. The frequency of the edge is the number of times that word i precedes word j by at most k positions in the document.
We note that the set A(D, k) always contains an edge from each node to itself. The frequency of the edge is the number of times that the word precedes itself in the document at a distance of at most k. Since any word precedes itself at distance 0 by default, the frequency of the edge is at least equal to the frequency of the corresponding word in the document.
Most text collections contain many frequently occurring words (known as stop-words), which are typically filtered out before text processing. Therefore, it is assumed that these words are removed from the text before the distance graph construction. In other words, stop-words are not counted while computing the distances for the graph, and are also not included in the node set N (C). This greatly reduces the number of edges in the distance graph representation. This also translates to better efficiency during processing.
We note that the order-0 representation contains only self loops with corresponding word frequencies. Therefore, this representation is quite similar to the vector-space representation. Representations of higher orders provide structural insights of different levels of complexity. An example of the distance graph representation for a well-known nursery rhyme "Mary had a little lamb" is illustrated in Figure 1. In this figure, we have illustrated the distance graphs of orders 0, 1 and 2 for the text fragment. The distance graph is constructed only with respect to the remaining words in the document, after the stop-words have already been pruned. The distances are then computed with respect to the pruned representation. Note that the distance graphs or order 0 contain only self loops. The frequencies of these self-loops in the order-0 representation corresponds to the frequency of the word, since this is also the number of times that a word occurs within a distance of 0 of itself. The number of edges in the representation will increase for distance graphs of successively higher orders. Another observation is that the frequency of the self loops in distance graphs of order 2 increases over the order-0 and order-1 representations. This is because of repetitive words like "little" and "lamb" which occur within alternate positions of one another. Such repetitions do not change the self-loop frequencies of order-0 and order-1 distance graphs, but do affect the order-2 distance graphs. We note that distance graphs of higher orders may sometimes be richer, though this is not necessarily true for orders higher than 5 or 10. For example, a distance graph with order greater than the number of distinct words in the document would be a complete clique. Clearly, this does not necessarily encode useful information. On the other hand, distance graphs of order-0 do not encode a lot of useful information either.
From a database perspective, such distance graphs can also be

represented in XML with attribute labels on the nodes corresponding to word-identifiers, and labels on the edges corresponding to the frequencies of the corresponding edges. Such a representation has the advantage that numerous data management and mining techniques for semi-structured data have already been developed. These can directly be used for such applications. Distance graphs provide a much richer representation for storage and retrieval purposes, because they partially store the structural behavior of the underlying text data.
An important characteristic of distance graphs is that they are relatively sparse, and contain a small number of edges for low values of the order k. As we will see in the experimental results presented in [1], it suffices to use low values of k for effective processing in most mining applications.
PROPERTY 1. Let f (D) denote the number of words in document D (counting repetitions), of which n(D) are distinct. Distance graphs of order k contain at least n(D)·(k+1)-k·(k-1)/2 edges, and at most f (D) · (k + 1) edges.
The modest size of the distance graph is extremely important from the perspective of storage and processing. In fact, the above observation suggests that for small values of k, the total storage requirement is not much higher than that required for the vectorspace representation. This is a modest price to pay for the semantic richness captured by the distance graph representation.
One advantage of the distance-graph representation is that it can be used directly in conjunction with either existing text applications or with structural and graph mining techniques, as follows: (a) Use with existing text applications: Most of the currently existing text applications use the vector-space model for text representation and processing. It turns out that the distance graph can also be converted to a vector-space representation. The main property which can be leveraged to this effect is that the distance-graph is sparse and the number of edges in it is relatively small compared to the total number of possibilities. For each edge in the distancegraph, we can create a unique "token" or "pseudo-word". The frequency of this token is equal to the frequency of the corresponding edge. Thus, the new vector-space representation contains tokens only corresponding to such pseudo-words (including self-loops). All existing text applications can be used directly in conjunction with this "edge-augmented" vector-space representation. (b) Use with structural mining and management algorithms: The database literature has seen an explosion of management and mining techniques for graph and XML mining. Since our distancebased representation can be naturally expressed as a graph or XML document, such techniques can also be used in conjunction with the distance graph representation. The advantages of such approaches are that they are specifically tailored to graph data, and can therefore determine novel insights in the underlying word-distances.
Both of the above methods have different advantages, and work well in different cases. The former provides ease in interoperability with existing text algorithms whereas the latter representation provides ease in interoperability with recently developed structural mining methods. In [1], we have presented details of methods and corresponding experimental results for problems such as clustering, classification, similarity search and plagiarism detection. We illustrate advantages both in terms of accuracy and the enablement of distance-based applications.
3. REFERENCES
[1] C. Aggarwal, P. Zhao. Graphical Models for Text: A New Paradigm for Text Representation and Processing, IBM Research Report, 2010.

900

Query Forwarding in Geographically Distributed Search Engines

B. Barla Cambazoglu
Yahoo! Research Barcelona, Spain
barla@yahoo-inc.com

Emre Varol
Bilkent University Computer Engineering Dept.
Ankara, Turkey
evarol@cs.bilkent.edu.tr

Enver Kayaaslan
Bilkent University Computer Engineering Dept.
Ankara, Turkey
enver@cs.bilkent.edu.tr

Cevdet Aykanat
Bilkent University Computer Engineering Dept.
Ankara, Turkey
aykanat@cs.bilkent.edu.tr

Ricardo Baeza-Yates
Yahoo! Research Barcelona, Spain
rbaeza@acm.org

ABSTRACT
Query forwarding is an important technique for preserving the result quality in distributed search engines where the index is geographically partitioned over multiple search sites. The key component in query forwarding is the thresholding algorithm by which the forwarding decisions are given. In this paper, we propose a linear-programming-based thresholding algorithm that significantly outperforms the current state-of-the-art in terms of achieved search efficiency values. Moreover, we evaluate a greedy heuristic for partial index replication and investigate the impact of result cache freshness on query forwarding performance. Finally, we present some optimizations that improve the performance further, under certain conditions. We evaluate the proposed techniques by simulations over a real-life setting, using a large query log and a document collection obtained from Yahoo!.
Categories and Subject Descriptors
H.3.3 [Information Storage Systems]: Information Retrieval Systems
General Terms
Algorithms, Design, Performance, Experimentation
Keywords
Search engines, distributed IR, query forwarding, optimization, linear programming, index replication, result caching
1. BACKGROUND
Commercial web search engines of the past relied on a single search site (data center), which processed queries issued
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

from all around the world. This approach had the typical scalability problems in centralized architectures. Moreover, queries issued from distant locations suffered from poor response times as the network latency between the user and the site became an issue. For such queries, either the query processing times had to be shortened, thus degrading the result quality, or users experienced unreasonable response times, which had implications on user satisfaction [16].
At this point, replicating the data (i.e., the web collection and the inverted index built upon it) over multiple, geographically distant search sites emerged as a feasible solution. In this strategy, each geographical region is mapped to a nearby search site. A search site processes over its full web index only the queries originating from the regions assigned to itself1. Although this strategy reduces network latencies, the scalability still remains as an issue since the entire web index had to be maintained on all search sites and queries are evaluated over the full web index.
A strategy that contrasts replication is to partition the data disjointly and assign each site only the documents obtained (crawled) from its region [8]. In this strategy, local queries of a region are evaluated over the partial index in the corresponding search site. The underlying assumption here is that users are interested more in documents located in their own region and local documents are more relevant for queries originating from the same region. As queries are now evaluated over small subsets of the web index, gains are possible in query processing time and throughput [8], along with other gains, such as those in web crawling [9].
Unfortunately, although the assumption about having high relevance between the documents and queries of the same region is reasonable, this is not true for all queries as some queries target non-regional documents [4]. This implies that evaluation over a partitioned index will lead to inferior search quality as some relevant, non-local documents are not retrieved. The problem of accessing non-local documents has two immediate solutions: taking the data to where it is sought and/or taking the queries to what they seek. The first is an offline solution that requires partial replication of the popular documents in a region on some non-local search sites. The second is an online solution that requires selective
1Herein, we refer to such queries as local queries.

90

forwarding of queries between search sites to extend coverage of search results. Our focus in this paper is on the latter approach, but we briefly touch to the former as well.
Selective query forwarding works as follows. The local search site receives a query and makes a decision about the quality of the locally computed results (relative to globally computed results, which would have been obtained through evaluation over the full index). If it is predicted that the local ranking misses some documents that would have appeared in the global ranking, a forecast is made about which search sites might have those documents. The query is then forwarded to those sites for further processing over non-local indexes and more results are retrieved. Finally, non-local and local results are merged and returned to the user.
The predictions made may lead to false positives (the query is forwarded to a site with no useful results, thus degrading performance) as well as false negatives (the query is not forwarded to a site with useful results, thus degrading the search quality). In this paper, our focus is on query forwarding techniques that preserve the search quality, i.e., those with no false negatives. This requires correctly identifying all search sites that will contribute to the global top k. In the mean time, the number of contacted sites with no useful results should be kept minimal as this has an impact on the performance and overall costs of the search engine.
A recent study [3] has proposed a thresholding technique that preserves the search quality while reducing the number of sites contacted. In this work, we build upon that work and propose a new thresholding algorithm that substantially improves the algorithm in [3] in terms of efficiency. Our algorithm has an offline phase, in which past user query logs are used to create offline queries, for which the maximum possible score attainable on each site is precomputed and globally replicated. In the online phase, this information is used in a linear programming (LP) formulation to set upper bounds on possible non-local site scores for new queries. Forwarding decisions are given based on comparisons between these bounds and the kth top score on the local site.
The following are the contributions of this paper: · We describe an LP-based thresholding algorithm that
significantly outperforms the current state-of-the-art [3]. · We evaluate a heuristic for partial index replication. · We investigate the impact of result caching and cache
freshness on query forwarding performance. · We present several optimizations that provide further
performance improvements under certain conditions. The rest of the paper is organized as follows. Section 2 presents the considered geographically distributed search engine architecture and the associated query forwarding problem. We describe the proposed thresholding algorithm in Section 3. Experimental framework is given in Section 4. Section 5 provides the performance results. Further optimizations are proposed and evaluated in Section 6. Related work is surveyed in Section 7. We conclude in Section 8.
2. QUERY FORWARDING PROBLEM
2.1 Architecture
We consider a distributed architecture with N geographically distant search sites, where each site is assigned a nearby geographical region and is responsible for crawling and indexing only the documents in its assigned region. That is, the global web index is disjointly (document-based) parti-

user

Step 6: final
result set
Step 1: user query

local site
Step 2: local
evaluation

Step 3: forwarded
query
Step 5: non-local result set

Step 3: forwarded
query

Step 5: non-local result set

non-local site
Step 4: non-local evaluation

Step 4: non-local evaluation

non-local site

non-local site
Figure 1: A geographically distributed search engine architecture with query forwarding.

tioned into N local indexes, and each local index is uniquely assigned to a search site. Also, each site is assigned the task of generating the top k results for queries issued from its own region and returning these results to its users.
A query is processed as follows (Fig. 1). The query is first issued to the local site, which evaluates the query over its partial index and computes a local top k result set. Then, a check is made to determine whether the global ranking over the full web index would bring results that are of higher quality than those in the local top k set. If it is guaranteed that the local top k set is identical to the global top k set, local results are immediately returned to the user. Otherwise, the local site identifies the non-local sites that store the documents that are missing in the local top k, but may appear in the global top k. The query is forwarded to those sites to retrieve missing results. When a non-local site receives a forwarded query, it processes the query over its own local index and generates a top k set. This result set is then transferred back to the local site, which forwarded the query. In the mean time, the local site waits for replies from all non-local sites that are contacted. Once all remote top k sets are received, they are merged2 at the local site to generate a global top k set, which is returned to the user.

2.2 Problem
The problem in selective query forwarding is to decide which search sites are likely to contain relevant results for the query and if retrieving those results will improve the results of the local site. If a query is forwarded to a non-local site and none of the returned results get into the final result set, the non-local site becomes unnecessarily burdened. Moreover, a delay is introduced in the query response time. On the other hand, if a non-local site had documents that would have appeared in the global result set but the query is not forwarded to that site, those documents are missed in the final result set and hence the search quality degrades.
The difficulty in the query forwarding problem is in correctly identifying which queries need to be forwarded and to which sites. Herein, we focus on solutions that preserve the search quality, i.e., the final result set returned to the user is guaranteed to be identical to the global top k set computed over the full web index. Hence, our objective is to mini-

2Result sets are merged according to document scores. We assume that global collection statistics are available on all sites, and scores generated by different sites are compatible.

91

mize any form of redundancy and inefficiency incurred by forwarding decisions that do not improve the search quality.

2.3 Performance Metrics

Let QL and QF be the sets of locally processed and forwarded queries, respectively. Let F q denote the set of non-
local sites that query q is forwarded to. We employ two per-
formance metrics: the fraction  of locally processed queries

 = |QL| ,

(1)

|QL| + |QF|

and the average number  of non-local sites hit per query



=

P
qQL QF

|F q| .

(2)

|QL| + |QF|

In addition to these metrics, we measure the average query

response time and the average query processing workload

(relative to query processing over the full index) of the search

engine. Since all of our optimizations are quality-preserving,

herein, we do not use a result quality metric (e.g., P@k). Let q denote a query submitted by some user uq to a local
search site S^q, and let S~i  F q, where 1  i  |F q|. Also, let I¯, I^q, and I~i denote the global index, the local index of S^q, and the local index of S~i, respectively. If a query is processed
locally, there are mainly two cost components in the query response time3. The first cost (steps 1 and 6 in Fig. 1) is

the user-to-site network latency (uq, S^q), which is incurred

while q is transferred from uq to S^q and also while the final results are transferred from S^q to uq. The second cost (step

2 in Fig. 1) is the computational cost t(q, I^q) of processing

q over I^q. The query response time then becomes

TL(q) = 2× (uq, S^q) + t(q, I^q).

(3)

If the query is forwarded, then there are two additional costs.
The first cost (steps 3 and 5 in Fig. 1) is the site-to-site network latency (S^q, S~i), which is incurred while q is transferred from S^q to S~i and also while non-local results are transferred from S~i to S^q. The second cost (step 4 in Fig. 1) is the non-local query processing cost t(q, I~i), i.e., the cost of processing q remotely on I~i. The response time becomes

TF(q) = TL(q) + max (2× (S^q, S~i) + t(q, I~i)). (4)
S~iF q

In Eq. (4), we take the maximum of all remote response

times since queries are transferred to non-local sites at the

same instant and the highest remote response time deter-

mines the waiting time of the local site. We can now com-

pute the average query response time Tavg as

Tavg

=

P
qQL

TL

(q)

+

P
qQF

|QL| + |QF|

TF(q) .

(5)

Let W (q, I) represent the workload4 incurred to the search

engine when evaluating q over an index I. For a given query

set Q, we compute the relative workload Wrel as the ratio of the total workload incurred in our architecture to the

workload incurred by evaluation over the full index, i.e.,

Wrel

=

P
qQ

(W

(q,

I^q

)

+

P
S~i F

q

P
qQ

W

(q,

I¯)

W (q, I~i)) .

(6)

3We assume that result merging as well as various other costs are negligible, as this is the case for low k and N values. 4Herein, we approximate the workload incurred by a query as the sum of inverted list lengths of all terms in the query. Interested readers may refer to [11] for other possibilities.

3. THRESHOLDING ALGORITHM

3.1 Preliminaries
We assume that queries are processed over the inverted
index in the AND mode [15], which is often the case in
practice. In this mode, only the documents that contain
all query terms are retrieved. The score of a document is
computed simply by summing the term scores, indicating the relevance of the term to the document (e.g., BM25)5.
Optionally, document-specific scores (e.g., PageRank) may
be added to the final score. The technique proposed herein
is applicable only to the former type of scoring. Extending
it to cover the latter type requires further research.
As our aim is to preserve the search quality of a central-
ized architecture, a query q should be forwarded to any non-
local site that would have at least one result in the global top k set. A non-local site S~ can contribute to this set only if the top score s(q, 1, S~) it computes for q is larger than the kth score s(q, k, S^) that local site S^ computes6. Obviously, it is not possible to know s(q, 1, S~) before evaluating q on S~. A simple but effective technique [3] for deciding whether q should be forwarded to S~ is based on computing an upper-bound m(q, S~) for s(q, 1, S~) and comparing this bound against s(q, k, S^). If m(q, S~)  s(q, k, S^) holds, it is guaranteed that S~ has no better documents than those in S^ and there is no need to forward q to S~. Otherwise, S~ may have better documents, and q has to be forwarded to S~. As the gap between m(q, S~) and s(q, 1, S~) increases, the query
is more likely to be forwarded to a site with no useful doc-
uments. Hence, the objective in this thresholding technique is to compute the m(q, S~) value as tight as possible, i.e., this bound should be as close as possible7 to s(q, 1, S~).

3.2 LP Formulation
Assume that we have the precomputed s(qi, 1, S~) value for every query qi in a set Q = {q1, . . . , qm} of m offline queries. Each qi = {ti1, . . . , tini } is composed of ni unique terms. We are given an online query q = {t1, . . . , tn} at local site S^ with a kth local score of s(q, k, S^). Given these, we formulate the problem of computing a tight m(q, S~) value as a linear programming (LP) problem as follows. We first introduce a real-valued variable xj for each term tj  q. We then find every offline query q  Q such that q is a proper subset of q, i.e., q  q. For every such q , we introduce an inequality
X xj  s(q , 1, S~), q s.t. q  Q and q  q, (7)
tj q
which always holds. We also have the set of inequalities

xj  0, tj s.t. tj  q,

(8)

which guarantee that the top scores for single-term queries (i.e., query terms) are always non-negative. After this setting, the thresholding problem reduces to finding

m(q, S~) = max X xj

(9)

tj q
subject to the linear constraints given in Eqs. 7 and 8 via

linear programming. In practice, there exist well-known,

efficient LP solvers for this and similar problems.

5Refer to Section 6.1 in [3] for more background on scoring. 6We omit superscripts on symbols for better readability. 7However, the inequality m(q, S~)  s(q, 1, S~) always holds.

92

We now illustrate the formulation by an example. Let q = {t1, t2, t3, t4} and Q = {q1, q2, . . . , q7} with q1 = {t1}, q2 = {t2}, q3 = {t3}, q4 = {t4}, q5 = {t1, t2}, q6 = {t2, t3}, and q7 = {t2, t3, t4}. Assume that precomputed top scores are s(q1, 1, S~) = 9.7, s(q2, 1, S~) = 8.1, s(q3, 1, S~) = 3.2, s(q4, 1, S~) = 4.9, s(q5, 1, S~) = 4.2, s(q6, 1, S~) = 4.7, and s(q7, 1, S~) = 5.1. These lead to the following set of inequalities
x1  9.7; x2  8.1; x3  3.2; x4  4.9;
x1 + x2  4.2; x2 + x3  4.7; x2 + x3 + x4  5.1;
x1  0; x2  0; x3  0; x4  0.
The maximum score satisfying these constraints is found as m(q, S~) = 9.3 (x1 = 4.2, x2 = 0, x3 = 0.2, x4 = 4.9).
3.3 Query Forwarding Algorithm
The forwarding algorithm contains an offline and an online phase. In the offline phase, offline queries are generated first. This can be done in different ways (see Section 4.4), e.g., synthetically by combining popular terms in the document collection or by extracting popular queries in past user query logs. Assuming such a set is available, the top scores are then computed for every query in this set over all local indexes. The computed values are replicated on all sites.
In the online phase, a query q is processed as follows. First, we evaluate q locally on S^ and record s(q, k, S^). If a query term tj  q does not appear in any of the offline queries, q is forwarded to every non-local site (case F-MissingInfo8) as it is impossible to compute any score bounds. For a nonlocal site S~, if there is a subquery q  q for which m(q, S~) = 0, q is not forwarded to S~ (case L-ZeroThreshold). Finally, for each non-local site S~ for which no decision is yet made, we separately solve the LP formulation of the previous section. If m(q, S~) > s(q, k, S^) holds, q is forwarded to S~ (case FHighLPBound); otherwise, it is not (case L-LowLPBound). We note that, in our LP formulation, it is possible to capture the F-MissingInfo case simply by introducing an equation xj   for every term tj  q such that tj  q for q  Q .
4. EXPERIMENTAL FRAMEWORK
4.1 Setup
We simulate a geographically distributed search engine architecture using two different setups. The first setup, referred to as Europe, consists of five search sites, located in Berlin (Germany), Madrid (Spain), Paris (France), Rome (Italy), and London (UK). The second setup, referred to as World, contains five relatively distant search sites, located in Canberra (Australia), Brazil (Brazil), Ottawa (Canada), Berlin (Germany), and Mexico City (Mexico). These two setups represent search architectures where the network latencies between the sites are low and high, respectively.
For both setups, we approximate the site-to-site network latency between any two sites by taking into account the speed of light on copper wire (200,000 km/s) and the birdfly distances between the cities that the sites are located. To approximate the user-to-site latency between a site and its users, we take an average over the latencies between the capital city where the site is located and the most populated five cities in the respective country. Computing latencies this way is reasonable as latencies are known to correlate well with geographical distance [12], and our data transfer costs
8We will use these labels later while discussing Fig. 10.

Measured latency (ms) Fraction of queries Fraction of queries

160

140

120

100

80

60

40

20

median latency between two computers

fitted line (y = 8.239 + 1.983x)

00

10 20 30 40 50 60 70

Predicted latency (ms)

Figure 2: Predicted versus measured network latencies.

Europe 1.0

World 1.0

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.0 1 2 3 4 5 Number of sites

0.0 1 2 3 4 5 Number of sites

Figure 3: Fraction of queries that appear on n distinct search sites.

are negligible as transferred result sets are very small. However, this approach ignores queuing delays and the fact that network connections are not necessarily on straight lines. Therefore, using several, geographically distant computers, we measured real network latencies and obtained a mapping from predicted latencies to actual values through regression (Fig. 2). All predicted values are converted to final, more accurate latency values via this mapping. Table 1 displays some statistics about the latency values used in our setups.

Table 1: Network latency statistics (in ms) Site-to-site latencies User-to-site latencies Min. Avg. Max. Min. Avg. Max.
Europe 12.0 19.6 26.9 10.6 11.5 12.6 World 43.9 109.8 172.8 9.0 16.3 23.9

4.2 Dataset
As the global document collection, we use a large crawl of the Web (about 200 million documents). This collection is obtained through various cleansing and filtering steps. Hence, it is high-quality and its documents have high potential to appear in real-life search results. Then, using a proprietary classifier9, a home country is predicted for every document, and disjoint subsets of documents are assigned to search sites (some documents are not assigned to any site). Finally, separate indexes are built on each subcollection.
For each site, we extract consecutive queries (about 19 million queries in total) from the query logs of the Yahoo! web search engine. Queries are passed through cleansing steps, such as case-folding, stop-word elimination, term uniquing, and reordering of query terms in alphabetical order. We omit queries issued by clicking on the next link and use only first page requests10. The query set of each site is separately sorted in increasing order of arrival times. The last quarter of each query set is used in the online phase. The rest are used in the offline phase. In our sample log, most queries are regional and occur in one site (Fig. 3).
4.3 Simulations
We compute thresholds and local top k results using a modified version of Terrier. In simulations, we assume that each search cluster node builds an index on three million documents. The total number of processors available to the overall search system is determined accordingly. Each site is assigned a number of processors proportional to its index size. Therefore, query processing times are comparable
9This is a production-level classifier that uses features such as language, IP, and domain to identify documents' regions. 10Next page requests may be handled by prefetching of result pages [14]. This is beyond the scope of this paper.

93

for search sites11. Query evaluation is simulated via a detailed simulator, which computes a separate response time for each query, using Eqs. (3) and (4). In query processing, we assume a processing cost of 200ns per posting. This is an average value obtained from Terrier, but we observed it to correlate well with real search engine timings. We also assume a 20ms overhead for preprocessing, per query.
4.4 Offline Query Generation
Our LP-based solution is applicable to online queries of arbitrary length and is especially suitable for long queries. However, our query set is composed of web queries, which are very short in nature. Using long queries in the offline query set does not bring much additional performance benefit12. Therefore, in our offline query set, we consider only singleand two-term queries13. This approach also reduces the storage requirement for the precomputed scores and their offline computation cost. Moreover, if we assume that an offline query can be accessed in O(1)-time using a hash table, the computational cost of accessing offline queries that are proper subsets of the online query becomes much lower as this can be done in O(|q|2)-time instead of O(2|q|)-time.
In this work, we generate two offline query sets, referred to as D1 and D2. D1 contains all terms (i.e., queries of length one) in the vocabulary of the collection. D2 contains all possible pairs of vocabulary terms (i.e., queries of length two). Obviously, the latter may not be feasible in a practical setting, but we still prefer to experiment with this set to observe the degree of benefit that our thresholding algorithm may provide. We also generate two more query sets, Q1 and Q2, using the query log. Q1 contains, as an offline query, all the terms in the vocabulary of the query log. Q2 contains subqueries of length two in each individual query in the log (but, not across the entire vocabulary as we do for D2).
For performance evaluation, we use selected combinations (unions) of the above-mentioned sets: Q1, D1, Q1-Q2, D1-Q2, and D1-D2. We omit combinations Q2, D2, Q1-D1, and Q1-D2 as they are less meaningful or useful. In our experiments, we use the D1 set as our baseline as this is identical to the technique discussed in [3] (see the discussion in Section 7). To measure the best possible performance, we also consider an Oracle algorithm that has no false positives, i.e., it forwards a query to only the non-local sites with positive contribution to the final result set. Due to space limitations, occasionally, we display the results for only a single setup (often, Europe).
5. PERFORMANCE
5.1 Effect of Offline Query Set
Fig. 4 shows the fraction of locally processed queries for different offline query sets, as k varies. It is interesting to observe that the Q1-Q2 set outperforms the baseline (D1) although it has fewer queries. When the baseline is combined with the term pairs extracted from the query log (i.e., D1-Q2), for k = 10, about 9.1% more queries are processed locally (10.2% for the World setup). The impractical D1D2 set performs quite close to the Oracle algorithm, which processes about 40% of the queries locally, for k = 10.
11We assume that indexes are entirely kept in main memory. 12A similar issue is mentioned before in the context of caching intersections of posting lists [15]. 13We implemented an LP solver, tailored to our purposes.

Fraction of locally processed queries

Europe 1.0

0.9

Oracle D1-D2

D1-Q2

0.8

Q1-Q2

D1

0.7

Q1

0.6

0.5

0.4

0.3

0.2

0.1

0.01 2 3 4 5 6 7 8 9 10 Number of results requested (k)

Average query response time (ms)

450 420 390 360 330 300 270 240 210 180 150 1201

World & Europe
World: Q1 World: D1 World: Q1-Q2 World: D1-Q2 World: D1-D2 World: Oracle Europe: Q1 Europe: D1 Europe: Q1-Q2 Europe: D1-Q2 Europe: D1-D2 Europe: Oracle
2 3 4 5 6 7 8 9 10 Number of results requested (k)

Figure 4: Fraction of lo- Figure 5: Average query cally processed queries. response times.

Fraction of queries

1.0

0.9

Q1 D1

Q1-Q2

0.8

D1-Q2

D1-D2

0.7

Oracle

Europe

0.6

0.5

0.4

0.3

0.2

0.1

0.0

< 100

< 200

< 400

< 800

Query response time (ms)

Fraction of queries

1.0

0.9

Q1 D1

Q1-Q2

0.8

D1-Q2

D1-D2

0.7

Oracle

World

0.6

0.5

0.4

0.3

0.2

0.1

0.0

< 100

< 200

< 400

< 800

Query response time (ms)

Figure 6: Fraction of queries that are answered under a certain response time (for k = 10).

The increase in the fraction of locally processed queries leads to a reduction in average query response times (Fig. 5). However, the distribution of response times is also important, i.e., we should check what fraction of queries can be processed under a given threshold time. It is empirically shown that after a certain response time threshold, users become frustrated and URL click-through rates go down, leading to financial losses for the search engine [16]. It is also shown that, given additional time for query processing, it is possible to improve the quality of search results [8]. In our simulations, we observe that the response time is a more critical issue for the World setup than Europe (Fig. 6). For Europe, only less than 10% of the queries cannot be answered under 400ms, whereas this rate varies in the 40%­55% range for the World setup, depending on the offline query set used.
According to Fig. 7, our technique achieves considerable reduction in the average number of non-local sites contacted. Additionally, in Fig. 8, we show the average number of sites that are active in processing a query. The second excludes any site that does not process the query on its local index. This may happen due to absence of a query term in the site's index, which explains the overlap of curves in Fig. 8 for Q1 and D1 as well as Q1-Q2 and D1-Q2. Fig. 9 shows the average relative workload values as computed by Eq. (6). We observe that, for k = 10, there is about 16% reduction in the workload when queries are evaluated over our architecture (assuming D1-Q2) relative to query evaluation over the full index (this goes up to 20% for the World setup).
Fig. 10 shows the average outcome of a forwarding decision for a (query, site) pair (recall the discussion in Section 3.3). In the figure, we observe the following: Since Q1 and Q1-Q2 sets miss some terms in the collection vocabulary, about 10% of test queries had to be forwarded to all nonlocal sites (case F-MissingInfo). Most of the improvement

94

Europe 4.0
3.5
3.0
2.5

5.0

Q1

4.5

D1 Q1-Q2

D1-Q2

4.0

D1-D2 Oracle

3.5

Europe

Number of active sites

Number of non-local sites

2.0

3.0

1.5

2.5

1.0

Q1

D1

Q1-Q2

0.5

D1-Q2

D1-D2

Oracle

0.01 2 3 4 5 6 7 8 9 10

Number of results requested (k)

2.0
1.5
1.01 2 3 4 5 6 7 8 9 10 Number of results requested (k)

Figure 7: Average num- Figure 8: Average number of non-local sites a ber of active sites (search query is forwarded to. backends) per query.

Fraction of processed index

Europe 1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

Q1

D1

0.2

Q1-Q2

D1-Q2

0.1

D1-D2 Oracle

0.01 2 3 4 5 6 7 8 9 10 Number of results requested (k)

Fraction of forwarding decisions

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Q1

Europe
F-MissingInfo F-HighLPBound L-LowLPBound L-ZeroThreshold L-LocalSite
D1 Q1-Q2 D1-Q2 D1-D2 Offline query set

Figure 9: Fraction of the Figure 10: Dissection of

evaluated index relative forwarding decisions into

to the global index.

various possible cases.

over the baseline is due to the proposed LP solution (cases F-HighLPBound and L-LowLPBound), which uses term pairs (e.g., see D1 versus D1-Q2). The fraction of L-ZeroThreshold cases correlates with the size of the offline query set used.
5.2 Effect of Partial Index Replication
Replicating globally popular documents on all sites leads to a high reduction in the forwarded query count [3]. The algorithm in [3] (herein, we call it R-freq) sorts the documents globally in decreasing number of occurrences in the top 200 results of training queries. A certain fraction of the most frequent documents are then replicated and indexed on all sites. This type of replication has two benefits for thresholding algorithms: local kth scores get higher as local sites have more documents, and thresholds computed for offline queries on non-local sites get lower as they are now computed over fewer documents. Both imply less forwarding.
A possible improvement over R-freq is to incorporate the storage cost incurred on the index due to replication of the document14. This is a greedy algorithm (R-cost) that tries to optimize per-byte benefit at any step by prioritizing documents according to the ratio of their occurrence frequencies and storage costs. In related experiments, we compute the occurrence frequencies using the top 10 results of training queries and then replicate on all sites 0.5% of documents with the highest benefit. According to Figs. 11 and 12, surprisingly, the improvement achieved by R-cost over R-freq is minor (0.3%­0.9% increase in the rate of locally processed queries and 0.9%­2.8% decrease in the number of non-local sites contacted per query). This is mainly because R-cost
14We estimate this cost by document's unique term count.

Number of non-local sites

Fraction of locally processed queries

Europe 1.0

no replication

0.9

frequency-based replication

cost-based replication 0.8

0.7

0.6

0.5

Europe 4.0

no replication

3.5

frequency-based replication

cost-based replication

3.0

2.5

2.0

0.4

1.5

0.3 1.0
0.2

0.1

0.5

0.0

Q1

D1 Q1-Q2 D1-Q2 D1-D2 Oracle

Offline query set

0.0

Q1

D1 Q1-Q2 D1-Q2 D1-D2 Oracle

Offline query set

Figure 11: Impact of Figure 12: Impact of

replication on locally pro- replication on non-local

cessed query rate.

sites per query.

Fraction of locally processed queries

Europe 1.0

w/o result cache

0.9

w/ result cache

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Q1

D1 Q1-Q2 D1-Q2 D1-D2 Oracle

Offline query set

Fraction of locally processed queries

Europe 1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.00 15 30 60

120

TTL (minutes)

Oracle D1-D2 D1-Q2 Q1-Q2 D1 Q1
240

Figure 13: Impact of re- Figure 14: Impact of sult caching on locally TTL on locally processed processed query rate. query rate.

fills the given replication budget with small documents that have low past occurrence frequencies. Although past occurrences correlate well with future occurrences at high frequency values, there is little correlation at low occurrence frequencies15. This limits the performance of R-cost.
5.3 Effect of Result Caching and TTL
Search engines cache the results of frequent and/or recent queries to reduce the query workload on backend clusters and improve query response times [2]. Queries that result in a hit in the cache are served by the cache. In our context, result caching has a significant impact on the number of forwarded queries. With result caching, the fraction of queries that can be locally processed increases by 35%­45%, depending on the offline query set used (Fig. 13). We also observe that more informative query sets receive a lower benefit. This is because, under result caching, only the queries that are seen for the first time (i.e., compulsory cache misses) are subject to forwarding. Most cache misses are tail queries, which are long. As we will see in Section 6.3, long queries are much less likely to be forwarded, and hence having more information in bound computations becomes less important.
The above discussion holds for search engines with indexes that are periodically rebuilt (e.g., once a week). If, however, there are incremental updates on the index, result cache entries may become stale. In practice, a quick solution is to associate a fixed time-to-live (TTL) value t with every cache entry [7]. A cache entry that is not refreshed for at
15An interesting discussion and possible solutions are available in the context of result caching [11]. But, application of those techniques are beyond the scope of our paper.

95

Average non-local top k Fraction of queries Fraction of queries

Europe 10

9

No optimization Q1

D1

8

Q1-Q2

D1-Q2

7

D1-D2

6

5

4

3

2

11 2 3 4 5 6 7 8 9 10 Number of results requested (k)

Europe 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1 2 3 4 5 6 7 8 9 10
Rank (r)

World 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1 2 3 4 5 6 7 8 9 10
Rank (r)

Figure 15: Number of Figure 16: Fraction of

local versus non-local queries where all results up

results requested.

to the rth rank are local.

Table 2: Fraction of queries according to length

Query length

Setup

1

2

3

4

>4

Europe 0.330 0.365 0.194 0.074 0.038

World 0.288 0.369 0.211 0.085 0.046

least t units of time is said to be expired, and any request for the entry is treated as a cache miss. In our context, such requests are subject to forwarding. According to Fig. 14, the performance saturates very quickly with increasing TTL16, due to the power-law distribution of query frequencies [2].

6. FURTHER OPTIMIZATIONS
We now describe various techniques that improve performance under certain conditions. Some of the optimizations in distributed IR are also applicable to our setting, e.g., non-local computations can be early terminated simply by transferring s(q, k, S^) values together with the query. However, here, we skip such techniques and focus on those that are more meaningful in a geographically distributed setting.
6.1 Non-local Top k Optimization
If a query is forwarded to a non-local site, the top k results are requested. However, we note that it suffices to request k - r results, where r is the lowest rank such that m(q, S~) < s(q, r, S^) holds. Hence, for some queries, it is possible to request fewer documents and reduce the number of remotely computed snippets, in addition to other savings in score computations. For k = 10, the saving is in the 5.3%­ 16.7% range, depending on the offline query set (Fig. 15).
6.2 Early Result Presentation
Search results are typically displayed in pages, containing 10 results. An interesting optimization is to show the user the local site's search results without waiting for replies of non-local sites. If it later turns out that s(q, k, S^)  s(q, 1, S~i) for every non-local site S~i, the query becomes served at the speed of a local query. Otherwise, non-local results are merged as a background job and the user's screen is refreshed with the correct results17. In our case, for about a quarter of queries, all top 10 results come from the local site. For the top result, this is so for more than half of queries (Fig. 16).
16We use small TTL values that are suitable to our sample query set. In practice, the TTL values are around a day [7]. 17Some vertical search sites use similar optimizations (e.g., http://www.kayak.com). The impact of this kind of result presentation on user satisfaction is open to investigation.

Table 3: Average saving (in ms) in query response saving by early query forwarding
Offline query set Setup Q1 D1 Q1-Q2 D1-Q2 D1-D2 Europe 12.1 13.4 12.1 13.4 13.4 World 15.7 18.1 15.7 18.1 18.1

Table 4: Query response time (in ms) as the number

of non-local sites a query is forwarded to varies

Number of non-local sites

Setup

0

1

2

3

4

Europe 87.3 152.0 157.1 161.5 190.2

World 121.4 376.6 435.1 484.5 524.1

6.3 Early Query Forwarding

We

note

that

if

m(q, S~)

>

P
tq

s(t,

(k - 1)/|q|

+ 1, S^)

holds18, q can be immediately forwarded to S~ without wait-

ing for completion of the local evaluation, which determines s(q, k, S^). This way, it becomes possible to overlap local

query evaluation with network transfer. This approach requires precomputing and storing s(t, (k - 1)/|q| + 1, S^)

values for all terms in the collection vocabulary. Since the

stored value depends on query length, covering all query lengths (assuming k is fixed to 10) requires storing all s(t, r, S^)

values for r  {1, 2, 3, 4, 5, k}. If long queries (according to

Table 2, less than 15% of queries have more than 3 terms) are

ignored, it suffices to store the scores only for r  {4, 5, k}.

Herein, we only store s(t, k, S^) values and observe the impact

on queries with a single term (about one-fifth of forwarded

queries have one term, as seen in Fig. 17). For affected

queries, the response time saving is up to 18ms (Table 3).

Fraction of forwarded queries Fraction of queries

World 0.40

World 0.7

Q1

Q1

0.35

D1 Q1-Q2

0.6

D1 Q1-Q2

D1-Q2

D1-Q2

0.30

D1-D2

D1-D2

0.5

0.25 0.4
0.20
0.3 0.15

0.2 0.10

0.05

0.1

0.00

1

2

3

> 3

Query length

0.0

0

1

2

3

4

Number of non-local sites

Figure 17: Fraction of Figure 18: Fraction of

forwarded queries as queries forwarded to n

query length varies.

non-local sites.

6.4 Remote Result Preparation
If the query is forwarded to only a single non-local site, the final results can be created and returned to the user by the non-local site as there are only two sets to be merged. This approach requires transferring the local top k result set together with the query, but may reduce the overall network latency in returning results to the user. Table 4 shows that there is a correlation between the average query response time and the number of non-local sites a query is forwarded to. The gap between the response time of local and forwarded queries is clearly seen. Our optimization, however, is applicable to a limited set of queries. According
18The right-hand side is an upper bound on s(q, k, S^).

96

Table 5: Average saving (in ms) in query response time by remote result preparation
Offline query set Setup Q1 D1 Q1-Q2 D1-Q2 D1-D2 Europe 10.8 10.8 10.4 10.4 10.4 World 16.6 16.6 16.2 16.2 16.2
to Fig. 18, about only 10% of our queries are forwarded to a single non-local site. For such queries, the saving in query response time is between 10ms and 16ms (Table 5).
7. RELATED WORK
Although there is much research on distributed IR [1], little research is done on multi-site, distributed search engines [3, 8]. Cambazoglu et al. present cost models and results, showing the potential of multi-site architectures for efficiency and relevance improvements [8]. Baeza-Yates et al. develop analytical models to compare operational costs of multi-site search systems against centralized systems [3].
The work in [3] is the closest to ours in that it also proposes an algorithm that tries to increase the number of locally processed queries by a thresholding technique, based on precomputation of maximum score contributions for all terms in the global vocabulary and replicating this information on all search sites. This way, it becomes possible to locally compute the maximum score a document can get on a non-local site, simply summing the maximum possible scores for query terms without evaluating the query remotely at all. It turns out that this technique is a limited case of our general solution (the same as using D1 in our setting).
We note that the query forwarding problem we deal with is somewhat different than the collection selection problem in federated IR [6]. In our system, queries are evaluated over a local index and some of them are forwarded. In federated IR, all queries are forwarded without any evaluation on a local index. We further note that P2P search systems [17] are also very different due to existence of a very high number of peers, their volatile nature, and limited availability. BaezaYates et al. [5] describe an architecture in which the global index is split into two tiers. In this architecture, queries are evaluated on one or two tiers, based on the decision of a machine-learned corpus predictor. In that architecture, documents are split into tiers by their importance or location.
Finally, Das et al. employ an LP-based solution in the context of databases for top k computation using materialized views [10]. Kumar et al. apply a similar technique to generalize top k thresholding algorithms by using precomputed intersections of posting lists [13]. To our knowledge, there is no work applying a similar technique in our context.
8. CONCLUSIONS
We showed that the fraction of locally processed queries in a multi-site search engine can be significantly increased by using an LP-based thresholding technique, and results are further improved by caching and replication. There are three research directions surfaced by our work. First, the applicability of our techniques to other problems (e.g., tiering [5]) should be investigated. Second, a trade-off analysis is needed between forwarding performance and offline query generation and storage overheads. Finally, the freshness of precomputed score thresholds needs further research.

9. ACKNOWLEDGEMENT
This publication is based on work performed in the framework of the Project COAST-ICT-248036, funded by the European Community, and partially supported by the Scientific and Technological Research Council of Turkey under grant EEEAG-109E019 and COST Action IC080 ComplexHPC.
10. REFERENCES
[1] R. Baeza-Yates, C. Castillo, F. Junqueira, V. Plachouras, and F. Silvestri. Challenges on distributed web retrieval. In 23rd Int'l Conf. on Data Engineering, pages 6­20, 2007.
[2] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, V. Plachouras, and F. Silvestri. The impact of caching on search engines. In Proc. 30th Int'l ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 183­190, 2007.
[3] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Plachouras, and L. Telloli. On the feasibility of multi-site web search engines. In Proc. 18th ACM Conf. on Information and Knowledge Management, pages 425­434, 2009 (best paper).
[4] R. Baeza-Yates, C. Middleton, and C. Castillo. The geographical life of search. In Proc. 2009 IEEE/WIC/ACM Int'l Joint Conf. on Web Intelligence and Intelligent Agent Technology, pages 252­259, 2009.
[5] R. Baeza-Yates, V. Murdock, and C. Hauff. Efficiency trade-offs in two-tier web search systems. In Proc. 32nd Int'l ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 163­170, 2009.
[6] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In Proc. 18th Int'l ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 21­28, 1995.
[7] B. B. Cambazoglu, F. P. Junqueira, V. Plachouras, S. Banachowski, B. Cui, S. Lim, and B. Bridge. A refreshing perspective of search engine caching. In 19th Int'l Conf. on World Wide Web, 2010 (accepted).
[8] B. B. Cambazoglu, V. Plachouras, and R. Baeza-Yates. Quantifying performance and quality gains in distributed web search engines. In Proc. 32nd Int'l ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 411­418, 2009.
[9] B. B. Cambazoglu, V. Plachouras, F. Junqueira, and L. Telloli. On the feasibility of geographically distributed web crawling. In Proc. 3rd Int'l Conf. on Scalable Information Systems, 2008.
[10] G. Das, D. Gunopulos, N. Koudas, and D. Tsirogiannis. Answering top-k queries using views. In Proc. 32nd Int'l Conf. on Very Large Data Bases, pages 451­462, 2006.
[11] Q. Gan and T. Suel. Improved techniques for result caching in web search engines. In Proc. 18th Int'l Conf. on World Wide Web, pages 431­440, 2009.
[12] B. Huffaker, M. Fomenkov, D. J. Plummer, D. Moore, and K. Claffy. Distance metrics in the internet. In Proc. Int'l Telecommunications Symposium, 2002.
[13] R. Kumar, K. Punera, T. Suel, and S. Vassilvitskii. Top-k aggregation using intersections of ranked inputs. In Proc. 2nd ACM Int'l Conf. on Web Search and Data Mining, pages 222­231, 2009.
[14] R. Lempel and S. Moran. Predictive caching and prefetching of query results in search engines. In Proc. 12th Int'l Conf. on World Wide Web, pages 19­28, 2003.
[15] X. Long and T. Suel. Three-level caching for efficient query processing in large web search engines. In Proc. 14th Int'l Conf. on World Wide Web, pages 257­266, 2005.
[16] E. Schurman and J. Brutlag. Performance related changes and their user impact. In Velocity: Web Performance and Operations Conf., 2009.
[17] C. Tang, Z. Xu, and M. Mahalingam. Peersearch: Efficient information retrieval in peer-to-peer networks. In Proc. of HotNets-I, ACM SIGCOMM, 2002.

97

A Survival Modeling Approach to Biomedical Search Result Diversification

Xiaoshi Yin1,2, Jimmy Xiangji Huang 2, Xiaofeng Zhou 2, Zhoujun Li 1
1School of Computer Science and Technology, Beihang University, Beijing, China. 2School of Information Technology, York University, Toronto, Canada.
xiaoshiyin@cse.buaa.edu.cn; jhuang@yorku.ca; lizj@buaa.edu.cn

ABSTRACT
In this paper, we propose a probabilistic survival model derived from the survival analysis theory for measuring aspect novelty. The retrieved documents' query-relevance and novelty are combined at the aspect level for re-ranking. Experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval. Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage & Retrieval]: Information Search & Retrieval General Terms: Performance, Experimentation
Keywords: Survival Modeling, Diversity, Biomedical IR
1. INTRODUCTION
In the biomedical domain, the desired information of a question (query) asked by biologists usually is a list of a certain type of entities covering different aspects that are related to the question [6], such as genes, proteins, diseases, mutations, etc. Hence it is important for a biomedical information retrieval (IR) system to provide comprehensive and diverse answers to fulfill biologists' information needs. In the TREC 2006 and 2007 Genomics tracks, the "aspect retrieval" was investigated. Its purpose was to study how a biomedical IR system can support a user gather information about the different aspects of a topic. Aspects of a retrieved passage could be a list of named entities or MeSH terms, representing answers that cover different portions of a full answer to the query. Aspect Mean Average Precision (Aspect MAP) was defined in the Genomics tracks to capture similarities and differences among retrieved passages. It is a measurement for diversity of the IR ranked list [5].
The Genomics aspect retrieval was firstly proposed in the TREC 2006 Genomics track and further investigated in the 2007 Genomics track. However, to the best of our knowledge, there is not too much previous work conducted on the Genomics aspect retrieval for promoting diversity in the ranked list. University of Wisconsin re-ranked the retrieved passages using a clustering-based approach named GRASSHOPPER to promote ranking diversity [4]. Unfortunately, for the Genomics aspect retrieval, this re-ranking method hurt their system's performance and decreased the Aspect MAP of the original results [4]. Later in the TREC 2007 Genomics track, most teams tried to obtain the aspect
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

level performance through their passage level results, instead of working on the aspect level retrieval directly [3, 6].
In this paper, we first propose a survival modeling approach to measuring the novel information provided by an aspect with respect to its occurrences. Then, the relevance and novelty of a retrieved document are combined at the aspect level. Evaluation results show that the proposed approach is effective in biomedical search result diversification.
2. SURVIVAL MODELING AND ANALYSIS FOR MEASURING NOVELTY
Survival analysis is a statistical methodology used for modeling and evaluating survival data, also called time-to-event data, where one is interested in the occurrence of events [2]. Survival time refers to a variable which measures the time from a particular starting time to a particular endpoint of interest. Events are usually referred as birth, death and failure that happen to an individual in the context of study. For example, in clinical trial, one may interested in the number of days that patient can survive in the study of the effectiveness of a new treatment for a disease. Formally, the survival function is defined as:
S(t) = P r(surviving longer than time t) (1)
= P r(T > t)
where t is a specific time, T is a random variable denoting the time of death, and "Pr" stands for probability. That is, the survival function gives the probability that the time of death is later than a specified time t. The survival function must be non-increasing: S(u)  S(t) if u > t. Usually one assumes S(0) = 1, that is, at the start of the study, the probability of surviving past time zero is one. The survival function is also assumed to approach zero as t increases without bound [2].
In the context of information retrieval, aspects covered by a document can be considered as treatments, a document can be considered as a patient in the clinical trial case. The number of times that an aspect has been observed can be considered as the survival time. The new information that can be provided by an aspect corresponds to the effectiveness of a treatment. One can expect that, in a ranked list, as the number of times that an aspect has been observed increases, the new information that this aspect can provide to a document decreases. For example, in a ranked document list, when aspect "stroke treatment" is observed in the jth document at the first time, the information provided by this aspect should be counted as completely new. We presume that "stroke treatment" in the jth document covers the

901

topic of "medications taken by mouth for long-term stroke

Aspect mean average precision (MAP), since our objective

treatment". Then, when aspect "stroke treatment" is ob-

is to promote diversity in the IR ranked list.

served again in the kth (k > j) document of the ranked list,

Evaluation results of using the proposed approach for doc-

it may provide new information about "injection for short-

ument re-ranking are shown in Table 1. The values in the

term stroke treatment", but it is also possible that it only

parentheses are the relative rates of improvement over the

provides redundant information about "medications taken

original results. As we can see, our approach achieves promis-

by mouth for long-term stroke treatment". As we can see,

ing and consistent performance improvements over all base-

this situation satisfies the properties of the survival function

line runs. Performance improvements can be observed on

described above.

both levels of evaluation measures. It is worth mention-

We assume that the occurrences of an aspect follow Pois-

ing that our approach can further improve the best result

son distribution. Then, the survival model derived from

(NLMinter) reported in the TREC 2007 Genomics track by

Equation 1 can be formally written as:

x
Saj (x) = P r(X > x) = 1 - e-
i=0

i i!

(2)

where  is the rate parameter of Poisson distribution. The

achieving 18.9% improvement on Aspect MAP and 11% improvement on Passage2 MAP.

on 2007's topics

MAP

Aspect Passage2

on 2006's topics MAP Aspect Passage2

value of Saj (x) states the probability of obtaining new information from aspect aj(j = 1, 2, ..., n; where n is the number
of observed aspects) after it has been observed x times. Note

NLMinter 0.2631 SvvModel 0.3117
(+18.5%) MuMshFd 0.2068

0.1148 0.1270 (+10.6%) 0.0895

Okapi06a 0.2176 SvvModel 0.2379
(+9.3%) Okapi06b 0.3147

0.0450 0.0472 (+4.9%) 0.0968

that in this paper, the aspects covered by a retrieved document are presented by concepts detected from Wikipedia [9].

SvvModel 0.2432 (+17.6%)
Okapi07 0.1428

0.0926 SvvModel 0.3236

(+3.5%)

(+2.8%)

0.0641 Okapi06c 0.2596

0.1009 (+4.2%)
0.0601

SvvModel 0.1660

0.0669 SvvModel 0.2697

0.0624

3. COMBINING NOVELTY AND RELEVANCE

(+16.2%) (+4.4%)

(+3.9%) (+3.8%)

For retrieved documents, the document rankings should depend on which documents the user has already seen. Suppose that we have ranked top i - 1 documents, and now we need to decide which document should be ranked at the ith position in the ranking list. The document that can deliver the most new and relevant aspects should be considered as the ith document in the ranking list. Assume that aspect novelty and aspect query-relevance are independent of each other. Then given previous ranked i - 1 documents, we rank the ith document using the following scoring function:

score(di; d1, ..., di-1) = P (N ew and Rel|di)

=

P (N ew and Rel|aj )P (aj )

aj Adi



P (N ew|aj )P (aj |Rel)

(3)

aj Adi

where aj is an aspect detected from document di, which follows Poisson distribution with an estimated rate parameter. P (N ew and Rel|aj) denotes the probability that aj is query-relevant and can provide new information as well.
P (N ew|aj) in Equation 3 states the probability of obtaining new information from aspect aj, which can be calculated using the survival models proposed in Section 2. P (N ew|aj) = 1 when i = 1. Since we do not usually have relevance information, P (aj|Rel) is unavailable. One possible solution, as introduced in [7], is to consider that
the best bet is to relate the probability of aspect aj to the conditional probability of observing aj given the query: P (aj|Rel)  P (aj|Q). P (aj|Q) can be calculated by the two-stage model presented in [9].

4. EXPERIMENTAL RESULTS
We conduct a series of experiments to evaluate the effectiveness of the proposed model on the TREC 2006 and 2007 Genomics collections. For the 2007's topics, three baseline runs are used, which are NLMinter [3], MuMshFd [8] and an Okapi run [1]. NLMinter and MuMshFd were two of the most competitive IR runs submitted to the TREC 2007 Genomics track. For 2006's topics, we test our approach on three Okapi runs. In this paper, we mainly focus on the

Table 1: Re-ranking Performance on Genomics topics
5. CONCLUSIONS
In this paper, we propose a survival modeling approach to promoting ranking diversity for biomedical information retrieval. The probabilistic survival model derived from the survival analysis theory measures the probability that novel information can be provided by an aspect with respect to its occurrences. Experimental results show that the proposed survival model can successfully capture the novel information delivered by aspects and achieve significant improvements on ranking diversity. We also show that combining the novelty and the relevance of a retrieved document at the aspect level is an effective way of promoting diversity of the ranked list, while keeping the relevance of retrieved documents. The proposed approach not only achieves promising performance improvements on the diversity based evaluation measure, but also on the relevance based evaluation measure.
Acknowledgements
This work is supported by NSERC Discovery Grant and an Early Researher Award of Ontario.
6. REFERENCES
[1] M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. William. Okapi at TREC-5. In Proc. of TREC-5, 1997.
[2] D. Cox and D. Oakes. Analysis of Survival Data. Chapman Hall. [3] D. Demner-Fushman and et. al. Combining resources to find
answers to biomedical questions. In Proc. of TREC-16, 2007. [4] A. B. Goldberg and et. al. Ranking biomedical passages for
relevance and diversity: University of Wisconsin, Madison at TREC Genomics 2006. In Proc. of TREC-15, 2006. [5] W. Hersh, A. Cohen, P. Roberts, and H. Rekapalli. TREC 2006 Genomics track overview. In Proc. of TREC-15, 2006. [6] W. Hersh, A. Cohen, L. Ruslen, and P. Roberts. TREC 2007 Genomics track overview. In Proc. of TREC-16, 2007. [7] V. Lavrenko and W. B. Croft. Relevance based language models. In Proceeding of the 24th ACM SIGIR. [8] N. Stokes and et. al. Entity-based relevance feedback for genomic list answer retrieval. In Proc. of TREC-16, 2007. [9] X. Yin, X. Huang, and Z. Li. Promoting ranking diversity for biomedical information retrieval using wikipedia. In Proc. of the 32nd ECIR, 2010.

902

Introduction to Probabilistic Models in IR
Victor P. Lavrenko
University of Edinburgh, School of Informatics, Edinburgh, U.K., v.lavrenko@gmail.com

Abstract:
Most of today's state-of-the-art retrieval models, including BM25 and language modeling, are grounded in probabilistic principles. Having a working understanding of these principles can help researchers understand existing retrieval models better and also provide industrial practitioners with an understanding of how such models can be applied to real world problems.
This half-day tutorial will cover the fundamentals of two dominant probabilistic frameworks for Information Retrieval: the classical probabilistic model and the language modeling approach. The elements of the classical framework will include the probability ranking principle, the binary independence model, the 2-Poisson model, and the widely used BM25 model. Within language modeling framework, we will discuss various distributional assumptions and smoothing techniques. Special attention will be devoted to the event spaces and independence assumptions underlying each approach. The tutorial will outline several techniques for modeling term dependence and addressing vocabulary mismatch. We will also survey applications of probabilistic models in the domains of cross-language and multimedia retrieval. The tutorial will conclude by suggesting a set of open problems in probabilistic models of IR.
Attendees should have a basic familiarity with probability and statistics. A brief refresher of basic concepts, including random variables, event spaces, conditional probabilities, and independence will be given at the beginning of the tutorial. In addition to slides, some hands on exercises and examples will be used throughout the tutorial.

ACM Categories & Descriptors:
H.3.3 Information Search and Retrieval; Retrieval Models
General Terms:
Algorithms, Experimentation, Theory
Keywords:
Probability Ranking Principle, Language Modeling, Estimation, Term Dependence, Cross-language Retrieval
Bios
Victor Lavrenko is a Lecturer in Informatics at the University of Edinburgh. He received his Ph.D. in Computer Science from the University of Massachusetts Amherst in 2004, and worked as a language technology consultant for the Credit Suisse Group prior to his appointment at Edinburgh. Victor presented tutorials on language modeling and probabilistic approaches to IR at SIGIR 2003 and SIGIR 2009. He has published research papers in and has reviewed for the SIGIR, CIKM, NAACL/HLT, KDD and NIPS conferences. Victor's research interests include formal models for searching text in multiple languages, annotating and retrieving images, and detecting and tracking novel events in the news. More information: http://homepages.inf.ed.ac.uk/vlavrenk

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
905

Multimedia Information Retrieval

Stefan Rüger
Knowledge Media Institute The Open University
Milton Keynes MK7 6AA, UK s.rueger@open.ac.uk

Abstract:
This tutorial is concerned with creating the best possible multimedia search experience. The intriguing bit here is that the query itself can be a multimedia excerpt: For example, when you walk around in an unknown place and stumble across an interesting landmark, would it not be great if you could just take a picture with your mobile phone and send it to a service that finds a similar picture in a database and tells you more about the building ­ and about its significance for that matter?
The ideas for this type of search have been around for a decade, but this tutorial will look at recent successes and take stock of the state-of-the-art. It examines the full matrix of a variety of query modes versus document types. How do you retrieve a music piece by humming? What if you want to find news video clips on forest fires using a still image? The tutorial discusses underlying techniques and common approaches to facilitate multimedia search engines: metadata driven search; piggy-back text search where automated processes create text surrogates for multimedia; automated image annotation; content-based search. The latter is studied in more depth looking at features and distances, and how to effectively combine them for efficient retrieval, to a point where the participants have the ingredients and recipe in their hands for building their own visual search engines.
Supporting users in their resource discovery mission when hunting for multimedia material is not a technological indexing problem alone. We will briefly look at interactive ways of engaging with repositories through browsing and relevance feedback, roping in geographical context, and providing visual summaries for videos. The tutorial emphasises state-of-the-art research in the area of multimedia information retrieval, which gives an indication of the research and development trends and, thereby, a glimpse of the future world.
ACM Categories & Descriptors:
H.3.3 [Information Search and Retrieval]: Retrieval models; Search process. H.3.1 [Content Analysis and Indexing]: Indexing methods
General Terms:
Algorithms, Design, Experimentation
Keywords:
Multimedia information retrieval, automated image annotation, browsing and relevance feedback, content-based search, metadata driven search, piggy-back text search
Short Biography:
Stefan Rüger joined The Open University's Knowledge Media Institute in 2006 to take up a chair in Knowledge Media. Before
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

that he was a Reader in Multimedia and Information Systems at the Department of Computing, Imperial College London, where he also held an EPSRC Advanced Research Fellowship (1999­ 2004). Stefan is a theoretical physicist by training (FU Berlin) and received his PhD in Computing in 1996 from TU Berlin for his work on artificial intelligence and, in particular, the theory of neural networks. Since then he has made a continuous journey from theory to its applications in multimedia retrieval. In 2009 he was awarded a Honorary Professorship (until 2014) at the University of Waikato for his work with the Greenstone Digital Library group. During his academic career he and his team have authored over 100 scientific publications in the area of multimedia information retrieval.
Community Service:
Stefan chairs the ­ originally EPSRC funded ­ research network on Multimedia Knowledge Management that comprises eight UK universities. As general (co-)chair he organised ECIR 2006, ECIR 2010 and ICTIR 2009. He was the programme chair of IRFC 2010 and is currently programme co-chair of WI 2010 and SAMT 2010. He was associate editor for MVA for 5 years, currently acts as associate editor for TOIS and has been reviewing for 25 other Computing journals, 50 international conferences and 11 research funders including the Commission of the European Communities and the European Research Council. Stefan is a Fellow of the Higher Education Academy in the UK; a member of the EPSRC College, ACM and BCS; and treasurer of the BCS Information Retrieval Specialist Group. Stefan also serves as a London Technology Network Business Fellow for the Centre for Research in Computing at The Open University. For further information and publications see http://people.kmi.open.ac.uk/stefan.
Teaching:
Stefan has been teaching since 1994 and obtained a postgraduate qualification "Certificate of Advanced Studies in Learning and Teaching" in 2002 following a formal one-year part-time postgraduate study at Imperial College London. He taught undergraduate courses and MSc courses to university students, and compact courses to business consultants, who participated in a part-time MSc programme "Computing in Industry" while continuing to work. Stefan's teaching and tutorial style is unconventional in the sense that he delivers a fully worked out script [1] with his slides and loosens up lectures with discussion points and exercises for which he gets the class to actively participate.
Reference:
[1] Stefan Rüger (2010). Multimedia information retrieval. Synthesis Lectures on Information Concepts, Retrieval and Services. Morgan & Claypool Publishers. DOI: 10.2200/S00244ED1V01Y200912ICR010

906

Information Retrieval Challenges in Computational Advertising
Andrei Broder, Evgeniy Gabrilovich, Vanja Josifovski
Yahoo! Research, Santa Clara, CA, USA {broder, gabr, vanjaj} @ yahoo-inc.com

Abstract:
Computational advertising is an emerging scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the "best match" between a given user in a given context and a suitable advertisement. The aim of this tutorial is to present the state of the art in Computational Advertising, in particular in its IR-related aspects, and to expose the participants to the current research challenges in this field. The tutorial does not assume any prior knowledge of Web advertising, and will begin with a comprehensive background survey.
Going deeper, our focus will be on using a textual representation of the user context to retrieve relevant ads. At first approximation, this process can be reduced to a conventional setup by constructing a query that describes the user context and executing the query against a large inverted index of ads. We show how to augment this approach using query expansion and text classification techniques tuned for the ad-retrieval problem. In particular, we show how to use the Web as a repository of queryspecific knowledge and use the Web search results retrieved by the query as a form of a relevance feedback and query expansion. We also present solutions that go beyond the conventional bag of words indexing by constructing additional features using a large external taxonomy and a lexicon of named entities obtained by analyzing the entire Web as a corpus.
The last part of the tutorial will be devoted to a potpourri of recent research results and open problems inspired by Computational Advertising challenges in text summarization, natural language generation, named entity recognition, computer-human interaction, and other SIGIR-relevant areas.
ACM Categories & Descriptors:
H.3.3 Information Search and Retrieval
General Terms:
Algorithms, Experimentation.
Keywords:
Online advertising, sponsored search, content match
Bio/Bios:
Andrei Broder is a Yahoo! Fellow and Vice President Computational Advertising. Previously he was an IBM Distinguished Engineer and the CTO of the Institute for Search
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

and Text Analysis in IBM Research. From 1999 until 2002 he was VP for Research and Chief Scientist at the AltaVista Company. He was graduated Summa cum Laude from Technion, the Israeli Institute of Technology, and obtained his M.Sc. and Ph.D. at Stanford University under Don Knuth. He has authored more than a hundred papers and was awarded twenty-five patents. He is a member of the US National Academy of Engineering, a fellow of ACM and of IEEE, and past chair of the IEEE Technical Committee on Mathematical Foundations of Computing.
http://research.yahoo.com/Andrei_Broder
Evgeniy Gabrilovich is a Senior Research Scientist and Manager of the NLP & IR Group at Yahoo! Research. Recently, he organized a workshop on the synergy between user-contributed knowledge and research in AI at IJCAI'09, and a workshop on information retrieval for advertising at SIGIR'09. Evgeniy presented tutorials on computational advertising at CIKM'09, IJCAI'09, ACL'08, and EC'08. He served on the program committees of WWW, WSDM, SIGIR, CIKM, AAAI, ACL, EMNLP, ICWSM, HLT, COLING, and JCDL. Evgeniy earned his M.Sc. and Ph.D. degrees in Computer Science from the Technion - Israel Institute of Technology.
http://research.yahoo.com/Evgeniy_Gabrilovich
Vanja Josifovski is a Principal Research Scientist at Yahoo! Research, where he works on search and advertisement technologies for the Internet. He is currently exploring designs for the next generation ad placement platforms for textual and behavioral advertising. Previously, Vanja was a Research Staff Member at the IBM Almaden Research Center working on several projects in database runtime and optimization, federated databases, and enterprise. He earned his M.Sc. degree from the University of Florida at Gainesville and his Ph.D. from the Linkoping University in Sweden. Vanja has published over 50 peer reviewed publications, authored around 40 patent applications, and was on the program committees of WWW, SIGIR, ICDE, VLDB, CIKM, ICDM, KDD and other major conferences in the database, information retrieval, and search areas.
http://research.yahoo.com/Vanja_Josifovski

908

Extraction of Open-Domain Class Attributes from Text: Building Blocks for Faceted Search

Marius Paca
Google Inc., mars@google.com

Abstract: Knowledge automatically extracted from text captures instances, classes of instances and relations among them. In particular, the acquisition of class attributes (e.g., "top speed", "body style" and "number of cylinders" for the class of "sports cars") from text is a particularly appealing task and has received much attention recently, given its natural fit as a building block towards the far-reaching goal of constructing knowledge bases from text. This tutorial provides an overview of extraction methods developed in the area of Web-based information extraction, with the purpose of acquiring attributes of opendomain classes. The attributes are extracted for classes organized either as a flat set or hierarchically. The extraction methods operate over unstructured or semi-structured text available within collections of Web documents, or over relatively more intriguing data sources consisting of anonymized search queries. The methods take advantage of weak supervision provided in the form of seed examples or small amounts of annotated data, or draw upon knowledge already encoded within human-compiled resources (e.g., Wikipedia). The more ambitious methods, aiming at acquiring as many accurate attributes from text as possible for hundreds or thousands of classes covering a wide range of domains of interest, need to be designed to scale to Web

collections. This restriction has significant consequences on the overall complexity and choice of underlying tools, in order for the extracted attributes to ultimately aid information retrieval in general and Web search in particular, by producing relevant attributes for open-domain classes, along with other types of relations among instances or among classes.
ACM Categories & Descriptors: H.3.3 Information Storage and Retrieval - Content Analysis and Indexing; I.2.7. Artificial Intelligence - Natural Language Processing
General Terms: Algorithms, Experimentation
Keywords: Information extraction, knowledge acquisition, class attributes, attribute extraction
Bio: Marius Paca is a Senior Research Scientist at Google. He graduated with a Ph.D. degree in Computer Science from Southern Methodist University in Dallas, Texas and an M.Sc. degree in Computer Science from Joseph Fourier University in Grenoble, France. Current research interests include factual information extraction from unstructured text and its applications to Web search.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
909

From Federated to Aggregated Search
Fernando Diaz
Yahoo! Labs, diazf@yahoo-inc.com
Mounia Lalmas
University of Glasgow, mounia@acm.org
Milad Shokouhi
Microsoft Research, milads@microsoft.com

Abstract:
Federated search refers to the brokered retrieval of content from a set of auxiliary retrieval systems instead of from a single, centralized retrieval system. Federated search tasks occur in, for example, digital libraries (where documents from several retrieval systems must be seamlessly merged) or peer-to-peer information retrieval (where documents distributed across a network of local indexes must be retrieved).
In the context of web search, aggregated search refers to the integration of non-web content (e.g. images, videos, news articles, maps, tweets) into a web search result page. This is in contrast with classic web search where users are presented with a ranked list consisting exclusively of general web documents. As in other federated search situations, the non-web content is often retrieved from auxiliary retrieval systems (e.g. image or video databases, news indexes).
Although aggregated search can be seen as an instance of federated search, several aspects make aggregated search a unique and compelling research topic. These include large sources of evidence (e.g. click logs) for deciding what non-web items to return, constrained interfaces (e.g. mobile screens), and a very heterogeneous set of available auxiliary resources (e.g. images, videos, maps, news articles). Each of these aspects introduces problems and opportunities not addressed in the federated search literature.
Aggregated search is an important future research direction for information retrieval. All major search engines now provide aggregated search results. As the number of available auxiliary resources grows, deciding how to effectively surface content from each will become increasingly important.
The goal of this tutorial is to provide an overview of federated search and aggregated search techniques for an intermediate information retrieval researcher. At the same time, the content will be valuable for practitioners in industry. We will take the audience through the most influential work in these areas and describe how they relate to real world aggregated search systems. We will also list some of the new challenges confronted in aggregated search and discuss directions for future work.
ACM Categories & Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval

General Terms
Algorithms, Design, Experimentation, Measurement
Keywords
Distributed information retrieval, Federated search, Aggregated search, Universal search, Vertical search, Metasearch
Bios:
Fernando Diaz is a research scientist at Yahoo! Labs. His primary research interests concern formal models of information retrieval, and more recently aggregated search. His research experience also includes distributed information retrieval approaches to web search, interactive and faceted retrieval, mining of temporal patterns from news and query logs, cross-lingual information retrieval, graph-based retrieval methods, and synthesizing information from multiple corpora. He received his PhD from the University of Massachusetts Amherst in 2008.
Mounia Lalmas holds a Microsoft Research/Royal Academy of Engineering Research Chair in Information Retrieval at the Department of Computing Science, University of Glasgow. Her research focuses on the development and evaluation of intelligent access to interactive heterogeneous and complex information repositories. From 2002 until 2007, she co-led the Evaluation Initiative for XML Retrieval (INEX). She is now working on technologies for aggregated search and bridging the digital divide. She is also looking at the use of quantum theory to model interactive information retrieval.
Milad Shokouhi is an applied researcher working for Bing at Microsoft Research Cambridge. Before joining Microsoft in 2007, he did his PhD on federated search at the Royal Melbourne Institute of Technology (RMIT) University. His research interests are federated search, query expansion, user studies and web search evaluation.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
910

Search and Browse Log Mining for Web Information Retrieval: Challenges, Methods, and Applications
Daxin Jiang
(Microsoft Research Asia, Beijing, China, djiang@microsoft.com)
Jian Pei
(Simon Fraser University, Burnaby, BC Canada, jpei@cs.sfu.ca)
Hang Li
(Microsoft Research Asia, Beijing, China, hangli@microsoft.com)

Abstract: Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users' permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve search results as well as online advertisement. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges.
In this tutorial, we focus on mining search and browse log data for Web information retrieval. We consider a Web information retrieval system consisting of four components, namely, query understanding, document understanding, query-document matching, and user understanding. Accordingly, we organize the tutorial materials along these four aspects. For each aspect, we will survey the major tasks, challenges, fundamental principles, and state-of-the-art methods.
The goal of this tutorial is to provide a systematic survey on largescale search/browse log mining to the IR community. It will help IR researchers to get familiar with the core challenges and promising directions in log mining. At the same time, this tutorial may also serve the developers of Web information retrieval systems as a comprehensive and in-depth reference to the advanced log mining techniques.
ACM Categories & Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: Search and browse logs, log data mining

Bios: Daxin Jiang is a Researcher at Microsoft Research Asia. His research focuses on data mining and information retrieval. He received Ph.D. in computer science from the State University of New York at Buffalo. He has published extensively in prestigious conferences and journals, and served as a PC member of many conferences. He received the Best Application Paper Award of SIGKDD'08 and the Runner-up for Best Application Paper Award of SIGKDD'04.
Jian Pei is an Associate Professor and the Associate Director, Research, of the School of Computing Science, Simon Fraser University. His research focuses on data mining and analytic queries in databases. With prolific publications in refereed journals and conferences, he is the recipient of several prestigious awards. He is an associate editor of ACM Transactions on Knowledge Discovery from Data (TKDD) and IEEE Transactions
on Knowledge and Data Engineering (TKDE). He has served regularly in the organization committees and the program committees of numerous international conferences and workshops. He is a senior member of both ACM and IEEE.
Hang Li is a Senior Researcher and Research Manager at Microsoft Research Asia. His research areas include natural language processing, information retrieval, statistical machine learning, and data mining. He graduated from Kyoto University and holds a PhD in computer science from the University of Tokyo. Hang has about 80 publications in international conferences and journals. He is associate editor of ACM Transaction on Asian Language Information Processing and area editor of Journal for Computer and Science Technology, etc. His recent academic activities include senior PC member of SIGIR 2010, WSDM 2010, and KDD 2010, area chair of ACL 2010, and PC member of WWW 2010.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
912

Information Retrieval for E-Discovery
David D. Lewis
David D. Lewis Consulting, sigir2010tutorial@DavidDLewis.com

Abstract:
Discovery, the process under which parties to legal cases must reveal documents relevant to the disputed issues is a core aspect of trials in the United States, and a lesser but important factor in other countries. Discovery on documents stored in computerized systems (known variously as electronic discovery, e-discovery, e-disco, EDD, and ED) is increasingly the major factor in discovery, and has become a multi-billion dollar industry.
I will discuss the basics of e-discovery, the scale and diversity of the materials involved, and the economics of identifying and reviewing potentially responsive material. I will then focus on three major IR areas of interest: search, supervised machine learning (including text classification and relevance feedback), and interface support for manual relevance assessment. For each, I will discuss technologies currently used in e-discovery, the evaluation methods applicable to measuring effectiveness, and existing research results not yet seeing commercial practice.
I will also outline research directions that, if successfully pursued, would potentially be of great interest in e-discovery applications. A particular focus will be on areas where researchers can make progress without access to operational e-discovery environments or "realistic" test collections. Connections will be drawn with the use of IR in related tasks, such as enterprise search, criminal investigations, intelligence analysis, historical research, truth and reconciliation commissions, and freedom of information (open records or sunshine law) requests.
ACM Categories & Descriptors:
H.3.3 Information Search and Retrieval; H.4.1 Office Automation; I.2.1 Artificial Intelligence, Applications and Expert Systems Law
General Terms:
Economics, Experimentation, Legal Aspects, Measurement
Keywords:
backups, computer forensics, document formats, duplicate detection, e-mail, electronic mail, OCR, text mining

Bio:
David D. Lewis, Ph.D. is a freelance computer scientist based in Chicago, IL, USA. He works in the areas of information retrieval, data mining, natural language processing, and the evaluation of complex information systems. Dave Lewis has consulted for startups, corporations, investors, universities, nonprofits, government agencies, and law firms. Before becoming a consultant he held research positions at AT&T Labs, Bell Labs, and the University of Chicago. He has published more than fifty scientific papers and six patents, and was elected a Fellow of the American Association for the Advancement of Science in 2006.
Dr. Lewis was one of the founders of the TREC Legal Track, the first large scale open evaluation of IR technology for electronic discovery in legal cases. As part of this work, he led the creation of the first large scale test collection for experimentation on ediscovery. He has served as a consulting expert in a number of legal cases, including writing a report to the court on the use of IR in e-discovery. He recently developed training materials for paralegals and lawyers working on e-discovery matters at a Fortune 100 company.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
913

On the Mono- and Cross-Language Detection of Text Reuse and Plagiarism 

Alberto Barrón-Cedeño
NLE Lab. ELiRF Research Group DSIC, Universidad Politécnica de Valencia
Valencia, Spain
lbarron@dsic.upv.es

ABSTRACT
Plagiarism, the unacknowledged reuse of text, has increased in recent years due to the large amount of texts readily available. For instance, recent studies claim that nowadays a high rate of student reports include plagiarism, making manual plagiarism detection practically infeasible.
Automatic plagiarism detection tools assist experts to analyse documents for plagiarism. Nevertheless, the lack of standard collections with cases of plagiarism has prevented accurate comparing models, making differences hard to appreciate. Seminal efforts on the detection of text reuse [2] have fostered the composition of standard resources for the accurate evaluation and comparison of methods.
The aim of this PhD thesis is to address three of the main problems in the development of better models for automatic plagiarism detection: (i) the adequate identification of good potential sources for a given suspicious text; (ii) the detection of plagiarism despite modifications, such as words substitution and paraphrasing (special stress is given to crosslanguage plagiarism); and (iii) the generation of standard collections of cases of plagiarism and text reuse in order to provide a framework for accurate comparison of models.
Regarding difficulties (i) and (ii) , we have carried out preliminary experiments over the METER corpus [2]. Given a suspicious document dq and a collection of potential source documents D, the process is divided in two steps. First, a small subset of potential source documents D  D is retrieved. The documents d  D are the most related to dq and, therefore, the most likely to include the source of the plagiarised fragments in it. We performed this stage on the basis of the Kullback-Leibler distance, over a subsample of document's vocabularies. Afterwards, a detailed analysis is carried out comparing dq to every d  D in order to identify potential cases of plagiarism and their source. This comparison was made on the basis of word n-grams, by considering n = {2, 3}. These n-gram levels are flexible enough to properly retrieve plagiarised fragments and their sources despite modifications [1]. The result is offered to the user to take the final decision. Further experiments were done in both stages in order to compare other similarity measures, such as the cosine measure, the Jaccard coefficient and diverse fingerprinting and probabilistic models.
Partially funded by the CONACYT Mexico 192021 grant and the Text-Enterprise 2.0 TIN2009-13391-C04-03 project.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

One of the main weaknesses of currently available models is that they are unable to detect cross-language plagiarism. Approaching the detection of this kind of plagiarism is of high relevance, as the most of the information published is written in English, and authors in other languages may find it attractive to make use of direct translations.
Our experiments, carried out over parallel and a comparable corpora, show that models of "standard" cross-language information retrieval are not enough. In fact, if the analysed source and target languages are related in some way (common linguistic ancestors or technical vocabulary), a simple comparison based on character n-grams seems to be the option. However, in those cases where the relation between the implied languages is weaker, other models, such as those based on statistical machine translation, are necessary [3].
We plan to perform further experiments, mainly to approach the detection of cross-language plagiarism. In order to do that, we will use the corpora developed under the framework of the PAN competition on plagiarism detection (cf. PAN@CLEF: http://pan.webis.de). Models that consider cross-language thesauri and comparison of cognates will also be applied.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Miscellaneous
General Terms
Experimentation
Keywords
Text similarity, plagiarism detection, cross-language plagiarism detection
1. REFERENCES
[1] A. Barr´on-Ceden~o, P. Rosso, and J. Bened´i. Reducing the Plagiarism Detection Search Space on the Basis of the Kullback-Leibler Distance. In A. F. Gelbukh, editor, CICLing 2009, volume LNCS (5449), pages 523­534, Mexico City, Mexico, 2009. Springer.
[2] P. Clough, R. Gaizauskas, S. Piao, and Y. Wilks. Measuring Text Reuse. In Proceedings of Association for Computational Linguistics (ACL2002), pages 152­159, Philadelphia, PA, 2002.
[3] M. Potthast, A. Barr´on-Ceden~o, B. Stein, and P. Rosso. Cross-Language Plagiarism Detection. Language Resources and Evaluation, Special Issue on Plagiarism and Authorship Analysis, 2010.

914

User Interface Designs to Support the Social Transfer of Web Search Expertise
Neema Moraveji
Learning Sciences & Technology Design, School of Education Human-Computer Interaction, Department of Computer Science
Stanford University
neema@stanford.edu

ABSTRACT
While there are many ways to develop search expertise, I maintain that most members of the general public do so in an inefficient manner. One reason is that, with current tools, is difficult to observe experts as a means of acquiring search expertise in a scalable fashion. This calls for a redesign of computer-mediated communication tools to make individual search strategies visible to other users. I present a research agenda to investigate this claim, which draws upon theories of social learning. I use designbased research to build novel systems that enable imitation-based learning of search expertise.
Categories and Subject Descriptors
H.1.2 [Human Factors]: Human Information Processing.
General Terms
Design, Human Factors
Keywords
Social, learning, information retrieval, transfer, expertise.
1. LEARNING SEARCH SOCIALLY
"Not all who wander are lost." ­J.R.R. Tolkien
Expertise in web search has become so crucial a skill that it is referred to as one of several `digital literacies' [3]. It includes generating appropriate keywords, understanding the reasoning behind ranking order, discerning legitimate from illegitimate pages, identifying relevance of individual sites, reformulating queries, and synthesizing information gleaned from results.
Web search interfaces have evolved in such a way that users can learn how to use them iteratively, through trial-and-error, rather than reading or following best practices. There is currently lacking a sociocultural practice of learning to search from one another.
Simply showing expert search queries and results to other users would not lead to conceptual learning. This is because any individual search is not often reusable for other users' information needs. That is, information needs vary widely and are contextrelevant, so viewing another's search results may not be most efficient for teaching expertise of the search process itself.
It is the mental processes underlying search that are important to imitate. To transfer these processes, we draw upon the theory of cognitive apprenticeship [2], which posits that experts should reveal tacit and implicit components of their thought processes to
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

the learner in order to help them construct a valid mental model of how to enact the practice.
We design and study technologies that enable individuals to a) transmit their own search processes and b) model others' search processes and practices. Such technologies would "make knowledge visible" [1] between participants, creating opportunities for cognitive apprenticeship. To this end, my research aims to answer the following research questions:
Can search expertise be positively and significantly impacted by technology-mediated social interaction? What are the design principles for such technologies?
Our first work that investigated this idea was the ClassSearch system, meant for a classroom environment. Search metadata from student computers was projected onto a shared display in tag cloud and related forms. Students used the newfound common ground created by the system to generate interactions between each other about search practices. This system motivated further inquiry into technology-mediated social learning of expertise.
The next research platform we built, LineHive.com, rests upon a simple premise: it is currently difficult to communicate a summary of a set of related websites and people would also use such a tool to transmit navigation and path information to one another. The user can choose what sites to string together into a list. To incent users to share search processes, we represent navigation paths as attractive, chronological timelines that can be embedded in any page as a widget (like a YouTube video). The timeline visualization is meant as a communicative proxy for users to share insights found on the Internet with one another. The `search expertise' learning goal is co-opting this medium.
My work will identify the characteristics of peer search processes that will positively impact one's expertise and will determine how and when to display those processes to improve expertise. It draws upon prior work in search instruction and social learning to design novel, scalable systems to improve search expertise.
2. REFERENCES
[1] Bereiter, C., Scardamalia, M. (2003). "Learning to Work Creatively with Knowledge" EARLI Advances in Learning and Instruction.
[2] Collins, A., Brown, J. S., & Newman, S. E. (1987). Cognitive apprenticeship: Teaching the craft of reading, writing and mathematics. BBN Labs, Cambridge, MA.
[3] Jenkins, H. Confronting the Challenges of Participatory Culture: Media Education for the 21st Century. 2006.

915

Leveraging User Interaction and Collaboration for Improving Multilingual Information Access in Digital
Libraries

Juliane Stiller
Berlin School of Library and Information Science, Humboldt University Dorotheenstr. 26
10117 Berlin, Germany
juliane.stiller@ibi.hu-berlin.de

ABSTRACT
The goal of interactive cross-lingual information retrieval systems is to support users in formulating effective queries and selecting the documents which satisfy their information needs regardless of the language of these documents. This dissertation aims at harnessing user-system interaction, extracting the added value and integrating it back into the system to improve cross-lingual information retrieval for successive users. To achieve this, user input at different interaction points will be evaluated. This will, among others, include interaction during user-assisted query translations, implicit and explicit relevance feedback and social tags. To leverage this input, explorative studies need to be conducted to determine beneficial user input and the methods of extracting it.
Categories and Subject Descriptors: H.3.7. [Information Storage and Retrieval]: Digital Libraries -user issues; H.3.7. [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Design, Human Factors, Languages
Keywords: Interactive CLIR, Digital Libraries, Social tags
1. INTRODUCTION
Digital Libraries which aggregate and provide access to collections in different languages are confronted with problems of multilingual information access (MLIA). The most challenging aspect is the search capability of the system, which is referred to as crosslingual information retrieval (CLIR). CLIR is characterized by differences in query and document language [3]. The main difficulties for CLIR are the disambiguation of the query term in the source and target language and the identification of the query language. Tools for CLIR such as dictionaries are not universally available in every language needed or in every domain covered in digital libraries.
This dissertation aims at harnessing user input for improving and extending existing CLIR processing tools and consequently CLIR systems in their entirety. The goal is to bridge a gap between research on interactive cross-lingual information retrieval and studies focusing on establishing sustainable ways to leverage user input.
In this context, user interaction and collaboration comprises social tagging and other system interaction components like query reformulations and relevance feedback.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. HARNESSING USER INPUT
In CLIR systems, interactive components are crucial to accomplish search tasks [2]. There are two main scenarios where the user input could be incorporated into the system to enhance multilingual information retrieval:
1. Harnessing multilingual tags for enriching metadata and disambiguating query terms and
2. Improving existing dictionaries by adding translations entered into the system by the user, e.g. directly from the user or via log files analysis.
In this project, interactive CLIR focuses on input from previous user-system interactions which can offer improvements for CLIR tools. This requires the identification of different interaction points, where users are encouraged to leave their feedback in the form of alternative translations, relevance feedback or tags. This input will be collected and integrated back into the CLIR system. To gather data, the user input at different points of user interaction will be analyzed to identify the added value. A method to harness this input and to integrate it back into the system will be determined accordingly, e.g. evaluation of query logs [1].
Europeana - a multilingual digital library - will serve as main research object for explorative studies. Unifying the different results from these experiments will provide a basis for determining the benefits of user interaction for CLIR in digital libraries. These results will serve as use cases for interaction which enhances CLIR and can easily be prototyped. The project will include a final analysis of the effectiveness of different interactive features within CLIR systems.
3. REFERENCES
[1] A. Bosca and L. Dini. Cacao project at the logclef track. In Working notes of the Cross Language Evaluation Forum (CLEF), 2009.
[2] J. Gonzalo. Scenarios for interactive cross-language retrieval systems. In Proceedings of the Workshop of Cross-Language Information Retrieval: A Research Roadmap Workshop held at the 25th Annual International ACM SIGIR Conference, 2002.
[3] D. W. Oard. Multilingual information access. In M. J. Bates and M. N. Maack, editors, Encyclopedia of Library and Information Sciences. Taylor & Francis, 3rd edition, 2009.

916

Entity Information Management in Complex Networks

Yi Fang
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
fangy@cs.purdue.edu

ABSTRACT
Entity information management (EIM) deals with organizing, processing and delivering information about entities. Its emergence is a result of satisfying more sophisticated information needs that go beyond document search. In the recent years, entity retrieval has attracted much attention in the IR community. INEX has started the XML Entity Ranking track since 2007 and TREC has launched the Entity track since 2009 to investigate the problem of related entity finding. Some EIM problems go beyond retrieval and ranking such as: 1) entity profiling, which is about characterizing a specific entity, and 2) entity distillation, which is about discovering the trend about an entity. These problems have received less attention while they have many important applications.
On the other hand, the entities in the real world or in the Web environment are usually not isolated. They are connected or related with each other in one way or another. For example, the coauthorship makes the authors with similar research interests be connected. The emergence of social media such as Facebook, Twitter and Youtube has further interweaved the related entities in a much larger scale. Millions of users in these sites can become friends, fans or followers of others, or taggers or commenters of different types of entities (e.g., bookmarks, photos and videos). These networks are complex in the sense that they are heterogeneous with multiple types of entities and of interactions, they are large-scale, they are multi-lingual, and they are dynamic. These features of the complex networks go beyond traditional social network analysis and require further research.
In this proposed research, I investigate entity information management in the environment of complex networks. The main research question is: how can the EIM tasks be facilitated by modeling the content and structure of complex networks? The research is in the intersection of content based information retrieval and complex network analysis, which deals with both unstructured text data and structured networks. The specific targeting EIM tasks are entity retrieval, entity profiling and entity distillation. In addition to the main research question, the following questions are considered: How can we accomplish a EIM task involving diverse entity and interaction types? How to model the evolution of entity profiles as well as the underlying complex networks?
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

How can the existing cross-language IR work be leveraged to build entity profiles with multi-lingual evidence?
I propose to use probabilistic models and discriminative models in particular to address the above research questions. In my research, I have developed discriminative models for expert search to integrate arbitrary document features [3] and to learn flexible combination strategies to rank experts in heterogeneous information sources [1]. Discriminative graphical models are proposed to jointly discover homepages by inference on the homepage dependence network [2]. The dependence of table elements is exploited to collectively perform the entity retrieval task [4]. These works have shown the power of discriminative models for entity search and the benefits of utilizing the dependencies among related entities. What I would like to do next is to develop a unified probabilistic framework to investigate the research questions raised in this proposal.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous
General Terms
Algorithms, Measurement, Performance, Experimentation
Keywords
Entity retrieval, Entity profiling, Social network analysis
1. REFERENCES
[1] Y. Fang, L. Si, and A. Mathur. Ranking experts with discriminative probabilistic models. In Proceedings of SIGIR Workshops, 2009.
[2] Y. Fang, L. Si, and A. Mathur. Discriminative graphical models for faculty homepage discovery. Information Retrieval, 2010.
[3] Y. Fang, L. Si, and A. Mathur. Discriminative models of integrating document evidence and document-candidate associations for expert search. In Proceedings of SIGIR, 2010.
[4] Y. Fang, L. Si, Z. Yu, Y. Xian, and Y. Xu. Entity retrieval by hierarchical relevance model, exploiting the structure of tables and learning homepage classifiers. In Proceedings of TREC-18, 2009.

917

Finding People and their Utterances in Social Media
Wouter Weerkamp
w.weerkamp@uva.nl ISLA, University of Amsterdam, Science Park 107, 1098 XG Amsterdam

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval
General Terms
Algorithms, Theory, Experimentation, Measurement
Keywords
Social media, information retrieval
ABSTRACT
Since its introduction, social media, "a group of internet-based applications that (. . . ) allow the creation and exchange of user generated content" [1], has attracted more and more users. Over the years, many platforms have arisen that allow users to publish information, communicate with others, connect to like-minded, and share anything a users wants to share. Text-centric examples are mailing lists, forums, blogs, community question answering, collaborative knowledge sources, social networks, and microblogs, with new platforms starting all the time. Given the volume of information available in social media, ways of accessing this information intelligently are needed; this is the scope of my research.
Why should we care about information in social media? Here are three examples that motivate my interest. (A) Viewpoint research; someone wants to take note of the viewpoints on a particular issue. (B) Answers to problems; many problems have been encountered before, and people have shared solutions. (C) Product development; gaining insight into how people use a product and what features they wish for, eases the development of new products. Looking at these examples of information need in social media, we observe that they revolve not just around relevance in the traditional sense (i.e., objects relevant to a given topic), but also around criteria like credibility, authority, viewpoints, expertise, and experiences. However, these additional aspects are typically conditioned on the topical relevance of information objects.
In social media, "information objects" come in several types but many are utterances created by people (blog posts, emails, questions, answers, tweets). People and their utterances offer two natural entry points to information contained in social media: utterances that are relevant and people that are of interest. I focus on three tasks in which the interaction between the two is key.
The first task concerns finding utterances in social media. Although this resembles a traditional ad hoc retrieval task, the lack of top-down rules and editors in social media entails the use of unexpected language: spelling and grammar errors are not corrected,
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

and the vocabulary is unrestricted, unlike edited content. In my research, I use two features of social media to overcome the problem of unexpected language: (i) I translate several credibility indicators to measurable features, and implement these in the blog post retrieval process to enhance retrieval effectiveness [2]; (ii) Utterances are surrounded by their environment, and this influences their content. I use this observation to introduce a model for query modeling using external collections [5], and investigate which context levels are useful in email retrieval [4].
The second task, finding people in social media, is operationalized in blog feed search: identify blogs that show a recurring interest in a given topic. This task shows similarities with expert finding, and models from this field have been successfully adopted. Blogbased models build and rank representations of bloggers based on their utterances, whereas post-based models rank utterances and aggregate scores to construct a ranking of bloggers. The former capture the centrality of the topic to the blog, but are not very efficient; the latter can identify interesting posts and are efficient. In [6, 7] I introduce a two-stage model that ranks utterances and constructs models for the blogs these utterances belong to and ranks these blog models. My two-stage model is more efficient than blog-based models, and more effective than post-based models.
The third and final task builds on the previous two, and focuses on finding utterances using people. Applied to search in email archives, personal profiles can be constructed from people's utterances. On top of these personal profiles, communication profiles are built, containing information extracted from threads, quotes and replies as well as linguistic clues. Communication profiles can indicate the role of people in a topic-dependent way. For a given topic, I use both communication and personal profiles of people to find utterances that are relevant to the topic [3].
References
[1] A. M. Kaplan and M. Haenlein. Users of the world, unite! the challenges and opportunities of social media. Business Horizons, 53(1):59­68, 2010.
[2] W. Weerkamp and M. de Rijke. Credibility improves topical blog post retrieval. In ACL-08: HLT, pages 923­931, 2008.
[3] W. Weerkamp and M. de Rijke. Communication and personal profiles in email search. In To be submitted, 2010.
[4] W. Weerkamp, K. Balog, and M. de Rijke. Using contextual information to improve search in email archives. In ECIR 2009, 2009.
[5] W. Weerkamp, K. Balog, and M. de Rijke. A generative blog post retrieval model that uses query expansion based on external collections. In ACL-ICNLP 2009, 2009.
[6] W. Weerkamp, K. Balog, and M. de Rijke. Blog feed search using a post index. Submitted, 2010.
[7] W. Weerkamp, K. Balog, and M. de Rijke. A two-stage model for blog feed search. In SIGIR 2010, 2010.

918

Leveraging User-Generated Content for News Search
Richard M. C. McCreadie
Department of Computing Science University of Glasgow Glasgow, G12 8QQ
richardm@dcs.gla.ac.uk

Categories and Subject Descriptors
H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
General Terms - Experimentation Keywords - News, Blogs, Social Media
ABSTRACT
Over the last few years both availability and accessibility of current news stories on the Web have dramatically improved [3]. In particular, users can now access news from a variety of sources hosted on the Web, from newswire presences such as the New York Times, to integrated news search within Web search engines. However, of central interest is the emerging impact that user-generated content (UGC) is having on this online news landscape. Indeed, the emergence of Web 2.0 has turned a static news consumer base into a dynamic news machine, where news stories are summarised and commented upon. In summary, value is being added to each news story in terms of additional content.
Importantly, however, while there has been movement in commercial circles to exploit this extra value to enrich online news [5], there has been little research from the academic community on how can be achieved. Indeed, the main purpose of this thesis is to research practical techniques for the integration of UGC to improve the news search component of the most ubiquitous of Web tools, i.e the Web search engine. Importantly, we identify the following three key aspects of news search which might be improved through the application of UGC.
Intuitively, the first task that the news vertical search aspect of a Web search engine needs to accomplish when confronted with a user query is to decide whether the query is in fact news-related, and hence requires news content to be included. However, queries themselves are sparse in nature, being often comprised of one of two tokens only. This presents issues when performing query classification, as there are few features to distinguish the news related queries. We attest that UGC can help alleviate this ambiguity. Indeed, we hypothesise that there is a strong link between the volume of UGC content being posted mentioning a query and the likelihood of that query being news-related within a specific timeframe.
Secondly, we consider the task of real-time event detection. It is imperative for search engines to maintain knowledge of the events of the moment, such that the results displayed are updated. Traditionally, systems have detected new events through the clustering of newswire articles [1]. However, in the current fast-paced news
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

search environment where users begin querying for events within a couple of minutes of their occurrence [4], relying on slow newswire reporting is unacceptable. On the other-hand, UGC sources such as Twitter provide a natural alternative, as the high post rate and popularity of news topics makes a site such as this an ideal medium from which to monitor emerging events. Indeed, many paid journalists maintain personal blogs and other social media accounts for the reporting of fast-breaking news stories [2].
Lastly, we examine the presentation of results to the user. The presentation of news articles to satisfy news-searches is generally accepted. However, with the ever-increasing pace of news reporting world-wide, there is now no guarantee that a trusted news source will have yet published upon the story. In these cases, one must look else-where for content to satisfy the user. We hypothesise that UGC is ideal for presentation in these cases as the delay between an event occurring and commentary appearing in UGC sources like Twitter or the Blogosphere is mear minutes. Moreover some information needs cannot be easily solved using newswire articles alone. For example, the correct result for the query `current news' would be a list of news stories ranked by their importance for the day in question. This is a difficult ranking problem, as `importance' is greatly dependent upon the perspective of the user. In this case, one solution might be to leverage `public opinion' as represented in UGC, for example by taking `the pulse of the Blogosphere'. Indeed, we have examined such during TREC 2009.
In conclusion, we have identified multiple areas of the newssearch process which cannot be satisfied by traditional newswire articles. We hypothesise that the application of user-generated content can be leveraged to improve the field of news-search in relation to the rich and timely information that UGC provides.
1. REFERENCES
[1] James Allan. Topic detection and tracking. Springer, 2002. [2] D. Matheson. Weblogs and the epistemology of the news: some trends
in online journalism. New Media and Society, 6(4):443­468, 2004. [3] Newspaper Association of America (NAA). Newspaper Web sites
attract more than 70 million visitors in June; over one-third of all Internet users visit newspaper Web sites, 2010. http: //www.naa.org/PressCenter/SearchPressReleases/ 2009/NEWSPAPER-WEB-SITES-ATTRACT-MORE-THAN-70MILLION-VISITORS.aspx, accessed on 25/01/2010. [4] J. Pedersen. Keynote speech. In the Third Annual Workshop on Search in Social Media, 2010. [5] Amit Singhal. Relevance meets the real-time Web, 2010. http://googleblog.blogspot.com/2009/12/ relevance-meets-real-time-web.html, accessed on 25/01/2010.

919

User Centered Story Tracking

Ilija Subasic´
Department of Computer Science Katholieke Universiteit Leuven
Celestijnenlaan 200A - bus 2402 3001 Heverlee, Belgium
ilija.subasic@cs.kuleuven.be

ABSTRACT
Using news provider services available on the Internet has for many people become the main medium for staying informed about the world. Such services support Internet users in story tracking: following the news developments over time. We regard a story as a set of time-stamped documents describing correlated subjects, such as for example persons, event descriptions, and topics. Story tracking differs from "regular" search, and should go beyond retrieving the most relevant documents, and refine the search results describing novel subjects of a story. These novel subjects are bursty (appearing significantly more frequently in a time window of search than in other time) content elements, for example: words, word n-grams, and sentences. Text-oriented versions of the story tracking task have been described in the TAC Update Summarization and TREC Novelty Detection and Adaptive Filtering tasks. Recently a number of methods have focused on mining for lower-level sub-sentential patterns. We refer to these approaches collectively as temporal text mining (TTM). Works in these areas are mostly concerned with the system side algorithms and automatic evaluation procedures. In this work we put emphasis on the users, and go beyond automatic algorithm evaluation by including users in the story tracking process and assessing how different approaches aid them in this process.
When tracking a story users have two main goals: (1) story understanding and (2) story search. The goal of story understanding is to comprehend the story's subjects and track their evolution. In order to achieve this, users will want to inspect the story as well as the underlying documents (story search). Here, finding the most relevant documents is only a means to the (generally more important) end of discovering the change and their evolution and comprehending the general the story development. This situation calls for systems that: (a) identify important subjects and their substructure, (b) show how these substructures emerge, change, and disappear over time, and (c) give users intuitive interfaces for interactively exploring the story landscape and at the same time the underlying document. The user should not be exposed to well-formatted, predefined and global patterns from a machine intelligence system, but should be an integral part of information processing. Following this idea, we have built an interactive semi-automatic visual tool for story tracking. We developed a method for bursty-patterns
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

detection based on the increase the in frequency of normalized bi-gram co-occurrences, and then use the discovered patterns to build a visual summarization as a graph. Interacting with the the graph enables users to build topics, or focus on entities they are interested in, and discover the "facts" behind the changes in a story. To test our methods we created two corpora comprising of a time-stamped set of documents and a set of editor-created "ground-truth" reference sentences. The developed tool and test procedures aim to answer following research questions: (Q1): Does the proposed graphical representation improve user comprehension and navigation of stories? (Q2): Can the proposed burst discovery algorithm based on local patterns be used for story tracking? (Q3): How can different bursty-pattern representation be used for discovering the underlying "facts" behind the changes in document sets? (Q4): How can users benefit from methods and interfaces for story tracking? (Q5): How should the documents be ordered for story tracking?
The main contributions of our so far carried out research to answer these questions are: (a) model story tracking as an interactive task [?], (b) define an evaluation framework for TTM methods, and (c) build tools for supporting story tracking [?]. The developed tool1 and the new evaluation techniques, will let us carry out novel user experiments.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process; H.1.2 [User/Machine Systems]: Human information processing
General Terms
Human Factors, Experimentation
Keywords
temporal text mining, visualization, evaluation
1. REFERENCES
[1] I. Subasi´c and B. Berendt. Discovery of interactive graphs for understanding and searching time-indexed corpora.Knowledge and Information Systems. DOI: 10.1007/s10115-009-0227-x
[2] B. Berendt and I. Subasi´c. Stories in time: A graph-based interface for news tracking and discovery. In WI-IAT'09 Workshops, 2009. IEEE Computer Society.
1beta version: http://sites.google.com/site/subasicilija/

920

Reverse Annotation Based Retrieval from Large Document Image Collections
Pramod Sankar K.
Center for Visual Information Technology, IIIT-Hyderabad, India
pramod_sankar@research.iiit.ac.in

ABSTRACT

jnj-aanamu

A number of projects are dedicated to creating digital libraries from scanned books, such as Google Books, UDL, Digital Library of India (DLI), etc. The ability to search in the content of document images is essential for the usability and popularity of these DLs. In this work, we aim toward building a retrieval system over 120K document images coming from 1000 scanned books of Telugu literature. This is a challenge because: i) OCRs are not robust enough for Indian languages, especially the Telugu script, ii) the document images contain large number of degradations and artifacts, iii) scalability to large collections is hard. Moreover, users expect that the search system accept text queries and retrieve relevant results in interactive times.
We propose a Reverse Annotation framework [1], that labels word-images by their equivalent text label in the offline phase. Reverse Annotation applies a retrieval based approach to recognition. Unlike traditional annotation/recognition that identifies keywords for data, Reverse Annotation identifies data that corresponds to a given keyword. It first selects a set of keywords which are considered useful for labeling and retrieval, such as those that repeat often, and ignoring stopwords and rare-words. Exemplars are obtained for each word from a crude OCR or human annotations. The labels are then propagated across the rest of the collection by matching words in the image-feature space. Since such a matching is computationally expensive, scalability is achieved using a fast approximate nearest neighbor technique based on Hierarchical K-Means. Once text labels are assigned, each document image is considered a bag-of-words over the labeled keywords. A standard search engine is used to build a search index for quick online retrieval. An example query and the retrieved results are shown in Figure 1. We are unaware of any conventional OCRs which can retrieve such images for the given query.
There are three major contributions of our work: i) recognizing the entire document collection together, instead of one-at-a-time; this means that the repetition of words in the test set is effectively used for improving accuracy, ii) speeding up recognition by clustering multiple instances of a given word, iii) recognising at the word-level, avoiding the pitfalls of character segmentation and recognition. Other OCR techniques that use word-level context still rely on inaccurate component-level classification.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: The retrieved word-images for an example query.
These words are hard to recognize with a conventional OCR.
Despite heavy degradations, our framework correctly re-
trieves document-images containing the query.
Using the techniques developed from this work, we were able to successfully build a retrieval system over our challenging dataset. To the best of our knowledge, this is the largest collection of document images that has been made searchable for any Indian language. Our algorithm is easily scalable to larger collections, and directly applicable to documents from other language scripts.
The first issue to discuss, is the fraction of word-images that remain unrecognized at the end of the Reverse Annotation phase. Rare-words, nouns etc. are not labeled in the test set. It is important to estimate the cost of not being able to answer such queries. If this cost is indeed high, we need to explore methods to label such infrequently occurring words in the collection. Needless to say, such methods should be computationally efficient without compromising on accuracy.
The other major issue to discuss is the evaluation of retrieval results. The true recall of the retrieval system cannot be computed, since it is impossible to identify every occurrence of the given query in such large data. Questions to be considered include: whether precision alone is a sufficient indicator of retrieval performance; whether there is some better document-level effectiveness assessment possible; and how best to estimate the relative satisfaction of the user's information need.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Miscellaneous
General Terms: Algorithms.
Keywords: Document Images, Recognition-free, Scalability.
1. REFERENCES
[1] Pramod Sankar, K. and C. V. Jawahar. Probabilistic Reverse Annotation for Large Scale Image Retrieval. In CVPR, 2007.

921

Aiming for User Experience in Information Retrieval

Towards User-Centered Relevance (UCR)

Frans van der Sluis
Human Media Interaction, University of Twente P.O. Box 217, 7500AE
Enschede, The Netherlands
f.vandersluis@utwente.nl

Betsy van Dijk
Human Media Interaction, University of Twente P.O. Box 217, 7500AE
Enschede, The Netherlands
bvdijk@ewi.utwente.nl

Egon L. van den Broek
Human Media Interaction, University of Twente P.O. Box 217, 7500AE
Enschede, The Netherlands
vandenbroek@acm.org

Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Human Factors, Experimentation, Algorithms.
Keywords: User Experience, Positive Affect, Relevance.
Research Proposal
The goal of an Information Retrieval (IR) system is to solve the information need of its user. Research on how this goal can best be achieved has mainly been dominated by the concept of relevance. What is relevant or not is generally performed by domain experts on the basis of topical similarity; i.e., topicality. Several studies have shown there is more to relevance than topicality; e.g., topicality, novelty, reliability, understandability, and scope [3]. As [2] concludes: "relevance, in the wider context, is a subjective, multidimensional, dynamic and situational phenomenon" (p. 63). The multi-faceted notion of relevance pleas for a human-centered approach. Essentially, an IR system should solve the information need of its user, with its user.
To enable the operationalization of the pivotal role of the user in solving the information need, we adopt a framework of User eXperience (UX). UX is a fuzzy concept, often defined as technology use beyond its instrumental value (e.g., topicality for IR). Several aspects of UX have been identified; e.g., usability, beauty, hedonic, emotions, temporality, situatedness, enjoyment, motivation, and challenge. Together, these aspects explain part of the UX [1] and are intrinsically related to persistence and effort in information problem solving. Hence, we hypothesize that solving an information need is fostered with an enhanced UX. We propose to focus on emotional factors: addressing the antecedents and consequences of, ideally, positive emotions.
At least two clear lines of research on emotion in IR can be identified. One line of research shows the effect of difficulty (or challenge) compared to the skills of the user. Namely, experienced difficulty leads to negative emotions. A second line of research is occupied with reading the emotional value of a text, image, or video. The emotional value of an Information Object (IO) can be considered the most direct antecedent to emotional experience. Accordingly, we pro-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

pose a retrieval model, which adjusts the (ranking of the) search results in a way optimal to the UX by evaluating the emotional value and difficulty of an IO; i.e., User-Centered Relevance (UCR).
The goal of the proposed research is to find an optimal relationship between UCR and topicality T , understandability U , and emotional value E. For this, a set of (textual) features is suggested, indicative of topicality T , complexity (readability C1, entropy C2, semantic coherence C3), and emotional value (emotional keyword spotting E1, lexical affinity E2). This leads to the following models:

R  {T, U, E} U  {C1...3} E  {E1,2}

UCR; (1) Understandability; (2)
Emotional value. (3)

In these models the user plays a central role: for topicality T , this is through the query; for emotional value E, the user's preferences will influence the final experienced emotion, and the understandability U is not only dependent upon the complexity C of the IO but also on the skills and knowledge of the user.
In order to create models 1-3, data is needed about when a document is perceived as understandable and is of positive emotional value as well as how this relates to UCR. Although some of the aspects of UCR (e.g., understandability) can be added directly to the interaction, we propose user studies to find the relative contribution of the different aspects to UCR. The suggested approach is to let a diverse set of stimuli (IOs) and searches be rated by subjects. One of the most salient challenges is in having a representative set of IOs and queries, and to cope with the difference between users and situations.

Acknowledgment
This work was part of the PuppyIR project, which is supported by a grant of the 7th Framework ICT Programme (FP7-ICT-2007-3) of the European Union.

References
[1] M. Hassenzahl and N. Tractinsky. User experience - a research agenda. The American Journal of Psychology, 25(2):91­97, 2006.
[2] I. Ruthven. Integration approaches to relevance. In A. Spink and C. Cole, editors, New Directions in Cognitive Information Retrieval, pages 61­80. Springer Netherlands, 2005.
[3] Y. C. Xu and Z. Chen. Relevance judgment: What do information users consider beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57(7):961­973, 2006.

924

Sponsors

SIGIR 2010 Sponsors & Supporters

Organizers

Supporters Gold

Silver Bronze Friends Exhibitors

xxviii

Preface from the General Chairs

Welcome to the 33rd edition of the ACM SIGIR conference on research and development in information retrieval. SIGIR is the premier international forum for the presentation of new research results and the demonstration of new systems and techniques in the broad field of information retrieval.
Welcome also to Switzerland! We are honored to be hosting SIGIR for the first time in the Latin part of Switzerland. Last time SIGIR was held in Switzerland was 14 years ago, with SIGIR'96, in Zürich.
Geneva is a global city at the crossing between France, Italy and Switzerland. It is also between mountains, lake and forests. Its international feeling and advantageous location makes it a beautiful place to visit. We hope that you enjoy your stay in Geneva.
You can ask everybody who did it before us and they will confirm you that organizing SIGIR is a pleasure, but also a big challenge. Although it is our names, and those of the PC chairs, that make it to the cover of the proceedings, we should not forget the tremendous work done by many other people. In fact, SIGIR 2010 is 2 General Chairs, 3 Program Chairs, 15 other Chairs, 40 Senior PC members, 472 PC members, and 54 additional reviewers. This conference would not have been possible without their work. Although you can find the names of all these people listed in these proceedings, we want to explicitly thank some of them. First and foremost, we thank those responsible for the technical program: our PC chairs, Hsin-Hsi Chen, Efthimis Efthimiadis, and Jacques Savoy, who tirelessly directed the work of the senior PC members, our posters and demo chairs, Peter Bruza, Gabriella Pasi and Ellen Voorhees, our tutorial chair, Djoerd Hiemstra, and our workshop chairs, Omar Alonso and Giambattista Amati. Also, following the success of recent editions, SIGIR 2010 program includes an Industry Track too, organized by David Harper, and Peter Schäuble.
Finally, our thanks go to many other individuals who worked largely behind the scenes to ensure that SIGIR 2010 would be a success, including our sponsorship chair, Dawei Song, our publicity chair, Fazli Can, our mentor chair, Ian Ruthven, the doctoral consortium chairs, Doug Oard and Paul Thomas, and the chair of best paper award, Norbert Fuhr.
SIGIR 2010 is, of course, sponsored by ACM, but has also received generous support from numerous companies and organizations. This enabled us to keep the registration affordable, even in a notoriously expensive place like Switzerland. Baidu, Google, Microsoft Research and Yahoo! Research were particularly generous with support at the "gold" level; the Information Retrieval Facility and Yandex provided support at the "silver" level; and IBM Research, the Special Interest Group on Information Systems of the Swiss Informatics Society (DBTA-SI) and the Swiss National Science Foundation (SNSF) supported the conference at the "bronze" level. Support has also come from Wolfram Research and the Universities of Neuchatel and Lugano, and most importantly from the University of Geneva, in particular in allowing us to accommodate SIGIR 2010 within its premises of Uni-Mail at the center of Geneva.
In addition, Cambridge University Press, Elsevier, Morgan & Claypool and Springer have each made arrangements to exhibit their publications at the conference.
We hope you will find the technical and social programs interesting and enjoyable and that you will take the opportunity to meet old friends and make new ones in the beautiful surroundings of Geneva.

Fabio Crestani SIGIR 2010 General Chair University of Lugano

Stéphane Marchand-Maillet SIGIR 2010 General Chair University of Geneva

iii

Preface from the Program Committee Chairs
Welcome to the 33rd ACM SIGIR International Conference of Research and Development on Information Retrieval. SIGIR 2010 has attracted a record-breaking number of papers signalling once again the importance of information retrieval research. We continue to see a steady growth of research output as well as a growing diversity of subjects in our field, where emerging topics, such as learning to rank, social media search, query logs analysis, recommender systems or advertising and search, are now reaching a relative maturity. This year we observe a continued interest in foundational aspects of IR, such as IR theory and evaluation studies, and also a growing interest on traditional topics, such a clustering and classification. If we want to summarize this SIGIR conference with a single word or phrase, we can suggest "users" or "users and queries" indicating the importance of users in search. But, we will let you discover this for yourself while delving through these conference proceedings.
There were 520 full paper submissions representing the work of IR researchers in more than 39 countries. Of these, 87 (16.7%) were accepted, representing the different geographic areas as follows: 42 from the Americas, 25 from Europe ­ Africa and 20 from Asia - Pacific. In addition to the full papers, a further 5 were offered the opportunity of presentation as posters. There were 90 (30.7%) posters, 10 (50%) demonstrations, 11 (52%) tutorials and 9 (50%) workshops accepted for inclusion in the technical program. A doctoral consortium with 11 PhD candidates is also part of the technical program. It is worth noting that more than half of the submitted papers (293, or 56%) have a student as the first author, an encouraging sign for the growth and vitality of the IR community. We are grateful to the keynote speakers Donna Harman from NIST, and Gary Flake from Microsoft, who agreed to share their ideas with the community.
The organization of a SIGIR conference is a demanding multi-year task, and its success depends on the dedicated effort of many people. We are grateful to the conference general chairs Fabio Crestani and Stéphane Marchand-Maillet for running SIGIR 2010. We are also grateful to the other chairs who helped create the SIGIR program: Peter Bruza, Gabriella Pasi, Ellen Voorhees, the posters and demonstration chairs; Omar Alonso and Giambattista Amati the workshop chairs; Djoerd Hiemstra the tutorials chair; Douglas W. Oard and Paul Thomas, managed the doctoral consortium; Norbert Fuhr chaired the best paper award committee; Ian Ruthven coordinated the mentoring program; and David Harper and Peter Schäuble for organizing the industry day. We also thank the SIGIR Executive for their guidance throughout this process.
The contributions in these proceedings represent a diverse and comprehensive coverage of IR research. The selection of quality contributions for the SIGIR conference is dependent on a two tier double blind reviewing process. We are indebted to the program committee comprising of 490 reviewers and 40 senior program committee (SPCs) members. Each reviewer was assigned up to 6 papers by the PC chairs and the SPCs in accordance to reviewers' stated subject expertise and each paper was allocated three reviewers. In cases where there was a wide range of scores or incomplete information, several primary reviewers helped us out with a number of additional reviews. The role of the SPCs was to oversee the process for their topic area, by resolving disagreements between reviewers and producing a meta review for each paper drawn from the primary reviews. The meta reviews served as the basis of discussion at the online Programme Committee meeting. Most area coordinators were responsible for 15 or more papers / meta reviews. SPCs were selected for their subject expertise in the different topic areas and attention was also paid to geographic representation and PC Committee experience. All the reviewing was double blind with the identity of authors being released only after the selection of papers was completed. The overall effort involved more than 2185 people as authors, reviewers, SPCs, and chairs. The volume of papers this year stressed the limits of everyone's time and we are grateful to the reviewers for their efforts to get reviews in on time, and to the SPCs for leading discussions, creating meta-reviews, and participating both virtually and in person at the online PC meeting. We are also thankful to Thomas Preuss for his support and responsiveness, which has ensured that the ConfMaster reviewing system run smoothly throughout the entire reviewing process. Finally, we acknowledge, and are inspired by, the commitment and fervent engagement of our research community towards the success of SIGIR.

Hsin-Hsi Chen National Taiwan University Asia & Pacific

Efthimis N. Efthimiadis University of Washington Americas

Jacques Savoy University of Neuchatel Africa & Europe

iv


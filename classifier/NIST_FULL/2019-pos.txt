Session 3C: Fact-checking, Privacy and Legal

SIGIR '19, July 21­25, 2019, Paris, France

Privacy-aware Document Ranking with Neural Signals

Jinjin Shao, Shiyu Ji, Tao Yang
Department of Computer Science, University of California Santa Barbara, California

ABSTRACT
The recent work on neural ranking has achieved solid relevance improvement, by exploring similarities between documents and queries using word embeddings. It is an open problem how to leverage such an advancement for privacy-aware ranking, which is important for top K document search on the cloud. Since neural ranking adds more complexity in score computation, it is difficult to prevent the server from discovering embedding-based semantic features and inferring privacy-sensitive information. This paper analyzes the critical leakages in interaction-based neural ranking and studies countermeasures to mitigate such a leakage. It proposes a privacy-aware neural ranking scheme that integrates tree ensembles with kernel value obfuscation and a soft match map based on adaptively-clustered term closures. The paper also presents an evaluation with two TREC datasets on the relevance of the proposed techniques and the trade-offs for privacy and storage efficiency.
ACM Reference Format: Jinjin Shao, Shiyu Ji, Tao Yang. 2019. Privacy-aware Document Ranking with Neural Signals. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331189
1 INTRODUCTION AND RELATED WORK
There is a growing demand for privacy protection in Internet or cloud-based information services [14, 26]. While searchable encryption (e.g. [10, 11, 15, 34, 35]) has studied secure document matching without ranking consideration, privacy for document ranking is addressed in [1, 8, 49, 53] for linear additive scoring and in [30] for tree ensembles. On the other hand, there is a significant advancement in neural ranking methods and it is an open problem to develop privacy-aware ranking leveraging neural models.
The previous research on neural ranking falls into the following two categories: interaction-based or representation-based models [42]. The earlier work has focused on the representation-based models [24, 47] where each document and a query are separately represented as vectors through neural computation and the final ranking is based on the similarity of the two representative vectors. The recent studies have focused on interaction-based neural ranking models [16, 22, 54] where the word or term-level similarity
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331189

of a query and a document is explored first based on their embedding vectors before applying additional neural computation. These studies have shown their interaction-based models outperform the earlier representation-based models and thus our paper addresses privacy issues for three interaction-based models, more specifically DRMM [22], KNRM [54], and CONV-KNRM [16].
Ranking requires arithmetic calculations based on feature vectors and homomorphic encryption [20, 41] is one idea offered to secure data while letting the server perform arithmetic calculations without decrypting the underlying data. But such a scheme is still not computationally feasible when many numbers are involved, because each addition or multiplication is extremely slow, meanwhile homomorphic encryption does not support the ability of comparing two results required by ranking. Neural ranking involves more computational complexity than a linear method or tree ensembles, and hiding feature computation becomes even harder.
Recent secure neural net research [33] addresses image classification using homomorphic encryption and two-party communication with garbled circuits, to meet a different privacy requirement (clients obtain predicted results without knowing the server decision model). The online processing time can cost 3.56 seconds for classifying each image and in addition, 296MB of data needs to be communicated between a client and a server for each image. While computed scores are still un-comparable at the server side, the cost is too expensive for ranking many documents, considering each document vector as an image vector. Order-preserving encryption techniques (e.g. [3, 45]) let a server compare the encrypted results but do not support arithmetic computation on encrypted numbers. There is a line of work perturbing feature values (e.g. [28]) for classification to achieve differential privacy. Our method is aimed at document search with a goal of preserving the exact ranking model and our design uses one round of client-server communication for faster response time. Problem Statement. This paper investigates how privacy consideration can be incorporated efficiently in neural ranking for top K cloud data search. To our best knowledge, this paper is the first effort to address privacy-aware interaction-based neural ranking. In specific, we identify statistical document information such as word frequency and occurrence that can be leaked during neural computation. Such information is required for a number of privacy attacks studied in the previous work [9, 27, 52]. To mitigate such a leakage, our techniques replace the exact kernel value with a privacy-aware tree ensemble model [7, 25, 30, 37]. We further propose a soft match map that captures non-exact similarity signals above a threshold while providing a privacy protection using term closures and kernel value obfuscation. Our evaluation using two TREC datasets shows the relevance of the proposed tree integration can even exceed the original baselines for NDCG scores when soft match maps are not used. There is a relevance trade-off when incorporating soft match maps.

305

Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France

2 BACKGROUNDS AND PROBLEM SETTINGS
Problem definition and feature vectors. The problem of top K document ranking is defined as follows: given a query q with multiple terms and candidate documents, the server forms a feature vector for each document and ranks these documents. A ranking feature is called raw if it is explicitly stored (in the posting list) associated with each document, and it is called composite if it is computed dynamically based on raw features. Examples of raw features directly used for ranking include frequency of text words appearing in a document, and the quality score of a document. Document ranking uses both raw and composite features, which can be query-dependent and may not be precomputed in advance before query time. An example of composite features is BM25 [32] which is the summation of term frequency based raw features. Learning-to-rank algorithms. Given a matched document represented by a set of raw and/or composite features, a linear ranking model uses a linear combination of document features while a treebased ensemble produces a set of decision trees using a boosting or bagging strategy [7, 25, 37].
An interaction-based neural ranking [16, 22, 54] can be formalized as performing the following computation flow:

RankingScore = N N (Ker (qì  dì)),

where qì and dì are two sequences of embedding vectors which can be representations for a unigram or a n-gram in a query and a document [16, 22, 54], or can be entity embeddings for existing
entities in a query and a document [55]. Those embedding vectors
can be learned from one of many existing neural network models
such as Word2Vec [39], GloVe [44] and relevance based word embedding [56]. Embedding vectors for n-grams can be generated by a convolution operation described in [16]. N N is a forward neural network to compute the final ranking score.
Operator  is the interaction between query q and document d and its output is the similarity of a query term and a document term for all possible pairs from q and d. In [16, 22, 54], cosine similarity is used for measuring term similarity with a score varying from -1 to 1. Let t, w denote the cosine similarity between the term vector of t and that of another term w.
Operator Ker represents the kernel value calculation, extracts term-level matching signals based on the similarity of all term
pairs from a query and a document, and generates a vector of real
values being taken as input for the forward neural computation. There are two methods for kernel computation. In the Histogram Pooling [22] method, there are R kernels and each kernel associates with an interval within [-1, 1], e.g., [0.5, 0.6). The kernel value of the j-th kernel is the number of similarity values that fall into the j-th interval [bj , bj+1]: Kj (t, d) = w d 1bj  t,w <bj+1 . For kernel pooling, we follow a definition in [16, 54] that each kernel
associates with a Radial Basis Function (RBF). The RBF kernel for the j-th kernel is defined by µj and j . Symbols µj and j denote the mean and standard deviation respectively. Let exp be the natural
exponential function and log be the natural logarithm function. The kernel value of the j-th kernel for a query term t is:

Kj

(t ,

d)

=

w

d

exp(-

(t ,

w -

2

2 j

µj

)2

).

SIGIR '19, July 21­25, 2019, Paris, France Jinjin Shao, Shiyu Ji, Tao Yang

The output of kernel computation is a kernel vector of size R:

( log K1(t, d), · · · , log KR (t, d))T .

t q

t q

Privacy requirement and threat model. A client owns all data and wants to outsource the search service to a cloud server which is honest-but-curious, i.e., the server will honestly follow the client's protocol, but will also try to learn any private information from the client data. The client builds an encrypted but searchable index and lets a server host such index. This paper does not consider the dynamic addition of new documents to the existing index, assuming the client can periodically overwrite the index in a cloud host server to include new content. To conduct a search query, the client sends several encrypted keywords and related information to the server. Our design only uses one round of client-server communication since multi-round active communication between the server and client (e.g. [23, 40]) incurs a much higher communication cost and response latency.
The biggest threat is the leakage of query and document plaintext. A server can also be interested in query access pattern and statistical information even if the query terms are encrypted. Finally the result patterns such as the overlapping of document IDs in multiple queries may also be interesting. This paper is focused on providing a privacy protection to avoid the leakage of document plaintext and also important feature values during ranking process.
The previous works on plaintext or query attacks in [9, 27, 52] assume the adversary knows partial information on term occurrence in addition to a subset of plaintext documents. By preventing the leakage of the term occurrence information of documents in a hosted dataset, threats from these attacks can be removed or greatly alleviated. Islam et al. [27] proposed the query recovery attack called IKK which can be revised to launch a plaintext attack to identify some words in an encrypted document collection. This assumes that the adversary is the server who has some prior knowledge as follows. 1) The server knows plaintext of na words that appear in this document collection, but does not know the encrypted word IDs. 2) The server knows co-occurrence probabilities of these na words in this document collection. This can be approximated by using a public dataset. 3) The server has obtained encrypted IDs of documents that contain a subset of known na words. With the above three pieces of information, the server is able to recover the encrypted word IDs for a good percentage of these na words, and detect the set of document IDs containing these encrypted word IDs. Cash et al. [9] has improved the plain text recoverability of the IKK attack with extra information such as term frequency, and pointed out that the inverted index or occurrence probabilities for a set of words can be inferred when knowing the term frequency of English words in each document. There are also attacks exploiting leaked document similarities [52]. Their attacks only work if the adversary knows occurrence frequency and co-occurrence frequency of selected terms in the entire document set.
Since all of the above attacks require term occurrence and use term frequency if possible, this paper will analyze the information leakage of interaction-based neural ranking methods on term frequency and occurrence in documents, and extend or redesign some of their components with a goal of hiding such statistical text information. It is worthy to note that some advanced techniques, e.g.

306

Session 3C: Fact-checking, Privacy and Legal Privacy-aware Document Ranking with Neural Signals

SIGIR '19, July 21­25, 2019, Paris, France SIGIR '19, July 21­25, 2019, Paris, France

ranking based on Convolutional Neural Network [43], require term
positions and such information can be leaked during computation.
Then term occurrences in a document can be easily inferred by a
server. Thus this paper does not investigate such ranking models.
3 LEAKAGE ANALYSIS AND DESIGN CONSIDERATIONS
We first examine the possible leakage of information in interaction-
based neural ranking in terms of term occurrence and frequency. Hiding term vectors. As mentioned above, there are three steps in the interaction-based model: interaction between query and doc-
ument terms, kernel value calculation, and forward neural network
computation. Our first thought is to hide term vectors using a hash-
ing function while preserving cosine similarities (or other similarity
metrics) between vectors. Such a protection can mask identities of
term vectors; however, if a server is allowed to observe the result of interaction between a query term t and each term w of document d, it can easily infer frequencies and occurrences of all query terms as follows: T F (t, d) = w d 1t,w =1, where 1t,w =1 equals to 1 if t, w = 1 and 0 otherwise.
Given the fact that we cannot store masked term vectors or
explicitly store the interaction matrix elements, we resort the fol-
lowing strategy where the result of interaction between query and
document terms is not computed by the server. Kernel-level protection. Our next design idea is to provide a kernel-level protection of privacy by precomputing kernel values in advance. Thus only the output of Ker is exposed to the server, which hides the vector computation process and the result of in-
teraction. Namely the owner of the dataset (as the client in our case) precomputes this kernel vector first for each term t that may interact with a document d, fìt,d = (a1, , · · · , aj , · · · , aR )T where aj = log Kj (t, d). These vectors, after precomputed by a client, are uploaded and stored in a server. During the run time with a query q, then the kernel vector for this query can be constructed as t q fìt,d . Then such a kernel vector is injected as an input to the forward neural computation step. Leakage from kernel vectors. Unfortunately as we analyze below, the above kernel-level protection can still leak term frequency,
which yields the leakage of term occurrence.
Notice for both histogram pooling and kernel pooling, the last kernel KR (t, d) is a special kernel representing the exact match of a query term with document terms. Without loss of generality, let this special one be the R-th kernel in this paper. For histogram pooling, an interval defined as [1, 1] is associated with this special kernel. This means that last element aR in fìt,d gets updated by one whenever a query term t exactly matches a document term in d. As a result, the server can infer the term frequency of a query term in any document by observing the result of aR .
Proposition 3.1. Given query term t and document d, in histogram pooling for the R-th kernel whose interval is [1, 1], term frequency TF(t, d) = w d 1t,w =1 = exp(aR ).
For the R-th kernel derived with kernel pooling [16, 54], µR is 1.0, and R is chosen to be a small positive real number, e.g., 0.001. We define the maximum cosine similarity between any two different terms in a vocabulary V where V is the collection of all terms in

the given dataset:

S¯ = max t, w .
t,w V ,t w

The following theorem shows a server can still approximate term frequency T F (t, d) using the value of last kernel aR .

Theorem 3.2. Given a query term t and a document d, in kernel

pooling for the R-th kernel, if S¯ < 1.0 -

2R2

ln

n 

,

then

|TF(t, d)

-

exp(aR )| < , where  is a small real value.

In our tested datasets in Section 6, S¯ is below 0.9. Using the

above theorem, condition S¯  0.947 < 1.0 -

2R2

ln

n 

is

true

when R  0.01,  = 0.01, and n  10, 000, and a server can easily

infer the frequency of a term in a document. Thus we are unable to

use aR and the next section will present a solution to address this.

It should be noted that the above analysis is true for computing

the interaction between a unigram query term with all unigrams of

a document in DRMM [22] and KNRM [54]. When computing the
interaction of a h-gram from a query and a -gram from a document where h  in CONV-KNRM [16], the cosine similarity of such a

pair cannot be 1. Thus for CONV-KNRM, we only need to worry

about the interaction of two terms with the same gram length.

4 PRIVACY-AWARE NEURAL RANKING
In this section, we propose three techniques for privacy-aware neural ranking: 1) replace the exact match kernel value with a traditional ranking method that uses exact word matching; 2) provide a soft match index (we call it soft match map) to include a set of kernel values with similar terms while enhancing privacy; 3) obfuscate kernel values in the soft match map.

4.1 Replacement of the Exact Match Kernel
Neural signals can be considered to be composed of two parts: exact match component represented by last kernel value KR (t, d) and the soft match component represented by K1(t, d), · · · , KR-1(t, d) as shown in Fig. 1(a). Since Theorem 3.2 indicates that the root cause of term frequency leakage is the inclusion of kernel value KR , we propose to drop this kernel value and compensate it by the including of a traditional ranking method that has better privacy protection, as illustrated in Figure 1(b).
We adopt a privacy-aware learning-to-rank tree ensemble model in [30] that encodes raw features with comparison preserving mapping (CPM) and derives a tree ensemble using encoded raw features. Raw ranking features mainly based on exact term matching are not leaked to the server. During our evaluation, these exact text matching features include BM25 for query words that appear in the title, BM25 for query words that appear in the body, and the proximity features [2, 19, 51, 57] with the minimum, maximum, and average of the squared min distance reciprocal of query word pairs in the title or in the body. We use word-pair or n-gram based features as a substitute to avoid composite proximity features based on word positions through arithmetic calculation. For ClueWeb09, extra raw features include PageRank and a binary flag indicating whether a document is from Wikipedia.
The above replacement is applied for computing the unigramto-unigram interaction in DRMM and KNRM. It is also applicable

307

Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France

Relevance score

Relevance score

Neural network computation

Neural network computation

......

......

Soft match kernels

Exact match kernel

Obfuscated soft match kernels

Private tree ensemble

Encrypted

(a)

(b)

raw features

Figure 1: Replacement of the exact match kernel

to the interaction of a h-gram query term with a h-gram document term for CONV-KNRM. We discuss more on this in Section 6. This replacement comes with two advantages. First, it removes the source of term frequency leakage in aR since an adversary is not able to recover feature values encoded with CPM [30]. Second, it can potentially boost ranking performance. Currently there is no known method to combine a traditional ranking method with a neural ranking model for a better relevance. A tree ensemble method has been proven to be effective before neural models gain more attention. For example, in the Yahoo! learning-to-rank challenge [13] in 2010, all winners have used some forms of tree ensembles. This replacement can provide a natural way to effectively combine a tree ensemble with a representation based neural model and we will evaluate the relevance impact in Section 6.

4.2 Obfuscation of Kernel Values
Even though we have precomputed the kernel value computation to avoid the leakage of term frequency to the server, there still exists a term frequency attack described in Appendix A when the histogram pooling is used. In that attack, the frequency of a term queried can be uncovered by a server if it is able to find all or many of encrypted keys (t, d) where t is a soft or exact term of document d in a soft match map. While we have not found a term frequency attack for the kernel pooling, we still want to be cautious.
To minimize the chance of leaking exact kernel values, we propose a many-to-one mapping to obfuscate kernel vector values. Intuitively, if kernel vector values from multiple different term document interactions are indistinguishable, the adversary has to make random guesses on real kernel vector values. In specific, we add the ceiling function to convert the floating point number to an integer in forming the kernel vector, and this change allows the revised soft match kernel values to accomplish k-anonymization [18, 50]. For the j-th element of a kernel vector based on R kernels,

aj =

logr (Kj (t, d)), 1,

if Kj (t, d) > 1, otherwise,

where the logarithmic base r is a privacy parameter that can be adjusted. As we show later in Section 5.2, the anonymous factor k is r R-1 - 1, since all zero kernel values are converted to 1. A large r value will add more anonymity for privacy while it may degrade
the relevance performance due to the lack of value differentiation
in kernel vectors.

SIGIR '19, July 21­25, 2019, Paris, France
Jinjin Shao, Shiyu Ji, Tao Yang
In DRMM, at the forward neural computation stage, the kernel values are re-scaled by document frequency weights of query terms. We use the same many-to-one function discussed above to obfuscate these weights. Our evaluation shows there is no visible relevance difference when r = 10.
4.3 Soft Match Maps
Kernel vector values are precomputed before query processing and such offline processing can be done efficiently on a parallel platform and/or with LSH approximation [31]. However, it is too expensive to store kernel vectors for all possible (t, d) pairs. For example, suppose there are R = 20 kernel values per term-document pair, for a dataset with 2M documents and a vocabulary of 250K terms, all these kernel vectors require 20 terabytes of space if each kernel value is stored using a reduced precision with 2 bytes. Although data compression may optimize this cost, the optimized storage cost is still excessively high.
Inspired by the inverted index, we propose a soft match index structure that contains kernel vectors of term-document pair (t, d) only if term t is reasonably related to document d, above a similarity threshold. This index data structure, called soft match map, has a key-value representation. The key of each entry is a hashed termdocument pair (t, d), and the value is kernel vector fìt,d generated from Ker for a term-document pair. Notice aR for R-th kernel is removed as discussed in Section 4.1 for interaction between a query term and a document term under the same gram length. We call a term for a document this map as exact term if this term appears in this document. If this term is not in this document but it is included in the map due to its similarity to another term in this document, we call this term as soft term of this document. We will access how a soft match map is accessed during search in Section 5.
Even though terms and documents in a soft match are encrypted and identified through numeral IDs, an adversary may infer the the occurrence of terms in a document which is a critical piece of information for privacy attack discussed in Section 2. In order to minimize the chance of leaking term occurrence in a document, we introduce the notion of  -similar term closure.
Definition 1. A set of terms C under vocabulary V is called a  -similar term closure if for any term t  C and there exists another term w  V such that t, w   , then w  C.
Here V is the collection of terms in a given dataset and we will discuss a clustering algorithm shortly that groups a set of terms as a term closure.
Definition 2. A soft match map SMM is closed under term closures if for any (t, d)  SMM, for any w in the same term closure of t, (w, d)  SMM.
There are two advantages of a closed soft match map. 1) From the relevance point view, a document that contains a word which is similar to a query word that can get some rank credit as the privacy-aware ranking only uses this soft match map to identify semantically related documents. 2) As shown in the next section, an adversary would have a hard time to detect if a word ID that appears in the document or not because other similar words in its term closure have all appeared in the soft match map. We will

308

Session 3C: Fact-checking, Privacy and Legal Privacy-aware Document Ranking with Neural Signals

SIGIR '19, July 21­25, 2019, Paris, France SIGIR '19, July 21­25, 2019, Paris, France

Car
0.734 0.726
0.715
Vehicle

0.279

0.524

Truck

Flatbed

0.305

Clustering with fixed threshold: C1: {Car, Truck, Vehicle, Flatbed, ...}
Adaptive clustering: C1: {Car, Truck, Vehicle}, C2: {FlatBed, ...}

Figure 2: Two clustering methods for term closure

analyze its privacy implication based on the notion of statistical indistinguishability in the next section.
In the rest of the paper, we will assume a soft match map is closed. We now describe how to partition a term vocabulary of a dataset into a disjoint set of term closures. There are trade-off factors to consider in controlling the size of term closures. With a larger size, the soft match map will accommodate more similar terms that can improve relevance, while creating more challenges for an adversary to distinguish and detect which term IDs in a soft match map appear in a document. On the other hand, a larger size demands more storage to host a soft match map. In the following, we discuss two algorithms that derive a disjoint set of term closures. Clustering with a fixed similarity threshold. Given a clustering threshold  , we cluster all terms in a closure C using a transitive closure computation as follows: if t  C, and t, w   , then w  C. This approach uses a uniform threshold for all clusters. With a small clustering threshold value, some clusters can have a very big size, which can result in a very large storage demand. With a large threshold value, some of term closures have a very small size, not big enough for the privacy purpose. Adaptive clustering with multiple thresholds and closure size control. Given p as a targeted closure size, and m sorted clustering thresholds 1 > 2 > · · · > m . We first apply a similarity clustering with a fixed threshold 1. We remove all clusters with the size no less than p. For the remaining terms, we apply a similarity clustering with a fixed threshold 2. Repeat this process until all cluster sizes are no less than p or we have applied clustering with all thresholds. This adaptive clustering provides a flexibility to group a sufficient number of similar terms in each closure while yielding a reduced storage demand. In our evaluation, p = 5 is used.
Figure 2 shows the difference of the above two clustering for a partial similarity graph of 4 terms from one of our testing datasets. The edges represent pairwise similarity scores. With fixed similarity threshold at 0.5, all four terms "Car", "Truck", "Vehicle", and "Flatbed" are clustered transitively into the same term closure. With adaptive clustering using a threshold set {0.9, 0.8, 0.7, 0.6, 0.5}, and closure target size 3, term "Flatbed" is not grouped with "Car", "Truck", and "Vehicle". This is because that when threshold 0.7 is used, terms "Car", "Truck", and "Vehicle" are grouped, reaching closure size 3 and they are removed to form a separate closure. "Flatbed" will then be grouped with other terms in the rest of the graph (which is not shown in this figure).

5 LEAKAGE AND PRIVACY OF SOFT MATCH MAPS
5.1 Search Process and Leakage Profile
The top K search process is described as follows. A client first sends randomized tokens for query terms and related information (called
trapdoor information [1]) to a server, and these terms include query

unigrams and/or multi-grams when needed. The server cannot map tokens into query terms since tokens are randomized. After the server receives tokens for all the query terms, a privacy-aware document retrieval model based on [1, 5, 6, 10, 11] produces a set of document candidates for further ranking. The server first computes the keys to access the CPM-encoded features and run a tree ensemble, where the leakage profile is studied in [1]. Then the server computes the keys to access the soft match map, and fetches associated kernel vectors to drive the forward neural computation.
Depending on the query processing semantics and privacy requirement, there are two methods to pass the query term list and document list to neural ranking, and each of which has a different leakage profile. The first method is based on the private search work of [1, 11]. Based on trapdoor information sent from a client, the server can compute and obtain a key (t, d) to access the soft match map. Essentially the server obtains a list of keys representing (t, d) pairs where t is a query term ID and d is a candidate document ID, but the server cannot decompose each key into two parts to obtain its term ID. Finally the server returns a ranked list of encrypted document IDs and these IDs are available in the map values of keys. For this case, we list the leakage profile as follows.
· Initially the server does not know any key (t, d) for the map. After processing queries, the server gradually learns more keys of the soft match map.
· Once the server knows a key (t, d), it can retrieve the soft kernel vector of this key and an encrypted document ID.
The second method is based on the work in [5, 6, 10], the server receives a list of query term IDs and a list of candidate documents ID. It then computes each key (t, d) and learns about term ID t and document ID d. For this case, there is additional leakage. · Gradually after processing more queries, the server learns keys
and kernel vectors, and also is able to build a partial soft inverted index for all queried terms. Based on the soft inverted index, the server is able to estimate partial document similarities based on the overlapping degree of soft terms between two documents. · Once all terms and documents are queried or processed, the server is able to build a complete soft forward index and soft inverted index. Namely give a list of documents that contain a soft term, and give a list of soft terms included in a document. By using the soft term postings, the server learns the membership information of each soft term closure.
Since the information listed above is slowly leaked as more queries are processed, we propose to re-index the dataset periodically and replace the index in the cloud with different term IDs during each update to mitigate the chance of letting the server exploit the entire soft inverted index. The next two subsections study the privacy properties with respect to exact term occurrence even if the entire soft index is leaked to a server adversary, since such information is required for the plaintext attacks discussed in the last part of Section 2. We will also discuss k-anonymity for kernel vector values.
5.2 k-anonymization of Kernel Value Vectors
We show our obfuscation mapping achieves k-anonymization (a standard notion from privacy literature [18, 50]) with respect to the kernel value Kj (t, d).

309

Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France

Definition 3. [50] Let V be a vector of real values representing R - 1 kernel values namely, V = [K1(t, d), K2(t, d), · · · , KR-1(t, d)]. Let V  = F (V ) where F is a transform function. V  is k-anonymous if and only if, for any V , there are at least k different V such that V  = F (V ). An algorithm F is called k-anonymization algorithm, if it outputs a k-anonymous vector V  for any soft matching signals vector V .
It is easy to show that the application of the above ceiling func-
tion to the above logarithmic mapping is k-anonymous. In the first group, there are r values 0, 1, 2, ..., (r - 1) mapped to 1, and in the k-th group, there are r k - r k-1 values r k-1, ..., r k - 1 mapped to k for k > 1. Since r  2, k-th group has rk - rk-1 > r (r - 1)  r values mapped to k, there are at least r values being mapped into the same value for each group. As there are (R - 1) kernels, given a sequence of logr (Kj (t, d)), there are r R-1 different sequences of Kj (t, d). Thus here we can confirm that there are r R-1 -1 sequences
of kernel values that the adversary cannot distinguish from the
real one. In our evaluation, we choose r = 10 and R = 20. Thus r R-1 = 1019, which is a very large number.
Proposition 5.1. The logarithmic mapping with ceiling obfuscation is k-anonymous with respect to soft match kernel values, where k = r R-1.

5.3 Obfuscation of Exact Term Occurrence

How strong a closed soft match map can be in avoiding the leakage of term occurrence? If an adversary including a server tries to detect an exact term of a document from a soft match map which contains both soft and exact terms with encrypted term IDs, we argue that other similar soft terms from the same closure behavior closely by looking at the structure and the kernel values in this soft match map for this document and the adversary should have a hard time to differentiate. We justify this argument based on the notion of statistical indistinguishability used in the cryptographic literature [4, 21].

Definition 4. -statistical indistinguishability. Any two dis-

tributions P and Q over a finite set U are -statistically indistin-

guishable if their statistical distance SD(P, Q)  , where statistical

distance is defined as [4, 21] SD(P, Q) = 1
2

x U |P(x) - Q(x)|.

If two distributions P and Q are -statistically indistinguishable,

then no adversary can successfully distinguish the samples from P

and

Q

with

probability

more

than

1 2

+



(Theorem

3.11

in

[4]).

For

our context, we define the distribution of a soft kernel value vector

as a discrete distribution over the soft kernels with respect to a given

term paired with a different document. In particular, given the ker-

nel vectors for documents d and d : fìt,d = (a1, , · · · , aj , · · · , aR )T ,

fìt , d 

=

(a , , ·
1

·

·

, aj,

·

·

·

, aR )T

,

where

each

ai

and

ai

may

be

obfus-

cated, the statistical distance between fìt,d and fìt,d is defined as

SD(fìt,d ,

fìt,d )

=

1 2

R-1 i =1

|ai

- ai |.

Definition 5. -statistically indistinguishable soft match

map. A soft match map is -statistically indistinguishable if for any document d and for any term w in d, for any document d  constructed

by replacing each term w in d with a subset of the closure C such that w  C, the soft kernel values of (w, d) and (w, d ) are -statistically

indistinguishable.

SIGIR '19, July 21­25, 2019, Paris, France
Jinjin Shao, Shiyu Ji, Tao Yang
In ClueWeb09 Dataset, using our algorithm where logarithmic base for kernel value obfuscation is r = 10, the derived soft match map is a -statistically indistinguishable with  being 0.004.
Theorem 5.2. Given a -statistically indistinguishable soft match map for document set D in which each term closure has at least p terms, if a server can derive the document occurrence of exact terms with N term-document pairs, there exist (2p - 1)N different document sets D~ such that the keys (term-document pairs) of its soft match map for D and D~ are identical, while soft kernel values of these two maps for corresponding terms are -statistically indistinguishable.
Theorem 5.2 shows that for any soft match map generated by a document set D, there are at least (2p - 1)N document sets D~ with different term occurrences and co-occurrences, while having very similar soft match maps. In [9], the adversary queries 150 single-word queries to launch an IKK attack. Assuming the average posting length is 100, the number of term-document pairs guessed for a document set D is N =15,000. If p = 2, then there are at least 315000  107157 document sets D~ to correspond to the guessed inverted index. The adversary has to choose the correct one from these 107157 options. Note that any two such options disagree on term occurrence for at least one document. Hence there are 107157 term occurrence profiles for the adversary to choose from, which is very unlikely to succeed.
6 EVALUATION
Here we evaluate relevance scores of the proposed privacy-aware neural ranking techniques using two TREC datasets and assess trade-offs of privacy and relevance, and storage and time cost. Datasets, features, and training. We use the following TREC test collections to do evaluations. 1) Robust04 uses TREC Disks 4 & 5 (excluding Congressional Records), which has about 0.5M news articles. 250 topic queries are collected from TREC Robust track 2004. 2) ClueWeb09-Cat-B uses ClueWeb09 Category B with 50M web pages. There are 150 topic queries from the TREC Web Tracks 2009, 2010 and 2011. Spam filtering is applied on ClueWeb09 Category B using Waterloo spam score with threshold 60. During indexing and retrieval, Krovetz Stemming [36] is used for both queries and documents.
Candidate documents with their encrypted feature vectors are retrieved from the inverted index built for the above datasets, following the work in [1, 12, 34]. For a privacy-aware tree ensemble, we use CPM [30] with LambdaMART based on RankLib 2.5 [17]. In each fold of training ranking model, 5-fold cross validation is used to select the best model based on NDCG@20, varying the number of leaves from 2 to 30 and the number of trees from 100 to 500.
To evaluate the impact of feature choices with the integration of the tree ensemble on the final ranking relevance, we have three options of features listed as follows.
1) G0 with term frequency features: BM25 scores for query terms in the title field, and BM25 scores for query terms in the body field of each document. TF-IDF scores for query terms in the title field, and TF-IDF scores for query terms in the body field of each document. 2) G1 with term frequency and proximity features: All features from G0, the squared minimum distance reciprocal of query term pairs in the title field, and the squared minimum distance reciprocal [2, 19, 51, 57] of query term pairs in the body

310

Session 3C: Fact-checking, Privacy and Legal
Privacy-aware Document Ranking with Neural Signals
field of each document. 3) G2 with term frequency, proximity, and page quality features: All features from G1, PageRank, and a binary flag indicating whether a document is from Wikipedia. This group is only for ClueWeb09 Category B.
The baseline models are DRMM, KNRM, and CONV-KNRM trained with 5-fold cross validation. We also choose a variant of CONV-KNRM, denoted by CONV-KNRM. For CONV-KNRM, we only use the interactions between query unigrams and document unigrams, between query unigrams and document bigrams, and between query bigrams and document unigrams. The interaction between query bigrams and document bigrams are not included to reduce storage space need. For both histogram pooling and kernel pooling, R=30 kernels are used. All soft match kernels are equally distributed in the cosine range. In kernel pooling,  is 0.10 for all soft match kernels. In CONV-KNRM, n-gram length is 2, and the number of CNN filters is 128 as used in the original work. All word embedding vectors are pre-trained, and are fixed in KNRM, CONVKNRM and CONV-KNRM. We use 300 dimension word embedding vectors trained on TREC Disks 4 & 5 or ClueWeb09 Category-Cat-B with Skip-gram + Negative sampling model [39]. All terms that appear less than 5 times are removed from embedding training.
We assess the use of the following 3 techniques denoted with T, O, and C where T stands for the replacement of the exact match kernel with LambdaMART/CPM, O stands for kernel value obfuscation, and C stands for using a closed soft match map. Notation A/T means ranking A with technique T while A/TOC means ranking A with all 3 techniques. All NDCG [29] values are within confidence interval ±0.01 with p-value < 0.05. Impact of replacing the exact match kernel with a LambdaMART/CPM tree ensemble. Table 1 shows the NDCG relevance of the three neural ranking models as the baseline and relevance after the replacement of the exact match kernel with LambdaMART/CPM based on the three groups of features G0, G1, and G2. Soft match maps and kernel value obfuscation are not incorporated. The boldfaced numbers are the highest NDCG scores within each ranking model. From this table we observe that neural ranking with the use of LambdaMART/CPM trees outperforms the original baseline in NDCG at all Positions 1, 3, 5, and 10 for ClueWeb. For example, compared with CONV-KNRM, CONV-KNRM/T with G2 can improve NDCG@1, NDCG@3, NDCG@5 and NDCG@10 by 2.23%, 2.45%, 2.67% and 4.23% on ClueWeb. For Robust04, tree ensemble integration delivers up to 3.91% improvement for CONVKNRM and up to 8.02% for KNRM, but degrades by up to -8.18% for DRMM. Comparing the use of G0, G1, and G2 for neural ranking integration, G2 is still most effective for ClueWeb and G1 is most effective for Robust04, which shows traditional signals still make a good contribution.
We examine NDCG scores reported in the previous work. For ClueWeb09-Cat-B, NDCG scores at Positions 1, 10 and 20 are 0.294, 0.289, 0.287 with CONV-KNRM in [16]. Our numbers are slightly higher, which can be caused by different data processing. Notice NDCG@20 in our run is 0.2950.
By comparing CONV-KNRM and CONV-KNRM, the absence of bigram-bigram interaction does yield a loss of NDCG score. For example, the loss is 8.56%, 6.29%, 7.04% and 6.96% for ClueWeb at Positions 1, 3, 5, and 10, respectively. That represents a tradeoff of privacy and relevancy. Adding the tree ensemble integration, most

SIGIR '19, July 21­25, 2019, Paris, France
SIGIR '19, July 21­25, 2019, Paris, France
NDCG scores for CONV-KNRM/T can be on par with those of CONV-KNRM and are 4.31% better for ClueWeb09 at Position 10. Impact of kernel value obfuscation. Table 2 shows the impact of kernel value obfuscation on NDCG scores incorporating LambdaMART/CPM with G2 for ClueWeb and G1 for Robust04. We choose two different logarithmic base r here: 5 and 10. Overall, the relevance with r = 5 is slightly better than r = 10, while both of them result in degradation in ranking accuracy compared with no obfuscation. For NDCG@1, the degradation with r = 10 is 1.7% for CONV-KNRM, 6.36% for KNRM, and 6.3% for DRMM. For CONVKNRM, its degradation is relatively smaller. Trade-offs between relevancy and storage efficiency. Table 3 studies the impact of using a closed soft match map with two clustering methods for term closures under different thresholds. This table is for CONV-KNRM/TOC only as this model delivers the highest NDCG scores with all privacy preserving techniques. For example, with the obfuscation base being 10, and the clustering threshold being 0.7, for DRMM/TOC in ClueWeb, NDCG@5 and NDCG@10 are 0.2704 and 0.2720, respectively. For KNRM/TOC in ClueWeb, NDCG@5 and NDCG@10 are 0.2972 and 0.2912, respectively. In Table 3, Column 6 in the middle shows the storage need in gigabytes to store a soft match map and related data after clustering with fixed thresholds while last column on the right is the storage demand with adaptive clustering. Each entry has two numbers X(Y). Y is the total storage for unigram-unigram interaction while X is the total storage for unigram-unigram, unigram-bigram, and bigram-unigram interaction. Number Y also represents the amount of storage space needed for KNRM and DRMM.
While clustering with a fixed threshold yields some relevance improvement over adaptive clustering, it requires an excessive amount of storage space. The adaptive clustering threshold 0.7 is a good trade-off with an acceptable storage space for hosting the ClueWeb dataset. In this setting, the relevance of CONV-KNRM is on par with the original CONV-KNRM baseline, lower than CONVKNRM/T. That represents a trade-off of relevancy, privacy and storage cost. It still requires 7.627TB space and we can use a number of high-end SSDs with parallel I/O. The latest high-end SSD products from Intel [48] and Samsung [38, 46] have achieved 1015µs IO latency with up-to 750K I/O operations per second. Thus for a soft match map hosted at a high-end SSD, the I/O access time of processing one query can still be reasonable. Estimation of online query processing time. The online query processing time cost consists of 3 phases: 1) private result retrieval and preliminary ranking, 2) private tree ensemble scoring , 3) neural ranking. Based on [1], Phase I costs 460 ms on average for ClueWeb. For re-ranking top 1,000 candidate documents, there are about upto 10K IO operations needed to fetch kernel vectors and features, and the total I/O time for accessing SSDs can take around 100 to 150ms with the above fast SSD performance parameters. Our experiments show that private tree ensemble scoring takes less than 2ms and all three neural models with TOC take about 10 ms or less. Thus overall the online query processing time is about 572ms to 622ms on average for ClueWeb. Notice that CONV-KNRM requires computation of term vectors and interaction matrices which could take 4-5 seconds. Thus even though our design pays extra cost in space, it does remove the expensive time spent for interaction computation.

311

Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France

SIGIR '19, July 21­25, 2019, Paris, France Jinjin Shao, Shiyu Ji, Tao Yang

Model

Table 1: Relevance impact of replacing exact match kernel with a tree ensemble

Feature group

ClueWeb09-Cat-B

Robust04

for ensemble NDCG@1 NDCG@3 NDCG@5 NDCG@10 NDCG@1 NDCG@3 NDCG@5 NDCG@10

LambdaMART/CPM DRMM DRMM/T KNRM KNRM/T
CONV-KNRM CONV-KNRM/T CONV-KNRM CONV-KNRM/T

G0 G1 G2
Baseline G0 G1 G2
Baseline G0 G1 G2
Baseline G0 G1 G2
G0 G1 G2

0.2498 0.2818 0.2893
0.2586
0.2635 0.2838 0.2887
0.2663
0.2736 0.3036 0.2999
0.3155
0.3031 0.3254 0.3225
0.2884
0.3038 0.3276 0.3175

0.2702 0.2725 0.2828
0.2659
0.2721 0.2778 0.2857
0.2739
0.2804 0.2974 0.3097
0.3124
0.3088 0.3187 0.3200
0.2927
0.2998 0.3099 0.3122

0.2571 0.2688 0.2873
0.2659
0.2623 0.2772 0.2822
0.2693
0.2798 0.2951 0.3154
0.3126
0.3154 0.3177 0.3210
0.2906
0.2962 0.3099 0.3239

0.2415 0.2653 0.2827
0.2634
0.2503 0.2645 0.2793
0.2681
0.2725 0.2903 0.3147
0.3085
0.3052 0.3099 0.3216
0.2870
0.2933 0.3117 0.3218

0.4819 0.5181
-
0.5049
0.4993 0.5114
-
0.4983
0.5158 0.5382
-
0.5373
0.5402 0.5556
-
0.5007
0.5149 0.5404
-

0.4465 0.4610
-
0.4872 0.4594 0.4658
-
0.4812
0.4908 0.5063
-
0.4875 0.5057 0.5042
-
0.4702
0.4827 0.5006
-

0.4257 0.4346
-
0.4747 0.4425 0.4501
-
0.4647
0.4768 0.4906
-
0.4742
0.4894 0.4927
-
0.4601
0.4768 0.4892
-

0.3982 0.4044
-
0.4528 0.4134 0.4158
-
0.4527
0.4592 0.4673
-
0.4586
0.4643 0.4693
-
0.4510
0.4535 0.4657
-

Table 2: Impact of kernel value obfuscation with different logarithmic bases

Model

Obfuscation

ClueWeb09-Cat-B

Robust04

base (r) NDCG@1 NDCG@3 NDCG@5 NDCG@10 NDCG@1 NDCG@3 NDCG@5 NDCG@10

DRMM/TO

Yes(10) Yes(5)
No

0.2703 0.2769 0.2887

0.2731 0.2757 0.2857

0.2740 0.2753 0.2822

0.2732 0.2722 0.2793

0.5078 0.5110 0.5114

0.4681 0.4669 0.4658

0.4449 0.4446 0.4501

0.4157 0.4221 0.4158

KNRM/TO

Yes(10) Yes(5)
No

0.2808 0.2875 0.2999

0.2929 0.2968 0.3097

0.2947 0.2988 0.3154

0.2906 0.2971 0.3147

0.5117 0.5100 0.5382

0.4639 0.4686 0.5063

0.4393 0.4451 0.4906

0.4130 0.4164 0.4673

CONV-KNRM/TO

Yes(10) Yes(5)
No

0.3121 0.3178 0.3175

0.3097 0.3067 0.3122

0.3165 0.3161 0.3239

0.3100 0.3100 0.3218

0.5221 0.5306 0.5404

0.4980 0.4987 0.5006

0.4906 0.4893 0.4892

0.4623 0.4613 0.4657

Table 3: NDCG score and storage demand for CONV-KNRM/TOC

Similarity

Clustering with fixed threshold

threshold NDCG@1 NDCG@3 NDCG@5 NDCG@10 Storage (GB)

Adaptive clustering NDCG@1 NDCG@3 NDCG@5 NDCG@10 Storage (GB)

Robust04

0.3

0.5225 0.4974 0.4915

0.4621

45,021 (2,144)

0.5127 0.4892 0.4845

0.4582

1,512 (72)

0.5

0.5154 0.4883 0.4780

0.4543

24,793 (1,181)

0.5078 0.4845 0.4756

0.4498

1,133 (54)

0.7

0.4886 0.4644 0.4486

0.4169

287 (13)

0.4899 0.4608 0.4414

0.4110

278 (13)

0.9

0.4953 0.4594 0.4415

0.4091

261 (12)

0.4913 0.4558 0.4397

0.4090

261 (12)

ClueWeb09-Cat-B

0.3

0.3136 0.3078 0.3149

0.3091 1.7 · 106 (82,308) 0.3052

0.3069

0.3142

0.3120 46,811 (2,231)

0.5

0.3073 0.3069 0.3114

0.3069 1.3 · 106 (61,877) 0.3056

0.3037

0.3113

0.3103 35,742 (1,705)

0.7

0.3064 0.3048 0.3122

0.3104

16,568 (792)

0.3067 0.3012 0.3088

0.3060 7,627 (366)

0.9

0.3069 0.3041 0.3105

0.3074

7,369 (354)

0.2963 0.3025 0.3117

0.3083 7,369 (354)

312

Session 3C: Fact-checking, Privacy and Legal
Privacy-aware Document Ranking with Neural Signals
7 CONCLUSION
The main contribution of this paper is a privacy-aware neural ranking scheme integrated with a tree ensemble for server-side top K document search. The key techniques include the replacement of
the exact kernel with a tree ensemble, a soft match map using obfus-
cated kernel values and term closures, and adaptive clustering for
term occurrence obfuscation and storage optimization. Our design
for privacy enhancement is to prevent the leakage of two critical
text signals in terms of term frequency and occurrence needed for
the attacks shown in the previous work and this paper.
The evaluation with two TREC datasets shows that the NDCG
can be improved noticeably by replacing the exact match kernel
of neural ranking with a LambdaMART tree ensemble. The obfus-
cation of kernel values does carry a modest relevance trade-off
for privacy. The adaptive clustering for term closures significantly
reduces the storage demand with some trade-off in relevance.
ACKNOWLEDGMENTS
This work is supported in part by NSF IIS-1528041 and a Google
faculty research award. It has used the NSF-supported resource
in the Extreme Science and Engineering Discovery Environment
(XSEDE) under allocation IRI190005. Any opinions, findings, con-
clusions or recommendations expressed in this material are those
of the authors and do not necessarily reflect the views of the NSF.
REFERENCES
[1] Daniel Agun, Jinjin Shao, Shiyu Ji, Stefano Tessaro, and Tao Yang. 2018. Privacy and efficiency tradeoffs for multiword top k search with linear additive rank scoring. In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee, 1725­1734.
[2] Jing Bai, Yi Chang, Hang Cui, Zhaohui Zheng, Gordon Sun, and Xin Li. 2008. Investigation of partial query proximity in web search. In Proceedings of the 17th international conference on World Wide Web. ACM, 1183­1184.
[3] Alexandra Boldyreva, Nathan Chenette, and Adam O'Neill. 2011. Orderpreserving encryption revisited: Improved security analysis and alternative solutions. In Annual Cryptology Conference. Springer, 578­595.
[4] Dan Boneh and Victor Shoup. 2015. A graduate course in applied cryptography. Draft 0.2 (2015).
[5] Raphael Bost. 2016. oo : Forward Secure Searchable Encryption. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 1143­1154.
[6] Raphael Bost and Pierre-Alain Fouque. 2017. Thwarting Leakage Abuse Attacks against Searchable Encryption ­ A Formal Approach and Applications to Database Padding. Cryptology ePrint Archive, Report 2017/1060. (2017). https://eprint.iacr.org/2017/1060.
[7] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.
[8] Ning Cao, Cong Wang, Ming Li, Kui Ren, and Wenjing Lou. 2014. PrivacyPreserving Multi-Keyword Ranked Search over Encrypted Cloud Data. IEEE Trans. Parallel Distrib. Syst. 25, 1 (2014), 222­233.
[9] David Cash, Paul Grubbs, Jason Perry, and Thomas Ristenpart. 2015. Leakageabuse attacks against searchable encryption. In CCS'15. ACM, 668­679.
[10] David Cash, Joseph Jaeger, Stanislaw Jarecki, Charanjit S Jutla, Hugo Krawczyk, Marcel-Catalin Rosu, and Michael Steiner. 2014. Dynamic Searchable Encryption in Very-Large Databases: Data Structures and Implementation.. In NDSS, Vol. 14. Citeseer, 23­26.
[11] David Cash, Stanislaw Jarecki, Charanjit S. Jutla, Hugo Krawczyk, Marcel-Catalin Rosu, and Michael Steiner. 2013. Highly-Scalable Searchable Symmetric Encryption with Support for Boolean Queries. In CRYPTO 2013. 353­373.
[12] David Cash and Stefano Tessaro. 2014. The Locality of Searchable Symmetric Encryption. In EUROCRYPT 2014. 351­368.
[13] Olivier Chapelle and Yi Chang. 2011. Yahoo! Learning to Rank Challenge Overview. J. of Machine Learning Research (2011), 1­24.
[14] Benny Chor, Eyal Kushilevitz, Oded Goldreich, and Madhu Sudan. 1998. Private Information Retrieval. J. ACM 45, 6 (Nov. 1998), 965­981.
[15] Reza Curtmola, Juan Garay, Seny Kamara, and Rafail Ostrovsky. 2011. Searchable symmetric encryption: improved definitions and efficient constructions. Journal

SIGIR '19, July 21­25, 2019, Paris, France
SIGIR '19, July 21­25, 2019, Paris, France
of Computer Security 19, 5 (2011), 895­934. [16] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional
neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 126­134.
[17] Van Dang. 2012. RankLib. https://sourceforge.net/p/lemur/wiki/RankLib/. (2012).
Accessed: 2018-05-20.
[18] Dotan Di Castro, Liane Lewin-Eytan, Yoelle Maarek, Ran Wolff, and Eyal Zohar. 2016. Enforcing k-anonymity in web mail auditing. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, 327­336.
[19] Tamer Elsayed, Nima Asadi, Lidan Wang, Jimmy J. Lin, and Donald Metzler. 2010. UMD and USC/ISI: TREC 2010 Web Track Experiments with Ivory. In Proceedings of the 19th Text REtrieval Conference, TREC 2010, Gaithersburg, Maryland, USA.
[20] Craig Gentry. 2009. Fully Homomorphic Encryption Using Ideal Lattices. In STOC '09. ACM, 169­178.
[21] Oded Goldreich. 2000. Foundations of Cryptography: Basic Tools. Cambridge University Press, New York, NY, USA.
[22] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval. In Proceedings of CIKM'16. ACM, 55­64.
[23] Haibo Hu, Jianliang Xu, Chushi Ren, and Byron Choi. 2011. Processing private queries over untrusted data cloud through privacy homomorphism. In ICDE. 601­612.
[24] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of CIKM'13. ACM, 2333­2338. [25] Muhammad Ibrahim and Mark Carman. 2016. Comparing Pointwise and Listwise Objective Functions for Random-Forest-Based Learning-to-Rank. ACM Transactions on Information Systems (TOIS) 34, 4 (2016), 20. [26] The Ponemon Institute. 2018. The 2018 global cloud data security study.
https://www2.gemalto.com/cloud-security-research. (2018). Accessed: 2018-
05-01.
[27] Mohammad Saiful Islam, Mehmet Kuzu, and Murat Kantarcioglu. 2012. Access
Pattern disclosure on Searchable Encryption: Ramification, Attack and Mitigation. In NDSS 2012. [28] Geetha Jagannathan, Krishnan Pillaipakkamnatt, and Rebecca N Wright. 2009. A practical differentially private random decision tree classifier. In 2009 IEEE International Conference on Data Mining Workshops. IEEE, 114­121. [29] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422­446.
[30] Shiyu Ji, Jinjin Shao, Daniel Agun, and Tao Yang. 2018. Privacy-aware Ranking with Tree Ensembles on the Cloud. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. ACM, 315­324.
[31] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural Ranking with Locality Sensitive Hashing. In The World Wide Web Conference (WWW '19). ACM, New York, NY, USA, 2858­2864.
[32] Karen Spärck Jones, Steve Walker, and Stephen E. Robertson. 2000. A probabilistic
model of information retrieval: development and comparative experiments. In Information Processing and Management. 779­840. [33] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018.
GAZELLE: A Low Latency Framework for Secure Neural Network Inference. In 27th USENIX Security Symposium (USENIX Security 18). 1651­1669. [34] Seny Kamara and Tarik Moataz. 2017. Boolean Searchable Symmetric Encryption with Worst-Case Sub-Linear Complexity. In Annual International Conference on the Theory and Applications of Cryptographic Techniques. Springer, 94­124. [35] Seny Kamara, Charalampos Papamanthou, and Tom Roeder. 2012. Dynamic searchable symmetric encryption. In Proceedings of the 2012 ACM conference on Computer and communications security. ACM, 965­976. [36] Robert Krovetz. 2000. Viewing morphology as an inference process. Artificial intelligence 118, 1-2 (2000), 277­294. [37] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331. [38] Chris Mellor. 2018. Samsung preps for Z-SSD smackdown on Intel Optane
drives. https://www.theregister.co.uk/2018/01/30/samsung_launching_zssd_
attack_on_intel_optane_drives. (2018). Accessed: 2019-01-28.
[39] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111­3119. [40] Muhammad Naveed, Manoj Prabhakaran, and Carl A Gunter. 2014. Dynamic searchable encryption via blind storage. In 2014 IEEE Symposium on Security and Privacy. IEEE, 639­654. [41] Pascal Paillier. 1999. Public-Key Cryptosystems Based on Composite Degree Residuosity Classes. In EUROCRYPT '99. 223­238. [42] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2017. A deep investigation of deep IR models. In SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17). [43] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng.
2017. Deeprank: A new deep architecture for relevance ranking in information

313

Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France

retrieval. In Proceedings of CIKM'17. ACM, 257­266. [44] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532­1543. [45] Raluca Ada Popa, Frank H. Li, and Nickolai Zeldovich. 2013. An Ideal-Security Protocol for Order-Preserving Encoding. In SP '13. IEEE Computer Society, 463­ 477.
[46] Samsung. 2018. Samsung Electronics Begins Mass Production of Industry's
Largest Capacity SSD - 30.72TB - for Next-Generation Enterprise Systems. https:
//bit.ly/2EFKp5N. (2018). Accessed: 2019-01-28.
[47] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014.
Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd International Conference on World Wide Web. ACM, 373­374.
[48] Lyle Smith. 2018. Intel Optane 800P NVMe SSD Review. https://www.
storagereview.com/intel_optane_800p_nvme_ssd_review. (2018). Accessed: 2019-
01-28.
[49] Wenhai Sun, Bing Wang, Ning Cao, Ming Li, Wenjing Lou, Y. Thomas Hou, and
Hui Li. 2014. Verifiable Privacy-Preserving Multi-Keyword Text Search in the Cloud Supporting Similarity-Based Ranking. IEEE Trans. Parallel Distrib. Syst. 25, 11 (2014), 3025­3035. [50] Latanya Sweeney. 2002. k-anonymity: A model for protecting privacy. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10, 05 (2002), 557­570.
[51] Tao Tao and ChengXiang Zhai. 2007. An exploration of proximity measures in information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 295­302.
[52] Guofeng Wang, Chuanyi Liu, Yingfei Dong, Kim-Kwang Raymond Choo, Peiyi
Han, Hezhong Pan, and Binxing Fang. 2018. Leakage Models and Inference Attacks on Searchable Encryption for Cyber-Physical Social Systems. IEEE Access 6 (2018), 21828­21839.
[53] Zhihua Xia, Xinhui Wang, Xingming Sun, and Qian Wang. 2016. A secure and dynamic multi-keyword ranked search scheme over encrypted cloud data. IEEE Transactions on Parallel and Distributed Systems 27, 2 (2016), 340­352.
[54] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 55­64.
[55] Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie-Yan Liu. 2018. Towards
Better Text Understanding and Retrieval through Kernel Entity Salience Modeling. In The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '18). ACM, 575­584. [56] Hamed Zamani and W Bruce Croft. 2017. Relevance-based word embedding. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 505­514. [57] Jiashu Zhao and Jimmy Xiangji Huang. 2014. An Enhanced Context-sensitive Proximity Model for Probabilistic Information Retrieval. In SIGIR. 1131­1134.

A KERNEL VALUE RECOVERY ATTACK

In this section, we describe an attack to recover the term frequency

from closed soft match maps based on histogram pooling even

though the exact match signals are removed. We assume that the in-
terval [-1, 1) for similarity value is divided as [-1, b2, · · · , bR-1, 1) where -1 = b1  b2 < b3 < · · · < bR-1 < bR = 1. Each kernel Kj is associated with an interval [bj , bj+1) where 1  j  R - 1. Note that
all intervals of these kernels are disjoint and their unions are inter-
val [-1, 1). Given a term t, a document d, and a kernel vector fìt,d in a soft match map is (a1, , · · · , aj , · · · , aR-1)T and aR is not included. Each kernel value aj = log Kj (t, d) = log( w d 1bj  t,w <bj+1 ). Thus

R-1

R-1

exp(aj ) = ( 1bj  t,w <bj+1 ).

j =1

j=1 w d

Since the union of all the disjoint intervals is [-1, 1), we can have

R-1

exp(aj ) =

1-1 t,w <1.

j =1

w d

SIGIR '19, July 21­25, 2019, Paris, France Jinjin Shao, Shiyu Ji, Tao Yang

Let the length of d be n, and term frequency of t contained in d is

TF(t, d), we have

R j =1

exp(aj )

=

n

and

exp(aR )

=

TF(t, d).

Then

TF(t, d) = n -

R-1 j =1

exp(aj

).

We also notice for any term t  that is not in document d, TF(t , d) =

0. But fìt,d is included in a soft match map because of term closure.

Thus n =

R-1 j =1

exp(a j ).

To launch this attack, we assume an adversary can scan the

soft match map SMM to obtain all keys (t, d) such that (t, d) 

SMM, and then take the following actions: 1) For each key (t, d),

obtain the kernel vector fìt,d = (a1, · · · , aR-1) in a soft match map.

Compute the sum of elements in this vector. St =

R-1 j =1

exp(a

j

),

where aj = log Kj (t, d). 2) Figure out the length of this document

as n = maxt :(t,d)S M M {St }. 3) Compute term frequency of any t in d as TF(t, d) = n - St .

B PROOFS
Proof of Theorem 3.2 Let t f denote TF(t, d) for simplicity here.
|tf - exp(aR )| = |tf - exp(log KR (t, d))| = |tf - KR (t, d)|

= tf - exp(- (t, w - µR )2 )

w d

2R2

= tf -

(t, w - 1.0)2

exp(-

)-

(t, w exp(-

- 1.0)2 )

.

w d,w =t

2R2

w d,w t

2R2

Since

if

w

=

t,

then

t, w

=

1.0,

exp(-

(

t

,

w -1.0)2 2R2

)

=

exp(0)

=

1.

Thus if the length of d is n, we have

(t, w - 1.0)2

tf -

1-

exp(-

)

w d,w =t w d,w t

2R2

=

(t, w - 1.0)2

exp(-

)

 (n - tf) exp(- (S¯ - 1.0)2 )

w d,w t

2R2

2R2

 n exp(- (S¯ - 1.0)2 ) < . 2R2
Proof of Theorem 5.2

For each term closure, if at least one term in that closure appears
in document d, all terms in that closure would have precomputed kernel values with d. For each closure C, there is a total of 2|C | - 1 non-empty subsets and thus there are totally 2|C | - 2 ways to
replace the exact terms in closure C appeared in d with another
subset of C containing soft terms. When a server tries to guess the
existence of exact term occurrence (t, d), it has to locate the correct one from all 2|C | - 1 ways of forming d using exact or soft terms from C, which is at least 2p - 1. When a server tries to figure out N term occurrence (t, d) pairs, there are (2p - 1)N different document

sets that produce the soft match maps with the same keys. The soft match maps of these (2p - 1)N different document sets are -statistically indistinguishable because we use a subset of a term

closure for replacement and the original soft match map is closed and -statistically indistinguishable.

314

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

A General Framework for Counterfactual Learning-to-Rank

Aman Agarwal
Cornell University Ithaca, NY
aman@cs.cornell.edu
Ivan Zaitsev
Cornell University Ithaca, NY
iz44@cornell.edu
ABSTRACT
Implicit feedback (e.g., click, dwell time) is an attractive source of training data for Learning-to-Rank, but its naive use leads to learning results that are distorted by presentation bias. For the special case of optimizing average rank for linear ranking functions, however, the recently developed SVM-PropRank method has shown that counterfactual inference techniques can be used to provably overcome the distorting effect of presentation bias. Going beyond this special case, this paper provides a general and theoretically rigorous framework for counterfactual learning-to-rank that enables unbiased training for a broad class of additive ranking metrics (e.g., Discounted Cumulative Gain (DCG)) as well as a broad class of models (e.g., deep networks). Specifically, we derive a relaxation for propensity-weighted rank-based metrics which is subdifferentiable and thus suitable for gradient-based optimization. We demonstrate the effectiveness of this general approach by instantiating two new learning methods. One is a new type of unbiased SVM that optimizes DCG ­ called SVM PropDCG ­, and we show how the resulting optimization problem can be solved via the Convex Concave Procedure (CCP). The other is Deep PropDCG, where the ranking function can be an arbitrary deep network. In addition to the theoretical support, we empirically find that SVM PropDCG significantly outperforms existing linear rankers in terms of DCG. Moreover, the ability to train non-linear ranking functions via Deep PropDCG further improves performance.
KEYWORDS
Learning to rank, presentation bias, counterfactual inference
ACM Reference Format: Aman Agarwal, Kenta Takatsu, Ivan Zaitsev, and Thorsten Joachims. 2019. A General Framework for Counterfactual Learning-to-Rank. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331202
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331202

Kenta Takatsu
Cornell University Ithaca, NY
kt426@cornell.edu
Thorsten Joachims
Cornell University Ithaca, NY
tj@cs.cornell.edu
1 INTRODUCTION
Implicit feedback from user behavior is an attractive source of data in many information retrieval (IR) systems, especially ranking applications where collecting relevance annotations from experts can be economically infeasible or even impossible (e.g., personal collection search, intranet search, scholarly search). While implicit feedback is often abundant, cheap, timely, user-centric, and routinely logged, it suffers from inherent biases. For example, the position of a result in a search ranking strongly affects how likely it is to be seen by a user and thus clicked. So, naively using click data as a relevance signal leads to sub-optimal performance.
A counterfactual inference approach for learning-to-rank (LTR) from logged implicit feedback was recently developed to deal with such biases [15]. This method provides a rigorous approach to unbiased learning despite biased data and overcomes the limitations of alternative bias-mitigation strategies. In particular, it does not require the same query to be seen multiple times as necessary for most generative click models, and it does not introduce alternate biases like treating clicks as preferences between clicked and skipped documents.
The key technique in counterfactual learning is to incorporate the propensity of obtaining a particular training example into an Empirical Risk Minimization (ERM) objective that is provably unbiased [28]. While it was shown that this is possible for learning to rank, existing theoretical support is limited to linear ranking functions and optimizing average rank of the relevant documents as objective [15]. In this paper, we generalize the counterfactual LTR framework to a broad class of additive IR metrics as well as non-linear deep models. Specifically, we show that any IR metric that is the sum of individual document relevances weighted by some function of document rank can be directly optimized via Propensity-Weighted ERM. Moreover, if an IR metric meets the mild requirement that the rank weighting function is monotone, we show that a hinge-loss upper-bounding technique enables learning of a broad class of differentiable models (e.g. deep networks).
To demonstrate the effectiveness of the general framework, we fully develop two learning-to-rank methods that optimize the Discounted Cumulative Gain (DCG) metric. The first is SVM PropDCG, which generalizes a Ranking SVM to directly optimize a bound on DCG from biased click data. The resulting optimization problem is no longer convex, and we show how to find a local optimum using the Convex Concave Procedure (CCP). In CCP, several iterations of convex sub-problems are solved. In the case of SVM PropDCG,

5

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

these convex sub-problems have the convenient property of being a Quadratic Program analogous to a generalized Ranking SVM. This allows the CCP to work by invoking an existing and fast SVM solver in each iteration until convergence. The second method we develop is Deep PropDCG, which further generalizes the approach to deep networks as non-linear ranking functions. Deep PropDCG also optimizes a bound on the DCG, and we show how the resulting optimization problem can be solved via stochastic gradient descent for any network architecture that shares neural network weights across candidate documents for the same query.
In addition to the theoretical derivation and the justification, we also empirically evaluate the effectiveness of both SVM PropDCG and Deep PropDCG, especially in comparison to the existing SVM PropRank method [15]. We find that SVM PropDCG performs significantly better than SVM PropRank in terms of DCG, and that it is robust to varying degrees of bias, noise and propensity-model misspecification. In our experiments, CCP convergence was typically achieved quickly within three to five iterations. For Deep PropDCG, the results show that DCG performance is further improved compared to SVM PropDCG when using a neural network, thus demonstrating that the counterfactual learning approach can effectively train non-linear ranking functions. SVM PropDCG and Deep PropDCG are also seen to outperform LambdaRank in terms of DCG.
2 RELATED WORK
Generative click models are a popular approach for explaining the bias in user behavior and for extracting relevance labels for learning. For example, in the cascade model [9] users are assumed to sequentially go down a ranking and click on a document, thus revealing preferences between clicked and skipped documents. Learning from these relative preferences lowers the impact of some biases [13]. Other click models ([3, 7, 9], also see [8]) train to maximize loglikelihood of observed clicks, where relevance is modeled as a latent variable that is inferred over multiple instances of the same query. In contrast, the counterfactual framework [15] does not require latent-variable inference and repeat queries, but allows directly incorporating click feedback into the learning-to-rank algorithm in a principled and unbiased way, thus allowing the direct optimization of ranking performance over the natural query distribution.
The counterfactual approach uses inverse propensity score (IPS) weighting, originally employed in survey sampling [11] and causal inference from observational studies [24], but more recently also in whole page optimization [34], IR evaluation with manual judgments [25], and recommender evaluation [17, 26]. This approach is similar in spirit to [32], where propensity-weighting is used to correct for selection bias when discarding queries without clicks during learning-to-rank.
Recently, inspired by the IPS correction approach for unbiased LTR, some algorithms (Ai et al. [2], Hu et al. [12]) have been proposed that jointly estimate the propensities and learn the ranking function. However, this requires an accurate relevance model to succeed, which is at least as hard as the LTR from biased feedback problem in question. Moreover, the two-step approach of propensity estimation followed by training an unbiased ranker allows direct optimization of any chosen target ranking metric independent of

Metric

 (rank)

AvRank DCG P r ec @k
RBP-p [20]

rank -1/log(1 + rank) -1rankk /k -(1 - p)/prank

Table 1: Some popular linearly decomposable IR metrics that can be directly optimized by Propensity-Weighted ERM.  (r ) is the rank weighting function.

the propensity estimation step. While our focus is on directly optimizing ranking performance
in the implicit feedback partial-information setting, several approaches have been proposed for the same task in the full-information supervised setting, i.e. when the relevances of all the documents in the training set are known. A common strategy is to use some smoothed version of the ranking metric for optimization, as seen in SoftRank [30] and others [6, 14, 35, 36]. In particular, SoftRank optimizes the expected performance metric over the distribution of rankings induced by smoothed scores, which come from a normal distribution centered at the query-document mean scores predicted by a neural net. This procedure is computationally expensive with an O(n3) dependence on the number of documents for a query. In contrast, our approach employs an upper bound on the performance metric, whose structure makes it amenable to the Convex Concave Procedure for efficient optimization, as well as adaptable to non-linear ranking functions via deep networks.
Finally, several works exist [4, 5, 23, 30] that have proposed neural network architectures for learning-to-rank. We do not focus on a specific network architecture in this paper, but instead propose a new training criterion for learning-to-rank from implicit feedback that in principle allows unbiased network training for a large class of architectures.

3 UNBIASED ESTIMATION OF RANK-BASED
IR METRICS
We begin by developing a counterfactual learning framework that covers the full class of linearly decomposable metrics as defined below (e.g. DCG). This extends [15] which was limited to the Average Rank metric. Suppose we are given a sample X of i.i.d. query instances xi  P(x), i  [N ]. A query instance can include personalized and contextual information about the user in addition to the query string. For each query instance xi , let ri (y) denote the user-specific relevance of result y for instance xi . For simplicity, assume that relevances are binary, ri (y)  {0, 1}. In the following, we consider the class of additive ranking performance metrics, which includes any metric that can be expressed as

(y|xi , ri ) =

 (rank(y|y)) · ri (y).

(1)

y y

y denotes a ranking of results, and  () can be any weighting func-

tion that depends on the rank rank(y|y) of document y in ranking y.

A broad range of commonly used ranking metrics falls into this class,

and Table 1 lists some of them. For instance, setting  (rank) = rank

gives the sum of relevant ranks metric (also called average rank

when normalized) considered in [15], and  (rank)

=

-1 log(1+rank)

6

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

gives the DCG metric. Note that we consider negative values wherever necessary to make the notation consistent with risk minimization.
A ranking system S maps a query instance xi to a ranking y. Aggregating the losses of individual rankings over the query distribution, we can define the overall risk (e.g., the expected DCG) of a system as



R(S) = (S(x)|x, r) d P(x, r).

(2)

A key problem when working with implicit feedback data is that
we cannot assume that all relevances ri are observed. In particular, while a click (or a sufficiently long dwell time) provides a noisy indicator of positive relevance in the presented ranking y¯i , a missing click does not necessarily indicate lack of relevance as the user may
not have observed that result. From a machine learning perspective,
this implies that we are in a partial-information setting, which we
will deal with by explicitly modeling missingness in addition to relevance. Let oi  P(o|xi , y¯i , ri ) denote the 0/1 vector indicating which relevance values are revealed. While oi is not necessarily fully observed either, we can now model its distribution, which
we will find below is sufficient for unbiased learning despite the missing data. In particular, the propensity of observing ri (y) for query instance xi given presented ranking y¯ is then defined as Q(oi (y) = 1|xi , y¯i , ri ).
Using this counterfactual setup, an unbiased estimate of (y|xi , ri ) for any ranking y can be obtained via IPS weighting

^ I PS (y|xi , y¯i , oi )

=

 (rank(y|y)) . y:oi (y)=1 Q(oi (y) = 1|xi , y¯i , ri )

(3)

ri (y)=1

This is an unbiased estimator since,

Eoi [^ I P S (y|xi , y¯i , oi )]

(4)

=

Eoi

y:oi

(y )=1

 (rank(y|y))·ri (y) Q(oi (y) = 1|xi , y¯i , ri )



=

Eoi
y y

oi (y)· (rank(y|y))·ri (y) Q(oi (y) = 1|xi , y¯i , ri )

=

Q(oi (y) = 1|xi , y¯i , ri ) ·  (rank(y|y)) · ri (y)

y y

Q(oi (y) = 1|xi , y¯i , ri )

=

 (rank(y|y)) ri (y)

y y

= (y|xi , ri ),

assuming Q(oi (y) = 1|xi , y¯i , ri ) > 0 for all y that are relevant ri (y) = 1. The above proof is a generalized version of the one in [15] for the Average Rank metric. Note that the estimator in Equation (3)
sums only over the results where the feedback is observed (i.e., oi (y) = 1) and positive (i.e., ri (y) = 1), which means that we do not have to disambiguate whether lack of positive feedback (e.g., the
lack of a click) is due to a lack of relevance or due to missing the
observation (e.g., result not relevant vs. not viewed).

Using this unbiased estimate of the loss function, we get an unbiased estimate of the risk of a ranking system S

R^I P S (S)

=

1 N

N i =1

y :oi

(y )=1

 (rank(y|S(xi ))) Q(oi (y) = 1|xi , y¯i , ri

)

.

(5)

ri (y)=1

Note that the propensities Q(oi (y) = 1|xi , y¯i , ri ) are generally unknown, and must be estimated based on some model of user behavior. Practical approaches to estimating the propensities are given in [1, 10, 15, 33].

4 UNBIASED EMPIRICAL RISK MINIMIZATION FOR LTR
The propensity-weighted empirical risk from Equation (5) can be used to perform Empirical Risk Minimization (ERM)
S^ = argminS S R^I PS (S) .
Under the standard uniform convergence conditions [31], the unbiasedness of the risk estimate implies consistency in the sense that given enough training data, the learning algorithm is guaranteed to find the best system in S. We have thus obtained a theoretically justified training objective for learning-to-rank with additive metrics like DCG. However, it remains to be shown that this training objective can be implemented in efficient and practical learning methods. This section shows that this is indeed possible for a generalization of Ranking SVMs and for deep networks as ranking functions.
Consider a dataset of n examples of the following form. For each query-result pair (xi , yi ) that is clicked, let qi = Q(oi (y) = 1|xi , y¯i , ri ) be the propensity of the click according to a click propensity model such as the Position-Based Model [15, 33]. We also record the candidate set Yi of all results for query xi . Note that each click generates a separate training example, even if multiple clicks occur for the same query.
Given this propensity-scored click data, we would like to learn a scoring function f (x, y). Such a scoring function f naturally specifies a ranking system S by sorting candidate results Y for a given query x by their scores.

Sf (x)  argsortY { f (x, y)}

(6)

Since rank(y|Sf (x)) of a result is a discontinuous step function of the score, tractable learning algorithms typically optimize a substitute loss that is (sub-)differentiable [13, 30, 36]. Following this route, we now derive a tractable substitute for the empirical risk of (5) in terms of the scoring function. This is achieved by the following hinge-loss upper bound [15] on the rank

rank(yi |y) - 1 =

1f (xi,y)-f (xi,yi )>0

y Yi

y yi

 max(1 - (f (xi , yi ) - f (xi , y)), 0).
y Yi y yi

Using this upper bound, we can also get a bound for any IR metric that can be expressed through a monotonically increasing weighting function  (r ) of the rank. Note that this monotonicity condition is satisfied by all the metrics in Table 1. By rearranging terms and

7

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

applying the weighting function  (r ), we have

 (rank(yi |y))   1 + max(1 - (f (xi , yi ) - f (xi , y)), 0) .
y Yi y yi
This provides the following continuous and subdifferentiable upper bound R^hI PinSe (f ) on the propensity-weighted risk estimate of (5). R^I P S (Sf )  R^hI PinSe (f )

=1 n 1 n i=1 qi



1+ max(1 - (f (xi , yi )- f (xi , y)), 0)
y Yi

(7)

y yi

Focusing on the DCG metric, we show in the following how this upper bound can be optimized for linear as well as non-linear neural network scoring functions. For the general class of additive IR metrics, the optimization depends on the properties of the weighting function  (r ), and we highlight them wherever appropriate.

4.1 SVM PropDCG

The following derives an SVM-style method, called SVM PropDCG, for learning a linear scoring function f (x, y) = w · (x, y), where w is a weight vector and (x, y) is a feature vector describing the match between query x and result y. For such linear ranking functions ­ which are widely used in Ranking SVMs [13] and many other learning-to-rank methods [19] ­, the propensity-weighted ERM bound from Equation (7) can be expressed as the following SVM-type optimization problem.

w^

=

argminw, 

1w 2

·w

+

C n

n i =1

1 qi



iy + 1
y Yi

s.t . y  Y1 \{y1} : w · [(x1, y1) - (x1, y)]  1-1y

...

y  Yn \{yn } : w · [(xn, yn ) - (xn, y)]  1-ny iy : iy  0

C is a regularization parameter. The training objective optimizes the L2-regularized hinge-loss upper bound on the empirical risk estimate (7). This upper bound holds since for any feasible (w,  ) and any monotonically increasing weighting function  (r )

 1 + max(1 - (f (xi , yi ) - f (xi , y)), 0)
y Yi y yi

=  1+ max(1 - w · [(xi , yi ) - (xi , y)], 0)   1+ iy .

y Yi y yi

y Yi

As shown in [15], for the special case of using the sum of relevant ranks as the metric to optimize, i.e.  (r ) = r , this SVM optimization problem is a convex Quadratic Program which can be solved efficiently using standard SVM solvers, like SVM-rank [14], via a one-slack formulation.

Moving to the case of DCG as the training metric via the weight-

ing function  (r )

=

-1 log(1+r

)

,

we

get

the

following

optimization

problem for SVM PropDCG

w^

=

argminw, 

1w 2

·w

-

C n

n i =1

1 qi

log(

1 y Yi iy + 2)

s.t . jy  Yi \{yi } : w · [(xi , yi ) - (xi , y)]  1-iy

jy : iy  0.

This optimization problem is no longer a convex Quadratic Pro-

gram. However, all constraints are still linear inequalities in the

variables w and  , and the objective can be expressed as the dif-

ference of two convex functions h an . Let h(w)

=

1 2

w

2 and

( )

=

C n

n1 j=1 qi log(

1
y Yi

iy +2) .

Then

the

function

h

is

the

L2

norm of the vector w and is thus a convex function. As for the func-

tion , the function k

:x



1 log x

is convex as it is the composition

of a

a convex

decreasing

function

(x



1 x

)

with

a concave function

(x  log x). So, since the sum of affine transformations of a convex

function is convex,  is convex.

Such an optimization problem is called a convex-concave problem1 and a local optimum can be obtained efficiently via the Convex-

Concave Procedure (CCP) [18]. At a high level, the procedure works

by repeatedly approximating the second convex function with its

first order Taylor expansion which makes the optimization problem

convex in each iteration. The Taylor expansion is first done at some

chosen initial point in the feasible region, and then the solution of

the convex problem in a particular iteration is used as the Taylor

approximation point for the next iteration. It can be shown that

this procedure converges to a local optimum [18]. Concretely, let wk ,  k be the solution in the kth iteration. Then,

we have the Taylor approximation

^( ;  k ) =

= ( k ) + ( k )T ( -  k )

( k ) - C n 1 n j=1 qi

y Yi iy - iky y Yi iky + 2 log2 y Yi iky + 2

Letting qi = qi y Yi iky + 2 log2 y Yi iky + 2 , and dropping the additive constant terms from ^, we get the following convex
program that needs to be solved in each CCP iteration.

argminw, 

1w 2

·w

+

C n

n i =1

1 qi

y Yi

iy

s.t . iy  Yi \{yi } : w · [(xi , yi ) - (xi , y)]  1-iy

iy : iy  0

Observe that this problem is of the same form as SVM PropRank, the Propensity Ranking SVM for the average rank metric, i.e.  (r ) = r (with the caveat that qi are not propensities). This nifty feature allows us to solve the convex problem in each iteration of the CCP using the fast solver for SVM PropRank provided in [15]. In our

1More generally, the inequality constraints can also be convex-concave and not just convex

8

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

experiments, CCP convergence was achieved within a few itera-

tions ­ as detailed in the empirical section. For other IR metrics,

the complexity and feasibility of the above Ranking SVM optimiza-

tion procedure will depend on the form of the target IR metric. In

particular, if the rank weighting function  (r ) is convex, it may be

solved directly as a convex program. If  (r ) is concave, then the

CCP may be employed as shown for the DCG metric above.

An attractive theoretical property of SVM-style methods is the

ability to switch from linear to non-linear functions via the Kernel

trick. In principle, kernelization can be applied to SVM PropDCG

as is evident from the representer theorem [27]. Specifically, by

taking the Lagrange dual, the problem can be kernelized analogous

to [13]. While it can be shown that the dual is convex and strong

duality holds, it is not clear that the optimization problem has

a convenient and compact form that can be efficiently solved in

practice. Even for the special case of the average rank metric,  (r ) =

r , the associated kernel matrix Kiy, jy has a size equal to the total

number of candidates

n i =1

|Yi

|

squared,

making

the

kernelization

approach computationally infeasible or challenging at best. We

therefore explore a different route for extending our approach to

non-linear scoring functions in the following.

4.2 Deep PropDCG
Since moving to non-linear ranking functions through SVM kernelization is challenging, we instead explore deep networks as a class of non-linear scoring functions. Specifically, we replace the linear scoring function f (x, y) = w · (x, y) with a neural network

f (x, y) = N Nw [(x, y)]

(8)

This network is generally non-linear in both the weights w and the features (x, y). However, this does not affect the validity of the hinge-loss upper bound from Equation (7), which now takes the form

1 n

j

n =1

1 qi



1+ max(1 - (N Nw [(xi , yi )] - N Nw [(xi , y)]), 0)
y Yi

y yi

During training, we need to minimize this function with respect to

the network parameters w. Unlike in the case of SVM PropDCG,

this function can no longer be expressed as the difference of a con-

vex and a concave function, since N Nw [(xi , yi )] is neither convex nor concave in general. Nevertheless, the empirical success of opti-

mizing non-convex N Nw [(xi , yi )] via gradient descent to a local

optimum is well documented, and we will use this approach in the

following. This is possible since the training objective is subdif-

ferentiable as long as the weighting function  (r ) is differentiable.

However, the non-linearity of  (r ) adds a challenge in applying

stochastic gradient descent methods to our training objective, since

the objective no longer decomposes into a sum over all (xi , y) as in standard network training. We discuss in the following how to

handle this situation to arrive at an efficient stochastic-gradient

procedure.

For concreteness, we again focus on the case of optimizing DCG

via  (r )

=

-1 log(1+r

)

.

In

particular,

plugging

in

the

weighting

function

11 11 11

+

+ /



2 6

6

+ 11

Figure 1: Deep PropDCG schema for computing the loss from one query instance. The blue document is the positive (clicked) result, and the red documents are the other candidates. The neural net NN is used to compute document scores for each set of candidate features. Pairs of scores are passed through the hinge node, and then finally the weighting function is applied as shown.

for DCG, we get the Deep PropDCG minimization objective

1 n

n j =1

-1 qi

log-1

2+ max(1-(N
y Yi

Nw

[(xi , yi )]-N Nw

[(xi , y)]),

0)

y yi

to which a regularization term can be added (our implementation uses weight decay).
Since the weighting function ties together the hinge losses from pairs of documents in a non-linear way, stochastic gradient descent (SGD) is not directly feasible at the level of individual documents. In the case of DCG, since the rank weighting function is concave, one possible workaround is a Majorization-Minimization scheme [28] (akin to CCP): upper bound the loss function with a linear Taylor approximation at the current neural net weights, perform SGD at the level of document pairs (yi , y) to update the weights, and repeat until convergence.
While this Majorization-Minimization scheme in analogy to the SVM approach is possible also for deep networks, we chose a different approach for the reasons given below. In particular, given the success of stochastic-gradient training of deep networks in other settings, we directly perform stochastic-gradient updates at the level of query instances, not individual (xi , y). At the level of query instances, the objective does decompose linearly such that any subsample of query instances can provide an unbiased gradient estimate. Note that this approach works for any differentiable weighting function  (r ), does not require any alternating approximations as in Majorization-Minimization, and processes each candidate document y including the clicked document yi only once in one SGD step.
For SGD at the level of query instances, a forward pass of the neural network ­ with the current weights fixed ­ must be performed on each document y in candidate set Yi in order to compute the loss from training instance (xi , yi ). Since the number of documents in each candidate set varies, this is best achieved by processing

9

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

each input instance (including the corresponding candidate set) as a (variable-length) sequence so that the neural net weights are effectively shared across candidate documents for the same query instance.
This process is most easily understood via the network architecture illustrated in Figure 1. The scoring function N Nw [(xi , yi )] is replicated for each result in the candidate set using shared weights w. In addition there is a hinge-loss node H (u, v) = max(1-(u -v), 0) that combines the score of the clicked result with each other result in the candidate set Yi . For each such pair (yi , y), the corresponding hinge-loss node computes its contribution hj to the upper bound on the rank. The result of the hinge-loss nodes then feeds into a single weighting node (hì) =  1 + j hj that computes the overall bound on the rank and applies the weighting function. The result is the loss of that particular query instance.
Note that we have outlined a very general method which is agnostic about the size and architecture of the neural network. As a proof-of-concept, we achieved superior empirical results over a linear scoring function even with a simple two layer neural network, as seen in Section 5.8. We conjecture that DCG performance may be enhanced further with deeper, more specialized networks. Moreover, in principle, the hinge-loss nodes can be replaced with nodes that compute any other differentiable loss function that provides an upper bound on the rank without fundamental changes to the SGD algorithm.
5 EMPIRICAL EVALUATION
While the derivation of SVM PropDCG and Deep PropDCG has provided a theoretical justification for both methods, it still remains to show whether this theoretical argument translates to improved empirical performance. To this effect, the following empirical evaluation addresses three key questions.
First, we investigate whether directly optimizing DCG improves performance as compared to baseline methods, in particular, SVM PropRank as the most relevant method for unbiased LTR from implicit feedback, as well as LambdaRank, a common strong nonlinear LTR method. Comparing SVM PropDCG to SVM PropRank is particularly revealing about the importance of direct DCG optimization, since both methods are linear SVMs and employ the same software machinery for the Quadratic Programs involved, thus eliminating any confounding factors. We also experimentally analyze the CCP optimization procedure to see whether SVM PropDCG is practical and efficient. Second, we explore the robustness of the generalized counterfactual LTR approach to noisy feedback, the severity of the presentation bias, and misspecification of the propensity model. And, finally, we compare the DCG performance of Deep PropDCG with a simple two layer neural network against the linear SVM PropDCG to understand to what extent non-linear models can be trained effectively using the generalized counterfactual LTR approach.
5.1 Setup
We conducted experiments on synthetic click data derived from two major LTR datasets, the Yahoo Learning to Rank Challenge corpus and LETOR4.0 [22]. LETOR4.0 contains two separate corpora: MQ2007 and MQ2008. Since MQ2008 is significantly smaller than

Dataset # Avg. train clicks # Train queries # Features

Yahoo LETOR4.0

173,986 25,870

20,274

699

1,484

46

Table 2: Properties of the two benchmark datasets.

Model

Avg. DCG (Yahoo) Avg. DCG (LETOR4.0)

SVM Rank LambdaRank SVM PropRank SVM PropDCG Deep PropDCG

0.6223 ± 8e-4 0.6435 ± 4e-4 0.6410± 1e-3 0.6468± 2e-3 0.6517 ± 4e-4

0.6841 ± 2e-3 0.6915 ± 4e-3 0.7004 ± 1e-2 0.7043 ± 1e-2 0.7244 ± 4e-3

Table 3: Performance comparison of different methods on two benchmark datasets ( = 1, - = 0.1, + = 1).

Yahoo Learning to Rank Challenge, with only 784 queries, we follow

the data augmentation approach proposed in [21], combining the

MQ2007 and MQ2008 train sets for training and using the MQ2008

validation and test sets for validation and testing respectively.

Our experiment setup matches [15] for the sake of consistency

and reproducibility. Briefly, the training and validation click data

were generated from the respective full-information datasets (with

relevances binarized) by simulating the position-based click model.

Following [15], we use propensities that decay with presented rank

of the result as pr =

1 r

 . The rankings that generate the clicks are

given by a "production ranker" which was a conventional Ranking

SVM trained on 1 percent of the full-information training data.

The parameter  controls the severity of bias, with higher values

causing greater position bias.

We also introduced noise into the clicks by allowing some irrele-

vant documents to be clicked. Specifically, an irrelevant document

ranked at position r by the production ranker is clicked with prob-

ability pr times -. When not mentioned otherwise, we used the parameters  = 1, - = 0.1 and + = 1, which is consistent with

the setup used in [15]. Other bias profiles are also explored in the

following.

Both the SVM PropRank and SVM PropDCG models were trained

and cross-validated to pick the regularization constant C. For cross-

validation, we use the partial feedback data in the validation set and

select based on the IPS estimate of the DCG [29]. The performance

of the models is reported on the binarized fully labeled test set

which is never used for training or validation.

5.2 How do SVM PropDCG and Deep PropDCG compare against baselines?
We begin the empirical evaluation by comparing our counterfactual LTR methods again standard methods that follow a conventional ERM approach, namely LambdaRank and SVM-Rank. We generate synthetic click data using the procedure describe above, iterating over the training set 10 times for the Yahoo dataset and 100 times for MQ2008. This process was repeated over 6 independent runs, and we report the average performance along with the standard deviation over these runs. The regularization constant C for all SVM methods was picked based on the average DCG performance across the validation click data sampled over the 6 runs. Table 2

10

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

0.66

SVM PropDCG

0.65

SVM PropRank

Noise-free Full-info Skyline

0.64

Production Ranker

Avg. DCG of Relevant Results

0.63

0.62

0.61

0.6

0.59

0.58

0.57

0.56 1.7E3

1.7E4 Number of Training Clicks

1.7E5

Figure 2: Test set Avg DCG performance for SVM PropDCG and SVM PropRank ( = 1, - = 0.1)

13.5

SVM PropDCG

SVM PropRank

13

Noise-free Full-info Skyline Production Ranker

12.5

Avg. Rank of Relevant Results

12

11.5

11

10.5

10 1.7E3

1.7E4 Number of Training Clicks

1.7E5

Figure 3: Test set Avg Rank performance for SVM PropDCG and SVM PropRank ( = 1, - = 0.1)

shows the average number of clicks along with other information about the training sets.
As a representative for non-linear LTR methods that use a conventional ERM approach, we also conducted experiments with LambdaRank as one of the most popular tree-based rankers. We use the LightGBM implementation [16]. During training, LambdaRank optimizes Normalized Discounted Cumulative Gain (NDCG). Since LambdaRank is a full-information method, we used clicks as relevance labels, i.e. all clicked documents as relevant and all nonclicked documents as irrelevant. The hyperparameters for LambdaRank, namely learning rate and the number of leaves were tuned based on the average DCG of clicked documents in the validation sets. More specifically, we performed a grid search to finetune learning rate from 0.001 to 0.1 and the number of leaves from 2 to 256. After tuning, we selected the learning rate to be 0.1, and the number of leaves to be 64 for the Yahoo dataset and 4 for MQ2008. We also made sure each split does not use more than 50% of the input features.
As shown in in Table 3, the counterfactual ERM approach via IPS weighting and directly optimizing for the target metric DCG yield superior results for SVM PropDCG and Deep PropDCG. The best results on both benchmarks are achieved by Deep PropDCG,

0.7

SVM PropDCG

SVM PropRank

SVM PropDCG-5x

SVM PropRank-5x

0.65

Avg. DCG of Relevant Results

0.6

0.55

0.5

0

0.5

1

1.5

2

Severity of Presentation Bias

Figure 4: Test set Avg DCG performance for SVM PropDCG and SVM PropRank as presentation bias becomes more severe in terms of  (n = 45K and n = 225K, - = 0).

which learns a two-layer neural network ranker. We conjecture that more sophisticated network architectures can further improve performance.
5.3 How does ranking performance scale with training set size?
Next, we explore how the test-set ranking performance changes as the learning algorithm is given more and more click data. The resulting learning curves are given in Figures 2 and 3. The click data has presentation bias with  = 1 and noise with - = 0.1. For small datasets, results are averaged over 3 draws of the click data. Both curves show the performance of the Production Ranker used to generate the click data, and the SVM skyline performance trained on the full-information training set. Ideally, rankers trained on click data should outperform the production ranker and approach the skyline performance.
Figure 2 shows that the DCG performance of both SVM PropDCG and SVM PropRank. As expected, both improve with increasing amounts of click data. Moreover, SVM PropDCG performs substantially better than the baseline SVM PropRank in maximizing test set DCG.
More surprisingly, Figure 3 shows both methods perform comparably in minimizing the average rank metric, with SVM PropDCG slightly better at smaller amounts of data and SVM PropRank better at larger amounts. We conjecture that this is due the variancelimiting effect of the DCG weights in SVM PropDCG when substituting the propensity weights qi with the new constants qi in the SVM PropDCG CCP iterations. This serves as implicit variance control in the IPS estimator similar to clipping [15] by preventing propensity weights from getting too big. Since variance dominates estimation error at small amounts of data and bias dominates at large amounts, our conjecture is consistent with the observed trend.
5.4 How much presentation bias can be tolerated?
We now vary the severity of the presentation bias via  ­ higher values leading to click propensities more skewed to the top positions

11

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

0.7

SVM PropDCG

SVM PropRank

0.65

Avg. DCG of Relevant Results

0.6

0.55

0.5 0

0.05

0.1

0.15

0.2

0.25

0.3

Noise Level

Figure 5: Test set Avg DCG performance for SVM PropDCG and SVM PropRank as the noise level increases in terms of  (n = 170K,  = 1).

0.7

SVM PropDCG

SVM PropRank

0.65

Avg. DCG of Relevant Results

0.6

0.55

0.5

0

0.5

1 (true)

1.5

2

Assumed Propensity Model (eta)

Figure 6: Test set Avg DCG performance for SVM PropDCG and SVM PropRank as propensities are misspecified (true eta = 1, n = 170K, - = 0.1).

­ to understand its impact on the learning algorithm. Figure 4 shows the impact on DCG performance for both methods. We report performance for two training set sizes that differ by a factor of 5 (noise - = 0). We see that SVM PropDCG is at least as robust to the severity of bias as SVM PropRank. In fact, SVM PropRank's performance degrades more at high bias than that of SVM PropDCG, further supporting the conjecture that the DCG weighting in SVM PropDCG provides improved variance control which is especially beneficial when propensity weights are large. Furthermore, as also noted for SVM PropRank in [15], increasing the amount of training data by a factor of 5 improves performance of both methods due to variance reduction, which is an advantage that unbiased learning methods have over those that optimize a biased objective.
5.5 How robust is SVM PropDCG to noise?
Figure 5 shows the impact of noise on DCG performance, as noise levels in terms of - increase from 0 to 0.3. The latter results in click data where 59.8% of all clicks are on irrelevant documents. As expected, performance degrades for both methods as noise in-

creases. However, there is no evidence that SVM PropDCG is less robust to noise than the baseline SVM PropRank.
5.6 How robust is SVM PropDCG to misspecified propensities?
So far all experiments have had access to the true propensities that generated the synthetic click data. However, in real-world settings propensities need to be estimated and are necessarily subject to modeling assumptions. So, we evaluate the robustness of the learning algorithm to propensity misspecification.
Figure 6 shows the performance of SVM PropDCG and SVM PropRank when the training data is generated with  = 1, but the propensities used in learning are misspecified according to the  on the x-axis. The results show that SVM PropDCG is at least as robust to misspecified propensities as SVM PropRank. Both methods degrade considerably in the high bias regime when small propensities are underestimated ­ this is often tackled by clipping [15]. It is worth noting that SVM PropDCG performs better than SVM PropRank when misspecification leads to propensities that are underestimated, further strengthening the implicit variance control conjecture for SVM PropDCG discussed above.
5.7 How well does the CCP converge?
Next, we consider the computational efficiency of employing the CCP optimization procedure for training SVM PropDCG. Recall that the SVM PropDCG objective is an upper bound on the regularized (negative) DCG IPS estimate. It is optimized via CCP which repeatedly solves convex subproblems using the SVM PropRank solver until the objective value converges.
In Figure 7, optimization progress vs number of iterations as indicated by the change in objective value as well as the training DCG SNIPS estimate [29] is shown for 17K training clicks and the full range of regularization parameter C used in validation. The figure shows that the objective value usually converges in 3-5 iterations, a phenomenon observed in our experiments for other amounts of training data as well. In fact, the convergence tends to take slightly fewer iterations for larger amounts of data. The figure also shows that progress in objective is well-tracked with progress in the training DCG estimate, which suggests that the objective is a suitable upper bound for DCG optimization.
It is worth noting that restarting the optimizer across multiple CCP iterations can be substantially less time consuming than the initial solution that SVM PropRank computes. Since only the coefficients of the Quadratic Program change, the data does not need to be reloaded and the optimizer can be warm-started for quicker convergence in subsequent CCP iterations.
5.8 When does the non-linear model improve over the linear model?
We have seen that SVM PropDCG optimizes DCG better than SVM PropRank, and that it is a robust method across a wide range of biases and noise levels. Now we explore if performance can be improved further by introducing non-linearity via neural networks. Since the point of this paper is not a specific deep architecture but

12

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

Figure 7: Optimization progress with respect to the number of CCP iterations. The objective value is shown in the left plots, and the training set DCG estimate on the right plots. Each plot corresponds to a particular value of regularization constant C (n = 17K,  = 1, - = 0.1).

0.66 SVM PropDCG
0.65 Deep PropDCG

Avg. DCG of Relevant Results

0.64

0.63

0.62

0.61

0.6

0.59

0.58

0.57

0.56 1.7E3

1.7E4 Number of Training Clicks

1.7E5

Figure 8: Test set Avg DCG performance for SVM PropDCG and Deep PropDCG ( = 1, - = 0.1)

a novel training objective, we used a simple two-layer neural network with 200 hidden units and sigmoid activation. We expect that specialized deep architectures will further improve performance.
Figure 8 shows that Deep PropDCG achieves improved DCG compared to the linear SVM PropDCG given enough training data. For small amounts of training data, the linear model performs better, which is to be expected given the greater robustness to overfitting of linear models.
We also expect improved performance from tuning the hyperparameters of Deep PropDCG. In fact, we only used default parameters for Deep PropDCG, while we optimized the hyperparameters of SVM PropDCG on the validation set. In particular, Adam was used for stochastic gradient descent with weight decay regularizer at 10-6, minibatch size of 1000 documents and 750 epochs. The learning rate began at 10-6 for the first 300 epochs, dropping by one order of magnitude in the next 200 epochs and another order of

magnitude in the remaining epochs. We did not try any other hyperparameter settings and these settings were held fixed across varying amounts of training data.
6 CONCLUSION
In this paper, we proposed a counterfactual learning-to-rank framework that is broad enough to cover a broad class of additive IR metrics as well as non-linear deep network models. Based on the generalized framework, we developed the SVM PropDCG and Deep PropDCG methods that optimize DCG via the Convex-Concave Procedure (CCP) and stochastic gradient descent respectively. We found empirically that SVM PropDCG performs better than SVM PropRank in terms of DCG, that it is robust to a substantial amount of presentation bias, noise and propensity misspecification, and that it can be optimized efficiently. DCG was improved further by using a neural network in Deep PropDCG.
There are many directions for future work. First, it is open for which other ranking metrics it is possible to develop efficient and effective methods using the generalized counterfactual framework. Second, the general counterfactual learning approach may also provide unbiased learning objectives for other settings beyond ranking, like full-page optimization and browsing-based retrieval tasks. Finally, it is an open question whether non-differentiable (e.g. tree-based) ranking models can be trained in the counterfactual framework as well.
7 ACKNOWLEDGMENTS
This research was supported in part by NSF Awards IIS-1615706 and IIS-1513692, an Amazon Research Award, and the Criteo Faculty Research Award program. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.

13

Session 1A: Learning to Rank 1

SIGIR '19, July 21­25, 2019, Paris, France

REFERENCES
[1] Aman Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Li, Marc Najork, and Thorsten Joachims. 2019. Estimating Position Bias without Intrusive Interventions. In International Conference on Web Search and Data Mining (WSDM). 474­482.
[2] Qingyao Ai, Keping Bi, Cheng Luo, Jiafeng Guo, and W. Bruce Croft. 2018. Unbiased Learning to Rank with Unbiased Propensity Estimation. In The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). ACM, New York, NY, USA, 385­394. https://doi.org/10.1145/ 3209978.3209986
[3] Alexey Borisov, Ilya Markov, Maarten de Rijke, and Pavel Serdyukov. 2016. A Neural Click Model for Web Search. In Proceedings of the 25th International Conference on World Wide Web (WWW). 531­541.
[4] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to Rank Using Gradient Descent. In Proceedings of the 22Nd International Conference on Machine Learning (ICML). ACM, New York, NY, USA, 89­96.
[5] Christopher J Burges, Robert Ragno, and Quoc V Le. 2007. Learning to rank with nonsmooth cost functions. In Advances in Neural Information Processing Systems (NeurIPS). 193­200.
[6] Olivier Chapelle and Mingrui Wu. 2010. Gradient Descent Optimization of Smoothed Information Retrieval Metrics. Information Retrieval 13, 3 (June 2010), 216­235. https://doi.org/10.1007/s10791-009-9110-3
[7] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model for web search ranking. In International Conference on World Wide Web (WWW). ACM, 1­10.
[8] Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. 2015. Click Models for Web Search. Morgan & Claypool Publishers.
[9] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An Experimental Comparison of Click Position-bias Models. In International Conference on Web Search and Data Mining (WSDM). ACM, 87­94.
[10] Zhichong Fang, A. Agarwal, and T. Joachims. 2019. Intervention Harvesting for Context-Dependent Examination-Bias Estimation. In ACM Conference on Research and Development in Information Retrieval (SIGIR).
[11] D. G. Horvitz and D. J. Thompson. 1952. A Generalization of Sampling Without Replacement from a Finite Universe. J. Amer. Statist. Assoc. 47, 260 (1952), 663­ 685.
[12] Ziniu Hu, Yang Wang, Qu Peng, and Hang Li. 2018. A Novel Algorithm for Unbiased Learning to Rank. (2018). arXiv:cs.IR/1809.05818
[13] T. Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). 133­142.
[14] T. Joachims, T. Finley, and Chun-Nam Yu. 2009. Cutting-Plane Training of Structural SVMs. Machine Learning 77, 1 (2009), 27­59.
[15] Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased Learning-to-Rank with Biased Feedback. In ACM International Conference on Web Search and Data Mining (WSDM). ACM, New York, NY, USA, 781­789.
[16] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Advances in Neural Information Processing Systems (NeurIPS), I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). 3146­3154.
[17] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. 2011. Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms. In International Conference on Web Search and Data Mining (WSDM).

297­306. [18] Thomas Lipp and Stephen Boyd. 2016. Variations and extension of the convex­
concave procedure. Optimization and Engineering 17, 2 (2016), 263­287. [19] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Foundations and
Trends in Information Retrieval 3, 3 (2009), 225­331. [20] Alistair Moffat and Justin Zobel. 2008. Rank-biased Precision for Measurement
of Retrieval Effectiveness. ACM Transactions on Information Systems (TOIS) 27, 1 (2008), 2:1­2:27. [21] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng. 2017. DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. In ACM Conference on Information and Knowledge Management (CIKM). ACM, 257­266. [22] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013). [23] L. Rigutini, T. Papini, M. Maggini, and F. Scarselli. 2011. SortNet: Learning to Rank by a Neural Preference Function. IEEE Transactions on Neural Networks 22, 9 (Sept 2011), 1368­1380. [24] Paul R. Rosenbaum and Donald B. Rubin. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika 70, 1 (1983), 41­55. [25] T. Schnabel, A. Swaminathan, P. Frazier, and T. Joachims. 2016. Unbiased Comparative Evaluation of Ranking Functions. In ACM International Conference on the Theory of Information Retrieval (ICTIR). [26] T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In International Conference on Machine Learning (ICML). [27] B. Schoelkopf and A. J. Smola. 2002. Learning with Kernels. The MIT Press, Cambridge, MA. [28] A. Swaminathan and T. Joachims. 2015. Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization. Journal of Machine Learning Research (JMLR) 16 (Sep 2015), 1731­1755. [29] A. Swaminathan and T. Joachims. 2015. The Self-Normalized Estimator for Counterfactual Learning. In Neural Information Processing Systems (NeurIPS). [30] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. SoftRank: Optimizing Non-smooth Rank Metrics. In ACM International Conference on Web Search and Data Mining (WSDM). ACM, New York, NY, USA. [31] V. Vapnik. 1998. Statistical Learning Theory. Wiley, Chichester, GB. [32] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016. Learning to Rank with Selection Bias in Personal Search. In ACM Conference on Research and Development in Information Retrieval (SIGIR). ACM. [33] Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. 2018. Position Bias Estimation for Unbiased Learning to Rank in Personal Search. In ACM International Conference on Web Search and Data Mining (WSDM). [34] Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, and Qiaozhu Mei. 2016. Beyond Ranking: Optimizing Whole-Page Presentation. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining (WSDM). 103­112. [35] Mingrui Wu, Yi Chang, Zhaohui Zheng, and Hongyuan Zha. 2009. Smoothing DCG for Learning to Rank: A Novel Approach Using Smoothed Hinge Functions. In ACM Conference on Information and Knowledge Management (CIKM). ACM, New York, NY, USA, 1923­1926. https://doi.org/10.1145/1645953.1646266 [36] Yisong Yue, T. Finley, F. Radlinski, and T. Joachims. 2007. A Support Vector Method for Optimizing Average Precision. In ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 271­278.

14

Session 4B: Queries

SIGIR '19, July 21­25, 2019, Paris, France

Information Needs, Queries, and Query Performance Prediction

Oleg Zendel
olegzendel@campus.technion.ac.il Technion

Anna Shtok
annie.shtok@gmail.com

Fiana Rabier
fiana@verizonmedia.com Yahoo Research

Oren Kurland
kurland@ie.technion.ac.il Technion

J. Shane Culpepper
shane.culpepper@rmit.edu.au RMIT University

ABSTRACT
The query performance prediction (QPP) task is to estimate the effectiveness of a search performed in response to a query with no relevance judgments. Existing QPP methods do not account for the effectiveness of a query in representing the underlying information need. We demonstrate the far reaching implications of this reality using standard TREC-based evaluation of QPP methods: their relative prediction quality patterns vary with respect to the effectiveness of queries used to represent the information needs. Motivated by our findings, we revise the basic probabilistic formulation of the QPP task by accounting for the information need and its connection to the query. We further explore this connection by proposing a novel QPP approach that utilizes information about a set of queries representing the same information need. Predictors instantiated from our approach using a wide variety of existing QPP methods post prediction quality that substantially transcends that of applying these methods, as is standard, using a single query representing the information need. Additional in-depth empirical analysis of different aspects of our approach further attests to the crucial role of query effectiveness in QPP.
ACM Reference Format: Oleg Zendel, Anna Shtok, Fiana Rabier, Oren Kurland, and J. Shane Culpepper. 2019. Information Needs, Queries, and Query Performance Prediction. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184. 3331253
1 INTRODUCTION
Ad hoc (query-based) retrieval effectiveness can significantly vary across queries for a variety of retrieval methods [10]. This fact enabled a large body of work on query performance prediction (QPP) [10] whereby the goal is to estimate search effectiveness in the absence of human relevance judgments. There are two common approaches to this problem. Pre-retrieval predictors analyze the query and the corpus prior to retrieval time [14, 22, 23, 29, 38, 39, 48, 59]; e.g., queries containing terms with high IDF (inverse document
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/3331184.3331253

frequency) values should presumably be more effective [14, 23]. Post-retrieval predictors use information induced from the result list of top-retrieved documents [1, 2, 5, 9, 11, 14­18, 20, 22, 25, 30, 31, 33­ 37, 41­44, 49, 51, 56, 57, 60, 61]. For example, some post-retrieval predictors are based on analyzing the retrieval scores of documents in the result list (e.g., [17, 35, 36, 41, 49, 61]).
A careful examination of past work in this area reveals that most current approaches rely on predicting the performance across a set of queries, where each query represents a unique information need; often on a single corpus using a single retrieval method. This is largely an artifact of the test collections commonly used to evaluate new techniques. For example, the TREC [52] initiative has historically defined an information need as a topic using a title, description, and narrative. The title is commonly viewed as a keyword query that a user might issue to a search engine, and the description and narrative more clearly define the user's real information need. While QPP methods have been shown to be effective in this setting, many (specifically, pre-retrieval methods) were found to be ineffective in predicting the relative effectiveness of different queries that represent the same information need (a.k.a., query variations or reformulations) [48]1. Consequently, Thomas et al. [48] postulated that QPP methods essentially predict informationneed performance rather than query performance. Indeed, current QPP approaches do not explicitly account for the information need, nor for the extent to which a query effectively represents it for retrieval2. In fact, this is also the case for the formal fundamental probabilistic basis of most prediction methods [31, 43], where coupling an information need with the query used to represent it does not allow to account for the effectiveness of the query in representing the need for retrieval.
We show that the implications of this coupling are more farreaching. Specifically, using the UQV datasets [3, 8], where humangenerated query variations are available for TREC topics, we show that the relative prediction quality patterns of various QPP methods, when evaluated in the standard setting of different queries representing different information needs, vary with respect to the effectiveness of queries used to represent the underlying need.
Motivated by our empirical findings that attest to the importance of modeling the information need and its connection with queries used to represent it, we revise the probabilistic formalism of the QPP task [43] which is the basis for most QPP methods. Our formalism
1Pre-retrieval methods are also not effective in predicting the performance of different document lists retrieved for the same query using different retrieval methods [31]. Some work [50] demonstrated the limited merit of using pre-retrieval methods to select between personalized and original query variants. 2The main exception we are aware of is the work of Sondak et al. [44].

395

Session 4B: Queries SIGIR '19, July 21­25, 2019, Paris, France

SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.

reflects a transition from addressing the basic question of "What is the probability that this retrieved list is relevant (effective) for this query?" [43] to addressing the question of "What is the probability that this retrieved list is relevant (effective) for a (latent) information need that is represented by this query?".
We use our revised probabilistic formalism to address a novel prediction challenge: query performance prediction using reference queries. That is, how can we estimate retrieval effectiveness for a given query using knowledge derived from additional (reference) queries that represent the same information need. Our proposed approach, which can be instantiated using any existing queryperformance predictor, accounts for the association between the query and the reference queries and the predicted performance for the latter. Extensive empirical evaluation attests to the clear merits of our approach. Specifically, the prediction quality is substantially better than that of using existing predictors which do not utilize reference queries.
Summary of Contributions. In this work, we show that the effectiveness of a query in representing the underlying information need has a significant impact on QPP quality. We then define a probabilistic QPP framework which encapsulates the fundamental relationship between queries and an information need, and show how it can be easily instantiated using a variety of QPP approaches to improve prediction quality. An extensive empirical evaluation shows that combining information from multiple reference queries dramatically improves prediction quality for queries regardless of their effectiveness in representing the information need, and independent of the QPP method used to instantiate our approach.
2 RELATED WORK
The prediction framework presented by Carmel et al. [11] for estimating topic/information need difficulty accounts for the fact that the information need can be represented by different queries. However, in contrast to our work, the model instantiated from the framework predicts performance for a single query without using information from alternative queries that represent the same information need, and without accounting for how well the query represents the information need.
Previous work exists on predicting performance for a single query by estimating the presumed extent to which it represents the underlying information need [44]. This predictor was also used by Roitman et al. [36], where the estimate is based on properties of a pseudo-feedback-based relevance model induced from the retrieved list [27]. There has also been work on ranking query reformulations [19, 37, 54] by estimating the extent to which each reformulation represents the information need. The task we pursue is different from these previous lines of work: we predict performance for a query using information from other queries representing the same information need. The estimator from Sondak et al. [44] is used as a baseline reference comparison in this work.
Other relevant recent work on using reference document lists for QPP was proposed by Shtok et al. [43]. The reference lists are retrieved by using different retrieval methods for a set of queries. Performance is then predicted based on the similarity between the retrieved list for a query and its corresponding reference lists. Our prediction model uses reference queries and assumes that the

retrieval method is fixed. Some of the estimates derived in this work result in predictors that utilize reference document lists retrieved for different queries; hence, these specific predictors are similar in spirit to those proposed by Shtok et al. [43]. However, we utilize estimates (predictors) for the retrieval effectiveness of the reference queries, while the (novel) models devised and evaluated in [43] do not account for the effectiveness of the reference lists. Furthermore, the work of Shtok et al. [43] couples the query with the information need as in prior work on QPP, while our focus in this work is on explicitly de-coupling the two.
Query Variations. There is a long history of research that explores the relationship between a single query and the information needs it represents. Early work exploring the impact of query representation and effectiveness was presented by Belkin et al. [6]. Shortly thereafter, Belkin et al. [7] operationalized these insights to significantly improve retrieval effectiveness by fusing the results from multiple query variations for a single information need.
A more recent seminal study of query reformulation in user search sessions in a production search engine also explored the importance of subtle query variations on the overall effectiveness of search results returned to the user [24]. Subsequent studies have shown that query variations can be used in learning-to-rank models to significantly boost query performance [19, 40]. Sheldon et al. [40] and other recent studies [55] have shown that highly-effective query variations, rewrites, and suggestions can be generated offline using random walks over a bipartite graph constructed over click data extracted from large search engine query logs [13]. In the absence of access to commercial search engine logs, effective manual query variations can also be generated using crowdsourcing [3, 4, 8]. We use the UQV query collections [3, 8] as the query variations are publicly available, and hence the results are reproducible.
Using human query variations, Thomas et al. [48] showed empirically that existing pre-retrieval predictors essentially predict information-need difficulty rather than query difficulty. Their work inspires this work, where we attempt to de-couple information needs and queries in modeling the QPP task. We strengthen their findings by showing that the relative prediction quality patterns of various query-performance predictors (both pre- and post- retrieval ones) vary with respect to the effectiveness of queries used to represent the information needs. In contrast to our work, a model of the QPP task that accounts for the information need was not presented and the challenge of predicting performance using reference queries was not addressed. That is, Thomas et al. [48] show that it can be very hard to make a prediction that distinguishes the performance of queries representing a single information need, while we show that using multiple representations for an information need can significantly improve the quality of prediction for the performance of a single query for an information need.
Supervised Query Performance Prediction. It is also possible to improve prediction quality using supervised [12, 21, 26, 31, 33­ 35, 56] or weakly supervised [57] techniques. In general, these approaches, and others [42, 51, 61], integrate various unsupervised query performance prediction methods. In this work, we do not explore the use of query variations in supervised QPP techniques. Rather, we demonstrate the merits of our approach with many unsupervised pre- and post-retrieval predictors that are commonly the

396

Session 4B: Queries Information Needs, Queries, and Query Performance Prediction

a
QC

M

I

b
Q CM

R

L

R

L

Figure 1: Graphical model representation of (a) the probabilistic model underlying existing query performance prediction methods, and (b) the extended model that accounts for the information need.

basis for these supervised approaches. Extending our new framework to a supervised learning framework is an interesting problem that is orthogonal to our own.

3 PREDICTION FRAMEWORK

The query performance prediction (QPP) task is to estimate the effectiveness of a search performed in response to a query in the absence of human relevance judgments [10]. As noted by Raiber and Kurland [31], the task can be framed as estimating the probability:

p(R = 1|Q, C, M),

(1)

where R, Q, C and M are random variables that take as values relevance status (1 stands for relevant), queries, corpora and retrieval methods, respectively. That is, the task is to estimate the probability for a relevance event, discussed below, given the ranked retrieval of a query over a corpus.
Previous work on QPP is based, in spirit, on fixing the values of C and M, and estimating Equation 1 [31]. That is, prediction is performed over different queries used for retrieval with the same retrieval method on the same corpus. Since the choice of a query, retrieval method and corpus entails a retrieved list, the QPP task pursued in past work can be framed, as recently suggested by Shtok et al. [43], as estimating:

p(R = 1|Q, L),

(2)

where L is a random variable that takes retrieved lists as values. Figure 1 (a) depicts the graphical model of dependence relations between the random variables used in Equations 1 and 2.
Estimating Equation 2 for a specific query and retrieved list amounts to addressing the question [43]:"What is the probability that this retrieved list is relevant to this query?". This question is an extension, from a single document to a ranked document list, of the question underlying probabilistic retrieval: "What is the probability that this document is relevant to this query?" [46]. A relevance status for a retrieved list can be operationally defined in terms of document relevance [43]; e.g., by thresholding a documentrelevance-based evaluation measure applied to the retrieved list. It was shown that many post-retrieval QPP methods can be derived from Equation 2 [43].

3.1 A missing Piece: The Information Need
A more careful examination of past work on QPP reveals the following: evaluation of prediction quality was performed by using

SIGIR '19, July 21­25, 2019, Paris, France SIGIR '19, July 21­25, 2019, Paris, France

queries representing different information needs -- specifically, TREC topics served as information needs, and each topic was represented by a title, which was treated as a query. Various predictors, including pre-retrieval methods that analyze the query and the corpus but not the retrieved list, were shown to yield high prediction quality in this evaluation setting. However, recent work shows that pre-retrieval predictors are ineffective in predicting the relative performance of different queries used to represent the same information need [48]. Now, these two prediction tasks -- for queries representing the same or different information needs -- cannot be differentiated at the model level using Equations 1 and 2, since the underlying information need is not explicitly accounted for.
Furthermore, the (implicit) coupling of the information need with the query in previous work on QPP has led to ignoring the fact that different queries representing the same information need can exhibit very different performance characteristics. As a case in point, consider the WIG predictor [61] which uses the (corpus normalized) mean retrieval score of the top retrieved documents as a prediction value. If the query does not effectively represent the information need, then high retrieval scores which attest to improved match between the document and the query3 need not necessarily indicate high effectiveness. Moreover, the effectiveness of the query in representing the information need for retrieval can depend on the corpus and retrieval method. For example, a query that can be relatively ineffective for a simple bag-of-words ranking function such as Okapi BM25 [32] or query likelihood [45] might still be highly effective if a retrieval method which captures higher level term dependencies is used (e.g., [28]).
Given the observations discussed above, we revise the basic probabilistic modeling of the QPP task from Equations 1 and 2 to account for the information need. The QPP task becomes estimating:

p(R = 1|I , Q, C, M) = p(R = 1|I, Q, L);

(3)

I is a random variable that takes information needs as values; and, recall that the value of L is uniquely determined by the values of Q, C and M. Figure 1 (b) depicts the dependencies between the random variables. Note that an assignment to I induces a probability distribution over assignments of Q; this is the probability that a user selects a specific query to represent an information need. The retrieval effectiveness of this selection depends on the retrieval method and corpus that together with the query determine the retrieved list.
Assignments of the random variables in Equation 3 result in a novel revised fundamental question of the QPP task: "What is the probability that this retrieved list is relevant to this (latent) information need given that the need is represented by this query?."

3.2 QPP using Reference Queries
To further explore the importance of accounting for the connection between the information need and the query in the QPP task, we pursue the following novel challenge: predicting retrieval performance for a given query using information about additional queries that presumably represent the same information need.

3WIG is mainly used for ranking functions that are based on the surface-level similarity between the query and documents.

397

Session 4B: Queries SIGIR '19, July 21­25, 2019, Paris, France

Formally, let q be the query used for retrieval (Q = q) and i
be the information need it represents (I = i). Let Qi be a set of queries, henceforth referred to as reference queries, that represent i; i.e., q  Qi . p(Q = q|I = i) > 0 (q q).
We predict query performance by estimating p(R = 1|I = i, Q =
q, C = c, M = m) from Equation 3 using the reference queries in Qi
as disjoint proxies for the information need:

p^Ref (R = 1|i, q, c, m) d=ef

p^(R = 1|i, q, q, c, m)p^(q|i, q, c, m);

q Qi
(4)

herein, p^ denotes an estimate for p; p^Ref is an estimate that utilizes

reference queries; we omit random variables from formulations

(except for the relevance status) for brevity.

We now examine the estimates on the right hand side of Equation 4. p^(q|i, q, c, m) is an estimate for the likelihood that the reference query q is the one selected to represent the information need

i. Based on the relationships between random variables assumed in Figure 1 (b), the likelihood p(q|i, q, c, m) does not depend on the

corpus, retrieval method or on other queries used to represent the need4. In addition, we use the query q as a signal about the (latent)

information need to derive the following approximation which is inspired by the relevance-model framework [27]5:

p^(q|i, q, c, m) = p^(q|i)  p^(q|q).

(5)

Below we use inter-query association measures to derive p^(q|q). We next examine p^(R = 1|i, q, q, c, m) in Equation 4. This is an
estimate for the probability of a relevance event given two queries that represent the information need i. The actual retrieved list is not specified as it can be produced in several ways; e.g., the queries can be concatenated to yield a single query used for retrieval, or the lists retrieved for the two queries can be fused to produce a single list. To devise a generic estimate which potentially applies to different approaches of fusing information about the two queries for retrieval, we make the following basic assumption: the retrieval effectiveness of using two queries representing the same information need is based, among other factors, on the extent to which each is an effective representative of the information need with respect to the corpus and retrieval method used. Specifically, the estimate we use is a linear interpolation (fusion), with a free parameter , of the estimates for a relevance event for the two queries:

p^(R = 1|i, q, q, c, m) d=ef (1 - )p^(R = 1|i, q, c, m)+ p^(R = 1|i, q, c, m); (6)

Plugging the estimates from Equations 5 and 6 in Equation 4 and assuming that p^(q|q) is a probability distribution over q  Qi , we
arrive at:

p^Ref (R = 1|i, q, c, m) d=ef (1 - )p^(R = 1|i, q, c, m)+



p^(R = 1|i, q, c, m)p^(q|q).

q Qi
(7)

4For simplicity, we assume that queries are generated independently for an information need. Note that this conditional independence does not contradict our use of the queries
as disjoint events in Equation 4. Disjoint events cannot be independent. 5The probability of generating a term from a relevance model is approximated by the
probability to generate it given the observed query [27].

SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.

Equation 7 is based on backing off from a direct estimate for a rele-
vance event, p^(R = 1|i, q, c, m), to a mixture-based estimate that uses
additional queries representing the same information need. More specifically, the higher the association (p^(q|q)) of the given query (q) with reference queries (q) that are effective representatives of the information need -- i.e., those with high p^(R = 1|i, q, c, m) --
the higher the estimate for a relevance event for q.

3.3 Deriving Specific Predictors
We next derive specific predictors based on Equation 7. First, follow-
ing standard practice in past work on QPP using probabilistic models [25, 33, 35, 43], we use the values P(q) and P(q), assigned by an existing performance predictor P to q and q, for p^(R = 1|i, q, c, m) and p^(R = 1|i, q, c, m), respectively. The predictor uses information induced from the query (q or q) and the corpus (c) and might
also use information induced from the document list retrieved for the query using the retrieval method m. The prediction principles
underlying existing predictors were derived from Equations 1 and
2 in recent work [43]. Now, if the query is coupled with the information need, as assumed in past work, then p^(R = 1|i, q, c, m) and p^(R = 1|i, q, c, m) become p^(R = 1|q, c, m) and p^(R = 1|q, c, m),
respectively, and we indeed resort to Equation 2. Our next goal is devising the estimate p^(q|q). To this end, we
use inter-query association measures, A, as described below. The
resultant prediction value we use, following Equation 7, is:

PRef (q)

d=e f

(1

-

)P

(q)

+



|

1 Qi

|

q Qi

P(q)A(q, q).

(8)

We do not normalize the inter-query association values to form a probability estimate p^(q|q) over q  Qi , as such normalization

resulted in substantially degraded prediction quality. (Actual num-

bers are omitted as they convey no additional insight.) This is not a

surprise: one cannot expect Qi to include all potential queries that represent i. As a result, normalization negatively distorts the esti-

mate for the true relative extent to which the reference queries are

associated with the given query, and thereby the extent to which

they represent the information need. This badly affects prediction

across information needs -- the standard prediction quality evalua-

tion paradigm that we subscribe to in this paper. Furthermore, we

show in Section 4 that the average association of the given query

with the reference queries is already a descent basis for prediction.

(We further discuss this in Section 3.3.1.) Hence, normalization with

respect to the associations negatively affects prediction quality. On

the other hand, in practice, we might have a different number of

reference queries for each information need. To avoid the resulting

bias, we use the

1 | Qi |

normalization factor in Equation

8.

The first inter-query association measure, A, we consider is

the Jaccard coefficient between q and q; the resultant predictor,

based on Equation 8, is Ref-Jaccard[P]. The second measure is

the ratio between the overlap (in terms of number of documents) at the top-k ranks of the document lists retrieved for q and q

from c using the retrieval method m; k is a free parameter; the

resultant predictor is denoted Ref-Overlap[P]. The third measure

is the Rank-Biased-Overlap (RBO) [53] between the lists retrieved for q and q computed at rank k with parameter p. In contrast to

the overlap measure, RBO also accounts for the ranks at which

398

Session 4B: Queries Information Needs, Queries, and Query Performance Prediction

documents appear; the resulting predictor is Ref-RBO[P]. We note that the fact that the overlap and RBO measures are based on information induced from the corpus and the retrieval method does not contradict the fact that, according to Figure 1, queries are generated only based on the information need. The same way users might utilize knowledge and assumptions about the corpus/retrieval method (e.g., the language and style used in the corpus) to formulate queries, the prediction method can utilize all information at hand so as to predict the use of a specific query given an example of another query representing the same information need.

3.3.1 Special-case predictors. To study the contribution to pre-
diction quality of the two factors that govern the utilization of
reference queries in Equation 8 -- i.e, the predicted performance
of the reference queries and their association with the query --
we consider two predictors that are special cases of our prediction model. The first, named OnlyAsso, assumes that p^(R = 1|i, q, c, m) in Equation 7 is the same constant for all q  Qi ; i.e., the reference queries are assumed to be equi-effective, and prediction is only
based on the association between the given query and the reference queries in Qi . Following Equation 8, the predicted value is:

POnlyAsso (q) d=ef

(1

-

)P

(q)

+



|

1 Qi

|

A(q, q).
q Qi

(9)

The second predictor assumes that all reference queries are associated to the same extent with q and hence are equi-likely to represent the information need i. The resultant prediction value, following Equation 8, is:

POnlyRef (q) d=ef

(1 - )P(q) +  1 |Qi |

P (q ).
q Qi

(10)

This prediction method, termed OnlyRef, is based on the following assumption: the unweighted average of the performance-prediction values assigned to queries representing the information need is a good approximation to the performance value that should be predicted for any query (q) representing the need.

4 EVALUATION
4.1 Experimental Setup
Two TREC datasets were used for experiments. The first is ROBUST which is composed of 528, 155 (mainly) news articles and is associated with 249 TREC topics6 (301-450 and 600-700). The second dataset is the Category B of the ClueWeb12 collection (CW12 hereafter), which is composed of around 50 million web pages, and is associated with 100 TREC topics (201-300). The set of queries per TREC topic includes the original topic title and additional query variations [3, 8]: human generated queries that represent the topic.7 We removed duplicate query variations per topic and queries with out-of-vocabulary terms. On average, there are 12.75 (with standard deviation of 6.81) and 46.11 (with standard deviation of 18.66) unique variations per topic for ROBUST and CW12, respectively.

6One topic was removed from the original set due to absence of relevant documents
in the relevance judgments files. 7The variations are available at https://tinyurl.com/robustuqv and https://tinyurl.com/
clue12uqv.

SIGIR '19, July 21­25, 2019, Paris, France
SIGIR '19, July 21­25, 2019, Paris, France
We applied Krovetz stemming to all queries and documents; stopwords on the INQUERY list were removed only from queries. The Indri toolkit (www.lemurproject.org) was used for experiments.
Following common practice in work on QPP [10, 31, 35, 43, 57], we use language-model-based retrieval: the query-likelihood model [45] was used to retrieve the document lists; retrieval scores are log query likelihood; document language models were Dirichlet smoothed with the smoothing parameter set to 1000 [58].
Our proposed framework is not limited to specific predictors; we aim to demonstrate consistent patterns on a set of commonly used post- and pre-retrieval predictors. We consider six pre-retrieval predictors that were shown to be highly effective in past work [22]: AvgIDF [15], MaxIDF [15], AvgSCQ [59], MaxSCQ [59], AvgVAR [59] and MaxVAR [59]. The post-retrieval predictors considered are Clarity [14], NQC [41], WIG [61], QF (query feedback) [60] and their UEF [42] counterparts. In addition, we report the performance for SMV [47] and the recently proposed RSD [36] predictor that was instantiated in our experiments using WIG.
To measure prediction quality, we use Pearson correlation between the true AP (at cutoff 1000) values attained for queries using relevance judgments and the values assigned to them by a predictor [10]. We also used Kendall's Tau for evaluation [10] for all of the experiments shown. All trends were consistent across both correlation measures, and due to space limitations, the Kendall's Tau results are omitted as they provide no additional insight.
The free-parameter values of the predictors were set using the train-test approach used in prior QPP work [25, 31, 35, 57]. Specifically, we randomly split the topics into two equal-sized sets, where each of the two sets served in turn as the test fold. The parameter values yielding the highest Pearson correlation (see above) over the training fold were applied to the test fold. The reported prediction quality of the split is the average prediction quality of the two test folds. The partitioning procedure was repeated 30 times and we report the average prediction quality over these 30 splits. Statistically significant differences are computed using the two-tailed paired t-test at a 95% confidence level with Bonferroni correction.
The number of top-retrieved documents in all the post-retrieval predictors except for UEF was selected from {5, 10, 25, 50, 100, 250, 500, 1000}. The number of documents used to construct relevance model #1 (RM1) [27] for Clarity and QF was set to values in the same range. For UEF we set the number of top-retrieved documents to values in {25, 50, 100, 250, 500, 1000}, used Pearson correlation to measure inter-list similarity as recommended by Shtok et al. [42], and constructed RM1 using the same number of documents used for measuring the similarity. The overlap between two retrieved lists in QF was computed at the following cutoff values: {5, 10, 25, 50, 100}. For Clarity, QF, UEF and RSD, RM1 was constructed from unsmoothed document-language models and the number of terms was clipped to 100 [31]; for RSD we also experimented with an unclipped RM1 that was reported to be effective in prior work [44]. In addition, for RSD we used 100 sampled lists. For the list-based inter-query association measures used in our approach, Overlap and RBO, the cutoff k is selected from {5, 10, 25, 50, 100, 250, 500}; this range of values was also used to set the sampled-list size in RSD. The value of p in RBO is set to 0.95 following prior recommendations [43]. The value of  was selected from {0, 0.1, . . . , 1}.

399

Session 4B: Queries
SIGIR '19, July 21­25, 2019, Paris, France
Table 1: Prediction quality with respect to query effectiveness: TREC title queries (Title) and query variations with the maximal (Max), median (Med) and minimum (Min) AP per topic. The top (bottom) block presents previously proposed pre-retrieval (post-retrieval) predictors. The best prediction quality per corpus and predictor is boldfaced. The highest number in each column is underlined.

ROBUST

CW12

Title Max Med Min Title Max Med Min

AvgIDF AvgSCQ AvgVar MaxIDF MaxSCQ MaxVar

.357 .415 .314 .133 .432 .458 .327 .233 .243 .339 .281 .220 .440 .469 .328 .247 .405 .466 .378 .247 .421 .453 .337 .214 .396 .412 .391 .235 .377 .326 .369 .297 .338 .385 .375 .297 .400 .373 .424 .347 .420 .456 .441 .354 .372 .378 .374 .290

Clarity

.409 .384 .460 .417 .029 .130 .213 .189

NQC

.477 .551 .435 .166 .509 .548 .393 .181

WIG

.475 .511 .454 .391 .535 .549 .500 .416

SMV

.424 .535 .411 .159 .462 .520 .320 .183

RSD

.489 .521 .376 .185 .549 .574 .325 .249

QF

.487 .391 .356 .429 .280 .405 .248 .248

UEF(Clarity) .522 .517 .541 .468 .276 .294 .263 .292

UEF(NQC) .523 .558 .444 .236 .438 .435 .355 .288

UEF(WIG) .509 .385 .367 .352 .441 .387 .333 .348

UEF(QF)

.495 .444 .435 .451 .345 .298 .263 .324

Table 2: Main result: Prediction quality of Ref-RBO when
predicting performance for queries which are TREC's topic titles. The baseline is applying P to the title query as is standard. `' marks statistically significant differences with the
baseline. The best result in a column is underlined.

P
AvgIDF AvgSCQ AvgVar MaxIDF MaxSCQ MaxVar
Clarity NQC WIG SMV RSD QF UEF(Clarity) UEF(NQC) UEF(WIG) UEF(QF)

ROBUST

baseline
.357 .243 .405 .396 .338 .420
.409 .477 .475 .424 .489 .487 .522 .523 .509 .495

Ref-RBO
.603 .595 .604 .611 .601 .606
.598 .586 .590 .584 .568 .588 .603 .579 .589 .575

CW12

baseline
.432 .440 .421 .377 .400 .372
.029 .509 .535 .462 .549 .280 .276 .438 .441 .345

Ref-RBO
.669 .696 .673 .627 .675 .637
.613 .664 .691 .646 .650 .662 .578 .658 .658 .594

4.2 Experimental Results
4.2.1 Query effectiveness. The classic QPP task is to predict retrieval effectiveness for queries, where each represents a different

SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.

1.0

Variations

0.8

Median

Title

0.6

ROBUST

AP

0.4

0.2

0.0 0

50

100

150

200

249

Topic

1.0

Variations

0.8

Median Title

CW12

0.6

AP

0.4

0.2

0.0 0

50

100

Topic

Figure 2: The retrieval effectiveness (AP@1000) of query variations and TREC title queries for each topic. Each point on the x-axis is a different topic; the topics are ordered on the x-axis by the median effectiveness -- represented by the curves -- of all known variants for the topic.

information need (topic). In most work to date, a TREC topic title was used as the representative query for a topic. In Table 1 we study the prediction quality when queries of different effectiveness are used to represent each topic: the query variation with the highest AP (Max), median8 AP (Med) and lowest AP (Min) per topic9.
Overall, we see that the best prediction quality is in most cases attained when the variation with the maximal AP (Max) represents the topic. (Refer to the boldfaced numbers.) The lowest prediction quality is almost always observed when the chosen query is the one with the minimal AP (Min). More generally, we see that prediction quality varies considerably depending on the effectiveness of the queries used. For example, for ROBUST, the best predictor for title queries is UEF(NQC) (0.523) whereas for Min queries it is UEF(Clarity) (0.468); i.e., the decision about the best predictor to use also appears to depend on the effectiveness of the query used to represent the information need (topic). To shed some light on this phenomenon, in Figure 2 we present the retrieval effectiveness (AP@1000) attained for the title queries, and all additional query
8For topics with an even number of query variations, the larger of the two middle values was chosen. 9Query variations with a zero AP were omitted for this analysis; there are 71 such variations in CW12 representing 31 topics and 34 in ROBUST representing 11 topics.

400

Session 4B: Queries
Information Needs, Queries, and Query Performance Prediction
Table 3: Prediction quality when using different inter-query association measures in our approach. `b' and `r': statistically significant differences with baseline and Ref-RBO in a column (except for OnlyAsso), respectively. `o': statistically significant differences with OnlyAsso in a row. Bold: best result in a column per dataset.

ROBUST

MaxIDF AvgSCQ WIG UEF(Clarity) OnlyAsso

baseline

.396 .243 .475

.522

-

Ref-Jaccard Ref-Overlap Ref-RBO Ref-Geo OnlyRef

.....646331819934153rbobrbroboo

....535296945413rbobroboo .290rb

.506rbo .588b .590b .475ro .471rb

.....655550592333912rbobrbroboo

.269 .582 .588 .187
-

CW12

MaxIDF AvgSCQ WIG UEF(Clarity) OnlyAsso

baseline

.377 .440 .535

.276

-

Ref-Jaccard Ref-Overlap Ref-RBO Ref-Geo

....656372270773rbobrrbooo

....766420934868rbobrrboo

..760133rbrboo .691b .534ro

....655225772185rbobrrbooo

.504 .720 .685 .071

OnlyRef

.383r .490rb .537r

.447rb

-

variations per topic. We see that the difference in retrieval effectiveness among query variations per topic can be quite striking. While for some queries the AP is nearly 0, most topics have at least one variant with an AP higher than 0.3. In addition, we see that using the TREC title queries for retrieval does not necessarily yield median retrieval effectiveness per topic. For some topics, the performance can be much better or much worse than the median. In other words, the relative effectiveness of the title queries in representing the underlying information need (topic) varies across topics.
4.2.2 Main result: Using reference queries for prediction. We now turn to study the merits of using reference queries to predict retrieval effectiveness. In this section, and in Sections 4.2.3-4.2.6, prediction quality is evaluated using standard practice in work on QPP [10]; that is, prediction is performed for a set of queries, each of which is the title of a different TREC topic. In Section 4.2.7 we evaluate prediction quality when each topic is represented by a query variation which is not necessarily the topic title.
Table 2 presents the prediction quality of Ref-RBO, which was derived from Equation 8 using RBO as the inter-query association measure. We show in Section 4.2.3 that RBO is one of the most effective inter-query association measures among those considered. Hence, Ref-RBO is the main instantiation of our approach that we focus on throughout this section. We hasten to point out that the prediction quality patterns we report for Ref-RBO are consistent with those attained when using the other inter-query association measures, as exemplified in Section 4.2.3.
We can see in Table 2 that for all the considered predictors P, for both datasets, Ref-RBO, which relies on reference queries, substantially and statistically significantly outperforms the baseline:

SIGIR '19, July 21­25, 2019, Paris, France
SIGIR '19, July 21­25, 2019, Paris, France
ROBUST Ref-RBO
0.6

0.5

Pearson

0.4 0.3
0.0 0.7

AvgSCQ MaxIDF UEF(Clarity) WIG

0.2

0.4

0.6

0.8

1.0



CW12 Ref-RBO

0.6

Pearson

0.5

AvgSCQ

0.4

MaxIDF

UEF(Clarity)

0.3

WIG

0.0

0.2

0.4

0.6

0.8

1.0



Figure 3: The effect of  on prediction quality (Equation 8).
applying the predictor P to the title query, as is standard, without using reference queries. Note that this baseline is a specific case of our approach when setting  = 0 in Equation 8. The best prediction quality attained for Ref-RBO (refer to the underlined numbers) surpasses the best results attained by any baseline by a large margin. We can clearly conclude that there is much merit in using our QPP approach that utilizes reference queries.
4.2.3 Inter-query association measures. Table 3 presents the prediction quality of our approach with the various inter-query association measures proposed in Section 3.3. Hereafter, due to space limitations, we only report the results for two pre-retrieval and two post-retrieval predictors which yield the best prediction quality (per collection) in Table 2 when used in our approach.
In addition, we present the results for the two special cases of our approach: OnlyAsso, which assumes that reference queries are effective to the same extent (Equation 9), and OnlyRef, which assumes that reference queries are uniformly distributed (Equation 10). We also experimented with a previously proposed estimate for p(q|i) [44], termed Geo. We report the prediction quality of using Equation 8, where Geo is used instead of the inter-query association measure; the resulting predictor is denoted Ref-Geo.
We can see in Table 3 that our approach statistically significantly outperforms the baseline in the vast majority of cases regardless of the inter-query association measure employed. The best prediction quality is almost always attained for ROBUST by Ref-RBO and for CW12 by Ref-Overlap. The lowest numbers are observed for RefGeo; this is presumably because Geo is not effective in predicting

401

Session 4B: Queries SIGIR '19, July 21­25, 2019, Paris, France

Pearson

ROBUST Ascending

0.7

0.6

0.6 0.5
0.5

Pearson

0.4

AvgSCQ

0.4

MaxIDF

0.3

UEF(Clarity)

WIG

0.3

0

10

20

30

40

0

# of reference queries

CW12 Ascending

0.8

0.6

0.7

Pearson

Pearson

0.5

0.6

AvgSCQ

0.5

0.4

MaxIDF

0.4

UEF(Clarity)

0.3

WIG

0.3

0

10

20

30

40

0

# of reference queries

SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.
ROBUST Descending

AvgSCQ MaxIDF UEF(Clarity) WIG

10

20

30

40

# of reference queries

CW12 Descending

AvgSCQ MaxIDF UEF(Clarity) WIG

10

20

30

40

# of reference queries

Figure 4: The effect of the number of reference queries on the prediction quality of Ref-RBO (Equation 8) when reference queries are added in Ascending or Descending order of AP effectiveness.

performance for queries representing the same information need in our approach -- i.e., the reference queries we use.
We also see in Table 3 that even when all reference queries are assumed to be associated with the query to the same extent (OnlyRef), or when we do not use prediction for the reference queries in our approach (OnlyAsso), the prediction quality of our approach surpasses that of the baseline in the vast majority of the cases; nonetheless, using both estimates in our approach results in better prediction quality in the vast majority of cases.
As already noted, pre-retrieval predictors are typically more efficient than post-retrieval predictors because they can be computed before the retrieval is performed. All the predictors instantiated in our framework are post-retrieval. The only exception is when Ref-Jaccard is used together with a pre-retrieval predictor P. Interestingly, this combination results in very high prediction quality. A case in point, for CW12, the prediction quality of RefJaccard[AvgSCQ] (.608) surpasses that of all the baseline pre- and post-retrieval predictors presented in Table 2.
4.2.4 The effect of  on prediction quality. In Figure 3 we study the effect of  on the prediction quality of Ref-RBO. (Recall that  = 0 amounts to the baseline: applying an existing predictor directly to the query as is standard.) Similar patterns were observed for the other inter-query association measures considered. We see

that the best prediction quality is always attained for   0.7, i.e., when a high weight is given to the reference queries; yet, in most cases the optimal  is (slightly) smaller than 1, attesting to the additional contribution to prediction quality of also accounting for the prediction performed directly to the query.
4.2.5 The effectiveness of the reference queries. Thus far, all the available query variations served as the reference queries in our framework regardless of their retrieval effectiveness. In what follows, we divide the variations into two halves: the queries with the highest (High) and lowest (Low) AP values per topic. In Table 4 we study the merits of using each of the two sets (in comparison to using all variations) as reference queries. We observe the following: (i) using High yields better prediction quality than using Low or using all the variations; the differences are statistically significant in the vast majority of cases; and, (ii) using each of the sets (including Low) is superior to not using reference queries at all (i.e., the baseline); that is, using even poor variations as reference queries can be beneficial for prediction using our approach.
4.2.6 Varying the number of reference queries. We next study the effect of the number of reference queries on prediction quality. In Figure 4 we show the prediction quality of Ref-RBO as a function of the number of reference queries used; the queries are added one by

402

Session 4B: Queries
Information Needs, Queries, and Query Performance Prediction
Table 4: Prediction quality with respect to the effectiveness of reference queries. Low and High: using the set of queries with the highest and lowest AP values, respectively. All: using all queries. `b' and `a' mark statistically significant difference with the baseline and All, respectively. `l' marks statistically significant differences between Low and High.

ROBUST

CW12

P

Quantile baseline Ref-RBO baseline Ref-RBO

AvgSCQ

Low .243 High .243 All .243

MaxIDF

Low .396 High .396 All .396

Low .522 UEF(Clarity) High .522
All .522

Low .475

WIG

High .475

All .475

.495ab .440 .634abl .440 .595b .440 .525ab .377 .647abl .377 .611b .377 .562ab .276 .639abl .276 .603b .276 .533ab .535 .652abl .535 .590b .535

.555ab .730abl .696b .492ab .671abl .627b .413ab .636abl .578b .577ab .721abl .691b

one either in ascending or descending order of AP effectiveness10. We can see that the highest prediction quality is attained when using a relatively small number of highly effective queries: only one query is needed in ROBUST and about five queries are required in CW12. However, when using less effective reference queries, a much larger number of queries is needed to reach the highest prediction quality. For CW12, prediction quality gradually improves as more queries are added. For ROBUST, a plateau is attained after adding about twenty queries.
4.2.7 Putting it all together. Thus far, we demonstrated the clear merits of our approach when predicting performance for titles of TREC topics which served for queries. In Table 1 we showed that prediction quality of existing predictors varies considerably depending on the effectiveness of the query for which performance is predicted. Obviously, the TREC title query might be the best or the worst in terms of representing the topic (information need). Indeed, Figure 2 showed that in some cases, the effectiveness of the title query can be quite different than the median effectiveness of variants representing the topic. So, an important question we consider next is whether our approach is effective in predicting performance for queries of varying effectiveness in terms of representation of the underlying information need. It is important to differentiate this question from the one we explored in Section 4.2.5: the impact on prediction quality of using reference queries of different effectiveness to predict the performance of title queries.
Table 5 present the results for Ref-RBO when prediction is performed for the queries with the highest (Max), median (Med) and lowest (Min) AP per topic. We present for reference the prediction quality of predicting performance for title queries. All variations
10If the number of variations (on the x-axis) exceeds the number of variations for a specific topic, all variations available for this topic are used as reference queries.

SIGIR '19, July 21­25, 2019, Paris, France
SIGIR '19, July 21­25, 2019, Paris, France
Table 5: Prediction quality of Ref-RBO for queries with the maximal (Max), median (Med) and minimal (Min) AP, and for the title queries. `' marks statistically significant differences with the baseline: applying P directly to the query as is standard. Best result in a column in a block is boldfaced.

P AvgSCQ MaxIDF UEF(Clarity) WIG

ROBUST

CW12

baseline Ref-RBO baseline Ref-RBO

Max .339

.583

.469

.747

Med .281

.631

.328

.660

Min .220

.724

.247

.695

Title .243

.595

.440

.696

Max .412

.588

.326

.697

Med .391

.627

.369

.614

Min .235

.694

.297

.688

Title .396

.611

.377

.627

Max .517

.583

.294

.659

Med .541

.644

.263

.557

Min .468

.698

.292

.725

Title .522

.603

.276

.578

Max .511

.595

.549

.774

Med .454

.644

.500

.698

Min .391

.714

.416

.688

Title .475

.590

.535

.691

available for a topic are used as reference queries. We can conclusively see that our approach substantially outperforms the baseline regardless of the effectiveness of the query for which prediction is performed. This finding attests to the robustness of our approach with respect to existing predictors.
5 CONCLUSIONS AND FUTURE WORK
We demonstrated important connections between a query, an information need, and the prediction quality achievable with many commonly used query performance predictors. Specifically, we showed that the relative prediction quality patterns of existing predictors can substantially vary with respect to the effectiveness of the queries for which performance is predicted.
Accordingly, we reformulated the probabilistic foundation of the query-performance-prediction (QPP) task by explicitly accounting for the underlying information need and its connection to queries used to represent it. We then presented a novel QPP approach that incorporates additional information from the information need, in the form of queries that can represent it. The approach, which can be instantiated using any existing performance predictor, dramatically improves prediction quality irrespective of the effectiveness of the query for which prediction is performed, or of the QPP method used to instantiate the approach.
We intend to continue our exploration of the relationship between predicting performance for queries representing different information needs and predicting performance for queries representing the same information need, the latter of which remains as a grand challenge for the IR community.

403

Session 4B: Queries
SIGIR '19, July 21­25, 2019, Paris, France
Acknowledgements. We thank the reviewers for their comments.
This work was partially supported by the Israel Science Foundation (grant no. 1136/17), the Australian Research Council's Discovery Projects Scheme (DP170102231), a Google Faculty Award, and an Amazon Research Award.
REFERENCES
[1] G. Amati, C. Carpineto, and G. Romano. 2004. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR. 127­137.
[2] J. A. Aslam and V. Pavlu. 2007. Query Hardness Estimation Using Jensen-Shannon Divergence Among Multiple Scoring Functions. In Proc. of ECIR. 198­209.
[3] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. 2016. UQV100: A Test Collection with Query Variability. In Proc. of SIGIR. 725­728.
[4] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. 2017. Retrieval Consistency in the Presence of Query Variations. In Proc. of SIGIR. 395­404.
[5] N. Balasubramanian and J. Allan. 2010. Learning to select rankers. In Proc. of SIGIR. 855­856.
[6] N. J. Belkin, C. C., W. B. Croft, and J. P. Callan. 1993. The effect of multiple query representations on information retrieval system performance. In Proc. of SIGIR. 339­346.
[7] N. J. Belkin, P. Kantor, E.A. Fox, and J.A. Shaw. 1995. Combining evidence of multiple query representation for information retrieval. Information Processing and Management 31, 3 (1995), 431­448.
[8] R. Benham and J. S. Culpepper. 2017. Risk-Reward Trade-offs in Rank Fusion. In Proc. of ADCS. 1­8.
[9] Y. Bernstein, B. Billerbeck, S. Garcia, N. Lester, F. Scholer, and J. Zobel. 2005. RMIT University at TREC 2005: Terabyte and Robust Track. In Proc. of TREC-14.
[10] D. Carmel and E. Yom-Tov. 2010. Estimating the Query Difficulty for Information Retrieval. Morgan & Claypool Publishers.
[11] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. 2006. What makes a query difficult?. In Proc. of SIGIR. 390­397.
[12] A.-G. Chifu, L. Laporte, J. Mothe, and Md Z. Ullah. 2018. Query Performance Prediction Focused on Summarized Letor Features. In Proc. of SIGIR. 1177­1180.
[13] N. Craswell and M. Szummer. 2007. Random walks on the click graph. In Proc. of SIGIR. 239­246.
[14] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2002. Predicting query performance. In Proc. of SIGIR. 299­306.
[15] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2004. A Language Modeling Framework for Selective Query Expansion. Technical Report IR-338. Center for Intelligent Information Retrieval, University of Massachusetts.
[16] R. Cummins. 2011. Predicting Query Performance Directly from Score Distributions. In Proc. of AIRS. 315­326.
[17] R. Cummins. 2014. Document Score Distribution Models for Query Performance Inference and Prediction. ACM Transactions on Information Systems 32, 1 (2014), 2.
[18] R. Cummins, J. M. Jose, and C. O'Riordan. 2011. Improved query performance prediction using standard deviation. In Proc. of SIGIR. 1089­1090.
[19] V. Dang, M. Bendersky, and W. B. Croft. 2010. Learning to rank query reformulations. In In Proc. of SIGIR. 807­808.
[20] F. Diaz. 2007. Performance prediction using spatial autocorrelation. In Proc. of SIGIR. 583­590.
[21] C. Hauff, L. Azzopardi, and D. Hiemstra. 2009. The Combination and Evaluation of Query Performance Prediction Methods. In Proc. of ECIR. 301­312.
[22] C. Hauff, D. Hiemstra, and F. de Jong. 2008. A survey of pre-retrieval query performance predictors. In Proc. of CIKM. 1419­1420.
[23] B. He and I. Ounis. 2004. Inferring Query Performance Using Pre-retrieval Predictors. In Proc. of SPIRE. 43­54.
[24] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proc. of WWW. 387­396.
[25] O. Kurland, A. Shtok, S. Hummel, F. Raiber, D. Carmel, and O. Rom. 2012. Back to the Roots: A Probabilistic Framework for Query-performance Prediction. In Proc. of CIKM. 823­832.
[26] K. Kwok, L. Grunfeld, H. Sun, P. Deng, and N. Dinstl. 2004. TREC 2004 Robust Track Experiments using PIRCS. In Proc. of TREC-13.
[27] V. Lavrenko and W. B. Croft. 2001. Relevance-Based Language Models. In Proc. of SIGIR. 120­127.
[28] D. Metzler and W. B. Croft. 2005. A Markov random field model for term dependencies. In Proc. of SIGIR. 472­479.
[29] J. Mothe and L. Tanguy. 2005. Linguistic features to predict query difficulty. In ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and Applications. http://www.haifa.il.ibm.com/sigir05-qp/papers/Mothe.pdf

SIGIR '19, July 21­25, 2019, Paris, France
O. Zendel et al.
[30] J. Pérez-Iglesias and L. Araujo. 2010. Standard Deviation as a Query Hardness Estimator. In Proc. of SPIRE. 207­212.
[31] F. Raiber and O. Kurland. 2014. Query-performance prediction: Setting the expectations straight. In Proc. of SIGIR. 13­22.
[32] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at TREC-3. In Proc. of TREC-3.
[33] H. Roitman. 2018. An Extended Query Performance Prediction Framework Utilizing Passage-Level Information. In Proc. of ICTIR. 35­42.
[34] H. Roitman. 2018. Query Performance Prediction using Passage Information. In Proc. of SIGIR. 893­896.
[35] H. Roitman, S. Erera, O. S. Shalom, and B. Weiner. 2017. Enhanced Mean Retrieval Score Estimation for Query Performance Prediction. In Proc. of ICTIR. 35­42.
[36] H. Roitman, S. Erera, and B. Weiner. 2017. Robust Standard Deviation Estimation for Query Performance Prediction. In Proc. of ICTIR. 245­248.
[37] H. Scells, L. Azzopardi, G. Zuccon, and B. Koopman. 2018. Query Variation Performance Prediction for Systematic Reviews. In Proc. of SIGIR. 1089­1092.
[38] F. Scholer and S. Garcia. 2009. A case for improved evaluation of query difficulty prediction. In Proc. of SIGIR. 640­641.
[39] F. Scholer, H. E. Williams, and A. Turpin. 2004. Query association surrogates for Web search. JASIST 55, 7 (2004), 637­650.
[40] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. 2011. LambdaMerge: merging the results of query reformulations. In Proc. of WSDM. 795­804.
[41] A. Shtok, O. Kurland, and D. Carmel. 2009. Predicting query performance by query-drift estimation. In Proc. of ICTIR. 305­312.
[42] A. Shtok, O. Kurland, and D. Carmel. 2010. Using statistical decision theory and relevance models for query-performance prediction. In Proccedings of SIGIR. 259­266.
[43] A. Shtok, O. Kurland, and D. Carmel. 2016. Query Performance Prediction Using Reference Lists. ACM Trans. Inf. Syst. 34, 4 (2016), 19:1­19:34.
[44] M. Sondak, A. Shtok, and O. Kurland. 2013. Estimating query representativeness for query-performance prediction. In Proc. of SIGIR. 853­856.
[45] F. Song and W. B. Croft. 1999. A general language model for information retrieval. In Proc. of SIGIR. 279­280.
[46] K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments - Part 1. Information Processing and Management 36, 6 (2000), 779­808.
[47] Y. Tao and S. Wu. 2014. Query Performance Prediction By Considering Score Magnitude and Variance Together. In Proc. of CIKM. 1891­1894.
[48] P. Thomas, F. Scholer, P. Bailey, and A. Moffat. 2017. Tasks, Queries, and Rankers in Pre-Retrieval Performance Prediction. In Proc. of ADCS. 11:1­11:4.
[49] S. Tomlinson. 2004. Robust, Web and Terabyte Retrieval with Hummingbird Search Server at TREC 2004. In Proc. of TREC-13.
[50] Eduardo Vicente-López, Luis M. Campos, Juan M. Fernández-Luna, and Juan F. Huete. 2018. Predicting IR Personalization Performance Using Pre-retrieval Query Predictors. J. Intell. Inf. Syst. 51, 3 (2018), 597­620.
[51] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. 2006. On ranking the effectiveness of searches. In Proc. of SIGIR. 398­404.
[52] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiments and evaluation in information retrieval. The MIT Press.
[53] W. Webber, A. Moffat, and J. Zobel. 2010. A Similarity Measure for Indefinite Rankings. ACM Trans. Inf. Syst. 28, 4, Article 20 (Nov. 2010), 38 pages.
[54] M. Winaver, O. Kurland, and C. Domshlak. 2007. Towards robust query expansion: Model selection in the language model framework to retrieval. In Proc. of SIGIR. 729­730.
[55] D. Yin, Y. Hu, J. Tang, T. Daly, M. Zhou, H. Ouyang, J. Chen, C. Kang, H. Deng, C. Nobata, J.-M. Langlois, and Y. Chang. 2016. Ranking relevance in yahoo search. In Proc. of KDD. 323­332.
[56] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. 2005. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proc. of SIGIR. 512­519.
[57] H. Zamani, W. B. Croft, and J. S. Culpepper. 2018. Neural Query Performance Prediction using Weak Supervision from Multiple Signals. In Proc. of SIGIR. 105­ 114.
[58] C.-X. Zhai and J. D. Lafferty. 2001. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. In Proc. of SIGIR. 334­342.
[59] Y. Zhao, F. Scholer, and Y. Tsegay. 2008. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proc. of ECIR. 52­64.
[60] Y. Zhou and W. B. Croft. 2006. Ranking robustness: a novel framework to predict query performance. In Proc. of CIKM. 567­574.
[61] Y. Zhou and W. B. Croft. 2007. Query performance prediction in web search environments. In Proc. of SIGIR. 543­550.

404

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations

Laura Dietz
University of New Hampshire
Durham, NH, USA
dietz@cs.unh.edu

ABSTRACT
Related work has demonstrated the helpfulness of utilizing information about entities in text retrieval; here we explore the converse: Utilizing information about text in entity retrieval. We model the relevance of Entity-Neighbor-Text (ENT) relations to derive a learning-to-rank-entities model.
We focus on the task of retrieving (multiple) relevant entities in response to a topical information need such as "Zika fever". The ENT Rank model is designed to exploit semi-structured knowledge resources such as Wikipedia for entity retrieval. The ENT Rank model combines (1) established features of entity-relevance, with (2) information from neighboring entities (co-mentioned or mentioned-on-page) through (3) relevance scores of textual contexts through traditional retrieval models such as BM25 and RM3.
ACM Reference Format: Laura Dietz. 2019. ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331257
1 INTRODUCTION
Entity retrieval is important in many different applications where entities are sought in response to a textual description, type definition, or set of related entities. Information needs in natural language, structured SPARQL queries, or hybrids have been explored [6]. Often only a single entity is requested, such as in factoid question answering, conversational retrieval, or quizzes. In contrast, this work1 studies entity retrieval where, in response to a short information need, all topically related entities are to be retrieved. The motivation is to support authors in writing comprehensive articles about (yet) unfamiliar topics. While the information need is only expressed in a short keyword query, the topic is expected to have several interesting facets which should all be covered. We anticipate that knowing the set of relevant entities, ordered from central to side-topic is helpful for the author. Results also can inform conversational agents with background information on this
1Code and data available at https://www.cs.unh.edu/~dietz/appendix/ent-rank/
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07...$15.00 https://doi.org/10.1145/3331184.3331257

topic. As our approach is modeling the context of relevant entities, this work also constitutes a first step towards fully automatic article composition approaches or even a new way to find information rather than documents through search engines [1].
An example topic is "Zika fever". Despite being a short unambiguous keyword query, several facets need to be covered such as "Signs and Symptoms", "Causes", or "Epidemiology in Americas". Several entities must be mentioned for this topic, such as "Aedes Mosquitoes" which are the vector for transmission, the "2015­2016 Zika fever epidemic" and other outbreaks, that Zika fever is a "Flavivirus", it causes muscle weakness due to nerve damage also called "Guillain-Barré syndrome", the "Neonatal infection" which is the most serious concern, and that "Dengue Fever" is a similar disease with confusable symptoms, and that "Lethal ovitraps" are used to trap adult Aedes mosquitos. Many, but not all of these relevant entities are mentioned on the Wikipedia page for Zika fever2.
Topical entity retrieval task: Given an article title as query, retrieve a ranking of relevant entities. Relevance is defined based on whether the entity must, should, or could be mentioned in an article on this topic.
The entity retrieval task of the TREC Complex Answer Retrieval track [14] (CAR) offers a suitable benchmark to study this task. The benchmark includes a large amount of training data that is synthetically derived from entities mentioned on Wikipedia pages. This benchmark is complemented by manual assessments. While the official CAR queries provide titles with an outline of facets (e.g., "Zika fever/Causes"), this work focuses on retrieving entities in response to the title queries alone.
The CAR benchmark includes an easily parsable dump of English Wikipedia pages (December 2016). The structure of each Wikipedia page, i.e., headings, paragraphs, and entity links are provided for each page, as well as meta data such as redirect names, categories, and inlinks. Query pages are excluded from the dump. In this work we make use of this format, which could also be derived for other text-centric knowledge resources in bio-medical (NCBI/pubmed), finance (Bloomberg), and news (Washington Post) domains or websites such as www.howstuffworks.com.
Current entity retrieval approaches focus on the development of relevance features. One set of features is derived from a knowledge graph (or Wikipedia), such as names, types, linked entities, and free-form descriptions [3, 36]. Another set of features is derived from entity links in queries and unstructured text documents [18, 31]. Neighbor relations are derived from knowledge graph links [3] or co-mentions in text (i.e, two entities being mentioned in near proximity) [21]. So far, related work would consider all neighbor
2 https://en.wikipedia.org/wiki/Zika_fever

215

Session 2C: Knowledge and Entities
SIGIR '19, July 21­25, 2019, Paris, France
relations from the same source as equally important for the query. In contrast, this work models the relevance of neighbor relations through textual contexts with different measures of relevance.
Contributions. We introduce the ENT Rank framework for integrating relevance information from the entity, its neighbors, and context. We provide a versatile learning-to-rank-entities algorithm that can be optimized for any rank evaluation metric, such as meanaverage precision. The ENT Rank framework can incorporate any existing entity relevance feature and can be easily customized. We demonstrate that even with simple features derived from unfielded unigram models, such as BM25 and RM3, ENT Rank provides a competitive retrieval method. In a comparison between ENT Rank and established methods on TREC Complex Answer Retrieval [14] and DBpedia-entity v2 [20], ENT Rank places best or second-best.
The idea behind ENT Rank is to use text fragments with entity links, so-called contexts, to define neighbor relations between entities. This allows us to derive a hypergraph, where entities are represented as nodes, and context-neighbor relations are represented as edges. Preserving the association between each context and neighbor relation, allows us to use text-retrieval models to predict the relevance of a neighbor relation for the query. Furthermore, relevance information from context-neighbor relations is used to complement traditional entity relevance features.
Outline. In Section 2 we provide an overview of the state-of-theart on this task. Section 3 introduces the ENT Rank framework and motivates different special cases through random walks. Section 4 discusses the entity, neighbor, and context features used in the experimental evaluation using entity retrieval benchmarks from Complex Answer Retrieval in Section 5 and DBpedia-entity v2 in Section 6.
2 RELATED WORK
Entity retrieval was introduced to integrate information retrieval and semantic search [3]. It is often motivated by the large number of named entities mentioned in search requests [28]. Different flavors of this task are to retrieve related entities through a set of entities, type descriptions, or topics of expertise [4, 5, 12]. Entity retrieval can provide answers to questions, such as "Who invented the paper clip?" In the context of Complex Answer Retrieval, entity retrieval offers entities that should to be discussed in different parts of the answer [14].
Entity retrieval from knowledge base documents. Successful approaches to all these variants of entity retrieval center around a representation of each entity as a fielded document. After full-text indexing, entity retrieval can be addressed by traditional retrieval models for ad hoc document retrieval [28]. Tonon et al. combine full text search with structured queries [33]. Balog et al. [2] suggests a term-based and category-based entity representation, where term statistics are derived from documents representing the entity (such as a Wikipedia page). For queries consisting of terms, categories, and related entities, Balog et al. use a generative retrieval model based on Kullback-Leibler divergence of entity and category language model. Meij et al. [24] further include information from search histories. Garigliotti et al. focus on category or type information [16].

SIGIR '19, July 21­25, 2019, Paris, France
Laura Dietz
Raviv et al. [29] extend the sequential dependence model (SDM) [26] to different entity fields, name, document, and type. Zhiltsov [27, 36] suggests the parametrized fielded sequential dependence model (PFSDM), which assigns different weights to matches of different fields, query term types, and bigrams. The retrieval approach is based on the weighted sequential dependence model [8], which combines unigram, bigram, window bigrams with additional information using a Markov random field. The weights for these features are trained with coordinate ascent. Chen et al. [9] demonstrates that learning-to-rank frameworks offer further improvements.
Entity linking tools annotate unstructured text with mentions of entities, providing a new avenue for entity retrieval. Hasibi et al. [18] applies entity linking to queries, to extend the SDM approach with another dependency. Schuhmacher et al. uses entity links in web documents for entity retrieval in a pseudo-relevance feedback approach: Inspecting retrieved web documents, entities are ranked high if they are mentioned in (many) high-ranked documents.
Ad hoc document retrieval with entities. By approaching entity retrieval as retrieval of fielded documents, combinations of ad hoc entity retrieval and document retrieval explored. Raviv et al. [30] suggests to represent queries and documents as bag-of-words and bag-of-entity-links for ad hoc document retrieval tasks. Liu et al. [23] rank documents through relevant entities. While the relevance of entities is latent, indicators of entity relevance are derived from entity links and Freebase abstracts. Xiong et al. suggests a discriminative machine learning approach to incorporate different meta information about entities into the document ranking model. Dalton et al. [11] compute an entity-term-category expansion model based on a feedback run of retrieved documents and sources of entity information: entity links in the query, a ranking of Wikipedia pages (i.e., an entity ranking), and entity link information in documents. Recently, neural network approaches for joint entity-document ranking are further leveraging this connection [35].
Entity linking. Entity linking methods annotate unstructured text with hyperlink-like positional references to Wikipedia. Fast and reliably entity linking toolkits, such as TagMe and Nordlys [15, 19], are readily available. Entity linking combines spotting of possible entity mentions with the disambiguation among similarly named entities. Several information retrieval approaches to entity linking use the fielded entity representations discussed above [17]. Text surrounding the spot can be cast as a search query for entity retrieval [10]. Special features of short text such as tweets [25] can be incorporated.
Graphs, Relations, and Neighbors. Knowledge graphs contain information about how entities are related through RDF triples with relation types. Alternatively, relations with "cheap semantics" [3] can be derived from hyperlinks on Wikipedia, or entities that are mentioned in the same document. Kotov et al. [21] combines both explicit relations available from ConceptNet together with information which entities are mentioned near one another (cf. HAL [21]). With application to question answering, Bast et al. [7] learn weights on different relations by matching corpus-based templates to demanded relation types. This approach is based on the idea of

216

Session 2C: Knowledge and Entities
ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations

SIGIR '19, July 21­25, 2019, Paris, France
SIGIR '19, July 21­25, 2019, Paris, France

weakly supervised relation extraction to generate training data for relevant relations.
3 ENT-RANK APPROACH
The difficulty of using neighbor relations for entity retrieval lies in the presence of many connections of which the majority are typically not relevant for the query. One example is the entity "South America" which is relevant for the query "Zika fever" as a location of a major outbreak. However, many contexts about South America are unrelated to the Zika fever, such as political incidents or environmental issues due to the loss of rainforest. In fact, there are so many interesting topics to discuss about South America that there is no room to mention the Zika fever outbreak on South America's Wikipedia page.
We notice an asymmetry of relevance: just because South America is relevant for a discussion of the Zika fever, it does not mean that the Zika fever is equally relevant for a discussion about South America. Therefore, short entity descriptions, such as the introductory Wikipedia paragraph, are often not mentioning relevant connections. We compensate this lack with a text-oriented approach. We hypothesize that whenever two entities are mentioned in a relevant context, it is a strong indicator that both entities are topically relevant. The relevance of the context is predicted through textbased retrieval models. We define the relevance of context-neighbor relations based on the relevance of contexts and entities. We use entity links in context to estimate (1) the relevance of an entity and (2) the relevance of the neighbor relations. This has the advantage that we can access a wide range of facts about entities, including those with vague semantics that do not fit easily in a relation schema. Our approach is intended to be combined with established entity relevance models discussed in the related work.
The remainder of this section introduces the construction of the ENT Rank framework. In response to a query, (1) a ENT hypergraph is defined. (2) Preserving entities as nodes, the hypergraph is converted into the binary ENT multi-graph G, which (3) is associated with query-specific node and edge feature vectors to obtain the ENT feature vector graph G. This graph is used in (4) the ENT learning-to-rank-entities model for training and ranking prediction.
3.1 ENT Hypergraph and Binary Multi-Graph
All Wikipedia pages (or alternatively, documents from any corpus) are annotated with entity links and segmented into contexts, such as paragraphs, sections, and pages. Each context induces a hyperedge between all entities that are linked therein. The association between hyperedge and the context is preserved. The construction is depicted in Figure 1.
This approach offers the option of a full-text search index from which hyperedges can be retrieved with different retrieval models such as BM25. We suggest to create the graph from several input rankings of entities e and contexts t. The hypergraph forms the basis for reasoning about the relevance of edges.
Given a search query, the ENT Rank approach formalizes the connections between entities ei , contexts tk , and neighboring entities ej as a binary multi-graph G = (V , R), where nodes V represent entities ei and edges R represent directed context-neighbor

Derived ENT hyper-graph

Pages with entity links are split into contexts which induce neighbor relations

Legend: Context Entity Neighbors Entity link Owner Identity

Figure 1: ENT Hypergraph is created from contexts with entity links. Example contexts are paragraphs, pages, and sections on Wikipedia pages.
relations r = (ei , tk , ej ). Figure 2 depicts an example with two contexts t1, t2 that mentions three entities, e1, e2, e3. They induce a graph G with V = {e1, e2, e3}, and multi-edges R for each of the six directed neighbor relations (e1, t1, e2), (e2, t1, e1), (e2, t1, e3), . . . from t1 and two directed neighbor relations (e2, t2, e3), (e3, t2, e2) from context t2. As both contexts mention entity e2 and e3, these induce two edges from e1 to e2 depicted in black and gray (hence a multi-graph).
3.2 ENT Feature Vector Graphs
We endow nodes ei  V and context-neighbor edges (ei , tk , ej ) in G with feature vectors as follows, deriving the ENT feature vector graph G for the search query. Node feature vectors fìei are comprised of features that indicate the (direct) relevance of the entity ei for the query. Many established entity relevance features have been discussed in the related work--these are directly applicable to the ENT-Rank model as node features. The novel contribution of ENT rank lies in the incorporation of relevance indicators from context-based neighbor relations. Every multi-edge (ei , tk , ej ) is endowed with an edge feature vector fì(ei,tk,ej ) that is comprised of features that indicate how relevant the context-based neighbor relation is for the query. A wide range of features can be included such as the relevance of the context measured by a BM25 score, the saliency of the entity in the context, the role of the neighbor relationship, the similarity of neighbors, and other entity features of the neighbor. The concrete list of features used in this work is given in Section 4.
3.3 ENT Learning-to-Rank-Entities Model
The major challenge in using the ENT Rank model for entity ranking is the vast amount of heterogeneous feature choices: Offering multiple sources for contexts, different neighbor roles, different entity (node) relevance features, different context (edge) relevance features can result in hundreds of combinations to explore. We suggest the following learning-to-rank approach to choose the ideal

217

Session 2C: Knowledge and Entities
SIGIR '19, July 21­25, 2019, Paris, France

Input contexts t1
e1 e2 e3

t2
e2 e3

Derived ENT multi-graph

e2

e1

e3

SIGIR '19, July 21­25, 2019, Paris, France
Laura Dietz

Legend:

Entity mentions in context

Node associated with entity ei

ei

Neighbor relation (ei, tk, ej )

Figure 2: The binarized ENT-multi graph is derived from contexts, where each context t1 and t2 induces a (directed) neighbor relation between all entity pairs that are mentioned in it. Multiple edges are induced between entities that co-occur in two
contexts, here e2 and e3.

weighted combination of these choices, given sufficient training data.
The entity ranking is derived from the ENT feature vector graph
G using a learning-to-rank model with weight parameters ì and
ì. As customary in learning-to-rank, the weight parameters are trained across many queries; dependence on the query is expressed through the features. We first discuss the prediction of a ranking, second how to train the weight parameters, and finally give a motivation that is based on random walks. We define features gì of an entity pair as,



gì(ei , ej ) =

fì(ei,tk,ej )

(1)

k :ei,ej tk

Ranking prediction. Given trained node and multi-edge parame-
ters ì and ì and a query-specific feature vector graph G. We define the rank score of an entity (i.e., node) ej as the sum of both linear models for nodes and multi-edges as follows. We will give a detailed motivation for this equation below.

score(ej )

=

ì (

fìej

1 +
|V | )(

 ì gì(ei , ej )
i

)

(2)

=

ì ì

1 |V

|

ifìegìj(ei , ej )

Here |V | is the number of nodes in the graph. The second line follows after rearranging inner vector products and stacking weight
parameters ì and ì into a single weight parameter vector which
contains all entries in ì followed by all entries in ì. Likewise, node and multi-edge feature vectors are stacked, after summing vectors across all multi-edges (ei , tk , ej ) that connect to ej . The summing here refers to a component-wise vector addition.

Training. The weight parameters are trained to achieve optimal entity ranking performance on the training set. In this work, we use mini-batched coordinate-ascent as a training algorithm, but other training algorithms are equally applicable. Coordinate-ascent is an iterative algorithm that optimizes the weight of one feature at at time in a round-robin fashion until no further improvement in ranking performance can be achieved. The ranking performance is evaluated with mean-average precision (MAP). We use a variant called mini-batch stochastic gradient ascent, which performs each

iteration on a different random subset of 150 training queries. The algorithm is stopped when the relative change in MAP is less than 1%. This convergence is usually achieved within 5 iterations, since our features are all positively correlated with relevance.

Motivation. The ENT learning-to-rank-entities model is inspired

by random walks with restarts [32], where P(ej ) is the probability of chosing node ej during restart. Due to space constraints, this work only discusses the simple case of weighted degree central-

ity, i.e., random walks with only one step, for which an analytic

solution to the optimization problem is available.

Nodes are initialized uniformly at random (i.e.,

1 |V

|

).

The

transi-

tion from node ei to ej is given by the transition probability P(ei 

ej |ei ) given that the random surfer is on the start node ei . Using

teleportation probability   (0, 1), transition probabilities are de-

noted as matrix T, where

Tij = P(ej ) + (1 -  )P(ei  ej |ei ).

Under degree centrality (i.e., one random walk step) the score

of the receiving node ej is

score(ej ) =

1 |V |

 Ti j
i

=

1 P(ej )+(1- ) |V |

( P (ei
i

)  ej |ei )

The fraction of |V | vanishes from the first term, when the teleport

is summed over all sending nodes.

For learning-to-rank-entities we model the restart probabilities

and transition probabilities as linear models of node feature vec-

tors ìf and edge feature vectors gì. The ratio of teleportation versus

otrfatnhseittiroanin( i1n-g

) is absorbed into parameters and estimated as part process. Since only a rank-equivalent rank score is

necessary, we let

P (ej ) ra=nk ììfej

(3)

(1 -  )P (ei  ej |ei ) ra=nk ì gì(ei , ej )

(4)

score(ej )

ra=nk ììfej

+

1 |V |

 ì gì(ei , ej )
i

(5)

These are combined into Equation 5. Thereby, we arrive at the formulation which is given in Equation 2.

218

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations

SIGIR '19, July 21­25, 2019, Paris, France

3.4 Options for Multi-edge Feature Vectors
We envision features ìf and gì in the ENT Rank feature graph G to be tailored to the application domain. Before providing details on the set of features used in this work, we discuss how we envision information about contexts, neighbors, and relation types to be integrated into the ENT Rank framework. Our suggestions are based on probabilistic random walks.

3.4.1 Neighbor features. When entity features of the sending neighbor ei are available, the feature vector of the multi-edge (ei , tk , ej ) can be derived by letting
fì(ei,tk,ej ) = ìf(ei ) Following Equation 2, this results in a rank score for ej that is

score(ej )

ra=nk ììfej

+

1 |V |


i k :ei,ej tk

ììfei

where ì and ì control the importance of different neighbor features versus entity features. This formulation naturally incorporates the multiplicity of multi-edges between ei and ej . In the context of semi-supervised classification, this model is also known as linear neighborhood propagation [34].

3.4.2 Relation-typed neighbor features. As mentioned earlier, different entities can play different roles in the context, such as being the owner of the context versus being mentioned in the context. These roles can define a type of the neighbor relation (e.g., ownerlink). Furthermore, different types of contexts can be considered, such as paragraph, section, or page. We suggest to incorporate different context and neighbor types of (ei , tk , ej ) as relation types r , by reserving separate blocks in the feature vector ìf for different relation types. These blocks can be stacked to obtain the feature vector ìf(ej , r ).
0
...
0 ìf(ej , r ) = ìf(ej )  block for relation type r
0
...
0

If across multiple contexts, entity ej and ei have different roles, we suggest to copy the entity feature vector of neighbor ei into all corresponding relation type blocks. The consequence is that the training algorithm would then not only learn to balance entity features of ej versus neighbor ei , but also assign different importance weights depending on neighbor relation type and context type.

3.4.3 Context-Relevance Features. When contexts are retrieved from a full-text index, contexts tk are naturally associated with features from text retrieval models, such as the retrieval score of the context under a BM25 model or the RM3 expansion models with different hyperparameters. Each of these retrieval models would contribute a separate relevance feature for the context tk (or a reasonable default value if the context is not included in the ranking).

We incorporate the case of context feature vectors based on re-

lated work [37] on random walks for hypergraphs. Zhou et al. sug-

gest the following random surfer process: A random surfer on node

ei first surfs to an adjacent hyper edge tk proportionally to its edge weight (tk ), next the surfer chooses one node adjacent to the hyperedge uniformly at random to surf to. This process includes the

possibility of surfing back to the starting node ei . Under this model, the transition probability from node ei to ej

vnisieagcTtithvkeieisinstcopboryerojrPp,eot(shepriteoio nmndaaserlgjtto|ioeniEa)|tlq1ktu|raatn(itoskkint):i.o4(Wneiinp,htrekoon,ubemjra)bfuie|ltl1aitktityp|ulrfe(retohkgmy)rp.aneporheddefgoeerismtcouolenaj--

tion where the feature vector for transition from ei to ej can be expressed as features of connecting hyperedges tk :

 ìf(ei , ej ) =
k :ei,ej tk

1 |tk

|

ìf(tk

)

Here |tk | denotes the number of entities mentioned in the context. In relation to Equation 1, it follows that

fì(ei,tk,ej )

=

1 |tk

|

ìf(tk

)

(6)

Our experiments empirically confirm that dividing hyperedge feature vectors by the number of neighbors provides slightly better
results than the unnormalized alternative, fì(ei,tk,ej ) = ìf(tk ).

3.4.4 Combinations of Multi-edge Features Vectors. We envision
that multi-edge feature vectors ìf are composed of both relationtyped neighbor features, context features, and many other feature sources by stacking feature vectors into one combined feature vector,

fì(ei,tk,ej ) =

ìf(ej , r )

1 |tk

|

ìf(tk

)

...

We use this representation to construct the edge feature vector for ENT Learning-to-rank-entities for use in Equation 1.

4 WIKIPEDIA FEATURES FOR ENT RANK
In this study we use the following set of retrieval-based features for entities feature vectors ìf(ej ) and multi-edge feature vectors fì(ei , tk , ej ) as described in Section 3.4.4. The features used in the evaluation are derived from a 2016 Wikipedia dump and a corpus of paragraphs (as provided with the TREC CAR data)--other datasets of knowledge graphs and text are equally applicable.
From the Wikipedia dump and a text corpus we extract the following types of information which are used as a source of contexts and/or entity relevance, from which we derive ENT feature vector graphs.
· Page: Full-text of Wikipedia pages, including all visible text including title, headings, and content paragraphs. For the graph only bi-directional entity links are included as neighbors (i.e., links to pages that link back).
· Entity: Knowledge graph representation of entities using only head information such as title, lead text, and name variations

219

Session 2C: Knowledge and Entities
SIGIR '19, July 21­25, 2019, Paris, France

derived from anchor text of incoming links, redirects, and disambiguations. This is the typical representation commonly used by entity linking methods such as TagMe [15]. The graph structure is derived only from bi-directional entity links. · Section: Sections (top-level) of a Wikipedia pages as a representation of topical entity aspects, which include heading and section content, as well as page title and lead text. For the graph, all outgoing entity links are used. · Paragraph: Paragraphs from the corpus with full text and entity links preserved. The graph structure is derived from entity links.
In this work use the TREC CAR benchmark. We derive page, entity, and section from the allButBenchmark data (omitting query pages) and derive paragraph data from the paragraphCorpus.

4.1 Entity and Context-Relevance Features

For each representation of page, entity, section, paragraph, we create an full-text search index with a single text field. Using this index and the keyword query (e.g., the page title or concatenation of headings), we use the following retrieval models to produce a ranking.

· BM25: The Lucene-BM25 model with default parameters without expansion.
· BM25-RM: A BM25 ranking with RM3-style query expansion on a BM25 feedback run.
· QL: The Querylikelihood model with Dirichlet smoothing (µ = 1500).
· QL-RM: QL ranking with RM3-style query expansion on a QL feedback run.

We use a fixed interpolation for RM variations for input runs: query

terms weighted by 1.0; expansion terms weighted by expansion

probability. We learn a refined interpolation between QL and QL-

RM as part of an larger learning-to-rank-entities model.

Drawing inspiration from the entity context model described by

Dalton et al. [11], we further include the following entity-expansion

model: We represent a pseudo-relevance feedback run of contexts

d as a bags-of-entities e. Using entity links instead of words, the

relevance model [22] is used to compute expansion entities as in

Equation 7.



pEcmX(e |q) = p(d |q)p(e |d)

(7)

d

We use entity-expansion model in two variations:

· EcmX: A ranking of expansion entities ranked by their expansion probability p(e |q).
· EcmPsg: Expanding BM25 or QL with top 20 expansion entities under p(e |q) to retrieve a new ranking of contexts via an RM3-like combination of query term matches in text field and expansion entity matches in the entity link field.

When multiple rankings are to be combined, an effective alterna-

tive to learning to rank is unsupervised rank aggregation. All dis-

tinct rank

items score

d across all rankings R from reciprocal ranks

are
R

assigned a new aggregated

1 rank(d

)

.

We

include

aggre-

gated rank features for entities and each context type:

SIGIR '19, July 21­25, 2019, Paris, France
Laura Dietz
· Entity feature Aggr: Rank aggregation across all entity rankings (i.e., rankings from page and entity index, and rankings with the EcmX expansion model).
· For each context type, Aggr: Rank aggregation across all context rankings of this type (paragraph, page, or section).
Feature vectors are derived from all of these rankings:
· Entity relevance: Features ìf are derived from rank scores. We use BM25, QL, BM25-RM, QL-RM, BM25-EcmPsg and QL-EcmPsg scores when retrieving from page and entity indexes in addition to the scores of the EcmX model on all representations.
· Context relevance: Features gì use rank scores of BM25, QL, BM25-RM, QL-RM, BM25-EcmPsg, and QL-EcmPsg retrieval from the context representation (paragraph, section, and page).
The ENT Hypergraph is created from the top 1000 of all entity rankings and context rankings. As we use retrieval models that only assign positive retrieval scores, missing features are set to zero. Finally, Z-score normalization is applied.
4.2 Relation-typed Neighbor Features
We include neighbor features as described in Section 3.4.1 based on entity features described above. The relation type is based on the context-type (paragraph, page, or section) and the roles two entities play the context. Here we only use two roles, Link if the entity is mentioned in the context or Owner if the context is derived from the Wikipedia page of the entity. For relation types of a multi-edge (ei , tk , ej ) we include all combinations of context type following neighbor-relation types
· Link-Link: when both entities are mentioned in the same context (i.e., co-coupled nodes).
· Owner-Link: when entity ej is linked in a context owned by entity ei (and vice versa, Link-Owner).
· Owner-Self: modelling loops of an entity with itself through the context.
Owner roles are not available for paragraph contexts, as these are derived from the paragraphCorpus of the CAR data set.
5 EVALUATION ON COMPLEX ANSWER RETRIEVAL
The TREC Complex Answer Retrieval track (CAR), hosted by NIST, aims to support users who seek a comprehensive answer in response to a topical keyword query. The track targets a scenario where a suitable answer needs to cover a range of backgrounds and context of the answer. In this light, a short "yes", a single sentence, or a single entity are not desired answers. While a short answer can often be effectively found through keyword matches, it is rather difficult to identify a large set of entities that are sufficiently relevant to be mentioned in the populated outline.
The CAR dataset [13] comes with a large collection of about 5.41 million (permitted) Wikipedia pages in easily accessible format, which we use as a collection of Wikipedia pages.3 Additionally, a large corpus of paragraphs with hyperlinks to Wikipedia pages is provided.
3Resource "unprocessedAllButBenchmark", available at http://trec-car.cs.unh.edu

220

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations

SIGIR '19, July 21­25, 2019, Paris, France

CAR queries are hierarchical page outlines that consist of a page title and headings. These outlines are to be populated with passages from a paragraph corpus and/or entities from a provided Wikipedia dump (pages of test queries are removed). We focus on the entity retrieval task. Two kinds of relevance data are provided, automatic and manual. Automatic relevance data is created synthetically from the Wikipedia page that corresponds to the query (those pages are not included in the dump). For the entity retrieval task, an entity is defined as relevant if the corresponding Wikipedia page contains a hyperlink to the entity. Synthetic relevance data is complemented by manual assessments conducted by NIST using pool-based evaluation. We make use of the following subsets provided in the TREC CAR v2.1 data release.4
· BenchmarkY1train-auto: 117 title queries, 1,816 title-heading queries, and 13,031 automatic entity assessments.
· BenchmarkY2test-manual: 271 title-heading queries, and 8,415 manual entity assessments.
· BenchmarkY2test-auto: 976 title-heading queries and 17,044 automatic entity assessments.
List of experiments. While the goal of this paper is to retrieve entities in response to short title queries, we evaluate our ENT Rank model both on title queries and title-heading queries for which official baselines are available. Following the track guidelines, we always train on the benchmarkY1train queries. In the page-level experiment we train/test on title queries from benchmarkY1trainauto using 5-fold cross validation. To compare to the state-of-theart in CAR, we conduct a section-level experiment trained titleheading queries and qrels from benchmarkY1train-auto and evaluated on benchmarkY2test-manual and benchmarkY2test-auto. The keyword query in the section-level experiment is formed by concatenating the title, the heading, and parent headings of the section. We complement the experiments with a study of the running example "Zika fever", before continuing with experiments on DBpedia-Entity in Section 6.
We evaluate resulting entity rankings by metrics R-Precision (Rprec), Mean-average Precision (MAP), Normalized Discounted Cumulative Gains (ndcg@10 and ndcg@100); conducting significance testing with paired-t-tests. More results available in the online appendix.
Experimental Setup. To carry out these experiments, full-text indexes and rankings were created with Lucene 7, using the English analyzer for tokenization of text and whitespace tokenization for entity ids. (Using the standard analyzer on text suffers from a 50% performance loss.)
We apply our mini-batched coordinate ascent learning-to-rankentities algorithm (Section 3.3) to optimize parameter vectors ì and ì across all queries to achieve the best mean-avg precision ranking performance. We use a mini-batch size of 150 queries. We use five random restarts of which we choose the model with the best evaluation score on the training folds. Rankings are predicted on the remaining data for each fold, then concatenated for evaluation.
4 http://trec- car.cs.unh.edu/datareleases/v2.1/

Table 1: Page-level results on benchmarkY1train title
queries measured in MAP. Comparison of feature subsets
and context types: paragraph, section, and page. Significantly higheror lowerthan AllExp () according to 1% (5%)
paired-t-test.

Run
AllExp JustAggr
No Entity No Neighbor No Context
Only Entity ExpEcm No Expansion

Paragraph
0.311 0.274
0.280 0.318 0.299
0.287 0.226 0.227

Section
0.291 0.158
0.156 0.278 0.280
0.287 0.220 0.148

Page
0.287 0.211
0.235 0.274 0.275
0.282 0.260 0.113

Next, feature vectors ìf for nodes and gì for binarized edges are constructed using retrieval models as detailed in Section 4. We apply Z-score normalization to all feature vectors during training, which is inverted to obtain graph visualizations.
5.1 Page-level Experiment on TREC CAR
We study the advantage of the ENT learning-to-rank-entities model with respect to the features described in Section 4. We analyze the retrieval performance achieved on the following subsets of features described:
(1) AllExp: Use all described features. (2) JustAggr: Combining multiple entity and context rankings
with unsupervised rank aggregation. The relative weight between different context rankings and entity features needs to be trained. (3) No Entity: All entity features were excluded. (4) No Neighbor: All neighbor features were excluded. (5) No Context: All context-relevance features were excluded (neighbor features are not affected). (6) Only Entity: Only entity features are included. (7) ExpEcmX: Like AllExp, but only rankings from EcmX are included. (8) No Expansion: Like AllExp, but only BM25 and QL rankings without expansion are included.
For comparison, the strongest input entity retrieval feature is QL EcmX on the Paragraph index, with MAP of 0.21. Inspecting the trained model parameters, we find that this feature also receives one of the highest weights among EcmX entity features. One of the strongest edge features is QL without expansion. This is interesting, because the EcmX entity feature is equivalent to using QL edge features in the ENT Rank framework with Owner-Self roles. This can be seen when Equation 6 is inserted into Equations 1 and 2. Of course, ENT Rank is a more flexible and powerful model as will be demontrated in the evaluation on the DBpedia v2 dataset in Section 6.
Table 1 displays the ranking performance in MAP across different feature subsets and context types. The best performance is achieved for paragraph contexts with all features included. Neither

221

Session 2C: Knowledge and Entities
SIGIR '19, July 21­25, 2019, Paris, France
Figure 3: The 2-hop neighbor relation graph for example query "Zika fever" and entity South America. Edge weights are predicted with the ENT Rank model using paragraph contexts. The graph was not manually cleaned.
section nor page contexts are not significantly improving over entity features alone. We conclude that large contexts, such as pages, are not effective to model neighbor relations. Generally the inclusion of more features, as in AllExp, does not hurt. For paragraph contexts, the lower values in No Entity, No Neighbor, and No Context demonstrate that all components of the ENT Rank model provide value. Lower score of JustAggr demonstrates the benefits of machine learning.
The best variant of ENT Rank, AllExp on paragraph contexts achieves Rprec of 0.356 and ndcg@100 of 0.674.
While excluded for brevity, similar results are also obtained for title-queries in benchmarkY1test and benchmarkY2test.
5.2 Case Study: Zika fever
We demonstrate the algorithm by analyzing the results for our motivating example query "Zika fever".
Figure 3 displays the 2-hop neighborhood relation graph for the example query entity "Zika virus" and entity South America. Edge weights are predicted with the ENT Rank model on paragraph contexts with the AllExp subset. The resulting graph includes many topical connections between South America and Infection, World Health Organization, and Mosquito.
We want to remark that the knowledge base provided with TREC CAR v2.1 does not include the page Zika fever (since test queries are held out). Furthermore the Wikipedia page of South America

SIGIR '19, July 21­25, 2019, Paris, France
Laura Dietz
Table 2: Rank at which the target entity South America is found for example query "Zika fever". As the query terms are not mentioned on the target's Wikipedia page, it is not in runs with RM, EcmPsg, or no expansion.

Input ranking

rank

Paragraph BM25 EcmX 55

Page BM25 EcmX

102

Section BM25 EcmX

119

Entity BM25 EcmX

146

ENT Rank rank

ExpEcmX

49

Only Entity 66

AllExp

86

JustAggr

109

does not mention the connection to Zika fever. Therefore, all connections in the ENT graph in Figure 3 are identified through contexts, neighbor relations, and EcmX features.
Table 2 displays the rank at which South America can be found in different input rankings (Table 2, left). The ENT Rank models (right) place South America at an even higher rank than any of the input rankings, demonstrating that the model can successfully incorporate different information sources even in challenging cases.
5.3 Section-level Experiment on TREC CAR
To compare ENT Rank to baseline systems from the TREC CAR challenge, we train ENT Rank models on title-heading queries from benchmarkY1train, then predict entities for benchmarkY2test outlines. Table 3 compares the two best ENT Rank variants with the two best entity retrieval systems from TREC CAR, "UNH-e-L2R" and "UNH-e-mixed".5 ENT Rank either outperforms or is equivalent to the CAR baseline systems. We want to remark, that ENT Rank runs did not contribute to the pool for manual assessments, giving baseline systems a slight advantage.
6 EVALUATION ON DBPEDIA-ENTITY V2
We further evaluate our approach on several established entity retrieval datasets, provided in the DBpedia-Entity v2 benchmark [20]. The benchmark includes the following categories of queries with updated relevance judgments using a pool of methods:
SemSearch ES are short and ambiguous named entity queries. 113 queries such as "brooklyn bridge".
INEX-LD are IR-style keyword queries for linked data. 99 queries such as "electronic music genres".
List Search contain list search queries. 115 queries such as "Professional sports teams in Philadelphia".
QALD-2 is comprised of questions for linked data. 140 queries such as "Who is the mayor of Berlin?". Question-specific stopwords were removed by Hasibi et al.
The benchmark is designed for the English part of DBpedia from October 2015. As our algorithm makes heavy use of the Wikipedia article structure (paragraphs, sections, entity links in addition to meta data), we project the DBpedia-Entity v2 benchmark onto the Wikipedia dataset provided with TREC CAR (English part from December 2016). Only 2% of assessed entities could not be aligned because of page re-organizations. Since our method did not contribute to the assessment pool, it retrieves many unjudged documents. To enable a fair comparison, unjudged entries are removed
5Available at http://trec-car.cs.unh.edu/results/trec-car-y2-appendix.html

222

Session 2C: Knowledge and Entities

SIGIR '19, July 21­25, 2019, Paris, France

ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations

SIGIR '19, July 21­25, 2019, Paris, France

Table 3: Comparison of section-level retrieval on TREC CAR benchmarkY2test between best performing ENT Rank variants in comparison to two best entity retrieval baseline systems. Significantly higher or lower according to 5% paired-t-test.

CAR Rank 1: UNH-e-L2R CAR Rank 2: UNH-e-mixed ENT Rank AllExp ENT Rank ExpEcm ENT Rank JustAggr ENT Rank No Expansion

MAP 0.146
0.142 0.136 0.156 0.161 0.152

Automatic
Rprec ndcg@10 0.181 0.258 0.175 0.275 0.161 0.239 0.180 0.254 0.186 0.270
0.179 0.255

ndcg@100 0.316 0.298 0.391 0.427 0.428 0.416

MAP 0.310 0.260 0.276
0.307
0.322 0.323

Manual
Rprec ndcg@10 0.315 0.453 0.278 0.386 0.275 0.395 0.304 0.443
0.312 0.443
0.317 0.448

ndcg@100 0.514 0.435 0.538 0.578 0.592 0.590

from the ranking. The benchmark also provides contributed baseline runs. These were also projected onto the 2016 Wikipedia dump, and likewise unjudged entities were removed to obtain a fair comparison (obtaining different evaluation results than described on the benchmark's web page).
Datasets are merged for training with 5-fold cross validation. Table 4 displays the result of our suggested ENT Rank model in comparison to the best-performing baselines BM25F-CA and FSDMELR. With the exception of the SemSearch ES subset, our ENT Rank method outperforms all twelve baseline systems. ENT Rank especially improves on recall-oriented measures MAP and ndcg@100.
Inspecting the feature weights reveal that--in comparison to complex answer retrieval­these datasets require that weight is placed on entity features. Restricting the features to the ExpEcmX subset does drastically hurt the performance. In contrast, limiting features to only access un-expanded BM25 and QL runs, obtains relatively good results. When entity features are removed, ENT rank increases the weight of neighbor features, thereby practically recovering a retrieval performance of 0.671 ndcg@100.
7 CONCLUSION
We propose ENT Rank, a framework for modeling entity-neighbortext relations for entity retrieval. While ENT Rank can incorporate a wide range of context, neighbor, and entity features, here we focus on features that are derived from traditional text retrieval methods, such as BM25, and neighbor relations that are based on co-occuring entity links. We explore different sizes of contexts and find that paragraph-sized contexts work best.
The approach is evaluated through several experiments on the TREC Complex Answer Retrieval and DBpedia-Entity v2 benchmarks which include title-heading queries, semantic search queries, and question answering queries. ENT Rank is consistently the best or second-best method among a set of eleven baseline systems that participated in TREC CAR and twelve systems from DBpediaEntity. A case study on the running example "Zika fever" demonstrates the ability to detect relevant entities even when their relevance cannot be concluded from their Wikipedia page alone.
In future, we would like to use ENT Rank to not only rank entities, but also to provide a useful order among entities and support them with text. Such a system could support both human article authors and automated conversational agents with background knowledge. One day, such systems might respond to web search requests with automatically written Wikipedia articles that do not exist yet.

Table 4: Results of ENT Rank on the DBpedia-Entity v2
dataset. Baselines BM25F-CA and FSDM+ELR [20]. Significantly higheror lowerthan BM25F-CA () baseline accord-
ing to 5% paired-t-test.

All
BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr

MAP
0.454 0.440 0.465 0.476 

Rprec
0.433 0.416 0.430 0.438

ndcg@100
0.680 0.663 0.702 0.711

ndcg@10
0.545 0.537 0.536 0.544

SemSearch_ES
BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr

0.606 0.620 0.601 0.590

0.549 0.550 0.532 0.506

0.782 0.791 0.783 0.779

0.671 0.694 0.666 0.658

ListSearch
BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr

0.441 0.422 0.478 0.493

0.427 0.404 0.450 0.471

0.689 0.665 0.733 0.744

0.550 0.533 0.542 0.549

INEX_LD
BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr ENT Rank Only Entity

0.420 0.399 0.437 0.439 0.443

0.414 0.395 0.412 0.422 0.425

0.666 0.645 0.693 0.696 0.702

0.525 0.511 0.520 0.519 0.532

QALD2
BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr

0.366 0.339 0.366 0.396

0.359 0.332 0.346 0.366

0.600 0.572
0.618 0.639

0.455 0.432 0.439 0.465

Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 1846017. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

223

Session 2C: Knowledge and Entities
SIGIR '19, July 21­25, 2019, Paris, France
REFERENCES
[1] James Allan, Bruce Croft, Alistair Moffat, and Mark Sanderson. 2012. Frontiers, challenges, and opportunities for information retrieval: Report from SWIRL 2012 the second strategic workshop on information retrieval in Lorne. In ACM SIGIR Forum, Vol. 46. ACM, 2­32.
[2] Krisztian Balog, Marc Bron, and Maarten De Rijke. 2011. Query modeling for entity search based on terms, categories, and examples. ACM Transactions on Information Systems (TOIS) 29, 4 (2011), 22.
[3] Krisztian Balog, Edgar Meij, and Maarten De Rijke. 2010. Entity search: building bridges between two worlds. In Proceedings of the 3rd International Semantic Search Workshop. 9.
[4] Krisztian Balog, Pavel Serdyuko, and Arjen de Vries. 2011. A neighborhood relevance model for entity linking. In TREC.
[5] Krisztian Balog, Pavel Serdyuko, Arjen de Vries, Paul Thomas, and Thijs Westerveld. 2009. Overview of the TREC 2009 Entity Track. In TREC.
[6] Holger Bast, Alexandru Chitea, Fabian Suchanek, and Ingmar Weber. 2007. Ester: efficient search on text, entities, and relations. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 671­678.
[7] Hannah Bast and Elmar Haussmann. 2015. More accurate question answering on freebase. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. 1431­1440.
[8] Michael Bendersky, Donald Metzler, and W Bruce Croft. 2010. Learning concept importance using a weighted dependence model. In Proceedings of the third ACM international conference on Web search and data mining. ACM, 31­40.
[9] Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An empirical study of learning to rank for entity search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 737­740.
[10] Jeffrey Dalton and Laura Dietz. 2013. A Neighborhood Relevance Model for Entity Linking. In Proceedings of the 10th Conference on Open Research Areas in Information Retrieval. 149­156.
[11] Jeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity Query Feature Expansion Using Knowledge Base Links. In SIGIR.
[12] Gianluca Demartini, Tereza Iofciu, and Arjen P De Vries. 2009. Overview of the INEX 2009 entity ranking track. In International Workshop of the Initiative for the Evaluation of XML Retrieval. Springer, 254­264.
[13] Laura Dietz and Ben Gamari. 2018. TREC CAR 2.0: A Data Set for Complex Answer Retrieval. http://trec-car.cs.unh.edu. Version 2.0.
[14] Laura Dietz, Ben Gamari, Jeff Dalton, and Nick Craswell. 2018. TREC Complex Answer Retrieval Overview. TREC.
[15] Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-fly annotation of short text fragments (by wikipedia entities). In Proc. of CIKM. 1625­1628.
[16] Darío Garigliotti and Krisztian Balog. 2017. On Type-Aware Entity Retrieval. In SIGIR. 27­34.
[17] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2015. Entity Linking in Queries: Tasks and Evaluation. In ICTIR. 171­180.
[18] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2016. Exploiting Entity Linking in Queries for Entity Retrieval. In ICTIR. 209­218.
[19] Faegheh Hasibi, Krisztian Balog, Darío Garigliotti, and Shuo Zhang. 2017. Nordlys: A toolkit for entity-oriented and semantic search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1289­1292.
[20] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. DBpedia-entity v2: a test

SIGIR '19, July 21­25, 2019, Paris, France
Laura Dietz
collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1265­ 1268. [21] Alexander Kotov and ChengXiang Zhai. 2012. Tapping into knowledge base for concept feedback: Leveraging ConceptNet to improve search results for difficult queries. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM 2012). ACM, 403­412. [22] Victor Lavrenko and W Bruce Croft. 2001. Relevance based language models. In Proc. of SIGIR-01. 120­127. [23] Xitong Liu and Hui Fang. 2015. Latent entity space: a novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18, 6 (2015), 473­503. [24] Edgar Meij, Marc Bron, Laura Hollink, Bouke Huurnink, and Maarten de Rijke. 2011. Mapping queries to the Linking Open Data cloud: A case study using DBpedia. Web Semantics: Science, Services and Agents on the World Wide Web 9, 4 (2011), 418­433. [25] Edgar Meij, Wouter Weerkamp, and Maarten De Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining. ACM, 563­572. [26] Donald Metzler and W Bruce Croft. 2005. A Markov random field model for term dependencies. In Proc. of SIGIR-05. 472­479. [27] Fedor Nikolaev, Alexander Kotov, and Nikita Zhiltsov. 2016. Parameterized Fielded Term Dependence Models for Ad-hoc Entity Retrieval from Knowledge Graph. In SIGIR. [28] Jeffrey Pound, Peter Mika, and Hugo Zaragoza. 2010. Ad-hoc object retrieval in the web of data. In Proceedings of the 19th international conference on World wide web (WWW2010). ACM, 771­780. [29] Hadas Raviv, David Carmel, and Oren Kurland. 2012. A ranking framework for entity oriented search using Markov random fields. In Proceedings of the 1st Joint International Workshop on Entity-Oriented and Semantic Search. ACM, 1. [30] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document Retrieval Using Entity-Based Language Models. In SIGIR. [31] Michael Schuhmacher, Laura Dietz, and Simone Paolo Ponzetto. 2015. Ranking Entities for Web Queries Through Text and Knowledge. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM 2015). ACM, 1461­1470. [32] Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk with restart and its applications. In Sixth International Conference on Data Mining (ICDM'06). IEEE, 613­622. [33] Alberto Tonon, Gianluca Demartini, and Philippe Cudré-Mauroux. 2012. Combining Inverted Indices and Structured Search for Ad-hoc Object Retrieval. In SIGIR. [34] Fei Wang and Changshui Zhang. 2008. Label Propagation through Linear Neighborhoods. IEEE Transactions on Knowledge and Data Engineering 1, 20 (2008), 55­67. [35] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-entity duet representations for document ranking. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 763­772. [36] Nikita Zhiltsov, Alexander Kotov, and Fedor Nikolaev. 2015. Fielded Sequential Dependence Model for Ad-Hoc Entity Retrieval in the Web of Data. In SIGIR. 253­262. [37] Denny Zhou, Jiayuan Huang, and Bernhard Schölkopf. 2007. Learning with hypergraphs: Clustering, classification, and embedding. In Advances in neural information processing systems. 1601­1608.

224

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

Statistical Significance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors

Julián Urbano
Delft University of Technology The Netherlands
urbano.julian@gmail.com

Harlley Lima
Delft University of Technology The Netherlands
h.a.delima@tudelft.nl

Alan Hanjalic
Delft University of Technology The Netherlands
a.hanjalic@tudelft.nl

ABSTRACT
Statistical signi cance testing is widely accepted as a means to assess how well a di erence in e ectiveness re ects an actual di erence between systems, as opposed to random noise because of the selection of topics. According to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is the most popular choice among IR researchers. However, previous work has suggested computer intensive tests like the bootstrap or the permutation test, based mainly on theoretical arguments. On empirical grounds, others have suggested non-parametric alternatives such as the Wilcoxon test. Indeed, the question of which tests we should use has accompanied IR and related elds for decades now. Previous theoretical studies on this matter were limited in that we know that test assumptions are not met in IR experiments, and empirical studies were limited in that we do not have the necessary control over the null hypotheses to compute actual Type I and Type II error rates under realistic conditions. Therefore, not only is it unclear which test to use, but also how much trust we should put in them. In contrast to past studies, in this paper we employ a recent simulation methodology from TREC data to go around these limitations. Our study comprises over 500 million p-values computed for a range of tests, systems, e ectiveness measures, topic set sizes and e ect sizes, and for both the 2-tail and 1-tail cases. Having such a large supply of IR evaluation data with full knowledge of the null hypotheses, we are nally in a position to evaluate how well statistical signi cance tests really behave with IR data, and make sound recommendations for practitioners.
KEYWORDS
Statistical signi cance, Student's t-test, Wilcoxon test, Sign test, Bootstrap, Permutation, Simulation, Type I and Type II errors
ACM Reference Format: Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical Signi cance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331259
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331259

1 INTRODUCTION
In the traditional test collection based evaluation of Information Retrieval (IR) systems, statistical signi cance tests are the most popular tool to assess how much noise there is in a set of evaluation results. Random noise in our experiments comes from sampling various sources like document sets [18, 24, 30] or assessors [1, 2, 41], but mainly because of topics [6, 28, 36, 38, 43]. Given two systems evaluated on the same collection, the question that naturally arises is "how well does the observed di erence re ect the real di erence between the systems and not just noise due to sampling of topics"? Our eld can only advance if the published retrieval methods truly outperform current technology on unseen topics, as opposed to just the few dozen topics in a test collection. Therefore, statistical signi cance testing plays an important role in our everyday research to help us achieve this goal.
1.1 Motivation
In a recent survey of 1,055 SIGIR and TOIS papers, Sakai [27] reported that signi cance testing is increasingly popular in our eld, with about 75% of papers currently following one way of testing or another. In a similar study of 5,792 SIGIR, CIKM and ECIR papers, Carterette [5] also observed an increasing trend with about 60% of full papers and 40% of short papers using signi cance testing. The most typical situation is that in which two IR systems are compared on the same collection, for which a paired test is in order to control for topic di culty. According to their results, the t-test is the most popular with about 65% of the papers, followed by the Wilcoxon test in about 25% of them, and to a lesser extent by the sign, bootstrap-based and permutation tests.
It appears as if our community has made a de facto choice for the t and Wilcoxon tests. Notwithstanding, the issue of statistical testing in IR has been extensively debated in the past, with roughly three main periods re ecting our understanding and practice at the time. In the 1990s and before, signi cance testing was not very popular in our eld, and the discussion largely involved theoretical considerations of classical parametric and non-parametric tests, such as the t-test, Wilcoxon test and sign test [16, 40]. During the late 1990s and the 2000s, empirical studies became to be published, and suggestions were made to move towards resampling tests based on the bootstrap or permutation tests, while at the same time advocating for the simplicity of the t-test [31­33, 46]. Lastly, the 2010s witnessed the wide adoption of statistical testing by our community, while at the same time it embarked in a long-pending discussion about statistical practice at large [3, 4, 26].
Even though it is clear that statistical testing is common nowadays, the literature is still rather inconclusive as to which tests are more appropriate. Previous work followed empirical and theoretical

505

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

arguments. Studies of the rst type usually employ a topic splitting methodology with past TREC data. The idea is to repeatedly split the topic set in two, analyze both separately, and then check whether the tests are concordant (i.e. they lead to the same conclusion). These studies are limited because of the amount of existing data and because typical collections only have 50 topics, so we can only do 25­25 splits, abusing the same 50 topics over and over again. But most importantly, without knowledge of the null hypothesis we can not simply count discordances as Type I errors; the tests may be consistently wrong or right between splits. On the other hand, studies of theoretical nature have argued that IR evaluation data does not meet test assumptions such as normality or symmetry of distributions, or measurements on an interval scale [13]. In this line, it has been argued that the permutation test may be used as a reference to evaluate other tests, but again, a test may di er from the permutation test on a case by case basis and still be valid in the long run with respect to Type I errors1.
1.2 Contributions and Recommendations
The question we ask in this paper is: which is the test that, maintaining Type I errors at the  level, achieves the highest statistical power with IR-like data? In contrast to previous work, we follow a simulation approach that allows us to evaluate tests with unlimited IR-like data and under full control of the null hypothesis. In particular, we use a recent method for stochastic simulation of evaluation data [36, 39]. In essence, the idea is to build a generative stochastic model of the joint distribution of e ectiveness scores for a pair of systems, so that we can simulate an arbitrary number of new random topics from it. The model contains, among other things, the true distributions of both systems, so we have full knowledge of the true mean scores needed to compute test error rates.
We initially devised this simulation method to study the problem of topic set size design [36], but after further developments we recently presented preliminary results about the t-test with IR data [39]. The present paper provides a thorough study in continuation. Our main contributions are as follows:
· A description of a methodology that allows us, for the rst time, to study the behavior of statistical tests with IR data.
· A large empirical study of 500 million p-values computed for simulated data resembling TREC Adhoc and Web runs.
· A comparison of the typical paired tests (t-test, Wilcoxon, sign, bootstrap-shift and permutation) in terms of actual Type I, Type II and Type III errors2.
· A comparison across several measures (AP, nDCG@20, ERR@20, P@10 and RR), topic set sizes (25, 50 and 100), signi cance levels (0.001­0.1), e ect sizes (0.01­0.1), and for both the 2-tailed and the 1-tailed cases.
Based on the results of this study, we make the following conclusions and recommendations:
· The t-test and the permutation test behave as expected and maintain the Type I error rate at the  level across measures, topic set sizes and signi cance levels. Therefore, our eld
1For instance, under H0 a test that always returns 1 minus the p-value of the permutation test, will always be discordant with it but have the same Type I error rate. 2Type III errors refer to incorrect directional conclusions due to correctly rejected non-directional hypotheses (ie. correctly rejecting H0 for the wrong reason) [19].

is not being conservative in terms of decisions made on the grounds of statistical testing. · Both the t-test and the permutation test behave almost identically. However, the t-test is simpler and remarkably robust to sample size, so it becomes our top recommendation. The permutation test is still useful though, because it can accommodate other test statistics besides the mean. · The bootstrap-shift test shows a systematic bias towards small p-values and is therefore more prone to Type I errors. Even though large sample sizes tend to correct this e ect, we propose its discontinuation as well. · We agree with previous work in that the Wilcoxon and sign tests should be discontinued for being unreliable. · The rate of Type III errors is not negligible, and for measures like P@10 and RR, or small topic sets, it can be as high as 2%. Testing in these cases should be carried with caution.
We provide online fast implementations of these tests, as well as all the code and data to replicate the results found in the paper3.
2 RELATED WORK
One of the rst accounts of statistical signi cance testing in IR was given by van Rijsbergen [40], with a detailed description of the t, Wilcoxon and sign tests. Because IR evaluation data violates the assumptions of the rst two, the sign test was suggested as the test of choice. Hull [16] later described a wider range of tests for IR, and argued that the t-test is robust to violations of the normality assumption, specially with large samples. However, they did not provide empirical results. Shortly after, Savoy [32] stressed van Rijsbergen's concerns and proposed bootstrapping as a general method for testing in IR because it is free of distributional assumptions.
An early empirical comparison of tests was carried out by Wilbur [44], who simulated system runs using a simple model of the rate at which relevant documents are retrieved [20]. Albeit unrealistic, this model allowed a preliminary study of statistical tests under knowledge of the null hypothesis. They showed that non-parametric tests, as well as those based on resampling, behaved quite well in terms of Type I error rate, also indicating preference for bootstrapping. Zobel [46] compared the t-test, Wilcoxon and ANOVA with a random 25­25 split of TREC Ad hoc topics, and found lower discordance rates with the t-test. However, they suggested the Wilcoxon test because it showed higher power and it has more relaxed assumptions. Although they did not study signi cance tests, Voorhees and Buckley [43] used TREC Ad hoc and Web data to analyze the rate of discordance given the observed di erence between two systems. Inspired by these two studies, Sanderson and Zobel [31] used several 25­25 splits of TREC Ad hoc topics and found that the t-test has lower discordance rates than the Wilcoxon test, followed by the sign test. Cormack and Lynam [11] later used 124­124 topic splits from the TREC Robust track to compare actual and expected discordance rates, and found the Wilcoxon test to be more powerful than the t and sign tests, though with higher discordance rates. Voorhees [42] similarly used 50­50 splits of the TREC Robust topics to study concordance rates of the t-test with score standardization. Finally, Sakai [25] also advocated for the bootstrap when evaluating e ectiveness measures, but did not compare to other tests.
3 https://github.com/julian-urbano/sigir2019-statistical

506

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

The most comprehensive comparison of statistical tests for IR is probably the one by Smucker et al. [33]. From both theoretical and empirical angles, they compared the t, Wilcoxon, sign, bootstrapshift and permutation tests. From a theoretical standpoint, they recommend the permutation test and compare the others to it using TREC Ad hoc data. They propose the discontinuation of the Wilcoxon and sign tests because they tend to disagree, while the bootstrap and t-test show very high agreement with the permutation test. In a later study, they showed that this agreement is reduced with smaller samples, and that the bootstrap test appears to have a bias towards small p-values [34]. Inspired by Voorhees [42] and Smucker et al. [33], Urbano et al. [37] performed a largescale study with 50­50 splits of the TREC Robust data to compare statistical tests under di erent optimality criteria. They found the bootstrap test to be the most powerful, the t-test the one with lowest discordance rates, and the Wilcoxon test the most stable.
In summary, we nd in the literature:
· Both theoretical and empirical arguments for and against speci c tests.
· Even though discordance rates can not be used as proxies to the Type I error rate because the null hypothesis is unknown [11], several studies made a direct or indirect comparison, suggesting that tests are too conservative because discordance rates are below the signi cance level  = 0.05.
· Most studies analyze statistical tests with AP scores, with few exceptions also using P@10. In general, they found higher discordance rates with P@10, arguing that AP is a more stable measure and conclusions based on it are more reliable.
· Studies of 2-tailed tests at  = 0.05 almost exclusively. · Except [29] and [44], no empirical study was carried out with
control of the null hypothesis. However, the rst one does not study the paired test case, and the second was based on unrealistic generative models for simulation.
Although previous work substantially contributed to our understanding of signi cance testing in IR, a comprehensive study of actual error rates is still missing in our eld. An attempt at lling this gap was very recently presented by Parapar et al. [21]. Their approach exploits score distribution models from which new relevance pro les may be simulated (ie. ranked lists of relevance scores) with indirect control over the AP score they are expected to produce. Therefore, their method simulates new runs for the same topic. However, the question of statistical testing in IR involves hypotheses about the performance of two systems on a population of topics from which the test collection is sampled. To study these hypotheses, one therefore needs to simulate new topics for the same systems. In contrast to [21], our method does simulate the e ectiveness scores of the given systems on random topics, directly and with full control over the distributions. Based on this simulation method, we aim at making informed recommendations about the practice of statistical testing in IR.
3 METHODS
This section provides a brief description of the ve statistical tests we consider, and then outlines the method we follow to simulate evaluation data with which to compute actual error rates.

3.1 Statistical Tests
Assuming some e ectiveness measure, let B1, . . . , Bn be the scores achieved by a baseline system on each of the n topics in a collection, and let E1, . . . , En be the scores achieved by an experimental system on the same topics. For simplicity, let Di = Ei - Bi be the di erence for a topic, and let B, E and D be the mean scores over topics. Under normal conditions, researchers compute the mean scores, and if
D > 0 they test for the statistical signi cance of the result using a paired test. At this point, a distinction is made between directional and non-directional null hypotheses.
A non-directional null hypothesis has the form H0 : µB = µE , meaning that the mean score of both systems is the same; the alternative is H1 : µB µE . This is called non-directional because it tests for equality of means, but if the hypothesis is rejected it does not say anything about the direction of the di erence. In this case, one uses a test to compute a 2-tailed or 2-sided p-value. A directional null hypothesis has the form H0 : µB  µE , meaning that the mean performance of the baseline system is larger than or equal to that of the experimental system4; the alternative is H1 : µB < µE . In this case, one computes a 1-tailed or 1-sided p-value.
For simplicity, let us assume D > 0 unless otherwise stated.

3.1.1 Student's t-test. The case of a paired two-sample test for µB = µE is equivalent to the one-sample test for µD = 0. In general, the distribution of a sample mean has variance  2/n. When this mean is normally distributed, and  is unknown but estimated with the sample standard deviation s, the standardized mean follows a t-distribution with n - 1 degrees of freedom [35]. The test statistic is therefore de ned as

t=

D .

(1)

sD/ n

Using the cdf of the t-distribution, the 1-tailed p-value is calculated

as the probability of observing an even larger t statistic:

p1 = 1 - Ft (t ; n - 1).

(2)

Because the t-distribution is symmetric, the 2-tailed p-value is simply twice as large: p2 = 2 · p1. When the data are normally distributed, one can safely use the t-test because the mean is then normally distributed too. If not, and by virtue of the Central Limit Theorem, it can also be used if the sample size is not too small.
In our experiments we use the standard R implementation in function t.test.

3.1.2 Wilcoxon Signed Rank test. This is a non-parametric test that
disregards the raw magnitudes of the di erences, using their ranks instead [45]. First, every zero-valued observation is discarded5, and
every other Di is converted to its rank Ri based on the absolute values, but maintaining the correct sign: sign(Ri ) = sign(Di ). The test statistic is calculated as the sum of positive ranks

W = Ri .

(3)

Ri >0

Under the null hypothesis, W follows a Wilcoxon Signed Rank distribution with sample size n0 (the number of non-zero observations).

4H0 : µB  µE is also valid, but irrelevant to an IR researcher. 5Other ways of dealing with zeros have been proposed, but they are well out of the scope of this paper. See for instance [23], and [7] for a comparison.

507

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

The 1-tailed p-value is computed as

p1 = 1 - FWSR(W ; n0),

(4)

and the 2-tailed p-value is simply twice as large: p2 = 2 · p1. In our experiments we use the standard R implementation in
function wilcox.test.

3.1.3 Sign test. In this case the data are looked at as if they were

coin tosses where the possible outcomes are Di > 0 or Di < 0, therefore having complete disregard for magnitudes [8, §3.4]. The

test statistic is the number of successes, that is, the number of topics

where Di > 0:

S = I[Di > 0],

(5)

where I[·] is 1 if · is true or 0 otherwise. Under the null hypothesis, S follows a Binomial distribution with 50% probability of success and n0 trials, where n0 is again the number of topics where Di 0. The 1-tailed p-value is the probability of obtaining at least S successes:

p1 = 1 - FBinom(S - 1; n0, 0.5).

(6)

The 2-tailed p-value is simply twice as large: p2 = 2 · p1. van Rijsbergen [40] proposed to use a small threshold h such that if
|Di |  h then we consider that systems are tied for topic i. Following Smucker et al. [34], we set h = 0.01.
In our experiments we use our own implementation to compute
S and n0, and then use the standard R implementation in function binom.test to compute the p-values.

3.1.4 Permutation test. This test is based on the exchangeability principle: under the null hypothesis both systems are the same, so the two scores observed for a topic actually come from the same system and we simply happened to label them di erently [14, 22, §II]. With n topics, there are 2n di erent permutations of the labels, all equally likely. The one we actually observed is just one of them,

so the goal is to calculate how extreme the observed D is.
In practice, the distribution under the null hypothesis is esti-
mated via Monte Carlo methods. In particular, the following may be repeated T times. A permutation replica Dj is created by randomly swapping the sign of each Di with probability 0.5 (i.e. permuting the labels), and the mean Dj is recorded. The 1-tailed p-value is computed as the fraction of replicas where the mean is as large as

the observed mean:

1 p1 = T

j

I Dj  D .

(7)

The 2-tailed p-value is similarly the fraction of replicas where the magnitude of the mean is at least as large as the one observed:

1 p2 = T

j

I

|D

 j

|

 |D|

.

(8)

The precision of the p-values depends on how many replicas we
create. As noted by Efron and Tibshirani [12, ch. 15], an answer to
this issue may be given by realizing that T · p1 has a Binomial distribution with T trials and probability of success p1. The coe cient of variation for the estimated p^1 is

c (p^1) =

p^1(1 - p^1) T.

(9)

If we do not want our estimate of p1 to vary more than % of its

value, then we need to set T

=

(1-p1 )  2p1

.

For

instance,

for

a

target

 = 1% error on a p1 = 0.05 (i.e., an error of 0.0005), we need

T = 190,000 replicas. Under symmetricity assumptions, for p2 we

may assume p2 = 2 · p1. Smucker et al. [34] and Parapar et al. [21]

used 100,000 replicas in their experiments, which yield an error of

0.00045 for a target p2 = 0.01 (4.5%), and an error of 0.001 for a

target p2 = 0.05 (2%). Because we want to study small signi cance

levels, in our experiments we use T = 1 million replicas. This yields

an error of 0.00014 for a target p2 = 0.01 (1.4%), and 0.0003 for

p2 = 0.05 (0.62%). For the 1-tailed case, errors are 0.0001 (1%) and

0.0002 (0.43%), respectively.

Because this test is computationally expensive, we used our

own implementation in C++, using the modern Mersenne Twister

pseudo-random number generator in the C++11 standard.

3.1.5 Bootstrap test ­ Shi method. This test follows the bootstrap

principle to estimate the sampling distribution of the mean under

the null hypothesis [12, §16.4]. The following is repeated T times.

A

bootstrap

sample

D

 j

is

created

by

resampling

n

observations

with replacement from D = 1/T j Dj be the

{Di }in=1, mean of

and its mean these means,

Dj is recorded. Let which will be used

to shift the bootstrap distribution to have zero mean. The 1-tailed

p-value is computed as the fraction of bootstrap samples where the

shifted bootstrap mean is at least as large as the observed mean:

1 p1 = T

j

I Dj - D  D .

(10)

The 2-tailed p-value is similarly the fraction of shifted bootstrap samples where the magnitude of the mean is at least as large as the one observed:

1 p2 = T

j

I |Dj - D|  |D| .

(11)

As with the permutation test, we compute T = 1 million bootstrap samples. Wilbur [44] and Sakai [25] used 1,000 samples, Cormack and Lynam [10] used 2,000, Savoy [32] used up to 5,000, and both Smucker et al. [33] and Parapar et al. [21] used 100,000. Again, we use our own implementation in C++11 for e ciency.

3.2 Stochastic Simulation
In order to simulate realistic IR evaluation that allows us to compare statistical tests, we use the method recently proposed by Urbano and Nagler [39], which extends an initial method by Urbano [36] to study topic set size design methods. In essence, they build a generative stochastic model M that captures the joint distribution of e ectiveness scores for a number of systems. An observation from this model would be the vector of scores achieved by the system on the same topic. This model contains two parts: the marginal distribution of each system, that is, its distribution of e ectiveness scores regardless of the other systems, and a copula [17] to model the dependence among systems, that is, how they tend to behave for the same topic. Because we will focus on paired statistical testing, in our case we will only contemplate bivariate models of only two systems B and E.

508

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

Direction fit
simulate

Ei

E = FE-1(V)

margin fE

Vi= FE(Ei)

V

U

B = F-B1(U)

Ui= FB(Bi)

Bi

margin fB

copula c

Figure 1: Stochastic simulation model. To t the model (red): 1) the per-topic scores Bi and Ei are used to t the margin distributions fB and fE , 2) the pseudo-observations Ui and Vi are computed with the cdf functions, and 3) they are used to t the copula c. To simulate from the model (blue): 1) new pseudo-observations U and V are generated from the copula, and 2) the nal scores B and E are computed with the inverse cdf functions.

Given a model M, we can inde nitely simulate evaluation scores for the same two systems but on new, random topics. The model de nes the two individual marginal distributions FB and FE , so it provides full knowledge about the null hypothesis because we know µB and µE beforehand. For the simulated data to be realistic, Urbano and Nagler [39] argue that the model rst needs to be exible enough to accommodate arbitrarily complex data; this is ensured by allowing a range of non-parametric models for both the margins and the copula. Finally, a realistic model can be instantiated by tting it to existing TREC data using a model selection criterion that rewards
t over simplicity (e.g. log-likelihood instead of BIC). As they note, the resulting model is not a model of the true performance of the existing systems whose data was used in the tting process, but rather of some hypothetical systems that behave similarly.
Figure 1 summarizes how these stochastic models are built and used. To t the model (red ow), we start from the existing effectiveness scores of systems B and E over n topics: the paired {(Bi , Ei )}in=1 observations. The individual B1, . . . , Bn scores from the rst system are used to t the marginal distribution FB , which determines the true mean score µB , and likewise with the second system. The cdf of the tted margins are used to transform the original scores to pseudo-observations {(Ui = FB (Bi ), Vi = FE (Ei ))}ni=1, such that Ui , Vi  Uniform. The copula c is then tted to these pseudo-observations. To simulate a new topic from the model (blue
ow), a new pseudo-observation (U , V ) is randomly generated from the copula, which is then transformed back with the inverse cdf
to obtain the nal observation B = FB-1(U ), E = FE-1(V ) . By construction, we have B  FB and E  FE , so that we simulate new scores under full knowledge of the true system distributions.
We use this simulation method in two di erent ways. First, in order to study Type I error rates we need to generate data under the

null hypothesis H0 : µB = µE . We achieve this by assigning FE  FB after tting the margins and the copula. This way, we simulate data from two systems that have a certain dependence structure but the same margin distribution, so that µB = µE by construction. Second, and in order to study Type II and Type III error rates, we need to generate data with di erent e ect sizes. In particular, for a xed di erence  , we will need to make sure that µE = µB +  . Given an already tted stochastic model, Urbano and Nagler [39, §3.4] show how this requirement can be met by performing a slight transformation of FE that still maintains underlying properties of the distribution such as its support. For instance, if FE corresponds to P@10 scores, the transformed distribution will also generate valid P@10 scores. This way, we simulate data from two systems that have a certain dependence structure and whose expected scores are at a xed distance  .
In this paper we use version 1.0 of the simIReff R package6 by Urbano and Nagler [39]. simIReff o ers a high degree of exibility to model IR data. In particular, it implements 6 distribution families for the margins (Truncated Normal, Beta, Truncated Normal Kernel Smoothing, Beta Kernel Smoothing, Beta-Binomial and Discrete Kernel Smoothing), and 12 copula families for the dependence structure (Gaussian, t, Clayton, Gumbel, Frank, Joe, BB1, BB6, BB7, BB8, Tawn 1 and Tawn 2) plus rotations. When transforming a distribution so that it has a certain expected value, we require a maximum absolute tolerance of 10-5.
4 EVALUATION
Because our goal is to evaluate the behavior of statistical tests on a variety of IR data, we chose systems and measures representative of ad hoc retrieval. In particular, we used the runs from the TREC 5­8 Ad hoc and TREC 2010­13 Web tracks. In terms of measures, we chose AP, P@10 and RR for the Ad hoc runs, and nDCG@20 and ERR@20 for the Web runs. We therefore evaluate tests on measures producing smooth but asymmetric distributions (AP, nDCG@20 and ERR@20), discrete (P@10), and with non-standard, non-uniform support (RR).
As is customary in this kind of experiments, we only use the top 90% of the runs to avoid errorful system implementations. This results in a total of 326 Ad hoc runs and 193 Web runs, from which we can use about 14,000 and 5,000 run pairs, respectively. In terms of topic set sizes, we will simulate results on n = 25, 50, 100 topics, representing small, typical and large collections.
4.1 Type I Errors
In order to evaluate the actual Type I error rate of the statistical tests, we proceed as follows. For a target measure and topic set size, we randomly select two systems from the same collection and
t the corresponding stochastic model as described in section 3.2 (same margins). From this model we simulate the scores on n new random topics, and compute the 2-tailed and 1-tailed p-values with the 5 tests we consider. Recall that under this stochastic model, both systems have the same distribution and therefore the null hypothesis is known to be true, so any statistically signi cant result would therefore count as a Type I error. This is repeated 1,667,000 times, leading to  8.3 million 2-tailed p-values and  8.3 million
6 https://cran.r-project.org/package=simIReff

509

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

AP - 25 topics

nDCG@20 - 25 topics

ERR@20 - 25 topics

P@10 - 25 topics

RR - 25 topics

.05 .1

.05 .1

.05 .1

.05 .1

.05 .1

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

t-test Wilcoxon Sign Permutation Bootstrap

.001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1

Significance level 

Significance level 

Significance level 

Significance level 

Significance level 

AP - 50 topics

nDCG@20 - 50 topics

ERR@20 - 50 topics

P@10 - 50 topics

RR - 50 topics

.05 .1

.05 .1

.05 .1

.05 .1

.05 .1

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1

Significance level 

Significance level 

Significance level 

Significance level 

Significance level 

AP - 100 topics

nDCG@20 - 100 topics

ERR@20 - 100 topics

P@10 - 100 topics

RR - 100 topics

.05 .1

.05 .1

.05 .1

.05 .1

.05 .1

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1

Significance level 

Significance level 

Significance level 

Significance level 

Significance level 

Figure 2: Type I error rates of 2-tailed tests. Plots on the same column correspond to the same e ectiveness measure, and plots on the same row correspond to the same topic set size. When indiscernible, the t, permutation and bootstrap tests overlap.

AP - 50 topics

nDCG@20 - 50 topics

ERR@20 - 50 topics

P@10 - 50 topics

RR - 50 topics

.05 .1

.05 .1

.05 .1

.05 .1

.05 .1

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001 .005 .01

Type I error rate

.001

t-test Wilcoxon Sign Permutation Bootstrap

.005 .01

.05 .1

Significance level 

.001

.005 .01

.05 .1 .001

Significance level 

.005 .01

.05 .1 .001

Significance level 

.005 .01

.05 .1 .001

Significance level 

.005 .01

.05 .1

Significance level 

Figure 3: Type I error rates of 1-tailed tests with 50 topics. When indiscernible, the t, permutation and bootstrap tests overlap.

1-tailed p-values for every measure and topic set size combination. The grand total is therefore just over 250 million p-values.
Figure 2 shows the actual error rates for the 2-tailed tests and for signi cance levels  in [0.001, 0.1]; each plot summarizes  8.3 million p-values. Ideally all lines should go through the diagonal, meaning that the actual error rate ( -axis) is the same as the nominal error rate  (x-axis). A test going above the diagonal is making more Type I errors than expected, and a test going below the diagonal is a conservative test making fewer errors than expected; both situations should be avoided to the best of our ability. Looking at AP scores, we can see that the Wilcoxon and sign tests are consistently making more errors than expected, specially at low  levels. As the sample size increases, they are overcon dent and show a clear bias towards small p-values. The bootstrap test is similarly overcon dent, but it approaches the expected behavior as the sample size increases, because the sampling distribution is better estimated

with large samples. The permutation test behaves quite better and also approaches the diagonal as sample size increases, and the t-test is remarkably close to ideal behavior even with small sample sizes. With nDCG@20 and ERR@20 we see very similar behavior, with the t-test tracking the ideal error rate nearly perfectly, perhaps with the exception of the t-test being conservative at low  levels with small samples in ERR@20.
When we look at a measure like P@10 that produces clearly discrete distributions, it is remarkable how well the t and permutation tests behave. This is probably because the distributions of di erences in P@10 are very symmetric in practice, which facilitates the e ect of the Central Limit Theorem. For the same reason, the Wilcoxon test behaves better than before, but again becomes overcon dent with large samples. The sign test also su ers from large sample sizes by increasing error rates, while the bootstrap test behaves better with more topics. For RR we see very similar

510

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

AP - 25 topics

nDCG@20 - 25 topics

ERR@20 - 25 topics

P@10 - 25 topics

RR - 25 topics

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

t-test Wilcoxon Sign Permutation Bootstrap
.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
AP - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
nDCG@20 - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
ERR@20 - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
P@10 - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
RR - 50 topics

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
AP - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
nDCG@20 - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
ERR@20 - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
P@10 - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
RR - 100 topics

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

Power 0.0 0.2 0.4 0.6 0.8 1.0

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

Figure 4: Power of 2-tailed tests as a function of  and at  = 0.05. Plots on the same column correspond to the same topic set size, and plots on the same row correspond to the same e ectiveness measure.

results, again with the exception of the t-test being conservative at low  levels with small samples. The permutation test shows nearly ideal behavior for RR too.
Figure 3 shows similar plots but for the 1-tailed case and only with n = 50 topics. The results are consistent with the 2-tailed case: the permutation and t-tests show nearly ideal behavior and the bootstrap test has a bias towards small p-values. When sample size increases (not shown7), this bias is reduced, but the Wilcoxon and sign tests become even more unreliable again.
For the typical IR case of n = 50 topics and  = 0.05, we see that the t-test and the permutation tests maintain a Type I error rate of just 0.05, while the bootstrap test is at 0.059 in the 2-tailed case and 0.054 in the 1-tailed case. At the  = 0.01 level, the t-test and the permutation test maintain a 0.01 error rate, with the bootstrap going up to 0.014.
4.2 Type II Errors
In order to evaluate the actual Type II error rate, we need to simulate from systems whose true mean scores are at a distance  from each other, and then observe how error rates are a ected by the magnitude of this distance. We rst randomly select a baseline system B from the bottom 75% of runs for a collection and measure, and then pre-select the 10 systems whose mean score is closest to the target µB +  . From these 10, we randomly choose the experimental system E and t the stochastic model to simulate, but we additionally transform FE so that its expected value is µE = µB +  ,
7Plots for other topic set sizes are provided as supplementary material.

as described in section 3.2. Under this model, the null hypothesis is known to be false, so any result that is not statistically signi cant would therefore count as a Type II error. This is repeated 167,000 times for each value of  in 0.01, 0.02, . . . , 0.1. In total, this leads to 8.35 million 2-tailed p-values and 8.35 million 1-tailed p-values for every measure and topic set size combination. The grand total is therefore just over 250 million p-values as well.
As is customary in this kind of analysis, we report power curves instead of Type II error rates, and do so only for the typical signi cance level  = 0.05.8 Figure 4 shows the power curves of the 2-tailed tests as a function of the e ect size  . The rst thing we notice is that there are clear di erences across measures; this is due to the variability of their distributions. For instance, di erences in P@10 or RR have between 2 and 4 times the standard deviation of the other measures. Across all measures we can also see clearly how power increases with the sample size and with the e ect size. These plots clearly visualize the e ect of the three main factors that a ect statistical power: variability, e ect size and sample size. The de nition of the t statistic in eq. (1) nicely formulates this.
We can see nearly the same pattern across measures. The sign test is consistently less powerful because it does not take magnitudes into account, and the bootstrap test is nearly consistently the most powerful, specially with small samples. The second place is disputed between the Wilcoxon, t and permutation tests. For instance, the Wilcoxon test is slightly more powerful for AP or ERR@20, but less so for nDCG@20 or P@10. For the most part though, these three
8Plots for other  levels are provided online as supplementary material.

511

Session 5B: Efficiency, Effectiveness and Performance

AP -  = 0.01
t-test Wilcoxon Sign Permutation Bootstrap

nDCG@20 -  = 0.01

ERR@20 -  = 0.01

SIGIR '19, July 21­25, 2019, Paris, France

P@10 -  = 0.01

RR -  = 0.01

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

.001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1

Significance level 

Significance level 

Significance level 

Significance level 

Significance level 

AP -  = 0.03

nDCG@20 -  = 0.03

ERR@20 -  = 0.03

P@10 -  = 0.03

RR -  = 0.03

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

Power 0.0 0.1 0.2 0.3 0.4 0.5

.001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1 .001

.005 .01

.05 .1

Significance level 

Significance level 

Significance level 

Significance level 

Significance level 

Figure 5: Power of 2-tailed tests as a function of  and at  = 0.01, 0.03, with 50 topics.

tests perform very similarly, specially the t and permutation tests. When the sample size is large, all tests are nearly indistinguishable in terms of power, with the clear exception of the sign test. Results in the 1-tailed case are virtually the same, except that power is of course higher. The reader is referred to the online supplementary material for the full set of results.
Figure 5 o ers a di erent perspective by plotting power as a function of the signi cance level, and for the selection  = 0.01, 0.03 and 50 topics. These plots con rm that the bootstrap and Wilcoxon tests have the highest power, specially for AP, nDCG@20 and ERR@20. That the bootstrap-shift test appears to be more powerful was also noted for instance by Smucker et al. [33], Urbano et al. [37], but the slightly higher power of the Wilcoxon test comes as a surprise to these authors. Conover [8, §5.7,§5.11] points out that, for certain heavy-tailed distributions, the Wilcoxon test may indeed have higher asymptotic relative e ciency compared to the t and permutation tests. For small samples, Conover et al. [9] report similar
ndings in the two-sample case. In summary, all tests except the sign test behave very similarly,
with very small di erences in practice. The bootstrap and Wilcoxon tests are consistently a little more powerful, but the results from last section indicate that they are also more likely to make Type I errors. Given that the t-test and the permutation test behave almost ideally in terms of Type I errors, and similar to the others in terms of power, it seems clear that they are the best choice, also consistently across measures and sample sizes.
4.3 Type III Errors
The majority of literature on statistical testing is devoted to analyzing the Type I error rate, while the study of Type II errors and power has received considerably less attention. There is however a third type of errors that is virtually neglected in the literature. Type III errors refer to cases in which a wrong directional decision is made after correctly rejecting a non-directional hypothesis9 [19]. Imagine
9The related concept of major con ict was studied for instance by Voorhees [42] and Urbano et al. [37], but without control over the null hypothesis.

we observe a positive improvement of our experimental system
E over the baseline B, that is, D > 0. If we follow the common procedure of running a 2-tailed test for the non-directional hypothesis H0 : µB = µE and reject it, we typically make the directional decision that µE > µB on the grounds of D > 0 and p2   . In such case, we make a directional decision based on a non-directional hypothesis, but that decision could very well be wrong. In our case, it could be that the baseline system really is better and the set of topics was just unfortunate. That is, H0 would still be correctly rejected, but for the wrong reason. In modern terminology, Type III errors are sometimes referred to as Type S errors (sign) [15].
Using the same data from section 4.2, we can study the rate of Type III errors, coined  by Kaiser [19]. Because our simulated data always had a positive e ect  , we actually count cases in which
D < 0 and the non-directional hypothesis was rejected by p2  . In these cases we would incorrectly conclude that the baseline is signi cantly better than the experimental system. Again, we only report here results at  = 0.05.
Figure 6 shows the rate of Type III errors as a function of the e ect size  . As anticipated by the Type I error rates, the t and permutation tests lead to much fewer errors than the other tests, with the Wilcoxon and sign tests being again more error-prone and the bootstrap test somewhere in between. The e ect of the topic set size is clearly visible again in that the Wilcoxon and sign test become even more unreliable while the others signi cantly reduce the error rate. In particular, the bootstrap test catches up with the t and permutation tests. Di erences across measures are once again caused by the variability of their distributions. RR is by far the most variable measure and hence more likely to observe outliers that lead to wrong conclusions. On the other hand, ERR@20 is the most stable measure and the tests show better behavior with it.
We note that the error rates reported in Figure 6 are over the total number of cases. An arguably more informative rate can be computed over the number of signi cant cases, that is, the fraction of statistically signi cant results that could actually lead to Type III errors. Even though space constraints do not permit to report these plots, Figures 4 and 6 together provide a rough idea. For

512

Session 5B: Efficiency, Effectiveness and Performance

AP - 25 topics
t-test Wilcoxon Sign Permutation Bootstrap

nDCG@20 - 25 topics

ERR@20 - 25 topics

SIGIR '19, July 21­25, 2019, Paris, France

P@10 - 25 topics

RR - 25 topics

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
AP - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
nDCG@20 - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
ERR@20 - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
P@10 - 50 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
RR - 50 topics

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
AP - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
nDCG@20 - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
ERR@20 - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
P@10 - 100 topics

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 
RR - 100 topics

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

Type III error rate 0.000 0.004 0.008 0.012

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size 

Figure 6: Type III error rates in directional 2-tailed tests at  = 0.05. Plots on the same column correspond to the same topic set size, and plots on the same row correspond to the same e ectiveness measure. When indiscernible, the t, permutation and bootstrap tests overlap.

instance, with AP and 50 topics, the Type III error rate at the typical  = 0.01 is 0.0069 with the t-test. With this e ect size, 9.47% of comparisons come up statistically signi cant, which means that 7.3% of signi cant results could lead to a Type III error. As another example, with P@10 and 50 topics the Type III error rate is 0.0064, with power at 8.9%. This means that 7.2% of signi cant results could lead to erroneous conclusions.
5 CONCLUSIONS
To the best of our knowledge, this paper presents the rst empirical study of actual Type I and Type II error rates of typical paired statistical tests with IR data. Using a method for stochastic simulation of evaluation scores, we compared the t-test, Wilcoxon, sign, bootstrap-shift and permutation tests on more than 500 million p-values, making it also a 10-fold increase over the largest study to date. Our analysis also comprises di erent e ectiveness measures (AP, nDCG@20, ERR@20, P@10 and RR), topic set sizes (25, 50 and 100), signi cance levels (0.001 to 0.1), and both 2-tailed and 1-tailed tests, making it the most comprehensive empirical study as well.
Our results con rm that the sign and Wilcoxon tests are unreliable for hypotheses about mean e ectiveness, specially with large sample sizes. One could argue that this is because they do not test hypotheses about means, but about medians. However, because of the symmetricity assumption, they are legitimately used as alternatives to the t-test with less strict assumptions. As suggested by previous research, the t-test and permutation test largely agree

with each other, but the t-test appears to be slightly more robust to variations of sample size. On the other hand, the bootstrap-shift test is shown to have a clear bias towards small p-values. While this leads to higher power, our results con rm that it also has higher Type I error rates, so there is virtually no gain over the t or permutation tests. This bias decreases with sample size, so the only situation favorable to the bootstrap appears to be that of (very) large test collections. The Wilcoxon test is found in a similar situation of high power but high Type I errors, specially as sample size increases.
An advantage of our study is that it allows us to move past the typical discordance ratios used so far in the literature to compare tests. As discussed here and in previous work, without knowledge of the null hypothesis these discordances do not imply Type I errors. Thanks to our methodology based on simulation, we computed actual Type I error rates and found that both the t-test and the permutation test maintain errors at the nominal  level remarkably well, and they do so across measures and sample sizes. The tests are not being too conservative as previously suggested, and even though some e ectiveness measures are indeed more unstable than others, that does not imply a higher error rate in the tests.
Based on our ndings, we strongly recommend the use of the ttest for typical hypotheses pertaining to mean system e ectiveness, and the permutation test for others. We provide further evidence that the bootstrap-shift test, although with nice theoretical properties, does not behave well unless the sample size is large, so we also recommend its discontinuation.

513

Session 5B: Efficiency, Effectiveness and Performance

SIGIR '19, July 21­25, 2019, Paris, France

ACKNOWLEDGMENTS
Work carried out on the Dutch national e-infrastructure (SURF
Cooperative) and funded by European Union's H2020 programme
(770376-2­TROMPA).
Whichever tree, plant, or bird you're now part of, thank you, Mom.
REFERENCES
[1] Omar Alonso and Stefano Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Information Processing & Management 48, 6 (2012), 1053­1066.
[2] Peter Bailey, Nick Craswell, Ian Soboro , Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. 2008. Relevance Assessment: Are Judges Exchangeable and Does it Matter?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 667­674.
[3] Ben Carterette. 2012. Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments. ACM Transactions on Information Systems 30, 1 (2012).
[4] Ben Carterette. 2015. Bayesian Inference for Information Retrieval Evaluation. In International Conference on the Theory of Information Retrieval. 31­40.
[5] Ben Carterette. 2017. But Is It Statistically Signi cant?: Statistical Signi cance in IR Research, 1995-2014. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 1125­1128.
[6] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas, Javed A. Aslam, and James Allan. 2008. Evaluation Over Thousands of Queries. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 651­658.
[7] William J. Conover. 1973. On Methods of Handling Ties in the Wilcoxon SignedRank Test. J. Amer. Statist. Assoc. 68, 344 (1973), 985­988.
[8] William J. Conover. 1999. Practical Nonparametric Statistics. Wiley. [9] William J. Conover, Oscar Wehmanen, and Fred L. Ramsey. 1978. A Note on
the Small-Sample Power Functions for Nonparametric Tests of Location in the Double Exponential Family. J. Amer. Statist. Assoc. 73, 361 (1978), 188­190. [10] Gordon V. Cormack and Thomas R. Lynam. 2006. Statistical Precision of Information Retrieval Evaluation. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 533­540. [11] Gordon V. Cormack and Thomas R. Lynam. 2007. Validity and Power of t-test for Comparing MAP and GMAP. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 753­754. [12] Bradley Efron and Robert J. Tibshirani. 1998. An Introduction to the Bootstrap. Chapman & Hall/CRC. [13] Marco Ferrante, Nicola Ferro, and Silvia Pontarollo. 2017. Are IR Evaluation Measures on an Interval Scale?. In Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval. 67­74. [14] Ronald A. Fisher. 1935. The Design of Experiments. Oliver and Boyd. [15] Andrew Gelman and Francis Tuerlinckx. 2000. Type S Error Rates for Classical and Bayesian Single and Multiple Comparisons Procedures. Computational Statistics 15, 3 (2000), 373­390. [16] David Hull. 1993. Using Statistical Testing in the Evaluation of Retrieval Experiments. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 329­338. [17] Harry Joe. 2014. Dependence Modeling with Copulas. CRC Press. [18] Timothy Jones, Andrew Turpin, Stefano Mizzaro, Falk Scholer, and Mark Sanderson. 2014. Size and Source Matter: Understanding Inconsistencies in Test Collection-Based Evaluation. In ACM International Conference on Information and Knowledge Management. 1843­1846. [19] Henry F. Kaiser. 1960. Directional Statistical Decisions. Psychological Review 67, 3 (1960), 160­167. [20] Davis B. McCarn and Craig M. Lewis. 1990. A mathematical Model of Retrieval System Performance. Journal of the American Society for Information Science 41, 7 (1990), 495­500. [21] Javier Parapar, David E. Losada, Manuel A. Presedo Quindimil, and Alvaro Barreiro. 2019. Using score distributions to compare statistical signi cance tests for information retrieval evaluation. Journal of the Association for Information Science and Technology (2019).

[22] Edwin J.G. Pitman. 1937. Signi cance Tests Which May be Applied to Samples From any Populations. Journal of the Royal Statistical Society 4, 1 (1937), 119­130.
[23] John W. Pratt. 1959. Remarks on Zeros and Ties in the Wilcoxon Signed Rank Procedures. J. Amer. Statist. Assoc. 54, 287 (1959), 655­667.
[24] Stephen Robertson and Evangelos Kanoulas. 2012. On Per-Topic Variance in IR Evaluation. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 891­900.
[25] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics Based on the Bootstrap. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 525­532.
[26] Tetsuya Sakai. 2014. Statistical Reform in Information Retrieval? ACM SIGIR Forum 48, 1 (2014), 3­12.
[27] Tetsuya Sakai. 2016. Statistical Signi cance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 5­14.
[28] Tetsuya Sakai. 2016. Topic Set Size Design. Information Retrieval 19, 3 (2016), 256­283.
[29] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 1045­1048.
[30] Mark Sanderson, Andrew Turpin, Ying Zhang, and Falk Scholer. 2012. Di erences in E ectiveness Across Sub-collections. In ACM International Conference on Information and Knowledge Management. 1965­1969.
[31] Mark Sanderson and Justin Zobel. 2005. Information Retrieval System Evaluation: E ort, Sensitivity, and Reliability. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 162­169.
[32] Jacques Savoy. 1997. Statistical Inference in Retrieval E ectiveness Evaluation. Information Processing and Management 33, 4 (1997), 495­512.
[33] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Statistical Signi cance Tests for Information Retrieval Evaluation. In ACM International Conference on Information and Knowledge Management. 623­632.
[34] Mark D. Smucker, James Allan, and Ben Carterette. 2009. Agreement Among Statistical Signi cance Tests for Information Retrieval Evaluation at Varying Sample Sizes. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 630­631.
[35] Student. 1908. The Probable Error of a Mean. Biometrika 6, 1 (1908), 1­25. [36] Julián Urbano. 2016. Test Collection Reliability: A Study of Bias and Robustness to
Statistical Assumptions via Stochastic Simulation. Information Retrieval Journal 19, 3 (2016), 313­350. [37] Julián Urbano, Mónica Marrero, and Diego Martín. 2013. A Comparison of the Optimality of Statistical Signi cance Tests for Information Retrieval Evaluation. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 925­928. [38] Julián Urbano, Mónica Marrero, and Diego Martín. 2013. On the Measurement of Test Collection Reliability. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 393­402. [39] Julián Urbano and Thomas Nagler. 2018. Stochastic Simulation of Test Collections: Evaluation Scores. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 695­704. [40] Cornelis J. van Rijsbergen. 1979. Information Retrieval. Butterworths. [41] Ellen M. Voorhees. 2000. Variations in Relevance Judgments and the Measurement of Retrieval E ectiveness. Information Processing and Management 36, 5 (2000), 697­716. [42] Ellen M. Voorhees. 2009. Topic Set Size Redux. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 806­807. [43] Ellen M. Voorhees and Chris Buckley. 2002. The E ect of Topic Set Size on Retrieval Experiment Error. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 316­323. [44] W. John Wilbur. 1994. Non-parametric Signi cance Tests of Retrieval Performance Comparisons. Journal of Information Science 20, 4 (1994), 270­284. [45] Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics Bulletin 1, 6 (1945), 80­83. [46] Justin Zobel. 1998. How Reliable are the Results of Large-Scale Information Retrieval Experiments?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 307­314.

514

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

Variance Reduction in Gradient Exploration for Online Learning to Rank

Huazheng Wang, Sonwoo Kim, Eric McCord-Snook, Qingyun Wu, Hongning Wang
Department of Computer Science, University of Virginia Charlottesville, VA 22904, USA
{hw7ww,sak2m,esm7ky,qw2ky,hw5x}@virginia.edu

ABSTRACT
Online Learning to Rank (OL2R) algorithms learn from implicit user feedback on the fly. The key to such algorithms is an unbiased estimate of gradients, which is often (trivially) achieved by uniformly sampling from the entire parameter space. Unfortunately, this leads to high-variance in gradient estimation, resulting in high regret during model updates, especially when the dimension of the parameter space is large.
In this work, we aim at reducing the variance of gradient estimation in OL2R algorithms. We project the selected updating direction (i.e., the winning direction) into a space spanned by the feature vectors from examined documents under the current query (termed the "document space" for short), after an interleaved test. Our key insight is that the result of an interleaved test is solely governed by a user's relevance evaluation over the examined documents. Hence, the true gradient introduced by this test is only reflected in the constructed document space, and components of the proposed gradient which are orthogonal to the document space can be safely removed, for variance reduction purpose. We prove that this projected gradient is still an unbiased estimation of the true gradient, and show that this lower-variance gradient estimation results in significant regret reduction. Our proposed method is compatible with all existing OL2R algorithms which rank documents using a linear model. Extensive experimental comparisons with several state-of-the-art OL2R algorithms have confirmed the effectiveness of our proposed method in reducing the variance of gradient estimation and improving overall ranking performance.
CCS CONCEPTS
· Information systems  Learning to rank; · Theory of computation  Online learning algorithms;
KEYWORDS
Online learning to rank; Dueling bandit; Variance Reduction
ACM Reference Format: Huazheng Wang, Sonwoo Kim, Eric McCord-Snook, Qingyun Wu, Hongning Wang. 2019. Variance Reduction in Gradient Exploration for Online Learning to Rank . In Proceedings of the 42nd International ACM SIGIR
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331264

Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331264
1 INTRODUCTION
Online Learning to Rank (OL2R) [6] is a family of online learning solutions, which exploit implicit feedback from users to directly optimize parameterized rankers on the fly. It has drawn increasing attention in research community in recent years due to its advantages over classical offline learning to rank algorithms [10]. First, it avoids the expensive and time consuming process of offline result relevance annotation. Second, as it directly learns from user feedback, it optimizes the ranking results to best reflect current user preferences [15]. Third, because the model is updated on the fly, there is no need to store user click history offline, which alleviates many privacy concerns [21].
One strain of OL2R algorithms, represented by Dueling Bandit Gradient Descent (DBGD) [24], optimize a linear scoring function by exploring the parameter space via interleaved test. Algorithms of this type first propose an exploratory direction as a tentative model update direction, and then update the current ranker if the proposed direction provides better ranking utility. In practice, result utility is usually inferred from user clicks on an interleaved list of ranking results from each ranker [23]. The key technical insight of DBGD-type algorithms is that the expectation of selected directions is an unbiased estimate of true gradient of the unknown loss function for ranking [5]. As a result, DBGD is essentially a stochastic online gradient descent algorithm. However, because the exploration directions are uniformly sampled from the entire parameter space, when the dimensionality of the space is high (which is usually the case in practice), the variance in gradient estimation becomes large. This directly slows down the learning convergence of the algorithm and inevitably increases sample complexity.
Recently, several works in OL2R have realized this deficiency of gradient exploration in DBGD, and propose various types of solutions to improve its learning efficiency. One type of studies explore multiple random directions in each iteration of model update. Unbiased estimate of gradient is maintained in this type of revisions of DBGD, as the directions are still uniformly sampled. Model estimation variance is expected to be reduced by testing more exploratory directions; but, in practice, as the users would only examine a finite number of documents under each query (e.g., due to position bias [9]), the sensitivity of interleaved test drops as a result of more exploratory rankers having to be tested at once. This unfortunately introduces additional variance in model estimation. Another type of research constrains the sampling space for gradient exploration. However, this line of solutions cannot guarantee the

835

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

estimated gradient remains unbiased, and thus face high risk of converging towards a sub-optimal solution.
Although empirically effective, previous OL2R solutions neglect an important property of click-based result utility evaluation: users only perceive utility from the documents that they actually examine. As a result, the true gradient is only revealed by features playing an essential role in ranking those examined documents under this query. Here we define essential features in ranking a particular set of documents as those features with non-zero variance among the documents. Assume in an interleaved test, one ranking feature takes a constant value in all examined documents under this query, such that it has no effect in differentiating the quality of those documents. Then, the proposed exploratory direction's contribution to the ranker update on this particular dimension cannot be justified by this test result. Random gradient exploration hence introduces an arbitrary update on this dimension, which inevitably leads to high estimation variance over time. This example can be generalized to situations where multiple (even correlated) features have no effect in differentiating the utility of examined documents in the result of an interleaved test. Because in practice users usually only examine a handful of documents under each query [4, 9], but each document consists of hundreds or even thousands of ranking features, the variance introduced by random exploration on those non-essential features could be considerably large.
The above analysis suggests that an interleaved test only reveals the projection of true gradient in the spanned space of examined documents under a test query (termed the "document space" in this paper). With this as our motivation, we decide to project the winning direction back into the document space so as to reduce the variance introduced by random gradient exploration. We construct the document space from inferred users' result examinations [4], which are not observable in the user response but can be statistically modeled. Because this projection is independent from how the proposal directions are created, this solution can be directly applied to any DBGD-type OL2R algorithm. We theoretically prove that the projected direction is still an unbiased estimate of the true gradient, i.e., model convergence is guaranteed, and also prove the reduced variance directly leads to considerable regret reduction in online model update. We compare the proposed method with several stateof-the-art OL2R algorithms on a collection of large-scale learning to rank datasets and confirmed the effectiveness of our method.
2 RELATED WORK
One key family of OL2R methods root in Dueling Bandit Gradient Descent (DBGD) [24], which uses online gradient descent to solve a bandit convex optimization problem [5]. In each iteration, DBGD uniformly samples a random direction from the entire parameter space to create an exploratory ranker, and uses an interleaved test [15] to compare the current ranker with the exploratory one. If the exploratory ranker is preferred, the proposed direction is used as the gradient to update the model. This procedure yields an unbiased estimate of the true gradient [22]. However, the variance of DBGD's gradient estimation is high due to the nature of uniform exploration of the entire parameter space, which limits its learning efficiency.
Recently, attempts have been made to improve the learning efficiency of DBGD-type algorithms. Schuth et al. [17] proposed a

Multileave Gradient Descent (MGD) algorithm to explore multiple stochastic directions in each iteration with multi-interleaving comparison [18]. Zhao and King [25] developed a Dual-Point Dueling Bandit Gradient Descent algorithm to sample two stochastic vectors with opposite directions as the candidate gradients. The basic idea of this line of solutions is to test more exploratory directions at once so as to obtain the true gradient estimate sooner. However, their gradient exploration is still within the entire feature space. As users often only examine a small number of documents under each query, the sensitivity of interleaved test drops due to more exploratory rankers needing to be tested. In a different direction of solutions, researchers proposed to constrain the sampling space for gradient exploration. Hofmann et al. chose to filter the stochastic directions by historical comparisons before an interleaved test [7]. Oosterhuis et. al [12] proposed exploring gradients in a subspace constructed by a set of preselected reference documents from an offline training corpus. Wang et al. [20] proposed using historical interactions to avoid repeatedly exploring less promising directions, which also reduces gradient exploration to a subspace. However, the variance of gradient exploration is reduced at a cost of introducing bias into gradient approximation, so that such algorithms have a risk of converging to sub-optimal results.
Our solution falls into this second category of variance reduction for DBGD-type algorithms. Distinct from previous attempts to restrict gradient exploration before an interleaved test, we instead modify the selected direction after the test. As users' result examination is affected by the ranked results, which are in turn determined by the proposed exploratory directions, restricting the exploration space before the interleaved test potentially introduces bias in the subsequent interleaved test and model update. Our solution is based on the insight that only the projected true gradient in the document space can be revealed by an interleaved test. Hence, we decide to project the selected direction after each interleaved test, and thus guarantee an unbiased estimate of true gradient. Since the document space is expected to be smaller than the entire parameter space (as it is constructed only by the examined documents), the projected gradient enjoys low variance and leads to faster model convergence in online update.
3 METHOD
In this section we describe our proposed document space gradient projection method for online learning to rank. We first describe the problem setup in Section 3.1. And then we describe Document Space Projected Dueling Bandit Gradient Descent (DBGD-DSP) algorithm as an example of our proposed general solution in Section 3.2. Our gradient projection method is independent from how the exploratory gradient is proposed, and thus can be directly applied to any existing DBGD-type algorithm to reduce its variance of gradient estimation. We rigorously prove the unbiasedness of our gradient estimation in Section 3.3 and analyze the regret of DBGDDSP in Section 3.4. The same procedure and conclusions can be applied to any DBGD-type algorithm of interest.
3.1 Problem Setup
The estimation of OL2R models can be formalized as a dueling bandit problem [24]. In iteration t, an OL2R algorithm receives a

836

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

query and associated candidate documents, which are represented as a set of d-dimensional query-document pair feature vectors Xt = {x1, x2, ..., xs }. The algorithm takes two actions: first, it proposes two rankers, whose parameters are denoted as w, w ;
second, it ranks the given documents with these two rankers ac-

cordingly. An oracle (i.e., user) compares (duels) the two rankers'

results and provides feedback. In practice, an interleaving method

[15] is applied to merge the ranking lists of the two rankers and

display the resulting ranked list to the user. User preference is in-

ferred from the click feedback. Thus, the ranker that contributes more clicked documents is preferred. We denote w  w  for the event that w is preferred over w . The comparison between two
individual rankers is determined independently of other comparisons performed before with a probability P (w  w |Xt ), such that P (w  w |Xt ) = Pt (w  w ) = ft (w, w ). ft (w, w ) can be viewed as the distinguishability of the two rankers w and w  by an interleave comparison under query Xt .
We quantify the performance of an online learning algorithm

using cumulative regret defined as follows:

T

R(T ) = ft (w, wt ) + ft (w, wt),

(1)

t =1

where wt and wt are rankers compared at time t, and w is the best

ranker in ground-truth. As a result, the distinguishability measure
ft (w, w) indicates the loss of proposing a sub-optimal ranker w. We denote ft (wt , w) as ft (w) for simplicity. The goal of an OL2R algorithm is to optimize its parameter towards w according to loss

ft (w). A desired OL2R algorithm should have a sublinear regret in a finitie time horizon T , so that the one-step regret is quickly

decreasing to zero over time.

In this work, we make the following assumptions similar to [24].
We assume an unknown utility function vt (w) that quantifies the quality of a ranker w over query Xt . The utility function vt is assumed to be differentiable, strongly concave and Lv -Lipschitz, which means |vt (x) - vt (y)|  Lv |x - y|.
A link function  describes the probabilistic comparison of utili-

ties of two rankers as,

Pt w  w  = ft (w, w ) =  vt (w) - vt (w ) .

The link function should be rotation-symmetric, which means
 (x) = 1 -  (-x). We assume the link function is L -Lipschitz and second order L2-Lipschitz. The link function behaves like a
cumulative probability distribution function. For example, a com-
mon choice of link function is the standard logistic function  (x) =
1
1+exp(-x) , which satisfies all the assumptions.

3.2 Document Space Projected Dueling Bandit Gradient Descent
We describe our proposed Document Space Projected Dueling Ban-
dit Gradient Descent (DBGD-DSP) in Algorithm 1. We should note it fits all OL2R algorithm settings. At the beginning of iteration t, user initiates a query Xt . We denote wt as the parameter of the current ranker. DBGD-DSP first uniformly samples a vector ut from d dimensional unit sphere Sd-1 (i.e., |ut |2 = 1) as an exploratory direction, and proposes a candidate ranker wt = wt + ut , where  is the step size of exploration. The algorithm then uses the two

Algorithm 1 Document Space Projected Dueling Bandit Gradient

Descent (DBGD-DSP)

1: Inputs:  , 

2: Initiate w1 = sample_unit_vector() 3: for t = 1 to T do

4: Receive query Xt = {x1, x2, ..., xs } 5: ut = sample_unit_vector() 6: wt = wt + ut 7: Generate ranked lists l(Xt , wt ), l(Xt , wt) 8: Set Lt = Interleave {l(Xt , wt ), l(Xt , wt)} , and present Lt

to user

9: Receive click positions Ct on Lt , and infer click credits

10:

{ct , ct } if ct

 ct

then

11:

wt +1 = wt

12: else

13:

Based on Ct , infer user examined top mt documents in

Lt .

14:

Solve the orthogonal projection matrix At for document

space St = span({xLt,1, xLt,2, ..., xLt,mt }).

15:

Project ut onto St by t = Atut

16:

wt +1 = wt + t

17: end if

18: end for

rankers (wt and wt) to generate ranking lists l(Xt , wt ) and l(Xt , wt) accordingly, and combines them with an interleaving method, such
as Team Draft Interleaving [15] or Probabilistic Interleaving [8]. The
user examines the result list and provides implicit click feedback
to indicate their relevance evaluation of the results. The interleav-
ing method uses this implicit feedback to infer which ranker is
preferred by the user. If the exploratory ranker is preferred (i.e.,
wins the duel), previous DBGD-style algorithms update the current ranker by wt+1 = wt + ut , where  is the learning rate; otherwise the current ranker stays intact. This gradient exploration strategy
yields an unbiased estimate of the true gradient [5], in terms of
expectation. However, since the exploratory gradient ut is required to be uni-
formly sampled from the entire d dimensional unit sphere Sd-1, the model update suffers from high variance in its gradient estimation, especially when d is large, as in practice. Various improvements to this issue have been proposed in the past, but they still introduce
other difficulties, such as variance and bias trade-off [7, 12, 20], and
test sensitivity and efficiency [18, 25].
Unlike previous works that reduce the sampling space of gradi-
ent exploration before the interleaved test [7, 12, 20], we change
the winning direction after the test. The key insight is that only the projected true gradient in the spanned space of examined documents under query Xt (denoted as document space St ) can be revealed by an interleaved test. For example, as shown in Figure 1, a DBGD-style algorithm is comparing the current ranker wt and wt = wt + ut with a uniformly sampled exploration direction ut . The user examines top m documents, e.g., {x1, ..xm }, of the interleaved ranking list (of course m is unknown to the algorithm) and wt wins the duel. The estimated gradient ut can therefore be separated into two components, one component t that belongs to

837

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

ut gt

S t'

gt'

w tS
t

w t'

w*

u t'

w

w

Figure 1: Illustration of model update for DBGD-DSP in a
three dimensional space. Dashed lines represent the trajectory of DBGD following different update directions. ut is the selected direction by DBGD, which is in the 3-d space. Red bases present the document space St on a 2-d plane. ut is projected onto St to become t for model update.

the document space St = span{x1, ..xm } and the other component ut - t that is orthogonal to document space St . The orthogonal component ut -t does not affect the ranking among the examined documents, i.e. (wt + ut )T xi = (wt + t )T xi , and thus does not contribute to the loss function and true gradient estimation. Intuitively, ut - t is not supported by the observed interleaved test, as anything sampled from the complement of St cannot be verified by the examined documents. As a result, it is safe to exclude the direction ut -t from model update, which we later prove maintains the unbiasedness of the original DBGD-type gradient estimation, and reduces the variance. As illustrated in Figure 1, although ut will eventually lead to the same model estimation, as it is unbiased, this
guarantee is only obtained in expectation. The variance could po-
tentially be large: for example, the blue and purple updating traces
slow down model convergence, when the number of observations
is finite.
As shown in line 14 to 16 of Algorithm 1, we solve for the orthogonal projection matrix At of document space St , and project the selected direction ut onto the document space St after each interleaved test. We leave the detailed design of constructing document space and solving projection matrix At in Section 3.5. Before that, we first rigorously prove the projection maintains an unbiased
estimate of true gradient in Section 3.3. Since the document space is
constructed only by the examined documents, the rank of document
space is expected to be smaller than the entire parameter space.
This directly leads to lower variance and faster model convergence.
We show that our document space projection reduces the variance of gradient estimation from d to Rank(At ) in Section 3.4, and then analyze its benefit for regret reduction from low-variance gradient
estimation.

3.3 Unbiasedness of Gradient Estimation

We now prove that our document space projected gradient is an

unbiased estimate of true gradient in the sense of expectation [24]. We define Zt (w) as the event of w winning the duel with wt ,

Zt (w) =

1 w.p. 1 - Pt (wt  w) 0 w.p. Pt (wt  w)

Then the gradient used for model update in DBGD-DSP (as described in Algorithm 1) can be described as,

ht = -Zt (wt + ut )t .

(2)

Note that by adding a negative sign we view our model update as online gradient descent wt +1 = wt - t .
We now show in the following theorem that this is an unbiased
gradient estimation of true gradient. By defining a smoothed version of ft as f^t (w) = Eu B[ft (w + u)], we have:
Theorem 3.1. The projected gradient t in DBGD-DSP is an unbiased estimate of true gradient, i.e.,

E[ht ]

=

 d



f^t

(w

)

(3)

over random unit vector ut .

Proof. Based on the Lemma 1 of [24], we have

E [ht ] = E [-Zt (wt + ut )At ut ] = Eut Sd-1 [ft (w +  At ut )ut ] Define Ft (w) = ft (At w), we have

E[ht ] = Eut Sd-1 [ft (wt +  At ut )ut ]

= Eut Sd-1 [Ft (At-1wt + ut )ut ]

=

 d

Eut Bd [Ft (At-1wt

+ ut )ut ]

=

 d

F^t (At-1wt )

=

 d

At f^t (wt )

=

 d

f^t (wt )

where the fourth equality is based on Stokes' Theorem. The last

equality holds because gradient f^t (wt ) belongs to document space

St , and thus projecting it by At maps back to itself.



The guarantee of unbiased gradient estimation is a major advantage of our proposed document space gradient projection method, compared with previous attempts to reduce the gradient exploration space, such as Oosterhuis et. al [12] and Wang et al. [20]. Our method enjoys reduced variance of gradient estimate (which will be proved next), without the risk of converging towards a suboptimal solution. We should note that the above is independent from the mechanism of how the proposal directions are generated, as shown in the first four steps of proof above. As a result, if the input direction to our projection procedure is unbiased, the resulting update direction is also unbiased. This enables our solution's generalization to other types of DBGD algorithms.

3.4 Regret Analysis of DBGD-DSP
We now analyze the regret of our proposed DBGD-DSP algorithm, starting with its variance of gradient update.

Lemma 3.2. The variance of gradient update in DBGD-DSP is bounded by

E[|ht |2] = Eut Sd-1

| - Zt (wt + ut )Atut |2

 Rank(At ) . d

838

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

Proof.

E[|ht |2] = Eut | - Zt (wt + ut )Atut |2
 Eut |At ut |2 = Eut (At ut )(At ut ) = tr Eut AtututAt //apply the trace trick = tr At Eut ut ut At

= tr

At

1 d

I

At

=

1 tr
d

At At

1 = d tr (At ) //a projection matrix is idempotent

=

Rank(At ) d

where tr(·) denotes the matrix trace operation. The sixth equality

holds because ut is uniformly sampled from a unit sphere, and

its covariance matrix Eut

ut ut

is

1 d

I

.

Since

At

is

an

orthogonal

projection matrix, the eighth equality holds for At At = At . 

Remark. The variance of gradient update in DBGD [24] is bounded by Eut | - Zt (wt + ut )ut |2  1.

Comparing the variance of gradient update in DBGD-DSP with

DBGD,

our

method

reduces

the

variance

from

1

to

Rank(At d

)

.

Since

the dimension of projection matrix At is d-by-d, we have Rank(At ) 

d, which guarantees the reduction of variance in DBGD-DSP com-

paring to that in DBGD. The rank of At is also bounded by the number of examined documents mt , since document space St is constructed by these mt examined documents. In practice, users

would only examine a handful of documents [4, 9], while the rank-

ing feature dimension is expected to be much larger. We argue that mt  d, such that our document space projection achieves considerable variance reduction.

The significance of this variance reduction can be intuitively

understood from Figure 1: though different traces of model update

would eventually lead to the same converged model, if one has a

sufficiently large amount of interactions with users, the one with

lower variance would always require less observations. A faster

converging algorithm leads to user satisfaction earlier. Next, we

verify this benefit by proving the reduction of regret introduced by

the reduced variance in gradient estimation.

Theorem 3.3. By setting



m

=

max
t

mt

,



=

 2Rm ,  13LT 1/4

=

Rm 

,

T

the expected regret of DBGD-DSP as defined in Eq (1) is upper bounded

by,

E[Re]  2T T 3/426RmL,

(4)

where

T

=



L

 13LT

1/4



L 13LT 1/4 - Lv L2 2Rm

The proof is obtained by extending Theorem 2 in [24]. We omit the details due to space limit, and emphasize that the key difference is introduced by replacing variance of gradient estimation from

Eut | - Zt (wt + ut )ut |2 to Eut | - Zt (wt + ut )Atut |2 . Since

the the

variance regret of

of gradient estimation is reduced 
DBGD can be reduced from O( dT

from 1 3/4) to

Oto(RmanTkd(3A/t4)),,

where m is the maximum number of documents included in a docu-

ment space under a single query. Again, as the number of included

ranking features is oftentimes much larger than the number of doc-

uments a user would examine under a single query, the reduction of

regret is considerable. Moreover, as the reduction of variance from

our project-based method is independent from the way about how

the proposal directions are generated, our method can be generally

applied to most existing DBGD-type OL2R algorithms to improve

their learning convergence.

3.5 Practical Treatments of Document Space Projection
Now we discuss several practical treatments of our proposed Docu-
ment Space Projection method, including the construction of docu-
ment space and orthogonal projection matrix.
In our theoretical analysis, we have assumed the knowledge of
users' examined documents and corresponding projection matrix.
However, in practice, a user's result examination is unobserved.
A rich body of research has been developed to perform statistical
inference of it, collectively known as click modeling [3, 4]. Any
of these existing click modeling solutions can be plugged into our
solution framework, i.e., line 13 of Algorithm 1. In this work, we
simply follow [9] to infer user examination by the last clicked position: given the click position list Ct , we use the last clicked position cl,t to approximate the last examined position Mt by setting Mt = cl,t + k, where k is a hyper-parameter. Based on sequential examination hypothesis of click modeling, every document before the last clicked position is examined, and we use k to approximate the number of positions following the last clicked position that
was still examined. We leave more comprehensive study of click
modeling in our solution as future work.
The above treatment provides a reasonable inference of examined documents. However, it requires a careful choice of k for each query (preferably). If k is set too large, variance of gradient estimate will increase (as proved in Lemma 3.2). If k is too small, the document space may not include all examined documents, and it is
at risk of introducing bias in gradient projection. To avoid bias in
constructing the document space, we also consider adding histori-
cally examined documents to the current query's document space. Specifically, we add r recently examined documents to the current document space St to compensate the potentially overlooked examined documents in the current query.
In line 14 of Algorithm 1, we solve the orthogonal projection matrix At of document space St . At could be computed by several methods. Denote Dt as a d-by-mt matrix where each column is the feature vector for an examined document. One can use QR decom-
position or Singular Value Decomposition (SVD) to solve for its orthonormal basis Vt , and projection matrix can then be constructed by At = VtVtT . In our experiments, we chose SVD for constructing the basis of document space, because of its widely available and
efficient large-scale implementation. But the choice for the con-
struction of this project matrix does not affect the convergence nor
unbiasedness of our proposed solution.

839

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

4 EXPERIMENTS
To demonstrate our proposed Document Space Projection method's empirical efficacy, we compare the performance of several state-ofthe-art OL2R algorithms on five public learning to rank datasets, with and without our document space projection method applied.
4.1 Experiment Setup
· Datasets. We tested our algorithms and the baselines on five benchmark datasets: including MQ2007, MQ2008, NP2003 [11], MSLR-WEB10K [14], and the Yahoo! Learning to Rank Challenge dataset [2]. In each of the five datasets, each query-document pair is encoded as a vector of ranking features. These features include PageRank, TF.IDF, Okapi-BM25, URL length, language model score, and many more varied by dataset.
The MQ2007 and MQ2008 datasets are collected from the 2007 and 2008 Million Query track at TREC [19]. MQ2007 contains about 1700 queries, and MQ2008 contains about 800 queries, which represent a mix of informational and navigational search intents. They both have 46-dimensional feature vectors to represent querydocument pairs, and the document relevance are labeled in three grades: 0 (not relevant), 1 (relevant), and 2 (most relevant).
The NP2003 dataset also comes from the TREC Web track, consisting of queries crawled from the .gov domain. It is comprised of about 150 navigational-focused queries, with over 1000 document relevance assessments per query. It uses 64 ranking features, and the document relevance labels are binary (0 and 1 only).
The MSLR-WEB10K dataset was released by Microsoft in 2010, and consists of 10,000 queries with relevance assessments coming from a labeling set from the Microsoft Bing search engine. It has 136 ranking features, and the relevance judgments range from 0 (not relevant) to 4 (most relevant).
The Yahoo! Learning to Rank Challenge dataset was also released in 2010, as an effort on part of Yahoo! to promote the dataset as well as research into better learning to rank algorithms. The dataset contains about 36,000 queries, 883,000 assessed documents, and 700 ranking featuress. Again, the relevance judgments range from 0 (not relevant) to 4 (most relevant)
This diversity in the structure of the datasets that we chose to test on helps us to evaluate our algorithms more holistically. While small, the MQ2007 and MQ2008 sets have been around for a long time and have a good mix of query types. NP2003 gives us insight into how the algorithms perform on navigational search intents specifically, which are markedly different in nature from informational search intents. MSLR-WEB10K and the Yahoo! dataset are large-scale datasets used by actual commercial search engines, which give us a better understanding of how the algorithms perform in practice. Since each dataset was split into training, testing, and validation subsets, we used the training sets for online experiments to measure cumulative performance, and used the testing sets for evaluating offline performance. · Simulated User Interactions. Based on an online learning to rank framework proposed in [13], we use the standard setup to simulate user interactions. Within this framework, we used the Cascade Click Model to simulate user click behavior. This model assumes that a user interacts with a set of search results by linearly scanning the list from top and making a decision for each document

Table 1: Configurations of simulation click models.

Click Probability

Stop Probability

R 0 123 4 01234

Per 0.0 0.2 0.4 0.8 1.0 0.0 0.0 0.0 0.0 0.0

Nav 0.05 0.3 0.5 0.7 0.95 0.2 0.3 0.5 0.7 0.9

Inf 0.4 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5

as to whether or not to click. In the model, the probability of a click for a given document is conditioned on the relevance label of that document, as a user is expected to be more likely to click on relevant documents. After evaluating each document, the user must decide whether or not to continue perusing the list. This decision's probability distribution is again conditioned on the relevance of the current document, as a user is more likely to stop looking through the results if he/she has already satisfied their information need. These aforementioned probabilities can be altered to simulate different types of users and interactions.
As illustrated in Table 1, we use three different click model probability configurations to represent three different types of users. First, we have the perfect user, who clicks on all relevant documents and does not stop browsing until they have visited all of the documents. This type of users contribute the least noise, as they make no mistakes and the feedback is entirely accurate. Second, we have the navigational user, who is very likely to click on the first highly relevant document that he/she sees and stops there. Third, we have the informational user, who, in his/her search for information, sometimes clicks on irrelevant documents, and as such contributes a significant amount of noise in click feedback. · Evaluation Metrics. As set forth in [16], cumulative (online) Normalized Discounted Cumulative Gain (NDCG) and offline NDCG are commonly used metrics for evaluating OL2R algorithms. Cumulative NDCG is calculated by summing NDCG scores from successive iterations with a discount factor  set to 0.995. We assess our model's estimation convergence via cosine similarity between the current weight vector and a reference weight vector (considered to be the optimal vector) as estimated by an offline learning-to-rank algorithm trained with the complete true relevance judgment labels. Due to its superior empirical performance, we used LambdaRank [1] with no hidden layer in our experiments to estimate this reference weight vector. In each experiment, the number of iterations T was set to 10,000, and the current query Xt was randomly sampled from the dataset in each iteration. We execute all the experiments 15 times with different random seeds, and report and compare the average performance in all experiments. · Evaluation Questions. To better understand the advantages of our proposed algorithms, we aim to answer the following evaluation questions through the course of our experiments.
Q1: Can our proposed Document Space Projection method consistently improve the performance of state-of-the-art OL2R algorithms?
Q2: Do gradients rectified by our document space projection explore the gradient space more efficiently?
Q3: How do different hyper-parameter settings alter the performance of our document space projection?
· Baseline Algorithms. We choose the following three state-ofthe-art OL2R algorithms as our baselines for comparison:

840

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

NDCG NDCG NDCG

0.72 perfect

0.70

0.68

0.66

0.64

0.62 0

2000

4000

6000

Impressions

(a) Perfect

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

8000

10000

0.72 navigational
0.70

0.68

0.66

0.64

0.62

0.60

0.58 0

2000

4000

6000

Impressions

8000

(b) Navigational

10000

0.70 informational
0.68

0.66

0.64

0.62

0.60

0.58

0.56 0

2000

4000

6000

Impressions

8000

(c) Informational

10000

Figure 2: Offline NDCG@10 on Yahoo! dataset.

- DBGD [24]: A single direction uniformly sampled from the whole parameter space is explored.
- MGD [17]: Multiple directions are explored in one iteration to reduce the gradient estimation variance. Multileaving is used to compare multiple rankers. If there is a tie, the model updates towards the mean of all winners.
- NSGD[20]: Multiple directions are sampled from the null space of previously poorly performing gradients. Ties are broken by evaluating the tied candidate rankers on a recent set of difficult queries.
We apply our proposed Document Space Projection to the baseline algorithms, and compare them with DBGD-DSP, MGD-DSP and NSGD-DSP, respectively.
4.2 Performance of Document Space Projection
We begin our experimental analysis by answering our first evaluation question. We compared all algorithms over 3 click models and 5 datasets. We set the hyper-parameters of DBGD, MGD and NSGD according to their original papers. Following [17, 24], we set the exploration step size  to 1 and learning rate  to 0.1. Both MGD and NSGD explore 9 proposal directions in one iteration. For our document space projection method, we consider k = 3 documents following the last clicked position as examined documents, and add r = 10 recently examined documents into document space St . We use SVD to solve for orthonormal basis Vt of the document space St , and compute the projection matrix by At = VtVt.
We reported the offline NDCG@10 and online cumulative NDCG @10 after 10,000 iterations in Table 2 and Table 3. Due to space limit, we only reported the offline performance during the 10,000 iterations over 3 click models on Yahoo dataset, a large-scale realworld L2R dataset with 700 ranking features, in Figure 2. MGD improves the online performance over DBGD by exploring multiple rankers simultaneously, and NSGD further improves over MGD by exploring gradients in a constrained subspace, as shown in Table 2. We observe that our proposed document space projection method consistently improves the online performance of all baseline algorithms. Recall that in Section 3.4 our theoretical analysis suggested that document space projection reduces the gradient estimation variance and improves the regret (online performance) with respect to the ratio between the rank of document space and feature dimension. Correspondingly, we observe that indeed we improved the OL2R models' ranking performance significantly over MSLR-WEB10K and Yahoo datasets, which are collected from realworld commerical search engines and have much higher feature

dimensions (130 and 700 respectively). This result demonstrates the potential of document space projection to improve large-scale realworld OL2R applications with high-dimension ranking features, as our algorithm attains satisfactory performance earlier than other OL2R algorithms measured by online NDCG@10. We also notice that the standard deviation of those models' ranking performance is reduced when applying document space projection, which confirms our analysis of variance reduction in Lemma 3.2.
From Figure 2 and Table 3 we notice that document space projection mostly improves offline performance over baseline algorithms. Figure 2 shows that document space projection significantly accelerates the convergence rate over the baseline algorithms, because of the reduced variance in gradient estimation. We also observe that applying document space projection under the perfect click model may lead to degraded performance, for example DBGD on MQ2007 and Yahoo dataset. This is because document space projection guarantees an unbiased gradient estimation under the assumption of known result examinations, as discussed in Section 3.3. However, since in practice a user's result examination is unobserved, we approximated the examined documents by including all documents before the last clicked position and k additional documents after the last clicked position. The perfect click model is an ideal case that users' stop probability is set to 0.0 (see Table 1) and every document is examined. Here, the document space needs to include all displayed documents to guarantee the unbiasedness, which requires a significantly larger k compared to the k used for navigational and informational click models. We argue that in practice since users only examine a handful of documents, we could well-approximate the examined documents with a reasonable choice of k. More sophisticated click models can also be introduced. We will analyze the effect of k in Section 4.3. In addition, we also observe that under informational click model the performance of NSGD-DSP is slightly decreased compared with original NSGD over three datasets. Note that since NSGD does not guarantee its gradient exploration is unbiased, further projecting its gradient may also lead to a biased gradient update and thus a sub-optimal model.
4.3 Analysis of Document Space Projection
To answer the second evaluation question, we design two experiments to show the effectiveness of document space projected gradient. In the first experiment, we study the utility of document space projected gradient. We compare the ranking performance of linearly interpolating the unrectified direction ut and its document space projected version t , i.e., t + (1 - )ut , based on the MGD

841

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

Table 2: Online NDCG@10, standard deviation and relative improvement of document space projection of each algorithm after 10,000 queries.

Click Model Algorithm MQ2007

MQ2008

MSLR-WEB10K

NP2003

Yahoo

Perfect

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

679.3 (21.6) 689.1 (19.5)(+1.44%) 689.1 (14.6) 757.3 (16.2)(+9.90%) 684.4 (20.5) 732.5 (20.0)(+7.03%)

847.1 (38.4) 858.0 (39.2)(+1.29%) 859.4 (38.1) 919.5 (42.2)(+6.99%) 867.5 (40.3) 904.3 (38.0)(+4.24%)

532.2 (15.3) 553.6 (13.1)(+4.02%) 558.3 (7.0) 626.4 (9.6)(+12.20%) 589.5 (14.2) 635.6 (12.8)(+7.82%)

1130.2 (43.3) 1198.8 (40.0) (+6.07%) 1192.9 (44.6) 1335.3 (39.1)(+11.94%) 1274.9 (47.4) 1368.5 (41.1)(+7.34%)

1165.5 (22.6) 1198.8 (33.5)(+2.86%) 1201.9 (16.3) 1309.4 (10.6) (+8.94%) 1162.3 (12.9) 1270.1 (2.5)(+9.27%)

Navigational

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

646.1 (23.4) 664.9 (26.9)(+2.91%) 632.7 (15.5) 694.5 (15.7)(+9.77%) 660.1 (24.5) 724.6 (24.5)(+9.77%)

817.9 (45.5) 830.3 (44.1)(+1.52%) 827.5 (35.5) 882.3 (40.0)(+6.62%) 849.1 (36.6) 895.8 (34.2)(+5.50%)

517.5 (20.9) 543.1 (14.8)(+4.95%) 538.2 (7.2) 586.9 (9.5)(+9.05%) 562.1 (18.8) 608.3 (12.1) (+8.22%)

1062.3 (55.4) 1140.1 (52.5)(+7.32%) 1115.4 (44.6) 1300.9 (39.6)(+16.63%) 1211.1 (66.5) 1296.2 (24.3) (+7.03%)

1133.3 (40.8) 1199.4 (34.6)(+5.83%) 1171.3 (20.4) 1290.2 (15.3) (+10.15%) 1186.2 (16.8) 1283.4 (7.2)(+8.19%)

Informational

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

583.4 (46.0) 620.1 (40.8)(+6.29%) 621.2 (18.2) 671.4 (18.9)(+8.08%) 629.7 (25.3) 703.6 (29.2)(+11.74%)

763.9 (55.1) 782.4 (51.8) (+2.42%) 817.5 (45.3) 865.9 (37.7)(+5.92%) 814.9 (37.1) 871.3 (48.3)(+6.92%)

472.4 (34.6) 522.1 (18.6) (+10.52%) 538.3 (10.8) 580.5 (10.4)(+7.84%) 532.9 (15.2) 597.9 (14.1)(+12.20%)

849.8 (144.5) 992.5 (81.1)(+16.79%) 1107.9 (46.2) 1274.5 (42.9)(+15.04%) 1123.5 (59.8) 1222.8 (43.8)(+9.03%)

1107.3 (46.6) 1158.5 (22.0)(+4.62%) 1146.6 (37.5) 1268.1 (16.4)(+10.60%) 1110.5 (10.9) 1204.7 (9.6)(+8.48%)

Table 3: Offline NDCG@10, standard deviation and relative improvement of document space projection of each algorithm after 10,000 queries.

Click Model Algorithm MQ2007

MQ2008

MSLR-WEB10K

NP2003

Yahoo

Perfect

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

0.484 (0.023) 0.480 (0.020) (-0.83%) 0.495 (0.022) 0.501 (0.021)(+1.21%) 0.488 (0.019) 0.491 (0.022)(+0.61%)

0.683 (0.023) 0.685 (0.024) (+0.29%) 0.691 (0.020) 0.695 (0.022)(+0.58%) 0.689 (0.024) 0.691 (0.025)(+0.29%)

0.331 (0.009) 0.333 (0.011) (+0.6%) 0.334 (0.003) 0.409 (0.006)(+22.46%) 0.397 (0.012) 0.398 (0.008) (+0.25%)

0.737 (0.056) 0.738 (0.059) (+0.14%) 0.746 (0.048) 0.748 (0.055)(+0.27%) 0.743 (0.050) 0.750 (0.042) (+0.94%)

0.688 (0.011) 0.681 (0.013) (-1.02%) 0.715 (0.002) 0.725 (0.003)(+1.40%) 0.691 (0.005) 0.717 (0.004)(+3.76%)

Navigational

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

0.463 (0.028) 0.465 (0.024)(+0.43%) 0.426 (0.019) 0.467 (0.021)(+9.62%) 0.473 (0.022) 0.478 (0.020)(+1.06%)

0.667 (0.021) 0.668 (0.023)(+0.15%) 0.664 (0.016) 0.684 (0.017)(+3.01%) 0.676 (0.024) 0.683 (0.026)(+1.04%)

0.320 (0.012) 0.327 (0.011)(+2.19%) 0.321 (0.003) 0.331 (0.005)(+3.12%) 0.389 (0.013) 0.376 (0.014)(-3.34%)

0.728 (0.054) 0.734 (0.052)(+0.82%) 0.740 (0.048) 0.744 (0.053)(+0.54%) 0.732 (0.053) 0.788 (0.006)(+7.65%)

0.663 (0.020) 0.656 (0.013)(-1.06%) 0.703 (0.010) 0.714 (0.006)(+1.56%) 0.686 (0.008) 0.711 (0.001)(+3.64%)

Informational

DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP

0.410 (0.034) 0.427 (0.027)(+4.15%) 0.406 (0.020) 0.444 (0.025)(+0.44%) 0.469 (0.018) 0.466 (0.019)(-0.64%)

0.641 (0.031) 0.632 (0.031)(-1.4%) 0.651 (0.020) 0.669 (0.018)(+0.67%) 0.674 (0.023) 0.668 (0.026)(-0.89%)

0.294 (0.022) 0.309 (0.011)(+32.65%) 0.317 (0.003) 0.325 (0.004)(+0.33%) 0.360 (0.013) 0.340 (0.018)(-5.56%)

0.699 (0.063) 0.692 (0.062)(-1.00%) 0.726 (0.050) 0.738 (0.054)(+0.74%) 0.733 (0.056) 0.789 (0.013)(+7.64%)

0.623 (0.037) 0.63 (0.030)(1.12%) 0.668 (0.044) 0.701 (0.005)(+4.94%) 0.663 (0.015) 0.685 (0.004)(+3.32%)

algorithm on MSLR-WEB10K dataset. Similar observations were obtained on other datasets, but due to space limit we have to omit those detailed results. We report the online and offline performance by varying  from 0 (which is equivalent to the original MGD algorithm) and 1 (which is MGD-DSP) in Figure 3 (a) and (b). We can clearly observe a trend of increasing online performance over all

three click models when we increase , i.e., trust more on the projected direction t for model update. This confirms the effectiveness of the projected direction t within document space comparing with the unrectified direction ut from the entire parameter space. The offline performance is generally robust to the setting of  for
navigational and information click models. This is expected since

842

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

(a) Online performance of
linearly interpolating ut and its projection t

Cosine Similarity to w*
NDCG

0.5 informational
0.4
0.3

0.75 perfect
0.70
0.65

(b) Offline performance of
linearly interpolating ut and its projection t

0.2
0.1
0.0 0

MGD MGD-DSP

1000

2000

3000

Impressions

4000

5000

0.60
0.55
0.50 0

DBGD-DSP-GT DBGD-DSP MGD-DSP-GT MGD-DSP

1000

2000

3000

4000

5000

Impressions

(c) Cosine similarity between offline best model w and online (d) Comparing with ground-truth
document space model

Figure 3: Analyzing Document Space Projection.

(a) Including k documents following last clicked position

(b) Including r recently examined documents

Figure 4: Hyper-parameter tuning for Document Space Projection.

both MGD and MGD-DSP are unbiased and will eventually converge to similar offline performance after sufficiently large number of iterations (we had 10,000 iterations in our experiments).
In the second experiment, we trained an offline LambdaRank model [1] using the complete annotated relevance labels in the largescale MSLR-WEB10K dataset. Then given this w, we compared cosine similarity between the online estimated model parameters with and without DSP in each iteration using MGD as the baseline. We show the result of first 5,000 iterations. In Figure 3 (c) we can observe that MGD-DSP converges faster and better to w than MGD. This suggests the rectified gradient is more effective than the original one. We also compared with an oracle algorithm that knows the ground-truth examined documents, denoted as DSP-GT, to validate the effectiveness of our approximated document space. We show the result on DBGD and MGD under the perfect click model in Figure 3(d). We notice that oracle algorithms performed similar to our proposed algorithm with an approximated document space, which confirms the effectiveness of the approximation heuristics.
To answer the third evaluation question, we compare different hyper-parameters used for constructing the document space on MSLR-WEB10K dataset. We vary k from 0 to 7 and report the result in Figure4 (a). We notice that for navigational and informational click models, a relatively small k achieved the best performance, i.e., k = 3. This corresponds to the observation that users do not continue to examine many documents after their last click under these two click models. However, under perfect click model the models' performance increases with a larger k. This aligns with the conclusions from our discussion in Section 4.2 that under the perfect click model, we need to set a much larger k to accurately

construct the document space and guarantee an unbiased gradient estimate.
In Figure 4(b), we vary r . As we discussed in Section 3.5, we are motivated to add recently examined documents to compensate for potentially overlooked examined documents in the current query. The effect of different choices of r is more noticeable under the perfect click model. This echoes our analysis above that under perfect click model some examined documents may be overlooked when k is not large enough. Thus correctly setting up r could reduce the bias in document space construction and compensate the final performance. From the result figure, we notice that setting r = 20 provides the best result. Under navigational and informational click models, the algorithm is generally robust to the choice of r . This is because the approximations of examined documents are already accurate with a reasonable setting of k.
5 CONCLUSION
In this paper, we propose and develop the Document Space Projection (DSP) method for reducing variance in gradient estimation and improving online learning to rank performance. The key insight of DSP is to recognize that the interleaved test only reveals the projection of true gradient on the spanned space of examined documents. Including anything beyond this space for model update only introduces noise. Thus our method projects the selected model update direction back to the document space to reduce its variance. We proved that DSP maintains an unbiased gradient estimate, and it can substantially improve the regret bound for DBGD-style algorithms via the reduced variance. Through our extensive experiments, we found that DSP is able to provide statistically significant improvements to several state-of-the-art OL2R models, both in terms of variance reduction and overall performance, especially when the number of ranking features is large.
Currently, we are using a heuristic method to construct the document space. However, we did observe that the performance of DSP varies under different click models for simulated user click feedback, i.e., different underlying examination behaviors. As for our future work, we plan to incorporate different click modeling solutions for more accurate document space construction. It would also be meaningful to study how to perform document space based exploratory direction generation, before the interleaved test. Exploratory direction pre-selection is expected to further accelerate the gradient exploration and improve user satisfaction during online learning, but we also need to ensure it is unbiased.

843

Session 9C: Learning to Rank 2

SIGIR '19, July 21­25, 2019, Paris, France

ACKNOWLEDGMENTS
We thank the anonymous reviewers for their insightful comments.
This work was supported in part by National Science Foundation
Grant IIS-1553568 and IIS-1618948 and Bloomberg Data Science
Ph.D. Fellowship.
REFERENCES
[1] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.
[2] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Proceedings of the Learning to Rank Challenge. 1­24.
[3] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model for web search ranking. In Proceedings of the 18th international conference on World wide web. ACM, 1­10.
[4] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Proceedings of the 2008 international conference on web search and data mining. ACM, 87­94.
[5] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. 2005.
Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 385­394. [6] Artem Grotov and Maarten de Rijke. 2016. Online learning to rank for information retrieval: SIGIR 2016 Tutorial. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 1215­ 1218.
[7] Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. 2013.
Reusing historical interaction data for faster online learning to rank for IR. In Proceedings of the sixth ACM international conference on WSDM. ACM, 183­192. [8] Katja Hofmann, Shimon Whiteson, and Maarten De Rijke. 2011. A probabilistic method for inferring preferences from clicks. In Proceedings of the 20th ACM international conference on Information and knowledge management. ACM, 249­ 258.
[9] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2017. Accurately interpreting clickthrough data as implicit feedback. In ACM SIGIR Forum, Vol. 51. Acm, 4­11.
[10] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331.
[11] Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li. 2007. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval, Vol. 310.

[12] Harrie Oosterhuis and Maarten de Rijke. 2017. Balancing Speed and Quality in Online Learning to Rank for Information Retrieval. In Proceedings of the 2017 ACM CIKM. ACM, 277­286.
[13] Harrie Oosterhuis and Maarten de Rijke. 2018. Differentiable Unbiased Online Learning to Rank. Proceedings of the 27th ACM International Conference on Information and Knowledge Management - CIKM '18 (2018). https://doi.org/10. 1145/3269206.3271686
[14] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets.
arXiv:cs.IR/1306.2597
[15] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. 2008. How does clickthrough data reflect retrieval quality?. In Proceedings of the 17th ACM CIKM. ACM, 43­52.
[16] Anne Schuth, Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Lerot: An online learning to rank framework. In Proceedings of the 2013 workshop on Living labs for information retrieval evaluation. ACM, 23­26.
[17] Anne Schuth, Harrie Oosterhuis, Shimon Whiteson, and Maarten de Rijke. 2016. Multileave gradient descent for fast online learning to rank. In Proceedings of the Ninth ACM International Conference on WSDM. ACM, 457­466.
[18] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten de Rijke. 2014. Multileaved comparisons for fast online evaluation. In Proceedings of the 23rd ACM CIKM. ACM, 71­80.
[19] Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval. Vol. 1. MIT press Cambridge.
[20] Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, and Hongn-
ing Wang. 2018. Efficient exploration of gradient space for online learning to rank. In The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 145­154. [21] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016. Learning to rank with selection bias in personal search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 115­124. [22] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The k-armed dueling bandits problem. J. Comput. System Sci. 78, 5 (2012), 1538­1556. [23] Yisong Yue, Yue Gao, Oliver Chapelle, Ya Zhang, and Thorsten Joachims. 2010.
Learning more powerful test statistics for click-based retrieval evaluation. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 507­514. [24] Yisong Yue and Thorsten Joachims. 2009. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 1201­1208. [25] Tong Zhao and Irwin King. 2016. Constructing reliable gradient exploration for online learning to rank. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 1643­1652.

844

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

Asking Clarifying Questions in Open-Domain Information-Seeking Conversations

Mohammad Aliannejadi
Università della Svizzera italiana (USI) mohammad.alian.nejadi@usi.ch
Fabio Crestani
Università della Svizzera italiana (USI) fabio.crestani@usi.ch
ABSTRACT
Users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries, which may be a frustrating experience. Alternatively, systems can improve user satisfaction by proactively asking questions of the users to clarify their information needs. Asking clarifying questions is especially important in conversational systems since they can only return a limited number of (often only one) result(s).
In this paper, we formulate the task of asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation methodology for the task and collect a dataset, called Qulac, through crowdsourcing. Our dataset is built on top of the TREC Web Track 2009-2012 data and consists of over 10K question-answer pairs for 198 TREC topics with 762 facets. Our experiments on an oracle model demonstrate that asking only one good question leads to over 170% retrieval performance improvement in terms of P@1, which clearly demonstrates the potential impact of the task. We further propose a retrieval framework consisting of three components: question retrieval, question selection, and document retrieval. In particular, our question selection model takes into account the original query and previous question-answer interactions while selecting the next question. Our model significantly outperforms competitive baselines. To foster research in this area, we have made Qulac publicly available.
1 INTRODUCTION
While searching on the Web, users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries. Alternatively, systems can decide to proactively ask questions to clarify users' intent before returning the result list [9, 33]. In other words, a system can assess the level of confidence in the results and decide whether to return the results or ask questions from the users to clarify their information need. The questions can be aimed
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331265

Hamed Zamani
University of Massachusetts Amherst zamani@cs.umass.edu
W. Bruce Croft
University of Massachusetts Amherst croft@cs.umass.edu
Figure 1: Example conversations with clarifying questions from our dataset, Qulac. As we see, both users, Alice and Robin, issue the same query ("dinosaur"), however, their actual information needs are completely different. With no prior knowledge, the system starts with the same clarifying question. Depending on the user's answers, the system selects the next questions in order to clarify the user's information need. The tag "No answer" shows that the asked question is not related to the information need.
to clarify ambiguous, faceted or incomplete queries [44]. Asking clarifying questions is especially important in conversational search systems for two reasons: (i) conversation is the most convenient way for natural language interactions and asking questions [22] and (ii) a conversational system can only return a limited number of results, thus being confident about the retrieval performance becomes even more important. Asking clarifying questions is a possible solution for improving this confidence. Figure 1 shows an example of such a conversation selected from our dataset. We see that both users, Alice and Robin, issue the same query, "dinosaur." Assuming that the system does not have access to any prior personal or contextual information, the conversation starts with the same clarifying question. The rest of the conversation, however, depends on the users' responses. In fact, the users' responses aid the system to get a better understanding of the underlying information need.
A possible workflow for an information system with clarifying questions is shown in Figure 2. As we can see, Alice initiates a conversation by submitting her query to the system. The system then retrieves a list of documents and estimates its confidence on the

475

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

result list (i.e., "Present Results?"). If the system is not sufficiently confident to present the results to the user, it then starts the process of asking clarifying questions. As the first step, it generates a list of candidate questions related to Alice's query. Next, the system selects a question from the candidate question list and asks it from the user. Based on Alice's answer, the system retrieves new documents and repeats the process.
In this paper, we formulate the task of selecting and asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation framework based on faceted and ambiguous queries and collect a novel dataset, called Qulac,1 building on top of the TREC Web Track 2009-2012 collections. Qulac consists of over 10K questionanswer pairs for 198 TREC topics consisting of 762 facets. Inspired from successful examples of crowdsourced collections [2, 4], we collected clarifying questions and their corresponding answers for every topic-facet pair via crowdsourcing. Our offline evaluation protocol enables further research on the topic of asking clarifying questions in a conversational search session, providing a benchmarking methodology to the community.
Our experiments on an oracle model show that asking only one good question leads to over 100% retrieval performance improvement. Moreover, the analysis of the oracle model provides important intuitions related to this task. For instance, we see that asking clarifying questions can improve the performance of shorter queries more. Also, clarifying questions exhibit a more significant effect on improving the performance of ambiguous queries, compared to faceted queries. We further propose a retrieval framework following the workflow of Figure 2, consisting of three main components as follows: (i) question retrieval; (ii) question selection; and (iii) document retrieval. The question selection model is a simple yet effective neural model that takes into account both users' queries and the conversation context. We compare the question retrieval and selection models with competitive term-matching and learning to rank (LTR) baselines, showing their ability to significantly outperform the baselines. Finally, to foster research in this area, we have made Qulac publicly available.2
2 RELATED WORK
While conversational search has roots in early Information Retrieval (IR) research, the recent advances in automatic voice recognition and conversational agents have created increasing interest in this area. One of the first works in conversational IR dates back to 1987 when Croft and Thompson [16] proposed I3R that acted as an expert intermediary system, communicating with the user in a search session. A few years later Belkin et al. [7] characterized information-seeking strategies for conversational IR, offering users choices in a search session based on case-based reasoning. Since then researchers in the fields of IR and natural language processing (NLP) have studied various aspects of this problem. Early works focused on rule-base conversational systems [45, 47], while another line of research has investigated spoken language understanding approaches [1, 18, 29] for intelligent dialogue agents in the domain of flight [19] and train trip information [5]. The challenge was to understand the user's request and query a database
1Qulac, pronounced ku:l2k, means blizzard and wonderful in Persian. 2Code and data are available at https://github.com/aliannejadi/qulac.

Figure 2: A workflow for asking clarifying questions in an
open-domain conversational search system.
of flight or train schedule information accordingly. The recent advances of conversational agents have attracted research in various aspects of conversational information access [3, 6, 40, 49]. One line of research analyzes data to understand how users interact with voice-only systems [39]. Radlinski and Craswell [33] proposed a theoretical framework for conversational search highlighting the need for multi-turn interactions with users for narrowing down their specific information needs. Also, Trippas et al. [42] studied conversations of real users to identify the commonly-used interactions and inform a conversational search system design. Moreover, research on query suggestion is relevant to our work if we consider suggesting queries as a means of clarifying users' intent in a traditional IR setting [33]. Result diversification and personalizing is one of the key components for query suggestion [20], especially when applied to small-screen devices. In particular, Kato and Tanaka [21] found that presenting results for one facet and suggesting queries for other facets is more effective on such devices.
Research on clarifying questions has attracted considerable attention in the fields of NLP and IR. People have studied humangenerated dialogues on question answering (QA) websites, analyzing the intent of each utterance [32] and, more specifically, clarifying questions [9]. Kiesel et al. [22] studied the impact of voice query clarification on user satisfaction and found that users like to be prompted for clarification. Much work has been done on interacting with users for recommendation. For instance, Christakopoulou et al. [12] designed a system that can interact with users to collect more detailed information about their preferences in venue recommendation. Also, Sun and Zhang [40] utilized a semi-structured user query with facet-value pairs to represent a conversation history and proposed a deep reinforcement learning framework to build a personalized conversational recommender system. Focusing on clarifying questions, Zhang et al. [53] automatically extracted facet-value pairs from product reviews and considered them as questions and answers. They proposed a multi-memory network to ask questions for improved e-commerce recommendation. Our work is distinguished from these studies by formulating the problem of asking clarifying questions in an open-domain information-seeking conversational setting where several challenges regarding extracting topic facets [23] are different from a recommendation setting.
In the field of NLP, researchers have worked on question ranking [34] and generation [35, 46] for conversation. These studies rely on large amount of data from industrial chatbots [31, 46], query logs [37], and QA websites [34, 35, 41]. For instance, Rao and Daumé

476

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

[34] proposed a neural model for question selection on a simulated dataset of clarifying questions and answers extracted from QA websites such as StackOverflow. Later, they proposed an adversarial training for generating clarifying questions for a given product description on Amazon [35]. Also, Wang et al. [46] studied the task of question generation for an industrial chatbot. Unlike these works, we study the task of asking clarification question in an IR setting where the user's request is in the form of short queries (vs. a long detailed post on StackOverflow) and the system should return a ranked list of documents.
3 PROBLEM STATEMENT
A key advantage of a conversational search system is its ability to interact with the user in the form question and answer. In particular, a conversational search system can proactively pose questions to the users to understand their actual information needs more accurately and improve its confidence in the search results. We illustrate the workflow of a conversational search system, focusing on asking clarifying questions.
As depicted in Figure 2, once the user submits a query to the system, the Information Need Representation module generates and passes their information need to the Retrieval Model, which returns a ranked list of documents. The system should then measure its confidence in the retrieved documents (i.e., Present Results? in Figure 2). In cases where the system is not sufficiently confident about the quality of the result list, it passes the query and the context (including the results list) to the Question Generation Model to generate a set of clarifying questions, followed by the Question Selection Model whose aim is to select one of the generated questions to be presented to the user. Next, the user answers the question and the same procedure repeats until a stopping criterion is met. Note that when the user answers a question, the complete session information is considered for selecting the next question. In some cases, a system can decide to present some results, followed by asking a question. For example, assume a user submits the query "sigir 2019" and the system responds "The deadline of SIGIR 2019 is Jan. 28. Would you like to know where it will be held?" As we can see, while the system is able to return an answer with high confidence, it can still ask further questions [50]. In this work, we do not study this scenario; however, one can investigate it for exploratory search.
3.1 A Facet-Based Offline Evaluation Protocol
The design of an offline evaluation protocol is challenging because conversation requires online interaction between a user and a system. Hence, an offline evaluation strategy requires humangenerated answers to all possible questions that a system would ask, something that is impossible to achieve in an offline setting. To circumvent this problem, we substitute the Question Generation Model in Figure 2 with a large bank of questions, assuming that it consists of all possible questions in the collection. Although this assumption is not absolutely realistic, it reduces the complexity of the evaluation significantly as human-generated answers to a limited set of questions can be collected offline, facilitating offline evaluation.
In this work, we build our evaluation protocol on top of the TREC Web track's data. TREC has released 200 search topics, each

of which being either "ambiguous" or "faceted."3 Clarke et al. [13] defined these categories as follows: "... Ambiguous queries are those that have multiple distinct interpretations. ... On the other hand, facets reflect underspecified queries, with different aspects covered by the subtopics..." The TREC collection is originally designed to evaluate search result diversification. In contrast, here we build various conversation scenarios based on topic facets.
Formally, let T = {t1, t2, . . . , tn } be the set of topics (queries) that initiates a conversation. Moreover, we define F = {f1, f2, . . . , fn } as the set of facets with fi = { f1i , f2i , . . . , fmi i } defining different facets of ti , where mi denotes the number of facets for ti . Further, let Q = {q1, q2, . . . , qn } be the set of clarifying questions belonging to every topic, where qi = {qi1, qi2, . . . , qzi i } consists of all clarifying questions that belong to ti ; zi is the number of clarifying questions for ti . Here, our aim is to provide the users' answers to all clarifying questions considering all topics and their corresponding facets. Therefore, let A(t, f , q)  a define a function that returns answer a for a given topic t, facet f , and question q. Hence, to enable offline evaluation, A requires to return an answer for all possible values of t, f , and q. In this work, T and F are borrowed from the TREC Web track 2009-2012 data. Q is then collected via crowdsourcing and A(t, f , q) is also modeled by crowdsourcing (see Section 4). It is worth noting that we also borrow the relevance assessments of the TREC Web track, after breaking them down to the facet level. For instance, suppose the topic "dinosaur" has 10 relevant documents, 6 of which are labeled as relevant to the first facet, and 4 to the second facet. In Qulac, the topic "dinosaur" is broken into two topic-facet pairs together with their respective relevance judgments.
4 DATA COLLECTION
In this section, we explain how we collected Qulac (Questions for lack of clarity), that is, to the best of our knowledge, the first dataset of clarifying questions in an IR setting. As we see in Figure 1, each topic is coupled with a facet. Therefore, the same question would receive a different answer based on the user's actual information need. We follow a four-step strategy to build Qulac. In the first step we define the topics and their corresponding facets. In the second step, we collect a number of candidate clarifying questions (Q) for each query through crowdsourcing. Then, in the third step, we assess the relevance of the questions to each facet and collect new questions for those facets that require more specific questions. Finally, in the last step, we collect the answers for every queryfacet-question triplet, modeling A. In the following subsections, we elaborate on every step of our data collection procedure.
4.1 Topics and Facets
As we discussed earlier, the problem of asking clarifying questions is particularly interesting in cases where a query can be interpreted in various ways. An example is shown in Figure 1 where two different users issue the same query for different intents. Therefore, any data collection should contain an initial query and description of its facet, describing the user's information need. In other words, we define a target facet for each query. Faceted and ambiguous queries make an ideal case to study the effect of clarifying questions in a conversational search system for the following reasons: (i) the user
3In this work, we use the term "facet" to refer to the subtopics of both faceted and ambiguous topics.

477

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

information need is not clear from the query; (ii) multiple facets of the same query could satisfy the user's information need; (iii) asking clarifying questions related to any of the facets provide a high information gain. Therefore, we choose the TREC Web track's topics4 [15] as the basis for Qulac. In other words, we take the topics of TREC Web track 09-12 as initial user queries. We then break each topic into its facets and assume that each facet describes the information need of a different user (i.e., it is a topic). As we see in Table 1, the average facet per topic is 3.85 ± 1.05. Therefore, the initial 198 TREC topics5 leads to 762 topic-facet pairs in Qulac. Consequently, for each topic-facet pair, we take the relevance judgements associated with the respective facet.
4.2 Clarifying Questions
It is crucial to collect a set of reasonable questions that address multiple facets of every topic6 while containing sufficient negative samples. This enables us to study the effect of retrieval models under the assumption of having a functional question generation model. Therefore, we asked human annotators to generate questions for a given query based on the results they observed on a commercial search engine as well as query auto-complete suggestions.
To collect clarifying questions, we designed a Human Intelligence Task (HIT) on Amazon Mechanical Turk.7 We asked the workers to imagine themselves acting as a conversational agent such as Microsoft Cortana where an imaginary user had asked them about a topic. Then, we described the concept of facet to them, supporting it with multiple examples. Finally, we asked them to follow the steps below to figure out the facets of each query and generate questions accordingly:
(1) Enter the same query in a search engine of their choice and scan the results in the first three pages. Reading the title of the results as well as scanning the snippets would give them an idea of different facets of the query on the Web.
(2) For some difficult queries such as "toilet," scanning the results would not help in identifying the facets. Therefore, inspired by [8], we asked the workers to type the query in the search box of the search engine, and press the space key after typing the query. Most commercial search engines provide a list of query auto-complete suggestions. Interestingly, in most cases the suggested queries reflect various aspects of the same query.
(3) Finally, we asked them to generate six questions related to the query, aiming to address the facets that they had figured out.
We assigned two workers to each HIT, resulting in 12 questions per topic in the first round. In order to preserve language diversity of the questions, we limited each worker to a maximum of two HITs. HITs were available to workers residing in the U.S. with an approval rate of over 97%. After collecting the clarifying questions, in the next step, we explain how we verified them for quality assurance.
4.3 Question Verification and Addition
In this step, we aim to address two main concerns: (i) how good are the collected clarifying questions? (ii) are all facets addressed by at least one clarifying question? Given the high complexity of
4 https://trec.nist.gov/data/webmain.html 5 The official TREC relevance judgements cover 198 of the topics. 6Candidate clarifying questions should also address out-of-collection facets. 7 http://www.mturk.com

Table 1: Statistics of Qulac.

# topics

198

# faceted topics

141

# ambiguous topics

57

# facets Average facet per topic Median facet per topic # informational facets # navigational facets

762 3.85 ± 1.05 4 577 185

# questions # question-answer pairs Average terms per question Average terms per answer

2,639 10,277 9.49 ± 2.53 8.21 ± 4.42

this step, we appointed two expert annotators for this task. We instructed the annotators to read all the collected questions of each topic, marking invalid and duplicate questions. Moreover, we asked them to match a question to a facet if the question was relevant to the facet. A question was considered relevant to a facet if its answer would address the facet. Finally, in order to make sure that all facets were covered by at least one question, we asked the annotators to generate an additional question for the facets that needed more specific questions. The outcome of this step is a set of verified clarifying questions, addressing all the facets in the collection.

4.4 Answers
After collecting and verifying the questions, we designed another HIT in which we collected answers to the questions for every facet. The HIT started with detailed instructions of the task, followed by several examples. The workers were provided with a topic and a facet description. Then we instructed them to assume that they had submitted the query with their actual information need being the given facet. Then they were required to write the answer to one clarifying question that was presented to them. To avoid the bias of other questions for the same facet, we included only one question in each HIT. If a question required information other than what workers were provided with, we instructed the workers to identify it with a "No answer" tag. Each worker was allowed to complete a maximum of 100 HITs to guarantee language diversity. Workers were based in the U.S. with an approval rate of 95% or greater.
Quality check. During the course of data collection, we performed regular quality checks on the collected answers. The checks were done manually on 10% of submissions per worker. In case we observed any invalid submissions among the sampled answers of one user, we then studied all the submissions of the same user. Invalid submissions were then removed from the collection and the worker was banned from the future HITs. Finally, we assigned all invalid answers to other workers to complete. Moreover, we employed basic behavioral check techniques in the design of the HIT. For example, we disabled copy/paste features of text inputs and tracked workers' keystrokes. This enabled us to detect and reject low-quality submissions.

5 SELECTING CLARIFYING QUESTIONS
In this section, we propose a conversational search system that is able to select and ask clarifying questions and rank documents based on the user's responses. The proposed system retrieves a set of questions for a given query from a large pool of questions,

478

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

containing all the questions in the collection. At the second stage, our proposed model, called NeuQS, aims to select the best question to be posed to the user based on the query and the conversation context. This problem is particularly challenging because the conversational interactions are in natural language, highly depending on the previous interactions between the user and the system (i.e., conversation context).
As mentioned earlier in Section 3, a user initiates the conversation by submitting a query. Then the system should decide whether to ask a clarifying question or present the results. At every stage of the conversation, the previous questions and answers exchanged between the user and the system are known to the model. Finally, the selected question and its corresponding answer should be incorporated in the document retrieval model to enhance the retrieval performance.
Formally, for a given topic t let h = {(q1, a1), (q2, a2), . . . , (q |h|, a |h| )} be the history of clarifying questions and their corresponding answers exchanged between the user and the system (i.e., context). Here, the ultimate goal is to predict q, that is the next question that the system should ask from the user. Moreover, let a be the user's answer to q. The answer a is unknown to the question selection model, however, the document retrieval model retrieves documents once the system receives the answer a. In the following, we describe the question retrieval model, followed by the question selection and the document retrieval models.

5.1 Question Retrieval Model
We now describe our BERT8 Language Representation based Question Retrieval model, called BERT-LeaQuR. We aim to maximize the recall of the retrieved questions, retrieving all relevant clarifying questions to a given query in the top k questions. Retrieving all relevant questions from a large pool of questions is challenging, because questions are short and context-dependent. In other words, many questions depend on the conversation context and the query. Also, since conversation is in the form of natural language, termmatching models cannot effectively retrieve short questions. For instance, some relevant clarifying questions for the query "dinosaur" are: "Are you looking for a specific web page?" "Would you like to see some pictures?"
Yang et al. [51] showed that neural models outperform termmatching models for question retrieval. Inspired by their work, we learn a high-dimensional language representation for the query and the questions. Formally, BERT-LeaQuR estimates the probability p(R = 1|t, q), where R is a binary random variable indicating whether the question q should be retrieved (R = 1) or not (R = 0). t and q denote the query (topic) and the candidate clarifying question, respectively. The question relevance probability in the BERT-LeaQuR model is estimated as follows:

p(R = 1|t, q) =  T (t ), Q (q) ,

(1)

where T and Q denote topic representation and question representation, respectively.  is the matching component that takes the aforementioned representations and produces a question retrieval score. There are various ways to implement any of these components.

8BERT: Bidirectional Encoder Representations from Transformers

We implement T and Q similarly using a function that maps a sequence of words to a d-dimensional representation (V s  Rd ). We use the BERT [17] model to learn these representation functions. BERT is a deep neural network with 12 layers that uses an attention-based network called Transformers [43]. We initialize the BERT parameters with the model that is pre-trained for the language modeling task on Wikipedia and fine-tune the parameters on Qulac with 3 epochs. BERT has recently outperformed state-of-the-art models in a number of language understanding and retrieval tasks [17, 27]. We particularly use BERT in our model to incorporate the knowledge from the vast amount of unlabeled data while learning the representation of queries and questions. In addition, BERT shows promising results in modeling short texts.
The component is modeled using a fully-connected feed-forward network with the output dimensionality of 2. Rectified linear unit (ReLU) is employed as the activation function in the hidden layers, and a softmax function is applied on the output layer to compute the probability of each label (i.e., relevant or non-relevant). To train BERT-LeaQuR, we use a cross-entropy loss function.

5.2 Question Selection Model

In this section, we introduce a Neural Question Selection Model

(NeuQS) which selects questions with a focus on maximizing the

precision at the top of the ranked list. The main challenge in the

question selection task is to predict whether a question has diverged

from the query and conversation context. In cases where a user has

given a negative answer(s) to previous question(s), the model needs

to diverge from the history. In contrast, in cases where the answer

to the previous question(s) is positive, questions on the same topic

that ask for more details are preferred. For example, as we saw

in Figure 1, when Robin answers the first question positively (i.e.,

being interested in dinosaur books), the second question tries to

narrow down the information to a specific type of dinosaur.

NeuQS incorporates multiple sources of information. In partic-

ular, it learns from the similarity of a query, a question and the

context as well as retrieval and performance prediction signals. In

particular, NeuQS outputs a relevance score for a given query t,

question q, and conversation context h. Formally, NeuQS can be

defined as follows:

score =  T (t ), H (h), Q (q), (t, h, q),  (t, h, q) , (2)

where  is a scoring function for a given query representation T (t ), context representation H (h), question representation Q (q), retrieval representation (t, h, q), and query performance representa-

tion  (t, h, q). Various strategies can be employed to model each of

the components of NeuQS.

We model the components T and Q similarly to Section 5.1.

Further, the context representation component H is implemented

as follows:

H (h) =

1 |h|

|h|
QA (qi , ai ) ,
i

(3)

where QA (q, a) is an embedding function of a question q and

answer a. Moreover, the retrieval representation (t, h, q)  Rk

is implemented by interpolating the retrieval score of the query,

context and question (see Section 5.3) and the score of the top k

retrieved documents is used. Finally, the query performance prediction (QPP) representation component  (t, h, q)  Rk consists

479

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

of the performance prediction score of the ranked documents at different ranking positions (for a maximum of k ranked documents). We employed the  QPP model for this component [28]. We take the representations from the [CLS] layer of the pre-trained uncased BERT-Base model (i.e., 12-layer, 768-hidden, 12-heads, 110M parameters). To model the function  we concatenate and feed T (t ), H (h), Q (q), (t, h, q), and  (t, h, q) into a fully-connected feed-forward network with two hidden layers. We use ReLU as the activation function in the hidden layers of the network. We use a pointwise learning setting using a cross-entropy loss function.

5.3 Document Retrieval Model

Here, we describe the model that we use to retrieve documents given a query, conversation context, and current clarifying question as well as user's answer. We use the KL-divergence retrieval model [24] based on the language modeling framework [30] with Dirichlet prior smoothing [52] where we linearly interpolate two likelihood models: one based on the original query, and one based on the questions and their respective answers.
For every term w of the original query t, conversation context h, the current question q, and answer a, the interpolated query probability is computed as follows:

p(w |t, h, q, a) =  × p(w |t ) + (1 -  ) × p(w |h,q,a ) , (4)

where t denotes the language model of the original query, and h,q,a denotes the language model of all questions and answers that have been exchanged in the conversation.  determines the
weight of the original query and is tuned on the development set.
Then, the score of document d is calculated as follows:

p(d |t, h, q, a) =

p(wk |t, h, q, a) log(p(wk |d ) ,

(5)

wk 

where  is the set of all the terms present in the conversation. We use

Dirichlet's smoothing for terms that do not appear in d. We use the

document retrieval model for two purposes: (i) ranking documents

after the user answers a clarifying question; (ii) ranking documents

of a candidate question as part of the NeuQS (see Section 5.2). Hence,

the model does not see the answer in the latter case.

6 EXPERIMENTS
6.1 Experimental Setup
Dataset. We evaluate BERT-LeaQuR and NeuQS on Qulac, following a 5-fold cross-validation. We follow two strategies to split the data, (i) Qulac-T: we split the train/validation/test sets based on topics. In this case, the model has not seen the test topics in the training data; (ii) Qulac-F: here we split the data based on their facets. Thus, the same test topic might appear in the training set, but with a different facet.
In order to study the effect of multi-turn conversations with clarifying questions, we expand Qulac to include multiple artificially generated conversation turns. To do so, for each instance, we consider all possible combinations of questions to be asked as the context of conversation. Take t1 as an example where we select a new question after asking the user two questions. Assuming that t1 has four questions, all possible combinations of questions in the conversation context would be: (q1, q2), (q1, q3), (q1, q4), (q2, q3), (q2, q4), (q3, q4). Notice that the set of candidate clarifying questions for

Table 2: Performance of question retrieval model. The superscript * denotes statistically significant differences compared to all the baselines (p < 0.001).

Method

MAP Recall@10 Recall@20 Recall@30

QL BM25 RM3 LambdaMART RankNet BERT-LeaQuR

0.6714 0.6715 0.6858 0.7218 0.7304
0.8349*

0.5917 0.5938 0.5970 0.6220 0.6233
0.6775*

0.6946 0.6848 0.7091 0.7234 0.7314
0.8310*

0.7076 0.7076 0.7244 0.7336 0.7500
0.8630*

each multi-turn example would be the ones that have not appeared in the context. The number of instances grows significantly as we enlarge the length of the conversation, leading to a total of 907,366 instances in the collection. At each turn of the conversation, we select the question from all candidate questions of the same topic and facet, having the same conversation history. In other words, they share the same context. Since the total number of unique conversational contexts is 75,200, a model should select questions for 75,200 contexts from all 907,366 candidate questions.
Question retrieval evaluation metrics. We consider four metrics to evaluate the effectiveness of question retrieval models: mean average precision (MAP) and recall for the top 10, 20, and 30 retrieved questions (Recall@10, Recall@20, Recall@30). Our choice of measures is motivated by the importance of achieving high recall for this task.
Question selection evaluation metrics. Effectiveness is measured considering the performance of retrieval after adding the selected question to the retrieval model as well as the user answer. Five standard evaluation metrics are considered: mean reciprocal rank (MRR), precision of the top 1 retrieved document (P@1), and normalized discounted cumulative gain for the top 1, 5, and 20 retrieved documents (nDCG@1, nDCG@5, nDCG@20). We use the relevance assessments as they were released by TREC. However, we modify them in such a way to evaluate the performance with respect to every facet. For instance, if one topic consists of 4 facets it is then broken into 4 different topics each inheriting its own relevance judgements from the TREC assessments.
The choice of evaluation metrics is motivated by considering three different aspects of the task. We choose MRR to evaluate the effect of asking clarifying questions on ranking the first relevant document. We report P@1 and nDCG@1 to measure the performance for scenarios where the system is able to return only one result. This is often the case with voice-only conversational systems. Moreover, we report nDCG@5 and nDCG@20 as conventional ranking metrics to measure the impact of asking clarifying questions in a traditional Web search setting. Notice that nDCG@20 is the preferred evaluation metric for the ClueWeb collection due to the shallow pooling performed for relevance assessments [14, 26].
Statistical test. We determine statistically significant differences using the two-tailed paired t-test with Bonferroni correction at a 99.9% confidence interval (p < 0.001).
Compared methods. We compare the performance of our question retrieval and selection models with the following methods:

480

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

Table 3: Performance comparison with baselines. WorstQuestion and BestQuestion respectively determine the lower and upper bounds. The superscript * denotes statistically significant differences compared to all the baselines (p < 0.001).

Method
OriginalQuery  -QPP LambdaMART RankNet NeuQS

Qulac-T Dataset

MRR P@1 nDCG@1 nDCG@5 nDCG@20

0.2715 0.3570 0.3558 0.3573
0.3625*

0.1842 0.2548 0.2537 0.2562
0.2664*

0.1381 0.1960 0.1945 0.1979
0.2064*

0.1451 0.1938 0.1940 0.1943
0.2013*

0.1470 0.1812 0.1796 0.1804
0.1862*

MRR
0.2715 0.3570 0.3501 0.3568 0.3641*

Qulac-F Dataset

P@1 nDCG@1

0.1842 0.2548 0.2478 0.2559
0.2682*

0.1381 0.1960 0.1911 0.1986
0.2110*

nDCG@5
0.1451 0.1938 0.1896 0.1944 0.2018*

nDCG@20
0.1470 0.1812 0.1773 0.1809 0.1867*

WorstQuestion 0.2479 0.1451 BestQuestion 0.4673 0.3815

0.1075 0.3031

0.1402 0.2410

0.1483 0.2077

0.2479 0.4673

0.1451 0.3815

0.1075 0.3031

0.1402 0.2410

0.1483 0.2077

· Question retrieval: ­ BM25, RM3, QL: we index all the questions using Galago.9 Then, for a given query we retrieve the documents using BM25 [38], RM3 [25], and QL [30] models. ­ LambdaMART, RankNet: for every query-question pair, we use the scores obtained by BM25, RM3, and QL as features to train LambdaMART [48] and RankNet [10] implemented in RankLib.10 For every query, we consider all irrelevant questions as negative samples.
· Question selection: ­ OriginalQuery reports the performance of the document retrieval model only with the original query (Eq. (4) with  = 1). ­  -QPP: we use a simple yet effective query performance predictor,  [28] as an estimation of a question's quality. We calculate the  predictor of the document retrieval model with the following input: original query, the context, and candidate questions. We then select the question with the highest  value. ­ LambdaMART, RankNet: we consider the task of question selection as a ranking problem where a list of candidate questions should be ranked and the one with the highest rank is chosen. Therefore, we use LambdaMART [48] and RankNet [10] as two LTR baselines. The list of features are: (i) a flag determining if a question is open or not; (ii) a flag indicating if the answer to the last question in the context is yes or no; (iii)  [28] performance predictor of the current question; (iv) the Kendall's  correlation of the ranked list at 10 and 50 of the original query and the current question; (v) the Kendall's  correlation of the ranked list at 20 and 50 of the current question and previous question-answer pairs in the context; (vi) Similarity of the current question and the query based on their BERT representations; (vii) Similarity of the current question and previous question-answer pairs in the context based on their BERT representations. ­ BestQuestion, WorstQuestion: in addition to all the baselines, we also report the retrieval performance when the worst and the best question is selected for an instance. BestQuestion (WorstQuestion) selects the candidate question for which the MRR value of the retrieval model is the maximum (minimum).
9 https://sourceforge.net/p/lemur/galago/ 10 https://sourceforge.net/p/lemur/wiki/RankLib/

 MRR

1.0 0.5 0.0 -0.5

faceted ambiguous nav.

inf.

1

2

3

4

5 10

topic type

facet type

# query terms

Figure 3: Impact of topic type, facet type, and query length

on the performance of BestQuestion oracle model, com-

pared to OriginalQuery.

Note that the retrieval scores are calculated knowing the selected question and its answer (i.e., oracle model). Our goal is to show the upper and lower bounds.

6.2 Results and Discussion

Question retrieval. Table 2 shows the results of question retrieval for all the topics. As we see, BERT-LeaQuR is able to outperform all baselines. It is worth noting that the model's performance gets better as the number of retrieved documents increases. This indicates that BERT-LeaQuR is able to capture the relevance of query and questions when they lack common terms. In fact, we see that all term-matching retrieval models such as BM25 are significantly outperformed in terms of all evaluation metrics.
Oracle question selection: performance. Here we study the performance of an oracle model, i.e. assuming that an oracle model is aware of the answers to the questions. The goal is to show to what extent clarifying questions can improve the performance of a retrieval system. As we see in the lower rows of Table 3 selecting best questions (BestQuestion model) helps the model to achieve substantial improvement, even in the case that the retrieval model is very simple. This shows the high potential gain of asking good clarifying questions on the performance of a conversational system. Particularly, we examine the relative improvement of the system after asking only one question and observe that BestQuestion achieves over 100% relative improvement in terms of different evaluation metrics (MRR: 0.2820  0.5677, P@1: 0.1933  0.4986, nDCG@1: 0.1460  0.3988, nDCG@5: 0.1503  0.2793, nDCG@20: 0.1520  0.2265). It is worth mentioning that we observe the highest relative improvements in terms of nDCG@1 (=173%) and P@1 (=158%), exhibiting a high potential impact on voice-only conversational systems.

Oracle question selection: impact of topic type and length. We analyze the performance of BestQuestion based on the number

481

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

NeuQS

LambdaMART

RankNet

-QPP

OriginalQuery

Qulac-T
0.36 0.34

Qulac-F 0.36

Qulac-T
0.22 0.21 0.20 0.19

Qulac-F

Qulac-T
0.19
0.18

Qulac-F

MRR
MRR nDCG@1 nDCG@20

0.32

0.34

0.18

0.17

0.17

0.30

0.32

0.28

1

2

31

0.302

3

# conversation turns

# conversation turns

0.16 0.15

1

2

31

2

3

# conversation turns

# conversation turns

0.16

1

2

31

2

3

# conversation turns

# conversation turns

Figure 4: Performance0.c28omparison with the baselines for different number of conversation turns (k  {1, 2, 3}).

1

2

3

of query terms and topic type. We see tha#t tchoenrveelrastaitvioenimtuprrnosvement

Impact of clarifying questions on facets. We study the differ-

of BestQuestion is negatively correlated with the number of query

ence of MRR between NeuQS and OriginalQuery on all facets. Note

terms (Pearson's r = -0.2, p  0.001), suggesting that shorter

that for every facet we average the performance of NeuQS at dif-

queries require clarification in more cases. Also, comparing the

ferent conversation turns. Our goal is to see how many facets are

topic types (ambiguous vs. faceted), we see a significant difference

impacted positively by asking clarifying questions. NeuQS is im-

in the relative improvement. The average MRR for ambiguous

proves the effectiveness of retrieval by selecting relevant questions

topics is 0.3858, compared with the faceted topics with average

for a considerable number of facets on both data splits. In partic-

MRR of 0.2898. The difference was statistically significant (2-way

ular, the performance for 45% of the facets is improved by asking

ANOVA, p  0.001).

clarifying questions, whereas the performance for 19% is worse.

Question selection. Table 3 presents the results of the document retrieval model taking into account a selected question together with its answer. We see that all models outperform OriginalQuery, confirming that asking clarifying questions is crucial in a conversation, leading to high performance gain. For instance, compared to OriginalQuery, a model as simple as  -QPP achieves a 31% relative improvement in terms of MRR. Also, NeuQS consistently outperforms all the baselines in terms of all evaluation metrics on both data splits. All the improvements are statistically significant. Moreover, NeuQS achieves a remarkable improvement in terms of both P@1 and nDCG@1. These two evaluation metrics are particularly important for voice-only conversational systems where the system must return only one result to the user. The obtained improvements highlight the necessity and effectiveness of asking clarifying questions in a conversational search system, where they are perceived as natural means of interactions with users.
Impact of data splits. We compare the performance of models on both Qulac-T and Qulac-F data splits. We see that the LTR baselines perform worse on Qulac-F. Notice that the performance difference of LambdaMART among the splits is statistically significant in terms of all evaluation metrics (p < 0.001). RankNet, on the other hand, exhibits a more robust performance, i.e., the difference of its performance on the two splits is not statistically significant. Unlike the baselines, NeuQS exhibits a significant improvement in terms of all evaluation metrics on Qulac-F (p < 0.05), except for nDCG@5. This suggests that the baseline models are prone to overfitting on queries and conversations in the training data. As mentioned, Qulac-F's train and test sets may have some queries and questions in common, hurting models that are weak at generalization.
Impact of number of conversation turns. Figure 4 shows the performance of NeuQS as well as the baselines for different conversation turns. We evaluate different models at k turns (k  {1, 2, 3}). We see that the performance of all models improves as the conversation advances to multiple turns. Also, we see that all the models consistently outperform the OriginalQuery baseline at different number of turns. Finally, we see that NeuQS exhibits robust performance, outperforming all the baselines at different turns.

Case study: failure and success analysis. Finally, we analyze representative cases of failure and success of our proposed framework. We list three cases where selecting questions using NeuQS improves the retrieval performance, as well as three other examples in which the selected questions lead to decreased performance. MRR reports the difference of the performance of NeuQS and OriginalQuery in terms of MRR. As we see, the first three examples show the selected questions that hurt the performance (i.e., MRR < 0.0). The first row is an example where the user's response to the question is negative; however, the user provides additional information about their information need (i.e., facet). We see that even though the user has provided additional information, the performance drops. This is perhaps due to existence of no common terms between the additional information (i.e., "dog is too hot") and the facet (i.e., "excessive heat on dogs"). This is more evident when we compare this example with a successful answer: "No, I would like to know the effects of excessive heat on dogs." The second row of the table shows a case where the answer to the question is positive, but there is no common terms between the question and the facet. Again, the intuition here is that the retrieval model is not able to take advantage of additional information when it has no terms in common with relevant documents. The third row of the table shows another failure example where the selected question is not relevant to the facet and the user provides no additional information. This is a typical failure example where the system does not get any positive feedback, but could still use the negative feedback to improve the ranking. This can be done by diverging from the documents that are similar to the negative feedback.
As for the success examples, we have listed three types. The first example ("east ridge high school") is where the system is able to ask an open question. Open questions are very hard to formulate for open-domain information-seeking scenarios; however, it is more likely to get useful feedback from users in response to such questions. The fifth row shows an example of a positive feedback. The performance gain, in this case, is perhaps due to the existence of term "biography" in the question which would match with relevant documents. It is worth noting that the question and the query in this example have no common terms. This highlights the importance

482

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

Table 4: Failure and success examples of NeuQS. Failure and success are measured by the difference in performance of NeuQS and OriginalQuery in terms of MRR (MRR).

Query

Facet Description

Selected Question

User's Answer

dog heat

What is the effect of excessive heat Would you like to know how to care No, I want to know what happens

on dogs?

for your dog during heat?

when a dog is too hot.

sit and reach How is the sit and reach test prop- Do you want to know how to per- Yes, I do.

test

erly done?

form this test?

alexian broth- Find Alexian Brothers hospitals. ers hospital

Are you looking for our schedule of No, I don't need that. classes or events?

MRR -0.86
-0.75
-0.54

east ridge high Information about the sports pro- What information about East Ridge I'm looking for information about

school

gram at East Ridge High School in High School are you looking for? their sports program.

Clermont, Florida

euclid

Find information on the Greek Do you want a biography?

Yes.

mathematician Euclid.

rocky moun- Who are the sports reporters for the Would you like to read recent news No, I just want a list of the reporters

tain news

Rocky Mountain News?

about the Rocky Mountain News? who write the sports for the Rocky

Mountain News.

+0.96 +0.93 +0.88

of employing a language-representation-based question retrieval model (e.g., BERT-LeaQuR) as opposed to term-matching IR models. The last example shows a case where the answer is negative, but the user is engaged in the conversation and provides additional information about the facet. We see that the answer contains keywords of the facet description (i.e., "reporters," "sports"), improving the score of relevant documents that contain those terms.
7 LIMITATIONS AND FUTURE WORK
Every data collection comes with some limitations. The same is valid for Qulac. First, the dataset was not collected from actual conversations. This decision was mainly due to the unbalanced workload of the two conversation participants. In our crowdsourcing HITs, the task of question generation required nearly 10 times more effort compared to the task of question answering. This makes it challenging and more expensive to pair two workers as participants of the same conversation. There are some examples of this approach in the literature [11, 36]; however, they address the task of reading comprehension, a task that is considerably simpler than identifying topic facets. A possible future direction is to provide a limited number of pre-generated questions (say 10) to the workers to select from, so that the complexity of the task would be significantly reduced.
Furthermore, Qulac is built for single-turn conversations (i.e., one question; one answer). Even though there are questions that can be asked after one another to form a multi-turn conversation, our data collection approach does not guarantee the existence of multi-turn conversations that involve the same participants. Also, we believe that the quality of generated clarifying questions highly depends on how well the selected commercial search engine is able to diversify the result list. We aimed to minimize this bias by asking workers to scan at least three pages of the result list. Also, the questions added by expert annotators guarantees the coverage of all facets (see Section 4.3). Finally, as we mentioned, faceted and ambiguous queries are good examples of topics that a conversational system needs to clarify; however, this task cannot be limited only to such queries. One can collect a similar data for exploratory

search scenarios, where asking questions can potentially lead to more user engagement while doing exploratory search.
In this work, our main focus was on question selection. There are various directions that can be explored in the future. One interesting problem is to explore various strategies of improving the performance of the document retrieval model as new information is added to the model. Moreover, we assumed the number of conversation turns to be fixed. Another interesting future direction is to model the system's confidence at every stage of the conversation so that the model is able to decide when to stop asking questions and present the result(s).
8 CONCLUSIONS
In this work, we introduced the task of asking clarifying questions in open-domain information-seeking conversations. We proposed an evaluation methodology which enables offline evaluation of conversational systems with clarifying questions. Also, we constructed and released a new data collection called Qulac, consisting of 762 topic-facet pairs with over 10K question-answer pairs. We further presented a neural question selection model called NeuQS along with models on question and document retrieval. NeuQS was able to outperform the LTR baselines significantly. The experimental analysis provided many insights of the task. In particular, experiments on the oracle model demonstrated that asking only one good clarifying question leads to over 150% relative improvement in terms of P@1 and nDCG@1. Moreover, we observed that asking clarifying questions improves the model's performance for a substantial percentage of the facets. In some failure cases, we saw that a more effective document retrieval model can potentially improve the performance. Finally, we showed that, asking more clarifying questions leads to better results, once again confirming the effectiveness of asking clarifying questions in a conversational search system.

483

Session 5A: Conversation and Dialog

SIGIR '19, July 21­25, 2019, Paris, France

ACKNOWLEDGMENTS
This work was supported in part by the RelMobIR project of the
Swiss National Science Foundation (SNSF), in part by the Center
for Intelligent Information Retrieval, and in part by NSF grant
IIS-1715095. Any opinions, findings and conclusions or recommen-
dations expressed in this material are those of the authors and do
not necessarily reflect those of the sponsors.
REFERENCES
[1] Mohammad Aliannejadi, Masoud Kiaeeha, Shahram Khadivi, and Saeed Shiry Ghidary. 2014. Graph-Based Semi-Supervised Conditional Random Fields For Spoken Language Understanding Using Unaligned Data. In ALTA. 98­103.
[2] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. In Situ and Context-Aware Target Apps Selection for Unified Mobile Search. In CIKM. 1383­1392.
[3] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. Target Apps Selection: Towards a Unified Search Framework for Mobile Devices. In SIGIR. 215­224.
[4] Omar Alonso and Maria Stone. 2014. Building a Query Log via Crowdsourcing. In SIGIR. 939­942.
[5] Harald Aust, Martin Oerder, Frank Seide, and Volker Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication 17, 3-4 (1995), 249­262.
[6] Seyed Ali Bahrainian and Fabio Crestani. 2018. Augmentation of Human Memory: Anticipating Topics that Continue in the Next Meeting. In CHIIR. 150­159.
[7] Nicholas J Belkin, Colleen Cool, Adelheit Stein, and Ulrich Thiel. 1995. Cases, scripts, and information-seeking strategies: On the design of interactive information retrieval systems. Expert systems with applications 9, 3 (1995), 379­395.
[8] Jan R. Benetka, Krisztian Balog, and Kjetil Nørvåg. 2017. Anticipating Information Needs Based on Check-in Activity. In WSDM. 41­50.
[9] Pavel Braslavski, Denis Savenkov, Eugene Agichtein, and Alina Dubatovka. 2017. What Do You Mean Exactly?: Analyzing Clarification Questions in CQA. In CHIIR. 345­348.
[10] Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. 2005. Learning to rank using gradient descent. In ICML. 89­96.
[11] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in Context. In EMNLP. 2174­2184.
[12] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational Recommender Systems. In KDD. 815­824.
[13] Charles L. A. Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the TREC 2009 Web Track. In TREC.
[14] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. 2011. Overview of the TREC 2011 Web Track. In TREC.
[15] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In TREC.
[16] W. Bruce Croft and R. H. Thompson. 1987. I3R: A new approach to the design of document retrieval systems. JASIS 38, 6 (1987), 389­404.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 (2018).
[18] Yulan He and Steve J. Young. 2005. Semantic processing using the Hidden Vector State model. Computer Speech & Language 19, 1 (2005), 85­106.
[19] Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS Spoken Language Systems Pilot Corpus. In HLT. 96­101.
[20] Di Jiang, Kenneth Wai-Ting Leung, Lingxiao Yang, and Wilfred Ng. 2015. Query suggestion with diversification and personalization. Knowl.-Based Syst. 89 (2015), 553­568.
[21] Makoto P. Kato and Katsumi Tanaka. 2016. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation. In WSDM. 133­142.
[22] Johannes Kiesel, Arefeh Bahrami, Benno Stein, Avishek Anand, and Matthias Hagen. 2018. Toward Voice Query Clarification. In SIGIR. 1257­1260.
[23] Weize Kong and James Allan. 2013. Extracting query facets from search results. In SIGIR. 93­102.
[24] John Lafferty and Chengxiang Zhai. 2001. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In SIGIR. 111­119.
[25] Victor Lavrenko and W. Bruce Croft. 2001. Relevance-Based Language Models. In SIGIR. 120­127.
[26] Xiaolu Lu, Alistair Moffat, and J. Shane Culpepper. 2016. The effect of pooling and evaluation depth on IR metrics. Inf. Retr. Journal 19, 4 (2016), 416­445.
[27] Harshith Padigela, Hamed Zamani, and W. Bruce Croft. 2019. Investigating the Successes and Failures of BERT for Passage Re-Ranking. arXiv:1903.06902 (2019).

[28] Joaquín Pérez-Iglesias and Lourdes Araujo. 2010. Standard Deviation as a Query Hardness Estimator. In SPIRE. 207­212.
[29] Roberto Pieraccini, Evelyne Tzoukermann, Z. Gorelov, Jean-Luc Gauvain, Esther Levin, Chin-Hui Lee, and Jay Wilpon. 1992. A speech understanding system based on statistical representation of semantics. In ICASSP. 193­196.
[30] Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In SIGIR. 275­281.
[31] Minghui Qiu, Liu Yang, Feng Ji, Wei Zhou, Jun Huang, Haiqing Chen, W. Bruce Croft, and Wei Lin. 2018. Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce. In ACL (2). 208­213.
[32] Chen Qu, Liu Yang, W. Bruce Croft, Johanne R. Trippas, Yongfeng Zhang, and Minghui Qiu. 2018. Analyzing and Characterizing User Intent in Informationseeking Conversations. In SIGIR. 989­992.
[33] Filip Radlinski and Nick Craswell. 2017. A Theoretical Framework for Conversational Search. In CHIIR. 117­126.
[34] Sudha Rao and Hal Daumé. 2018. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information. In ACL (1). 2736­2745.
[35] Sudha Rao and Hal Daumé III. 2019. Answer-based Adversarial Training for Generating Clarification Questions. arXiv:1904.02281 (2019).
[36] Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. CoQA: A Conversational Question Answering Challenge. arXiv:1808.07042 (2018).
[37] Gary Ren, Xiaochuan Ni, Manish Malik, and Qifa Ke. 2018. Conversational Query Understanding Using Sequence to Sequence Modeling. In WWW. 1715­1724.
[38] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In TREC. 109­126.
[39] Damiano Spina, Johanne R. Trippas, Lawrence Cavedon, and Mark Sanderson. 2017. Extracting audio summaries to support effective spoken document search. JASIST 68, 9 (2017), 2101­2115.
[40] Yueming Sun and Yi Zhang. 2018. Conversational Recommender System. In SIGIR. 235­244.
[41] Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng, and Dongyan Zhao. 2017. How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models. In ACL (2). 231­236.
[42] Johanne R. Trippas, Damiano Spina, Lawrence Cavedon, Hideo Joho, and Mark Sanderson. 2018. Informing the Design of Spoken Conversational Search: Perspective Paper. In CHIIR. 32­41.
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 (2017).
[44] Alexandra Vtyurina, Denis Savenkov, Eugene Agichtein, and Charles L. A. Clarke. 2017. Exploring Conversational Search With Humans, Assistants, and Wizards. In CHI Extended Abstracts. 2187­2193.
[45] Marilyn A. Walker, Rebecca J. Passonneau, and Julie E. Boland. 2001. Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems. In ACL. 515­522.
[46] Yansen Wang, Chenyi Liu, Minlie Huang, and Liqiang Nie. 2018. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. In ACL (1). 2193­2203.
[47] Jason D. Williams, Antoine Raux, Deepak Ramachandran, and Alan W. Black. 2013. The Dialog State Tracking Challenge. In SIGDIAL. 404­413.
[48] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Inf. Retr. 13, 3 (2010), 254­270.
[49] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System. In SIGIR. 55­64.
[50] Rui Yan, Dongyan Zhao, and Weinan E. 2017. Joint Learning of Response Ranking and Next Utterance Suggestion in Human-Computer Conversation System. In SIGIR. 685­694.
[51] Liu Yang, Hamed Zamani, Yongfeng Zhang, Jiafeng Guo, and W. Bruce Croft. 2017. Neural Matching Models for Question Retrieval and Next Question Prediction in Conversation. arXiv:1707.05409 (2017).
[52] Chengxiang Zhai and John Lafferty. 2017. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. SIGIR Forum 51, 2 (2017), 268­276.
[53] Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W. Bruce Croft. 2018. Towards Conversational Search and Recommendation: System Ask, User Respond. In CIKM. 177­186.

484

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Time-Limits and Summaries for Faster Relevance Assessing

Shahin Rahbariasl
srahbari@uwaterloo.ca University of Waterloo Waterloo, Ontario, Canada
ABSTRACT
Relevance assessing is a critical part of test collection construction as well as applications such as high-recall retrieval that require large amounts of relevance feedback. In these applications, tens of thousands of relevance assessments are required and assessing costs are directly related to the speed at which assessments are made. We conducted a user study with 60 participants where we investigated the impact of time limits (15, 30, and 60 seconds) and document size (full length vs. short summaries) on relevance assessing. Participants were shown either full documents or document summaries that they had to judge within a 15, 30, or 60 seconds time constraint per document. We found that using a time limit as short as 15 seconds or judging document summaries in place of full documents could significantly speed judging without significantly affecting judging quality. Participants found judging document summaries with a 60 second time limit to be the easiest and best experience of the six conditions. While time limits may speed judging, the same speed benefits can be had with high quality document summaries while providing an improved judging experience for assessors.
KEYWORDS
Relevance Assessing; Time Limits; Document Summaries
ACM Reference Format: Shahin Rahbariasl and Mark D. Smucker. 2019. Time-Limits and Summaries for Faster Relevance Assessing. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331270
1 INTRODUCTION
The idea of a speed accuracy tradeoff (SAT) is well known and embedded in people's common sense understanding [4]. For many tasks, we intuitively know that we can speed our work at the risk of having quality suffer. Likewise, we know that to obtain high quality work, we need to give ourselves time to do a job carefully.
When assessors are hired to collect relevance judgments for tasks such as test collection construction [9], speed and accuracy are chief concerns. The faster that assessors judge, the sooner we are done and usually the lower the cost. While faster judgments are
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331270

Mark D. Smucker
mark.smucker@uwaterloo.ca University of Waterloo
Waterloo, Ontario, Canada
better, we want to maintain accuracy or at least make an informed decision about the tradeoff between speed and accuracy.
Many factors influence the speed and accuracy of relevance judging with the chief ones being the assessors, the search topic or task, and the items being judged. For example, Smucker and Jethani [10] found that crowdsourced assessors judged newswire articles at an average rate of 15 seconds per document while supervised assessors in a laboratory setting took 27 seconds per document. In their experiment, they found lab workers to have somewhat better ability to discriminate relevant from non-relevant documents, but this difference was not statistically significant. For legal e-discovery tasks, Oard and Webber [8] report that a few minutes per document is typical.
In Heitz [4]'s review of the speed accuracy tradeoff, he notes that behavioral science has come to see the process as one where sensory information is accumulated in a sequential fashion and lower accuracy results from less accumulated information on which to make a decision. Assuming that we can extend this understanding of the speed accuracy tradeoff to relevance assessing, we can think of an assessor as scanning, skimming, and reading a document and at any moment, we can stop the sequential input and ask for a relevance judgment. The less time the assessor has, the less material has been consumed on which to make a judgment. Placing a time limit on judging is akin to limiting the assessor to a summary of the document, albeit a summary formed in their mind via their reading behavior up to the point at which time runs out.
In this paper, we investigate the effect of time limits and document summaries on relevance assessing speed and accuracy. We conducted a controlled laboratory study that used a factorial design and varied the time allowed (15, 30, and 60 seconds) and the document form (full length newswire or a summary). Our summaries were created based on a model of relevance for the topic, and thus our results for summaries represent the potential of summaries rather than the specific performance of a given summarization algorithm.
We found that providing more time does result in better discrimination between relevant and non-relevant documents, but most of this advantage was for full documents rather than summaries, i.e. time limits less than 60 seconds hurt the discrimination ability of assessors when judging full documents. Little difference was found in discrimination ability for short summaries at the various time limits. In addition, no statistically significant difference was found in accuracy of judging for the time limits or for full documents vs. document summaries.
After each search task, we asked participants about the ease of judging and other factors, and judging summaries with a time limit of 60 seconds was most favored by participants. Indeed, the average time to judge a summary under the 60 second limit was only 13.4 seconds. In other words, with effectively no time limit, judging

901

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

summaries can be fast with little loss in quality. Time limits with full documents can be used to force the relevance assessor to make a judgment based on their limited examination of the document with little loss in accuracy, but this comes with the additional cost of time pressure and stress on the assessors.
2 RELATED WORK
In a series of four experiments, Maddalena et al. [6] investigated crowdsourced assessors' judging behavior and the effect of placing time limits on judging. In these experiments, the crowdworkers judged newswire articles from TREC 8. If workers were given a time limit, Maddalena et al. found that a shorter time of 15 seconds resulted in better assessor agreement with the official relevant judgments as compared to a longer 30 seconds time limit. Other experiments investigated judging without time limits, time limits from 3 to 30 seconds and the advantage of a fixed judging time vs. a time limit. Time limits less than 15 seconds resulted in lower judging quality. They also discovered that a fixed judging time of 25-30 seconds was best. A fixed judging time requires the assessor to submit the judgment at the end of the time period and likely encourages the assessor to keep reading a documment until the time is up. In contrast to our work, Maddalena et al. [6] focused solely on crowdsourced workers while we used university students in a supervised setting and also investigated summaries vs. full documents.
Wang and Soergel [12] performed a user study on different parameters in relevance assessment including agreement, speed, and accuracy. Moderate to a substantial agreement was reported between the two groups of students. There was no significant difference in the speed of the two groups of assessors but the speed varied between the individuals.
Similarly, Wang [11] studied the accuracy, agreement, speed and perceived difficulty in relevance judgments for e-discovery. Wang found speed and perceived difficulty to be correlated as well as perceived difficulty and accuracy, but Wang did not find a correlation between accuracy and speed. Regarding speed, only a small fraction of documents slowed down the assessors. In contrast to our work and Maddalena et al. [6], Wang [11] did not apply time limits and thus working faster or slower would have been at the discretion of the assessors. Assuming most assessors make a legitimate attempt to do a good job, it is reasonable to assume that assessors without time limits will modulate their time to achieve a uniform level of accuracy.
3 MATERIALS AND METHODS
We conducted a 2x3 factorial experiment. Our two factors were document type (full document or summary) and time limit (15, 30, and 60 seconds). In this section we describe the details of the experiment.
3.1 User Study
After receiving ethics clearance from our university's office of research ethics, we recruited participants, and after pilot testing, we had 60 people participate in the final study. Each participant started with a tutorial describing the experiment and practice using the interface to judge the relevance of 5 documents.

For the main task, participants judged 20 documents for each of six search topics that were different from the topic used in the tutorial phase. In total, participants judged 120 documents. The 2x3 factorial design resulted in 6 treatments. For a given topic, a participant received a single treatment. Of the 20 documents for each topic, half were relevant and half were non-relevant. Each participant saw a randomized order of the topics and documents. The six treatments were balanced across users and task order with a Latin square. Before each search task, participants answered a pretask questionnaire that asked them about their knowledge about the topic and other matters. After each search task, participants answered a questionnaire about the judging experience. We paid each participant $20 for their participation.
3.2 User Interface
We designed the user interface to show the participants one document at time. Participants could see the title of the document and either the full document or a short document summary. The only actions allowed were to submit a judgment of relevant or non-relevant. Through the whole study, participants could see the search topic and its description for that task. Participants were also provided with information about the number of documents and the time left for judging each document.
The study involved three different time constraints for sets of 20 documents: 15, 30, and 60 seconds per document. When participants ran out of time, the document was hidden and participants had to submit their judgment to proceed. We recorded the overall time they spent on judging the document including the time after hiding the document.
3.3 Topics and Documents
In total, we used seven topics from the 2017 TREC Common Core track [1] and documents from the New York Times annotated corpus. Six topics were for the main task (topic ids = 310, 336, 362, 426, 427, 436), and one was for the tutorial phase (id=367). We combined the topic description and narrative into a single description for display to the participants.
We carefully selected documents for the experiment to avoid biasing judging errors toward false positives or false negatives. For each topic, we first computed a document ordering using reciprocal rank fusion (RRF) [3] from the runs submitted by groups participating in the 2017 TREC Common Core track. Documents ranked highly by RRF are considered likely to be relevant by the majority of participating retrieval systems. Any highly ranked non-relevant document likely looks relevant on its surface and likely will lead to false positives by assessors. Likewise, a relevant document ranked lowly by RRF will likely generate false negatives by assessors. To get a good mix of documents, we divided each RRF list into an ordered list of NIST judged relevant and a list of non-relevant documents. We then split each list in half to produce four lists and then randomly sampled 5 documents from each list. In this fashion, we have balanced the types of errors that users will make when judging.

902

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

3.4 Document Summaries
For three out of the six main tasks, we showed the users paragraphlong summaries instead of full documents. We use a modification of the method of Zhang et al. [14] to create the summaries. This method trains a classifier to recognize relevant material and after dividing a document into paragraphs, selects the paragraph with the highest likelihood of being relevant as the summary. Our modification is that we trained the classifier based on the known relevant and non-relevant documents from the NIST judgments. As such, our document summaries represent an upper bound on the technique's potential for selecting the most relevant paragraph from a document.

3.5 Measures of Accuracy and Discrimination
We define the NIST relevance judgments to be the truth and measure performance against them. As such, a basic measure is accuracy:

Accur acy

=

|T P |

+

|T P | + |T N | |F P | + |T N | +

|FN |

(1)

where |T P | is the number of true positives (NIST says relevant and participant says relevant), and |T N | is the number of true negatives (agree on non-relevant), |F P | is the number of false positives, and |F N | is the number of false negatives. Accuracy cannot tell us about the types of errors being made, and thus we also report the true positive rate (TPR) and false positive rate (FPR). The TPR is the fraction of relevant documents judged as relevant, and the FPR is the fraction of non-relevant documents judged as relevant. Relevance judging can be understood as a discrimination task, and as such, a measure of discrimination ability is d  [5]. Since true positive or false positive rates of 1 and 0 can lead to infinities in the computation of d , we employ a standard smoothing mechanism of adding a pseudo-document to the count of documents judged. Therefore, the estimated true positive rate (eTPR) is defined as: eT PR = (|T P | + 0.5)/(|T P | + |F N | + 1) , and estimated false positive rate (eFPR) is calculated as: eF PR = (|F P | + 0.5)/(|F P | + |T N | + 1) , and d  is computed as: d  = z(eT PR) -z(eF PR) , where the function z is the inverse of the normal distribution function.
To measure statistical significance of results, we used generalized linear mixed-effects models, as implemented in the lme4 [2] package in R. Both the participants and topics are random effects and the experiment factors (time limits and document type) are fixed effects. We measure the statistical significance of the factor on the dependent variable (e.g. accuracy) by building a complete model and then a model without the factor that we are testing. By comparing these two models with a likelihood ratio test, we obtain the reported p-value.

4 RESULTS AND DISCUSSION
Table 1 shows the average time participants took to judge a document, and Table 2 shows the fraction of judgments that exceeded the time limit. As the time limit increases, the average time to judge a document increases regardless of whether it is a full document or a summary. When the time limit is 60 seconds, participants rarely exceed the time limit and we see that full documents take on average 22.6 seconds to judge in comparison to only 13.4 seconds for a summary. When a time limit of 15 seconds is imposed, both

Time Limit (seconds)

Doc. Type 15 30

60 Mean p-value

Full doc. 9.8 15.2 22.6 Summary 9.1 11.5 13.4

15.9 11.3

p  0.001

Mean 9.5 13.4 18.0 13.6

p-value

p  0.001

Table 1: Average time in seconds to judge a document.

Time Limit (seconds)

Doc. Type 15 30

60 Mean

Full doc. 0.25 0.14 Summary 0.17 0.04

0.04 0.14 0.00 0.07

Mean

0.21 0.09

0.02 0.11

Table 2: Fraction of judgments that exceeded time limit.

Time Limit (seconds)

Doc. Type 15 30 60 Mean

Full doc. 0.70 0.71 0.73 0.71

Summary 0.73 0.72 0.74 0.73

Mean 0.71 0.71 0.74 0.72

p-value

p = 0.09

Table 3: Average accuracy.

p-value p = 0.18

full documents and summaries take only 9.8 and 9.1 seconds respectively. In effect, the 15 seconds time limit forces an assessor to consume material on par with a paragraph long summary. For similar judging tasks, Zhang [13] found average, unrestricted (no time limits) judging times of 22.7 seconds for summaries and 50.0 seconds for full documents.
Table 3 shows the average accuracy for the 60 participants and the six experimental treatments. As explained in Section 3.5, we report the statistical significance of each of the experimental factors' effect on accuracy, and as Table 3 shows, neither the time limits nor the document type had a statistically significant effect on accuracy. In contrast to accuracy, Table 5 shows that time has a statistically significant effect on the d  measure of discrimination ability. As with accuracy, discrimination ability with summaries is not different from full documents at a statistically significant level.
Tables 6 and 7 show the true positive and false positive rates for each of the six treatments. The higher true positive rate for summaries along with the effectively same false positive rate as for judging full documents shows that summaries have the potential to help assessors identify relevant material while not simply raising the false positive rate. With full documents, as the participants had more time, they were able to consume more material and suppress false positives. Thus, while there is little advantage to judging full documents in terms of accuracy, the evidence shows that full documents result in more conservative judgments.
Finally, Table 4 shows the results for a selection of the posttask questionnaire's questions. In comparison to full documents, participants found judging summaries with a 60 second time limit, which is akin to having no time limit for such small amounts of text,

903

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Question

Treatment (time limit, doc. type)

(15, F) (30, F) (60, F) (15, S) (30, S) (60, S) All

Difficulty (1=very difficult, 5=very easy)

2.5

3.1

2.8

3.2

3.4

3.6 3.1

Experience (1=very unenjoyable, 5=very enjoyable) 3.0

3.0

2.9

3.2

3.5

3.4 3.2

Mood (1=very bored, 5=very engaged)

3.3

3.3

3.2

3.6

3.3

3.2 3.3

Concentration (1=very hard, 5=very easy)

2.9

3.0

3.1

3.4

3.5

3.7 3.3

Confidence (1=very uncertain, 5=very confident) 3.2

3.2

3.4

3.3

3.6

3.8 3.4

Time Pressure (1=very stressed, 5=very relaxed)

2.1

2.4

2.8

2.3

2.9

3.0 2.6

Accuracy (1=very inaccurate, 5=very accurate)

3.2

3.4

3.4 3.4

3.7 3.9 3.5

Table 4: Post task questionnaire. Participants rated their feelings and self-perceived performance. A value of 3 is "neutral".

Time Limit (seconds)

Doc. Type 15 30 60 Mean p-value

Full doc. 1.07 1.18 1.31 Summary 1.27 1.22 1.36

1.19 1.28

p = 0.13

Mean 1.17 1.20 1.34 1.23

p-value

p = 0.047

Table 5: Average ability to discriminate (d ).

Doc. Type Full doc. Summary

Time Limit (seconds) 15 30 60 0.64 0.64 0.64 0.70 0.66 0.67

Mean 0.64 0.68

p-value p = 0.03

Mean 0.67 0.65 0.65 0.66

p-value

p = 0.63

Table 6: Estimated true positive rates.

Time Limit (seconds) Doc. Type 15 30 60 Mean p-value

Full doc. 0.28 0.26 0.22 Summary 0.28 0.27 0.23

0.25 0.26

p = 0.29

Mean 0.28 0.26 0.23 0.26

p-value

p < 0.001

Table 7: Estimated false positive rates.

to be easier, more enjoyable, and less stressful. At the same time, participants were more confident and believed they were more accurate in their judging of summaries under the 60 second limit.
5 CONCLUSION
We conducted a user study and investigated the impact of time limits and document size on relevance assessing. We found no difference in the quality of judgments with summaries in comparison to full documents. As noted in Section 3.4, our summaries represent an upper bound on performance as we selected the paragraph most likely to be relevant given a classifier trained using known relevance judgments. Even so, these results are in line with past research that has shown little difference in relevance judging accuracy between summaries and full documents [7].

Imposing a time limit can speed judging of both full documents and summaries without a loss of accuracy, which shows how little material assessors need to consume from typical newswire documents for making judgments. An aggressive time limit of 15 seconds produced judgments at a rate of 9.5 seconds per document, but such time limits increased the stress on the participants and reduced their enjoyment of the task.
Summaries appear to be a better solution to speeding relevance judging than time limits. With a generous time limit of 60 seconds, our assessors averaged 13.4 seconds per document and this treatment resulted in the best experience as measured by our post-task questionnaire.
ACKNOWLEDGMENTS
Thanks to Nimesh Ghelani and Adam Roegiest for their technical contributions. Thanks to Gordon Cormack and Maura Grossman for their feedback. This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (Grants CRDPJ 468812-14 and RGPIN-2014-03642).
REFERENCES
[1] Allan, J., E. Kanoulas, D. Li, C. V. Gysel, D. Harman, and E. Voorhees (2017). Trec 2017 common core track overview. In TREC.
[2] Bates, D., M. Mächler, B. Bolker, and S. Walker (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67(1), 1­48.
[3] Cormack, G. V., C. L. Clarke, and S. Buettcher (2009). Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In SIGIR, pp. 758­759.
[4] Heitz, R. P. (2014). The speed-accuracy tradeoff: history, physiology, methodology, and behavior. Frontiers in Neuroscience 8, 150.
[5] Macmillan, N. and C. Creelman (2005). Detection theory: a user's guide. [6] Maddalena, E., M. Basaldella, D. De Nart, D. Degl'Innocenti, S. Mizzaro, and G. De-
martini (2016). Crowdsourcing relevance assessments: The unexpected benefits of limiting the time to judge. In Fourth AAAI Conf. on Human Comp. and Crowdsourcing. [7] Mani, I., G. Klein, D. House, L. Hirschman, T. Firmin, and B. Sundheim (2002). Summac: a text summarization evaluation. Natural Language Engineering 8(1), 43­68. [8] Oard, D. W. and W. Webber (2013). Information retrieval for e-discovery. Foundations and Trends in Information Retrieval 7(2-3), 99­237. [9] Sanderson, M. (2010). Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4(4), 247­375. [10] Smucker, M. D. and C. P. Jethani (2011). The crowd vs. the lab: A comparison of crowd-sourced and university laboratory participant behavior. In Proceedings of the SIGIR 2011 Workshop on crowdsourcing for information retrieval. [11] Wang, J. (2011). Accuracy, agreement, speed, and perceived difficulty of users' relevance judgments for e-discovery. In Proceedings of SIGIR Information Retrieval for E-Discovery Workshop, Volume 1. [12] Wang, J. and D. Soergel (2010). A user study of relevance judgments for ediscovery. Proc. of the Assoc. for Information Sci. and Tech. 47(1), 1­10. [13] Zhang, H. (2019). Increasing the Efficiency of High-Recall Information Retrieval. PhD thesis, University of Waterloo. [14] Zhang, H., M. Abualsaud, N. Ghelani, A. Ghosh, M. D. Smucker, G. V. Cormack, and M. R. Grossman (2017). UWaterlooMDS at the TREC 2017 common core track. In TREC.

904

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Learning More From Less
Towards Strengthening Weak Supervision for Ad-Hoc Retrieval

Dany Haddad
danyhaddad@utexas.edu The University of Texas at Austin
ABSTRACT
The limited availability of ground truth relevance labels has been a major impediment to the application of supervised methods to ad-hoc retrieval. As a result, unsupervised scoring methods, such as BM25, remain strong competitors to deep learning techniques which have brought on dramatic improvements in other domains, such as computer vision and natural language processing. Recent works have shown that it is possible to take advantage of the performance of these unsupervised methods to generate training data for learning-to-rank models. The key limitation to this line of work is the size of the training set required to surpass the performance of the original unsupervised method, which can be as large as 1013 training examples. Building on these insights, we propose two methods to reduce the amount of training data required. The first method takes inspiration from crowdsourcing, and leverages multiple unsupervised rankers to generate soft, or noise-aware, training labels. The second identifies harmful, or mislabeled, training examples and removes them from the training set. We show that our methods allow us to surpass the performance of the unsupervised baseline with far fewer training examples than previous works.
CCS CONCEPTS
· Information systems  Retrieval models and ranking.
KEYWORDS
Information retrieval, Noisy Labels, Weak Supervision, Neural Network, Deep Learning
ACM Reference Format: Dany Haddad and Joydeep Ghosh. 2019. Learning More From Less: Towards Strengthening Weak Supervision for Ad-Hoc Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331272
1 INTRODUCTION
Classical ad-hoc retrieval methods have relied primarily on unsupervised signals such as BM25, TF-IDF, and PageRank as inputs
Work done while interning at CognitiveScale.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331272

Joydeep Ghosh
ghosh@ece.utexas.edu The University of Texas at Austin
to learning-to-rank (LeToR) models. Supervision for these models is often supplied in the form of click-stream logs or hand-curated rankings, both of which come with their issues and limitations. First, both sources are typically limited in availability and are often proprietary company resources. Second, click-stream data is typically biased towards the first few elements in the ranking presented to the user [2] and are noisy in general. Finally, such logs are only available after the fact, leading to a cold start problem. These issues motivate the search for an alternate source of "ground truth" ranked lists to train our LeToR model on.
In [7], Dehghani et al. show that the output of an unsupervised document retrieval method can be used to train a supervised ranking model that outperforms the original unsupervised ranker. More recently, [13] proposed a hierarchical interaction based model that is trained on a similarly generated training set. These works have shown the potential of leveraging unsupervised methods as sources of weak supervision for the retrieval task. However, they require training on as many as 1013 training examples to surpass the performance of the unsupervised baseline [7, 13].
In this work, we substantially reduce this number by making more effective use of the generated training data. We present two methods that make improvements in this direction, and beat the unsupervised method using fewer than 10% of the training rankings compared to previous techniques.
In the first method, we take a crowdsourcing approach and collect the output of multiple unsupervised retrieval models. Following [19], we learn a joint distribution over the outputs of said retrieval models and generate a new training set of soft labels. We call this the noise-aware model. The noise-aware model does not require access to any gold labels1.
Our second method builds on the idea of dataset debugging and identifies training examples with the most harmful influence [10] (the labels most likely to be incorrect) and drops them from the training set. We call this the influence-aware model.
2 RELATED WORK
Much of the prior work in handling noisy datasets has been in the context of a classifier from noisy labels. In the binary classification context, noise is seen as a class-conditional probability that an observed label is the opposite of the true label [8, 14]. In the ranking context, we typically expect that models trained using pairwise or listwise loss functions will far outperform pointwise approaches [11]. Since the label of a pair is determined by the ordering of the documents within the pair, it is not immediately obvious how the class-conditional flip probabilities translate to this formulation. The relationship to listwise objectives is not straightforward either.
1To differentiate them from labels originating from weak supervision sources, we refer to relevance scores assigned by a human as "gold" labels

857

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

In [5] and [6], Dehghani et al. introduce two semi-supervised student-teacher models where the teacher weights the contribution of each sample in the student model's training loss based on its confidence in the quality of the label. They train the teacher on a small subset of gold labels and use the model's output as confidence weights for the student model. [5] shows that using this approach, they can beat the unsupervised ranker using ~75% of the data required when training directly on the noisy data. They train a cluster of 50 gaussian processes to form the teacher annotations which are used to generate soft labels to fine-tune the student model.
In [19], Ratner et al. transform a set of weak supervision sources, that may disagree with each other, into soft labels used to train a discriminative model. They show experimentally that this approach outperforms the naïve majority voting strategy for generating the target labels. This inspires our noise-aware approach.
In [10], Koh et al. apply classical results from regression analysis to approximate the change in loss at a test point caused by removing a specific point from the training set. They show experimentally that their method approximates this change in loss well, even for highly non-linear models, such as GoogLeNet. They also apply their method to prioritize training examples to check for labeling errors. Our influence-aware approach uses influence functions [10] to identify mislabeled training examples.

3 PROPOSED METHODS

3.1 Model Architecture

In this work, we only explore pairwise loss functions since they typi-

cally lead to better performing models than the pointwise approach.

Listwise approaches, although typically the most effective, tend to

have high training and inference time computational complexity

due to their inherently permutation based formulations [11].

We consider a slight variant of the Rank model proposed in [7] as our baseline model. We represent the tokens in the ith query as tiq and the tokens in the ith document as tid . We embed these tokens in a low dimensional space with a mapping E : V  Rl

where V is the vocabulary and l is the embedding dimension. We

also learn token dependent weights W : V  R. Our final repre-

sentation for a query q is a weighted sum of the word embeddings:

vq =

t

q j

t

q

W~ q

(t

q j

)E(t

q j

)

where W~ q

indicates

that

the

weights

are

normalized to sum to 1 across tokens in the query q using a soft-

max operation. The vector representation for documents is defined

similarly.

In addition, we take the difference and elementwise products

of the document and query vectors and concatenate them into a
single vector vq,d = [vq , vd , vq - vd , vq  vd ]. We compute the relevance score of a document, d, to a query, q by passing vq,d through a feed-forward network with ReLU activations and scalar output. We use a tanh at the output of the rank model and use the

raw logit scores otherwise. We represent the output of our model
parameterized by  as f (x;  ). Our training set Z is a set of tuples z = (q, d1, d2, sq,d1 , sq,d2 )
where sq,di is the relevance score of di to q given by the unsupervised ranker. The pairwise objective function we minimize is given

by:

L(Z;  ) = L(f (vq,d1 ;  ) - f (vq,d2 ;  ), relq,(d1,d2)) (1)
z Z

Lce (x, y) = y · log( (x)) + (1 - y) · log(1 -  (x))

(2)

Lhine (x, y) = max{0,  - sign(y) · x }

(3)

Where relq,(d1,d2)  [0, 1] gives the relative relevance of d1 and d2 to q. L is either Lce or Lhine for cross-entropy or hinge loss, respectively. The key difference between the rank and noise-aware
models is how relq,(d1,d2) is determined. As in [7], we train the rank model by minimizing the max-margin loss and compute relq,(d1,d2) as sign(sq,d1 - sq,d2 ).
Despite the results in [21] showing that the max-margin loss

exhibits stronger empirical risk guarantees for ranking tasks using

noisy training data, we minimize the cross-entropy loss in each of

our proposed models for the following reasons: in the case of the noise-aware model, each of our soft training labels are a distribution

over {0, 1}, so we seek to learn a calibrated model rather than

one which maximizes the margin (as would be achieved using a hinge loss objective). For the influence-aware model, we minimize

the cross-entropy rather than the hinge loss since the method of

influence functions relies on having a twice differentiable objective.

3.2 Noise-aware model
In this approach, relq,(di,dj )  [0, 1] are soft relevance labels. For each of the queries in the training set, we rank the top documents by relevance using k unsupervised rankers. Considering ordered pairs of these documents, each ranker gives a value of 1 if it agrees with the order, -1 if it disagrees and 0 if neither document appears
in the top 10 positions of the ranking. We collect these values into a matrix   {-1, 0, 1}m×k for m document pairs. The joint distribution over these pairwise preferences and the true pairwise orderings y is given by:

m

1 Pw (, y) = Z (w) exp( i

wT (i , yi ))

(4)

Where w is a vector of learned parameters and Z (w) is the par-

tition function. A natural choice for  is to model the accuracy

of each individual ranker in addition to the pairwise correlations between each of the rankers. So for the ith document pair, we have the following expression for i (i , yi ):

i = [{i j = yi }1j k ||{i j = il 0}j l ]

Since the true relevance preferences are unknown, we treat them

as latent. We learn the parameters for this model without any gold relevance labels y by maximizing the marginal likelihood (as in

[19]) given by:

max log
w

y

Pw (, y)

(5)

We use the Snorkel library2 to optimize equation 5 by stochastic

gradient descent, where we perform Gibbs sampling to estimate

the gradient at each step. Once we have determined the parameters of the model, we can evaluate the posterior probabilities Pw (yi |i ) which we use as our soft training labels.

2
https://github.com/HazyResearch/snorkel

858

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

3.3 Influence Aware Model
In this approach, we identify training examples that hurt the generalization performance of the trained model. We expect that many of these will be incorrectly labeled, and that our model will perform better if we drop them from the training set. The influence of removing a training example zi = (xi , yi ) on the trained model's loss at a test point ztest is computed as [10]:

Table 1: Results comparison with smoothing.

Noise- Influence-

Rank Model

QL

Aware Aware

NDCG@10 Prec@10 MAP

0.3881  0.3952 0.3535  0.3621 0.2675  0.2774

0.4008 0.3843 0.3657 0.3515 0.2792 0.2676

L(ztext ;  )  Idr op (zi , ztest )

(6)

=

1 n



L(zt

es

t

;



)T

H-1 

L(zi

;



)

(7)

where H is the Hessian of the objective function. If Idrop (zi , ztest ) is negative, then zi is a harmful training example for ztest since it's inclusion in the training set causes an increase in the loss at that point. Summing this value over the entire test set gives us Idrop (zi ). We compute Idrop (zi ) for each training example zi , expecting it to represent zi 's impact on the model's performance at test time. In our setup, we know that some of our training examples are
mislabeled; we expect that these points will have a large negative value for Idrop . Of course, for a fair evaluation, the ztest points are taken from the development set used for hyperparameter tuning
(see section 4).
We address the computational constraints of computing (7) by
treating our trained model as a logistic regression on the bottle-
neck features. We freeze all model parameters except the last layer
of the feed-forward network and compute the gradient with re-
spect to these parameters only. This gradients can be computed in
closed form in an easily parallelizable way, allowing us to avoid
techniques that rely on autodifferentiation operations [16]. We compute H-1 L(ztest ;  ) for every ztest using the method of conjugate gradients following [20]. We also add a small damping term
to the diagonal of the Hessian to ensure that it is positive definite
[12].

4 DATA PREPROCESSING AND MODEL TRAINING
We evaluate the application of our methods to ad-hoc retrieval on the Robust04 corpus with the associated test queries and relevance labels. As in [7], our training data comes from the AOL query logs [15] on which we perform similar preprocessing. We use the Indri3 search engine to conduct indexing and retrieval and use the default parameters for the query likelihood (QL) retrieval model [18] which we use as the weak supervision source. We fetch only the top 10 documents from each ranking in comparison to previous works which trained on as many as the top 1000 documents for each query. To compensate for this difference, we randomly sample nne additional documents from the rest of the corpus for each of these 10 documents. We train our model on a random subset of 100k rankings generated by this process. This is fewer than 10% the number of rankings used in previous works [7, 13], each of which contains far fewer document pairs.

3
https://www.lemurproject.org/indri.php

For the word embedding representations,W , we use the 840B.300d GloVe [17] pretrained word embedding set4. The feed-forward net-
work hidden layer sizes are chosen from {512, 256, 128, 64} with up
to 5 layers. We use the first 50 queries in the Robust04 dataset as
our development set for hyperparameter selection, computation of Idrop and early stopping. The remaining 200 queries are used for evaluation.
During inference, we rank documents by the output of the feed-
forward network. Since it is not feasible to rank all the documents
in the corpus, we fetch the top 100 documents using the QL retrieval
model and then rerank using the trained model's scores.
4.1 Model Specific Details
For the noise-aware model, we generate separate rankings for each query using the following retrieval methods: Okapi BM25, TF-IDF,
QL, QL+RM3 [1] using Indri with the default parameters. For the influence-aware model, we train the model once on the
full dataset and then compute Idrop (zi ) for each training point dropping all training examples with a negative value for Idrop (zi ) which we find to typically be around half of the original training
set. We then retrain the model on this subset. Interestingly, we find that using a smaller margin, , in the train-
ing loss of the rank model leads to improved performance. Using a smaller margin incurs 0 loss for a smaller difference in the model's
relative preference between the two documents. Intuitively, this
allows for less overfitting to the noisy data. We use a margin of 0.1
chosen by cross-validation. The noise-aware and influence-aware models train end-to-end in
around 12 and 15 hours respectively on a single NVIDIA Titan Xp.
5 EXPERIMENTAL RESULTS
We compare our two methods against two baselines, the unsupervised ranker (QL) and the rank model. Compared to the other unsupervised rankers (see section 4.1) used as input to the noise-aware model, the QL ranker performs the best on all metrics. Training the rank model on the results of the majority vote of the set of unsupervised rankers used for the noise-aware model performed very similarly to the rank model, so we only report results of the rank model. We also compare the results after smoothing with the normalized QL document scores by linear interpolation.
The results in tables 1 and 2 show that the noise-aware and influence-aware models perform similarly, with both outperforming the unsupervised baseline. Bold items are the largest in their row
and daggers indicate statistically significant improvements over the rank model at a level of 0.05 using Bonferroni correction. Figure 1 shows that the rank model quickly starts to overfit. This does
4
https://nlp.stanford.edu/projects/glove/

859

Short Research Papers 1A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Table 2: Results comparison without smoothing.

Rank Model Noise-Aware Influence-Aware

NDCG@10 Prec@10 MAP

0.2610 0.2399 0.1566

 0.2886 0.2773  0.1831

0.2966  0.2742 0.1839

Figure 1: Test NDCG@10 during training
not contradict the results in [7] since in our setup we train on far fewer pairs of documents for each query, so each relevance label error has much greater impact. For each query, our distribution over documents is uniform outside the results from the weak supervision source, so we expect to perform worse than if we had a more faithful relevance distribution. Our proposed approaches use an improved estimate of the relevance distribution at the most important positions in the ranking, allowing them to perform well.
We now present two representative training examples showing how our methods overcome the limitations of the rank model.
Example 5.1. The method in section 3.2 used to create labels for the noise-aware model gives the following training example an unconfident label (~0.5) rather than a relevance label of 1 or 0: (q="town of davie post office", (d1=FBIS3-25584, d2=FT933-13328)) where d1 is ranked above d2. Both of these documents are about people named "Davie" rather than about a town or a post office, so it is reasonable to avoid specifying a hard label indicating which one is explicitly more relevant.
Example 5.2. One of the most harmful training points as determined by the method described in section 3.3 is the pair (q="pictures of easter mice", (d1=FT932-15650, d2=LA041590-0059)) where d1 is ranked above d2. d1 discusses the computer input device and d2 is about pictures that are reminiscent of the holiday. The incorrect relevance label explains why the method identifies this as a harmful training example.
6 CONCLUSIONS AND FUTURE WORK
We have presented two approaches to reduce the amount of weak data needed to surpass the performance of the unsupervised method that generates the training data. The noise-aware model does not require ground truth labels, but has an additional data dependency on multiple unsupervised rankers. The influence-aware model requires a small set of gold-labels in addition to a re-train of the model,

although empirically, only around half the dataset is used when
training the second time around.
Interesting paths for future work involve learning a better joint distribution for training the noise-aware model or leveraging ideas from [22] to construct soft training labels rather than for the query
performance prediction task. Similarly, we could apply ideas from
unsupervised LeToR [4] to form better noise-aware labels. For the influence-aware model, we could use the softrank loss [3] rather than cross-entropy and instead compute set influence rather than
the influence of a single training example [9].
REFERENCES
[1] Nasreen Abdul-Jaleel, James Allan, Bruce Croft, Fernando Diaz, Leah Larkey,
Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. 2004. Umass at trec 2004: Notebook. academia.edu (2004). [2] Qingyao Ai, Keping Bi, Cheng Luo, Jiafeng Guo, and W Bruce Croft. 2018. Unbiased Learning to Rank with Unbiased Propensity Estimation. In The 41st International ACM SIGIR Conference. ACM Press, New York, New York, USA, 385­394. https://doi.org/10.1145/3209978.3209986
[3] Ricardo Baeza-Yates, Berthier de Araújo Neto Ribeiro, et al. 2007. Learning to rank with nonsmooth cost functions. NIPS (2007).
[4] Avradeep Bhowmik and Joydeep Ghosh. 2017. LETOR Methods for Unsupervised Rank Aggregation. In the 26th International Conference. ACM Press, New York, New York, USA, 1331­1340. https://doi.org/10.1145/3038912.3052689
[5] Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, and Bernhard Schölkopf. 2017. Fidelity-Weighted Learning. arXiv.org (Nov. 2017). arXiv:cs.LG/1711.02799v2
[6] Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, and Jaap Kamps. 2017. Learning to Learn from Weak Supervision by Full Supervision. arXiv.org (Nov. 2017), 1­8. arXiv:1711.11383
[7] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In the 40th International ACM SIGIR Conference. ACM Press, New York, New York, USA, 65­74. https://doi.org/10.1145/3077136.3080832
[8] Xinxin Jiang, Shirui Pan, Guodong Long, Fei Xiong, Jing Jiang, and Chengqi Zhang. 2017. Cost-sensitive learning with noisy labels. JMLR (2017).
[9] Rajiv Khanna, Been Kim, Joydeep Ghosh, and Oluwasanmi Koyejo. 2018. Interpreting Black Box Predictions using Fisher Kernels. arXiv.org (Oct. 2018). arXiv:cs.LG/1810.10118v1
[10] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. arXiv.org (March 2017), 1­11. arXiv:1703.04730
[11] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331. https://doi.org/10.1561/ 1500000016
[12] James Martens. 2010. Deep learning via Hessian-free optimization. (2010).
[13] Yifan Nie, Alessandro Sordoni, and Jian-Yun Nie. 2018. Multi-level Abstraction Convolutional Model with Weak Supervision for Information Retrieval. In The 41st International ACM SIGIR Conference. ACM Press, New York, New York, USA, 985­988. https://doi.org/10.1145/3209978.3210123
[14] Curtis G Northcutt, Tailin Wu, and Isaac L Chuang. 2017. Learning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels. arXiv.org (May 2017). arXiv:1705.01936
[15] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. Infoscale (2006), 1­es. https://doi.org/10.1145/1146847.1146848
[16] Barak Pearlmutter. 1994. Fast exact multiplication by the Hessian. MIT Press 6, 1 (Jan. 1994), 147­160. https://doi.org/10.1162/neco.1994.6.1.147
[17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2015. GloVe:
Global Vectors for Word Representation.
[18] Jay M Ponte and W Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. SIGIR (1998), 275­281. https://doi.org/10.1145/290941. 291008
[19] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. 2017. Snorkel. Proceedings of the VLDB Endowment 11, 3 (Nov. 2017), 269­282. https://doi.org/10.14778/3157794.3157797
[20] Jonathan R Shewchuk. 1994. An introduction to the conjugate gradient method
without the agonizing pain. (1994). [21] Hamed Zamani and W Bruce Croft. 2018. On the Theory of Weak Supervision for
Information Retrieval. ACM, New York, New York, USA. https://doi.org/10.1145/ 3234944.3234968
[22] Hamed Zamani, W Bruce Croft, and J Shane Culpepper. 2018. Neural Query Performance Prediction using Weak Supervision from Multiple Signals. In The 41st International ACM SIGIR Conference. ACM Press, New York, New York, USA, 105­114. https://doi.org/10.1145/3209978.3210041

860

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

On Topic Diiculty in IR Evaluation: The Eect of Systems, Corpora, and System Components

Fabio Zampieri
University of Udine Udine, Italy
zampieri.fabio@spes. uniud.it

Kevin Roitero
University of Udine Udine, Italy
roitero.kevin@spes. uniud.it

J. Shane Culpepper Oren Kurland

RMIT University

Technion

Melbourne, Australia

Haifa, Israel

shane.culpepper@ kurland@ie.technion.

rmit.edu.au

ac.il

Stefano Mizzaro
University of Udine Udine, Italy
mizzaro@uniud.it

ABSTRACT
In a test collection setting, topic diculty can be dened as the average eectiveness of a set of systems for a topic. In this paper we study the eects on the topic diculty of: (i) the set of retrieval systems; (ii) the underlying document corpus; and (iii) the system components. By generalizing methods recently proposed to study system component factor analysis, we perform a comprehensive analysis on topic diculty and the relative eects of systems, corpora, and component interactions. Our ndings show that corpora have the most signicant eect on topic diculty.
ACM Reference Format: Fabio Zampieri, Kevin Roitero, J. Shane Culpepper, Oren Kurland, and Stefano Mizzaro. 2019. On Topic Diculty in IR Evaluation: The Eect of Systems, Corpora, and System Components. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19). ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3331184.3331279
1 INTRODUCTION
Topic diculty, dened as the average eectiveness of a set of systems on a topic [9, 10], is a well-studied problem in the IR literature. It is loosely related to the problem of Query Performance Prediction (QPP), which aims to estimate the eectiveness of a system for a given query when no relevance judgments are available [2]. In classical QPP, however, the aim is to predict the performance of a specic system for a specic query; in this paper we study topic diculty for a set of systems. This is a dierent problem that can be justied by the aim of understanding the "general" diculty of a topic [7­10]. It also leads naturally to the research issue of nding representative sets of systems, i.e., sets for which diculty would generalize to other sets. Our overall goal is to understand the eect of three factors (the set of systems, the document corpus, and the system components) on topic diculty. To the best of our knowledge, this problem has only been investigated from a system eectiveness perspective. We achieve this goal by extending factor analysis methods recently proposed to study the eect of system components on eectiveness of systems [4­6]. We address four research questions:

RQ1. Given a collection, what is the eect of choosing a dierent set of systems on the diculty of topics?
RQ2. Given a set of systems, what is the eect of the corpus of documents (or sub-corpora of a corpus) on topic diculty?
RQ3. What is the eect of system components on topic diculty? RQ4. What is the relative eect of choosing dierent systems, cor-
pora, and system components on topic diculty?
2 RELATED WORK
A body of related work focuses on studying factors that aect system eectiveness, such as topic composition, collection, and system components. Sanderson et al. [11] investigated the eect of splitting a TREC collection into sub-collections based on system eectiveness, and identied several interesting sub-collection effects induced by the splits. Banks et al. [1] provided an overview of methods that can be applied to analyze the performance of IR systems on TREC collections and its relation to topics, collections and other factors. One common statistical tool used for this problem is the Analysis of Variance (ANOVA), which was recently used by Ferro and Silvello [5] to compare combinations of collections, metrics, and systems. They showed that stop lists, IR models, and component interactions have a signicant but small eect on overall system eectiveness. The same approach was adopted by Ferro and Sanderson [4] and Ferro et al. [3], whose experiments show the existence of a signicant sub-corpus eect relative to system eectiveness; however, the eect is smaller than both system and topic eects, with topic eect being the most signicant. Similar experiments using the sub-corpora of a single collection showed that the system eect is smaller than the topic eect [4]. However, none of these studies specically addresses the eect of factors on topic diculty which we study here. Moreover, all of them compare sub-corpora of the same collection, which has some drawbacks. TREC corpora are built with a "working assumption" that they are somehow complete, and working on sub-corpora can sometimes negate this assumption. In this paper, we do not only analyze what happens on incomplete sub-corpora, but we are also able to compare across dierent corpora.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331279

3 EXPERIMENTS
3.1 Experimental Setting
Datasets. Table 1 summarizes the datasets used for our experiments. We focus on ve TREC (Text REtrieval Conference) collections. Our datasets are purposefully chosen to include overlapping sets of topics, systems, and corpora. The set of R04 topics includes TREC6 topics (301-350), TREC7 topics (351-400), TREC8 topics (401450), half of the Robust03 topics (601-650), and 50 additional topics

909

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Datasets used in our experiments.

Acronym
TREC6 TREC7 TREC8 R04 C17

Track
Ad Hoc Ad Hoc Ad Hoc Robust Common Core

Year
1997 1998 1999 2004 2017

Topics
50 50 50 249 50

Ocial
74 103 129 110 75

Unocial
158 158 158 158 158

Table 2: The number of common topics between collections.

R04 C17 TREC6 TREC7 TREC8

C17

50 50

TREC6 50 11

50

TREC7 50 17

0

50

TREC8 50 16

0

0

50

Table 3: Corpora of documents used in the datasets.

Acronym Corpus name

TREC6-8 R04 C17

FT

The Financial Times

x

x

FR

Federal Register

x

x

CR

Congressional Record

x

FBIS

FBI Service

x

x

NYT

The New York Times

x

that were specically introduced in R04. C17 has 50 topics, which were also originally included in the R04 set of topics; C17 has a few topics that overlap with TREC6-8 (see Table 2). Table 3 shows the document corpora used in each collection: R04 and TREC6-8 share, apart from C17, the same corpora; C17 is based only on NYT.
For each of the TREC collections we use the ocially-submitted runs. We also supplement available runs using several open source search engines in order to produce system congurations that are directly comparable across collections: Terrier, Atire, and Indri (www.terrier.org, www.atire.org, www.lemurproject.org). The 158 system variants are generated by systematically alternating and combining the ranker, stemmer, and stopword congurations, but xing congurations to be identical across all test collections. Henceforth we distinguish between ocial systems/runs (O) from TREC, and unocial system congurations (U) generated by us. Both O and U systems produce ranked lists of 1000 documents. Metrics. We use Average Precision (AP) as an eectiveness measure. Given a system si and a topic tj , we denote the corresponding score which is a real number between 0 and 1 as AP(si , tj ). By averaging athme eAaPsuvraeluoefstoopviecrdeiachcutlotypi[c9, ,w1e0]o: bAtAaiPn(ttjh)e=Avm1erÕagmie=1AAPP((AsiA, tPj )),. A high AAP value indicates that the topic is easy, and a low AAP indicates that the topic is dicult for a specic collection and set of system runs. We use Kendall's as the primary correlation coecient in this work, as it is well-suited to compute partial correlations in fully-ranked data [1].
3.2 Results
RQ1: System Eects. We rst illustrate and discuss how topic diculty changes when we select a dierent set of systems. In Figure 1, scatter plots of AAP values for R04 and C17 topics are shown; the other collections, not shown due to space limits, exhibit similar trends. Columns correspond to subsets of systems, each containing 30 elements (with the exception of the rst column, which represents the set of all systems), while rows correspond

aOO systems

best systems woUst systems Uandom systems

0.8  0.8

0.8  0.63

0.8  0.81

0.8  0.84

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

8

8

0.8  0.7

0.8  0.6

0.8  0.72

0.8  0.65

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00

0.25 0.50
2

0.75

0.00.00

0.25 0.50
2

0.75

0.00.00

0.25 0.50
2

0.75

0.00.00

0.25 0.50
2

0.75

Figure 1: Scatterplots of AAP values for C17 (rst row) and R04 (second row), computed over dierent sets of systems (y-axis: U = Unocial; x-axis: O = Ocial).

2

8

aOO systems
0.8  0.48
0.6 0.4

best systems woUst systems Uandom systems

0.8  0.51

0.8  0.43

0.8  0.46

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.8  0.38
0.6 0.4

0.8  0.36
0.6 0.4

0.8  0.4
0.6 0.4

0.8  0.37
0.6 0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.8  0.84
0.6

0.8  0.76
0.6

0.8  0.76
0.6

0.8  0.75
0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.8  0.95
0.6

0.8  0.87
0.6

0.8  0.92
0.6

0.8  0.94
0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

2

8

Figure 2: Scatterplots of AAP values computed over R04 vs. C17 (rst two rows), and R04 vs. TREC6 (3rd and 4th rows), using either the ocial (O) runs (1st and 3rd row) or the unocial (U) ones.

to collections. For each plot, a point is dened by the AAP value computed over the set of ocial systems (on the x axis) and the AAP value computed over the set of unocial systems (on the y axis). High correlations are observed in almost every case. Selecting a particular group of systems does not seem to aect the correlation, even though a signicant overall drop can be seen when values are computed using only the best systems (i.e., the 30 best ocial and the 30 best unocial). Therefore, for a given corpus, topic diculty seems quite stable and does not appear to change much across dierent sets of systems, although they heavily dier in terms of implementation and components. The correlation values drop, however, when relying only on the most eective systems.

910

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

all systems
0.8  0.27

best systems woUst systems Uandom systems

0.8  0.3

0.8  0.23

0.8  0.26

0.6

0.6

0.6

0.6

8

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.8  0.39

0.8  0.4

0.8  0.38

0.8  0.39

0.6

0.6

0.6

0.6

8

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

0.00.00 0.25 0.50 0.75

Figure 3: Scatterplots of AAP values computed over R04 subcollections: FT vs FR (1st row) and FT vs. FBIS (2nd row).

RQ2: Corpora Eects. We now turn to the eect of document corpora on topic diculty. In Figure 2, we see that the correlation between AAP values of R04 and C17 is 0.48 for ocial systems (1st row, 1st column), and 0.38 for unocial ones (2nd row, 1st column). It is somewhat higher for ocial systems, although they dier across collections whereas the unocial congurations are identical. Similar results are observed when selecting a particular subset of systems (columns 2-4). In contrast, the correlations between R04 and TREC6 are very high: 0.84 when computed over ocial systems (3rd row, 1st column), and 0.95 when computed over unocial systems (4th row, 1st column). Also in this case, selecting a subset of systems does not seem to aect correlations. We obtained the same results for TREC7-8 (not shown here).
As R04 and C17 include dierent document corpora (see Table 3), these results suggest that topic diculty is indeed quite sensitive to the document corpus. When comparing these results to previous work [3, 4], we observe two dierences: only sub-corpora were used, not dierent corpora as we do here, and system eectiveness was studied, not topic diculty as we do here.
Figure 3 provides also evidence to sub-corpora eects over R04. It shows how topic diculty changes across the sub-corpora of R04 (shown in Table 3). Here again, the correlation of AAP values computed over dierent sub-collections is very low: the highest correlation is between AAP values computed over FT and FBIS (2nd row), while other values do not exceed 0.3.
To summarize: (i) we nd very low correlations when changing signicantly the corpus (R04 vs. C17), thereby generalizing the nding about low correlations on dierent sub-corpora also to the case of dierent complete corpora; (ii) in one case (R04 vs. C17), we nd the strange result that computing AAP using the same unocial system set leads to lower correlation than when using the ocial--and dierent--system set; but this is not conrmed on other datasets; nally (iii) if the changes to the corpus are small (R04 vs. TREC6) then correlations are high. RQ3: System Component Eects. We now turn to our third research question, which focuses on the impact of system components on topic diculty; in particular, we consider stemming and query expansion. Since these are quite dramatic changes to the systems, we expect quite signicant changes to AAP values, and probably low correlations. Figure 4 shows, for each topic in the R04 and C17 collections, the dierence of AAP values computed over the baselines (i.e., systems without stemmer and query expansion) and

R04

.rRvetz  = 0.76
0.8 0.6 0.4 0.2 0.0

 = 0.72
0.8

PRrter

0.6

0.4

0.2

0.0

 = 0.76
0.8

 = 0.74
0.8

0.6

0.6

C17

0.4

0.4

0.2

0.2

0.0

0.0

Figure 4: Dierences between AAP values computed over baselines (i.e., systems without stemmer and query expansion) and those computed over systems using stemmers.

 = 0.78 0.8
0.6
0.4

R04

 = 0.77 0.8

C17

0.6

0.4

0.2

0.2

0.0

0.0

Figure 5: Dierences in AAP computed over baselines and over systems using query expansion.

when using two common stemmers (Krovetz and Porter). Due to space limitations, we do not show the results for the all stemmer and collection combinations. For many of the topics, stemming leads to little or no signicant improvement in terms of AAP. In a few cases, however, there are signicant increases and decreases in AAP, which occur for the same topics across dierent stemmers. The highest dierences in AAP was observed for the R04 topics (see the 1st row), which appear to be quite sensitive to the stemmer used.
Figure 5 shows the AAP dierences between the baselines and systems using query expansion for R04 and C17. For R04 (1st column), we see frequent increases in AAP and infrequent decreases. However, for C17 (2nd column) decreases in AAP are negligible (the same is also true for TREC6-8, not shown).
The results show that system components can have variable eects on topic diculty. In particular, we see that, for a xed subset of topics in a given collection, topic diculty can considerably change if we add a stemming or query expansion to the set of

911

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Table 4: ANOVA table for the model described by Eq. 1.

Factor

SS

DF F

p-value 2

corpus

1.5537 2 140.299 < 1e-6

system

48.4639 168 52.0968 < 1e-6

topic

3045.68 248 2217.86 < 1e-6

corpus:topic 1120.13 496 407.84 < 1e-6

corpus:system 6.4594 336 3.4718 < 1e-6

0.0003 0.0103 0.6603 0.2423 0.0009

systems. However, the correlations, shown in Figures 4 and 5, are quite high: somehow unexpectedly, relative topic diculty remains quite stable despite the changes to the systems (stemming or query expansion) are quite signicant. RQ4: Comparing relative eects with ANOVA. In an attempt to provide a more principled and, at the same time, concise analysis, we investigate the eects of systems, corpora, and system components using ANOVA as part of our nal research question. In particular, we dene two ANOVA models (see Equations (1) and (2)), which are described below. Tables 4 and 5 show the outcome of each ANOVA test. For each factor, we report the Sum of Squares (SS), the Degrees of Freedom (DF), the F statistics, the p-value, and the eect-size ( 2) which quanties the proportional variance of each factor [4­6]. The rst model decomposes the eectiveness (measured by AP) into system, topic, and corpus eects:

AP (i, j) = µ + si + tj + cz + czsi + cztj + i j

(1)

where terms identify AP(i, j) of i-th system and j-th topic, grand mean (µ), z-th corpus (cz ), corpus-system (czsi ) and corpus-topic (cztj ) interactions, and model error ( ij ). Table 4 shows the results of the ANOVA analysis for Eq. (1). All eects are statistically signicant. Systems have a small eect (0.0103), while topics have the greatest eect (0.6603). The interaction eect between corpus and topic is also large but, perhaps surprisingly, both the relative eect of the corpus, and the interaction between corpus and system is negligible. The second model focuses on system components:

AP(i, j) = µ + si + tj + moq + stk + qe + cz + czsi + cztj + i j (2)

where terms identify IR model (moq ), stemmer (stk ), query expansion (qe ), corpus-system (czsi ) and corpus-topic (cztj ) interactions. The results of the ANOVA test for Eq. (2) are shown in Table 5. All eects are statistically signicant, and the topic eect is the largest (0.8157); the system eect is signicant but small. Again, somewhat surprisingly, the corpus interactions have a negligible eect on AP scores. All other eects are not signicant. In summary, the ANOVA analyses show that AP scores are aected mostly by topics and systems, with the greatest eects being attributable to the topic eect; furthermore, system components, corpus, and the interaction between corpus and systems have very little eect on AP. Nevertheless, the impact of topics on AP clearly varies based on the corpus.

4 CONCLUSIONS AND FUTURE WORK
This is the rst study that specically addresses topic diculty in a systematic way: we use dierent corpora, not just sub-corpora; we run the same set of systems across dierent datasets; and we rely on datasets featuring common topics. To do so, we exploit the topic overlap between C17 and R04 with previous collections, and we supplement our analysis using a comprehensive set of unocial but reproducible systems.

Table 5: ANOVA table for the model described by Eq. 2.

Factor

SS

DF F

p-value 2

corpus

15.7907 2 1133.24 < 1e-6

topic

2528.42 248 1463.35 < 1e-6

system

52.6792 168 45.007 < 1e-6

ir_model

2.8554 22 18.6294 < 1e-6

qe

2.0049 1 287.777 < 1e-6

stemmer

0.3708 6 8.8723 < 1e-6

corpus:system 5.9907 336 2.5591 < 1e-6

corpus:qe

0.2012 2 14.4394 < 1e-6

0.0050 0.8157 0.0166 0.0008 0.0006 0.0001 0.0011 6.045e-05

We nd that topic diculty is aected by the document corpora
of collections: there is a signicant corpus-eect on topic diculty
in all of the collections tested. Also, there is a signicant system-
eect, although not so large. Finally, we see a smaller eect of
system components on topic diculty, with the exception of a
few limited cases. Although the standard ANOVA analysis shows a
strong variance across topics and system eects that are higher than
the corpus eects, we alsof nd that topic diculty is reasonably
stable across system sets and system components, thus conrming
that it is a reasonable and measurable concept. We found only
two exceptions with low correlations: the comparison across the
dierent corpora of R04 and C17 and the comparison across R04 sub-
corpora (Figures 2 and 3). Although the latter might be due to the
incomplete nature of sub-corpora, the former conrms that topic
diculty is mostly aected by the underlying document collection.
In the future we plan to extend the analysis to more collections,
to ne-tune the parameters of the unocial systems to each dataset,
and to study more system and topic components. Acknowledgements. This work was partially supported by the Israel Science Foundation (grant no. 1136/17), the Australian Research Council's Discovery Projects Scheme (DP170102231), a Google Faculty Award, and an Amazon Research Award.
REFERENCES
[1] David Banks, Paul Over, and Nien-Fan Zhang. 1999. Blind men and elephants: Six approaches to TREC data. Information Retrieval 1, 1 (1999), 7­34.
[2] David Carmel and Elad Yom-Tov. 2010. Estimating the query diculty for information retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services 2, 1 (2010), 1­89.
[3] Nicola Ferro, Yubin Kim, and Mark Sanderson. 2019. Using Collection Shards to Study Retrieval Performance Eect Sizes. ACM TOIS 5, 44 (2019), 59.
[4] Nicola Ferro and Mark Sanderson. 2017. Sub-corpora impact on system eectiveness. In Proceedings of the 40th ACM SIGIR. ACM, 901­904.
[5] Nicola Ferro and Gianmaria Silvello. 2016. A general linear mixed models approach to study system component eects. In 39th ACM SIGIR. 25­34.
[6] Nicola Ferro and Gianmaria Silvello. 2018. Toward an anatomy of IR system component performances. JASIST 69, 2 (2018), 187­200.
[7] Donna Harman and Chris Buckley. 2009. Overview of the reliable information access workshop. Information Retrieval 12, 6 (2009), 615­641.
[8] Stefano Mizzaro, Josiane Mothe, Kevin Roitero, and Md Zia Ullah. 2018. Query Performance Prediction and Eectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin. In The 41st ACM SIGIR (SIGIR '18). 1233­1236.
[9] Stefano Mizzaro and Stephen Robertson. 2007. Hits Hits TREC: Exploring IR Evaluation Results with Network Analysis. In Proceedings 30th SIGIR. 479­486.
[10] Kevin Roitero, Eddy Maddalena, and Stefano Mizzaro. [n. d.]. Do Easy Topics Predict Eectiveness Better Than Dicult Topics?. In ECIR2017. 605­611.
[11] Mark Sanderson, Andrew Turpin, Ying Zhang, and Falk Scholer. 2012. Dierences in eectiveness across sub-collections. In Proc. of the 21st ACM CIKM. 1965­1969.

912

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Query-Task Mapping

Michael Völske Ehsan Fatehifar
Benno Stein
<firstname>.<lastname>@uni-weimar.de Bauhaus-Universität Weimar Weimar, Germany
ABSTRACT
Several recent task-based search studies aim at splitting query logs into sets of queries for the same task or information need. We address the natural next step: mapping a currently submitted query to an appropriate task in an already task-split log. This query-task mapping can, for instance, enhance query suggestions--rendering efficiency of the mapping, besides accuracy, a key objective.
Our main contributions are three large benchmark datasets and preliminary experiments with four query-task mapping approaches: (1) a Trie-based approach, (2) MinHash LSH, (3) word movers distance in a Word2Vec setup, and (4) an inverted index-based approach. The experiments show that the fast and accurate inverted index-based method forms a strong baseline.
ACM Reference Format: Michael Völske, Ehsan Fatehifar, Benno Stein, and Matthias Hagen. 2019. Query-Task Mapping. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331286
1 INTRODUCTION
Users often turn to a search engine to fulfill an underlying task that led to the information need expressed in a query. The field of task-based search aims to understand the tasks behind information needs, in order to develop better support tools. Recent research has focused on observing user behavior during task-based search [15] or on splitting query logs into tasks and subtasks [24]. Given a task-split query log, we focus on the natural next step: map a new query to the most appropriate task. Query-task mapping may be used to derive task-based query embeddings [25] or to identify query suggestions [33]. Since query suggestions have to be derived in milliseconds, efficiency is a crucial factor besides effectiveness. Hence, our study analyzes runtime along with accuracy.
We create three benchmarking datasets:1 one based on search session and mission detection corpora [8, 21], another based on the
1Data available from: https://webis.de/data/webis-qtm-19.html
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331286

Matthias Hagen
matthias.hagen@informatik.uni-halle.de Martin-Luther-Universität Halle-Wittenberg
Halle (Saale), Germany
TREC Sessions and Tasks tracks [4, 14] combined with a corpus of TREC-based search sessions [9], and the third built from wikiHow questions. We enlarge each dataset with query suggestions from Google and Bing to reach several tens of thousands of queries and annotate the task information. In a preliminary study, we test four query-task mapping methods on our new datasets: (1) a Triebased approach, (2) Minhash LSH Forest, (3) word movers distance in a Word2Vec setup, and (4) an Elasticsearch-based BM25 retrieval. In our experiments, the fast and accurate retrieval approach turns out to be a strong query-task mapping baseline.
2 RELATED WORK
Research on matching queries with the same information need has recently shifted focus from single-user oriented session and mission detection [6­8, 10, 12, 13, 15, 18, 21] to the more multi-user oriented problem of splitting search logs into tasks [3, 11, 17, 22­24].
The studies on search sessions aimed to either match a current query to one of the previous queries of the same user submitted either within the same (time-based) physical session, or to some set of queries (search mission) from before [13, 30]. The goal then was to better support a user with their search, either by better understanding the information need based on the directly preceding queries (as in the TREC Sessions tracks [4]), or by helping a user resume a previously abandoned information need [15, 29]. Already then, session and mission detection techniques recognized runtime as important to the online setting [8].
Recently, the focus has shifted away from the notion of individual users' search missions towards one of complex tasks that can re-occur across users. A complex search task is a multi-aspect or multi-step information need comprising subtasks which might recursively be complex; planning a journey is a typical example [1], and studies have aimed to subdivide query logs into clusters of same-task queries [19]. As before, the goal is to support individual users, but this time by leveraging what others have done in similar situations. One idea is to suggest related queries from the identified query-task clusters like in the TREC Task tracks' setting [14].
Grouping the queries of some larger log into tasks and potentially subtasks has been tackled in different ways ranging from Hawkes processes [17], Bayesian Rose trees [24], entity relations [31], to DBpedia categories [32]. However, no large annotated datasets of logs split into tasks are available. And, maybe even more importantly, the problem of quickly mapping a currently submitted query to an appropriate task in a task-split background log has not been really studied in the literature so far. We address both issues by providing three large benchmarking datasets of task-split queries and an empirical study of four approaches for query-task mapping.

969

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

3 BENCHMARKING DATASETS
We provide three new datasets of queries split into tasks with different "characteristics:" (1) based on available search session / mission corpora, (2) based on queries for TREC topics, and (3) based on wikiHow questions. Table 1 gives a high-level overview of the dataset construction and basic statistics.
3.1 Session-based Dataset
Research on session and mission detection has produced three publicly available corpora of queries sampled from the AOL query log [28] annotated with individual users' search sessions [7, 8, 20]. Since the newer corpus of Hagen et al. [8] and Gayo-Avello's corpus [7] are based on the same sample, we use the corpora of Lucchese et al. [20] and of Hagen et al. [8] as our basis.
Lucchese et al. [20] sampled from the 500 time-based sessions with the largest number of queries from the first week of the AOL log. The 1424 queries from 13 users in this corpus are manually annotated with logical session information (i.e., which queries in a time-based session of one user were probably submitted for the same information need). Note that the corpus does not contain all queries from the sampled users. We manually annotated the 771 unique queries from the corpus (clicks in the AOL log are often logged as "another" query) with task information across users, taking into account Lucchese et al.'s session information. Altogether, we identified 223 tasks with 3.5 unique queries on average.
Hagen et al. [8] took all queries from the 215 AOL users in GayoAvello's corpus [7], removed 88 users with fewer than 4 queries, and annotated the remaining 8840 queries from 127 users with peruser logical session and search mission information (1378 missions). We manually annotated the 3750 unique queries from the corpus with task information across users, taking into account existing session and mission information. Altogether, we identified 1298 tasks (some search missions indeed were identical between users) with 2.9 queries on average. We then merged the unique queries from both corpora (4502 queries) and manually checked whether some tasks were similar enough to be merged. This resulted in 1423 tasks for both corpora combined with an average of 3.2 queries.

Table 1: Statistics of the benchmark datasets. Rows with "+" are cumulative, omitting duplicate task-query pairs.

Tasks

Session-based dataset Lucchese et al. [20] + Hagen et al. [8] + Google suggestions + Bing suggestions
TREC-based dataset Webis-TRC-12 [9] + TREC + Google suggestions + Bing suggestions
WikiHow-based dataset WikiHow + Google suggestions + Bing suggestions

223 1,423 1,423 1,423
150 276 276 276
7,202 7,202 7,202

Queries
771 4,502 29,441 41,780
3,848 7,771 38,478 47,514
15,914 119,283 119,292

Queries per Task min avg max
1 3.5 55 1 3.2 147 1 20.7 924 1 29.4 1,368
1 25.7 122 1 28.2 144 8 139.4 858 8 172.2 997
1 2.2 22 1 16.6 197 1 16.6 197

To enlarge the dataset to tens of thousands of queries, we submit each original query to Google and Bing and scrape the query suggestions that we then add to the same task. We discard suggestions that have the original query as a prefix, but do not continue with a new term.2 Manual spot checks showed the task assignment to be reasonable for the remaining suggestions, with a small number of exceptions where the search engines returned suggestions in a different language, which were removed semi-automatically; further spot checks showed the remaining suggestions to be accurate. We gathered 24,939 unique suggestions from Google (queries from the original data were not added again) and 12,339 from Bing (queries already suggested by Google were not taken twice), resulting in a larger corpus of 41,780 queries for 1,423 tasks (30 queries per task).
3.2 TREC-based Dataset
Our TREC-based dataset uses the queries from the TREC Session tracks 2012­2014 [4], from the TREC Tasks tracks 2015 and 2016 [14], and from the Webis-TRC-12 [9]. The Webis-TRC-12 is based on the search logs of writers who wrote essays on the 150 topics used at the TREC Web tracks 2009­2011 while doing their background research using a search engine (13,881 submitted queries, 3848 unique). At the TREC Session tracks, 4666 queries (3248 unique) were collected as user search sessions on 60 different topics. The TREC Tasks tracks 2015 and 2016 each had 50 topics with 547 and 405 unique queries, respectively. We merged the 7771 unique queries from all the above sources and manually checked whether some of the potentially 310 tasks are identical, resulting in 276 tasks (28 queries per task). We again collected 30,707 query suggestions from Google and 9,036 from Bing, resulting in 47,514 unique queries for 276 tasks (172 queries per task).
3.3 WikiHow-based Dataset
Our third dataset is based on crawling 198,163 questions from wikiHow,3 inspired by Yang and Nyberg's idea of extracting steps for completing task-based search intents from the procedural knowledge collected at this platform [33]. However, we do not aim to extract steps, but to identify different questions on the same task.
On wikiHow, each question is linked to other recommended questions, but spot checks showed that only those questions that mutually link to each other can be considered as on the same task such that we restrict the extraction to these cases. This way, we gathered 15,914 questions split into 7202 tasks. As before, we enlarge the dataset by obtaining 103,369 suggestions from Google and 9 additional ones Bing (for these long and specific questions, the suggestions were usually identical) for every question; this results in 119,292 queries for 7202 tasks (17 queries per task).
4 EXPERIMENTAL ANALYSIS
We compare four straightforward query-task mapping methods on our new benchmarking datasets with respect to their accuracy and efficiency, both in terms of preprocessing and online query processing.4 Table 2 summarizes the results.
2For instance, for the original query [how to open a can], we would discard [how to open a canadian bank account] if returned as a suggestion. 3 www.wikihow.com 4Experiment machines had Intel Xeon 2608L-v4 CPUs and 128GB of DDR4 memory

970

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

4.1 Query-Task Mapping Approaches
Our experiments take inspiration from the taxonomy of Metzler et al. [26] to cover a range of different short-text retrieval paradigms: (1) a Trie-based approach (lexical match on a surface representation), (2) MinHash LSH (stemmed representation), (3) word movers distance in a Word2Vec setup (expanded representation), and (4) an inverted index-based approach (probabilistic matching).
Trie-based Approach. The trie data structure, first described by De La Briandais [5], matches strings based on prefixes. We construct a trie for all queries within the task-split dataset during pre-processing, and for query-task mapping assign a new query q to the task associated with the query found as the longest prefix of q. If queries from multiple tasks qualify, we choose the majority vote. We use the implementation from the Google Pygtrie library.5
MinHash LSH. MinHash (the min-wise independent permutations locality sensitive hashing scheme) is a technique for estimating the similarity between sets via representation as a compact signature. During preprocessing, we hash the queries' binary term vectors. To efficiently find the most similar entries for a new query, we employ the implementation from the datasketch library6 which combines MinHash with Bawa et al.'s Locality-Sensitive Hashing (LSH) Forest scheme [2].
Word Movers Distance. Using word embeddings, the word movers distance [16] measures the distance of two strings (i.e., queries in our case) as the minimum distance that the embedded words of one string need to "travel" to reach the embedded words of the other string. We employ the pre-trained Word2Vec embeddings [27] from the publicly available GoogleNews-vectors-negative300 corpus to embed all terms in a query, which is then assigned to the task of its closest WMD neighbor. We use the Fast Word Mover's Distance implementation from the wmd-relax library.7
Index-based Search. As a final approach, we use an inverted indexbased method, whereby we store the queries in the log in an Elasticsearch index, with a field for their task.8 To perform query-task mapping for a new query, we simply submit it to the index, and assign the task of the top result.
4.2 Pre-Processing and Mapping Efficiency
As for the pre-processing efficiency, we just measured the time needed to build the necessary data structures on the full datasets. Building the trie took about 10 seconds for the smaller datasets and 25 seconds for the largest. This is very close to the time needed to look up all query terms in the pre-trained word-embedding model for the WMD method, but quite a bit faster than computing the hashes for MinHash LSH Forest, which takes about one minute for the smaller datasets and three minutes for the largest. Building the inverted indexes takes about 30 seconds for the smaller corpora and about one minute for the largest.
Query-task mapping runtime was averaged over 10,000 test queries left out from the pre-processing. Mapping a query to its
5 https://github.com/google/pygtrie 6 https://ekzhu.github.io/datasketch/lsh.html 7 https://github.com/src- d/wmd- relax 8https://www.elastic.co/, version 5.6.14, retrieval model: BM25

Table 2: Summary of our experimental results. Accuracy values shown with 95% confidence intervals.

Dataset

Trie LSH

Preprocessing time (entire dataset)

Session-based

10.03s 53.79s

TREC-based

13.26s 62.09s

Wikihow-based 28.00s 141.65s

Query-task mapping time (per query)

Session-based 0.46ms 2.42ms

TREC-based

0.51ms 2.50ms

Wikihow-based 0.33ms 2.28ms

Query-task mapping accuracy

Session-based 0.69±0.02 0.66±0.02

TREC-based

0.66±0.03 0.68±0.03

Wikihow-based 0.48±0.02 0.41±0.02

WMD
9.60s 11.14s 26.50s
7.16s 9.24s 22.65s
0.67±0.03 0.73±0.03 0.55±0.03

Index
24.14s 26.90s 53.48s
2.80ms 2.95ms 4.21ms
0.78±0.03 0.80±0.03 0.63±0.02

tasks is a matter of milliseconds using the trie approach or MinHash LSH Forest. Compared to these runtimes, using WMD it took 23 seconds on average to map a single query to its task on the largest dataset--prohibitively slow for an online setup without any further efficiency tweaks that were beyond the scope of our study. Using the index-based method, determining the task of a query again only takes a few milliseconds.
4.3 Query-Task Mapping Accuracy
We measure accuracy on every dataset as the ratio of correct task mappings across 50 runs of 100 independently sampled test queries in a leave-one-out manner: each test query is removed from its task individually, the datasets without that one query are pre-processed, and the methods are asked to map the now "new" query to a task. Overall, our approaches map at least one in three, and at most four out of five test queries to the correct task. The index-based method clearly performs best on all three datasets while the slow WMD approach is second best twice.
Out of our three datasets, the smaller Session- and TREC-based ones pose easier query-task mapping problems, with all methods getting at least two thirds of the test queries correct. This is explained in part by the smaller datasets having fewer tasks, and comparatively more queries per task; beyond that, previous research on one of the underlying query logs [9] found related queries to often share prefixes, boosting not just the Trie-based method, but the other exact-word-match based ones (Index and LSH), as well.
By contrast, all four methods exhibit their worst query-task mapping performance on the WikiHow-based dataset--the largest both in terms of tasks and total number of queries, but with the smallest average number of queries per task. The fact that the distributional similarity (rather than exact match) based WMD method declines comparatively less in accuracy here points to the prevalence of tasks with less-directly related queries, and some spot checks in the data bear this out: queries with the same task often share synonyms, rather than exactly identical terms.
To elaborate on this insight, Figure 1 shows the results of an additional experiment on two of our datasets. Here, we retrieve the top k (where 1  k  11) results with each method, and then assign

971

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Figure 1: Query-task mapping accuracy under a majority voting scheme. Bands show 95% confidence intervals.
the majority task. Since the Trie data structure does not induce any ordering among the results in the same trie node, we use the deepest k trie nodes on the path to the input query that contain a larger number of individual result queries; with increasing k the Trie method thus approaches a simple majority (or Zero-Rule) classifier. On the less noisy Session-based dataset, this has a more detrimental effect on the Trie method's accuracy. Conversely, the Index and WMD methods benefit a bit more from a majority vote among a few selectively chosen top results on the noisier WikiHow-based dataset than they do on the Session-based one.
5 CONCLUSION
We consider the problem of query-task mapping: given a query log split into tasks and a new query, identify the most appropriate task for the query. This problem is not as well studied as the problem of splitting a log into tasks while also larger datasets of task-split queries are missing. To close this gap and to foster research on query-task mapping, our first main contribution are three large publicly available benchmarking datasets (two with about 50,000 queries and one with about 120,000 queries annotated with tasks). As our second contribution, we compare accuracy and efficiency of four mapping approaches on these datasets in a preliminary study. Our experiments show that an inverted indexbased method forms a strong baseline (accuracy above 0.6 at under 6 milliseconds per query).
Interesting directions for future research include the development of more accurate fast methods, and the generalization of our experiments to even larger datasets. Such larger datasets will most likely contain highly similar tasks that turned out to be the hardest for the tested baselines to distinguish; all methods performed worse on the bigger corpus compared to the smaller ones. In our experiments, all queries had an annotated ground truth task that was also shared by other queries. Also including queries not part of any task may form an interesting addition to the experimental setup (mapping methods should then return that the query is unrelated to any known task). For instance, the index-based method could employ a retrieval score threshold as a guidance in that direction.
REFERENCES
[1] Ahmed Hassan Awadallah, Ryen W. White, Patrick Pantel, Susan T. Dumais, and Yi-Min Wang. 2014. Supporting complex search tasks. In Proceedings of CIKM 2014, 829­838.
[2] Mayank Bawa, Tyson Condie, and Prasanna Ganesan. 2005. LSH Forest: Selftuning indexes for similarity search. In Proceedings of WWW 2005, 651­660.

[3] Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. The query-flow graph: Model and applications. In Proceedings of CIKM 2008, 609­618.
[4] Ben Carterette, Evangelos Kanoulas, Mark M. Hall, and Paul D. Clough. 2014. Overview of the TREC 2014 Session track. In Proceedings of TREC 2014.
[5] Rene De La Briandais. 1959. File searching using variable length keys. In Proceedings of IRE-AIEE-ACM 1959, 295­298.
[6] Debora Donato, Francesco Bonchi, Tom Chi, and Yoëlle S. Maarek. 2010. Do you want to take notes?: Identifying research missions in Yahoo! search pad. In Proceedings of WWW 2010, 321­330.
[7] Daniel Gayo-Avello. 2009. A survey on session detection methods in query logs and a proposal for future evaluation. Information Sciences 179, 12 (2009), 1822­1843.
[8] Matthias Hagen, Jakob Gomoll, Anna Beyer, and Benno Stein. 2013. From search session detection to search mission detection. In Proceedings of OAIR 2013, 85­92.
[9] Matthias Hagen, Martin Potthast, Michael Völske, Jakob Gomoll, and Benno Stein. 2016. How writers search: Analyzing the search and writing logs of non-fictional essays. In Proceedings of CHIIR 2016, 193­202.
[10] Daqing He, Ayse Göker, and David J. Harper. 2002. Combining evidence for automatic web session identification. Information Processing & Management 38, 5 (2002), 727­742.
[11] Wen Hua, Yangqiu Song, Haixun Wang, and Xiaofang Zhou. 2013. Identifying users' topical tasks in web search. In Proceedings of WSDM 2013, 93­102.
[12] Bernard J. Jansen, Amanda Spink, Chris Blakely, and Sherry Koshman. 2007. Defining a session on web search engines. JASIST 58, 6 (2007), 862­871.
[13] Rosie Jones and Kristina Lisa Klinkner. 2008. Beyond the session timeout: Automatic hierarchical segmentation of search topics in query logs. In Proceedings of CIKM 2008, 699­708.
[14] Evangelos Kanoulas, Emine Yilmaz, Rishabh Mehrotra, Ben Carterette, Nick Craswell, and Peter Bailey. 2017. TREC 2017 Tasks track overview. In Proceedings of TREC 2017.
[15] Alexander Kotov, Paul N. Bennett, Ryen W. White, Susan T. Dumais, and Jaime Teevan. 2011. Modeling and analysis of cross-session search tasks. In Proceedings of SIGIR 2011, 5­14.
[16] Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From word embeddings to document distances. In Proceedings of ICML 2015, 957­966.
[17] Liangda Li, Hongbo Deng, Anlei Dong, Yi Chang, and Hongyuan Zha. 2014. Identifying and labeling search tasks via query-based Hawkes processes. In Proceedings of KDD 2014, 731­740.
[18] Zhen Liao, Yang Song, Yalou Huang, Li-wei He, and Qi He. 2014. Task trail: An effective segmentation of user search behavior. IEEE Trans. Knowl. Data Eng. 26, 12 (2014), 3090­3102.
[19] Zheng Lu, Hongyuan Zha, Xiaokang Yang, Weiyao Lin, and Zhaohui Zheng. 2013. A new algorithm for inferring user search goals with feedback sessions. IEEE Trans. Knowl. Data Eng. 25, 3 (2013), 502­513.
[20] Claudio Lucchese, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2011. Identifying task-based sessions in search engine query logs. In Proceedings of WSDM 2011, 277­286.
[21] Claudio Lucchese, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2013. Discovering tasks from search engine query logs. ACM Trans. Inf. Syst. 31, 3 (2013), 14.
[22] Rishabh Mehrotra, Prasanta Bhattacharya, and Emine Yilmaz. 2016. Deconstructing complex search tasks: A Bayesian nonparametric approach for extracting sub-tasks. In Proceedings of NAACL 2016, 599­605.
[23] Rishabh Mehrotra and Emine Yilmaz. 2015. Terms, topics & tasks: Enhanced user modelling for better personalization. In Proceedings of ICTIR 2015, 131­140.
[24] Rishabh Mehrotra and Emine Yilmaz. 2017. Extracting hierarchies of search tasks & subtasks via a Bayesian nonparametric approach. In Proceedings of SIGIR 2017, 285­294.
[25] Rishabh Mehrotra and Emine Yilmaz. 2017. Task embeddings: Learning query embeddings using task context. In Proceedings of CIKM 2017, 2199­2202.
[26] Donald Metzler, Susan T. Dumais, and Christopher Meek. 2007. Similarity measures for short segments of text. In Proceedings of ECIR 2007. 16­27.
[27] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv abs/1301.3781 (2013).
[28] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A picture of search. In Proceedings of Infoscale 2006, 1.
[29] Procheta Sen, Debasis Ganguly, and Gareth J. F. Jones. 2018. Tempo-lexical context driven word embedding for cross-session search task extraction. In Proceedings of NAACL 2018. 283­292.
[30] Amanda Spink, Minsoo Park, Bernard J. Jansen, and Jan O. Pedersen. 2006. Multitasking during web search sessions. Inf. Process. Manage. 42, 1 (2006), 264­275.
[31] Manisha Verma and Emine Yilmaz. 2014. Entity oriented task extraction from query logs. In Proceedings of CIKM 2014, 1975­1978.
[32] Manisha Verma and Emine Yilmaz. 2016. Category oriented task extraction. In Proceedings of CHIIR 2016, 333­336.
[33] Zi Yang and Eric Nyberg. 2015. Leveraging procedural knowledge for taskoriented search. In Proceedings of SIGIR 2015, 513­522.

972

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

An Analysis of Query Reformulation Techniques for Precision Medicine

Maristella Agosti, Giorgio Maria Di Nunzio, Stefano Marchesin
Department of Information Engineering University of Padua, Italy
{maristella.agosti,giorgiomaria.dinunzio,stefano.marchesin}@unipd.it

ABSTRACT
The Precision Medicine (PM) track at the Text REtrieval Conference (TREC) focuses on providing useful precision medicine-related information to clinicians treating cancer patients. The PM track gives the unique opportunity to evaluate medical IR systems using the same set of topics on two different collections: scientific literature and clinical trials. In the paper, we take advantage of this opportunity and we propose and evaluate state-of-the-art query expansion and reduction techniques to identify whether a particular approach can be helpful in both scientific literature and clinical trial retrieval. We present those approaches that are consistently effective in both TREC editions and we compare the results obtained with the best performing runs submitted to TREC PM 2017 and 2018.
CCS CONCEPTS
· Information systems  Specialized information retrieval; Ontologies; Query reformulation.
KEYWORDS
Medical IR; query reformulation; precision medicine
ACM Reference Format: Maristella Agosti, Giorgio Maria Di Nunzio, Stefano Marchesin. 2019. An Analysis of Query Reformulation Techniques for Precision Medicine. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184. 3331289
1 MOTIVATIONS
Medical Information Retrieval (IR) helps a wide variety of users to access and search medical information archives and data [4]. In [7, chapter 2], a classification of textual medical information is proposed: 1) Patient-specific information which applies to individual patients. This type of information can be structured, as in the case of an Electronic Health Record (EHR), or can be free narrative text. 2) Knowledge-based information that has been derived and organized from observational or experimental research. In the case of clinical research, the information is most commonly provided by books
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331289

and journals, but can take a wide variety of other forms, including computerized media. Therefore, the design of effective tools to access and search textual medical information requires, among other things, enhancing the query through expansion and/or rewriting techniques that leverage the information contained within knowledge resources. In this context, Sondhi et al. [12] identified some challenges arising from the differences between general retrieval and medical case-based retrieval. In particular, state-of-the-art retrieval methods, combined with selective query term weighing based on medical thesauri and physician feedback, improve performance significantly [3, 13].
In 2017 and 2018, the Precision Medicine (PM) [10] track1 at the Text REtrieval Conference (TREC)2 focused on an important use case in clinical decision support: providing useful precision medicine-related information to clinicians treating cancer patients. This track gives a unique opportunity to evaluate medical IR systems since the experimental collection is composed of a set of topics (synthetic cases created by precision oncologists) for two different collections that target two different tasks: 1) retrieving biomedical articles addressing relevant treatments for a given patient, and 2) retrieving clinical trials for which a patient ­ described in the information need ­ is eligible.
The objective of our study is to take advantage of this opportunity and evaluate several state-of-the-art query expansion and reduction techniques to examine whether a particular approach can be helpful in both scientific literature and clinical trials retrieval. Given the large number of participating research groups to this TREC track, we are able to compare the best experiments submitted to the PM track based on the results which were obtained applying our approach in the last two years. The experimental analysis shows that there are some common patterns in query reformulation that allow the retrieval system to achieve top performing results in both tasks.
The rest of the paper is organized as follows: Section 2 describes the approach used to evaluate different query reformulation techniques. Section 3 presents the experimental setup and compares the results obtained using our approach with the best performing runs from TREC PM 2017 and 2018. Finally, Section 4 reports some final remarks and concludes the paper.
2 APPROACH
The approach we propose for query expansion/reduction in a PM task comprises three steps, plus an additional fourth step required only for the retrieval of clinical trials. The steps are: (i) indexing, (ii) query reformulation, (iii) retrieval and (iv) filtering.
1 http://www.trec- cds.org/ 2 https://trec.nist.gov/

973

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Indexing Step. We create the following fields to index clinical trials collections: <docid>, <text>, <max_age>, <min_age> and <gender>. Fields <max_age>, <min_age> and <gender> contain information extracted from the eligibility section of clinical trials and are required for the filtering step. The <text> field contains the entire content of each clinical trial -- and therefore also the information stored within the fields described above.
To index scientific literature collections, we create the following fields: <docid> and <text>. As for clinical trials, the <text> field contains the entire content of each target document.
Query Reformulation Step. The approach relies on two types of query reformulation techniques: query expansion and query reduction.
Query expansion: We perform a knowledge-based a priori query expansion. First, we rely on MetaMap [2], a state-of-the-art medical concept extractor, to extract from each query field all the Unified Medical Language System (UMLS)3 concepts belonging to the following semantic types4: Neoplastic Process (neop), Gene or Genome (gngm) and Cell or Molecular Dysfunction (comd). The gngm and comd semantic types are related to the query <gene> field, while neop is related to the <disease> field. For those collections where an additional <other> field is included -- which considers other potential factors that may be relevant -- MetaMap is used on <other> with no restriction on the semantic types, as its content does not consistently refer to any particular semantic type.
Second, for each extracted concept, we consider all its name variants contained into the following knowledge sources: National Cancer Institute5 (NCI), Medical Subject Headings6 (MeSH), SNOMED CT7 (SNOMEDCT) and UMLS Metathesaurus8 (MTH). All knowledge sources are manually curated and up-to-date.
The expanded queries consist in the union of the original terms with the set of name variants. For example, consider a query only containing the word "melanoma" -- which is mapped to the UMLS concept C0025202. The set of name variants for the concept "melanoma" contains, among many others: cutaneous melanoma; malignant melanoma; malignant melanoma (disorder); etc. Therefore, the final expanded query is the union of the original term "melanoma" with all its name variants.
Additionally, we expand queries that do not mention any kind of blood cancer (e.g. "lymphoma" or "leukemia") with the term solid. This expansion proved to be effective in [5] where the authors found that a large part of relevant clinical trials do not mention the exact disease. A more general term like solid tumor is preferable and more effective.
Query reduction: We reduce original queries by removing, whenever present, gene mutations from the <gene> field. To clarify, consider a topic where the <gene> field mentions "BRAF (V600E)". After the reduction process, the <gene> field becomes "BRAF". The reduction process aims to mitigate the over-specificity of topics, since the information contained in a topic is too specific compared to those contained in the target documents [8].
3 https://www.nlm.nih.gov/research/umls/ 4 https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml 5 https://www.cancer.gov/ 6 https://www.ncbi.nlm.nih.gov/mesh/ 7 http://www.snomed.org/ 8 https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/

Additionally, we remove the <other> field from those collections that include it -- since it contains additional factors that are not necessarily relevant, thus representing a potential source of noise in retrieving precise information for patients.9
Retrieval Step. We use BM25 [11] as retrieval model. Additionally, query terms obtained through query expansion are weighted lower than 1.0 to avoid introducing too much noise in the retrieval process [6].
Filtering Step. The eligibility section in clinical trials comprises, among others, three important demographic aspects that a patient needs to satisfy to be considered eligible for the trial, namely: minimum age, maximum age and gender; where minimum age and maximum age are the minimum and the maximum age, respectively, required for a patient to be considered eligible for the trial, while gender is the required gender.
Therefore, after the retrieval step, we filter out from the list of candidate trials those for which a patient is not eligible -- i.e. his/her demographic data (age and gender) does not satisfy the three aforementioned eligibility criteria aforementioned. In those cases where part of the demographic data is not specified, a clinical trial is kept or discarded on the basis of the remaining demographic information. For instance, if the clinical trial does not specify a required minimum age, then it is kept or discarded based on its maximum age and gender required values.
3 SETUP AND EVALUATION
In this section, we describe the experimental collections and the setup used to conduct and evaluate our approach. Then, we compare the results obtained with our approach with those of the best performing systems from TREC PM 2017 and 2018. All these systems make use of external knowledge sources to enhance retrieval performance; moreover, most of them are complex multi-stage retrieval systems, like those proposed in [5, 8], while the approach we present is quite simple and straightforward ­ facilitating its reproducibility.10
Experimental Collections. Both tasks in TREC PM use the same set of topics, but with two different collections: scientific literature, clinical trials.
Topics consists of 30 and 50 synthetic cases created by precision oncologists in 2017 and 2018, respectively. In 2017, topics contain four key elements in a semi-structured format: (1) disease (e.g. a type of cancer), (2) genetic variants (primarily present in tumors), (3) demographic information (e.g. age, gender), and (4) other factors (which could impact certain treatment options). In 2018, topics contain three of the four key elements used in 2017: (1) disease, (2) genetic variants, and (3) demographic information.
Scientific Literature consists of a set of 26,759,399 MEDLINE11 abstracts, plus two additional sets of abstracts: (i) 37,007 abstracts from recent proceedings of the American Society of Clinical Oncology (ASCO), and (ii) 33,018 abstracts from recent proceedings of the American Association for Cancer Research (AACR). These
9In a personal communication with the organizers of the track, we have been informed that it was difficult to convince the oncologists why the other field was even necessary. 10Source code available at: https://github.com/stefano-marchesin/TREC_PM_qreforms 11 https://www.nlm.nih.gov/bsd/pmresources.html

974

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

additional datasets were added to increase the set of potentially relevant treatment information. In fact, precision medicine is a fastmoving field where keeping up-to-date with the latest literature can be challenging due to both the volume and velocity of scientific advances. Therefore, when treating patients, it may be helpful to present the most relevant scientific articles for an individual patient. Relevant literature articles can guide precision oncologists to the best-known treatment options for the patient's condition.
Clinical Trials consists of a total of 241,006 clinical trial descriptions, derived from ClinicalTrials.gov12 -- a repository of clinical trials in the U.S. and abroad. When none of the available treatments are effective on oncology patients, the common recourse is to determine if any potential treatments are undergoing evaluation in a clinical trial. Therefore, it would be helpful to automatically identify the most relevant clinical trials for an individual patient. Precision oncology trials typically use a certain treatment for a certain disease with a specific genetic variant (or set of variants). Such trials can have complex inclusion and/or exclusion criteria that are challenging to match with automated systems.
Experimental Setup. We use Whoosh,13 a pure Python search engine library, for indexing, retrieval and filtering steps. For BM25, we keep the default values k1 = 1.2 and b = 0.75 provided by Whoosh ­ as we found them to be a good combination [1]. For query expansion, we rely on MetaMap to extract and disambiguate concepts from UMLS. We summarize the procedure used for each experiment below. Indexing
· Index clinical trials using the following created fields: <docid>, <text>, <max_age>, <min_age> and <gender>;
· Index scientific abstracts using the following created fields: <docid> and <text>.
Query reformulation
· Use MetaMap to extract from each query field the UMLS concepts restricted to the following semantic types: neop for <disease>, gngm/comd for <gene> and all for <other>;
· Extract from the concepts all name variants belonging to NCI, MeSH, SNOMED CT and MTH knowledge sources;
· Expand (or not) topics that do not mention "lymphoma" or "leukemia" with the term solid;
· Reduce (or not) queries by removing, whenever present, gene mutations from the <gene> field;
· Remove (or not) the <other> field.
Retrieval
· Adopt any combination of the reformulation strategies; · Weigh expanded terms with a value k  {0, 0.1, 0.2, ..., 1}; · Perform a search using expanded queries with BM25.
Filtering
· Filter out clinical trials for which the patient is not eligible.
Evaluation Measures. We use the official measures adopted in the TREC PM track: inferred nDCG (infNDCG), R-precision (Rprec) and Precision at rank 10 (P_10). Precision at rank 5 and at rank 10 were used only for the Clinical Trials task 2017 and are not
12 https://clinicaltrials.gov/ 13 https://whoosh.readthedocs.io/en/latest/intro.html

reported in this work for space reasons. The inferred nDCG was not computed for the task Clinical Trials 2017 since the sampled relevance judgments are not available.
Comparison. In Table 1, we report the results of our experiments (upper part) and compare them with the top performing participants at TREC 2017 and 2018 (lower part). Given the large number of experiments, we decided to present the top 5 runs ordered by P_10 for each year and for each task. Each line shows a particular combination (yes or no values) of semantic types (neop, comd, gngm), usage and expansion of <other> field (oth, oth_exp), query reduction (orig), and expansion using weighted solid (tumor) keyword. We use the symbol `·' to indicate that the features oth, oth_exp are not applicable for year 2018 due to the absence of the <other> field in 2018 topics. We report the results for both Scientific Literature (sl) and Clinical Trials (ct) tasks. We highlight in bold the top 3 scores for each measure, and we use the symbols  and  to indicate two combinations that performed well in both 2017 and 2018. For the TREC PM participants, we select those participants who submitted runs in both years and reached the top 10 performing runs in at least two measures [9, 10]. The results reported in the lower part of Table 1 indicate the best score obtained by a particular run for a specific measure; the best results of a participant are often related to different runs. The symbol `-' means that the measure is not available, while `<' indicates that none of the runs submitted by the participant achieved the top 10 performing runs. For comparison, we add for each measure the lowest score required to enter the top 10 TREC results list, and the score obtained by the best combination of our approach -- indicated by the line number ­ as if we were participants of these tracks.
In 2018, there is a clear distinction in terms of performances among the combinations that achieve the best results for the sl and the ct tasks. For the sl task, considering the semantic type neop expansion without using the umbrella term solid provides the best performances for all the measures considered. On the other hand, two of the best three runs for the ct task (line 5 and 9), use no semantic type expansion, but rely on the solid (tumor) expansion with weight 0.1.
In 2017, the situation is completely different. Lines 12 and 13 show two combinations that are in the top 3 performing runs for both sl and ct. These two runs use query reduction and a weighted 0.1 solid (tumor) expansion. The use of a weighted 0.1 solid expansion as well as a reduced query (orig = n) seems to improve performances consistently for all measures in 2017. The semantic type gngm seems more effective than neop, while comd does not seem to have any positive effect at all.
Another element that shows how difficult these two tasks are is the fact that top performing systems in 2017 do not achieve the same results in 2018. Our study therefore helps researchers to select (or remove) semantic types to build strong baselines for both tasks.
4 CONCLUSIONS AND FINAL REMARKS
In this paper, we proposed and evaluated several state-of-the-art query expansion and reduction techniques for scientific literature and clinical trials retrieval. The experimental analysis showed that no clear pattern emerges for both tasks. In general, a query expansion approach using a selected set of semantic types helps the

975

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Semantic Type

Field Other

line year neop comd gngm oth oth_exp orig solid

1 2018 y

y

n

·

·

y

n

2 2018 y

n

n

·

·

y

n

3 2018 y

n

y

·

·

y

n

4 2018 n

n

n

·

·

y

n

5 2018 n

n

n

·

·

y 0.1

6 2018 n

y

n

·

·

y

n

7 2018 y

n

n

·

·

n

n

8 2018 n

n

y

·

·

y

n

9 2018 n

n

n

·

·

n 0.1

10 2018 y

n

y

·

·

n

n

11 2017 y

n

y

n

n

n 0.1

12 2017 n

n

y

n

n

n 0.1

13 2017 n

n

n

n

n

n 0.1

14 2017 y

n

n

n

n

n 0.1

15 2017 n

n

n

n

n

n

n

16 2017 y

n

y

n

n

y 0.1

17 2017 n

n

y

n

n

y

n

sl P_10 0.5660 0.5640 0.5480 0.5460 0.5440 0.5440 0.5420 0.5340 0.5300 0.5140 0.5033 0.4900
0.4800 0.4767 0.4733 0.4733 0.4633

ct P_10 0.5540 0.5600 0.5660 0.5680 0.5740 0.5540 0.5700 0.5640
0.5820 0.5680 0.3759 0.3931
0.4034 0.3862 0.3931 0.3828 0.3862

sl infNDCG
0.4912 0.4961 0.4941 0.4876 0.4877 0.4853 0.4636 0.4877 0.4635 0.4572 0.3984 0.3881 0.3931 0.3974 0.3943 0.3567 0.3442

ct infNDCG
0.5266 0.5264 0.5292 0.5411 0.5403 0.5403 0.5345 0.5337
0.5446 0.5393
-

sl Rprec 0.3288 0.3288 0.3266 0.3240 0.3247 0.3236 0.3180 0.3229 0.3148 0.3144 0.2697 0.2677
0.2728 0.2714 0.2732 0.2329 0.2254

ct Rprec 0.4098 0.4138 0.4116 0.4197 0.4179 0.4130 0.4134 0.4106
0.4205 0.4122 0.3206 0.3263
0.3361 0.3202 0.3241 0.3253 0.3243

18 2018 19 2018 20 2018 21 2018 22 2018
2018 2018 23 2017 24 2017 25 2017 26 2017 27 2017 2017 2017

TREC PM Participant Identifier UTDHLTRI UCAS udel_fang NOVASearch Poznan
Top 10 threshold Best combination of our approach
UTDHLTRI udel_fang NOVASearch
Poznan UCAS Top 10 threshold Best combination of our approach

0.6160 0.5980 0.5800
< < 0.5800 (1) 0.5660 0.6300 0.5067 < < < 0.4667 (11) 0.5033

0.5380 0.5460 0.5240 0.5520 0.5580 0.5240 (9) 0.5820 0.4172
< 0.3966 0.3690 0.3724 0.3586 (13) 0.4034

0.4797 0.5580 0.5081
< < 0.4710 (2) 0.4961 0.4647 0.3897 < < < 0.3555 (11) 0.3984

0.4794 0.5347 0.5057 0.4992 0.4894 0.4736 (9) 0.5446
-

< 0.3654 0.3289
< < 0.2992 (1) 0.3288 0.2993 0.2503 < < 0.2282 0.2282 (15) 0.2732

0.3920 0.4005 0.3967 0.3931 0.4101 0.3658 (9) 0.4205
(13) 0.3361

Table 1: Results for the TREC PM tasks 2017 and 2018. Details are reported in Section 3.

retrieval of scientific literature, while a query reduction approach without expansion, but a small weighted solid (tumor) keyword expansion, improves performances of the clinical trials task. Nevertheless, we found that a particular combination (marked as ) performs well in both tasks ­ in particular the clinical trials task ­ and could have been one of the top 10 performing runs across many evaluation measures in both TREC PM 2017 and 2018. Therefore, this run can be considered as a baseline on which stronger multi-stage systems can be built.
ACKNOWLEDGMENTS
The authors thank Ellen Vorhees and Kirk Roberts for their helpful insights regarding the interpretation of the data collection. The work was partially supported by the ExaMode project,14 as part of the European Union H2020 research and innovation program under grant agreement no. 825292.
REFERENCES
[1] M. Agosti, G.M. Di Nunzio, and S. Marchesin. 2018. The University of Padua IMS Research Group at TREC 2018 Precision Medicine Track. In Proc. of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, Nov. 14-16, 2018.
[2] A.R. Aronson. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. In Proc. of the AMIA Symposium. American Medical Informatics Association, 17­21.
[3] L. Diao, H. Yan, F. Li, S. Song, G. Lei, and F. Wang. 2018. The Research of Query Expansion Based on Medical Terms Reweighting in Medical Information Retrieval. EURASIP Jour. on Wireless Communications and Networking 2018, 1 (04 May 2018), 105. https://doi.org/10.1186/s13638-018-1124-3
14 htttp://www.examode.eu/

[4] L. Goeuriot, G.J.F. Jones, L. Kelly, H. Müller, and J. Zobel. 2016. Medical Information Retrieval: Introduction to the Special Issue. Information Retrieval Journal 19, 1 (01 Apr 2016), 1­5. https://doi.org/10.1007/s10791-015-9277-8
[5] T.R. Goodwin, M.A. Skinner, and S.M. Harabagiu. 2017. UTD HLTRI at TREC 2017: Precision Medicine Track. In Proc. of the Twenty-Sixth Text REtrieval Conference, TREC 2017, Gaithersburg, Maryland, USA, Nov. 15-17, 2017.
[6] H. Gurulingappa, L. Toldo, C. Schepers, A. Bauer, and G. Megaro. 2016. SemiSupervised Information Retrieval System for Clinical Decision Support. In Proc. of the Twenty-Fifth Text REtrieval Conference, TREC 2016, Gaithersburg, Maryland, USA, Nov. 15-18, 2016.
[7] W. Hersh. 2009. Information Retrieval: A Health and Biomedical Perspective. 2009 Springer Science + Business Media, LLC, New York, NY, USA.
[8] M. Oleynik, E. Faessler, A. Morassi Sasso, A. Kappattanavar, B. Bergner, H. Freitas
da Cruz, J.P. Sachs, S. Datta, and E. Böttinger. 2018. HPI-DHC at TREC 2018: Precision Medicine Track. In Proc. of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, Nov. 14-16, 2018. [9] K. Roberts, D. Demner-Fushman, E.M. Voorhees, W.R. Hersh, S. Bedrick, and A.J. Lazar. 2018. Overview of the TREC 2018 Precision Medicine Track. In Proc. of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, Nov. 14-16, 2018. [10] K. Roberts, D. Demner-Fushman, E.M. Voorhees, W.R. Hersh, S. Bedrick, A.J.
Lazar, and S. Pant. 2017. Overview of the TREC 2017 Precision Medicine Track. In Proc. of the Twenty-Sixth Text REtrieval Conference, TREC 2017, Gaithersburg, Maryland, USA, Nov. 15-17, 2017. [11] S. Robertson and H. Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333­389.
[12] P. Sondhi, J. Sun, C. Zhai, R. Sorrentino, and M.S. Kohn. 2012. Leveraging Medical
Thesauri and Physician Feedback for Improving Medical Literature Retrieval for Case Queries. Jour. of the American Medical Informatics Association: JAMIA 19, 5 (Sep-Oct 2012), 851­858. https://doi.org/10.1136/amiajnl-2011-000293
[13] D. Zhu, S. Wu, B. Carterette, and H. Liu. 2014. Using Large Clinical Corpora for Query Expansion in Text-Based Cohort Identification. Jour. of Biomedical Informatics 49 (2014), 275 ­ 281. https://doi.org/10.1016/j.jbi.2014.03.010

976

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Multiple Query Processing via Logic Function Factoring

Matteo Catena
ISTI-CNR Pisa, Italy matteo.catena@isti.cnr.it
ABSTRACT
Some extensions to search systems require support for multiple query processing. This is the case with query variations, i.e., different query formulations of the same information need. The results of their processing can be fused together to improve effectiveness, but this requires to traverse more than once the query terms' posting lists, thus prolonging the multiple query processing time. In this work, we propose an approach to optimize the processing of query variations to reduce their overall response time. Similarly to the standard Boolean model, we firstly represent a group of query variations as a logic function where Boolean variables represent query terms. We then apply factoring to such function, in order to produce a more compact but logically equivalent representation. The factored form is used to process the query variations in a single pass over the inverted index. We experimentally show that our approach can improve by up to 1.95× the mean processing time of a multiple query with no statistically significant degradation in terms of NDCG@10.
ACM Reference Format: Matteo Catena and Nicola Tonellotto. 2019. Multiple Query Processing via Logic Function Factoring. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331297
1 INTRODUCTION
Some information retrieval tasks require the capability to process multiple queries (multi-queries for short), i.e., queries composed by different sub-queries which must be processed as an ensemble to generate the multi-query's results. For instance, rank fusion allows to produce a single effective list of results from the processing of different query variations. In essence, query variations represent a set of different textual formulations of the same information need. In fact, users can submit very different queries to a retrieval system while trying to solve the same search task. For example, consider a scenario where users want to know which are the positive and negative aspects of wind power. Some users will submit the query variation "wind power advantages and disadvantages", while others may issue something like "wind power pros and cons" or "wind power good or bad" [1].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331297

Nicola Tonellotto
ISTI-CNR Pisa, Italy nicola.tonellotto@isti.cnr.it
Belkin et al. studied the effect of combining different query variations of the same topic to retrieve relevant documents, and they showed that merging together the top documents of different query variations can improve the effectiveness of the retrieval system for a specific information need [3]. The authors explain such improvement by observing that information representation and retrieval are a complex activity, and that no single query formulation can completely address such complexity. Therefore, they justify the usage of multiple query variations on the basis that their combination will address different aspects of the information need, and retrieve more relevant documents.
Boosting search effectiveness using query variations requires the ability to efficiently process multi-queries composed by several variations as sub-queries, and rank fusion techniques can be leveraged for this purpose [2­4]. These techniques compute a separate ranked list of results for each sub-query, and such lists are then aggregated to generate the multi-query's final results. Such multiquery processing strategy can be very effective, but is also a time consuming operation. In fact, a search system may need to traverse its inverted index multiple times to process a multi-query, once for each of its sub-queries.
In this work, we show how to optimize the processing of multiqueries composed by query variations as sub-queries. Firstly, we propose to process such multi-queries as a disjunction of conjunctive sub-queries, i.e., a document is a result for a multi-query only if it contains all the terms for at least one of its sub-queries. We express such condition as a logic function, which we use to test whether a document is a result for the multi-query. For instance, to satisfy the topic presented at the beginning of this section, a document must match a multi-query like "(wind AND power AND pros AND cons) OR (wind AND power AND good AND bad)". Documents are matched against this multi-query in a document-at-a-time fashion and, since the underlying logical formula is in disjunctive normal form, we refer to this approach as DNF matching. Our experiments will show that DNF is both efficient and effective.
However, notice how DNF incurs in redundant computations. In our example, DNF will intersect twice the posting lists related to the terms "wind" and "power". Therefore, we propose to further optimize DNF by factoring its underlying logic function representation. Factoring represents a logic function in a factored form, i.e., as either a single Boolean variable or as a sum or product of factored forms [5]. While being equivalent, factored forms are more compact than disjunctive normal forms, i.e., they reduce the number of times a Boolean variable must be inspected to determine the value of the whole formula. In terms of multi-query processing, our example multi-query can be optimized as "(wind AND power) AND

937

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

((pros AND cons) OR (good AND bad))", which requires to intersect the "wind" and "power"'s posting lists just once. In this work, we propose to automatically optimize multi-queries via logic function factoring, and we refer to this approach as FDNF matching.
Experiments conducted on the ClueWeb12 (Cat. B) and UQV100 datasets shows that DNF is faster than the state-of-the-art Single Pass CombSUM (SP-CS) [4] in processing multiple-queries composed by up to four query variations, while providing statistically indistinguishable effectiveness in terms of NDCG@10, and FDNF exhibits even lower query processing times than DNF/SP-CS in processing multiple-queries composed by up to seven variations.

2 DNF AND FDNF MATCHING
In this section we describe how multi-queries are processed by Disjunctive Normal Form (DNF) query processing strategy, and how this strategy can be optimized by leveraging logic function factoring (FDNF).
A multi-query Q is a set {q1, . . . , qn } of sub-queries qi . Each sub-query in Q contains one or more terms, i.e., qi = {t1, . . . , tm }. The sub-queries in a multi-query are unique, but they can have some terms in common. Abusing our notation, a term belongs to Q if at least a sub-query of Q contains it.
To process a multi-query, all its sub-queries must be processed and their top results must be fused together, to produce a final list of top documents to be returned to the user. Depending on the search system setting, sub-queries can be processed in disjunctive mode or conjunctive mode. According to the standard Boolean model, a document d matches a disjunctive sub-query q, i.e., d is a result for q, if d contains at least one of the sub-query terms. Conversely, a document matches a conjunctive sub-query q if it contains all the sub-query terms [7]. We use a Boolean variable vt to indicate if term t appears in a given document. We denote the logic `OR' operator with the sum symbol `+', and the logic `AND' operator with the multiplication symbol `·'. Therefore, a document matches a disjunctive sub-query q if the logic function t q vt evaluates to true, while it matches a conjunctive sub-query q if the logic function
t q vt evaluates to true. In practice, the documents matching a disjunctive (resp. conjunctive) query are retrieved from an inverted index by performing the union (resp. intersection) of the posting lists related to the query terms. Conjunctive query processing is, in general, more efficient than disjunctive processing, even if the latter is considered more effective than the former [8].
In this work, we assume that all the sub-queries of a given multiquery Q are either processed in disjunctive mode or conjunctive mode. When the sub-queries are disjunctive, a document matches Q if it contains at least one term belonging to one of the sub-queries composing the multi-query. As a consequence, processing Q when its sub-queries are disjunctive is equivalent to process a single query containing all the terms of Q. When sub-queries are conjunctive, a document is a matching result for Q only if it contains all terms of at least one sub-query. This condition can be expressed as a logic function in a disjunctive normal form, i.e., a given document matches the multi-query Q if the following logic function is true

vt

(1)

q Q t q

We propose to leverage the disjunctive normal form to process multi-queries, assuming all its sub-queries are processed in conjunctive mode. The corresponding multi-query processing strategy (referred to as DNF) is implemented as follows. For each sub-query q  Q, DNF creates an ephemeral posting list [6] to iterate on-thefly over the lists of documents containing all the terms in q. This is equivalent to intersect the posting lists of all such terms, which is typically an efficient operation to perform [8]. Once the ephemeral lists are created, the results for Q are retrieved by traversing these lists in parallel and by scoring the results using some retrieval model in a document-at-a-time disjunctive fashion.
While DNF is efficient, it may need to traverse the same posting lists multiple times (see Sec. 1 for an example). This limits the efficiency of DNF, since the processing time increases with the number of posting lists traversals [8]. In particular, DNF needs to traverse the posting list for a query term t every time the corresponding Boolean variable vt appears in Eq. (1). Ideally, we would like to keep such logic function as compact as possible by reducing the number of occurrences of its Boolean variables and, consequently, to minimize the number of posting lists traversals.
Factoring permits to represent a logic function in a factored form, i.e., as either a single Boolean variable or as a sum/product of factored forms. For instance, (v1 · v2) + (v2 · v3) can be factored as v2 · (v1 +v3). A logic function and its factored forms are equivalent, but the factored forms are shorter, i.e., they contain less literals. Therefore, logic function factoring can be used to reduce the number of posting list traversals in DNF.
Brayton illustrates several techniques to factor logic functions [5]. The simplest one, literal factoring, recursively factors a logic function by considering it as an algebraic expression, i.e., by ignoring its Boolean nature and by considering logic OR/AND operators as arithmetic sums/products. At each step, literal factoring divides the logic function by its most frequent literal using elementary algebra. Then, the algorithm recursively factors the resulting quotient and rest of the division. The factoring stops when the input logic function of the recursive step is a literal.
In this work, we use literal factoring to optimize multi-query processing as follows. Firstly, we factor the logic function associated to the multi-query to process. Then, for every logic AND operator in the factored form, we generate an ephemeral posting list to iterate over the intersection of the posting lists of its operands. Similarly, for every logic OR operator we generate an ephemeral posting list to iterate over their union. Finally, we use the ephemeral list associated to the root operator of the factored form to traverse the inverted index and to retrieve the matching document for the multi-query. We refer to this approach as factored DNF, or FDNF for short. Since Eq. (1) and its factored form are equivalent, DNF and FDNF generate the same query results.
3 EXPERIMENTAL SETUP
The experiments in Section 4 are conducted to address the following research questions: RQ1. Which multi-query processing strategy gives the best results
in terms of effectiveness? RQ2. Which multi-query processing strategy gives the best results
in terms of efficiency?

938

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

In our experiments, we compare the effectiveness and efficiency of the different processing strategies for multi-queries: SP-CS, DNF, and FDNF. SP-CS is the state-of-the-art strategy for processing multi-queries when considering their sub-queries as disjunctive [4]. Thanks to its disjunctive nature, SP-CS needs to traverse just once the posting list of the query terms to process a multi-query (see Sec. 2). SP-CS associates a score s(d, Q) to each document d matching multi-query Q as follows:

s(d, Q) = nt · s(d, t),

(2)

t Q

where nt is the number of sub-queries of Q containing term t, and s(d, t) is the document-term score according to some ranked retrieval model (e.g., BM25). The efficiency of SP-CS can be further improved by exploiting dynamic pruning techniques [8], and their authors find that the MaxScore algorithm is the most suitable while dealing with multiple query variations. Therefore, we adopt the same optimization in our experiments.
We compare SP-CS's performance with our DNF multi-query processing strategy, which considers sub-queries as conjunctive, and with FDNF, which optimizes multi-queries by performing the logic factoring of the sub-queries. Note that, when n = 1, a multiquery coincides with its unique sub-query, and the SP-CS (resp. DNF/FDNF) query processing produces the same results as the processing of a single query in disjunctive (resp. conjunctive) mode. In particular, SP-CS coincides with the traditional MaxScore algorithm, while DNF/FDNF coincides with the ranked AND algorithm [8].
In the following experiments we measure the mean NDCG and recall at cutoffs 10 and 1,000 to evaluate the effectiveness of our multi-query processing strategies. We also measure the mean processing times (over three runs), and the mean number of scored postings for these strategies to evaluate their efficiency.
To build our multi-queries, we use the UQV100 dataset [1]. UQV100 contains 100 topics, associated to human-generated query variations. On average, each topic is associated to about 57 unique query variations. For a given topic, we use its query variations to generate sub-queries. Hence, in our experiments, we generate a multi-query for every topic in UQV100, for a total of 100 multiqueries. We vary the number n of unique variations per multi-query, ranging from 1 to all the available unique variations. In this way, we aim at measuring how effectiveness and efficiency change as multi-queries become more and more complex. Following [2], query variations are selected in decreasing order of popularity, i.e, when n = 1, we build the multi-queries using the most popular variation for each topic, when n = 2 we use the two most popular variations, and so on. For each multi-query, we retrieve the top 1,000 matching documents and we use BM25 as the underlying retrieval model.
Experiments are conducted using the Terrier search engine1. The platform is hosted on a dedicated Ubuntu 16.04.5 server, Linux kernel version is 4.4.0-142-generic. The server has an Intel i7-4770K processor and 32 GB RAM. The inverted index used for the experiments is obtained by indexing ClueWeb12 (Cat. B) corpus, without applying any stemming technique nor stopword removal. The index is kept in main memory, compressed with Elias-Fano.

1 http://www.terrier.org

4 RESULTS
Table 1 reports the results, in terms of NDCG and recall, for SP-CS and DNF/FDNF, when we vary the number n of sub-queries in the multi-queries from 1 to all the available ones. DNF and FDNF always return the same matching documents as explained in Sec. 2.
As we can see, there is no statistically significant difference in terms of NDCG@10 between SP-CS and DNF. However, the NDCG@1000 of SP-CS is much higher than the one obtained by DNF when processing the multi-queries. This is explainable with the better recall generally obtainable by disjunctive w.r.t. conjunctive processing, since the latter tend to favor precision over recall [8]. When processing a multi-query composed by a single variation, we observe that SP-CS's NDCG@1000 is 31% higher than DNF. However, DNF's NDCG@1000 increases as more query variations are added to the multi-queries, reducing the gap with SP-CS. When n = 10, DNF's NDCG@1000 is only 3% lower than SP-CS, and less than 0.2% lower when all the variations are used. In fact, multiple query variations help DNF to mitigate the low recall incurred by conjunctive processing, while retaining its high precision.
To conclude on RQ1, we find that the best results in terms of effectiveness are obtained by processing the multi-queries according SP-CS. DNF/FDNF show a similar NDCG@10 (no statistically significant difference) and an inferior NDCG@1000, due to the low recall of conjunctive query processing. However, DNF's NDCG@1000 increases with the number of sub-queries in the multi-queries, being comparable with SP-CS when all the query variations are used.
To address RQ2, Table 2 reports the mean processing time (in milliseconds) and mean number of processed postings for the SPCS, DNF, and FDNF processing strategies, when using n = 1, 2, . . . unique queries variations as sub-queries in the tested multi-queries to retrieve 1000 documents. The mean processing times of DNF increase as the number of sub-queries n increases. DNF is faster than SP-CS with few query variation, although it always processed less postings than SP-CS. The improvement in mean processing time oscillates between approximately 1.67× to 1.15×, depending on the value of n.
FDNF is even faster than DNF, thanks to the factoring of multiqueries. As discussed in Section 2, DNF may need to open a posting list multiple times, to process sub-queries in conjunctive mode. FDNF mitigates this issue by factoring the multi-queries, and this explains why FDNF always processes less postings than DNF, with smaller response times.. Moreover, for up to 7 query variations, FDNF improves the mean response times by approx. 1.04× up to 1.95× with respect to SP-CS. On average, FDNF can process four sub-queries in a time close to that required by SP-CS to process just one, with no statistically significant degradation of NDCG@10, as seen for RQ1.
To conclude on RQ2, we find that our proposed FDNF multiquery processing strategy obtains smaller mean processing times w.r.t. SP-CS and our proposed DNF for up to seven sub-queries, allowing to process multiple sub-queries within acceptable time thresholds.

939

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: NDCG, with cutoff at 10 and 1000, and recall, for SP-CS, and DNF/FDNF when using n unique query variations. The best results for each value of n are in bold. SP-CS's results with  are statistically significant w.r.t. DNF/FDNF's according to paired t-test (p < 0.01).

SP-CS

DNF/FDNF

n NDCG@10 NDCG@1000 Recall NDCG@10 NDCG@1000 Recall

1 0.3059 2 0.3391 3 0.3331 4 0.3401 5 0.3480 6 0.3442 7 0.3390 8 0.3408 9 0.3465 10 0.3561

0.4335 0.4637 0.4691 0.4774 0.4832 0.4839 0.4855 0.4871 0.4937 0.4990

0.5735 0.6079 0.6177 0.6224 0.6282 0.6305 0.6374 0.6401 0.6444 0.6519

0.3000 0.3330 0.3254 0.3329 0.3434 0.3403 0.3372 0.3394 0.3462 0.3551

0.3317 0.4023 0.4225 0.4357 0.4546 0.4573 0.4639 0.4693 0.4779 0.4853

0.3751 0.4813 0.5206 0.5364 0.5624 0.5712 0.5864 0.5971 0.6046 0.6152

all 0.3538

0.5064

0.6634 0.3539

0.5055

0.6593

Table 2: Mean processing time (Time, in ms) and mean number of processed postings (Post., in millions of postings) for SP-CS, DNF, and FDNF when using n unique queries variations. The best results for each value of n are in bold.

SP-CS

DNF

FDNF

n Time Post. Time Post. Time Post.

1 95 0.84 49 0.09 47 0.09 2 117 1.06 70 0.21 60 0.19 3 135 1.24 101 0.33 84 0.28 4 150 1.42 130 0.46 104 0.39 5 157 1.46 163 0.61 122 0.50 6 167 1.56 197 0.75 149 0.62 7 177 1.65 232 0.89 170 0.72 8 201 1.91 266 1.03 201 0.84 9 217 2.11 295 1.11 217 0.90 10 235 2.22 332 1.25 244 1.00

all 621 5.46 2289 10.46 1699 9.03

5 CONCLUSIONS
In this paper, we addressed the problem of efficiently processing multiple queries, i.e., queries composed by different sub-queries that represents variations of a same information need. Following the standard Boolean model, firstly we proposed to represent a group of sub-queries as a logic function where Boolean variables represent query terms. Secondly, we proposed to process a multiple query in disjunctive normal form (DNF), i.e., processing in parallel every sub-query as an intersection of posting lists and the resulting intersected lists as a union of posting lists. Thirdly, we proposed to apply factoring to the logic function of a multiple query, to produce a more compact but logically equivalent representation, and we presented a new processing strategy for multiple queries based on the factored form of their logic function (FDNF).

We experimented our proposed processing strategies using the TREC ClueWeb12 collection, and the UQV100 set of query variations. As baseline, we selected the state-of-the-art SP-CS processing strategy for query variations. Our experiments showed that both DNF and FDNF do not significantly degrade the effectiveness in terms of NDCG@10 with respect to SP-CS. Moreover, our FDNF strategy can improve by up to 1.95× the mean processing time viz. SP-CS.
As future works, we plan to investigate different literal factoring algorithms, taking into account the properties of Boolean algebra as well as new factoring optimizations, exploiting different sub-queries characteristics, such as the posting list lengths.
ACKNOWLEDGMENTS
We would like to thanks Joel Mackenzie for his help in the assessment of the experimental results. This paper is partially supported by the BIGDATAGRAPES project (grant agreement N780751) that received funding from the EU H2020 research and innovation programme under the Information and Communication Technologies work programme.
REFERENCES
[1] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A Test Collection with Query Variability. In Proc. SIGIR. 725­728.
[2] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2017. Retrieval Consistency in the Presence of Query Variations. In Proc. SIGIR. 395­404.
[3] Nicholas J. Belkin, Colleen Cool, W. Bruce Croft, and James P. Callan. 1993. The effect multiple query representations on information retrieval system performance. In Proc. SIGIR. 339­346.
[4] Rodger Benham, Joel Mackenzie, Alistair Moffat, and J. Shane Culpepper. 2018. Boosting Search Performance Using Query Variations. CoRR abs/1811.06147 (2018).
[5] Robert K. Brayton. 1987. Factoring Logic Functions. IBM Journal of Research and Development 31, 2 (1987).
[6] Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2017. Efficient & Effective Selective Query Rewriting with Efficiency Predictions. In Proc. SIGIR. 495­504.
[7] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press.
[8] Nicola Tonellotto, Craig Macdonald, and Iadh Ounis. 2018. Efficient Query Processing for Scalable Web Search. FnT in IR 12, 4-5 (2018), 319­500.

940

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Text Retrieval Priors for Bayesian Logistic Regression

Eugene Yang
IR Lab, Georgetown University Washington, DC, USA
eugene@ir.cs.georgetown.edu

David D. Lewis
Cyxtera Technologies Dallas, TX, USA
sigir2019paper@davelewis.com

Ophir Frieder
IR Lab, Georgetown University Washington, DC, USA
ophir@ir.cs.georgetown.edu

ABSTRACT
Discriminative learning algorithms such as logistic regression excel when training data are plentiful, but falter when it is meager. An extreme case is text retrieval (zero training data), where discriminative learning is impossible and heuristics such as BM25, which combine domain knowledge (a topical keyword query) with generative learning (Naive Bayes), are dominant. Building on past work, we show that BM25-inspired Gaussian priors for Bayesian logistic regression based on topical keywords provide better effectiveness than the usual L2 (zero mode, uniform variance) Gaussian prior. On two high recall retrieval datasets, the resulting models transition smoothly from BM25 level effectiveness to discriminative effectiveness as training data volume increases, dominating L2 regularization even when substantial training data is available.
CCS CONCEPTS
· Information systems  Clustering and classification; · Computing methodologies  Supervised learning by classification; Regularization; · Theory of computation  Bayesian analysis.
KEYWORDS
text classification, regularization, ad hoc retrieval, Bayesian priors, Bayesian logistic regression
ACM Reference Format: Eugene Yang, David D. Lewis, and Ophir Frieder. 2019. Text Retrieval Priors for Bayesian Logistic Regression. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331299
1 INTRODUCTION
Discriminative learning methods such as regularized logistic regression are widely used in applications, such as text categorization, where large amounts of labeled training data are available. When little or no labeled data is available, as in ad hoc retrieval and relevance feedback, heuristics such as BM25, based on domain knowledge (queries) and generative learning (Naive Bayes, for example), are dominant [11]. This dichotomy reflects results showing that
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331299

generative approaches dominate at small training set sizes, while discriminative ones dominate for large training sets [8].
In high recall retrieval (HRR) tasks such as systematic review in medicine [13] and electronic discovery in the law [2], however, algorithms must deal with training sets of varying size. HRR projects often begin with keyword queries, and build training sets by iterative active learning [1]. A range of training set sizes from zero to handfuls to thousands of examples are encountered during a typical project. A single, simple algorithmic approach that spans all training set sizes is desirable.
Our contribution is a deceptively simple synthesis: logistic regression that regularizes toward the coefficient values of a good text retrieval query (BM25 in our case) rather than toward values of zero as is usual. We describe how this approach builds on previous work in Bayesian logistic regression for text data, and draws on two theoretical interpretations of IDF weighting. We test our approach on two HRR datasets and find that our proposed methods dominate both standard L2 regularization and statistical text retrieval baselines at all training set sizes.
2 BACKGROUND
Regularization--the penalization of solutions that deviate from prior expectations--is a key technique for avoiding fitting to accidental properties of data in supervised learning. A common approach is the so-called L2 penalty, which is proportional to the squares of the coefficients (thus the square of the L2 norm). L2 penalties not only improve generalization, but they aid convergence of optimization algorithms used for fitting to training data [14].
L2 penalties can be given a Bayesian interpretation [4]. Assume a conditional probability model y = f (x; w) (logistic regression, for instance) parameterized by a d +1-dimensional vector of coefficients w (one per feature, plus an intercept). In the Bayesian framework, we encode our expectations about likely coefficient values as a prior probability distribution.
Suppose that the prior is a product of independent univariate gaussian distributions N (bj , j 2), where j ranges over coefficients. Let D = (x1, y1), ..., (xn, yn ) be a data set where each y value is assumed to be generated by applying f (x; w) independently to the corresponding x. Bayes Rule then gives this posterior probability distribution:

p(w|D)

=

p (D |w)p (w) p(D)

n
p(yi |w; xi)
i =1
=

d +1


1

e-

(wj -bj 2j 2

)2

j=1 2 j 2

p(D)

1045

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

where p(D|w) is the conditional probability of seeing the y values given the corresponding x's, and p(D) is the unconditional probability.
When d is large, as in information retrieval, we typically ask algorithms to produce the MAP (maximum a posteriori) estimate for w and use that as our predictive model. This is equivalent to finding the maximum of this penalized loss:

n

d



w

=

argmax

 

-

ln p(yi |w; xi) +

 j (wj - bj )2 

w

 

i =1

j =1

 





where bj is the prior mode for coefficient j, and j is inversely

proportional to the standard deviation of the prior on coefficient j.

The usual L2 penalization scheme corresponds to an assumption

that all coefficients will be small (prior mode 0), and that belief in

that smallness is the same for all coefficients (uniform standard

deviation, and thus penalty).

Some studies have relaxed these assumptions. Dayanik, et al [3]

used IDF (inverse document frequency values) weights computed

from a category corpus to set the mode or standard deviation of

Bayesian priors on coefficients of a logistic regression model. In

more recent work, we used keyword queries with no category

corpus to set modes (only) of Bayesian priors for logistic regression

[15].

Several authors have proposed first training a generative model

such as Naive Bayes or Rocchio, and then using the resulting co-

efficients as a prior or parameter space constraint on training a

logistic regression model [6, 17]. This method has questionable

statistical foundations when applied to a single training set, but is

more justifiable when applied to multiple training sets in transfer

learning [9].

3 IDF-BASED REGULARIZATION
Modern term weighting schemes such as BM25 [11] were developed to deal with the ultimate low-data scenario: ranked retrieval with no training data. We suggest this provides a simpler alternative than past approaches for incorporating domain knowledge in supervised learning for text analytics problems.
The notion of IDF weighting is key. Justifications for IDF weighting of query terms fall into two major classes [10], and give a new perspective on Dayanik's methods for constructing priors [3]. In probabilistic IR models, IDF weights appear as the coefficients of a generative learning model (Naive Bayes) trained on a blind negative labeling of an entire data set. This assumes that all keywords are positively associated with relevance, and suggests a prior where the modal value of a coefficient is proportional to its IDF.
In contrast, information-theoretic interpretations of IDF view rare terms as being more informative about relevance. This view suggests that less training data should be required to produce a large coefficient for high IDF terms than low IDF terms. In other words, priors for high IDF terms should have a higher variance, regardless of their mode. This translates to a smaller penalty on coefficient magnitude.
Based on these perspectives, we tested the following four schemes for determining priors:
· UPQM: Uniform penalty (i.e., uniform standard deviation of priors) for all coefficients. Prior mode equal to 1 + log QT F

where QT F is the number of occurrences of the term in the keyword query. When each term occurs only once, this is identical to Dayanik's Mode method [3]. · UPQIM: Uniform penalty. Prior mode equal to a BM25 query. Similar to Dayanik's Mode/TFIDF method, but requiring only a single query, not a corpus of queries or category descriptions. · IIPQM: Inverse IDF penalty: penalty is inversely proportional to IDF. QTF modes. · IIPQIM: Inverse IDF penalty and BM25 modes.
Using the same notation, we refer to conventional L2 regularization, with a uniform penalty toward zero modes, as UPZM.
Our four methods leave three prior values unspecified: the prior mode and penalty for the intercept coefficient of the logistic regression model, and the base prior penalty for term coefficients. The intercept affects only calibration of the model, not ranking, so for these experiments we used a fixed zero mode prior.
Choosing a base penalty value, however, is needed both as a uniform penalty for UP methods, and to be divided by IDF values in IIP methods. Dayanik, et al chose their base penalty value by using 10-fold cross-validation [3]. This required a minimum of 10 training examples, and arguably was unstable well above that size. Since priors provide their main benefit for small training sets, we eschewed cross-validation and instead explored a range of base penalty values (2-24 to 216) to determine if a plausible default value exists.
Our method is easy to implement, since most existing logistic regression code bases support L2 penalties in their optimization code (albeit only in UPZM form). We modified the existing sklearn.linear_model.SGDClassifier package from scikit-learn to support nonzero modes and variable penalties in logistic regression [15]. We provide the modified version on GitHub1. The changes to support the penalties used in this paper required only about 30 lines of new or edited code (assuming IDFs are computed externally).
4 METHODS
4.1 Data Sets
We used two test collections drawn from high recall retrieval research: the Jeb Bush Collection and RCV1-v2.
The Jeb Bush Collection consists of email to and from a state governor in the United States [12]. It consists of 290,099 files, of which 274,124 are unique based on MD5 hash. The TREC 2015 and 2016 Total Recall Tracks defined a total of 44 topics on the Jeb Bush data and distributed short titles and labeled data for each [5, 12]. The 2016 labels were ternary, so we treated both "relevant" and "important" as positive and "non-relevant" as negative. To ensure enough positive examples for accurate estimation of effectiveness, our experiments used only the 33 topics with at least 160 positive documents. We used the topic title as our keyword query. This provided from one to five keywords per topic.
RCV1-v2 is a widely used text categorization dataset [7]. It consists of 804,414 newswire stories categorized by professional editors. Of the 823 categories, we chose the 82 categories that had at least
1 https://github.com/eugene- yang/priorsgd

1046

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Mean testset R-precision for logistic regression variants with base penalty strength of 1.0 and various training set sizes. Percentages are relative improvements of knowledge-based priors over a uniform prior. With no training data (size 0), UPQM and IIPQM act as QTF queries, UPQIM and IIPQIM act as BM25 queries, and UPZM ranks randomly.

Jeb Bush Collection (10 replicates)

RCV1-v2 (1 replicate)

Size UPZM UPQM

UPQIM

IIPQM

IIPQIM UPZM UPQM

UPQIM

IIPQM

IIPQIM

0

0.5 34.6 (6395%) 38.6 (7133%) 34.6 (6395%) 38.6 (7133%) 5.1 37.1 (630%) 37.9 (645%) 37.1 (630%) 37.9 (645%)

1 13.1 37.9 (190%) 39.3 (200%) 39.3 (200%) 40.3 (208%) 17.6 38.4 (118%) 38.5 (118%) 38.7 (120%) 39.0 (121%)

2 13.1 29.6 (127%) 39.2 (200%) 39.7 (204%) 41.0 (214%) 16.0 34.9 (118%) 38.6 (142%) 39.2 (145%) 39.1 (145%)

4 17.3 33.2 ( 92%) 40.5 (135%) 41.8 (142%) 43.3 (151%) 22.1 36.1 ( 64%) 39.1 ( 77%) 40.2 ( 82%) 39.7 ( 80%)

8 22.9 36.9 ( 62%) 42.2 ( 84%) 46.4 (103%) 44.9 ( 96%) 30.1 41.4 ( 38%) 40.0 ( 33%) 43.5 ( 45%) 40.4 ( 34%)

16 29.8 42.1 ( 41%) 43.8 ( 47%) 52.5 ( 76%) 47.7 ( 60%) 39.7 47.2 ( 19%) 41.5 ( 5%) 49.5 ( 25%) 41.7 ( 5%)

32 38.3 47.6 ( 24%) 46.5 ( 22%) 58.2 ( 52%) 50.8 ( 33%) 49.0 53.3 ( 9%) 43.7 (-11%) 56.0 ( 14%) 44.0 (-10%)

64 47.3 53.5 ( 13%) 49.6 ( 5%) 62.6 ( 32%) 54.5 ( 15%) 57.0 59.0 ( 4%) 46.8 (-18%) 61.4 ( 8%) 47.2 (-17%)

128 55.2 59.7 ( 8%) 54.0 ( -2%) 66.2 ( 20%) 59.8 ( 8%)

63.0 63.8 ( 1%) 50.7 (-20%) 66.4 ( 5%) 52.2 (-17%)

10,000 positive documents with an eye toward future studies of variation across training sets. Each category has a Reuters Business Briefing (RBB) description of between one and seventeen words that we use as our keyword query.
Text processing simply replaced punctuation with whitespace, and then formed tokens at whitespace boundaries. We used BM25 within document weights, i.e. saturated TF weights.
4.2 Evaluation
The impact of prior knowledge depends on training set size. As usual in supervised learning research, we nested smaller training sets in larger ones. Training sets of size 1 consisted of a single randomly selected positive example. Larger training sets (from 2 to 128 documents by powers of two) were balanced 50/50 between random positive and random negative examples, mimicking the balance sought by the active learning algorithms used in high recall retrieval [2]. All training data were drawn from a random 40% of the collection, with 60% reserved for estimating effectiveness.
Variability in effectiveness between training sets is high for small training sets and low richness. To produce more stable results for the Jeb Bush collection we averaged across ten randomly drawn training sets of 128 documents and their included balanced subsets. With the larger number, and higher richness, of categories for RCV1-v2 averaging over replicates was less necessary for stability (and more computationally expensive), and was deferred for future work.
Documents were ranked by logistic regression scores, with ties (common in some conditions) broken by MD5 hash of document ID. We used R-precision (precision of documents above a cutoff equal to the number of testset relevant documents) as our effectiveness measure, as is common in HRR research. We computed test set R-precision for each run, averaged it across replicates when used, and then averaged across categories for a given training set size and penalty level.
5 RESULTS AND ANALYSIS
We compare our four methods with two baselines: a BM25 query formed from the keyword query for each topic, and UPZM logistic regression. Text retrieval baselines are rarely used in research on

Figure 1: Mean test set R-precision for training sets of size 1 and 128, as a function of the base regularization penalty (log2 scale) on the Jeb Bush collection.
domain knowledge in supervised learning, but should be. As Table 1 shows, BM25 (with effectiveness of 38.6% and 37.9% on Jeb Bush and RCV1-v2 respectively) dominates UPZM until 16 to 32 examples are available.
Table 1 shows the mean R-precision values for each of our four methods plus UPZM, all with a base penalty of 1.0 (Section 5.1). All four IDF-based priors vastly outperform UPZM for small training set sizes. The QM methods, particularly IIPQM, maintain this dominance for all training set sizes. The QIM methods are more skewed, excelling a bit at smaller training set sizes and faltering a bit at larger ones.
5.1 Choosing the Base Penalty
Figure 1 shows the impact of varying the base penalty value for each of the supervised methods, on training sets of size 1 and 128

1047

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Figure 2: A heatmap display of the interaction between base penalty strength and training set size, both with log2 scaling, for UPZM and IIPQM on Jeb Bush. Lighter cells indicate higher average R-precision. Colors are scaled row-by-row, so that lighter
colors indicate dominance for that training set size across both algorithms and all penalties.

from the Jeb Bush data. We see classical regularization curves with maximum effectiveness at intermediate regularization strengths.
Training on a single positive example provides insight into the methods. At high penalties (right hand side of graph), the example is largely ignored. Our methods converge to their prior modes: a QTF query for QM methods, and a BM25 query for QIM methods. Conversely, at low penalties (left side), the prior mode is largely ignored. IIP methods converge to BM25 querying-by-document [16]. UP methods (including UPZM) converge to unweighted queryingby-document with saturated TF weights. Since we did not omit stopwords or use within-document IDF weighting, the resulting effectiveness is little better than random.
The joint relationship between training set size and base penalty strength is shown in Figure 2. We use a heatmap to compare UPZM and IIPQM for all penalty strengths and training set sizes (on a log base 2 scale) for the Jeb Bush data. Lighter values indicate higher values of mean R-precision. The color scale is normalized separately for each row taking both UPZM and IIPQM values into account, since we are interested in relative effectiveness for a given training set size.
We see that UPZM is dominated under almost all conditions. The IIPQM results show that a base penalization in the range 2-4 to 24 provides good average effectiveness across all training set sizes. Examining the averaged RCV1-v2 data shows a similar "ridge of light." Per-category data on both datasets shows this range of penalties is optimal for most individual categories as well.
6 FUTURE WORK
Our experiments examined the generalization behavior of IDFbased priors under controlled variations in base penalty strength and training set size. Operational HRR scenarios are more complex. First, a variety of active learning algorithms and batch size schemes are used to iteratively generate training sets [2]. Second, an HRR dataset is both the source of training data and the target of prioritization, so that documents that are labeled no longer need to be scored. Both factors will require experimentation to understand in full.
The success of inverse IDF regularization penalties when keywords are available suggests using this technique even without

prior knowledge. IDF weighting has been used in application areas as diverse as images, video, music, and genomic data, so the technique may have broad applicability.
7 CONCLUSION
Regularized logistic regression is a standard workhorse for machine learning, but has faltered when applied to tiny training sets. We show that its effectiveness can be improved under all conditions, and vastly for small training sets, by IDF-based priors.
REFERENCES
[1] Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D Smucker, Gordon V Cormack, and Maura R Grossman. 2018. A System for Efficient High-Recall Retrieval.. In SIGIR. 1317­1320.
[2] Gordon F. Cormack and Maura F. Grossman. 2014. Evaluation of machinelearning protocols for technology-assisted review in electronic discovery. SIGIR 2014 (2014), 153­162. https://doi.org/10.1145/2600428.2609601.
[3] Aynur Dayanik, David D Lewis, David Madigan, Vladimir Menkov, and Alexander Genkin. 2006. Constructing informative prior distributions from domain knowledge in text classification. In SIGIR 2006. ACM.
[4] Alexander Genkin, David D. Lewis, and David Madigan. 2007. Large-Scale Bayesian Logistic Regression for Text Categorization. Technometrics 49, 3 (Aug. 2007), 291­304. https://doi.org/10.1198/004017007000000245
[5] Maura R. Grossman, Gordon V. Cormack, and Adam Roegiest. 2016. TREC 2016 Total Recall Track Overview.
[6] David D. Lewis and William A Gale. 1994. A sequential algorithm for training text classifiers. In SIGIR 1994. 3­12.
[7] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. JMLR 5 (2004), 361­397.
[8] Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in neural information processing systems. 841­848.
[9] Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22, 10 (2010), 1345­1359.
[10] Stephen Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF. JDoc 60, 5 (2004), 503­520.
[11] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. F&T in IR 3, 4 (2009), 333­389.
[12] Adam Roegiest and Gordon V. Cormack. 2015. TREC 2015 Total Recall Track Overview. (2015).
[13] Harrisen Scells and Guido Zuccon. 2018. Generating Better Queries for Systematic Reviews.. In SIGIR. 475­484.
[14] Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding machine learning: From theory to algorithms. Cambridge university press.
[15] Eugene Yang, David D. Lewis, and Ophir Frieder. 2019. A Regularization Approach to Combining Keywords and Training Data in Technology-Assisted Review. In ICAIL 2019. Montreal, Canada.
[16] Eugene Yang, David D. Lewis, Ophir Frieder, David Grossman, and Roman Yurchak. 2018. Retrieval and Richness when Querying by Document. DESIRES (2018).
[17] Yi Zhang. 2004. Using bayesian priors to combine classifiers for adaptive filtering. In SIGIR 2004. ACM, 345­352.

1048

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Understanding the Interpretability of Search Result Summaries

Siyu Mi
Department of Computer Science, Virginia Tech siyu6@vt.edu
ABSTRACT
We examine the interpretability of search results in current web search engines through a lab user study. Particularly, we evaluate search result summary as an interpretable technique that informs users why the system retrieves a result and to which extent the result is useful. We collected judgments about 1,252 search results from 40 users in 160 sessions. Experimental results indicate that the interpretability of a search result summary is a salient factor influencing users' click decisions. Users are less likely to click on a result link if they do not understand why it was retrieved (low transparency) or cannot assess if the result would be useful based on the summary (low assessability). Our findings suggest it is crucial to improve the interpretability of search result summaries and develop better techniques to explain retrieved results to search engine users.
KEYWORDS
Interpretability; search result summary; click behavior.
ACM Reference Format: Siyu Mi and Jiepu Jiang. 2019. Understanding the Interpretability of Search Result Summaries. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331306
1 INTRODUCTION
Machine learning techniques are becoming increasingly powerful today, but they are also more and more sophisticated and difficult to understand for human beings. What we need is not only accurate models but also models that are explainable to us. Understanding how the model works may also help the user make better decisions and further improve the model.
Despite many discussions of explainable AI and machine learning recently [3, 4, 9, 11, 13], few previous work explicitly examined the interpretability of search results. Some latest studies [14, 15] applied explainable techniques such as LIME [12] to interpret search result ranking. However, it remains unclear how helpful these techniques are in terms of helping search engine users. Also, we believe current search engines do have already offered some interpretable functionalities, but no previous work examined them in such a way.
We note that information retrieval, among many research fields with extensive use of machine learning, is in fact one of the earliest
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331306

Jiepu Jiang
Department of Computer Science, Virginia Tech jiepu@vt.edu
to offer interpretations for system decisions and outputs. Particularly, a query-biased search result summary [16] delivers two important information to search engine users:
· Why the system retrieves a search result -- we use transparency to refer to the ability of a summary to interpret this information. Through selecting sentences with high coverage of query terms and highlighting keywords in URLs and snippets, search result summaries inform users keyword matching and term frequency are important criterion for retrieving and ranking search results.
· To which extent a result would be useful -- we use assessability to refer to the ability of a summary to explain search result relevance. We believe assessability is a unique aspect of interpretability offered by search result summary, as many other machine learning applications directly present system outputs to end users.
We report results from a lab user study for evaluating the interpretability of search result summaries in existing web search engines. We recruit participants to work on different search tasks using an experimental search system, where the results and summaries came from the API of a commercial web search engine. We collect their judgments regarding both the interpretability and the (expected) usefulness of search result summaries after each session.
· Participants have high transparency and assessability ratings for current search engine's summaries.
· The summaries' transparency and assessability judgments positively correlate with each other and usefulness ratings.
· Results of a regression analysis suggests that the transparency and assessability of summaries have significant effects on users' click decisions when they browse a SERP.
2 USER STUDY
2.1 Experimental Design
We conducted a lab user study to evaluate the interpretability of search results in web search engines. We instructed participants to work on assigned search tasks in an experimental system and make judgments about the retrieved results afterward. We recorded users' search behavior and collected search result judgments.
Our study used a 2×2 within-subject design to balance different types of search tasks. The tasks come from the TREC session tracks [1] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classification framework [8]. The targeted task product is either factual (to locate facts) or intellectual (to enhance understanding about a topic). The goal of a task is either specific (clear and fully developed) or amorphous (an ill-defined or unclear goal). We divided participants into groups of four. Participants in the same group worked on the same four tasks (one task for each type) but using a different sequence (rotated using a Latin square). We assigned different tasks to each group to increase task diversity. We also included a training task at the beginning to help users understand the whole procedure.

989

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Figure 1: Screenshots for search and judgment pages.
The experimental system sends user requests to Bing API and returns Bing search results and query suggestions. Figure 1 (left) shows a SERP from our system. We displayed search results in the same look as Google did at the time of our study, including the font size, weight, and color for title, URL, and snippet. Our SERP only showed result abstracts and related searches. We also highlighted words1 in URLs and snippets as Bing did at the time of the study.
For each task, we asked participants to collect information using our experimental search system to address the problem stated in the task description. We instructed the participants that they could issue different queries and click on multiple result links. The participants could request to finish a session if they believe they had finished the task requirements. Our system would also terminate a session automatically after 10 minutes. On average, the participants spent 262 seconds in a search session.
2.2 Search Result Judgments
We collected two types of judgments after each session: Summary Judgments. We asked participants to evaluate search result summaries retrieved during the search session. Table 1 shows the summary judgment questions2. We included two interpretability questions (transparency and assessability as defined in Section 1). We also asked participants to assess the expected usefulness of the result based on the summary (usefulness). The judgment page customized some of the questions based on the query retrieved the result summary, where $q is the actual query string. Participants responded to the three judgment questions using a five point Likertscale from 1 (Strongly Disagree) to 5 (Strongly Agree).
Figure 1 (right) shows an example page for collecting summary judgments. We displayed a summary in the same look we showed it on the SERP, except that we disabled the URL link (we hope users to make judgments based on the summary itself). We also showed task description and the search query retrieved the result summary on the page to help users make judgments. Result judgments (not reported in this paper). After judging a summary, users needed to read the result web page and respond to a few
1 Bing search API returned highlighted URLs and snippets. The highlighted words may have not appeared in the search query. 2 During the training session, we instructed participants that they needed to answer the questions based on the whole search result abstract (although the wording of the questions used "snippet").

Table 1: Search result summary judgment questions.

Transparency Assessability Usefulness

By looking at the snippet, I can understand why the search engine returned this result for my keywords "$q". By looking at the snippet, I can tell if the result is useful or not without opening the link. By looking at the snippet, I expect the result webpage to include useful information for the search task.

Table 2: Summary of collected result judgments.

Priority of Summary Judgments 1 Clicked results 2 Not clicked, possibly viewed 3 Not clicked, possibly not viewed

Judged/Total (%) 546/588 (92.9%) 273/578 (47.2%) 433/3,056 (14.2%)

judgment questions regarding the result document. Here we only focus on summary judgments and do not report result judgments.
Priority of Judgments. A user could issue multiple queries and retrieve a large number of results during a session. It is impractical to require participants to judge all the retrieved results due to the time constraints of a lab study. Thus, we generated a priority list of judgments as in Table 2. We gave clicked results the highest priority because they are connected with more search behaviors (e.g., dwell time) than other results. We divided the unclicked results into "possibly viewed" and "possible not viewed" ones and gave the former a higher priority. We consider an unclicked result summary as "possibly viewed" if the result is at a higher rank than at least one clicked results on the same SERP--previous eye-tracking studies showed that users have high chances to have viewed the summaries at a higher rank than a clicked result [7].
Participants judged results in the priority list one after another until they spent 10 minutes on judgments. We shuffled the sequence of results within each priority group. We did not set any time limits for judging a result to ensure participants have enough time.
2.3 Collected Data
The user study included 40 participants (20 are female) from a university in the United States. We recruited participants through fliers posted to the main buildings of the campus and a Facebook group targeting the whole university. We compensated each participant $20 for their time (about 1.5 hours). We required all participants to be English native speakers and older than 18. The participants included 33 undergraduate students, 6 graduate students, and one staff. About 2/3 of the participants were studying STEM majors. Their average age was 21.95 (SD = 6.07).
In total, we collected user behavior and result judgments from 160 sessions (excluding training sessions). On average, participants issued 2.63 queries, clicked on 3.67 results, and judged 7.83 results (including both summaries and result web pages) per session.
3 ANALYSIS
We examine the collected search result summary judgments in this section. We particularly focus on the following research questions:

990

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Figure 2: Distribution of search result summary ratings.

Table 3: Spearman's correlation of judgments (N = 1, 252).

Transparency Assessability Usefulness

Transparency

1.0

-

-

Assessability

0.483

1.0

-

Usefulness

0.617

0.476

1.0

All the correlations are statistically significant at 0.001 level.

· RQ1: Are search result summaries from current search engines transparent and assessable to users? (Section 3.1)
· RQ2: How do transparency and assessability relate to each other and usefulness? (Section 3.2)
· RQ3: Does the interpretability of search result summary influence click decisions? (Section 3.3)
When reporting results, we use ns for "not significant at 0.05 level" and , , and  for p < 0.05, 0.01, and 0.001, respectively.
3.1 Interpretability and Search Task
Participants rated the search result summaries in our experiments (returned by Bing API) as highly transparent and assessable.
Figure 2 plots the distribution of the collected judgments. 87% of the judged summaries received a transparency rating as high as 4 (44.2%) or 5 (42.7%). 74% of the judged summaries have an assessability rating of 4 (43.5%) or 5 (30.4%). This shows that users considered top-ranked result summaries from current web search engines as highly transparent and assessable. However, it remains unclear if users' perceptions of the two properties are accurate.
3.2 Interpretability, Relevance, and Usefulness
The collected transparency and assessability judgments positively correlate with each other. They also positively correlate with usefulness, but with different strengths.
Table 3 reports the Spearman's correlation of the three judgments among all the assessed search result summaries (N = 1, 252). We observed a moderate positive correlation between transparency and assessability ( = 0.483, p < 0.001), suggesting possible connections between the two aspects of interpretability. Both transparency and assessability are also positively correlated with usefulness (but transparency has a relatively stronger correlation), suggesting possible connections between interpretability and search result quality.
Figure 3 and Figure 4 further disclose some details of the correlation between each pair of judgments. We divided the judged summaries into groups based on transparency ratings and assessability ratings ("low" for  3, "med" for 4, and "high" for 5). Figure 3 and Figure 4 report the mean and standard error of ratings for different group of summaries.
As Figure 4 shows, the differences of the "high" assessability group with the other two ("med" and "low") are much larger than the differences between "low" and "med" groups (although all the differences are statistically significant at 0.001 level). In contrast, we observed consistent clear differences in usefulness ratings between summaries with "high", "med", and "low" transparency ratings in

Figure 3: Comparison of summaries with low (N = 163), medium (N = 554), and high (N = 535) transparency levels.
Figure 4: Comparison of summaries with low (N = 326), medium (N = 545), and high (N = 381) assessability levels.
Figure 3. This further explains the stronger correlation between transparency and usefulness comparing to other pairs of judgments.
3.3 Interpretability and Click Behavior
We continue to examine the relationship between interpretability and click decisions. We only consider summary judgments for clicked results and the "possibly viewed" results (N = 819).
Figure 5 plots the "click rates" for summaries with different levels of transparency and assessability. Here the click rate is calculated as: # judged and clicked summaries / # judged summaries. Figure 5 suggests some possible connections between the two interpretability measures and click decisions. However, it remains unclear whether the different click rates are simply because the two interpretability measures are correlated with usefulness (a widely examined factor for click decision).
We further performed a logistic regression analysis for users' click decisions among the clicked and "possibly viewed" results. We used a multilevel regression model because the data violates the independence assumption for regular logistic regression--we have multiple judgments within a session and a user can perform

991

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Figure 5: Click rates for summaries with different levels of transparency and assessability (only consider clicked results and "possibly viewed" results).
multiple sessions. We model user and session as random effects and examine the list of variables in Table 5 as fix effects. The list of independent variables of interests included: · Interpretability judgments ­ transparency and assessability. · Usefulness [5, 6, 10] ­ it is widely assumed that users would click
on a result link if the summary looks useful. This is also the fundamental basis for using click as implicit feedback. · The rank of the summary on the original SERP ­ Joachim et al. [7] hypothesized that users are more likely to click on top-ranked results regardless of relevance/usefulness due to a trust bias. · The number of query terms matched in the summary ­ Both Yue et al. [17] and Clarke et al. [2] examined the attractiveness bias of click behavior. Here we use keyword matching as measures for attractiveness. We separately look into the title, URL, and snippet of summaries. We examined results using different text preprocessing methods and found they do not much influence the conclusions. The reported results used the Krovetz stemming and removed stop words (the INQUERY stop word list). Table 4 reports the results of the regression analysis. Unsurprisingly, we observed a significant positive effect of usefulness on click decisions (b = 0.547, p < 0.001). Among the three variables for measuring attractiveness of a summary, the number of query terms in URL also shows a significant positive effect on click (b = 0.186, p < 0.05). We did not observe any significant effect of rank on click decisions, probably because of our selection of results. In addition to a list of widely examined factors, we have also observed significant positive effects of both transparency (b = 0.419, p < 0.01) and assessability (b = 0.388, p < 0.01) on click decisions. This further confirms that transparency and assessability are salient factors influencing search engine users' click decisions. It also helps clarify that the differences of click rates observed in Figure 5 are less likely due to the correlation of transparency and assessability with usefulness.
4 CONCLUSION
The interpretability of artificial intelligence systems has attracted a lot of attention these days. We believe IR system is one of the earliest to provide interpretable techniques to users--search result summary informs users why a result was retrieved and to which extent the result would be useful (after opening the link). We explicitly evaluate this classic, important, yet neglected interpretable

Table 4: Multilevel regression: click as dependent variable.

Independent Variables Rank on the SERP Transparency Assessability Usefulness # query terms in title # query terms in URL # query terms in snipepet

Estimate Std. Error Sig.

0.046

0.04

0.419

0.16



0.388

0.12



0.547

0.11



0.065

0.07

0.186

0.08



-0.051

0.03

technique in IR systems through a lab user study. Our findings are
illuminating in several different ways:
First, our results suggest that search result summary plays an
important role in explaining system's decisions (the retrieval of a
particular result) and outputs (the usefulness of a result), as showed
from the high transparency and assessability ratings by users.
Second, our results disclose a new salient factor--the interpretabil-
ity of search result summary--influencing users' click decisions.
This suggests search engines can improve the interpretability of
search results to optimize users' click decisions. Another important
implication is that click models may also need to take into account
interpretability to better model click data.
Third, we also recommend that new explainable techniques for
search engines and search results should be fully compared with
existing query-biased search result summary.
REFERENCES
[1] B. Carterette, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: The TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016.
[2] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. White. The influence of caption features on clickthrough patterns in web search. In SIGIR '07, pages 135­142, 2007.
[3] F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608, 2017.
[4] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. ACM Computing Surveys, 51(5):93:1­93:42, 2019.
[5] J. Jiang, D. He, and J. Allan. Comparing in situ and multidimensional relevance judgments. In SIGIR '17, pages 405­414, 2017.
[6] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017.
[7] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05, pages 154­161, 2005.
[8] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Information Processing & Management, 44(6):1822­1837, 2008.
[9] Z. C. Lipton. The mythos of model interpretability. In 2016 ICML Workshop on Human Interpretability in Machine Learning, pages 96­100, 2016.
[10] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016.
[11] T. Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267:1­38, 2019.
[12] M. T. Ribeiro, S. Singh, and C. Guestrin. "Why should I trust you?": Explaining the predictions of any classifier. In KDD '16, pages 1135­1144, 2016.
[13] M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic explanations. In AAAI '18, pages 1527­1535, 2018.
[14] J. Singh and A. Anand. Posthoc interpretability of learning to rank models using secondary training data. In 2018 SIGIR Workshop on ExplainAble Recommendation and Search, 2018.
[15] J. Singh and A. Anand. EXS: Explainable search using local model agnostic interpretability. In WSDM '19, pages 770­773, 2019.
[16] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In SIGIR '98, pages 2­10, 1998.
[17] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: Examining result attractiveness as a source of presentation bias in clickthrough data. In WWW '10, pages 1011­1018, 2010.

992

Short Research Papers 2A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

A study on the Interpretability of Neural Retrieval Models using DeepSHAP

Zeon Trevor Fernando
L3S Research Center Hannover, Germany
fernando@l3s.de

Jaspreet Singh
L3S Research Center Hannover, Germany
singh@l3s.de

Avishek Anand
L3S Research Center Hannover, Germany
anand@l3s.de

ABSTRACT
A recent trend in IR has been the usage of neural networks to learn retrieval models for text based adhoc search. While various approaches and architectures have yielded significantly better performance than traditional retrieval models such as BM25, it is still difficult to understand exactly why a document is relevant to a query. In the ML community several approaches for explaining decisions made by deep neural networks have been proposed ­ including DeepSHAP which modifies the DeepLift algorithm to estimate the relative importance (shapley values) of input features for a given decision by comparing the activations in the network for a given image against the activations caused by a reference input. In image classification, the reference input tends to be a plain black image. While DeepSHAP has been well studied for image classification tasks, it remains to be seen how we can adapt it to explain the output of Neural Retrieval Models (NRMs). In particular, what is a good "black" image in the context of IR? In this paper we explored various reference input document construction techniques. Additionally, we compared the explanations generated by DeepSHAP to LIME (a model agnostic approach) and found that the explanations differ considerably. Our study raises concerns regarding the robustness and accuracy of explanations produced for NRMs. With this paper we aim to shed light on interesting problems surrounding interpretability in NRMs and highlight areas of future work.
ACM Reference Format: Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval Models using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184. 3331312
1 INTRODUCTION
Deep neural networks have achieved state of the art results in several NLP and computer vision tasks in the last decade. Along with this spurt in performance has come a new wave of approaches trying to explain decisions made by these complex machine learning models. Explainablilty and interpretability are key to deploying
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331312

NNs in the wild and having them work in tandem with humans. Explanations can help debug models, determine training data bias and understand decisions made in simpler terms in order to foster trust. Recently in IR, models such as DRMM [5], MatchPyramid [10], PACRR-DRMM [8] and others have shown great promise in ranking for adhoc text retrieval. While these models do improve state-ofthe-art on certain benchmarks, it is sometimes hard to understand why exactly these models are performing better. With the increased scrutiny on automated decision making systems, including search engines, it is vital to be able to explain decisions made. In IR however, little to no work has been done on trying to explain the output of complex neural ranking models.
In the ML community, several post-hoc non-intrusive methods have been suggested recently which enable us to train highly accurate and complex models while also being able to get a sense of their rationale. One of the more popular approaches to producing explanations is to determine the input feature attributions for a given instance and it's prediction according to a given model. The output of such a method is typically visualized as a heat map over the input words/pixels. Several approaches have been proposed in this direction for image and text classification but their applicability to adhoc text retrieval and ranking remains unexplored. In this paper we study the applicability of one such method designed specifically for neural networks ­ DeepSHAP [7], to explain the output of 3 different neural retrieval models. DeepSHAP is a modification of the DeepLift [15] algorithm to efficiently estimate the shapley values over the input feature space for a given instance. The shapley value is a term coined by Shapley [14] in cooperative game theory to refer to the contribution of a feature in a prediction. More specifically, shapley values explain the contribution of an input feature towards the difference in the prediction made vs the average prediction value.
The objective of our work is to utilize DeepSHAP to explain NRMs which should ideally be a trivial pursuit since they are standard neural networks. However, in our experiments, we found that DeepSHAP's explanations are highly dependent on a reference input which is used to compute the average prediction. This is inline with recent work that suggests approaches like DeepLIFT lack robustness [4]. In this work, we ponder on the question, what makes a good reference input distribution for neural rankers? In computer vision, a plain black image is used as the reference input but what is the document equivalent of such an image in IR? Furthermore, we found that explanations produced by the model introspective DeepSHAP are considerably different from the model agnostic approach ­ LIME [12]. Although both models produce local explanations, the variability is concerning.

1005

Short Research Papers 2A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

2 RELATED WORK
There are two main approaches to interpretability in machine learning models: model agnostic and model introspective approaches. Model agnostic approaches [12, 13] generate post-hoc explanations for the original model by treating it as a black box by learning an interpretable model on the output of the model or by perturbing the inputs or both. Model introspective approaches on one hand include "interpretable" models such as decision trees [6], attentionbased networks [20], and sparse linear models [19] where there is a possibility to inspect individual model components (path in a decision tree, feature weights in linear models) to generate useful explanations. On the other hand, there are gradient-based methods like [16] that generates attributions by considering the partial derivative of the output with respect to the input features. Following this, there were many works [1, 2, 7, 15] that generate attributions by inspecting the neural network architectures.
Interpretability in ranking models Recently there have been few works focused on interpretability [17, 18] and diagnosis of neural IR models [9, 11]. In the diagnostic approaches, they use the formal retrieval constraints ("axioms") defined for traditional retrieval models to find the differences between neural IR and learningto-rank approaches with hand-crafted features through a manual error analysis [9] or build diagnostic datasets based on the axioms to empirically analyse these models [11]. In [18] they built a explainable search system (EXS) that adapts a local model agnostic interpretability approach (LIME [12]) to explain the relevance of a document for a query for various neural IR models. In [17] they propose an approach that understands the query intent encoded by NRMs by learning a simple ranking model with an expanded query that approximates the original ranking.
To the best of our knowledge, this is the first work that looks at model introspective interpretability specifically for NRMs.
3 DEEPSHAP FOR IR
DeepSHAP is a local model-introspective interpretability method to approximate the shapley values using DeepLIFT [15]. DeepLIFT explains the difference in output/prediction from some `reference' output with respect to the difference of the input (to explain) from a `reference' input. The authors define a function analogous to partial derivatives to compute the feature importance scores and use the chain rule to backpropagate the activation differences from the output layer to the original input. The choice of reference input depends on domain specific knowledge; For example, in digit classification task on the MNIST dataset, they use a reference input of all-zeros as that is the background of the images. For object detection in images, a plain black image is often used.
In the context of IR, DeepSHAP can be used to explain why a document is relevant to query (according to a given NRM) by computing the shapley values for words in the document. The words with high shapley values indicate that they are important towards this prediction of relevance. However, to accurately compute the shapley values using DeepSHAP a reference input is needed. What makes a good background image in the context of IR?
Unlike classification tasks, in ranking we have at least 2 inputs which are in most cases the query and document tokens. In this work we fix the reference input for the query to be same as that of

the query-document instance to be explained and experiment with various reference inputs for the document. The intuition behind doing so is to gain an average reference output in the locality of the query.
The various document reference inputs that we considered in our experiments are:
OOV The reference document consists of `OOV' tokens. For, DRMM and MatchPyramid models the embedding vector for `OOV' comprises of all-zeros which is similar to the background image used for MNIST. But for PACRR-DRMM, it is the average of all the embedding vectors in the vocabulary.
IDF lowest The reference document is constructed by sampling words with low IDF scores. These words are generally stopwords or words that are similar to stop-words so they should, in general, be irrelevant to the query.
QL lowest The reference document comprises of sampled words with low query-likelihood scores that are derived from a language model of the top-1000 documents.
COLLECTION rand doc The reference document is randomly sampled from the rest of the collection minus the top-1000 documents retrieved for the query.
TOPK LIST rand doc from bottom The reference document is randomly sampled from the bottom of the top-1000 documents retrieved.
These variants were designed based on the intuition that the reference input document would comprise of words that are irrelevant to the query and thus DeepSHAP should be able to pick the most important terms from the input document that explain relevance to the query.
4 EXPERIMENTAL SETUP
In our experiments, we aim to answer the following research questions:
· Are DeepSHAP explanations sensitive to the type of reference input in the case of NRMs?
· Can we determine which reference input produces the most accurate local explanation?
To this end, we describe the experimental setup we used to address these questions. We describe the various NRM's we considered and how we used LIME to evaluate the correctness of explanations produced by DeepSHAP.
4.1 Neural Retrieval Models
DRMM [5] This model uses a query-document term matching count histogram as input into a feed forward neural network (MLP) to output a relevance score along with a gating mechanism that learns query term weights.
MatchPyramid [10] This model uses a query-document interaction matrix as input to a 2D CNN to extract matching patterns. The output is then fed into a MLP to get a relevance score.
PACRR-DRMM [8] This model creates a query-document interaction matrix that is fed into multiple 2D CNNs with different kernel sizes to extract n-gram matches. Then, after k-max pooling across each q-term, the document aware q-term encodings are fed into a MLP, like in DRMM to obtain a relevance score.

1006

Short Research Papers 2A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Figure 1: Confusion matrices of various DeepSHAP background document methods comparing Jaccard similarities.

4.2 Evaluation
To conduct the experiments, we used the Robust04 test collection from TREC. We used Lucene to index and retrieve documents. Next, we trained the NRMs using their implementations in MatchZoo1 [3]. All the hyparameters were tuned using the same experimental setup as described in the respective papers. We chose to study explanations for the distinguished set of hard topics2 (50) from the TREC Robust Track 2004. We generate the explanations from LIME and DeepSHAP for the top-3 documents retrieved for each query and use only these for our quantitative experiments.
Evaluating explanations Since no ground truth explanations are available for a neural model, we use LIME based explanations as a proxy. We found that it can approximate the locality of a query document pair well, using a simple linear model that achieved an accuracy of over 90% across all NRMs considered in our experiments. To produce the explanations from LIME we used the implementation found in 3 along with the score-based modification suggested in [18]. The primary parameters for training a LIME explanation model are the number of perturbed samples to be considered and the number of words for the explanation. The number of perturbed samples is set to 5000 and the number of words is varied based on the experiment. We used the DeepSHAP implementation provided here 4. Note that we ignore the polarity of the explanation terms provided by both methods in our comparison since the semantics behind the polarities in LIME and DeepSHAP are different. We are more interested in the terms chosen as the explanations in both cases.
5 RESULTS AND DISCUSSION
5.1 Effect of reference input document
Figure 1 illustrates the overlap between the explanation terms produced when varying the reference input. Immediately we observe that the overlap between explanations produced is low; below 50% in most cases and consistently across all NRMs. Each reference input method has its own distinct semantics and this is reflected by the low overlap scores. We also find that there is no consistent trend across NRMs. For MatchPyramid, OOV and QL have highest overlap whereas for PACRR-DRMM its OOV and COL that have highest overlap even though both models share similarities in input
1 https://github.com/N TMC- Community/MatchZoo/tree/1.0 2 https://trec.nist.gov/data/robust/04.guidelines.html 3 https://github.com/marcotcr/lime 4 https://github.com/slundberg/shap

representation and parts of the model architecture. Table 3 shows explanations for DRMM across variants. Once again we see how the explanations can differ significantly if we are not careful in selecting the reference input. For IR, finding the background image seems to be a much harder question.
Our results show how explanations are highly sensitive to the reference input for NRMs chosen in our experiments. This is also indication that a single reference input method may not be the best for every NRM.
5.2 Accuracy of reference input methods
To help identify which reference input method is most accurate in explaining a given query-document pair, we compared the LIME explanations for the same against it's corresponding DeepSHAP explanations. In general we found that DeepSHAP produces more explanation terms whereas LIME's L1 regularizer constrains the explanations to only the most important terms. Additionally, the discrepancy between the explanations can be attributed to LIME being purely local, whereas DeepSHAP has some global context since it looks at activations for the whole network which may have captured some global patterns. Hence, to estimate which reference input surfaces the most `ground truth' explanation terms we only computed recall at top 50 and 100 (by shapley value magnitude) DeepSHAP explanation terms (in Table 1).
The first interesting insight is that some NRMs are easier to explain whereas others are more difficult. PACRR-DRMM consistently has a recall less than 0.7, whereas the DeepSHAP explanations of DRMM effectively capture almost all of the LIME explanation terms. When comparing reference input variants within each NRM we find that there is no consistent winner. For DRMM, QL is the best which indicates that sampling terms which are relatively generic for this query in particular is a better `background image' than sampling generic words from the collection (IDF).
In the case of MatchPyramid, TOPK LIST is the worst performing but it is more difficult to distinguish between the approaches here. The best approach surprisingly is OOV. This can be attributed to how MatchPyramid treats OOV terms. The OOV token is represented by an all-zeros embedding vector that is used for padding the input interaction matrix whereas in DRMM, OOV tokens are filtered out. These preprocessing considerations prove to be crucial when determining the right input reference. Moving on to PACRRDRMM, we once again find that QL is the best method even though DeepSHAP struggles to find all the LIME terms.

1007

Short Research Papers 2A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Comparison of recall measures at top-k (50, 100) terms from DeepSHAP without polarity against the top-k (10, 20, 30) ground-truth terms from LIME for ROBUST04 difficult queries (50)

SHAP variants OOV IDF QL COLLECTION TOPK LIST.

DRMM

MatchPyramid

PACRR-DRMM

top-10

top-20

top-30

top-10

top-20

top-30

top-10

top-20

top-30

recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100

0.789 0.905 0.672 0.845 0.615 0.812 0.793 0.843 0.656 0.726 0.566 0.640 0.582 0.582 0.388 0.388 0.299 0.299

0.830 0.917 0.723 0.871 0.658 0.841 0.795 0.832 0.653 0.711 0.565 0.633 0.633 0.633 0.446 0.446 0.362 0.362

0.894 0.955 0.754 0.892 0.670 0.856 0.765 0.821 0.638 0.711 0.556 0.636 0.643 0.643 0.462 0.462 0.367 0.367

0.760 0.881 0.673 0.841 0.620 0.815 0.783 0.824 0.639 0.709 0.552 0.630 0.621 0.621 0.429 0.429 0.343 0.343

0.639 0.821 0.606 0.794 0.578 0.788 0.759 0.811 0.624 0.702 0.545 0.627 0.625 0.625 0.425 0.425 0.340 0.340

Table 2: Comparison of mean squared error (MSE) and accuracy (ACC) of LIME's linear model across various NRMs. Low MSE and high accuracy shows that it is able to fit and generalize in the query-document locality.

NRM
DRMM MatchPyramid PACRR-DRMM

TRAIN MSE
0.00631 0.01827 0.00165

Linear Regression TEST MSE TRAIN ACC

0.00633 0.01839 0.00160

0.92662 0.90367 0.98857

TEST ACC
0.92654 0.90387 0.98980

Table 3: An example of words selected by LIME and SHAP methods for the query `cult lifestyles' and document `FBIS3-843' which is about clashes between cult members and student union's activists at a university in Nigeria. Words unique to a particular explanation method are highlighted in bold.

LIME

OOV

IDF

QL

COL.

TOPK

cult style followers elite saloon student home members march september

cult followers
black fraternities degenerate
sons academic american
tried household

cult style followers suspects belong reappearing household black fraternities degenerate

cult style elite saloon final march friday september arms closed

cult black fraternities degenerate sons followers style home household avoid

cult numbers english college university fallouts buccaneers feudings activists troubles

6 CONCLUSION AND FUTURE WORK
In this paper we suggested several reference input methods for DeepSHAP that take into account the unique semantics of document ranking and relevance in IR. Through quantitative experiments we found that it is indeed sensitive to the reference input. The distinct lack of overlap in most cases was surprising but in line with recent works on the lack of robustness in interpretability approaches. We also tried to evaluate which reference method is more accurate by comparing against LIME. Here we found that reference input method selection is highly dependent on the model at hand. We believe that this work exposes new problems when dealing with model introspective interpretability for NRMs. A worthwhile endeavor will be to investigate new approaches that explicitly take into account the discreteness of text and the model's preprocessing choices when generating explanations.

This work was supported by the Amazon research award on
`Interpretability of Neural Rankers'.
REFERENCES
[1] Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2017. "What is relevant in a text document?": An interpretable machine learning approach. PLOS ONE 12 (2017), 1­23.
[2] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PLOS ONE 10 (2015), 1­46.
[3] Yixing Fan, Liang Pang, Jianpeng Hou, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2017. MatchZoo: A Toolkit for Deep Text Matching. (2017). arXiv:1707.07270
[4] Amirata Ghorbani, Abubakar Abid, and James Y. Zou. 2019. Interpretation of Neural Networks is Fragile. In AAAI '19.
[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM '16. ACM, 55­64.
[6] Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. 2015. Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9, 3 (2015), 1350­1371.
[7] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30. 4765­4774.
[8] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-Query Interactions. In EMNLP '18.
[9] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2017. A Deep Investigation of Deep IR Models. arXiv preprint (2017). arXiv:1707.07700
[10] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching As Image Recognition. In AAAI'16. 2793­2799.
[11] Daan Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models. In ECIR '19. 489­503.
[12] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In KDD '16. ACM, 1135­1144.
[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: HighPrecision Model-Agnostic Explanations. In AAAI '18.
[14] Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory of Games 2, 28 (1953), 307­317.
[15] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating Activation Differences. arXiv preprint (2017). arXiv:1704.02685
[16] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR Workshop (2014).
[17] Jaspreet Singh and Avishek Anand. 2018. Interpreting search result rankings through intent modeling. arXiv preprint (2018). arXiv:1809.05190
[18] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (WSDM '19). ACM, 770­773.
[19] Berk Ustun and Cynthia Rudin. 2016. Supersparse Linear Integer Models for Optimized Medical Scoring Systems. Machine Learning 102, 3 (2016), 349­391.
[20] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In International Conference on Machine Learning - Volume 37 (ICML'15). 2048­2057.

1008

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

A New Perspective on Score Standardization

Julián Urbano
Delft University of Technology The Netherlands
urbano.julian@gmail.com

Harlley Lima
Delft University of Technology The Netherlands
h.a.delima@tudelft.nl

Alan Hanjalic
Delft University of Technology The Netherlands
a.hanjalic@tudelft.nl

ABSTRACT
In test collection based evaluation of IR systems, score standardization has been proposed to compare systems across collections and minimize the effect of outlier runs on specific topics. The underlying idea is to account for the difficulty of topics, so that systems are scored relative to it. Webber et al. first proposed standardization through a non-linear transformation with the standard normal distribution, and recently Sakai proposed a simple linear transformation. In this paper, we show that both approaches are actually special cases of a simple standardization which assumes specific distributions for the per-topic scores. From this viewpoint, we argue that a transformation based on the empirical distribution is the most appropriate choice for this kind of standardization. Through a series of experiments on TREC data, we show the benefits of our proposal in terms of score stability and statistical test behavior.
KEYWORDS
Evaluation, test collection, score standardization, statistical testing
ACM Reference Format: Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. A New Perspective on Score Standardization. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331315
1 INTRODUCTION
In the traditional Cranfield paradigm for test collection based evaluation in Information Retrieval (IR), systems are evaluated and compared by assessing their effectiveness on the set of topics contained in a collection. Specifically, an effectiveness measure like Average Precision is used to score every system with every topic, and the per-system mean scores over topics are often used as the single indicator of performance to rank systems [4]. It is well known in the IR literature that this paradigm does not allow to compare the performance of systems tested on different collections. The main reason for this is the very large variability we find in topic difficulty [1, 6, 8]. A system with a good score on one collection may very well achieve a low score on another. Even when comparing systems using the same collection, not all topics contribute equally to the final score because of their differences in difficulty (see for
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331315

instance the bottom-left plot in Figure 1). Therefore, the observed

differences in mean scores may be disproportionately due to a few

topics in the collection [2, 9].

To mitigate this problem, Webber et al. [9] proposed a two-step

standardization process to look at scores relative to the difficulty of

the topic. First, given a raw effectiveness score x of some system

on some topic, a traditional z-score is computed

x -µ z=  ,

(1)

where µ and  are the mean and standard deviation of the system

scores for the topic. The effect is twofold: whether the topic is easy

or hard (high or low µ), the distribution of z-scores is centered at

zero; and whether systems perform similarly for the topic or not

(low or high  ), the z-scores have unit variance. Thanks to this first

step, all topics contribute equally to the final scores.

The second step is a transformation of the z-score so that the final

standardized score is bounded between 0 and 1, as is customary

in IR measures. Webber et al. [9] propose to use the cumulative

distribution function (cdf ) of the standard normal distribution,

which naturally maps z-scores on to the unit interval:

= (z).

(2)

Recently, Sakai [3] proposed a simple linear transformation of the z-score instead of the non-linear transformation applied by :

= Az + B.

(3)

On the grounds of Chebyshev's inequality, they further suggested A= 0.15 and B = 0.5 so that at least 89% of the scores will fall within [0.05, 0.95]. Furthermore, and to ensure that standardized scores always stay within the unit interval, they proposed to simply censor
between 0 and 1, computing = max(min(1, Az + B), 0) in reality. In this paper we show that the standardizations by Webber et al. [9] and Sakai [3] are actually special cases of a general class of standardizations consisting in assuming a specific distribution for the per-topic scores, and that they differ only in what distribution they assume. From this new perspective, we argue that the empirical distribution is actually the most appropriate choice because of its properties. We also carry out two experiments on TREC data that show how our proposal behaves better than both raw scores and the previous standardization schemes.

2 SCORE STANDARDIZATION
Let F be the distribution of scores by some population of systems on some particular topic and according to some specific measure like AP. If we knew this distribution, we could standardize a raw score x simply by computing = F (x) = P(X  x), which naturally bounds
between 0 and 1. The reasoning is that the cdf actually tells us where x is with respect to the rest of scores that one may expect for the topic, precisely computing the fraction of systems with lower

1061

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

1062

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

with slight deviations if censoring is needed. In the case of E-std, we see nearly constant mean and variance. This is achieved by design, because, in general, if X  F , then Y = F (X ) follows a standard uniform. Therefore, E-std produces standardized scores that are uniformly distributed, ensuring = 0.5 and s = 1/12.
The point still remains that µ and  are also unknown. The way around this limitation is to estimate them from a previous set of systems (called standardization systems by [3, 9]). Thus, given the scores X1, . . . , Xn of these systems, the estimates are the per-topic sample mean µ^ =X and standard deviation ^ =sX . For E-std, these are precisely the data used in eq. (7) to standardize. In principle, these standardization systems should represent the system population of interest, which ultimately determines the topic difficulty through the per-topic distributions. In our view, the most reasonable choice would be the state of the art systems, which in a TREC collection are arguably the set of systems participating in the track.
3 EXPERIMENTS
This section reports on two experiments to assess the effect of standardization. In the first one, we consider system comparisons using the same test collection (within-collection), while in the second one we consider comparisons between systems evaluated on different collections (between-collection). Comparisons will be made between results produced by the raw scores, N-std, U-std and E-std. For completeness, we also evaluate the standardization scheme that simply computes the z-score as in eq. (1) and therefore produces unbounded scores. This scheme is called z-std.
The data used in our experiments are the TREC 2004 Robust (RB) and TREC 2006 Terabyte (TB) collections. The RB data contains 110 systems evaluated on the 99 TREC 2003­2004 Robust topics. The TB data contains 61 systems on the 149 TREC 2004­2006 Terabyte topics. In terms of measures, we use AP and nDCG.
3.1 Within-Collection Comparisons
In order to investigate the effect of standardization on withincollection comparisons, we proceed as follows. We randomly sample 50 topics from the full set and compute the raw scores and the standardized scores as per each of the standardization schemes. From these data we compute three statistics. First, we compute the correlation between the ranking of systems by raw scores and the ranking by standardized scores, using Kendall's  and Yilmaz's ap [10]1. A high correlation indicates that the standardized scores are not much different from the raw scores, so in principle we look for lower coefficients. The third indicator evaluates the statistical power of the evaluation. In particular, we run a 2-tailed paired t-test between every pair of systems and, under the assumption that the null hypothesis is indeed false, look for schemes that maximize power. The process is repeated 10,000 times with both the RB and TB datasets, on both AP and nDCG.
Figure 2 shows the results for a selection of collection-measure combinations2. The two plots in the first row show the distributions of  correlations. As expected, U-std and z-std perform very similarly because the former is simply a linear transformation of
1In particular, we compute b and ap,b to deal with tied systems. See [5] for details. 2All plots, along with data and code to reproduce results, are available from https://github.com/julian-urbano/sigir2019-standardization.

N-std U-std z-std E-std

Robust - AP

20

Terabyte - AP
N-std U-std z-std E-std

Density

0 5 10

Density 0 5 15 25

Density 0 5 10 15

Density 0 5 10 15 20 25

0.86

0.90

0.94



Robust - nDCG

N-std U-std z-std E-std

0.98

0.86

0.90

0.94



Terabyte - nDCG

N-std U-std z-std E-std

0.98

0.75

0.80

0.85 0.90 ap

0.95

1.00

Robust - AP

0.75

0.80

0.85 0.90 ap

0.95

1.00

Terabyte - nDCG

Power 0.55 0.65 0.75

Power 0.45 0.55 0.65

0.001

raw N-std U-std z-std E-std

0.005

0.020 0.050

Significance level 

0.001

raw N-std U-std z-std E-std

0.005

0.020 0.050

Significance level 

Figure 2: Within-collection comparisons. First row:  correlation between rankings of systems with raw and standardized scores (lower is better); rugs mark the means. Second row: ap correlation (lower is better). Third row: power of paired t-tests at various  levels (higher is better).

the latter; differences come from the necessity to censor outliers in U-std. Indeed, because they are both a linear transformation of the raw scores, they produce the most similar rankings. N-std results in slightly lower correlations, but E-std sets itself clearly apart from the others, yielding significantly lower  scores. The plots in the second row show even clearer differences in terms of ap . We see that U-std and z-std are almost identical, but more importantly we see that N-std and E-std are even further away, likely because they eliminate outliers that could affect the top ranked systems.
The two plots in the last row show statistical power for a range of significance levels. We can first observe that all standardization schemes achieve higher power than the raw scores, showing a clear advantage of the standardization principle. Once again, U-std and z-std perform nearly identically, and both are outperformed by N-std and, specially, E-std.
3.2 Between-Collection Comparisons
Here we study how standardization affects between-collection comparisons. In this case, we randomly sample two disjoint subsets of 50 topics each and compute raw and standardized scores on both subsets. Because topics are sampled from the full set, both results can be regarded as coming from two different collections having different topics from the same population. In this case we are not interested in how standardized scores compare to raw scores, but rather on how stable the results are between both sets of topics, so we compute the following four statistics. First, we compute the  and ap correlations between both rankings. We seek high correlations, indicating high score stability across topic sets. Third, for

1063

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

every system we run a 2-tailed unpaired t-test between both sets. By definition, the null hypothesis is true because we are comparing a system to itself simply on a different sample, so we expect as many Type I errors as the significance level . Finally, we run another test between every system on one collection and every other system on the other collection, looking again to maximize statistical power under the assumption that all systems are different and thus null hypotheses are false. As before, this process is repeated 10,000 times with both the RB and TB datasets, on both AP and nDCG.
Figure 3 shows the results for a selection of collection-measure combinations. The plots in the first two rows show that standardization generally produces more stable results, as evidenced by raw scores yielding lower correlations. U-std and z-std perform very similarly once again, and E-std generally outperforms the others, producing slightly more stable comparisons between collections. An exception can be noticed for ap on the TB dataset, which requires further investigation.
The third row of plots show the Type I error rates. We can see that all scoring schemes behave just as expected by the significance level . This evidences on the one hand the robustness of the t-test [7] (recall the diversity of distributions from the boxplots in Figure 1), and on the other hand that standardization neither harms nor helps from the point of view of Type I errors (this is rather a characteristic of the test). Finally, the last two plots show the power achieved by the tests when comparing different systems. Here we first notice that all standardization schemes are substantially more powerful than the raw scores, achieving about twice as much power. While the results are very similar in the RB set, we see clear differences in the TB set, with E-std once again outperforming the other schemes.
4 CONCLUSIONS
In this paper we revisit the problem of score standardization to make IR evaluation robust to variations in topic difficulty. We introduced a new scheme for standardization based on the distributions of pertopic scores, and showed that previous methods by Webber et al. [9] and Sakai [3] are special cases of this scheme. From this point of view we propose the empirical distribution as an alternative, and discuss a number of points that highlight its superiority.
In experiments with TREC data, we showed that, even though the raw and standardized rankings are the same topic by topic, the rankings by mean scores may differ considerably. In addition, standardization achieves higher statistical power. Thus, standardization offers an alternative and quite different view on system comparisons. However, it is important to note that these comparisons are made on a different scale altogether, so one may not just use standardized scores to make statements about raw scores. Nonetheless, standardization with the empirical distribution is arguably more faithful to our notion of relative system effectiveness.
Future work will follow three main lines. First, we will study additional datasets and measures for generality. However, because TREC collections are usually limited to 50 topics, we also plan on using recent simulation methods so that we can analyze more data [7]. Finally, we will study the stability of E-std for varying numbers of systems. This is interesting because, even though the empirical function converges to the true distribution, it is unclear how large the set of systems needs to be for the results to be stable.

Density

Density 0 2 4 6 8 10

Robust - nDCG
raw N-std U-std z-std E-std

10 15

Terabyte - nDCG
raw N-std U-std z-std E-std

5

0

0.65

0.70 0.75 0.80 0.85 
Robust - AP
raw N-std U-std z-std E-std

0.90

0.95

0.65

0.70 0.75 0.80 0.85 
Terabyte - AP
raw N-std U-std z-std E-std

0.90

0.95

Density 02468

Density 02468

0.050

0.5

0.6

0.7

0.8

0.9

0.5

0.6

0.7

0.8

0.9

ap

ap

Robust - AP

Terabyte - AP

0.050

raw N-std U-std z-std E-std

raw N-std U-std z-std E-std

Type I error rate

Type I error rate

0.001 0.005

0.001 0.005

0.001

0.005

0.020

Significance level 

Robust - nDCG

0.050

0.001

0.005

0.020 0.050

Significance level 

Terabyte - nDCG

Power 0.4 0.5 0.6 0.7

Power 0.3 0.4 0.5 0.6

0.001

raw N-std U-std z-std E-std

0.005

0.020 0.050

Significance level 

0.001

raw N-std U-std z-std E-std

0.005

0.020 0.050

Significance level 

Figure 3: Between-collection comparisons. First row:  correlation between the rankings of systems produced by the two collections (higher is better); rugs mark the means. Second row: ap correlation (higher is better). Third row: Type I error rate of unpaired t-tests at various  levels (diagonal is better). Fourth row: statistical power (higher is better).

ACKNOWLEDGMENTS
Work carried out on the Dutch national e-infrastructure (SURF Cooperative) and funded by European Union's H2020 programme (770376-2 TROMPA). Eva.say( Hello World! );
REFERENCES
[1] D. Bodoff. 2008. Test Theory for Evaluating Reliability of IR Test Collections. Information Processing and Management 44, 3 (2008), 1117­1145.
[2] J. Guiver, S. Mizzaro, and S. Robertson. 2009. A Few Good Topics: Experiments in Topic Set Reduction for Retrieval Evaluation. ACM TOIS 27, 4 (2009), 1­26.
[3] T. Sakai. 2016. A Simple and Effective Approach to Score Standardization. In ACM ICTIR. 95­104.
[4] M. Sanderson. 2010. Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247­375.
[5] J. Urbano and M. Marrero. 2017. The Treatment of Ties in AP Correlation. In SIGIR ICTIR. 321­324.
[6] J. Urbano, M. Marrero, and D. Martín. 2013. On the Measurement of Test Collection Reliability. In ACM SIGIR. 393­402.
[7] J. Urbano and T. Nagler. 2018. Stochastic Simulation of Test Collections: Evaluation Scores. In ACM SIGIR.
[8] E. Voorhees. 2005. Overview of the TREC 2005 Robust Retrieval Track. In TREC. [9] W. Webber, A. Moffat, and J. Zobel. 2008. Score Standardization for Inter-
collection Comparison of Retrieval Systems. In AMC SIGIR. 51­58. [10] E. Yilmaz, J.A. Aslam, and S. Robertson. 2008. A New Rank Correlation Coefficient
for Information Retrieval. In AMC SIGIR. 587­594.

1064

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Content-Based Weak Supervision for Ad-Hoc Re-Ranking

Sean MacAvaney
IRLab, Georgetown University sean@ir.cs.georgetown.edu
Kai Hui
Amazon kaihuibj@amazon.com
ABSTRACT
One challenge with neural ranking is the need for a large amount of manually-labeled relevance judgments for training. In contrast with prior work, we examine the use of weak supervision sources for training that yield pseudo query-document pairs that already exhibit relevance (e.g., newswire headline-content pairs and encyclopedic heading-paragraph pairs). We also propose filtering techniques to eliminate training samples that are too far out of domain using two techniques: a heuristic-based approach and novel supervised filter that re-purposes a neural ranker. Using several leading neural ranking architectures and multiple weak supervision datasets, we show that these sources of training pairs are effective on their own (outperforming prior weak supervision techniques), and that filtering can further improve performance.
ACM Reference Format: Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. ContentBased Weak Supervision for Ad-Hoc Re-Ranking. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331316
1 INTRODUCTION
A lack of manual training data is a perennial problem in information retrieval [18]. To enable training supervised rankers for new domains, we propose a weak supervision approach based on pairs of text to train neural ranking models and a filtering technique to adapt the dataset to a given domain. Our approach eliminates the need for a query log or large amounts of manually-labeled indomain relevance judgments to train neural rankers, and exhibits stronger and more varied positive relevance signals than prior weak supervision work (which relies on BM25 for these signals).
Others have experimented with weak supervision for neural ranking (see Section 2.2). Our weak supervision approach differs from these approaches in a crucial way: we train neural rankers
Work conducted while the author was at the Max Planck Institute for Informatics.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331316

Andrew Yates
Max Planck Institute for Informatics ayates@mpi-inf.mpg.de
Ophir Frieder
IRLab, Georgetown University ophir@ir.cs.georgetown.edu
using datasets of text pairs that exhibit relevance, rather than using a heuristic to find pseudo-relevant documents for queries. For instance, the text pair from a newswire dataset consisting of an article's headline and its content exhibits an inherent sense of relevance because a headline often provides a concise representation of an article's content. To overcome possible domain differences between the training data and the target domain, we propose an approach to filter the training data using a small set of queries (templates) from the target domain. We evaluate two filters: an unsupervised heuristic and using the neural ranker itself as a discriminator.
We evaluate our approaches by training several leading neural ranking architectures on two sources of weak supervision text pairs. We show that our methods can significantly outperform various neural rankers when trained using a query log source (as proposed by [5]), the ranker when trained on a limited amount of manuallylabeled in-domain data (as one would encounter in a new domain), and well-tuned conventional baselines. In summary, we (1) address existing shortcomings of weak supervision to train neural rankers by using training sources from text pairs, (2) address limitations related to domain differences when training rankers on these sources using novel filtering techniques, and (3) demonstrate the effectiveness of our methods for ad-hoc retrieval when limited in-domain training data is available. Our code is public for validation and further comparisons.1
2 BACKGROUND AND RELATED WORK 2.1 Neural IR models
Ad-hoc retrieval systems rank documents according to their relevance to a given query. A neural IR model (nir) aims to measure the interaction between a query-document pair (q, d) with a real-value relevance score rel = nir(q, d). The model nir is trained to minimize pairwise loss between training triples consisting of a query q, relevant document d+, and non-relevant document d-. Neural retrieval models can be categorized as semantic matching models (which create dense query/document representations) or as relevance matching models (which compare query and document terms directly, often through a query-document similarity matrix). We focus on relevance matching models because they generally show better performance than semantic matching models. We test our approach on three leading neural rankers:
KNRM [16] uses Gaussian kernels applied to each individual similarity score and log-summed across the document dimension. A final dense learning-to-rank phase combines these features into a relevance score.
1
https://github.com/Georgetown- IR- Lab/neuir- weak- supervision

993

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Conv-KNRM [4] is a variant of KNRM which applies convolution filters of lengths 1­3 over word embeddings before building cross-matched (matching all kernel lengths with one another) similarity matrices. The rest of the ranking process is identical to KNRM.
PACRR [8] uses square convolutional kernels over the similarity matrix to capture soft n-gram matches. k-max pooling is applied to retain only the strongest signals for each query term, and signals are combined with a dense layer.
2.2 Weak supervision
In IR, weak supervision uses pseudo-relevant information to train a ranking model in place of human judgments. Early work on weak supervision for IR focused on training learning-to-rank models [2], using web anchor text [1] and microblog hashtags [3] for weak supervision. More recently, Dehghani et al. [5] proposed a weak supervision approach that makes use of the AOL query log and BM25 results as a source of training data. Aside from limitations surrounding the availability of query logs, their approach suffers from limitations of BM25 itself: it assumes that documents ranked higher by BM25 are more relevant to the query than documents ranked lower. Others have suggested using a similar approach, but using news headlines [9], also assuming relevance from BM25 rankings. Still others have employed a Generative Adversarial Network to build training samples [15], but this limits the generated data to the types of relevance found in the training samples, making it a complementary approach. In contrast, our approach uses freelyavailable text pairs that exhibit both a high quality and large size.
3 METHOD 3.1 Ranking- and content-based sources
Recall that pairwise training consists of a set of training triples, each consisting of a query q, relevant document d+, and non-relevant document d-. We describe two sources of weak supervision training data that replace human-generated relevance judgments: rankingbased and content-based training sources.
Ranking-based training sources, first proposed by [5], are defined by a collection of texts T , a collection of documents D, and an unsupervised ranking function R(q, d) (e.g., BM25). Training triples are generated as follows. Each text is treated as a query q  T . All documents in D are ranked using R(·), giving Dq . Relevant documents are sampled using a cutoff c+, and non-relevant documents are sampled using cutoff c-, such that d+  Dq [0 : c+] and d-  Dq [c+ : c-]. This source is referred to as ranking-based because the unsupervised ranker is the source of relevance.2
Content-based training sources are defined as a collection of text pairs P = {(a1, b1), (a2, b2), ..., (a |P |, b |P |)} and an unsupervised ranking function R(q, d) (e.g., BM25). The text pairs should be semantically related pairs of text, where the first element is similar to a query, and the second element is similar to a document in the target domain. For instance, they could be heading-content pairs of news articles (the headline describes the content of the article content). For a given text pair, a query and relevant document are
2Our formulation of ranking-based sources is slightly different than what was proposed by Dehghani et al. [5]: we use cutoff thresholds for positive and negative training samples, whereas they suggest using random pairs. Pilot studies we conducted showed that the threshold technique usually performs better.

selected (q, d+)  P. The non-relevant document is selected from the collection of documents in B = {b1, b2, ..., b |P | }. We employ R(·) to select challenging negative samples from Bq . A negative cutoff c- is employed, yielding negative document d-  Bq [0 : c-] - {d+}. We discard positive samples where d+ is not within this range to eliminate overtly non-relevant documents. This approach can yield documents relevant to q, but we assert that d+ is more relevant.
Although ranking-based and content-based training sources bear
some similarities, important differences remain. Content-based
sources use text pairs as a source of positive relevance, whereas
ranking-based sources use the unsupervised ranking. Furthermore,
content-based sources use documents from the pair's domain, not
the target domain. We hypothesize that the enhanced notion of rel-
evance that content-based sources gain from text pairs will improve
ranking performance across domains, and show this in Section 4.

3.2 Filter framework

We propose a filtering framework to overcome domain mismatch

that can exist between data found in a weak supervision training

source and data found in the target dataset. The framework consists
of a filter function FD (q, d) that determines the suitability of a given weak supervision query-document pair (q, d) to the domain D. All relevant training pairs (q, d+)  S for a weak supervision source S are ranked using FD (q, d+) and the cmax maximum pairs are chosen: SD = maxc(qm,dax+)S FD (q, d+). To tune FD (·) to domain D, a set of template pairs from the target domain are employed. The
set of pairs TD is assumed to be relevant in the given domain.3
We assert that these filters are easy to design and can have broad

coverage of ranking architectures. We present two implementations
of the filter framework: the kmax filter, and the Discriminator filter. k-Maximum Similarity (kmax) filter. This heuristic-based
filter consists of two components: a representation function rep(q, d) and a distance function dist(r1, r2). The representation function captures some matching signal between query q and document d

as a vector. Since many neural ranking models consider similarity

scores between terms in the query and document to perform soft term matching [4, 7, 8, 16], this filter selects the k maximum cosine

similarity scores between the word vectors of each query term and

all

terms

in

the

document:

maxk
d

j

d

sim(qi , dj )

:

qi



q.

Since neural models can capture local patterns (e.g., n-grams),

we use an aligned mean square error. The aligned MSE iterates over

possible configurations of elements in the representation by shifting

the position to find the alignment that yields the smallest distance.

In other words, it represents the minimum mean squared error

given all rotated configurations of the query. Based on the shift
operation and given two interaction representation matrices r1 and r2, the aligned distkmax (r1, r2) is defined as the minimum distance when shifting r1 for s  [1, |r1|). More formally: distkmax (r1, r2) = mins|r=11| MSE shift(r1, s), r2 .
Using these two functions, the filter is simply defined as the min-

imum distance between the representations of it and any template

pair from the target domain:

FD

(q,

d

)

=

min
(q , d  ) TD

dist

(r ep (q,

d ),

r ep (q

,

d

))

(1)

3
Templates do not require human judgments. We use sample queries and an unsupervised ranker to generate TD . Manual judgments can be used when available.

994

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Discriminator filter. A second approach to interaction filtering is to use the ranking architecture R itself. Rather than training R to distinguish different degrees of relevance, here we use R to train a model to distinguish between samples found in the weak supervision source and TD . This technique employs the same pairwise loss approach used for relevance training and is akin to the
discriminator found in generative adversarial networks. Pairs are
sampled uniformly from both templates and the weak supervision source. Once RD is trained, all weak supervision training samples are ranked with this model acting as FD (·) = RD (·).
The intuition behind this approach is that the model should
learn characteristics that distinguish in-domain pairs from out-of-
domain pairs, but it will have difficulty distinguishing between
cases where the two are similar. One advantage of this approach
is that it allows for training an interaction filter for any arbitrary ranking architecture, although it requires a sufficiently large TD to avoid overfitting.
4 EVALUATION 4.1 Experimental setup
Training sources. We use the following four sources of training data to verify the effectiveness of our methods:
- Query Log (AOL, ranking-based, 100k queries). This source uses the AOL query log [12] as the basis for a ranking-based source, following the approach of [5].4 We retrieve ClueWeb09 documents for each query using the Indri5 query likelihood (QL) model. We fix c+ = 1 and c- = 10 due to the expense of sampling documents from ClueWeb.
- Newswire (NYT, content-based, 1.8m pairs). We use the New York Times corpus [13] as a content-based source, using head-
lines as pseudo queries and the corresponding content as pseudo
relevant documents. We use BM25 to select the negative articles, retaining top c- = 100 articles for individual headlines. - Wikipedia (Wiki, content-based, 1.1m pairs). Wikipedia article heading hierarchies and their corresponding paragraphs have
been employed as a training set for the Trec Complex Answer Re-
trieval (CAR) task [10, 11]. We use these pairs as a content-based
source, assuming that the hierarchy of headings is a relevant
query for the paragraphs under the given heading. Heading-
paragraph pairs from train fold 1 of the Trec CAR dataset [6]
(v1.5) are used. We generate negative heading-paragraph pairs for each heading using BM25 (c- = 100). - Manual relevance judgments (WT10). We compare the rankingbased and content-based sources with a data source that consists
of relevance judgments generated by human assessors. In par-
ticular, manual judgments from 2010 Trec Web Track ad-hoc task (WT10) are employed, which includes 25k manual relevance judgments (5.2k relevant) for 50 queries (topics + descriptions, in line with [7, 8]). This setting represents a new target domain,
with limited (yet still substantial) manually-labeled data.
4
Distinct non-navigational queries from the AOL query log from March 1, 2006 to May 31, 2006 are selected. We randomly sample 100k of queries with length of at least 4. While Dehghani et al. [5] used a larger number of queries to train their model, the state-of-the-art relevance matching models we evaluate do not learn term embeddings (as [5] does) and thus converge with fewer than 100k training samples.
5
https://www.lemurproject.org/indri/

Training neural IR models. We test our method using several state-of-the-art neural IR models (introduced in Section 2.1): PACRR [8], Conv-KNRM [4], and KNRM [16].6 We use the model architectures and hyper-parameters (e.g., kernel sizes) from the best-performing configurations presented in the original papers for all models. All models are trained using pairwise loss for 200 iterations with 512 training samples each iteration. We use Web Track 2011 (WT11) manual relevance judgments as validation data to select the best iteration via nDCG@20. This acts as a way of fine-tuning the model to the particular domain, and is the only place that manual relevance judgments are used during the weak supervision training process. At test time, we re-rank the top 100 Indri QL results for each query.
Interaction filters. We use the 2-maximum and discriminator filters for each ranking architecture to evaluate the effectiveness of the interaction filters. We use queries from the target domain (Trec Web Track 2009­14) to generate the template pair set for the target domain TD . To generate pairs for TD , the top 20 results from query likelihood (QL) for individual queries on ClueWeb09 and ClueWeb127 are used to construct query-document pairs. Note that this approach makes no use of manual relevance judgments because only query-document pairs from the QL search results are used (without regard for relevance). We do not use query-document pairs from the target year to avoid any latent query signals from the test set. The supervised discriminator filter is validated using a held-out set of 1000 pairs. To prevent overfitting the training data, we reduce the convolutional filter sizes of PACRR and ConvKNRM to 4 and 32, respectively. We tune cmax with the validation dataset (WT11) for each model (100k to 900k, 100k intervals).
Baselines and benchmarks. As baselines, we use the AOL ranking-based source as a weakly supervised baseline [5], WT10 as a manual relevance judgment baseline, and BM25 as an unsupervised baseline. The two supervised baselines are trained using the same conditions as our approach, and the BM25 baselines is tuned on each testing set with Anserini [17], representing the best-case performance of BM25.8 We measure the performance of the models using the Trec Web Track 2012­2014 (WT12­14) queries (topics + descriptions) and manual relevance judgments. These cover two target collections: ClueWeb09 and ClueWeb12. Akin to [5], the trained models are used to re-rank the top 100 results from a querylikelihood model (QL, Indri [14] version). Following the Trec Web Track, we use nDCG@20 and ERR@20 for evaluation.
4.2 Results
In Table 1, we present the performance of the rankers when trained using content-based sources without filtering. In terms of absolute score, we observe that the two n-gram models (PACRR and ConvKNRM) always perform better when trained on content-based sources than when trained on the limited sample of in-domain data. When trained on NYT, PACRR performs significantly better. KNRM performs worse when trained using the content-based sources, sometimes significantly. These results suggest that these contentbased training sources contain relevance signals where n-grams
6
By using these stat-of-the-art architectures, we are using stronger baselines than those used in [5, 9]. 7https://lemurproject.org/clueweb09.php, https://lemurproject.org/clueweb12.php 8Grid search: b  [0.05, 1] (0.05 interval), and k1  [0.2, 4] (0.2 interval)

995

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Ranking performance when trained using contentbased sources (NYT and Wiki). Significant differences compared to the baselines ([B]M25, [W]T10, [A]OL) are indicated with  and  (paired t-test, p < 0.05).

Model

Training

BM25 (tuned w/ [17])

PACRR

WT10 AOL

NYT Wiki

Conv-KNRM WT10 AOL

NYT Wiki

KNRM

WT10 AOL

NYT Wiki

WT12
0.1087
B 0.1628 0.1910
W B 0.2135 W B 0.1955
B 0.1580 0.1498
A B 0.1792 0.1536
B 0.1764 B 0.1782 W 0.1455 A W 0.1417

nDCG@20
WT13
0.2176
0.2513 0.2608 A W B 0.2919 A B 0.2881
0.2398 0.2155 A W B 0.2904 A 0.2680 0.2671 0.2648 A 0.2340 0.2409

WT14
0.2646
0.2676 0.2802
W 0.3016 W 0.3002 B 0.3197
0.2889
B 0.3215 B 0.3206
0.2961 0.2998
0.2865 0.2959

are useful, and it is valuable for these models to see a wide variety of n-gram relevance signals when training. The n-gram models also often perform significantly better than the ranking-based AOL query log baseline. This makes sense because BM25's rankings do not consider term position, and thus cannot capture this important indicator of relevance. This provides further evidence that contentbased sources do a better job providing samples that include various notions of relevance than ranking-based sources.
When comparing the performance of the content-based training sources, we observe that the NYT source usually performs better than Wiki. We suspect that this is due to the web domain being more similar to the newswire domain than the complex answer retrieval domain. For instance, the document lengths of news articles are more similar to web documents, and precise term matches are less common in the complex answer retrieval domain [10].
We present filtering performance on NYT and Wiki for each ranking architecture in Table 2. In terms of absolute score, the filters almost always improve the content-based data sources, and in many cases this difference is statistically significant. The one exception is for Conv-KNRM on NYT. One possible explanation is that the filters caused the training data to become too homogeneous, reducing the ranker's ability to generalize. We suspect that Conv-KNRM is particularly susceptible to this problem because of language-dependent convolutional filters; the other two models rely only on term similarity scores. We note that Wiki tends to do better with the 2max filter, with significant improvements seen for Conv-KNRM and KNRM. In thse models, the discriminator filter may be learning surface characteristics of the dataset, rather than more valuable notions of relevance. We also note that cmax is an important (yet easy) hyper-parameter to tune, as the optimal value varies considerably between systems and datasets.
5 CONCLUSION
We presented an approach for employing content-based sources of pseudo relevance for training neural IR models. We demonstrated that our approach can match (and even outperform) neural ranking models trained on manual relevance judgments and existing ranking-based weak supervision approaches using two different

Table 2: Ranking performance using filtered NYT and Wiki. Significant improvements and reductions compared to unfiltered dataset are marked with  and  (paired t-test, p < 0.05).

Model PACRR
Conv-KNRM
KNRM

Training
NYT w/ 2max w/ discriminator
Wiki w/ 2max w/ discriminator
NYT w/ 2max w/ discriminator
Wiki w/ 2max w/ discriminator
NYT w/ 2max w/ discriminator
Wiki w/ 2max w/ discriminator

kmax
200k 500k
700k 800k
100k 800k
400k 700k
100k 300k
600k 700k

WebTrack 2012­14

nDCG@20 ERR@20

0.2690
0.2716  0.2875

0.2136 0.2195 0.2273

0.2613 0.2568 0.2680

0.2038 0.2074 0.2151

0.2637  0.2338 0.2697

0.2031 0.2153 0.1937

0.2474 0.2609 0.2572

0.1614  0.1828
0.1753

0.2220 0.2235 0.2274

0.1536
 0.1828  0.1671

0.2262  0.2389
0.2366

0.1635  0.1916
0.1740

sources of data. We also showed that performance can be boosted using two filtering techniques: one heuristic-based and one that re-purposes a neural ranker. By using our approach, one can effectively train neural ranking models on new domains without behavioral data and with only limited in-domain data.

REFERENCES
[1] Nima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo Test Collections for Learning Web Search Ranking Functions. In SIGIR.
[2] Leif Azzopardi, Maarten de Rijke, and Krisztian Balog. 2007. Building Simulated
Queries for Known-item Topics: An Analysis Using Six European Languages. In SIGIR. [3] Richard Berendsen, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke.
2013. Pseudo Test Collections for Training and Tuning Microblog Rankers. In SIGIR. [4] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search. In WSDM '18. [5] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In SIGIR. [6] Laura Dietz and Ben Gamari. 2017. TREC CAR: A Data Set for Complex Answer
Retrieval. (2017). http://trec-car.cs.unh.edu Version 1.5.
[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval. In CIKM '16.
[8] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. PACRR: A Position-Aware Neural IR Model for Relevance Matching. In EMNLP.
[9] Bo Li, Ping Cheng, and Le Jia. 2018. Joint Learning from Labeled and Unlabeled Data for Information Retrieval. In COLING '18.
[10] Sean MacAvaney, Andrew Yates, Arman Cohan, Luca Soldaini, Kai Hui, Nazli
Goharian, and Ophir Frieder. 2018. Overcoming low-utility facets for complex answer retrieval. Information Retrieval Journal (2018), 1­24. [11] Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. 2017. Benchmark for Complex Answer Retrieval. In ICTIR '17. [12] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In Proceedings of the 1st International Conference on Scalable Information Systems. [13] Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia 6, 12 (2008), e26752. [14] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. Citeseer, 2­6. [15] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng
Zhang, and Dell Zhang. 2017. IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. In SIGIR. [16] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In SIGIR. [17] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR. [18] Hamed Zamani, Mostafa Dehghani, Fernando Diaz, Hang Li, and Nick Craswell. 2018. Workshop on Learning from Limited or Noisy Data for IR. In SIGIR.

996

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

CEDR: Contextualized Embeddings for Document Ranking

Sean MacAvaney
IRLab, Georgetown University sean@ir.cs.georgetown.edu
Arman Cohan
Allen Institute for Artificial Intelligence armanc@allenai.org
ABSTRACT
Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on Trec benchmarks, we find that several existing neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.
ACM Reference Format: Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR: Contextualized Embeddings for Document Ranking. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331317
1 INTRODUCTION
Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5, 6, 20]. Meanwhile, pretrained contextualized language models (such as ELMo [16] and BERT [4]) have shown great promise on various natural language processing tasks [4, 16]. These models work by pre-training LSTM-based or transformer-based [19] language models on a large corpus, and then by performing minimal task fine-tuning (akin to ImageNet [3, 23]).
Prior work has suggested that contextual information can be valuable when ranking. ConvKNRM [1], a recent neural ranking model, uses a convolutional neural network atop word representations, allowing the model to learn representations aware of context in local proximity. In a similar vein, McDonald et al. [12] proposes
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331317

Andrew Yates
Max Planck Institute for Informatics ayates@mpi-inf.mpg.de
Nazli Goharian
IRLab, Georgetown University nazli@ir.cs.georgetown.edu
an approach that learns a recurrent neural network for term representations, thus being able to capture context from the entire text [12]. These approaches are inherently limited by the variability found in the training data. Since obtaining massive amounts of highquality relevance information can be difficult [24], we hypothesize that pretrained contextualized term representations will improve ad-hoc document ranking performance.
We propose incorporating contextualized language models into existing neural ranking architectures by using multiple similarity matrices ­ one for each layer of the language model. We find that, at the expense of computation costs, this improves ranking performance considerably, achieving state-of-the-art performance on the Robust 2004 and WebTrack 2012­2014 datasets. We also show that combining each model with BERT's classification mechanism can further improve ranking performance. We call this approach CEDR (Contextualzed Embeddings for Document Ranking). Finally, we show that the computation costs of contextualized language models can be dampened by only partially computing the contextualized language model representations. Although others have successfully used BERT for passage ranking [14] and question answering [22], these approaches only make use of BERT's sentence classification mechanism. In contrast, we use BERT's term representations, and show that they can be effectively combined with existing neural ranking architectures.
In summary, our contributions are as follows: - We are the first to demonstrate that contextualized word repre-
sentations can be successfully incorporated into existing neural architectures (PACRR [6], KNRM [20], and DRMM [5]), allowing them to leverage contextual information to improve ad-hoc document ranking. - We present a new joint model that combines BERT's classification vector with existing neural ranking architectures (using BERT's token vectors) to get the benefits from both approaches. - We demonstrate an approach for addressing the performance impact of computing contextualized language models by only partially computing the language model representations. - Our code is available for replication and future work.1
2 METHODOLOGY
2.1 Notation
In ad-hoc ranking, documents are ranked for a given query according to a relevance estimate. Let Q be a query consisting of query terms {q1, q2, ..., q |Q | }, and let D be a document consisting of terms {d1, d2, ..., d |D | }. Let ranker (Q, D)  R be a function that returns
1 https://github.com/Georgetown- IR- Lab/cedr

1101

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

a real-valued relevance estimate for the document to the query. Neural relevance ranking architectures generally use a similarity matrix as input S  R|Q |×|D |, where each cell represents a similarity score between the query and document: Si, j = sim(qi , dj ). These similarity values are usually the cosine similarity score between the word vectors of each term in the query and document.
2.2 Contextualized similarity tensors
Pretrained contextual language representations (such as those from ELMo [16] and BERT [4]) are context sensitive; in contrast to more conventional pretrained word vectors (e.g., GloVe [15]) that generate a single word representation for each word in the vocabulary, these models generate a representation of each word based on its context in the sentence. For example, the contextualized representation of word bank would be different in bank deposit and river bank, while a pretrained word embedding model would always result in the same representation for this term. Given that these representations capture contextual information in the language, we investigate how these models can also benefit general neural ranking models.
Although contextualized language models vary in particular architectures, they typically consist of multiple stacked layers of representations (e.g., recurrent or transformer outputs). The intuition is that the deeper the layer, the more context is incorporated. To allow neural ranking models to learn which levels are most important, we choose to incorporate the output of all layers into the model, resulting in a three-dimensional similarity representation. Thus, we expand the similarity representation (conditioned on the query and document context) to SQ,D  RL×|Q |×|D | where L is the number of layers in the model, akin to the channel in image processing. Let contextQ,D(t, l)  RD be the contextualized representation for token t in layer l, given the context of Q and D. Given these definitions, let the contextualized representation be:
SQ,D[l, q, d] = cos(contextQ,D(q, l), contextQ,D(d, l)) (1)
for each query term q  Q, document term d  D, and layer l  [1..L]. Note that when q and d are identical, they will likely not receive a similarity score of 1, as their representation depends on the surrounding context of the query and document. The layer dimension can be easily incorporated into existing neural models. For instance, soft n-gram based models, like PACRR, can perform convolutions with multiple input channels, and counting-based methods (like KNRM and DRMM) can count each channel individually.
2.3 Joint BERT approach
Unlike ELMo, the BERT model encodes multiple text segments simultaneously, allowing it to make judgments about text pairs. It accomplishes this by encoding two meta-tokens ([SEP] and [CLS]) and using text segment embeddings (Segment A and Segment B). The [SEP] token separates the tokens of each segment, and the [CLS] token is used for making judgments about the text pairs. During training, [CLS] is used for predicting whether two sentences are sequential ­ that is, whether Segment A immediately precedes Segment B in the original text. The representation of this token can be fine-tuned for other tasks involving multiple text segments, including natural language entailment and question answering [22].

We explore incorporating the [CLS] token's representation into existing neural ranking models as a joint approach. This allows neural rankers to benefit from deep semantic information from BERT in addition to individual contextualized token matches.
Incorporating the [CLS] token into existing ranking models is straightforward. First, the given ranking model produces relevance scores (e.g., n-gram or kernel scores) for each query term based on the similarity matrices. Then, for models using dense combination (e.g., PACRR, KNRM), we propose concatenating the [CLS] vector to the model's signals. For models that sum query term scores (e.g., DRMM), we include the [CLS] vector in the dense calculation of each term score (i.e., during combination of bin scores).
We hypothesize that this approach will work because the BERT classification mechanism and existing rankers have different strengths. The BERT classification benefits from deep semantic understanding based on next-sentence prediction, whereas ranking architectures traditionally assume query term repetition indicates higher relevance. In reality, both are likely important for relevance ranking.
3 EXPERIMENT
3.1 Experimental setup
Datasets. We evaluate our approaches using two datasets: Trec Robust 2004 and WebTrack 2012­14. For Robust, we use the five folds from [7] with three folds used for training, one fold for testing, and the previous fold for validation. For WebTrack, we test on 2012­14, training each year individually on all remaining years (including 2009­10), and validating on 2011. (For instance, when testing on WebTrack 2014, we train on 2009­10 and 2012­13, and validate on 2011.) Robust uses Trec discs 4 and 52, WebTrack 2009­ 12 use ClueWeb09b3, and WebTrack 2013­14 uses ClueWeb124 as document collections. We evaluate the results using the nDCG@20 / P@20 metrics for Robust04 and nDCG@20 / ERR@20 for WebTrack.
Models. Rather than building new models, in this work we use existing model architectures to test the effectiveness of various input representations. We evaluate our methods on three neural relevance matching methods: PACRR [6], KNRM [20], and DRMM [5]. Relevance matching models have generally shown to be more effective than semantic matching models, while not requiring massive amounts of behavioral data (e.g., query logs). For PACRR, we increase kmax = 30 to allow for more term matches and better back-propagation to the language model.
Contextualized language models. We use the pretrained ELMo (Original, 5.5B) and BERT (BERT-Base, Uncased) language models in our experiments. For ELMo, the query and document are encoded separately. Since BERT enables encoding multiple texts at the same time using Segment A and Segment B embeddings, we encode the query (Segment A) and document (Segment B) simultaneously. Because the pretrained BERT model is limited to 512 tokens, longer documents are split such that document segments are split as evenly as possible, while not exceeding the limit when combined with the query and control tokens. (Note that the query is always included in full.) BERT allows for simple classification fine-tuning, so we also experiment with a variant that is first fine-tuned on the
2520k documents; https://trec.nist.gov/data_disks.html 350M web pages, https://lemurproject.org/clueweb09/ 4733M web pages, https://lemurproject.org/clueweb12/

1102

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

same data using the Vanilla BERT classifier (see baseline below), and further fine-tuned when training the ranker itself.
Training and optimization. We train all models using pairwise hinge loss [2]. Positive and negative training documents are selected from the query relevance judgments (positive documents limited to only those meeting the re-ranking cutoff threshold k using BM25, others considered negative). We train each model for 100 epochs, each with 32 batches of 16 training pairs. Gradient accumulation is employed when the batch size of 16 is too large to fit on a single GPU. We re-rank to top k BM25 results for validation, and use P@20 on Robust and nDCG@20 on WebTrack to select the best-performing model. We different re-ranking functions and thresholds at test time for each dataset: BM25 with k = 150 for Robust04, and QL with k = 100 for WebTrack. The re-ranking setting is a better evaluation setting than ranking all qrels, as demonstrated by major search engines using a pipeline approach [18]. All models are trained using Adam [8] with a learning rate of 0.001 while BERT layers are trained at a rate of 2e-5.5 Following prior work [6], documents are truncated to 800 tokens.
Baselines. We compare contextualized language model performance to the following strong baselines:
- BM25 and SDM [13], as implemented by Anserini [21]. Finetuning is conducted on the test set, representing the maximum performance of the model when using static parameters over each dataset.6 We do not report SDM performance on WebTrack due to its high cost of retrieval on the large ClueWeb collections.
- Vanilla BERT ranker. We fine-tune a pretrained BERT model (BERT-Base, Uncased) with a linear combination layer stacked atop the classifier [CLS] token. This network is optimized the same way our models are, using pairwise cross-entropy loss and the Adam optimizer. We use the approach described above to handle documents longer than the capacity of the network, and average the [CLS] vectors from each split.
- TREC-best: We also compare to the top-performing topic TREC run for each track in terms of nDCG@20. We use uogTrA44xu for WT12 ([10], a learning-to-rank based run), clustmrfaf for WT13 ([17], clustering-based), UDInfoWebAX for WT14 ([11], entity expansion), and pircRB04t3 for Robust04 ([9], query expansion using Google search results).7
- ConvKNRM [1], our implementation with the same training pipeline as the evaluation models.
- Each evaluation model when using GloVe [15] vectors.8
3.2 Results & analysis
Table 1 shows the ranking performance using our approach. We first note that the Vanilla BERT method significantly outperforms the tuned BM25 [V] and ConvKNRM [C] baselines on its own. This is encouraging, and shows the ranking power of the Vanilla BERT model. When using contextualized language term representations without tuning, PACRR and DRMM performance is comparable to that of GloVe [G], while KNRM sees a modest boost. This might be
5Pilot experiments showed that a learning rate of 2e-5 was more effective on this task than the other recommended values of 5e-5 and 3e-5 by [4]. 6k1 in 0.1­4 (by 0.1), b in 0.1­1 (by 0.1), SDM weights in 0­1 (by 0.05). 7We acknowledge limitations of the TREC experimentation environment. 8glove.42B.300d, https://nlp.stanford.edu/projects/glove/

try curb growth population order raise

Relevant (FT934-7698) GloVe ELMo BERT (ft)

0.6

0.3

0.7

Non-relevant (LA032990-0138) GloVe ELMo BERT (ft)

tuesday

abandoned

plan

citywide

curb 0.6

0.2

0.5

construction

curbing population
growth curbing population growth
curb ##ing population growth curbing population growth curbing population growth curb ##ing population growth

Figure 1: Example similarity matrix excerpts from GloVe, ELMo, and BERT for relevant and non-relevant document for Robust query 435. Lighter values have higher similarity.

(a)

(b)

Figure 2: (a) Processing rates by document length for GloVe, ELMo, and BERT using PACRR. (b) Processing rate and dev performance of PACRR when using a subset of BERT layers.

due to KNRM's ability to train its matching kernels, tuning to specific similarity ranges produced by the models. (In contrast, DRMM uses fixed buckets, and PACRR uses maximum convolutional filter strength, both of which are less adaptable to new similarity score ranges.) When fine-tuning BERT, all three models see a significant boost in performance compared to the GloVe-trained version. PACRR and KNRM see comparable or higher performance than the Vanilla BERT model. This indicates that fine-tuning contextualized language models for ranking is important. This boost is further enhanced when using the CEDR (joint) approach, with the CEDR models always outperforming Vanilla BERT [V], and nearly always significantly outperforming the non-CEDR versions [N]. This suggests that term counting methods (such as KNRM and DRMM) are complementary to BERT's classification mechanism. Similar trends for both Robust04 and WebTrack 2012­14 indicate that our approach is generally applicable to ad-hoc document retrieval tasks.
To gain a better understanding of how the contextual language model helps enhance the input representation, we plot example similarity matrices based on GloVe word embeddings, ELMo representations (layer 2), and fine-tuned BERT representations (layer 5). In these examples, two senses of the word curb (restrain, and edge of street) are encountered. The first is relevant to the query (it's discussing attempts to restrain population growth). The second is not (it discusses street construction). Both the ELMo and BERT models give a higher similarity score to the correct sense of the term for the query. This ability to distinguish different senses of terms is a strength of contextualized language models, and likely can explain some of the performance gains of the non-joint models.
Although the contextualized language models yield ranking performance improvements, they come with a considerable cost at inference time--a practical issue ignored in previous ranking work [14, 21]. To demonstrate this, in Figure 2(a) we plot the processing rate of GloVe, ELMo, and BERT.9 Note that the processing

9Running time measured on single GeForce GTX 1080 Ti GPU, data in memory.

1103

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Ranking performance on Robust04 and WebTrack 2012­14. Significant improvements to [B]M25, [C]onvKNRM, [V]anilla BERT, the model trained with [G]lOve embeddings, and the corresponding [N]on-CEDR system are indicated in brackets (paired t-test, p < 0.05).

Robust04

WebTrack 2012­14

Ranker

Input Representation

P@20

nDCG@20

nDCG@20

ERR@20

BM25 SDM [13] TREC-Best ConvKNRM Vanilla BERT

n/a n/a n/a GloVe BERT (fine-tuned)

0.3123 0.3749 0.4386 0.3349 [BC] 0.4042

0.4140 0.4353 0.5030 0.3806 [BC] 0.4541

0.1970 -
0.2855 [B] 0.2547 [BC] 0.2895

0.1472 -
0.2530 [B] 0.1833 [BC] 0.2218

PACRR PACRR PACRR PACRR CEDR-PACRR

GloVe ELMo BERT BERT (fine-tuned) BERT (fine-tuned)

0.3535 [C] 0.3554 [C] 0.3650 [BCVG] 0.4492 [BCVG] 0.4559

[C] 0.4043 [C] 0.4101 [C] 0.4200 [BCVG] 0.5135 [BCVG] 0.5150

0.2101 [BG] 0.2324
0.2225 [BCG] 0.3080 [BCVGN] 0.3373

0.1608 [BG] 0.1885
0.1817 [BCG] 0.2334 [BCVGN] 0.2656

KNRM KNRM KNRM KNRM CEDR-KNRM

GloVe ELMo BERT BERT (fine-tuned) BERT (fine-tuned)

0.3408 [C] 0.3517 [BCG] 0.3817 [BCG] 0.4221 [BCVGN] 0.4667

0.3871 [CG] 0.4089 [CG] 0.4318 [BCVG] 0.4858 [BCVGN] 0.5381

[B] 0.2448 0.2227
[B] 0.2525 [BCVG] 0.3287 [BCVG] 0.3469

0.1755 0.1689 [B] 0.1944 [BCVG] 0.2557 [BCVG] 0.2772

DRMM DRMM DRMM DRMM CEDR-DRMM

GloVe ELMo BERT BERT (fine-tuned) BERT (fine-tuned)

0.2892 0.2867 0.2878 [CG] 0.3641 [BCVGN] 0.4587

0.3040 0.3137 0.3194 [CG] 0.4135 [BCVGN] 0.5259

0.2215 [B] 0.2271 [BG] 0.2459 [BG] 0.2598 [BCVGN] 0.3497

0.1603 0.1762 [BG] 0.1977 [B] 0.1856 [BCVGN] 0.2621

rate when using static GloVe vectors is orders of magnitude faster than when using the contextualized representations, with BERT outperforming ELMo because it uses the more efficient Transformer instead of an RNN. In an attempt to improve the running time of these systems, we propose limiting the number of layers processed by the model. The reasoning behind this approach is that the lower the layer, the more abstract the matching becomes, perhaps becoming less useful for ranking. We show the runtime and ranking performance of PACRR when only processing only up to a given layer in Figure 2(b). It shows that most of the performance benefits can be achieved by only running BERT through layer 5; the performance is comparable to running the full BERT model, while running more than twice as fast. While we acknowledge that our research code is not completely optimized, we argue that this approach is generally applicable because the processing of these layers are sequential, query-dependent, and dominate the processing time of the entire model. This approach is a simple time-saving measure.
4 CONCLUSION
We demonstrated that contextualized word embeddings can be effectively incorporated into existing neural ranking architectures and suggested an approach for improving runtime performance by limiting the number of layers processed.
REFERENCES
[1] Zhuyun Dai, Chenyan Xiong, James P. Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search. In WSDM.
[2] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In SIGIR.
[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition.
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805 (2018).

[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM.
[6] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2018. Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval. In WSDM.
[7] Samuel Huston and W Bruce Croft. 2014. Parameters learned in the comparison of retrieval models using term dependencies. Technical Report (2014).
[8] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.
[9] Kui-Lam Kwok, Laszlo Grunfeld, H. L. Sun, and Peter Deng. 2004. TREC 2004 Robust Track Experiments Using PIRCS. In TREC.
[10] Nut Limsopatham, Richard McCreadie, M-Dyaa Albakour, Craig MacDonald, Rodrygo L. T. Santos, and Iadh Ounis. 2012. University of Glasgow at TREC 2012: Experiments with Terrier. In TREC.
[11] Xitong Liu, Peilin Yang, and Hui Fang. 2014. Entity Came to Rescue - Leveraging Entities to Minimize Risks in Web Search. In TREC.
[12] Ryan McDonald, Yichun Ding, and Ion Androutsopoulos. 2018. Deep Relevance Ranking using Enhanced Document-Query Interactions. In EMNLP.
[13] Donald Metzler and W. Bruce Croft. 2005. A Markov random field model for term dependencies. In SIGIR.
[14] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. CoRR abs/1901.04085 (2019).
[15] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP.
[16] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.
[17] Fiana Raiber and Oren Kurland. 2013. The Technion at TREC 2013 Web Track: Cluster-based Document Retrieval. In TREC.
[18] Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, and Saurabh Tiwary. 2018. Optimizing Query Evaluations Using Reinforcement Learning for Web Search. In SIGIR.
[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In NIPS.
[20] Chenyan Xiong, Zhuyun Dai, James P. Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In SIGIR.
[21] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR.
[22] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-End Open-Domain Question Answering with BERTserini. CoRR abs/1901.04085 (2019).
[23] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks?. In NIPS.
[24] Hamed Zamani, Mostafa Dehghani, Fernando Diaz, Hang Li, and Nick Craswell. 2018. SIGIR 2018 Workshop on Learning from Limited or Noisy Data for Information Retrieval. In SIGIR.

1104

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

The Impact of Score Ties on Repeatability in Document Ranking

Jimmy Lin1 and Peilin Yang
1 David R. Cheriton School of Computer Science, University of Waterloo jimmylin@uwaterloo.ca

ABSTRACT
Document ranking experiments should be repeatable. However, the interaction between multi-threaded indexing and score ties during retrieval may yield non-deterministic rankings, making repeatability not as trivial as one might imagine. In the context of the open-source Lucene search engine, score ties are broken by internal document ids, which are assigned at index time. Due to multi-threaded indexing, which makes experimentation with large modern document collections practical, internal document ids are not assigned consistently between different index instances of the same collection, and thus score ties are broken unpredictably. This short paper examines the effectiveness impact of such score ties, quantifying the variability that can be attributed to this phenomenon. The obvious solution to this non-determinism and to ensure repeatable document ranking is to break score ties using external collection document ids. This approach, however, comes with measurable efficiency costs due to the necessity of consulting external identifiers during query evaluation.
ACM Reference Format: Jimmy Lin and Peilin Yang. 2019. The Impact of Score Ties on Repeatability in Document Ranking. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184. 3331339
1 INTRODUCTION
It should generate no controversy to assert that repeatability of document ranking experiments in information retrieval research is a desirable property. To be precise, running the same ranking model over the same document collection with the same queries should yield the same output every time. Yet, this simple property is not trivial to achieve in modern retrieval engines that take advantage of multi-threaded indexing. In this paper, we explore corner cases that yield non-repeatable rankings: observed non-determinism is attributable to score ties, or documents in the collection that receive the same score with respect to a particular ranking model.
Anserini, an open-source information retrieval toolkit built on Lucene [10, 11], provides the context for our study. The system evolved from previous IR reproducibility experiments [2, 5] where
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331339

Lucene exhibited a good balance between efficiency and effectiveness compared to other open-source search engines. A large user and developer base, numerous commercial deployments at scale, as well as a vibrant ecosystem provide additional compelling arguments for building an IR research toolkit on top of Lucene.
The multi-threaded indexer that Anserini implements on top of Lucene is able to rapidly index large modern document collections-- for example, building a simple non-positional index on the popular ClueWeb small collections each takes around an hour on a typical server [11]. A consequence of the multi-threaded design is that documents are inserted into the index in a non-deterministic order, which means that different index instances over the same collection may be substantively different. This has implications for documents that receive the same score at retrieval time--by default, the Lucene query evaluation algorithm breaks ties by an internal document id, which is assigned based on document insertion order. Since these internal document ids are not stable across different index instances, document ranking experiments may not be repeatable.
Is this a big deal? We argue yes, from a number of perspectives. While arbitrary tie-breaking behavior has a relatively small impact on simple "bag of words" queries (typically, differences in the fourth decimal place in terms of standard evaluation metrics), effectiveness differences can be magnified for relevance feedback runs that utilize two-stage retrieval based on an initial ranking. Repeatable runs form a cornerstone of regression testing in modern software development--without exact repeatability, it is difficult to separate non-deterministic bugs (so called "heisenbugs") from inherent execution instability. Without a suite of regression tests, sustained progress on a complex codebase becomes difficult. For example, Lin et al. [5] reported cases of different results from runs that purport to use the same ranking model from the same system on the same test collection (by the same research group, even).
The goal of this paper and our contribution is a detailed study of the impact of score ties from the perspective of repeatability across a number of different test collections for a specific search engine. We empirically characterize differences in effectiveness that can be attributed to arbitrary interleaving of documents ingested during multi-threaded indexing. The solution to repeatable document ranking is fairly obvious: ties should be broken deterministically by external collection document ids (which are stable) instead of internal indexer-assigned ids. However, this comes at a measurable cost in query evaluation performance, which arises from the need to consult external identifiers during query evaluation.
2 EXPERIMENTAL DESIGN
All experiments in this paper were conducted with Anserini v0.1.0, which is based on Lucene 6.3.0; all code necessary to replicate our experiments are available on GitHub at http://anserini.io/.

1125

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

To examine the impact of score ties across a diverse range of document types, we considered three newswire collections, two tweet collections, and two web collections:
· TREC 2004 Robust Track, on TREC Disks 4 & 5. · TREC 2005 Robust Track, on the AQUAINT Corpus. · TREC 2017 Common Core Track, on the New York Times Anno-
tated Corpus. · TREC 2011/2012 Microblog Tracks (on the Tweets2011 collec-
tion) and TREC 2013/2014 Microblog Tracks (on the Tweets2013 collection). · TREC 2010­2012 Web Tracks (on ClueWeb09b) and TREC 2013­ 2014 Web Tracks (on ClueWeb12-B13).
For each document collection, we used Anserini to build five separate indexes from scratch. For each index, we performed a retrieval run using topics from the corresponding test collections. In each of these runs, Anserini used Lucene's default tie-breaking technique based on arbitrarily-assigned internal document ids--which as we have noted above, is not consistent between index instances due to multi-threading. Differences in effectiveness between these runs quantify the impact of score ties.
In Anserini, we have modified the query evaluation algorithm to use the external collection id to break score ties, which means that retrieval results are repeatable across different index instances. This is accomplished via the Sort abstraction in Lucene, which allows the developer to specify how ranked lists are sorted. Naturally, the default is by the score produced by the ranking model. For newswire and web collections, we added lexicographic sort order of collection document ids as the tie breaker. For tweets, ties are broken by reverse chronological order (i.e., most recent tweet first). In our experiments, we refer to this as the repeatable ranking condition, which provides the "ground truth" for comparison against Lucene's default tie-breaking behavior above.
With the exception of tweet collections, we considered the following ranking models: BM25 and query likelihood, and the RM3 query expansion technique applied to both. For tweet collections, we only considered query likelihood since BM25 is known not to be effective. In Anserini, RM3 is implemented as a two-stage process: a relevance model is estimated from documents in an initial ranked list, which then forms an expanded query that retrieves the final results. Thus, there are two source of variability due to score ties--when applying a rank cutoff in the initial retrieval as well as the final ranking.
All runs retrieved up to 1000 hits and were evaluated in terms of standard retrieval metrics: for newswire and tweet collections, we computed average precision (AP) and precision at rank 30 (P30) using trec_eval. For the web collections, we computed NDCG@20 using gdeval.pl (since the shallow pool depths make AP unreliable). As a final detail, to ensure that the evaluation program respects our tie-breaking approach, Anserini slightly perturbs scores of tied documents when writing output in TREC format so that the rank order is consistent with the score order. This is necessary, for example, because when trec_eval computes a metric, it loads in the run and sorts the documents by score, breaking ties internally. We did not want an external evaluation tool to impose its own tie-breaking approach, but rather respect the rankings generated by our system.

3 RESULTS
The results of our experiments on the newswire collections are shown in Table 1. Under the columns with the names of the metric ("AP" and "P30"), we report the effectiveness of the repeatable runs (consistent tie-breaking with external document ids). The columns marked "min--max" report minimum and maximum scores across the five different indexes given arbitrary tie-breaking (internal document ids). Note that for some cases, the effectiveness of the repeatable condition falls outside the min­max range. The columns marked "" show the largest absolute observed difference across all runs, including the repeatable condition. Results on the tweet collections are shown in Table 2 and results on the web collections are shown in Table 3; both are organized in exactly the same manner as Table 1. Note that for the web collections we only report effectiveness in terms of NDCG@20 due to the shallow pools used in the construction of the qrels.
We see that the variability attributed to tie-breaking behavior yields minor effectiveness differences, usually in the fourth decimal place, but sometimes in the third decimal place. Overall, observed variability in AP is smaller than P30 because for AP, the differences come from documents that straddle the rank cutoff of 1000, where score contributions are small. Results show that RM3 can exhibit (but not always) greater variability because score ties impact both the selection of documents for extracting feedback terms as well as the final ranking. For tweets, we observe greater variability, due to more score ties since many tweets have the same length. Again, differences in P30 are more pronounced than AP. Results on the web collections are consistent with the other collections.
In absolute terms, the observed score variability is fairly small. However, to put these differences in perspective, incremental advances in many popular NLP and IR shared tasks today, for example, SQuAD and MS MARCO, are measured in the third decimal place. Leaving aside whether such leaderboard-driven research is good for the community as a whole, we note that the amount of variability observed in our experiments can approach the magnitude of differences in successive advances in "the state of the art".
More importantly, as argued in the introduction, this variability makes regression testing--which is a cornerstone of modern software development--very difficult. Typically, in regression testing, for floating point values the developer specifies a tolerance when comparing test results with expected results, for example, to compensate for precision errors. In our case, it is not clear how the developer should set this tolerance. A value too large would fail to catch genuine bugs, while a value too small would cause frequent needless failures, defeating the point of regression testing.
As discussed in the previous section, the solution to repeatable document ranking is relatively straightforward--instead of depending on the internal document id to break score ties, we should use external (collection-specific) document ids. Anserini implements exactly this solution using a Lucene-provided abstraction, as described in the previous section. This approach, however, comes at a cost in terms of efficiency, since the query evaluation algorithm must consult an external id as part of its inner loop during postings traversal. Lookup of an external id requires some form of memory access, likely leading to pointer chasing that can potentially disrupt data and cache locality.

1126

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Model
BM25 BM25+RM3 QL QL+RM3

TREC 2004 Robust Track topics, Disks 4 & 5

AP

min--max

 P30

min--max

0.2501 0.2757 0.2468 0.2645

0.2498 -- 0.2501 0.2756 -- 0.2757 0.2464 -- 0.2469 0.2643 -- 0.2644

0.0003 0.0001 0.0005 0.0002

0.3123 0.3256 0.3083 0.3153

0.3120 -- 0.3124 0.3253 -- 0.3257 0.3076 -- 0.3080 0.3149 -- 0.3151


0.0004 0.0004 0.0007 0.0004

Model
BM25 BM25+RM3 QL QL+RM3

TREC 2005 Robust Track topics, AQUAINT Collection

AP

min--max

 P30

min--max

0.2003 0.2511 0.2026 0.2480

0.2000 -- 0.2006 0.2506 -- 0.2513 0.2019 -- 0.2026 0.2471 -- 0.2483

0.0006 0.0007 0.0005 0.0012

0.3660 0.3873 0.3713 0.4007

0.3660 -- 0.3673 0.3860 -- 0.3880 0.3693 -- 0.3720 0.4007 -- 0.4013


0.0013 0.0020 0.0027 0.0006

TREC 2017 Common Core Track topics, New York Times Collection

Model

AP

min--max

 P30

min--max



BM25 BM25+RM3 QL QL+RM3

0.1996 0.2633 0.1928 0.2409

0.1997 -- 0.1998 0.2632 -- 0.2635 0.1929 -- 0.1929 0.2408 -- 0.2409

0.0002 0.0003 0.0001 0.0001

0.4207 0.4893 0.4327 0.4647

0.4213 -- 0.4220 0.4867 -- 0.4893 0.4327 -- 0.4333 0.4640 -- 0.4647

0.0007 0.0026 0.0006 0.0007

Table 1: Variability in effectiveness attributed to score ties on three newswire collections. The first column for each metric ("AP" and "P30") shows values with consistent tie-breaking. Columns marked "min--max" report minimum and maximum scores across five different indexes with arbitrary tie-breaking. Columns marked "" report the largest absolute observed difference (including consistent tie-breaking).

TREC 2011 and 2012 Microblog Track topics, Tweets2011 Collection

Model

AP

min--max

 P30

min--max



QL

0.2787 0.2761 -- 0.2770 0.0026 0.3673 0.3636 -- 0.3667 0.0037

QL+RM3 0.3178 0.3157 -- 0.3173 0.0021 0.3954 0.3929 -- 0.3975 0.0046

TREC 2013 and 2014 Microblog Track topics, Tweets2013 Collection

Model

AP

min--max

 P30

min--max



QL

0.3357 0.3345 -- 0.3348 0.0012 0.5429 0.5406 -- 0.5423 0.0023

QL+RM3 0.3692 0.3695 -- 0.3699 0.0007 0.5484 0.5528 -- 0.5542 0.0058

Table 2: Variability in effectiveness attributed to score ties on tweet collections, organized in the same way as Table 1.

TREC 2010--2012 Web Track topics, ClueWeb09b

Model

NDCG@20

min--max



BM25 BM25+RM3 QL QL+RM3

0.1407 0.1524 0.1211 0.1340

0.1405 -- 0.1408 0.1524 -- 0.1525 0.1210 -- 0.1212 0.1340 -- 0.1342

0.0003 0.0001 0.0002 0.0002

TREC 2013 and 2014 Web Track topics, ClueWeb12-B13

Model

NDCG@20

min--max



BM25 BM25+RM3 QL QL+RM3

0.1216 0.1080 0.1146 0.0920

0.1216 -- 0.1216 0.1077 -- 0.1083 0.1146 -- 0.1154 0.0920 -- 0.0926

0.0000 0.0006 0.0008 0.0006

Table 3: Variability in effectiveness attributed to score ties on web collections, organized in the same way as Table 1.

The efficiency costs of repeatable experiments are quantified in Table 4 for the three largest collections used in our experiments. Here, we report average query evaluation latency (in seconds) under the non-repeatable and repeatable conditions, averaged over five trials on an iMac Pro desktop machine (2.3 GHz Intel Xeon W processor) running macOS High Sierra. The final column shows the increase in query latency due to consistent tie-breaking using external document ids. In all cases we first ran the experiments a few times to warm up underlying operating system caches, and then captured measurements over the next sets of trials. Query evaluation was performed using a single thread.
For simple bag-of-words queries, we observe a measurable slowdown in query latency, which quantifies the cost of repeatability. Across the web collections, this slowdown is approximately 20%, but for tweets the latency costs are a bit higher, most likely due to more prevalent score ties. Not surprisingly, query evaluation with RM3 is much slower due to its two-stage process: here, however, the behavior between tweet and web collections diverge. For web

1127

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

TREC 2013 and 2014 Microblog Track topics, Tweets2013

Model Non-Repeatable Repeatable



QL QL+RM3

0.46s 1.05s

0.58s +26% 2.00s +90%

TREC 2010--2012 Web Track topics, ClueWeb09b

Model

Non-Repeatable Repeatable



BM25 BM25+RM3

0.18s 3.92s

0.23s +23% 4.18s +7%

TREC 2013--2014 Web Track topics, ClueWeb12-B13

Model

Non-Repeatable Repeatable



BM25 BM25+RM3

0.22s 4.23s

0.26s +18% 4.61s +9%

Table 4: Latency differences between non-repeatable and repeatable document ranking, where repeatability is achieved by consistently breaking ties using external document ids.

collections, the slowdown is less compared to bag-of-words queries because a significant amount of time is spent reading document vectors from the index and estimating relevance models during query evaluation. As a result, the amount of time actually spent in the postings traversal inner loop is proportionally smaller. Tweets, however, are much shorter, and so estimating relevance models is relatively fast. The larger expanded queries require consulting more postings and scoring more documents, thus leading to large slowdowns for repeatable runs.
We acknowledge that these results are implementation specific, tied to the exact mechanism by which external document ids are consulted. Our current implementation uses existing Lucene abstractions for controlling the sort order of results, but greater efficiencies might be possible with a more invasive modification of Lucene internals. Nevertheless, our broader point remains true: repeatability inevitably comes at some cost in performance.

4 RELATED WORK
We are, of course, not the first to have noticed score ties in document ranking and to examine their impact. Cabanac et al. [3] studied the behavior of the widely-used trec_eval tool (also used in our study) and concluded that tie-breaking strategy can have substantive impact on conclusions about the relative effectiveness of different runs in terms of average precision. More recently, Ferro and Silvello [4] found similar issues in their study of rank-biased precision. McSherry and Najork [7] proposed efficient ways to compute various IR metrics in the presence of score ties (predating both above papers); broader adoption would have helped address many of the issues in this paper, but these techniques have not gained traction. The organizers of the TREC Microblog Tracks [8] have discovered that tie-breaking heuristics can have a large impact on effectiveness, since tweets are short and hence many retrieved results share the same score. In particular, reverse chronological sorting of tweets with the same score in ranked retrieval increases effectiveness and makes sense from the task perspective.
While most of the above cited papers focus on the implications of scoring ties for IR evaluation, others have examined different aspects of the phenomenon. For example, Wu and Fang [9] used score

ties to prioritize relevance signals in document ranking. Z. Yang et al. [12] explored different levels of score rounding as a way to accelerate query processing--for example, taking advantage of approximate scoring regimes. This relates to impact quantization [1], and JASS [6] is an example of a recent system that exploits approximate scoring for anytime ranking.
This study builds on previous work, but examines a new angle that to our knowledge has not been explored--the impact of score ties from the perspective of experimental repeatability. Recent ACM guidelines1 articulate the role of repeatability as an important foundation of scientific methodology with computational artifacts. Without repeatability, replicability and reproducibility are not possible. Building on this thread, our work makes a contribution towards firmly establishing repeatability in IR experiments using Lucene.
5 CONCLUSIONS
The conclusions from our examination of score ties are fairly clear: Although absolute differences in effectiveness metrics are relatively small--in the third decimal place at the most--these differences nevertheless pose challenges for regression testing. Without rigorous regression testing, it is difficult to put progress on solid footing in terms of software engineering best practices, since developers cannot be certain if a new feature introduced a bug. Fortunately, the solution to repeatable runs is fairly straightforward, which we have implemented: score ties should be broken by external collection ids. However, this comes with a measurable efficiency cost in terms of slowdown in query evaluation. As a concrete recommendation for navigating this tradeoff, we suggest that non-repeatable runs are acceptable for prototyping, but any permanent contributions to a codebase must pass slower regression tests that make repeatability a requirement.
Acknowledgments. This work was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada.
REFERENCES
[1] V. Anh, O. de Kretser, and A. Moffat. 2001. Vector-Space Ranking with Effective Early Termination. In SIGIR. 35­42.
[2] J. Arguello, M. Crane, F. Diaz, J. Lin, and A. Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR). SIGIR Forum 49, 2 (2015), 107­116.
[3] G. Cabanac, G. Hubert, M. Boughanem, and C. Chrisment. 2010. Tie-Breaking Bias: Effect of an Uncontrolled Parameter on Information Retrieval Evaluation. In CLEF. 112­123.
[4] N. Ferro and G. Silvello. 2015. Rank-Biased Precision Reloaded: Reproducibility and Generalization. In ECIR. 768­780.
[5] J. Lin, M. Crane, A. Trotman, J. Callan, I. Chattopadhyaya, J. Foley, G. Ingersoll, C. Macdonald, and S. Vigna. 2016. Toward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In ECIR. 408­420.
[6] J. Lin and A. Trotman. 2015. Anytime Ranking for Impact-Ordered Indexes. In ICTIR. 301­304.
[7] F. McSherry and M. Najork. 2008. Computing Information Retrieval Performance Measures Efficiently in the Presence of Tied Scores. In ECIR. 414­421.
[8] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. 2011. Overview of the TREC-2011 Microblog Track. In TREC.
[9] H. Wu and H. Fang. 2013. Tie Breaker: A Novel Way of Combining Retrieval Signal. In ICTIR. 72­75.
[10] P. Yang, H. Fang, and J. Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR. 1253­1256.
[11] P. Yang, H. Fang, and J. Lin. 2018. Anserini: Reproducible Ranking Baselines Using Lucene. Journal of Data and Information Quality 10, 4 (2018), Article 16.
[12] Z. Yang, A. Moffat, and A. Turpin. 2016. How Precise Does Document Scoring Need to Be? In AIRS. 279­291.
1 https://www.acm.org/publications/policies/artifact-review-badging

1128

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Critically Examining the "Neural Hype": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models

Wei Yang,1 Kuang Lu,2 Peilin Yang, and Jimmy Lin1
1 David R. Cheriton School of Computer Science, University of Waterloo 2 Department of Electrical and Computer Engineering, University of Delaware

ABSTRACT
Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed skepticism that neural ranking models were actually improving ad hoc retrieval effectiveness in limited data scenarios. He provided anecdotal evidence that authors of neural IR papers demonstrate "wins" by comparing against weak baselines. This paper provides a rigorous evaluation of those claims in two ways: First, we conducted a meta-analysis of papers that have reported experimental results on the TREC Robust04 test collection. We do not find evidence of an upward trend in effectiveness over time. In fact, the best reported results are from a decade ago and no recent neural approach comes close. Second, we applied five recent neural models to rerank the strong baselines that Lin used to make his arguments. A significant improvement was observed for one of the models, demonstrating additivity in gains. While there appears to be merit to neural IR approaches, at least some of the gains reported in the literature appear illusory.
ACM Reference Format: Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining the "Neural Hype": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10. 1145/3331184.3331340
1 INTRODUCTION
In a recent SIGIR Forum opinion piece, Lin [11] criticized the state of information retrieval research, making two main points. First, he lamented the "neural hype" and wondered that for "classic" ad hoc retrieval problems (limited relevance judgments and no behavioral data), whether neural ranking techniques represented genuine advances in effectiveness. As anecdotal evidence, he discussed two recent papers that demonstrated improvements over weak baselines, but in absolute terms, the reported results were no better than a well-tuned bag-of-words query expansion baseline.
In this paper, we attempt a rigorous evaluation of these claims. Focusing specifically on the test collection from the TREC 2004 Robust Track, a meta-analysis of the literature shows no upward trend in reported effectiveness over time. The best reported results
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331340

on the collection are from a decade ago, and no recent paper (using neural approaches or otherwise) has reported effectiveness close to those levels. Analysis of over one hundred papers confirms that the baseline comparison conditions are often not as strong as they should be. Thus, Lin's criticism that comparisons to weak baselines still pervade the IR community rings true.
As a follow up, we applied a number of recent neural ranking models from the MatchZoo toolkit [5] to rerank the strong baselines that Lin used to make his arguments. Out of five neural models, one was able to significantly improve upon Lin's results. In other words, the effectiveness gains from one neural model is additive with respect to a strong baseline--which provides evidence that neural IR can lead to "real" improvements. Nevertheless, four out of the five models examined were not able to significantly beat the baseline, suggesting that gains attributable to neural approaches are not as widespread as the literature suggests. The absolute average precision values we report are among the highest for neural models that we are aware of, although in absolute terms they are still much lower than the best known results.
2 META-ANALYSIS
The broader context of Lin's article is a recent series of papers that reflects a general angst (at least by some researchers) about the state of machine learning and its applications, in particular regarding empirical rigor and whether genuine advances are being made [12, 17]. These issues are not new, and similar discussions have been brewing in IR for a while. The landmark study by Armstrong et al. [3] in 2009 found that comparisons to weak baselines pervade the literature. A decade later, is this still the case?
We began by conducting a meta-analysis to rigorously examine Lin's criticism. His argument specifically focused on document ranking models that could be trained with commonly-available evaluation resources; specifically, such models should not require behavioral log data. As he argued, the test collection from the TREC 2004 Robust Track (Robust04 for short) is the best exemplar of such data. In order to restrict the scope of our meta-analysis, we followed this line of reasoning and compiled a list of all papers that have reported experimental results on Robust04.
We exhaustively examined every publication from 2005 to 2018 in the following venues to identify those that reported results on Robust04: SIGIR, CIKM, WWW, ICTIR, ECIR, KDD, WSDM, TOIS, IRJ, IPM, and JASIST. This was supplemented by Google Scholar searches to identify a few additional papers not in the venues indicated above. Our meta-analysis was conducted in January 2019, but after the paper acceptance we included a few more papers. A number of exclusion criteria were applied, best characterized as discarding corner cases--for example, papers that only used a subset of the topics or papers that had metrics plotted in a graph. In total,

1129

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

AP

0.400 0.375 0.350

non-neural models neural models TREC best TREC median Anserini RM3

0.325

0.300

0.275

0.250

0.225

0.200

05

06

07

08

09

10

11

12

13

14

15

16

17

18

19

Year

Figure 1: Visualization results on Robust04, where baseline and best AP scores are represented by empty and filled circles.

we examined 130 papers; of these, 109 papers contained extractable average precision values that formed the basis of the results reported below. Note that some papers did not report AP, and thus were excluded from consideration. All papers and associated data are publicly available for verification and further analysis.1
For each of the 109 papers, we extracted the highest average precision score achieved on Robust04 by the authors' proposed methods, regardless of experimental condition (ignoring oracle conditions and other unrealistic setups). We further categorized the papers into neural (18) and non-neural (91) approaches. Methods that used word embeddings but not neural networks directly in ranking were considered "neural" in our classification. From each paper we also extracted the authors' baseline: in most cases, these were explicitly defined; if multiple were presented, we selected the best. If the paper did not explicitly mention a baseline, we selected the best comparison condition using a method not by the authors (or based on previous work).
A visualization of our meta-analysis is presented in Figure 1. For each paper, we show the baseline and the best result as an empty circle and a filled circle (respectively), connected by a line. All papers are grouped by their publication year. Neural approaches are shown in blue, and non-neural approaches in red. We also show two regression trendlines, for non-neural (red) as well as neural approaches (blue). A number of reference conditions are plotted as horizontal lines: the best submitted run at the TREC 2004 Robust Track (TREC best) at 0.333 AP is shown as a solid black line, and the median TREC run under the "title" condition at 0.258 AP is shown as a dotted black line (TREC median). Finally, we show the effectiveness of an untuned RM3 run (i.e., default parameters) from the Anserini system (see Section 3).
Our meta-analysis shows that researchers still frequently compare against weak baselines: In 36 papers (33%), the baseline was below the TREC median. In fact, 25 papers (23%) reported best results that are below the TREC median and 65 papers (60%) reported best results that are below untuned RM3 in Anserini. Across all 109 papers, only 6 (5.5%) reported scores higher than the TREC best.
1 https://github.com/lintool/robust04- analysis

The highest AP we encountered was by Cormack et al. [4] in 2009, at 0.3686. Across over a decade's worth of publications, we see no obvious upward trend in terms of effectiveness.
Focusing specifically on the neural approaches, 8 out of 18 papers (44%) used a baseline that is below the TREC median; in fact, 4 papers (22%) reported best results that were still below the TREC median. The best results in most papers (12 or 67%) are still below untuned RM3 in Anserini. The highest reported scores we encountered were 0.3278 AP reported by Yang et al. [25] and 0.5381 nDCG@20 reported by MacAvaney et al. [13] (the authors did not report AP results and hence the paper was excluded from the 18). Only recently have neural models beat Lin's baselines, and the best neural models still remain quite a bit worse than the best non-neural models in terms of AP.
It is noted that not all papers purport to advance retrieval effectiveness (for example, papers about efficiency, proposing different frameworks, etc.). Nevertheless, we believe that our visualization provides an accurate high-level snapshot of the state of the field on this test collection. It appears that Lin's admonishments about continued use of weak baselines and skepticism about neural ranking models are warranted.
3 EXAMINING ADDITIVITY
Beyond revealing comparisons to weak baselines as widespread, Armstrong et al. [3] further examined why exactly this was methodologically problematic. Such comparisons lead to improvements that "don't add up" because of non-additive gains. The prototypical research paper on ad hoc retrieval proposes an innovation and compares it to a baseline that does not include the innovation; as expected, the innovation leads to increases in effectiveness. In this way, researchers collectively introduce dozens of different innovations, all of which improve on their respective baselines.
The key question, however, is whether the effectiveness gains of these innovations are additive. This might not occur, for example, if they exploit the same relevance signals. To put more precisely, does an improvement over a weak baseline still hold if applied to a strong baseline? If the answer is no, then gains over weak baselines may be illusory, and from a methodological perspective, we should

1130

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

not accept gains as "real" and "meaningful" unless they improve over strong baselines. Armstrong et al. [3] presented some evidence that many improvements are not additive, a finding which has been confirmed and expanded on by Kharazmi et al. [10]. However, the debate is not fully settled, as Akcay et al. [2] demonstrated additivity in search result diversification after better parameter tuning.
In the second part of our study, we explicitly examine the additivity hypothesis with respect to recent neural ranking models. Specifically, we applied neural ranking models on top of the strong baselines that Lin used to make his arguments, which showed that a well-tuned implementation of query expansion based on RM3 [1] beats the average precision reported in two recent neural IR papers, anonymously referred to as "Paper 1" and "Paper 2".
3.1 Experimental Setup
We began by replicating Lin's results with the Anserini toolkit [23], using exactly the same experimental settings (tokenization, stemming, etc.) described in an online guide.2 These runs used exactly the same cross-validation splits as Paper 1 (two-fold) and Paper 2 (five-fold), thus supporting a fair comparison.
On top of Lin's runs, we applied a number of neural ranking models from MatchZoo (version 1.0) [5]: DSSM [9], CDSSM [18], DRMM [7], KNRM [21], DUET [15]. These models were selected because they were specifically designed for ad hoc retrieval; other models available in MatchZoo, such as ARC-I [8], MV-LSTM [20], and aNMM [22] were mainly designed for short texts and not geared towards handling documents (which are much longer). MatchZoo is implemented in Keras, using the TensorFlow backend.
The neural models were deployed in a reranking setup, where the output of the models were linearly interpolated with scores from the RM3 baseline: score =  · scoreNN + (1 -  ) · scoreRM3. Note that this design allows the possibility of disregarding the RM3 scores completely, with  = 1. In our architecture, Anserini passes the raw text (minus markup tags) of the retrieved documents to MatchZoo, which internally handles document processing (tokenization, embedding lookup, etc.) prior to inference.
Following established practice, all models were trained using only the documents in the baseline RM3 runs that appear in the Robust04 relevance judgments. We used word vectors pre-trained on the Google News corpus (3 billion words). The entire test collection has 249 topics (with relevance judgments). For the two-fold cross-validation condition to match Paper 1, we randomly sampled 25 topics from the training fold as the validation set; the other fold serves as the test set. For the five-fold cross-validation condition to match Paper 2, we selected three folds for training, one fold for validation, and used the remaining fold for testing. In all cases, we selected model parameters to maximize average precision on the development test. The weight  for score interpolation with RM3 was selected in the same manner. We set the maximum training epochs to five and used early stopping with five patience iterations. The batch size was set to 100 and all "title" queries were padded to ten tokens. Other hyperparameters were tuned using the validation set. All models were trained on an NVIDIA GeForce GTX 1080 GPU; it takes about one minute to train the DRMM model and a few hours for the others.
2 http://anserini.io

Condition
BM25 [7] DRMM [7]

AP
0.255 0.279

2-fold results from Paper 1

Paper 1

0.2971

BM25+RM3

0.2987

+ DSSM

0.2993

+ CDSSM

0.2988

+ DRMM

0.3126

+ KNRM

0.3033

+ DUET

0.3021

5-fold results from Paper 2

Paper 2

0.272

BM25+RM3

0.3033

+ DSSM

0.3026

+ CDSSM

0.2995

+ DRMM

0.3152

+ KNRM

0.3036

+ DUET

0.3051

NDCG@20
0.418 0.431
0.4398 0.4467 0.4455 0.4646 0.4423 0.4471
0.4514 0.4491 0.4468 0.4718 0.4441 0.4502

Table 1: Experimental results applying neural models to rerank a strong baseline;  indicates statistical significance.

3.2 Results
Our experimental results are shown in Table 1. Of all the neural models we examined in MatchZoo, only the original DRMM paper evaluated on Robust04; the first two rows show the DRMM results and their BM25 baseline (both copied from the original paper [7]). The paper reported a fairly substantial gain in AP, but based on our meta-analysis, the baseline is right around the TREC median and the DRMM score is still below Anserini RM3.
The second and third blocks of Table 1 report results from the two-fold and five-fold cross-validation conditions to match Paper 1 and Paper 2. Results from Paper 1 and Paper 2 are provided for reference (neither report NDCG@20). Note that our BM25+RM3 results are slightly higher than the results reported by Lin [11] because of code improvements after the publication of the article. We see that our "baseline" already beats the best results reported in Paper 1 and Paper 2. Based on our meta-analysis, an AP score of 0.3033 (five-fold) beats 86 out of 109 papers (79%) and all but two neural models.
Experiments show that reranking our strong baseline with neural models yields small improvements in many cases.3 Statistical significance of metric differences was assessed using a paired twotailed t-test: the only observed significant difference is with DRMM (p = 0.0032). Even correcting for multiple hypothesis testing (e.g., Bonferroni correction), this difference remains statistically significant. Our five-fold cross-validation result of 0.3152 with DRMM beats 98 out of 109 papers (90%) and all but one neural model; while this can certainly be characterized as a competitive result based on our meta-analysis, it is still quite far from the best known result on Robust04 (0.3686 AP).
3The reader might wonder how it is possible that a neural model actually makes results worse, since a setting of  = 1.0 would ignore the neural model scores. However, due to cross-validation, this may not be the learned parameter.

1131

Short Research Papers 2C: Search

SIGIR '19, July 21­25, 2019, Paris, France

3.3 Discussion
We specifically tackle a number of shortcomings and limitations of our study. First, only the five models implemented in MatchZoo were examined, and the quality of those implementations might be questioned. We concede this point, and so our findings apply to only the MatchZoo implementations of the various neural models. Nevertheless, MatchZoo has gained broad acceptance in the community as a solid experimental platform on which to explore neural ranking tasks.
The next obvious objection is that we've only examined these particular five neural ranking models. This, of course, is valid criticism, but an exhaustive study of all models would be impractical. We argue that the models selected are representative of the types of approaches pursued by researchers today, and that these results suffice to support at least some tentative generalizations.
The next criticism we anticipate concerns our evidence combination method, simple linear interpolation of scores. While there are much more sophisticated approaches to integrating multiple relevance signals, this approach is commonly used [6, 16, 19, 24, 26]. In a separate experiment where we explicitly ignored the retrieval scores, effectiveness was significantly lower. We leave open the possibility of better evidence aggregation methods, but such future work does not detract from our findings here.
Another possible criticism of our study is the limited data condition, since we are training with only TREC judgments. Surely, the plethora of training data that comes from behavioral logs must be considered. While we do not dispute the effectiveness of neural approaches given large amounts of data, exploring the range of data conditions under which those models work is itself interesting. We note a stark contrast here: for NLP tasks, researchers have been able to extract gains from neural approaches with only "modest" amounts of data (as a rough definition, datasets that can be created outside an industrial context without behavioral logs). If it is the case that IR researchers cannot demonstrate gains except with data only available to large companies--this in itself would be an interesting statement about neural IR. Mitra and Craswell [14] classified DRMM as a lexical matching modeling (in fact, the model explicitly captures tf and idf). DUET is a hybrid lexical/semantic matching model, while the others are semantic matching primarily. One possible interpretation of our findings is that TREC judgments alone are not sufficient to train semantic matching models.
Finally, there is a modeling decision worth discussing: In our experiments, all models except for DRMM truncate the length of the input document to the first K tokens (the text2_maxlen parameter in MatchZoo). Somewhat surprisingly, this is a practical issue that does not appear to be discussed in previous papers, but has a direct impact on model training time. We performed a coarse-grained sweep of the parameter and discovered that a value of K above 200 appears to be sufficient and doesn't seem to alter effectiveness substantially (one contributing factor might be the writing style of news articles). The results reported here use a K value of 500, which is longer than most documents, but still yields reasonable model training times. We believe that document truncation can be ruled out as a reason why four of the five neural ranking models do not yield additive improvements.

4 CONCLUSIONS
We believe that our study supports the following conclusions: At
least with respect to the Robust04 test collection, it does not appear
that the IR community as a whole has heeded the admonishments
of Armstrong et al. [3] from a decade ago. Our meta-analysis shows
that comparisons to weak baselines still pervade the literature. The
high water mark on Robust04 in terms of average precision was
actually set in 2009, and no reported results since then (neural or
otherwise) come close. Focusing specifically on five neural ranking
models in MatchZoo, we find that only one is able to significantly
improve upon a well-tuned RM3 run in a reranking setup on this
collection. That is, at least under this limited data scenario, effec-
tiveness gains from most neural ranking models do not appear to
be additive. While neural networks no doubt represent an exciting
direction in information retrieval, we believe that at least some of
the gains reported in the literature are illusory.
Acknowledgments. This work was supported in part by the Natu-
ral Sciences and Engineering Research Council (NSERC) of Canada.
REFERENCES
[1] Abdul-Jaleel et al. 2004. UMass at TREC 2004: Novelty and HARD. TREC. [2] Akcay et al. 2017. On the Additivity and Weak Baselines for Search Result
Diversification Research. ICTIR. [3] Armstrong et al. 2009. Improvements That Don't Add Up: Ad-hoc Retrieval
Results Since 1998. CIKM. [4] Cormack et al. 2009. Reciprocal Rank Fusion Outperforms Condorcet and Indi-
vidual Rank Learning Methods. SIGIR. [5] Fan et al. 2017. MatchZoo: A Toolkit for Deep Text Matching. arXiv:1707.07270. [6] Ganguly et al. 2015. Word Embedding Based Generalized Language Model for
Information Retrieval. SIGIR. [7] Guo et al. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. CIKM. [8] Hu et al. 2014. Convolutional Neural Network Architectures for Matching Natural
Language Sentences. NIPS. [9] Huang et al. 2013. Learning Deep Structured Semantic Models for Web Search
using Clickthrough Data. CIKM. [10] Kharazmi et al. 2016. Examining Additivity and Weak Baselines. TOSI 34, 4
(2016), Article 23. [11] Lin. 2018. The Neural Hype and Comparisons Against Weak Baselines. SIGIR
Forum 52, 2 (2018), 40­51. [12] Lipton and Steinhardt. 2018. Troubling Trends in Machine Learning Scholarship.
arXiv:1807.03341. [13] MacAvaney et al. 2019. CEDR: Contextualized Embeddings for Document Rank-
ing. arXiv:1904.07094. [14] Mitra and Craswell. 2017. Neural Models for Information Retrieval.
arXiv:1705.01509. [15] Mitra et al. 2017. Learning to Match using Local and Distributed Representations
of Text for Web Search. WWW. [16] Rao et al. 2019. Multi-Perspective Relevance Matching with Hierarchical
ConvNets for Social Media Search. AAAI. [17] Sculley et al. 2018. Winner's Curse? On Pace, Progress, and Empirical Rigor. ICLR
Workshops. [18] Shen et al. 2014. Learning Semantic Representations using Convolutional Neural
Networks for Web Search. WWW. [19] Van Gysel et al. 2018. Neural Vector Spaces for Unsupervised Information
Retrieval. TOIS 36, 4 (2018), Article 38. [20] Wan et al. 2016. A Deep Architecture for Semantic Matching with Multiple
Positional Sentence Representations.. AAAI. [21] Xiong et al. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. SIGIR. [22] Yang et al. 2016. aNMM: Ranking Short Answer Texts with Attention-Based
Neural Matching Model. CIKM. [23] Yang et al. 2018. Anserini: Reproducible Ranking Baselines Using Lucene. JDIQ
10, 4 (2018), Article 16. [24] Yang et al. 2019. End-to-End Open-Domain Question Answering with BERTserini.
NAACL. [25] Yang et al. 2019. Simple Applications of BERT for Ad Hoc Document Retrieval.
arXiv:1903.10972. [26] Zamani and Croft. 2016. Embedding-based query language models. ICTIR.

1132

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Dynamic Sampling Meets Pooling

Gordon V. Cormack, Haotian Zhang, Nimesh Ghelani, Mustafa Abualsaud, Mark D. Smucker, Maura R. Grossman, Shahin Rahbariasl, and Amira Ghenai
University of Waterloo, Ontario, Canada

ABSTRACT
A team of six assessors used Dynamic Sampling (Cormack and Grossman 2018) and one hour of assessment effort per topic to form, without pooling, a test collection for the TREC 2018 Common Core Track. Later, official relevance assessments were rendered by NIST for documents selected by depth-10 pooling augmented by move-to-front (MTF) pooling (Cormack et al. 1998), as well as the documents selected by our Dynamic Sampling effort. MAP estimates rendered from dynamically sampled assessments using the xinfAP statistical evaluator are comparable to those rendered from the complete set of official assessments using the standard trec_eval tool. MAP estimates rendered using only documents selected by pooling, on the other hand, differ substantially. The results suggest that the use of Dynamic Sampling without pooling can, for an order of magnitude less assessment effort, yield informationretrieval effectiveness estimates that exhibit lower bias, lower error, and comparable ability to rank system effectiveness.
ACM Reference Format: Gordon V. Cormack, Haotian Zhang, Nimesh Ghelani, Mustafa Abualsaud, Mark D. Smucker, Maura R. Grossman, Shahin Rahbariasl, and Amira Ghenai. 2019. Dynamic Sampling Meets Pooling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331354
1 INTRODUCTION
This work evaluates the first in situ application of Dynamic Sampling (DS) [3] to construct a test collection for large-scale information retrieval (IR) evaluation (see [9]). In contrast to the de facto standard pooling method, DS estimates IR effectiveness measures like MAP by applying a statistical estimator to a sample of a very large universe of documents, drawn independently from any system to be evaluated. DS promises to yield unbiased estimates not only for the methods known at the time of construction, but also for methods yet-to-be invented. Prior to this effort, DS had been validated only through post-hoc simulation.
In preparation for the TREC 2018 Common Core Track [2], we built a test collection by employing a combination of Continuous Active Learning (CAL) [5], Interactive Search and Judging (ISJ) [7], and DS to identify a sample of 19,161 documents from a universe of 39,214, each of which we assessed to be relevant or not relevant. The 19,161 documents were shared with NIST, where they were
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331354

again assessed, along with 8,902 documents identified by depth-10 pooling, and 1,010 identified by move-to-front (MTF) pooling [7] to form the 26,233 official TREC relevance assessments (qrels).
A primary concern was the speed with which we could render the assessments necessary to form a viable test collection, so as to conform to our particular resource constraints, as well as those of anyone else who might wish to employ our method. After an exploratory phase in which one author used a rudimentary CAL tool and search engine to find as many relevant documents as possible in about 13 minutes per topic, a separate team of five of the authors spent about 45 minutes per topic using HiCAL 1 [1, 12], which we adapted for this purpose, to conduct further searches and to draw a dynamic stratified sample of 300 additional documents per topic for assessment. In total, about 50 person-hours (i.e., one hour per topic) were devoted to compiling our relevance assessments.
To evaluate the outcome of our DS effort, we consider the following five questions:
· How nearly all of the relevant documents were included in the universe of documents identified using DS, from which the sample was drawn?
· What is the accuracy of MAP estimates derived from the sample, measured by the average difference (bias) and RMS difference between the estimates and ground truth, per the official NIST assessments?
· What is the accuracy with which systems are ranked according to MAP estimates, measured by the rank correlation ( or AP ), with the ranking afforded by ground truth?
· How do these outcomes compare to the NIST depth-10 and MTF pooling efforts?
· How might our or NIST's efforts have yielded better outcomes, with a different allocation of assessment effort?
Overall, our results indicate that assessment budgets would be better spent if allocated to DS rather than to pooling.
2 SPEEDY ASSESSMENT
In the first of three distinct assessment phases, one of the authors used CAL, as implemented in the TREC 2015 Total Recall Track Baseline Implementation (BMI),2 to identify potentially relevant paragraphs, which were rendered for assessment, along with the document containing them, on an ASCII terminal. Of 3,601 documents rendered during this phase, 1,391 were assessed relevant. For a few topics where no relevant documents were seen within the first several dozen rendered documents, the assessor reverted to a search engine,3 finding a total of 47 more relevant documents over all of these topics. Nevertheless, the assessor found no relevant documents for several topics, and fewer than ten relevant
1See https://github.com/hical. 2See http://cormack.uwaterloo.ca/trecvm/. 3See http://stefan.buettcher.org/cs/wumpus/index.html.

1217

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Method: DS NIST Depth-10 Depth-10 + MTF

Avg. Coverage: 0.88 0.82 0.47

0.49

Min. Coverage: 0.58 0.48 0.11

0.13

Max. Coverage: 1.00 1.00 1.00

1.00

Table 1: Coverage of document-selection methods.

documents for 20 topics. Phase 1 consumed a total of 11.1 hours (i.e., 13.2 minutes per topic).
The second and third phases were conducted by a team of five different authors, who used the HiCAL system to render assessments for 10 topics each. Phase 2 involved the use of HiCAL's search and full-document-display mode to find more documents relevant to the 20 topics for which the first phase had found fewer than 10. Phase 2 was allocated 30 minutes of assessment time for each of these 20 topics (i.e., 12 minutes per topic overall).
Phase 3 involved the use of HiCAL's CAL and paragraph-onlydisplay mode to present paragraphs, excerpted from potentially relevant documents, for assessment. HiCAL was modified to present excerpts from only a sample of documents, selected using DS with strategy D25 [3] (see Section 7), and a budget of 300 assessments per topic. The initial training set consisted of all assessments from phases 1 and 2. Assessment time for phase 3 averaged 33 minutes per topic, bringing the total for all three phases to about one hour per topic.4
3 COVERAGE
We define coverage to be the fraction of all relevant documents in the DS universe. In other words, coverage is the recall of the DS effort, before sampling and before relevance assessment. In order to estimate coverage of our assessment strategy compared to others, it is necessary to estimate the number of relevant documents for each topic. To this end, DS provides an unbiased statistical estimate of the number of relevant documents in the universe it identifies as the sample space from which its sample is drawn. Here, the size of the universe was 39,214 documents, from which a sample of 19,161 was drawn. Pooling and MTF independently identified 1,305 documents--of which 503 were relevant--from the DS universe, which were counted directly and removed from the sample space. Pooling and MTF identified 404 additional relevant documents outside the universe. Overall, our best estimate indicates that there are 5,457 relevant documents in the corpus.
Table 1 shows the average, minimum, and maximum coverage over 50 topics achieved by the construction methods under consideration. The DS universe covers 88% of all relevant documents, on average, and 58%, in the worst case, for this collection. The NIST qrels have lower coverage because, although they include more assessments, they do not constitute a statistical sample and cannot be extrapolated to a larger population. The depth-10 pool, whether augmented or not by MTF, has substantially inferior recall.
4Due to a system misconfiguration, assessors spent several additional minutes in a false start for phase 3, the results of which were discarded.

4 ACCURACY

Figure 1 shows scatterplots and summary statistics comparing the MAP estimates afforded by DS using the xinfAP estimator5 [11]

to the official TREC results, for each of the runs submitted to the

TREC 2018 Common Core Track. Points denoted "M" represent

so-called manual runs; points denoted "R" indicate runs that relied

on legacy relevance assessments for the same topics but a different

corpus; points denoted " " indicate fully automatic runs. We see

that our DS assessments yield low bias6 (b = 0.02) and RMS error

(RMSE

=

0.02),

with

negligible

variance

( 2

=

2
b

-

RMSE2



0).

 = 0.88 and AP = 0.83 correlation scores [10] are typical of those

arising from inter-assessor disagreement.

The top right panel shows the result of substituting NIST as-

sessments in place of our own, to eliminate inter-assessor disagreement.  = 0.95 and AP = 0.92 are substantially higher, while bias essentially vanishes, exposing a small variance as evidenced by RMSE = 0.01.

The bottom two panels show results for depth-10 pooling, and

depth-10 pooling augmented by MTF. Rank correlations are slightly

lower, while bias and error are substantially higher.

5 STATISTICAL GROUND TRUTH
The method we employed to estimate coverage (Section 3) yields a revised but statistically valid sample of the DS universe, augmented by 404 relevant documents outside the DS universe. Statistical estimates derived from this sample using xinfAP--which we dub statistical ground truth--should, in theory, be more accurate than the official ground truth derived from from the same assessments using trec_eval.
We can evaluate the extent to which the statistical and official ground truths agree. The left panel of Figure 2 shows strong but imperfect agreement. The right panel of Figure 2 and the top-right panel of Figure 1, evaluate accuracy of the same NIST-assessed DS sample, according to the statistical and official ground truth, respectively. The statistical ground truth indicates much higher accuracy. If the statistical ground truth is indeed the gold standard, this result suggests that the dynamic sample alone--without any documents discovered by pooling or MTF--may also yield ground truth more accurate than the official ground truth.

6 INCREASING COVERAGE
We used a fixed budget of 300 assessments for the third phase of our assessments, which had previously been shown to achieve good results for the D25 sampling strategy [3] (see Section 7). Arguably good results were achieved here; however, Table 1 indicates that coverage for at least one topic was less than 60%. We investigated whether the cause of occasionally poor coverage was assessor disagreement or inadequacy of the assessment budget.
Figure 3 shows coverage for each of the 50 topics, as well as the average, as a function of the total number of assessments for the three phases. We see that the slope for most of the curves is near-zero when the budget is exhausted. But for several of the topics--notably those with coverage of less than about 80%--the slope is noticeably positive, indicating that, had the budget been extended
5
trec.nist.gov/data/clinical/sample_eval.pl 6The arithmetic mean of error.

1218

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

0.5

0.5

q Automatic R Feedback M Manual

q Automatic R Feedback M Manual

0.4

infAP as per DS irels, NIST assessments

0.4

infAP as per DS irels and assessments

0.3

0.2

Tau = 0.88 Tau AP = 0.83 RMSE = 0.02 Bias = -0.02

MM

q

RMRRRRRRRRRR

q q q

q RqqRqqqRqRRRRRqqRRqqq q qRqRqRRRR R

q R

R

qq

qqq

0.1

0.2

0.3

M M

Tau = 0.95

Tau AP = 0.92 RMSE = 0.01 Bias = 0.00

RMRRRRRRRRR qqq

qRqqRqRRRRqRqRqqqRqRRRRqqRR

qR

q

qq

R qR

qq

qqq

0.1

0.0

0.0

0.5

0.4

MAP as per qrels Depth-10 + MTF

0.3

MAP as per qrels Depth-10

0.0

0.1

0.2

0.3

0.4

0.5

MAP as per NIST qrels and assessments

q Automatic R Feedback M Manual
MM

Tau = 0.93 Tau AP = 0.89 RMSE = 0.05 Bias = 0.04
q qq qR R

qqq RqqRqRRRRqRqqRqqqqRqqRRRRqRR q
q q R

RMRRRRRRRRRR

0.2

0.3

0.4

0.5

0.0

0.1

0.2

0.3

0.4

0.5

MAP as per NIST qrels and assessments

q Automatic R Feedback M Manual
Tau = 0.94 Tau AP = 0.91 RMSE = 0.05 Bias = 0.05

M M MRRRRRRRRRR R
RqqRqRRRRqRqqRqqqqRqqRRRRqqRRqqqq q q R

q qq

qR R

0.2

0.1

0.1

q q
qqq

q q
qqq

0.0

0.0

0.0

0.1

0.2

0.3

0.4

0.5

MAP as per NIST qrels and assessments

0.0

0.1

0.2

0.3

0.4

0.5

MAP as per NIST qrels and assessments

Figure 1: Accuracy of MAP estimates using DS vs. pooling methods, compared to official TREC 2018 Common Core Track evaluation. The top-left panel shows results for DS with relevance assessments by the authors; the top-right panel shows results for DS with official relevance assessments by NIST. The bottom-left panel shows results for the depth-10 pool with relevance assessments by NIST; the bottom-right shows results for the depth-10 pool augmented by MTF, with relevance assessments by NIST.

0.5

0.5

q Automatic R Feedback M Manual

q Automatic R Feedback M Manual

0.4

infAP as per DS irels, NIST assessments

0.4

MAP as per NIST qrels and assessments

0.3

0.2

Tau = 0.96 Tau AP = 0.94 RMSE = 0.01 Bias = 0.00

M M
RMRRRRRRRRR
qqq qRRqRRqRRRqRqqRqRqqRqRRRqRq Rq

R qR

q qq

q q

qq

0.1

0.2

0.3

M M

Tau = 0.98

Tau AP = 0.97 RMSE = 0.00 Bias = 0.00

MRRRRRRRR R qq

qRRqRRqRRRqRqqRqRqqRqRRRqRq

Rq

q

qq

R qR

qq

qq

0.1

0.0

0.0

0.0

0.1

0.2

0.3

0.4

0.5

infAP as per DS+NIST irels, NIST assessments

0.0

0.1

0.2

0.3

0.4

0.5

infAP as per DS+NIST irels, NIST assessments

Figure 2: Accuracy of official NIST qrels (left) and DS with NIST assessments (right), according to statistical ground truth.

1219

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

1.0

0.9

0.8

0.7

System Recall

0.6

0.5

0.4

0.3

0.2

0.1
0.0 0.0

0.2

0.4

0.6

Review Effort (% Assessments)

50 topics Averaged

0.8

1.0

Figure 3: Coverage as a function of overall assessment effort.

for these topics, higher coverage would have been achieved. We also note that several topics achieving ostensibly high coverage have substantial slope when the budget is exhausted, suggesting a shortfall in the ground truth estimate of the number of relevant documents. These results suggest that using the "Knee Method" as a stopping criterion, which has been shown to work well for CAL [4], might be preferable to a fixed assessment budget.
A similar approach might also have yielded better results for MTF, which was constrained by a restrictive assessment budget [2].
7 DISCUSSION AND LIMITATIONS
For brevity, we present MAP results estimated by xinfAP, consistent with common practice. In a companion article [6], Cormack and Grossman show that better estimates of MAP, as well as precision at cutoff (P@k), rank-biased-precision (RBP), and normalized discounted cumulative gain (NDCG) may be derived from a DS test collection, using the purpose-built dyn_eval estimator.
Estimates derived from DS assume that the DS universe includes substantially all relevant documents; DS yields an unbiased, nonuniform statistical sample from which MAP and other evaluation measures are derived. Hence, the effectiveness of any run--not just a member of an evaluation pool--may be evaluated using a DS collection.
One can increase coverage by increasing the size of the universe, at the expense of higher variance. The evidence presented here suggests the the DS universe does indeed contain a substantial majority of the relevant documents. Future work may explore the influence of three parameters that balance the tension between coverage, sampling budget, and skew of the sampling rate in favour of likely relevant documents. The sampling budget of 300 was occasioned by our target of one hour of assessment time per topic. Given the fact that we had no pool of runs, we were further constrained to use content-only features for the learner. Within these constraints, strategies D12, D25, and D50, reflecting different tradeoffs between coverage and sampling rate, appeared equally good [3], and we chose the median. Further investigation might yield a better tradeoff. If it were logistically feasible, a flexible assessment budget with an average of 300 documents per topic, and an amenable stopping criterion, might have yielded better results, for the same overall assessment budget.
We have measured the effect of assessor disagreement only on the MAP estimates derived from identical sets of assessments. Our

results show that DS conducted by one set of assessors (the authors) can achieve high coverage in the eyes of another (the NIST assessors). If NIST assessors had conducted the DS assessments, would coverage have been higher, and, if so, how much higher?
Finally, we note that the relevance determinations of assessors are influenced by context; in particular, the order and richness of the documents they are shown [8]. DS assessors are shown the most-likely relevant documents first, which suggests they are likely be more stringent in their judgment of relevance, at least at the outset. As the density of relevant documents inevitably decreases, some assessors may have a tendency to "reach" and thus, be more likely to judge a document relevant than at the outset. Our design controls for this possible effect with respect to the NIST assessments for the DS sample, because the NIST assessors were unaware of the order in which the DS documents were discovered, or, indeed, whether a document was identified by pooling, by DS, or both. Our assessments with respect to the DS sample, and NIST's assessments with respect to MTF, might have been so influenced.
8 CONCLUSION
Independent of NIST assessments or any pool of submitted runs, a small team of researchers spent 50 hours to create a set of sampled relevance assessments that effectively scores and ranks systems according to MAP over 50 topics. This level of effort represents an order-of-magnitude reduction in human effort, compared to typical efforts like TREC, and does not rely on pooling, which is both logistically challenging and a potential source of bias against systems not contributing to the pool. DS avoids this source of bias and, as both theory and empirical evidence show, does not introduce bias against the TREC runs that had no influence on the DS selection strategy, or our relevance assessments.
ACKNOWLEDGMENTS
Special thanks to Ellen M. Voorhees for integrating the results of our DS process into the official NIST assessment effort.
REFERENCES
[1] Abualsaud, M., Ghelani, N., Zhang, H., Smucker, M. D., Cormack, G. V., and Grossman, M. R. A system for efficient high-recall retrieval. In SIGIR 2018.
[2] Allan, J., Harman, D., Kanoulas, E., and Voorhees, E. TREC 2018 Common Core Track overview. In TREC 2018.
[3] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [4] Cormack, G. V., and Grossman, M. R. Engineering quality and reliability in
technology-assisted review. In SIGIR 2016. [5] Cormack, G. V., and Grossman, M. R. Evaluation of machine-learning protocols
for technology-assisted review in electronic discovery. In SIGIR 2014. [6] Cormack, G. V., and Grossman, M. R. Unbiased low-variance estimators for
precision and related information retrieval effectiveness measures. In SIGIR 2019 (2019). [7] Cormack, G. V., Palmer, C. R., and Clarke, C. L. Efficient construction of large test collections. In SIGIR 1998. [8] Roegiest, A., and Cormack, G. V. Impact of review-set selection on human assessment for text classification. In SIGIR 2016. [9] Sanderson, M., et al. Test collection based evaluation of information retrieval systems. Foundations and Trends® in Information Retrieval 4, 4 (2010). [10] Yilmaz, E., Aslam, J. A., and Robertson, S. A new rank correlation coefficient for information retrieval. In SIGIR 2008. [11] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008. [12] Zhang, H., Abualsaud, M., Ghelani, N., Smucker, M. D., Cormack, G. V., and Grossman, M. R. Effective user interaction for high-recall retrieval: Less is more. In CIKM 2018.

1220

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Unbiased Low-Variance Estimators for Precision and Related Information Retrieval Effectiveness Measures

Gordon V. Cormack
University of Waterloo
ABSTRACT
This work describes an estimator from which unbiased measurements of precision, rank-biased precision, and cumulative gain may be derived from a uniform or non-uniform sample of relevance assessments. Adversarial testing supports the theory that our estimator yields unbiased low-variance measurements from sparse samples, even when used to measure results that are qualitatively different from those returned by known information retrieval methods. Our results suggest that test collections using sampling to select documents for relevance assessment yield more accurate measurements than test collections using pooling, especially for the results of retrieval methods not contributing to the pool.
ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2019. Unbiased Low-Variance Estimators for Precision and Related Information Retrieval Effectiveness Measures. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10. 1145/3331184.3331355
1 INTRODUCTION
The thesis of this work is that information retrieval (IR) test collections [5] should use relevance assessments for documents selected by statistical sampling, not depth-k pooling or dynamic pooling methods like hedge [1]. To this end, we define the dynamic (dyn) estimator1 for precision at cutoff (P@k), which is easily generalized to rank-biased precision (RBP) and discounted cumulative gain (DCG). In contrast to the well-known inferred (inf ) and extended inferred (xinf ) estimators [8], dyn is unbiased. In comparison to the statistical (stat) estimators [4], dyn has substantially lower variance for a given assessment budget.
When used to estimate mean P@k (MP@k) and other measures over a set of topics interpreted as a sample of a larger population of topics, the dyn estimator, coupled with an amenable sampling strategy, and a larger sample of topics, can achieve lower variance than pooling methods, for a given assessment budget.
dyn is a Horvitz-Thompson estimator [3] for the difference between the true value of P@k and a learned prior estimate. In the special case where the prior estimate is fixed at 0, dyn P@k is equivalent to stat P@k, which is also unbiased but, according to the theory underlying this work, higher variance.
1See open source dyn_eval implementation at cormack.uwaterloo.ca/sample.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331355

Maura R. Grossman
University of Waterloo

inf P@k and xinf P@k, in contrast, compute a separate esti-

mate for each stratum of sampled assessments. However, no such

estimate is possible whenever a retrieval result to be measured

contains none of the assessed documents from a particular stratum.

To resolve this singularity, inf uses Lindstone smoothing, which

effectively substitutes a default constant value c when the estimate

would otherwise be 0 . The published descriptions of inf and xinf
0

use c = 1 ; the reference implementation used for this study uses
2

c

=

1 3

.

The

net

effect

is

that

inf

measurements are biased toward

the value c, with small values of P@k being overestimated, and

large values underestimated.

The theory that an estimator is unbiased may be falsified by iden-

tifying any possible combination of topics, documents, relevance

assessment sample, and retrieval results, for which it yields a biased

estimate. The experiment described here takes as ground truth the

documents, topics, and relevance assessments from the TREC 8

Ad Hoc test collection [6], assuming the topics to be a random

sample drawn from a population of topics with precisely the same

mean and sample variance, and the assessments to be complete and

infallible. To measure bias, we consider two sets of retrieval results:

the 129 "TREC runs" submitted to TREC for evaluation, and 129

"dual runs" engineered to have precisely the same true P@k as the

TREC runs, in 1-1 correspondence. The dual runs were formed by

randomly permuting the ranks of the relevant documents in each

run, while preserving the ranks of non-relevant documents.
The results of this adversarial testing show no bias for dyn or stat, small but significant bias for xinf (which subsumes inf ), and

very large bias, both within and between the TREC and dual runs,

for depth-k pooling and hedge. To address the argument that biased

measurements do not matter as long as they accurately rank the relative effectiveness of the runs, we calculate median  correla-

tion between the rankings achieved by repeated measurements of

MP@k versus ground truth, for the TREC runs, the dual runs, and

their union.

2 MEASURING P@K

Given a document d  D and a topic t  T , binary relevance

rel(d) = 1 indicates that an infallible assessor would judge d rel-

evant to t; rel(d) = 0 indicates otherwise. Given a ranked list of

documents r = r1r2 · · · rn from D and a topic t, our concern is how

1
best to measure P@k = k

min(k , n ) i =1

rel(ri

),

understanding

that

the

infallible assessor is a hypothetical entity whose judgments can at

best be approximated by real assessors, under controlled conditions,

rendering rel(d) for a subset J of all possible d. In this work, we

assume the fiction that rel(d) =

rel(d )

(d  J ) .

0

(d / J )

If J is a statistical sample of D drawn without replacement such

that each d  D is drawn with prior probability  (d) = Pr[d  J ] >

945

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

0, we have the unbiased stat estimator:

stat P@k

=

1 k

min(k , n ) i =1

rel(ri )  (ri )

.

The dyn estimator harnesses a model M(d) estimating Pr[rel(d) =

1]:

min(k , n )

1

1

dyn P@k = k

i=1 M(ri ) +  (ri ) ·

rel(ri ) - M(ri ) 0

(ri  J ) . (ri / J )

Provided M(d) is independent of the outcome d  J , dyn P@k is unbiased. If J is a stratified sample drawn without replacement, this constraint is met when M(d) is derived from {rel(d )|d   J \ strat(d)}, where strat(d) is the stratum from which d is drawn.
A given IR method yields a different ranking r (t), with a particular P@k, denoted P@k(t), for any given topic t  T . MP@k = E[P@k(T )] quantifies the effectiveness of the method. Given a uniform random sample T from T , the dyn unbiased estimate of
MP@k is
1 dyn MP@k = |T | t T dyn P@k(t) .

3 STRATIFIED SAMPLING
The simplest sampling strategy that we consider divides J into equal-sized strata with equal sampling rates. For this strategy, M is learned using cross-validation, holding out each stratum in turn,
and using the remaining strata for training. In the present study we used logistic regression to maximize (logit) likelihood L(d) over the training examples. We calibrated these estimates by adding a constant prior (log odds) p, and converting to probability:

1 M(d) = 1 + exp(-(p + L(d))) . p was determined numerically to solve

M(d) =
d

r el (d )  (d) = E

d

rel(d) ,

so that the predicted number of relevant documents in the training examples would match the sample estimate.

4 PROPORTIONAL TO SIZE SAMPLING
Variance can be reduced using a model P(d) that predicts Pr[rel(d)] prior to determining  (d), and, as nearly as possible, makes  (d)  P(d). P may be derived from any relevance-ranking method. In
the present study we used reciprocal-rank fusion of the rankings submitted to TREC. We partitioned D into N strata of exponentially
increasing size, so that higher-ranked documents were assigned to smaller strata. An equal number of documents n were drawn from each stratum Si , and the exponential growth rate  calculated numerically to cover D when the size of the smallest stratum |S0|= s:
N -1
 = min  . s · s · (1 + )i  |D|
i =0
|Si |= s · (1 +  )i (i  0..N - 2)
N -2
|SN -2 |= |D|- |Si | .
i =0

5 TESTING BIAS AND VARIANCE
Error err is the difference between a measurement est and ground
truth tru. Bias b is the average of err over repeated measurements. MSE is the average of err2. Variance e2rr = MSE - b2. To test our claim that dyn MP@k is unbiased, ground truth need not be perfect,
and the retrieval results to be measured need not be derived from real IR systems, as long as they are independent of J .
In evaluating an estimator, it is necessary to consider that MP@k is defined over a population T , whereas dyn MP@k and other estimators are derived from a set of topics T , deemed to be a random sample of T . The expectation, and therefore the bias of an estimate, is the same, whether we consider it to be an estimate of MP@k = E P@k(T ) or of E P@k(T ). On the other hand, even ground truth for T has non-zero variance T2 when used as an estimator for MP@k :

T2

=

1 |T |

Var P@k(T ) 

1 |T |-1

Var P@k(T ) .

The inequality holds in expectation, and for the purposes of ground

truth, we deem it equal:

T2

=

1 |T |-1

Var P@k(T ) .

The overall variance of an estimator is therefore

e2st = e2rr + T2 . Similarly, RMSE depends on what is being estimated:

RMSerr =

2
b

+e2rr

,

RMSET = T ,

RMSEest = b2 +e2rr + T2 .
6 EXPERIMENT
Using the TREC 8 data as ground truth, we compared the bias and variance of dyn MP@10 to competing statistical and non-statistical estimators with two different assessment budgets: 100 assessments per topic, and 400 assessments per topic. We also compared the effect of quadrupling the number of topics, as an alternative to quadrupling the per-topic assessment budget, given a larger assessment budget.
Ground truth was derived from the 86, 830 relevance assessments (qrels) from the TREC 8 Ad Hoc task. For each of 50 topics, documents were selected for assessment using depth-100 pooling for 71 of the 129 runs. For each topic, we defined ground truth rel(d) = 1 if d was assessed and judged relevant; otherwise rel(d) = 0. An unbiased estimator should be able to estimate measurements with regard to this ground truth. For reference, we compared RMSET achieved by the estimators to ground-truth RMSET calculated using the 86, 830 ground-truth assessments (1, 737, on average, per topic).
To compute dyn, stat, and trec_eval2 estimates, we used our own implementation, available for download as the dyn_eval toolkit,1 which is input/output compatible with trec_eval, and has
2See trec.nist.gov/trec_eval/.

946

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Estimator
xinf stat dyn xinf stat dyn depth-5 hedge
xinf stat dyn xinf stat dyn depth-20 hedge
TREC

Sample
Uniform Uniform Uniform
PPS PPS PPS Non-random Non-random
Uniform Uniform Uniform
PPS PPS PPS Non-random Non-random
Exhaustive

Systematic Bias RMS Summary over TREC Runs

b



b

err

b

100 assessments per topic

RMSerr

-0.0041 0.0004 0.0707 0.0443 0.0834

0.0019 0.0010 0.0078 0.1154 0.1156

-0.0006 0.0008 0.0061 0.0941 0.0943

0.0427 0.0001 0.0859 0.0103 0.0865

0.0008 0.0003 0.0029 0.0311 0.0312

0.0002 0.0002 0.0031 0.0282 0.0284

-0.0258 0.0000 0.0349 0.0000 0.0349

-0.0296 0.0000 0.0330 0.0000 0.0330

400 assessments per topic

0.0017 0.0003 0.0113 0.0365 0.0382

0.0002 0.0005 0.0032 0.0514 0.0515

-0.0002 0.0003 0.0027 0.0383 0.0384

0.0277 0.0001 0.0517 0.0065 0.0521

0.0002 0.0001 0.0011 0.0107 0.0107

0.0000 0.0001 0.0007 0.0082 0.0082

-0.0011 0.0000 0.0034 0.0000 -0.0014 0.0000 0.0029 0.0000
1737 assessments per topic 0.0000 0.0000 0.0000 0.0000

0.0034 0.0029
0.0000

RMSE |T |= 50 |T |= 200

0.0928 0.1226 0.1027 0.0955 0.0513 0.0496 0.0535 0.0523

0.0768 0.0616 0.0515 0.0883 0.0256 0.0248 0.0403 0.0386

0.0558 0.0656 0.0559 0.0661 0.0420 0.0415 0.0408 0.0407

0.0294 0.0328 0.0280 0.0556 0.0209 0.0206 0.0205 0.0204

0.0402 0.0202

Table 1: TREC Runs

Estimator
xinf stat dyn xinf stat dyn depth-5 hedge
xinf stat dyn xinf stat dyn depth-20 hedge
TREC

Sample
Uniform Uniform Uniform
PPS PPS PPS Non-random Non-random
Uniform Uniform Uniform
PPS PPS PPS Non-random Non-random
Exhaustive

Systematic Bias RMS Summary over Dual Runs

b


b

b

err

100 assessments per topic

RMSerr

-0.0043 0.0004 0.0709 0.0454 0.0842

-0.0001 0.0010 0.0112 0.1163 0.1168

-0.0018 0.0009 0.0093 0.0997 0.1001

-0.0653 0.0001 0.1311 0.0116 0.1316

0.0018 0.0006 0.0071 0.0734 0.0008 0.0004 0.0045 0.0465

0.0737 0.0468

-0.2202 0.0000 0.2381 0.0000 0.2381

-0.1277 0.0000 0.1375 0.0000 0.1375

400 assessments per topic

0.0006 0.0003 0.0114 0.0356 0.0374

-0.0020 0.0004 0.0049 0.0510 0.0512

-0.0012 0.0004 0.0036 0.0405 0.0407

-0.0261 0.0001 0.0698 0.0092 0.0704

-0.0006 0.0002 0.0027 0.0264 0.0266

-0.0003 0.0001 0.0016 0.0162 0.0163

-0.1088 0.0000 0.1196 0.0000 0.1196

-0.0526 0.0000 0.0574 0.0000 0.0574

1737 assessments per topic 0.0000 0.0000 0.0000 0.0000

0.0000

RMSE |T |= 50 |T |= 200

0.0935 0.1237 0.1081 0.1377 0.0842 0.0619 0.2415 0.1434

0.0772 0.0626 0.0546 0.1327 0.0425 0.0311 0.2389 0.1390

0.0552 0.0654 0.0575 0.0813 0.0485 0.0438 0.1263 0.0703

0.0292 0.0329 0.0288 0.0728 0.0243 0.0218 0.1213 0.0609

0.0402 0.0202

Table 2: Dual Runs

been verified to produce identical results. To render xinf estimates, we used the reference implementation from TREC.3
3See trec.nist.gov/data/clinical/sample_eval.pl.

For statistical estimation, we considered two sampling strategies: equal-probability sampling, and probability proportional-to-size sampling (PPS) with 20 strata, as described in Section 4. For nonstatistical estimation, we considered two strategies: depth-k pooling

947

Short Research Papers 1B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Measure
MRBP ( = 0.9) MAP
NDCG

b 0.0000 0.0015 0.0032

dyn 
b
0.0001 0.0000 0.0001

RMSE 0.0360 0.0283 0.0323

TREC RMSE 0.0351 0.0280 0.0312

Table 3: dyn MRBP, MAP, and NDCG using PSS and 400 assessments per topic, compared to TREC ground truth.

and hedge, a best-of-breed dynamic pooling method. For depth-k pooling, we were not able to enforce a strict budget of 100 or 400 assessments per topic; as proxies we used k = 5 and k = 20, with 110 and 385 assessments per topic, on average.
For each of the 129 TREC runs and each of 129 dual runs, we measured MP@10 100 times using each statistical estimation strategy, calculating bias, variance, and error as described in Section 5 above. For the non-statistical strategies, we measured MP@10 only once, since er r = 0.
Results over sets of runs are summarized by:
· b: The mean bias over all runs, an indicator of systematic bias;
·  : the (im)precision of the systematic bias estimate;
b
· RMS b: run-specific bias, over and above systematic bias; · RMS err : the (im)precision of the P@10(T ) estimate; · RMS err: the (in)accuracy of the P@10(T ) estimate; · RMSEest: the (in) accuracy of the P@10(T ) estimate.
Tables 1 and 2 shows these summary statistics for the TREC and dual runs. The results before the top break in each table use a budget of 100 assessments per topic; the results after the break use a budget of 400 assessments per topic, and the TREC gold standard uses 1, 737 assessments per topic.
We see that, as predicted, the stat and dyn estimates are unbiased, while xinf shows small but significant bias, and the non-statistical methods show substantial bias in favour of the TREC runs. PPS improves the dyn and stat estimates, but harms xinf. For a total assessment budget of 20, 000 documents, which is at the low end of typical TREC efforts, a budget of 100 assessments per topic for 200 topics allows dyn to surpass the accuracy of exhaustive assessment for 50 topics, for less than a quarter of the assessment effort. A budget of 400 assessments per topic for 50 topics, yields insubstantially different accuracy from exhaustive assessment. A budget of 400 assessments per topic for 200 topics yields insubstantially different accuracy from exhaustive assessment for 200 topics, with less than half the effort of exhaustive assessment for 50 topics.
7 RBP, MAP, AND NDCG
The dyn MRBP estimator is a straightforward modification to dyn MP@k. dyn MAP and dyn NDCG are somewhat biased because they divide by a normalization factor, which is also an estimate. But the normalization factor is invariant between runs, and therefore has minimal impact. Table 3 shows the results for these estimators compared to exhaustive assessment. As expected, the dyn MRBP shows no significant bias, while dyn MAP and dyn MNDCG show

significant but insubstantial bias, with the net effect that all estimates have comparable accuracy to exhaustive assessment.
8 DISCUSSION AND CONCLUSION
It has been argued that bias does not matter as long as runs are properly ranked. We combined the TREC and dual runs, and ranked them using dyn MP@10, depth-20 pooling, and hedge, achieving rank correlations  = 0.934,  = 0.715, and  = 0.826, respectively. In contrast, when only the TREC runs are considered, the correlations are  = 0.972,  = 0.996, and  = 0.995. These results call into question the viability of using rank correlation over known runs as a measure of test collection accuracy.  , like RMSE and other proposed measures, conflates bias and variance. We really have no idea whether differences in these measures reflect bias or variance.
An unbiased estimator like dyn MP@k, dyn MRBP, or dyn DCG imposes no limit on the number of topics that could be assessed, splitting the assessment budget among them. Practical considerations like the overhead of obtaining and vetting topics are likely to dominate. A minimally biased estimator like dyn MAP or dyn NDCG constrains the number of topics that may be used to a number such that its bias is insubstantial compared to variance. Even so, the optimal number of topics appears to be substantially higher than 50.
xinf, the most commonly used estimator, shows significant bias, occasioning substantial effort to discover amenable sampling strategies [7]. stat, on the other hand, shows substantial variance. Our design of dyn was directly inspired by these estimators, adopting the "default value" (albeit learned rather than constant) of xinf, and the Horvitz-Thompson estimator of stat. Our approach to stratification and our motivating application was Dynamic Sampling [2], from which dyn derives its name. Our results suggest that dyn should yield better results than stat for this purpose.
There is no limit on the number of documents or the number of relevant documents per topic to which dyn may be applied. To keep variance reasonable, it is necessary to identify a sample space that contains substantially all of the relevant documents. Twenty years ago, depth-100 pooling was found to be adequate--if imperfect--for a collection of one-half million documents and topics with 100 or fewer relevant documents. Since that time, the number of documents and the number of relevant documents have increased, while assessment budgets have decreased. Statistical sampling offers a solution.
REFERENCES
[1] Aslam, J. A., Pavlu, V., and Savell, R. A unified model for metasearch and the efficient evaluation of retrieval systems via the hedge algorithm. In SIGIR 2003.
[2] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [3] Horvitz, D. G., and Thompson, D. J. A generalization of sampling without
replacement from a finite universe. Journal of the American Statistical Association 47, 260 (1952), 663­685. [4] Pavlu, V., and Aslam, J. A practical sampling strategy for efficient retrieval evaluation. Northeastern University (2007). [5] Sanderson, M., et al. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247­375. [6] Voorhees, E., and Harman, D. Overview of the eighth text retrieval conference. In TREC 8 (1999). [7] Voorhees, E. M. The effect of sampling strategy on inferred measures. In SIGIR 2014. [8] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008.

948

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Quantifying Bias and Variance of System Rankings

Gordon V. Cormack
University of Waterloo gvcormac@uwaterloo.ca
ABSTRACT
When used to assess the accuracy of system rankings, Kendall's  and other rank correlation measures conflate bias and variance as sources of error. We derive from  a distance between rankings in Euclidean space, from which we can determine the magnitude of bias, variance, and error. Using bootstrap estimation, we show that shallow pooling has substantially higher bias and insubstantially lower variance than probability-proportional-to-size sampling, coupled with the recently released dynAP estimator.
ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2019. Quantifying Bias and Variance of System Rankings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331356
1 INTRODUCTION
Since large-scale test collections were first applied in the context of TREC [10], concern has been expressed regarding the extent to which they afford "fair" or "unbiased" rankings of informationretrieval (IR) systems, and the extent to which those rankings are "stable" or "reliable" [3, 14]. We frame the problem in terms of measurement accuracy [11], where bias b and standard deviation  are orthogonal, dimensionally meaningful meta-measurements of (lack of) fairness and (lack of) stability, and RMS error RMSE =
b2 +  2 quantifies overall (lack of) accuracy. This work derives an amenable definition of b and  from Kendall's  rank correlation (§2). The same derivation applies to any of the plethora of rank-similarity scores that have been been employed in ad-hoc strategies to evaluate the fairness and stability of test collections [4, 7, 12, 16, 17]. We show how to measure |b |,  , and RMSE using bootstrap re-sampling (§3). Using the TREC 8 Ad Hoc test collection [15] as a reference standard, we evaluate four techniques--two statistical and two non-statistical--for building test collections constrained by an assessment budget (§4). Our evaluation reveals substantial differences in bias that would be masked by RMSE or  alone. Orthogonal measurements of b and  allow us to predict the effect of different budget-allocation strategies, which are borne out by the bootstrap results (§4.1). Through the use of adversarial testing, we show that one method is substantially unbiased, even when ranking results that are dissimilar to the TREC submissions (§4.2).
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331356

Maura R. Grossman
University of Waterloo maura.grossman@uwaterloo.ca

We use bootstrap sampling to compare the official TREC 2018 Common Core test collection [1] to an alternate statistical collection created in advance by the University of Waterloo [9], along with its companion dynAP estimator1 [6]. We find that there are two sources of bias between the collections: (i) different relevance assessments for the same documents; and (ii) additional documents in the TREC collection selected using shallow sampling methods (§5).
This work contributes a new methodology for test-collection evaluation, and uses that methodology to show that shallow pooling introduces bias far beyond what is shown by rank-correlation measures over submitted TREC results. Of the statistical estimators, the older and more well established infAP [18] shows clear bias, while dynAP shows negligible bias. The results indicate that a statistical test collection can yield comparable accuracy over 50 topics, and considerably better accuracy over more, compared to exhaustive assessment.

2 BIAS AND VARIANCE FOR RANKING
In this paper, a "test result" [11] is a ranking of systems according to
their measured effectiveness. Closeness of agreement is quantified by a rank-similarity coefficient like Kendall's  , with the maximum value 1 denoting equality.
We interpret system rankings to be points in Euclidean space, where  (x, y) = 1 -  (x, y) is the the distance between x and y, whose location in space is unspecified. For test results X and Y , we define the expected squared distance  between them:

(X , Y ) = E  2(X , Y ) . The variance  2 of a given X is one-half the squared distance be-

tween itself and an independent and identically distributed test

result:

 2(X )

=

1

(X

,

X


)

(i .i .d .

X

,

X


)

.

2

When G is the gold-standard ranking, squared bias b2 is the amount

by which the squared distance between X and G exceeds that which

is attributable to variance:

b2(X ) = (X , G) -  2(X ) -  2(G) .

It follows that mean-squared error

MSE(X ) = b2(X ) +  2(X ) . Bias b(X ) is a vector whose direction is unspecified; however, its magnitude |b |(X ) is sufficient for our purpose.
It is worth noting that the ground truth ranking is a random variable G, rather than the particular outcome G =  derived for the particular topics and assessments of the reference test collection from which G is derived. Under the assumption underlying virtually all reported statistical tests--that the set of topics in a collection
is a random sample of a population of topics--the distributions of
1See cormack.uwaterloo.ca/sample/.

1089

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

G and X may estimated by bootstrap re-sampling (see [8]). Other random factors, notably the choice of documents for assessment and the assessor's relevance determinations, may be simulated in conjunction with the bootstrap.
When presenting empirical results, we report magnitude of bias |b|, standard deviation  , and root-mean-squared error RMSE.
3 THE BOOTSTRAP
To estimate bias and variance, we conduct repeated tests in which random factors associated with the selection of topics, and the selection of documents to assess, are simulated. For statistical methods like infAP and dynAP, we first repeat the sampling method 100 times for each of nt topics in a reference collection. For each sample, we measure AP (not MAP, at this point) for each of ns available system results, saving the results in an nt × 100 × ns table. For nonstatistical methods, like pooling and hedge [2], repeated per-topic measurements are unnecessary, so the table contains nt × 1 × ns measurements.
We then draw 1, 000 samples of nt topics, with replacement, from the nt topics of the reference collection. Typically, but not necessarily, nt = nt , thus preserving the variance of the mean. For each topic t within a sample, and for each system s, an AP measurement is drawn from the table at position (t, r , s), where r is chosen at random. For each system, the nt measurements are averaged to yield a MAP score. The resulting MAP scores are used to rank the ns systems.
 is estimated as the empirical distribution of  (A, B), where A is the set of 1, 000 bootstrap rankings for one collection, and B is the set of 1, 000 rankings for another. Estimates of  and |b| follow.
4 EXPERIMENT 1: ASSESSMENT BUDGETS
For our first experiment, we assume the TREC 8 Ad Hoc test collection to yield ground truth. We are concerned with ranking not only the 129 systems whose results were submitted to TREC 8 for evaluation, but also dissimilar systems. To simulate the results of dissimilar systems, we engineered an additional 129 results, each derived from a distinct TREC result by randomly permuting the order of the relevant documents. As a consequence, corresponding submitted and engineered results have identical AP and MAP scores, and identical ground-truth ranks.
Using the ground-truth ranking for the submitted results, and additionally, the ground-truth ranking for the submitted and engineered results combined, we evaluate the bias, variance, and MSE of rankings derived from fewer assessments of the TREC collection.
The TREC collection contains 50 topics and 86,830 assessments, an average of 1,737 assessments per topic. Initially, we evaluated four methods of reducing assessment cost seventeen-fold to about 5,000 assessments:
· The well-known infAP estimator, applied to a probabilityproportional-to-size (PPS) sample for each topic consisting of five documents drawn from each of 20 strata, for a total of 5,000 assessments;
· The recently released dynAP estimator, applied to the same PPS sample, for a total of 5,000 assessments;

· The trec_eval evaluator, applied to a variable number of documents per topic selected by depth-5 pooling, for a total of 5,542 assessments (avg. 111 per topic);
· trec_eval, applied to 100 documents per topic, selected by hedge, for a total of 5,000 assessments.
We then evaluated the effect of quadrupling the assessment budget in two ways: (i) by increasing the number of topics from 50 to 200; and (ii) by increasing the number of assessed documents per topic from 100 to 400. We further evaluated a budget of 80,000--about the same as at TREC--by quadrupling both the number of topics and the number of judgments per topic.
Table 1 shows ranking accuracy with respect to the submitted TREC results; Table 2 shows ranking accuracy with respect to the submitted results augmented by the engineered results. The topleft panel of each table shows accuracy for 5, 000 assessments. The result of quadrupling the number of topics is shown to the right, while the results of quadrupling the number of assessments per topic is shown below. The bottom row shows the accuracy of the reference TREC collection, under the assumption that it is unbiased.
For 5, 000 assessments and the TREC results, the RMSE results show little to choose between dynAP and hedge, with between 15% and 20% higher error than the reference collection. The |b | and  results show that dynAP has less than one-quarter the bias, but hedge has lower variance, with the net effect that their overall error is similar.
4.1 Effect of More Topics
From these results we can predict predict the effect of quadrupling the number of topics, which is borne out by the top-right panel: |b | is essentially unchanged, while  is approximately halved. The net effect is that RMSE for dynAP is approximately halved, about 17% higher than for the reference collection.  for hedge is similarly halved but |b | is unchanged, so RMSE is reduced by only one-third, about 55% higher than for the reference collection.
From the initial results we can only bound the effect of quadrupling the number of assessments per topic: |b| and  will both generally be reduced, but b2 +  2 cannot fall below  for the reference collection. The bottom-left panel confirms this prediction, except for the fact that the bootstrap estimate of RMSE for hedge is slightly smaller than for the reference collection. This apparently preposterous result result may be explained by random error in the bootstrap estimate. Overall, this panel suggests that, with a budget of 400 assessments per topic, hedge has slightly lower overall error, but still four-times higher bias, than dynAP. Nevertheless, the results for hedge--and all other methods--are far superior when the same overall budget of 20, 000 is apportioned over 200 topics with 100 assessments per topic, instead of 50 topics with 400 assessments per topic.
The effect of quadrupling the number of topics, with a budget of 400 assessments per topic, is the same as with a budget of 100 assessments per topic: |b | is invariant, while  is halved. Overall, for a budget of 80, 000 assessments, dynAP achieves an RMSE score that is insubstantially different from that achieved by a reference collection with 331,320 assessments.

1090

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

infAP dynAP depth-5 hedge
infAP dynAP depth-20 hedge
Reference

|b |



RMSE

5,000 assessments, 50 topics

0.0309 0.0952 0.1001 0.0117 0.0907 0.0914

0.0800 0.0839 0.1160

0.0500 0.0806 0.0948 20,000 assessments, 50 topics

0.0113 0.0826 0.0833

0.0055 0.0806 0.0808

0.0381 0.0783 0.0871
0.0159 0.0777 0.0794 86,830 assessments, 50 topics
0 0.0797 0.0797

|b |



RMSE

20,000 assessments, 200 topics

0.0293 0.0488 0.0124 0.0458

0.0569 0.0475

0.0815 0.0419 0.0916

0.0498 0.0398 0.0638 80,000 assessments, 200 topics

0.0110 0.0419 0.0033 0.0408

0.0433 0.0409

0.0387 0.0394 0.0552

0.0154 0.0394 0.0423 331,320 assessments, 200 topics
0 0.0405 0.0405

Table 1: Accuracy of ranking TREC system results. () RMSE values less than Reference are explained by chance error in the bootstrap estimate.

infAP dynAP depth-5

|b |

 RMSE

5,000 assessments, 50 topics

0.1510 0.0939 0.1778

0.0319 0.1078 0.1125

0.3071 0.0695 0.3149

|b |



RMSE

20,000 assessments, 200 topics

0.1517 0.0480 0.1591

0.0347 0.0560 0.0658

0.3007 0.0360 0.3029

hedge
infAP dynAP depth-20

0.1263 0.0773 0.1481 20,000 assessments, 50 topics 0.0495 0.0847 0.0981 -0.0145 0.0856 0.0843 0.2013 0.0688 0.2127

0.1267 0.0389 0.1325 80,000 assessments, 200 topics 0.0503 0.0430 0.0662 -0.0093 0.0442 0.0432 0.1961 0.0338 0.1990

hedge 0.0866 0.0780 0.1166 86,830 assessments, 50 topics
Reference 0 0.0797 0.0797

0.0869 0.0398 0.0956 331,320 assessments, 200 topics
0 0.0405 0.0405

Table 2: Accuracy of ranking TREC and dissimilar system results. () Cases where the b2(X ) < 0 are explained by chance error in the bootstrap estimate, and reported as - -b2(X ).

4.2 Adversarial Testing
Table 2 affirms the same predictions regarding assessment-budget
allocation, but tells a different story regarding the accuracy of the
test collections. When the engineered results are ranked with the
TREC results, bias is roughly tripled for all methods subject to a 5, 000-document assessment budget, while variance is increased moderately. For infAP, depth-5, and hedge, bias dominates variance,
calling into question whether these methods provide reasonable
accuracy, regardless of their RMSE. The dynAP results represent a closer call. |b | is one-third of  , rendering it a small but noticeable component of RMSE. Quadrupling the number of topics exacerbates the influence of |b|, but still yields RMSE scores better than the reference collection for 50 topics.
Quadrupling the per-topic assessment budget dramatically reduces |b | for dynAP, to the point that the bootstrap estimate of b2 is negative (shown as |b |= - -b2), indicating that we are unable to distinguish |b | from zero. When the number of topics is also quadrupled, we are still unable to distinguish |b| from zero, while  and RMSE are about 10% greater than RMSE for the reference collection with 200 topics; and half the RMSE for the reference
collection with 50 topics.

5 EXPERIMENT 2: ALTERNATE ASSESSMENTS
Before TREC 2018, the University of Waterloo used Dynamic Sampling [5], and 19, 161 of their own assessments to create relevance assessments (irels) as input the dyn estimator, thereby forming a test collection for the 2018 Common Core Track, which was released, along with the dyn estimator, as UWcore181 [9]. To form the official TREC test collection, NIST (re)assessed the 19, 161 documents, and 5, 767 additional documents selected by a combination of depth-10 and move-to-front pooling, to create the official TREC relevance assessments (qrels) as input to trec_eval. To examine the impact on system ranking of using Waterloo versus TREC assessments, we compare UWcore18 to UWcore18N, in which the Waterloo assessments are replaced by NIST assessments. To examine the impact of using an additional 5, 767 TREC assessments, while eschewing dyn in favour of trec_eval, we compare UWcore18N to the NIST test collection.
Table 3 shows the accuracy with which the three test collections rank 69 of the 73 system results submitted to TREC, excluding the four submitted by Waterloo. The top panel shows inter-collection

1091

Short Research Papers 2B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

G
UWcore18 UWcore18N
NIST

UWcore18
0.0067 0.0364 0.0710

UWcore18N --|b |-- 0.0887 0.0064 0.0369

NIST
0.0752 0.0364 0.0084

UWcore18 UWcore18N
NIST

0.0828 0.1186 0.1089

--RMSE-- 0.1210 0.0826 0.0903

0.1090 0.0869 0.0794

Table 3: Accuracy results for alternative TREC 2018 Common Core Collections. The top panel shows pairwise bias; the bottom panel shows RMSE. () In the top panel, asymmetry and differences from 0 on the diagonal are explained by chance error in the bootstrap estimate.

bias |b| using each collection as ground truth to measure the bias of each other collection, including itself. The top panel should be symmetric, and its diagonal should be 0. Deviations from this prediction may be attributed to random error in the bootstrap estimates. The diagonal of the bottom panel shows  for each collection, while the non-diagonal elements show inter-system RMSE. Our theory predicts that the bottom panel will be symmetric only if the  values are equal, which they nearly are.
There is substantial bias--about equal to  --between UWcore18 and UWcore18N, indicating that some systems score better according to the Waterloo assessments, while others score better according to NIST's. We are not in a position to render an opinion on which set of assessments is better and suggest that evaluation using the two sets of assessments should be considered to be separate experiments, and both results considered when comparing system results.
Bias between UWcore18N and NIST, while smaller in magnitude, may be more of a concern, because it reflects different measurements of ostensibly the same value. The results from our first experiment show that shallow pooling methods exhibit strong bias, while dynAP does not. Together, these results suggest that the intercollection bias |b |= 0.0364 may be attributed in large part to the NIST collection. If this were the case, it would offset an apparent advantage in  for NIST.
We posit that better results would have been achieved, had the budget of 5, 767 NIST assessments allocated to shallow pooing been used to assess an additional 15 topics using Dynamic Sampling. Using our bootstrap simulation, we project that UWcore18N, if extended to 65 topics, would achieve  = 0.0724, lower than the official 50-topic collection.
The evidence suggests the current UWcore18N test collection, notwithstanding its slightly higher variance, is preferable to the official TREC test collection, because it is less likely to be biased, particularly with respect to novel IR methods.
6 DISCUSSION AND LIMITATIONS
Kutlu et al. [12] provide an excellent survey of rank-similarity measures and their shortcomings, in support of a "significance

aware" approach that takes into account the results of pairwise significance tests. Our approach exposes the variances of G and X directly--separate from bias--rather than obliquely through an amalgam of test results, distilled into a dimensionless overall score.
Previous work has evaluated fairness and stability separately, by
perturbing the topics or the results to be ranked so as to calculate a
separate summary measure for each [3, 14]. Our bootstrap sample was inadequate to quantify b when  
|b | 0. Further analytic and empirical work is needed to compute confidence intervals for the bootstrap estimates. In theory, the
variance of the variance estimates can be determined from the distributions of G and X , but those estimates should be verified by meta-experiments.
Our experiments rely on on an assumption--contradicted by the
evidence [13]--that an assessor will give the same relevance deter-
mination for a given document, regardless of the sampling strategy.
Whether real assessor behaviour would have any substantive posi-
tive or negative effect on rankings remains to be determined.
Orthogonal estimates of bias and variance have predictive ability
lacking in a single score conflating the two, or separate uncalibrated
scores. Harnessing this predictive ability, we offer evidence that
shallow pooling methods introduce unreasonable amounts of bias,
while offering hardly lower variance than dynAP, which represents
a substantial improvement over infAP. Based on this evidence, we
have reason to suggest that the UWcore18 or UWcore18N statisti-
cal test collections are more accurate than the official TREC 2018
Common Core test collection.
REFERENCES
[1] Allan, J., Harman, D., Voorhees, E., and Kanoulas, E. TREC 2018 Common Core Track overview. In TREC 2018.
[2] Aslam, J. A., Pavlu, V., and Savell, R. A unified model for metasearch, pooling, and system evaluation. In CIKM 2003.
[3] Buckley, C., and Voorhees, E. M. Evaluating evaluation measure stability. In SIGIR 2000.
[4] Carterette, B. On rank correlation and the distance between rankings. In SIGIR 2009.
[5] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [6] Cormack, G. V., and Grossman, M. R. Unbiased low-variance estimators for
precision and related information retrieval effectiveness measures. In SIGIR 2019. [7] Cormack, G. V., and Lynam, T. R. Power and bias of subset pooling strategies.
In SIGIR 2007. [8] Cormack, G. V., and Lynam, T. R. Statistical precision of information retrieval
evaluation. In SIGIR 2006. [9] Cormack, G. V., Zhang, H., Ghelani, N., Abualsaud, M., Smucker, M. D.,
Grossman, M. R., Rahbariasl, S., and Grossman, M. R. Dynamic sampling meets pooling. In SIGIR 2019. [10] Harman, D. K. The TREC Test Collections. In TREC: Experiment and Evaluation in Information Retrieval (2005), E. M. Voorhees and D. K. Harman, Eds., pp. 21­52. [11] International Organization for Standardization. ISO 5725-1: 1994: Accuracy (Trueness and Precision) of Measurement Methods and Results-Part 1: General Principles and Definitions. International Organization for Standardization Geneva, Switzerland, 1994. [12] Kutlu, M., Elsayed, T., Hasanain, M., and Lease, M. When rank order isn't enough: New statistical-significance-aware correlation measures. In CIKM 2018. [13] Roegiest, A., and Cormack, G. V. Impact of review-set selection on human assessment for text classification. In SIGIR 2016. [14] Sakai, T. On the reliability of information retrieval metrics based on graded relevance. Inf. Process. Manag. 43, 2 (2007), 531­548. [15] Voorhees, E., and Harman, D. Overview of the Eighth Text REtrieval Conference. In TREC 8 (1999). [16] Voorhees, E. M. Variations in relevance judgments and the measurement of retrieval effectiveness. Inf. Process. Manag. 36, 5 (2000). [17] Yilmaz, E., Aslam, J. A., and Robertson, S. A new rank correlation coefficient for information retrieval. In SIGIR 2008. [18] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008.

1092

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Selecting Discriminative Terms for Relevance Model

Dwaipayan Roy
Indian Statistical Institute, Kolkata dwaipayan_r@isical.ac.in

Sumit Bhatia
IBM Research, Delhi, India sumitbhatia@in.ibm.com

Mandar Mitra
Indian Statistical Institute, Kolkata mandar@isical.ac.in

ABSTRACT
Pseudo-relevance feedback based on the relevance model does not take into account the inverse document frequency of candidate terms when selecting expansion terms. As a result, common terms are often included in the expanded query constructed by this model. We propose three possible extensions of the relevance model that address this drawback. Our proposed extensions are simple to compute and are independent of the base retrieval model. Experiments on several TREC news and web collections show that the proposed modifications yield significantly better MAP, precision, NDCG, and recall values than the original relevance model as well as its two recently proposed state-of-the-art variants.
CCS CONCEPTS
· Information systems  Query representation; Query reformulation.
ACM Reference Format: Dwaipayan Roy, Sumit Bhatia, and Mandar Mitra. 2019. Selecting Discriminative Terms for Relevance Model. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331357
1 INTRODUCTION
Query expansion (QE) is a popular technique used for handling the vocabulary mismatch problem in information retrieval [6]. In pseudo-relevance feedback (PRF) based query expansion methods, the top-ranked documents retrieved by using the original query are used to select terms for query expansion. As a result, they alleviate the need to use external sources of information for query expansion making PRF based retrieval models [1, 10, 12] as amongst the most widely used query expansion methods in practice.
RM3 [8] is one of the most commonly used method for PRF and experiments reported by Lv and Zhai [11] demonstrated its robustness on different collections when compared with other feedback methods. They also found that RM3 term weighing is prone to select generic words as expansion terms that cannot help identify relevant documents. This occurrence of noise is a well-known shortcoming of PRF methods in general due to presence of non-relevant documents in the pseudo-relevant set (as precision is often less than one) [10]. The pseudo-relevant document set contains considerable
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331357

noise that can lead to the expanded query drifting away from the original query [10]. Cao et al. [2] studied the utility of terms selected for expansion and found that only about a fifth of the expansion terms identified for query expansion contributed positively to an improvement in retrieval performance. Rest of the terms either had no impact on retrieval performance or led to the reduction in final retrieval performance. Background Work: In order to prevent the inclusion of common terms in expanded queries, different methods have been proposed to compute better term weights for PRF models such as incorporating proximity of expansion terms to query terms [12], classifying expansion terms as good and bad by using features derived from term distributions, co-occurrences, proximity, etc. [2]. Parapar and Barreiro [15] proposed RM3-DT, an alternative estimation of term weighting in RM by subtracting the collection probability of the term from its document probability, thus giving a high weight to terms that have a higher probability of being present in the pseudorelevant document set rather than the whole collection.
Clinchant and Gaussier [4] described an axiomatic framework and discussed five axioms for characterizing different PRF models. Building upon this framework, various improvements and modifications of the original relevance model have been proposed [1, 13, 14] to select better terms for query expansion and compute better weights by incorporating new constraints and assumptions in the computation of scores of terms present in the set of feedback documents. Recently, Montazeralghaem et al. [14] proposed two additional constraints to consider interdependence among previously established characteristics of pseudo-relevant feedback (PRF) models. The first constraint (TF-IDF constraint) considers the interrelationship of TF and IDF on PRF models and the second constraint (relevance score constraint) focuses on the interdependence of the feedback weight of selected terms and the relevance scores of documents containing the term. They proposed RM3+All, an extension of the RM3 model that accounts for these constraints. Our Contributions: We propose three alternate heuristics for selecting the terms for query expansion that involve minimal computations on top of relevance model. Our selection process is simple and re-ranks the terms present in feedback documents based on the ratio of likelihoods of these terms being generated by the relevant and non-relevant document sets. We perform a thorough empirical evaluation of the proposed methods on standard TREC collections, ranging from TREC ad hoc collections to ClueWeb09 and compare the performance two state-of-the-art baselines (RM3DT [15] and RM3+All [14]). Despite being simple, the proposed methods significantly outperform RM3, RM3-DT, and RM3+ALL in terms of retrieval performance, are more robust and are computationally more efficient. We make our implementation available as a Lucene plugin1 to enable further reuse and replication of our experiments.
1 https://github.com/dwaipayanroy/rm3idf

1253

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

2 PROPOSED METHOD
2.1 Intuition Behind Proposed Approaches
According to the relevance model (RM) [9], for a given query Q = {q1, . . . , qk }, the feedback weight of an expansion term w is computed as follows.

FWrm (w ) = P (w | R) = P (w |d ) P (q |d )

(1)

d D

q Q

Here, D is the pseudo-relevant document set.
In RM3 [8], the feedback weight is estimated by linearly interpolating the query likelihood model of w (P(w |Q)) with the original relevance model R to obtain R as follows.

FWrm3(w ) = P (w | R) =  P (w |Q ) + (1 -  )P (w | R)

(2)

Here,   [0, 1] is the interpolation parameter, and P(w |Q) is estimated using maximum likelihood estimation.
Let us consider a term t that is present in a large number of documents in the collection. If t is present in the pseudo-relevant document set, the value of P(t |R) (according to Equation 1) is likely to be quite high, thereby increasing the possibility of selecting t as an expansion term [4, 5, 7]. However, since t is also present in a large number of documents outside the pseudo-relevant document set, this term lacks the discriminating power to distinguish between relevant and non-relevant documents. Furthermore, during retrieval with the expanded query, such terms may adversely affect the computation time by unnecessarily increasing the query length. Additionally, if the term weights in the expanded query are sum-normalized (as done by Lavernko and Croft [9], Jaleel et al. [8]), these common terms may together reduce the weights of important terms in the expanded query. In sum, the inability of the RM3 model to filter out such terms makes it vulnerable to a potential degradation in retrieval performance. Intuitively, terms with a high discriminative power are expected to have different distributions over the set of relevant and non-relevant documents. Mathematically, this difference can be captured by the ratio of the occurrence probabilities of the term t being selected from the relevance (R) and non-relevance (N ) classes, similar to the ideas by Robertson and Zaragoza [16]. Formally, for a given query Q, we consider two separate language models, associated respectively with the relevance class (R, estimated following RM [9]) and non-relevance class (N ) of the query. To estimate N , we choose the commonly-adopted alternative of using the whole document collection C since, for a typical query Q, most of the documents in the collection are used to be non-relevant. Based on this intuition, we now describe three approaches to compute the feedback term weights to select and weigh terms for query expansion.

2.2 RM3+1
In our first proposed approach, we compute the weights of candidate expansion terms by using P(w |R)/P(w |N ). The probability of selection of a term from the relevance class (P(w |R)) can be computed using Equation (1). We adopt the common practice of approximating the non-relevance class is by using the collection model C. Note that the basic intuition behind incorporating the non-relevance model as a factoring component is to identify terms with discriminating features that can also be considered as an estimation of rareness. Formally, the feedback weight of term w in the

Document Collection #Docs Collection Type

Query Query Set Fields

Query Dev Test Ids Set Set

TREC Disk 1, 2

News

741,856

Title

TREC 1 ad-hoc TREC 2, 3 ad-hoc

51-100  101-200



TREC Disks 4, 5 News exclude CR

TREC 6 ad-hoc 301-350 

528,155 Title TREC 7, 8 ad-hoc 351-450

TREC Robust 601-700



GOV2 Web

25,205,179 Title

TREC Terabyte 1 701-750  TREC Terabyte 2,3 751-850



WT10G CW09B-S70

Web

1,692,096 50,220,423

Title

WT10G ClueWeb09

451-550 

1-200



Table 1: Dataset Overview

expanded query is computed as follows:

FW (w )

=

P (w | R) P (w | N)



P (w | R) P (w | C)



P (w | R)  r (w )

(3)

where r (w)  1/P(w |C) can be interpreted as a measure of rareness of w. We may replace r (w) by any other measure of a term's rareness.
For our experiments, we use the traditional inverse document frequency factor in place of r (w) (since this yields better retrieval
effectiveness). Next, the weights are sum-normalized and the top N terms with the highest FW (w) values are selected as expansion
terms. The normalized feedback weights (denoted NFW ) are then combined with the query likelihood model Q. Mathematically, the final weight of a term w in the expanded query is given by:

FWrm3+1 (w ) =  P (w |Q ) + (1 -  )  NFW (w )

(4)

If Dirichlet smoothing is used (as recommended in [18]), then, dur-
ing retrieval using the expanded query Q, the score of a document d is computed in practice as follows:

Score(d,

Q)

=

NFW rm31+ (q)
q Q

×

log

µ .P (q | C) + |d |.P (q |d ) P (q | C) µ + |d |

(5)

where NFW (q) represents the normalized weight of q in the expanded query, µ is the smoothing parameter, and |d | represents the number of indexed words in d.

2.3 RM3+2
In RM3+1 , a term w is weighted on the basis of a linear combination between the query language model (P(w |Q)) and the ratio between
term selection probability from RM (Equation (1)) and from the

non-relevance model. As an alternative approach, we consider the RM3 model (instead of RM in RM3+1 ) to approximate the relevance class, i.e, a term w is weighted by the probability ratio of how

likely w is from the query-relevance model (RM3) and from the

non-relevance model. Formally, candidate terms are weighted using

a P

ratio similar to (w |R)/P(w |N )

the one used in RM3+1 , but unlike to formulate the expansion term

RM3+1 , we use weight. Here,

P(w |R) is approximated by Equation 2, and P(w |N ) is estimated

as in RM3+1 . Thus, the weight of term w in the expanded query is

computed as follows.

P (w | R) P (w | R)

FW rm32+ (w ) = P (w | N) = P (w | C)

(6)

  P (w |Q )  r(w ) + (1 -  )P (w | R)  r(w )

Note that in this approach, the final score for a document incorporates the rareness information "twice": first, as a part of the computation of FW (q) (Equation (6)), and again during retrieval because of the factor of P(q|C) in the denominator in Equation (5).

1254

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

This overemphasis on the rareness of query terms may promote highly rare (but noisy) terms at the cost of useful, but more frequent query terms leading to a possible drop in retrieval effectiveness.
2.4 RM3+3
To address the potential issues due to double application of rareness information we adopt an approach similar to Carpineto et al. [3] where candidate expansion terms are ranked using one weighing function, and a different function is used to determine the final weight of the selected expansion terms. Specifically, we first rank the candidate expansion terms using Equation (6), and select the top N terms. Since rareness information is used in Equation (6), we hope that this will avoid the problem of selecting low-IDF words as expansion terms. Next, the final feedback weight for the selected terms is computed using Equation (2) instead of Equation (6). This avoids the use of a "double" IDF factor during final retrieval. Formally, the feedback term weight is computed as follows.
FW rm33+ (w) = P(w |R) = P(w |Q) + (1 -  )P(w |R) (7)
3 EXPERIMENTS
3.1 Datasets and Experimental Settings
We evaluate the proposed modifications to RM3 using standard TREC news and web collections. For parameter tuning, we split the topic sets into development and testing sets as summarized in Table 1.All the collections are stemmed using Porter's stemmer and stopwords are removed prior to indexing using Apache Lucene. For the initial retrieval using the original, unexpanded queries, we used the Dirichlet smoothed language model.All the baselines and the proposed methods were implemented using Lucene.
Baselines: For comparing with expansion based methods, we choose three baselines ­ (i) RM3 [8], (ii) RM3DT [15], a variant of RM3 that promotes divergent terms, and (iii) RM3+All [14], a recently proposed state-of-the-art modification of RM3.
Parameter Tuning: There are three parameters associated with the RM3 based QE method: number of documents (M), number of terms (N ), and smoothing parameter () that adjusts the importance of query terms. The parameters M and N were varied in the range {10, 15, 20} and {30, 40, 50, 60, 70} respectively and  was varied in the range {0.1, . . . , 0.9} in steps of 0.1. The parameters were tuned individually for each of the methods on the development topics, and applied on the corresponding test topics (see Table 1). There are no additional parameters associated with RM3DT, RM3+All, and the proposed methods. The smoothing parameter µ is varied in the range {100, 200, 500, 1000, 2000, 5000} and set to 1000 at which the optimal performance is observed on development topic sets. All our experiments were performed on a VM with Intel Xeon 2.80GHz CPU containing 80 cores, and 100GB of RAM.
3.2 Results and discussion
Retrieval Performance: Table 2 presents the results of retrieval experiments with all the baselines over all the test collections. We observe that for all the test collections, all of the three proposed modifications outperform the traditional RM3 [8], as well as the

two baselines, for most of the evaluation measures. We also note

that the second proposed compared to RM3+1 and

RmMet3h+3o.dF(oRrMT3R+2E)Cis

seen to be less effective news topics, the mean

average precision (MAP), recall and NDCG at rank 10 are almost

always seen to be significantly better for both RM3+1 and RM3+3 than

the baselines. Improvement is also observed for P@10, however

they are not significant for most of the topic sets. Results for TREC

Terabyte track 2 and 3 topics exhibit similar improvements over the baselines. For ClueWeb09 topics, RM3+3 achieves significant improvements in MAP, recall, and P@10 values over RM3 and RM3DT. Compared to RM3+All, RM3+3 achieves significantly better MAP and recall values. We also report robustness index (RI) [17]

to compare the robustness of methods when compared with the

original RM3 model. We observe that the performance of both RM3+1 and RM3+3 is consistently more robust than the baselines. RM3DT achieves negative RI values for Terabyte 2,3 and ClueWeb

collections indicating that more queries suffered in terms of retrieval

performance than the queries that gained in retrieval performance. On the other hand, RM3+3 consistently achieved best RI values across all collections, except for the ClueWeb collection where it is a very close second. Thus, in terms of retrieval performance, RM3+3 achieves overall best performance among the baselines and the three proposed heuristics, followed by RM3+1 .

Computational Latency: For each test collection, Table 4 reports

the total time taken by different methods to execute all the queries

(averaged over ten runs). Note that, we only compare the elapsed

time for expansion term selection as the actual retrieval times

(traversing inverted lists) would be common for all the methods.

We observe that the execution time for the three proposed methods are consistently less than RM3+All. Further, RM3+3 takes the least amount of additional time over RM3 for finding expansion terms for

Terabyte 2,3 and ClueWeb collections. Reading results in Tables 4 and 2 together, we conclude that the RM3+3 method, for most cases, achieves best retrieval performance, is robust, and computes the

expansion terms faster than other methods studied.

Qualitative Analysis: We compare expansion terms selected by RM3, RM3DT, RM3+All and RM3+3 (best among the proposed modifications). Table 3 presents the top 15 expansion terms for topic
nuclear proliferation (from TREC123 collection) for those
methods. Observe that as discussed before, RM3 is prone to se-
lecting common terms (low IDF) as exemplified by year in the
list.On the other hand, RM3+All selects terms with significantly
high IDF values. For example, expansion terms kalayeh, qazvin
and yongbyon have collection frequency 1, 4 and 8 respectively
in TREC 123 collection. However, such extremely rare terms may
often be due to noise rather than topical relevance to the original query. In contrast, RM3+3 strikes a balance by preventing frequent terms from occupying top weights as well as not prioritizing very
rare terms.

4 CONCLUSION
We proposed three modifications to RM3 model to promote selection of discriminative terms for query expansion. A thorough empirical evaluation was performed using TREC news and Web collections and results compared with two state-of-the-art RM3 variants for promoting high IDF terms for query expansion. The

1255

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Metrics

TREC23

MAP P@10 Recall NDCG@10
RI

TREC78Rb

MAP P@10 Recall NDCG@10
RI

MAP P@10 Terabyte 2,3 Recall NDCG@10
RI

CW09B

MAP P@10 Recall NDCG@10
RI

LM
0.2325 0.4990 0.6142 0.5065
-
0.2550 0.4372 0.7172 0.4406
-
0.2918 0.5680 0.7076 0.4943
-
0.1065 0.2258 0.4494 0.1693
-

RM3
0.29360 0.54400 0.67340 0.54490
-
0.29060 0.45130 0.78280 0.4478
-
0.32010 0.60100 0.73640 0.51550
-
0.1081 0.23010 0.45390 0.1699
-

RM3DT
0.29890 0.54380 0.67830 0.54400
0.11
0.29310, 1 0.45910 0.78810 0.45030, 1
0.12
0.31680 0.59950 0.73850 0.51340
-0.05
0.1078 0.22970 0.45140
0.1695
-0.01

RM3+All
0.29960, 1 0.54100 0.67960 0.55110, 1, 2
0.16
0.29460, 1, 2 0.46170 0.78910 0.45490, 1
0.19
0.32120, 2 0.60040 0.73910 0.52310, 1, 2
0.14
0.1081 0.23210 0.45910
0.1702
0.10

RM3+1 0.30540, 1, 2, 3
0.55400 0.69060, 1, 2, 3 0.55380, 1, 2, 3
0.12
0.29980, 1 0.47040, 1, 2
0.79630 0.45510, 1
0.11
0.33720, 1, 2, 3 0.61300
0.75030, 1, 2, 3 0.52340, 1, 2
0.24
0.11370, 1, 2, 3 0.23180
0.47020, 1, 2, 3
0.1692
0.23

RM3+2 0.30300, 1 0.54900 0.68590, 1, 2, 3 0.55100, 1
0.11
0.29520, 1 0.47140, 1, 2, 3
0.79230 0.45600
0.06
0.32370, 2 0.61200 0.73390 0.52020, 2
0.02
0.1080
0.2328
0.4487
0.1664
0.00

RM3+3 0.30750, 1, 2, 3
0.54100 0.68300, 1, 2, 3
0.53990
0.30
0.30360, 1, 2, 3 0.46780, 1, 2 0.79760, 1, 2, 3 0.45770, 1, 2
0.27
0.33230, 1, 2, 3 0.61700, 1, 2, 3 0.74460, 1, 2 0.52520, 1, 2
0.32
0.11480, 1, 2, 3 0.23740, 1, 2 0.46870, 1, 2, 3
0.1737
0.22

Table 2: Performance of the proposed methods and different baselines. Statistically significant improvements (measured by paired t-Test with 95% confidence) over LM, RM3, RM3DT and RM3+ALL are indicated by superscript 0, 1, 2 and 3, respectively, with maximum improvement in bold-faced.

nuclear proliferation

RM3 RM3DT RM3-All

nuclear. prolifer, weapon, 10, nation, year, soviet, 1, state, spread, date, regim, call, goal, limit nuclear, prolifer, intellig, cia, gate, mr, countri, weapon, soviet, iraq, chemic, effort, commun, agenc nuclear, prolifer, treati, weapon, farwick, pakistan, ntg, spector, qazvin, signatori, southasia, argentina, kalayeh, yongbyon, mcgoldrick

RM3+3 nuclear, prolifer, treati, weapon, soviet, intern, signatori, nation, armamaent, assess, review, regim, union, spread, peac

Table 3: Top 15 expansion terms selected by RM3, RM3+All and RM3+3

Topics

RM3

Latency

RM3DT RM3+ALL

RM3+1

RM3+2

RM3+3

TREC23

314 320 (1%) 390 (24%) 325 (3%) 353 (12%) 349 (11%)

TREC78Rb 756 773 (2%) 847 (12%) 769 (1%) 818 (8%) 778 (2%)

Tb 2,3

997 1005 (0.7%) 1046 (4%) 1004 (0.7%) 1009 (1%) 1002 (0.5%)

CW09B 1547 1789 (15%) 1897 (22%) 1825 (17%) 1806 (16%) 1743 (12%)

Table 4: Average computational latency in milliseconds for different methods on test queries. Percentage increase over RM3 (in parentheses); lowest increase in bold.

results suggested that the proposed heuristics (especially RM3+3 ) yield significant improvements in performance when compared
with the baselines. Further, the improvements are more robust and
the proposed heuristics are computationally more efficient than the
baselines.
REFERENCES
[1] M. Ariannezhad et al. 2017. Iterative Estimation of Document Relevance Score for Pseudo-Relevance Feedback. In Proc. of ECIR. 676­683.
[2] G. Cao, J. Nie, J. Gao, and S. Robertson. 2008. Selecting Good Expansion Terms for Pseudo-relevance Feedback. In Proc. of 31st ACM SIGIR. ACM, 243­250.
[3] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. 2001. An information-theoretic approach to automatic query expansion. ACM Trans. Inf. Syst. 19, 1 (2001), 1­27.
[4] Stéphane Clinchant and Eric Gaussier. 2013. A Theoretical Analysis of PseudoRelevance Feedback Models. In Proc. of ICTIR 2013. Article 6, 8 pages.

[5] Ronan Cummins. 2017. Improved Query-Topic Models Using Pseudo-Relevant PóLya Document Models. In Proc. of the ACM ICTIR 2017. 101­108.
[6] G. Furnas, T. Landauer, L. Gomez, and S. Dumais. 1987. The Vocabulary Problem in Human-system Communication. Commun. ACM 30, 11 (1987), 964­971.
[7] H. Hazimeh and C. Zhai. 2015. Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback. In Proc. of ICTIR 2015. 141­150.
[8] N. A. Jaleel, J. Allan, W. B. Croft, F. Diaz, L. S. Larkey, X. Li, M. D. Smucker, and C. Wade. 2004. UMass at TREC 2004: Novelty and HARD. In Proc. TREC '04.
[9] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proc. of 24th SIGIR (SIGIR '01). ACM, New York, NY, USA, 120­127.
[10] Kyung Soon Lee, W B. Croft, and J. Allan. 2008. A cluster-based resampling method for pseudo-relevance feedback. In SIGIR. 235­242.
[11] Y. Lv and C. Zhai. 2009. A Comparative Study of Methods for Estimating Query Language Models with Pseudo Feedback. In Proc. of 18th ACM CIKM. 1895­1898.
[12] Y. Lv and C. Zhai. 2010. Positional relevance model for pseudo-relevance feedback. In Proc. of 33rd ACM SIGIR. ACM, 579­586.
[13] A. Montazeralghaem, H. Zamani, and A. Shakery. 2016. Axiomatic Analysis for Improving the Log-Logistic Feedback Model. In Proc. of 39th ACM SIGIR. 765­768.
[14] A. Montazeralghaem, H. Zamani, and A. Shakery. 2018. Theoretical Analysis of Interdependent Constraints in Pseudo-Relevance Feedback. In SIGIR. 1249­1252.
[15] Javier Parapar and Álvaro Barreiro. 2011. Promoting Divergent Terms in the Estimation of Relevance Models. In Proc. of Third ICTIR'11. 77­88.
[16] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (April 2009), 333­389.
[17] T. Sakai, T. Manabe, and M. Koyama. 2005. Flexible pseudo-relevance feedback via selective sampling. ACM TALIP 4, 2 (2005), 111­135.
[18] C. Zhai and J. Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM TOIS. 22, 2 (2004), 179­214.

1256

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Corpus-based Set Expansion with Lexical Features and Distributed Representations

Puxuan Yu, Zhiqi Huang, Razieh Rahimi, and James Allan
Center for Intelligent Information Retrieval University of Massachusetts Amherst
{pxyu,zhiqihuang,rahimi,allan}@cs.umass.edu

ABSTRACT
Corpus-based set expansion refers to mining "sibling" entities of some given seed entities from a corpus. Previous works are limited to using either textual context matching or semantic matching to fulfill this task. Neither matching method takes full advantage of the rich information in free text. We present CaSE, an efficient unsupervised corpus-based set expansion framework that leverages lexical features as well as distributed representations of entities for the set expansion task. Experiments show that CaSE outperforms state-of-the-art set expansion algorithms in terms of expansion accuracy.
ACM Reference Format: Puxuan Yu, Zhiqi Huang, Razieh Rahimi, and James Allan. 2019. Corpusbased Set Expansion with Lexical Features and Distributed Representations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184. 3331359
1 INTRODUCTION
Corpus-based set expansion ­ i.e., finding in a given corpus the complete set of entities that belong to the same semantic class of a few seed entities ­ is a critical task in information retrieval and knowledge discovery. For example, given the input seed set {Massachusetts, Virginia, Washington}, a set expansion method is expected to output all other states in the United States. Set expansion is broadly useful for a number of downstream applications, such as question answering [14, 23], taxonomy construction [19], relation extraction [9], and query suggestion [1].
Most corpus-based approaches [5, 12, 15­18] are based on the assumption of distributional similarity [6], which, in the context of set expansion, can be understood on two levels: (1) contexts are in textual form so that expanded sets can be explained by reversing the process; and, (2) contexts are features of a latent model (e.g., Word2Vec [13] and BERT [4]) to generate distributed representations of entities. Each dimension of an embedding vector represents an unknown latent concept. Either perspective can be adopted to fulfill the task, though they both have limits. The former transforms
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331359

the task of finding sibling entities to finding optimal textual patterns. For an entity to be considered a candidate, it has to meet the "hard match" condition: sharing at least one textual pattern with at least one seed. Thus, many target entities end up with low relevance scores especially on smaller corpora. On the other side, distributed representations of entities do not require exact matching of textual patterns because they are calculated according to terms within a certain window, regardless of term arrangement. Therefore, not only sibling entities, but also other semantically related entities, such as twin or parent entities, are included in the final result.
Different from prior methods which explored either side of the distributional hypothesis, we propose CaSE (Corpus-based Set Expansion) framework that combines the two distributional similarity approaches. CaSE constructs a pool of candidate entities with lexical features and improves the ranking scores of target entities using the similarity of distributed representations with regard to user input. Among the two major approaches in corpus-based set expansion, CaSE is categorized as a one-time entity ranking method. Compared to iterative pattern-based bootstrapping, it is much more efficient at query time and is capable of avoiding semantic drift. In addition, unlike many other corpus-based set expansion techniques [7, 16, 18], CaSE does not rely on prior knowledge of relations among entities (e.g., web lists, knowledge bases) to work well. This is crucial because such external resources might not be available for certain languages or domains.
The major contributions of this paper are: (1) we propose the CaSE framework, which combines lexical context matching and distributed representations for set expansion; and, (2) our analysis discovers that inclusion relation between the entity sets and discrimination power of entity contexts can affect set expansion performance. The implementation and evaluation dataset described here are publicly available1.
2 RELATED WORK
Web-based Set Expansion: Web-based methods ­ including Google Sets [22], SEAL [23] and Lyretail [2] ­ submit queries consisting of seed entities to search engines and analyze the retrieved documents. The assumption that top-ranked webpages cover other entities in the same semantic class is not always true. Also, extracting data from online platforms can be time-consuming at query time. Therefore, most recent studies are proposed in an offline setting.
Corpus-based Set Expansion: Thelen and Riloff [21] described using certain contextual patterns to tag words with limited coarsegrained types. Roark and Charniak [15] first introduced a general set expansion solution based on co-occurrence of entities. Later,
1 https://github.com/PxYu/entity-expansion

1153

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

methods that define membership functions based on co-occurrences of entities with contexts were proposed [5, 17]. Instead of text corpora, SEISA [7] uses offline query logs and web lists, and does set expansion with an iterative similarity aggregation function. EgoSet [16] constructs clusters of entities using textual patterns and user-generated ontology respectively, and outputs clusters after refinement.
The most recent and comparable methods to our approach are SetExpan [18] and SetExpander [12]. Besides selecting contexts based on distributional similarity, SetExpan also leverages coarsegrained types from Wikipedia as features. SetExpan proposed resetting the context pool before each iteration to address the "semantic drift" problem, which turned out to be unsolved since false entities persist in later iterations. In addition, SetExpan takes hundreds of seconds per issued query, making it difficult to use with applications which involve user interaction. SetExpander takes the second perspective of distributional similarity, and generates variants of distributed representations from different patterns. Similarity scores of each candidate computed per representation with seed entities are treated as features, based on which an MLP binary classifier decides whether a candidate should be in the expanded set. Besides the limitation of solely using distributed representations, patterns such as explicit lists [17] cover only a small portion of entities.

3 METHODOLOGY

Intuitively, CaSE expands input seed entities by semantically related

entities that frequently share important contexts with seeds. The

first step is to extract features from the contexts of seed entities in

the corpus. Different features can be extracted from contexts of en-

tities. Potential features for entity e0 in sentence "w-2w-1e0w1w2" include unigrams (w1), n-grams (w1w2), and skip-grams (w-1_w1).

Skip-grams impose strong positional constraints [16], reducing the

risk of finding relevant concepts rather than true sibling entities.

The other alternative is to directly use predefined patterns, e.g.,

"such as e0, e1 and e2", for set expansion. However, Shi et al. [20]

showed that for large corpora, the construction of syntactic con-

texts has better accuracy and introduces less noise compared to

pattern based methods. Therefore, we extract skip-gram features

from entity contexts.

Some preprocessing steps are performed on the text corpus to

improve run-time efficiency. First, we extract the set of entities

E = {ei | i = 1, 2, · · · , N } in the given text corpus. We then con-

sider a window of size 4 around each entity mention in the corpus

and extract four skip-grams [-3, 0], [-2, 1], [-1, 2], and [0, 3] where

[-x, y] means keeping x words before and y words after the en-

tity mention. This setting allows more matchings and thus creates

candidate pool with higher recall. Let i = {ij | j = 1, 2, · · · , Mi }

denote the extracted skip-grams for ei . Then, the set of all skip-

grams in the corpus is  =

N i =1

i .

Based

on

these,

we

create

a

frequency matrix N ×M = {ij | i = 1, 2, . . . , N ; j = 1, 2, . . . , M },

where N = |E|, M = ||, and cell value ij is the number of co-

occurrences of entity i with skip-gram j.

We also acquire a distributed representation for each entity either

by training on the local corpus or using pre-trained representations.

Each entity ei is thus represented as a D dimensional embedding i · in matrix N ×D = {ik | i = 1, 2, . . . , N ; k = 1, 2, . . . , D}.

3.1 Context Feature Selection
At query time, we first build the set of candidate entities. Suppose the set of seeds S = {sq | q = 1, 2, . . . , L} is a subset of E, then the union of the skip-grams of seed entities, s , is a subset of . For a particular query, we derive a sub-matrix s from  by column projection; columns of s are the context features of seeds, s, and the rows represent all entities that share at least one context with
at least one seed. These entities are considered as candidate entities
for expansion.
We use s to quantitatively measure the correlation between seeds and skip-grams. First, we compute cqj as the co-occurrences of seed entity sq with skip-gram j over the total occurrences of j in the corpus. Then, the c-weight for skip-gram j given the current query is defined as:

L

L

cj = cqj =

q=1

q=1

qj

N i =1

i

j

.

(1)

This weight shows the quality of skip-grams, in that the higher the c-weight, the more relevant the skip-gram is to the seeds. Since candidate entities are obtained by selecting entities that share skipgrams with seed entities, weighting skip-grams of seed entities can be used to rank candidate entities.

3.2 Entity Search via Semantic Representation
We use semantic similarity between seed and candidate entities to further evaluate candidate entities. In preprocessing steps, we acquire a D dimensional word embedding matrix . The comparison between a seed entity and a candidate entity is equivalent to computing the cosine similarity of two corresponding rows. Denoting the cosine similarity of seed entity sq and candidate entity ei as cos(ei , sq ), the relatedness of ei to all seeds is

i

=

1 L

L
h(cos(ei , sq )),
q=1

(2)

where L is the length of the query and h(·) is an increasing and strictly positive function. The intuition behind h(·) is that the mathematical difference between cos(a, x) = 0.9 and cos(a, y) = 0.8 is not a sufficient description of the semantic difference between x and y. Finally, The score of entity ei with skip-gram j , denoted by ij , comprises three parts: the c-weight of j , the semantic similarity with seeds of ei , and the smoothed frequency of entity skip-gram co-occurrences. Formally, ij = cj · i · (ij ), where (·) is a concave function. Because an entity could associate with multiple skip-grams, the final score of ei is the summation over all possible skip-grams.

i =

j

ij =

1 L

h(cos(ei , sq ))
q

j

cqj (i j ) (3)
q

We compute i for each entity in the candidate pool. The set expansion result is the set of entities with top x highest scores, where
x is a predefined cutoff.

1154

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

4 EXPERIMENTS
4.1 Compared Methods
· Word2Vec [13]: We trained word embedding on our corpus using skip-gram Word2Vec model, where window size and number of iterations are set to 6 and 15, respectively. We then use embedding vectors of entities to retrieve the K nearest neighbors of seed entities as the expansion result.
· BERT [4]: BERT is an empirically powerful embedding model for several NLP tasks. We use a pre-trained BERT model (uncased, Large, 1,024 dimensions) to generate embeddings for all entities and perform KNN ranker similar to Word2Vec baseline.
· SetExpander [12]: We perform preprocessing, training and inference in the default setting on evaluation corpora. Implementation is distributed under Intel's NLP Architect Framework 2.
· SetExpan [18]: We run SetExpan in its default settings with preprocessing steps identical to CaSE.
· CaSE: The unsupervised set expansion framework we proposed. Functions h(·) and (·) in our model are set to power and root functions as h(cos(ei , sq )) = cos(ei , sq )7, and (ij ) = ij . There are three variations of CaSE: ­ CaSE-mdr: A simpler version of CaSE without distributional embeddings of entities, i.e., ij = cj · (ij ). ­ CaSE-BERT: CaSE model where distributed representations are acquired from a pre-trained BERT model. ­ CaSE-W2V: CaSE model where distributed representations are acquired from a locally trained Word2Vec model.
4.2 Experimental Setup
Datasets and Preprocessing: We use three corpora to evaluate CaSE. (1) AP89 is a collection of 84,678 news reports published by Associated Press in 1989. (2) WaPo is the TREC Washington Post Corpus which contains 608,180 news articles and blog posts from Jan. 2012 to Aug. 2017. (3) Wiki is a subset of English Wikipedia data dump from Oct. 2013, containing 463,819 Wikipedia entries. Consistent with prior work [18], we primarily use a data-driven phrase mining tool AutoPhrase [11] to obtain entity mentions. We adopt the entity mention list from Word2Phrase (part of the Word2Vec [13] Toolkit) as a trivial filter to improve precision. To reduce noise in the larger WaPo and Wiki corpora, four or fewer occurrences of entities in skip-grams are ignored, i.e., cells in  with values ij < 5 are set to 0.
Constructing queries: We build a collection of 62 semantic sets for evaluating set expansion algorithms as the selected combination of MRSCs [16], INEX-XER sets [3], SemSearch sets [10], and 12 additional sets from web resources [8]. To evaluate the sensitivity of our algorithm to the number of seed entities, we build queries with length ranging from 2 to 5. For each set consisting of n entities, we build min 100, nCm queries with m random seeds.
Evaluation Metrics: Set expansion algorithms retrieve a ranked list of entities in response to a query. We evaluate the top 100 retrieved entities for each query by all methods described in Section 4.1, except the SetExpan method where all retrieved entities after 10 iterations are evaluated. Mean Average Precision (MAP) is calculated for different queries with the same length across all
2 http://nlp_architect.nervanasys.com/term_set_expansion.html

1.0

SetExpander

SetExpan

0.8

CaSE-W2V

0.6

MAP

0.4

0.2

0.0 1

10

20

30

40

Set ID

50

60

Figure 1: Set-wise MAP of SetExpander, SetExpan and CaSE-W2V running 2-seed queries on Wiki corpus. Sets are ordered by MAP of CaSE-W2V decreasing.

Figure 2: MAP of all compared methods on Wiki.
evaluation sets. Statistical significant tests are performed using the two-tailed paired t-test at the 0.05 level.
4.3 Results and Discussion
Table 1 summarizes the overall performance of different methods for queries with different lengths on three corpora. The results indicate that the best variation of CaSE is CaSE-W2V, which shows robust improvements upon baselines on all corpora for queries of different length (Table 1 and Figure 2). In set-wise comparison, CaSE-W2V outperforms SetExpan and SetExpander with few exceptions (Figure 1) where entities hardly share skip-grams.
Robustness against input length: Intuitively, one might expect better performance given longer queries. SetExpan removes sub-optimal contexts in feature selection, thus showing the expected trend. Embeddings based methods demonstrate contrary behaviors, mainly because more seeds introduce more twin entities at top. CaSE does not remove features but weights them, and further weights entities with distributed similarity. As Table 1 shows, CaSE performs well even with few seeds, and improves slowly as the number of seeds increases.
Gap among evaluation sets: Figure 1 shows that some semantic sets are easier to expand than others. This result partially confirms earlier work showing that the performance of set expansion models improves as the frequencies of candidate entities increase [17]. To specifically show the correlation between entity frequencies and performance of set expansion, we define a composite property for each set T . For each entity ei in T , we first calculate the average of number of entities that occur in each skip-gram associated with entity ei , which is denoted by ki . A higher k value means the entity occurs in general contexts shared by more entities. Then,

1155

Short Research Papers 3A: AI, Mining, and others

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Retrieval accuracy (MAP) across all evaluation queries of all compared methods on different corpora. : statistically significant (95% confidence interval) improvement compared to SetExpan, the strongest baseline.

AP89

WaPo

Wiki

#seeds

2

3

4

5

2

3

4

5

2

3

4

5

Word2Vec BERT SetExpander SetExpan

0.032 0.103 0.058 0.095

0.030 0.094 0.067 0.103

0.027 0.091 0.073 0.111

0.027 0.087 0.076 0.117

0.046 0.078 0.046 0.083

0.041 0.072 0.054 0.094

0.037 0.063 0.060 0.103

0.035 0.061 0.065 0.111

0.082 0.062 0.070 0.106

0.075 0.058 0.079 0.119

0.071 0.055 0.082 0.126

0.066 0.050 0.086 0.131

CaSE-mdr CaSE-BERT CaSE-W2V

0.117 0.117 0.118 0.117 0.095 0.089 0.088 0.089 0.161 0.161 0.158 0.155 0.132 0.133 0.136 0.136 0.112 0.109 0.109 0.108 0.179 0.183 0.182 0.180 0.161 0.170 0.171 0.173 0.140 0.141 0.143 0.145 0.236 0.249 0.252 0.253

1.0

hypothetical curve

CaSE-W2V

0.8

0.6

MAP

0.4

0.2

0.0 0

2000

4000

6000

8000

K

Figure 3: Relations between composite property K and setwise MAP of CaSE-W2V on Wiki corpus.

the composite property of the set is defined as K = [ki /

M j =1

i

j

],

where

M j =1

i j

is

the

frequency

of

ei

in

the

corpus.

Figure

3

shows

the correlation between the defined metric K and set-wise MAP

performance of different sets using our proposed model. Intuitively,

lower MAP is expected for sets with higher K. Therefore, we fit

an exponentially decreasing function to points in the diagram of

Figure 3. There exists some outlier sets whose MAP performance is

low even with low K values. Investigating outlier sets, we discover

that these sets are conceptually subsets of some supersets, e.g., set

"allies of World War II" is a subset of set "all countries in the world".

The reason why outliers under-achieve in terms of MAP is that it

is difficult for set expansion models to disambiguate more specific

concepts from contexts unless directed to correct knowledge.

5 CONCLUSION AND FUTURE WORK
We present an unsupervised corpus-based set expansion framework, CaSE. We show that weighting entities directly with distributed embeddings and indirectly via lexical features significantly improves expansion accuracy of set expansion. In the future, we plan to improve CaSE's performance on less frequent sets by narrowing the scope of input, similar to a QA system.

ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

REFERENCES
[1] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. 2008. Contextaware query suggestion by mining click-through and session data. In Proceedings SIGKDD. ACM, 875­883.
[2] Z. Chen, M. Cafarella, and H. Jagadish. 2016. Long-tail vocabulary dictionary extraction from the web. In Proceedings WSDM. ACM, 625­634.
[3] A. P. De Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. 2007. Overview of the INEX 2007 entity ranking track. Springer, 245­251.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805 (2018).
[5] Z. Ghahramani and K. A. Heller. 2006. Bayesian sets. In Advances in neural information processing systems. 435­442.
[6] Z. S. Harris. 1954. Distributional structure. Word 10, 2-3 (1954), 146­162. [7] Y. He and D. Xin. 2011. Seisa: set expansion by iterative similarity aggregation. In
Proceedings of the 20th international conference on World wide web. ACM, 427­436. [8] C. Kelly and L. Kelly. 2019. http://www.manythings.org/ [9] J. Lang and J. Henderson. 2013. Graph-based seed set expansion for relation
extraction using random walk hitting times. In Proceedings NAACL/HLT. 772­776. [10] Y. Lei, V. Uren, and E. Motta. 2006. Semsearch: A search engine for the semantic
web. In KEOD. Springer, 238­245. [11] J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. 2015. Mining quality phrases from
massive text corpora. In Proceedings SIGMOD. ACM, 1729­1744. [12] J. Mamou, O. Pereg, M. Wasserblat, I. Dagan, Y. Goldberg, A. Eirew, Y. Green, S.
Guskin, P. Izsak, and D. Korat. 2018. Term Set Expansion based on Multi-Context Term Embeddings: an End-to-end Workflow. arXiv:1807.10104 (2018). [13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111­3119. [14] J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question answering using constraint satisfaction: QA-by-Dossier-with-Constraints. In Proceedings ACL. Association for Computational Linguistics, 574. [15] B. Roark and E. Charniak. 1998. Noun-phrase co-occurrence statistics for semiautomatic semantic lexicon construction. In Proceedings COLING. Association for Computational Linguistics, 1110­1116. [16] X. Rong, Z. Chen, Q. Mei, and E. Adar. 2016. Egoset: Exploiting word egonetworks and user-generated ontology for multifaceted set expansion. In Proceedings WSDM. ACM, 645­654. [17] L. Sarmento, V. Jijkuon, M. De Rijke, and E. Oliveira. 2007. More like these: growing entity classes from seeds. In Proceedings CIKM. ACM, 959­962. [18] J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, and J. Han. 2017. Setexpan: Corpus-based set expansion via context feature selection and rank ensemble. In ECML-PKDD. [19] J. Shen, Z. Wu, D. Lei, C. Zhang, X. Ren, M. T. Vanni, B. M. Sadler, and J. Han. 2018. HiExpan: Task-guided taxonomy construction by hierarchical tree expansion. In Proceedings SIGKDD. ACM, 2180­2189. [20] S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. 2010. Corpus-based semantic class mining: distributional vs. pattern-based approaches. In Proceedings COLING. 993­1001. [21] M. Thelen and E. Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings EMNLP. Association for Computational Linguistics, 214­221. [22] S. Tong and J. Dean. 2008. System and methods for automatically creating lists. US Patent 7,350,187. [23] R. C. Wang, N. Schlaefer, W. W. Cohen, and E. Nyberg. 2008. Automatic set expansion for list question answering. In Proceedings EMNLP. Association for Computational Linguistics, 947­954.

1156

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries

Saar Kuzi
skuzi2@illinois.edu University of Illinois at Urbana-Champaign
Anusri Pampari
anusri@stanford.edu Stanford University
ABSTRACT
In this paper, we address the problem of difficult queries by using a novel strategy of collaborative query construction where the search engine would actively engage users in an iterative process to continuously revise a query. This approach can be implemented in any search engine to provide search support for users via a "Help Me Search" button, which a user can click on as needed. We focus on studying a specific collaboration strategy where the search engine and the user work together to iteratively expand a query. We propose a possible implementation for this strategy in which the system generates candidate terms by utilizing the history of interactions of the user with the system. Evaluation using a simulated user study shows the great promise of the proposed approach. We also perform a case study with three real users which further illustrates the potential effectiveness of the approach.
ACM Reference format: Saar Kuzi, Abhishek Narwekar, Anusri Pampari, and ChengXiang Zhai. 2019. Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, Paris, France, July 21­25, 2019 (SIGIR '19), 4 pages. https://doi.org/10.1145/3331184.3331362
1 INTRODUCTION
The current search engines generally work well for popular queries where a large amount of click-through information can be leveraged. Such a strategy may fail for long-tail queries, which are entered by only a small number of users. Thus, for such queries, a search engine generally would have to rely mainly on matching the keywords in the query with those in documents. Unfortunately, such a method would not work well when the user's query does not include the "right" keywords. Users in such cases would often end up repeatedly reformulating a query, yet they still could not find the relevant
This work was done while the author was a student at UIUC.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331362

Abhishek Narwekar
narweka@amazon.com Amazon Alexa
ChengXiang Zhai
czhai@illinois.edu University of Illinois at Urbana-Champaign
documents. Unfortunately, there are many such queries, making it a pressing challenge for search engines to improve their accuracy.
In this paper, we address this problem and propose a general strategy of collaborative query construction where the search engine would actively engage users in an iterative process to revise a query. The proposed strategy attempts to optimize the collaboration between the user and the search engine and is based on the following assumptions: (1) Ideal query: For any difficult query, there exists an ideal query that, if constructed, would work well. This assumption allows us to re-frame the problem of how to help users as the problem of how to construct an ideal query. (2) Usersystem collaboration: User-system collaboration can be optimized by leveraging the strength of a search engine in "knowing" all the content in the collection and the strength of a user in recognizing a useful modification for the query among a set of candidates. (3) User effort: When facing a difficult query, the user would be willing to make some extra effort to collaborate with the search engine.
Our main idea is to optimize the user-system collaboration in order to perform a sequence of modifications to the query with the goal of reaching an ideal query. While the proposed strategy includes multiple ways to edit the query, we initially focus on studying a specific editing operator where the system suggests terms to the user to be added to the query at each step based on the history of interactions of the user with the system.
We perform an evaluation with a simulated user which demonstrates the great promise of this novel collaborative search support strategy for improving the accuracy of difficult queries with minimum effort from the user. The results also show that suggesting terms based on user interaction history improves effectiveness without incurring additional user effort. Finally, we conduct a case study with three real users that demonstrates the potential effectiveness of our approach when real users are involved.
2 RELATED WORK
The main novelty of our work is the idea of collaborative construction of an ideal query, specific algorithms for iterative query expansion, and the study of their effectiveness for difficult queries.
Previous works have studied approaches for interactive query expansion (e.g., [2, 4, 10]). According to these works, the user needs to select terms to be added to each query independently. Our framework is more general both in performing a sequence of query modifications to optimize the user-system collaboration and in allowing potentially other query modifications than simply adding terms.

1221

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

Furthermore, we propose methods which suggest terms to the user based on the history of interactions of the user with the system.
On the surface, our approach is similar to query suggestion already studied in previous works [3]. However, there are two important differences: (1) The suggested queries in our approach are expected to form a sequence of queries incrementally converging to an ideal query whereas query suggestion is done for each query independently. (2) The suggested queries in our method are composed of new terms extracted from the text collection, but the current methods for query suggestion tend to be able to only suggest queries taken from a search log.
Other works focused on developing query suggestion approaches for difficult queries [6, 11]. In general, ideas from past works on query suggestion can be used in our approach for generating the set of query modifications that are suggested to the user.
There is a large body of work on devising approaches for automatic query reformulation. One common method is to automatically add terms to the user's query [5]. Other approaches include, for example, substitution or deletion of terms [12]. The various ideas, which are suggested by works in this direction, can be integrated into our collaborative approach by devising sophisticated methods for term suggestion.
3 COLLABORATIVE QUERY CONSTRUCTION
Our suggested Collaborative Query Construction (CQC) approach is based on the Ideal Query Hypothesis (IQH), which states that for any information need of a user, there exists an ideal query that would allow a retrieval system to rank all the relevant documents above the non-relevant ones. The IQH implies that if a user has perfect knowledge about the document collection, then the user would be able to formulate an ideal query. The IQH is reasonable because it is generally possible to uniquely identify a document by just using a few terms that occur together in it but not in others. This point was also referred to in previous work as the perfect query paradox [8]. We note that the IQH may not always hold; for example, when there are duplicate documents. Nevertheless, it provides a sound conceptual basis for designing algorithms for supporting users in interactive search. Based on the IQH, the problem of optimizing retrieval accuracy can be reduced to the problem of finding the ideal query. Thus, based on this formulation, the main reason why a search task is difficult is that the user does not have enough knowledge to formulate the ideal query. In this paper, we address this problem by helping a user to construct an ideal query.
Our collaborative query construction process is represented by a sequence of queries, Q1, Q2, ..., Qn , where Q1 is the user's initial query, Qn is an ideal query, and Qi+1 is closer to Qn than Qi and the gap between Qi and Qi+1 is small enough for the user to recognize the improvement of Qi+1 over Qi . From the system's perspective, at any point of this process, the task is to suggest a set of candidate queries, while the user's task is to choose one of them. In this paper, we focus on a specific approach in which the query refinement is restricted to only adding one extra term to the query at each step. That is, a single collaborative iteration of revising a query Qi would be as follows: (1) Present the user a list of m candidate terms, Ti (not already selected). (2) The user selects a term, t  Ti . (3) Qi+1 = Qi {t }. (4) Qi+1 is used to retrieve a result list Di+1.

One advantage of using such an approach is that the gap be-

tween two adjacent queries is expected to be small enough for the

user to recognize the correct choice. Furthermore, although this

implementation strategy is very simple, theoretically speaking, the

process can guarantee the construction of any ideal query that

contains all the original query terms if the system can suggest ad-

ditional terms in the ideal query but not in the original query and

the user can recognize the terms to be included in the ideal query.

We assume that the original query terms are all "essential" and

should all be included in the ideal query. While true in general, in

some cases this assumption may not hold, which would require the

removal or substitution of terms in the initial query. In this paper,

however, we focus on term addition as our first strategy and leave

the incorporation of other operations for future work.

Following the game-theoretic framework for interactive IR [13],

our approach can be framed as the following Bayesian decision

problem where the goal is to decide a candidate set of terms Ti to

suggest to the user in response to the current query Qi :



Ti

=

arg min
T V -Qi

L(T , Hi , Q , U )p(Q |Hi , U )dQ ;
Q

(1)

where (1) Ti is a candidate set of terms to be presented to the user (a subset of the vocabulary V ). (2) Hi is all the information from the history of interactions of the user with the system. (3) Q is a unigram language model representing a potential ideal query. (4) U

denotes any relevant information about the user. (5) L(T , Hi , Q , U ) is a loss function assessing whether T is a good choice for Hi , U , and Q . (6) p(Q |Hi , U ) encodes the current belief about the ideal
query. The integral indicates the uncertainty about the ideal query,

which can be expected to be reduced as we collect more information

from the user.

While in general we need to assess the loss of an entire can-

didate set T , in the much simplified method that we will actually

explore, we choose T by scoring each term and then applying a

threshold to control the number of terms. That is, we assume that

the loss function on a term set T can be written as an aggregation

of the loss on each individual term. As an additional simplifica-

tion, we approximate the integral with the mode of the posterior probability about the ideal query, ^ Q . Thus, our decision problem would become to compute the score of each term t, not already selected by the user, as follows: s(t) = -L(t, Hi , ^ Q , U ); where ^ Q = arg maxQ p(Q |Hi , U ). Computationally, the algorithm boils down to the following two steps: (1) Given all of the observed information Hi and U , compute ^ Q . (2) Use ^ Q along with Hi and U to score each term in the vocabulary but not already in Qi .

4 TERM SCORING
According to the previous section, the optimal scoring function s(t) is based on the negative loss -L(t, Hi , ^ Q , U ). Intuitively, the loss of word t is negatively correlated with its probability according to ^ Q . We thus simply define our scoring function as s(t) = p(t |^ Q ). That is, our problem is now reduced to infer ^ Q given all of the observed information Hi and U .
Next, we suggest a model for inferring ^ Q , which is based on Pseudo-Relevance Feedback (PRF). This model is an extension of
the relevance model RM1 [7] to incorporate Hi (We leave the incorporation of U for future work as such data is not available to us.):

1222

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

p(t |^ Q ) =

p(t |d) · p(d |Q1, Hi ).

(2)

d Di

p(t |d) is estimated using the maximum likelihood approach. We

approximate p(d |Q1, Hi ) using a linear interpolation:

p(d |Q1, Hi ) = (1 -  ) · p(d |Q1) +  · p(d |Hi );

(3)

p(d |Q1) is proportional to the reciprocal rank of d w.r.t Q1;   [0, 1].
In order to estimate p(d |Hi ), two types of historical information are considered: (1) The terms selected by the user previously (HiT ). (2) The result lists presented to the user previously (HiD ). We combine these two components as follows: p(d |Hi ) = p(d |HiD ) · p(HiD |Hi ) + p(d |HiT ) · p(HiT |Hi ). (We assume p(HiD |Hi ) = p(HiT |Hi ).)
In order to estimate p(d |HiD ), we assume that documents which appear in the result list presented to the user in the current iteration,

and that were absent in the previous result list, represent aspects

of the information need that are more important to the user. We

thus

estimate p(d |HiD ) p(d |HiD ) =

as follows: 1
rankDi (d) ·

Z

D

d



Di

\ Di-1;

(4)

p(d |HiD ) = 0 for all other documents; rankDi (d) is the rank of document d in the result list Di ; ZD is a normalization factor.
We estimate p(d |HiT ) such that high importance is attributed to documents in which terms that were previously selected by the

user are prevalent.

i -1

p(d |HiT ) = p(d |tj , HiT ) · p(tj |HiT );

(5)

j =1

tj is the term selected by the user in the j'th iteration. p(d |tj , HiT ) is set to be proportional to the score of d with respect to tj as calculated by the system's ranking method. Assuming that terms

selected in more recent iterations are more important than older

ones, we estimate p(tj |HiT ) as: p(tj |HiT )

=

exp(-µ ·(i ZT

-j

))

;

ZT

is a

normalization factor; µ is a free parameter and is set to 0.5.

To conclude, we assign a probability to each term which is a lin-

ear interpolation of its probabilities in the documents in the result

list, where the interpolation weights are influenced by: (1) the rank

of the document, (2) the presence of the document in the previous

list, and (3) the frequency of terms that were previously selected.

Query representation: According to our approach, the query Qi is composed of the original query Q1 and the terms selected by the user. The terms in Qi are weighted based on a probability distribution such that the probability of a term t in V is: p(t |Qi ) = i ·pmle (t |Q1)+(1-i )·p(t |H ); p(t |H ) is proportional to the weight that was assigned to the term by the scoring method if this term
was previously selected, and is set to 0 otherwise; pmle (t |Q1) is the maximum likelihood estimate of t in Q1; i  [0, 1].

5 EVALUATION
The evaluation of the proposed strategy has two challenges: (1) The proposed approach is of interactive nature. (2) We are interested in focusing on difficult queries. We address these challenges by constructing a new test collection based on an existing collection that would focus on difficult queries and experimenting with simulated users. Finally, we perform a case study with three real users. Experimental setup: We use the ROBUST document collection

Table 1: Simulated user performance. Statistically significant differences with RM3 are marked with asterisk. All differences with the initial query are statistically significant.

Initial RM3 CQC

p @5
.000 .036 .057

Single Term

p@10 M RR success@10

.000 .053

.000

.040 .083

.238

.090 .127

.457

p @5
.000 .040 .137

Five Terms

p@10 M RR success@10

.000 .053

.000

.049 .090

.219

.136 .209

.447

(TREC discs 4 and 5-{CR}). The collection is composed of 528,155

newswire documents, along with 249 TREC topics which their titles

serve as queries (301-450, 601-700). Stopword removal and Krovetz

stemming were applied to both documents and queries. The Lucene

toolkit was used for experiments (lucene.apache.org). The BM25

model was used for ranking [9]. We use the following strategy to

construct our test set. We first perform retrieval for all queries.

Then, we remove from the collection the relevant documents that

are among the top 10 documents in each result list. After doing

that, we remain with 105 queries for which p@10 = 0 when per-

forming retrieval over the modified collection. We use these queries

for our evaluation, along with the modified collection. We report

performance in terms of precision (p@ {5, 10}) and Mean Recip-

rocal Rank (MRR@1000), which is more meaningful than Mean

Average Precision in the case of such difficult queries (it measures

how much effort a user has to make in order to reach the very

first relevant document). We also report the fraction of queries for

which a method resulted in p@10 > 0, denoted success@10. The

two-tailed paired t-test at 95% confidence level is used in order to

determine significant differences in performance.

Our approach involves free parameters, which are set to effective

ones following some preliminary experiments. We should point

out that our research questions are mainly about how promising

the proposed approach is as a novel interaction strategy, which is

generally orthogonal to the optimization of these parameters. The

number of terms suggested to the user, m, is set to 5. The number

of documents used in our PRF-based term scoring method is set

to 100. The interpolation parameter in Equation 3, , is set to 0.8.

The value of i , the weight given to the original query, is set to

max(0.4,

|Q1 |Qi

| |

);

we

chose

this

weighting

function

as

to

attribute

high importance to the original query when a small amount of

expansion is used. We compare the performance of our approach

with that of using the original query, and of using an automatic

query expansion approach in which a set of terms is automatically

added to the original query once. We set the number of expansion

terms to be equal to the number of terms that were added by the

user in the collaborative process. We use the RM3 [1] expansion

model (free parameters are set as in the collaborative approach).

Simulation study: In order to do a controlled study of our ap-

proach, we experiment with a simulated user. Given a list of term

suggestions, the simulated user chooses a term with the highest

t f .id f score in the relevant documents. Specifically, for each query

we concatenate all relevant documents and compute t f .id f based

on the single concatenated "relevant document". Our main result for

the simulated user experiment is presented in Table 1. We report the

performance when a single term or five terms are added. According

to the results, the collaborative approach (CQC) is very effective.

Specifically, after adding a single term to the query, users are able to

1223

Short Research Papers 3B: Recommendation and Evaluation

SIGIR '19, July 21­25, 2019, Paris, France

p@10 success@10
p@10 success@10

CQC-Q

CQC-H

0.13

CQC

CQC-Q

CQC-H

0.52

CQC

0.50 0.12

0.48 0.11

0.46 0.10

0.44

0.09

1

2

3

4

5

Additional Terms

1

2

3

4

5

Additional Terms

Figure 1: Performance of the different model components.

see a noticeable improvement on the first page of search results in about 45% of these difficult queries that did not return any relevant document initially (success@10). Furthermore, our approach outperforms the initial query to a statistically significant degree for all evaluation measures (for both number of terms) and is also much more effective than RM3. Our term scoring method utilizes both the original query and the user interaction history. We are interested in examining the relative importance of these individual components. Setting  = 0 in Equation 3 results in a model that uses only the original query (CQC-Q). Setting  = 1 results in a model that uses only user history (CQC-H). The results are presented in Figure 1. Focusing on p@10, we can see that all components are very effective. Comparing the different components, we can see that CQC-H is outperformed by CQC-Q for a small number of terms, and the opposite holds for a large number. In terms of success@10, we can see that all model components achieve the highest performance when two terms are added, with CQC-H being the best performing one. Interestingly, success@10 decreases as more terms are added. That is, while adding more terms to the query can improve the average performance, it results in a less robust approach. Case study with real users: We are interested in examining whether real users can recognize the "good" terms suggested by the system. To gain some initial understanding regarding this issue, we conducted a case study with three real users. We note that the conclusions that can be drawn from this study are limited due the small number of users. Yet, this study is still useful for getting some intuition regarding the utility of the approach. Each participant performed three iterations of the collaborative process for 30 queries. Specifically, we selected queries that achieved the highest performance in terms of p@10 after adding a single term by the simulated user. We chose these queries as we are interested to study the following research question: given a term scoring method that can provide effective terms, can the user identify them? For each query, the user was presented with the initial query, a text describing the topic, and the guidelines regarding how a relevant document should look like (all are part of the TREC topics). After issuing a query, the users are presented with a result list of 10 documents (a title and a short summary of 5 sentences are presented).
In Figure 2, we compare the performance of the real users with that of the simulated user. According to the results, retrieval performance can be very good when terms are selected by real users. Specifically, all users reach success@10 of around 0.5. That is, after adding a single term, at least one relevant result is obtained for about 50% of the queries. In Table 2, we present examples of queries along with the terms that were selected by a single real user and a simulated user. We also report the performance that resulted from adding a term. The first query serves as an example where the real user outperforms the simulated user by a better choice of terms. The second query is an example where the simulated user

0.35

Simulated User

Real User (1)

0.30

Real User (2)

0.8

Real User (3)

0.25

0.6 0.20

0.15

0.4

0.10

0.2

Simulated User

Real User (1) 0.05

Real User (2)

Real User (3)

0.00

0.0

0

1

2

3

0

1

2

3

Additional Terms

Additional Terms

Figure 2: Real users vs. Simulated user. Table 2: Query Examples. The performance of the query

(p@10) after adding a term is reported in the brackets.

curbing population growth Stirling engine antibiotics ineffectiveness

Real User Simulated Real User Simulated Real User Simulated

plan (0.0) china (0.1) company (0.0)
cfc (0.9) infection (0.2)
drug (0.1)

family (0.2) economic (0.1) financial (0.0)
hcfc (1.0) research (0.2) pharmaceutical (0.2)

birth (0.6) rate (0.2) group (0.0) hyph (1.0) study (0.2) product (0.1)

outperforms the real user presumably by recognizing the correct technical terms. Finally, the last query is an example where both users achieve similar performance, but using different terms.
6 CONCLUSIONS AND FUTURE WORK
We proposed and studied a novel strategy for improving the accuracy of difficult queries by having the search engine and the user collaboratively expand the original query. Evaluation with simulated users and a case study with real users show the great promise of this strategy. In future work, we plan to devise more methods for term scoring, incorporate more operations for query modification, and perform a large-scale user study. Acknowledgments. This material is based upon work supported by the National Science Foundation under grant number 1801652.
REFERENCES
[1] Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. [n. d.]. UMass at TREC 2004: Novelty and HARD. Technical Report.
[2] Peter Anick. 2003. Using terminological feedback for web search refinement: a log-based study. In Proceedings of SIGIR. ACM, 88­95.
[3] Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Mendoza. 2004. Query recommendation using query logs in search engines. In International Conference on Extending Database Technology. Springer, 588­596.
[4] Nicholas J. Belkin, Colleen Cool, Diane Kelly, S-J Lin, SY Park, J Perez-Carballo, and C Sikora. 2001. Iterative exploration, design and evaluation of support for query reformulation in interactive information retrieval. Information Processing & Management 37, 3 (2001), 403­434.
[5] Claudio Carpineto and Giovanni Romano. 2012. A Survey of Automatic Query Expansion in Information Retrieval. ACM Comput. Surv. (2012).
[6] Van Dang and Bruce W Croft. 2010. Query reformulation using anchor text. In Proceedings of WSDM. ACM, 41­50.
[7] Victor Lavrenko and W Bruce Croft. 2001. Relevance based language models. In Proceedings of SIGIR. ACM, 120­127.
[8] David Dolan Lewis. 1992. Representation and learning in information retrieval. Ph.D. Dissertation. University of Massachusetts at Amherst.
[9] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR. Springer-Verlag New York, Inc., 232­241.
[10] Ian Ruthven. 2003. Re-examining the potential effectiveness of interactive query expansion. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM, 213­220.
[11] Yang Song and Li-wei He. 2010. Optimal rare query suggestion with implicit user feedback. In Proceedings of WWW. ACM, 901­910.
[12] Xuanhui Wang and ChengXiang Zhai. 2008. Mining term association patterns from search logs for effective query reformulation. In Proceedings of CIKM. ACM, 479­488.
[13] ChengXiang Zhai. 2016. Towards a game-theoretic framework for text data retrieval. IEEE Data Eng. Bull. 39, 3 (2016), 51­62.

1224

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

ery Performance Prediction for Pseudo-Feedback-Based Retrieval

Haggai Roitman
IBM Research ­ Haifa haggai@il.ibm.com
ABSTRACT
The query performance prediction task (QPP) is estimating retrieval e ectiveness in the absence of relevance judgments. Prior work has focused on prediction for retrieval methods based on surface level query-document similarities (e.g., query likelihood). We address the prediction challenge for pseudo-feedback-based retrieval methods which utilize an initial retrieval to induce a new query model; the query model is then used for a second ( nal) retrieval. Our suggested approach accounts for the presumed e ectiveness of the initially retrieved list, its similarity with the nal retrieved list and properties of the latter. Empirical evaluation demonstrates the clear merits of our approach.
ACM Reference Format: Haggai Roitman and Oren Kurland. 2019. Query Performance Prediction for Pseudo-Feedback-Based Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331369
1 INTRODUCTION
The query performance prediction task (QPP) has attracted much research attention [2]. The goal is to evaluate search e ectiveness with no relevance judgments. Almost all existing QPP methods are (implicitly or explicitly) based on the assumption that retrieval is performed using (only) document-query surface-level similarities [2]; e.g., standard language-model-based retrieval or Okapi BM25.
We address the QPP challenge for a di erent, common, retrieval paradigm: pseudo-feedback-based retrieval [3]. That is, an initial search is performed for a query. Then, top-retrieved documents, considered pseudo relevant, are utilized to induce a query model (e.g., expanded query form) that is used for a second ( nal) retrieval.
Thus, in contrast to the single-retrieval setting addressed in almost all prior work on QPP, here the e ectiveness of the nal result list presented to the user depends not only on the retrieval used to produce it (e.g., properties of the induced query model), but also on the initial retrieval using which the query model was induced. A case in point, if the initial retrieval is poor, it is highly unlikely
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331369

Oren Kurland
Technion ­ Israel Institute of Technology kurland@ie.technion.ac.il
that the nal result list will be e ective regardless of the querymodel induction approach employed. Accordingly, our novel approach for QPP for pseudo-feedback-based retrieval accounts for the presumed e ectiveness of the initially retrieved list, its association with the nal retrieved list and properties of the latter.
Empirical evaluation shows that the prediction quality of our approach substantially transcends that of state-of-the-art prediction methods adopted for the pseudo-feedback-based retrieval setting -- the practice in prior work on QPP for pseudo-feedback-based retrieval [6, 14, 15].
2 RELATED WORK
In prior work on QPP for pseudo-feedback-based retrieval, existing predictors were applied either to the nal retrieved list [6, 14] or to the initially retrieved list [15]. We show that our prediction model, which incorporates prediction for both lists and accounts for their association, substantially outperforms these prior approaches.
The selective query expansion task (e.g., [1, 5, 13]) is to decide whether to use the pseudo-feedback-based query model, or stick to the original query. In contrast, we predict performance for a list retrieved using the query model.
In several prediction methods, a result list retrieved using a pseudofeedback-based query model is used to predict the performance of the initially retrieved list [15, 20]. In contrast, our goal is to predict the e ectiveness of the nal result list; to that end, we also use prediction performed for the initial list.
3 PREDICTION FRAMEWORK
Suppose that some initial search is applied in response to a query q over a document corpus D. Let Dinit be the list of the k most highly ranked documents. Information induced from the top documents in Dinit is used for creating a new query model (e.g., expanded query) used for a second retrieval; i.e., these documents are treated as pseudo relevant. We use Dscnd to denote the result list, presented to the user who issued q, of the k documents most highly ranked in the second retrieval.
Our goal is to predict the e ectiveness of Dscnd . To this end, we appeal to a recently proposed query performance prediction (QPP) framework [16]. Speci cally, the prediction task amounts to estimating the relevance likelihood of Dscnd , p(Dscnd |q, r ), where r is a relevance event1.
We can use reference document lists to derive an estimate for p(Dscnd |q, r ) [16]:
1This is post-retrieval prediction which relies on analyzing the retrieved list. The relevance of a retrieved list is a notion that generalizes that for a single document [16]. At the operational level, a binary relevance judgment for a list can be obtained by thresholding any list-based evaluation measure.

1261

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

p^(Dsc

nd

|q,

r

)

def
=

p^(Dscnd |q, L, r )p^(L|q, r );

(1)

L

L is a document list retrieved for q; herein, p^ is an estimate for p. The underlying idea is that strong association (e.g., similarity) of Dscnd with reference lists L (i.e., high p^(Dscnd |q, L, r )) which are presumably e ective (i.e., high p^(L|q, r )) attests to retrieval e ectiveness.
It was shown that numerous existing post-retrieval prediction methods can be instantiated from Equation 1 where a single reference list is used. Similarly, here we use Dinit as a reference list:

p^(Dscnd |q, r )  p^(Dscnd |q, Dinit , r )p^(Dinit |q, r ).

(2)

That is, by the virtue of the way Dscnd is created -- i.e., using information induced from Dinit -- we assume that Dinit is the
most informative reference list with respect to Dscnd 's e ectiveness. A case in point, an expanded query constructed from a poor

initial list (i.e., which mostly contains non-relevant documents) is

not likely to result in e ective retrieval.

3.1 Instantiating Predictors

Equation 2 can be instantiated in various ways, based on the choice

of estimates, to yield a speci c prediction method. To begin with,

any post-retrieval predictor, P, can be used to derive p^(Dinit |q, r )

[16].

For p^(Dscnd |q, Dinit , r ) in Equation 2, we use logarithmic inter-

polation:

p^(D s c n d

|q,

Dini

t,

r

)

def
=

p^[P](Dscnd |q, r ) p^(Dscnd |Dinit , r )(1- );

(3)

 ( [0, 1]) is a free parameter. The estimate p^[P](Dscnd |q, r ) corre-

sponds to the predicted e ectiveness of Dscnd , where the predic-

tion, performed using the post-retrieval predictor P, ignores the

knowledge that Dscnd was produced using information induced from Dinit .
The estimate p^(Dscnd |Dinit , r ) from Equation 3, of the association between Dscnd and Dinit , is usually devised based on some symmetric inter-list similarity measure sim(Dscnd , Dinit ) [16]. However, as Roitman [11] has recently suggested, a more e ective esti-

mate can be derived by exploiting the asymmetric co-relevance rela-

tionship between the two lists (cf., [10]); that is, p^(Dscnd |Dinit , r )

is the likelihood of Dscnd given that a relevance event has hap-

pened and Dinit was observed:

p^(D s c nd

|Dinit

,

r

)

def
=

p^(D s c nd

|Dinit

)

d

Dscnd

p^(d

|Dscnd

) p^(d

p^(d, r |Dinit

|Dinit ) )p^(r |Dinit

)

;

(4)

d

is

a

document.

Following

Roitman

[11],

we

use

p^(D s c nd

|Dinit

)

def
=

sim(Dscnd , Dinit ). Similarly to some prior work [7, 11], for p^(r |Dinit )

we use the entropy of the centroid (i.e., the arithmetic mean) of the

language models of documents in Dinit . We further assume that p^(d |Dscnd ) and p^(d |Dinit ) are uniformly distributed over Dscnd and Dinit , respectively. Finally, to derive p^(d, r |Dinit ), we follow
Roitman [11] and use the corpus-based regularized cross entropy

(CE) between a relevance model, R [Dinit ], induced from Dinit , and

a language model, pd (·), induced from d:

p^(d, r |Dinit )

def
=

CE(R [Dinit ] ||pd (·)) - CE(R [Dinit ] ||p D (·));

(5)

p^D (·) is a language model induced from the corpus. Further details about language model induction are provided in Section 4.1.

4 EVALUATION
4.1 Experimental setup
4.1.1 Datasets. We used for evaluation the following TREC corpora and topics: WT10g (451-550), GOV2 (701-850), ROBUST (301450, 601-700) and AP (51-150). These datasets are commonly used in work on QPP [2]. Titles of TREC topics were used as queries. We used the Apache Lucene2 open source search library for indexing and retrieval. Documents and queries were processed using Lucene's English text analysis (i.e., tokenization, lowercasing, Porter stemming and stopping). For the retrieval method -- both the initial retrieval and the second one using the induced query model -- we use the language-model-based cross-entropy scoring (Lucene's implementation) with Dirichlet smoothed document language models where the smoothing parameter was set to 1000.

4.1.2 Pseudo-feedback based retrieval. Let cx (w) denote the oc-

currence

count

of

a

term

w

in

a

text

(or

text

collection)

x

;

let

|x

|

def
=

w x

cx (w)

denote

x 's

length.

Let

px[µ ] (w )

def
=

cx (w )+µpD (w ) |x |+µ

de-

note

x 's

Dirichlet-smoothed

language

model,

where pD (w)

def
=

c

D (w |D|

)

.

For

a

query q

and a

set of

pseudo-relevant

documents F



Dinit , pF (·) denotes a pseudo-feedback-based query model.

We use three state-of-the-art pseudo-feedback-based (PRF) query-

model induction methods. All three incorporate query anchoring

as described below. The rst is the Relevance Model [8] (RM):

pF

(w

)

def
=

pd[0] (w )pq[µ ] (d ),

(6)

d F

where

pq[µ

]

(d

)

def
=

pd[µ ] (q ) dF p^d[µ](q)

and

pd[µ ] (q )

def
=

w q cd (w) log pd[µ](w).

The second is the Generative Mixed Model [19] (GMM) which is

estimated using the following EM algorithm iterative update rules:

t

(n)(w

)

def
=

(1-

)p

(n-1) F

(w

)

(1-

)p

(n-1) F

(w

)+

pD

(w

)

,

pF(n)(w )

def
=

d F cd (w )t (n)(w )
wV d F cd (w )t (n)(w ) .

The third is the Maximum-Entropy Divergence Minimization Model [9]

(MEDMM): pF (w)  exp

1 

d F p^q[µ](d) log pd[0](w) -

 

p

D

(w

)

.

We

applied

query

anchoring

[8,

9,

19]

to

all

three

models:

pF

,



(w

)

def
=

pqMLE (w) + (1 - )pF (w); pqMLE (w) is the maximum likelihood es-

timate of w with respect to q and   [0, 1].

We used the n most highly ranked documents in the initial re-

trieval for query-model induction (i.e., inducing pF (·)). Then, a sec-
ond query qf was formed using the l terms w assigned the highest pF (w). We resubmitted qf to Lucene3 to obtain Dscnd .

4.1.3 Baseline predictors. As a rst line of baselines, we use Clarity [4], WIG [20] and NQC [17], which are commonly used post-retrieval QPP methods [2]. These baselines are also used for P in Eq. 3. Clarity [4] is the divergence between a language model induced from a retrieved list and that induced from the corpus.

2 http://lucene.apache.org 3Expressed in Lucene's query parser syntax as: w1^pF (w1) w2^pF (w2) . . . wl^pF (wl ).

1262

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Prediction quality. Boldface: best results per basic QPP method and query-model induction method. Underlined: best results per query-model induction method. '' marks a statistically signi cant di erence between PFR-QPP and either the second best predictor (in case PFR-QPP is the best) or the best predictor (in case PFR-QPP is not).

Method
ListSim
NQC(Dscnd |qf ) NQC(Dscnd |q) NQC(Dinit |q) RefList(NQC) PFR-QPP(NQC)
Clarity(Dscnd |qf ) Clarity(Dscnd |q) Clarity(Dinit |q) RefList(Clarity) PFR-QPP(Clarity)
WIG(Dscnd |qf ) WIG(Dscnd |q) WIG(Dinit |q) RefList(WIG) PFR-QPP(WIG)
WEG(Dscnd |qf ) WEG(Dscnd |q) WEG(Dinit |q) RefList(WEG) PFR-QPP(WEG)

RM
.442
.293 .071 .483 .535 .513
.292 .327 .363 .481 .408
.270 .253 .237 .338 .370
.231 .141 .353 .443 .456

WT10g

GMM MEDMM

.532

.337

.228

.182

.051

.092

.397

.424

.531

.415

.557

.410

.325

.316

.227

.368

.350

.314

.567 .557

.388 .398

.307

.388

.105

.153

.221

.224

.384 .466

.311 .353

.205

.331

.134

.239

.311

.313

.483 .575

.371 .436

RM
.490
.599 .437 .486 .517 .596
.230 .278 .282 .480 .615
.263 .583 .562 .581 .630
.585 .513 .532 .527 .660

GOV2

GMM MEDMM

.432

.410

.545

.353

.418

.283

.414

.414

.486

.457

.549 .550

.157

.130

.200

.084

.261

.264

.469 .497

.414 .490

.301

.448

.424

.276

.498

.498

.562 .603

.480 .575

.548

.432

.504

.390

.470

.409

.481 .562

.427 .481

RM
.543
.653 .475 .635 .654 .671
.450 .412 .452 .582 .589
.424 .651 .649 .660 .665
.661 .566 .635 .654 .688

ROBUST

GMM MEDMM

.528

.436

.637

.622

.492

.620

.605

.602

.631 .661

.621 .642

.393

.409

.350

.349

.441

.401

.575 .607

.535 .566

.361

.381

.455

.430

.618

.578

.638 .682

.637 .648

.656

.693

.571

.674

.619

.616

.633 .664

.632 .688

RM
.537
.655 .574 .550 .607 .670
.313 .236 .320 .589 .652
.159 .414 .554 .639 .650
.627 .560 .526 .580 .675

AP GMM
.343
.617 .479 .536 .530 .640
.408 .350 .456 .519 .585
.281 .281 .614 .580 .634
.562 .491 .474 .467 .552

MEDMM
.407
.454 .530 .502 .572 .650
.339 .270 .308 .511 .651
.285 .226 .505 .608 .643
.575 .575 .518 .555 .664

WIG [20] and NQC [17] are the corpus-regularized4 mean and
standard deviation of retrieval scores in the list, respectively. We
further compare with the Weighted Expansion Gain (WEG) [6] method
­ a WIG alternative which regularizes with the mean score of doc-
uments at low ranks of the retrieved list instead of the corpus.
We use three variants of each of the four predictors described
above. The rst two directly predict the e ectiveness of the nal
retrieved list Dscnd using either (i) the original query q (denoted P(Dscnd |q)), or (ii) the query qf (denoted P(Dscnd |qf )) which was induced from Dinit as described above (cf., [15, 20]). The third variant (denoted P(Dinit |q)) is based on predicting the performance of Dscnd by applying the predictor to Dinit as was the case in [15].
To evaluate the impact of our inter-list association measure in
Eq. 4, we use two additional baselines. The rst, denoted ListSim
[16], uses sim(Dscnd , Dinit ) to predict the performance of Dscnd . The second, denoted RefList(P) [7, 16], treats Dinit as a pseudoe ective list of Dscnd and estimates Dscnd 's performance by:

p^Re

f

Li

st

(r

|Dscnd

,

q)

def
=

sim(Dscnd , Dinit )p^[P](Dinit |q, r ),

where P is one of the four basic QPP methods described above. There are two important di erences between our proposed method and RefList. First, we use the query q in the list association measure in Eq. 3. Second, we use an asymmetric co-relevance measure between the two lists in Eq. 4 compared to the symmetric one used in RefList.

4.1.4 Setup. Hereinafter, we refer to our proposed QPP method from Eq. 2 as PFR-QPP: Pseudo-Feedback based Retrieval QPP.

4To this end, the corpus is treated as one large document.

PFR-QPP(P) is a speci c predictor instantiated using the base predictor P. We predict for each query the e ectiveness of the 1000 documents (i.e., k = 1000) most highly ranked in the nal result list Dscnd . Prediction quality is measured using the Pearson correlation between the ground truth AP@1000 (according to TREC's relevance judgments) and the values assigned to the queries by a prediction method.
Most prediction methods described above incorporate free parameters. Following the common practice [2], we set m  k ­ the number of documents in a given list (i.e., either Dscnd or Dinit ) used for calculating a given predictor's value; with m  {5, 10, 20, 50, 100, 150, 200, 500, 1000}. We applied term clipping with l terms (l  {10, 20, 50, 100}) to the relevance model used in Clarity and PFRQPP. Following [16], we realized the ListSim, RefList and PFRQPP baselines using Rank-Biased Overlap (RBO(p)) [18] as our listsimilarity measure sim(·) (with p = 0.95, further following [16]). For our PFR-QPP method, we set   {0, 0.1, 0.2, . . . , 0.9, 1.0}. Query models are induced from the n = 50 top documents. For the GMM and MEDMM models, we set their free-parameters to previously recommended values, i.e., GMM ( = 0.5) [19] and MEDMM ( = 1.2,  = 0.1) [9]. Unless stated otherwise, the query anchoring and clip-size parameters in all models were xed to  = 0.9 and l = 20, respectively. The prediction quality for other (, l) settings is studied in Section 4.2.
Following [12, 17], we trained and tested all methods using a 2-fold cross validation approach. Speci cally, in each dataset, we generated 30 random splits of the query set; each split had two folds. We used the rst fold as the (query) train set. We kept the second fold for testing. We recorded the average prediction quality

1263

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Pearson

NQC
0.7

0.6

0.5

0.4

0.3 0

0.2 0.4 0.6 0.8 

WIG
0.7

0.6

0.5

0.4

0.3 0

0.2 0.4 0.6 0.8 

Pearson

Pearson

0.7 0.6 0.5 0.4 0.3
0
0.7 0.6 0.5 0.4 0.3
0

Clarity
0.2 0.4 0.6 0.8 
WEG
0.2 0.4 0.6 0.8 

Pearson

Pearson

NQC
0.7 0.6 0.5 0.4 0.3
10 30 50 70 90 l
WIG
0.7 0.6 0.5 0.4 0.3
10 30 50 70 90 l

Pearson

Pearson

Clarity
0.7
0.6
0.5
0.4
0.3 10 30 50 70 90 l
WEG
0.7
0.6
0.5
0.4
0.3 10 30 50 70 90 l

Pearson

(a) Query anchoring ().

(b) Number of terms (l ).

Figure 1: Sensitivity to free-parameter values of the relevance model used for query-model induction.

over the 30 splits. Finally, we measured statistically signi cant differences of prediction quality using a two-tailed paired t-test with p < 0.05 computed over all 30 splits.
4.2 Results
Table 1 reports the prediction quality of our method and the baselines. We can see that in the vast majority of cases, our PFR-QPP approach statistically signi cantly outperforms the baselines.
Applying basic QPP methods. We rst compare the three variants of the four basic QPP methods. We observe that, in most cases, utilizing the PRF-induced query qf for predicting the performance of the nal list Dscnd (P(Dscnd |qf )), yields better prediction quality than using the original query q (P(Dscnd |q)). In addition, predicting the performance of Dscnd by applying the base predictor to the initially retrieved list Dinit (P(Dinit |q)) yields high prediction quality -- sometimes even higher than applying the predictor to Dscnd . These ndings provide further support the motivation behind PFR-QPP: integrating prediction for the initially retrieved list and the nal retrieved list and accounting for their asymmetric co-relevance relation.
PFR-QPP vs. reference-list based alternatives. First, in line with previous work [7, 15, 16], the high prediction quality of ListSim and RefList in our setting shows that the similarity between the two lists is an e ective performance indicator. Moreover, combining prediction for the performance of the initial list with its similarity with the nal list (i.e., RefList) yields prediction quality that transcends in most cases that of using only the similarity (i.e., ListSim). Finally, our PFR-QPP method which uses prediction for both the initial and nal lists, and accounts for their asymmetric co-relevance relationship, outperforms both ListSim and RefList in most cases, and often to a statistically signi cant degree.
Sensitivity to query-model induction tuning. Using the ROBUST dataset and the relevance model (RM), Figure 1 reports the e ect on prediction quality of varying the value of the query anchoring parameter (; while xing l = 20) and the number of terms used after clipping (l; while xing  = 0) in the query model, and hence, in qf . As can be seen, decreasing  or increasing l decreases the prediction quality of all methods. With reduced query anchoring or when using more terms, the induced queries (qf ) tend to become

more "verbose", with less emphasis on the original query q. Indeed, a recent study showed that existing QPP methods are less robust for long queries [12]. Finally, we see that for any value of  and l, PFR-QPP outperforms the baselines.
5 CONCLUSIONS
We addressed the QPP task for pseudo-feedback-based retrieval, where the nal retrieved list depends on an initially retrieved list ­ e.g., via a query model induced from the latter and used to produce the former. Our approach accounts for the predicted e ectiveness of each of the two lists as well as to their asymmetric co-relevance relation. Empirical evaluation showed that our approach signi cantly outperforms a variety of strong baselines.
ACKNOWLEDGEMENT
We thank the reviewers for their comments. This work was supported in part by the Israel Science Foundation (grant no. 1136/17)
REFERENCES
[1] Giambattista Amati, Claudio Carpineto, and Giovanni Romano. Query di culty, robustness, and selective application of query expansion. In Proceedings of ECIR, pages 127­137, 2004.
[2] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of SIGIR, pages 1196­1197, New York, NY, USA, 2012. ACM.
[3] Claudio Carpineto and Giovanni Romano. A survey of automatic query expansion in information retrieval. ACM Comput. Surv., 44(1):1:1­1:50, January 2012.
[4] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query performance. In Proceedings of SIGIR, pages 299­306, New York, NY, USA, 2002. ACM.
[5] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. A framework for selective query expansion. In Proceedings of CIKM, pages 236­237, New York, NY, USA, 2004. ACM.
[6] Ahmad Khwileh, Andy Way, and Gareth J. F. Jones. Improving the reliability of query expansion for user-generated speech retrieval using query performance prediction. In CLEF, 2017.
[7] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri Rom. Back to the roots: A probabilistic framework for query-performance prediction. In Proceedings of CIKM, pages 823­832, New York, NY, USA, 2012. ACM.
[8] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In Proceedings of SIGIR.
[9] Yuanhua Lv and ChengXiang Zhai. Revisiting the divergence minimization feedback model. In CIKM '14.
[10] Fiana Raiber, Oren Kurland, Filip Radlinski, and Milad Shokouhi. Learning asymmetric co-relevance. In Proceedings of ICTIR, pages 281­290, 2015.
[11] Haggai Roitman. Enhanced performance prediction of fusion-based retrieval. In Proceedings of ICTIR, pages 195­198, New York, NY, USA, 2018. ACM.
[12] Haggai Roitman. An extended query performance prediction framework utilizing passage-level information. In Proceedings of ICTIR, pages 35­42, New York, NY, USA, 2018. ACM.
[13] Haggai Roitman, Ella Rabinovich, and Oren Sar Shalom. As stable as you are: Re-ranking search results using query-drift analysis. In Proceedings of HT, pages 33­37, New York, NY, USA, 2018. ACM.
[14] Harrisen Scells, Leif Azzopardi, Guido Zuccon, and Bevan Koopman. Query variation performance prediction for systematic reviews. In Proceedings of SIGIR, pages 1089­1092, New York, NY, USA, 2018. ACM.
[15] Anna Shtok, Oren Kurland, and David Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proceedings of SIGIR, pages 259­266, New York, NY, USA, 2010. ACM.
[16] Anna Shtok, Oren Kurland, and David Carmel. Query performance prediction using reference lists. ACM Trans. Inf. Syst., 34(4):19:1­19:34, June 2016.
[17] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst., 30(2):11:1­11:35, May 2012.
[18] William Webber, Alistair Mo at, and Justin Zobel. A similarity measure for inde nite rankings. ACM Trans. Inf. Syst., 28(4):20:1­20:38, November 2010.
[19] Chengxiang Zhai and John La erty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01.
[20] Yun Zhou and W. Bruce Croft. Query performance prediction in web search environments. In Proceedings of SIGIR, pages 543­550, New York, NY, USA, 2007. ACM.

1264

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

Enhanced News Retrieval: Passages Lead the Way!

Matteo Catena
ISTI-CNR, Pisa, Italy matteo.catena@isti.cnr.it

Ophir Frieder
Georgetown University, USA ophir@ir.cs.georgetown.edu

Cristina Ioana Muntean
ISTI-CNR, Pisa, Italy cristina.muntean@isti.cnr.it

Franco Maria Nardini
ISTI-CNR, Pisa, Italy francomaria.nardini@isti.cnr.it

Raffaele Perego
ISTI-CNR, Pisa, Italy raffaele.perego@isti.cnr.it

Nicola Tonellotto
ISTI-CNR, Pisa, Italy nicola.tonellotto@isti.cnr.it

ABSTRACT
We observe that most relevant terms in unstructured news articles are primarily concentrated towards the beginning and the end of the document. Exploiting this observation, we propose a novel version of the classical BM25 weighting model, called BM25 Passage (BM25P), which scores query results by computing a linear combination of term statistics in the different portions of news articles. Our experimentation, conducted using three publicly available news datasets, demonstrates that BM25P markedly outperforms BM25 in term of effectiveness by up to 17.44% in NDCG@5 and 85% in NDCG@1.
CCS CONCEPTS
· Information systems  Probabilistic retrieval models;
ACM Reference Format: Matteo Catena, Ophir Frieder, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, and Nicola Tonellotto. 2019. Enhanced News Retrieval: Passages Lead the Way!. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331373
1 INTRODUCTION
Passage retrieval, present in literature for decades [10], is the task of retrieving only portions of documents, i.e., passages, relevant to a particular information need. At times, passage retrieval is viewed as an intermediate step in other information retrieval tasks, e.g., question answering and summarization.
Believing that certain passages pose greater relevance to a given query, we investigate how such relevance can be exploited to improve retrieval on a particular domain, specifically news retrieval. We differ from both existing passage retrieval [10] and passage detection [6] efforts, where the aim is to either retrieve only highly relevant passages or detect unrelated injected passages from within documents, respectively. In contrast, our goal is not to answer a query by retrieving single passages or detect injected unrelated passages, but to focus on improving the effectiveness of a retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331373

system in retrieving entire news articles. To this end, we exploit passage relevance, capitalizing on their keyword density.
Specifically, we introduce a variant of the well-known BM25 weighting model [7], called BM25 Passage (BM25P), to improve the effectiveness of a news retrieval system. BM25P takes into account the entire news article when assigning a relevance score; however, BM25P distinguishes the importance of different news passages by assigning different weights to different passages. BM25P exploits such portions of text by creating a weighted linear combination of term frequencies per passage, improving the effectiveness of the news retrieval. To derive the weights, we analyze the density of highly discriminative terms in the collection of documents, measured in term of inverse document frequency, and observe where they are distributed throughout the content. This approach is efficient since it is query independent and is applied at index construction time, i.e., pre-retrieval.
The exploitation of term positions in Information Retrieval applications is common. One of the most notable examples related to our work is the BM25F weighting model [9], where term statistics are computed separately for the different fields that make up a document (e.g., title, headings, abstract and body) and then combined together within a BM25-based model. Our proposal resembles closely BM25F, but it considers the positions of the highly relevant terms occurring in the body of unstructured documents.
Term positions are also exploited in the news context for news summarization and classification tasks [2­4]. In news recommendation, the first few sentences and the article title are known to boost the performance of recommender systems. The performance of the system can be further improved by considering the rest of the document, and the best results can be observed when using the whole article text, as in our approach [1, 12]. This result suggests that although news articles tend to concentrate relevant content in the beginning, this does not necessarily imply that the remaining sections of the text can be ignored without hindering accuracy. By making the best of these two observations, we analyze the distribution of the occurrences of highly relevant terms and note that news documents belonging to different collections are consistently characterized by areas with different densities of highly relevant terms. We thus exploit this fact to improve a classical IR weighting model such as BM25. To the best of our knowledge, this is the first contribution in this direction exploiting the in-document distributions of impactful terms within news documents in the BM25 weighting function.

1269

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

2 PROBLEM

In modern information retrieval systems, given a user query, a relevance score is associated with the query-document pairs. Such
relevance score is computed by exploiting a heuristic similarity
function, estimating, according to some statistical procedure, the
probability of relevance of a document with respect to a query. Then, the documents retrieved are ranked by their score, and the K
documents with the highest score are returned to the user.
The BM25 scoring function is among the most successful query-
document similarity functions, whose roots lie in the Probabilistic
Relevance Framework [7]. In most IR systems, the relevance score sq (d) for a document d given a query q follows the general outline given by the best match strategy: sq (d) = t q st (q, d), where st (q, d) is a term-document similarity function that depends on the number of occurrences of term t in document d and query q, on other document statistics such as document length, and on
term statistics such as the inverse document frequency (IDF). In particular, in the BM25 weighting model, the relevance score st (q, d) is given by:

st (q, d) = wq k1

(k1 + 1)t f

(1

-

b

)

+

b

dl av  _d l

wIDF , +tf

(1)

where t f is the in-document term frequency, dl is the document

length, av_dl is the average document length of the collection, wq

is a query-only weight, b and k1 are parameters (defaults b = 0.75,

k1 = 1.2). The wI DF component is the IDF factor, which is given

by wI DF

=

log

N -Nt +0.5 Nt +0.5

,

where

N

is the number of documents

in

the collection, and Nt is the document frequency of term t.

When taking into account the fields that make up a document

(e.g., title, headings, abstract and body), each field may be treated as

a separate collection of (unstructured) documents over the whole

collection, and the relevance score of a document can be computed

as a weighted linear combination of the BM25 scores over the indi-

vidual fields. However, in [9] the authors proved that such a linear

combination of scores has several drawbacks, such as breaking the

t f saturation after a few occurrences (a document matching a single

query term over several fields could rank higher than a document

matching several query terms in one field only), or affecting the

document length parameter (when the document length is referred

to the actual field weight rather than the whole document). Hence,

the authors suggested the BM25F weighting model for structured

documents, computing a weighted linear combination of field-based

term frequencies and then plugging that combination into the BM25

formula. The novel t f factor boosts the specific fields without alter-

ing collection statistics. The BM25F model is considered one of the

most successful Web search and corporate search algorithms [8].

With unstructured documents we lack the strong relevance sig-

nals derived from the term frequency of the query keywords in the

different fields available in Web document. However, we formulate

the hypothesis that in curated unstructured documents such as

news articles it is possible to leverage the distribution of keywords

in the documents to derive analogous strong relevance signals.

To validate our hypothesis on the structure of news articles and

to quantify the impact of some distinguishing document portions

(referred to as passage in the following) over other portions, we

analyze the density of highly discriminative terms in large news

Top 15

Top 10

Aquaint

0.13 0.09 0.09 0.09 0.09 0.10 0.09 0.09 0.09 0.14

0.21

RCV1 0.13 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.15

Signal 0.14 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.14

0.18

Aquaint RCV1

0.15 0.08 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.15 0.15 0.08 0.08 0.08 0.08 0.09 0.09 0.09 0.09 0.17

0.15

Signal Aquaint

0.16 0.09 0.09 0.08 0.08 0.09 0.08 0.08 0.09 0.16 0.21 0.07 0.07 0.07 0.07 0.08 0.07 0.07 0.07 0.20

0.12

RCV1 0.21 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.21

0.09

Signal 0.21 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.22

1 2 3 4 5 6 7 8 9 10

Top 5

Figure 1: Probability distribution for the positions of key terms occurrences in the news articles of the three collections used.

corpora. We consider a term as discriminative, hereinafter key term, if it appears in only a few documents, i.e., it has a high IDF value.
For each news article in our three test collections (detailed in Sec. 3), we identify the positions of the occurrences of the k terms with the highest IDF. To aggregate such positional information, we evenly split each news article d into a set P(d) of 10 passages having about the same length1. Then, we compute the distributions of the occurrences of the key terms in each of these passages. Finally, we average these values over the entire dataset, giving the distributions shown as heatmaps in Fig. 1 for the top 5, 10 and 15 key terms. As demonstrated by the plots, independently from the datasets considered, the first and last parts of news articles are more likely to include key terms than the remaining parts. Moreover, the lower the number k of the top key terms considered, the more skewed the probability distribution. The higher likelihood of key terms occurring in the opening passages was expected. Several news writing guides highlight the need of engaging the reader instantly and summarizing what the story is all about in the opening sentences. The thumbnail rule states that the first sentence(s) should contain all of the who, what, when, where, why and how of the news2. On the other hand, no specific rule for closing the news articles is given in writing guides, and the very high likelihood observed even for the last part of the news articles is surprising. Moreover, slight differences in the probabilities are apparent even for the middle passages. Such analysis motivated us to investigate if exploiting this probability distribution, by weighting differently these areas in the news article, can enhance retrieval effectiveness.
Hence, we propose a variant of BM25 called BM25P which uses different weights for the different passages. Our proposed BM25P model computes a linear combination t fP of the term frequencies t fi in each passage i of document d (re-scaled by the parameter ):

t fP =

wi · t fi .

(2)

i P (d)

As suggested in [9] we plugged the term frequency t fP into the original BM25 formula (Eq. (1)), rather than summing the BM25 scores per passage. The empirical probability distribution depicted

1Tests conducted with different values of |P (d ) | are not discussed for lack of space. 2 http://handbook.reuters.com.

1270

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

in Fig. 1 gives us a clear indication of the impact of each passage within the document from the point of view of important terms. This probability distribution is used to compute the term frequency weights: wi is directly proportional to the probability distribution of important terms in the i-th passage. We re-scale all weights with the hyperparameter  to amplify the importance of highly relevant terms in impactful passages. In the following we will use the distributions of top-5, top-10 and top-15 key terms as different passage weighting methods to be plugged into BM25P, which we henceforth refer to as BM25P5, BM25P10, and BM25P15. Note that BM25P with all passage weights and  set to 1 is equivalent to BM25.
https://www.overleaf.com/project/5c506da7b0bc603b37fb19de

3 EXPERIMENTAL SETUP
The experimental assessment of the proposed weighting model relies on the following corpora of English news articles:
· the AQUAINT Corpus by Linguistic Data Consortium (Aquaint), · the Signal Media One-Million News Articles Dataset (Signal), · the Reuters Corpus, Volume 1, version 2 (RCV1).
The 2005 Robust and HARD TREC tracks provide 50 queries and their associated relevance judgements for the Aquaint dataset. The Signal and RCV1 datasets do not provide any evaluation data. Hence, for these two datasets, we adopt the methodology described in [5] and use the news titles as pseudo-queries. According to this methodology, there is only one relevant news article for each query, i.e., the article to which the title belongs to. All other articles of the collection are considered to be non-relevant. Specifically, for each of these two datasets we randomly selected 40, 000 documents to generate the same number of pseudo-queries for each collection. Statistics for the three datasets are summarized in Table 1.

Table 1: Statistics of the three collections used. Dataset # Queries avg. QLen # Docs avg. DocLen

Aquaint Signal RCV1

50 40,000 40,000

2.60 1,033,000 249.42

6.64 1,000,000 224.22

5.77

804,000

147.38

For each dataset, we index the unstructured body of news articles (by ignoring titles and all collection-specific fields such as source, category, media type, and publishing date) into positional indices, with Terrier. This type of index provides us with the positions of query term occurrences within the document, to differently weight the contribution of matching terms.
With the query relevance data built as detailed above, we investigate if, by weighting news passages differently, our proposed BM25P model is able to improve retrieval effectiveness w.r.t. BM25. We answer this research question by retrieving the top 1, 000 documents for each query from the respective news corpus by using BM25 and BM25P. With BM25P, documents are virtually divided into 10 passages weighted as discussed above.
Once queries have been processed, we observe the rank of the relevant documents retrieved and compare the results obtained for BM25P with the BM25 ones. To measure retrieval effectiveness, we consider NDCG@k and MRR metrics. NDCG@k is used to evaluate the performance on the Aquaint dataset, where we have multiple relevant documents per query. Conversely, MRR, as the

Table 2: NDCG at different cutoffs for BM25 and BM25P ( = 10) on the Aquaint collection. We highlight statistical significant differences w.r.t. BM25 with  for p < 0.01 and  for p < 0.05 according to the two sample t-test [11].

NDCG@k
NDCG@1 NDCG@3 NDCG@5 NDCG@10 NDCG@15 NDCG@20

BM25
0.200 0.291 0.280 0.270 0.269 0.273

BM25P5
0.310 +55.0%  0.303 +4.12% 0.288 +3.03% 0.271 +0.11% 0.271 +0.65% 0.268 -2.11%

BM25P10
0.370 +85.0% 0.335 +15.01% 0.329 +17.44% 0.298 +10.20%  0.296 +9.96% 0.289 +5.81% 

BM25P15
0.290 +45.0%  0.317 +8.78% 0.301 +7.39% 0.291 +7.44% 0.290 +7.91%  0.282 +3.35% 

mean of the reciprocal of the rank of the first relevant result, allows us to quantify how good is a given retrieval method in pushing a relevant result towards top rank positions, especially for the Signal and RCV1 datasets, where only one relevant document per query is known. We also evaluate the baseline BM25 and the weighting methods proposed for BM25P, i.e., BM25P5, BM25P10, and BM25P15, for different values of the  hyper-parameter.

4 EXPERIMENTAL RESULTS
The experiments conducted aim to assess whether BM25P achieves a better overall ranking quality with respect to BM25. Table 2 reports the NDCG at different cutoffs measured on the Aquaint dataset for BM25, BM25P5, BM25P10, and BM25P15. All these tests were performed with  = 10. We highlight that BM25P consistently outperforms BM25. Indeed, BM25P10 results the best setting for the passage weights, with improvements over BM25 that are always statistically significant apart from a single case (NDCG@3). The relative improvement ranges from 5.81% for NDCG@20 to 85% for NDCG@1. Moreover, in five of the six cases, BM25P10 shows statistically significant results with p-values of p < 0.01. The other proposed methods, i.e., BM25P5 and BM25P15, also improve NDCG over BM25, although with smaller relative benefits. In only one case, NDCG@20 with BM25P5, our weighting model has a lower NDCG than BM25, but the difference is not statistically significant.
We further investigate the performance of BM25P against BM25 on the Aquaint dataset, by varying  to assess the impact of this hyper-parameter on the retrieval effectiveness measured in terms of NDCG@5. We present the results of this investigation in Figure 2. Results show that, for   10, BM25P always performs better than BM25. For BM25P5 and BM25P10, the effectiveness does not sensibly increase for  values greater than 10, while for BM25P15 the performance tends to increase even if it is not able to outperform the one of BM25P10 for any value of  . In conclusion BM2510 with  = 10 is the best weighting model in terms of NDCG@5.
It is worth highlighting that, since the Aquaint dataset provides 50 queries only, the achievement of statistically significant improvements is particularly challenging. Therefore, we investigate the robustness of such improvements by testing BM25P also on the Signal and RCV1 datasets. For each one of these datasets we have in fact 40, 000 pseudo-queries obtained from the news titles as previously discussed. The results of these additional experiments are reported in Table 3, where we evaluate the retrieval performance in terms of MRR for the Signal, RCV1 and Aquaint datasets.
The results show that BM25P performs significantly better than BM25 on all three datatsets, thus confirming the results achieved by

1271

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

0.32

0.30

NDCG@5

0.28

0.26

BM25P5

BM25P10

0.24

BM25P15

BM25

0

10

20

30

40

50



Figure 2: NDCG@5 for BM25 and BM25P, on Aquaint, for BM25P and for different values of  from 5 to 50.

BM25P in terms of NDCG@k on the Aquaint collection. Indeed, results also confirm that the best performing method on this dataset is BM25P10 when  = 10. A slightly different result is achieved for the Signal and RCV1 datasets, where the best performing method results to be BM25P5. Indeed, on these collections, BM25P5, BM25P10 and BM25P15 always show statistically significant improvements w.r.t. BM25 with p < 0.01 for   10.
Table 3 reports the MRR while varying the value of . MRR is higher for   10 than for  < 10. When  = 10, the average value of the scaled weights wi is equal to 1, i.e., the value of  divided by the number of passages. When  < 10, the average value of the scaled weights wi becomes lesser than 1, thus penalizing the contribution of t fP with respect to the document length normalization in the denominator of Eq.(1). Conversely, the mean of the weights is greater than or equal to 1 when   10, and the initial and final passages of the news can get larger weights than the others passages. The best performing setting is BM25P10 ( = 10) for Aquaint and BM25P5 ( = 20) for Signal and RCV1. A possible explanation of this slight difference is that pseudo-queries of Signal and RCV1 benefit from the skewed probability distribution of BM25P5, which gives a larger importance to the first and last passages and seems to better approximate where the pseudo-queries match. Indeed, results achieved with MRR for Aquaint are consistent with the ones discussed for NDCG@k; namely BM25P10 is the best method and statistically outperforms BM25. BM25P5 and BM25P15 also behave well on Aquaint, but the improvement is statistically significant just for few values of  in the case of BM25P15. BM25P10 uses top 10 highest IDF terms in each document to create a probability distribution of their positions. We also look at top 15 in the case of BM25P15, but increasing the number of terms for computing the distribution does not yield better results. We can conclude that 10 terms for Aquaint and 5 terms for Signal and RCV1 achieve the best results and the distribution flattens as we increase this number (see Figure 1), making it closer to the uniform weighting of BM25.
5 CONCLUSIONS
For news articles, we observed that a common stylistic feature is the preponderance of occurrences of key terms (i.e., terms with an high IDF value) at the beginning and at the end of the article. We proposed BM25P, a variant of BM25, which considers key term

Table 3: MRR for BM25 and BM25P on the three collections for different values of  . We report statistical significance w.r.t. BM25 with  for p < 0.01 and  for p < 0.05.


Model

1

5

10 20 30 40 50

BM25 0.485 0.485 0.485 0.485 0.485 0.485 0.485

Aquaint

BM25P5 BM25P10

0.438 0.458

0.518 0.547 0.548 0.544 0.554 0.554 0.577 0.591 0.578 0.588  0.589 0.586

BM25P15 0.446 0.532 0.540 0.547 0.545 0.558 0.558

Signal

BM25 0.342 0.342 0.342 0.342 0.342 0.342 0.342 BM25P5 0.268 0.337 0.351 0.356 0.356 0.354 0.352 BM25P10 0.276 0.340 0.350 0.353 0.352 0.351 0.349 BM25P15 0.276 0.339 0.349 0.351 0.350 0.348 0.347

RCV1

BM25 0.340 0.340 0.340 0.340 0.340 0.340 0.340
BM25P5 0.258 0.344 0.363 0.369 0.365 0.360 0.356 BM25P10 0.253 0.339 0.356 0.360 0.356 0.351 0.347 BM25P15 0.249 0.334 0.351 0.355 0.351 0.346 0.342

distribution variations among the different passages of the news.
In BM25P such distribution information is used to assign different
weights to the occurrences of query terms, depending on which
passage they appear in, boosting or reducing the importance of
certain passages in the document, typically giving greater impor-
tance to the first and last passages. This distinguishes BM25P from
the traditional BM25 which does not consider the position of the
occurrences in the document. Our experiments showed that, by
differently weighting news passages, BM25P markedly improves
NDCG and MRR with respect to using BM25. In particular, we ob-
served that BM25P significantly improves NDCG on Aquaint with
percentages up to 85% for small cutoffs, while the MRR computed
on Signal and RCV1 increases of 4.1% and 8.5% respectively.
As future work we plan to study the impact of (adaptively) vary-
ing the number of passages weighted ­ here set equal to 10 ­ and the
use of our BM25P model in conjunction with BM25F for retrieving
semi-structured news articles.
Acknowledgements. This paper is supported by the EU H2020 BIGDATAGRAPES (grant agreement No¯780751).
REFERENCES
[1] Toine Bogers and Antal van den Bosch. 2007. Comparing and Evaluating Information Retrieval Algorithms for News Recommendation. In Proc. RecSys. ACM, 141­144.
[2] Jose M. Chenlo and David E. Losada. 2014. An empirical study of sentence features for subjectivity and polarity classification. Inf. Sc. 280 (2014), 275 ­ 288.
[3] Dipanjan Das and André F.T. Martins. 2007. A survey on automatic text summarization. Lit. Survey for the Lang. and Stat. II course at CMU 4 (2007), 192­195.
[4] Chin-Yew Lin. 1999. Training a Selection Function for Extraction. In Proc. CIKM. ACM, 55­62.
[5] Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. Content-Based Weak Supervision for Ad-Hoc Re-Ranking. In SIGIR 2019.
[6] Saket Mengle and Nazli Goharian. 2009. Passage detection using text classification. JASIST 60, 4 (2009), 814­825.
[7] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1996. Okapi at TREC-3. 109­126.
[8] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (April 2009), 333­389.
[9] Stephen E. Robertson, Hugo Zaragoza, and Michael Taylor. 2004. Simple BM25 Extension to Multiple Weighted Fields. In Proc. CIKM. ACM, 42­49.
[10] Gerard Salton, James Allan, and Chris Buckley. 1993. Approaches to Passage Retrieval in Full Text Information Systems. In Proc. SIGIR. ACM, 49­58.
[11] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Statistical Significance Tests for Information Retrieval Evaluation. In Proc. CIKM. ACM, 623­632.
[12] Anastasios Tombros and Mark Sanderson. 1998. Advantages of Query Biased Summaries in Information Retrieval. In Proc. SIGIR. ACM, 2­10.

1272

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

LIRME: Locally Interpretable Ranking Model Explanation

Manisha Verma
Verizon Media, New York, USA manishav@verizonmedia.com
ABSTRACT
Information retrieval (IR) models often employ complex variations in term weights to compute an aggregated similarity score of a query-document pair. Treating IR models as black-boxes makes it difficult to understand or explain why certain documents are retrieved at top-ranks for a given query. Local explanation models have emerged as a popular means to understand individual predictions of classification models. However, there is no systematic investigation that learns to interpret IR models, which is in fact the core contribution of our work in this paper. We explore three sampling methods to train an explanation model and propose two metrics to evaluate explanations generated for an IR model. Our experiments reveal some interesting observations, namely that a) diversity in samples is important for training local explanation models, and b) the stability of a model is inversely proportional to the number of parameters used to explain the model.
CCS CONCEPTS
· Information systems  Information retrieval; Content analysis and feature selection; Retrieval models and ranking;
KEYWORDS
Interpretability, Ranking, Point-wise explanations
ACM Reference format: Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings of Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, Paris, France, July 21­25, 2019 (SIGIR '19), 4 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
It has been shown that complex machine learning (ML) models can encode different biases [4] and may present myopic results [2] to users, leading researchers to investigate ways of `explaining' model outcomes. Providing explanations can also be now required by law, mandated by regulations such as GDPR [1], wherein commercial ranking services may now be required to `explain' its consumers why a document is retrieved for a query. A wide range of modelagnostic local explanation approaches have been proposed for ML tasks [5, 8], most of them focusing on providing an instance-wise explanation of the output of the model as either a subset of input
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn

Debasis Ganguly
IBM Research, Dublin, Ireland debasis.ganguly1@ie.ibm.com
features [3, 10], or a weighted distribution of feature importance [6, 9]. While the explanation space itself and methods to generate explanations are widely known in practise for classification tasks, their utility is largely unexplored for ranking tasks. There is little existing work in the IR community to systematically investigate ways of generating explanations for an IR model. Given that IR models involve complex variations in term weighting functions for scoring query-document pairs, some models may not be easy to `explain' to a search engine user, who may have questions such as `Why does a search engine retrieve document D at rank k?'.
In this work, with the motivation of `explanations' in IR, we explore ways of generating and evaluating explanations. We focus on model-agnostic point-wise explanations, i.e. estimating `explanation vectors' with respect to a retrieval model without any knowledge about its internals. The weight of each token in the term importance vector indicates its contribution to a ranking model's output for a given query-document pair. This vector can then be analyzed to see the relative importance of terms contributing positively (e.g. frequent presence of informative terms) or negatively (otherwise) to the score of a document.
To estimate the explanation vector for a query-document pair (Q, D), it is useful to study how a model behaves on variations of D. Since we aim to generate model-agnostic explanations, we study a model's behavior by examining the retrieval scores of different sub-samples drawn from D. Each IR model, due to its different ways of addressing the document lengths, term weights or collection statistics, may behave differently on these sub-samples. Local explanation models will be as effective as the words sampled for producing explanations. Explanation models (such as [9]) trained on poor samples will generate noisy and illegible explanations for documents. Therefore, we investigate three ways of sampling terms for model training and evaluate these sampling strategies for different explanation lengths. Our proposed explanation model then seeks to capture local effects on the sub-samples and predict a distribution of term importance potentially capturing the IR model's inherent term weighting characteristics.
We propose two metrics for evaluating the stability and correctness of the generated explanations. The first metric evaluates the sensitivity of the explanation model, i.e. the scale of change in explanations with change in model parameters. The second evaluation metric is more specific to IR, in which we evaluate the effectiveness of the explanations in terms of document relevance. Our experiments on the TREC ad hoc dataset indicate that sampling methods that are biased with tf-idf or positional information produce weaker explanations than those generated by uniformly sampling words from documents. We also found that explanation stability decreases with an increase in the number of explanation words; and that this effect is more pronounced for non-relevant documents.

1281

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

2 RELATED WORK
Singh et. al. [12] investigate methods to train an explanation model for a given base ranker. The document rankings generated from the explanation model are then compared to those generated by the base model. Contrastingly, in our work, the focus is to evaluate different sampling strategies and automatically evaluate the consistency and effectiveness of explanation models. Specifically, our work explores methods to generate data for explanation model training and evaluate its effectiveness across queries. The metrics and sampling methods explored in this work can be adapted easily to train and evaluate new explanation models such as those proposed in [11, 12]. We do not investigate ways of transforming document scores into class probabilities [13], as explanation models can be trained directly on `scores' assigned by any ranker (even those trained using deep learning). Our sampling methods and evaluation metrics are ranker agnostic and can be used to evaluate explanations of different lengths across queries.
3 LOCAL EXPLANATION OF IR MODELS
In classification tasks, model predictions can be understood by analyzing the predictions of simpler (human interpretable) variations of the data instances [9]. These simpler and human interpretable explanations take different forms for different tasks, e.g., the authors of [9] argue that one can explain the labels of images predicted by a classifier with a number of focused regions extracted from an input image. Similarly, for text classification, one can examine the changes in predictions of the classifier when different subset of terms are sampled from an input document.
Since the working principle of a ranking task is different from that of classification, in this work we investigate different ways of generating model-agnostic interpretable explanations for ranked lists. Formally, given a set of documents D and a query Q, the ranking function S(D, Q) induces a total order on the set D. For traditional IR models, such as BM25 or language models (LM), the similarity function is computed by aggregating the term weight contributions from matching terms. A ranking function can be represented as S(D, Q) = t DQ w(t, D), where w(t, D) represents the term weight of term t in a document D [7].
To generate explanations, it is required to select set of simple instances or sub-instances, where each sub-instance is comprised of partial information extracted from a particular document. We employ a weighted squared loss to predict the score of the entire document D with respect to the input query. We call this method locally interpretable ranking model explanation (LIRME).

M
L(D, Q,  ; ) = (D, Di) S(D, Q) - S(Di, Q) 2 +  ||

i =1

M

p

= (D, Di) S(D, Q) - jw(tj , Di))2 +  ||.

i =1

j =1

(1)

In Equation 1, Di = i (D) denotes the ith sample extracted from a

document D comprised of p unique terms;  is an L1 regularization

term; and   Rp denotes a vector of p real-valued parameters used

to approximate the score query Q. Additionally, the

of the sub-sample weight of the loss

D(iD w, Ditih),riessapseicmt itloartihtye

between the document D and its sub-sample Di. A standard way to define  in Equation 1 is with a kernel function of the form

(D, D)

=

exp(-

x2 h

),

x

= arccos(D, D)

(2)

where arccos(D, D) denotes the cosine-distance (angle) between a

document D and a sub-document sampled from it, and h denotes

the width of a Gaussian kernel.

The weighted loss function of Equation 1 predicts S(D, Q) using

the given samples. Since a retrieval model computes the score of an

entire document and also the scores of its sub-samples, the predicted vector ^  Rp estimates the importance of each term, e.g. the jth component of ^ denotes the likelihood of term tj in contributing positively to the overall score S(D, Q).
It is expected that weights in ^ that correspond to a query term

will have larger weights (denoting higher importance). Non-query terms with high weights in ^ are potentially the ones that are

semantically related to the query and hence are likely to be relevant

to its underlying information need. A visualization of these terms

may then provide the desired explanation of an observed score of a

document D with respect to Q (high or low).

3.1 Sampling of Explanation Instances

We now describe three different ways to define the sampling func-

tion  (D) that can be used to construct a set of samples around the

neighbourhood of D for the purpose of predicting the parameter vector, ^ , to explain a retrieval model.

Uniform Sampling:. A simple way to sample from the neigh-

bourhood of an given document D is to sample terms with a uniform

likelihood (with replacement). This ensures that there is no bias

towards term selection leading to likely generation of a diverse set

of samples for a document.

Biased Sampling:. Another way to sample terms is to set the

sampling probability of a term proportional to its tf-idf weight

seeking to generate sub-samples with informative terms.

Masked Sampling:. In contrast to a bag-of-words based sam-

pling approach, an alternative way is to extract segments of text

from a document, somewhat analogous to selecting regions from

an image [9]. More specifically, in this sampling method we first

specify a segment size, say k, and then segment a document D (com-

prised of

|D|

tokens) into

|D | k

number of chunks. A

chunk is then

made visible in the sub-sample with probability v (a parameter).

4 EXPLANATION EVALUATION METRIC
We now consider ways of automatically evaluating the quality of an explanations generated using different sampling methods. Since it is costly and laborious to manually label the quality of explanations for each query-document pair, we propose two metrics that exploit relevance judgments to measure explanation quality at scale. We focus on 2C's ­ consistency and correctness for evaluating explanations described in following sections.

4.1 Explanation Consistency
An explanation vector ^ Q,D can be used to determine which terms are important for explaining the score of a document D with respect to a query Q, i.e. S(D, Q). The first desirable quality of an explanation method is that the relative ranking of important terms should

1282

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

not change significantly with variations in the parameters of the model, or in other words, a particular choice of samples around the pivot document, D, should not result in considerable differences in the predicted explanation vector.
Variances in term rankings (explanation terms sorted in decreasing order by their weights) can be measured with weighted interrank correlations of a particular query-document pair averaged over a number of samples. More specifically, these correlations are computed between the ordered lists of the explanation vector and the ground-truth terms, R(Q) derived from documents judged relevant for the query Q. In particular, the term sampling probabilities from this ground-truth set, R(Q), are used to derive the reference ordered list for computing rank correlations. As sampling probabilities, we used LM-JM, i.e., language modeling with collection smoothing (Jelinek-Mercer) weights to induce a reference order on R(Q), with an objective to sample frequently occurring informative terms from R(Q). Formally speaking,

R(Q)(w) = 

f (w,
v R(Q )

R (Q )) f (v, R(Q))

+

(1

-

) cf(w) , cs

(3)

where R(Q) denotes the set of relevant terms extracted from R(Q), f and cf respectively denote term and collection frequencies, and
cs denotes collection size. We then assume that an ideal explanation
system should seek to predict the same ranking of terms as induced
by the decreasing order of term weights. Formally speaking, if  (^ Q,D ) denotes a sequence of terms sorted by the decreasing values of the components in ^ Q,D (predicted from Equation 1), then we measure the average rank correlation coefficient with
respect to the ground-truth ranking of terms as

 (Q, D)

=

1 ||

 ( (^ Q,D ), R(Q))

^ Q, D 

(4)

ECON = 1

 (Q, D),

|Q|

Q  Q D TOP(Q )

where  represents the set of different explanation vectors obtained with different samples, e.g. variations in the L1-regularization and kernel widths of LIRME (h in Equation 2), TOP(Q) denotes the set of top-retrieved documents for a query Q and  denotes the Kendall's rank correlation coefficient between the predicted and the groundtruth ordering of relevant terms. Note that the individual correlation scores  (Q, D) for a query-document pair are averaged over a set of benchmark queries Q present in collection judgments such as TREC datasets.

4.2 Explanation Correctness
Intuitively, an explanation may be considered to be effective if it attributes higher weights to the components of ^ Q,D that correspond to relevant terms, i.e. the terms occurring in documents that are
judged relevant by assessors. We measure explanation correctness by computing similarity between explanation vector terms ^ Q,D and relevant terms R(Q). In particular, for a query-document pair (Q, D) we compute correctness as

ECOR

=

1 |Q|

Q  Q D TOP(Q )

^ Q,D · R(Q) |^ Q,D ||R(Q)|

(5)

(a) Consistency (ECON)

(b) Correctness (ECOR)

Figure 1: ECON and ECOR for top-5 retrieved documents.

(a) Relevant documents

(b) Non-relevant documents

Figure 2: ECON for relevant and non-relevant documents.

(a) Relevant documents

(b) Non-relevant documents

Figure 3: ECOR for relevant and non-relevant documents. where R(Q) represents the distribution of terms in the judged relevant documents. Similar to consistency ECON. we aggregate the relevance similarity values over a set of queries and number of top documents retrieved for each query.

5 EXPERIMENTS
The objectives of our experiments are to investigate - a) what term sampling approaches are effective in terms of the metrics consistency and ECOR (Section 4) to explain the scores assigned to querydocument pairs by a retrieval model; and b) what is the optimal size of the explanation vector, i.e., the number of explanation terms (p) for yielding consistent and relevant explanations.
For our experiments, we use a standard benchmark dataset, namely the TREC-8, comprising 50 topics. To generate different sub-samples for explanation, we also employ uniform kernel in addition to Gaussian kernel, i.e. apply (D,  (D)) = 1. We generate explanations for top 5 documents for each TREC-8 query Q, i.e. TOP(Q) = 5 that are retrieved with LM-JM. The LM-JM score values constitute the S(D, Q) values in Equation 1. For all our experiments, we set M (i.e. the number of document sub-samples) to 200.
In Figure 1a, we report expected consistency (ECON) values with a number of different LIRME settings. An interesting observation is that bag-of-words sampling (both uniform and tf-idf biased) yield

1283

Short Research Papers 3C: Search

SIGIR '19, July 21­25, 2019, Paris, France

(a) Uniform sampling

(b) Tf-idf sampling

(c) Masked samples (v = 0.1, k = 5)

Figure 4: Visualization of explanation vectors ^ (Q, D) estimated for a sample (relevant) document `LA071389-0111' (D) and

query (Q) `counterfeiting money' (TREC-8 id 425). The Y-axis shows explanation terms, while the X-axis plots their weights.

higher consistency than the masking-based sampling. This indicates that on an average the relative ranks of the term weights in the explanation vector ^ are more stable for these two sampling methods in comparison to the masking based sampler. Our experiments indicate that samples generated from a masking-based sampler exhibit less diversity because of a smaller degree of freedom in choosing individual terms independent of its context. Moreover, in the case of bag-of-words, uniform sampling yields higher consistency (ECON values). This is because the set of samples, Di tend to become similar to each other if the sampling strategy is biased towards informative terms. This again leads to a smaller variances in the scores of the sub-samples, which prevents the explanation loss function to effectively learn the term weights contributing to large increments or decrements in the similarity scoring function, S(D, Q). Similar observations can be made if the averaging is split across the two partitions of relevant and non-relevant documents for each query in the dataset.
Biased sampling with tf-idf weights results in higher ECOR (as seen from Figure 1b). This shows that with tf-idf biased sampling, the explanation vector is better able to predict the terms frequently occurring in the relevant documents. The masking sampler produces worse results in terms of ECOR in comparison to the bagof-words sampling approaches. While uniform sampling results in higher consistency, tf-idf biased sampling shows a higher recall with respect to the set of relevant terms. ECOR values tend to increase with the size of the explanation vector (p), which indicates that explanations are more effective with a higher number of parameters. However, consistency values ECON tend to decrease with increase in the model parameters because of the increase in likelihood of a change in the relative ordering of the terms by predicted weight values. When split across the set of relevant and non-relevant documents separately, it can be seen from Figure 3a that for relevant documents, ECOR values increase with more explanation terms relevant, whereas for the non-relevant ones these values tend to decrease. This behaviour shows that LIRME is able to predict high weights for true relevant terms.
For each sampling strategy investigated, Figure 4 plots the terms with their associated weights from explanation vectors, ^ , as histograms for a document judged relevant for the query `counterfeiting money' (TREC-8 query id 425). All explanation vectors, independent of the sampling strategy, contribute positive weights to the terms constituting the query (e.g. see the weights of the word `counterfeit'). However, for non-query terms, the explanation weights vary across sampling methods which indicates that the choice of sampling method can considerably impact the quality of

the local explanations generated by any LIRME. Another observation is that sampling approaches were mostly able to find terms (output as negative weights) that are seemingly not relevant to the information need of the example query, such as the terms `phoenix', `agent' etc. From a user's point-of-view, the positive weights of a set of terms in a document are likely to help him discover the associated relevant sub-topics within it, whereas the negative weights on the other hand could indicate potential non-relevant aspects.
6 CONCLUSION
While research in explaining outputs of classification models exists, there is little work on explaining results of a ranking model. In this work, we addressed the research question: Why does an IR model assign a certain score (affecting its rank) to a document D for a query Q? We investigated the effectiveness of different sampling methods in generating local explanations for a document scored with respect to a query by an IR model. We also proposed two evaluation metrics to measure the consistency and correctness of explanations generated using different sampling methods. Our experiments indicate that sampling methods that use term position information produce weaker explanations than those generated by uniform or tf-idf based sampling of words from documents.
REFERENCES
[1] E. Alepis, E. Politou, and C. Patsakis. Forgetting personal data and revoking consent under the GDPR: Challenges and proposed solutions. Journal of Cybersecurity, 4(1), 03 2018.
[2] T. Bolukbasi, K. Chang, J. Y. Zou, V. Saligrama, and A. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. CoRR, abs/1607.06520, 2016.
[3] J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan. Learning to explain: An information-theoretic perspective on model interpretation. arXiv preprint arXiv:1802.07814, 2018.
[4] L. Dixon, J. Li, J. Sorensen, N. Thain, and L. Vasserman. Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67­73. ACM, 2018.
[5] Z. C. Lipton. The mythos of model interpretability. arXiv:1606.03490, 2016. [6] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions.
In Advances in Neural Information Processing Systems, pages 4765­4774, 2017. [7] D. Metzler and W. Bruce Croft. Linear feature-based models for information
retrieval. Inf. Retr., 10(3):257­274, June 2007. [8] G. Montavon, W. Samek, and K. Müller. Methods for interpreting and under-
standing deep neural networks. CoRR, abs/1706.07979, 2017. [9] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. In Proc. of KDD'16, pages 1135­1144, 2016. [10] M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic
explanations. In AAAI Conference on Artificial Intelligence, 2018. [11] J. Singh and A. Anand. Interpreting search result rankings through intent model-
ing. arXiv preprint arXiv:1809.05190, 2018. [12] J. Singh and A. Anand. Posthoc interpretability of learning to rank models using
secondary training data. arXiv preprint arXiv:1806.11330, 2018. [13] J. Singh and A. Anand. EXS: Explainable Search Using Local Model Agnostic
Interpretability. In Proc. of WSDM '19, pages 770­773, 2019.

1284

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

Parrot: A Python-based Interactive Platform for Information Retrieval Research

Xinhui Tu
School of Computer Science, Central China Normal University Wuhan, China tuxinhui@mail.ccnu.edu.cn

Jimmy Huang
School of Information Technology, York University Toronto, Canada jhuang@yorku.ca

Jing Luo
College of Computer Science and Technology, Wuhan University of
Science and Technology, China luojing@wust.edu.cn

Runjie Zhu
School of Information Technology, York University Toronto, Canada
sherryzh@cse.yorku.ca

Tingting He
School of Computer Science, Central China Normal University Wuhan, China tthe@mail.ccnu.edu.cn

ABSTRACT
Open source softwares play an important role in information retrieval research. Most of the existing open source information retrieval systems are implemented in Java or C++ programming language. In this paper, we propose Parrot1, a Python-based interactive platform for information retrieval research. The proposed platform has mainly three advantages in comparison with the existing retrieval systems: (1) It is integrated with Jupyter Notebook, an interactive programming platform which has proved to be effective for data scientists to tackle big data and AI problems. As a result, users can interactively visualize and diagnose a retrieval model; (2) As an application written in Python, it can be easily used in combination with the popular deep learning frameworks such as Tersorflow and Pytorch; (3) It is designed especially for researchers. Less code is needed to create a new retrieval model or to modify an existing one. Our efforts have focused on three functionalists: good usability, interactive programming, and good interoperability with the popular deep learning frameworks. To confirm the performance of the proposed system, we conduct comparative experiments on a number of standard test collections. The experimental results show that the proposed system is both efficient and effective, providing a practical framework for researchers in information retrieval.
CCS CONCEPTS
· Information systems  Information retrieval.
KEYWORDS
information retrieval, deep learning, Python
1https://github.com/IR-Community/Parrot or http://www.yorku.ca/jhuang/parrot
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331393

ACM Reference Format: Xinhui Tu, Jimmy Huang, Jing Luo, Runjie Zhu, and Tingting He. 2019. Parrot: A Python-based Interactive Platform for Information Retrieval Research. In 42nd Int'l ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331393
1 INTRODUCTION AND MOTIVATION
Over the past several decades, many open source softwares have been built to facilitate the research of information retrieval, such as Okapi [7][8][15], Indri2, Galago3, Terrier4, and Anserini [18]. These softwares greatly improve the efficiency of the evaluation of retrieval models on standard test collections. Most of the existing open source information retrieval systems are implemented in Java or C++ programming language. With the advance of information technology, researchers expect that the IR softwares can keep up with the new requirements, such as good usability, interactive programming, and good interoperability with the popular deep learning frameworks.
Most of the existing retrieval systems such as Terrier and Lucene5 are over-abstraction. The codes of a retrieval model may be scattered across a number of classes. Users have to understand the complex relationship between the classes before they implement a new retrieval model or modify an existing one.
The interactive programming platforms such as Jupyter Notebook6 have proved to be effective for data scientists to tackle big data and AI problems. Project Jupyter supports interactive data science and scientific computing via the development of open-source software. The scientific publication Nature has released an article on the advantages of Jupyter Notebooks for scientific research [16]. Compared with the dynamic programming languages like Python, static programming languages such as Java and C++ are not well supported by Jupyter Notebook.
Over the past decade, deep learning has achieved remarkable success in a wide range of domains, such as speech recognition, computer vision, and natural language processing [10]. Over the
2 https://www.lemurproject.org/indri/ 3 https://www.lemurproject.org/galago.php 4 http://terrier.org 5 http://lucene.apache.org/ 6 https://jupyter.org/

1289

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

Listing 1: The example of how to implement BM25 model
from parrot import *
ap90 = datasets.ap90.load("/trec/ap90")
class BM25Model(Model): def __init__(self , k1 = 1.2, b = 0.75): self.k1 = k1; self.b = b
def pre_compute(self , df, ctf , ctn , C, N): self.avgdl = C / N self.idf = log(1 + (N - df + 0.5) / (df + 0.5))
def score_term(self , tf, ctn , dl): b = self.b; k1 = self.k1 tf_part = tf * (k1 + 1) / (tf + k1 * (1 - b + b * dl / self.avgdl)) return tf_part * self.idf
result_list = [] for k in frange(0, 1, 0.1):
model = BM25Model(k1 = 1.2, b = k) result = model.run(dataset = ap90) result_list.append(result)
past few years, many researchers tried to explore deep learning based approaches to information retrieval [12]. It has been proved that neural ranking models are very effective when enough training data is provided [13]. Deep learning frameworks such as Pytorch7 and Tensorflow8 provide an effective way to implement a neural ranking model. Because most of the popular deep learning frameworks are written in the Python programming language, it is not easy to integrate them with the existing Java or C++ based retrieval systems. As a popular Python-based project, MatchZoo [3] has implemented a lot of deep text matching models. However, it don't provide necessary functions to conduct all experimental steps of ad-hoc retrieval.
Motivated by the above considerations, we propose Parrot, a Python-based interactive platform for information retrieval research. We aim not only to provide an interactive computing platform for researches to diagnose and visualize retrieval models interactively but also to bridge the gap between the popular deep learning frameworks and the retrieval system. Parrot is built on top of PyLucene9, which is a Python extension for accessing Java Lucene. Our system provides a lot of intuitive APIs which allow researchers to write less code to create a new model or to modify an existing one. Our efforts have focused on three functionalists: good usability, interactive programming, and good interoperability with the popular deep learning frameworks. Furthermore, Parrot ships with a lot of example codes for standard TREC retrieval tasks, providing a convenient way for researchers to run the baselines.
The existing retrieval models can be classified into three categories: hand-crafted models, learning to rank models, and neural ranking models. Hand-crafted models, such as TF-IDF [17], BM25
7 https://pytorch.org/ 8 https://www.tensorflow.org/ 9 http://lucene.apache.org/pylucene/

Listing 2: The example of how to use DRMM model
from parrot import *
mq2007 = datasets.letor_mq2007.load("/trec/mq2007")
drmm = DRMMModel() drmm . build () drmm.cross_validation(dataset = mq2007 , epochs = 30)
[1][19], and LM [9][14], are very simple and easy to understand. These models integrate three major variables to determine the relevance of a query-document pair: within-document term frequency, document length, and the specificity of the term in the collection [4]. Learning to rank models employ machine learning techniques over hand-crafted IR features [11]. In contrast to classical learning to rank models, more recently proposed neural ranking models directly learn text representations or matching functions [2][5][13]. Both learning to ranking models and neural ranking models require large-scale training data before they can be deployed. Parrot provides a robust framework to run the different types of retrieval models.
To confirm the efficiency of the proposed system, we conducted comparative experiments on a number of standard test collections. The experimental results show that Parrot has better searching and indexing performance than Indri10, a popular open source retrieval tool written in C++ programming language. Parrot is a good choice for researchers who prefer to program in Python. It provides a practical framework for researchers in information retrieval.
2 MAIN COMPONENTS
Parrot includes three major components: data preparation, retrieval model, and evaluation and diagnosis. The purpose of the data preparation component is to build the index and convert different types of data files into standard form files. Retrieval model component aims to provide a lot of classic retrieval models, including most classic hand-crafted models and a number of neural ranking models. The component of evaluation and diagnosis provides a number of functions to evaluate and diagnose retrieval models.
2.1 Data preparation
Different types of data are required to run different types of retrieval models. To run a hand-crafted retrieval model, we need the data as follows.
· Inverted Index is a data structure storing a mapping from content, such as words or numbers, to its locations in a document. The indexing module first converts different types of the document into the standard form document, and then builds an inverted index.
· Topic and Judgment files are used to evaluate the performance of a retrieval model. The topic and judgment parsers extract information from the original TREC files and convert them into standard form files.
10 https://www.lemurproject.org/indri/

1290

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

Listing 3: The example code of how to use the evaluation function to generate a MAP figure plot_single_trend(
x_list = frange(0, 1, 0.1), y_list = result_list , x_label="b", y_label="MAP")
Figure 1: The precision change of the BM25 model with different parameter b
Different from the hand-crafted models, neural ranking models need a lot of training data which may be prepared manually by human assessors. The following data are needed to run a neural ranking model.
· Word Dictionary lists the words which are used as the input of neural ranking models. We use predefined rules to filter out words that are too frequent, too rare or noisy.
· Corpus File records the original text of the collection, which is used to learn representations of language.
· Training and test data are used to train and evaluate a ranking model. Typically, we separate a data set into the training set and the testing set. We use most of the data for training and a smaller portion of the data for testing.
Parrot provides a convenient way for researchers to prepare the datasets for the standard test collections. It ships with a lot of example codes for standard TREC retrieval tasks, including AP90, DISK12, DISK45, WT2G, WT10G, and GOV2. List 1 and List 2 show the example codes for data preparation.
2.2 Retrieval model
Parrot is designed especially for researchers in information retrieval. It provides intuitive APIs for them to access the information which is required to build a retrieval model.
Most of the hand-crafted ranking models exploit three parts, such as term frequency, inverse document frequency, and document length normalization, to score a query-document pair [4]. The significant difference between these models is how they combine the three factors. In Parrot, we provide a base class to rank a set of documents for a query. As a result, researchers only need to focus on how to score a term when they implement a new retrieval model. List 1 shows an example of how to implement the BM25 model.

Listing 4: The example of how to conduct a case diagnosis
explain_case( result = result_list[0], doc = 0)
In recent years, neural ranking models have become a significant focus in information retrieval. The experimental results indicate that they achieve better performance in comparison with the existing methods [6]. As a software written in Python, Parrot has excellent interoperability with the popular deep learning frameworks such as Tersorflow and Pytorch, which will facilitate the development of neural ranking models. Because MatchZoo has implemented a lot of deep text matching models include a number of popular neural ranking models, we don't need to reinvent the wheel. We provides a unified data preparation module for different neural ranking models. List 2 shows an example of how to use the DRMM [13] model for ranking.
2.3 Evaluation and diagnosis
The component of evaluation and diagnosis provides a lot of functions for researchers to evaluate and diagnose a retrieval model, which is very important for a retrieval system designed for researchers. Researchers can use these functions in an interactive way under Jupyter notebook.
This component includes two modules: the evaluation module and the diagnosis module. Evaluation module provides several widely used metric such as MAP and NDCG to evaluate the performance of a retrieval model. It can output the TREC-compatible ranking results. It also provides functions for users to interactively visualize the performance of a retrieval model. Parameter sensitivity analysis is a commonly used method to find the optimal parameter value for a retrieval model. List 3 shows the example code of how to generate a MAP figure, which is depicted in Figure 1. The figure shows the precision change of the BM25 model with different parameter values.
Sometimes, a ranking model may give a low score to a relevant document. Researchers need to study the scoring details to figure out why the ranking model fails. Diagnosis module provides case diagnose functions for users to investigate the bad cases of a ranking result. List 4 shows an example of how to conduct a case diagnosis.
3 PERFORMANCE EVALUATION
To confirm the performance of our system, we conducted comparative experiments on several standard test collections. We conducted the experiments on a workstation with Xeon processors running Ubuntu.
Invert indexing is an essential component of a retrieval system. In dealing with large document collections, operational efficiency is necessary for a retrieval system. Parrot is built based on PyLucene, a Python extension for accessing Java Lucene. In this paper, we conducted indexing experiments on the five standard test collections. Table 1 shows the indexing performances of Parrot and Indri. The experimental results show that Parrot is faster than Indri in term of indexing time.

1291

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Indexing performance of Parrot and Indri

Collection Disk12 Disk45 WT2G WT10G Gov2

docs 742k 528k 246k 1.69m 25.2m

terms 129m 175m 182m 752m 17.3b

Indri time size 00:12:03 2.5GB 00:06:42 1.9GB 00:06:58 2.2GB 00:39:51 9.6GB 13:26:07 215GB

Parrot(pos) time size 00:01:34 519MB 00:01:28 432MB 00:02:41 442MB 00:04:53 3.0GB 02:28:36 11GB

Parrot(doc) time size 00:03:02 2.5GB 00:02:39 2.1GB 00:04:07 2.4GB 00:09:16 12GB 06:08:29 337GB

Table 2: Retrieval efficiency on Gov2, using a single process

Latency (ms) throughput (qps)

Indri

2003

0.52

Parrot

341

2.96

Table 3: : Effectiveness comparisons between Parrot and Indri on standard TREC test collections

Collection Queries
BM25 (I) BM25 (P) LM (I) LM (P)

Disk12 51-200
0.2040 0.2296 0.2269 0.2281

Disk45 301-450 601-700
0.2478 0.2514 0.2516 0.2502

WT2G 401-450
0.3152 0.3145 0.3116 0.2985

WT10G 451-550
0.1955 0.2009 0.1915 0.2104

Gov2 701-850
0.2970 0.3051 0.2995 0.3011

To verify the search efficiency of Parrot, we also conducted search experiments on the Gov2 collection. We issued 100,000 queries sequentially against both the Parrot and Indri indexes. Table 2 lists the experimental results, including latency (ms) and throughput (queries per second, or qps). The experimental results demonstrate that Parrot is more efficient than Indri in terms of search performance.
Finally, we compared the retrieval effectiveness of Parrot and Indri. We conducted the experiments on five standard TREC collections. Table 3 shows the experimental results, where (I) refers to Indri and (P) refers to Parrot. The results indicate that Parrot and Indri achieve comparable performance in terms of mean precision.
4 CONCLUSION AND FUTURE WORK
In this paper, we have presented a Python-based toolkit to perform IR experimentation within Jupyter notebook. As mentioned above, we believe this will help us in (1) providing an interactive experimental framework for IR; (2) facilitating the development of neural ranking models, and (3) improving the efficiency of the research in information retrieval. In the future, we will implement classical learning-to-rank models.
ACKNOWLEDGMENTS
This work is substantially supported by the National Natural Science Foundation of China under the grant number 61572223. This work is also supported by the Natural Sciences and Engineering

Research Council (NSERC) of Canada, an NSERC CREATE award
in ADERSIM, the York Re-search Chairs program, and an Ontario
Research Fund-Research Excellence award in BRAIN Alliance. We
thank anonymous reviewers for their thorough review comments.
REFERENCES
[1] M Beaulieu, M Gatford, Xiangji Huang, S Robertson, S Walker, and P Williams. 1997. Okapi at TREC-5. NIST SPECIAL PUBLICATION SP (1997), 143­166.
[2] Yixing Fan, Jiafeng Guo, Yanyan Lan, Jun Xu, Chengxiang Zhai, and Xueqi Cheng. 2018. Modeling Diverse Relevance Patterns in Ad-hoc Retrieval. arXiv preprint arXiv:1805.05737 (2018).
[3] Yixing Fan, Liang Pang, JianPeng Hou, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2017. Matchzoo: A toolkit for deep text matching. arXiv preprint arXiv:1707.07270 (2017).
[4] Hui Fang, Tao Tao, and Chengxiang Zhai. 2011. Diagnostic evaluation of information retrieval models. ACM Transactions on Infor. Sys. 29, 2 (2011), 7.
[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 55­64.
[6] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and Xueqi Cheng. 2019. A Deep Look into Neural Ranking Models for Information Retrieval. arXiv preprint arXiv:1903.06902 (2019).
[7] Xiangji Huang and Qinmin Hu. 2009. A bayesian learning approach to promoting diversity in ranking for biomedical information retrieval. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. ACM, 307­314.
[8] Xiangji Huang, Fuchun Peng, Dale Schuurmans, Nick Cercone, and Stephen E Robertson. 2003. Applying machine learning to text segmentation for information retrieval. Information Retrieval 6, 3-4 (2003), 333­362.
[9] John Lafferty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 111­119.
[10] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature 521, 7553 (2015), 436.
[11] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331.
[12] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv preprint arXiv:1705.01509 (2017).
[13] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng. 2017. Deeprank: A new deep architecture for relevance ranking in information retrieval. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM, 257­266.
[14] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 275­281.
[15] Stephen E Robertson. 1997. Overview of the okapi projects. Journal of documentation 53, 1 (1997), 3­7.
[16] Helen Shen. 2014. Interactive notebooks: Sharing the code. Nature News 515, 7525 (2014), 151.
[17] Amit Singhal, Gerard Salton, Mandar Mitra, and Chris Buckley. 1996. Document length normalization. Infor. Processing & Management 32, 5 (1996), 619­633.
[18] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1253­1256.
[19] Jiashu Zhao, Jimmy Xiangji Huang, and Ben He. 2011. CRTER: using cross terms to enhance probabilistic information retrieval. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 155­164.

1292

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

Information Retrieval Meets Scalable Text Analytics: Solr Integration with Spark

Ryan Clancy, Jaejun Lee, Zeynep Akkalyoncu Yilmaz, and Jimmy Lin
David R. Cheriton School of Computer Science University of Waterloo

ABSTRACT
Despite the broad adoption of both Apache Spark and Apache Solr, there is little integration between these two platforms to support scalable, end-to-end text analytics. We believe this is a missed opportunity, as there is substantial synergy in building analytical pipelines where the results of potentially complex faceted queries feed downstream text processing components. This demonstration explores exactly such an integration: we evaluate performance under different analytical scenarios and present three simple case studies that illustrate the range of possible analyses enabled by seamlessly connecting Spark to Solr.
ACM Reference Format: Ryan Clancy, Jaejun Lee, Zeynep Akkalyoncu Yilmaz, and Jimmy Lin. 2019. Information Retrieval Meets Scalable Text Analytics: Solr Integration with Spark. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331395
1 INTRODUCTION
In the realm of data science, Apache Spark has emerged as the de facto platform for analytical processing, with broad adoption in both industry and academia. While not originally designed for scalable text analytics, it can nevertheless be applied to process large document collections in a scalable, distributed fashion. However, using Spark for text processing is hampered by the lack of integration with full-text indexes, particularly useful in applications where the data scientist wishes to analyze only a subset of the collection. By default, the only approach for selecting a collection subset is a brute-force scan over every document with a filter transformation to retain only the desired documents. For selective queries that only match a small number of documents, this is obviously inefficient.
In the realm of search, Apache Solr has emerged as the de facto platform for building production applications. Other than a handful of commercial web search engines that deploy custom infrastructure to achieve the necessary scale, most organizations today take advantage of Solr, including Best Buy, Bloomberg, Comcast, Disney, eHarmony, Netflix, Reddit, and Wikipedia. Although Solr is designed to be scalable via a distributed, partitioned architecture, the platform is primarily engineered around providing low-latency
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331395

user-facing search. As such, it does not provide any analytics capabilities per se.
The current state of the broader ecosystem sees little overlap between Spark for general-purpose analytical processing on the one hand and Solr for production search applications on the other. This is a missed opportunity in creating tremendous synergies for text analytics, which combines elements of search as well as analytical processing. As a simple example, the output of a (potentially complex, faceted) query can serve as the input to an analytical pipeline for machine learning. Spark, by default, cannot take advantage of index structures to support efficient end-to-end execution of such pipelines. Lin et al. [9] previously described using Lucene indexes to support predicate pushdown optimizations in the Pig scripting language, but the approach never gained widespread adoption and Pig has generally given way to more modern analytics platforms such as Spark. Terrier-Spark [10] represents a recent effort to integrate Spark with an IR engine, but the project has a slightly different focus on building experimental IR pipelines as opposed to generalpurpose text analytics. Nevertheless, we draw inspiration from Terrier-Spark, particularly its integration with Jupyter notebooks to support interactive explorations.
The contribution of this demonstration is an exploration of how Solr and Spark can be integrated to support scalable text analytics. We investigate the performance characteristics of using Solr as a document store, taking advantage of its powerful search functionality as a pushdown predicate to select documents for downstream processing. This is compared against an alternative where the raw documents are stored in the Hadoop Distributed File System (HDFS), requiring brute-force scans over the entire collection to select subsets of interest. As expected, our results confirm that significant performance improvements are realized when Spark leverages Solr's querying capabilities, especially for queries that match few documents. With the addition of simulated per-document processing workloads, Solr is unequivocally faster--even over large fractions of the entire collection--since processing costs mask I/O costs.
With Solr/Spark integration, we present three case studies that illustrate the range of interesting analyses enabled by end-to-end text processing pipelines. These examples include kernel density estimation to study the temporal distribution of tweets, namedentity recognition to visualize document content, and link analysis to explore hyperlink neighborhoods.
2 SOLR/SPARK INTEGRATION
To integrate Solr with Spark, we adopt the most obvious architecture where Solr serves as the document store. We assume that a document collection has already been ingested [2].
Solr is best described as a REST-centric platform, whereas Spark programs define sequences of data-parallel transformations (e.g.,

1313

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

filter, map, etc.) over a data abstraction called Resilient Distributed Datasets (RDDs). The crux of the integration effort thus involves bridging Solr output (i.e., the result of search queries) with RDDs. To this end, we take advantage of an open-source library called spark-solr,1 which constructs RDDs from Solr search results (not surprisingly, called SolrRDDs). This library leverages appropriate Solr indexes and exploits data locality by fetching documents from the Solr shard that is co-located with the same node as the Spark worker requesting the documents.
As part of initial explorations, we compared the performance of SolrRDD to a much simpler approach of mapping over an RDD of docids (that is, strings) and using the direct Solr APIs to fetch the documents to create the initial RDD for Spark processing. This seemed like a reasonable experiment since there is evidence that with modern networks and the trend toward storage disaggregation [5], data locality is no longer a serious concern [1]. Nevertheless, we found that SolrRDD was substantially faster than our "parallel document fetch" implementation, and thus we focused our efforts building on SolrRDD.
From a performance perspective, the baseline for comparing Solr/Spark integration is Spark processing over documents stored on HDFS. In this setup, all document manipulations require scanning the entire collection. Previous studies have found that such brute-force operations are not as inefficient as one might think at first glance; for example, researchers have explored document ranking using this architecture [4, 6]. Such designs lead to simple implementations that enjoy excellent data locality and can take advantage of high disk throughput.
Intuitively, we can characterize the performance tradeoffs between the two alternatives as follows: Suppose a data scientist wishes to process the entire collection. Fetching documents from Solr will likely be slower than simply scanning the entire collection on HDFS sequentially, since Solr index structures come with associated overheads. At the other end of the spectrum, suppose a data scientist is only interested in analyzing a single document. In this case, Solr would be obviously much faster than scanning the entire collection. Thus, the selectivity of the document subset, in addition to other considerations such as hardware configurations and the processing workload, will determine the relative performance of the two approaches. The balance of these various factors, however, is an empirical question.
3 PERFORMANCE EVALUATION
We set out to empirically characterize the performance tradeoffs between the designs discussed in the previous section, principally examining two characteristics: selectivity and workload.
3.1 Experimental Setup
Our experiments were conducted on a cluster with ten nodes. Each node has 2× Intel E5-2670 @ 2.60GHz (8 cores, 16 threads) CPUs, 256GB RAM, 6×600GB 10k RPM HDDs, 10GbE networking, running Ubuntu 14.04 with Java 1.8. One node is responsible for running the master services (YARN ResourceManager, HDFS NameNode) while the remaining nine nodes each hosts an HDFS DataNode, a Solr shard, and are available for Spark executor allocation (via YARN).
1 https://github.com/lucidworks/spark-solr

Note that our processors are of the Sandy Bridge architecture, which was introduced in 2012 and discontinued in 2015, and thus we can characterize these computing resources as both "modest" and "dated". Similar amounts of compute power could be found on a single high-end server today.
We examined the following document collections:
· The New York Times Annotated Corpus, a collection of 1.8 million news article, used in the TREC 2017 Common Core Track.
· Tweets2013, a collection of 243 million tweets gathered over February and March of 2013, used in the TREC Microblog Tracks [8].
· ClueWeb09b, a web crawl comprising 50.2 million pages gathered by CMU in 2009, used in several TREC Web Tracks.
All collections were ingested into Solr using Anserini [2, 13, 14]. For comparison, all collections were also loaded into HDFS. In both cases, the same document processing (e.g., tokenization, stopword removal, etc.) was applied using Lucene Analyzers.
Our performance evaluation focused on two characteristics of large-scale text analytics: the number of documents to process (selectivity) and the per-document processing time (workload). In order to understand the first factor, we randomly selected terms according to document frequency, ranging from 10% to 60%. These terms were then issued as queries whose results were fed into Spark. For "processing", we simulated three different workloads by simply sleeping for 0ms, 3ms, or 30ms (per document).
While running experiments, we used the master node as the driver while running Spark jobs in client mode. Each job used 9 executors with 16 cores and was allocated 48GB of RAM per executor. This allowed us to take full advantage of the available cluster resources and exploit data locality as Spark workers were co-located on the same nodes as HDFS DataNodes and Solr shards.
3.2 Experimental Results
Figure 1 summarizes the results of our experiments on ClueWeb09b, varying input selectivity and processing workload. We report averages over five runs (where each run is based on a different randomlyselected term at the specified selectivity) and include 95% confidence intervals. Results on the other collections yield similar findings and hence are omitted.
The left bar graph in Figure 1 simulates no per-document processing and captures the raw I/O capacity of both architectures. As expected, total execution time does not vary much with selectivity when brute-force scanning the collection on HDFS, since the entire document collection needs to be read regardless. Also as expected, the performance of using Solr as a pushdown predicate to select subsets of the collection depends on the size of the results set. For small subsets, Solr is more efficient since it exploits index structures to avoid needless scans of the collection. Execution time for Solr grows as more and more documents are requested, and beyond a certain point, Solr is actually slower than a scan over the entire collection due to the overhead of traversing index structures. This crossover point occurs at around half the collection--that is, if an analytical query yields more results, a brute-force scan over the entire collection will be faster.
The above results assume that no time is spent processing each document, which is obviously unrealistic. In the middle and right

1314

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

Figure 1: Average total execution time (over five trials) on ClueWeb09b with simulated per-document workloads of 0ms (left), 3ms (middle), and 30ms (right). Error bars denote 95% confidence intervals.

bar graphs in Figure 1, we simulate per-document processing latencies of 3ms and 30ms. The takeaway from these results is that Solr is always faster than a brute-force scan over the entire collection on HDFS. As the per-document workload increases, processing time occupies a growing fraction of the overall execution time and masks latencies associated with fetching a large number of documents from Solr. Thus, from these experiments we can conclude that, except in the most extreme case where text analytics is dominated by I/O, predicate pushdown via Solr is beneficial.

4 CASE STUDIES
We present three case studies that illustrate the range of analyses enabled by our Solr/Spark integration, taking advantage of existing open-source tools. While these analyses are certainly possible without our platform, they would require more steps: issuing queries to Solr, extracting the result documents from the collection, and importing them into downstream processing tools. In practice, this would likely be accomplished using one-off scripts with limited generality and reusability. In contrast, we demonstrate end-to-end text analytics with seamless integration of Spark and Solr, with a Jupyter notebook frontend.
4.1 Temporal Analysis
Kernel density estimation (KDE), which has been applied to extract temporal signals for ranking tweets [3], can be used to explore the distribution of tweets over time. In this case study, we investigated the creation time of tweets that contain certain keywords from the Tweets2013 collection (243 million tweets) [8].
The top graph in Figure 2 shows results for four keywords, aggregated by hour of day (normalized to the local time zone): "coffee", "breakfast", "lunch", and "dinner". The bottom graph shows results for the following keywords, aggregated by day of week: "school", "party", and "church". In all cases, we began with a query to Solr, aggregated the creation time of the retrieved tweets, and then used Spark's MLlib2 to compute the KDE.
The results show diurnal and weekly cycles of activity. Peaks for the three daily meals occur where we'd expect, although Twitter users appear to eat breakfast (or at least tweet about it) relatively late. Coffee is mentioned consistently throughout the waking hours
2 https://spark.apache.org/mllib/

Figure 2: Results of Kernel Density Estimation on creation time of tweets to capture diurnal and weekly activity cycles.
of the day. In terms of weekly cycles, unsurprisingly, "church" peaks on Sunday, "party" peaks on Saturday, and mentions of school drop off on weekends. The core of this analysis is around 15 lines of code, highlighting the expressivity of Solr/Spark integration.
4.2 Entity Analysis
We can take advantage of named-entity recognition (NER) to provide a broad overview of a corpus, corresponding to what digital humanities scholars call "distant reading" [12]. For example, what musical artists are prominently discussed in the New York Times? The answer is shown in Figure 3 as a word cloud, which was created by taking all documents that have the term "music" (154k documents), feeding the text into Stanford CoreNLP [11] to extract named entities, and then piping the results to an off-the-shelf tool.3 We performed minor post-processing to remove entities that are single words, so common names such as "John" do not dominate.
3 https://github.com/amueller/word_cloud

1315

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

Figure 3: Word cloud for "music" from the New York Times
The people mentioned are, perhaps unsurprisingly, famous musicians such as Bob Dylan, Frank Sinatra, and Michael Jackson, but the results do reveal the musical tastes of New York Times writers. All of this can be accomplished in around 20 lines of code.
4.3 Webgraph Analysis
Network visualizations facilitate qualitative assessment by revealing relationships between entities. In this case study, we extracted links referenced by websites in the ClueWeb09b collection that contain the polysemous term "jaguar" (among many referents, a car manufacturer and an animal), which could reveal interesting clusters corresponding to different meanings. In order to produce a sensible visualization, we began by randomly sampling 1% of the documents that contain the term. Next, we extracted all outgoing links from these sources with the Jsoup HTML parser before aggregating by domain. Finally, we selected the top three most frequently-occurring links from each source node to reduce clutter. All of this can be accomplished in around 30 lines of code.
By feeding the edge list to Gephi,4 we ended up with the network visualization in Figure 4 using a Multilevel Layout [7]. For better clarity in the visualization, we pruned nodes with small degrees. Unsurprisingly, the visualization features a large cluster centered around google.com, and multiple smaller clusters corresponding to websites associated with different meanings of the term.
5 CONCLUSIONS
In this work we have demonstrated the integration of Solr and Spark to support end-to-end text analytics in a seamless and efficient manner. Our three usage scenarios only scratch the surface of what's possible, since we now have access to the rich ecosystem that has sprung up around Spark. With PySpark, which provides
4 https://gephi.org

Figure 4: Network visualization for "jaguar".
Python bindings for Spark, we gain further integration opportunities with PyTorch, TensorFlow, and other deep learning frameworks, enabling access to state-of-the-art models for many text processing tasks, all in a single unified platform.
Acknowledgments. This work was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada, the Canada Foundation for Innovation Leaders Fund, and the Ontario Research Fund.
REFERENCES
[1] G. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica. 2011. Disk-Locality in Datacenter Computing Considered Irrelevant. In HotOS.
[2] R. Clancy, T. Eskildsen, N. Ruest, and J. Lin. 2019. Solr Integration in the Anserini Information Retrieval Toolkit. In SIGIR.
[3] M. Efron, J. Lin, J. He, and A. de Vries. 2014. Temporal Feedback for Tweet Search with Non-Parametric Density Estimation. In SIGIR. 33­42.
[4] T. Elsayed, F. Ture, and J. Lin. 2010. Brute-Force Approaches to Batch Retrieval: Scalable Indexing with MapReduce, or Why Bother? Technical Report HCIL-201023. University of Maryland, College Park, Maryland.
[5] S. Hendrickson, S. Sturdevant, T. Harter, V. Venkataramani, A. Arpaci-Dusseau, and R. Arpaci-Dusseau. 2016. Serverless Computation with OpenLambda. In HotCloud.
[6] D. Hiemstra and C. Hauff. 2010. MapReduce for Information Retrieval Evaluation: "Let's Quickly Test This on 12 TB of Data". In CLEF. 64­69.
[7] Y. Hu. 2005. Efficient, High-Quality Force-Directed Graph Drawing. Mathematica Journal 10, 1 (2005), 37­71.
[8] J. Lin and M. Efron. 2013. Overview of the TREC-2013 Microblog Track. In TREC. [9] J. Lin, D. Ryaboy, and K. Weil. 2011. Full-Text Indexing for Optimizing Selection
Operations in Large-Scale Data Analytics. In MAPREDUCE. 59­66. [10] C. Macdonald. 2018. Combining Terrier with Apache Spark to Create Agile
Experimental Information Retrieval Pipelines. In SIGIR. 1309­1312. [11] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and D. McClosky. 2014.
The Stanford CoreNLP Natural Language Processing Toolkit. In ACL Demos. 55­60. [12] F. Moretti. 2013. Distant Reading. [13] P. Yang, H. Fang, and J. Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR. 1253­1256. [14] P. Yang, H. Fang, and J. Lin. 2018. Anserini: Reproducible Ranking Baselines Using Lucene. JDIQ 10, 4 (2018), Article 16.

1316

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

An Experimentation Platform for Precision Medicine

Vincent Nguyen
vincent.nguyen@anu.edu.au CSIRO Data61 & Australian National University
Canberra, ACT, Australia
ABSTRACT
Precision medicine--where data from patients, their genes, their lifestyles and the available treatments and their combination are taken into account for finding a suitable treatment--requires searching the biomedical literature and other resources such as clinical trials with the patients' information. The retrieved information could then be used in curating data for clinicians for decisionmaking. We present information retrieval researchers with an online system which enables experimentation in search for precision medicine within the framework provided by the TREC Precision Medicine (PM) track. A number of query and document processing and ranking approaches are provided. These include some of the most promising gene mention expansion methods, as well as learning-to-rank using neural networks.
CCS CONCEPTS
· Information systems  Learning-to-rank; Query reformulation; Specialized information retrieval;
KEYWORDS
Health informatics, Literature search, Domain-Specific Search
ACM Reference Format: Vincent Nguyen, Sarvnaz Karimi, and Brian Jin. 2019. An Experimentation Platform for Precision Medicine. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331396
1 INTRODUCTION
Improving the treatment success of cancer patients relies on providing the right information to practising clinicians. While some of this information is published in the biomedical literature, searching among over 27 million MEDLINE abstracts, with new articles added each minute, makes it difficult if not impossible to know all the latest treatment options. Similarly, it is not straightforward to find clinical trials that a patient is eligible for. The goal of the TREC Precision Medicine (PM) track [13, 14] is to facilitate research in this domain by providing cancer patient scenarios (Figure 1) and
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331396

Sarvnaz Karimi Brian Jin
firstname.lastname@csiro.au CSIRO Data61
Sydney, NSW, Australia

disease

Acute lymphoblastic leukemia

gene

ABL1, PTPN11

demographic 12-year-old male

Figure 1: A sample topic from TREC PM 2018 (Topic 1).

their corresponding clinical trials1 as well as treatment options in MEDLINE and abstracts from The American Association for Cancer Research (AACR) and the American Society of Clinical Oncology (ASCO). This track is in its third year, and 59 teams collectively participated in the past two years (32 teams with 258 runs in 2017 and 27 teams with 193 runs in 2018). These teams report a number of different techniques. Some are common, such as expanding genes and variants in the topics [6], and some are unique, such as creating a specialized knowledge graph for the task [17].
We provide a platform for researchers to experiment with some of the most popular query expansion and ranking methods for the PM track. We also implement learning-to-rank using the latest deep learning-based methods in text classification, including a language representation model called Bidirectional Encoder Representations from Transformers (BERT) [5] and Universal Language Model Finetuning (ULMFiT) [7]. Our aim is to facilitate experimentation within this area, and hence advance the field. Our system is built on top of the A2A2 system [8] which is designed for experimentation within the TREC Clinical Decision Support track.
2 RELATED SYSTEMS
Literature on search for precision medicine is limited, with most relevant studies reported by the TREC PM 2017 and 2018 participants [13, 14]. These reports however often are work in progress and lack enough details on the methods and implementation details, making it difficult to reproduce the results.
There are other related systems in place. One is proposed by Koopman et al. [9] with a task-based search engine to assist in clinical search. Another platform is EvALL [1] where the output of different systems can be compared in the same setting. An information retrieval experimentation platform that uses Domain Specific Language (DSL) is proposed by Scells et al. [15] which can be used for systematic reviewing in the legal or medical domain.
Marshall et al. [11] release an open-source web-based system, RobotReviewer, which takes input biomedical articles or clinical trials and processes them for extraction and synthesis of evidence for the practice of Evidence-Based Medicine. We see our system
1 https://clinicaltrials.gov/ 2 https://www.vizie.csiro.au/trec-eval

1357

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

as a complementary first step before RobotReviewer to find the articles to be processed for evidence.
3 INDEXING
Our system provides the index of three collections: MEDLINE abstracts, AACR and ASCO abstracts, and clinical trials that were part of the TREC PM supplied data collections. All documents in the MEDLINE collection are stemmed with stopwords removed automatically at index time by Solr3. Medline abstracts documents have the following fields indexed: pmid (Pubmed ID), pmcid (Pubmed Central ID), title, abstract, article type, MeSH headings, article keywords and date published. Similarly, clinical trials are indexed with the following fields: brief title, minimum age, maximum age, official title, brief summary, detailed description, nct-id (clinical trial registry number), intervention type and intervention, inclusion and exclusion criteria, condition browse (MeSH keywords), referenced MEDLINE identifiers (if they exist) and primary outcome. Inclusion and exclusion criteria are extracted by using regular expressions and can be used to restrict the demographics of retrieved documents during query time. The age of patients are converted to age in days to avoid floating point and date arithmetic. Finally, the AACR and ASCO abstracts are indexed with the fields: id, meeting, title and abstract.
4 QUERY PROCESSING TECHNIQUES
We provide query expansion options as shown on the left-handside of Figure 2. Apart from standard query expansion techniques, such as pseudo-relevance feedback, expansion with domain-specific terminology from Unified Medical Language System (UMLS) metathesaurus, and expansion using semantically related terms determined with word embeddings of Wikipedia and MEDLINE, we provide gene and disease expansion as explained below.
Gene Expansion. Gene expansion uses genes identified by Metamap [2] to expand the topics using one of the four available options: (1) Expansion using Metamap, which expands the query using Metamap suggested concepts that are restricted to the following semantic types: Phenomenon or Process, Cell or Molecular Dysfunction, Molecular Biology Research Technique, Enzyme, Amino Acid, Peptide, or Protein, Gene or Genome, Biologically Active Substance, Pharmacologic Substance, Genetic Function, Organic Chemical, Neoplastic process, Molecular Function and Receptor. (2) Expansion using Wikipedia API4. (3) Expansion using the Human Gene Ontology5 expands the abbreviated gene names with the first match found in the ontology. For instance, cyclin-dependent kinase 4 is expanded from cdk4 through the use of the ontology.
Users can also use a combination of these options.
Disease Expansion. Disease expansion relies on Metamap to identify the disease names. They can then be expanded using one the three options:
· Metamap filtering, which uses UMLS concepts restricted to the semantics types of: Disease or Syndrome, Sign or Symptom, Pathologic Function, Anatomical Abnormality,
3Solr version 6.6.0 http://lucene.apache.org/solr/ 4 https://pypi.python.org/pypi/wikipedia 5http://www.geneontology.org/ (accessed 16 Jan 2018)

Clinical Drug, Clinical Attribute and Neoplastic Process. Using these semantics types, we extract UMLS concepts using MetamapLite [3] which we denote as TM , and extract terms using the Wikipedia API which we denote as TW . We use the set intersection, TM  TW , in order to produce a final set of expansion terms. · Semantic variation, in which disease mentions are expanded by finding semantically relevant words (at most three) with either Wikipedia, or MEDLINE word embeddings which are trained using Word2Vec.6
Demographic Attribute Expansion and Filtering. Clinical trials documents present demographic attributes transparently. As such, we are able to normalise demographic attributes found within queries to exact matches found within the clinical trial corpus. If Normalize demographics is chosen, queries containing strings indicating a child (e.g. person at the age of 6) will be expanded with the word child, and a query with the term female will be expanded with the terms woman women, and similarly for male with man men. The ages in the queries were normalised to be expressed in days to conform with the index. We use Solr's boolean query operators to exclude clinical trial documents that do not contain the appropriate demographic target group. An example of such a boolean query is:
-gender:male AND maximum_age:[0 TO 5110] This operation excludes documents that are either for males or individuals over the age of 15 (5,510 days). Conversely, it will restrict the document result set to contain only patients that are both female and under the age of 15.
5 RANKING MODELS
We provide BM25 and language modelling ranking models from Apache Solr. For the case of BM25, we provide the option of tuning the b and k1 parameters. Language model uses Dirichlet similarity.
Learning-to-rank. We have four different implementations of learning-to-rank (LTR) available:
(1) SVM with Word Embedding: We use word embeddings created by Chiu et al. [4] from PubMed, where they released their best hyper-parameters that are empirically identified. Mean word embeddings are used to represent documents.
(2) SVM with LETOR [12] features: Features are TF-IDF of each term in the query in each of the document facets, including title, abstract, and the full text (for clinical trials). We also use BM25 score of each facet, TF-IDF values, language model features (Dirichlet and Jelinek-Mercer) of each facet, and length of each facet.
(3) ULMFit [7]: We use a language-model encoder for the generic WikiText-103 model and then fine-tune a Recurrent Neural Network (RNN) classifier using the language model encoder for LTR.
(4) BERT [5]: The pre-trained bert-base-uncased model is finetuned with default BertConfig settings with a BERT classifier and a scaled loss function based on label frequency (more
6 https://radimrehurek.com/gensim/models/word2vec.html

1358

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

frequent label loss is scaled down, while less frequent label loss is scaled up).7
Training for the LTR models is based on the training source that the users select (2017 or 2018 topics). One limitation on this setting is its fixed set of fine-tuned parameters. In a later version of the system, we will add more options to make it more flexible.

6 RE-RANKING USING CITATIONS
Clinical trials documents often cite MEDLINE articles. We implement a heuristic in order to utilise these relationships between clinical trial and MEDLINE articles. Given a query, if a MEDLINE article is referenced in a highly relevant or high scoring clinical trial, it receives a small boost based on the rank of that clinical trial document. We also applied the same boost such that clinical trials received a boost to their scores if they are linked to a high scoring MEDLINE document. These boosts were small and decreased exponentially with reciprocal rank:

Sd = Sd + b(Rd )

(1)

where Sd is the score of a document, and b is a boosting function that uses reciprocal ranking of a matched document in the second
or paired document set,

b(Rd )

=

1 exp(Rd )

(2)

Merging search results using federated search. A user can choose to only search on clinical trials or only literature (a combination of MEDLINE, and ACCR & ASCO abstracts). However, if they choose both, our system can merge the results using federated search algorithms. We use the Generalized Document Scoring [10] (GDS) algorithm to achieve this. The GDS algorithm calculates the document score by determining the amount of overlap between the document and thequery. This score is normalized to be between 0 (no overlap) and 2 (complete overlap). However, a drawback of using this method is that the query is treated as a bag-of-words (each term is given the same weight) and it does not distinguish important terms such as disease and gene mentions. In order to mitigate this, after applying GDS, we boost the scores of documents containing the disease name and gene mutation.
However, a limitation of GDS is that it can only be applied to one field at a time or the entire document at once, which is undesirable as the most important parts of the document are limited to only a few facets such as title, abstract and article keywords for MEDLINE documents, brief/official title, brief summary, detailed description and abstract for clinical trial and title and abstract for AACR & ASCO articles. We hence apply GDS to each field if they exist. Otherwise, we take the rank score detailed by the equation below as fallback:

RS (d ,

f

)

=

1-

r ankd |D|  10

(3)

where RS is the rank score function, d is a document in all retrieved

documents D, f is the facet that doesn't exist on the current docu-

ment and rankd denotes the rank of the document.

7We use and modify the code provided in: https://github.com/huggingface/ pytorch- pretrained- BERT.

The facets for each document are normalized using weights; this ensures that when comparing collections with a different number of fields, for example, AACR & ASCO which do not have a keyword field while MEDLINE does, we are able to fairly compare between collections.
Alternatively, we use another more simple merging strategy called Randomized Round Robin (SRR). In the round robin merging algorithm, each document from each collection is interlaced into a final ranked list. This ensures that the final merged list will maintain the same ordering between documents of the same collection regardless of the differences in magnitude of scores between the collections. The variant, randomised round robin, will randomly select which collection to sample in the round robin merging process for each document.
7 EVALUATION METRICS
The system generates the results offline and creates an email notification. The execution time depends on the load of the system as well as what combination of the methods have been selected. For simple runs which use standard ranking models, it can be on the order of minutes. The retrieved results are evaluated using TREC standard scripts (trec_eval and sample_eval). Each search request generates results using the following metrics: infNDCG [16], infAP, iAP (inferred Average Precision), iP@10, NDCG, P@10 (Precision at rank 10), R-Prec (Recall-Precision), B-Pref and MAP (Mean Average Precision).
8 DEMONSTRATION SYSTEM
A screenshot of the experimental design page is shown in Figure 2. It shows a setting where a list of topics can be chosen as well as some of the implemented techniques.
At this point in time, the learning-to-rank models are fixed to our trained models. However, these will get expanded with the options of selecting between the features (for SVM LETOR), and changing hyperparameters for the models based on neural networks. Apart from the online system, the code for the system will be publicly available on a GitHub repository.
9 ACKNOWLEDGEMENT
Authors would like to acknowledge the contributions of Maciej Rybinski (CSIRO Data61) in improving the system post submission of this article. We also thank CSIRO Precision Health Future Science Platform (PH FSP) funding of the project. Vincent Nguyen is on CSIRO Research Office top-up PhD scholarship on the project and is also supported by the Australian Government Research Training Program Scholarship.
REFERENCES
[1] E. Amigó, J. Carrillo-de Albornoz, M. Almagro-Cádiz, J. Gonzalo, J. RodríguezVidal, and F. Verdejo. 2017. EvALL: Open Access Evaluation for Information Access Systems. In SIGIR. 1301­1304.
[2] A. Aronson and F. Lang. 2010. An overview of MetaMap: Historical perspective and recent advances. Journal of the American Medical Informatics Association 17, 3 (2010), 229­236.
[3] A. Aronson, W. Rogers, and D. Demner-Fushman. 2017. MetaMap Lite: an evaluation of a new Java implementation of MetaMap. Journal of the American Medical Informatics Association 24, 4 (2017), 841­844.

1359

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

Figure 2: System interface showing some of the options provided for the users.

[4] B. Chiu, G. Crichton, A. Korhonen, and S. Pyysalo. 2016. How to Train good Word Embeddings for Biomedical NLP. In ACL Workshop on Biomedical Natural Language Processing. 166­174.
[5] J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018). arXiv:1810.04805
[6] T. Goodwin, M. Skinner, and S. Harabagiu. 2017. UTD HLTRI at TREC 2017: Precision Medicine Track. In TREC. Gaithersburg, MD.
[7] J. Howard and S. Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In The 56th Annual Meeting of the Association for Computational Linguistics. Melbourne, Australia, 328­339.
[8] S. Karimi, V. Nguyen, F. Scholer, B. Jin, and S. Falamaki. 2018. A2A: Benchmark Your Clinical Decision Support Search. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. Ann Arbor, MI, 1277­1280.
[9] B. Koopman, G. Zuccon, and J. Russell. 2017. A Task-oriented Search Engine for Evidence-based Medicine. In SIGIR. Shinjuku, Tokyo, Japan, 1329­1332.
[10] P. Li, P. Thomas, and D. Hawking. 2013. Merging Algorithms for Enterprise Search. In ADCS. 42­49.

[11] I. Marshall, J. Kuiper, E. Banner, and B. Wallace. 2017. "Automating Biomedical Evidence Synthesis: RobotReviewer. In ACL. Vancouver, Canada, 7­12.
[12] T. Qin, T-Y Liu, J. Xu, and H. Li. 2010. LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval. Information Retrieval 13, 4 (2010), 346­374.
[13] K. Roberts, D. Demner-Fushman, E. Voorhees, W. Hersh, S. Bedrick, and A. Lazar. 2018. Overview of the TREC 2018 Precision Medicine Track. In TREC. Gaithersburg, MD.
[14] K. Roberts, D. Demner-Fushman, E. Voorhees, W. Hersh, S. Bedrick, A. Lazar, and S. Pant. 2017. Overview of the TREC 2017 Precision Medicine Track. In TREC. Gaithersburg, MD.
[15] H. Scells, D. Locke, and G. Zuccon. 2018. An Information Retrieval Experiment Framework for Domain Specific Applications. In SIGIR. Ann Arbor, MI, 1281­ 1284.
[16] E. Yilmaz and J.A. Aslam. 2006. Estimating Average Precision with Incomplete and Imperfect Judgments. In CIKM. 102­111.
[17] X. Zhou, X. Chen, J. Song, G. Zhao, and J. Wu. 2018. Team Cat-Garfield at TREC 2018 Precision Medicine Track. In TREC. Gaithersburg, MD.

1360

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

cwl_eval: An Evaluation Tool for Information Retrieval

Leif Azzopardi
University of Strathclyde Glasgow, UK
leifos@acm.org

Paul Thomas
Microsoft Canberra, Australia pathom@microsoft.com

Alistair Moffat
The University of Melbourne Melbourne, Australia
ammoffat@unimelb.edu.au

ABSTRACT
We present a tool ("cwl_eval") which unifies many metrics typically used to evaluate information retrieval systems using test collections. In the C/W/L framework metrics are specified via a single function which can be used to derive a number of related measurements: Expected Utility per item, Expected Total Utility, Expected Cost per item, Expected Total Cost, and Expected Depth. The C/W/L framework brings together several independent approaches for measuring the quality of a ranked list, and provides a coherent user model-based framework for developing measures based on utility (gain) and cost.Here we outline the C/W/L measurement framework; describe the cwl_eval architecture; and provide examples of how to use it. We provide implementations of a number of recent metrics, including Time Biased Gain, U-Measure, Bejewelled Measure, and the Information Foraging Based Measure, as well as previous metrics such as Precision, Average Precision, Discounted Cumulative Gain, Rank-Biased Precision, and INST. By providing state-of-the-art and traditional metrics within the same framework, we promote a standardised approach to evaluating search effectiveness.
ACM Reference Format: Leif Azzopardi, Paul Thomas, and Alistair Moffat. 2019. cwl_eval: An Evaluation Tool for Information Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331398
1 INTRODUCTION
Effectiveness evaluation has played a central role in the development of information retrieval systems [11]. Over the years many metrics have been proposed, with the more recent ones employing multi-valued and/or discounted relevance values (Discounted Cumulative Gain (DCG) [3] and Rank Biased Precision (RBP) [5]); cost (or time) associated with viewing result items (Time Biased Gain (TBG) [12]); and the way in which users adapt their interactions according to their goals and constraints (INST [7], Bejewelled Player Model (BPM) [14], and Information Foraging Theory (IFT) [1]). With each metric proposed a new evaluation script is typically introduced (or sometimes not), with the explanation in part that the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331398

metrics are more sophisticated and go beyond the typical assumptions made by trec_eval, and in part because of the complexity already present in trec_eval. The growth in the diversity of tools means that researchers wanting to use "state-of-the-art" metrics
in their work need to obtain a variety of independent scripts (if
they exist), and manage their differing input formats [9]. As a con-
sequence there has been slow uptake of newer metrics despite
evidence that metrics like INST, TBG, BPM, and IFT are able, in
various ways, to make more accurate predictions of performance,
and/or are more correlated with user satisfaction than traditional approaches; and despite the standard set of trec_eval metrics making questionable modelling assumptions (Average Precision (AP)
[8]), or having mathematical infelicities (Reciprocal Rank (RR) [2]).
We describe a common, extensible, open-source resource for evaluation metrics, ensuring back-compatibility with trec_eval, so that previous measurements of performance remain available in
a single tool, and allowing the community to also employ these newer measures. In its simplest form, cwl_eval takes the same input as trec_eval, but also provides additional options.
To provide the theoretical underpinnings we draw upon the C/W/L framework proposed by Moffat et al. [8]. This framework provides a standardised and intuitive way to encode a wide range of metrics, and enables reporting of not just Expected Utility per item inspected, the rate at which gain is acquired, but also the related measurements of Expected Total Utility, the gain accumulated from the whole list; Expected Cost per item inspected; Expected Total Cost, the cost incurred in examining the results list; and Expected Depth, the number of items a searcher examines given the user model
encoded within the metric. The C/W/L framework is described in detail elsewhere [1, 7, 8].
In brief, it models the manner in which probabilistic users examine a
ranked list, assuming that each user reads top down and is governed by a conditional continuation probability 0  Ci  1 that indicates the fraction that proceed to examine the item at depth i + 1, given that they have examined the item at depth i. Different choices of C = Ci  then reflect different beliefs about user behaviour and hence define different metrics. A vector of weights W = Wi  can be derived from C, where Wi is an assessment of the expected proportion of user attention directed at the item at rank i. Using these weights the expected utility (EU) of a ranking is computed
as the dot product between the relevance (that is, gain) vector and W; and the expected total utility (ETU), how much the user takes from the interaction as a whole, via a similar computation. We
can also calculate the expected cost (EC) of examining an item,
the dot product between the weights and a cost vector [1, 12], and
the expected total cost (ETC). The costs used for the latter two
computations can be in units of documents, characters, reading
seconds, and so on. For example, the U-Measure [10] calculates
cost in characters, while TBG and IFT use seconds. Finally, we can

1321

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

calculate the expected depth (ED), that is, how far down the ranked list a randomly-selected user will go.
Fundamentally, the C/W/L framework generates a wide range of measurements regarding the predicted user interactions with the ranked list of search results; and hence, conversely, allows for a wide range of observational information to be used to derive metrics that relate to user behaviour, and thus to useful measurement.
2 TOOLKIT, APP AND NOTEBOOKS
Toolkit. cwl_eval defines three core classes. First, the Ranking class is minimally composed of two ordered lists representing the gains and costs of the items returned from the search. Attributes such as total gain and total relevant items are calculated from these two lists. The ranking is then passed to the CWLRuler, which is composed of a number of CWLMetrics; or directly to a single CWLMetric. The latter produces the measurements.
The heart of the toolkit is the CWLMetric class, which is used as the basis of different measures. It requires the definition of the C, W and/or L vectors. (Moffat et al. [6] define the connections between them.) Typically, C is the easiest commencement point, and W and L are computed from it by the base class. By creating a Ranking object, and a Metric object, it is possible to ask the Metric for C, W, or L for that Ranking. The examples below show how this is useful in exploring how metrics behave, and the different kinds of user models that they encode. Adding a new metric to the toolkit is simple: first inherit from the CWLMetric class, and then describe the C vector.
Graded Gain Values. Järvelin and Kekäläinen [3] make use of graded gains and non-binary relevance judgements; Moffat and Zobel [5] similarly allow fractional utility. cwl_eval also allows fractional gains to be input as part of a standard TREC qrels file.
Residuals. In their description of RBP Moffat and Zobel [5] introduce the notion of residuals, the score uncertainty introduced by unjudged documents. In an ideal evaluation, all documents in the ranking would have associated gain values, and metric score measurements would be precise. Howevere, in most practical experimentation only a small subset of the documents in the ranking have been judged. The standard (and trec_eval) approach is to regard unjudged documents as being non-relevant, with a gain of zero. The same convention is adopted in cwl_eval, and as a result, most (but not all) C/W/L metrics are computed as lower bounds on the "true" score. The residual is then the difference between that lower score and a matching upper value that arises from supposing that every single unjudged document--right through to the end of the collection--is fully relevant.
In particular, high residual values indicate an experimental context in which there might be non-trivial imprecision in the measured values, with further relevance judgements being the only way to be sure whether that is in fact the case. Note that residuals should not be thought of as being confidence intervals, or as having any statistical basis. Their sole purpose is to provide a bounding range on the eventual score, based on the partial evidence supplied by incomplete relevance judgements.
Application. The program cwl_eval.py provides a similar interface to trec_eval, and in simplest form is used via

python cwl_eval.py <TREC -QRELs > <TREC -RESULTS >
where <TREC-QRELS> is the name of a standard four-column TRECformatted relevance file, and <TREC-RESULTS> a standard six-column TREC-formatted result file. When no cost file is specified, cwl_eval assumes the cost of each item to be 1.0. If a cost file is specified via the flag "-c <COSTS>", then document-specific costs are used to calculate expected (and total) costs.
Note the relationship between the measurements: EU (the expected rate at which gain is acquired per item viewed, averaged over all users) multiplied by ED yields ETU. Similarly, EC (the expected cost per item examined, in the cost units supplied), multiplied by ED, is equal to ETC. Reporting one measurement (EU, the per-itemviewed rate at which gain is accumulated, as is typically the case with Precision and RBP, for example) provides part of the picture; reporting both the expected number of items per unit of cost, and the expected cost, provides more information. Further, note that the rate of gain per unit of cost can be calculated by taking ETU and dividing it by ETC. As already noted, costs can be specified in any units, for example, documents, seconds, characters, kilobytes, etc. If seconds are used, then EC and ETC are seconds per document, and total seconds, respectively.
By default cwl_eval outputs a list of metric scores (Precision, RR, AP, RBP, INST, and so on). It can also be readily configured to report other sets of metrics, using the flag "-m <METRICS>" and listing the metrics to be reported in a file, one per line, for example:
PrecisionCWLMetrics(k=10) INSTCWLMetric(T=2.5) APCWLMetric () RBPCWLMetric ( theta =0.3) TBGCWLMetric(h=20)
It is straightforward to select and configure metrics. For example, cwl_bpm.py provides an implementation of the Bejewelled Player measure [14], which minimally takes two parameters: T , the total amount of benefit desired, and K, the total amount of cost available to be spent. Adding BPMCWLMetric(T=4, K=10) to the list of metrics means that the static Bejewelled Player Model will be computed.
For convenience, three other flags can be set: "-b <BIBFILE>", to save the BibTEX associated with the metrics specified/used; "-n" to include the column names of each measurement in the output; and "-r" to compute and report residuals where it is appropriate to do so. Note that all output is provided on a per query/topic basis, the presumption being that a subsequent processing phase will compute average scores over topics and also (if required) carry out statistical tests.
Tests. As part of the development, we checked cwl_eval's output of a range of traditional metrics (P@k, AP, NDCG@k) for consistency with trec_eval. For this we used standard TREC test collections, and then ranked the documents for the corresponding topics using BM25. Those runs were then fed into both evaluation tools, and the scores for the metrics were compared.
One issue that needed to be addressed was the internal sorting performed by trec_eval, which ignores the provided ranking, and sorts the list by score, and then reverse document identifier (see Yang et al. [13] for further discussion). When we re-sorted the runs according to this criteria, cwl_eval provided the same scores as trec_eval. Scripts for all of these validation tests are provided in

1322

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

the cwl_eval GitHub repository. We also checked the results against those of inst_eval1 [4], and the internal tests of the irmetrics package2, to verify that the scores were equivalent; scores agreed to within rounding error.
Implementation. The C/W/L Evaluation Toolkit, and cwl_eval, is available at https://github.com/ireval/cwl. A demonstration of using the toolkit in Jupyter Notebooks is provided at https://github. com/ireval/cwl-examples.
3 C/W/L EXAMPLES
To provide examples of how the C/W/L framework can be used to see the inner workings of the different metrics, we created a number of Jupyter Notebooks, which show: (i) how the different metrics can be instantiated; (ii) how to access the different vectors; and (iii) how to plot them.
Example Topics. For the examples below, we have assumed that we have two topics (T1 and T2). The table below shows the corresponding gain vectors (in the range 0­1) and cost vectors (item inspection times, in seconds), to depth ten in each case.
rank i 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 T1 Gain 0.0, 0.0, 0.2, 0.4, 1.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0
Cost 1.2, 0.6, 0.4, 0.6, 3.6, 1.6, 0.6, 2.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 1.8
T2 Gain 1.0, 0.0, 1.0, 0.4, 0.0, 0.2, 0.0, 0.0, 1.0, 0.2, 0.0, 0.4, 0.0, 0.0, 0.0 Cost 3.2, 1.6, 1.4, 0.6, 3.6, 1.6, 0.6, 1.6, 2.6, 0.2, 1.2, 0.2, 0.2, 0.6, 1.8

Sample Output. Below we have included two examples of the output from cwl_eval, where: (i) no cost information is included, and (ii) cost information is included. Table 1 shows the resulting output when costs are not provided (only T1 is reported). Here, the default cost per item is assumed to be one (a unit cost), and so the Expected Cost per item viewed is one, and the Expected Total Cost is the same as Expected Depth.
Table 2 shows the output when the document costs are provided. Now the expected cost per item depends on how much time it takes to process each item, and the proportion of attention allocated to each item. In this example the expected total cost is now different from the expected depth, by a factor related to the weighted cost of processing the inspected items. Costs are in the units provided, so here the output units are seconds/document (EC) or seconds (ETC).
1 https://github.com/ielab/inst_eval 2 https://github.com/Microsoft/irmetrics- r

Table 1: cwl_eval output without cost information.

Topic Metric

EU ETU EC ETC ED

T1 AP

0.2722 1.6000 1.0000 5.8776 5.8776

T1 RR

0.0667 0.2000 1.0000 3.0000 3.0000

T1 P@5

0.3200 1.6000 1.0000 5.0000 5.0000

T1 NDCG-k@10 0.2270 1.0314 1.0000 4.5436 4.5436

T1 INST-T=2

0.1545 0.6069 1.0000 3.9220 3.9292

T1 TBG-H@2

0.1752 0.5981 1.0000 3.4142 3.4142

T1 BPM-Dynamic-... 0.3200 1.6000 1.0000 5.0000 5.0000

T1 IFT-...

0.0659 0.1097 1.0000 1.6649 1.6649

Figure 1: C/W/L function plots for three metrics for Topics T1 and T2. Top row: RBP,  = 0.8; middle row, TBG, H = 2; bottom row, BPM, T = 2, K = 10.0, hc = 0.5, hb = 0.5

Inside Metrics. The repository includes a Jupyter Notebook to show how we can visualise and inspect the models within the metrics. Figure 1 shows the C/W/L functions for a subset of the metrics on the two example topics. For RBP the plots are the same for topics T1 and T2--this is because RBP is not sensitive to either the cost values or the gain values. Both TBG and BPM consider both the gain and cost vectors to determine how likely a user is to continue down the ranked list (C), and hence the proportion of attention that is paid to each result (W) and how likely the user is to stop at a given rank (L).
Visualising the Measurements. Since cwl_eval outputs a series of measurements given the metric, it is now possible to contextualise the usual Expected Utility measurement with respect to the

Table 2: cwl_eval output with costs information.

Topic Metric

EU ETU EC ETC ED

T1 AP

0.2722 1.6000 1.1681 6.8653 5.8776

T1 RR

0.0667 0.2000 0.7333 2.2000 3.0000

T1 P@5

0.3200 1.6000 1.2800 6.4000 5.0000

T1 NDCG-k@10 0.2270 1.0314 1.1827 5.3738 4.5436

T1 RBP@0.6

0.1287 0.3218 1.0208 2.5520 2.5000

T1 TBG-H@2

0.2143 0.7195 1.1513 3.8663 3.3582

T1 BPM-Dynamic... 0.3200 1.6000 1.2800 6.4000 5.0000

T1 IFT...

0.0748 0.1269 1.0857 1.8412 1.6959

1323

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

Figure 2: Example of how measurements change over metric parameters. Left column: RBP (where  was varied from 0.1 to 0.9); middle: TBG (where the half-life parameter is varied from 0.25 to 2); Right: BPM (where T was varied from 0.5 to 4.0). Top row: Expected Utility (EU)
per item inspected vs Expected Depth (ED); middle: Expected Total Utility (ETU) vs ED; bottom: Expected Total Cost (ETC) vs ED.

Expected Depth, and to visualise how the EU and ED change when the parameters of the metrics are varied. Figure 2 shows how Expected Utility and Expected Total Utility, and Expected Total Cost, vary as a function of ED for three metrics. Each plotted point represents a particular parameter setting combination for the metric and its corresponding predictions.
4 SUMMARY
We have described the C/W/L evaluation framework, a toolkit and an application for evaluating information retrieval systems. This work represents the unification of various metrics within one package, enabling direct comparison between the estimates of such metrics. It also provides the foundations for the development of new utility- and cost-based metrics.
REFERENCES
[1] Leif Azzopardi, Paul Thomas, and Nick Craswell. Measuring the utility of search engine result pages: An information foraging based measure. In Proc. SIGIR, pages 605­614, 2018.
[2] Norbert Fuhr. Some common mistakes in IR evaluation, and how they can be avoided. SIGIR Forum, 52(2):32­41, 2017.
[3] Kalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002.

[4] Bevan Koopman and Guido Zuccon. A test collection for matching patient trials. In Proc. SIGIR, pages 669­672, 2016.
[5] Alistair Moffat and Justin Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, 2008.
[6] Alistair Moffat, Paul Thomas, and Falk Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. CIKM, pages 659­668, 2013.
[7] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. INST: An adaptive metric for information retrieval evaluation. In Proc. Aust. Doc. Comp. Symp., pages 5:1­5:4, 2015.
[8] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. Incorporating user expectations and behavior into the measurement of search effectiveness. ACM Trans. Inf. Syst., 35(3):24:1­24:38, 2017.
[9] Joao Palotti, Harrisen Scells, and Guido Zuccon. Trectools: an open-source python library for information retrieval practitioners involved in trec-like campaigns. In Proc. SIGIR, 2019.
[10] Tetsuya Sakai and Zhicheng Dou. Summaries, ranked retrieval and sessions: A unified framework for information access evaluation. In Proc. SIGIR, pages 473­482, 2013.
[11] Mark Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.
[12] Mark D. Smucker and Charles L. A. Clarke. Time-based calibration of effectiveness measures. In Proc. SIGIR, pages 95­104, 2012.
[13] Ziying Yang, Alistair Moffat, and Andrew Turpin. How precise does document scoring need to be? In Proc. AIRS, pages 279­291, 2016.
[14] Fan Zhang, Yiqun Liu, Xin Li, Min Zhang, Yinghui Xu, and Shaoping Ma. Evaluating web search with a bejeweled player model. In Proc. SIGIR, pages 425­434, 2017.

1324

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

TrecTools: an Open-source Python Library for Information Retrieval Practitioners Involved in TREC-like Campaigns

João Palotti
Qatar Computing Research Institute Doha, Qatar
jpalotti@hbku.edu.qa

Harrisen Scells
The University of Queensland Brisbane, Australia h.scells@uq.net.au

Guido Zuccon
The University of Queensland Brisbane, Australia g.zuccon@uq.edu.au

ABSTRACT
This paper introduces TrecTools, a Python library for assisting Information Retrieval (IR) practitioners with TREC-like campaigns. IR practitioners tasked with activities like building test collections, evaluating systems, or analysing results from empirical experiments commonly have to resort to use a number of different software tools and scripts that each perform an individual functionality ­ and at times they even have to implement ad-hoc scripts of their own. TrecTools aims to provide a unified environment for performing these common activities.
Written in the most popular programming language for Data Science, Python, TrecTools offers an object-oriented, easily extensible library. Existing systems, e.g., trec_eval, have considerable barrier to entry when it comes to modify or extend them. Furthermore, many existing IR measures and tools are implemented independently of each other, in different programming languages. TrecTools seeks to lower the barrier to entry and to unify existing tools, frameworks and activities into one common umbrella. Widespread adoption of a centralised solution for developing, evaluating, and analysing TREC-like campaigns will ease the burden on organisers and provide participants and users with a standard environment for common IR experimental activities.
TrecTools is distributed as an open source library under the MIT license at https://github.com/joaopalotti/trectools
CCS CONCEPTS
· Information systems  Evaluation of retrieval results; Test collections;
ACM Reference Format: João Palotti, Harrisen Scells, and Guido Zuccon. 2019. TrecTools: an Opensource Python Library for Information Retrieval Practitioners Involved in TREC-like Campaigns. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331399
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331399

1 INTRODUCTION
Rigorous empirical evaluation is a cornerstone of Information Retrieval (IR) research, as demonstrated by the many research efforts centred around the creation of test collections and resources in evaluation campaigns and shared-tasks like the Text Retrieval Conference (TREC), the Conference and Labs of the Evaluation Forum (CLEF), the NII Testbeds and Community for information access Research (NTCIR), etc..
Creating test collections for IR evaluation and participating to such shared tasks is time consuming ­ with a large amount of effort spent in implementing the evaluation settings, rather than the methods to address the IR task, e.g., creating baselines, selecting documents for assessment, performing relevance assessment, measuring system effectiveness, etc. In this paper we present TrecTools1, an open source Python software package to support the creation and use of IR evaluation resources. TrecTools aims to support both IR campaign organizers and participants to deal with a number of recurrent, tedious and time-consuming procedures. For evaluation campaigns organisers, TrecTools allows one to (1) easily create common IR baselines using popular toolkits such as Indri [20] and Terrier [12], (2) create document pools for relevance assessment from retrieval runs, making many popular pooling strategies available, (3) evaluate each considered run using a wide range of evaluation measures, (4) perform statistical significance analysis between runs and baselines, and (5) create standard visualisations and reporting of IR systems effectiveness. For evaluation campaigns participants and subsequent users (i.e., those that did not originally participate in the campaign), TrecTools allows one to perform the relevant aforementioned tasks at the campaign level (i.e., across all participants) or at an individual level (i.e., for a single participant in a campaign).
Before TrecTools, IR practitioners were required to use a series of separate, independent and unlinked tools such as trec_eval for computing evaluation measures, workspaces e.g., in R/Matlab for statistical analysis and result plotting, etc., and were required to implement their own routines for other tasks, such as pooling. TrecTools aims to provide practitioners with a unified environment to perform these common tasks, as well as standard and verified (i.e., by automatically comparing the output of existing tools, e.g., trec_eval, using unit tests) implementations of common approaches (evaluation measures, pooling strategies, etc.) for each task. In particular, TrecTools goes beyond the popular software package trec_eval [4] by additionally implementing a number of common IR workflows, such as document pool creation and result visualisation, as well as extend the range of evaluation measures
1See http://www.ielab.io/trectools

1325

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

supported. TrecTools is built upon standard Data Science libraries in Python, such as Numpy, Scipy, Pandas and Matplotlib, intending to allow for a rapid and smooth learning curve for new users.
2 RELATED WORK
trec_eval is perhaps the most popular software used by IR researchers. This tool takes as input a run and a relevance assessments (qrels) files, and outputs many of the core IR evaluation measures, including mean average precision (MAP), binary preference (bpref) and precision at different cut-offs (e.g., P@10). The input formats imposed by trec_eval (i.e., the TREC result and qrel formats) have become a de-facto standard for other, subsequent tools. However, there are a number of evaluation measures not implemented in trec_eval, including ad-hoc measures like Rank Biased Precision (RBP) [14], diversity measures like -nDCG [5], risk-sensitive measures like U-risk [24], and multi-dimensional relevance measures [27]. TrecTools, on the other hand, implements all the measures listed above, and eases the implementation of new measures through extensible classes; we furthermore plan to implement new measures arising from the C/W/L framework [3], e.g., IFT goal and rate, and the bejewelled player model. In addition, it facilitates the analysis of the evaluation results by providing automated statistical analysis, plots generation and LATEXtables generation.
Similar to TrecTools, the Java-based EvALL tool provides native and verified implementations of most IR evaluation measures, and includes features such as statistical significance analysis, results visualisation, and LATEXtables generation [1]. TrecTools goes however beyond EvALL's functionalities by supporting other tasks in the collection creation pipeline, e.g, providing utilities for creating retrieval baselines and assessment pools.
PyIndri [23] and Pytrec_eval [22] are Python interfaces to the Indri IR toolkit and trec_eval, respectively. While comparable to TrecTools with respect to baselines creation and IR measures provision (although still limited to those in trec_eval), these tools lack many of TrecTools' functionalities (e.g., results analysis, tables and assessment pools generation, etc.).
Other tools have been released to aid evaluation campaign organisers, including VisualPool [10] and ircor [21]. VisualPool allows to visualise the results of using different document pooling strategies, thus informing collection creators about the effect of using a strategy over another for selecting documents for relevance assessment. Many of the popular pooling strategies implemented in VisualPool are also available in TrecTools, e.g., Depth@K [19], CombMAXTake@N or CombMNZTake@N [13], RRFTake@N [6] and RBPTake@N [14]. ircor is an R package that provides implementations of correlation measures for comparing results rankings or system rankings. This tool implements for example  -AP [26], a extension of the Kendall- correlation that puts higher weight to matches found at higher rank positions. TrecTool also provides an implementation of  -AP (as well as, Kendall- , Pearson and Spearman). Nevertheless, ircor implements variations of  -AP that cope with ties [21]: a functionality not yet present in TrecTool ­ but planned for future releases.
3 TRECTOOLS FEATURES
TrecTools is implemented in Python using standard Data Science libraries and using the object-oriented paradigm. Each of the key

components of an evaluation campaign is mapped to a class: classes for runs (TrecRun), topics/queries (TrecTopic), assessment pools (TrecPools), relevance assessments (TrecQrel) and the evaluation results (TrecRes). Evaluation results can be produced by TrecTools itself using the evaluation metrics implemented in the tool, or be imported from the output file of trec_eval and derivatives. General features of TrecTools are shown in Figure 1. The features that are currently implemented and available to use in TrecTools are as follows.
Querying IR Systems. Benchmark runs can be obtained directly from one of the IR toolkits that are integrated in TrecTools. There is support for issuing full-text queries to Indri, Terrier2 and PISA3 toolkits. Future releases will include other toolkits (e.g., Elasticsearch, Anserini [25], etc.) and support for specific query languages (e.g., Indri's query language, Boolean queries). Examples of baselines creation are given in Figure 2.
Pooling Techniques. The following techniques for pool creation from a set of runs are implemented: Depth@K [19], Take@N [11], Comb-Min/Max/Med/Sum/ANZ/MNZ [13], RRFTake@N [6], RBPTake@N [14]. Examples are shown in Figure 3.
Evaluation Measures. Currently implemented and verified measures include widely used metrics such as Precision at depth K, Recall at depth K, MAP, NDCG, Bpref and RBP [14], as well as recently developed ones, such as uBpref [15], uRBP [27] and the MM framework [17]. Implemented in TrecTools is the option to break ties using document score (i.e., similar to trec_eval), or document ranking (i.e., similar to the original implementation of RBP4). Additionally, TrecTools also allows to compute the residual of the evaluation measure and analyse the relative presence of un-assessed documents. Examples are given in Figure 4.
Correlation and Agreement Analysis. The Pearson, Spearman, Kendall and ap correlations between system rankings can be computed directly using TrecTools. Agreement measures between relevance assessment sets can be obtained with Kappa or Jaccard. Examples are provided in Figures 5 and 6.
Fusion Techniques. Runs can be fused using the following techniques: Comb-Max/Min/Sum/Mnz/Anz/Med ( both using scores and document rankings) [9], RBPFusion [14], RRFFusion [6], or BordaCountFusion [2]. Fusion techniques are provided for metaanalysis. Examples are shown in Figure 7.
4 CONCLUSION
In this paper we introduced TrecTools, an open source Python library for assisting IR practitioners with TREC-like evaluation campaigns. Some of the use cases for campaign organisers using TrecTools include automatically creating baselines by querying the Indri and Terrier IR toolkits (Figure 2), creating meta-rankings using multiple runs (Figure 3), performing analysis of agreement between assessments made by different assessors (e.g., Kappa coefficient, Figure 6), and producing visualisations and LATEX results tables (Figure 1). Some of the use cases for participants of campaigns and regular users include performing statistical significance analysis between runs and LATEXresults tables (Figures 4). While TrecTools
2Thanks to Craig Macdonald for implementing support for Terrier v5.0. 3Thanks to Antonio Mallia for implementing support for PISA (https://github.com/ pisa- engine/pisa) 4Available at https://people.eng.unimelb.edu.au/ammoffat/abstracts/mz08acmtois.html

1326

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

from trectools import TrecQrel , procedures
qrels_file = "./qrel/robust03_qrels.txt" qrels = TrecQrel(qrels_file)
# Generates a P@10 graph with all the runs in a directory path_to_runs = "./robust03/runs/" runs = procedures.list_of_runs_from_path(path_to_runs , "*.gz")
results = procedures.evaluate_runs(runs , qrels , per_query=True) p10 = procedures.extract_metric_from_results(results , "P_10") procedures.plot_system_rank(p10 , display_metric="P@10") # Sample output with one run for each participating team in robust03:
Figure 1: Code Snippets and toy examples with TrecTools. Note the plot is generated by simply calling the method procedures.plot_system_rank().
does not help with obtaining relevance assessments, it can be integrated into existing tools such as Revelation! [8]. TrecTools is by no means a `completed' package: it is open to new evaluation measures and activities as suggested and contributed by the community ­ and it is fully extensible. Despite the care taken for correctness of results, no software is bullet proof -- although, for TrecTools, unit tests are written for each component of the library and methods are also validated against previously released tools for an activity, if any. TrecTools has already been successfully used throughout the creation and results analysis of the CLEF eHealth evaluation campaigns [7, 16, 18, 28].
Acknowledgements. Guido Zuccon is the recipient of an Australian Research Council DECRA Research Fellowship (DE180101579).
REFERENCES
[1] Enrique Amigó, Jorge Carrillo-de Albornoz, Mario Almagro-Cádiz, Julio Gonzalo, Javier Rodríguez-Vidal, and Felisa Verdejo. 2017. Evall: Open access evaluation for information access systems. In SIGIR. ACM, 1301­1304.
[2] Javed A. Aslam and Mark Montague. 2001. Models for Metasearch. In SIGIR. ACM, 276­284. https://doi.org/10.1145/383952.384007
[3] Leif Azzopardi, Paul Thomas, and Alistair Moffat. 2019. cwl_eval: An Evaluation Tool for Information Retrieval. In SIGIR. ACM.
[4] Chris Buckley et al. 2004. The trec_eval evaluation package. [5] Charles LA Clarke, Maheedhar Kolla, Gordon V Cormack, Olga Vechtomova,
Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and diversity in information retrieval evaluation. In SIGIR. ACM, 659­666. [6] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods.. In SIGIR, Vol. 9. 758­759. [7] Jimmy, Guido Zuccon, João Palotti, Lorraine Goeuriot, and Liadh Kelly. 2018. Overview of the CLEF 2018 Consumer Health Search Task. In CLEF. http: //ceur- ws.org/Vol- 2125/invited_paper_17.pdf [8] Bevan Koopman and Guido Zuccon. 2014. Relevation!: An open source system for information retrieval relevance assessment. In SIGIR. 1243­1244.

from trectools import TrecTopics , TrecTerrier , TrecIndri
# Loads some topics from a file (e.g., topics.txt) """ <topics > <topic number ="201" type="single"> <query >amazon raspberry pi </query > <description > You have heard quite a lot about cheap computing as being
the way of the future , including one recent model called a Raspberry Pi. You start thinking about
buying one , and wonder how much they cost. </description > </topic > </topics > """ topics = TrecTopics().read_topics_from_file("topics.txt") # Or...load topics from a Python dictionary topics = TrecTopics(topics={'201': u'amazonraspberrypi'}) topics.printfile(fileformat="terrier") #<topics > # <top > # <num >201</num > # <title >amazon raspberry pi </title > # </top > # </topics >
topics.printfile(fileformat="indri") #<parameters > # <trecFormat >true </trecFormat > # <query > # <id >201 </id > # <text ># combine( amazon raspberry pi ) </text > # </query > # </parameters >
topics.printfile(fileformat="indribaseline") #<parameters > # <trecFormat >true </trecFormat > # <query > # <id >201 </id > # <text >amazon raspberry pi </text > # </query > # </parameters >
tt = TrecTerrier(bin_path="<PATH >/terrier/bin/") # where trec_terrier.sh is located
# Runs PL2 model from Terrier with Query Expansion tr = tt.run(index="<PATH >/terrier/var/index", topics="topics.xml.gz",
qexp=True , model="PL2", result_file="terrier.baseline", expTerms=5, expDocs=3,
expModel="Bo1")
ti = TrecIndri(bin_path="~/<PATH >/indri/bin/") # where IndriRunQuery is located
ti.run(index="<PATH >/indriindex", topics , model="dirichlet", parameters ={ " mu " :2500} ,
result_file="trec_indri.run", ndocs=1000, qexp=True , expTerms=5, expDocs =3)
Figure 2: Code Snippets for manipulating topic formats and
querying IR toolkits (shown here: Terrier and Indri).
[9] Joon Ho Lee. 1997. Analyses of Multiple Evidence Combination. In SIGIR. ACM, 267­276. https://doi.org/10.1145/258525.258587
[10] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2017. Visual Pool: A Tool to Visualize and Interact with the Pooling Method. In SIGIR. ACM, 1321­1324. https://doi.org/10.1145/3077136.3084146
[11] Aldo Lipani, Joao Palotti, Mihai Lupu, Florina Piroi, Guido Zuccon, and Allan Hanbury. 2017. Fixed-cost pooling strategies based on IR evaluation measures. In ECIR. Springer, 357­368.
[12] Craig Macdonald, Richard McCreadie, Rodrygo LT Santos, and Iadh Ounis. 2012. From puppy to maturity: Experiences in developing Terrier. Proc. of OSIR at SIGIR (2012), 60­63.
[13] Craig Macdonald and Iadh Ounis. 2006. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM. ACM, 387­396.
[14] Alistair Moffat and Justin Zobel. 2008. Rank-biased Precision for Measurement of Retrieval Effectiveness. ACM Trans. Inf. Syst. 27, 1, Article 2 (Dec. 2008), 27 pages. https://doi.org/10.1145/1416950.1416952
[15] Joao Palotti, Lorraine Goeuriot, Guido Zuccon, and Allan Hanbury. 2016. Ranking health web pages with relevance and understandability. In SIGIR. ACM, 965­968.
[16] João Palotti, Guido Zuccon, Lorraine Goeuriot, Liadh Kelly, Allan Hanbury, Gareth J. F. Jones, Mihai Lupu, and Pavel Pecina. 2015. ShARe/CLEF eHealth Evaluation Lab 2015, Task 2: User-centred Health Information Retrieval. In CLEF.
[17] Joao Palotti, Guido Zuccon, and Allan Hanbury. 2018. MM: A new Framework for Multidimensional Evaluation of Search Engines. In CIKM. ACM, 1699­1702.

1327

Demonstration Papers 2: Evaluation & Entities

SIGIR '19, July 21­25, 2019, Paris, France

from trectools import TrecPool , TrecRun
r1 = TrecRun("./robust03/runs/input.aplrob03a.gz") r2 = TrecRun("./robust03/runs/input.UIUC03Rd1.gz")
len(r1.topics()) # 100 topics
# Creates document pools with r1 and r2 using different strategies:
# Strategy1: Creates a pool with top 10 documents of each run: pool1 = TrecPool.make_pool ([r1 , r2], strategy="topX", topX =10) # Pool with
1636 unique documents.
# Strategy2: Creates a pool with 2000 documents (20 per topic) using the reciprocal ranking strategy by Gordon , Clake and Buettcher:
pool2 = TrecPool.make_pool ([r1 ,r2], strategy="rrf", topX=20, rrf_den =60) # Pool with 2000 unique documents.
# Check to see which pool covers better my run r1 pool1.check_coverage(r1 , topX =10) # 10.0 pool2.check_coverage(r1 , topX =10) # 8.35
# Export documents to be judged using Relevation! visual assessing system pool1.export_document_list(filename="mypool.txt", with_format="relevation")
Figure 3: Code Snippets for generating and exporting document pools using different pooling strategies.

from trectools import TrecQrel , TrecRun , TrecEval

# A typical evaluation workflow r1 = TrecRun("./robust03/runs/input.aplrob03a.gz") r1.topics()[:5] # Shows the first 5 topics: 601, 602, 603, 604, 605

qrels = TrecQrel("./robust03/qrel/robust03_qrels.txt")

te = TrecEval(r1 , qrels) rbp , residuals = te.getRBP() p100 = te.getPrecisionAtDepth (100)

# RBP: 0.474, Residuals: 0.001 # P@100: 0.186

# Check if documents retrieved by the system were judged: r1.get_mean_coverage(qrels , topX=10) # 9.99 r1.get_mean_coverage(qrels , topX=1000) # 481.390 # On average for system 'input.aplrob03a' participating in robust03 , 480
documents out of 1000 were judged.

# Loads another run r2 = TrecRun("./robust03/runs/input.UIUC03Rd1.gz")

# Check how many documents , on average , in the top 10 of r1 were retrieved in the top 10 of r2
r1.check_run_coverage(r2 , topX =10) # 3.64

# Evaluates r1 and r2 using all implemented evaluation metrics result_r1 = r1.evaluate_run(qrels , per_query=True) result_r2 = r2.evaluate_run(qrels , per_query=True)

# Inspect for statistically significant differences between the two runs for P_10 using two -tailed Student t-test
pvalue = result_r1.compare_with(result_r2 , metric="P_10") # pvalue: 0.0167

Figure 4: Code snippets showing evaluation options avail-
able in TrecTools.
[18] João Palotti, Guido Zuccon, Jimmy, Pavel Pecina, Mihai Lupu, Lorraine Goeuriot, Liadh Kelly, and Allan Hanbury. 2017. CLEF 2017 Task Overview: The IR Task at the eHealth Evaluation Lab - Evaluating Retrieval Methods for Consumer Health Search. In CLEF. http://ceur-ws.org/Vol-1866/invited_paper_16.pdf
[19] K Spark-Jones. 1975. Report on the need for and provision of an'ideal'information retrieval test collection. Computer Laboratory (1975).
[20] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. 2­6.
[21] Julián Urbano and Mónica Marrero. 2017. The Treatment of Ties in AP Correlation. In SIGIR. 321­324.
[22] Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_Eval: An Extremely Fast Python Interface to Trec_Eval. In SIGIR. ACM, 873­876.
[23] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. 2017. Pyndri: a Python Interface to the Indri Search Engine. In ECIR, Vol. 2017. Springer.
[24] Lidan Wang, Paul N Bennett, and Kevyn Collins-Thompson. 2012. Robust ranking models via risk-sensitive optimization. In SIGIR. ACM, 761­770.

from trectools import misc , TrecRun , TrecQrel , procedures
qrels_file = "./robust03/qrel/robust03_qrels.txt" path_to_runs = "./robust03/runs/"
qrels = TrecQrel(qrels_file)
runs = procedures.list_of_runs_from_path(path_to_runs , "*.gz")
results = procedures.evaluate_runs(runs , qrels , per_query=True)
# check the system correlation between P@10 and MAP using Kendall's tau for all systems participating in a campaign
misc.get_correlation( misc.sort_systems_by(results , "P_10"), misc.sort_systems_by(results , "map"), correlation = "kendall") # Correlation: 0.7647
# check the system correlation between P@10 and MAP using Tau's ap for all systems participating in a campaign
misc.get_correlation( misc.sort_systems_by(results , "P_10"), misc.sort_systems_by(results , "map"), correlation = "tauap") # Correlation: 0.77413

Figure 5: Code Snippets for obtaining correlation measures from a set of runs.

# Code snippet to check correlation between two sets of relevance assessment (e.g., made by different cohorts - assessments made by medical doctors Vs. crowdsourced assessments)
from trectools import TrecQrel

original_qrels_file = "./robust03/qrel/robust03_qrels.txt" # Changed the first 10 assessments from 0 to 1 modified_qrels_file = "./robust03/qrel/mod_robust03_qrels.txt"

original_qrels = TrecQrel(original_qrels_file) modified_qrels = TrecQrel(modified_qrels_file)

# Overall agreement

original_qrels.check_agreement(modified_qrels) # 0.99

# Fleiss' kappa agreement

original_qrels.check_kappa(modified_qrels) # P0: 1.00, Pe = 0.90

# Jaccard similarity coefficient

original_qrels.check_jaccard(modified_qrels) # 0.99

# 3x3 confusion matrix (labels 0, 1 or 2)

original_qrels.check_confusion_matrix(modified_qrels)

# [[122712

10

0]

#[

0 5667

0]

#[

0

0 407]]

Figure 6: Code Snippets for obtaining agreement measures from a pair of relevance assessments.

from trectools import TrecRun , TrecEval , fusion

r1 = TrecRun("./robust03/runs/input.aplrob03a.gz") r2 = TrecRun("./robust03/runs/input.UIUC03Rd1.gz")

# Easy way to create new baselines by fusing existing runs:

fused_run = fusion.reciprocal_rank_fusion ([r1 ,r2])

TrecEval(r1 , qrels).getPrecisionAtDepth (25)

# P@25: 0.3392

TrecEval(r2 , qrels).getPrecisionAtDepth (25)

# P@25: 0.2872

TrecEval(fused_run , qrels).getPrecisionAtDepth(25) # P@25: 0.3436

# Save run to disk with all its topics fused_run.print_subset("my_fused_run.txt", topics=fused_run.topics())

Figure 7: Code Snippets for generating fusing two runs (Reciprocal Rank fusion shown here).
[25] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking baselines using Lucene. JDIQ 10, 4 (2018), 16.
[26] Emine Yilmaz, Javed A Aslam, and Stephen Robertson. 2008. A new rank correlation coefficient for information retrieval. In SIGIR. ACM, 587­594.
[27] Guido Zuccon. 2016. Understandability biased evaluation for information retrieval. In ECIR. Springer, 280­292.
[28] Guido Zuccon, João Palotti, Lorraine Goeuriot, Liadh Kelly, Mihai Lupu, Pavel Pecina, Henning Mueller, Julie Budaher, and Anthony Deacon. 2016. The IR Task at the CLEF eHealth Evaluation Lab 2016: User-centred Health Information Retrieval. In CLEF, Vol. 1609. 15­27.

1328

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

Solr Integration in the Anserini Information Retrieval Toolkit

Ryan Clancy,1 Toke Eskildsen,2 Nick Ruest,3 and Jimmy Lin1
1 David R. Cheriton School of Computer Science, University of Waterloo 2 Royal Danish Library 3 York University Libraries

ABSTRACT
Anserini is an open-source information retrieval toolkit built around Lucene to facilitate replicable research. In this demonstration, we examine different architectures for Solr integration in order to address two current limitations of the system: the lack of an interactive search interface and support for distributed retrieval. Two architectures are explored: In the first approach, Anserini is used as a frontend to index directly into a running Solr instance. In the second approach, Lucene indexes built directly with Anserini can be copied into a Solr installation and placed under its management. We discuss the tradeoffs associated with each architecture and report the results of a performance evaluation comparing indexing throughput. To illustrate the additional capabilities enabled by Anserini/Solr integration, we present a search interface built using the open-source Blacklight discovery interface.
ACM Reference Format: Ryan Clancy, Toke Eskildsen, Nick Ruest, and Jimmy Lin. 2019. Solr Integration in the Anserini Information Retrieval Toolkit. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331401
1 INTRODUCTION
The academic information retrieval community has recently seen growing interest in using the open-source Lucene search library for research. Recent events to promote such efforts include the Lucene4IR [3] workshop held in Glasgow, Scotland in 2016 and the Lucene for Information Access and Retrieval Research (LIARR) Workshop at SIGIR 2017 [2]. In an evaluation of seven open-source search engines conducted in 2015 [4], Lucene fared well in comparisons of both effectiveness and efficiency. These results provide compelling evidence that IR researchers should seriously consider Lucene as the foundation of their work.
Advocates of using Lucene for IR research point to several advantages: building on a widely-deployed open-source platform facilitates replicability and brings academic research closer into alignment with "real-world" search applications. Lucene (via integration with Solr) powers search in production deployments at Bloomberg, Netflix, Comcast, Best Buy, Disney, Reddit, and many more sites.
Anserini [6, 7] is a recently-introduced IR toolkit built on Lucene specifically designed to support replicable IR research. It provides
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331401

efficient multi-threaded indexing for scaling up to large web collections and strong baselines for a broad range of collections. The meta-analysis of Yang et al. [8] encompassing more than 100 papers using the test collection from the TREC 2004 Robust Track showed that the well-tuned implementation of RM3 query expansion in Anserini is more effective than most of the results reported in the literature (both from neural as well as non-neural approaches). A key feature of Anserini is its adoption of software engineering best practices and regression testing to ensure that retrieval results are replicable by other members of the community, in the sense defined by recent ACM guidelines.1 That is, replicability is achieved when an independent group can obtain the same results using the authors' own artifacts (i.e., different team, same experimental setup).
This demonstration builds on Anserini and explores the question of how to best integrate it with Solr. We explore and evaluate different architectures, and highlight the new capabilities that Anserini/Solr integration brings.
2 SYSTEM ARCHITECTURE
The first obvious question worth addressing for the academic audience is: Why not just build on top of Solr in the first place? Why is Anserini, for example, built around Lucene instead of Solr? We begin by first articulating the distinction between Lucene and Solr.
2.1 Lucene vs. Solr
Lucene defines itself as a search library. Grant Ingersoll, a Lucene committer as well as the CTO and co-founder of Lucidworks, a company that provides commercial Lucene products and support, offers the analogy that Lucene is like "a kit of parts" [7]. It doesn't prescribe how one would assemble those parts (analysis pipelines, indexer, searcher, etc.) into an application that solves a real-world search problem. Solr fills this void, providing a "canonical assembly" in this analogy.
Solr is a complete end-to-end search platform that uses Lucene for its core indexing and retrieval functionalities. Designed as a web application, Solr is "self-contained" in the sense that all interactions occur via HTTP-based API endpoints. Although there are many client libraries that facilitate access to Solr instances in a variety of programming languages, this design has two main drawbacks from the perspective of IR research:
· Solr APIs were designed with developers of search applications in mind, and thus expose endpoints for indexing, search, administration, and other common operations. However, these APIs lack access to low-level Lucene internals needed by many researchers. While it is in principle possible to expose these functionalities as additional service endpoints for client access, this introduces friction for IR researchers.
1 https://www.acm.org/publications/policies/artifact- review- badging

1285

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

· Since Solr is a web application architecturally, it necessarily runs as a distinct process. Thus, conducting IR experiments involves first starting and configuring the Solr server (i.e., something for the client to connect to). This procedure makes conducting ad hoc retrieval experiments unnecessarily complicated.
In other words, the Solr "canonical assembly" was not designed with IR researchers in mind. This is where Anserini comes in: it builds directly on Lucene and was specifically designed to simplify the "inner loop" of IR research on document ranking models. The system allows researchers to conduct ad hoc experiments on a broad range of test collections right out of the box, with adaptors for standard TREC document collections and topic files as well as integration with standard evaluation tools such as trec_eval. A researcher issues one command for indexing and a second command for performing a retrieval run--and is able to replicate results for a range of ranking models, from baseline bag-of-words ranking to competitive approaches that exploit phrase queries as well as (pseudo-)relevance feedback. Anserini provides all these functionalities without the various overheads of Solr, e.g., managing a separate server, latencies associated with network traffic, etc.
2.2 Anserini Shortcomings
While Anserini already supports academic information retrieval research using standard test collections, there are two main missing capabilities.
The existing focus on supporting document ranking experiments means that the project has mostly neglected interactive search interfaces for humans. These are needed, for example, by researchers exploring interactive search and other human-in-the-loop retrieval techniques. Although Anserini has been integrated with other search frontends such as HiCAL [1], such efforts have been ad hoc and opportunistic. One obvious integration path is for Anserini to expose API endpoints for integration with different search interfaces. However, these are exactly the types of APIs that Solr already provides, and so such an approach seems like duplicate engineering effort with no clear-cut benefit.
As another shortcoming, Anserini does not currently support distributed retrieval over large document collections in a partitioned manner, which is the standard architecture for horizontal scale-out. Although previous experiments have shown Anserini's ability to scale to ClueWeb12, the largest IR research collection currently available (733 million webpages, 5.54TB compressed), using a single monolithic index, the observed query latencies are not suitable for interactive searching [6]. Building a distributed search engine is non-trivial, but this is a problem Solr has already solved--with numerous deployments in production demonstrating the robustness of its design. Once again, it makes little sense for Anserini to reinvent the wheel in building distributed search capabilities.
2.3 Anserini/Solr Integration
Given the two shortcomings discussed above, it makes sense to explore how Anserini can be more tightly integrated with Solr. Different possible architectures are shown in Figure 1. On the left, denoted (a), we show the current design of Anserini, where documents are ingested and inverted indexes are created directly using Lucene (and stored on local disk). In the middle, denoted (b), we

(a)

Anserini

Lucene

(b)

Anserini

Solr Lucene

(c)

Anserini

Solr Lucene

Solr Lucene

Solr Lucene

SolrCloud
Figure 1: Different architectures for integrating Anserini with Solr. In order from left: (a) the current Anserini design; (b) Anserini indexing into a single-node SolrCloud instance (on the same machine); (c) Anserini indexing into a multinode SolrCloud cluster.
show an architecture where Anserini is used as a frontend for document processing, but indexing itself is handled by Solr (which uses Lucene behind the scenes to construct the inverted index). In this design, an IR researcher uses the same exact Anserini indexing tool as before, with the exception of specifying configuration data pointing to a Solr instance. In principle, this Solr instance could be residing on the same machine that is running Anserini, as indicated in (b), or it could be on a different machine (not shown). Separating the frontend from the backend incurs network traffic, but allows distributing load across multiple machines.
Introducing this additional layer of indirection allows us to take advantage of Solr's existing capabilities. For example, we get SolrCloud, which is the ability to set up a cluster of Solr servers for distributed retrieval, "for free". This is shown in the rightmost diagram in Figure 1, denoted (c), where Anserini indexes into a SolrCloud cluster. The use of Solr also means that the backend can interoperate with any number of search interfaces and other frontends in the broader ecosystem (see Section 4). The downside, however, is that indexing occurs over an HTTP-based API endpoint, which is obviously less efficient than directly writing index structures to disk. Solr clients perform automatic batching to amortize the connection costs, but the performance penalty of such a setup is an empirical question to be examined.
An alternative approach to integrating Anserini with Solr is to build indexes directly on local disk, and then copy those indexes into an already running Solr instance. This is possible because Solr itself builds on Lucene, and thus all we need to do is to properly synchronize Solr index metadata with the index structures directly built by Anserini. This works even with a SolrCloud cluster: we can build inverted indexes over individual partitions of a collection, and then copy the data structures over to the appropriate node. In such an approach, the Anserini indexing pipeline remains unchanged, but we need a number of auxiliary scripts to mediate between Solr and the pre-built index structures.
3 EXPERIMENTAL EVALUATION
3.1 Setup
Hardware. Our experiments were conducted on the following:
· A "large server" with 2× Intel E5-2699 v4 @ 2.20GHz (22 cores, 44 threads) processors, 1TB RAM, 26×6TB HDDs, running Ubuntu 16.04 with Java 1.8.
· A ten node cluster of "medium servers", where each node has 2× Intel E5-2670 @ 2.60GHz (8 cores, 16 threads) processors, 256GB

1286

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

Collection
NYTimes Gov2 ClueWeb09b ClueWeb12-B13 Tweets2013

# docs
1.8M 25.2M 50.2M 52.3M 243M

Large Server

Lucene Solr (single-node)

4m14s ± 6s 1h1m ± 3m 2h40m ± 2m 3h9m ± 2m 3h44m ± 2m

2m53s ± 11s 1h52m ± 3m 4h49m ± 2m 6h6m ± 9m 3h13m ± 10m

Lucene Shard
3m12s ± 9s 18m6s ± 29s 44m33s ± 1m 46m52s ± 1m 2h58m ± 3m

Medium Server

Lucene Solr (single-node)

4m17s ± 6s 1h14m ± 1m
4h53m ± 2m

5m16s ± 50s 2h13m ± 6m
5h29m ± 4m

Cluster
Solr (multi-node)
3m25s ± 15s 50m30s ± 35s 2h15m ± 9m 2h4m ± 4m 3h55m ± 4m

Table 1: Total indexing time (mean ± standard deviation) for various architectures on different collections.

RAM, 6×600GB 10k RPM HDDs, 10GbE networking, running Ubuntu 14.04 with Java 1.8. In the cluster setup, one node is used as the driver while the remaining nine nodes form a SolrCloud cluster. For comparison purposes, we also ran experiments on an individual server.
Note that processors in the medium servers date from 2012 (Sandy Bridge architecture) and the processors in the large server date from 2015 (Broadwell architecture), so there are significant differences in terms of compute power, both compared to each other and compared to current generation hardware.
Document Collections. We use a number of standard IR document collections in our evaluation:
· The New York Times Annotated Corpus, a collection of 1.8 million news article, used in the TREC 2017 Common Core Track.
· Gov2, a web crawl of 25.2 million .gov webpages from early 2004, used in the TREC Terabyte Tracks.
· ClueWeb09b, a web crawl comprising 50.2 million webpages gathered by Carnegie Mellon University in 2009, used in the TREC Web Tracks.
· ClueWeb12-B13, a web crawl comprising 52.3 million webpages gathered by Carnegie Mellon University in 2012 as the successor to ClueWeb09b, also used in the TREC Web Tracks.
· Tweets2013, a collection of 243 million tweets gathered over February and March of 2013, used in the TREC Microblog Tracks [5].
Architectures. We examined a few different architectures, as outlined in Figure 1. In all cases, we built full positional indexes and also store the raw document texts.
· Lucene. The default implementation for Anserini, and our baseline, has a single, shared Lucene IndexWriter for all threads indexing to disk. We set the thread count to be equal to the number of physical CPU cores and use a write buffer of 2GB. This corresponds to Figure 1(a).
· Solr. Anserini is used as a frontend for indexing into a singlenode SolrCloud instance, corresponding to Figure 1(b), as well as a nine node SolrCloud cluster, corresponding to Figure 1(c). In the single-node case, the Anserini frontend and the SolrCloud instance both reside on the same server. In the SolrCloud cluster, the Anserini frontend runs on one of the medium servers while the remaining nine servers each host a single Solr shard. Although strictly not necessary in the single-node case, we nevertheless use SolrCloud to simplify implementation. In both cases we use a dedicated CloudSolrClient for each indexing thread (with one thread per physical CPU core); batch size is set to 500 for ClueWeb09b and 1000 for the other collections (the smaller batch

size for ClueWeb09b is necessary to avoid out-of-memory errors). We set Solr's ramBufferSizeMB to 2GB, matching the Lucene condition, and define a schema to map Anserini fields to the appropriate types in Solr. The performance difference between Lucene and the single-node SolrCloud instance characterizes Solr overhead, and performance comparisons between single-node and multi-node SolrCloud quantifies the speedup achievable with distributed indexing. · Lucene Shard. In this configuration, Anserini builds indexes over 1/9th of each collection. This models the scenario where we separately build indexes over document partitions "locally" and then copy each index to the corresponding server in SolrCloud. We use the same settings as the Lucene configuration above. Comparisons between this condition and a multi-node SolrCloud cluster characterizes the overhead of distributed indexing.
3.2 Results
Table 1 shows indexing performance for the various architectures described above. We report means with standard deviations over three trials for each condition. Note that due to the smaller disks on the medium servers, we were not able to index ClueWeb09b and ClueWeb12-B13 under the single-node condition.
These experiments show that Anserini indexing into Solr has substantial performance costs. In our results table, the "Lucene" vs. "Solr (single-node)" columns quantify the overhead of Solr's application framework--the extra layer of indirection that comes with REST APIs. For small collections, the overhead is modest (and for NYTimes on the large server, Solr is actually faster), but the overhead can be quite substantial for the large collections. For example, on ClueWeb12-B13, indexing into Solr takes almost twice as long as directly writing local indexes.
We can compare "Solr (single-node)" vs. "Solr (multi-node)" to understand the performance characteristics of distributed SolrCloud. Information is limited since we only have a cluster of medium servers, and limited drive capacity prevented comparisons on the ClueWeb collections. Nevertheless, it is clear that we do not achieve linear speedup: perfect scalability implies that we would be able to index the entire collection in 1/9th of the time on a cluster of nine nodes. However, we are pleased with the ease of multi-cluster setups in SolrCloud, since a distributed architecture would be necessary to support query latencies for interactive search on even larger collections (e.g., the full ClueWeb collections).
From these experiments, we discovered that tuning of various configurations (e.g., thread counts, batch sizes, and buffer sizes) has a large impact on performance. For example, on a single node,

1287

Demonstration Papers 1: Interactive IR Systems

SIGIR '19, July 21­25, 2019, Paris, France

we essentially run into a producer­consumer queuing problem: Anserini threads are "producing" documents that are "consumed" by Solr threads. It is difficult to perfectly balance throughput, and thus one side is often blocking, waiting for the other. In our experiments, we have taken reasonable effort to optimize various parameter settings across all our experimental conditions, but have not specifically tuned parameters for individual collections--and thus it is likely that more fine-grained collection-specific tuning can further increase performance. Nevertheless, we believe that our results reasonably reflect the true capabilities of Lucene and Solr in the various configurations, as opposed to performance that has been hobbled due to poor parameter settings.
Comparing "Lucene Shard" vs. "Lucene" on the medium server, we see that indexing 1/9th of the collection does not take 1/9th of the time. This is an artifact of our current implementation, where we still scan the entire collection (i.e., parsing every document) in order to determine which shard a document belongs in. Thus, we encounter substantial document processing overhead in this naïve implementation of Lucene sharding, i.e., nine passes over the collection, building a shard index in each pass. The overall indexing time could be reduced by running the indexing in parallel on each of the servers hosting the Solr shards. Nevertheless, we believe that with an improved implementation, our alternative integration strategy--building indexes for each shard locally and then copying them to the appropriate SolrCloud server--can be viable.
Finally, we note that all our experiments were conducted on magnetic spinning disks. SSDs have very different characteristics, and it would be desirable to replicate these experiments with more modern server configurations.
4 INTERACTIVE SEARCH
An important capability enabled by Anserini/Solr integration is entrée into the rich ecosystem of Solr frontends. In particular, this allows IR researchers to leverage efforts that have been invested in creating Solr-based search interfaces. This would specifically benefit researchers working on interactive IR, who often have the need to create custom search interfaces, to, for example, support user studies. Users have come to expect much from such interfaces, and instead of trying to implement these features from scratch, researchers can reuse existing components.
As a demonstration of Solr's capabilities, we have adapted Blacklight2 as a search interface to Anserini. Blacklight is an open-source Ruby on Rails engine that provides a discovery interface for Solr. It offers a wealth of features, including faceted search and browsing, keyword highlighting, and stable document URLs. The entire system can be customized via standard Rails templating mechanisms. Blacklight has a vibrant developer community and has gained broad adoption in the library and archives space, being deployed at dozens of university libraries and cultural heritage institutions.
Figure 2 shows a screenshot from our custom Blacklight instance, dubbed "Gooselight", searching over the collection of 243 million tweets from the TREC 2013 Microblog Track [5] indexed via our Anserini/Solr integration. Here, we highlight the flexibility of Blacklight by rendering results using Twitter's official API, which shows
2 http://projectblacklight.org

Figure 2: Screenshot of Gooselight, a search interface using Blacklight that connects directly to Anserini/Solr.
media previews and threaded discussions (if available), and provides direct links to the original source and other Twitter "actions". Use of this rendering API also allows our search interface to respect Twitter's terms of service regarding private and deleted tweets.
5 CONCLUSIONS
With Anserini/Solr integration, we argue that it is possible to "have your cake and eat it too". Anserini continues to support the tight "inner loop" of IR research (i.e., model refinement), but now additionally offers the broader capabilities described here.
Acknowledgments. This work was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada, the Canada Foundation for Innovation Leaders Fund, and the Ontario Research Fund.
REFERENCES
[1] M. Abualsaud, N. Ghelani, H. Zhang, M. Smucker, G. Cormack, and M. Grossman. 2018. A System for Efficient High-Recall Retrieval. In SIGIR. 1317­1320.
[2] L. Azzopardi, M. Crane, H. Fang, G. Ingersoll, J. Lin, Y. Moshfeghi, H. Scells, P. Yang, and G. Zuccon. 2017. The Lucene for Information Access and Retrieval Research (LIARR) Workshop at SIGIR 2017. In SIGIR. 1429­1430.
[3] L. Azzopardi, Y. Moshfeghi, M. Halvey, R. Alkhawaldeh, K. Balog, E. Di Buccio, D. Ceccarelli, J. Fernández-Luna, C. Hull, J. Mannix, and S. Palchowdhury. 2017. Lucene4IR: Developing Information Retrieval Evaluation Resources Using Lucene. SIGIR Forum 50, 2 (2017), 58­75.
[4] J. Lin, M. Crane, A. Trotman, J. Callan, I. Chattopadhyaya, J. Foley, G. Ingersoll, C. Macdonald, and S. Vigna. 2016. Toward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In ECIR. 408­420.
[5] J. Lin and M. Efron. 2013. Overview of the TREC-2013 Microblog Track. In TREC. [6] P. Yang, H. Fang, and J. Lin. 2017. Anserini: Enabling the Use of Lucene for
Information Retrieval Research. In SIGIR. 1253­1256. [7] P. Yang, H. Fang, and J. Lin. 2018. Anserini: Reproducible Ranking Baselines Using
Lucene. JDIQ 10, 4 (2018), Article 16. [8] W. Yang, K. Lu, P. Yang, and J. Lin. 2019. Critically Examining the "Neural Hype":
Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models. In SIGIR.

1288

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

Event Tracker: A Text Analytics Platform for Use During Disasters

Charles Thomas
2198970t@student.gla.ac.uk University of Glasgow, UK

Richard McCreadie
richard.mccreadie@glasgow.ac.uk University of Glasgow, UK

Iadh Ounis
iadh.ounis@glasgow.ac.uk University of Glasgow, UK

ABSTRACT
Emergency management organisations currently rely on a wide range of disparate tools and technologies to support the monitoring and management of events during crisis situations. This has a number of disadvantages, in terms of training time for new staff members, reliance on external services, and a lack of integration (and hence poor transfer of information) between those services. On the other hand, Event Tracker is a new solution that aims to provide a unified view of an event, integrating information from emergency response officers, the public (via social media) and also volunteers from around the world. In particular, Event Tracker provides a series of novel functionalities to realise this unified view of the event, namely: real-time identification of critical information, automatic grouping of content by the information needs of response officers, as well as real-time volunteers management and communication. This is supported by an efficient and scalable backend infrastructure designed to ingest and process high-volumes of real-time streaming data with low latency.
CCS CONCEPTS
· Information systems  Decision support systems.
KEYWORDS
Crisis Management, Social Media, Real-time Analytics
ACM Reference Format: Charles Thomas, Richard McCreadie, and Iadh Ounis. 2019. Event Tracker: A Text Analytics Platform for Use During Disasters. In Proceedings of the 42nd International ACM SIGIR Conference onResearch and Development in Information Retrieval (SIGIR '19), July 21­25,2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331406
1 INTRODUCTION
A 68% rise in the use of social networking sites since 2005 [15] has introduced an abundance of real-time information online. This has enabled new ways for the public to contact response agencies [4], and grants those in the Emergency Management sector a new means of accessing potentially life-saving information. With a threefold increase in natural disasters over the past 35 years [11], it is extremely important that emergency response agencies have the tools available to ensure that they can monitor social media streams
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25,2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331406

in real-time and provide assistance to the public in a quick and effective manner.
Data posted on social media platforms regarding a disaster or world events could potentially provide a wide range of valuable information to emergency response services. For example, responding to cries for help, accessing first-hand observations or simply gaining an insight into public opinions could provide an addedvalue to these agencies. However, with the wealth of information now available, it is critical that the data can be easily visualised and navigated, ensuring that emergency services can quickly find and act upon key information.
There also lies potential in the connection of volunteers with formal emergency response officers during crisis situations. Systems designed to assist emergencies services during crises face typically a number of challenges, including a lack of coordination and communication between the officers and other formal entities as well as an unwillingness to engage and form relationships with digital volunteer groups [19]. A successful connection with both physical and digital volunteers could indeed aid response efforts immensely, with a wealth of potential skills and resources becoming available [20].
A range of systems have been developed in the past with the goal to support emergency response efforts during disasters [4]. These systems, however, have had little impact on the sector. It has been found that reasons behind this include the insufficient training of personnel, which require time to make adequate use of the platforms, the lack of guidelines for their use, and apprehension over social media trustworthiness [9].
In this paper, we propose a new system, Event Tracker, which aims to support the monitoring and management of events during disasters and crisis situations. The system intends to support three primary tiers of users - emergency response officers and their teams; physical and digital volunteers; as well as the victims during the crisis. Indeed, we argue that emergency response agencies should be given the tools to navigate large volumes of social media data with ease, supported by functionalities such actionable information feeds and criticality estimation. Moreover, volunteers should be able to provide information directly to these response agencies, either using their first-person knowledge of the situation, or aiding the data navigation and highlighting specific information and relevant details. Finally, victims during a disaster should be able to straightforwardly access advice and information from emergency services, ensuring that they are equipped with up-to-date information that could lead to their safety.
2 RELATED WORK
To-date, there have been a range of techniques and systems proposed to support crisis management. Below we provide a brief overview of key technologies and initiatives that are relevant to our proposed Event Tracker system.

1341

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

Finding Relevant Information: A core function of any crisis management solution is getting key information to the right people. This is operationalised either via volunteer efforts [6] or through automatic technologies [18] for categorising and prioritising reports. Indeed, a survey [5] of approaches identified eight dimensions of categorisation, namely: by information provided/contained; fact vs. subjective vs. emotional content; by information source; by credibility; by time; by location; by embedded links; or by environmental relevance (physical, built or social). Automatic categorisation efforts have focused on supervised learning technologies, often backed by human annotation of crisis data [12]. Building on this work, Event Tracker provides fully automatic real-time reports alerting that are predicted to contain critical and actionable information, as well as state-of-the-art content-based categorisation based on a crisis event ontology (from TREC-IS, which we discuss below).
Crisis Content Processing and TREC-IS: As discussed above, a range of automatic technologies to support crisis management have previously been proposed. However, individually they have had little impact on crisis management as a whole [16]. One reason for this lack of impact is data quality [10]. To avoid a similar fate, Event Tracker's automatic content categorisation service targets a crisis ontology being maintained by an active initiative, namely TREC Incident Streams (http://trecis.org). TREC-IS is a new track at the Text Retrieval Conference (TREC) started in 2018 and designed to unite academia and industry around research into automatically processing social media streams during emergency situations and categorising information and aid requests made on social media for emergency service operators. TREC-IS provides training datasets for training crisis content categorisation systems, as well as an on-going (bi-yearly) evaluation challenge aimed at increasing the TRL of such systems.
Related Systems: Before discussing the design and architecture of Event Tracker, it is worth highlighting some related systems that we either build upon or learn from. First, EAIMS [13], was a prototype crisis management system that aimed at exploiting social media data to provide real-time detection of emergency events (demoed at SIGIR 2016), along with search, sentiment analysis, discussion-thread extraction and summarisation functionalities. The main drawback of EAIMS is that it was primarily designed only for use by high-level, head-office, emergency response staff, in contrast to Event Tracker that also targets volunteer groups. Twitter Moments is a relatively new feature of the Twitter platform, added in 2015. A `Moment' is a user curated collection of Tweets, which allows users to comment on stories, promote news or create round-ups of different world events [2]. Twitter Moments has great potential for providing a highlevel overview of an event. However, a key downfall is that Moments are popularity-driven, so local content such as individual calls for help will be missed. Instead, Event Tracker relies on recall-focused crawling in conjunction to automatic content categorisation to find actionable information quickly and accurately. Furthermore, AIDR is a platform developed at the Qatar Computing Research Institute that collects and classifies crisis-related Tweets [12]. AIDR makes use of crowdsourcing to label Tweets, which can then be used to train an automatic classifier. This ensures that the system performs well for each new event registered, since automatic classification

using only pre-trained models can lead to low accuracy. Currently, Event Tracker relies on pre-trained classification models, however its underlying microservice architecture is flexible and allows for the integration of additional services like AIDR. Finally, Twitcident is another web-based system for filtering and analysing social media data on real-world incidents, such as disasters. Similar to EAIMS, Twitcident [1] provides incident detection and search technologies, enabling emergency services to filter data to aid their response efforts. However, a relatively recent study showed that Twitcident did not provide much value to response agencies in practice, as it could not provide early warnings regarding critical information [3]. For this reason, Event Tracker integrates criticality estimation tools to provide real-time reporting of crucial information.
3 EVENT TRACKER ARCHITECTURE
As argued earlier, the goal of Event Tracker is to provide an integrated platform that provides both automated low-latency ingestion and augmentation of report streams (either manually entered or crawled from social media) with effective support tools that enable response officers and volunteers to collaborate together to generate a unified operational picture during an emergency. In effect, this means that the platform must support: 1) low-level support for different report streams (news reports, social media and manual form-filling by call-centre operators), 2) integration of fast text processing technologies for real-time report tagging, as well as both 3) classical on-request information access (search and content look-up) and 4) continuous push notification servicing.
As such, Event Tracker uses a flexible architecture as illustrated in Figure 2. The lowest ingestion layer provides multi-stream integration, using a common data back-end to [13]. Above this sits the augmentation layer, which enables modular and scalable integration of report tagging microservices (in this case for identification of actionable information and information criticality analysis) using a distributed Apache Flink back-end. The output of the augmentation layer is then processed by the activity layer, that handles the `business logic' for the application (data and user management), feeding either persistent storage structures (a search index and database) and/or the front-end directly via push notifications. Indeed, during a disaster event, the volume and rate at which relevant content that needs to be processed may vary greatly (anywhere from 10 posts/min to 4,000 posts/min [7]). Thus, the architecture behind the Event Tracker is designed to handle high-volume streams of data.
4 KEY FUNCTIONALITIES
Event Tracker integrates a number of functionalities following the design vision discussed earlier. Below, we summarise these main functionalities.
Generating Actionable Information Feeds: As discussed above, getting key information to the right people is crucial for a successful crisis management system, and one of the main reasons why previous systems have failed. Event Tracker combats this by providing multiple actionable information feeds through user dashboards, each of which is designed with a different intention of the value it can provide to the response agencies. Figure 1 (a) pictures the Twitter Feed, a real-time, filterable collection of all data gathered across the duration of a given crisis event. The Media Feed, displayed in

1342

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

(b) Event Map

(d) Media Feed

(a) Twitter Feed

(c) Critical Feed

(e) Event Tracker Feed

Figure 1: A selection of different functionalities available on the dashboards on Event Tracker

Interaction Layer
Content Search Information Feeds Critical Alerts Response Groups Crisis Mapping

Storage Layer

Activity Layer
Actor System

Augmentation Layer

Augmentation
Criticality Estimation

Layer Actionable
Information Identification

Ingestion Layer
Unified Ingestion API

Live Feeds

Data Entry

Social Media

Newswire

Call Centre

C&C Terminal

Figure 2: Conceptual architecture

Figure 1 (d), extracts all media collected, enabling response officers to, for example, assess damage during and after a disaster. Automatic content-based categorisation, through the TREC-IS initiative, examined in Section 2, is exploited by Event Tracker to provide emergency response agencies with a set of Tweet Category Feeds, in an attempt to increase how quickly and accurately these agencies can navigate the collection of data to find actionable information. Different agencies with different motivations can customise the set of feeds to distinctive categories such as `First Party Observation' or `Official Reports'. Each feed on Event Tracker aims to improve the situational awareness of the agencies, which is vital in effective decision making in complex and dynamic environments [8].
Criticality Alerting: Early warnings and alerts regarding critical information posted online during a crisis situation is imperative for a quick response from emergency services. Aid requests could be

answered quickly, increasing for example the likelihood that a victim could be assisted in time. Event Tracker aims to accomplish this by also making use of the TREC-IS initiative to provide real-time identification of critical information. Fast classification and text processing technologies are exploited to immediately label incoming messages with a criticality score of low, medium, high, or critical, which can then be displayed alongside Tweets on a user's dashboard. Continuous push notifications servicing is also harnessed by Event Tracker to populate the Critical Feed, pictured in Figure 1 (c), to highlight reports which are predicted to contain critical and immediately actionable information.
Communicating Amongst Response Associations & Groups: To support crisis management, there are a large number of associations that could help monitor and manage a crisis (from the Red Cross to smaller Regional Volunteer Groups). Each event being tracked on the system can be related to multiple response associations, each of which may provide support in different manners. These associations can communicate through Event Tracker, using the Event Tracker Feed (Figure 1 (e)). Members of the response teams can use this feed to communicate directly with other response agencies, passing on any relevant information. As discussed, there has been a lack of successful systems that integrate formal response agencies with volunteers, and so for each event registered on Event Tracker, an open volunteering group is created, which anyone can join. The system can be used to increase the coordination and communication between these entities, with both volunteers on the ground reporting information, or digital volunteers highlighting critical reports to official response officers by embedding Tweets into their posts.
Additional Functionalities: Along with these key functionalities, Event Tracker provides some other features which are worth mentioning. First, we describe the Crisis mapping functionality.Figure 1 (b) pictures the Event Map module on Event Tracker, which displays both the location of the related event and the geo-tagged Tweets that have been collected over the duration of the crisis. As the information feeds, this aids in increasing the situational awareness of the

1343

Demonstration Papers 3: Applications

SIGIR '19, July 21­25, 2019, Paris, France

response agencies. The module could be extended in the future to display the locations of other reports, to improve data management. Next, we describe the On-request information access functionality. Making use of the Terrier search engine [14], Event Tracker allows users to explore the collected data with ease. New feeds are created on the user's dashboard, enabling multiple searches to run concurrently. For example, emergency response agencies can make use of this feature to discover reports relating to specific queries, such as `fire'.
Figure 3: Potential Tweet Category feeds on user dashboards
5 ILLUSTRATIVE USE CASES
Focussing on multiple types of users during a crisis event, Event Tracker has several use cases. To illustrate the working of Event Tracker, let's consider two possible user cases. In the first use case, consider an emergency response officer, who uses the system to view all the actionable information feeds and navigate the incoming reports of data. During a simulation of the 2013 Australia Bushfire crisis, Event Tracker marked the Tweet "Horrid emergency unfolding at #faulconbridge - fire is 50m behind #springwood shs - near hundreds of homes, norman lindsay gallery #nswfires" as a high criticality emerging threat. Had Event Tracker been deployed over this event, a response officer accessing this report could have contributed an effective response. During an event, these officers may be monitoring multiple communication channels [17], which may lead to sporadic focus on each of the information feeds. As a consequence, some important information about tnhe event might be missed. Instead, with Event Tracker, the criticality alerting component would notify the user with any new reports that are predicted to be of vital importance, ensuring that the end-user can act quickly and efficiently. As a second case, consider a volunteer on the ground during a disaster. They could make use of Event Tracker to coordinate with response agencies to ensure that they are providing the best help they can. They can also make reports for the response agencies, to bring any significant information they possess to the attention of the response officers.
6 CONCLUSIONS
In this paper, we have presented Event Tracker, a modular and extensible prototype system designed to support the monitoring

and management of events during crisis situations. The system leverages a flexible architecture, which is designed for low latency, and high-volume streams of data to provide functionalities such as actionable information feeds and criticality alerting. Event Tracker also enables the communication between various emergency response agencies and volunteers, and provides additional functionalities such as crisis mapping and on-request information access. In the short term, we aim to make Event Tracker available to the participants of the TREC-IS track. In particular, due to the modular augmentation layer Event Tracker is built upon, microservices such as those proposed by different groups participating to the TREC-IS track could be integrated and deployed, enabling different demonstrations to the end-users, whereby the corresponding outcomes and effectiveness of different technologies can be evaluated and further investigated.

REFERENCES

[1] Fabian Abel, Claudia Hauff, Geert-Jan Houben, Richard Stronkman, and Ke

Tao. 2012. Twitcident: Fighting fire with information from Social Web streams.

WWW'12 - Proceedings of the 21st Annual Conference on World Wide Web Com-

panion (04 2012). https://doi.org/10.1145/2187980.2188035

[2] Liz Alton. 2018.

Everything you need to know about

Twitter Moments.

https://business.twitter.com/en/blog/

Everything-you-need-to-know-about-Twitter-Moments.html. Accessed:

09.02.2019.

[3] Kees Boersma, Dominique Diks, Julie Ferguson, and Jeroen Wolbers. 2016. From

Reactive to Proactive Use of Social Media in Emergency Response: A Critical Discus-

sion of the Twitcident Project. https://doi.org/10.4018/978-1-4666-9867-3.ch014

[4] Carlos Castillo. 2016. Big Crisis Data: Social Media in Disasters and Time-

Critical Situations. Cambridge University Press. https://doi.org/10.1017/

CBO9781316476840

[5] Carlos Castillo. 2016. Big crisis data: social media in disasters and time-critical

situations. Cambridge University Press.

[6] Lise Ann St Denis, Amanda L Hughes, and Leysia Palen. 2012. Trial by fire:

The deployment of trusted digital volunteers in the 2011 shadow lake fire. In

Proceedings of ISCRAM.

[7] R. McCreadie et al. 2016. D4.7 - Integrated Search over Social Media. Deliverable,

SUPER FP7 Project (2016).

[8] J. Harrald and T. Jefferson. 2007. Shared Situational Awareness in Emergency

Management Mitigation and Response. In 2007 40th Annual Hawaii International

Conference on System Sciences (HICSS'07). 23­23. https://doi.org/10.1109/HICSS.

2007.481

[9] Starr Hiltz, Jane Kushma, and Linda Plotnick. 2014. Use of Social Media by U.S.

Public Sector Emergency Managers: Barriers and Wish Lists. https://doi.org/10.

13140/2.1.3122.4005

[10] Starr Roxanne Hiltz, Jane A Kushma, and Linda Plotnick. 2014. Use of Social

Media by US Public Sector Emergency Managers: Barriers and Wish Lists.. In

Proceedings of ISCRAM.

[11] Peter Hoeppe. 2015. Trends in weather related disasters - Consequences for

insurers and society. (2015).

[12] Muhammad Imran, Carlos Castillo, Ji Lucas, Patrick Meier, and Sarah Vieweg. [n.

d.]. AIDR: Artificial intelligence for disaster response. In Proceedings of WWW.

[13] Richard McCreadie, Craig Macdonald, and Iadh Ounis. 2016. EAIMS: Emergency

Analysis Identification and Management System. https://doi.org/10.1145/2911451.

2911460

[14] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig MacDonald, and

Christina Lioma. 2006. Terrier : A High Performance and Scalable Information

Retrieval Platform. In Proceedings of OSIR'2006.

[15] Andrew Perrin. 2015. Social Media Usage: 2005-2015. Accessed: 16.02.2019.

[16] C Reuter, G Backfried, MA Kaufhold, and F Spahr. 2018. ISCRAM turns 15: A

Trend Analysis of Social Media Papers 2004-2017. In Proceedings of ISCRAM.

[17] Andrea H Tapia, Kathleen A Moore, and Nichloas J Johnson. 2013. Beyond the

trustworthy tweet: A deeper understanding of microblogged data use by disaster

response and humanitarian relief organizations.. In Proceedings of ISCRAM.

[18] Marie Truelove, Maria Vasardani, and Stephan Winter. 2015. Towards credibility

of micro-blogs: characterising witness accounts. GeoJournal (2015).

[19] J Twigg and Irina Mosel. 2017. Emergent groups and spontaneous volun-

teers in urban disaster response. Environment and Urbanization 29 (08 2017),

095624781772141. https://doi.org/10.1177/0956247817721413

[20] Joshua Whittaker, Blythe McLennan, and John Handmer. 2015. A review of

informal volunteerism in emergencies and disasters: Definition, opportunities

and challenges. International Journal of Disaster Risk Reduction 13 (2015), 358 ­

368. https://doi.org/10.1016/j.ijdrr.2015.07.010

1344

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

Improving the Accuracy of System Performance Estimation by Using Shards

Nicola Ferro
ferro@dei.unipd.it Dept. of Information Engineering, University of Padua
Padua, Italy
ABSTRACT
We improve the measurement accuracy of retrieval system performance by better modeling the noise present in test collection scores. Our technique draws its inspiration from two approaches: one, which exploits the variable measurement accuracy of topics; the other, which randomly splits document collections into shards. We describe and theoretically analyze an ANalysis Of VAriance (ANOVA) model able to capture the effects of topics, systems, and document shards as well as their interactions. Using multiple TREC collections, we empirically confirm theoretical results in terms of improved estimation accuracy and robustness of found significant differences. The improvements compared to widely used test collection measurement techniques are substantial. We speculate that our technique works because we do not assume that the topics of a test collection measure performance equally.
CCS CONCEPTS
· Information systems  Test collections; Retrieval effectiveness;
KEYWORDS
effectiveness model; ANOVA; multiple comparison
ACM Reference Format: Nicola Ferro and Mark Sanderson. 2019. Improving the Accuracy of System Performance Estimation by Using Shards. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.3338062
1 INTRODUCTION
Measuring the difference in performance between two Information Retrieval (IR) systems using an offline test collection has long been recognized as noisy. Attempts to improve the accuracy of such measurement are extensive and diverse. Techniques explored include multiple evaluation measures; different significance tests; alternate acquisitions of relevance judgments; and determining the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3338062

Mark Sanderson
mark.sanderson@rmit.edu.au Computer Science, School of Science, RMIT University
Melbourne, Australia
ideal number of topics. Surveys [30] and descriptions of best practice [29] detail such attempts. There are, however, less explored approaches to improving performance measurement accuracy.
Robertson and Kanoulas [26] pointed out a common assumption in the use of test collections namely "all topics are considered equally valuable". They examined this assumption by measuring (via bootstrapping) the confidence intervals of each topic score and of each system. The intervals were found to be variable across topics but largely independent of system. The researchers concluded that some topics measure performance more accurately than others.
Ferro and Sanderson [11] examined splitting the documents of a test collection into shards, measuring the performance of systems on each shard. They used an ANOVA model to understand if system performance changed across shards. The authors mentioned that significant differences between systems on sharded collections were more common than on unsharded. However, the reasons for the result was not explored as the experiment was designed to address a different research question. Voorhees et al. [39] randomly split a collection in half. The authors stated that the two resulting shards allowed more accurate performance measurement. However, it was reported that splitting the collection further did not improve accuracy; reasons for no improvement were not examined in detail.
We describe research that takes the Robertson and Kanoulas view that topics have unequal value and combines it with the ANOVA approach of Ferro and Sanderson [11] and the sharding method of Voorhees et al. [39]. We ask: Can the unequal value of topics be exploited to improve measurement of system performance accuracy on a test collection? We make the following contributions:
· We validate an ANOVA model via a theoretical examination, showing why explicitly accounting for differences across topics yields accuracy improvements.
· We experimentally show that the model identifies notable numbers of significant differences between systems.
· We experimentally show that the differences are not due to measurement error of the significance formulas.
Next, related work is described followed by ANOVA models and their properties. The setup and report of experimental findings are described before conclusions and future work are detailed.
2 RELATED WORK
We review three research areas: topics with few relevance judgements, ANOVA modeling, and the sharding of collections.
2.1 Topics with few relevance judgments
There is an assumption, in test collection based evaluation, that all topics are valuable equally. Performance is measured by taking the

805

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

arithmetic mean of topics scores. Swanson [33] described such a process in 1960. When the mean is taken, each topic score contributes equally regardless of the accuracy of that measure. The potential for error was described by Voorhees [36]: "When [topics] have very few relevant documents (fewer than five or so), summary evaluation measures such as average precision are themselves unstable; tests that include many such queries are more variable". Soboroff [32] pointed out that rank cut off evaluation measures (e.g. precision at 10) will have an upper bound < 1 for topics with few relevant documents.
The notion that not all topics have equal value was implicitly exploited in work identifying a subset of test collection topics that rank systems similarly to a full topic set [13, 19]. To the best of our knowledge, however, Cormack and Lynam [9] were the first to incorporate an unequal view of topics into test collection measurement. They treated each topic as a "separate test", calculating topic confidence intervals using a bootstrap approach. Topics with  5 relevant documents were subject to a "Small-R Correction" to overcome measurement instability.
Robertson [25] considered the broader question of what is the "per-topic noise or error" present in the topics of a test collection. The paper considered if evaluation measures could be adapted to cope with an unequal view of topics. Later, Robertson and Kanoulas [26] measured the variance of topic scores by bootstrapping from the document collection. The researchers found that topics showed different levels of variance, but the variance was relatively consistent across systems. The researchers described a significance test that incorporated topic score variation. Comparisons between the new test and the commonly used t-test showed some differences in the conclusions one might draw when comparing systems.
More recently, Yang et al. [41] examined how much rankings of systems were affected by per-topic score variance and if there was any impact on significance tests. They found that the variance did not affect overall rankings notably, but that the number of significant differences observed between systems dropped.
Note, there is much research on subjects such as query difficulty prediction [42], topic score normalization [40], average average precision [20], GMAP [24], etc. Such work focuses on so-called difficult topics, we focus on topics for which measurement is variable.
2.2 ANOVA modeling
ANOVA can decompose the data of an IR experiment into a model of factors, into interactions between those factors, and into a level of unmodeled error. Tague-Sutcliffe and Blustein [34] described an example of this approach by comparing the variation in performance across two factors: topics and systems. The former was found to be larger than the latter. Measurement of interaction between topics and systems was not possible owing to a lack of replicates of topic*system measures. Banks et al. [2] approximated such an interaction, suggesting it would be strong and significant. Later, Bodoff and Li [3] used a test collection with multiple relevance assessments to obtain the required replicates. The authors reported that the magnitude of the topic*system interaction factor was less than the topic factor, but greater than the system factor.
Both Ferro and Sanderson [11] and Voorhees et al. [39] generated replicates by sharding a document collection. This enabled them to measure the topic*system effect. We describe that work next.

2.3 Sharding
Voorhees et al. [39] used a bootstrap ANOVA approach that drew on a sample of the scores of topics measured across different systems and shards. The researchers tested on the TREC-3, TREC-8, and 2006 Terabyte track collections. Success of the approach was measured by counting the number of significant differences found between systems submitted to TREC tracks. The researchers found substantially more such differences were measured than with conventional approaches. Two shards were used. When three or five shards were tried, the researchers found the number of significant differences dropped, the reasons for which were not examined in detail. The relative impact of each component of the technique ­ bootstrap ANOVA, the approach to multiple comparisons, and sharding method ­ was not described.
As part of a study on the interaction between different types of shards and system scores, Ferro and Sanderson [11] described a series of ANOVA models tested on the TREC-7 and TREC-8 adhoc test collections. Like the previous research, the value of these models was quantified by the number of significant differences measured between systems. The researchers showed that a more sophisticated ANOVA model produced the highest number of significant differences measured between systems. However, the shards were very skewed in size.
The research described shows that the topics of test collections can produce scores of different variance, which can impact the measurement of significance between systems. There is, as yet, not an extensive body of research examining such topic variability. Most work has explored bootstrap approaches from document collections to assess the variance. The recent examination of sharding has not been explored in conjunction with the work on topic variability. We explore the connection between these two lines of inquiry examining the style of ANOVA modeling used by Ferro and Sanderson [11]. We also measure the accuracy of the model across a range of sharding configurations that have not been examined before.

3 METHODOLOGY
Suppose we have T topics, R systems, and S shards and thus N = T ·R ·S total samples. We can form the following six ANOVA models:

yij = µ ·· + i + j + ij

(MD1)

Main Effects
yijk = µ ··· + i + j + ijk

(MD2)

Main Effects
yijk = µ ··· + i + j +

(  )i j

+ ijk

(MD3)

Main Effects Interaction Effects
yi jk = µ ··· + i + j + k + (  )i j + i jk

(MD4)

Main Effects Interaction Effects
yi jk = µ ··· + i + j + k + (  )i j + ( )jk + i jk

(MD5)

Main Effects Interaction Effects
yi jk = µ ··· + i + j + k + (  )i j + ( )ik + ( )jk + i jk

Main Effects

Interaction Effects

(MD6)

806

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

Where:
· yijk is the performance score of three factors, the i-th topic (i = 1, . . . ,T ) retrieving on the j-th system (j = 1, . . . , R) from the k-th shard (k = 1, . . . , S);
· µ ··· is the grand mean; · i = µi ·· - µ ··· is the effect of the i-th topic, where µi ·· is the
marginal mean of the topic; · j = µ ·j · - µ ··· is the effect of the j-th system, where µ ·j · is
the marginal mean of the system; · k = µ ··k - µ ··· is the effect of the k-th shard, where µ ··k is
the marginal mean of the shard; · (  )i j = µi j · - µi ·· - µ ·j · + µ ··· is the interaction between
topics and systems, where µij · is the marginal mean of the interaction between the i-th topic and j-th system; · ( )ik = µi ·k - µi ·· - µ ··k + µ ··· is the interaction between topics and shards, where µi ·k is the marginal mean of the interaction between the i-th topic and k-th shard; · ( )jk = µ ·jk - µ ·j · - µ ··k + µ ··· is the interaction between systems and shards, where µ ·jk is the marginal mean of the interaction between the j-th system and k-th shard; and · ijk is the error of the model in predicting yijk .
Model (MD1) was used by Tague-Sutcliffe and Blustein [34]
and Banks et al. [2]. It can be viewed as a classic approach to
measuring significance on a test collection, as in this form, it is
operationally similar to a t-test. The model components have two subscripts (i, j) because the collection does not have shards. Model (MD2) is model (MD1) but with shards. While model (MD1) has only
one performance score for each (topic, system) pair, in model (MD2)
the shards provide replicates scores for the pairs when estimating
the model parameters.
The presence of replicates is exploited in model (MD3) by adding
a topic*system interaction factor. Model (MD3) was used by Robert-
son and Kanoulas [26] and Voorhees et al. [39], though Voorhees
et al. did not rely on classical ANOVA, instead adopting a bootstrap
approach [10]. Model (MD4) explicitly accounts for a shard factor
and model (MD5) adds the interaction between systems and shards.
Both models are close to models proposed by Ferro and Sanderson
[11], but they omitted the topic*system interaction in their models.
Model (MD6) adds a topic*shard interaction, by leveraging the
presence of more replicates for each (topics, shard) pair ­ there are as many replicates as the number of used systems R. It is the focus on our work here1.
3.1 Exploiting topic variability with the model
How does improved measurement accuracy arise from a more so-
phisticated ANOVA model applied over a test collection whose
documents are randomly split into shards? Models add more factors with the goal of better fitting the data. Since the total Sum of Squares (SS) is the same for all models, each new factor should explain a further part of the total SS. As a consequence, there is a
reduction of the error SS, i.e. the leftover unexplained by the model,
and, broadly speaking, this leads to a more accurate estimate.
1We have also examined different sharding approaches and how they impact the effect size of ANOVA model factors [12]. That paper does not examine in detail the impact of the model on significance tests.

How does model (MD6) exploit the variable measurement of topics? With random even sized shards, the probability of having relevant documents in a shard is uniform across the shards. This probability is smaller for topics with fewer relevant documents and greater for topics with more relevant documents. Therefore, for each topic, the number of shards without any relevant documents is proportional to the number of relevant documents for that topic. Model (MD6) accounts for this by explicitly considering ( )ik , i.e. the topic*shard interaction effect. When there are no relevant documents for a topic on a given shard, we set the score to undefined for all the systems with respect to that topic on that shard. The more shards without relevant documents for a topic, the more undefined values there are, which is reflected in the estimation of the ( )ik factor. Therefore, the estimation of the SS of the topic*shard interaction factor directly removes from the total SS the variability due to these intrinsic differences among topics, reducing the error SS and giving us the possibility of a more accurate estimation of the differences among systems. Instead of seeing shards as a mere "technical trick" to obtain replicates, we can look at them as a form of "diagnostic tool", which allows us to systematically probe measurement differences across topics and to account for the differences in a model.
We next consider a series of questions about model (MD6):
· How do different models affect the significant differences among systems, accounting for multiple comparisons?
· How do we compute confidence intervals from the model? · How do we estimate effect size? · Is it legitimate to use undefined values?
The following sections will answer the questions by showing that model (MD6) provides benefits in all these areas and, most importantly, makes estimations concerning the system factor independent of undefined values due to the sharding process.

4 MULTIPLE COMPARISONS

If one simultaneously compares multiple system pairs, the proba-
bility of committing a Type I error increases and the Family-wise Error Rate (FWER) (the probability of committing at least one Type I error) is FWER = 1 - (1 - )c , where c is the total number of comparisons to be performed [15, pp. 7­8]. It is crucial to control
Type I errors when performing multiple comparisons [4, 6, 29].
Tukey [35] proposed the Honestly Significant Difference (HSD) test, which creates confidence intervals for all pairwise differences between factor levels, while controlling the FWER. Two systems u and v are considered significantly different when:

|tk |

=

|µ^·u · - µ^·v · |
M Se r r or

>

Q

 R,d

fe

r

r

or

(1)

T ·S

where: µ^·u · and µ^·v · are the marginal means of the systems u and v as estimated from the actual data; d ferror are the Degrees of Freedom (DF) of the error; MSerror is the Mean Squares (MS) of

the error, i.e. an estimation of the variance left unexplained; and

Q

 R

,d

fe

r

r

or

is

the

upper

100  (1 - )-th

percentile

of

the

studentized

range distribution [22]. Note, that in the case of the model (MD1)

the denominator of eq. (1) becomes just T , since the whole corpus

is constituted by a single shard and thus S = 1.

807

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

R = 5 systems; df error = 100; Q5, 100 = 3.93 R = 5 systems; df error = 500; Q5, 500 = 3.86 R = 25 systems; df error = 100; Q25, 100 = 5.32 R = 25 systems; df error = 500; Q25, 500 = 5.17 R = 75 systems; df error = 100; Q75, 100 = 6.12 R = 75 systems; df error = 500; Q75, 500 = 5.91
Figure 1: Studentized range distribution QR,dferror for different numbers of systems to be compared and different degrees of freedom of the error. The lines in each plot corresponds to different DF of the error QR,dferror for  = 0.05: red lines are for 100 DF, blu lines are for 500 DF. Solid lines are for R = 5 systems; dashed lines are for R = 25 systems; and, dotted lines are for R = 75 systems.

Figure 1 shows the Cumulative Density Function (CDF) of the

Studentized range distribution for different numbers of compared

systems and different values of the DF of the error. The DF lines are almost superimposed on each other. The values of QR,dferror are equal, apart from the lower values of DF where they are marginally

different. The main difference across plots is that increasing the

number of systems to be compared shifts the CDF to the right.

In a typical IR setting where R systems are compared, the factor

Q

 R

,d

fe

r

r

or

in eq. (1) is practically constant. As a consequence, even

if models from (MD1) to (MD6) lead to different values d ferror ,

the

models "see"

the

same

value

of

Q

 R,d

fer

r

or

and, therefore, the

size of the interval needed to consider two systems as significantly

different mostly depends on the factor

M Se r r or T ·S

.

In models (MD2) to (MD6), the marginal means µ^·u · and µ^·v ·

of the compared systems are the same as well as the T · S factor;

therefore, differences in the size of the intervals are due only to the MSerror factor. Since the typical benefit of having richer models
is to reduce the size of the error, we expect MSerror to decrease2 and, consequently, the test statistic |tk | increases, allowing us to

detect more significant differences. The increasingly richer models

lead to a more accurate estimate of the actual differences among
systems. Moreover, the MSerror is further divided by T · S, which suggests that, for a given number of topics T , increasing the number of shards S should provide further benefits.
The test statistic |tk | allows us to compute the p-value

p = P QR,dferror  |tk |

(2)

of observing a more extreme value of the Studentized range distri-
bution. We can then compare this p-value to the desired significance level  and, if it is  , the two systems u and v are significantly dif-

ferent, still controlling the FWER. Eqs. (1) and (2) are two equivalent

ways to perform multiple comparisons controlling the FWER.

2
Strictly, the SS of the error decreases because the additional factors in a model explain

more of the total SS, leaving less to the SS of the error. However, M Ser r or

=

SSer ror dfer r or

,

if a richer model causes a drop in d fer r or , this decreased denominator may lead to a

greater M Ser r or , even if S Ser r or is decreased. However, as a first approximation, it

is enough to consider both quantities as decreasing as we add factors to a model.

5 CONFIDENCE INTERVALS
We consider three types of confidence interval.

5.1 Tukey

The Tukey HSD test of eq. (1) allows us to define exact confidence intervals for the system main effects, still controlling the FWER. Hochberg and Tamhane [15] suggest creating a half-width confidence interval around the marginal mean of a system u

µ^·u ·

±

1 2

Q

 R

,d

fe

r

r

or

MSe r r or T ·S

(3)

Systems u and v are significantly different, according to the Tukey

HSD test of eq. (1), if and only if their confidence intervals of eq. (3)

do not overlap [15, p. 116]. From model (MD2) to (MD6), we expect that confidence intervals will reduce as MSerror decreases.

5.2 Standard Error of the Mean
The confidence interval of eq. (3) differs from the typical confidence interval based on the Standard Error of the Mean (SEM):

µ^·u · ± tT/·2S -1

^u2 T ·S

(4)

where

^u2

=

T

1 ·S -1

T i =1

S k =1

(yiuk

-

µ^·u ·)2

is

the

sample

variance

of the u-th system and tT/·2S-1 is the upper 100 1-/2 -th percentile

of the Student's t distribution with T ·S -1 degrees of freedom. Note,

these are the confidence intervals used by Ferro and Sanderson [11]

when showing the improved accuracy due to the use of shards.

Differently from the confidence interval of eq. (3), those of eq. (4)

do not depend on any of the more accurate ANOVA models, they

just depend on the underlying data. Moreover, they do not account

for any multiple comparison adjustment since they consider each

system in isolation. While the confidence intervals of eq. (3) have

the same size for all systems as they need to control for FWER, the

confidence intervals of eq. (4) change size from system to system

as they depend on the sample variance of each system.

5.3 ANOVA

We can define the following confidence interval [29, p. 57], which falls between those of eq. (3) and those of eq. (4)

µ^·u · ± td/f2er r or

MSe r r or T ·S

(5)

As with eq. (3), the interval depends on the ANOVA model and

its ability to explain the data. As with eq. (4), the interval does not

adjust for multiple comparisons. Different from eq. (4) but similar to

eq. (3), the interval has the same size for all systems. As above, the term td/f2error is practically constant, following the discussion about eq. (3), we expect the confidence interval of eq. (5) to reduce either

as the ANOVA models become richer or if we use more shards.

The difference between eq. (3) and eq. (5) is the replacement of

1 2

QR

,d

fe

r

r

o

r

with td/f2error . The former is typically 2-3 times bigger

than the latter. The bigger the difference, the bigger the number of

systems R to be compared. This lets us understand the magnitude

of adjustment needed to keep the FWER controlled. Consequently,

the confidence intervals of eq. (3) are bigger than those of eq. (5).

808

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

<latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>
Shard

1 System

1 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

2 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

3 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

1 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

2 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y211 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y221 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y231 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

3 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y311 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y321 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y331 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

4 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

<latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>
Shard

2 System

1 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

2 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

3 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

1 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y112 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y122 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y132 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

2 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y212 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y222 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

y232 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

3 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

4 <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

x <latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit><latexitsha1_base64="(null)">(null)</latexit>

Topic Topic

Figure 2: Example of T = 4 topics, R = 3 systems, S = 2 shards

6 EFFECT SIZE

We also consider the effect size of a factor, which accounts for the amount of variance explained by the model, by means of an

unbiased estimator [23, 28]:

^

2 f

ac

t



=

dff act (Ff act - 1) dff act (Ff act - 1) + N

(6)

where Ff act is the F-statistic and dff act are the degrees of freedom for the factor while N is the total number of samples. The common

rule

of

thumb

[27]

when

classifying

^

2 f

act



effect

size

is:

0.14

and

above is a large size effect; 0.06­0.14 is a medium size effect; and

0.01­0.06 is a small size effect. Note, ^ 2f act  can be negative, in

such cases it is considered as zero.

7 EFFECT OF UNDEFINED VALUES

A notable challenge with sharding a document collection is topics
may not have any relevant documents in a shard. Ferro and Sander-
son [11] dealt with this by keeping only the topics for which there
was at least one relevant document in each shard, thus reducing
the number of usable topics. Voorhees et al. [39] resampled shards
until all shards contained relevant documents for all the topics.
However, this introduces bias since the shards stop being random.
Both approaches fail as the number of shards increase. As described above, we substitute an undefined value. We demon-
strate that we can substitute undefined values with any value x and these values do not affect the identification of significant dif-
ferences, the calculation of confidence intervals, and the effect size
of the system factor. We report here the main propositions but, for
space reasons, we cannot report the corresponding proofs. Detailed
proofs are reported in the electronic appendix available online as
supplementary material to the paper. In the example of Figure 2 we have T = 4 topics, R = 3 systems
and S = 2 shards. Topic 1 has no relevant documents in shard 1 and, therefore, all the systems have the undefined value x for that topic. Similarly, topic 3 has no relevant documents in shard 2 and topic 4 has no relevant in 1 and 2. Note, when relevant documents are missing, a whole "row" is filled in with x. This regularity, allows us to achieve a balanced design where the comparison of systems is independent of the undefined values.

Definition 7.1. Given a shard k  [1, S], Xk is the set of the indexes i of the topics that have no relevant documents on that

shard:

Xk = i  [1,T ] yijk = x j  [1, R]

(7)

In Figure 2, we have X1 = {1, 4} and X2 = {3, 4}. Note that, for

any shard k, there are |Xk | · R undefined values and, in total, there

are R

S k =1

|Xk

|

undefined

values.

Proposition 7.2. Given models from (MD2) to (MD6) and a system j  [1, R], its estimated marginal mean is given by:

ST
1

xS

µ^·j ·

=

T

·S

k =1

i =1

yi j k

+T

·S

|Xk |
k =1

(8)

i Xk

µ^·j ·
Therefore, for any pair of systems u  [1, R] and v  [1, R], u v, the difference of their estimated marginal means µ^·u · - µ^·v · is independent of the undefined values.

Note,

that

the

first

element

µ^

 ·j

·

of

eq.

(8)

is

the

estimated

marginal

mean of the system factor ignoring the undefined values. This is

not the estimated marginal mean removing undefined values, since

the denominator T · S still accounts for all the values, both defined

and undefined. The second element is the contribution to estimated

marginal mean due only to the undefined values. It is constant and

equal for all the systems. Therefore, the regularity in the pattern

of undefined values allows us to separate the contributions due to the systems from those due to undefined values, which are the

same for all the systems. Proposition 7.2 has three consequences:

(1) The numerator of eq. (1), i.e. the multiple comparison among

systems, is not affected by the undefined values. (2) Eq. (8) shows that the shift due to undefined values is the

same for all the systems and, therefore, does not affect the
Rankings of Systems (RoS), i.e. the ordering of the system by their estimated marginal mean. If a Kendall's  corre-

lation [17] was measured between the RoS on the whole
corpus and the RoS when using shards,  is not affected by the undefined values. (3) For each shard we could have at worst |Xk | = T , i.e. a shard for which no topic has relevant documents. However, test

collections generally have at least one relevant document

for each topic and, since shards are a partition of the whole

corpus,

it

follows

that

|Xk |

<

T.

Therefore

1 T ·S

S k =1

|Xk

|

is

always strictly < 1. The effect of the undefined value is to

shift the estimated marginal mean of the system factor by

a fraction of that undefined value. From this perspective, setting x = 0, our choice in the experimentation, is not

lowering the mean system performance but just leaving

them at their level.

Proposition 7.3. Given models from (MD2) to (MD6), the SS of the system factor and, as a consequence, the MS of the system factor are independent of the undefined values.
Proposition 7.4. Given model (MD6), the residuals ijk are independent from the undefined values. Therefore, the SS of the error and, as a consequence, the MS of the error are independent of the undefined values.

Note that Proposition 7.4 holds only in the case of model (MD6) and only thanks to the topic*shard interaction ( )ik factor.

809

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

Indeed, as shown in the appendix, all the estimated marginal
means have a form similar to eq. (8), i.e. a mean contribution due to defined values plus a mean contribution due to undefined values. However, only the topic*shard interaction ( )ik has the form

µ^i ·k =

µ^i·k x

if i Xk if i  Xk

(a) Model (MD1).

(b) Model (MD6).

which cancels out the undefined values when yijk = x and makes the residuals ijk independent from them. In this sense, in Section 3.1, we said that the topic*shard interaction ( )ik is the factor
dealing with the intrinsic differences among topics, since it is able to
separate defined from undefined values. As we discussed, the num-

Figure 3: Comparison of different types of confidence intervals on T08 for AP on the whole corpus (left) and on TIP_RNDE_03 shards (right). On the x-axis there are the systems ordered by descending performance.

ber of undefined values is proportional to the number of relevant

documents for a topic and, therefore, the topic*shard interaction ( )ik factor accounts for the unequal value of topics.
Therefore, model (MD6) is not only a more precise model because, thanks to the more factors it considers, it is able to explain more variance than all the other models, leading to more accurate estimations of the differences among systems. But, especially, it is also the model with the most desirable properties, thanks to the presence of the topic*shard interaction ( )ik factor. Indeed, proposition 7.4 has two consequences:
(1) The denominator of eq. (1) is independent of the undefined values. This, jointly with Proposition 7.2, means that undefined

37], stratified sampling [7] and move-to-front [8] approaches;
72 system runs retrieving 10,000 documents for each topic.
We any mapped multi-graded relevance judgments to binary by
treating everything above not relevant as relevant. For each corpus, we created S randomly formed even sized shards,
where S  {2, 3, 4, 5, 10, 25, 50}. We label the shards of a corpus as <corpus>_RNDE_S; e.g., the WAPO corpus split into 5 shards is labeled WAPO_RNDE_05. For each shard size, we re-sampled 10 times; i.e., in the case of WAPO_RNDE_05 we have 10 independent sets of 5 random even size shards on the WAPO corpus. For space reasons, we report only some combinations of measures and tracks but the observed

values do not affect the identification of significantly differ- trends hold also for the other results.

ent systems. Consequently, the confidence intervals of eq. (3) are independent from the undefined values. The same holds

For each corpus split into shards, system runs retrieving from the corpus were also sharded. A run was split into the same number

for the confidence intervals of eq. (5).

of shards as the corresponding corpus. The random document split

(2) Recall that the F-statistic of the system factor is given by used to shard a corpus was the same split used to shard a run. Such

Fsys t em

=

M Ssys t em M Se r r or

where MSsystem

=

S Ssys t em d fsys t em

and

splitting is a simulation of how a system would retrieve documents

MSe r r or

=

S Se r r or d fe r r or

.

Since

both

SSsys t em

(Proposition 7.3)

on each shard. Past empirical work showed the simulation to work well Sanderson et al. [31].

and SSerror (Proposition 7.4) are independent from the undefined We consider the following evaluation measures: Average Precision

values, it follows that the F-statistic of the system factor is

(AP) [5], Precision at ten retrieved documents (P@10), Rank-Biased

also independent from undefined values. They do not af-

Precision (RBP) [21], and Normalized Discounted Cumulated Gain

fect the significance of this factor. Moreover, it follows that

the

effect

size

of

the

system

factor

^

2 sy

st

em



of

eq.

(6)

is

(nDCG) [16]. We calculated RBP by setting p = 0.8 as persistence parameter while we use a lo10 discounting function in nDCG,

independent of the undefined values.

to consider not too impatient users. We considered  = 0.05 to

8 EXPERIMENTAL SETUP

determine if a factor is statistically significant. Our experimental source code is at: https://bitbucket.org/frrncl/sigir2019-fs-code/.

To empirically test the analyses above, we experimented on the collections, topics, and system runs of the following datasets:
· Adhoc track T08 [38]: 528,155 documents of the TIPSTER disks 4-5 corpus minus congressional record (TIP); 50 topics, each with binary relevance judgments drawn from a pool depth of 100; 129 system runs retrieving 1,000 documents for each topic.
· Web track T09 [14]: 1,692,096 documents of the WT10g Web corpus; 50 topics, each with multi-graded relevance judgments and a pool depth of 100; 104 system runs retrieving 1,000 documents for each topic.
· Common Core track T27 [1]: 595,037 documents of the Washington Post corpus (WAPO); 50 topics, each with multigraded relevance judgments; relevance judgments were obtained mixing depth-10 pools with multi-armed bandit [18,

9 EXPERIMENTS
We conduct three experiments.
9.1 Confidence Intervals
We study the three types of confidence intervals under different ANOVA models. Figure 3 compares the intervals on the whole corpus using model (MD1) and on three shards using model (MD6).
In the case of the whole corpus and model (MD1) in Figure 3a, we see that, as expected, the Tukey confidence intervals (eq. (3)) are larger than the ANOVA ones (eq. (5)) since the latter do not account for multiple comparisons. We also see that the Tukey intervals of eq. (3) are similar to the SEM intervals (eq. (4)), which are independent from any model of the data and just consider each system in isolation. The fact that model-dependent confidence intervals

810

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

(Tukey ones) look close to model-independent ones (SEM ones) suggests that the topic and system factors of model (MD1) are not enough to accurately explain the data.
When using the shards (Figure 3b), we note that both the Tukey and ANOVA confidence intervals are smaller than SEM, suggesting that model (MD6) better explains the underlying data thanks to the additional factors it considers.
Note, that this difference between model (MD1) and (MD6) is not due to the increased number of samples passing from the whole corpus to shards but to the better ability of model (MD6) to explain the data. Indeed, the additional beneficial effect of increasing the number of samples is apparent in Figure 3b from the fact that all the confidence intervals get smaller when using shards, but this would happen for whatever model.
Figure 4 shows how the Tukey confidence intervals change across different models. The black dotted line is the system performance (marginal mean of the system j factor) on the whole corpus, i.e. the same line shown in Figure 3a in the case of AP. The continuous line is the system performance (marginal mean of the system j factor) on shards, i.e. the same line shown in Figure 3b in the case of AP; note that the green line for model (MD2), the orange one for model (MD3), and the red one for model (MD6) are superimposed since the marginal mean of the system j factor is the same in all these models. The shaded areas in the color of the line of each model represent the Tukey confidence interval for the corresponding model; for example, gray shaded area is for model (MD1) while the red shaded area is for model (MD6).
For all measures, the confidence interval using model (MD1) on the whole corpus is bigger than the confidence interval when using the other models. In particular, comparing the confidence intervals of models (MD1) and (MD2), which are computed without and with shards respectively. Comparing models (MD2), (MD3), and (MD6), we see the increasingly complex models improve the accuracy by shrinking the confidence interval. Moreover, comparing model (MD3) to model (MD6) we see that adding shard*system and topic*shard factors substantially reduce the intervals.
We report the Kendall's  correlation between the RoS on the whole corpus and on shards in the title of the plots in Figure 4. We can see that in three of the four plots,  > 0.9, the empirical threshold used to consider to ranking equivalence [36]. This suggests that we are not only improving accuracy but also maintaining coherence with what happens in traditional analyses.
Figure 5 compares the Tukey confidence intervals of eq. (3) for different shard numbers using model (MD6) in the case of AP on T08. As expected, the confidence intervals tend to reduce as the number of shards increases, due to the increased number of measurements on the shards. Kendall's  remains > 0.9, suggesting that the increased number of shards does not substantially deteriorate the agreement of the RoS on the whole corpus.

9.2 Multiple Comparisons

Table 1 reports summary statistics for multiple comparison analyses

on T08 using different splits for AP. We observe a large system

effect

size

(^

2 sy

s



).

We

also

can

see

a

drop

in

^

2 sys



passing

from

model (MD1), i.e. the whole corpus, to model (MD2), i.e. the same

model but using shards. The shards appear to introduce a new

factor, which interacts with the other factors and thus the size of ^ 2sys  reduces. However, as the models account for more factors ((MD2)-(MD6)), ^ 2sys  increases, suggesting that the more a model explains the data, the more prominent ^ 2sys  becomes. In the case of model (MD6) and for fewer shards, ^ 2sys  can be notably bigger than on the whole corpus.
Considering the number of significantly different pairs (columns Sig and NotSig), we see how moving from (MD1) ­ a classic significance testing approach ­ to any shard-based model always increases the number of pairs. More shards also means more significantly different pairs. However, there is a limited gain in using more shards: in the case of model (MD2) passing from two to five shards gives a 13.28% increase in the number of pairs but passing from five to ten produces only a 0.15% gain. More complex models are less sensitive to the increase in the number of shards, since they detect almost all the significantly different pairs already at a low number of shards. For example, in the case of model (MD6) passing from two to five shards gives just a 0.78% increase in the number of significantly different pairs while passing from five to ten produces a 0.20% increase.
The more sophisticated a model, the more significant differences are detected. However, not all models are equally impactful. From model (MD2) to (MD3), i.e. adding the topic*system interaction, produces notable increases while passing from model (MD3) to (MD4) and (MD5), do not provide substantial benefits. However, model (MD6), i.e. adding the topic*shard interaction, makes another substantial increase in the number of significant differences, confirming the importance of this factor.
If we consider the group of the systems insignificantly different from the top performing system (column TopG), we can appreciate another benefit of using shards. The number of systems in the top group drops from 7 when using the whole corpus to 1 when using shards and the more descriptive models, suggesting that the increased accuracy in estimating differences among systems allows us to detect that the top system is actually different from others.
9.3 Robustness to Shard Sampling
Table 2 show the summary of the analyses for AP across different shard sizes when using ten samples for each shard size. The Kendall's  column reports the average value of  over the samples and its 95% confidence interval. For all the tracks, the  values are quite high with small confidence intervals. This suggest that the RoS is quite stable and does not depend much on the specific random shards. Similar considerations hold also in the case of the Tukey confidence interval, which gets smaller has the shard size increases and whose values are similar across shard samples. This suggests that the detection of significantly different systems is not affected much by the specific random shards at hand.
The total number of significantly different pairs support this hypothesis since we can see how the confidence interval around this value is small, indicating that their number does not change much when the shard sample changes. The final column reports the fraction of significant pairs found in common across all 10 samples. Here, there is a notable level of consistency across the samples.

811

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

(a) AP.

(b) P@10.

(c) nDCG.

(d) RBP.

Figure 4: The Tukey confidence intervals (eq. (3)) of four measures across four models. On T08 with TIP_RNDE_10 shards. On the x-axis there are the systems ordered by descending performance.

(a) TIP_RNDE_02 shards.

(b) TIP_RNDE_04 shards.

(c) TIP_RNDE_10 shards.

(d) TIP_RNDE_50 shards.

Figure 5: Comparing confidence intervals of eq. (3) using models (MD1) and (MD6) for AP on T08 with different shard numbers. On the x-axis there are the systems ordered by descending performance.

10 CONCLUSIONS AND FUTURE WORK
At the start of the paper, we asked: can an unequal value of topics be exploited to improve measurement of system performance accuracy on a test collection?
We described and validated, theoretically and empirically, an
ANOVA model combined with a random sharding technique. We
showed that the model (MD6) measures substantially more signifi-
cant differences between IR systems than conventional approaches,

as represented by model (MD1). While it is true that a more sophisticated ANOVA model is expected to reduce measurement error, the scale of improvement seen with (MD6) is perhaps less expected. We showed that model (MD6) agrees well with the RoS taken from conventional test collection measurement and that the increased significance is not due to measurement error.
Past work has examined the question of whether the variability of topic measurement can be exploited to improve the accuracy of IR system measurement, we contend that our research shows that

812

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

Table 1: Comparing models for three shard sizes across 8256 system pairs, AP, track T08.

TIP_RNDE_02,  = 0.9717

TIP_RNDE_05,  = 0.9707

TIP_RNDE_10,  = 0.9598

Model vs Model ^ 2sys Sig NotSig TopG ^ 2sys Sig NotSig TopG ^ 2sys Sig NotSig TopG

MD1 ­

0.3991 3423 4833

7 0.3991 3423 4833

7 0.3991 3423 4833

7

MD2 ­ MD1

0.3500 4067 4189

4 0.2556 4607 3649

2 0.1595 4614 3642

2

-12.29% +18.81% -13.33% -42.86% -35.95% +34.59% -24.50% -71.43% -60.02% +34.79% -24.64% -71.43%

MD3 ­ MD1 MD2

0.5678 5175 3081

1 0.3495 5133 3123

1 0.1840 4831 3425

1

+42.28% +51.18% -36.25% -85.71% -12.42% +49.96% -35.38% -85.71% -53.90% +41.13% -29.13% -85.71%

+62.22% +27.24% -26.45% -75.00% +36.74% +11.42% -14.41% -50.00% +15.31% +4.70% -5.96% -50.00%

MD4 ­ MD1 MD2 MD3

0.5693 5180 3076

1 0.3511 5140 3116

1 0.1849 4833 3423

1

+42.67% +51.33% -36.35% -85.71% -12.03% +50.16% -35.53% -85.71% -53.66% +41.19% -29.17% -85.71%

+62.66% +27.37% -26.57% -75.00% +37.34% +11.57% -14.61% -50.00% +15.92% +4.75% -6.01% -50.00%

+0.27% +0.10% -0.16%

­ +0.44% +0.14% -0.22%

­ +0.53% +0.04% -0.06%

­

MD5

­ MD1 MD2 MD3 MD4

0.5675 5173 3083

1

+42.22% +51.12% -36.21% -85.71%

+62.15% +27.19% -26.40% -75.00%

-0.05% -0.04% +0.06% -0.26%

-0.32% -0.14% +0.23% -0.70%

0.3486 5129 3127

1

-12.65% +49.84% -35.30% -85.71%

+36.38% +11.33% -14.31% -50.00%

­ -0.08% +0.13% -0.59%

­ -0.21% +0.35% -1.12%

0.1829 4818 3438

1

-54.18% +40.75% -28.86% -85.71%

+14.62% +4.42% -5.60% -50.00%

­ -0.27% +0.38%

­

­ -0.31% +0.44%

­

MD6

­ MD1 MD2 MD3 MD4 MD5

0.7143 5889 2367

1 0.5235 5935 2321

1 0.3777 5947 2309

1

+78.99% +72.04% -51.02% -85.71% +31.19% +73.39% -51.98% -85.71% -5.36% +73.74% -52.22% -85.71%

+104.07% +44.80% -43.49% -75.00% +104.82% +28.83% -36.39% -50.00% +136.71% +28.89% -36.60% -50.00%

+25.80% +13.80% -23.17% +49.79%

­ +15.62% -25.68%

­ +105.29% +23.10% -32.58%

­

+25.46% +13.69% -23.05% +49.14%

­ +15.47% -25.51%

­ +104.20% +23.05% -32.54%

­

+25.86% +13.84% -23.22% +50.18%

­ +15.71% -25.78%

­ +106.52% +23.43% -32.84%

­

Table 2: Summary of analyses for AP using 10 samples of each random split and model (MD6).

Split TIP_RNDE_02 TIP_RNDE_03 TIP_RNDE_04 TIP_RNDE_05 TIP_RNDE_10 TIP_RNDE_25 TIP_RNDE_50

T08 ­ 8256 system pairs compared  CI Width Sig. Pairs Frac. Sig Pairs

0.9803

0.0540 5142.20

0.6228

0.9745

0.0551 5085.90

0.6160

0.9680

0.0546 5104.10

0.6182

0.9689

0.0549 5051.20

0.6118

0.9613

0.0538 5008.70

0.6067

0.9418

0.0445 5242.80

0.6350

0.9189

0.0351 5462.40

0.6616

Split

T09 ­ 5356 system pairs compared  CI Width Sig. Pairs Frac. Sig Pairs

WT10g_RNDE_02 0.9609

0.0732 2808.30

0.5243

WT10g_RNDE_03 0.9453

0.0717 2874.00

0.5366

WT10g_RNDE_04 0.9380

0.0683 2947.70

0.5504

WT10g_RNDE_05 0.9275

0.0657 3034.50

0.5666

WT10g_RNDE_10 0.9037

0.0530 3426.80

0.6398

WT10g_RNDE_25 0.8813

0.0389 3748.00

0.6998

WT10g_RNDE_50 0.8675

0.0288 3893.60

0.7270

T27 ­ 2556 system pairs compared

Split

 CI Width Sig. Pairs Frac. Sig Pairs

WAPO_RNDE_02 0.9764

0.0460 1821.50

0.7126

WAPO_RNDE_03 0.9634

0.0495 1791.20

0.7008

WAPO_RNDE_04 0.9617

0.0485 1800.10

0.7043

WAPO_RNDE_05 0.9583

0.0480 1802.70

0.7053

WAPO_RNDE_10 0.9470

0.0460 1822.80

0.7131

WAPO_RNDE_25 0.9219

0.0410 1848.30

0.7231

WAPO_RNDE_50 0.8812

0.0337 1853.30

0.7251

this is an approach with great promise. Model (MD6) allows us to make better use of existing test collections.
Our work in this particular direction of research is relatively new. Consequently, there are a number of avenues of future work:
· We want to compare our method with the recently published work of Voorhees et al. [39]. Their method also produces a substantial increase in the number of significant differences measured. However, their method of controlling for multiple

significance test comparisons is more liberal than the method we use. There is the potential for combining our approaches, their technique uses a bootstrapping technique new to IR research, our technique uses a new ANOVA model. · The metric of success, number significant differences, could be replaced by comparing the predictive power of our method with conventional methods. We could measure which of two systems is better on one test collection and see if those systems are similarly ordered on another test collection. · What happens if we consider topics as random factors and/or heteroskedastic data, following the approach adopted by Robertson and Kanoulas [26]? · Can we turn model (MD6) into a tool for designing better offline test collections, since it provides us with means for coping with differences across topics? · Can model (MD6) also allow us to build test collections with fewer relevance judgments or topics while maintaining currently attainable measurement accuracies? · Does this approach for offline testing tell us anything about online testing? Like the topics of test collections, online topics will have high and low numbers of relevant; do we need to think about how averaging works there too? · How much of a benefit will model (MD6) bring to performance measurement on test collections where topics with very few relevant documents are rare or non-existent?
11 ACKNOWLEDGMENTS
This research is supported in part by the Australian Research Council's Discovery Projects Scheme (DP180102687).
The work is also partially funded by the "DAta BenchmarK for Keyword-based Access and Retrieval" (DAKKAR) Starting Grants project sponsored by University of Padua and Fondazione Cassa di Risparmio di Padova e di Rovigo.

813

Session 9B: Relevance and Evaluation 2

SIGIR '19, July 21­25, 2019, Paris, France

REFERENCES
[1] J. Allan, D. K. Harman, E. Kanoulas, and E. M. Voorhees. TREC 2018 Common Core Track Overview. In E. M. Voorhees and A. Ellis, editors, The Twenty-Seventh Text REtrieval Conference Proceedings (TREC 2018). National Institute of Standards and Technology (NIST), Special Publication, Washington, USA, 2019.
[2] D. Banks, P. Over, and N.-F. Zhang. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval, 1(1-2):7­34, May 1999.
[3] D. Bodoff and P. Li. Test theory for assessing ir test collections. In W. Kraaij, A. P. de Vries, C. L. A. Clarke, N. Fuhr, and N. Kando, editors, Proc. 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007), pages 367­374. ACM Press, New York, USA, 2007.
[4] L. Boytsov, A. Belova, and P. Westfall. Deciding on an Adjustment for Multiplicity
in IR Experiments. In G. J. F. Jones, P. Sheridan, D. Kelly, M. de Rijke, and T. Sakai, editors, Proc. 36th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2013), pages 403­412. ACM Press, New York, USA, 2013.
[5] C. Buckley and E. M. Voorhees. Retrieval System Evaluation. In D. K. Harman and E. M. Voorhees, editors, TREC. Experiment and Evaluation in Information Retrieval, pages 53­78. MIT Press, Cambridge (MA), USA, 2005.
[6] B. A. Carterette. Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments. ACM Transactions on Information Systems (TOIS), 30(1):4:1­4:34, 2012.
[7] B. A. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over
Thousands of Queries. In T.-S. Chua, M.-K. Leong, D. W. Oard, and F. Sebastiani, editors, Proc. 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008), pages 651­658. ACM Press, New York, USA, 2008.
[8] G. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient Construction of Large
Test Collections. In W. B. Croft, A. Moffat, C. J. van Rijsbergen, R. Wilkinson, and J. Zobel, editors, Proc. 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998), pages 282­289. ACM Press, New York, USA, 1998.
[9] G. V. Cormack and T. R. Lynam. Statistical Precision of Information Retrieval
Evaluation. In E. N. Efthimiadis, S. Dumais, D. Hawking, and K. Järvelin, editors, Proc. 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006), pages 533­540. ACM Press, New York, USA, 2006. [10] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman and Hall/CRC, USA, 1994.
[11] N. Ferro and M. Sanderson. Sub-corpora Impact on System Effectiveness. In N. Kando, T. Sakai, H. Joho, H. Li, A. P. de Vries, and R. W. White, editors, Proc. 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017), pages 901­904. ACM Press, New York, USA, 2017.
[12] N. Ferro, Y. Kim, and M. Sanderson. Using collection shards to study retrieval performance effect sizes. ACM Transactions on Information Systems (TOIS), 37(3): 30:1­30:40, May 2019. ISSN 1046-8188. doi: 10.1145/3310364.
[13] J. Guiver, S. Mizzaro, and S. Robertson. A few good topics: Experiments in topic set reduction for retrieval evaluation. ACM Transactions on Information Systems (TOIS), 27(4):21, 2009.
[14] D. Hawking. Overview of the TREC-9 Web Track. In E. M. Voorhees and D. K. Harman, editors, The Ninth Text REtrieval Conference (TREC-9), pages 87­103. National Institute of Standards and Technology (NIST), Special Publication 500-
249, Washington, USA, 2000. [15] Y. Hochberg and A. C. Tamhane. Multiple Comparison Procedures. John Wiley &
Sons, USA, 1987.
[16] K. Järvelin and J. Kekäläinen. Cumulated Gain-Based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS), 20(4):422­446, October 2002.
[17] M. G. Kendall. Rank correlation methods. Griffin, Oxford, England, 1948. [18] D. E. Losada, J. Parapar, and A. Barreiro. Feeling Lucky? Multi-armed Bandits for
Ordering Judgements in Pooling-based Evaluation. In S. Ossowski, editor, Proc. 2016 ACM Symposium on Applied Computing (SAC 2016), pages 1027­1034. ACM Press, New York, USA, 2016.
[19] R. Mehrotra and E. Yilmaz. Representative & informative query selection for learning to rank using submodular functions. In Proceedings of the 38th international ACM sigir conference on research and development in information retrieval, pages 545­554. ACM, 2015.
[20] S. Mizzaro and S. Robertson. Hits hits trec: exploring ir evaluation results with network analysis. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 479­486. ACM, 2007.
[21] A. Moffat and J. Zobel. Rank-biased Precision for Measurement of Retrieval Effectiveness. ACM Transactions on Information Systems (TOIS), 27(1):2:1­2:27, 2008.
[22] D. Newman. The Distribution of Range in Samples from a Normal Population, Expressed in Terms of an Independent Estimate of Standard Deviation. Biometrika, 31(2):20­30, July 1939.

[23] S. Olejnik and J. Algina. Generalized Eta and Omega Squared Statistics: Measures of Effect Size for Some Common Research Designs. Psychological Methods, 8(4): 434­447, December 2003.
[24] S. Robertson. On GMAP: and other transformations. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 78­83. ACM, 2006.
[25] S. Robertson. On document populations and measures of IR effectiveness. In Proceedings of the 1st International Conference on the Theory of Information Retrieval (ICTIR'07), Foundation for Information Society, pages 9­22, 2007.
[26] S. E. Robertson and E. Kanoulas. On Per-topic Variance in IR Evaluation. In W. Hersh, J. Callan, Y. Maarek, and M. Sanderson, editors, Proc. 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2012), pages 891­900. ACM Press, New York, USA, 2012.
[27] A. Rutherford. ANOVA and ANCOVA. A GLM Approach. John Wiley & Sons, New York, USA, 2nd edition, 2011.
[28] T. Sakai. Metrics, Statistics, Tests. In N. Ferro, editor, Bridging Between Information Retrieval and Databases - PROMISE Winter School 2013, Revised Tutorial Lectures, pages 116­163. Lecture Notes in Computer Science (LNCS) 8173, Springer, Hei-
delberg, Germany, 2014. [29] T. Sakai. Laboratory Experiments in Information Retrieval, volume 40 of The
Information Retrieval Series. Springer Singapore, 2018. [30] M. Sanderson. Test Collection Based Evaluation of Information Retrieval Systems.
Foundations and Trends in Information Retrieval (FnTIR), 4(4):247­375, 2010. [31] M. Sanderson, A. Turpin, Y. Zhang, and F. Scholer. Differences in Effectiveness
Across Sub-collections. In X. Chen, G. Lebanon, H. Wang, and M. J. Zaki, editors, Proc. 21st International Conference on Information and Knowledge Management (CIKM 2012), pages 1965­1969. ACM Press, New York, USA, 2012. [32] I. Soboroff. On evaluating web search with very few relevant documents. In M. Sanderson, K. Järvelin, J. Allan, and P. Bruza, editors, Proc. 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004), pages 530­531. ACM Press, New York, USA, 2004. [33] D.R. Swanson. Searching Natural Language Text by Computer. Science, 132(3434): 1099­1104, 1960. ISSN 0036-8075. URL https://www.jstor.org/stable/1706747.
[34] J. M. Tague-Sutcliffe and J. Blustein. A Statistical Analysis of the TREC-3 Data. In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 385­ 398. National Institute of Standards and Technology (NIST), Special Publication
500-225, Washington, USA, 1994. [35] J. W. Tukey. Comparing Individual Means in the Analysis of Variance. Biometrics,
5(2):99­114, June 1949.
[36] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, September 2000.
[37] E. M. Voorhees. On Building Fair and Reusable Test Collections using Bandit
Techniques. In A. Cuzzocrea, J. Allan, N. W. Paton, D. Srivastava, R. Agrawal,
A. Broder, M. J. Zaki, S. Candan, A. Labrinidis, A. Schuster, and H. Wang, editors, Proc. 27th International Conference on Information and Knowledge Management (CIKM 2018), pages 407­416. ACM Press, New York, USA, 2018. [38] E. M. Voorhees and D. K. Harman. Overview of the Eigth Text REtrieval Conference (TREC-8). In E. M. Voorhees and D. K. Harman, editors, The Eighth Text REtrieval Conference (TREC-8), pages 1­24. National Institute of Standards and Technology (NIST), Special Publication 500-246, Washington, USA, 1999.
[39] E. M. Voorhees, D. Samarov, and I. Soboroff. Using Replicates in Information Retrieval Evaluation. ACM Transactions on Information Systems (TOIS), 36(2): 12:1­12:21, September 2017.
[40] W. Webber, A. Moffat, and J. Zobel. Score standardization for inter-collection
comparison of retrieval systems. In T.-S. Chua, M.-K. Leong, D. W. Oard, and F. Sebastiani, editors, Proc. 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008), pages 51­58. ACM Press, New York, USA, 2008.
[41] M. Yang, P. Zhang, and D. Song. A study of per-topic variance on system comparison. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR '18, pages 1181­1184. ACM, 2018. ISBN 978-1-4503-5657-2. doi: 10.1145/3209978.3210122. URL http://doi.acm.org/
10.1145/3209978.3210122.
[42] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query
difficulty: including applications to missing content detection and distributed
information retrieval. In R. Baeza-Yates, N. Ziviani, G. Marchionini, A. Moffat, and J. Tait, editors, Proc. 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005), pages 512­519. ACM Press, New York, USA, 2005.

814


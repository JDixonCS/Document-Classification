SGT Framework: Social, Geographical and Temporal Relevance for Recreational Queries in Web Search

Stewart Whiting
School of Computing Science, University of Glasgow, Scotland, UK.
stewh@dcs.gla.ac.uk
ABSTRACT
While location-based social networks (LBSNs) have become widely used for sharing and consuming location information, a large number of users turn to general web search engines for recreational activity ideas. In these cases, users typically express a query combining desired activity type, constraints and suitability, around an explicit location and time ­ for example, "parks for kids in NYC in winter", or "cheap bars for bachelor party in san francisco". In this work we characterize such queries as recreational queries, and propose a relevance framework for ranking points of interest (POIs) to present in the web search recreational vertical using signals from query logs and LBSNs. The first part of this framework is a taxonomy of recreational intents, which we derive from those previously seen in query logs and other behavioral data. Based on the most popular recreational intents, we proceed to outline a new relevance model combining social, geographical and temporal information. We implement a prototype and conduct a preliminary user-study evaluation. Results show the proposed relevance model and bundles greatly improve user satisfaction for recreational queries.
1. INTRODUCTION
LBSNs provide a platform for sharing and consuming local information during our every day lives through location check-ins and tips. When considered collectively, this user-generated information is very useful for understanding user mobility and preference at unprecedented scale.
Location is an essential part of web search, where a major percentage of queries have an explicit or implicit location intent [7, 8]. In this work, we focus on a specific segment of queries that have a recreational intent with a geographic preference. As an example, say that a family is planning an upcoming weekend city trip, and so are seeking suitable activity ideas. A query such as "things to do in london with kids" will trigger the recreational vertical to provide a short list of the most popular generic POIs such as tourist attractions and sightseeing spots. Real recreational queries are shown in Figure 1.
However, the recreational activity vertical does not contain specific suggestions tailored to the user's recreational intents, which
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914743

Omar Alonso
Microsoft Corp. Mountain View, California, USA.
omalonso@microsoft.com
Figure 1: Recreational queries for "things to do in san francisco..." in the Bing query auto-suggest feature.
may comprise implicit and explicit expectations ­ such as what they want and when they want to do it (e.g., season, weekday/weekend, time of date, etc.), in addition to constraints and preferences such as being `outside', `cheap', `romantic', or `with kids', etc. As a result, the activity vertical POIs are often not relevant, so the user must resort to collectively reviewing the web search results for ideas. To address this issue, we aim to present to the user a number of suggestions ­ based on LBSN check-in and tip data ­ that summarize the activities relevant to their intent. Note that this scenario is distinguished from established venue recommendation techniques employed by LBSNs since we have no previous venue ratings provided. Hence, only the user's explicitly provided query and implicit context (e.g., their geo-location, or the current time) can be used to infer the user's intent and expectations in the relevance model.
In this paper we present SGT, a framework that combines social, geographical and temporal information for presenting results that address the user's explicit (i.e., query-specified) and implicit (i.e., assumed) search intent. A central part of our framework is a taxonomy that drives the modeling and user experience.
We are interested in connecting the relevant information provided by two, potentially, different crowds - the users who are looking for things to do in search engine, and the users who have performed an activity in a LBSN like Facebook or Foursquare. The goal is to use the data from LBSN to satisfy the user search query. To achieve this, we would like to understand the different types of recreational intents and queries and other user behavior, model the relevance of venues to different queries, combine them in bundles, and finally present the results to the user.
With this in mind, in this preliminary work we set out to answer the following three research questions: (1) What type of recreational queries are users issuing to a search engine, and what is their prevalence? (2) Can a retrieval model be built to offer relevant places for recreational queries, based on LBSN data? (3) Is a single linear list of relevant venues, or a set of intent-oriented bundles more effective for presenting results to users?
In answering these questions, we make the following contributions: (i) a taxonomy of common recreational queries, (ii) a relevance model incorporating LBSN data, (iii) an approach to generating result bundles for composing the results, and finally (iv) preliminary evaluation using a large-scale real-world data set.

1001

2. RELATED WORK
The majority of work on LBSN data has concentrated on building context-aware venue recommender systems, where the context is typically the user's past preferences (expressed through checkins) and any explicitly requested venue requirements and filtering. Because of the recent popularity of recommender systems, several evaluation tasks for optimising such systems have originated in the community. The recent TREC Contextual Suggestion (CS) Track [4] provided user rating data and user queries accompanied with context such as query submission time. While the recreational search scenario we outline in this paper has many similarities to this TREC CS task, we do not have past user preferences to consider in the POI relevance model since the majority of web search users are either anonymous, or do not frequently use an LBSN.
There has been considerable analysis of the social, geographical and temporal patterns and trends present in LBSN data from many perspectives. Check-ins have been found to contain many structural patterns, caused by geographic and social constraints of user's lives [3] ­ which must be incorporated into any relevance framework. Likewise, the preference for different activity types changes dramatically (e.g., going to the movie theatre on a Friday evening, versus going to the park in the morning at a weekend) [1]. Further, [5] models how check-in preferences change over different periods time ­ from hour-to-hour through to seasons of the year. In essence, in this work we integrate these findings in a retrieval model specifically for supporting recreational search in web search.
3. RECREATIONAL QUERY TAXONOMY
In this section, we present and discuss the SGT recreational query taxonomy as a structure of the constraints and preferences most commonly expressed in queries by users during recreational information seeking.
We study common recreational query patterns observed in a variety of user-generated content and behavioral logs. In particular, we mined a month of web and mobile search query logs and browser behavioral data from the Microsoft Bing search engine. In each source we looked for queries and links which contained to the substring seed text "things to", "places to" and "what to", and a city recognised by a disambiguating location entity tagger. Note we find the typical nature and expectations of recreational activity varies considerably from place to place, for instance, "romantic" queries are much more popular in Paris than in Chicago. In the following sections we elaborate the top-level taxonomy aspects.
3.1 Taxonomy Aspects
The taxonomy comprises the following high-level aspects: (i) geographical constraint, (ii) temporal constraint, (iii) activity preference, (iv) activity suitability, and (v) activity constraint.
Each aspect corresponds to the recreational activity requirements expressed in real user queries. In many cases the aspects are not independent, with a single query often containing many aspects (e.g., `cheap things to do with kids in nyc in january'). In the following subsections we describe each aspect along with illustrative query examples from a real query log.
Geographical Constraint. Most important is the place where the user is, or alternatively, is going. Locations expressed by a user include a country, state or city. City names must be disambiguated using established techniques [6]. Relative locations such as `near' and `around' must be resolved relative to specific location. In our experiments reported in Section 6, we use major world cities as the base geographical constraint.
Temporal Constraint. Time will define relevant POIs [3, 5, 1]. For example, eating will differ during the day as people desire

breakfast, lunch, dinner or late night snacks. Similarly, it is unlikely a user will visit a park on a Sunday night in a cold city.
Time periods include seasons, months, weekday/weekends, day of the week and the time of day (e.g., morning, afternoon, evening and night). Alternatively, events such as Christmas, New Year, Thanksgiving and Memorial Day, etc., can be translated into an upcoming time anchor. Relative temporal constraints can be provided in the form of "now", "today", "tonight" and "this weekend" etc., can also be translated into temporal constraints. In this work we do not consider current or upcoming POI trendiness in ranking.
Activity Preference. Of course, many users will have a preference for the recreational activity they are seeking. They may wish to eat or drink, whether at a restaurant or in a lively outdoor area. Tourists are likely to want sightseeing areas and major POIs, in contrast to residents who may prefer cafes and parks to relax. Similarly, those looking for outdoor recreational activities will find parks and trails much more relevant.
Activity Suitability. In many cases, user's seek activities with certain characteristics to match an activity purpose. For instance, couples may wish to find a location that is romantic and therefore suitable for a date or anniversary. This could be a restaurant, or something to do, such as visiting a romantic sightseeing spot.
For different age groups, users may look for locations which are suitable for kids, such as babies, toddlers, teenagers, etc. Social group suitability may include friends, family and bachelor/bachelorette parties. Weather may also be a major factor in some locations and seasons.
Activity Constraint. Especially common are preferences for "free", "cheap", "unique" and "educational" activities. For nightlife recreational queries we often saw "lively" constraints, and likewise the "quiet" constraint for bars, cafes and restaurants where the objective was likely work-related. Disabled access and dogfriendly constraints were also commonly seen.
Together, these SGT taxonomy aspects can be used to represent complex recreational intents expressed in recreational queries, such as "unique romantic things to do this weekend in new york".
3.2 Query Classification & Aspect Popularity
For each taxonomy aspect we create a classifier incorporating related terms and phrases. We use this to classify and map user query and POI tip text (from the LBSN) to the aspects which they relate in the taxonomy. The query or tip keywords present to identify each aspect are intuitive, so we do not list them exhaustively here. For instance, the synonyms "for kids" or "for children" and "kid-friendly" are used for the "kids" age suitability aspect.
Since tips and queries are short text, term frequency is usually 1 so a simple binary term presence classifier is effective. Future work will explore more elaborate probabilistic and natural language classifiers which can better handle positive and negative sentiments expressed in more complex phrases, such as `not good for kids'and `my kids loved this place' in tip text.
Taxonomy Aspect Popularity in Search Queries. With a longtail distribution typical in search, we found the ten most common recreational aspects expressed in queries were for: (1) tourism, (2) eating, (3) fun, (4) for kids, (5) weekend, (6) near, (7) free, (8) outside, (9) with dog, and (10) birthday. Notably, we found the popularity of these aspects varies hugely between cities, and for the same city at different times. Accordingly, we propose activity bundles to better support this behaviour in Section 5. Based on this taxonomy, in the next section we outline the SGT relevance model to rank POIs for recreational queries.

1002

4. MODELLING POI RELEVANCE
We extract a list of POIs for each major world city from LBSN data, and model various signals from the social data to quantify each location's relevance to a recreational query. For LBSN data, we use the full 2014 check-in and POI tip datasets from the popular Foursquare and Facebook LBSNs (a combined total of over 1B+ check-ins, with a considerable number including textual tips).
Check-in times tell us when places are popular. Likewise, tips tell us what people think about POIs, offering several clues such as whether they are good for kids, a romantic date, lively, or quiet etc. All these signals must be combined to filter and rank relevant POIs for queries spanning multiple aspects of our taxonomy.
In this work we use a very large dataset of LBSN data, and as we are most interested in major world cities with plenty of check-in data available (hence, sparsity is not a problem), at this stage we employ a basic maximum likelihood estimation (MLE) model to rank POIs based on the probability that the taxonomy aspects inferred from check-in and tip signals match the taxonomy-classified recreational query aspects. Accordingly, for a given recreational query qrec relating to a specified city, we compute and rank the likelihood of a POI poii being relevant using the base MLE ranking formula as follows:

MLE(qrec) = arg maxP(poii|qt , qa)

(1)

poii  poic

poic is set of all POIs in the desired city (future work will explore more fine-grained locations). qt is the likelihood of the POI being relevant given any temporal activity constraints expressed in the recreational query. And finally, qa is the POI relevance to any given SGT taxonomy aspects expressed in the recreational query. We elaborate on deriving the temporal and taxonomy aspect popularity components in the following two sections.

4.1 Temporal Ranking Component
From past check-ins, we observe the probability that a user will check-in at the POI during any given time period. We consider all permutations for parts of the day (morning/afternoon/evening/night), day of the week (Monday-Sunday), weekdays/weekends and season, similar to that presented in [5]. Thus, in the base MLE ranking equation (Equation 1), we define the temporal relevance qt of poii probabilistically as P(poii|qt ) = P(poii|City, PartO f Day, DayO fWeek,WeekdayWeekend, Season).
Recent trends such as new and trending POIs will not be observed in past data. We leave time series modelling to capture these more elaborate temporal trends to future work.

4.2 Aspect Ranking Component
To rank POIs for recreational queries, we must quantify the relevance of every POI to each aspect in the SGT taxonomy. We do this by examining the feedback in LBSN POI tips to determine which of those POIs, and POI categories, are most likely to satisfy a recreational query based. In essence, this means we higher rank POIs for a romantic recreational query when the POI is both in a typically romantic category (e.g., sightseeing spots, or French restaurants), and has tips mentioning that it is itself romantic.
Using the same keyword classifier detailed in Section 3.2, we classify all the POI tips into our taxonomy aspects. For example, the tip: "my kids loved the great food here - and it was cheap" would be classified with the aspects: "for kids", "cheap" and "great". In the first instance, this allows us to observe which local POIs have a propensity towards certain taxonomy aspects, based solely on their own tips. For example, major city parks with good play areas and open space have many tips mentioning

fun, children and dogs. For the most popular POIs in major cities, this technique alone helps us quantify the relevance of the POI for the most common taxonomy aspects. However, the sparsity of tips for the majority of individual POIs and smaller cities means this approach quickly becomes ineffective.
To overcome this sparsity issue (especially for the long-tail) we rely on higher-level POI categories (e.g., "bowling alley", "playground", "zoo", "theme bar" and "beach", etc.) to determine the appropriateness of a POI to a given aspect via its category. This approach allows use to still present and effectively rank POIs with few or no tips relating to a given taxonomy aspect.
With this in mind, we organise aspect classified POI tips by their respective POI category ­ rather than their particular POI. From this, we compute a score that each POI category is relevant to each SGT taxonomy aspect. Henceforth, for each taxonomy aspect ai, and POI category ci we compute the category relevance score as:

categoryrelevance(ai, ci) =

f

req(ai, ci f req(ai)

)

×

log10

(

f

req(ai))

(2)

Where f req(ai, ci) is the number of tips from the POI category ci classified with the given aspect, and f req(ai) is the number of tips from any POI category classified with the given aspect. The final log term provides down-scaling for the overall most popular categories (e.g., bars), which tend to appear indiscriminately in all aspects, allowing more specific ­ yet, highly relevant ­ niche categories to rank highly.
The result of this scoring function provides intuitive ranking of categories for each taxonomy aspect. For example, this function ranks the POI categories "mini golf", "paintball field", "theme park ride", "go kart track" and "theme park" highest for the "fun" taxonomy aspect. Likewise, it ranks "playground", "science museum", "toy/game store", "theme park" and "zoo" as top for the "kids" taxonomy aspect.
To define aspect relevance for a POI to a recreational query, we combine POI-specific, if available, and POI category evidence together. Thus, we define the aspect relevance qa for poii as aspectrelevance(poii, qa) = (1 + P(qa|poii) × (categoryrelevance(qi, poicategory)) . P(qa|poii) is the probability that the POI poii has a tip classified with the query aspect qa. The preceding 1 + . . . term is to smooth zero probabilities. Hence, in this model the POI-specific term can be seen as a boost for the typically more much more powerful POI category score term.

5. RESULT BUNDLES
Finally, we consider how to effectively present POI results to the user. When the user provides an ambiguous recreational query without any specific activity constraints, preferences or suitability (e.g., just "things to do in london"), we posit their need is exploratory ­ and so a more organised and diversified result presentation is required (as has been applied in web search scenarios [2]). Accordingly, we propose a simple POI result-bundling approach by activity aspect to maximise user understanding of all POI options, and thus overall satisfaction.
The prevailing approach used in the web search activity vertical is to simply present the top-k most relevant POIs in a linear list (accompanied by thumbnail) ­ as illustrated by (1) in Figure 2. This approach is limited since it offers unintuitive organisation of POI results, and little diversity. Hence, we look to group a small number of taxonomy aspect-related (e.g., "for kids", or "nightlife" etc.) POI results into bundles to present ­ illustrated by (2) in Figure 2.
Each bundle corresponds to set of most popular aspects we have observed for the city (or, from all cities in aggregate if no such ob-

1003

Evaluation

5

4.10

4

3.38

3.42

3.58

3

2

1

0

System 1 - for System 1 - for a System 2 - for System 2 - for a

you

tourist

you

tourist

5
4 3.05
3

3.10

3.97

3.92

2

1

Evaluation 0 System 1 - for System 1 - for a System 2 - for System 2 - for a

you

tourist

you

tourist

Figure 2: (1) linear list-based POI result presentation, and (2) bundle-based POI result list presentation.
servation is available, as is the case for smaller cities). These are the aspects we discussed in Section 3.2. Recall how we noted the aspect popularity varied from city-to-city, and indeed over time. For instance, Paris had far more recreational queries relating to romantic aspects. Likewise, for Santa Cruz (CA), recreational queries mainly related to beach, outdoor and leisure aspects were most popular, especially in summer.
For each city, we extracted the four most popular aspects as candidate result bundle aspects. As per the linear list, we rank the POIs using the SGT Framework presented in the previous section. However, we interleave the results into each bundle by selecting the top-k POI results by their category and relevance to the bundle taxonomy aspect. This means for San Francisco, one such bundle is "with kids" ­ so we present a bundle with a combination of top ranking POIs in the "playground", "science museum", "zoo", "theme park" and so forth categories.
6. EXPERIMENTAL PROCEDURE
We conducted a small user experiment to evaluate the effectiveness of the SGT framework for ranking POIs, and the presentation of POI results in linear lists versus bundles. 10 users were (5 males and 5 females, 20-40 years of age) with extensive web search experience. Participants were from 6 different countries, so tested the system for a wide range of major international cities.
After a brief training session, participants were asked to interact with two systems, and answer the same set of two questions for two tasks. System 1 is the linear list-based POI results, and System 2 is bundle-based POI presentation. The tasks and questions were as follows. Task 1: Choose a city you are very familiar with, but do not live in. This might be the city in which you grew up or attended school. Task 2: What is the major city you currently live in, or near? For each system, please type the name of the city and search for things to do. Based on your experiences, please answer the following questions: (Q1) Based your knowledge of the city, on a scale 1 to 5 (1=bad, 5=excellent), what is the quality of the POI suggestions? (Q2) On a scale 1 to 5 (1=bad, 5=excellent), how useful is this information for a user who is not familiar with your city but is looking for things to do?
7. RESULTS & DISCUSSION
Overall, questionnaire results presented in Figures 3 and 4 show that users in general like the POI results provided by the SGT framework for their recreational queries. In particular, we find that users much prefer bundle-based result presentation, especially for familiar cities ­ this is probably because they surface more unusual options beyond the typical top well-known POIs. We also note that they feel that tourists could benefit from them, demonstrating the potential for both major recreational query use cases. We note qualitative feedback which suggested we needed to introduce interaction to allow the user to specify the bundles they were presented.

Figure 3: Average system rating for a city the user is familiar

with, but does not live in (i.e., task 1).

5

5

4.10

4

3.38

3.42

3.58

3

4

3.05

3.1

3

2

2

1

1

0

System 1 - for System 1 - for a System 2 - for System 2 - for a

you

tourist

you

tourist

0

System 1 - for System 1

you

tour

Figure 4: Average system rating for a city the user lives in, or near (i.e., task 2).
8. CONCLUSION
In this work, we investigated POI result ranking and presentation for the large number of recreational search queries posed to a web search engine. We outlined the Social, Geographical and Temporal (SGT) Framework as a first step for modelling POI relevance in this scenario. Further, recognising the issues of presenting POI results in a linear list in the activity search vertical, we proposed a POI result bundling to improve the user comprehension of results.
With respect to the research questions outlined in Section 1, we make several findings. A wide range of broand and specific recreational queries are posed to web search engines. We constructed a taxonomy of these, and report a long-tail frequency distribution. We outlined the SGT framework for modelling POI relevance based on the LBSN tip and check-in data. Preliminary evaluation indicated this first approach satisfied users. Finally, we found bundlebased result presentation is much preferred for familiar and tourist user scenarios. This work represents a first step; future work will better address city POI information sparsity, user personalisation, and bundling strategies along with more conclusive evaluation.

9. REFERENCES
[1] S. Bannur and O. Alonso. Analyzing temporal characteristics of check-in data. In WWW '14, pages 827­832.
[2] H. Bota, K. Zhou, J. M. Jose, and M. Lalmas. Composite retrieval of heterogeneous web search. In WWW '14, pages 119­130.
[3] E. Cho, S. A. Myers, and J. Leskovec. Friendship and mobility: user movement in location-based social networks. In SIGKDD '11, pages 1082­1090. ACM.
[4] A. Dean-Hall, C. L. Clarke, J. Kamps, P. Thomas, N. Simone, and E. Voorhees. Overview of the trec 2013 contextual suggestion track. In TREC 2013.
[5] H. Gao, J. Tang, X. Hu, and H. Liu. Modeling temporal effects of human mobile behavior on location-based social networks. In CIKM '13, pages 1673­1678.
[6] C. B. Jones and R. S. Purves. Geographical information retrieval. International Journal of Geographical Information Science, 22(3):219­228, 2008.
[7] M. Sanderson and J. Kohler. Analyzing geographic queries. In Workshop on Geographic Information Retrieval, SIGIR '04.
[8] L. Wang, C. Wang, X. Xie, J. Forman, Y. Lu, W.-Y. Ma, and Y. Li. Detecting dominant locations from search queries. In SIGIR '05.

1004

Simple Dynamic Emission Strategies for Microblog Filtering

Luchen Tan, Adam Roegiest, Charles L. A. Clarke, and Jimmy Lin
David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada
{luchen.tan, aroegies, jimmylin}@uwaterloo.ca, claclark@gmail.com

ABSTRACT
Push notifications from social media provide a method to keep up-to-date on topics of personal interest. To be effective, notifications must achieve a balance between pushing too much and pushing too little. Push too little and the user misses important updates; push too much and the user is overwhelmed by unwanted information. Using data from the TREC 2015 Microblog track, we explore simple dynamic emission strategies for microblog push notifications. The key to effective notifications lies in establishing and maintaining appropriate thresholds for pushing updates. We explore and evaluate multiple threshold setting strategies, including purely static thresholds, dynamic thresholds without user feedback, and dynamic thresholds with daily feedback. Our best technique takes advantage of daily feedback in a simple yet effective manner, achieving the best known result reported in the literature to date.
1. INTRODUCTION
Filtering topical events from social media streams, such as Twitter, provides a means for users to keep up-to-date on topics of interest to them. If care is taken, these updates may even be pushed directly to the user through notifications on mobile devices or desktops. However, for push notifications to be successful, the user must be given means to control the frequency and volume of updates, avoiding indiscriminate and unwanted notifications. This frequency and volume depends both on the interests of the user, with topics of greater interest updated in greater volume, and on the topics themselves, with some topics naturally receiving updates more frequently than others.
We might update a user interested in polls for the 2016 U.S. presidential elections many times a day during the election cycle itself, but with updates stopping altogether after November 8. We might update a user interested in California residential water restrictions only when these restrictions change, perhaps a few times a year, but interest in the topic might persist for many years, as long as the user is a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914704

resident of the state. For causal sports and entertainment topics (cricket or the Kardashians), a user may not desire more than a few of the most significant updates per day, regardless of events taking place. For topics of great personal importance (a tornado warning or friend's wedding), we might push all updates.
The TREC Microblog tracks provide an experimental forum for research groups working in this area. In 2015, the track [4] required participating groups to monitor the live "spritzer" stream provided by Twitter over a period of ten days in July, selecting tweets relevant to 225 pre-defined interest profiles, each expressed through statements modeled after TREC ad hoc topics. Each tweet returned by participating systems was accompanied by the clock time at which the system would have pushed it to the user. This information was then used by the track organizers to compute various official evaluation measures for a subset of 51 interest profiles. The evaluation measures considered both the relevance of selected tweets and the time at which they were putatively pushed. In addition, the measures accounted for retweets, near-duplicate tweets, and other redundancies, reflecting an expectation that a user would not want to receive notifications about the same thing over and over again.
Along with this mobile notification scenario (called "scenario A") the evaluation supported a daily digest scenario (called "scenario B"). At the end of each of the ten days, participating systems returned a ranked list of tweets from that day, just as if a user was sent a summary of the day's events by email. Standard evaluation measures for ranked retrieval may be applied, provided that they also appropriately account for redundant content.
To achieve good results for scenario A, systems must successfully address three requirements implicit in the task:
1. A requirement to score individual tweets with respect to relevance. As a simplification, the evaluation focused on topical relevance. Social signals, such as the prominence of the source or its connection to the user, were not considered, so that the relevance of a tweet is primarily determined by its content.
2. A requirement for novelty, so that the system does not push redundant information. This requirement was operationalized by the assessors considering tweets chronologically: if two textually similar tweets arrive at different times, the later tweet is considered redundant if it does not contain substantive information beyond that found in the earlier tweet [9]. Again, only tweet content is considered, so that a duplicate tweet from a more prominent or authoritative source would still be considered redundant.

1009

3. A requirement to avoid pushing non-relevant information altogether. The evaluation measures for scenario A explicitly rewarded systems that avoided pushing non-relevant information. Thus, appropriate selection of thresholds was critical to success. In some cases, the ideal response for a given day was to push nothing, and the "empty" strategy of never pushing anything formed a challenging baseline that many systems failed to beat.
Since the first two requirements are inherent in any ranking task, we focus on the third requirement. After demonstrating the impact of ignoring the third requirement, we consider various strategies for establishing and maintaining thresholds for pushing tweets. We examine strategies under two assumptions: with and without user feedback. When feedback is not available, thresholds are established from historical information. When feedback is provided, it is limited to once-per-day judgments based on the scenario B output. Of particular importance is the establishment of a global score threshold, applied across all topics in the absence of feedback. Our best technique takes advantage of daily feedback in a simple yet effective manner, achieving the best known result reported in the literature to date.
2. RELATED WORK
Filtering has been a longstanding subject for information retrieval research. Earlier work on Topic Detection and Tracking (TDT) investigated algorithms for the discovery of new topics and maintaining awareness when these topics reappear in newswire streams or broadcast news [2]. Experimental results from TREC from the mid-1990s to early2000s indicated that simple IR techniques can achieve high quality results in TDT domains [1, 3, 10, 11]. For example, Allan et al. [1] used the most common words from 1 to 16 training stories to generate short queries which were compared to documents using TF-IDF based similarity.
More recently, researchers have examined filtering and tracking problems in the context of social media. However, Twitter introduces several issues not present in previous topic tracking tasks, especially in relation to tweets. In particular, tweets have a maximum length of 140 characters and this length limitation implies that meaningful words rarely occur more than once, suggesting that TF-IDF weighting schemes may be less useful. Lin et al. [5] investigated the use of four language modeling smoothing techniques to filter tweets, mitigating issues with sparse terms, i.e., the zero-probability problem. Zhao and Tajima [12] framed a retweet recommendation problem as a multiplechoice secretary problem. They examined Twitter "portal accounts", which retweet selected tweets for their followers, and considered a number of strategies for tweet selection. They proposed and compared a number of online and nearonline decision methods, including a history-based threshold algorithm, a stochastic threshold algorithm, a time-interval algorithm, and an "every k-tweets" algorithm.
3. TREC 2015 MICROBLOG TRACK
Topics (called "interest profiles") provided to participants in the TREC 2015 Microblog track included a short querylike description of the information need, called the "title" in TREC parlance. For example, topic MB235 has the title "California residential water restrictions". Other fields in a topic elaborate on the information need, providing a more

complete indication of what is and is not relevant. While track participants were permitted to use these other fields for filtering purposes, we focused on the more realistic task in which the user provides only the short query.
Track participants filtered the so-called Twitter "spritzer" stream over ten days in July 2015, selecting those relevant to 225 pre-defined interest profiles and recording their tweet ids for submission. In addition to the tweet ids, the push time was recorded for each tweet, indicating the time the system decided to push the notification. After all experimental runs were submitted to the track organizers, 51 of the interest profiles were selected for judging. Tweets were pooled and judged on a three-point scale. In evaluating a run, a tweet was considered redundant, providing no gain, if it did not contain substantial new information not found in previously pushed tweets [9].
The primary evaluation measure for scenario A is expected latency-discounted gain:

1N

ELG = N

GiDi

(1)

i=1

For N pushed tweets, Gi is the gain associated with tweet i based on the assigned relevance grade, after adjusting for redundancy. The temporal discount applied to tweet i is Di = max(0, (100 - Li)/100), where Li is the latency in minutes between the time the tweet appears in the stream and the time the system decides to push it. ELG is computed on a daily basis, over the tweets pushed by a system that day, with the system's overall score averaged across all topics and all days.
ELG has an interesting discontinuity when a system decides not to push anything. For some topics on some days, when no relevant tweets appear in the stream, this is the correct action. On such days, a system pushing any tweet receives a score of zero (since none can possibly be relevant). To reward systems that push nothing on such days, the value of ELG is defined to be one. On the other hand, for systems that push nothing on days when relevant tweets appear in the stream, the value of ELG is defined to be zero. Since no relevant tweets appeared for many topics on many days, the "empty" strategy of never pushing anything receives a nonzero ELG score. Indeed, this strategy forms a challenging baseline which many participating systems failed to beat.

4. BASELINE SYSTEM
The system used for the experiments reported in this paper was deployed for the TREC 2015 Microblog track [7], with the design of the system based on substantial pilot experiments conducted prior to the actual evaluation. The system achieved the best ELG score across the 32 automatic runs submitted by 14 participating groups.
All experiments described in this paper are from post hoc runs using a replay mechanism over tweet data captured during the evaluation period. Following the TREC evaluation, we performed error analysis to understand the contributions of various system components, and have distilled our algorithm into simple strategies to address the three requirements listed in the introduction, detailed below.
Relevance. Although we recognize that social signals and other non-content features are important for relevance, as a first step we only consider tweet content. For pre-processing, we apply language detection tools to eliminate non-English

1010

tweets, tokenize the tweets using Twokenize,1 and then apply some simple tweet quality heuristics, e.g., eliminating tweets containing less than five tokens.
For matching against the tweet stream, titles were tokenized by splitting on space and punctuation, and stopwords were removed. Since relevant tweets may not contain all, or even any, of the title terms, we employed a pseudo-relevance feedback step to expand the title terms with 5 hashtags and 10 other terms. This was accomplished by querying Twitter's own search engine with title terms at the beginning of each day and extracting the top terms using pointwise KL-divergence [6, 8].
For relevance scoring, we applied a simple matching formula developed through pilot testing. Given the short length of tweets, many "standard" features such as query term frequency appear to have limited value. We found that (binary) query term occurrence appears to be the key feature, with the occurrence of title terms having greater importance than the occurrence of expansion terms. To achieve a balance, we score tweet relevance as follows:

(wt

·

Nt

+

we

·

Ne)

·

Nt |T |

(2)

where Nt is the number of title terms that appear in the tweet, Ne is the number of expansion terms that appear in the tweet, and |T | is the number of title terms. Based on pilot experiments, weights were set to wt = 3 and we = 1.
Novelty. We de-duplicated tweets by computing unigram overlap between each new tweet and the tweets previously pushed for a given topic across all days. Tweets with 60% or more overlap were discarded and not further considered. As with the other parameters, we based this overlap setting on pilot experiments conducted prior to the actual TREC 2015 evaluation.
Thresholding. Thresholds for pushing tweets were based on the relevance score in Equation (2). Each day, a threshold is selected, and only tweets with scores greater than or equal to the threshold are considered for delivery. In addition, the evaluation placed a limit of k = 10 on the number of tweets that could be pushed each day. Once k tweets are pushed, all further system output is ignored.
Because of its simplicity, Equation (2) has the interesting property that reasonable thresholds are the same across all queries. Our simplest thresholding strategy is thus to select a single static global threshold (GT) across all queries and days. A simple dynamic strategy (without feedback) is to consider the top k from the previous day, selecting as a threshold the score of the kth tweet. However, under this strategy, we do not lower the threshold below a global minimum, selecting as a threshold max(GT, top-k yesterday).
For our TREC 2015 experiments, we used a global threshold of GT = 5 (determined based on previous pilot experiments). To better understand the impact of this threshold, Figure 1 shows the effectiveness of our system for these two strategies across a range of global threshold values. The baseline for this plot is the effectiveness of the empty strategy, i.e., never pushing anything, with low threshold values underperforming it. A global threshold of GT = 6 slightly outperforms our default threshold of GT = 5, which we retain for the remainder of this paper. We find that our simple strategy of dynamically adjusting the threshold without

1http://www.ark.cs.cmu.edu/TweetNLP/

0.50 0.45 0.40 0.35

ELG for different global thresholds
Oracle Run Empty Run GT max(GT, top-k yesterday)

0.30

ELG

0.25

0.20

0.15

0.10

0.05

0.00

oracle

pty em

GT=0

GT=1

GT=2

GT=3

GT=4

GT=5

GT=6

GT=7

GT=8

GT=9

Figure 1: ELG for different global thresholds, the oracle run, and the empty run.

feedback is not effective--the difference compared to using a simple global threshold is negligible for most settings.
The oracle run in Figure 1 represents the ELG achievable if we made an ideal selection of the global threshold for each topic at the beginning of each day. Clearly, substantial improvements can be achieved through better threshold selection. In the next section, we explore a dynamic emission strategy that uses feedback from each day's digest (scenario B) to select the threshold for the following day.

5. FEEDBACK STRATEGIES
Under scenario B, participants submitted a ranked list of tweets for each topic for each day, providing a daily digest of events for the hypothetical user. Building on the baseline system described above, we use each day's digest to provide relevance feedback for determining thresholds for the following day. While in reality each system saved a ranked list during each day of the evaluation period for later submission, and thus system results were not judged until the evaluation concluded, here we assume that relevance information is provided at the end of each day and available for immediate use. We imagine a user interacting with the results once per day, providing feedback as a way of adjusting the filter for the next day. While in practice this daily interaction might be too onerous, the results provide a sense of what gains could be achieved with ongoing feedback.
For the feedback strategies described below, we used our official scenario B run submitted to TREC 2015 as the daily digest--this ensured that all tweets have relevance judgments. Our method of relevance scoring, Equation (2), often assigns the same score to several tweets. We use the term "score block" to denote a set of tweets with the same score. Accordingly, we say that for a score si, it has a corresponding score block SBi. To determine a dynamic threshold using relevance feedback, we begin by combining all feedback received into a single list of score blocks. At the end of day one for a particular topic, we have 10 tweets worth of feedback, on day two, 20 tweets worth, and so on.
There are two edge cases to consider. In the first case, if there are no relevant tweets in any score block, we take

1011

Baseline Strategies GT = 5 GT = 6 Feedback Strategies avg gain weighted avg gain weighted avg gain+r score

ELG 0.3191 0.3303 (p = 0.3819) ELG 0.3257 (p = 0.5664) 0.3510 (p = 0.0004) 0.3678 (p  0.0000)

Table 1: Effectiveness of various emission strategies; p-values are generated from a paired sign test with GT = 5.

the maximum of the global threshold (GT ) and the highest score seen so far plus wt, the weight assigned to a title term match. In the second case, if all tweets are relevant, we take the minimum score seen so far.
If we have a mix of relevant and non-relevant tweets, we first compute the average gain for each score as follows:

gain(t)

avg

gain(si)

=

sj si tSBj
|SBj |

sj si

where gain(t) comes from the relevance judgments. We then weight each score's average gain by the proportion of relevant content provided by that score (i.e., the precision of that score):

weight(si)

=

|{t|t



SBi

 gain(t) |SBj |

>

0}|

sj

weighted avg gain(si) = avg gain(si) · weight(si)
While selecting a threshold that maximizes average gain doesn't perform particularly well, selecting a threshold that maximizes the weighted average gain significantly improves ELG (see Table 1). However, the weighted average gain method still returns too much non-relevant content. To further improve effectiveness, we use the relevance information to adjust the threshold based on the ratio between relevant and non-relevant content:

|{t|t  SBj  gain(t) = 0}|

r

score(si)

=

sj si
|{t|t



SBj



gain(t)

>

0}|

sj si

For use as a threshold, a score's r score must be less than some cutoff, . We can vary  to be more or less permissive of non-relevant tweets: via a coarse-grained parameter sweep, we find that  = 1.75 achieves substantially improved results on ELG (see Table 1). Note that if no score achieves an r score less than or equal to , we set the threshold for the next day to be the maximum of the global threshold (GT ) and the highest score seen so far, across all days.

6. CONCLUSION
Simple techniques for content matching and novelty can achieve good effectiveness for microblog filtering, provided that care is taken to set appropriate thresholds to avoid pushing non-relevant information. Referring back to Figure 1, our most effective technique achieves an ELG of 0.3678, which is still substantially below what might be achieved if

the optimal threshold could be determined for each topic at the beginning of each day, i.e., the oracle with an ELG of 0.4709. However, we significantly improve upon our already highly-effective baseline (already the best automatic run at TREC 2015). In fact, our technique achieves the best known result reported in the literature to date, including manual runs. Our experiments highlight the importance of proper threshold setting, and demonstrate that systems can automatically set appropriate thresholds using simple yet effective feedback techniques. While dynamic thresholds can be set from the tweets of previous days without feedback, such a strategy provides little value, at least with the simple technique we tried.
Our experiments show that dynamic thresholding using feedback has the potential to produce substantial and significant gains. While we have explored only one approach to this idea, through end-of-day relevance judgments, we can imagine more realistic interfaces, which might for example allow incremental judgments as tweets are pushed. In addition, we hope to incorporate social signals and other noncontent features into the relevance and novelty components of our system, with the goal of retaining our simple approach to thresholding, while improving overall effectiveness.
Acknowledgments. This work was supported in part by the U.S. National Science Foundation under awards IIS1218043 and CNS-1405688 and the Natural Sciences and Engineering Research Council of Canada (NSERC). Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.
7. REFERENCES
[1] J. Allan, R. Papka, and V. Lavrenko. On-line new event detection and tracking. SIGIR, 1998.
[2] J. G. Fiscus and G. R. Doddington. Topic detection and tracking evaluation overview. Topic Detection and Tracking. Kluwer, Norwell, MA, 2002.
[3] L. S. Larkey, F. Feng, M. Connell, and V. Lavrenko. Language-specific models in multilingual topic tracking. SIGIR, 2004.
[4] J. Lin, M. Efron, G. Sherman, Y. Wang, and E. M. Voorhees. Overview of the TREC-2015 Microblog track. TREC, 2015.
[5] J. Lin, R. Snow, and W. Morgan. Smoothing techniques for adaptive online language models: Topic tracking in tweet streams. SIGKDD, 2011.
[6] L. Tan and C. L. A. Clarke. Succinct queries for linking and tracking news in social media. CIKM, 2014.
[7] L. Tan, A. Roegiest, and C. L. A. Clarke. University of Waterloo at TREC 2015 Microblog Track. TREC, 2015.
[8] T. Tomokiyo and M. Hurst. A language model approach to keyphrase extraction. ACL Workshop on Multiword Expressions, 2003.
[9] Y. Wang, G. Sherman, J. Lin, and M. Efron. Assessor differences and user preferences in tweet timeline generation. SIGIR, 2015.
[10] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. Van Mulbregt. Topic tracking in a news stream. DARPA Broadcast News Workshop, 1999.
[11] Y. Yang, T. Ault, T. Pierce, and C. W. Lattimer. Improving text categorization methods for event tracking. SIGIR, 2000.
[12] X. Zhao and K. Tajima. Online retweet recommendation with item count limits. Web Intelligence and Intelligent Agent Technologies, 2014.

1012

Toward Estimating the Rank Correlation between the Test Collection Results and the True System Performance

Julián Urbano
Universitat Pompeu Fabra Barcelona, Spain
urbano.julian@gmail.com

Mónica Marrero
National Supercomputing Center Barcelona, Spain
monica.marrero@bsc.es

ABSTRACT
The Kendall  and AP rank correlation coefficients have become mainstream in Information Retrieval research for comparing the rankings of systems produced by two different evaluation conditions, such as different effectiveness measures or pool depths. However, in this paper we focus on the expected rank correlation between the mean scores observed with a test collection and the true, unobservable means under the same conditions. In particular, we propose statistical estimators of  and AP correlations following both parametric and non-parametric approaches, and with special emphasis on small topic sets. Through large scale simulation with TREC data, we study the error and bias of the estimators. In general, such estimates of expected correlation with the true ranking may accompany the results reported from an evaluation experiment, as an easy to understand figure of reliability. All the results in this paper are fully reproducible with data and code available online.
Keywords
Evaluation; Test Collection; Correlation; Kendall; Average Precision; Estimation
1. INTRODUCTION
The Kendall  [3] and AP [8] rank correlation coefficients are widely used in Information Retrieval to compare rankings of systems produced by different evaluation conditions, such as different assessors [6], effectiveness measures [4] or topic sets [1]. One reason for this success is their simplicity: they provide a single score that is easy to understand.
In this paper we tackle the problem of estimating the correlation between the ranking of systems obtained with a test collection and the true ranking under the same conditions. Such estimates can make a nice companion to a set of evaluation results, as a single figure of the reliability of the experiment. Voorhees and Buckley [7] proposed to report a similar figure in terms of sensitivity, that is, the minimum
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914752

difference required between two systems to ensure a maximum error rate in relative comparisons. Common practice nowadays is to report the p-value of a statistical significance test run either for each pair of systems (e.g. t-test) or for the whole set (e.g. ANOVA and F -test). They provide a sense of confidence about individual pairs of systems or about a swap somewhere in the ranking, but they do not give a general idea of how similar the observed ranking is to the truth.
We propose parametric and non-parametric approaches to estimate the  and AP correlations. Through large scale simulation with TREC data, we show that they have very low bias and small error even for mid-sized collections.

2. CORRELATION BETWEEN
TWO RANKINGS
Let A = a1, . . . , am and B = b1, . . . , bm be the mean scores of the same set of m systems as observed under two different evaluation conditions, such that ai and bi refer to the i-th system. In many situations we are interested in the distance between the two rankings. Considering systems in pairs, a distance can be computed by counting how many pairs are concordant or discordant between the two rankings: a pair is concordant if their relative order is the same in both rankings, and discordant if it is the opposite. Kendall [3] followed this idea to define his  correlation coefficient

 = #concordants-#discordants = 1-2 #discordants , (1)

total

total

which evaluates to -1 when the rankings are reversed, +1 when they are the same, and 0 when there are as many concordant pairs as there are discordant. Note that the term #discordants/total can be interpreted as the expected value of a random experiment: pick two arbitrary systems and return 1 if they are discordant, or 0 if they are concordant. The Kendall  coefficient can thus be interpreted in terms of the probability of discordance.
Yilmaz et al. [8] followed this idea to define a correlation coefficient with the same rationale as Average Precision. It is similar to Kendall  , but it penalizes more if swaps occur between systems at the top of the ranking, much like AP penalizes more if the non-relevant documents appear at the top of the results. In particular, they considered that one of the rankings, say B, is the true ranking and the other one is an estimate of it. The random experiment is now as follows: pick one system at random from A and another one ranked above it, and return 1 if they are discordant, or 0 if they are concordant. Their AP correlation coefficient can then be defined just as in (1) as follows:

1033

AP

=

1

-

2 m-

1

m

#discordants above i i-1

.

(2)

i=2

Note that AP also ranges between -1 and +1.

3. EXPECTED CORRELATION
WITH THE TRUE RANKING
The previous section contemplated the case where we compute the correlation between two given rankings A and B. In this section we study the case where we are given a ranking A obtained with the sample of topics in the test collection, and want to estimate its correlation with the true ranking B over the population of topics, which is of course unknown. For simplicity, let us first assume that the systems are already sorted in descending order by their mean score. Let us further define Dij as the random variable that equals 1 if systems i and j are discordant and 0 otherwise, that is, whether they are swapped in the true ranking. Both  and AP can be re-defined from (1) and (2) in terms of Dij alone:



=

1

-

4 m(m -

1)

m-1

m

Dij ,

(3)

i=1 j=i+1

AP

=

1

-

2 m-1

m-1

i-1

Dij i-1

.

(4)

i=1 j=1

Since they are just a linear combination of random variables, their expectations are as in (3) and (4) but replacing Dij with E[Dij]. Note that each Dij is a Bernoulli random variable, so its expectation is just the probability of discordance E[Dij] = P (µi - µj < 0) = pij. The problem of estimating the correlation with the true ranking thus boils down to estimating the probability that any two systems are swapped. The next subsection presents four ways of achieving this.

3.1 Estimating the Probability of Discordance
Since each pij is estimated independently from the other systems, let us simplify notation here to just p. In addition, let X1, . . . , Xn be the differences in effectiveness between the two systems and for each of the n topics in the collection. The problem is therefore to estimate p = P (µ < 0) from these n observations. Recall that systems are assumed to be ranked by mean observed scores, so X > 0.
In the following we present two parametric estimators based on the Central Limit Theorem (CLT) and then two non-parametric estimators based on resampling.

3.1.1 Maximum Likelihood (ML)
The CLT tells us that X is approximately normally distributed with mean µ and variance 2/n as n  . Using the cdf of the normal distribution we can therefore estimate the probability of discordance. However, our estimates are likely off with small samples (see Section 3.1.2), so we assume Xi  N (µ, 2) and employ the t distribution to account for the uncertainty in estimating 2. Standardizing, we have that n(X - µ)/  t(n - 1), so

p = P (µ < 0)  Tn-1

 -n

µ^ ^

,

(5)

where Tn-1 is the cdf of the t distribution with n - 1 degrees of freedom. The estimates µ^ and ^ are computed via Maximum Likelihood as

µ^ =

X

=

1 n

Xi,

(6)

^ = s · Cn,

(7)

s=

1 n-1

(Xi - X)2,

Cn =

n

- 2

1

((n - 1)/2) (n/2)

,

where s is the sample standard deviation. The Cn factor [2]
ensures that E[^] = . This bias correction is applied because, even though s2 is an unbiased estimator of 2, by
Jensen's inequality s is not an unbiased estimator of .

3.1.2 Minimum Squared Quantile Deviation (MSQD)

The problem when estimating  from a small sample is

that the observations are likely to be concentrated around

the mean and seldom occur near the tails. As a consequence,

(7) is likely to underestimate the true dispersion in the pop-

ulation. If the sample contains a few dozen observations this

is not expected to be a problem, but with very small samples

of, say, just 10 topics, it might be.

We propose a new and generic estimator to avoid this

problem. Let us consider a distribution function F with

parameter . A random sample from this distribution is

expected to uniformly cover the quantile space, that is, all

quantiles are equally likely to appear in the sample. Thus,

when we are given a sample we may force them to uniformly

cover the quantile space and then select the  that minimizes

the observed deviations. For instance, if our sample contains

only one observation, we force it to correspond to the quan-

tile 1/2; if we have two observations then we force them to

be the quantiles 1/3 and 2/3. In general, if Ri is the rank of

Xi within the sample, it will correspond to the Ri/(n + 1)

quantile, which is F -1

Ri n+1

;



. The squared quantile devi-

ation of an observation Xi is therefore

SQD(Xi; ) =

F -1

Ri n+

1

;



2
- Xi .

The Minimum Squared Quantile Deviation estimator is then the one that minimizes the sum of squared deviations:

^MSQD = arg min SQD (Xi; ).


Let us assume again a normal distribution, so that

F -1

Ri n+

1

;

µ,



 = µ +  2ei,

ei = erf-1

2

Ri n+

1

-

1

.

The sum of squared deviations is thus

µ2

-

2µ2e i

0

+

22e2i

-

2Xiµ

-

 2Xi 2ei

+

Xi2,

and the second term cancels out because ei = 0. To find the µ and  that minimize this expression, we simply differentiate, equal to 0, and solve. The partial derivatives are

dSQD dµ

=

dSQD d

=

2µ - 2Xi = 2nµ - 2

4e2i

-

 2 2Xiei,

Xi and

and therefore, the estimators are

1034

µ^

=

1 n

2

^ =

2

Xi = X,

Xi · erf-1

2

Ri n+1

-

1

erf -1

2

Ri n+1

-

1

2

.

(8) (9)

As above, the probability of discordance is estimated with the cdf of the t distribution as in (5), but using estimators (8) and (9) instead of (6) and (7).

3.1.3 Resampling (RES)

In both the ML and MSQD estimators above we assumed

that scores are normally distributed, but this is clearly not

strictly true. A non-parametric alternative is the use of re-

sampling to estimate the sampling distribution of the mean

and from there the probability of discordance.

Suppose we draw a random sample X1, . . . , Xn with re-

placement from our original observations, and compute their

sample mean X. This experiment is replicated T = 1, 000

times,

yielding

sample

means

X

1 ,

.

.

.

,

X

 T

.

By the law of

large numbers, the distribution of these sample means con-

verges to the sampling distribution of X as T  . The

probability of discordance can thus be estimated as the frac-

tion

of

times

that

X

 i

is

negative:

p

=

P (µ

<

0)



1 T

I

X

 i

<

0

.

(10)

3.1.4 Kernel Density (KD)
A potential problem with resampling from the original observations is again that estimates from very small samples are likely off. An alternative is to approximate the true pdf via Kernel Density Estimation, and use it to estimate the probability of discordance. The estimated pdf has the form

f^(x) =

1 nh

k

x - Xi h

,

where k is the pdf of the kernel and h is the bandwidth. Next, we need to estimate the sampling distribution of the mean, which is basically the distribution of the sum of n variables drawn from f^. For n = 2 this requires the evaluation of the self-convolution of f^ as follows:

f^X+X (x)

=

1 n2h2

ij

k

x-z -Xi h

k

z -Xj h

dz,

which involves the sum of n2 terms. In general, for n vari-

ables this requires the evaluation of nn terms, which is clearly

unfeasible even for small samples, so instead we resort to

Monte Carlo methods. As with the RES estimator, we gen-

erate mean

aXra.ndAomftesramT prleepXlic1a, t.i.o.n,sX, nthferopmrof^baabnidlitcyomofpudtiesctohre-

dance

is

estimated

as

the

fraction

of

times

that

X

 i

is

nega-

tive. We set T = 1, 000 replications and use gaussian kernels.

4. EVALUATION
4.1 Criteria
There are two properties of the correlation estimators that we are interested in, namely error and bias. Error refers to the expected difference between the estimate and the truth. Here we measure absolute error, thus quantifying the expected magnitude of the error when estimating the correlation of a given collection:

error = E |^(A) -  (A, µ)| .
Even if the error is small, it could tend to be in the same direction, that is, over- or underestimating the correlation. Bias refers to this tendency, measured as the expected difference between the estimated and the true correlation:
bias = E ^(A) -  (A, µ) .
If the bias is positive it means that the estimator tends to overestimate the correlations. In general, we seek estimators with small error and zero bias.
4.2 Methods, Data and Baselines
From the above definitions it is evident that we need to know the true ranking of systems µ, but this is of course unknown. To solve this problem we resort to the simulation method proposed by Urbano [5]. Given the topic-by-system matrix of scores B from an existing collection, it generates a new matrix A with the scores by the same set of systems over a new and random set of topics. There are two important characteristics of this method that are appealing for us. First, the simulated scores are realistic, as they maintain the same distributions and correlations among systems as in the original collection. Second, it is designed to ensure that the expected mean score of a system is equal to the mean score in the original collection, that is, E As = Bs. For us, this means that the true mean scores are fixed to be the mean scores in the original collection, that is, µs := Bs. This allows us to analyze the error and bias of the estimators with a large number of simulated, yet realistic test collections.
We use the TREC 6, 7 and 8 ad hoc collections as evaluated with Average Precision. As is common practice, we first drop the bottom 25% of results to avoid effects of possibly buggy systems. From each original collection, we simulate 1, 000 new collections of sizes n = 10, 20, . . . , 100 topics, leading to a total of 30, 000 simulated collections. For each of them, we estimate  and AP using each of the estimators defined above, and also compute the true correlations (recall that this is possible because the true system scores are fixed upfront when simulating new collections). Finally, for each correlation coefficient, original collection, topic set size and estimator, we compute expected error and bias.
Two baselines are used to compare our estimators to. They are based on a split-half method that randomly splits the available topic set in two subsets, and then computes the correlations as if one was the truth and the other one the estimate. This is replicated a number of times for different subset sizes, up to a maximum of n/2 topics. The observations are then used to fit a model and extrapolate the expected correlation with n topics. This simple estimator is found for instance in [7, 4]. Here we run 2, 000 replicates to fit the model y = a·eb·x, and sample topics with and without replacement, leading to baselines SH(w) and SH(w/o).
4.3 Results
Figure 1 shows that the error of the estimators is larger with small collections. This is somewhat expected, because collections with too few topics are unstable and the rankings of systems vary too much to begin with. The error seems to plateau at about 0.025 in all our estimators, though with small collections of just 10 topics they are expected to be off by about 0.065. With the usual 50 topics, the expected error is 0.035. We can finally observe that the typical SH

1035

Error 0.02 0.04 0.06 0.08 0.10 10 20 30 40 50 60 70 80 90 100
Error 0.02 0.04 0.06 0.08 0.10 10 20 30 40 50 60 70 80 90 100

tau - adhoc6

ML

RES

MSQD KD

SH(w/o) SH(w)

tauAP - adhoc6

topic set size tau - adhoc7

topic set size tauAP - adhoc7

Bias

0.00

0.04

0.08

10

20

30

tau - adhoc6

ML

RES

MSQD KD

SH(w/o) SH(w)

0.08

tauAP - adhoc6

0.04

Bias

0.00

70

60

50

40

30

20

10

100

90

80

70

60

50

40

topic set size tau - adhoc7

topic set size tauAP - adhoc7

100

90

80

0.08

0.08

0.04

Bias

0.04

Bias

Error 0.02 0.04 0.06 0.08 0.10 10 20 30 40 50 60 70 80 90 100
Error 0.02 0.04 0.06 0.08 0.10 10 20 30 40 50 60 70 80 90 100

0.00

0.00

100

90

80

70

60

50

40

30

20

10

100

90

80

70

60

50

40

30

20

10

topic set size tau - adhoc8

topic set size tauAP - adhoc8

topic set size tau - adhoc8

topic set size tauAP - adhoc8

0.08

0.08

0.04

Bias

0.04

Bias

Error 0.02 0.04 0.06 0.08 0.10 10 20 30 40 50 60 70 80 90 100
Error 0.02 0.04 0.06 0.08 0.10 10 20 30 40 50 60 70 80 90 100

0.00

0.00

100

90

80

70

60

50

40

30

20

10

100

90

80

70

60

50

40

30

20

10

topic set size

topic set size

Figure 1: Error of the estimators of  (left) and AP (right) for each of the three original collections.

estimators are clearly outperformed by all our proposed estimators. In general, with 30­40 topics they behave almost the same, but with small samples MSQD is slightly better.
Figure 2 shows that the correlations tend to be overestimated, especially with small collections, but this time we see clear differences among estimators. MSQD behaves much better than the others, especially with very small collections. With only 10 topics ML outperforms KD because there is just too little data to properly approximate the pdf , but with 20 or more topics it does a very good job at approximating the true distribution. ML, on the other hand, assumes a normal distribution and can therefore be less faithful to the data. Even at around 40­50 topics KD gets to slightly outperform MSQD for the same reason. Overall, they seem to plateau at about 0.004, and RES always performs worse than the others. Finally, the SH estimator with replacement has a roughly constant bias of about 0.055. The SH estimator without replacement shows a clearly biased behavior probably due to the choice of model.
5. CONCLUSION
In this paper we present two estimators of the Kendall  and AP rank correlation coefficients between the mean system scores produced by a test collection and the true, unobservable means. We proposed parametric and nonparametric alternatives, and through large scale simulation with realistic collections we showed that even with small topic sets the estimators have little bias and the errors are generally small with collections of medium size. These estimators may prove useful as an easy to understand indicator

topic set size

topic set size

Figure 2: Bias of the estimators of  (left) and AP (right) for each of the three original collections.

of reliability in the results of an evaluation experiment. In light of the expected error with individual collections,
our future work will mainly focus on the development of interval estimates. We also plan to study other estimators of discordance as well as the application of a fully bayesian approach to estimate correlations. All the results in this paper are fully reproducible with data and code available online at http://github.com/julian-urbano/sigir2016-correlation.
Acknowledgments. Work supported by the Spanish Government: JdC postdoctoral fellowship, and projects TIN201570816-R and MDM-2015-0502. Florentino dimisi´on.
References
[1] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. If I Had a Million Queries. In ECIR, 2009.
[2] W. H. Holtzman. The Unbiased Estimate of the Population Variance and Standard Deviation. Am. J. Psychology, 1950.
[3] M. G. Kendall. A New Measure of Rank Correlation. Biometrika, 1938.
[4] T. Sakai. On the Reliability of Information Retrieval Metrics Based on Graded Relevance. Inf. Proc. & Mngmnt, 2007.
[5] J. Urbano. Test Collection Reliability: A Study of Bias and Robustness to Statistical Assumptions via Stochastic Simulation. Information Retrieval, 2016.
[6] E. M. Voorhees. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In SIGIR, 1998.
[7] E. M. Voorhees and C. Buckley. The Effect of Topic Set Size on Retrieval Experiment Error. In SIGIR, 2002.
[8] E. Yilmaz, J. Aslam, and S. Robertson. A New Rank Correlation Coefficient for Information Retrieval. In SIGIR, 2008.

1036

Two Sample T-tests for IR Evaluation: Student or Welch?
Tetsuya Sakai
Waseda University, Japan.
tetsuyasakai@acm.org

ABSTRACT
There are two well-known versions of the t-test for comparing means from unpaired data: Student's t-test and Welch's t-test. While Welch's t-test does not assume homoscedasticity (i.e., equal variances), it involves approximations. A classical textbook recommendation would be to use Student's t-test if either the two sample sizes are similar or the two sample variances are similar, and to use Welch's t-test only when both of the above conditions are violated. However, a more recent recommendation seems to be to use Welch's t-test unconditionally. Using past data from both TREC and NTCIR, the present study demonstrates that the latter advice should not be followed blindly in the context of IR system evaluation. More specifically, our results suggest that if the sample sizes differ substantially and if the larger sample has a substantially larger variance, Welch's t-test may not be reliable.
Keywords
statistical significance; test collections; topics; variances
1. INTRODUCTION
The present study concerns IR evaluation where two means from different samples need to be compared. The classical approach for this would be to employ a two-sample (i.e., unpaired) t-test to discuss whether, given the observed sample means, the population means may be different. There are two well-known versions of the two-sample t-test: Student's t-test and Welch's t-test. While Welch's t-test does not assume homoscedasticity (i.e., equal variances), it involves approximations. A classical textbook recommendation would be to use Student's t-test if either the two sample sizes n1, n2 are similar or the two sample variances V1, V2 are similar, and to use Welch's t-test only when both of the above conditions are violated [3]. However, a more recent recommendation seems to be to use Welch's t-test unconditionally. For example, Daniel Laken's blog posted on January 26, 2015 recommends researchers to "Always use Welch's t-test instead of Student's t-test," while demonstrating the superiority of Welch's t-test using simu-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914684

lated data1. The t.test function provided in the stats library of the R language uses Welch's t-test by default2; hence researchers who use this function as a black box may well be using Welch's t-test all the time. In fact, in as far back as 1981, Gans [1] recommended "the automatic use of the Welch test alone" as an option, based on simulations. The present study seeks to obtain the right recommendations for the purpose of IR system evaluation using real data from TREC and NTCIR.

2. PRIOR ART
In the IR evaluation literature, researchers have focussed mainly on the paired data setting, because the most basic method for comparing two IR systems is to use the same test collection with a single topic set to compare two systems. For example, Smucker et al. [7] compared the sign test, the Wilcoxon signed rank test, the paired t-test, the bootstrap test and the randomisation test from the viewpoint of how similar the p-values of different test types are to one another, and concluded that the use of the two nonparametric tests (sign and Wilcoxon) should be discontinued. Urbano et al. [8] conducted a follow-up study on the same set of paired significance tests, using repeated topic set splitting experiments in a way similar to earlier studies (e.g. [10]) to quantify the discrepancies across the pairs of topic sets for comparing two systems. Contrary to the recommendation by Smucker et al., Urbano et al. report that "the permutation test is not optimal under any criterion."
In contrast to the aforementioned studies, the present study concerns two-sample t-tests for comparing two means, with sample sizes n1, n2. Possible applications of two-sample tests in IR include comparing sets of clicks from two different search engines, between-subject design user experiments, and comparing the "hardness" of two test collections using the same IR system. Sakai [4] used the two-sample bootstrap test (in addition to the paired bootstrap test) for the purpose of comparing different evaluation measures in terms of "discriminative power."

3. T-TEST: STUDENT'S AND WELCH'S

The common assumptions are as follows. We have scores

x11, . . . , x1n1 that each obey N (1, 12), as well as x21, . . . , x2n1

that each obey N (2, 22). The population means and variances

(·'s and ·2's) are unknown, and we want to test if 1 = 2,

given sample means x¯1

=

1 n1

n1 j=1

x1j

and x¯2

=

1 n2

n2 j=1

x2j

.

Let S1 =

n1 j=1

(x1j

- x¯1)2, S2

=

n2 j=1

(x2j

-

x¯2)2

for

later

purposes.

1http://daniellakens.blogspot.nl/2015/01/ always-use-welchs-t-test-instead-of.html 2http://127.0.0.1:27533/library/stats/html/t.test.html

1045

3.1 Student's t-test

Student's two-sample t-test further assumes homoscedasticity: 12 = 22. While this is a strong assumption, it is also known that this test is quite robust to assumption violations. For this test, we
first define a pooled variance V = (S1 + S2)/ where  is the degree of freedom for V given by  = 1 +2, 1 = n1 -1, 2 = n2 - 1. The test statistic is:

t0 =

x¯1 - x¯2 V (1/n1 + 1/n2)

(1)

which is compared against t(; ), the critical t value for  = n1 + n2 - 2 degrees of freedom with the significance level of .

3.2 Welch's t-test

The good news about Welch's t-test is that it does not assume homoscedasticity; the bad news is that it involves approximations [3], as briefly discussed below. Let V1 = S1/1, V2 = S2/2. For this test, the test statistic is

tw0 =

x¯1 - x¯2 V1/n1 + V2/n2

(2)

which is compared against t(; ), where



=

(

V1 n1

+

V2 n2

)2/{

(V1/n1 1

)2

+

(V2/n2 2

)2

}

.

(3)

Welch's t-test approximates the distribution of the following statistic by a 2 distribution with 0 degrees of freedom:

W

=

0(

V1 n1

+

V2 n2

)/(

12 n1

+

22 n2

)

.

(4)

Furthermore, it estimates 0 as ^0 =  using Eq. 3. Do these approximations have any consequences for IR evaluation?

3.3 Analytical Relationships
Nagata [3] clarifies the relationship between the above two ttests analytically. Let a = n2/n1, b = V2/V1. Then it is easy to derive that the ratio of the two test statictics tw0/t0 is given by:

tw0 t0

= g(a, b) =

(a

+ 1){n1(ab + 1) - (b + 1)} (a + b)(n1(a + 1) - 2)

.

(5)

Note that g(1, b) = g(a, 1) = 1. Hence, if either n1 = n2 or V1 = V2 holds, then t0 = tw0 holds. In practice, if the larger sample is no larger than 1.5 times the other, or if the larger variance is no larger than 1.5 times the other, t0 and tw0 will differ by at most 20% or so [3]. As for the degrees of freedom, it can be shown that:

 

= h(a, b)

=

{a2(an1

(a + b)2(n1 - 1)(an1 - 1) - 1) + b2(n1 - 1)}{(a + 1)n1

- 2}

.

(6)

Since h(1, 1) = 1 holds, having n1 = n2 and V1 = V2 is a sufficient condition for obtaining  = . Also, it can be verified that h(a, b) = / is much smaller than one if b = V2/V1 is close to one and a = n2/n1 is far from one. That is, Welch's t-test

has relatively low statistical power (due to its degree of freedom  being much smaller than Student's ) when the variances are

roughly equal but the sample sizes are quite different.

4. EXPERIMENTS
In order to compare Student's and Welch's t-tests in terms of reliability, we adopt a method similar to that of Webber et al. [11]: given a topic set of size n, with m runs that processed these topics,

Table 1: Test collections and runs used in this study.

TREC99

NTCIR97

Topics (n) 601-700 minus 672 (99) T41-385 minus 86, 331, 362 (97)

Runs (m) TREC 2004 robust (110) NTCIR-7 IR4QA Chinese (40)

Qrels

L2: relevant; L1: partially relevant; L0: judged nonrelevant

Table 2: Sample size ratios experimented in this study.

Target ratio

Actual ratio (TREC99) Actual ratio (NTCIR97)

50:50 (a = 1.0)

50:49

50:47

40:60 (a = 1.5)

40:59

40:57

30:70 (a = 2.3) 10:90 (a = 9.0)

30:69 10:89

30:67 10:87

randomly partition the topics into two sets of size n1 and n2, respectively. For each of the m runs and a given evaluation measure, conduct a two-sided, two-sample test to see if the two means for the same run are statistically significant. The ground truth is that they are not, since the scores actually come from the same system. The random partitioning is done B = 1000 times, so a test collection with m runs will yield Bm = 1000m p-values for each significance test type with a given evaluation measure.
Table 1 shows a summary of the two data sets used in this study. We chose them because (a) we wanted about n = 100 topics (or more if available) with graded relevance assessments; (b) we wanted data (with many runs) from different evaluation conferences to ensure generalisability. "TREC99" comprises 110 runs from the TREC 2004 robust track [9], with 99 robust track topics. While this data set has many runs, the 99 topics come from two robust track rounds: 50 from TREC 2003 and 49 from 2004. In contrast, "NTCIR97," which comprises 40 Simplified Chinese runs from the NTCIR-7 ACLIA IR4QA task [6], uses 97 topics that originate from a single round of the task.
For each data set with a given evaluation measure, we experimented with four different sample size ratios a = n2/n1 as shown in Table 2 to obtain a total of 4000m p-values. Recall that, according to Nagata [3], Student's and Welch's t statistics should not be vastly different for the a = 1 (n1 : n2 = 50 : 50) and a = 1.5 (n1 : n2 = 40 : 60) settings, regardless of how the two sample variances differ. We experimented with four evaluation measures: (binary) Average Precision (AP), Q-measure (Q), normalised Discounted Cumulative Gain (nDCG) and normalised Expected Reciprocal Rank (nERR). Unlike the other three measures, nERR is known to be suitable for navigational information needs due to its diminishing return property; for this very reason, it is known to be statistically less stable than the other measures [5].

5. RESULTS AND DISCUSSIONS
Figure 1 plots Welch's p-values against Student's p-values for the 110,000 two-sample t-tests conducted with the TREC99 data, for nDCG ((a)-(d)) and nERR ((e)-(h)) with different target sample size ratios. The graphs for TREC99 with AP and Q, as well all all graphs for NTCIR97, are omitted due to lack of space. However, the overall picture is the same for all evaluation measures and across the two data sets, and we believe that our findings are general. In each graph, the blue rectangle represents the situation where Student's t-test obtains a p-value smaller than  = 0.05 (i.e., a false positive) while Welch's t-test does not; the red rectangle represents the opposite situation; the bottom left square represents the situation where both tests obtain a false alarm at  = 0.05. It can be observed that when the target sample size ratio is a = 1 (n1 : n2 = 50 : 50), the two tests are indeed virtually identical and false alarms are very rare; as we gradually increase the sample size ratio until it reaches a = 9.0 (n1 : n2 = 10 : 90), we obtain more and more false alarms on both sides of the diagonal.

1046

Figure 1: Welch's vs. Student's p-values: TREC99; 110 runs; nDCG (top) and nERR (bottom).

Tables 3 and 4 show the false positive rates for all of our TREC99 and NTCIR97 experiments, respectively. Based on the discussion provided in Section 3.3, we categorised the observations into three classes: the first is for those where the variance ratio satisfies 2/3  b  3/2: recall that Student's and Welch's t statistics are expected to be similar to each other in this situation. The other two classes (b < 2/3 and b > 3/2) represent the situations where the sample variances are very different. For example, Table 3 Section (III) Column (d) provides the following information about our TREC99 experiments for nDCG with the sample size ratio a = 9.0 (with the actual sample sizes n1 = 10, n2 = 89 as shown in Table 1):
· For 64,447 out of the 110,000 observations, the sample variance ratio b = V2/V1 satisfied 2/3  b  3/2; for these observations, the false positive rate was 4.0% for Student's t-test and 2.6% for Welch's t-test;
· For 16,614 out of the 110,000 observations, b < 2/3; for these observations, the false positive rate was 4.9% for Student's t-test and 0.7% for Welch's t-test;
· For 28,939 out of the 110,000 observations, b > 3/2; for these observations, the false positive rate was 5.6% for Student's t-test and 14.1% for Welch's t-test;
· Overall, for the 110,000 observations, the false positive rate was 4.6% for Student's t-test and 5.3% for Welch's t-test. Thus, Welch's t-test slightly underperforms Student's, due to its very high false positive rate for the b > 3/2 setting.
More generally, we can observe the following consistent trends from Tables 3 and 4:
· When the two sample sizes are equal (Column (a)), Student's and Welch's t-tests perform equally well, regardless of the range of b3. The overall false positive rates are 4.9-5.2%.
3When n1 = n2, note that the variance ratio conditions b < 2/3

· When the two variances are similar (2/3  b  3/2), the false positive rates of the two tests are very small (1.0-4.8%), although, as discussed immediately below, Student's t-test yields more false positive than Welch's when the sample size ratio a = n2/n1 is extreme (Column (d));
· As we increase the sample size ratio a = n2/n1, Welch's ttest yields more and more false positives when b = V2/V1 > 3/2. That is, if the sample sizes differ substantially and if the larger sample has a substantially larger variance, Welch's ttest may not be reliable for two-sample IR evaluation. In particular, when a = n2/n1 = 9 (Column (d)), the false positive rates for Welch's t-test are 14.1-24.6% whereas those for Student's t-test are only 4.4-13.5%. Whereas, as we increase the sample size ratio a, Student's t-test yields more and more false positives when b  3/2 (i.e., 2/3  b  3/2 or b < 2/3), but this tendency is not as marked as Welch's for the b > 3/2 range. As a result, in terms of the overall false positive rates, Welch's t-test is actually slightly less reliable than the Student's t-test when a is very large.
The above results, based on real IR system runs from both TREC and NTCIR, suggest that the advice "Always use Welch's t-test instead of Student's t-test" should not be followed blindly in IR system evaluation.
6. CONCLUSIONS
For the purpose of reliable IR system evaluation, we compared two versions of two-sample t-tests: Student's t-test (which assumes homoscedasticity) and Welch's t-test (which relies on approximated distributions). Our topic set splitting experiments with runs from
and b > 3/2 are equivalent: it is just a matter of swapping the two samples, since the two tests are symmetric with respect to the two samples. So we should expect similar results in Column (a) for these two ranges of b. The slight caveat is that we actually have n1 n2 rather than n1 = n2, as n1 + n2 = 99 or 97.

1047

Table 3: TREC99 false positives at  = 0.05: Student/Welch.

The higher false positive rate in each condition is shown in

bold. The total number of observations for each variance ra-

tio is shown in parentheses.

(a) 50:50 a = 1.0

(b) 40:60 a = 1.5

(c) 30:70 a = 2.3

(d) 10:90 a = 9.0

2/3  b  3/2 b < 2/3
b > 3/2
All
2/3  b  3/2 b < 2/3
b > 3/2
All
2/3  b  3/2 b < 2/3
b > 3/2
All
2/3  b  3/2 b < 2/3
b > 3/2
All

3.5%/3.5% (91,286)
11.9%/12.0% (9,192)
15.0%/14.7% (9,522)
5.2%/5.2% (110,000)
3.7%/3.7% (90,299)
11.1%/11.1% (9,620)
13.3%/13.2% (10,081)
5.2%/5.2% (110,000)
4.4%/4.4% (94,936)
7.7%/7.7% (7,640)
8.4%/8.3% (7,424)
4.9%/4.9% (110,000)
4.8%/4.7% (107,590) 10.5%/10.8% (1,165) 13.9%/13.5% (1,245) 4.9%/4.9% (110,000)

(I) AP 3.4%/3.4%
(90,405) 10.9%/8.5%
(8,887) 13.0%/15.7%
(10,708) 4.9%/5.0% (110,000)
(II) Q 3.5%/3.5%
(89,563) 10.5%/7.9%
(9,252) 11.6%/14.3%
(11,185) 4.9%/5.0% (110,000)
(III) nDCG 4.4%/4.4%
(94,480) 7.0%/5.4%
(7,076) 7.9%/9.5%
(8,444) 4.9%/4.9% (110,000)
(IV) nERR 4.4%/4.4% (107,161) 10.4%/8.3%
(1,126) 14.4%/16.8%
(1,713) 4.6%/4.7% (110,000)

3.2%/3.1% (87,833)
9.7%/5.7% (9,374)
9.9%/15.9% (12,793)
4.5%/4.8% (110,000)
3.5%/3.4% (86,872)
8.9%/4.9% (9,763)
8.5%/14.2% (13,365)
4.6%/4.8% (110,000)
4.2%/4.2% (90,837)
6.6%/4.0% (7,710)
7.6%/12.1% (11,453)
4.7%/5.0% (110,000)
4.3%/4.3% (105,953) 6.0%/3.1% (1,196) 14.1%/20.6% (2,851) 4.6%/4.7% (110,000)

2.8%/1.6% (62,674)
7.7%/0.7% (16,938)
6.0%/18.4% (30,388)
4.5%/6.1% (110,000)
3.2%/1.9% (61,402)
7.6%/0.7% (17,285)
5.5%/16.9% (31,313)
4.5%/6.0% (110,000)
4.0%/2.6% (64,447)
4.9%/0.7% (16,614)
5.6%/14.1% (28,939)
4.6%/5.3% (110,000)
3.0%/2.1% (87,017)
1.7%/0.1% (7,485)
13.5%/24.2% (15,498)
4.4%/5.1% (110,000)

both TREC and NTCIR are consistent across different evaluation measures and across these two different IR venues. While neither our equal variance settings nor our equal sample size settings do not demonstrate any advantages of Student's t-test over Welch's t-test, our results do suggest that if the sample sizes differ substantially and if the larger sample has a substantially larger variance, Welch's t-test may be less reliable. Hence we argue that the advice "Always use Welch's t-test instead of Student's t-test" should not be followed blindly in IR system evaluation.
In practice, we recommend IR researchers to examine the sample sizes n1, n2 and the sample variances V1, V2 first and then make a conscious decision, rather than (say) relying on a default setting in the t.test function provided in R. We also recommend IR researchers to report explicitly which version of the two-sample ttest was used in their experiments, even if we may be able to spot a Welch's t-test when the degree of freedom reported is not an integer (Eq. 3).
Acknowledgement
I would like to thank Professor Yasushi Nagata for his helpful comments on my results.
7. REFERENCES
[1] D. J. Gans. Use of a preliminary test in comparing two sample means. Communications in Statistics - Simuation and Computation, 10(2):163­174, 1981.

Table 4: NTCIR97 false positives at  = 0.05: Student/Welch.

The higher false positive rate in each condition is shown in

bold. The total number of observations for each variance ra-

tio is shown in parentheses.

(a) 50:50 a = 1.0

(b) 40:60 a = 1.5

(c) 30:70 a = 2.3

(d) 10:90 a = 9.0

2/3  b  3/2 b < 2/3
b > 3/2
All
2/3  b  3/2 b < 2/3
b > 3/2
All
2/3  b  3/2 b < 2/3
b > 3/2
All
2/3  b  3/2 b < 2/3
b > 3/2
All

4.3%/4.3% (33,683) 8.8%/9.2% (3,233) 9.7%/8.9% (3,084) 5.1%/5.0% (40,000)
3.9%/3.9% (33,043) 9.0%/9.3% (3,575) 10.4%/9.9% (3,382) 4.9%/4.9% (40,000)
1.8%/1.8% (29,136) 12.9%/13.5% (5,578) 13.7%/13.0% (5,286) 4.9%/4.9% (40,000)
2.0%/2.0% (32,698) 17.9%/18.5% (3,715) 20.1%/19.2% (3,587) 5.1%/5.1% (40,000)

(I) AP 4.3%/4.3%
(33,587) 9.3%/6.1%
(2,965) 9.0%/11.8%
(3,448) 5.0%/5.1%
(40,000) (II) Q
4.1%/4.1% (33,085)
10.3%/7.1% (3,203)
9.5%/11.9% (3,712)
5.1%/5.1% (40,000) (III) nDCG
1.8%/1.8% (29,193)
14.2%/11.1% (5,076)
12.6%/16.1% (5,731)
4.9%/5.0% (40,000) (IV) nERR
1.7%/1.7% (32,303)
18.0%/14.7% (3,498)
18.7%/23.2% (4,199)
4.9%/5.1% (40,000)

4.2%/4.2% (32,716)
10.7%/5.1% (2,949)
7.5%/14.8% (4,335)
5.0%/5.4% (40,000)
4.1%/4.0% (32,135)
10.7%/5.6% (3,300)
8.1%/15.4% (45,65)
5.1%/5.4% (40,000)
1.8%/1.7% (27,739)
14.9%/8.2% (5,318)
9.9%/17.7% (6,943)
4.9%/5.3% (40,000)
1.5%/1.4% (31,171)
18.2%/10.7% (3,625)
15.1%/24.5% (5,204)
4.8%/5.3% (40,000)

3.6%/2.3% (24,263)
7.2%/0.4% (6,277)
6.8%/23.4% (9,460)
4.9%/7.0% (40,000)
3.5%/2.2% (23,172)
7.6%/0.4% (6,766)
6.4%/23.2% (10,062)
4.9%/7.2% (40,000)
2.2%/1.1% (18,548)
11.7%/1.1% (8,533)
4.4%/22.4% (12,919)
4.9%/8.0% (40,000)
2.2%/1.0% (21,570)
12.4%/1.7% (7,159)
5.0%/24.6% (11,270)
4.9%/7.8% (40,000)

[2] D. Hawking and N. Craswell. The very large collection and web tracks. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, chapter 9. The MIT Press, 2005.
[3] Y. Nagata. How to Understand Statistical Methods (in Japanese). Nikkagiren, 1996.
[4] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of ACM SIGIR 2006, pages 525­532, 2006.
[5] T. Sakai. Metrics, statistics, tests. In PROMISE Winter School 2013: Bridging between Information Retrieval and Databases (LNCS 8173), pages 116­163, 2014.
[6] T. Sakai, N. Kando, C.-J. Lin, T. Mitamura, H. Shima, D. Ji, K.-H. Chen, and E. Nyberg. Overview of the NTCIR-7 ACLIA IR4QA task. In Proceedings of NTCIR-7, pages 77­114, 2008.
[7] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of ACM CIKM 2007, pages 623­632, 2007.
[8] J. Urbano, M. Marrero, and D. Martín. A comparison of the optimality of statistical significance tests for information retrieval evaluation. In Proceedings of ACM SIGIR 2013, pages 925­928, 2013.
[9] E. M. Voorhees. Overview of the TREC 2004 robust retrieval track. In Proceedings of TREC 2004, 2005.
[10] E. M. Voorhees. Topic set size redux. In Proceedings of ACM SIGIR 2009, pages 806­807, 2009.
[11] W. Webber, A. Moffat, and J. Zobel. Score standardization for inter-collection comparison of retrieval systems. In Proceedings of ACM SIGIR 2008, pages 51­58, 2008.

1048

What Makes a Query Temporally Sensitive?
Craig Willis, Garrick Sherman, and Miles Efron
Graduate School of Library and Information Science University of Illinois at Urbana-Champaign
{willis8, gsherma2, mefron}@illinois.edu

ABSTRACT
This work takes an in-depth look at the factors that affect manual classifications of "temporally sensitive" information needs. We use qualitative and quantitative techniques to analyze 660 topics from the Text Retrieval Conference (TREC) previously used in the experimental evaluation of temporal retrieval models. Regression analysis is used to identify factors in previous manual classifications. We explore potential problems with the previous classifications, considering principles and guidelines for future work on temporal retrieval models.
1. INTRODUCTION
A growing body of information retrieval research argues that temporality should be modeled explicitly when scoring and ranking documents with respect to users' queries. Researchers have explored a variety of temporal retrieval models that explicitly incorporate time into document ranking [7, 2, 1]. They refer to general classes of "temporal queries" or "temporal information needs." Models have been proposed for "recency queries" [7, 2], "time-sensitive queries" [1], "implicitly temporal queries" [8], and "temporally biased queries" [5]. For evaluation, these studies rely on manual classifications of topics into temporal categories.
In this short paper, we take a deeper look into these manually classified topics to develop a clear understanding of what makes a query temporally sensitive? Previous manual classifications combine the temporal distribution of judged-relevant documents with common-sense notions of topic temporality without a clear explanation of the criteria or processes used in classification. If we cannot explain the processes being modeled, use of these manually classified topics for evaluation is of limited value.
To address this question, we analyze 660 topics from the Text Retrieval Conference (TREC) previously used in the experimental evaluation of temporal retrieval models. We employ qualitative and quantitative methods to identify topic characteristics that might affect the manual assessment of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914703

"temporal-sensitivity." The resulting coded topics are used in a set of regression analyses to assess the relationships between these characteristics and manually assigned categories. This paper's main contribution is an empirical assessment of the complexities that underpin temporal IR. This assessment helps us understand earlier temporal IR studies, while also suggesting novel ways to incorporate time effectively into retrieval.

2. TIME-SENSITIVE QUERIES

Topics 51-200 (Ad-hoc) 301-450 (Ad-hoc)
N1-100 (Novelty)
851-1050 (Blog) MB1-110 (Microblog)

Collections TREC Disks 1-2 AP (1988-89) TREC Disks 4-5 FT (1991-94); LA Times (1988-89)

AQUAINT

Xinhua

(1996-2000); NYT (1999-

2000)

Blog06 (Dec 6, 2005 - Feb

21, 2006)

Tweets 2011 (Jan 24,

2011 - Feb 8th, 2011)

Studies Jones & Diaz (2007)
Efron & Golovchinsky (2011); Dakka, Gravano & Ipeirotis (2012) Jones & Diaz (2007)
Peetz, Meij & de Rijke (2013) Efron, Lin, de Vries (2014)

Table 1: TREC topics and collections analyzed by the authors and their use in prior temporal retrieval studies.
In this section, we review examples of studies focused on temporal relevance. The list of topics and collections used in each of the studies are listed in Table 1.
Jones and Diaz [5] define three classes of queries based on the manual analysis of topics: temporally ambiguous (requesting multiple events), temporally unambiguous (requesting a single event), and atemporal (having no preference). Jones and Diaz manually classify 100 TREC topics based only on their title, description, and narrative fields. They also include 2003 Novelty track topics because they include topics classified as "event" or "opinion," which the authors suggest correspond to the "temporally unambiguous" and "atemporal" categories, respectively.
Efron and Golovchinsky [2] investigate models for recency queries. Topics are classified as "recency" if at least 2/3 of the relevant documents occur after the median document time and the topic has a "bona fide temporal dimension" based on manual review, the specific criteria for which are not specified. The resulting classification consists of 61 recency queries.
Dakka, Gravano, and Ipeirotis [1] investigate a broad class of queries which they refer to as "time-sensitive." They hypothesize that there are queries for which more relevant doc-

1065

uments are found at specific points in time, not just recently. They manually examine the title, description and narrative of each topic and identify queries associated with specific news events. If the topic information is insufficient to make a decision, they analyze the distribution of judged-relevant documents. The resulting classification consists of 86 temporally sensitive queries.
Finally, Peetz, Meij, and de Rijke [9] investigate the effect of temporal bursts in estimating query models. Building on the earlier studies, they evaluate their models using the previous manual classifications as well as a new collection based on TREC Blog06. As in the previous studies, the authors manually construct a subset of "temporal" queries based on topic descriptions and relevant document distributions. No specific criteria for classification are given.
3. WHAT MAKES A QUERY TEMPORALLY SENSITIVE?
Given the complex landscape described in the previous section, what in general makes a query temporally sensitive? Dakka et al [1] present a compelling definition. A query is temporally sensitive if "the relevant documents for the query are not spread uniformly over time, but rather tend to be concentrated at restricted intervals." This is an essential point, since many temporal retrieval models rely on the temporal distribution of results in document scoring. This is also why we do not include the topics developed for the NTCIR Temporalia test collections [4]: they are primarily concerned with temporal topicality, i.e. queries about a certain time, in contrast to our focus on temporal sensitivity, which relates to the unequal occurrence of relevant documents at certain points in time. Still, the distribution of relevant documents alone is not sufficient to determine true temporality. To address this, most of the studies listed above rely on common-sense notions of temporality based on the topic content considered independently of the distribution of relevant documents. A primary goal of the current study is to look deeper into these common-sense criteria with the aim of providing researchers a firmer basis for assessing which queries are likely to have a temporal dimension.
4. METHODS
4.1 Qualitative coding
We use content analysis [6] to identify characteristics of TREC topics potentially associated with temporal sensitivity. 660 topics were selected from the TREC Ad-hoc, Novelty, Blog, and Microblog tracks, all previously used by researchers to evaluate temporal retrieval models. The complete set of topics used in this study are listed in Table 1 along with the temporal constraints of each collection or sub-collection.
Two of the authors participated in the development of the codebook and subsequent coding of topics. Codes were defined based on characteristics of topics expected to be related to temporal sensitivity, informed by the literature. During this process, code definitions were refined and clarified. In the final coding, only topic title and description were used. Of the 660 topics, 330 were coded by both coders to allow for inter-rater consistency analysis. The final codebook is too large to publish in this short paper, but is available on-

line1. Coding was completed using the Dedoose2 service. After coding all 660 topics, the topic/code matrix was exported for subsequent reliability and regression analysis, as described in the following sections.
An example of a coded topic from the 2004 Novelty test collection is presented in Figure 1. This topic refers to a specific event and contains place entities as well as an explicit date. Topic N57 is categorized as an "event" by the TREC topic creator and is therefore an unambiguous temporal topic as defined by Jones and Diaz.
Title: (East Timor)P laceEntity Independence SpecificEvent
Description: (East Timor)P laceEntity vote for independence from
(Indonesia)P laceName in (August 1999)ExplicitDate Specif icEvent
Figure 1: TREC Novelty 2004 topic N57 example annotation
In addition to coding the topics based on the defined codes, the coders assigned a temporal designation to the distribution of relevant documents for each topic. Nonparametric densities were fit to the temporal distribution of relevant documents for topics with more than 20 relevant documents, following Dakka et al [1]. Each coder reviewed the relevant document distribution along with the total number of relevant documents for each topic and assigned one of four values based on subjective impressions about the degreee to which relevant documents were temporally constrained: too few observations (-1), low or no temporality (0), moderate temporality (1), and high temporality (2).
4.2 Reliability analysis
For this study, coder agreement is measured using Cohen's  for the classification of the distribution of relevant documents. For the broader qualitative coding task, we use a variation of percent overlap, since coding is performed on arbitrary segments of text. We define percent overlap as:
m overlap =
m + u1 + u2
Where m is the number of excerpts assigned the same code by both coders, u1 is the number of codes assigned to excerpts only by coder 1 and u2 is the number of codes assigned to excerpts only by coder 2. If both coders assign no codes to a topic, it is considered perfect agreement. We report the macro (calculated over all topics) and micro (calculated as a per-topic average) overlaps. Per-code overlaps are used to characterize coder agreement within each code.
4.3 Relevant document distributions
In each of the four prior studies enumerated in Section 2, the authors acknowledge using the distribution of judgedrelevant or pseudo-relevant documents in determining topic temporality. For this study, we use two different measures to analyze these distributions: the first-order time series autocorrelation (ACF) and the dominant power spectrum (DPS).
Jones and Diaz [5] use the ACF created by the temporal distribution of pseudo-relevant documents for a query as a predictor of query temporality. They note that queries with
1http://github.com/craig-willis/sigir-2016queries/codebook 2http://www.dedoose.com

1066

strong inter-day dependencies will have high ACF values, indicating predictability in the time series.
Similarly, He, Chang, and Lim [3] use the DPS as a predictor of the "burstiness" of temporal features for event detection. The DPS is the highest power spectrum, estimated using the periodogram.
In this study, both ACF and DPS measures are used to reduce the distribution of judged-relevant or pseudo-relevant documents to a single value for the regression analysis, as described in the next section.
4.4 Regression analysis
A primary goal of this study is to determine the characteristics that contribute to the manual judgment of topic temporality. We use logistic regression based on the generalized linear model (GLM) implementation in R. The predictors are binary presence indicators for each of the qualitative codes along with the ACF and DPS of the temporal distribution of true-relevant documents. The response variables are the binary temporal/non-temporal indicators manually assigned in the four studies. Model variables are selected using standard step-wise procedures based on the Akaike information criterion (AIC). Coefficients are reported using the log-odds and model fit is assessed using pseudo-R2.

5. RESULTS 5.1 Codes

Code PersonEntity PlaceEntity ExplicitDate PeriodicEvent OrganizationEntity SpecificEvent OtherEntity GenericEvent IndirectEventReference

% Agree 0.94 0.91 0.89 0.85 0.76 0.64 0.52 0.45 0.19

Table 2: Codes and percent agreement.
Our qualitative analysis suggests that three broad classes of features bear on query temporality: events, named entities, and explicit dates. It is intuitive that topics focused on specific and important events will have a higher degree of temporal relevance. Following the Topic Detection and Tracking definition, seminal events happen at specific times in specific places, often to individuals or other named entities (e.g., organizations). Perhaps the most essential code is the "SpecificEvent" ­ something important that happens at a particular time and place. Related to SpecificEvent is the "PeriodicEvent," which refers to an event that recurs periodically, such as the Super Bowl, World Cup, or Halloween. Jones and Diaz [5] note that many of the early adhoc queries were temporally ambiguous, referring to multiple events. We incorporate this concept through the "GenericEvent" code, which captures topics concerned with a class of specific events, such as earthquakes, elections, or strikes. While analyzing topics, it became apparent that some topics were likely to be inspired by a specific event, but without explicit reference in the topic description. This concept is captured through the "IndirectEventReference" code. The remaining codes are concerned with the identification of specific types of named entities, which are expected to have some association with topic temporality, and explicit dates.

Model Name Novelty Novelty (Rel)
Dakka Dakka (Rel) Efron Efron (Rel)
Peetz
Peetz (Rel)

Model
-3.767 + 5.848 · SpecEvt + 2.523 · Other
-3.539+7.006·SpecEvt+2.530·Other - 7.343 · ACF
0.134 + 0.878 · P lace
-0.917 + 0.393 · DP S -1.765 + 2.353  P lace + 1.410 · Other
-2.727+1.965·P lace +1.787·Other + 0.163 · DP S -0.336 + 1.682  SpecEvt + 0.982 · P erEvt + 0.672 · P erson - 0.6175 · Org
-1.245+1.218·SpecEvt+0.797·P eriod+ 2.835 · ACF  + 0.002 · DP S

R2 0.669 0.706
0.019 0.263 0.181 0.377
0.127
0.223

Table 3: Logistic regression models predicting prior topic classifi-
cations for each test collection without and with (Rel) ACF/DPS predictors. Model fit reported based pseudo-R2 after stepwise
variable selection based on AIC. Variable significance indicated by p < 0.05(), < 0.01( ), < 0.001( )

5.2 Code distributions

Entities
0.6

Events
0.6

Pct Pct

0.4

0.4

Code

Code

Date

Future

Org Other Person Place

Generic Indirect Periodic Specific

0.2

0.2

0.0 301-450 851-1050 MB1-110 N1-N100 Topics

0.0 301-450 851-1050 MB1-110 N1-N100 Topics

Figure 2: Percent of topics in each collection with codes assigned from the (a) entity code group and (b) events code group.

Figure 2 summarizes the percent of topics in each test collection with each code assigned. We can see that the Novelty and Microblog collections have a higher percentage of specific events than the Blog and ad-hoc collections. The ad-hoc collections have a higher number of generic events, which supports the findings of Jones and Diaz [5]. The Blog, Novelty, and Microblog test collections each have larger numbers of named entities in the topic titles and descriptions.
5.3 Reliability
To assess coding reliability, a total of 1,244 codes were assigned to 330 topics by the two coders. Higher overlap indicates greater agreement between coders. The macro percent overlap is 0.71 and micro percent overlap is 0.83, indicating that overall our codes may be applied with good consistency. The per-code overlap is reported in Table 2(c). As expected, some codes have higher agreement than others. Specifically, personal names (0.94), locations (0.91), and explicit dates (0.89) have very high agreement whereas indirect event references (0.19) and generic events (0.45) have lower agreement.
5.4 Regression analysis
In this section, we report the results of the logistic regression analysis, predicting the manually assigned categories for each test collection. The resulting models are reported in Table 3.
For the 2003-2004 Novelty collection, the response variable is the "opinion" (0) or "event" (1) classification of each

1067

Collection AP LA/FT Blog MB

 0.743 0.551 0.857 0.806

ACF 0.518 0.591 0.728 0.692

DP S 0.356 0.374 0.498 0.354

Table 4: Cohen's  for inter-coder agreement for classification of true-relevant document distributions. Pearson's  measuring correlation (average) between manual classifications and ACF/DPS values

topic, which is manually assigned by the TREC organizers. Following Jones and Diaz [5], we treat "event" as the temporal category. Logistic regression analysis is performed with and without the ACF and DPS predictors, as shown in Table 3. SpecificEvent and OtherEntity are significant predictors of the "event" category (p < 0.01), with a pseudo-R2 of 0.669. Including the ACF of the true-relevant distribution is significant, with a minor improvement in model fit. The high pseudo-R2 is unsurprising in this case, since the SpecificEvent code corresponds to the Novelty "event" category. It does, however, confirm our code definition.
Dakka et al manually classified "time-sensitive queries" for TREC topics 301-450. As reported in Table 3, only the PlaceEntity code is a significant predictor of the manual classification. However, the pseudo-R2 is very low (0.019). Dakka et al acknowledge examining the relevant document distributions for the LA Times and Financial Times sub collections. Including the DPS of the true-relevant document distribution increases the pseudo-R2 to 0.263, suggesting that the relevant document distribution played a significant role in the manual classification.
Efron and Golovchinsky also classified topics 301-450, in this case focusing on the identification of "recency" queries. As reported in Table 3, both PlaceEntity and OtherEntity are useful predictors of the temporal response. As with Dakka, including the DPS of the true-relevant distribution increases pseudo-R2 from 0.181 to 0.377. This again suggests that the distribution of relevant documents played an important role in the determination of topic temporality.
Finally, we look at Peetz et al's classification of the Blog0608 topics 850-1050. In this case, the SpecificEvent, PeriodicEvent, Person and Organization entities are useful predictors of the temporal category (pseudo-R2=0.127). Including DPS improves model fit (pseudo-R2=0.223), again suggesting that the distribution of relevant documents played a role in manual classification.
5.5 Relevant document distributions
As described in Section 3.1, non-parametric densities based on the temporal distribution of true-relevant documents are manually classified by two coders into four categories. The weighted Cohen's  is calculated to assess agreement between the two coders. Average Pearson's correlation () measures the correlation between these manual classifications and the per-topic ACF/DPS values.
The results reported in Table 4 show moderate (0.40-0.60) to high (0.60-0.80) coder agreement and higher correlation between the ACF and the manual classifications. These findings show that ACF and DPS effectively capture the degree to which relevant documents are temporally constrained.
6. DISCUSSION AND CONCLUSIONS
In this study, we have tried to identify characteristics of

TREC topics that can be used to explain manual classifications of "temporal sensitivity." Other researchers have classified topics without clear definitions or criteria. We have attempted to model these classifications by proposing features believed to indicate temporality. Features include the presence of different types of named entities, classes of events, and measures of the temporal distribution of judged relevant documents.
We were successful in modeling the "event" category in the Novelty track, based primarily on our "SpecificEvent" code. Event codes were also found to be useful predictors of the classification of Peetz et al [9]. However, we were generally unable to identify characteristics that fully explain the other manual classifications. They seem to consistently conflate two different concepts: the temporal distribution of judged-relevant documents and common-sense notions of topic temporality.
Since the first-order autocorrelation of the judged-relevant document distribution is highly correlated with manual judgements of temporality, we recommend using the ACF or other measure of distribution "burstiness" instead of manual assessment. In the future, common-sense notions of temporality should be clearly explicated.
If we cannot explain the process that determines the classifications, it raises questions about the value of these test collections for evaluation. Specifically, how can we be clear that the queries previously identified as "temporally sensitive" are truly so? This ambiguity also limits the utility of previous research, since it is unclear how to select queries for which the proposed models are well-suited.
7. ACKNOWLEDGMENTS
This work was supported in part by the US National Science Foundation under Grant No. 1217279. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation.
8. REFERENCES
[1] W. Dakka, L. Gravano, and P. Ipeirotis. Answering General Time-Sensitive Queries. IEEE Transactions on Knowledge and Data Engineering, 24(2):220­235, 2012.
[2] M. Efron and G. Golovchinsky. Estimation methods for ranking recent information. SIGIR 2011, 2011.
[3] Q. He, K. Chang, and E.-P. Lim. Analyzing feature trajectories for event detection. SIGIR 2007, 2007.
[4] H. Joho, A. Jatowt, and R. Blanco. NTCIR Temporalia : A Test Collection for Temporal Information Access Research. In Proceedings of WWW 2014, 2014.
[5] R. Jones and F. Diaz. Temporal profiles of queries. ACM Transactions on Information Systems, 25(3), 2007.
[6] K. Krippendorff. Content analysis: an introduction to its methodology. Sage, Beverly Hills, CA, 1980.
[7] X. Li and W. B. Croft. Time-based language models. CIKM 2003, 2003.
[8] D. Metzler, R. Jones, F. Peng, and R. Zhang. Improving search relevance for implicitly temporal queries. SIGIR 2009, 2009.
[9] M.-H. Peetz, E. Meij, and M. de Rijke. Using temporal bursts for query modeling. Information Retrieval, 17(1):74­108, 2013.

1068

Why do you Think this Query is Difficult? A User Study on Human Query Prediction

Stefano Mizzaro
University of Udine Via delle Scienze, 206
Udine, Italy
mizzaro@uniud.it
ABSTRACT
Predicting if a query will be difficult for a system is important to improve retrieval effectiveness by implementing specific processing. There have been several attempts to predict difficulty, both automatically and manually; but without high accuracy at a preretrieval stage. In this paper, we focus rather on understanding why a query is perceived by humans as difficult. We ran two separated but related experiments in which we asked humans to provide both a query difficulty prediction and reasons to explain their prediction. Results show that: (i) reasons can be categorized into 4 classes; (ii) reasons can be framed into closed questions to be answered on a Likert scale; and (iii) some reasons correlate in a coherent way with the human predicted numerical difficulty. On the basis of these results it is possible to derive hints to be provided to help users when formulating their queries and to avoid them to rely on their wrong perception of difficulty.
Keywords
Information retrieval; Query difficulty; difficulty understanding
1. QUERY DIFFICULTY PREDICTION
One of the outcomes of IR international evaluation campaigns is that system and query variability is high [5]. However, while there is some variability, some queries are difficult or easy for all the participants. For example in TREC Web 2014, which uses the ClueWeb 2012 corpus, the average ERR@20 for topic 278 "What are the lyrics to the theme song for "Mister Rogers' Neighborhood"?" is 0.0048 while the best run for that topic got 0.0820; this is a difficult topic for all systems. On the opposite, for topic 298 "medical care and jehovah's witnesses", the average ERR@20 is 0.5887 and the median is 0.5790; this is an easy topic.
The Reliable Information Access (RIA) workshop [5, 6] has been the first large scale attempt to try to understand query (and system) variability and difficulty. The two main conclusions of the failure analysis were: systems were missing an aspect of the query, generally the same aspect for all the systems, and "if a system can realize the problem associated with a given topic, then for well over half the topics studied, current technology should be able to improve
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914696

Josiane Mothe
Univ. de Toulouse, ESPE, IRIT UMR5505 CNRS, 118 route de Narbonne
Toulouse, France
josiane.mothe@irit.fr
results significantly" [6]. When considering failure analysis, 10 classes of topics were identified manually, but no indications were given on how to automatically assign a topic to a category.
Following these findings, there have been many attempts to automatically predict query difficulty. The purpose of a query difficulty predictor is to decide whether a system is able to properly answer the current query [2]. Different kinds of automatic predictors have been proposed in the literature both pre- [7] and post-retrieval [10], based on statistics only or considering some linguistic features [9]. Automatic predictors correlate with actual or observed system effectiveness, but the correlation is always weak, even if it is slightly higher when considering post-retrieval predictors than pre-retrieval ones (although post-retrieval predictors are less interesting, because more costly, than pre-retrieval) [7, 9, 10]. These results limit their practical use in real applications.
Another research direction is addressed by Hauff et al. who analyzed the relationship between predictions by non IR expert users and system effectiveness [8]. The authors considered various queries for a single topic or information need and measured the ability of users to judge the quality of query suggestions. They found that: (i) users are not good at predicting system failure; and (ii) the correlations between the users' prediction and both system effectiveness and automatic predictors are weak. We also had similar results when asking annotators to predict query difficulty, under several different experimental conditions, with different user groups, and both from the crowd and from participants in laboratory experiments.
In this paper, rather than focusing on query difficulty rating, we focus on reasons why users think a query is going to be easy or difficult for a search engine. We decided to consider users who are not necessarily IR experts since the latter know how systems work and may not be representative of a large variety of real search engine users. Therefore, when compared to RIA [5, 6], we ask to non experts to provide reasons that explain query difficulty (or ease). When compared to Hauff et al. [8], rather than just asking for ratings, we focus on explanations and comments on difficulty.
To know more on these reasons, we performed a user study made up of the two experiments described in the next two sections.
2. EXPERIMENT 1: ELICITING REASONS
The first experiment aimed at eliciting free text reasons why queries are perceived as difficult or easy by users.
2.1 Experimental Design
While we were interested mainly in the reasons why users think a query is going to be easy or difficult for a search engine, the task for human annotators was both to predict the difficulty a search engine may encounter to answer an information need and to explain

1073

 










Figure 1: Number of queries by annotation frequency

the reasons of their prediction. Indeed, it is probably more natural for annotators to choose an explicit rating first and then to focus on providing the reasons for it. When asked to annotate the topic difficulty, the main question the annotators had to think of was: is the system going to succeed/fail when processing this query? Does the annotator think the system will retrieve relevant information (an easy query) or not (difficulty query)?
Annotators were provided with the query (TREC topic title); they had to decide the difficulty on, and to comment on the difficulty rating they chose, using the query only. Annotators were asked to use a three level numeric scale: 1 for easy query, 2 for medium query, and 3 for difficult queries. They also could use 0 when they did not know, although they were encouraged to decide on the difficulty. In addition to grading the query difficulty, annotators were asked to indicate the reasons why they thought the query was easy/difficult. For a query and whatever the grade they gave, they could indicate both comments, on its difficult and easy natures. We did not provide any guidance to write the comments apart from using the keyword "Easy:" or "Difficult:" before any comment they write. The tool we provided does not allow them to go back to an annotation they had done previously.
The group of annotators was composed of 38 Master's Students (25 1st and 13 2nd year) in library and teaching studies; although they had been trained to use specialized search engines, they had just an introduction class on how search engines work. Annotators could choose as many topics they wanted from a set of 150 TREC topics. Topics were displayed in different order to avoid any bias, as the first topics may be treated differently because the task was new for annotators. Moreover, annotators could skip some topics if they wish; this was done to avoid them to work on a topic they did not understand or felt uncomfortable with. Since the annotation process is difficult, we tried to provide to the annotators the most favorable conditions. The drawback is that the number of annotations varies over topics; this makes numerical analyses more difficult (for example, when computing an average difficulty score the averages are computed over different numbers of scores) but we were here more interested in the reasons, and this kind of qualitative data is less prone to such difficulties.
In this experiment, we used TREC 6, 7, and 8 adhoc task topics.Although these collections are old, we think that the elicitation of reasons will not differ much with other collections. Of course, the annotators knew that the document collection is composed of newspaper articles from the 90s even though it may be difficult for an annotator nowadays to get a picture of what were the popular topics in newspapers more than twenty years ago.
2.2 Results and Analysis
We analyzed the comments that the annotators associated with the evaluation of difficulty. The objectives were (i) to see if there were some recurrent patterns, and (ii) to extract some trends in the

Table 1: Distribution of grades used by annotators

Grade:

Easy Medium Difficult Don't know

Frequency: 227

188

140

17

comments associated with classes of query difficulty as perceived by users/annotators.
2.2.1 Descriptive Statistics
Figure 1 shows the number of queries as a function of the number of times it has been annotated by any of the 38 annotators. For example, 22 queries have been annotated a single time (left part of the figure); the most annotated query has been annotated 25 times (right side of the figure). In total 107 queries have been annotated at least by one annotator and 65 three times or more. Table 1 shows the distribution of grades, i.e., the number of times a given grade has been used (e.g., grade Easy has been used 227 times whatever the annotator and the query are).
We collected 460 annotations in total (one annotation count for one topic, one annotator). 107 topics have been graded by at least one annotator; a little fewer have been commented (6 topics have no comment associated with them). It is of course possible that other comments might be generated for the other topics, but with around 70% of the topics being annotated we can be rather confident that most of the reasons have been elicited in this experiment.
2.2.2 Recoding the Free Text Comments
We recoded the free text comments. Table 2 shows some examples of recoding that was made. Each comment could be recoded into more than one recoded phrases; for example the comment "terms are too general, there will be many documents retrieved" has been recoded into Too-General-Words and Many-Documents. We found out that there were mostly four types of comments, as shown in the table: T (on the topic itself), Q (on the query), W (on the words used), and D (on the documents or on the collection, e.g., if the annotator thinks that some document exists in the collection).
The table also shows how many comments were recoded in each category; however, notice that recoding is always subjective and another recoder may have recoded differently, thus these numbers just provide trends. In a few cases (about 5%), the comment was not explicitly associated with one of these classes. This was for example the case when annotators wrote vague without detailing if it was a query term which they found vague or the topic itself. A concrete example is the one of Query 417 (Title: creativity) for which the 5 annotators considered the query as difficult using comments such as "too broad, not enough targeted", "far too vague",

Table 2: Examples of recoding, grouped into four categories.

Free text comment

Recoded phrase

T: Topic (274 comments, 35 recoded phrases)

"The topic is precise"

Precise-Topic

Q: Query (142 comments, 23 recoded phrases)

"The query is formulated in a clear way" Clear-Query

"The query is not precise at all"

Broad-Query

W: Words (180 comments, 28 recoded phrases)

"A single word in the query"

1-Word

"The term 'exploration' is polysemous" Polysemous-Word

D: Documents (143 comments, 15 recoded phrases)

"Risk of getting too many results"

Too-Many-Documents

"There are many documents on this" Many-Documents

1074

Table 3: Most frequent comments.

Easy because

Difficult because

Precise-Topic

66

Many-Documents

45

No-Polysemous-Word 31

Precise-Words

25

Clear-Query

19

Usual-Topic

16

Risk-Of-Noise

50

Broad-Topic

43

Missing-Context 34

Polysemous-Words 22

Several-Aspects 20

Missing-Where

16

"far too vague topic", "keyword used very broad, risk of noise", and "a single search term, risk of getting too many results". While the last comments are directed to one or the other class, the first two are not. We notice that the four categories are roughly equally distributed and have a good coverage.
After recoding, we got 740 annotations for 572 graded queries (each query could be annotated by various annotators; in addition several recoded phrases can be associated with a single comment). For recoding we used 105 different recoding phrases (4 of which are not associated with any of the four categories).
2.2.3 Comments Associated with Ease and Difficulty
Table 3 shows the most frequent recoded phrases associated with ease (left part) and difficulty (right part). Remember that a given query can be annotated by some comments associated with both. For example, while Precise-Topic is generally associated with ease (66 times), it is also associated with difficulty in 3 cases. In that case it is associated with other comments, e.g. "The topic is very precise but it may be too specific". In the same way, Many-Documents is mostly associated with ease and Too-Many-Documents to difficulty, although Many-Documents is also associated with difficulty (users may have in mind either a recall-oriented or a precisionoriented task). Also, when Many-Documents is used associated with difficulty, it is generally associated with Risk-Of-Noise.
2.2.4 Annotator Effect
It may be that some annotators are more likely to use some types of comments than others, either because of what they think about how systems work or because of their search experiences.
We analyzed the link between annotators and the four categories of comments, i.e., associated with Word (W), Query (Q), Topic (T), and Document (D). We grouped the recoded phrases belonging to each category and built a matrix that associates annotators and the four categories of comments associated with. We then used Correspondence Analysis (CA) [1] on that matrix to visualize in one shot the relationships. CA is close to Principal Component Analysis (PCA) in its principle and appropriate when analyzing categorical variables which is the case here. Compared to PCA, CA allows to display in the same space the variables and observations (rows and columns). The distance between objects of any kind is meaningful.
Figure 2 shows the two first axes of the corresponding CA. The horizontal axis divides the figure into two parts. The top part is more related to comments on W and T and so are the annotators in this part of the figure. The bottom part is related to Q and D categories; so are the annotators displayed in this part of the figure. Moreover, the bottom left corner of the figure is more related to comments on D, as the annotators who are in the same corner while the bottom right part is more associated with comments on Q. The annotators near the origin of the axes use annotations from all four categories, while the annotators in the top right corner are more inclined to use comments on Q (according to the horizontal axis) and on W (vertical axis), but do not use comments on D.

Figure 2: CA, first components: annotators (triangles) and comment categories (ellipses).
We went a step further to check if some annotators use some comments more than others by using the comments rather than the category of comments and found that it is indeed the case too.
3. EXPERIMENT 2: TESTING REASONS
The experiment described in the previous section allowed us to elicit reasons. However, free text questions have two drawbacks: first they need to be recoded and recoding is subjective, and second, annotators are sensitive to different features as we have shown in section 2.2.4. Based on these results, we decided to consider, rather than free text, closed and mandatory questions for the annotators to answer. Closed so that recoding will not be needed anymore, and mandatory so that the annotator effect will be less important. This is done in the second experiment, described in this section.
3.1 From Categories to Questions
We considered each of the 105 recoding phrases obtained in the previous experiment and transformed them into 32 closed questions (denoted with Qi in the following) that could be answered following a Likert scale. Examples of such questions are shown in the first column of Table 4. When recoding, we had tried to keep as most as possible the nuances annotators expressed. When rephrasing into questions, we removed these nuances to limit the number of questions and to remove some redundancy (e.g., precise, specific, focused, delimited, and clear were merged together).
3.2 Experimental Design
We performed a laboratory user study with 22 new volunteers mainly from our research institutes. They were recruited using generic emailing lists and they got a coupon for participating. Each of them was asked to annotate 10 queries (provided in a random order). We used 25 topics from TREC Web 2014 that uses ClueWeb 2012 corpus: we picked up the easiest 10, the most difficult 10, and the medium 5 according to the topic difficulty order presented in the overview paper [3]. We changed from TREC 6, 7, 8 to ClueWeb queries since the latter are more recent, might reflect more the types of current queries on the web, and are now more used in the IR community. Also, in the previous experiment some topics were not annotated: clearly the young students were not at ease with (some of the) old topics.
In order to make the statistical analysis more smooth and sound

1075

Q23 Q24 Q17 Q18 Q13 Q19

Table 4: Examples of questions (column 1) with their Pearson's correlations with human predicted difficulty (col. 2) and actual difficulty (col. 3). Bold indicates a p-value < 0.05, * <0.005.

Question

Correl.

Q1: The query contains vague word(s)

.52 -.30

Q3: The query contains word(s) relevant to the topic/query -.41 .43

Q10: The topic is unusual/uncommon/unknown

.52 .26

Q13: The topic has several/many aspects

.61* -.07

Q17: The topic is usual/common/known

.62* -.25

Q18: The number of documents on the topic in the web is high -.69* -.34

Q19: None or very few relevant documents will be retrieved .88* .32

Q20: Only relevant documents will be retrieved

-.47 .09

Q23: Many of the relevant documents will be retrieved

-.86* -.20

Q24: Many relevant documents will be retrieved

-.87* -.21

Q26: The number of query words is too high

.62* .45

Q28: The query contains various aspects

.46 -.12

Q30: The query is clear

-.53 .30

we collected the same number of predictions for each query; we thus consider the same number of annotators for each topic. Annotators had to annotate the level of difficulty of the query, but rather than asking them to provide the reason of their grading in free text only, we asked to answer the 32 predefined questions Qi using a five level scale, from -2 "I strongly disagree" to +2 "I strongly agree". With 32 Qi by 25 topics by 8 annotators each, we collected a total of 6400 reason ratings. The free text reasons they provided has not been analyzed yet.
3.3 Results
Table 4 shows on the second column the Pearson's correlation between the questions and the human prediction of difficulty. Only the 13 Qi having a statistically significant correlation (p-value < 0.05) are included. The seven Qi having values labeled with a * (Q13, Q17, Q18, Q19, Q23, Q24, Q26) have a correlation higher than 0.60 with a p-value < 0.005. These 13, and especially 7, Qi represent the reasons that, according to the users, correlate most with query difficulty. For example, users think a query is difficult because the topic has many aspects (Q13).
However, none of these 13 reasons that users think correlated with query difficulty, turns out to correlate with actual difficulty, with just one exception (Q26). This is shown in the third column in the table that reports the correlation with observed difficulty based on system effectiveness. We used the average ERR@20 as system effectiveness measure, calculated considering all the participant runs. Moreover, the correlation between the human prediction and actual difficulty is low (0.238, p-value 0.25), indeed a much lower value (and not statistically significant as well) than the correlation between Q26 and actual effectiveness, which is 0.45, p-value < 0.05. This means that to obtain a prediction of difficulty, it is much better to ask Q26 than to directly ask for a difficulty rating.
There is also the possibility that some Qi can be combined, maybe also with the difficulty prediction rating, and/or with automatic predictors, to obtain a more accurate numerical prediction. We do not have space here to present those results, but we note (see Figure 3) that some of the seven questions are redundant, and therefore there is no need to require the user to answer all of them.
4. DISCUSSION AND CONCLUSIONS
Other studies have shown that humans are bad query difficulty predictors [8] and generally think queries are easier for systems than they actually are. This paper is a first contribution to try to understand why users (rather than IR experts) think a query is easy

Q24
Q17
Q18
Q13
Q19
Q26
-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1
Figure 3: Pearson correlations between questions. Q18, Q19, Q23, and Q24 have a high correlation; they are redundant.
or difficult for a search engine. In a first user study, reasons were elicited from free text comments on query difficulty. After a manual recoding and a deep analysis, we found a set of 105 reasons classified into four categories. We then framed 32 closed questions that cover all the mentioned reasons and can be answered through a Likert scale. We used those 32 questions in a second user study, and showed that thirteen correlate with the human prediction of difficulty, and seven of them highly. On the other hand, these questions do not correlate with observed system difficulty. These questions can thus be seen as explanation why humans wrongly think queries are easy or difficult. These results can be useful when training search engine users [4], e.g., to help them to formulate queries and to provide them with hints about their wrong perception of system effectiveness. For example, a user can be told (by a teacher or a search engine) that the fact the query seems usual or common is not linked to system effectiveness.
References
[1] J.-P. Benzécri et al. Correspondence analysis handbook. Marcel Dekker New York, 1992.
[2] D. Carmel and E. Yom-Tov. Estimating the query difficulty for information retrieval. Morgan & Claypool, 2010.
[3] K. Collins-Thompson, C. Macdonald, P. Bennett, F. Diaz, and E. Voorhees. TREC 2014 Web Track Overview. In Text REtrieval Conference, 2015.
[4] E. Efthimiadis, J. M. Fernández-Luna, J. F. Huete, and A. MacFarlane. Teaching and learning in information retrieval. Vol. 31. Springer Science & Business Media, 2011.
[5] D. Harman and C. Buckley. The NRRC reliable information access (RIA) workshop. In Conf. on Research and Development in Inf. Retrieval, SIGIR, pages 528­529. ACM, 2004.
[6] D. Harman and C. Buckley. Overview of the reliable information access workshop. Information Retrieval, 12(6):615­641, 2009.
[7] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Conf. on Information and Knowledge Manag., CIKM, pages 1419­1420, 2008.
[8] C. Hauff, D. Kelly, and L. Azzopardi. A comparison of user and system query performance predictions. In Conf. on Inf. and knowledge management, CIKM, pages 979­988, 2010.
[9] J. Mothe and L. Tanguy. Linguistic features to predict query difficulty. In Predicting query difficulty Wp, Conf. on Research and Development in IR, SIGIR, pages 7­10, 2005.
[10] A. Shtok, O. Kurland, D. Carmel, F. Raiber, and G. Markovits. Predicting query performance by query-drift estimation. ACM, TOIS, 30(2):11, 2012.

1076

A Platform for Streaming Push Notifications to Mobile Assessors

Adam Roegiest, Luchen Tan, Jimmy Lin, and Charles L. A. Clarke
David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada
{aroegies, luchen.tan, jimmylin}@uwaterloo.ca, claclark@gmail.com

ABSTRACT
We present an assessment platform for gathering online relevance judgments for mobile push notifications that will be deployed in the newly-created TREC 2016 Real-Time Summarization (RTS) track. There is emerging interest in building systems that filter social media streams such as tweets to identify interesting and novel content in real time, putatively for delivery to users' mobile phones. In our evaluation design, all participants subscribe to the Twitter streaming API to identify relevant tweets with respect to a set of interest profiles. As the systems generate results, they are pushed in real time to our evaluation broker via a REST API. The broker then "routes" the tweets to assessors who have installed a custom app on their mobile phones. We detail the design of this platform and discuss a number of challenges that need to be tackled in this type of "Living Labs" setup. It is our goal that such an evaluation design will mitigate any issues that have arisen in traditional batch-style evaluations of this type of task.
1. INTRODUCTION
There is emerging interest in building push notification systems that filter social media streams, such as Twitter, to deliver relevant content to users' mobile phones. For example, the user might be a political news junkie interested in polls for the 2016 U.S. presidential elections and wishes to be notified whenever new results are posted on Twitter. She might also be interested in commentary by political pundits and reactions by the candidates. Such notifications must relevant (i.e., on topic), timely (i.e., the user desires poll results as soon as they are available), and novel (i.e., the user does not want tweets from multiple sources that cite the same poll). Techniques to address such information needs are becoming increasingly important as mobile devices continue to gain prominence for information access.
The TREC Microblog track in 2015 operationalized the push notification task in the so-called "scenario A" variant of the real-time filtering task [3]. Over the official evaluation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911463

period, which spanned ten days during July 2015, participating systems "listened" to Twitter's live tweet sample stream to identify relevant tweets with respect to 225 pre-defined "interest profiles" (each expressed through statements modeled after TREC ad hoc topics), 51 of which were later assessed. Each system identified up to ten tweets per day, which were ostensibly delivered to hypothetical users. In total, 14 groups submitted 37 runs to this evaluation.
As it was the first year, many aspects of the evaluation in 2015 represented substantial simplifications of the actual task. Instead of pushing notifications in real time, participating systems merely recorded the wall clock time at which the system would have pushed the notification. That is, all results were recorded locally and submitted in batch after the evaluation period ended. Thus, although the task itself is real time, the results were evaluated with standard batch protocols (pooling followed by semantic clustering; see Wang et al. [9] for more details). The goal of our work is to tackle this limitation: we present an assessment platform for gathering online relevance judgments for mobile push notifications. This platform will be deployed in the newly-created TREC 2016 Real-Time Summarization (RTS) track.
Real-Time Summarization at TREC 2016 represents a merger of the Microblog track, which ran from 2010 to 2015, and the Temporal Summarization track [1], which ran from 2013 to 2015. The creation of RTS was designed to leverage synergies between the two tracks in exploring prospective information needs over document streams containing novel and evolving information. Previously, Temporal Summarization evaluations operated by simulating a stream of documents using a static collection, which also represented a simplification of the underlying task model. We believe that by "joining forces", we could develop a more refined push notification task and associated evaluation infrastructure to support online user-in-the-loop evaluations, thereby growing the research community and pushing forward the state of the art.
2. GENERAL ARCHITECTURE
The TREC 2016 Real-Time Summarization track will operationalize a push notification task that largely follows the same task from the TREC 2015 Microblog track. Participants must deploy working systems that consume a live stream over a defined evaluation period. In 2016, this live stream will be comprised solely of tweets provided by the Twitter streaming API, but we are investigating the viability of alternate and additional sources, e.g., a stream of news stories or blog posts. The exact task definition is not

1077

Stream of Tweets

Participating TREC RTS Systems evaluation broker

Twitter API

Assessors

Figure 1: A high-level overview of the real-time push notification assessment platform.

relevant for the purposes of this work, which focuses on the assessment platform. The most substantial change from the previous evaluation is that systems now must submit their results in real time. This live submission is accomplished via a REST API and evaluation infrastructure that we have developed. The entire platform, including both server code and the mobile assessment app, is available open source.1
The high-level architecture of our assessment platform is shown in Figure 1. Our general approach builds on growing interest in so-called Living Labs [6] and related Evaluationas-a-Service (EaaS) [2] approaches that attempt to better align evaluation methodologies with user task models and real-world constraints to increase the fidelity of research experiments. In our architecture, participating systems all subscribe to the Twitter streaming API (a sample stream is freely available to all registered users) to identify relevant tweets with respect to interest profiles. Since tweets are being posted in real time, the evaluation organizers do not distribute any data ahead of time--they listen to the stream just like all participants to gather an archival copy of the tweets. A pilot study in 2015 [4] confirmed that multiple geographically-distributed listeners to the public Twitter sample stream receive effectively the same tweets (Jaccard overlap of 0.999 across six independent crawls over a three day sample in March 2015). As the participating systems identify relevant tweets, they are pushed in real time to the evaluation broker, which then routes the tweets to assessors who have installed a custom app on their mobile phones. We intend to recruit students to serve as the assessors.
This setup has a number of distinct advantages:
· Gathering relevance judgments in an online fashion has the potential to yield more situationally accurate assessments, particularly for rapidly developing events. With post hoc batch evaluations, there is always a bit of disconnect as the assessor needs to "imagine" herself at the time the update was pushed. With our evaluation framework, we remove this disconnect.
· An online evaluation platform allows for the possibility of user-submitted information needs, thus giving assessors the ability to judge tweets for interest profiles they are genuinely interested in.
· An online evaluation platform opens the door to providing realistic, online feedback to participants, thus potentially facilitating active learning approaches. There are, of course, many additional complexities that remain un-
1https://github.com/trecrts/trecrts-eval

resolved (discussed below), and in the near term we do not anticipate providing this option.
Note that online evaluation using our assessment platform does not preclude the use of standard batch evaluation methodologies that have been well studied and appropriately validated. Indeed, it is the plan that the Real-Time Summarization track in TREC 2016 will also apply the batch evaluation methodology developed for the Microblog track in 2015 in a post hoc manner. This dual evaluation approach will help us validate the reliability and stability of our online mobile assessment platform.
3. PLATFORM COMPONENTS
3.1 Evaluation Workflow
We have designed our assessment platform around the following workflow:
1. Systems listen to the live Twitter sample stream to identify interesting and novel tweets with respect to a set of pre-defined interest profiles. Although we provide boilerplate code to get started, participants are responsible for building and running their own systems.
2. Whenever a system identifies a relevant tweet with respect to an interest profile, the system submits the result to the evaluation broker via a REST API, which records the submission time.
3. The broker routes the tweet to the mobile phone of an assessor, where it is rendered as a push notification containing both the text of the tweet and the corresponding interest profile.
4. The assessor may choose to judge the tweet immediately, or if it arrives at an inopportune time, to ignore it. Either way, the tweet is added to a judging queue in a custom app on the assessor's mobile phone, which she can access at any time to judge the queue of accumulated tweets.
5. As the assessor judges tweets, the results are relayed back to the evaluation broker and recorded.
While the information retrieval community is already familiar with a variety of batch relevance assessment approaches, our platform explores a largely untouched space in evaluation design. We anticipate that a number of experimental design decisions will need to be empirically verified:
· Number of assessors: The size of the assessor pool will be largely dependent on the number of topics, participating systems, and the incentive structure for providing judgments.
· Topic assignment: There are many possibilities for mapping between topics and assessors. As with standard pooled batch evaluations, assigning all tweets for a given topic to a single assessor increases the consistency of judgments, but might result in an unbalanced load across assessors. Alternatively, we could split each topic across multiple assessors, but this approach may increase inconsistency.
· Assessor modeling: It is likely that assessors will vary in quality, will respond to push notifications with varying degrees of latency, and will provide differing numbers of judgments in a given time period. We anticipate the need to construct post hoc assessor models in order to understand and account for these assessor differences.

1078

· Tweet interleaving: It is unlikely that we will be able to devote an assessor to exclusively judge the output of a single system. Nor would this approach be desirable, since it would magnify the impact of assessor differences when performing system comparisons. Thus, assessors will see interleaved results from multiple systems. We have been separately investigating tweet interleaving strategies for evaluation [5], and we will apply the lessons learned from those experiments as appropriate.
3.2 Evaluation Broker
The evaluation broker (see Figure 1) serves as an intermediary between systems participating in the evaluation and the mobile assessors. The main role of the broker is to distribute interest profiles to participating systems, record system submissions, route submitted results to mobile assessors, and record judgments rendered by the assessors.
The broker is implemented in Node.js and backed by a MySQL database for persistent storage of result submissions and assessor judgments. Broker functionalities are implemented via different REST API endpoints. For example, systems submit a tweet for a particular interest profile using the following call:
POST /tweet/:topid/:tweetid/:clientid
where :topid specifies the topic (interest profile) identifier, :tweetid specifies the unique tweet identifier of the post, and :clientid specifies the client's unique identifier. The broker returns a 204 status code on success. It is expected that all participating systems will properly interface with the appropriate API endpoint during the evaluation period.
Currently, we have designed the broker to rate-limit the number of submissions by a system to ten tweets per topic per day, following the TREC 2015 Microblog track protocol. The broker further employs some common sense safeguards, e.g., to not bombard any individual assessor with an undue number of push notifications. The actual routing policy of how tweets are assigned to assessors is still currently under development, although we have already implemented a basic round robin approach.
3.3 Mobile Assessment App
For TREC 2016, we plan to recruit students from the University of Waterloo to serve as mobile assessors in a user study centered around the Real-Time Summarization task. As discussed, assessors will receive tweets as they are identified by participating systems in real time, on their mobile phones as push notifications through our custom app. A screenshot of the current app is shown in Figure 2. We envision the experimental study to proceed as follows:
· Assessors will be given a brief description of the task and an invitation to install the assessment app on their mobile phones.
· Assessors will log in to the app to indicate that they are ready to receive tweets. They may log out to stop receiving tweets when they unavailable (e.g., during an exam).
· Assessors will receive new tweets to judge as they become available (i.e., are pushed by participating systems). Tweets will be assessed with respect to the interest profile provided with the tweet.

Figure 2: Screenshot of the mobile assessment app.
· At any point, assessors will be able to end their participation in the task and to opt out of further work. After ending participation they will receive a code that can be exchanged for remuneration (at a rate yet to be determined).
· Otherwise, assessors will be able to judge tweets until the end of the evaluation period, after which they will receive the renumeration code.
We have built the mobile assessment app for both iOS and Android using the Cordova framework. Cordova is designed to leverage existing web technologies for the creation of mobile apps without requiring extensive platform-specific API knowledge. This makes cross-platform development (Android and iOS) significantly less labor-intensive compared to writing two separate native apps. The tradeoff is performance, which is not a concern for us since our assessment task is not processing intensive.
Note that the broker does not directly deliver tweets to the assessors. Rather, tweet identifiers are sent to the mobile phone and tweets are rendered by the assessment app from these identifiers via Twitter's OEmbed API,2 which displays any inline media content for the tweet. This mechanism was adopted to comply with the Twitter terms of service, which prohibit the distribution of tweet content, but
2https://dev.twitter.com/rest/reference/get/statuses/oembed

1079

the mechanism also allows us to offload tweet rendering (including complex multimedia content) to the device. Using OEmbed, we can render a tweet natively with minimal effort yet still provide a user experience similar to that of the official Twitter client.
4. BASELINE SYSTEM
Although the main focus of this demonstration is the mobile assessment platform itself, we also present a baseline implementation to facilitate system development and participation in the track. Our baseline system, called YoGosling, is a modified and extended version of the system that generated the best performing automatic run in TREC 2015 [7]. Following TREC, we performed error analysis and ablation studies to distill the original system down to the components that contributed most to overall effectiveness [8]. This system is built on the Anserini project, which is the University of Waterloo's Lucene-based search framework. The system is able to incrementally index the Twitter sample stream and provide real-time search capabilities.
YoGosling converts the title field of interest profiles into queries for searching Lucene and applies a simple relevance scoring method to rank tweets. After relevance scoring, duplicate and near-duplicate tweets are identified by a novelty detection component (based on simple Jaccard similarity), so that users are not given repetitiously annoying information. One of the most important lessons learned in building YoGosling is the proper setting of score thresholds and ignoring tweets that fall below the thresholds. Our simple scoring model is amenable to a global threshold that yields reasonable effectiveness, thus obviating the need for per-topic tuning (difficult due to the paucity of training data). Another interesting feature of YoGosling is a simple relevance feedback mechanism whereby users assess tweets once a day, and these judgments are used to set the score threshold for the next day. We experimentally show that this technique can yield substantial gains in effectiveness [8].
5. FUTURE WORK
One of the main enhancements we are planning to add is the ability for assessors to supply their own interest profiles directly through an API, so that they can actually receive tweets of personal interest. In TREC 2016, we plan to develop interest profiles manually based on assessor input, but the profiles will be vetted by the track organizers before distribution to participating systems. An automatic API for interest profile submission creates several non-trivial problems, including near-duplicate detection, limiting the release of personal information, and topic termination (since some topics may only be important for a limited time). These issues are exacerbated if there is the expectation that behavioral traces will be released as part of a training set (as is the case with TREC evaluations), and ethics issues similar to the public release of web query logs come into focus.
At present, we have a few ideas of how to assign interest profiles to assessors, but lack concrete empirical evidence as to what strategy will actually "work". As discussed, foreseeable issues include inter-assessor consistency, assessor latency (how quickly they respond to push notifications), and assessment volume (how many judgments they ultimately provide). Related, the strategy used for interleaving tweets has the potential to affect the judgments rendered, and in

turn the relative scoring of systems. Assessor differences introduce additional confounds. We have been exploring in parallel some of these questions via simulations [5], but the literature otherwise offers little guidance and we will have to make decisions about evaluation design for TREC 2016 based on limited empirical evidence.
6. CONCLUSIONS
In this demonstration, we described the architecture of an assessment platform for gathering online relevance judgments for mobile push notifications that will be deployed in the TREC 2016 Real-Time Summarization (RTS) track. Although the infrastructure is "feature complete" in terms of implementation, we freely admit that there are many unresolved issues regarding evaluation design. There is no substitute for operational experience in running a TREC evaluation, and we anticipate many lessons to be learned.
7. ACKNOWLEDGMENTS
This work was supported in part by the U.S. National Science Foundation under awards IIS-1218043 and CNS1405688 and the Natural Sciences and Engineering Research Council of Canada (NSERC). Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.
8. REFERENCES
[1] J. Aslam, M. Ekstrand-Abueg, V. Pavlu, F. Diaz, R. McCreadie, and T. Sakai. TREC 2014 Temporal Summarization Track overview. TREC, 2014.
[2] A. Hanbury, H. Mu¨ller, K. Balog, T. Brodt, G. V. Cormack, I. Eggel, T. Gollub, F. Hopfgartner, J. Kalpathy-Cramer, N. Kando, A. Krithara, J. Lin, S. Mercer, and M. Potthast. Evaluation-as-a-Service: Overview and outlook. arXiv:1512.07454, 2015.
[3] J. Lin, M. Efron, Y. Wang, G. Sherman, and E. Voorhees. Overview of the TREC-2015 Microblog Track. TREC, 2015.
[4] J. H. Paik and J. Lin. Do multiple listeners to the public Twitter sample stream receive the same tweets? SIGIR Workshop on Temporal, Social and Spatially-Aware Information Access, 2015.
[5] X. Qian, J. Lin, and A. Roegiest. Interleaved evaluation for retrospective summarization and prospective notification on document streams. SIGIR, 2016.
[6] A. Schuth, K. Balog, and L. Kelly. Overview of the Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab 2015. CLEF, 2015.
[7] L. Tan, A. Roegiest, and C. L. A. Clarke. University of Waterloo at TREC 2015 Microblog track. TREC, 2015.
[8] L. Tan, A. Roegiest, C. L. A. Clarke, and J. Lin. Simple dynamic emission strategies for microblog filtering. SIGIR, 2016.
[9] Y. Wang, G. Sherman, J. Lin, and M. Efron. Assessor differences and user preferences in tweet timeline generation. SIGIR, 2015.

1080

An Architecture for Privacy-Preserving and Replicable High-Recall Retrieval Experiments

Adam Roegiest
University of Waterloo

Gordon V. Cormack
University of Waterloo

ABSTRACT
We demonstrate the infrastructure used in the TREC 2015 Total Recall track to facilitate controlled simulation of "assessor in the loop" high-recall retrieval experimentation. The implementation and corresponding design decisions are presented for this platform. This includes the necessary considerations to ensure that experiments are privacy-preserving when using test collections that cannot be distributed. Furthermore, we describe the use of virtual machines as a means of system submission in order to to promote replicable experiments while also ensuring the security of system developers and data providers.
1. INTRODUCTION
The Total Recall track at TREC 2015 [15, 14] sought to investigate methods for achieving high-recall, with an assessor in the loop, through controlled simulation. The Total Recall track offered an online evaluation platform that allowed participants to produce systems that could request document assessments in a document-at-a-time manner, which is similar to the Microblog track's "evaluation as a service" methodology [11, 12]. This evaluation platform was primarily contained in a Web server (Section 3) that facilitated run creation, corpora distribution, the aforementioned document assessment process, and some basic online evaluation for training collections. Included in this platform was a baseline model implementation (the "BMI", Section 4.1), distributed as a VirtualBox virtual machine (VM), which participants could freely modify as they saw fit.
In developing this platform we sought to mitigate issues that have arisen in previous information retrieval research, and, in particular, high-recall retrieval experimentation. The primary issues we sought to address are as follows:
· Assessor-participant interaction and the effect on performance [9]
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17 - 21, 2016, Pisa, Italy c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911456

· Data anonymization and preserving the privacy of individuals in a corpus [1, 2]
· Re-usability of participant systems as on-going baselines for subsequent iterations of the track
· Lowering the barrier of participation in the track
We feel that we have successfully met these goals but we acknowledge that there are still improvements to be made. By controlling interaction through a simulated assessor (e.g., a system receives a binary judgement on a document), participant interaction does not rely on the ability of participants to get information out of the assessor but rather on the documents themselves. The BMI provides a working system to participants that they can use as a baseline for their experiments. Accordingly, there is less burden on participants to have an in-depth understanding of the Web server and can focus on their particular algorithms.
Furthermore, the track offered two modes of participation: an At-Home mode where interaction occurred over the Web; and a Sandbox mode where interaction occurred locally on a single machine. Participation in the Sandbox mode required participants to submit a working VM that would be ran, without Internet access, on test collections that were unknown to participants. Figure 1 provides a high-level conceptual depiction of how the live and sandbox modes differ.
We note that regardless of configuration, the data collections (the types and formats described in Section 2) are largely "plug-and-play", meaning that new data collections can, with little effort, be added to the platform. The result is that new experiments can be run on participant systems without necessitating participant action and without requiring participant effort -- other than that required to run the system. Furthermore, the Sandbox mode of participation allows collections to be used that would otherwise be "too hot" to distribute or would require onerous and time consuming anonymization that may not work.
In the remainder of this work, we describe the major components of the Total Recall track's platform: the test collections and their format (Section 2); the Web server and its sandboxing (Section 3); the baseline system, participant systems, and their sandboxing (Section 4). We conclude with a discussion of limitations and future improvements to the platform (Section 5). Throughout these sections, we will provide the underlying reasoning as to why a particular approach was taken when another might have sufficed.

1085

Participant Systems

Sandbox Machine Total Recall Server

Web API Server

Assessment and Log DB

Document Collections
Figure 1: A high-level look at how the various components interact in live and sandbox environments. Note that the dashed blue line denotes the Total Recall server and participant VMs running on the same machine.
2. TEST COLLECTIONS
For our purposes, we consider a test collection to be a set of documents along with a set of qrels, which are tuples mapping documents and topics (information needs) to relevance assessments. Data collections are transmitted to participant clients through the Total Recall service first by transmitting the set of documents and then by having systems requests relevance assessments in a document-at-a-time manner. This simulates the interaction between participants and a gold standard assessor. that was present in previous TREC Legal tracks [10, 7], TREC Filtering tracks [13], and the TREC Spam tracks [4, 3, 8], while also controlling that interaction to ensure that it is equitable amongst all participants. Furthermore, to ease system development we curated each test collection so that a single file contained a single document. This is contrary to many past TREC collections where a file might contain multiple documents and was performed to simplify document parsing. We additionally translated each document into a plain text rendering from whatever native format it was originally stored in (e.g., PST, WARC, etc) to ease the burden of system design as participants would not have to worry about processing multiple filetypes.
By taking this controlled approach to high-recall retrieval experimentation, we hope to mitigate any bias that might be perceived by spending "too much time" with a topic authority (gold standard assessor) or the ability to ask the right questions. The focus is then on the algorithms used and less so on the human interaction between topic authorities and participants. This should not be construed to suggest that such interaction is bad but that we merely sought to eliminate the confound that different approaches to such interaction may have on system evaluation. Furthermore, this approach facilitates experimentation with more topics over more test collections since there is no requirement to provide access to a high-quality topic authority (that is willing to volunteer valuable time), which has previously limited how many topics and how much interaction could be performed [10, 7].
For the purposes of system development, we used three publicly available test collections: 20-Newsgroups1; Reuters-217582; and, a variant of the Enron v2 e-mail col-
1http://qwone.com/~jason/20Newsgroups/ 2http://www.daviddlewis.com/resources/testcollections/

lection3. The inclusion of the first two datasets is to facilitate rapid development and testing of participant systems as they both contain approximately 20,000 documents. Neither was meant to be representative of a valid test collection but were merely there to help participant's gain confidence that their system was working. Moreover, we offered samples of these first two corpora as a mechanism to ensure participants understood and could correctly interact with the Web server, but without requiring the extensive processing time of a larger corpus. The Enron collection was our attempt to provide a representative collection, as it had previously been used in the TREC 2009 Legal track [10], that would more accurately indicate the effectiveness of participant systems. For all of these collections, we provided an online mechanism to provide recall, effort, precision, and F1 scores for checking system performance.
At the beginning of July, the At-Home phase of participation began and three additional corpora were released to participants (after signing the appropriate usage agreements). These collections were the official test collections and so participants did not receive any explicit system performance feedback as a means of preventing any potential meta-learning by systems. Although, we acknowledge that some participants may have had internal quality assurance processes that would have helped determine their own performance.
Not all corpora can be distributed by the service due to usage agreements or the risk of divulging sensitive data to the public, and so, generally, such restricted corpora will be distributed only in the form of Sandbox participation (discussed in Section 3.1). This helps to ameliorate issues with imperfect anonymization and the inability to give all interested parties access to the data. While no system that emits any information (e.g., even summary evaluation metrics) can ever be entirely secure, we believe that structuring the release of collections in this way aids in preserving the privacy of all parties when private collections are used. For all test collections, we refer interested parties to the track overview [15] for in-depth descriptions.
3. THE SERVER
The Total Recall server operates as a Web service that participant systems interact with over HTTP requests (i.e., a REST(ful) API). There are three main types of interactions between systems and the service:
· Request for information on a corpus and a link to download the corpus. Such details include the type of corpus (e.g., e-mail, newswire) and the language of the corpus (English only for 2015).
· Request for information on a topic to process, which includes the corresponding corpus and a description of the information need.
· Request for relevance assessment of documents with respect to a particular topic.
Secondary interactions can occur, including: starting and finalizing a run; result generation for developmental test collections; error log checking; and, other track specific actions beyond the scope of this paper.
reuters21578/ 3http://trec-legal.umiacs.umd.edu/corpora/trec/legal10/

1086

Figure 2 provides the general workflow that a client system might be expected to perform. For brevity we omit the actual API that was used to interact with the service and instead direct interested parties to the API documentation4. Note that all requests and responses to and from the server are encoded in JSON format, which has become a standard format for passing data between a Web server and client software.
In terms of implementation the entirety of the Total Recall web server is written in Node.js5, which is a Javascript runtime that uses an event-driven model for developing severside Web applications (e.g., REST(ful) APIs). Node.js is designed to be efficient and lightweight and is extremely popular in web development. While other architectures would have sufficed, Node.js was chosen due to its popularity and the opportunity to explore current state-of-the-art technologies in Web development. Where necessary (e.g., in storing judgments and a log of participant requests), the server relies on an installation of MySQL for persistant storage. As will be discussed in Section 5, using a default installation of MySQL was perhaps a poor choice and resulted in a small bottleneck near the end of the experimental period but has been corrected for following iterations. All code for the Total Recall track is available for public download in a git repository6.
3.1 Sandboxing the Server
To facilitate the use of test collections that require complicated transmission protocols (e.g., transmitted by a third party, such as NIST) or are "too hot to handle" (e.g., raw versions of confidential email), we provided the Sandbox mode of participation. In this mode, participants submit their systems as a VM (more details in Section 4) which we run in a restricted environment with access to the data.
Test collections do not necessarily have to be stored directly on the sandbox machine; instead data providers will be able to distribute test collections via USB flashdrive or external hard drive. Once the test collection is loaded into the service (using some relatively simple shell scripts), the participant systems are can be ran with these collections much in the same way that they would in the live scenario. However, the participant's VM is prohibited from accessing the Internet or Web in any way. This is done so that we can prevent any blatant data leakage from the sandbox environment. This restriction is enforced by limiting access of the clients machines only to the service and (optionally) by airwalling the sandbox machine (i.e., never connecting it to the Internet once the test collections are present). Air-walling occurred for one of the Sandbox collections due to the nature of the of the collection.
By enforcing these restrictions on the sandbox machine, the data is protected from unintended transmission as well as unintended dissemination of personal or private data. Furthermore, we can limit the only output of the sandbox server to be only summary evaluation measures and statistics (discussed in the track overview [15, 14]) once all participant systems have been ran. In doing so, the goal would be to prevent accidental distribution of private data that may be contained in qrels and document identifiers.
4quaid.uwaterloo.ca:33333/#/api 5See https://nodejs.org/en/ for more information. 6http://repo.trec-total-recall.com

Register System

Fetch Next Topic

Finalize Run

Yes

No

Fetch Corpus For Topic

No

Have Corpus?

Topics Remaining?

Yes

Yes

Process Corpus (e.g., tokenization,
indexing)

Request Document Assessment

Achieved high Recall?

No
Figure 2: The envisioned workflow of a Total Recall partic-
ipant system.

4. PARTICIPANT SYSTEMS
The envisioned workflow for a client system is depicted in Figure 2, and is implemented by the the Baseline Model Implementation (Section 4.1). Participants can construct several different systems to interact with the Total Recall service. Clients for At-Home experiments can be developed in several different ways: a purely automatic program (or set of programs) that performs the Total Recall task; a customized virtual machine that runs the above; as a "sidecar" (i.e., a directory containing additional scripts) supplied to the BMI's Virtual Machine. A "sidecar" submission can be run without any additional dependencies being added to the default BMI VM.
The Total Recall task is one that can be performed manually or through some combination of manual and automatic approaches. Accordingly, we allowed participants to submit a single manual run, which was to be performed before developing any automatic systems. Participants could submit manual runs using either the API, used by automatic systems, or a Web-based interface that we provided. The Web-based interface was quite simple and was largely just a pretty interface for the API. Out of the 3 manual teams, 2 used the Web interface and 1 directly communicated with the Web server via the API.
For sandbox submission, participants are required to submit systems that fall under the full VM or sidecar categories. This is done with the hope that systems would work out-ofthe-box, which was mostly true for TREC 2015. Previous experience with running participant systems in a sandbox for the TREC Spam tracks [8, 3, 4] led us to believe that allowing the submission of arbitrary code would lead to wasted effort in getting code to run correctly. As coordinators would then have to debug and fix-up any code that was not compatible with the sandbox environment (e.g., installing libraries, having necessary compiler versions). By using a VM, we hoped to limit the necessity of such tasks and were successful in this by having only one system require major coordinator intervention (out of 11 different systems).
A final benefit to requiring submissions of VMs or sidecars is that it allows commercial vendors of high-recall retrieval software to submit their products without requiring source code submission (as was required in previous tracks that used sandboxing). Vendors can submit a fully compiled version of their software that runs on the VM and so do not expose their intellectual property to track coordinators, data

1087

providers, or any other parties that might come into contact with the software. Extra steps can also be taken, such as encrypting the VM and/or software so that outside parties cannot access the software at all (outside of the normal interaction between client and server).
4.1 Baseline Model Implementation (BMI)
Our primary baseline is an augmented version of the Continuous Active Learning (CAL) method originally presented by Cormack and Grossman [5], which is called AutoTAR [6]. Unlike the original version of CAL, AutoTAR uses exponentially increasing batches sizes, and unlike the reported results of the AutoTAR paper, we use a tuned version of sofia-ml7 (based upon suggestions from the package's author).
This baseline is implemented as part of the virtual machine with sidecar approach, where the AutoTAR algorithm is refactored as a sidecar for a simple Debian VM. This was done to provide a model implementation of a sidecar and a meaningful working implementation that participants would be free to augment in the course of experimentation. The BMI is implemented using a combination of C++ and bash scripts (along with associated command line tools). The canonical implementation is available for download under GPL8.
Cormack and Grossman have shown that AutoTar generally outperforms a simple CAL implementation. Accordingly, our intent was to provide a reasonable baseline that could "fast-track" participants to a working system, without requiring them to worry about API programming. We hoped that the BMI would provide ample opportunity for participants to improve upon its results or be inspired by the technique and devise original algorithms of their own. Several of the participants made use of the BMI in their own submissions.
5. DISCUSSION
As with the development and use of any long running software, issues were likely to arise throughout the course of the track and did. Primarily, the biggest issue was a bottleneck in document assessment requests near the submission deadline. This was primarily due to the vanilla installation of MySQL which became bogged down when there were many queries being ran. In hindsight, this issue is relatively easy to solve by configuring MySQL to handle queries more effectively and by adding additional connections to the Web server so that multiple requests are not held up by a single connection to the database.
Surprisingly, the use of a REST(ful) API did not appear to cause participants too much trouble, in spite of our fears that it might during development. These troubles were likely ameliorated by providing tools like the BMI and the manual participation interface.
Another design decision to consider for future iterations is the use of full virtual machines. There are many merits to the use of virtual machines, primarily the fact that the entire system is self-contained and so security issues are more manageable due to more complete isolation from other software on the machine. This isolation comes with performance setbacks. For example, we could only have one par-
7https://code.google.com/archive/p/sofia-ml/ 8http://plg.uwaterloo.ca/~gvcormac/trecvm/

ticipant system running at a time due to the high overhead of both participant systems and the cost of virtualization on our sandbox machines. An alternative may be to use a more lightweight system, like Docker, which makes use of software containers. In short, software containers package an application together with its dependencies for running on an arbitrary (Linux) server. The easiest way to envision software containers is somewhere between virtual machines and auto-building tools (e.g., make, Maven). Accordingly, there is less emulation overhead that occurs with containers but at the expense greater exposure to the underlying system. A more thorough analysis of the costs and benefits is on-going and the inclusion of Docker containers may occur for TREC 2016.
6. CONCLUSIONS
We have presented a brief overview of the Total Recall track's online evaluation platform and the design decisions that went into its development. The use of this service is an attempt to develop a reusable system for evaluating information retrieval systems on both public and private data. In addition, we attempted to learn from issues that have arisen in previous TREC tracks dealing with high-recall retrieval and private collections. Interested parties are encouraged to read the track overview [15], visit the track website (trec-total-recall.org), join the mailing list9 to follow the progress of the track, and contribute to the development of the platform.
7. REFERENCES
[1] http://www.securityfocus.com/news/11497. [2] http://www.securitypronews.com/new-scam-tricking-
people-by-offering-new-facebook-profile-look-2012-11. [3] G. V. Cormack. TREC 2006 Spam Track Overview. In
Proc. TREC-2006. [4] G. V. Cormack. TREC 2007 Spam Track Overview. In
Proc. TREC-2007. [5] G. V. Cormack and M. R. Grossman. Evaluation of
Machine-learning Protocols for Technology-assisted Review in Electronic Discovery. In Proc. SIGIR 2014. [6] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv Preprint, 2015. [7] G. V. Cormack, M. R. Grossman, B. Hedin, and D. W. Oard. Overview of the TREC 2010 Legal Track. In Proc. TREC-2010. [8] G. V. Cormack and T. R. Lynam. TREC 2005 Spam Track Overview. In Proc. TREC-2005. [9] B. Hedin and D. W. Oard. Replication and automation of expert judgments: Information engineering in legal e-discovery. In Proc. Systems, Man and Cybernetics, 2009. [10] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. Overview of the TREC 2009 Legal Track. In Proc. TREC-2009. [11] J. Lin and M. Efron. Evaluation as a service for information retrieval. SIGIR Forum. [12] J. Lin and M. Efron. Overview of the trec-2013 microblog track. In Proc. TREC-2013. [13] S. Robertson and I. Soboroff. The TREC 2002 Filtering Track Report. In Proc. TREC 2002. [14] A. Roegiest and G. V. Cormack. Total recall track tools architecture overview. In Proc. TREC-2015. [15] A. Roegiest, G. V. Cormack, C. L. Clake, and M. R. Grossman. TREC 2015 Total Recall Track overview. In Proc. TREC-2015.
9Link available from track website.

1088

Simulating Interactive Information Retrieval

SimIIR: A Framework for the Simulation of Interaction

David Maxwell and Leif Azzopardi
School of Computing Science University of Glasgow Glasgow, Scotland
d.maxwell.1@research.gla.ac.uk Leif.Azzopardi@glasgow.ac.uk

ABSTRACT
Simulation provides a powerful and cost-effective approach to explore and evaluate how interactions between a searcher and system influence search behaviour and performance. With a growing interest in simulation and an increasing number of papers using such an approach, there is a need for a flexible framework for simulation. Thus, we present SimIIR, an open-source toolkit for building and conducting Interactive Information Retrieval (IIR) experiments. The framework consists of a number of high level components, including the simulation, the searcher and the system, all of which must be configured. The SimIIR framework provides a series of interchangeable components. Examples of these components include the querying strategies (how simulated queries are formulated) and stopping strategies (the depth to which a searcher will examine snippets and documents) that a simulated searcher will employ. We have implemented various existing strategies so that they can be used by other researchers to not only replicate and reproduce past experiments, but also create new experiments. This paper describes the SimIIR framework and the different components that can be configured and extended as required.
Keywords: Simulation, Search Behavior, Strategy, Stopping Strategies, Continuation Strategies, Querying Strategies, User Modeling
1. INTRODUCTION
The process of search is inherently interactive. During a search session, a searcher can issue a number of queries, examine a number of snippets, and assess documents for relevance to their information need. Despite this, the Information Retrieval (IR) community has centred much of its research around the so-called Cranfield Paradigm. Revolving around the concept of standardised test collections and relevance judgements, the paradigm makes a number
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911469

of key assumptions that are at odds with the interactive nature of search. Assumptions include a searcher: (i) issuing a single query; (ii) examining each document in turn and independently; and (iii) examining snippets and documents to a fixed depth. Simulation, as outlined by Keskustalo et al. [10], provides a means to go beyond the limitations presented by the Cranfield paradigm without conducting an expensive and time consuming user study. Simulation also offers a number of other benefits over user studies. The approach provides a rapid means to exploring `what-if ' scenarios, and also facilitates a range of evaluations, such as a comparison between systems and understanding searcher behaviours [4]. Simulations enable a range and variety of searchers to be created who do not suffer from issues such as fatigue or learning effects, unless specifically coded to do so. Simulated searchers are therefore highly controlled, and can therefore yield reproducible results [4].
With a growing interest in simulation and a growing number of researchers employing the technique in their work (e.g. [1, 2, 3, 6, 7, 9, 13, 14, 15, 17]), we argue that there is a need for an IIR-based simulation toolkit. A toolkit freely available to the IIR community will ensure that experiments conducted with it can be easily reproduced, as well as reducing the major overheads for creating simulations. To address this issue, we introduce an open-source framework for the simulation of IIR (SimIIR). Over the past two years, we have been building the SimIIR framework and used it to produce a number of different simulations [13, 14] - with more to come. The framework can be configured in a variety of different ways to support different search processes, different searcher configurations, different experimental conditions, and different search engines.
2. SIMIIR
Previous studies employing simulation have either: (i) considered aspects of the IIR process in isolation, such as query generation (e.g. [11]); or (ii) considered the search process as a whole (e.g. [14]). SimIIR is designed to capture and broadly reflect the complex processes involved within the wider IIR process, and has been developed from the ground up in the Python1 programming language using a highly modularised, class-based framework. Each component broadly represents a major stage in the IIR process2,
1Python is a general purpose, high level programming language available at http://www.python.org. The SimIIR framework is presently designed to run on Python 2.7.x environments. 2The SimIIR framework, complete with a variety of example simulation configuration files, is freely available at http://git.io/vZ5mH.

1141

curbing population growth

What measures have been taken worldwide and what wildlife extinction

countries have been

The spotted owl episode in

epAdwhtfoeahhfprsveiceueceicllrrh.taeibitvbe.p.viaeeoeonnppWtfbontauihhofaintlraeasngdaahatrrbroaktcedoicocceiymiaouwutnoobntnrtmundegsebheaaeed??inlnmrnontdenrpgcaoimasiftrusunaasersokctetliyis,adnnngctAeeskotnechfxntamphsoeftohteeenrcoiweihtrnnrviidartceceevosticasmel.isotitouohsontnieW.htg.hpoerho.atrfilfeeeiivwfsgseifshpnlottentdroecoltdittipehforUs.weefeeSvle.lnt

Documents discussing piracy on any body of water are...

Figure 1: An example of three topic descriptions used within the SimIIR framework. The first line denotes the topic title, with everything following the title considered the topic description.
with new components simply inheriting from the relevant base class. This allows for the easy experimentation of different simulated searchers using a variety of test collections, querying strategies, stopping strategies, and more. This also highlights one of the main advantages of SimIIR in that it can be easily adapted to suit ever more advanced components as the research in this area progresses. SimIIR has already been employed in several publications investigating stopping strategies by Maxwell et al. [13, 14].
SimIIR Components: In order to successfully run an experiment on SimIIR, a simulation must first be defined. A simulation is comprised of a series of topics, a search interface/engine, an output controller, and a series of simulated searchers - all of which are elaborated on in subsequent sections. A simulation in essence is loosely associated with the concept of a real-world experiment, such as a user study. The experiment would be comprised of a series of searchers or subjects, examining documents for relevance over one or more topics. The parameters for each of these settings are specified in an XML simulation configuration file and passed to SimIIR. The experiment itself consists of a series of runs, the total number of which can be calculated by summing the combinations of the specified number of searchers and topics. The final component comprising a simulation is the list of simulated searchers. Every simulated searcher is specified within a separate XML user configuration file. Within this file, a host of additional components are defined, namely: a querying strategy/generator; a classifier/decision maker for both snippets and documents; a stopping strategy; a logger; and a search context. The components which comprise the searcher are encoded within the underlying searcher model, providing the various actions that are observed.
2.1 Topics
We define a topic as a description of what a simulated searcher is expected to find relevant. These are specified by a series of topic description files, consisting of a title and description. Figure 1 illustrates examples of such files. All the topic files that have been generated for SimIIR thus far are based upon the topics provided by various TREC tracks, such as the TREC 2005 Robust Track.

ACTION ACTION ACTION ACTION ACTION ACTION ACTION ACTION ACTION ACTION ACTION

QUERY SERP SNIPPET DOC MARK SNIPPET DOC SNIPPET QUERY SERP SNIPPET

1200.0 1200.0 1200.0 1200.0 1200.0 1200.0 1200.0 1200.0 1200.0 1200.0 1200.0

13.9 15.4 16.7 40.5 43.1 44.4 68.2 69.5 83.4 84.9 86.2

extinction wildlife

EXAMINE_SERP

SNIPPET_REL

APW1998

EXAMINING_DOC

APW1998

CONSIDERED_REL APW1998

SNIPPET_REL

XIE1998

EXAMINING_DOC

XIE1998

SNIPPET_NR

NYT1999

efforts extinction wildlife

EXAMINE_SERP

SEEN_PREV

XIE1998

Figure 2: An example snippet of the output log file generated by SimIIR. Included is the action performed by the simulated searcher (e.g. QUERY, SNIPPET), the total time available (1200 seconds), the elapsed time of the simulated session, and the judgement for the action (if applicable), such as SNIPPET_REL for when a snippet is considered relevant.

2.2 Search Interface/Engine
We consider the search interface/engine component as an abstraction of a search engine and the Search Engine Results Page (SERP). After issuing the search interface/engine with a query, the component provides SimIIR with access to the SERP - a ranked list of snippets and associated documents. As the interface is highly genericised, any search engine with a Python wrapper can be easily coupled to the SimIIR framework. Presently, only a wrapper for the Whoosh IR toolkit3 has been implemented. Any additional configuration options for a particular search interface/engine can also easily be set in the simulation configuration file.
2.3 Output Controller
SimIIR also provides a flexible output controller with a variety of output options for the simulations that are run with the framework. Options are based upon the output files that are saved for each simulated run, and include options to save the interaction log of the simulated searchers (e.g. queries issued, snippets examined, documents examined), and whether to save the list of documents considered relevant by the simulated searchers. The file specified by the final option may then in turn be fed into an evaluation program such as trec_eval4 to obtain the values for the various measures and metrics that can be computed.
For each run, output files include: a .log file, which contains the complete set of interactions undertaken by the simulated searcher (refer to Figure 2 for an example of an output log); a .cfg file, containing the configuration of components for a given simulated searcher; an .out file, containing output from an evaluation program such as trec_eval (if enabled); a .queries file, containing a line break separated list of queries issued; and a .res file, containing a list of the documents considered relevant (in the format of a standard TREC results file). Runs are identified by unique identifiers specified in the simulation configuration and searcher configuration files to avoid results being overwritten.
2.4 Querying Strategies/Generators
The querying strategy/generator is the component responsible for the generation and selection of queries. A variety
3Whoosh is a pure Python indexing and search library, available at https://pypi.python.org/pypi/Whoosh. 4The trec_eval program is freely available to download and compile at http://trec.nist.gov/trec eval/.

1142

1 Examine Topic

2 Generate Queries

No, issue next query Yes
Stop?

3 Issue Query
Yes

View SERP

No, abandon SERP Continue?

6 Mark Document

No
Yes Document Relevant?

5 Read Document

4 Examine Snippet
No Snippet Relevant? Yes
Click Document

Figure 3: A flowchart of the Complex Searcher Model (CSM), from Maxwell et al. [14] - presently used within SimIIR as the underlying searcher model. The flowchart illustrates the key components of the model (in white) and decisions (in grey) that simulated searchers consider. Numbers are associated with the steps as described in Section 2.9. The CSM is adapted from the works of Baskaya et al. [6] and Thomas et al. [19].

of different simulated query generation strategies exist, such as the approaches discussed by Baskaya [5] and Keskustalo et al. [11]. Queries also need not be generated - a list of queries issued by real-world searchers can be used, if desired. This functionality has been already implemented through a PredeterminedQueryGenerator class, which takes as input a list of queries to be issued. This is useful for comparing the performance of real-world searchers and simulated searchers under similar conditions, as used by Maxwell et al. [14].
In addition to the generator detailed above, a number of query generation strategies have been operationalised as SimIIR classes. Several of the querying strategies proposed by Baskaya [5] and Keskustalo et al. [11] have already been implemented, such as the SingleTermQueryGenerator that generates single term queries, the BiTermQueryGenerator that generates two term queries, and the TriTermQueryGenerator that generates three term queries. All of these classes generate queries from the provided topic title and description text. Work is also underway to incorporate terms from sources other than the topic title and description, such as previously examined snippets and documents. This will result in a wider potential vocabulary for query generation.
2.5 Snippet/Document Classifiers
We next describe another major component of the SimIIR framework - classifiers. These components are responsible for determining the attractiveness of a given snippet - or the relevancy of a document - to the provided topic that the simulated searcher is tasked to examine. Snippet and document classifiers can be specified individually to enable the simulator to take into account the fact that searchers may judge snippets differently from documents. As an example, a searcher may be more liberal when deciding the attractiveness of a snippet as opposed to a document's relevancy.
A series of snippet/document classifiers have already been implemented and are ready for use. These include: a `TRECstyle' classifier, which judges everything to be relevant; an InformedTrecTextClassifier that uses TREC QRELS and probabilities (acting stochastically) to determine if a snippet/document should be considered attractive/relevant; and a LMTextClassifier which uses a language model (specified

by optional configuration parameters) that acts deterministically to determine attractiveness and/or relevancy.
2.6 Stopping Strategies
The stopping strategy decides where a simulated searcher should stop examining a list of ranked documents. This concept is called query stopping [14], as opposed to session stopping, which is presently determined by the logger component (refer to Section 2.7). A variety of stopping strategies have been previously proposed in the literature, such as the disgust and frustration point rules [8, 12].
As two recent publications used the SimIIR framework to simulate the effects of stopping strategies [13, 14], a variety of strategies have already been implemented. As an example, these include: a FixedDepthDecisionMaker, implementing a na¨ive approach where simulated searchers will stop after examining x snippets regardless of relevance; a NonrelDecisionMaker, which operationalises the aforementioned disgust and frustration point rules [8, 12], where simulated searchers will stop after judging x snippets non relevant; and an IftBasedDecisionMaker, a stopping strategy which operationalises the implicit stopping rule encoded within Information Foraging Theory (IFT) [14, 16].
2.7 Loggers
A logger is the component of the SimIIR framework that determines the costs of interaction for the simulated searcher when performing actions, such as issuing a query. Interaction costs can be defined in the user configuration file. The logger therefore tracks and records all interactions that take place within a simulated search session, and is for example able to state when a predetermined time limit elapses - thus providing some form of session stopping. The details stored within the logger component can then be sent to the output controller component upon completion of the simulated search session to produce the .log output file.
To date, we have fully implemented a FixedCostLogger that considers each of the different actions undertaken by a simulated searcher, and as the name suggests, imposes a fixed cost upon each of them. Currently, a variable cost logger is in the early stages of development. This more ad-

1143

vanced logger will for example provide the ability for SimIIR to impose variable costs that are dependent upon factors such as query or document length, for example [7, 18].
2.8 Search Contexts
The search context is the component of the SimIIR framework responsible for keeping a record of all issued queries, all examined snippets, documents (along with the judgements for each) and other interactions throughout a search session. The search context is in essence the `memory' of the simulated searcher, and interacts closely with the specified logger component to record all events (refer to Section 2.7). By referring to the search context, a snippet classifier can for example determine what snippets the simulated searcher has previously seen, whether they were considered relevant or non-relevant, and what text was observed.
From the basic search context, we have derived a relevance revision search context, as used by Maxwell et al. [13, 14]. When considering a snippet relevant, a simulated searcher using the revised relevance search context can then revise its judgement of said snippet if the associated document is subsequently deemed to be not relevant. Such a technique can influence the stopping point of the simulated searcher if using a stopping strategy based upon the frustration point and disgust stopping rules [8, 12] (refer to Section 2.6) for instance. We envisage that as work in the area of IIR simulation progresses, more complex search contexts can be implemented. For example, one such approach could be a `lossy' search context, where a simulated searcher would forget over time what snippets and documents have been examined.
2.9 The Searcher Model
When SimIIR is started, all the components are instantiated based upon the simulation and user configuration XML files. The process in which SimIIR simulates IIR is based upon the Complex Searcher Model (CSM), a model of interaction proposed by Maxwell et al. [14]. The model, represented as a flowchart in Figure 3, is based upon the stochastic model presented by Baskaya et al. [6], but includes additional decision points as described by Thomas et al. [19]. While the CSM does not presently represent every aspect of the IIR process, it does provide a better representation than has been used previously. The model can be extended as research in this area progresses.
Essentially, the simulated searcher begins by (1) examining the given topic and title description. From the title and description, the simulated searcher then (2) generates a series of queries which are issued to the underlying search engine. The simulated searcher then (3) issues a query from the generated list, and then (4) proceeds to examine the first/next snippet in the ranked list provided. The simulated searcher can also decide to issue a new query, thus returning to (3). If the snippet is considered relevant by the simulated searcher, (5) the document is then examined in full. If the document is also considered relevant, (6) the document is then marked relevant. If either the snippet or document are considered non-relevant, the simulated searcher then returns to (4) with the document unmarked.
3. SUMMARY
In this demonstration paper, we have described our new IIR simulation framework, SimIIR. The highly modularised architecture of the framework makes it straightforward to

implement new simulation components, and will aid in push-
ing research forward in this area. Future work will see the
development of more advanced components and the adap-
tion of the CSM to facilitate this.
Acknowledgments We would like to thank Professor Kalervo J¨arvelin and Dr Heikki Keskustalo at the University of Tampere, Finland for their co-operation and participation in a STSM, funded by the ESF supported MUMIA Cost Action (reference ECOST-STSM-IC1002-080914-049840). The lead author is also financially supported by the EPSRC, grant number 1367507. We would also like to thank Paul Thomas for his debugging efforts.
References
[1] L. Azzopardi. Query side evaluation: An empirical analysis of effectiveness and effort. In Proceedings of the 32nd ACM SIGIR, pages 556­563, 2009.
[2] L. Azzopardi. The economics in interactive information retrieval. In Proc. 34th ACM SIGIR, pages 15­24, 2011.
[3] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated queries for known-item topics: An analysis using six european languages. In Proc. 30th ACM SIGIR, pages 455­462, 2007.
[4] L. Azzopardi, K. J¨arvelin, J. Kamps, and M.D. Smucker. Report on the sigir 2010 workshop on the simulation of interaction. SIGIR Forum, 44(2):35­47, 2011.
[5] F. Baskaya. Simulating Search Sessions in Interactive Information Retrieval Evaluation. PhD thesis, University of Tampere, School of Information Sciences, Finland, 2014.
[6] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Modeling behavioral factors in interactive information retrieval. In Proc. 22nd ACM CIKM, pages 2297­2302, 2013.
[7] B. Carterette, A. Bah, and M. Zengin. Dynamic test collections for retrieval evaluation. In Proc. 5th ACM ICTIR, pages 91­100, 2015.
[8] W.S. Cooper. On selecting a measure of retrieval effectiveness part ii. implementation of the philosophy. J. of the American Soc. for Info. Sci., 24(6):413­424, 1973.
[9] K. J¨arvelin. Interactive relevance feedback with graded relevance and sentence extraction: Simulated user experiments. In Proc. 18th ACM CIKM, pages 2053­2056, 2009.
[10] H. Keskustalo, K. J¨arvelin, and A. Pirkola. Evaluating the effectiveness of relevance feedback based on a user simulation model: Effects of a user scenario on cumulated gain value. Information Retrieval, 11(3):209­228, 2008.
[11] H. Keskustalo, K. J¨arvelin, A. Pirkola, T. Sharma, and M. Lykke. Test collection-based ir evaluation needs extension toward sessions -- a case of extremely short queries. In Proc. 5th AIRS, pages 63­74, 2009.
[12] D.H. Kraft and T. Lee. Stopping rules and their effect on expected search length. IPM, 15(1):47 ­ 58, 1979.
[13] D. Maxwell, L. Azzopardi, K. J¨arvelin, and H. Keskustalo. An initial investigation into fixed and adaptive stopping strategies. In Proc. 38th ACM SIGIR, pages 903­906, 2015.
[14] D. Maxwell, L. Azzopardi, K. J¨arvelin, and H. Keskustalo. Searching and stopping: An analysis of stopping rules and strategies. In Proc. 24th ACM CIKM, pages 313­322, 2015.
[15] T. P¨a¨akk¨onen, K. J¨arvelin, J. Kek¨al¨ainen, H. Keskustalo, F. Baskaya, D. Maxwell, and L. Azzopardi. Exploring behavioral dimensions in session effectiveness. In Proc. 6th CLEF, pages 178­189, 2015.
[16] P. Pirolli and S.K. Card. Information foraging. Psychological Review, 106:643­675, 1999.
[17] M.D. Smucker. An analysis of user strategies for examining and processing ranked lists of documents. In Proc. of 5th HCIR, 2011.
[18] M.D. Smucker and C.L.A. Clarke. Time-based calibration of effectiveness measures. In Proc. 35th ACM SIGIR, pages 95­104, 2012.
[19] P. Thomas, A. Moffat, P. Bailey, and F. Scholer. Modeling decision points in user search behavior. In Proc. 5th IIiX, pages 239­242, 2014.

1144

Interleaved Evaluation for Retrospective Summarization and Prospective Notification on Document Streams

Xin Qian, Jimmy Lin, and Adam Roegiest
David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada

ABSTRACT
We propose and validate a novel interleaved evaluation methodology for two complementary information seeking tasks on document streams: retrospective summarization and prospective notification. In the first, the user desires relevant and non-redundant documents that capture important aspects of an information need. In the second, the user wishes to receive timely, relevant, and non-redundant update notifications for a standing information need. Despite superficial similarities, interleaved evaluation methods for web ranking cannot be directly applied to these tasks; for example, existing techniques do not account for temporality or redundancy. Our proposed evaluation methodology consists of two components: a temporal interleaving strategy and a heuristic for credit assignment to handle redundancy. By simulating user interactions with interleaved results on submitted runs to the TREC 2014 tweet timeline generation (TTG) task and the TREC 2015 real-time filtering task, we demonstrate that our methodology yields system comparisons that accurately match the result of batch evaluations. Analysis further reveals weaknesses in current batch evaluation methodologies to suggest future directions for research.
1. INTRODUCTION
As primarily an empirical discipline, evaluation methodologies are vital to ensuring progress in information retrieval. The ability to compare system variants and detect differences in effectiveness allows researchers and practitioners to continually advance the state of the art. One such approach, broadly applicable to any online service, is the traditional A/B test [12]. In its basic setup, users are divided into disjoint "buckets" and exposed to different treatments (e.g., algorithm variants); user behavior (e.g., clicks) in each of the conditions is measured and compared to assess the relative effectiveness of the treatments. As an alternative, information retrieval researchers have developed an evaluation methodology for web search based on interleaving results from two different comparison systems into a single ranked
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911494

list [8, 17, 6, 15, 7, 3, 16, 19], as well as recent extensions to more than two system [20]. Instead of dividing the user population into disjoint segments, all test subjects are exposed to these interleaved results. Based on user clicks, it is possible to assess the relative effectiveness of the two input systems with greater sensitivity than traditional A/B testing [17, 3], primarily due to the within-subjects design.
This paper explores interleaved evaluation for information seeking on document streams. Although we focus on a stream of social media updates (tweets), nothing in our formulation is specific to tweets. In this context, we tackle two complementary user tasks: In the retrospective summarization scenario, which is operationalized in the tweet timeline generation (TTG) task at TREC 2014 [13], the user desires relevant and non-redundant posts that capture key aspects of an information need. In the prospective notification scenario, operationalized in the real-time filtering task ("scenario A") at TREC 2015 [14], the user wishes to receive timely, relevant, and non-redundant updates (e.g., via a push notification on a mobile phone).
The contribution of this paper is the development and validation of an interleaved evaluation methodology for retrospective summarization and prospective notification on document streams, consisting of two components: a temporal interleaving strategy and a heuristic for credit assignment to handle redundancy. Although we can draw inspiration from the literature on interleaved evaluations for web search, previous techniques are not directly applicable to our tasks. We face a number of challenges: the important role that time plays in organizing and structuring system output, differing volumes in the number of results generated by systems, and notions of redundancy that complicate credit assignment. Our evaluation methodology addresses these complexities and is validated using data from the TREC 2014 and 2015 Microblog evaluations. Specifically, we simulate user interactions with interleaved results to produce a decision on whether system A is better than system B, and correlate these decisions with the results of batch evaluations. We find that our methodology yields accurate system comparisons under a variety of settings. Analysis also reveals weaknesses in current batch evaluation methodologies, which is a secondary contribution of this work.
2. BACKGROUND AND RELATED WORK
We begin by describing our task models, which are illustrated in Figure 1. We assume the existence of a stream of timestamped documents: examples include news articles coming off an RSS feed or social media posts such as tweets.

175

Retrospective  Summary

Prospective Notifications

now
Figure 1: Illustration of our task models. At some point in time ("now"), the user develops an information need: she requests a retrospective summary of what has happened thus far and desires prospective notifications of future updates.
In this context, we consider a pair of complementary tasks: suppose at some point in time the user develops an information need, let's say, about an ongoing political scandal. She would like a retrospective summary of what has occurred up until now, which might consist of a list of chronologicallyordered documents that highlight important developments. Once she has "come up to speed", the user might wish to receive prospective notifications (on her mobile phone) regarding future updates, for example, statements by the involved parties or the emergence of another victim. Retrospective summarization and prospective notification form two complementary components of information seeking on document streams. In both cases, users desire relevant and novel (nonredundant) content--they, for example, would not want to see multiple tweets that say essentially the same thing. In the prospective notification case, the user additionally desires timely updates--as close as possible to the actual occurrence of the "new development". This, however, isn't particularly important for the retrospective case, since the events have already taken place.
In this work, we present and evaluate an interleaved evaluation methodology for the retrospective summarization and prospective notification tasks described above. Although there has been substantial work on interleaved evaluation in the context of web search [8, 17, 6, 15, 7, 3, 16, 19], we face three main challenges:
1. Temporality plays an important role in our tasks. In web search, ranked lists from different systems can be arbitrarily interleaved (and in some cases the relative ordering of documents swapped) without significantly affecting users' interpretation of the results. In our task, however, the temporal ordering of documents is critical for the proper interpretation of system output.
2. We need to interleave results of different lengths. In web search, most interleaving strategies assume ranked lists of equal length, while this is not true in our case--some systems are more verbose than others.
3. We need to account for redundancy. In our tasks the notion of novelty is very important and "credit" is only awarded for returning non-redundant tweets. This creates a coupling effect between two systems where one's result might "mask" the novelty in the other. That is, a system's output becomes redundant only because the interleaving algorithm injected a relevant document before the document in question.
Nevertheless, there is a rich body of literature from which we can draw inspiration. In particular, we employ a simulationbased approach that is well-established for validating interleaved evaluations [6, 7, 16].
Our retrospective summarization and prospective notification tasks are grounded in the Microblog track evalua-

tions at TREC: specifically, the tweet timeline generation (TTG) task at TREC 2014 [13] and the push notification scenario ("scenario A") in the real-time filtering task at TREC 2015 [14]. Although there has been a substantial amount of work on developing systems that try to accomplish the tasks we study (see the TREC overview papers for pointers into the literature), our focus is not on the development of algorithms, but rather in evaluating system output. We adopt the framework provided by these tracks: relevance judgments and submitted runs are used in simulation studies to validate our interleaved evaluation methodology.
3. TASK AND METRICS
We begin by describing evaluations from TREC 2014 and 2015 that operationalize our retrospective summarization and prospective notification tasks.
3.1 Retrospective Summarization
Tweet timeline generation (TTG) was introduced at the TREC 2014 Microblog track. The putative user model is as follows: "At time T , I have an information need expressed by query Q, and I would like a summary that captures relevant information." The system's task is to produce a summary timeline, operationalized as a list of non-redundant, chronologically ordered tweets. It is imagined that the user would consume the entire summary (unlike a ranked list, where the user might stop reading at any time).
Redundancy was operationalized as follows: for every pair of tweets, if the chronologically later tweet contains substantive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise, the later tweet is redundant with respect to the earlier one. Thus, redundancy and novelty are antonyms; we use them interchangeably in opposite contexts. Due to the temporal constraint, redundancy is not symmetric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A, then B is redundant with respect to A, but not the other way around. The task also assumes transitivity. Suppose A precedes B and B precedes C: if B is redundant with respect to A and C is redundant with respect to B, then by definition C is redundant with respect to A.
The TTG assessment task can be viewed as semantic clustering--that is, we wish to group relevant tweets into clusters in which all tweets share substantively similar information. Within each cluster, the earliest tweet is novel; all other tweets in the cluster are redundant with respect to all earlier tweets. The track organizers devised a two-phase assessment workflow that implements this idea. In the first phase, all tweets are pooled and judged for relevance. In the second phase, relevant tweets for each topic are then clustered. We refer the reader to previous papers for more details [13, 22], but the final product of the human annotation process is a list of tweet clusters, each containing tweets that represent a semantic equivalence class.
In TREC 2014, TTG systems were evaluated in terms of set-based metrics (precision, recall, and F-score) at the cluster level. Systems only received credit for returning one tweet from each cluster--that is, once a tweet is retrieved, all other tweets in the cluster are automatically considered not relevant. In this study, we performed our correlation analysis against recall, for reasons that will become apparent later. The track evaluated recall in two different ways: unweighted and weighted. In the relevance assessment pro-

176

cess, tweets were judged as not relevant, relevant, or highly relevant. For unweighted recall (also called S-recall [23] and I-recall [18]), relevant and highly-relevant tweets were collapsed to yield binary judgments and all clusters received equal weight. For weighted recall, each cluster is assigned a weight proportional to the sum of relevance grades from every tweet in the cluster (relevant tweets receive a weight of one and highly-relevant tweets receive a weight of two).

3.2 Prospective Notification

In the real-time filtering task at TREC 2015 [14], the goal is for a system to identify interesting and novel content for a user in a timely fashion, with respect to information needs (called "interest profiles" but in actuality quite similar to traditional ad hoc topics). In the push notification variant of the task ("scenario A"), updates are putatively delivered in real time as notifications to users' mobile phones. A system was allowed to return a maximum of ten tweets per day per interest profile. The official evaluation took place over a span of ten days during July 2015, where all participating systems "listened" to Twitter's live tweet sample stream to complete the evaluation task; the interest profiles were made available prior to the evaluation period.
The assessment workflow was the same as the TTG task in TREC 2014 (see Section 3.1): relevance assessment using traditional pooling followed by semantic clustering. The task likewise used three-way judgments: not relevant, relevant, and highly relevant. We refer the reader to the TREC overview paper for more details [14].
The two metrics used to evaluate system runs were expected latency-discounted gain (ELG) and normalized cumulative gain (nCG). These two metrics are computed for each interest profile for each day in the evaluation period (explained in detail below). The final score of a run is the average of daily scores across all interest profiles.
The expected latency-discounted gain (ELG) metric was adapted from the TREC temporal summarization track [2]:

1 G(t)

(1)

N

where N is the number of tweets returned and G(t) is the gain of each tweet: not relevant tweets receive a gain of 0, relevant tweets receive a gain of 0.5, and highly-relevant tweets receive a gain of 1.0.
As with the TTG task, redundancy is penalized: a system only receives credit for returning one tweet from each cluster. Furthermore, per the track guidelines, a latency penalty is applied to all tweets, computed as MAX(0, (100 - d)/100), where the delay d is the time elapsed (in minutes, rounded down) between the tweet creation time and the putative time the tweet was delivered. That is, if the system delivers a relevant tweet within a minute of the tweet being posted, the system receives full credit. Otherwise, credit decays linearly such that after 100 minutes, the system receives no credit even if the tweet was relevant.
The second metric is normalized cumulative gain (nCG):

1 G(t)

(2)

Z

where Z is the maximum possible gain (given the ten tweet per day limit). The gain of each individual tweet is computed as above (with the latency penalty). Note that gain is not discounted (as in nDCG) because the notion of document ranks is not meaningful in this context.

Due to the setup of the task and the nature of interest profiles, it is possible (and indeed observed empirically) that for some days, no relevant tweets appear in the judgment pool. In terms of evaluation metrics, a system should be rewarded for correctly identifying these cases and not generating any output. We can break down the scoring contingency table as follows: If there are relevant tweets for a particular day, scores are computed per above. If there are no relevant tweets for that day, and the system returns zero tweets, it receives a score of one (i.e., perfect score) for that day; otherwise, the system receives a score of zero for that day. This means that an empty run (a system that never returns anything) may have a non-zero score.
4. INTERLEAVING METHODOLOGY
The development of an interleaved evaluation methodology requires answering the following questions:
1. How exactly do we interleave the output of two systems into one single output, in light of the challenges discussed in Section 2?
2. How do we assign credit to each of the underlying systems in response to user interactions with the interleaved results?
4.1 Interleaving Strategy
We begin by explaining why existing interleaving strategies for web search cannot be applied to either retrospective summarization or prospective notification. Existing strategies attempt to draw results from the test systems in a "fair" way: In balanced interleaving [8], for example, the algorithm maintains two pointers, one to each input list, and draws from the lagging pointer. Team drafting [17], on the other hand, follows the analogy of selecting teams for a friendly team-sports match and proceeds in rounds. Both explicitly assume (1) that the ranked lists from each system are ordered in decreasing probability of relevance (i.e., following the probability ranking principle) and (2) that the ranked lists are of equal length. Both assumptions are problematic because output in retrospective summarization and prospective notification must be chronologically ordered: a na¨ive application of an existing web interleaving strategy in the retrospective case would yield a chronologically jumbled list of tweets that is not interpretable. In the prospective case, we cannot "time travel" and push notifications "in the future" and then "return to the past". Furthermore, in both our tasks system outputs can vary greatly in verbosity, and hence the length of their results. This is an important aspect of the evaluation design as systems should learn when to "keep quiet" (see Section 3.2). Most existing interleaving strategies don't tell us what to do when we run out of results from one system. For these reasons it is necessary to develop a new interleaving strategy.
After preliminary exploration, we developed an interleaving strategy, called temporal interleaving, where we simply interleave the two runs by time. The strategy is easy to implement yet effective, as we demonstrate experimentally. Temporal interleaving works in the prospective case because time is always "moving forward". An example is shown in Figure 2, where we have system A on the left and system B on the right. The subscript of each tweet indicates its timestamp and the interleaved result is shown in the middle (note that tweet t28 is returned by both systems). One potential

177

System A t23 t28 t35 t46

Interleaved t23 t24 t28 t35 t46 t47

System B t24 t28 t47

Figure 2: Illustration of temporal interleaving. Note that tweet t28 is returned by both systems.

downside of this strategy is that all retrieved documents from both systems are included in the interleaved results, which increases its length--we return to address this issue in Section 5.3.
Our simple temporal interleaving strategy works as is for TTG runs, since system outputs are ordered lists of tweets. For push notifications, there is one additional wrinkle: which timestamp do we use? Recall that in prospective notification there is the tweet creation time and the push time (when the system identified the tweet as being relevant). We base interleaving on the push time because it yields a very simple implementation: we watch the output of two prospective notification systems and take the output as soon as a result is emitted by either system. However, we make sure to apply de-duplication: if a tweet is pushed by two systems but at different times, it will only be included once in the interleaved results.
4.2 User Interactions and Credit Assignment
In our interleaved evaluation methodology, output from the two different test systems are combined using the temporal interleaving strategy described above and presented to the user. We assume a very simple interaction model in which the user goes through the output (in chronological order) from earliest to latest and makes one of three judgments for each tweet: not relevant, relevant, and relevant but redundant (i.e., the tweet is relevant but repeats information that is already present in a previously-seen tweet). This extends straightforwardly to cases where we have graded relevance judgments: for the relevant and redundant judgments, the user also indicates the relevance grade. In retrospective summarization, the user is interacting with static system output, but in the prospective notification case, output is presented to the user over a period of time. This is called the "simple task", for reasons that will become clear shortly. We assume that users provide explicit judgments, in contrast to implicit feedback (i.e., click data) in the case of interleaved evaluations for web search; we return to discuss this issue in Section 5.4.
Based on user interactions with the interleaved results, we must now assign credit to each of the test systems, which is used to determine their relative effectiveness. Credit assignment for the relevant label is straightforward: credit accrues to the system that contributed the tweet to the interleaved results (or to both if both systems returned the tweet). However, credit assignment for a tweet marked redundant is more complex--we do not know, for example, if the redundancy

System A credit

Not Relevant

System B credit

Relevant

+1

+1

Redundant

Not Relevant

Relevant

+1

+0.66

Redundant

Figure 3: Example of interleaving credit assignment and redundancy handling.

was actually introduced by the interleaving. That is, the interleaving process inserted a tweet (from the other run) before this particular tweet that made it redundant.
We can illustrate this with the diagram in Figure 3. A dotted border represents a tweet contributed by system A (on the left) and a solid border represents a tweet contributed by system B (on the right). Suppose the assessor judged the tweets as they are labeled in the figure. The second and fifth tweets are marked relevant, and so system B gets full credit twice. Now let's take a look at the third tweet, contributed by system A, which is marked redundant--we can confidently conclude in this case that the redundancy was introduced by the interleaving, since there are no relevant tweets above that are contributed by system A. Therefore, we can give system A full credit for the third tweet. Now let's take a look at the sixth tweet: generalizing this line of reasoning, the more that relevant tweets above are from system B, the more likely that we're encountering a "masking effect" (all things being equal), where the redundancy is an artifact of the interleaving itself. To capture this, we introduce the following heuristic: the amount of credit given to a system for a tweet marked redundant is multiplied by a discount factor equal to the fraction of relevant and redundant tweets above that come from the other system. In this case, there are two relevant tweets above, both from system B, and one redundant tweet from system A, so system A receives a credit of 0.66.
More formally, consider an interleaved result S consisting of tweets s1 . . . sn drawn from system A and system B. We denote SA and SB as those tweets in S that come from system A and B, respectively. For a tweet si judged redundant, if si  SA, then we multiple its gain by a discount factor DA as follows:

DA(si)

=

|{sj |j

<

i  I(sj )  sj T (si)



SB }|

(3)

T (si) = |{sj|j < i  I(sj)  sj  SA}|+

(4)

|{sj |j < i  I(sj )  sj  SB}|

where I(s) is an indicator function that returns one if the user (previously) judged the tweet to be either relevant or redundant, or zero otherwise. On the other hand, if si  SB, we apply a discount factor DB that mirrors DA above (i.e., flipping subscripts A and B). If si is both in SA and SB, we apply both equations and give each system a different amount of credit (summing up to one).
We emphasize, of course, that this way of assigning credit for redundant judgments is a heuristic (but effective, from

178

our evaluations). For further validation, we introduce an alternative interaction model that we call the "complex task": in this model, the user still marks each tweet not relevant, relevant, and redundant, but for each redundant tweet, the user marks the source of the redundancy, i.e., which previous tweet contains the same information. With this additional source of information, we can pinpoint the exact source of redundancy and assign credit definitively (zero if the source of redundancy was from the same run, and one if from the other run). Of course, such a task would be significantly more onerous (and slower) than just providing three-way judgments, but this "complex task" provides an upper bound that allows us to assess the effectiveness of our credit assignment heuristic.
One final detail: In the prospective task, we still apply a latency penalty to the assigned credit, as in ELG. Thus, in the case of a tweet that was pushed by both systems, but at different times, they will receive different amounts of credit. In the interleaved results, of course, the tweet will appear only once--from that single judgment we can compute the credit assigned to each system.
To recap: we have presented a temporal interleaving strategy to combine system output, introduced a model for how users interact with the results, and devised a credit assignment algorithm (including redundancy handling) that scores the systems based on user interactions. From this, we arrive at a determination of which system is more effective. Do these decisions agree with the results of batch evaluations? We answer this question with simulation studies based on runs submitted to TREC 2014 (for retrospective summarization) and TREC 2015 (for prospective notification).
5. SIMULATION RESULTS
5.1 Retrospective Summarization
To validate our interleaved evaluation methodology for retrospective summarization, we conducted user simulations using runs from the TREC 2014 TTG task. In total, 13 groups submitted 50 runs to the official evaluation. For each pair of runs, we applied the temporal interleaving strategy described above and simulated user interactions with the "ground truth" cluster annotations. Each simulation experiment comprised 67,375 pairwise comparisons, which we further break down into 63,415 comparisons of runs from different groups (inter-group) and 3,960 comparisons between runs from the same group (intra-group). Wang et al. [22] were able to elicit two completely independent sets of cluster annotations, which they refer to as the "official" and "alternate" judgments. Thus, we were able to simulate user interactions with both sets of clusters.
First, we ran simulations using binary relevance judgments. Results are shown in Table 1. When comparing simulation results (which system is better, based on assigned credit) with the batch evaluation results (unweighted recall), there are four possible cases:
· The compared runs have different batch evaluation results and the simulation was able to detect those differences; denoted (Agree, ).
· The compared runs have the same batch result and the simulation assigned equal credit to both runs; denoted (Agree, ¬).

· The compared runs have different batch evaluation results but the simulation was not able to detect those differences; denoted (Disagree, ).
· The compared runs have the same batch result and the simulation falsely ascribed differences in effectiveness between those runs; denoted (Disagree, ¬).
In the first two cases, the batch evaluation and interleaved evaluation results are consistent and the interleaving can be said to have given "correct" results; this is tallied up as (Agree, Total) in the results table. In the last two cases, the batch evaluation and interleaved evaluation results are inconsistent and the interleaving can be said to have given "incorrect" results; this is tallied up as (Disagree, Total) in the results table.1
With "official" and "alternate" clusters, there are four ways we can run the simulations: simulate with official judgments, correlate with batch evaluation results using official judgments (official, official); simulate with alternate judgments, correlate with batch evaluation results using alternative judgments (alternate, alternate); as well as the symmetrical cases where the simulation and batch evaluations are different, i.e., (official, alternate) and (alternate, official). Table 1 shows all four cases, denoted by the first two columns. Finally, the two vertical blocks of the table denote the results of the "simple task" (simulated user provides three-way judgments) and the "complex task" (simulated user additionally marks the source of a redundant tweet).
There is a lot of information to unpack from Table 1. Focusing only on "all pairs" with the "simple task", we see that our simulation results agree with batch evaluation results 92%-93% of the time, which indicates that our interleaved evaluation methodology is effective. The inaccuracies can be attributed to the credit assignment heuristic for redundant labels--this can be seen from the "complex task" block, where accuracy becomes 100% if we ask the (simulated) user to mark the source of the redundancy. Of course, this makes the task unrealistically onerous, so we argue that our credit assignment heuristic strikes the right balance between accuracy and complexity.
With the (official, official) and the (alternate, alternate) conditions, we are simulating user interactions and computing batch results with the same cluster assignments. With the other two conditions, we simulate with one set of clusters and perform batch evaluations with the other--the difference between these two sets quantifies inter-assessor differences. Results suggest that the effect of using different assessors is relatively small--this finding is consistent with that of Wang et al. [22], who confirmed the stability of the TTG evaluation with respect to assessor differences.
The inter-group and intra-group comparisons suggest how well our interleaved evaluation methodology would fare under slightly different conditions. Runs by the same group (intra-group) often share similar algorithms (perhaps varying in parameters), which often yield runs that are similar in effectiveness (or the same). This makes differences more difficult to detect, and indeed, Table 1 shows this to be the
1Methodologically, our approach differs from many previous studies that take advantage of click data. For example, Chapelle et al. [3] studied only a handful of systems (far fewer than here) but across far more queries, and hence are able to answer certain types of questions that we cannot. Also, most previous studies do not consider system ties, with He et al. [6] being an exception, but they do not explicitly break out the possible contingencies as we do here.

179

Simulation Judgment



All Pairs

official alternate official alternate

official alternate alternate official

89.6% 88.8% 88.4% 88.7%

Inter-Group Pairs Only

official alternate official alternate

official alternate alternate official

91.1% 90.3% 89.9% 90.2%

Intra-Group Pairs Only

official alternate official alternate

official alternate alternate official

65.8% 65.1% 64.3% 64.7%

Agree ¬
3.7% 3.6% 3.6% 3.5%
2.8% 2.7% 2.7% 2.6%
18.1% 17.8% 17.9% 17.6%

Simple Task Disagree
Total  ¬ Total

93.3% 92.4% 92.0% 92.2%

3.0% 3.5% 3.8% 3.9%

3.7% 4.1% 4.2% 3.9%

6.7% 7.6% 8.0% 7.8%

93.9% 93.0% 92.6% 92.8%

2.8% 3.3% 3.6% 3.7%

3.3% 3.7% 3.8% 3.5%

6.1% 7.0% 7.4% 7.2%

83.9% 82.9% 82.2% 82.3%

5.8% 6.5% 7.3% 7.0%

10.3% 10.6% 10.5% 10.7%

16.1% 17.1% 17.8% 17.7%


92.6% 92.3% 89.2% 89.2%
93.9% 93.6% 90.7% 90.7%
71.6% 71.6% 65.3% 65.3%

Complex Task

Agree

Disagree

¬ Total  ¬ Total

7.4% 7.7% 5.6% 5.6%

100.0% 100.0%
94.8% 94.8%

0 0 3.1% 3.4%

0 0 2.1% 1.8%

0 0 5.2% 5.2%

6.1% 6.4% 4.4% 4.4%

100.0% 100.0%
95.1% 95.1%

0 0 2.9% 3.2%

0 0 2.0% 1.7%

0 0 4.9% 4.9%

28.4% 28.4% 24.0% 24.0%

100.0% 100.0%
89.3% 89.3%

0 0 6.3% 6.3%

0 0 4.4% 4.4%

0 0 10.7% 10.7%

Table 1: TTG simulation results for both the "simple task" and the "complex task". The "Agree" columns give the percentages of cases where the simulation results agree with the batch evaluation results, when the runs actually differ (), and when the runs don't differ (¬). The "Disagree" columns give the percentages of cases where the simulation results disagree with the batch evaluation results, when the runs actually differ (), and when the runs don't differ (¬).

60

MB178 (All Pairs)

40

¢ Assigned Credit

20

0

-20

-40

-60

-0.6 -0.4 -0.2

0.0

0.2

0.4

0.6

¢ Unweighted Recall

Figure 4: Scatterplot showing batch vs. simulation results for topic MB178.

case (lower agreement). In contrast, differences in effectiveness in runs between groups (inter-group) are slightly easier to detect, as shown by the slightly higher agreement.
To help further visualize our findings, a scatterplot of simulation results is presented in Figure 4 for a representative topic, MB 178, under the all-pairs, (official, official) condition. Each point represents a trial of the simulation comparing a pair of runs: the x coordinate denotes the difference based on the batch evaluation, and the y coordinate denotes the difference in assigned credit based on the simulation. We see that there is a strong correlation between simulation and batch results. Plots from other topics look very similar, except differing in the slope of the trendline (since credit is not normalized, but recall is).
The previous results did not incorporate graded relevance judgments. Our next set of experiments examined this refinement: relevant tweets receive a credit of one and highlyrelevant tweets receive a credit of two. The simulated user now indicates the relevance grade for the relevant and redun-

dant cases. There is, however, the question of which batch metric to use: the official TREC evaluation used weighted recall, where the weight of each cluster was proportional to the sum of the relevance grades of tweets in the cluster. This encodes the simple heuristic that "more discussed facets are more important", which seems reasonable, but Wang et al. [22] found that this metric correlated poorly with human preferences, suggesting that cluster size is perhaps not a good measure of importance. We ran simulations correlating against official weighted recall: the results were slightly worse than those in Table 1, but still quite good. For example, we achieved 90% accuracy in the (official, official) condition on the simple task, as opposed to 93%.
However, given the findings of Wang et al., these simulations might not be particularly meaningful. As an alternative, we propose a slightly different approach to computing the cluster weights: instead of the sum of relevance grades of tweets in the cluster, we use the highest relevance grade of tweets in the cluster. That is, if a cluster contains a highly-relevant tweet, it receives a weight of two; otherwise, it receives a weight of one. This weighting scheme has the effect that scores are not dominated by huge clusters. The results of these simulations are shown in Table 2.
From these experiments, we see that accuracy remains quite good, suggesting that our interleaved evaluation methodology is able to take advantage of graded relevance judgments. One important lesson here is that capturing "cluster importance" in TTG is a difficult task, and that it is unclear if present batch evaluations present a reasonable solution. Without a well-justified batch evaluation metric, we lack values against which to correlate our simulation outputs. Thus, these results reveal a weakness in current batch evaluations (indicating avenues of future inquiry), as opposed to a flaw in our interleaved evaluation methodology.
5.2 Prospective Notification
For prospective notification, we validated our interleaved evaluation methodology using runs submitted to the TREC 2015 real-time filtering task ("scenario A"). In total, there

180

Simulation Judgment



All Pairs official alternate official alternate

official alternate alternate official

89.9% 89.0% 88.6% 88.7%

Inter-Group Pairs Only

official alternate official alternate

official alternate alternate official

91.3% 90.4% 90.0% 90.2%

Intra-Group Pairs Only

official

official

66.5%

alternate alternate 65.8%

official

alternate 64.7%

alternate official

65.3%

Agree ¬
3.1% 3.0% 3.0% 3.0%
2.2% 2.2% 2.1% 2.1%
17.2% 16.8% 17.0% 16.9%

Simple Task Disagree
Total  ¬ Total

93.0% 92.0% 91.6% 91.7%

4.1% 4.7% 5.1% 1.3%

2.9% 3.3% 3.3% 3.0%

7.0% 8.0% 8.4% 8.3%

93.5% 92.6% 92.1% 92.3%

3.9% 4.5% 4.9% 5.0%

2.6% 2.9% 3.0% 2.7%

6.5% 7.4% 7.9% 7.7%

83.7% 82.6% 81.7% 82.2%

7.4% 8.2% 9.3% 8.6%

8.9% 9.2% 9.0% 9.2%

16.3% 17.4% 18.3% 17.8%

Complex Task

Agree

Disagree

 ¬ Total  ¬ Total

93.3% 92.9% 89.9% 90.0%

5.5% 5.7% 4.4% 4.4%

98.8% 98.6% 94.3% 94.4%

0.7% 0.8% 3.8% 4.0%

0.5% 0.6% 1.9% 1.6%

1.2% 1.4% 5.7% 5.6%

94.6% 94.2% 91.4% 91.5%

4.2% 4.5% 3.3% 3.3%

98.9% 98.7% 94.7% 94.7%

0.6% 0.7% 3.5% 3.8%

0.5% 0.6% 1.8% 1.5%

1.1% 1.3% 5.3% 5.3%

72.5% 72.6% 66.3% 66.3%

25.0% 25.0% 21.9% 22.0%

97.5% 97.6% 88.2% 88.3%

1.4% 1.4% 7.7% 7.6%

1.1% 1.0% 4.1% 4.1%

2.5% 2.4% 11.8% 11.7%

Table 2: TTG simulation results with graded relevance judgments, organized in the same manner as Table 1.

were 37 runs from 14 groups submitted to the official evaluation. This yields a total of 33,966 pairwise comparisons; 32,283 inter-group pairs and 1,683 intra-group pairs.
Simulation results (with graded relevance judgments) are shown in Table 3 for correlations against ELG and in Table 4 for correlations against nCG. The table is organized in the same manner as Tables 1 and 2, with the exception that we only have one set of cluster annotations available, so no "official" vs. "alternate" distinction.
Results of the simulation, shown under the rows marked retaining "quiet days", are quite poor. Analysis reveals that this is due to the handling of days for which there are no relevant tweets. Note that for days without any relevant tweets, there are only two possible scores: one if the system does not return any results, and zero otherwise. Thus, for interest profiles with few relevant tweets, the score is highly dominated by these "quiet days". As a result, a system that does not return anything scores quite highly; in fact, better than most submitted runs [14]. To make matters worse, since 2015 was the first year of this TREC evaluation, systems achieved high scores by simply returning few results, in many cases for totally idiosyncratic reasons--for example, the misconfiguration of a score threshold.
This property of the official evaluation is problematic for interleaved evaluations since it is impossible to tell without future knowledge whether there are relevant tweets for a particular day. Consider the case when system A returns a tweet for a particular day and system B does not return anything, and let's assume we know (based on an oracle) that there are no relevant tweets for that day: according to our interleaved evaluation methodology, neither system would receive any credit. However, based on the batch evaluation, system B would receive a score of one for that day. There is, of course, no way to know this at evaluation time when comparing only two systems, and thus the interleaved evaluation results would disagree with the batch evaluation results. The extent of this disagreement depends on the number of days across the topics for which there were no relevant tweets. Since the interest profiles for the TREC 2015 evaluation had several quiet days each, our interleaved evaluation methodology is not particularly accurate.

We argue, however, that this is more an artifact of the current batch evaluation setup than a flaw in our interleaved evaluation methodology per se; see Tan et al. [21] for further discussion. As the track organizers themselves concede in the TREC overview paper [14], it is not entirely clear if the current handling of days with no relevant tweets is appropriate. While it is no doubt desirable that systems should learn when to "remain quiet", the current batch evaluation methodology yields results that are idiosyncratic in many cases.
To untangle the effect of these "quiet days" in our interleaved evaluation methodology, we conducted experiments where we simply discarded days in which there were no relevant tweets. That is, if an interest profile only contained three days (out of ten) that contained relevant tweets, the score of that topic is simply an average of the scores over those three days. We modified the batch evaluation scripts to also take this into account, and then reran our simulation experiments. The results are shown in Table 3 and Table 4 under the rows marked discarding "quiet days". In this variant, we see that our simulation results are quite accurate, which confirms that the poor accuracy of our initial results is attributable to days where there are no relevant tweets. Once again, this is an issue with the overall TREC evaluation methodology, rather than a flaw in our interleaving approach. These findings highlight the need for additional research on metrics that better model sparse topics. In order to remove this confound, for the remaining prospective notification experiments, we discarded the "quiet days".
Our credit assignment algorithm is recall oriented in that it tries to quantify the total amount of relevant information a user receives, and so it is perhaps not a surprise that credit correlates with nCG. However, experiments show that we also achieve good accuracy correlating with ELG (which is precision oriented). It is observed in the TREC 2015 evaluation [14] that there is reasonable correlation between systems' nCG and ELG scores. There is no principled explanation for this, as prospective notification systems could very well make different precision/recall tradeoffs. However, there is the additional constraint that systems are not allowed to push more than ten tweets per day, so that a high-

181

Condition

Agree  ¬

Retaining "quiet days"

All Pairs

45.6%

Inter-Group Pairs 46.4%

Intra-Group Pairs 29.0%

16.6% 15.3% 41.4%

Discarding "quiet days"

All Pairs

56.7% 34.8%

Inter-Group Pairs 58.0% 33.9%

Intra-Group Pairs 32.0% 52.7%

Simple Task

Disagree

Total

 ¬ Total

62.2% 37.7% 0.1% 37.8% 61.7% 38.2% 0.1% 38.3% 70.4% 29.1% 0.5% 29.6%

91.5% 8.4% 0.1% 8.5% 91.9% 8.0% 0.1% 8.1% 84.7% 14.8% 0.5% 15.3%

Complex Task

Agree

Disagree

 ¬ Total

 ¬ Total

45.6% 16.6% 62.2% 37.6% 0.2% 37.8% 46.5% 15.3% 61.8% 38.1% 0.1% 38.2% 29.5% 41.7% 71.2% 28.6% 0.2% 28.8%

56.8% 34.8% 91.6% 8.3% 0.1% 8.4% 58.1% 33.9% 92.0% 7.9% 0.1% 8.0% 32.5% 53.0% 85.5% 14.3% 0.2% 14.5%

Table 3: Results of push notification simulations, correlating against ELG.

Condition

Agree  ¬

Retaining "quiet days"

All Pairs

52.8%

Inter-Group Pairs 53.5%

Intra-Group Pairs 38.3%

16.7% 15.3% 43.8%

Discarding "quiet days"

All Pairs

62.5% 35.2%

Inter-Group Pairs 63.7% 34.1%

Intra-Group Pairs 41.2% 55.4%

Simple Task

Disagree

Total

 ¬ Total

69.5% 30.0% 0.5% 30.5% 68.8% 30.7% 0.5% 31.2% 82.1% 17.3% 0.6% 17.9%

97.7% 97.8% 96.6%

2.1% 0.2% 2.1% 0.1% 2.9% 0.5%

2.3% 2.2% 3.4%

Complex Task

Agree

Disagree

 ¬ Total

 ¬ Total

52.8% 16.8% 69.6% 30.0% 0.4% 30.4% 53.6% 15.4% 69.0% 30.6% 0.4% 31.0% 37.8% 44.1% 81.9% 17.8% 0.3% 18.1%

62.7% 35.3% 98.0% 63.8% 34.3% 98.1% 40.9% 55.8% 96.7%

2.0% 1.9% 3.3%

0 2.0% 0 1.9% 0 3.3%

Table 4: Results of push notification simulations, correlating against nCG.

All Pairs Inter-Group Pairs Intra-Group Pairs

Summarization
92.8% 94.0% 73.7%

Notification
96.9% 97.9% 76.5%

Table 5: Lengths of interleaved results as a percentage of the sum of the lengths of the individual runs.
volume low-precision system would quickly use up its "daily quota". Additionally, we suspect that since TREC 2015 represented the first large-scale evaluation of this task, teams have not fully explored the design space.
5.3 Assessor Effort: Output Length
We next turn our attention to two issues related to assessor effort: the length of the interleaved system output (this subsection) and the effort involved in providing explicit judgments in our interaction model (next subsection).
One downside of our temporal interleaving strategy is that the interleaved results are longer than the individual system outputs. Exactly how much longer is shown in Table 5, where the lengths of the interleaved results are shown as a percentage of the sum of the lengths of the individual runs. The lengths are not 100% because the individual system outputs may contain overlap, and comparisons between runs from the same group contain more overlap. Nevertheless, we can see that temporal interleaving produces output that is substantially longer than each of the individual system outputs. This is problematic for two reasons: first, it means a substantial increase in evaluation effort, and second, the interleaving produces a different user experience in terms of the verbosity of the system.
There is, however, a simple solution to this issue: after temporal interleaving, for each result we flip a biased coin

and retain it with probability p. That is, we simply decide to discard some fraction of the results. Figure 5 shows the results of these experiments. On the x axis we sweep across p, the retention probability, and on the y axis we plot the simulation accuracy (i.e., agreement between simulation credit and batch results). The left plot shows the results for retrospective summarization using unweighted recall and the (official, official) condition; the rest of the graphs look similar and so we omit them for brevity. In the middle plot, we show accuracy against ELG for prospective notification and against nCG on the right (both discarding quiet days). Since there is randomness associated with these simulations, the plots represent averages over three trials.
We see that simulation results remain quite accurate even if we discard a relatively large fraction of system output. For the prospective task, accuracy is higher for lower p values because there are many intra-group ties. At p = 0, accuracy is simply the fraction of "no difference" comparisons. Based on these results, an experiment designer can select a desired tradeoff between accuracy and verbosity. With p around 0.5 to 0.6, we obtain an interleaved result that is roughly the same length as the source systems--and in that region we still achieve good prediction accuracy. It is even possible to generate interleaved results that are shorter than the input runs. Overall, we believe that this simple approach adequately addresses the length issue.
5.4 Assessor Effort: Explicit Judgments
Another potential objection to our interleaved evaluation methodology is that our interaction model depends on explicit judgments for credit assignment, as opposed to implicit judgments (i.e., clicks) in the case of interleaved evaluations for web ranking. This issue warrants some discussion, because the ability to gather implicit judgments based on be-

182

Retrospective Summarization: Unweighted Recall

100

100

Prospective Notification: ELG

Prospective Notification: nCG 100

80

80

80

Simulation accuracy

Simulation accuracy

Simulation accuracy

60

60

60

40

40

40

20

all_pairs

inter_systems

intra_systems

0

0

20

40

60

80

100

Retention probability p

20

all_pairs

inter_systems

intra_systems

0

0

20

40

60

80

100

Retention probability p

20

all_pairs

inter_systems

intra_systems

0

0

20

40

60

80

100

Retention probability p

Figure 5: Simulation accuracy as a function of retention probability p for unweighted recall on retrospective summarization (left); ELG (middle) and nCG (right) for prospective summarization.

Retrospective Summarization: Unweighted Recall

100

100

Prospective Notification: ELG

Prospective Notification: nCG 100

80

80

80

Simulation accuracy

Simulation accuracy

Simulation accuracy

60

60

60

40

40

40

20

all_pairs

inter_systems

intra_systems

00

20

40

60

80

100

User judgement probability r

20

all_pairs

inter_systems

intra_systems

00

20

40

60

80

100

User judgement probability r

20

all_pairs

inter_systems

intra_systems

00

20

40

60

80

100

User judgement probability r

Figure 6: Simulation accuracy as a function of user judgment probability r for unweighted recall on retrospective summarization (left); ELG (middle) and nCG (right) for prospective summarization.

havioral data greatly expands the volume of feedback we can easily obtain (e.g., from log data).
We have two responses: First, it is premature to explore implicit interactions for our tasks. For web search, there is a large body of work spanning over two decades that has validated the interpretation of click data for web ranking preferences--including the development of click models [1, 4], eye-tracking studies [5, 9], extensive user studies [10], and much more [11]. In short, we have a pretty good idea of how users interact with web search results, which justifies the interpretation of click data. None of this exists for retrospective summarization and prospective notification. Furthermore, interactions with tweets in our case are more complex: some tweets have embedded videos, images, or links. There are many different types of clicks: the user can "expand" a tweet, thereby showing details of the embedded object and from there take additional actions, e.g., play the embedded video directly, click on the link to navigate away from the result, etc. Not taking any overt action on a tweet doesn't necessary mean that the tweet is not relevant--the succinct nature of tweets means that relevant information can be quickly absorbed, perhaps without leaving any behavioral trails. Thus, any model of implicit interactions we could develop at this point would lack empirical grounding. More research is necessary to better understand how users interact with retrospective summarization and prospective notification systems. With a better understanding, we can then compare models of implicit feedback with the explicit feedback results presented here.
Our second response argues that in the case of prospective notifications, an explicit feedback model might not actually be unrealistic. Recall that such updates are putatively delivered via mobile phone notifications, and as such, they are

presented one at a time to the user--depending on the user's settings, each notification may be accompanied by an auditory or physical cue (a chime or a vibration) to attract the user's attention. In most implementations today the notification can be dismissed by the user or the user can take additional action (e.g., click on the notification to open the mobile app). These are already quite explicit actions with relatively clear user intent--it is not far-fetched to imagine that these interactions can be further refined to provide explicit judgments without degrading the user experience.
Nevertheless, the issue of assessor effort in providing explicit judgments is still a valid concern. However, we can potentially address this issue in the same way as the length issue discussed above. Let us assume that the user provides interaction data with probability r. That is, as we run the simulation, we flip a biased coin and observe each judgment with only probability r. In the prospective notification case, we argue that this is not unrealistic--the user "pays attention" to the notification message with probability r; the rest of the time, the user ignores the update.
Figure 6 shows the results of these experiments (averaged over three trials). On the x axis we sweep across r, the interaction probability and on the y axis we plot the simulation accuracy. The left plot shows the results for retrospective summarization using unweighted recall and the (official, official) condition. In the middle plot, we show accuracy against ELG for prospective notification and against nCG on the right (once again, discarding quiet days in both cases). Experiments show that we are able to accurately decide the relative effectiveness of the comparison systems even with limited user interactions.
The next obvious question, of course, is what if we combined both the length analysis and interaction probabil-

183

Retrospective Summarization: Unweighted Recall

100

100

Prospective Notification: ELG

Prospective Notification: nCG 100

80

80

80

Simulation accuracy Simulation accuracy Simulation accuracy

60

60

60

40

p = 0.2

p = 0.4

20

p = 0.6

p = 0.8

p = 1.0

0

0

20

40

60

80

100

Interaction probability r

40

p = 0.2

p = 0.4

20

p = 0.6

p = 0.8

p = 1.0

0

0

20

40

60

80

100

Interaction probability r

40

p = 0.2

p = 0.4

20

p = 0.6

p = 0.8

p = 1.0

0

0

20

40

60

80

100

Interaction probability r

Figure 7: Simulation accuracy combining both retention probability p and interaction probability r.

ity analysis? These results are shown in Figure 7, organized in the same manner as the other graphs (also averaged over three trials). For clarity, we only show results for all pairs. The interaction probability r is plotted on the x axis, with lines representing retention probability p = {0.2, 0.4, 0.6, 0.8, 1.0}. As expected, we are able to achieve good accuracy, even while randomly discarding system output, and with limited interactions. Given these tradeoff curves, an experiment designer can strike the desired balance between accuracy and verbosity.
6. CONCLUSIONS
In this paper, we describe and validate a novel interleaved evaluation methodology for two complementary information seeking tasks on document streams: retrospective summarization and prospective notification. We present a temporal interleaving strategy and a heuristic credit assignment method based on a user interaction model with explicit judgments. Simulations on TREC data demonstrate that our evaluation methodology yields high fidelity comparisons of the relative effectiveness of different systems, compared to the results of batch evaluations.
Although interleaved evaluations for web search are routinely deployed in production environments, we believe that our work is novel in that it tackles two completely different information seeking scenarios. Retrospective summarization and prospective notification are becoming increasingly important as users continue the shift from desktops to mobile devices for information seeking. There remains much more work, starting with a better understanding of user interactions so that we can develop models of implicit judgment and thereby greatly expand the scope of our evaluations, but this paper takes an important first step.
7. ACKNOWLEDGMENTS
This work was supported in part by the U.S. National Science Foundation (NSF) under awards IIS-1218043 and CNS-1405688 and the Natural Sciences and Engineering Research Council of Canada (NSERC). All views expressed here are solely those of the authors. We'd like to thank Charlie Clarke and Luchen Tan for helpful discussions.
8. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. SIGIR, 2006.
[2] J. Aslam, M. Ekstrand-Abueg, V. Pavlu, F. Diaz, R. McCreadie, and T. Sakai. TREC 2014 Temporal Summarization Track overview. TREC, 2014.

[3] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. Large-scale validation and analysis of interleaved search evaluation. ACM TOIS, 30(1):Article 6, 2012.
[4] O. Chapelle and Y. Zhang. A Dynamic Bayesian Network click model for web search ranking. WWW, 2009.
[5] L. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of user behavior in WWW search. SIGIR, 2004.
[6] J. He, C. Zhai, and X. Li. Evaluation of methods for relative comparison of retrieval systems based on clickthroughs. CIKM, 2009.
[7] K. Hofmann, S. Whiteson, and M. de Rijke. A probabilistic method for inferring preferences from clicks. CIKM, 2011.
[8] T. Joachims. Optimizing search engines using clickthrough data. KDD, 2002.
[9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM TOIS, 25(2):1­27, 2007.
[10] D. Kelly. Understanding implicit feedback and document preference: A naturalistic user study. SIGIR Forum, 38(1):77­77, 2004.
[11] D. Kelly and J. Teevan. Implicit feedback for inferring user preference: A bibliography. SIGIR Forum, 37(2):18­28, 2003.
[12] R. Kohavi, R. M. Henne, and D. Sommerfield. Practical guide to controlled experiments on the web: Listen to your customers not to the HiPPO. KDD, 2007.
[13] J. Lin, M. Efron, Y. Wang, and G. Sherman. Overview of the TREC-2014 Microblog Track. TREC, 2014.
[14] J. Lin, M. Efron, Y. Wang, G. Sherman, and E. Voorhees. Overview of the TREC-2015 Microblog Track. TREC, 2015.
[15] F. Radlinski and N. Craswell. Comparing the sensitivity of information retrieval metrics. SIGIR, 2010.
[16] F. Radlinski and N. Craswell. Optimized interleaving for online retrieval evaluation. WSDM, 2013.
[17] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? CIKM, 2008.
[18] T. Sakai, N. Craswell, R. Song, S. Robertson, Z. Dou, and C. Lin. Simple evaluation metrics for diversified search results. EVIA, 2010.
[19] A. Schuth, K. Hofmann, and F. Radlinski. Predicting search satisfaction metrics with interleaved comparisons. SIGIR, 2015.
[20] A. Schuth, F. Sietsma, S. Whiteson, D. Lefortier, and M. de Rijke. Multileaved comparisons for fast online evaluation. CIKM, 2014.
[21] L. Tan, A. Roegiest, J. Lin, and C. L. A. Clarke. An exploration of evaluation metrics for mobile push notifications. SIGIR, 2016.
[22] Y. Wang, G. Sherman, J. Lin, and M. Efron. Assessor differences and user preferences in tweet timeline generation. SIGIR, 2015.
[23] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. SIGIR, 2003.

184

That's Not My Question: Learning to Weight Unmatched Terms in CQA Vertical Search

Boaz Petersil
Dep. of Electrical Eng. Technion, Haifa, Israel boaz.petersil@gmail.com

Avihai Mejer
Yahoo Research Haifa, 31905, Israel amejer@yahoo-inc.com

Idan Szpektor
Yahoo Research Haifa, 31905, Israel idan@yahoo-inc.com

Koby Crammer
Dep. of Electrical Eng. Technion, Haifa, Israel koby@ee.technion.ac.il

ABSTRACT
A fundamental task in Information Retrieval (IR) is term weighting. Early IR theory considered both the presence or absence of all terms in the lexicon for ranking and needed to weight them all. Yet, as the size of lexicons grew and models became too complex, common weighting models preferred to aggregate only the weights of the query terms that are matched in candidate documents. Thus, unmatched term contribution in these models is only considered indirectly, such as in probability smoothing with corpus distribution, or in weight normalization by document length. In this work we propose a novel term weighting model that directly assesses the weights of unmatched terms, and show its benefits. Specifically, we propose a Learning To Rank framework, in which features corresponding to matched terms are also "mirrored" in similar features that account only for unmatched terms. The relative importance of each feature is learned via a click-through query log. As a test case, we consider vertical search in Community-based Question Answering (CQA) sites from Web queries. Queries that result in viewing CQA content often contain fine grained information needs and benefit more from unmatched term weighting. We assess our model both via manual evaluation and via automatic evaluation over a clickthrough log. Our results show consistent improvement in retrieval when unmatched information is taken into account. This holds both when only identical terms are considered matched, and when related terms are matched via distributional similarity.
Keywords: Unmatched Terms; Document Ranking; Communitybased Question Answering
1. INTRODUCTION
One of the fundamental tasks in Information Retrieval (IR) is term weighting, which refers to the assessment of a weight for each term appearing in the document collection, and similarly in the input query. Early Probabilistic IR theories considered the presence or absence of all terms in the lexicon for ranking, both in the query and the documents
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
© 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911496

[29, 26]. However, term weights in these models were found difficult to compute, and the main line of research around weighting models chose to consider mainly the weights of the query terms that are matched in candidate documents. Indeed, weighting schemes such as TF-IDF [31], BM25 [28] and statistical language models [33, 24, 40], as well as Learning To Rank (LTR) methods [20], are primarily based on considering the contribution of the matched terms, those query terms that appear also in the document. Though unmatched terms, i.e. terms that appear only in the query or only in the document, are not completely ignored, they are considered indirectly in these models, such as by using the document length for weight normalization or via corpus-based smoothing of maximum likelihood estimations.
As suggested by early probabilistic models we argue that analyzing directly unmatched terms may provide additional cues to the relevance of a candidate document to the query. Indeed, while the contribution of stop-words, such as determiners and modals, can be largely ignored, unmatched named entities are strong indicators of semantic differences between the query and the document. For example, for the query "most deadliest snake", the document title "where can I find a list of the deadliest snakes" is more relevant than "which is the most deadliest snake in Russia", though the first title is longer than the second, and second title contains all of the query terms in the right order.
Another intuition regarding direct modeling of unmatched terms refers to the percentage of query terms that are covered in the document. We would like to explicitly indicate that for two queries, a short one and a long one, if both match the same set of terms within a candidate document, this document is likely to be of less relevance for the longer query, which contains more unmatched terms, compared to the shorter one. As an example consider the queries "most deadliest snake" and "most deadliest snake in Russia" and the candidate document "where can I find a list of the deadliest snakes". We would like to explicitly express the lack of relevance of the document to the second query due to unmatched query terms.
We expect the subtleties between different types of unmatched terms to show especially for Web queries with finegrained information-need. Therefore, we focus in this paper on Web queries with question intent, which constitute 10% of the Web queries issued to a search engine [36]. Examples for such Web queries are those resulting in the searcher clicking on a question page belonging to Community-based Question Answering (CQA) sites, such as Yahoo Answers, StackExchange and Quora, and are called here CQA queries.

225

Our retrieval scenario is vertical search [25, 2], in which content of a CQA sub-collection should be retrieved on top of general Web search.
In this work, we address this vertical search task by introducing a term weighting model that directly considers the contribution of unmatched terms for ranking. However, instead of a probabilistic framework, we utilize LTR as a ranking framework. To this end, we employ a large set of state-of-the-art features, which capture various attributes of matched terms, both statistical ones (such as variants of TF-IDF) as well as syntactic ones (such as Part-Of-Speech (POS) tags) [20, 9]. We then design "mirror" features that evaluate similar attributes, but for the unmatched terms. These novel mirror features are provided, together with the features that correspond to matched terms, as input to an LTR algorithm, which learns the relative weights of the different features using click-through training data.
Prior work in document ranking noted that, occasionally, different terms in the query and the document actually convey related meanings or even the same meaning (e.g. `guy' ­ `man', `drink ' ­ `alcohol ') and should be considered as matching for document ranking. One common approach to handle this lexical gap is via translation models, which include some similarity measure between query and document terms as part of term matching [4, 17, 39, 14]. In order to analyze the contribution of our unmatched term modeling under such "soft matching" schemes, we introduce a "soft" variant of all the features we compute for matched terms in our LTR framework, which is based on distributional similarity between terms. We then provide a similar soft variant of our unmatched-term features that complement the soft matched-term features. This should enable the unmatchbased features to better account for only semantically unmatched terms instead of terms that have similar meanings.
We conducted experiments on a vertical search setting that searches a Web query over a large collection of question pages from Yahoo Answers. The contribution of our unmatch-based features for term weighting was evaluated under two setups: a) large-scale automatic evaluation over a click-through query log; b) manual evaluation of the top retrieved documents for a set of tested queries. We compared our model to a state-of-the-art LTR model that utilizes only features that correspond to matched terms. The tested models were assessed both under the exact matching modeling, in which only identical terms are considered matched, and the soft matching modeling, where terms may be partially matched via distributional similarity. Our novel features provided consistent improvement in document ranking on both scenarios, showing the benefit of directly considering unmatched terms for term weighting.
2. RELATED WORK
Unmatched terms were addressed in prior IR ranking models in different ways, both for general search and for CQA search. We distinguish between two types of unmatched terms: a) terms that appear in the candidate document but not in the query, denoted as excessive terms; b) terms that appear in the query but not in the candidate document for ranking, denoted as missing terms.
Probabilistic information retrieval theory accounts for presence or absence of all terms in the lexicon, both in the query and in a candidate document for ranking [29]. Similarly, early reformulation of language models for IR (LMIR) [26]

considered the query as a set of words, and modeled excessive terms in the document by their ability to generate terms not in the query. However, term weighting computation becomes a difficult problem under these frameworks [30, 33, 40], especially when no relevance feedback is considered. Therefore, recent ranking models, and specifically term weighting models, focus mainly on the matched terms between the query and a candidate document.
The overall ranking score of a document is typically the sum of the weights of the terms in the document that match (to some extent) the query terms. Therefore, document term weights in popular weighting schemes are non-negative and the effect of missing terms in a candidate document is considered indirectly by not contributing their weights to the document ranking score. This is the case in common probabilistic and vector space models, such as Binary Independence Model (BIM) [30], TF-IDF [31], Okapi BM25 [28], divergence from randomness [1], and multinomial language models, which view the query as a sequence of terms [33, 24, 41, 40]. Specifically, language models were extensively explored for CQA retrieval and were extended in different ways to incorporate meta data like categories [7], and the question focus and topic [13]. The same principle of scoring documents by summing matching term weights is also behind different weighting terms at the query side [3, 43].
In another line of ranking research, Learning to rank (LTR) approaches [20, 19] were introduced for learning to combine many features in a supervised way. Various learning algorithms were proposed, such as SVMRank [8] and LambdaMart [38], which may assign negative weights to some features. Still, the features themselves are typically derived for the matched terms, and therefore LTR algorithms learn the relative contribution of each feature with respect to the matched terms. Most derived features are statistical in nature, such as variants of term frequency and document frequency scores [20], and were also utilized in supervised ranking models in CQA [18, 37]. Carmel et al. [9] showed that utilizing features derived from syntactic analysis of the document title improves ranking performance for CQA queries. In a related task of answer sentence ranking within the field of Question Answering, tree kernels that incorporate semantic and syntactic features of the words provide state-of-theart performance [34]. Still, the overall approach weigh in the number of matched sub-trees but not the unmatched ones.
Missing terms, which appear in the query but not in the document, received considerable attention within attempts to address the lexical gap problem: improving the matching between query and document terms that are not lexically identical but convey similar meaning. One common approach incorporates a translation model as part of term weighting [4, 17]. This approach was found useful also for retrieving CQA content, where translation models were used within LMIR for retrieving related questions [16, 39, 42] as well as for ranking CQA documents for Web queries [37]. Recently, lexical semantic similarity between terms via distributed representations, such as word2vec [23], was found helpful in several IR tasks, including query term weighting [43] and as features in a LTR framework for answer retrieval [10]. Ganguly et al [14] employed similarity between word embedding vectors within a translation model for LMIR as means to overcome the lexical gap between queries and documents, where it outperformed a language model extended with latent topics.

226

Figure 2: POS tagging and dependency parse tree for the question Can someone suggest fun party games?. The upper label of each token is its POS tag and the lower label is its syntactic role.

Figure 1: Sum of IDF values (normalized) for the matched, excessive and missing terms, computed separately over all non-clicked documents ranked at positions 1 to 20 (the three plots) and over the clicked documents (three horizontal lines).
Prior work captured the effect of excessive terms (appearing only in the document) on the ranking score mainly by their contribution to overall document context or structure. Many vector space and probabilistic models (e.g. TF-IDF, BM25, language models) utilize the document's length as a degrading parameter for term weights, e.g. as the denominator of maximum likelihood estimation or in 2 vector normalization. Other models include all document terms when modeling a global latent representation for each document. One line of works uses latent topics (e.g. LDA [5]) as additional smoothing elements within LMIR [35]. This extension was shown useful also in retrieval of related CQA questions [6]. Another approach is to embed the document in a latent space. Latent Semantic Indexing [12] utilizes SVD to represent documents and queries within a reduced dimension space based on the main singular values of a term/document co-occurrence matrix. Lately, deep learning was shown useful for ranking by embedding the query and document texts into a shared latent space, within Web search [15] and within Question Answering [32]. Such approaches were not evaluated under the CQA vertical search setting yet, whose query length distribution and query attributes is quite distinguishable from general Web search and from question/answer datasets [9, 36].
3. YAHOO ANSWERS DATASET
In this work we perform our analysis and experiments on a large document collection taken from Yahoo Answers. Yahoo Answers is a popular CQA website containing questions about diverse topics, such as sports, healthcare, politics, science and many others. Each question page in the site consist of: a) a title, which is typically a short summary of the question, b) a body, containing a detailed description of the question, and c) all the answers provided for this question. We collected 54 million question pages from Yahoo Answers (referred to as our corpus) and indexed them using Lucene1.
We also randomly sampled Web queries that were issued to a popular search engine and resulted in a click on one of the pages in our corpus by analyzing the search log. For each sampled query the top 100 results from our corpus were re-
1lucene.apache.org

trieved using Lucene's BM25 ranking function over all fields (title, body, answers). We retained the set of queries for which the clicked page for the query (as extracted from the search log) was found among the top 100 Lucene results. After this process, our click-based query collection consists of 136,000 queries.
Since search-engines show mostly the title of a question page on the search result page, Carmel et al. [9] reason that the relevance of the title to the query is one of the main reasons for a user to click on the page. This especially makes sense as the title is often a good summary of the question in the page. Furthermore, both title and query are concise and usually do not contain redundant information. Therefore, we expect that unmatched term weighting would help in retrieval under this scenario. Following them [9], we analyze and model unmatched terms only between the title of a Yahoo Answers question page and the target query.
4. UNMATCHED TERM ANALYSIS
To further motivate our modeling approach we analyze the properties of the unmatched terms: terms that appear in the candidate document but not in the query (excessive), and terms that appear in the query but not in the candidate document (missing). To this end, we sampled 20,000 queries2 from our query collection and examined the matched and unmatched terms between each query and the titles of the retrieved documents (using Lucene). We compared the analysis statistics between documents that were clicked by the user who issued the query, denoted as clicked documents, and the other top retrieved documents, denoted as non-clicked documents.
4.1 IDF Distribution Analysis
First we examined the distribution of IDF values among the matched and unmatched terms. To this end, we computed the sum of IDF values for the matched terms and the excessive terms in each title (normalized by the title length). We also computed the IDF sum for the query's missing terms (normalized by the query length). These three indicators are plotted in Fig. 1, where each point in the plot represents the value averaged over all non-clicked titles ranked at the i'th position by the BM25 ranking, for i=1..20. The three horizontal lines in Fig. 1 correspond to the values of the three indicators averaged over the clicked documents. Note, higher values intuitively reflect more relevance in the matched term curve but less relevance in excessive and missing term statistics.
2Taken from our training set ­ see Sec. 5.4

227

Figure 3: Probability of a term in the title not to match any query term given its POS tag (top chart) or syntactic role (bottom chart)
The matching-term IDF values (the diamond shape points) indicate that the clicked document (the horizontal line) is comparable, on average, only to the document at the 4 th position. This means that, with respect to matched terms, top ranked non-clicked documents usually contain as much and even more IDF volume compared to the clicked document. Yet, excessive and missing term analyses reveal complementing phenomena. First, on average, only non-clicked documents that are ranked first have the excessive term indicator lower than the value for clicked documents. This may indicate that while several non-clicked documents contain "important" (high-IDF) query terms in their title, driving them to high ranking positions, they also contain additional "important" terms that do not appear in the query and may change the meaning of the title compared to the query.
Second, the indicator for missing terms in clicked documents stands out even more, as it is lower compared to all non-clicked documents. This could indicate that in CQA queries it takes more than one term to express the gist of the information needed. While some non-clicked documents may include in their title high-IDF query terms, which correspond to "important" terms, they also tend to leave-out more "important" terms compared to the clicked document.
Assuming that click analysis is a useful approximation of relevance analysis, these results suggest that matched term statistics reveal only some aspects of the title's relevance to a searcher's information need. More relevance aspects may be further exposed by explicitly modeling unmatched terms.
4.2 Syntactic Analysis of Excessive Terms
Carmel et al. [9] showed that document terms with different syntactic properties should be weighted differently for retrieval. Hence, we examine similar syntactic properties of excessive terms, namely POS tags and dependency roles.
To this end, all titles in our corpus were syntactically analyzed using the Stanford parser3 under the "all typed dependencies" setting. Then, for each title term in a retrieved document we extracted its POS tag and syntactic role (the
3http://nlp.stanford.edu/software/lex-parser.shtml

dependency relation in which the term is the dependent). Fig. 2 presents an example for this analysis. Finally, for each syntactic property we counted its total occurrences in each title and its occurrences within excessive terms, and computed their ratio. The ratio of these two counts represents the probability of each syntactic property to be an excessive term. In Fig. 3 we depict two probability families, averaged across all analyzed queries: a) for all clicked documents; and b) for the three highest ranking non-clicked documents (representing the "toughest" competitors to beat for ranking the clicked documents on top of non-clicked ones). For clarity, the charts contain only the results for the 15 most common POS tags and syntactic roles, and they are sorted in decreasing order of the probability value.
Looking at Fig. 3 we observe large differences in excessive term probability between different syntactic tags. For example, in clicked items, this probability for pronouns (PRP usually a low IDF stopword) is 0.75, while for proper nouns (NNP) it is only 0.35. Such large differences echoes previous observation [9] that there is a possible gain in modeling differently terms with different syntactic tags.
Comparing the statistics between clicked titles and the top non-clicked titles, we can see that in quite a few properties there are distinguishable differences between clicked and non-clicked titles. These differences suggest that excessive term weighting may improve if syntactic properties will be considered. As an example we look at verbs, which are important terms in a query (usually capturing the main activity asked about). The syntactic roles that are often associated with verbs in the bottom chart are `aux ', `cop', `root' and sometimes `conj '. These roles can be partitioned into two groups. The first group contains `aux ' and `cop', whose excessive probability is higher in non-clicked titles. The second group contains `root' and `conj ', which are more likely to be excessive in clicked titles. This shows that syntactic properties can provide more fine-grained distinctions between similar terms or even for the same term when assuming different roles.
As another example, the charts in Fig. 3 also show that nouns (POS tags: NN*, Dep roles: conj, nsubj, dobj, pobj, nn) are more likely to be excessive in non-clicked documents. As nouns typically contain the main participants of a question, it is important to match all (or most) of them to align the exact semantics of the query to that of a title. Therefore, directly assessing both matched and unmatched nouns could improve retrieval. In Fig. 3 the only reverse case is with `conj ', which is more likely to be excessive in clicked titles. Yet, conjunctive nouns, such as in the example "good websites that stream movies and tv shows", may be skipped (and become excessive terms) while maintaining the same semantic gist of the question.
The analysis in this section suggests that modeling statistical properties as well as syntactic properties of unmatched title terms may lead to better assessment of document relevance for CQA queries. We next explicitly construct features for unmatched terms, within a Learning To Rank (LTR) framework, which take these properties into consideration.
5. MODELING UNMATCHED TERMS
The task of our ranking algorithm is to rank a set of candidate documents D given a CQA query q. We follow a standard LTR scheme [19, 20, 9] and define a mapping function

228

Feature L1 L2 L3
L4
L5
L6
L7
L8
L9
L10
G1(p) G2(p) G3(p) G4(p) G5(sr) G6(sr) H1 H2 H3

Formulation

tq c(t, d) × (t  d)

tq log(c(t, d) + 1) × (t  d)

tq

c(t,d) |d|

× (t



d)

tq log

c(t,d) |d|

+1

× (t  d)

tq log

|C| df (t)

× (t  d)

tq log

log(

|C| df (t)

)

× (t  d)

tq log

|C| c(t,C)

+

1

× (t  d)

tq log

c(t,d) |d|

log(

|C| df (t)

)

+

1

× (t  d)

tq c(t, d) log

|C| df (t)

× (t  d)

tq log

c(t,d) |d|

|C| c(t,C)

+

1

× (t  d)

tdP OS(t)=p c(t, q) tdP OS(t)=p idf (t) × c(t, q) tdCP OS(t)=p c(t, q) tdCP OS(t)=p idf (t) × c(t, q) tdSR(t)=sr c(t, q) tdSR(t)=sr idf (t) × c(t, q)
BM25 score log(BM25 score) LMIR with Dirichlet smoothing parameter 

Table 1: Matched term features used in [9]; c(t, X) ­

term frequency of t in X; df (t) ­ document frequency

of t in our corpus C; idf (t) = log

|C| df (t)

; |X| ­ total

number of terms in X; P OS(t), CP OS(t) and SR(t)

are the POS tag, coarse-POS tag and syntactic role

of t respectively; () is the indicator function.

(q, d)



n
R

from

pairs

of

a

query

q

and

a

candidate

docu-

ment d to the vector space Rn. Our algorithm uses a weight

vector w to compute a score for each d  D via the inner

product s(q, d) = w · (q, d). Finally, the candidate docu-

ments are ranked according to the value of s(q, d), where the

higher the score for some document d, the higher its rank in

the retrieved list. The goal of the learning algorithm is to

find weights w such that more relevant documents will have

high score compared to less relevant ones.

Our work focuses in the design of a new feature mapping

(q, d) that captures both missing and excessive terms. We

build on the work of Carmel et al [9] who proposed only fea-

tures that consider matched terms and extend their mapping

in two ways: (1) taking into account unmatched terms; (2)

relaxing the notion of matched/unmatched terms, and al-

lowing soft-matching between terms in the query and the

candidate, based on distributional similarity.

We describe in Sec. 5.1 the state-of-the-art features pro-

posed by Carmel et al [9], which are our baseline and starting

point. Additionally, we present in Sec. 5.2 an abstraction of

these features having in mind our goal to introduce their

corresponding new features for unmatched terms. Finally,

in Sec. 5.3, we incorporate soft-matching into all features

presented until then. We use a previous weight learning

scheme [9] (Sec. 5.4) in order to replicate their work as a

baseline and have a fair comparison of our new features.

5.1 Matched Term Features
Carmel et al [9] also addressed the task of vertical search for CQA queries within an LTR framework (see Sec. 2). They proposed two types of features that analyze matched terms: standard statistical features [20], such as TF-IDF, and new syntactic-based features that collect matched term statistics for each POS tag and syntactic role separately. All these features are summarized in Tab. 1.
Carmel et al mainly analyzed the performance of the document title for matching the query, arguing that in CQA content, the title is a good summary of the question being answered within the document. Therefore, all statistics are derived only from the document's title, except for the BM25-related features (H1-2) which are computed over the whole document. Under this formulation, which we follow, q and d represent the list of terms in the query and the document's title respectively. Features L1-10 and H1-3 refer to standard statistical features, while G1-4(p) and G5-6(sr) are feature families that are generated for each POS tag p and syntactic role sr, respectively. For example G1(IN ) is the feature generated for the IN (preposition) POS tag and G5(root) is the feature generated for the root syntactic role.
Each of the features L1-10 and G1-6 can be viewed more abstractly as (possibly conditional) term summing of a product of two terms: (1) a count (or a function of it) of some event, denoted by fFi ; and (2) a boolean predicate or a numeric value indicating some matching between q and d:

Li(q, d) =
tq

fLi (t, d) × (t  d)

(1)

Gi(q, d) =

fGi (t, d) × c(t, q)

(2)

tdcondGi (t)

The matching indication part in Li is (t  d) ­ whether the query term appears in the document. The indication part of Gi is c(t, q) ­ the occurrence count of the title term in the query, which is 0 for unmatched. Examples for instantiating these abstractions with specific statistics are: a) for L2, fL2 := log(c(t, d) + 1); and b) for G1, fG1 := 1 and condG1 := (P OS(t) = p).

5.2 Unmatched Term Features
We now introduce our novel features, which induce the fFi signals in parallel to their counterpart matched term features L1-10 and G1-6. Yet, instead for the matched terms, the new feature families do so for the set of excessive terms, denoted by EXL and EXG, and for the set of missing terms, denoted by M IL and M IG. We present the generic representations of these feature families similarly to (1) and (2):

EXLi(q, d) =

tu(d)

fLi (t, d) × (1 - (t  q))

EXGi(q, d) =

fGi (t, d) × (1 - (t  q))

tdcondGi (t)

and,

M ILi(q, d) =

tu(q)

fLi (t, q) × (1 - (t  d))

M IGi(q, d) =

fGi (t, q) × (1 - (t  d))

tqcondGi (t)

229

Figure 4: Percentage of queries by length
The differences between these new feature families and their matched term counterparts are in: a) the matching indicators, which turn into unmatching indicators; and b) the term sets over which the summation is performed. For example, the unmatching indicator in the excessive feature family EXLi is (1 - (t  q)), which is 1 only if the document term is not in the query. In addition, the summation in EXLi is over u(d), which stands for the set of unique4 terms in d from which we pick the excessive terms.
We note that for M IGi, syntactic analysis of the query is required. We used the Stanford parser for query parsing as well, but found that the query dependency trees were of low quality. Therefore, in our experiments we only use the POS tags for queries, and hence only features M IG1-4, leaving the reliable generation of features M IG5-6 for future work.
5.3 Soft Matching Formulation
As discussed in Sec. 2, some mismatches in exact matching scheme should actually be accounted as (at least partial) matches, such as in the case of synonyms or related words, e.g. `guitar ' and `riff '. We extend our proposed features and present a novel soft matching formulation of all our matched and unmatched features (except BM25-related H1-2). To the best of our knowledge, this is the first formulation in the context of the standard set of LETOR features [20].
We start with a lexical similarity function sim(tq, td)  [0, 1] between a query term tq and a document term td. The closer the function's value is to 1 the more similar the two terms are. We follow recent successes with word embedding similarity and use in this work:
sim(tq, td) := max(cos(sg(tq), sg(dq)), 0) ,
where sg(t) is the word embedding vector of term t learned by the SkipGram algorithm [23]. We define sim(t, t) = 1 for every word similarity with itself and sim(t, u) = 0 if t = u and either t or u are not in the lexicon.
To incorporate the similarity score sim(tq, td) into our features we find the best matching counterpart term for each query term and for each document term:
bmd(tq) = arg max sim(tq, td)
td d
bmq(td) = arg max sim(tq, td)
tq q
s(t, d) = sim(t, bmd(t))
s(q, t) = sim(bmq(t), t) ,
4Term repetition is avoided since the number of occurrences of the term t in d is already counted in fLi .

where s(t, d) and s(q, t) are soft indicator functions that capture how well a query (document) term is matched against the document (query) via its similarity score with its best match. We note that if sim() would only return 1 for exact match and 0 otherwise, s() would become ().
Finally, we extend our features using bm() and s():

Lsi (q, d) =
tq

fLi (bmd(t), d) × s(t, d)

Gsi (q, d) =

fGi (t, d) × c(bmq(t), q) × s(q, t)

tdcondGi (t)

EXLsi (q, d) =

fLi (t, d) × (1 - s(q, t))

tu(d)

EXGsi (q, d) =

fGi (t, d) × (1 - s(q, t))

tdcondGi (t)

M ILsi (q, d) =

fLi (t, q) × (1 - s(t, d))

tu(q)

M IGsi (q, d) =

fGi (t, q) × (1 - s(t, d)) ,

tqcondGi (t)

where we simply replace (or augment where necessary) the indicator function with the soft indicator variant, and instead of gathering statistics from exact match occurrences, we gather them from the occurrences of the best-match. If a query term appears as-is in the document (exact match), our feature scores are exactly as for exact matching. Yet, when a query term does not exactly appear in the document (or vice versa) instead of returning a matched feature value of zero, we resort to counting with respect to its best soft match instead. We note that a similar formulation using best-matches is utilized by Liu et al [21] for computing similarity between short documents.

5.3.1 Language Model with Soft Matching
Prior work showed that extending LMIR with some similarity notion between terms improves retrieval results [39, 14]. We therefore extend our language model feature H3 in a similar way, following the formalism of Xue et al [39]:

H3s(q, d) = log(P (t|d))
tq

|d|

c (t, d)



P (t|d) = |d|+

(1 - )

|d|

+ Ptt (t|d) + |d|+ Pc (t)

Ptt(t|d) =

sim(t, td) × c (td, d) Z(t, d) =

Z(t, d)

|d|

sim(t, td) ,

td d

td d

where q is the query term list; d is the document term list
and |d| is its length; c(t, d) is the term-frequency of t in d;
Pc(t) is maximum likelihood estimation (MLE) of t in our corpus; Z is a probability normalizer; and  and  are hyper
parameters to be tuned. We note that H3s is a variant of Xue et al's language model.
Instead of using a translation table as Ptt, we followed Ganguly et al [14], who suggested a variant of Ptt based on a similarity function sim() (normalized into a probability distribution). Note, when sim() represents exact matching, H3s becomes H3.

230

5.4 Model Weight Learning
We learned the weights w of our ranking algorithm in a semi-supervised manner based on clickthrough data. The goal of the learning algorithm is to find a vector w such that for each query in the training data, the corresponding clicked document will be ranked as high as possible. We used the online variant of SVMRank [8] with the AROW update [11] as done before [9]. Specifically, for each training query the algorithm first re-ranks the top 100 documents retrieved by Lucene using the currently learned ranker. Then, it selects the top K ranking documents, excluding the clicked document. The algorithm then updates w such as for this query the clicked document would increase its ranking score compared to the selected K documents.
We split our query collection (see Sec. 3) into a 61,000 query training-set, a 14,000 query validation-set and a 61,000 query test-set. The validation-set was used to tune the various hyper-parameters for each tested model separately, namely, the number of training rounds, the value of K, and the AROW hyper-parameter r. The only hyper parameters that were tuned once for all models are  = 1,  = 0.5 for the H3 and H3s LMIR features. Specifically,  was tuned on the LETOR model and  was then tuned on a soft version of the LETOR. See Sec. 6.1 for details on the configuration of each tested model. Finally, to compute term similarity we used publicly available5 pre-trained word embedding vectors.
6. EXPERIMENTS
We evaluated our proposed models against several baselines via two settings: first, based on a large scale clickthrough data, and second, based on manual judgments.
6.1 Tested Models
We consider six baseline models:
· LSI: Latent Semantic Indexing [12], where ranking score is computed as the dot product between the LSI representations of the query and the document title. We utilized the top 200 dimensions of the SVD decomposition of our corpus6 as the latent space.
· BIM: Binary Independence Model [22] with unmatched probability estimation using pseudo relevance, taking the 'Relevant set' as 1, 3 or 5 top ranking documents by BM25.
· BM25: Using only the relevance score as provided by Lucene (feature H1 in Tab. 1).
· LETOR: Using only statistical features associated with matched terms (features L1-10 and H1-3 in Tab. 1).
· Matched: Using all the features associated with matched terms (all features in Tab. 1).
· SoftMatched: Using soft-matching formulation for the matched features, i.e. feature families Lsi , Gsi and feature H3s (as in Sec. 5.3), and H1-2 from Tab. 1.
We compare the baselines to our proposed models:
· Full: Combining unmatched and matched features under exact matching formulation, i.e. all features in Tab. 1 as well as feature families EXLi, EXGi,M ILi and M IGi (described in Sec. 5.2).
5https://code.google.com/p/word2vec/ 6Using RedSVD: http://code.google.com/p/redsvd/

· SoftFull: Combining unmatched and matched features under soft-matching formulation, i.e. feature families Lsi , Gsi , EXLsi , EXGsi , M ILsi , M IGsi and feature H3s (described in Sec. 5.3), and features H1-2 in Tab. 1.
We trained separately each of the LTR-based models using the algorithm in Sec. 5.4.
6.2 Automatic Evaluation
We conducted a large scale automatic evaluation using our 61,000 query test-set (see Sec. 5.4). For each query we retrieved the top 100 results from the document collection using Lucene, and then re-ranked the top results using each of the tested models. We report Mean Reciprocal Rank (MRR) and Binary Recall at position K (R@K), all derived from the rank position of the clicked document associated with each query. Fig. 4 depicts the query length distribution of our test-set. We remind the reader that CQA queries are usually longer than typical Web queries.
6.3 Manual Evaluation
We randomly sampled 1,000 queries of length 3 or more words from our test-set (shorter queries are scarce in our query collection - see Fig. 4). For each query we collected the top 10 documents as ranked by each of the tested models. Professional editors assessed the relevance of each document in the pool on three Likert-scale levels: (1) non-relevant, (2) partially-relevant, and (3) highly-relevant. Inspecting the evaluations, we found that usually only highly-relevant documents refer to relevant content. Hence, we report NDCG with weights of 0 for non-relevant, 1 for partially-relevant and 10 for highly-relevant. We also report Precision at K (P@K) taking only highly-relevant documents as relevant.
7. RESULTS
The results for the automatic and manual evaluations are summarized in Tab. 2 and Tab. 3, respectively. All statistical significance figures are computed using t-test. The results in both tables indicate a trend similar to the one reported by Cramel et el [9]. Namely, LETOR outperforms BM25 by a large margin (e.g. 10.5% increase in MRR) and adding syntactic features (Gi) on top of statistical features (Li, Hi) in the Matched model consistently provides additional improvement, e.g. 1.5% increase in MRR across all query lengths (Fig. 5). We thus refer to Matched as our main baseline. We note in passing that the performance of the LSI and BIM models was significantly lower than the LTR models (e.g. MRR of 0.202 for LSI, 0.164 for BIM) and adding them as additional features did not help either. We therefore excluded their performance report.
We next observe that adding soft term-matching to address the lexical gap between queries and documents (SoftMatched model) shows a nice improvement under the clickthrough automatic evaluation. For example, MRR is increased by 2.1% compared to Matched. In addition, manual evaluation also shows some improvement using soft matching, specifically at high rank positions. For example, P@3 is increased by 2.9% compared to Matched. On the other hand, the results for P@5 and P@10 are comparable to Matched. Analyzing our soft matching model, we found quite a few queries where exact matching provided better ranking than soft matching. For example, under the automatic evaluation setting, SoftMatched ranked the clicked

231

Model
BM25 LETOR Matched Full SoftMatched SoftFull

MRR

0.465 0.514 0.522 0.537 0.533 0.543

(-10.9%) (-1.5%)
(2.9%) (2.1%) (4.0%)

R@1

0.339 0.386 0.391 0.405 0.401 0.411

(-13.3%) (-1.3%)
(3.6%) (2.6%) (5.1%)

R@3

0.525 0.581 0.596 0.609 0.604 0.616

(-11.9%) (-2.5%)
(2.2%) (1.3%) (3.4%)

R@5

0.605 0.663 0.680 0.692 0.690 0.700

(-11.0%) (-2.5%)
(1.8%) (1.5%) (2.9%)

R@10

0.709 0.763 0.783 0.792 0.790 0.800

(-9.5%) (-2.6%)
(1.1%) (0.9%) (2.2%)

Table 2: Automatic evaluation results (and percentage of change compared to Matched model). All differences are statistically significant at p < 0.001.

Model
BM25 LETOR Matched Full SoftMatched SoftFull

NDCG

0.685 0.711 0.713 0.714

(-3.9%) (-0.3%)
(0.1%)

0.716 (0.4%)

0.719 (0.8%)

P@1

0.522 0.562 0.567 0.567

(-7.9%) (-0.9%)
(0.0%)

0.575 (1.4%)

0.577 (1.8%)

P@3

0.396 0.417 0.419 0.423 0.431 0.431

(-5.5%) (-0.5%)
(1.0%) (2.9%) (2.9%)

P@5

0.336 0.357 0.365 0.366

(-7.9%) (-2.2%)
(0.3%)

0.366 (0.3%)

0.369 (1.1%)

P@10

0.268 0.280 0.284 0.287

(-5.6%) (-1.4%)
(1.1%)

0.284

(0.0%)

0.289 (1.8%)

Table 3: Manual evaluation results (and percentage of change compared to Matched model). Values marked with / indicate differences that are statistically significant at p < 0.05 and p < 0.01, respectively, compared to the Matched model.

query: stick a fork in it Full: what does the phrase stick a fork in it mean? Matched: is it time to stick a fork in Angels and Dodgers?
query: doctorate vs phd Full: what is the difference between phd and doctorate? Matched: phd in nutrition or naturopathic doctorate?
query: my family in french Full: how do you say my family in french Matched: describe a family member in french?
Table 4: Examples where Full promoted better content at the top compared to Matched
document higher than Matched on 25% of the queries but that Matched ranked the clicked document higher than SoftMatched on 18.3% of the test-set. This may indicate that our current similarity function is noisy and could be improved in future work. While soft matching for retrieval was studied before, this is the first time it is applied in the CQA vertical search scenario. In addition, we are not aware of prior work that directly applies it to a large set of standard LTR features, specifically using similarity between word embedding vectors for lexical semantics (compared to the well studied translation models for this usage).
We now get to our main result, which is split into two parts, corresponding to the exact matching and soft matching settings. Under the exact matching setting, when adding features that directly address unmatched terms (Full model) we see a significant improvement in performance in the automatic evaluation compared to only using matched term features (Matched model). For example, MRR is increased by 2.9%, and similar trends occur for R@K. In Fig. 5 we plot MRR vs query length from which we observe that the MRR gap is maintained across all query lengths. The gap is

slightly decreasing towards longer queries, perhaps because matching many of the terms for longer queries within a question title contains enough information to indicate relevant content. Under the manual evaluation, some improvement is shown. Specifically P@3 and P@10 show an increase of  1% compared to Matched while P@1 and P@5 show comparable results. Tab. 4 shows examples where Full promoted better content at the top compared to Matched. These examples demonstrate how Full downgrades titles containing excessive information that changes the meaning of the title, such as `Angles' and `Dodgers' in the first example.
Both unmatched term features (Full) and soft matched term features (SoftMatched) provide a rather similar improvement over exact matching (Matched). However, they capture different aspects of query/document ranking, one is addressing the lexical gap between the two and the other is addressing the importance of terms that were not matched from either side. Combining both model approaches together (SoftFull model) shows that they convey somewhat complementing elements for ranking. Indeed, SoftFull is the best performing model under all metrics. Under automatic evaluation the improvement is rather additive with a nice gap in performance maintained across all query lengths from both SoftMatched and Matched. Under manual evaluation the improvement is more significant. It seems that under all metrics, but P@3, the combination of soft matching with direct unmatched term assessment is more powerful than each of its parts. This result may indicate that soft matching helps pinpointing the "true" unmatched terms and therefore improves the modeling of their contribution to ranking.
7.1 Excessive vs Missing Features
Our unmatched term features are composed of two types: a) those that address excessive terms, which occur only in the document (EX{L, G}i); and b) those that address missing terms, which occur only in the query (M I{L, G}i). We evaluated the contribution of each feature type independently by constructing two auxiliary models, both augmenting all matched term features (Matched). The first model adds only EX{L, G}i features, denoted MatchedAndExces-

232

Figure 5: MRR by query length

Query what is candys american dream
no virgin birth
92 accord misfire

Document Title
the story "Of Mice and Men", how does the excerpt from Candy relate to the American Dream and Garden of Eden? No Virgin birth? Was the Virgin Mary really a virgin or was this a mistranslation of the Greek? My '92 Accord misfires and loses power at about 2500 rpm. Possible causes?

Table 5: Examples where Full ranked a relevant clicked document low due to many excessive terms

sive, and the second model adds only the M I{L, G}i features, denoted MatchedAndMissing.
Fig. 6 presents the performance of the two new models compared to Matched and to Full on the automatic evaluation setting, measured via the MRR metric. The graph shows that while missing term features are not as strong indicators for irrelevance compared to excessive term features, they still directly improve MRR compared to Matched. In addition, while excessive term features contribute more to detecting irrelevant documents, a small improvement in MRR is gained when excessive and missing features are combined in the Full model.
Another observation from Fig. 6 is that MatchedAndExcessive improves over Matched for short queries much more than for long queries. One reason may be that for short queries there are more candidates in the corpus that contain the query terms, but many of them may have a lot of excessive information. As opposed to short web queries, in which the information need is generally wide, CQA queries tend to refer to very specific information needs even in such short queries, e.g. "characteristics of enzymes". If this hypothesis is true and our algorithm learned that a lot of excessive information indicates an irrelevant candidate, we expect the model to be particularly effective in filtering out such candidates. We note that for short queries of length 1-2 there is no improvement using missing features. This is not surprising, since there are no missing terms when matching queries of length 1, and the amount of missing information in queries of length two is at most a single term.

Figure 6: MRR by query length with the addition of Excessive vs. Missing Features
7.2 Error Analysis
To better understand the performance of our features, we conducted error analysis on cases in which Full, which considers both matched and unmatched term features, ranks a clicked document significantly lower than Matched, which employs only matched term features. To this end, we considered queries from our validation set in which Matched ranked the clicked document for the query in one of the top 3 positions while Full ranked it far below. We sorted the examples by the rank margin between Matched and Full and analyzed the 100 queries with the largest rank margin.
We found out that in 35 of the analyzed queries the top document ranked by Full was relevant and in 46 queries at least one of the top 3 documents was relevant. This is a known issue when using clickthrough logs as proxy to document relevance, as some unclicked documents may also be relevant to the query. Thus absolute model performance under such evaluation is biased. Yet, comparing ranking algorithms over a large-scale click-based gold labeling is useful for differentiating between their ranking performance [27].
Out of the 65 queries where the top candidate by Full was not relevant, we recognized two main reasons for this ranking failure. The most prominent reason, which occurred in 36 cases, is that some terms in the document title were not matched due to the exact-matching scheme used in Full, but would have considered matched under proper soft matching. Out of these 36 case, 13 queries had spelling errors and other phenomena, such as unigram/bigram variations (e.g. `countertop' vs. `counter top'). Such lexical variations are not recognized by our current soft term matching which uses word embedding.
The second phenomenon occurred in 16 out of the 65 queries. The clicked document title contains a lot of excessive terms, yet still fulfills the information need behind the query. Tab. 5 presents such examples. While our results show the potential in directly modeling unmatched terms, and specifically excessive terms as negative signals, a large number of such terms may accumulate into an unnecessary downgrading of the ranking score, and further research is required to develop more robust models.

233

8. CONCLUSIONS
In this work we proposed novel features in a Learning To Rank framework that directly assess the importance of missing and excessive terms within the task of term weighting for vertical search on CQA content. To better model truly unmatched terms we also presented a "soft matching" variant of all our features, basing it on distributional similarity between terms, where similar terms are considered partially both matched, and unmatched. Our experiments show improvement in document retrieval in all settings when unmatched information is taken into account.
In future research we plan to test whether our approach may contribute to other types of Web documents. One direction could be to explore how unmatched terms can be modeled in other parts of the document, which may require different features than the ones used in this paper.
9. ACKNOWLEDGEMENTS
The resrach was supported in part by the Yahoo Faculty Research and Engagement Program.
10. REFERENCES
[1] G. Amati, V. Rijsbergen, and C. Joost. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4), Oct. 2002.
[2] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. Sources of evidence for vertical selection. In SIGIR, 2009.
[3] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM, 2010.
[4] A. Berger and J. Lafferty. Information retrieval as statistical translation. In SIGIR, 1999.
[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 3:993­1022, 2003.
[6] L. Cai, G. Zhou, K. Liu, and J. Zhao. Learning the latent topics for question retrieval in community qa. In AFNLP, 2011.
[7] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. The use of categorization information in language models for question retrieval. In CIKM, 2009.
[8] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. Adapting ranking svm to document retrieval. In SIGIR, 2006.
[9] D. Carmel, A. Mejer, Y. Pinter, and I. Szpektor. Improving term weighting for community question answering search using syntactic analysis. In CIKM, 2014.
[10] R.-C. Chen, D. Spina, W. B. Croft, M. Sanderson, and F. Scholer. Harnessing semantics for answer sentence retrieval. In ESAIR Workshop, 2015.
[11] K. Crammer, A. Kulesza, and M. Dredze. Adaptive regularization of weight vectors. MLJ, 91(2):155­187, 2013.
[12] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. JAsIs, 41(6):391­407, 1990.
[13] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching questions by identifying question topic and question focus. In ACL, 2008.
[14] D. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word embedding based generalized language model for information retrieval. In SIGIR, 2015.
[15] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In CIKM, 2013.
[16] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in large question and answer archives. In CIKM, 2005.
[17] R. Jin, A. G. Hauptmann, and C. X. Zhai. Language model for information retrieval. In SIGIR, 2002.

[18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, Y. Maarek, D. Pelleg, and I. Szpektor. Predicting web searcher satisfaction with existing community-based answers. In SIGIR, 2011.
[19] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225­331, Mar. 2009.
[20] T. y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR Workshop on Learning to Rank for Information Retrieval, 2007.
[21] Y. Liu, C. Sun, L. Lin, Y. Zhao, and X. Wang. Computing semantic text similarity using rich features. In PACLIC, 2015.
[22] C. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[23] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS. 2013.
[24] D. R. Miller, T. Leek, and R. M. Schwartz. A hidden markov model information retrieval system. In SIGIR, 1999.
[25] V. Murdock and M. Lalmas. Workshop on aggregated search. SIGIR Forum, 42(2):80­83, Nov. 2008.
[26] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, 1998.
[27] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In CIKM, 2008.
[28] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333­389, Apr. 2009.
[29] S. E. Robertson and K. S. Jones. Relevance weighting of search terms. Journal of the American Society for Information science, 27(3):129­146, 1976.
[30] S. E. Robertson, C. J. van Rijsbergen, and M. F. Porter. Probabilistic models of indexing and searching. In SIGIR, 1980.
[31] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513­523, Aug. 1988.
[32] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In SIGIR, 2015.
[33] F. Song and W. B. Croft. A general language model for information retrieval. In CIKM, 1999.
[34] K. Tymoshenko and A. Moschitti. Assessing the impact of syntactic and semantic structures for answer passages reranking. In CIKM, 2015.
[35] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In SIGIR, 2006.
[36] R. W. White, M. Richardson, and W.-t. Yih. Questions vs. queries in informational search tasks. In WWW Companion, 2015.
[37] H. Wu, W. Wu, M. Zhou, E. Chen, L. Duan, and H.-Y. Shum. Improving search relevance for short queries in community question answering. In WSDM, 2014.
[38] Q. Wu, C. J. Burges, K. M. Svore, and J. Gao. Adapting boosting for information retrieval measures. Inf. Retr., 13(3):254­270, June 2010.
[39] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question and answer archives. In SIGIR, 2008.
[40] C. Zhai. Statistical language models for information retrieval. Synthesis Lectures on HLT, 1(1):1­141, 2008.
[41] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR, 2001.
[42] W. Zhang, Z. Ming, Y. Zhang, L. Nie, T. Liu, and T. Chua. The use of dependency relation graph to enhance the term weighting in question retrieval. In COLING, 2012.
[43] G. Zheng and J. Callan. Learning to reweight terms with distributed representations. In SIGIR, 2015.

234

A General Linear Mixed Models Approach to Study System Component Effects

Nicola Ferro
Department of Information Engineering University of Padua Padua, Italy
nicola.ferro@unipd.it
ABSTRACT
Topic variance has a greater effect on performances than system variance but it cannot be controlled by system developers who can only try to cope with it. On the other hand, system variance is important on its own, since it is what system developers may affect directly by changing system components and it determines the differences among systems. In this paper, we face the problem of studying system variance in order to better understand how much system components contribute to overall performances. To this end, we propose a methodology based on General Linear Mixed Model (GLMM) to develop statistical models able to isolate system variance, component effects as well as their interaction by relying on a Grid of Points (GoP) containing all the combinations of analysed components. We apply the proposed methodology to the analysis of TREC Ad-hoc data in order to show how it works and discuss some interesting outcomes of this new kind of analysis. Finally, we extend the analysis to different evaluation measures, showing how they impact on the sources of variance.
1. INTRODUCTION
The experimental results analysis is a core activity in Information Retrieval (IR) aimed at, firstly, understanding and improving system performances and, secondly, assessing our own experimental methods, such as robustness of experimental collection or properties of the evaluation measures. When it comes to explaining system performances and differences between algorithms, it is commonly understood [10, 17, 23] that system performances can be broken down to a reasonable approximation as
system performances = topic effect + system effect+
topic/system interaction effect
even though it is not always possible to estimate these effects separately, especially the interaction one.
It is well-known that topic variability is greater than system variability [23, 26]. Therefore, a lot of effort has been
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911530

Gianmaria Silvello
Department of Information Engineering University of Padua Padua, Italy
gianmaria.silvello@unipd.it
put in better understanding this source of variance [17] as well as in making IR systems more robust to it, e.g. [25, 28], basically trying to improve on the interaction effect. Nevertheless, with respect to an IR system, topic variance is a kind of "external source" of variation, which cannot be controlled by developers, but can only be taken into account to better deal with it.
On the other hand, system variance is a kind of "internal source" of variation, since it is originated by the choice of system components, may be directly affected by developers by working on them, and represents the intrinsic differences between algorithms. Its importance is witnessed by the wealth of research on how to compare systems performances in a reliable and robust way [1, 2, 4, 9, 20­23, 27].
However, a limitation of the current experimental methodology is that it allows us to evaluate IR systems only as a kind of "black-boxes", without an understanding of how their different components interact with each other and contribute to the overall performances. In other terms, we consider system variance as a single monolithic contribution and we cannot break it down into the smaller pieces (the components) constituting an IR system.
In order to estimate the effects of the different components of an IR system, we develop a methodology, based on General Linear Mixed Model (GLMM) and ANalysis Of VAriance (ANOVA) [13, 18], which makes us of a Grid of Points (GoP) containing all the possible combinations of inspected components. The proposed methodology allows us to break down the system effect into the contributions of stops lists, stemmers or n-grams and IR models, as well as to study their interaction.
We experimented on standard Text REtrieval Conference (TREC) Ad-hoc collections and produced a GoP by using the Terrier1 open source IR system [12]. This gave us a very controlled experimental setting, which allowed us to systematically fit our General Linear Model (GLM) and break down the system variance. Note that such a controlled experimental setting is typically not available in evaluation campaigns, such as TREC, where participating systems do not constitute a systematic sampling of all the possible combinations of components and often are not even described in such a detail to know exactly what components have been used.
We applied the proposed methodology to TREC 5, 6, 7, and 8 Ad-hoc collections and we employed different measures ­ AP, Precision at 10, RBP, nDCG@20, and ERR@20. This setup allows us not only to highlight how components contribute to the overall system variance but also to gain
1 http://www.terrier.org/

25

insights on how different evaluation measures impact on system and component variances.
The paper is organized as follows: Section 2 presents related work; Section 3 introduces our methodology; Section 4 experiments the proposed methodology; and, Section 5 draws conclusions and discusses future work.
2. RELATED WORKS
The impossibility of testing a single component by setting it aside from the complete IR system is a long-standing and well-known problem in IR experimentation, as early remarked by [16]. Component-based evaluation methodologies [6­8] have tried to tackle this issue by providing technical solutions for mixing different components without the need of building a whole IR system. However, even if these approaches allowed researchers to focus on the components of their own interest, they have not delivered yet estimates of the performance figures of each component.
The decomposition of performances into system and topic effects has been exploited by [1, 23] to analyze TREC data; [4] proposed model-based inference, using linear models and ANOVA, as an approach to multiple comparisons; [10] used multivariate linear models to compare non-deterministic IR systems among them and with deterministic ones. In all these cases, the goal is a more accurate comparison among systems rather than an analysis and breakdown of system variance per se. [17] applied GLMM to the study of per-topic variance by using simulated data to generate more replicates for each (topic, system) pair in order to estimate also the topic/system interaction effect; however, they did not use real data nor did focus on breaking down the system effect.
The idea of creating all the possible combinations of components has been proposed by [7], who noted that a systematic series of experiments on standard collections would have created a GoP, where (ideally) all the combinations of retrieval methods and components are represented, allowing us to gain more insights about the effectiveness of the different components and their interaction; this would have called also for the identification of suitable baselines with respect to which all the comparisons have to be made. Even though [7] introduced the idea of a GoP and how it could have been central to the decomposition of system component performances, they did not come up with an full-fledged methodology for analyzing such data and breaking down component performances, which is the contribution of the present work instead.
More recently, the proliferation of open source IR systems [24] has greatly ameliorated the situation, allowing researchers to run systematic experiments more easily. This led the community to further investigate what reproducible baselines are [5, 11] and the "Open-Source Information Retrieval Reproducibility Challenge" provided several of these baselines, putting some points in the ideal GoP mentioned above. We move a step forward with respect to [11] since we propose an actual methodology for exploiting such GoPs to decompose system performances and we rely on a much finer-grained grid, in terms of number of components and IR models experimented.
3. METHODOLOGY
The goal of the proposed methodology is to decompose the effects of different components on the overall system perfor-

mances. In particular, we are interested in investigating the effects of the following components: stop lists; Lexical Unit Generator (LUG), namely stemmers or n-grams; IR models, such as the vector space or the probabilistic model.
We create a Grid of Points (GoP) on a standard experimental collection by running all the IR systems resulting from all the possible combinations of the considered components (stop list, LUG, IR model); we consider stemmers and n-grams as alternative LUG components, thus we do not consider IR systems using both stemmer and n-grams.
Given a performance measure, such as Average Precision (AP), we produce a matrix Y , as the one shown in Figure 1, where each cell Yij represents a measurement on topic ti of the system sj. Note that the column average ­ i.e., µ·j ­ is the performance mean over all topics for a given system, e.g. Mean Average Precision (MAP); the row average ­ i.e., µi· ­ is the performance mean over all systems for a given topic.
A GLMM explains the variation of a dependent variable Y ("Data") in terms of a controlled variation of independent variables ("Model") in addition to a residual uncontrolled variation ("Error").
Data = Model + Error
The term "General" refers to the ability to accommodate distinctions on quantitative variables representing continuous measures (as in regression analysis) and categorical distinctions representing groups or experimental conditions (as in ANOVA). In our case, we deal with categorical independent variables, as for example different types of stemmers, which constitute the levels of such categorical variable. The term "Linear" indicates that the "Model" is expressed as a linear combination of factors, where the factors can be single independent variables or their combinations. In our case, we are interested both in single independent variables, i.e. the main effects of the different components alone, and their combinations, i.e. the interaction effects between components. The term "Mixed" refers to the fact that some independent variables are considered fixed effects ­ i.e. they have precisely defined levels, and inferences about its effect apply only to those levels ­ and some others are considered random effects ­ i.e. they describe a randomly and independently drawn set of levels that represent variation in a clearly defined wider population; a random factor is indicated by adding a single quote as superscript to the variable name. In our case, the different kinds of systems and components are fixed effects while topics are random effects.
The experimental design determines how you compute the model and how you estimate its parameters. In particular, it is possible to have independent measures designs where different subjects participate to different experimental conditions (factors) or repeated measures designs, where each subject participates to all the experimental conditions (factors). In our case systems and their components are the experimental conditions (factors) while topics are the subjects and, since each topic is processed by each system, we have a repeated measure design.
One advantage of repeated measures designs is a reduction in error variance due to the greater similarity of the scores provided by the same subjects; in this way, variability in individual differences between subjects is removed from the error. Basically, a repeated measure design increases the statistical power for a fixed number of subjects or, in other

26

terms, it allows us to reach a desired level of power with less subjects than those required in the independent measures design.
A final distinction is between crossed/factorial designs, where every level of one factor is measured in combination with every level of the other factors, and nested designs, where levels of a factor are grouped within each level of another nesting factor. In our case, we have a crossed/factorial design because in the generated GoP we experiment each possible combination of components.
3.1 Single Factor Repeated Measures Design

Subjects (Topics)

Factor A (Systems) A1 A2 ... Ap T10 Y11 Y12 ... Y1p µ1·
T20 Y21 Y22 ... Y2p µ2·

...
... ... ...

Yij

µi·

Tn0 Yn1 Yn2 ... Ynp µn· µ·1 µ·2 µ·j µ·p µ··

Figure 1: Single factor repeated measures design.

This design is the one typically used when ANOVA is applied to the analysis of the system performances in a track of an evaluation campaign, as in [1, 23], where the subjects are the topics and the factors are the system runs. Basically, in this context ANOVA is used to determine which experimental condition dependent variable score means differ, i.e. which systems are significantly different from others.
In our case, we are interested also in a second aspect, i.e. to determine what proportion of variation in the dependent variable can be attributed to differences between specific experimental groups or conditions, as defined by the independent variables. This turns into determining which proportion of variation is due to the topics and which one to the systems.
3.1.1 Model
The full GLMM model for the one-way ANOVA with repeated measures is:

Yij = µ·· + i + j + ij

(3.1)

Model

Error

where: Yij is the score of the i-th subject (topic) in the j-th factor (system); µ·· is the grand mean; i is the effect of the i-th subject i = µi· - µ·· where µi· is the mean of the i-th subject; j is the effect of the j-th factor j = µ·j - µ·· where µ·j is the mean of the j-th factor; ij is the error committed by the model in predicting the score of the i-
th subject in the j-th factor. It consists of a term ( )ij which is the interaction between the i-th subject and the j-th factor2; and, a term ij which is any additional error due to uncontrolled sources of variance.

2 In order to calculate interaction effects, you need to have several scores (repli-
cates) for each cell. The mean of the cell scores is taken as the best estimate of

3.1.2 Estimators
We have the following estimators for the parameters of the model above:

·

grand

mean:

µ^··

=

1 pn

p j=1

n i=1

Yij

·

mean of the i-th subject µ^i·

=

1 p

p j=1

Yij

and

its

effect

^i = µ^i· - µ^··

·

mean of the j-th factor µ^·j

=

1 n

n i=1

Yij

and

its

effect

^j = µ^·j - µ^··

· score of the i-th subject in the j-th factor Y^ij = µ^·· + ^i + ^j = µ^i· + µ^·j - µ^··

· prediction error of the i-th subject in the j-th experimental condition ^ij = Yij - Y^ij = Yij - µ^·· - ^i - ^j =
Yij - (µ^i· + µ^·j - µ^··)

3.1.3 Assessment
We can write the model of equation (3.1) introducing the estimated parameters as

Yij = µ^·· + ^i + ^j + ^ij = µ^·· + (µ^i· - µ^··) + (µ^·j - µ^··) + (Yij - (µ^i· + µ^·j - µ^··))
which leads to the following decomposition of the effects

Yij - µ^·· = µ^i· - µ^·· + µ^·j - µ^·· + Yij - (µ^i· + µ^·j - µ^··)

Total Effects Subject Effects Factor Effects

Error Effects

(3.2)

From equation (3.2), we can compute the sum of squares

(SS), degrees of freedom (DF), and mean squares (MS) as

follows:

· total effects SStot =

p j=1

n i=1

(Yij

-

µ^··)2

with

mean

squares

M Stot

=

SStot dftot

where dftot

=

pn - 1 where

dftot comes from the fact that we are summing up pn

scores and one degree of freedom is lost because of the

grand mean µ^··;

· subject effects

pn

n

SSsubj =

(µ^i· - µ^··)2 = p (µ^i· - µ^··)2

j=1 i=1

i=1

with

mean

squares

M Ssubj

=

SSsubj dfsubj

where dfsubj

=

n-1 where SSsubj considers that the quantity µ^i· -µ^··

is the same for all the p factors which the i-th sub-

ject experiences; dfsubj is calculated by summing up n

times the subject mean µ^i· where one degree of free-

dom is lost because of the grand mean µ^··;

· factor effects

pn

p

SSfact =

(µ^·j - µ^··)2 = n (µ^·j - µ^··)2

j=1 i=1

j=1

with

mean

squares

M Sfact

=

SSf act dff act

where dffact

=

p-1 where SSfact considers that the quantity µ^·j -µ^··

the cell score and is used to calculate interaction effects, with the discrepancy between the mean and the actual score providing the estimates of experimental error. If there is only one score per subject per factor, then a mean and its error cannot be calculated per subject per factor and without these estimates, the factor ij cannot be separated from the interaction effect ( )ij .

27

is the same for all the n subjects which experience the j-th factor; dffact is calculated by summing up p times the factor mean µ^·j where one degree of freedom is lost because of the grand mean µ^··;

· error effects

pn

SSerr =

(Yij - (µ^i· + µ^·j - µ^··))2

j=1 i=1

with mean squares M Serr

=

SSerr dferr

where dferr

=

(p - 1) (n-1) where dferr is calculated by summing up

n times the scores where one degree of freedom is lost

in the subject scores because of the subject mean µ^i·

and one degree of freedom is lost in the factor scores

because of the factor mean µ^·j.

Note that SStot = SSsubj + SSfact + SSerr. In order to determine if the factor effect is statistically significant, we compute the F statistics defined as:

Ff act

=

M Sfact M Serr

(3.3)

and compare it with the distribution F(dffact,dferr) under the null hypothesis H0 that there are not significant dif-

ferences in order to estimate the probability (p-value) that

Ffact has been observed by chance. We can set a significance

level  (typically  = 0.05) and, if p-value < , the factor

effect is considered statistically significant.

As introduced above, we are not only interested in de-

termining whether the factor effect is significant but also

which proportion of the variance is due to it, that is we

need to estimate its effect-size measure or Strength of As-

sociation (SOA). The SOA is a "standardized index and es-

timates a parameter that is independent of sample size and

quantifies the magnitude of the difference between popula-

tions or the relationship between explanatory and response

variables" [15].

We

use

the

^

2 f

act

SOA:

^ 2f act

= dffact(Ffact - 1) dffact(Ffact - 1) + pn

(3.4)

which is an unbiased estimator of the variance components associated with the sources of variation in the design.
The common rule of thumb [14] when classifying ^2fact effect size is: 0.14 and above is a large effect, 0.06­0.14 is a medium effect, and 0.01­0.06 is a small effect. ^2fact values could happen to be negative and in such cases they are considered as zero.
When you conduct experiments, two types of error may happen. A Type 1 error occurs when a true null hypothesis is rejected and the significance level  is the probability of committing a Type 1 error. A Type 2 error occurs when a false null hypothesis is accepted and it is concerned with the capability of the conducted experiment to actually detect the effect under examination. Type 2 errors are often overlooked because if they occur, although a real effect is missed, no misdirection occurs and further experimentation is very likely to reveal the effect.
The power is the probability of correctly rejecting a false null hypothesis when an experimental hypothesis is true

Power = 1 - 

where  (typically  = 0.2) is the Type 2 error rate.

To determine the power of an experiment, we compute the effect size parameter:

=

n

·

1

^ 2f act - ^ 2fact

(3.5)

and we compare it with its tabulated values for a given Type 1 error rate  to determine .

3.2 Factorial Repeated Measures Design
While single factor designs manipulate a single variable, factorial designs take into account two or more factors as well as their interaction. As an example a two factors repeated measure design can be defined extending the design described above, where we manipulated one factor (A), by adding an additional factor (B) and the interaction between them (AB).
We can therefore define a three factors design where we manipulate factors A, B and C which correspond to the stop lists, the LUG and the IR models respectively; with this design we can also study the interaction between component pairs as well as the third order interaction between them.
In Figure 2 we can see a table which extends to three factors the design presented in Figure 1 for a single factor. We can see that the systems are now decomposed into three main constituents: (i) factor A (stop lists) with p levels where, for instance, A1 corresponds to the absence of a stop list, A2 to the indri stop list, A3 to the terrier stop list and so on; (ii) factor B (LUG) with q levels where B1 corresponds to the absence of a LUG, B2 to the Porter stemmer, B3 to the Krovetz stemmer and so on; (iii) factor C (IR models) with r levels where C1 corresponds to BM25, C2 to TF*IDF and so on. We call this design a p × q × r factorial design. Each cell of the table in Figure 2, say Ynpqr, reports the mea-
surement (e.g., AP) on topic Tn, for the system composed by the stop list Ap, the LUG Bq and IR model Cr.
The full GLMM model for the described factorial ANOVA for repeated measures with three fixed factors (A, B, C) and
a random factor (T ) is:

Yijkl = µ···· + i + j + k + l +

Main Effects
jk + jl + kl + jkl

+ ijkl

(3.6)

Interaction Effects

Error

where: Yijkl is the score of the i-th subject in the j-th, k-th, and l-th factors; µ···· is the grand mean; i is the effect of the i-th subject i = µi··· - µ···· where µi··· is the mean of the i-th subject; j = µ·j·· - µ···· is the effect of the j-th factor, where µ·j·· is the mean of the j-th factor; k = µ··k· - µ···· is the effect of the k-th factor, where µ··k· is the mean of the k-th factor; and, l = µ···l - µ···· is the effect of the l-th factor where µ···l is the mean of the l-th factor; ijkl is the error committed by the model in predicting the score of the i-th subject in the three factors j, k, l.

It consists of all the interaction terms between the random

subjects and the fixed factors, such as ( )ij, ( )ik and so on, plus the error ijkl which is any additional error due to uncontrolled sources of variance. As in the single factor

design to calculate interaction effects with the subjects, you

need to have replicates; when there is only one score per

subject per factor the factor ijkl cannot be separated from the interaction effects with the random subjects.

28

Factor C (Models) Subjects (Topics)

A1

B1 B2 · · ·

T10 Y1111 Y1121

C1

T20

Y2111 ...

Y2121 ...

···

Tn0 Yn111 Yn121

T10 Y1112 Y1122

C2 T20

Y2112 ...

Y2122 ...

···

Tn0 Yn112 Yn122

T10

XXX T20 XXX

XXX X X X

Tn0

T10 Y111r Y112r

Cr

T20

Y211r ...

Y212r ...

···

Tn0 Yn11r Yn12r

Factor A (Stop Lists)

Factor B (Lexical Unit Generator)

A2

···

Bq B1 B2 · · · Bq

Y11q1 Y1211 Y1221

Y12q1

Y21q1 Y2211 Y2221

...

...

...

···

Y22q1 ...

···

Yn1q1 Yn211 Yn221

Yn2q1

Y11q2 Y1212 Y1222

Y12q2

Y21q2 ...

Y2212 ...

Y2222 ...

···

Y22q2 ...

···

Yn1q2 Yn212 Yn222

Yn2q2

XXX XXX XXX X X X XXX

XXX

Y11qr Y121r Y122r

Y12qr

Y21qr ...

Y221r ...

Y222r ...

···

Y22qr ...

···

Yn1qr Yn21r Yn22r

Yn2qr

Ap

B1 B2 · · ·

Y1p11 Y1p21

Y2p11 Y2p21

...

...

···

Ynp11 Ynp21

Y1p12 Y1p22

Y2p12 ...

Y2p22 ...

···

Ynp12 Ynp22

Bq
Y1pq1 Y2pq1
... Ynpq1 Y1pq2 Y2pq2
... Ynpq2

XXX XXX X X X XXX

Y1p1r Y1p2r

Y1pqr

Y2p1r ...

Y2p2r ...

···

Y2pqr ...

Ynp1r Ynp2r

Ynpqr

Figure 2: Three factors repeated measures design.

The estimators of the main effects can be derived by exten-

sion from those of the single factor design; for instance, the

grand

mean

is

µ^····

=

1 rqpn

r l=1

q k=1

p j=1

n i=1

Yij kl ,

the

mean

of

the

k-th

effect

is

µ^··k·

=

1 rpn

r l=1

p j=1

n i=1

Yijkl

and its estimator is ^k = µ^··k· - µ^····.

The estimators of the interaction factors are calculated as

follows, let us consider ()jk:

jk = µ^·jk· - (µ^···· + ^j + ^k)

(3.7)

where

µ^·jk·

=

1 nr

n i=1

r l=1

Yij kl ;

^j

=

µ^·j··

- µ^····;

and,

^k = µ^··k· - µ^····.

Similarly, we calculate the estimators for all the other in-

teraction factors ­ i.e. jl and kl; jkl is calculated by extending equation (3.7):

jkl = µ^·jkl - (µ^···· + ^j + ^k + ^l)

(3.8)

where

µ^·jkl

=

1 n

n i=1

Yijkl

and

^l

=

µ^···l

- µ^····.

In this design the error ijkl = Yijkl - Y^ijkl contains the

variance not explained by the main and interaction effects

discussed above and it is composed by all the interactions

of the subjects j with the other factors in the model in

addition to the uncontrolled sources of variance.

The sum of squares, mean squares and degrees of freedom

of the main effects can be derived by extension form those of

the one factor design. As an example, the degrees of freedom

of factor A are p - 1 and its sum of squares is:

r q pn

p

SSA =

^j2 = rqn (µ^·j·· - µ^····)2

l=1 k=1 j=1 i=1

j=1

As an example of the computations for the interaction terms, we consider the term A × B whose degrees of freedom are (p - 1)(q - 1) and whose sum of squares is:

r q pn

2

SSA×B =

jk

l=1 k=1 j=1 i=1

qp

= rn

(µ^·jk· - µ^·j·· - µ^··k· + µ^····)2

k=1 j=1

As in the single factor design case, the mean squares of a factor (both main and interaction) are calculated by dividing its sum of squares by its degrees of freedom, the F-test is calculated with equation (3.3), the SOA measure with equation (3.4), and the power with equation (3.5).

4. EXPERIMENTATION AND DISCUSSION
We considered three main components of an IR system: stop list, LUG and IR model. We selected a set of alternative implementations of each component and by using the Terrier open source system we created a run for each system defined by combining the available components in all possible ways. The components we selected are:
stop list: nostop, indri, lucene, smart, terrier;
LUG: nolug, weak Porter, Porter, Krovetz, Lovins, 4grams, 5grams;
model: BB2, BM25, DFRBM25, DFRee, DLH, DLH13, DPH, HiemstraLM, IFB2, InL2, InexpB2, InexpC2, LGD, LemurTFIDF, PL2, TFIDF.
Note that the stemmers and n-grams of the LUG component are used as alternatives, this means that we end up with two distinct groups of runs, one using the stemmers and one using the n-grams; the nolug component is common to both these groups. The group using the stemmers defines a 5 × 5 × 16 factorial design with a grid of points consisting of 400 runs; the group using the n-grams defines a 5 × 3 × 16 factorial design with a grid of points consisting of 240 runs.

29

Table 1: Single factor, ANOVA table for TREC 08

(stemmer group) using AP.

Source

SS

DF

MS

F

p-value

Topics

820.99

49 16.75 694.7235

0

Systems 36.44

399

0.09

7.4464

0

Error

88.20 19551 0.0045

Total

945.63 19999

We conducted single factor and three-factors ANOVA tests for both the groups on TREC 05, 06, and 08 collections, and by employing the following five measures: AP, P@10, nDCG@20, RBP and ERR@20. All the test collections are composed by 50 different topics and have binary relevance judgments; the corpus of TREC 05 is the TIPSTER disk 2 and 4 counting 525K documents, the corpus of TREC 06 is TIPSTER disk 4 and 5 counting 556K documents and the corpus of TREC 07 and 08 is the TIPSTER disk 4 and disk 5 (minus Congressional Record) counting 528K documents.
To ease reproducibility, the code for running the experiments is available at: http://gridofpoints.dei.unipd.it/.

4.1 Single Factor Repeated Measures Effects

We conducted 40 single factor ANOVA tests (4 collections

× 5 measures × 2 run groups), so for space reasons we cannot

report all the result; as an example, Table 1 reports the

synthesis data of the ANOVA test for TREC 08 using the

stemmer group of runs measured with AP.

From the sum of squares (SS) and the mean squares (MS),

we can see that topics explain a large portion of the total

variance. Nonetheless, the effect of the IR systems is statis-

tically significant (p-value 0). We can also see that the sum

of squares of the error is not negligible since it contains both

the variance of the unexplained topics/systems interaction

effect and the the other uncontrolled sources of variance.

From this table we can calculate the statistical power of

the experiment, which is 1 with a Type 1 error probabil-

ity  = 0.05, indicating that we are observing effects in a

reliable way.

Table

2

reports

the

^

2 system

SOA measure and the p-

value of the ANOVA test for the single factor models on all

the test collections for all the considered evaluation mea-

sures. The "LUG" column indicates the runs group we are

considering (stemmers or n-grams). This table shows that

despite the high variance of the topics, the system effect

sizes are generally large and this is consistent across all the

collections and measures. Moreover, system effect sizes of

stemmer runs group systems are large (> 0.14) for all the

collections and measures with the solely exception of AP

for TREC 05. Whereas, for the n-grams runs group we

can see that the system effect sizes are consistently smaller

than those of the stemmer group; this, supports the obser-

vation that "for English, n-grams indexing has no strong

impact" [3].

Table 2 shows that measures impact on the amount of

variance explained by the system effect. Generally, system

effect sizes are higher when nDCG@20 is used, followed by

RBP, P@10, AP and ERR@20. This could be related to two

characteristics of the measures: their discriminative power

and their user model. Indeed, if a measure is less discrimi-

native than another one, it could be able to grasp less vari-

ance in the system effect; on the other hand, different user

models mean looking at (very) different angles of system

performances and this can change the explained variance.

To explore a bit this hypothesis, in Table 3 we report the discriminative power of the five considered measures over the test collections calculated by employing the paired bootstrap test defined in [19]. We can see that there is some agreement between the system effect sizes for a measure and its discriminative power; for instance, ERR@20 explains less system variance than the other measures and this can be explained by its discriminative power which is the lowest amongst all measures; similarly, RBP and nDCG@20 have both comparable discriminative power and close system effect sizes. The main exception is AP which typically has the highest discriminative power but the smallest system effect size; this could be due to the user model behind AP, which is quite different from the one of the other measures and may counterbalance the higher discriminative power leading to a final lower system effect size.
4.2 Three Factors Repeated Measures Effects
In Table 4 we report the ANOVA table of a three factors test for the stemmer group of runs on TREC 08 measured with AP.
We can see that the sum of squares of the topics is the same as the one determined with the single factor design, as well as the error and the total sum of squares. The main difference with the one factor design is that the variance of the systems is now decomposed into three main effects (stop list, stemmer and IR model) and four interaction effects. In this case all the main effects are statistically significant meaning that they have a role in explaining systems variance; in particular, the stop list explains more variance than the model and the stemmer is the component with the lowest impact in this design. Amongst the interaction effects, only the stoplist*model effect is significant explaining a tangible portion of the systems variance. The statistical power for the main effects is 0.97 for the stop list, 0.66 for the stemmer and 0.99 for the model with a Type 1 error probability  = 0.05.
Table 5 reports the estimated 2 SoA for all the main and interaction effects and the p-values for all the ANOVA three-way tests we conducted; from this this table we can see that main and interaction effect sizes are consistent across the different collections.
Analyzing the main effect sizes reported in Table 5 we can see that for the stemmer group of runs the stop list has always a higher ^2 than the IR model and the stemmer and, with the solely exceptions of TREC 05 for AP and ERR@20, the stop list has a medium effect size. Whereas, n-grams tend to reduce the stop list effect and to increase the IR model one; this can be also seen from the n-grams*model interaction effect which is small but statistically significant, differently from the stemmer*model effect which is never significant.
These observations cast a light on the importance of linguistic pre-processing and linguistic resources, given that the role of the stop list is significant in an IR system as well as choosing between stemmers or n-grams. We can further analyze these aspects by looking at Figure 3; the plot on the left reports the main effects for the TREC 08 stemmer group case and we show the marginal means (response means) described in Section 3.2 for the effect under investigation on the y-axis and the various components on the x-axis.
From the first plot we see that the presence or absence of a stop list affects the system performances because the line connecting "no stop" and "indri" is not horizontal, whereas

30

marginal mean AP

nostop indri
lucene smart terrier nostem wkporter porter krovetz lovins
BB2 BM25 DFRBM25 DFRee
DLH DLH13
DPH HiemstraLM
IFB2 InL2 InexpB2 InexpC2 LGD LemurTFIDF PL2 TFIDF

0.26 0.25 0.24 0.23 0.22 0.21 0.20 0.19

Stop Lists

Stemmers

IR Models

Main Effects of Stop Lists, Stemmers, and IR Models for AP on collection TREC 08

Stop Lists=nostop

Tukey HSD Test for AP and Stop Lists on TREC 08

Stop Lists=indri

Stop Lists=lucene

Stop Lists=smart

Stop Lists=terrier

0.19

0.20

0.21

0.22

0.23

0.24

0.25

0.26

marginal mean AP

Tukey HSD Test for AP and Stemmers on TREC 08

Stemmers=nostem

Stemmers=wkporter

Stemmers=porter

Stemmers=krovetz

Stemmers=lovins

0.21

0.22

0.23

0.24

0.25

marginal mean AP

Tukey HSD Test for AP and IR Models on TREC 08

IR Models=BB2 IR Models=BM25 IR Models=DFRBM25 IR Models=DFRee
IR Models=DLH IR Models=DLH13
IR Models=DPH IR Models=HiemstraLM
IR Models=IFB2 IR Models=InL2 IR Models=InexpB2 IR Models=InexpC2 IR Models=LGD IR Models=LemurTFIDF IR Models=PL2 IR Models=TFIDF
0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 marginal mean AP

Figure 3: Main effects plots and Tukey HSD test plots for the stemmer group of runs on TREC 08 with AP.

Stop Lists
Stop Lists = nostop Stop Lists = indri Stop Lists = lucene Stop Lists = smart Stop Lists = terrier

nostem wkporter porter krovetz lovins

BB2BM2D5FRDBFMRD2eL5eHDLHD1P3HHiemIFsBtr2InaLL2MInexInpeBx2LpGCDL2emPuLr2TTFFIDIDFF 0.25 0.20 0.15 0.10 0.05

marginal mean AP

0.25 0.20 0.15 0.10 0.05

Stemmers
Stemmers = nostem Stemmers = wkporter Stemmers = porter Stemmers = krovetz Stemmers = lovins

0.25 0.20 0.15 0.10 0.05

marginal mean AP

0.25 0.20 0.15 0.10 0.05
nostop indri lucene smart terrier

IR Models
nostem wkporter porter krovetz lovins

IR Models = BB2 IR Models = BM25 IR Models = DFRBM25 IR Models = DFRee IR Models = DLH IR Models = DLH13 IR Models = DPH IR Models = HiemstraLM IR Models = IFB2 IR Models = InL2 IR Models = InexpB2 IR Models = InexpC2 IR Models = LGD IR Models = LemurTFIDF IR Models = PL2 IR Models = TFIDF

Figure 4: Interaction plots for the stemmer group of runs on TREC 08 with AP.

31

Table 2: Summary of single factor models on TREC collections. Each cell reports the ^2 for the System

effects and, within parentheses, the p-value for those effects. Large effect sizes (^2Systems > 0.14) are in bold.

Collection

LUG

Effects

AP

P@10

RBP

nDCG@20

ERR@20

TREC 05 TREC 06 TREC 07 TREC 08

Stemmers n-grams Stemmers n-grams Stemmers n-grams Stemmers n-grams

^ 2Systems ^ 2Systems
^ 2Systems ^ 2Systems
^ 2Systems ^ 2Systems
^ 2Systems ^ 2Systems

0.1223 (0.00) 0.0794 (0.00) 0.2108 (0.00) 0.1350 (0.00) 0.2155 (0.00) 0.1502 (0.00) 0.2774 (0.00) 0.1758 (0.00)

0.2023 (0.00) 0.1178 (0.00) 0.2458 (0.00) 0.1496 (0.00) 0.2568 (0.00) 0.1658 (0.00) 0.2780 (0.00) 0.1907 (0.00)

0.1970 (0.00) 0.1349 (0.00) 0.2716 (0.00) 0.1597 (0.00) 0.2894 (0.00) 0.1920 (0.00) 0.3025 (0.00) 0.2006 (0.00)

0.1879 (0.00) 0.1200 (0.00) 0.2742 (0.00) 0.1725 (0.00) 0.2977 (0.00) 0.1898 (0.00) 0.3118 (0.00) 0.2135 (0.00)

0.1406 (0.00) 0.1063 (0.00) 0.2377 (0.00) 0.1469 (0.00) 0.2445 (0.00) 0.1480 (0.00) 0.2484 (0.00) 0.1530 (0.00)

Table 3: Discriminative power of the evaluation

measures on TREC 05, TREC 06, TREC 07 and

TREC 08 for the stemmers and n-grams groups.

Group stemmer
n-grams

AP P@10 RBP nDCG@20 ERR@20
AP P@10 RBP nDCG@20 ERR@20

TREC 05 3011 .3774 .3152 .3448 .2014 .3180 .3025 .3852 .3260 .2832

TREC 06 .2748 .2687 .2589 .2698 .2235 .3553 .2656 .2539 .3130 .1978

TREC 07 .3591 .3222 .3302 .3169 .2096 .5184 .3660 .4193 .4292 .2549

TREC 08 .4743 .3171 .3422 .3834 .2388 .3498 .2977 .2797 .2938 .2416

Table 4: Three factor, ANOVA table for TREC 08

(stemmer group) using AP.

Source Topics Stop list Stemmer Model Stop list*Stemmer Stop list*Model Stemmer*Model Stop list*Stemmer*Model Error Total

SS 820.99
9.89 4.16 5.16 0.05 17.01 0.07 0.09 88.20 945.63

DF 49 4 4 15 16 60 60
240 19551 19999

MS 16.75
2.47 1.04 0.3443 0.03 0.28 0.001 0.00 0.005

F 3713.90
548.06 230.76
76.32 0.67
62.84 0.26 0.08

p 0.00 0.00 0.00 0.00 0.83 0.00 1.00 1.00

the lines connecting the different stop lists have much lower slope. In particular, we see that the choice of the stop list does not make a big difference with respect to use or not use a stop list; this can be further explored looking at the Tukey HSD test plot on the upper-right corner of the figure (in blue the selected component; in grey the components in the same group, i.e. not significantly different; in red the components in a different group, i.e. significantly different), where we can see that there are no significant differences between the "indri", "smart" and "terrier" stop lists, whereas the "lucene" stop list (which is composed by 15 words) is significantly different from the other three.
The main effect of the stemmer is always significant even though its size is quite small; nevertheless, the central plot of Figure 3 shows that there is a tangible difference between systems using or not using a stemmer. This can be seen also from the Tukey HSD test plot on the right; in particular, we can observe that there is no significant difference between the Porter and the Krovetz stemmer which are the stemmers with the highest impact on variance followed by the weak Porter and the Lovins ones.
Lastly, the plot on the right of Figure 3 reports the main effects of the IR models: they behave differently, as shown by several lines with high slopes, but the corresponding Tukey HSD shows that a many models are not significantly different one from the other. This can explain why the IR models effects are statistically significant but their effect sizes are not large.
For all the collections, consistently across the measures and both for the stemmer and the n-grams group, the higher

effect size is reported by the stop list*model interaction effect which is always of medium or large size. This effect shows us that the variance of the systems is explained for the bigger part by the stop list and the model components. For the stemmer group of TREC 08, this can be seen in the plots on the upper-right and lower-left corners of Figure 4 where the lines of the interaction between the stop lists and the models intersect quite often. Indeed, the interaction plots show how the relationship between one factor and a response depends on the value of the second factor. These plots display means for the levels of one factor on the x-axis and a separate line for each level of another factor; if two lines (or segments) are parallel then no interaction occurs, if the lines are not parallel then an interaction occurs and the more nonparallel the lines are, the greater the strength of the interaction.
The stop list*stemmer interaction effects are always not significant as we can see from the p-values of Table 5 and the interaction plots in the upper-left part of Figure 4 where the line segments are parallel. A very similar trend can be observed for the stemmer*model interaction effect.
It is interesting to note that the second order interactions for the n-grams group are all statistically significant and that, in particular, we can see that n-grams, differently than the stemmers, have a bigger effect on the stop list than on the IR model.
We observe that different measures see the stop lists in a comparable way in terms of effect size and this is consistent with what we have seen in the one factor analysis. This is valid also for the stemmer, with the exception of ERR@20 for which it has an almost negligible effect size even though it is statistically significant. In Table 5 we can see that AP and ERR@20 weight the effects in a similar way as it happened in the single factor analysis reported in Table 2. For the n-grams group all the measures are comparable and ERR@20 is not as low as it happens for the stemmers.
Lastly, we can see that the third order interaction are never significant.
5. CONCLUSIONS AND FUTURE WORK
In this paper we faced the issue of how system variance contributes to the overall performances and how to break it down into some of the main components constituting an IR system. To this end, we developed an analysis methodology consisting of two elements: a Grid of Points (GoP) created on standard experimental collections, where all the combinations of system components under examination are considered; and, a GLMM model to decompose the contribution of these components to the overall system variance, paired with some graphical tools for easily assessing the main and interaction effects.

32

Collection TREC 05 TREC 06
TREC 07 TREC 08

LUG Stemmers n-grams Stemmers n-grams Stemmers n-grams Stemmers n-grams

Effects
^ 2Stop Lists ^ 2Stemmers ^ 2IR Models ^ 2Stop Lists×Stemmers ^ 2Stop Lists×IR Models ^ 2Stemmers×IR Models ^ 2Stop Lists×Stemmers×IR Models ^ 2Stop Lists ^ 2n-grams ^ 2IR Models ^ 2Stop Lists×n-grams ^ 2Stop Lists×IR Models ^ 2n-grams×IR Models ^ 2Stop Lists×n-grams×IR Models ^ 2Stop Lists ^ 2Stemmers ^ 2IR Models ^ 2Stop Lists×Stemmers ^ 2Stop Lists×IR Models ^ 2Stemmers×IR Models ^ 2Stop Lists×Stemmers×IR Models ^ 2Stop Lists ^ 2n-grams ^ 2IR Models ^ 2Stop Lists×n-grams ^ 2Stop Lists×IR Models ^ 2n-grams×IR Models ^ 2Stop Lists×n-grams×IR Models ^ 2Stop Lists ^ 2Stemmers ^ 2IR Models ^ 2Stop Lists×Stemmers ^ 2Stop Lists×IR Models ^ 2Stemmers×IR Models ^ 2Stop Lists×Stemmers×IR Models ^ 2Stop Lists ^ 2n-grams ^ 2IR Models ^ 2Stop Lists×n-grams ^ 2Stop Lists×IR Models ^ 2n-grams×IR Models ^ 2Stop Lists×n-grams×IR Models ^ 2Stop Lists ^ 2Stemmers ^ 2IR Models ^ 2Stop Lists×Stemmers ^ 2Stop Lists×IR Models ^ 2Stemmers×IR Models ^ 2Stop Lists×Stemmers×IR Models ^ 2Stop Lists ^ 2n-grams ^ 2IR Models ^ 2Stop Lists×n-grams ^ 2Stop Lists×IR Models ^ 2n-grams×IR Models ^ 2Stop Lists×n-grams×IR Models

AP 0.0432 (0.00) 0.0178 (0.00) 0.0219 (0.00) -0.0005 (0.98) 0.0632 (0.00) -0.0019 (1.00) -0.0115 (1.00) 0.0165 (0.00) 0.0170 (0.00) 0.0208 (0.00) 0.0016 (0.00) 0.0296 (0.00) 0.0050 (0.00) -0.0063 (1.00) 0.0750 (0.00) 0.0112 (0.00) 0.0557 (0.00) -0.0007 (1.00) 0.1153 (0.00) -0.0020 (1.00) -0.0119 (1.00) 0.0241 (0.00) 0.0340 (0.00) 0.0404 (0.00) 0.0026 (0.00) 0.0465 (0.00) 0.0058 (0.00) -0.0033 (0.99) 0.0747 (0.00) 0.0227 (0.00) 0.0441 (0.00) 0.0001 (0.36) 0.1209 (0.00) -0.0018 (1.00) -0.0113 (1.00) 0.0237 (0.00) 0.0208 (0.00) 0.0563 (0.00) 0.00 (0.0001) 0.0517 (0.00) 0.0200 (0.00) -0.0055 (1.00) 0.0986 (0.00) 0.0439 (0.00) 0.0535 (0.00) -0.0003 (0.83) 0.1565 (0.00) -0.0022 (1.00) -0.0111 (1.00) 0.0396 (0.00) 0.0037 (0.00) 0.0550 (0.00) 0.0035 (0.00) 0.0928 (0.00) 0.0080 (0.00) -0.0038 (0.99)

P@10 0.0632 (0.00) 0.0217 (0.00) 0.0458 (0.00)
-0.00 (0.46) 0.1118 (0.00)
-0.00 (0.49) -0.0099 (1.00) 0.0272 (0.00) 0.0105 (0.00) 0.0341 (0.00) 0.0015 (0.00) 0.0544 (0.00) 0.0047 (0.00) -0.0040 (0.99) 0.0852 (0.00) 0.0082 (0.00) 0.0596 (0.00) -0.0007 (0.99) 0.1483 (0.00) -0.0016 (0.99) -0.0109 (1.00) 0.0282 (0.00) 0.0144 (0.00) 0.0516 (0.00) 0.0034 (0.00) 0.0628 (0.00) 0.0091 (0.00) -0.0019 (0.94) 0.0830 (0.00) 0.0157 (0.00) 0.0525 (0.00) 0.0009 (0.00) 0.1624 (0.00) -0.0009 (0.95) -0.0103 (1.00) 0.0344 (0.00) 0.0059 (0.00) 0.0552 (0.00) 0.0014 (0.00) 0.0818 (0.00) 0.0126 (0.00) -0.0044 (1.00) 0.0913 (0.00) 0.0165 (0.00) 0.0615 (0.00) -0.0005 (0.98) 0.1765 (0.00) -0.0014 (0.99) -0.0105 (1.00) 0.0423 (0.00) 0.0031 (0.00) 0.0545 (0.00) 0.0023 (0.00) 0.1129 (0.00) 0.0050 (0.00) -0.0040 (0.99)

RBP 0.0638 (0.00) 0.0116 (0.00) 0.0452 (0.00) -0.0004 (0.97) 0.1145 (0.00)
-0.00 (0.48) -0.0109 (1.00) 0.0288 (0.00) 0.0211 (0.00) 0.0391 (0.00) 0.0020 (0.00) 0.0571 (0.00) 0.0049 (0.00) -0.0034 (0.99) 0.0904 (0.00) 0.0068 (0.00) 0.0692 (0.00) -0.0007 (0.99) 0.1709 (0.00) -0.0017 (1.00) -0.0116 (1.00) 0.0305 (0.00) 0.0126 (0.00) 0.0563 (0.00) 0.0036 (0.00) 0.0673 (0.00) 0.0111 (0.00) -0.0008 (0.72) 0.0997 (0.00) 0.0163 (0.00) 0.0601 (0.00) 0.0004 (0.09) 0.1856 (0.00) -0.0014 (0.99) -0.0111 (1.00) 0.0395 (0.00) 0.0132 (0.00) 0.0623 (0.00) 0.0023 (0.00) 0.0958 (0.00) 0.0116 (0.00) -0.0031 (0.99) 0.1000 (0.00) 0.0190 (0.00) 0.0666 (0.00) -0.0005 (0.98) 0.1969 (0.00) -0.0020 (1.00) -0.0110 (1.00) 0.0445 (0.00) 0.0008 (0.00) 0.0548 (0.00) 0.0024 (0.00) 0.1231 (0.00) 0.0059 (0.00) -0.0032 (0.99)

nDCG@20 0.0605 (0.00) 0.0188 (0.00) 0.0409 (0.00) -0.0004 (0.94) 0.1047 (0.00) -0.0008 (0.95) -0.0107 (1.00) 0.0256 (0.00) 0.0288 (0.00) 0.0275 (0.00) 0.0019 (0.00) 0.0483 (0.00) 0.0050 (0.00) -0.0056 (1.00) 0.0932 (0.00) 0.0126 (0.00) 0.0696 (0.00) -0.0004 (0.94) 0.1671 (0.00) -0.0017 (1.00) -0.0112 (1.00) 0.0306 (0.00) 0.0249 (0.00) 0.0545 (0.00) 0.0033 (0.00) 0.0746 (0.00) 0.0093 (0.00) 0.0004 (0.36) 0.1023 (0.00) 0.0146 (0.00) 0.0653 (0.00) 0.0004 (0.08) 0.1919 (0.00) -0.0018 (1.00) -0.0110 (1.00) 0.0362 (0.00) 0.0154 (0.00) 0.0663 (0.00) 0.0025 (0.00) 0.0874 (0.00) 0.0145 (0.00) -0.0030 (0.99) 0.1006 (0.00) 0.0268 (0.00) 0.0707 (0.00) -0.0006 (0.99) 0.2006 (0.00) -0.0018 (1.00) -0.0110 (1.00) 0.0479 (0.00) 0.0023 (0.00) 0.0637 (0.00) 0.0029 (0.00) 0.1277 (0.00) 0.0050 (0.00) -0.0034 (0.99)

ERR@20 0.0476 (0.00) 0.0000 (0.00) 0.0311 (0.00) -0.0005 (0.99) 0.0826 (0.00) 0.0009 (0.05) -0.0102 (1.00) 0.0225 (0.00) 0.0188 (0.00) 0.0308 (0.00) 0.0015 (0.00) 0.0424 (0.00) 0.0040 (0.00) -0.0048 (1.00) 0.0673 (0.00) 0.0015 (0.00) 0.0638 (0.00) -0.0001 (0.64) 0.1539 (0.00) -0.0013 (0.99) -0.0107 (1.00) 0.0296 (0.00) 0.0104 (0.00) 0.0494 (0.00) 0.0032 (0.00) 0.0646 (0.00) 0.0080 (0.00) -0.0010 (0.78) 0.0802 (0.00) 0.0056 (0.00) 0.0513 (0.00) 0.0002 (0.21) 0.1571 (0.00) 0.0007 (0.12) -0.0107 (1.00) 0.0290 (0.00) 0.0112 (0.00) 0.0382 (0.00) 0.0017 (0.00) 0.0793 (0.00) 0.0082 (0.00) -0.0034 (0.99) 0.0799 (0.00) 0.0071 (0.00) 0.0521 (0.00) -0.0004 (0.95) 0.1622 (0.00) -0.0016 (0.99) -0.0102 (1.00) 0.0304 (0.00) 0.0093 (0.00) 0.0307 (0.00) 0.0032 (0.00) 0.0940 (0.00) 0.0040 (0.00) -0.0028 (0.99)

Table 5: Summary of three factor models on the TREC Ad-hoc collections. Each cell reports the estimated 2 SoA for the specified effects and, within parentheses, the p-value for those effects. Medium and large
effect sizes are in bold; not significant effects are highlighted.

33

We conducted a thorough experimentation on TREC collections and used different evaluation measures to show how the proposed approach works and to gain insights on the considered components, i.e. stop lists, stemmers and n-grams, and IR models.
We found that the most prominent effects are those of stop lists and IR models, as well as their interactions, while stemmers and n-grams play a smaller role. Moreover, we have seen that stemmers produce more variation on system performances than n-grams. Overall, this highlights importance of linguistic resources.
Finally, measures explain system and component effects differently one from the other and not all the measures seem to be suitable for all the cases as it happens for ERR@20 which almost does not detect the stemmer effect. These insights can be useful to understand where to invest effort and resources for improving components, since they give us an idea of the actual impact of a family of components on the overall performances.
As far as future work is concerned, we plan to extend the proposed methodology in order to be able to capture also interaction between topics/systems and topics/components. Indeed, to estimate interaction effects, more replicates would be needed for each (topic, system) pair, as [17] simulated, and they are not possible in the present settings, since running more than once the same system on the same topics produces exactly the same results.
Moreover, we plan to further investigate the impact of the measures on the determination of effect sizes. A possible approach could be to conduct a four-factor analysis, using measures as additional factor. However, even if the measure scores are normalized in the range [0, 1], they do not mean the exactly the same thing, i.e. AP = 0.20 is not exactly the same as ERR = 0.20 because of their different user models. A possibility for smoothing these differences and make the scores more directly comparable could be to normalize them by the maximum value achieved on the dataset, thus reasoning in term of ratios.
Lastly, an open challenge is how to run this kind of analysis on the systems which participated to past TREC editions. A first obstacle is that often there is no precise description of all the components used in these systems and so their metadata should be enriched in the way we suggested in [5]. A second obstacle is that the GoP would be very sparse and many combinations would be missing; therefore, we would need to rely on unbalanced GLMM and, probably, to consider the components as random factors.
6. REFERENCES
[1] D. Banks, P. Over, and N.-F. Zhang. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval, 1:7­34, May 1999.
[2] L. Boytsov, A. Belova, and P. Westfall. Deciding on an Adjustment for Multiplicity in IR Experiments. In SIGIR 2013, pp. 403­412, 2013.
[3] S. Bu¨ttcher, C. L. A. Clarke, and G. V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. The MIT Press, USA, 2010.
[4] B. A. Carterette. Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments. ACM TOIS, 30(1):4:1­4:34, 2012.
[5] E. Di Buccio, G. M. Di Nunzio, N. Ferro, D. K. Harman, M. Maistro, and G. Silvello. Unfolding Off-the-shelf IR Systems for Reproducibility. In SIGIR RIGOR 2015, 2015.

[6] N. Ferro, R. Berendsen, A. Hanbury, M. Lupu, V. Petras, M. de Rijke, and G. Silvello. PROMISE Retreat Report ­ Prospects and Opportunities for Information Access Evaluation. SIGIR Forum, 46(2):60­84, 2012.
[7] N. Ferro and D. Harman. CLEF 2009: Grid@CLEF Pilot Track Overview. In CLEF 2009, pp. 552­565. LNCS 6241, 2010.
[8] A. Hanbury and H. Mu¨ller. Automated Component-Level Evaluation: Present and Future. In CLEF 2010, pp. 124­135. LNCS 6360, 2010.
[9] D. A. Hull. Using Statistical Testing in the Evaluation of Retrieval Experiments. In SIGIR 1993, pp. 329­338, 1993.
[10] G. K. Jayasinghe, W. Webber, M. Sanderson, L. S. Dharmasena, and J. S. Culpepper. Statistical comparisons of non-deterministic IR systems using two dimensional variance. IPM, 51(5):677­694, 2015.
[11] J. Lin, M. Crane, A. Trotman, J. Callan, I. Chattopadhyaya, J. Foley, G. Ingersoll, C. Macdonald, and S. Vigna. Toward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In ECIR 2016, pp. 357­368. LNCS 9626, 2016.
[12] C. Macdonald, R. McCreadie, R. L. T. Santos, and I. Ounis. From Puppy to Maturity: Experiences in Developing Terrier. OSIR at SIGIR, pp. 60­63, 2012.
[13] S. Maxwell and H. D. Delaney. Designing Experiments and Analyzing Data. A Model Comparison Perspective. Lawrence Erlbaum Associates, 2nd ed, 2004.
[14] K. R. Murphy and B. Myors. Statistical power analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests (2nd ed.). Lawrence Erlbaum, 2004.
[15] S. Olejnik and J. Algina. Generalized Eta and Omega Squared Statistics: Measures of Effect Size for Some Common Research Designs. Psychological Methods, 8(4):434­447, 2003.
[16] S. E. Robertson. The methodology of information retrieval experiment. In Information Retrieval Experiment, pp. 9­31. Butterworths, 1981.
[17] S. E. Robertson and E. Kanoulas. On Per-topic Variance in IR Evaluation. In SIGIR 2012, pp. 891­900, 2012.
[18] A. Rutherford. ANOVA and ANCOVA. A GLM Approach. John Wiley & Sons, 2nd ed, 2011.
[19] T. Sakai. Evaluating Evaluation Metrics based on the Bootstrap. In SIGIR 2006, pp. 525­532, 2006.
[20] T. Sakai. Statistical reform in information retrieval? SIGIR Forum, 48(1):3­12, 2014.
[21] J. Savoy. Statistical Inference in Retrieval Effectiveness Evaluation. IPM, 33(44):495­512, 1997.
[22] M. D. Smucker, J. Allan, and B. A. Carterette. A Comparison of Statistical Significance Tests for Information Retrieval Evaluation. In CIKM 2007, pp. 623­632, 2007.
[23] J. M. Tague-Sutcliffe and J. Blustein. A Statistical Analysis of the TREC-3 Data. In Overview of TREC-3, pp. 385­398. NIST, SP 500-225, 1994.
[24] A. Trotman, C. L. A. Clarke, I. Ounis, J. S. Culpepper, M.-A. Cartright, and S. Geva. Open Source Information Retrieval: a Report on the SIGIR 2012 Workshop. ACM SIGIR Forum, 46(2):95­101, 2012.
[25] L. Wang, P. N. Bennett, and K. Collins-Thompson. Robust Ranking Models via Risk-Sensitive Optimization. In SIGIR 2012, pp. 761­770, 2012.
[26] W. Webber, A. Moffat, and J. Zobel. Score Standardization for Inter-Collection Comparison of Retrieval Systems. In SIGIR 2008, pp. 51­58, 2008.
[27] W. J. Wilbur. Non-parametric significance tests of retrieval performance comparisons. J. Inf. Science, 20(4):270­284, 1994.
[28] P. Zhang, S. Dawei, J. Wang, and Y. Hou. Bias­variance analysis in estimating true query model for information retrieval. IPM, 50(1):199­217, 2014.

34

Learning for Efficient Supervised Query Expansion via Two-stage Feature Selection

Zhiwei Zhang1, Qifan Wang2, Luo Si13, Jianfeng Gao4
1Dept of CS, Purdue University, IN, USA 2Google Inc, Mountain View, USA 3Alibaba Group Inc, USA 4Microsoft Research, Redmond, WA, USA
{zhan1187,lsi}@purdue.edu, wqfcr@google.com, jfgao@microsoft.com

ABSTRACT
Query expansion (QE) is a well known technique to improve retrieval effectiveness, which expands original queries with extra terms that are predicted to be relevant. A recent trend in the literature is Supervised Query Expansion (SQE), where supervised learning is introduced to better select expansion terms. However, an important but neglected issue for SQE is its efficiency, as applying SQE in retrieval can be much more time-consuming than applying Unsupervised Query Expansion (UQE) algorithms. In this paper, we point out that the cost of SQE mainly comes from term feature extraction, and propose a Two-stage Feature Selection framework (TFS) to address this problem. The first stage is adaptive expansion decision, which determines if a query is suitable for SQE or not. For unsuitable queries, SQE is skipped and no term features are extracted at all, which reduces the most time cost. For those suitable queries, the second stage is cost constrained feature selection, which chooses a subset of effective yet inexpensive features for supervised learning. Extensive experiments on four corpora (including three academic and one industry corpus) show that our TFS framework can substantially reduce the time cost for SQE, while maintaining its effectiveness.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval
Keywords
Query Expansion; Supervised Learning; Efficiency
1. INTRODUCTION
Queries provided by users can sometimes be ambiguous and inaccurate in an information retrieval system, which may generate unsatisfactory results. Query expansion (QE) is a well known technique to address this issue, which expands the original queries with some extra terms that are
Part of this work was done while the first author interned at Microsoft.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911539

predicted to be relevant [26]. It is hoped that these expanded terms can capture user's true intent that is missed in original query, thus improving the final retrieval effectiveness. In the past decades, various applications [26, 6] have proved its value.
Unsupervised QE (UQE) algorithms used to be the mainstream in the QE literature. Many famous algorithms, such as relevance model (RM) [21] and thesaurus based methods [29], have been widely applied. However, recent studies [5, 22] showed that a large portion of expansion terms selected by UQE algorithms are noisy or even harmful, which limits their performance. Supervised Query Expansion (SQE) is proposed to overcome this disadvantage by leveraging the power of supervised learning. Most of existing SQE algorithms [5, 22, 13, 25, 2] follow a classical machine learning pipeline: (1) utilize UQE to select initial candidate terms; (2) features of candidate terms are extracted; (3) pre-trained classifiers or rankers are utilized to select the best terms for expansion. Significant effectiveness improvement has been reported over their unsupervised counterparts, and SQE has become the new state-of-the-art.
Besides effectiveness, efficiency is another important issue in QE-involved retrieval [35]. As we will show later, UQE algorithms are usually very efficient to apply. Therefore, when UQE is adopted in retrieval, the major inefficiency comes from the second retrieval, which retrieves the entire corpus for the expanded queries. This issue is traditionally handled by indexing or documents optimization [35, 3, 30]. But recently Diaz [11] showed that simply reranking the retrieval results of original queries can already provide nearly identical performance with very low time costs, particularly for precision-oriented metrics.
However, the efficiency issue of SQE algorithms imposes new challenge beyond the UQE case. Compared with UQE algorithms, SQE requires extra time to apply supervised learning, which can incur significant time cost. Moreover, this issue is unique to SQE, and cannot be addressed by previous QE efficiency methods such as indexing optimization or reranking. Unfortunately, although important, this issue has been largely neglected in the literature.
The above observations motivate us to propose new research to address this SQE efficiency problem. In this paper, we point out that the major time cost of applying SQE algorithms comes from term feature extraction. Indeed leveraging extensive features can enhance the effectiveness of supervised learning, so that better expansion terms can be selected. However, it also inevitably decreases the efficiency. Aiming at this point, we propose a Two-stage Fea-

265

ture Selection framework (TFS) to balance the two conflicting goals. The first stage is Adaptive Expansion Decision (AED), which predicts whether a query is suitable for SQE or not. For unsuitable queries, SQE is skipped with no features being extracted, so that the time cost is reduced most. For suitable queries, the second stage conducts Cost Constrained Feature Selection (CCFS), which chooses a subset of effective yet inexpensive features for supervised learning. We then instantiate TFS for a RankSVM based SQE algorithm. Extensive experiments on four corpora (including three academic and one industry corpus) show that our TFS framework can substantially reduce the time cost of SQE algorithm, meanwhile maintaining its effectiveness.
The rest of the paper is organized as follows: Sec. 2 introduces the preliminaries of our work, including problem analysis and literature review; Sec. 3 presents the Two-stage Feature Selection framework and its instantiation; Sec. 4 gives all experiments, and in Sec. 5 we conclude this paper.

2. PRELIMINARIES
In this section, we will thoroughly analyze the SQE efficiency problem. Meanwhile we will review the literature, and point out the difference between our work and previous works. The discussions below are presented in three subsections, each covering one specific aspect.

2.1 QE Algorithm Analysis

First we will review some basics about query epansion.
QE Formulation. Suppose we have a user query q with n terms q = {tqi |i = 1 : n}. Suppose m expansion terms are selected by a QE algorithm, denoted as {tei |i = 1 : m}. Then the expanded query qe is their union, i.e. qe = {tq}  {te}. Each term t  qe is weighted by the interpolated probability

P (t|qe) = (1 - )P (t|q) + PQE(t)

(1)

where P (t|q) is the probability of term t occurring in q (i.e.

P (t|q)

=

frequency of term t in query length |q|

q ),

PQE (t)

is

the

term

probabil-

ity given by QE algorithm, and  is the interpolation coeffi-

cient to be tuned. As can be seen, the key question here is

how to select good expansion terms.

UQE versus SQE. Unsupervised QE (UQE) algorithms

used to be the mainstream in QE literature. For exam-

ple, some well known UQE algorithms include relevance

model (RM) [21], positional relevance model [24], and mix-

ture model [36]. UQE algorithms are very popular because

on one hand their formulations are in general simple, and

on the other hand their empirical performance is quite rea-

sonable. However, recent works [5, 22] observed that a large

portion of the expansion terms from UQE can be noisy or

even harmful, which limits their performance.

SQE tackles this problem by introducing supervised learn-

ing to predict the quality of candidate expansion terms. Cao

et al. [5] proposed perhaps the first SQE research, where

they designed a set of term features and applied SVM for

term classification (either good or bad). Later Lee et al.

[22] claimed that ranking oriented term selection outper-

forms classification oriented methods. Gao et al. [13, 12]

applied SQE to web search, where search log is utilized for

candidate term generation. Some other extensions include

QE robustness [25], query reformulation [2], etc. A common

pipeline of SQE training and testing [5, 13] is summarized

in Alg. 1. Notice here we only concern test-time efficiency,

rather than the training-time efficiency.

Algorithm 1 SQE Training and Testing Pipeline

 Training SQE model H

1: For training query q, record its retrieval accuracy rq (e.g.

2:

ERR@20). Each time,

SaelseicntglMe ccaannddididaatetetteerrmmstc{tisci

|i=1:M} via UQE. appended to q, i.e.

qc = qtc; record its retrieval accuracy rcq; then rcq = rcq -rq

is the label for tc.

3: Extract term features Ft for all tc, and train a classifier (based

orannikfingrocqrd>er0oof rrcqr)cq,

 0) or denoted

train a as H.

ranker

(based

on

the

 Testing (i.e. applying H in QE retrieval)
1: For testing query q, use UQE to select M candidate terms. 2: Extract Ft for all candidate terms. 3: Apply H to get top m terms for expansion.

2.2 QE Efficiency Analysis
Now we will analyze the efficiency issue when QE is applied in retrieval.
QE in Retrieval. The retrieval process with QE can be described as follows. Let C denote the target corpus upon which we will run and evaluate retrieval; denote S as the resource from which expansion terms are extracted. In traditional pseudo relevance feedback (PRF) scenario [6], S = C. In more general scenario, S is not necessarily the same as C. For example, in web search, C (e.g. Clueweb09) might be too low-quality to be used for QE [1]; instead some other resources of higher quality can be used as S (e.g. search log [10] or Wikipedia [1]). Assuming a retrieval algorithm (e.g. BM25 or KL divergence) is utilized, then a typical process of QE in retrieval is summarized in Table 1:
Table 1: QE in retrieval with and without reranking. (1) First Retrieval: search original query q on resource S; (2) Applying QE Model: select expansion terms {te} from S. (3) Second Retrieval:
(Full) retrieve corpus C for expanded query qe. ------------­OR------------­ (Reranking)
(A) If C = S, retrieve C for q, denote the results as L; If C = S, then let the results of first retrieval as L;
(B) Rerank L for expanded query qe.

In the above table we list two possible implementations of second retrieval. The full second retrieval is more traditional, in which the entire target corpus C is retrieved for expanded query qe. This, however, is painfully timeconsuming, particularly on large scale corpus. Recently Diaz [11] suggested to rerank the retrieval results of original query q as the results for qe. Diaz pointed out that this reranking implementation can provide nearly identical performance as the full second retrieval, particularly for precision-oriented evaluation metrics. Our preliminary experiments also verified this statement. Therefore throughout this paper, we will utilize reranking as the default implementation for second retrieval. Notice in the (A) step, we present the different implementation details regarding both PRF scenario (C = S) and non-PRF scenario (C = S).
Existing QE Efficiency Studies. Despite the usefulness of reranking, the majority of existing works on QE efficiency still focused on how to speed up the full second retrieval. As far as we know, all of these works addressed the problem by optimizing underlying data structures such as indexing or document representation. Billerbeck et al. [3] proposed to use compact document summaries to reduce retrieval time. Lavrenko et al. [20] pre-calculated pairwise document similarities to reduce the amount of calculation

266

Figure 1: Comparison of the time cost of each retrieval step.
AED and CCFS are the two-stages in our TFS framework,
the target corpus C is Cw09BNS, resource S is Wikipedia,
the number of expansion terms is 20, and the averaged time
costs per query are reported by running experiments using
a single-thread program on a single PC. The blue line is the
averaged retrieval time cost for original query.
when searching expanded queries. Wu et al. [35] utilized a special index structure named impact-sorted indexing that improves the scoring procedures in retrieval. Theobald et al. [30] proposed the idea of merging inverted list of different terms in an incremental on-demand manner so that document scan can be delayed as much as possible. Unfortunately, our goal now is not the second retrieval, which is handled by reranking as [11] does. Nor can the inefficiency challenge of SQE be handled by the above data-level approaches.
2.3 SQE Efficiency Analysis
Now we will show why the efficiency issue of SQE is a unique and challenging problem beyond the UQE case.
Step-wise Time Cost Analysis. First let's see how UQE and SQE differs in the time cost spent on each retrieval step. On Clueweb09-B corpus, we conduct UQE (RM [21]) and SQE (applying RankSVM [19] based on Cao et al's work [5]) for QE with 20 expansion terms. As comparison, we apply our Two-stage Feature Selection framework (TFS) to SQE. Notice that although UQE does not involve term feature extraction, we can still apply adaptive expansion decision to UQE. In Figure 1, we show the time cost of each retrieval step with respect to Table 1. More experiment details can be found in Sec. 4. Here we mainly discuss the observations that motivate our research.
We can observe that, indeed applying SQE model can be much more time-consuming than applying UQE model, which supports our previous statement and validates our motivation. Notice here, step "FirstRet" and "ApplyQE" are involved in expansion term selection, while "SecRet" includes only retrieving C for original query q and reranking for expanded query qe. Also notice the reranking in second retrieval only incurs very low time cost.
Feature Extraction in SQE. It is then natural to ask, which part of SQE incurs the major time cost, and why?
We argue that term feature extraction is the major inefficient part in SQE models. Recall the testing phase in Alg. 1, there are three steps to apply SQE model. The first step is essentially UQE, which is in general very efficient. The third step, which applies learned SQE model H for term classification or ranking, is also efficient in practice. For example, many SQE works [5, 22, 13, 2] adopted linear model, which is extremely fast yet effective. Therefore, the second step of term feature extraction forms the majority of inefficiency.

Figure 2: With more time spent, more term features are
extracted, and higher retrieval accuracy is achieved.
But why would we spend much time on term feature extraction? This is because predicting the quality of terms is very challenging, so we want plenty of powerful features to enhance the learning algorithm [5, 13, 22]. In Figure 2, we show the retrieval accuracy (ERR@20) when different time cost is spent on term feature extraction. The purple triangle is UQE which does not extract any term feature (i.e. time cost equals zero); the blue rectangle is the full SQE model with all available term features (defined later). In the middle is our proposed cost constrained feature selection method. Clearly, with more time spent, more term features will be obtained, and the retrieval accuracy is higher as well. However, this inevitably degrades the efficiency.
We can further observe that, with more term features, although the retrieval accuracy of SQE is usually higher, the marginal gain becomes smaller. This motivates us to find a subset of features such that their total cost is low, meanwhile the effectiveness is reasonably maintained. This coincides with the idea of feature selection [15], although most of such methods only concern the number of selected features while ignoring their difference in time cost, which can be suboptimal.
3. TWO-STAGE FEATURE SELECTION FOR
EFFICIENT SQE RETRIEVAL
Based on the above analysis, in this section we will present the proposed Two-stage Feature Selection (TFS) framework. Below we will first present TFS as a general framework, then instantiate it for an example SQE algorithm.
3.1 A General Framework
Assume the initial full set of term features are Ft, where subscript t indicates the features are for terms. As analyzed earlier, when retrieval effectiveness is the only goal, Ft tends to become abundant and inefficient. Therefore, the goal of our TFS framework is to select a subset of term features Ft from Ft, so that the effectiveness and efficiency can be optimally balanced. As mentioned earlier, the TFS framework includes the following two stages:
· Adaptive Expansion Decision (AED), which predicts whether a query is suitable for SQE or not. For unsuitable queries, SQE is skipped with no term features being extracted, which reduces the time cost most. To this end, AED builds a classifier VAED with pre-defined query feature Fq, so that VAED(Fq) < 0 for unsuitable queries and VAED(Fq) > 0 for suitable ones.
· Cost Constrained Feature Selection (CCFS), which selects a subset of effective yet inexpensive term features for SQE to apply. For those SQE-suitable queries that pass the first stage, this second stage can further reduce the time cost

267

Table 2: SQE in Retrieval with TFS.

(1) First retrieval:

(1.1) Search original query q on resource S.

(1.2) AED: Extract query feature Fq. If VAED(Fq )  0, directly go to step (3.1); otherwise continue.

(2) Apply QE model:

(2.1) Apply UQE to generate candidate expansion terms.

(2.2) (2.3)

CSeCleFcSt :tehxetrbaecstttteerrmmfseatotufroersmFtexfpoarncdaenddiqduaetreyteqrem. s.

(3) Second retrieval (Reranking):

(3.1) If VAED(Fq )  0: if target corpus is the resource (i.e. C = S), then return the retrieval results in step

(1.1); if C = S, retrieve C for original query q. Exit.

(3.2) If VAED(Fq ) > 0: if C = S, then rerank the retrieval results in step (1.1) for qe; if C = S, retrieve C for original query q, then rerank the results for qe. Exit.

to some extent. To this end, CCFS builds a feature selector VCCF S , which requires the time cost uf of each term feature f , and a pre-defined overall time cost upper bound U . In this way, there is Ft = VCCF S (Ft) and fFt uf  U .
Accordingly, the complete retrieval process is shown in Table 2, which is self-evident to interpret. Below we will give more details about the two stages.

3.1.1 Adaptive expansion decision (AED)
Training. The training process of AED classifier VAED is as follows. For training query q, we first retrieve corpus C and record its retrieval accuracy as rq (e.g. ERR@20 value). Then we apply the SQE model H from Alg. 1 to get the expanded query qe. Retrieve C for qe by following the procedures in Table 2, and denote the retrieval accuracy as rqH. Then if rqH > rq, we assign label +1 to query q, which means SQE can help improve the retrieval effectiveness for q; otherwise we assign -1 to q. Finally, we extract query features Fq for q, and adopt some supervised learning algorithm (e.g. SVM) to get the classifier VAED.
Discussion. The idea of AED stems from query performance prediction. It is known that query expansion may hurt the retrieval effectiveness for some queries [25, 7]. Therefore, accurately predicting those queries and avoid applying expensive SQE model to them can substantially improve the efficiency. This idea of adaptive expansion has also been applied in [18, 27, 23, 8], although their works mainly focused on retrieval effectiveness and did not report the efficiency advantage that this method might bring.

3.1.2 Cost constrained feature selection (CCFS)
Despite the existence of AED, queries that pass AED still face the problem of expensive term feature extraction in SQE. Now we will explain how cost constrained feature selection is designed for those SQE-suitable queries.
Algorithm Design. As mentioned, CCFS aims to select a subset of term features Ft from the complete feature set Ft, so that the overall time cost will not exceed a pre-defined upper bound U , i.e. Ft = VCCF S (Ft) with fFt uf  U .
Our CCFS algorithm is formulated as follows. Since the SQE model H is used to predict the quality of expansion terms, we assume X  RNK as the feature matrix for N candidate terms, where each row is a K-dimensional feature vector for each term. Denote Y  RN1 as the corresponding labels for terms in X, which is calculated as the rcq in Alg. 1. Assume the SQE model H is learned via a loss function LH(X, Y |), where  is the model parameter. We introduce feature selector d  {0, 1}K , where the ith element

Algorithm 2 Cost Constrained Feature Selection
Input Learning algorithm H, algorithm parameter , data X  RNK , label Y  RN1, algorithm loss function LH(X, Y |), feature cost vector u  RK1, final cost upper bound U , cost decrease U .
Output Feature selector d  {0, 1}K satisfying uT d  U , and the learned parameter .
1: t  0, d = 1K1, U 0 = uT 1K1 2: do 3: X  Xd, learn  = arg min LH(X, Y |). 4: Learn d = arg mind LH(Xd, Y |), s.t. d  {0, 1}K ,
uT d  U i. 5: U t+1  U t - U, t  t + 1. 6: while U i > U
di = 1 means the ith feature is selected. With fixed d, those unselected features in X will become invalid, which is equivalent to X  Xd. Together  and d form a revised learning objective as follows:
d,  = arg min LH(Xd, Y |), s.t. uT d  U. (2)
The optimization process is shown in Alg. 2, where a coordinate descent strategy is adopted to iteratively optimize w.r.t  and d. During the iteration, we gradually decrease the cost upper bound (i.e. U ), so that the feature selection process can be smooth. In extreme case where U  U 0, Alg. 2 will produce the same results as vanilla H where no selection occurs (i.e. d = 1K1).
Discussion. Feature selection is a hot research topic in machine learning [15], and has been successfully adopted in information retrieval [13, 14, 33]. Popular methods include L1 based regularization [28, 13], cascading structure [31], feature number constraint [34], etc. Theoretically, any feature selection method can reduce time cost. But in practice we find better effectiveness can be achieved if the time cost of each feature can be explicitly considered. Unfortunately, most of previous research did not model such feature cost difference, which can be suboptimal. Wang et al. [33] proposed a greedy search algorithm for time cost aware feature selection in learning to rank. We also compare this method in our experiments, which shows our formulation outperforms this greedy design. CCFS can also be applied to the AED if necessary, which is straightforward. But due to space limitation, we simplify our experiments by only applying CCFS to term features.
3.2 Instantiation for RankSVM based SQE
So far we have elaborated all the details of TFS as a general framework. Now we will instantiate it with respect to a representative SQE algorithm, to show the implementation details. For SQE, we adopt a RankSVM based SQE method. Linear model has been widely applied in SQE literature [2, 13, 12]. Moreover, a ranking perspective has been proven to be very effective [22, 12] for SQE. Therefore, we believe RankSVM will make a representative example. Also Lavrenko's Relevance Model (RM) is utilized as the UQE model to generate candidate expansion terms, which is arguably one of the most successful UQE algorithms.
3.2.1 AED
The AED stage is simple to implement. Here we utilize SVM with Gaussian kernel for VAED training. The adopted query features Fq are listed in Table 3, which are all well known query performance prediction features in the literature [17, 23, 16, 37].

268

Table 3: Query Features Fq

Description

Formulation

Qry length

|q|

Qry Entropy Qry Clarity 1 Qry Clarity 2
Feedback Radius
Qry IDF Var Max IDF

tq -P (t|DF )log2p(t|DF )

tq

P (t|q)log

P (t|q) P (t|C)

tq

-P (t|DF

)log

P (t|DF ) P (t|C)

1 |DF |

dDF

td

P

(t|d)log

P (t|d) p(t|d¯)

,

P (t|d¯) =

1 |DF |

dDF

P (t|d)

var(idf(t 

q)),

idf (t) =

log2 (|C|+0.5)/|Dt | log2 (|C |+1)

max idf (t  q)

AvICTF

1 |q|

log2

tq

N Nt

Qry Collection Similarity Max of QCS
Qry Variability (QVar)
Max of QVar
Similar Qry Click Score Similar Qry Click Rank

tq (1 + logNt)log(1 +

|C| |Dt |

)

tq

maxtq (1 + logNt)log(1

+

|C| |Dt |

)

1 |Dt |

(wd,t - w¯t )2 , w¯t dDt

=

1 |Dt |

dDt

wd,t ,

wd,t

=

1 + logNd,t

 log(1 +

|C| |Dt |

)

maxtq

1 |Dt |

(wd,t
dDt

-

w¯t )2

1 |Qsim |

RankScore(q , click
q Qsim

doc)

1 |Qsim |

Rank(q , click
q Qsim

doc)

Here q is the given query, t represents term, DF is the pseudo relevant documents obtained from first retrieval of unexpanded

query, C is the entire corpus, |C| is the total document number in

corpus, d represents document, N is the total term number in

corpus, Nt is the number of term t in corpus, Dt is the set of documents containing t, Nd,t is the number of t in doc d. The last two features are based on search log for industry dataset, which

calculate the average clicked doc score/ranks of similar queries

(Qsim) in search log (see Sec. 4 for more details).

3.2.2 CCFS

Applying Alg. 2 in practice requires a deeper insight into the SQE model itself. Nonetheless, as we show below, Alg. 2 can generate elegant solutions that are easy to implement.
Objective. For training queries q, let xqi  R|Ft| be the feature vector for the ith candidate term of q. Here the term features Ft are adopted from [5, 13], and are listed in Table 4. Following the notations of u, U, d in Alg. 2, the objective of cost constrained RankSVM is as follows:

w, d

=

arg min

1 wT w 2

+

G |P |

q,i,jP

q,i,j q,i,j

s.t.(q, i, j)  P : wT (xqi  d) > wT (xqj  d) + 1 - q,i,j , q,i,j  0,

d  {0, 1}|Ft|, uT d  U

(3)

Here P is the set of pairwise candidate terms (q, i, j) where

riq q,i,j

> =

|rjqr;iq

 is element-wise multiplication; and we add - rjq| as loss weight that emphasizes large

relevance difference, which works well in practice. Notice

for RankSVM, there's no need to add offset.

Eq. 3 is how Eq. 2 looks like when RankSVM objective

[19] is introduced. The first three lines (if without d) of

Eq. 3 constitute vanilla RankSVM (with slight modification

of q,i,j), while the feature selector d and the last line of constraints formulate the cost constrained version of RankSVM.

The outcome d indicates what features are selected under

cost upper bound U , and the w is the resulted RankSVM

model based on the selected features.

Optimization. We solve Eq. 3 by converting it into the

dual form via Lagrange multiplier [4], which gives:

min
k ,d

K k=1

k

-

1 2

s.t.

0



k



Gk , K

K k=1

k (xk



d)

T

K k=1

k (xk



d)

d  {0, 1}|Ft|, uT d  U (4)

Table 4:
Description UQEScore Term Doc Num Term Prob in Corpus
Term Distribution
Co-occurrence with single query term
Co-occurrence with pair of query terms
Term Proximity
Document Number of t,e together Probability in similar queries in search log Probability in docs that similar queries clicked

Term Features Ft
Formulation

log score of UQE model

log|De |

log(Ne/N )

Nd,e

log dDF

Nd,t

dDF td

W in(t,e|d)

log

1 |q|

tq

dDF dDF

Nd,t td

W in(ti,j ,e|d)

log

1 |ti,j q|

|ti,j q|

dDF dDF

Nd,t td

log

tq

W

in(t,e)dist(t,e|DF W in(t,e)

)

tq

log(

I(t, e|d) + 0.5)

dDF

log

1 |Qsim |

P (e|q)
qQ

log

1 |Dclick

|

dDclick

P (e|d)

Here e is the target term. DF means the working set of documents. W in(t, e|d) is the co-occurrence times that term t and e appear
within distance 10 in d. W in(ti,j , e|d) is the co-occurrence times that e appear within distance 10 with both ti, tj . dist(t, e|DF ) is the minimal terms between t and e in doc set DF . The last two rows are search log based term features for industry dataset, which
calculate the probability of e in similar queries and their clicked
documents. These two features are essentially one-step random walk
features in a more general context [13]. In [5], the doc working set
DF has two choices, one is the pseudo relevant docs and the other is the entire corpus. The latter, however, is prohibitively expensive in
large dataset. Therefore, we relax DF as follows: assume the number of pseudo relevant docs is K1, and the number of final evaluation is K2 (actually fixed as 1000 in this paper); DF is set as the top {0.5K1, K1, 2K1, 2.5K2, 5K2} docs from first retrieval.

where (q, i, j) in Eq. 3 is re-indexed by k = 1 : K with

each rjq ,

k representing one (q, i, j) otherwise yk = -1; xk =

triplet; xqi - xqj

yk ; k

= =

1 if riq q,i,j . 

> =

[1, ..., K ] is the dual parameter to be learned, and we can

get w as

K

w=

k(xk  d)

(5)

k=1

Based on Eq. 4, now the CCFS problem can be easily

optimized by iteratively solving step 3 and step 4 in Alg. 2.

(Step 3) Fix d and optimize w. When d is fixed, we

can absorb d into X as X  X  d. Under this circum-

stance, Eq. 3 and Eq. 4 become the standard RankSVM

training problem without cost constraint, for which we can

utilize existing algorithms for optimization. Considering the

potential large scale of pairwise term comparison, here we

adopt cutting plane method [19] for efficient optimization.

(Step 4) Fix w and optimize d. With fixed w, now we

aim to find the optimal d. Notice that from step 3, we have

w=

K k=1

k  xk

(recall

X

is

updated

to

absorb

previous

d

in step 3). In this way, Eq. 4 can be reformulated as follows:

d = arg max

K
k=1 k(xk

 d)

T

K
k=1 k(xk  d)

K

K

T

= arg max

k=1 kxk 

k=1 kxk

d

(6)

= arg max(w  w)T d
d

s.t. d  {0, 1}|Ft|, uT d  U

This is a standard linear integer programming problem, for which we utilize Gurobi1 for efficient optimization.
During iteration, the upper bound U is meant to be a tunable parameter for the users. In practice, we can set U as a portion of the overall time cost, i.e. U =   fFt uf ,
1http://www.gurobi.com/

269

where



take

values

such

as

{1,

1 2

,

1 4

,

...}.

U

controls the

number of iterations. In our implementation, we set U =

, fFt uf -U
#I ter

where

#I ter

is

the

desired

iteration

number

(e.g. #Iter = 5 or 10).

4. EXPERIMENTS
So far we have elaborated all the details of the proposed TFS framework. Below we will present extensive experiments to verify its validity.
4.1 Datasets
We adopt four corpora for experiments, including three academic and one industry corpus.
Robust04. This dataset includes about 0.5 million high quality documents. 250 queries (301-450 and 601-700) provided by TREC'04 robust track [32] are utilized for the experiments. MAP is the primary evaluation metric. Notice here by primary evaluation metric, we mean the metric that is used to rank TREC competition teams.
Cw09BNS. Clueweb09 category B is used, which includes 50 million web pages. We utilize University of Waterloo's spam scores [9]2 to remove those with spam scores lower than 70, which leaves 29 million web pages. 150 queries (51 to 200 from TREC'10/11/12 web track) are examined. ERR@20 is the primary evaluation metric. We denote this dataset as Cw09BNS, as NS stands for no spam.
Cw12BNSLD. Clueweb12 category B is used, which also includes 50 million web pages. Since category B contains very few relevant documents that are labeled by TREC, we add all the labeled relevant documents into this dataset. Again University of Waterloo's spam scores [9]3 are applied to remove those spam web pages (with the same threshold 70), which leaves about 15 million documents. 50 queries from TREC'13 web track are utilized, with ERR@20 being the primary evaluation metric. We denote this dataset as Cw12BNSLD, as LD means labeled documents are added.
Industry. This is a large scale web page corpus collected from a major search engine company (i.e. Bing). We incorporate an industry corpus to diversify our experiment settings. For example, the availability of industry search log provides a new resource for query expansion, as well as the search log related features in Table 3 and Table 4. Specifically, this corpus includes about 50 million web pages and 2000 queries. NDCG@20 is the primary evaluation metric as in previous research on a similar industry corpus [13].
4.2 Settings
Now we present all the detailed experiment settings. Corpus Preprocessing. We utilize Indri4, one of the most popular academic search systems, to index all our corpora in the form of inverted index [38]. Krovetz stemmer is applied for stemming, and standard InQuery stopwords are removed. Except stopwords removal, we do not conduct any further pruning that might reduce document lengths. Code & Hardware. In accordance with Indri index, all our algorithms and experiments are implemented in C++ using Lemur/Indri API. The code is compiled by GCC4.7.3 with -O3 option. The code runs in a single thread on a single lab Linux server, which is equipped with a AMD 64bit
2https://plg.uwaterloo.ca/gvcormac/clueweb09spam/ 3http://www.mansci.uwaterloo.ca/ msmucker/cw12spam/ 4http://www.lemurproject.org/indri.php

Table 5: Examples of Search Log Records

Query

Clicked URL

Score Rank

bloomberg news

http://www.bloomberg.com/news/

201

1

firefox com

http://www.mozilla.org/ en-US/firefox/fx/

81

2

gibson althea

http://en.wikipedia.org/ wiki/Althea Gibson

145

3

Smaller ranks and higher scores represent a better match between queries and clicked URLs. Notice these search log queries should not be misinterpreted as the 2000 queries for QE test. They actually serve as S to find the relevant web pages for those 2000 queries.

2.0GHz quad-core CPU, 12G memory and a 3TB disk that contains all the indexed corpora.
QE scenarios. As mentioned in Sec. 2, we can get different QE scenarios, depending on the resource S upon which expansion terms are extracted. For Robust04, we apply the traditional pseudo relevance feedback (PRF) for query expansion, where resource S is identical to the target corpus C. Top 20 documents retrieved for q are considered relevant, from which expansion terms are extracted.
This PRF scenario, however, did not work well on the other corpora, which include web pages of relatively low quality [9]. We find that, on Cw09BNS and Cw12BNSLD, even after filtering spams, the traditional PRF still does not work well, which is also reported in [1]. Therefore, we try the strategy of S = C.
For Cw09BNS and Cw12BNSLD, we follow the suggestion of Bendersky et al. in [1] to use Wikipedia as S. Top 5 ranked Wikipedia documents of original query q are considered relevant, upon which QE is applied. On Industry corpus, we follow the idea of Gao et al. in [13] to use search log as S. The search log is also acquired from the same search engine company, which includes one million distinct click records. Each record contains four elements: user issued query, clicked URL, the score and the rank of the clicked URL returned by the search engine. A snapshot of the search log records is shown in Table 5. For each of the 2000 queries to be experimented, we first find the top 20 similar queries from the log; then the corresponding clicked web pages are considered relevant, from which expansion terms are extracted. This is actually a one-step random walk in search log [13].
Models & Parameters. Following [5], we utilize KL divergence (KLD) as the basic retrieval model for all the experiments below. The Dirichlet coefficient is set as 1500. The UQE and SQE algorithm are the same as explained in Sec. 3.2, i.e. relevance model for UQE and RankSVM for SQE. For both of them, we empirically set the number of expansion terms as m = 20. Other values of m will be examined in Sec. 4.8. For SQE, the number of candidate terms are empirically set as M = 100. Selected expansion terms are added to the original query by probability interpolation, as introduced in Eq. 1. The interpolation coefficient  is tuned over a finite set of values {0, 0.1, ..., 0.9, 1} on the training/validation set.
Evaluation Metrics. Both retrieval effectiveness and efficiency will be evaluated. For effectiveness, MAP and Prec@20 are used for Robust04, and ERR@20 and NDCG@20 are utilized for the other three web page corpora. For efficiency, we report the retrieval time costs per query, which is averaged on each query set.
Training/Validation/Testing. For all the query sets, based on the order of their query IDs, we select the first 40% queries for training all the models (e.g. SQE and TFS), the

270

Table 6: Retrieval Performance on Robust04

MAP()



Prec @20



Time (sec)

%

OrigRet

0.268

-

0.345

-

0.13

-

UQE UQE

0.319 0.321

0

0.369

0

0.002 0.373 0.004

0.61 0.78

0 +27.9%

SQE

SQE

SQE(

1 4

)

0.327 0.329
0.325

0 0.002
-0.002

0.381 0.383
0.378

0 0.002
-0.003

4.73 4.65
1.66

0 -1.7%
-64.9%

SQE(

1 4

)

0.327

0

0.380 -0.001 1.76 -62.8%

Table 7: Retrieval Performance on Cw09BNS

ERR @20()



NDCG @20



Time (sec)

%

OrigRet

0.129

-

0.169

-

9.5

-

UQE UQE

0.149 0.159

0 0.01

0.190 0.194

0 0.004

11.3

0

11.67 +3.3%

SQE

SQE

SQE(

1 4

)

SQE(

1 4

)

0.181 0.187
0.176
0.186

0 0.006
-0.005
0.005

0.197 0.191
0.186
0.191

0 -0.006
-0.011
-0.006

27.9 20.7
15.6
13.8

0 -25.8%
-44.1%
-49.5%

middle 10% queries for parameter validation, and the remaining 50% queries for testing evaluation. All experiments are repeated three times to report averaged time cost.
TFS Notation. As the two stages in TFS can be applied independently, we will utilize superscript  and  to indicate the case when AED and CCFS are applied alone, such as UQE and SQE. When the full set of TFS is applied for SQE, then we denote as SQE.
4.3 More on Time Cost
As mentioned, the time costs reported below are all obtained by running experiments using a single thread on a Linux server. The absolute value might appear larger than previous works (mainly on Cw09BNS and Cw12BNSLD), for which we feel necessary to give a full explanation.
Versus Previous Studies. The reason why previous studies such as [3, 35, 20] reported very low time costs of QE retrieval is mainly due to their selection and pre-processing upon the corpus. For example, (1) Lavrenko et al. [20] and Billerbeck et al. [3] utilized corpora that only contains O(105  106) documents; in comparison our Clueweb09/12 and Industry corpora have O(107) documents, which are at least 10 times larger. (2) Billerbeck [3] and Wu [35] reduced the document length into 20  100 terms long, while the averaged document length for our corpora are 500  800, which are again about 5  40 times larger. The difference of corpus size and document length is the major reason why our reported time costs are larger than previous studies.
Versus Indri Official. With the above settings, our reported time costs are actually quite reasonable. As a proof, the Indri official website5 reported an averaged time cost of 20 seconds per query (wall clock time) on Cw09B (50 million docs, with spam, one thread program on a 3.0GHz CPU, average query length is about 4), while ours is 9.5 seconds per query (29 million docs, no spam, 2.0GHz CPU, average query length is 2.4). After normalizing various factors6, we can conclude that our time cost per query is very close to that reported by Indri official website. Although this is not an exact comparison, it indeed partially supports our claim.
5http://www.lemurproject.org/clueweb09/indri-howto.php
6We divide the averaged time cost per query by the number of documents (in millions) and the averaged query length, and multiply the CPU frequency (in GHz). The result can be seen as an atomic time cost for a single query term on a million documents using 1GHz CPU. In this way, the atomic time cost from official Indri website is 0.3 seconds, while ours is 0.27, which is very close.

Table 8: Retrieval Performance on Cw12BNSLD

ERR @20()



NDCG @20



Time (sec)

%

OrigRet

0.258

-

0.618

-

4.37

-

UQE UQE

0.261 0.262

0 0.001

0.632 0.626

0 -0.006

6.13 6.27

0 +2.3%

SQE

SQE

SQE(

1 4

)

SQE(

1 4

)

0.291 0.292
0.287
0.288

0 0.001
-0.004
-0.003

0.660 0.664
0.665
0.665

0 0.004
0.005
0.005

23.5 18.1
10.5
9.1

0 -23%
-55.3%
-61.3%

Table 9: Retrieval Performance on Industry

NDCG @20()



ERR @20



Time (sec)

%

OrigRet

0.372

-

0.253

-

11.5

-

UQE UQE

0.387 0.391

0

0.260

0

0.004 0.268 0.008

12.1 12.3

0 +1.7%

SQE

SQE

SQE(

1 4

)

SQE(

1 4

)

0.403 0.408
0.4
0.406

0 0.005
-0.003
0.003

0.281 0.285
0.276
0.279

0 0.004
-0.005
-0.002

22.7 19.1
15.1
13.9

0 -15.8%
-33.5%
-38.8%

Versus Engineering Strategy. The absolute value of time costs can be reduced by engineering strategies such as better hardware or distributed/parallel computing, which are widely adopted in commercial search engines like Bing and Google. However, such devices are usually very expensive to equip, and are not available to us. Moreover, accurately counting the time costs in distributed/parallel computing environment becomes difficult, because usually the computing resouces (e.g. CPU or memory) are automatically allocated and can vary as time passes. The advantage of us utilizing single thread program on single computer is that, the overall time costs directly reflects the amount of computation (thus the efficiency), and makes it easy to compare different retrieval methods.

4.4 Overall Performance

We first present the major results of retrieval performance

on the four corpora, as shown in Table 6, 7, 8 and 9.

Comparison Methods. We conduct extensive compar-

ison with the following retrieval configurations:

(1) Retrieval for original queries without QE (OrigRet);

(23) UQE and UQE+AED (UQE);

(47)

SQE,

SQE+AED

(SQE),

SQE+CCFS

(SQE(

1 4

) ),

and

SQE+TFS

(SQE(

1 4

)

).

Here

(

1 4

)

is

an

example

pa-

rameter for upper bound U in CCFS, which means the upper

bound U is a quarter of the overall time costs of all term fea-

tures,

i.e.

U

=

1 4

fFt uf . Other choices of upper bounds

will be examined in Sec. 4.5.

As mentioned earlier, by default reranking (top 1000 doc-

uments) is utilized in the second retrieval to report the time

costs for the above retrieval methods.

Table Explanation. We adopt evaluation metrics re-

garding both effectiveness (e.g. ERR@20, NDCG@20, MAP)

and efficiency (Time in seconds). Here () indicates pri-

mary evaluation metric. We treat UQE and SQE as the

baseline, so that we can show how AED, CCFS and TFS

improve the efficiency respectively. We use column ""

to represent the effectiveness difference between UQE/SQE

and their speedup versions, and use % for the relative time

cost reduction. For example, on Cw09BNS, SQE vs SQE

has ERR@20 difference 0.187-0.181=0.006; their time cost

change (%, in percentage) is (20.7-27.9)/27.9=-25.8%.  and

 label the positive and negative effectiveness difference that

are statistically significant, and  means the time cost reduc-

tion upon SQE is statistically significant (t-test, p < 0.05).

271

Figure 3: Comparison between different feature selection methods. The horizontal axis is the time cost for term feature extraction (excluding any retrieval time). Purple triangles at left end of curves are UQE method with no term features, and blue rectangles at right end are SQE algorithm with all available features. AED is not applied here.

Major Observations. From the tables we can draw the following two major observations.
(1) SQE is more effective but also less efficient than UQE and OrigRet. Compared with OrigRet and UQE, the retrieval effectiveness of SQE can be substantially higher. However, the time costs are also substantially larger. This verifies our motivation that the efficiency issue of SQE is an important research topic.
(2) Both AED and CCFS can substantially improve the efficiency of SQE, meanwhile only incurring insignificant effectiveness fluctuation. In the above tables, we progressively add each component to SQE, so that one can see how the efficiency is progressively improved. In general we can conclude that for SQE, CCFS achieves higher efficiency than AED, and their combination (i.e. our TFS framework) achieves the most efficiency improvement. For effectiveness, although both positive and negative changes are observed, most of them are statistically insignificant (t-test, p<0.05). I.e. most of the effectiveness changes are not labeled by  or . Therefore, we can conclude that our TFS framework can well maintain the effectiveness of SQE.
We also notice that for UQE, UQE has slightly increased time costs. There are two reasons for this phenomenon. First, for UQE there is no expensive term feature extraction, so that AED only skips the generation of UQE expansion terms and the reranking process in second retrieval. Since these two steps are already very fast, the reduced time cost is not substantial. Second, AED will result in some extra time costs for query feature extraction as well as the application of AED classifier. Therefore, the overall time costs of UQE will be slightly higher than UQE. But notice, the absolute value of such increase is very low (at most 0.37 seconds). Furthermore, as we will show in Sec. 4.9, the efficiency improvement of AED can be very substantial, if the full second retrieval is applied instead of reranking, which verifies the usefulness of AED.
Below, we will present more experiments to thoroughly investigate AED and CCFS. As CCFS will also be utilized in AED experiments, we will first analyze CCFS for sake of clear presentation.
4.5 Cost Constrained Feature Selection
In the above we have verified the validity of feature selection in speeding up SQE. Now we will investigate how our CCFS algorithm in Alg. 2 performs in this task.
Comparison Methods. The following two algorithms are compared with our CCFS algorithm.
L1-RankSVM. L1 regularization is a very popular feature selection method. When feature selection occurs, we replace the L2 regularizer in vanilla RankSVM (Eq. 3) with

L1 regularizer ||w||1. By adjusting the coefficient G, ||w||1

will function to different extent, thus resulting in various

combinations of features. Notice this method is unaware of

the difference in the time costs of extracting each feature.

L1General library7 [28] is utilized for optimization.

Wang's method [33]. This is a greedy algorithm for cost

aware feature selection, proposed by Wang et al. in [33] for

learning to rank. In this algorithm, each feature is assigned

a profit score, which is the ratio between feature weight

and time cost. Features are sorted by profit scores; then

top features are selected until the time cost upper bound is

reached, which makes it a greedy selection. Different from

L1-RankSVM, this is a cost aware feature selection method.

For both Alg. 2 and Wang's method, we use the same cost

upper bounds as U =  fFt uf as explained earlier, where

we

adjust



to

different

values

such

as

{1,

1 2

,

1 4

,

...}

to

get

all

the nodes along the curves in Figure 3. For L1-RankSVM,

each node represents a different G value, which is tuned on

training set so that different time costs are obtained.

Overall Results. In Figure 3 we illustrate the curves of

the three methods on all corpora. These curves represent

the retrieval effectiveness when various feature extraction

time is spent (excluding any retrieval time). The purple tri-

angles at the left end of curves represent UQE algorithm,

i.e. no term feature is extracted. The blue rectangles at the

right end of curves represent SQE algorithm with all avail-

able term features. In the middle, various feature selection

methods show different effectiveness-cost tradeoff. We can

clearly observe that more features can produce higher re-

trieval accuracy, but this inevitably takes more time thus

decreasing the efficiency.

CCFS performs best, particularly at low time cost.

Comparing the three feature selection methods, we can see

that our CCFS algorithm outperforms the others, especially

when the feature cost is low. L1-RankSVM penalizes the

number of selected features. That means, each feature is

treated equally, ignoring the cost difference among differ-

ent features. Since expensive features can be included, to

reach a certain time cost, usually L1-RankSVM will over-

penalizes the number of selected features, which deteriorates

the retrieval effectiveness. Wang's method greedily selects

features based on their profit scores, i.e. the ratio between

feature weight and cost. Here the feature weights are the

ones derived when all features are available. However, this

selection process is suboptimal, because for a single feature,

its weight will become different when fewer other features are

available. Therefore, the profit score may not accurately re-

flect the importance of individual features, particularly when

7http://www.cs.ubc.ca/schmidtm/Software/L1General.html

272

Figure 4: Experiments for AED. Red curves correspond to the SQE curves in Figure 3. The UQE nodes (with zero term feature cost) are shown separately for better illustration. The horizontal axis is the overall retrieval time. The blue vertical line is the time cost of OrigRet, which is plotted as reference.

few features exist (i.e. time cost is low). In comparison, our CCFS algorithm iteratively updates learning objective, and decreases the cost upper bound smoothly. Therefore, CCFS performs better, particularly when time cost is low.

4.6 Adaptive Expansion Decision
Now we will examine how AED affects the effectiveness and efficiency of UQE and SQE retrieval. We show the performance in Figure 4, where UQE, SQE and SQE algorithms are all examined before and after applying AED. For SQE, the CCFS curves from Figure 3 are utilized.
For both SQE and SQE, their performance is left-shifted after applying AED. Moreover, the SQE curves shrink after AED. This means, the process of term feature extraction and second retrieval (reranking) are skipped for some of the queries (i.e. SQE-unsuitable), which makes the averaged time costs over all queries become smaller.
For UQE, the time costs of UQE is slightly higher. This has been explained in Sec. 4.4, which are due to the fast process of reranking and UQE expansion term generation, as well as the existence of AED overhead cost.
The extent of efficiency improvement of AED depends on
the number of skipped queries. In Table 10 we give the detailed number of skipped queries on each corpus for SQE.
From the perspective of effectiveness, we can observe that on all corpora, for all of UQE, SQE and SQE, their effectiveness after applying AED is improved or at least maintained than before applying AED. This is particularly helpful in achieving a good balance between effectiveness and efficiency.

Table 10: Number of skipped queries in AED for SQE.

Dataset

#Test Query #Skipped Query Percentage

Robust04

125

8

6.4%

Cw09BNS

75

32

42.67%

Cw12BNSLD

25

8

32%

Industry

1000

361

36.1%

4.7 More on Step-wise Time Cost

In Figure 1 we have shown the step-wise time costs on

Cw09BNS for UQE, SQE and their speedup improvements.

There for CCFS we adopt the same upper bound as in Ta-

ble 7 (i.e.

U

=

1 4

fFt uf ). This is a non Pseudo Rele-

vance Feedback (PRF) scenario where S is Wikipedia and C

is Cw09BNS. In this case, the time cost of second retrieval

will be large because in second retrieval we need to firstly

search query q on C then apply the reranking.

Now we further show the step-wise time costs for PRF

scenario on Robust04 in Figure 5(a). In this case, S = C =

Robust04, so we only need to retrieve q once in first retrieval,

and the second retrieval only needs to rerank the results of

first retrieval. In this case, the cost of second retrieval will be much smaller than in non PRF scenario.
4.8 The Effect of Number of Expansion Terms
Now let's see how different number of expansion terms m affects the retrieval effectiveness and efficiency. In Figure 5(b), we show the effectiveness-cost curves when m = {10, 20, 30} for SQE on Industry corpus. We can see the effectiveness of m = 20, 30 is similar, while that of m = 10 is quite degraded. The time cost gap between OrigRet and SQE curves includes the cost of first retrieval (i.e. searching S), applying AED, extracting term features, etc. Also notice the overall time cost is not obviously affected as more expansion terms are utilized. This phenomenon is mainly due to the application of reranking. Otherwise if a full second retrieval is applied, the time cost of second retrieval will be (approximately) linear with the number of m, which can be very huge in practice (see Sec. 4.9).
4.9 Reranking vs Full Second Retrieval
Finally, for the SQE retrieval process, we compare the two strategies for second retrieval: reranking vs full second retrieval. Although we have argued the validity of reranking and have utilized it throughout the above experiments, we still feel it necessary to present a formal comparison with full second retrieval due to the following two reasons. First, as reviewed in Sec. 2.2, we find most of existing QE efficiency works [3, 35] only focused on indexing or document optimization, while ignored the value of reranking. It is only very recent that Diaz [11] pointed this out. Here we'd like to add more proof to support reranking. Second, in the above experiments for UQE and UQE, we observed that pure AED may not result in substantial speedup, and point out that the application of reranking is the major reason for that. By further showing the time costs of full second retrieval, we can justify the value of AED.
In Figure 5(c) and (d), we show the performance of reranking top 1000 documents for expanded queries qe with 20 expansion terms. We can see that for both the case of UQE and SQE, reranking does not incur obvious effectiveness loss, yet results in substantial efficiency improvement. Particularly for the UQE case, the speedup becomes obvious when full second retrieval is utilized on Cw09BNS. These observations verify our adoption of reranking instead of full second retrieval, as well as the usefulness of AED on efficiency.
5. CONCLUSION & FUTURE WORK
Supervised query expansion (SQE) has recently become the state-of-the-art in the QE literature, which usually outperforms the unsupervised counterparts. To obtain good retrieval effectiveness, SQE usually extracts many term fea-

273

(a)

(b)

(c)

(d)

Figure 5: (a) Step-wise costs on Robust04, which is under PRF scenario. (b) Performance of SQE on Industry

with different number of expansion terms. (c) Reranking vs full second retrieval for UQE and UQE on Cw09BNS.

(d) Reranking vs full second retrieval for SQE and SQE on Industry.

tures to predict the quality of expansion terms. However, this can seriously decrease its efficiency. This issue has not been studied before, nor can it be handled by previous datalevel QE efficiency methods such as indexing or documents optimization. To address this problem, in this paper we propose a Two-stage Feature Selection (TFS) framework, which includes Adaptive Expansion Decision and Cost Constrained Feature Selection. Extensive experiments on four corpora show that the proposed TFS framework can significantly improve the efficiency of SQE algorithm, while maintaining its good effectiveness.
6. REFERENCES
[1] M. Bendersky, D. Fisher, and W. B. Croft. Umass at trec 2010 web track: Term dependence, spam filtering and quality bias. In TREC, 2010.
[2] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In WSDM, pages 443­452, 2012.
[3] B. Billerbeck and J. Zobel. Efficient query expansion with auxiliary data structures. In Information System, pages 573­584, 2006.
[4] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[5] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR, pages 243­250, 2008.
[6] C. Carpineto and G. Romano. A survey of automatic query expansion in information retrieval. In ACM Computing Surveys, 2012.
[7] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In CIKM, 2009.
[8] K. Collins-Thompson and P. N. Bennett. Predicting query performance via classification. In ECIR, pages 140­152, 2010.
[9] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. IR, pages 441­465, 2011.
[10] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic query expansion using query logs. In WWW, pages 325­332, 2002.
[11] F. Diaz. Condensed list relevance models. In ICTIR, 2015.
[12] J. Gao, S. Xie, X. He, and A. Ali. Learning lexicon models from search logs for query expansion. In EMNLP, pages 666­676, 2012.
[13] J. Gao, G. Xu, and J. Xu. Query expansion using path-constrained random walks. In SIGIR, pages 563­572, 2013.
[14] X. Geng, T.-Y. Liu, T. Qin, and H. Li. Feature selection for ranking. In SIGIR, pages 407­414, 2007.
[15] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. JMLR, pages 1157­1182, 2003.
[16] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In CIKM, pages 1419­1420, 2008.

[17] B. He and I. Ounis. Query performance prediction. In Information Systems, pages 585­594, 2006.
[18] B. He and I. Ounis. Combining fields for query expansion and adaptive query expansion. IPM, pages 1294­1307, 2007.
[19] T. Joachims. Training linear svms in linear time. In KDD, pages 217­226, 2006.
[20] V. Lavrenko and J. Allan. Real-time query expansion in relevance models. In Tech Report, Univ Massachusetts Amherst, 2006.
[21] V. Lavrenko and W. B. Croft. Relevance-based language models. In SIGIR, pages 120­127, 2001.
[22] C.-J. Lee, R.-C. Chen, S.-H. Kao, and P.-J. Cheng. A term dependency-based approach for query terms ranking. In CIKM, pages 1267­1276, 2009.
[23] Y. Lv and C. Zhai. Adaptive relevance feedback in information retrieval. In CIKM, pages 255­264, 2009.
[24] Y. Lv and C. Zhai. Positional relevance model for pseudo-relevance feedback. In SIGIR, pages 579­586, 2010.
[25] Y. Lv, C. Zhai, and W. Chen. A boosting approach to improving pseudo-relevance feedback. In SIGIR, pages 165­174, 2011.
[26] C. D. Manning, P. Raghavan, and H. Schu¨tze. An introduction to information retrieval. In Cambridge University Press, 2009.
[27] S. C.-T. ownsend Y un Zhou W. Bruce Croft. A language modeling framework for selective query expansion. In Univ Massachusetts Amherst Tech Report, 2004.
[28] M. Schmidt. Graphical model structure learning with l1-regularization. Univ British Columbia PhD Thesis, 2010.
[29] H. Schu¨tze and J. O. Pedersen. A coocurrence-based thesaurus and two applications to information retrieval. In IPM, 1997.
[30] M. Theobald, R. Schenkel, and G. Weikum. Efficient and self-tuning incremental query expansion for top-k query processing. In SIGIR, pages 242­249, 2005.
[31] P. Viola and M. J. Jones. Robust real-time face detection. IJCV, pages 137­154, 2004.
[32] E. M. Voorhees. Overview of the trec 2004 robust retrieval track. In TREC, pages 69­77, 2004.
[33] L. Wang, D. Metzler, and J. Lin. Ranking under temporal constraints. In CIKM, pages 79­88, 2010.
[34] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for svms. In NIPS, pages 668­674, 2000.
[35] H. Wu and H. Fang. An incremental approach to efficient pseudo-relevance feedback. In SIGIR, pages 553­562, 2013.
[36] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM, pages 403­410, 2001.
[37] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In ECIR, pages 52­64, 2008.
[38] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys.

274

Fast First-Phase Candidate Generation for Cascading Rankers

Qi Wang
Computer Science & Eng. New York University
qiwang@nyu.edu

Constantinos Dimopoulos
Computer Science & Eng. New York University
constantinos@nyu.edu

Torsten Suel
Computer Science & Eng. New York University
torsten.suel@nyu.edu

ABSTRACT
Current search engines use very complex ranking functions based on hundreds of features. While such functions return high-quality results, they create efficiency challenges as it is too costly to fully evaluate them on all documents in the union, or even intersection, of the query terms. To address this issue, search engines use a series of cascading rankers, starting with a very simple ranking function and then applying increasingly complex and expensive ranking functions on smaller and smaller sets of candidate results. Researchers have recently started studying several problems within this framework of query processing by cascading rankers; see, e.g., [5, 13, 17, 51].
We focus on one such problem, the design of the initial cascade. Thus, the goal is to very quickly identify a set of good candidate documents that should be passed to the second and further cascades. Previous work by Asadi and Lin [3, 5] showed that while a top-k computation on either the union or intersection gives good results, a further optimization using a global document ordering based on spam scores leads to a significant reduction in quality. Our contribution is to propose an alternative framework that builds specialized single-term and pairwise index structures, and then during query time selectively accesses these structures based on a cost budget and a set of early termination techniques. Using an end-to-end evaluation with a complex machinelearned ranker, we show that our approach finds candidates about an order of magnitude faster than a conjunctive top-k computation, while essentially matching the quality.
1. INTRODUCTION
Search engines are continuously optimizing their ranking functions in order to improve result quality. This is usually achieved through more and more complex ranking functions based on large sets of features, including features derived from text, link structure, past queries, and online or proprietary data sets and knowledge bases through various data extraction and mining techniques. However, these complex
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy @ 2016 ACM. ISBN 978-1-4503-4069-4/16/07 $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911515.

ranking functions create significant performance challenges for the engines, as evaluating them on large numbers of documents is very expensive. Given the billions of queries that have to be processed each day, it would not be feasible to apply such functions directly on all documents passing the initial Boolean filter, even in the conjunctive case.
To address this challenge, all current major engines appear to be using a cascading approach to query processing that approximates the results of such ranking functions through a series of increasingly more complex intermediate functions. Thus, after query analysis and rewriting, the search engine first applies a very simple ranking function, similar to BM25 or a Cosine measure, that only uses a few features and can be very efficiently applied on large numbers of candidates. We refer to this process as the first cascade. Next, in the second cascade, a somewhat more complex function based on a larger set of features is applied to, say, the top few thousand results from the first cascade. In the third and further cascades, even more complex functions are applied to smaller and smaller sets of surviving candidates of the previous cascades. This approach was described and formalized in [51].
Thus, the goal of the cascading approach is to return (almost) the same results as we would get from applying the complex function on all candidates, at a fraction of the computational cost. Its proper implementation, however, poses several challenges that have recently received some attention in the literature [5, 13, 17, 32, 40, 51]. In particular, work has focused on four distinct challenges: (1) How to design good sequences of increasingly complex ranking functions and associated cutoffs (number of results kept for the next cascade) [51]; (2) how to efficiently apply a complex ranking function to candidates by using early-exit strategies [13]; (3) how to design ranking functions for the first cascades that preserve many good candidates for subsequent cascades, as opposed to focusing on how to order a few top results [17]; (4) how to implement the first cascade efficiently through an optimal choice of Boolean filters and various early-termination techniques [3, 5].
We focus on the last challenge, which is important as the first cascade is executed on large numbers of candidates. The results in [3, 5] indicated that for the first cascade, a conjunctive filter does essentially as well as a disjunctive one, while saving a lot of time. However, a naive attempt to use a global document ordering to avoid a complete conjunctive traversal of the index structures resulted in significant losses in end-to-end result quality.
In this paper, we propose an approach that runs about an order of magnitude faster than even highly optimized con-

295

junctive and disjunctive top-k computations, while achieving essentially the same end-to-end result quality. More precisely, given a complex ranking function that needs to be approximated by a cascading approach, an inverted index structure for a document collection, and a training query trace, we show how to build an auxiliary layer of index structures, and how to select which parts of this layer to consult on a submitted query, in order to obtain high-quality candidates with a limited computational and space budget.
In particular, our contributions are as follows:
1. We design a framework that builds specialized singleterm and pairwise index structures subject to a given space constraint. It uses limited size query traces to train query term language models and posting quality models.
2. We propose an online selection algorithm based on a cost budget that, for a given query, decides the access depth for each available structure according to the posting quality model, and present query processing and lookup strategies that further improve performance.
3. We provide an end-to-end evaluation of our proposed architecture on ClueWeb09B, and show that our approach can identify candidates about an order of magnitude faster than previous published results, with negligible quality loss.
The remainder of the paper is organized as follows: Section 2 presents some background and discusses related work. In Section 3 we give a high-level overview of our approach, and present our solutions for several components. Next, Section 4 outlines the experimental setup, and Section 5 presents the experimental evaluation of the proposed framework. Finally, Section 6 provides some concluding remarks.
2. BACKGROUND AND RELATED WORK
In this section, we provide some background on inverted indexes, query processing and early termination techniques, and cascade ranking architectures, and discuss related work.
Inverted Indexes: Commercial search engines perform query processing based on the widely used inverted index [58]. Given a collection of N documents, each document is assigned a unique identifier (docID) from 0 to N -1. The inverted index consists of a set of inverted lists and a lexicon. In particular, for each distinct term t in the collection, there is an inverted list Lt. Each Lt is a list of postings specifying the documents that t appears in. Typically, each posting contains the docID of a document containing t and the frequency of t in the document; however, there may also be other information, such as the positions of the term occurrences in the document, or a precomputed impact score. The lexicon contains for each unique term in the collection a pointer to the corresponding inverted list. Inverted index compression is crucial for search engine performance and many techniques have been proposed [6, 45].
Index Layout: There are many ways to organize the inverted lists. In document-sorted indexes, each list is sorted by docID, resulting in small delta gaps (d-gaps) between consecutive docIDs that lead to a smaller compressed size [45]. Impact-sorted indexes sort the postings in each list in decreasing order of impact scores. Thus, high-scoring query results tend to be located near the front of the lists, potentially enabling a smart query processing algorithm to skip

most of the rest of the list. However, this approach leads to poor compressibility compared to document-sorted indexes, and may require random lookups into lists for docIDs that score high on one query term but low on others. Finally, impact-layered indexes split each inverted list into a small number of layers based on impact scores. Our approach uses two layers, where the first layer of high-impact postings is sorted by impact, and the second layer by docID to allow for efficient random lookups.
Index Traversal: During query processing, index structures can be traversed in different ways [50]. In Documentat-a-time (DAAT), each list has a pointer to a current posting and one document is processed at a time; then pointers are moved forward in docID space. The top results are usually maintained in a min-heap structure. In Term-at-a-time (TAAT) traversal, a list is fully traversed before accessing the next one. Partially scored documents are kept as accumulators in a hash table or other structure; this structure can be a bottleneck if it grows beyond the CPU caches. Lastly, there are hybrids between DAAT and TAAT. Note that DAAT is mainly suitable for document-sorted indexes, while TAAT works well with impact-sorted ones.
2.1 Query Processing and Early Termination
The simplest form of query processing applies a Boolean filter (AND/OR) on the inverted lists of the query terms, and then ranks all documents passing the filter. A good ranking function should (a) provide a good approximation of the relevance of a document with respect to a query and (b) be efficiently computable using the information stored in the index. Well-known examples include Cosine measures and BM25 [6]. Most of these simple ranking schemes have the property that the score of a document d for the full query q is the sum (or other simple combination) of per-term scores; i.e., score(q, d) = tq s(t, d), where s(t, d) is the impact score of term t in d.
Early Termination: A na¨ive query processing approach is inefficient, and ends up decompressing and accessing large parts of the inverted lists. To improve on this, researchers have proposed many early termination (ET) algorithms that try to find good results while accessing and scoring only a small part of the relevant inverted lists. ET algorithms are called safe if they always return the same results as an exhaustive algorithm, and unsafe otherwise [48]. ET techniques are widely used in commercial engines and academic systems, and include the following approaches:
· Index Tiering: A collection is partitioned into, say, 2 or 3, disjoint subsets of documents called tiers, where the first tier contains the highest-quality documents; queries are executed on the first tier and only selectively routed to other tiers [41, 42].
· Pruning: In static pruning, postings considered unlikely to ever be useful are deleted from the index [8, 12, 24, 37]. In dynamic pruning, inverted lists are typically organized in impact-sorted or impact-layered form, and algorithms focus on high-impact postings and only selectively access lower-impact postings for promising documents. One very widely studied example are the FA, TA, and NRA algorithms in [23].
· Skipping: For document-sorted indexes, there are various techniques for skipping unimportant parts of the inverted lists [10, 14, 19, 20, 21].

296

Our approach here is unsafe, and based on dynamic pruning with impact-layered structures for both terms and term pairs (intersections of two terms), as described later.
2.2 Cascading Ranking Architectures
In modern commercial engines, query processing is based on cascading ranking schemes [10, 13, 51], where each cascade includes a ranker that provides candidate documents to subsequent cascades. The first cascade is usually based on a very simple ranking function that is evaluated on large parts of the index structure to get an initial set of candidates. Thus, this function must be very fast, while providing a reasonably high-quality set of candidates. Subsequent cascades are executed on fewer candidates using more complex and expensive ranking functions. The challenge in designing such cascading architectures is to select a set of cascades and associated ranking functions that achieves high end-to-end quality at low cost.
Cascading setups are crucial for the performance and quality of modern commercial engines, and a number of papers have recently focused on this setup [4, 5, 13, 17, 32, 40, 51]. We focus on optimizing the efficiency of the first cascade in such architectures, a problem recently studied in [4, 5].
2.3 Comparison to Previous Work
We now discuss the relationship of our approach to previous work. The particular problem we consider is based on the setup in [5, 4, 17]. Thus, we have a two-phase cascading architecture, where the first phase obtains an initial candidate set of, say, several hundred or thousand documents, while the second phase reranks these candidates using a more sophisticated ranker based on dozens or hundreds of features. Our goal is to design very fast ET algorithms for the first phase that achieve end-to-end quality comparable to more exhaustive approaches. This is essentially the problem addressed by Asadi and Lin in [4, 5].
In particular, [5] investigates the efficiency/effectiveness trade-off for various first-phase candidates generation approaches. They experiment with conjunctive WAND [10], disjunctive WAND, and two conjunctive algorithms that first obtain the intersection and then rerank results based on BM25 or spam score, and conclude that conjunctive WAND provides the best trade-off. Work in [4] shows how to accelerate the intersection-based approaches using Bloom Filters. Our contribution here is to provide a method that achieves quality comparable to their best methods at lower cost.
Another relevant recent work [52] proposes a document prioritization method for selective evaluation of documents that achieves a better efficiency/effectiveness balance in the first cascade. The running times reported in [52] are significantly slower than ours, though some of the ideas could potentially be used to further optimize the lookup phase of our approach.
Our algorithm is based on a layered index organization and performs a limited-depth access to impact-sorted singleterm and term-pair structures, followed by random lookups. As such, it is closely related to the well-know FA algorithm proposed by Fagin [23], and also to ET algorithms for impact-layered indexes introduced in [39]. There are many subsequent papers that further developed and often combined these approaches to solve various IR ranking problems, including, e.g., [2, 7, 9, 33, 47, 48].
We note that [28, 29] describe a number of access and

lookup strategies for top-k query processing in database applications. One algorithm that is somewhat similar to our approach is the MPro algorithm in [28], which seeks to minimize lookup costs through sorted access. Work in [48] suggested methods for selecting the access depth into the available single-term impact-sorted lists. However, [48] focused only on single-term impact-sorted lists and did not provide cost-based query processing algorithms in the context of cascading rankers. Our approach is different from both of the above as we are proposing a framework for constructing additional impact-sorted index structures, including pairwise structures, based on query term distributions and posting quality models and subject to space constraints, plus an online depth selection algorithm and lookup strategies.
Our approach relies heavily on term-pair index structures, introduced in [34] and subsequently studied in a number of papers such as [11, 15, 26, 30, 44, 54, 56, 57]. Our work is most closely related to [30], which also applies the approach in [23] to pairwise structures. The main difference is our focus on cascading ranking schemes, and our framework for optimizing index structures and index traversals based on a limited space and access cost budget.
Also related is the work in [11], which proposes building single and pairwise impact-sorted lists that are then completely accessed during query processing. Though related, our work is different in two ways: While [11] and earlier work in [44, 54, 56, 57] assume a ranking function that directly takes proximity into account, we assume a more complex function in a cascading setup. Also, the resulting auxiliary structures in [11] are much larger than the basic index size; in contrast, our query language and posting quality models allow us to achieve high speed with only a limited increase in size. We note that our results could potentially be improved by adding special pairwise structures for terms occurring close to each other in a document, as done in [11, 54].
Finally, [1, 27] study techniques for learning better index structures given a set of documents and queries. In particular, [27] can be seen as essentially learning an ordering of index postings that is better than the "natural" impactbased ordering used, e.g., in [23]. We note that this issue is orthogonal to our approach, and could be combined to possibly yield additional benefits. Lastly, while we construct our first-layer structures using off-line preprocessing, one could also approach this via a suitable caching mechanism, such as those in [25, 38, 46] for other types of structures. Finally, our improvements are in addition to any speedups achieved through result caching, since our query traces do not have significant numbers of repeated queries.
3. PROBLEM SETUP AND APPROACH
We now define and discuss our problem setup, give a highlevel description of our approach, and then provide more details about the various steps that are involved.
3.1 Problem Setup
We are given a complex ranking function CF , a simple ranking function SF used as the first cascade, and a rank cutoff c for the first cascade, meaning that only c results from the first cascade will be evaluated by the complex ranking function, which will then return the top k, k  c, results to the user. Our goal is to implement the first cascade to run as fast as possible without significantly decreasing end-to-end result quality. In our implementation, we allow unsafe early

297

termination techniques, that is, the c results we give to CF may be different than those obtained from an exhaustive top-c computation using SF .
We measure quality in two ways: (i) Overlap@(k,c), meaning how many of the top-k results that would be returned by an exhaustive application of CF (i.e., ccf =  or at least fairly large) are returned with our first cascade implementation that evaluates c candidates and (ii) NDCG@k, which is the normalized discounted cumulative gain that considers the order of the results.
Problem Discussion: Note that while the above definition assumes only two cascades, SF and CF , this does not really limit our approach as long as any additional intermediate cascades do a good job at approximating CF , i.e., do not lose too many good results among those nominated by SF for further processing. We believe this is a reasonable assumption in practice, and assume that subdividing the second cascade into further cascades is a separate problem. For the same reason, the reported running times are only for the first cascade, as the cost of the second cascade should only depend on c.
While an unsafe implementation of the first cascade could in principle achieve better quality than a safe disjunctive or conjunctive top-c computation using SF , this is not really expected. Thus, our goal is to do (almost) as well as these two choices, shown to be good in [3, 5], while being much faster than these and other non-safe competitors.
3.2 Our Overall Approach
We now describe our approach, which starts with an existing inverted index for the collection that could be used to run queries using SF . We then create additional auxiliary index structures to quickly identify promising candidates. We create two kinds of structures, single-term structures, and term-pair, or pairwise, structures, which together make up the first layer of the index, while the complete inverted index1 makes up the second layer. For the single-term structures in the first layer, we choose, for each inverted list, some number of high-scoring postings, and arrange them by impact score in decreasing order. The pairwise structures are obtained by intersecting two inverted lists, and keeping a certain number of high-scoring two-term postings, where the score of such a posting is the sum of the impact scores of the two constituent postings. These structures are also sorted in decreasing order of impact score. We later discuss how to select which postings to put into the first layer, based on query traces and impact scores.
When a query enters, the first cascade is now executed by first selecting and accessing some prefix of the much smaller relevant structures in the first layer of the index. Afterwards, we perform a limited number of lookups into the second layer, to obtain additional scores for some promising documents for which we have found partial scores in the first layer. Finally, we identify c documents for further evaluation by CF . We show the overall index structure in Figure 1, where a query "dog cat mouse" is processed.
For our problem setup, there are various technical problems to address. In particular, when building the first layer, we need to decide how deep we should build the single-term structures, and for which pairs of terms we should build a pairwise structure and up to what depth. The goal is for the structures to be deep enough to find most good results,
1Except for short lists that are completely in the first layer.

Figure 1: Our index structure, with first layer on the left and second layer on the right. In the top left are single-term structures, and in the bottom left are pairwise structures, each sorted by decreasing impact score. For a query "dog cat mouse" our method might decide to access a certain prefix (shown in red) of each relevant single-term list, and of one of the available pairwise structures (in this case, for "cat" and "mouse"), based on some access budget.
but not so deep that there is a large increase in overall index size. When a query enters, we need to decide which of the applicable index structures to consult and up to what depth ­ always using all potentially relevant structures in the first layer up to their full depth would not be efficient. We also need to decide what lookups into the second layer should be performed to identify the c results to be forwarded to the complex ranker for full evaluation. We later propose and evaluate solutions for all these problems.
Overall, our approach has the following steps that need to be implemented. During indexing, we have two steps:
· Modeling: We build two types of models by performing training on a query trace of limited size, (a) a language model for the queries that allows us to predict how frequent certain terms and combinations of terms are in the query trace, and (b) a quality model relating the rank of a posting in a first-layer structure to its likelihood of being a top result under CF .
· Index Building: We build the first layer based on the constructed models, by carefully choosing which structures to build and up to what depth, subject to a maximum space budget.
Later, when a query enters the system, we execute the following sequence of steps:
· Online Greedy Depth Selection: We consider the list of relevant structures, and decide which of them should be accessed and to what depth, based on the quality model and based on a simple model for query processing costs. In fact, costs will be modeled based on the aggregate access depth into the first-layer structures and the number of lookups into the second layer, with a certain budget available for a query.
· Query Processing: We throw the accessed structures and their corresponding access depths, as selected in the previous step, into a simple but fast inmemory query processor.

298

· Second-Layer Lookups: We decide for which candidate documents we should perform lookups into the second layer to get more precise scores.
· Final Selection: We choose the c results that should be evaluated by the complex ranker.
3.3 Index Construction
We now describe the two steps in the index construction in more detail. First, we build two models, one for query and query term distribution, and one to model the quality contributions of different parts of the index structures. These models are then stored for later use during index building and query processing.
Modeling: For the first model, we use standard Language Modeling tools, in particular the MIT Language Modeling (MITLM) toolkit2, based on Kneser-Ney smoothing. We train these models on part of our query trace (distinct from any queries used in the evaluation), to obtain estimates of two probabilities, p(t), the probability that term t occurs in a random incoming query, and p(t1, t2), the probability that both t1 and t2 occur in such a query. We refer to these as our query language models.
For the second model, given a posting p for a term t that has rank r in its list (i.e., has the r-th highest impact score in its list), we want a rough estimate of the likelihood that the posting belongs to a top-k result under the complex ranker CF , given a random query containing t. This is done by issuing training queries and, for each posting in one of the query term lists, storing its rank, the length of its inverted list, and whether it is part of a top-k result for the query. We bucketize the list lengths and relative ranks within lists into dozens of ranges (classes) each. Then we aggregate our data into a two-dimensional table A where A[i, j] estimates the probability that a posting belonging to list length class i and relative rank class j (which might, say, correspond to a list length between 1000 and 1500 and rank between 120 and 160) leads to a top-k result. This approach gives sufficiently accurate predictions, while allowing extremely fast lookups during index building and query processing to get an estimate of p(top-k|t), the likelihood that a posting is part of a top-k result given that its term t is part of a query.
We then repeat the process for term intersections, where we create a table where for each posting in the intersection of two terms t1 and t2, we use the length of the intersection, and the rank of the posting in the intersection, to estimate p(top-k|t1, t2), the likelihood that a pairwise posting is part of a top-k result given that both t1 and t2 occur in a query. We refer to these models for single lists and term intersections as posting quality models.
Index Building: Given a space budget, our next goal is to build a first layer containing term and term-pair postings that are likely to lead to top-k results under random queries. To do this, we allocate separate space budgets to the singleterm and pairwise structures in the first layer. For singleterm lists, we greedily pick postings from the highest ranks of the inverted lists to add to the first layer. That is, we try to pick postings with the highest value of p(t) · p(top-k|t). Since our estimate for this value based on the models is expected to be a monotonically decreasing step function in each list, we can sort each list by increasing rank, and select chunks of postings with equal value from the beginning of
2Available at https://code.google.com/p/mitlm/

the lists until the budget is exhausted using a heap to decide from which list to pick.
We repeat this greedy selection process for pairwise postings. Since there is a huge number of term pairs, we first restrict the space by only considering intersections for terms t1 and t2 with p(t1, t2)   for some small . Then these intersections are created and sorted by impact, and we again select chunks of postings from the beginning of the intersections based on our estimates of p(t1, t2) · p(top-k|t1, t2), until the budget is exhausted.
All structures in the first layer are kept sorted from highest to lowest impact score. The single-term postings are of the form (docID, impact), while the term-pair posting layout is (docID, impactt1 , impactt2 ). Note that for single-term structures, we do not remove the postings in the first layer from the second layer, but create a copy of the postings, so this uses extra space. The reason is that we only access a limited amount of the first-layer structure, and thus we need to make sure that a lookup into the second layer can retrieve all postings. An exception are very short lists, of length less than 100, where we always move the entire list into the first layer and access it fully on any query containing the term; thus, these lists do not increase space usage (though their overall size is small). We also added an additional rule that limited the depth of any selected single-term and pairwise structure to the maximum access depth for queries, typically several thousand postings, as any posting deeper than the access depth would never be used anyway.
3.4 Query Processing
We now describe the steps involved in query processing: the online greedy depth selection, the query processor, the second-layer lookups, and the final selection of candidates.
Online Greedy Depth Selection: Given a query, we first identify all relevant structures available in the first layer. This usually includes all single-term structures for the query, since our language model assigns non-zero probabilities to all terms and the first postings tend to have very high values of p(top-k|t), especially for short lists. Only some of the pairwise structures will typically be available for a query.
For each query, we have a cost budget that determines how much of the relevant structures we can access. For example, we might have a budget b = 1000, meaning that we can only access a total of b postings from the structures. Then for each structure we select a (possibly empty) prefix of postings. This is done greedily using a heap, as during index building, except that we select chunks of postings based on p(top-k|t) and p(top-k|t1, t2), respectively, without multiplying by p(t) and p(t1, t2) (since at this point the query already contains the terms). We note that we could perform various refinements to this approach, by charging different costs for pairwise versus single-term postings, or assigning different budgets to queries based on their difficulty.
Query Processor: We now run a fast and simple query processor on the selected structures and their corresponding depths. This processor copies the selected prefixes of the first-layer structures into an array and then runs a fast Radix Sort to sort postings by docID. A subsequent scan then aggregates the impact scores for each docID, and creates a bit filter for each docID stating which terms may require lookups into the second layer. During the scan we also filter redundant lookups as follows: Suppose a posting with docID 7 and impact 2.9 was found in the prefix of the list

299

for "cat", and that we have also accessed all pairwise postings for the pair "cat dog" with score 2.1 or higher. Then we do not have to perform a lookup into the "dog" list in the second layer for docID 7 ­ if such a posting existed we would have seen it as part of a pairwise posting.
We initially implemented a TAAT query processor using a hash table for the accumulators. However, we found that the sorting-based approach was much faster, by a factor of 2 to 3. Such a sorting-based approach is possible because we fix the access depth for each structure at the start of the query.
Second Layer Lookups and Final Candidate Selection: Next, we check the candidates and their accumulated scores, where most of these scores are partial, and many may have been only seen for one of the query terms. We now decide for which of these candidates we should perform lookups into the second layer to complete their scores, subject to a budget on the number of lookups (say, a few thousand per query). This is done using the accumulated partial score, as this provides a strong signal for relevance. We select the candidates with the highest partial scores, by running a randomized approximate selection algorithm where we first draw a sample of the impact scores, sort this sample, pick a suitable threshold from the sample, and then keep all candidates with impact above this score. Finally, we perform all necessary lookups for these candidates into the second layer, and keep the c candidates with highest completed BM25 scores, to be submitted to CF .

4. EXPERIMENTAL SETUP
In this section, we describe the data sets, ranking models, evaluation metrics, and setup of our experiments.
Datasets: All our experiments were conducted on the ClueWeb09B collection, which consists of 50, 220, 423 documents, 86, 532, 822 distinct terms, and 17 billion postings. For evaluation, we used the TREC 2009 Million Query Track (40k), which we refer to as Million09. Our training set for the modeling step includes 30k queries selected at random from Million09, while the testing set for performance evaluation consists of 3k from the remaining 10k queries. Table 1 shows the query lengths for the testing query set.

Query length 2

3 4 5

# queries 1408 954 517 121

Table 1: Query length distribution for the 3k testing queries.
The TREC 2010 to 2012 Web Track topics (150) were used for training the machine-learned complex ranker CF . For the language models, we used linear interpolation of a model for the training queries of the query set with a model for a randomly selected sample of 1.5 million documents, using the MIT Language Modeling (MITLM) toolkit.
Ranking models: We selected the BM25 ranking scheme as our first ranker, as it is widely used as a simple ranker in the literature and satisfies the desired properties of being both computationally fast and providing a reasonable set of initial candidates.
Recent studies [31, 51] show that ranking schemes obtained using learning-to-rank methods with dozens or hundreds of features outperform traditional bag-of-words models in terms of quality. There are a number of learningto-rank tools that are available. We decided to use LambdaMart [53] to learn our complex ranker CF , as it is consid-

ered one of the most effective learning-to-rank models [22, 35]. We trained on the 150 queries from TREC 2010 to 2012 based on standard features from the literature [5, 36, 49]. Table 2 lists a subset of these features. The anchor text features were extracted using the data from [18], while the spam and pagerank values are from [16]. The distribution feature refers to the dispersion of term occurrences across a specific document, when the document is split into pieces of fixed size, say 100 terms.
Evaluation metrics: The main aspects in the design of a scalable web search architecture include quality, time, and space. Thus, we evaluated the proposed framework on these three aspects. We measure the end-to-end quality of the proposed methods with Overlap@(k,c) and NDCG@k. In particular, the end-to-end effectiveness evaluation within the cascading ranker setup is performed as follows. In the first cascade, the top-c documents are obtained based on our method and then, in the second cascade, the CF is applied to these documents in order to return the final top-k.
For the effectiveness of the first cascade, we measure Overlap@(k,c) as the fraction of top-k documents obtained when applying CF to all top 2000 results of a safe disjunctive BM25, that are also found among the c candidates computed by our algorithm. While it was not feasible to apply the complex ranker to all documents in the union of query terms, we found in preliminary experiments that beyond the top 2000 there was little change in the final top-k. This choice is also directly supported by the recommendations in [35]. Thus, an overlap of 1.0 for a query means that all top-k results were found among the c candidates of our method. We use k = 10 unless stated otherwise.
We evaluated the speed of our methods using average query latency (in milliseconds) for generating the candidates. That is, we measure the time elapsed from when a query arrives until the time when the top-c candidates are ready to be evaluated by CF . (We do not count the time for applying CF as it is the same for all methods.) The space overhead was measured as the percentage of a baseline full index.
Index: The second layer was indexed and compressed using a version of PForDelta [59] proposed in [55]. The algorithms were implemented using C++ and compiled using gcc with -O3 optimization. The experiments were conducted on a single core of a 2.27Ghz Intel Xeon (E5520) CPU. All data structures and indexes are memory-resident.
Parameters: Overall, the proposed framework utilizes the following parameters: (a) the access cost budget, i.e., the number of postings to access per query, (b) the number of documents to perform lookups on, (c) the number of candidates to forward to CF , c, and (d) the space budget, i.e., the amount of additional space beyond a standard index. During selection of the term-pair structures in the first layer, we considered only pairs with probability (according to the language model of MITLM) at least 1.99  10-16 for the Million09 query trace. Next, we present the experimental evaluation of our approach based on these parameters.
5. EXPERIMENTAL EVALUATION
In this section, we present the experimental evaluation of our methods in terms of effectiveness, efficiency, and space.
5.1 Without Space Constraints
In the first experiment, we evaluate the proposed candi-

300

Textual Positional Query-based Dependency Models Document-based

BM25, language model, anchor text language model [36] absolute first position, relative first position, distribution
query size, fraction of numbers in query, list length language model of sequential and full dependent query terms [36] document length, document bytes, url length, url nesting, # outlinks, position in alexa, spam and pagerank [16]

Table 2: Categories of features utilized for CF training.

date generation algorithm under the assumption that there is no space budget; i.e., all possible first layer single-term and pairwise structures are available at query time. Although this scenario is unrealistic, as the space overhead of the pairwise structures can be very large, it shows the potential of the proposed method.
We compare our method against a na¨ive baseline which, given a cost budget b, accesses all relevant structures of the first layer at equal depth. Thus, if there are 5 available structures for a specific query, and the cost budget is 5k postings, the baseline would access the first 1k postings in each structure. Note that this approach is similar to the Fixed method proposed in [48]. We evaluate three versions of this na¨ive approach, for the cases where only single-term, only pairwise, and both types of structures are available, to show that both types of structures give benefit. Moreover, we implemented a clairvoyant selection algorithm that knows apriori which of the docIDs in the postings in the first layer result in top-k results, and then selects prefixes of the structures in an optimal way, thus giving an upper bound on the quality that can be achieved with any depth-limited access scheme on impact-sorted structures. This algorithm is included to observe how close to optimal our algorithm is.
Setup: We assume unlimited space budget and use the following parameter settings and selection strategies: c is 500, the algorithms exhaustively perform lookups on all docIDs seen in the accesses of the first-layer structures (thus the number of lookups is only bounded by the access depth), and the c candidates are selected based on highest BM25.
Effectiveness: Figure 2 shows Overlap@(500, 10), i.e., the fraction of correct top-10 results preserved within the c = 500 candidates, as the access cost budget varies from 500 to 20000, and with the first layer consisting of different structures. Obviously, the clairvoyant algorithm achieves the best quality for all access budgets, as it is as an upper bound of our method. On the other hand, we observe that for the na¨ive method, having only pairwise structures consistently outperforms having only single-term ones, but having both achieves the best quality, which is 0.8864 when the cost budget is 2000. The Greedy selection algorithm outperforms all na¨ive methods and achieves quality close to the optimal clairvoyant one, even with moderate access budget. For instance, the quality is 0.946 for the 2000 access budget. Thus, more than 94% of the same top-10 results are returned. Finally, as the cost budget increases, quality increases for all algorithms, since we consider more postings as candidates. In particular, Greedy achieves quality really close to Clairvoyant for the 5000 access budget.
Efficiency: In Table 3, we report the average query processing time in milliseconds of the Greedy algorithm when the access budget varies from 500 to 20000. We observed that the access budget is a very good proxy for query processing time in the case of unlimited lookups, and thus we only report the time for Greedy; the numbers for the na¨ive and clairvoyant algorithms with the same access budget are very similar. According to Table 3, we see that it is possible

Figure 2: Effectiveness of the first-phase selection algorithms at various access cost budgets on Million09 queries, assuming no limit on space.
to achieve overlap close to 0.946 for access budget 2000, in 0.51ms. As the access budget increases, query processing becomes slower, as we process more postings. Thus, both quality and speed are very good when there is no space constraint. Next, we evaluate if comparable numbers can be obtained with limited space.
Access budget 500 1000 2000 5000 10000 20000 avg qp (ms) 0.22 0.33 0.51 0.95 1.6 2.79
Table 3: Efficiency of the Greedy algorithm with various access cost budgets on Million09 queries, assuming no limit on space.
5.2 With Space Constraints
In the next experiment, we drop the assumption of unlimited space budget and focus on more realistic scenarios. More specifically, we allow a specific percent of space overhead for the first-layer structures over the full second-layer index. Given various space budgets, we evaluate the Greedy algorithm in terms of quality and speed. In this experiment, we use the setup and parameters of the previous experiment, and assume that the first layer consists of both single-term and pairwise structures. We still do all lookups to complete the partial scores of all docIDs seen in the first layer.
Effectiveness: In Table 4, we present the effectiveness of the Greedy algorithm (measured by Overlap@(500, 10)) when a specific percentage of space overhead is allowed for the first layer structures, and the access cost budget is 2000 and 5000. For the single-term structures, we decided to keep up to 2000 and 5000 postings of every list, which correspond to 7.1% and 9.9% of the full index, respectively. On top of this small fixed space overhead, we have a limited space budget for pairwise structures that is allocated according to the Greedy allocation algorithm. The reported space in Table 4 includes only the pairwise structures in the first layer. First, we observe that our proposed method works quite well even with limited space budget, with moderate quality loss. (We look at NDCG numbers later.) Increasing

301

the space budget of pairwise structures to more than 50% does not seem to provide significant quality gains unless a lot of space is available. When the access cost is larger, again better quality is achieved as more candidates are considered.

Space 0.1

0.3

0.5

0.7

1



2000 0.8093 0.8359 0.8412 0.8426 0.8428 0.946

5000 0.8717 0.895 0.9003 0.9028 0.9037 0.9755

Table 4: Effectiveness of the Greedy algorithm (Overlap@(500, 10)) at various space budget setups, for 2000 and 5000 access budgets on the Million09 query trace.

A space overhead of 57.1% (7.1% singles and 50% pairs) for 2000 access budget, and 59.9% for 5000 is acceptable given the significant performance speedup that we achieve. For example, in the Maguro system [42], a 20x index size increase is justified for a 3x performance improvement.
Efficiency: Table 5 shows the average query processing time of the Greedy algorithm for various space budget, when access budget is 2000 or 5000. As mentioned before, the access budget provides a reliable proxy for performance and this is evident in Table 5. The Greedy algorithm requires on average 0.551ms and 1.055ms, for 2000 and 5000 access budget, respectively, still assuming no limit on lookups. As the space budget increases, the performance for both access budgets becomes better. The reason is that more high quality postings appear in the pairwise structures, which results in fewer lookups for the missing terms.

Space budget 0.1 0.3 0.5 0.7

1



2000

0.581 0.56 0.558 0.556 0.555 0.5

5000

1.125 1.08 1.065 1.055 1.049 0.957

Table 5: Efficiency of the Greedy algorithm with varying space budget, for 2000 and 5000 access budgets on the Million09 query trace, measured in ms.

5.3 Lookup Selection Strategies

In the previous experiments, all algorithms exhaustively perform lookups in the second layer. However, as we will see, the lookup selection policy plays an important role in the performance. Thus, we now look at better lookup strategies.
First, we investigate how query processing costs are distributed. Table 6 shows the average time overhead of each part of the query processing cost for 5000 access budget and 3000 lookups. Recall that query processing includes the online greedy depth selection, the radix sort, the scan for aggregating scores and lookup pruning, selection by sampling, lookups into the second layer, and another selection to get the c candidates. From Table 6, it is obvious that lookups are a large part of the total cost, as they involve decompression and access to many random blocks.

Parts of Query Processing
Online greedy depth selection Radix-sort
Scan for merge/aggregate/filter Selection
Second-layer lookups Second selection Total

time (in ms)
0.0070 0.1539 0.1049 0.0311 0.5394 0.0275 0.8638

Table 6: Query processing time breakdown.

Instead of exhaustively performing all lookups, we keep only a certain number m of candidates for lookups, based on partial BM25 score. To do this, we perform a randomized approximate selection as described earlier. We evaluate the

quality and the speed of this lookup strategy for access budgets of 2000 and 5000 and a 0.5 pairwise space budget on top of the single-term structures of each access budget. Figure 3 presents the Overlap@(500, 10) and the average query processing time for several configurations of m lookups, under both access budgets. More specifically, we allow lookups of {500, 1k, 2k, 3k, 5k} candidates for access budget 5000, and {500, 750, 1k, 1.5k, 2k} for access budget 2000 (from left to right). We see that better performance can be achieved with the proposed lookup method at the cost of some quality loss, and thus there is a trade-off between quality and time.
Figure 3: Effectiveness/Efficiency evaluation of our lookup strategy on Million09 queries.
Next, we compare the speed of our approach with other recently published methods. While top-10 query processing can be done in a few milliseconds [19], selecting the top 500 is much more expensive for most methods. Table 7 presents running times for different top-500 candidate generation methods. We implemented all methods in C++ and ran them on the ClueWeb09B data set. For safe methods, we pick BMW-LB, the fastest disjunctive method to our knowledge from [19], and BMA, the fastest conjunctive method from [21]. For unsafe methods, we implemented the best approach from [52], which is the tree-based Priority with pruning, and the best method from [43], BMW-CS, which uses 10% of the index as first layer. Our method with 5k3k setup is much faster, since it only evaluates at most 5k postings, which means that less than 5k documents for each query are evaluated, while other methods usually evaluate many more documents. Both our online and offline posting selection mechanism contributes significantly to its good performance. BMA has the second fastest speed, since it is a conjunctive algorithm that only evaluates a limited number of documents. BMW-CS performs BMW on the first layer, which is the top 10 percent of the full index based on impact score. BMW-CS is different from our approach as we use pairwise structures in addition to singles in the first layer, and they do not have a fixed access budget per query or the greedy depth selection techniques. Their speed is much slower than ours, while still faster than BMW-LB. Priority is slower than BMW-LB, for top-500. For space overhead, BMA and BMW-CS both use less than 5% of the index size to store the Max-Block index. The two tiers in BMW-CS are disjoint, so the first tier takes no extra space. BMW-LB takes about 25% to cache the LB index, while priority takes no extra space. Our method with 5k-3k setup takes 59.9% extra space. Note that commercial search engines are often willing to accept significant space overheads for relatively small improvements in speed[42].
We now evaluate the effectiveness of our methods. Table 8 shows the Overlap@(500, 10) for all the methods, and for

302

Method Paper Time (ms) Space

BMW-LB [19] 26.74
 25%

BMA [21] 15.12 <5%

BMW-CS [43] 19.31 <5%

Priority [52] 51.24 0%

5k-3k Our 0.863 59.9%

Table 7: Running times and space overhead of different approaches for top-500 documents retrieval.

our approach with three different setups. BMW-LB has the best result, and BMA is almost as good, but slightly worse. This means that conjunctive query processing is almost as good as disjunctive in terms of quality, which was also shown in [5]. In Table 9, we rerank the results of all the methods according to the complex ranker. BMW-LB+CF is much better than BMW-LB, which means that the complex ranker performs well on identifying more relevant results. The 5k3k setup achieves consistently the best quality among our methods at all cutoff levels, and it's also slightly better than the other two unsafe methods for both Overlap and NDCG. Overall, we see that under NDCG, our approach can very quickly, in less than a millisecond, identify results that are almost as good as a safe disjunctive approach (BMW-LB).
BMW-LB BMA BMW-CS Priority 5k-3k 2k-2k 2k-500 0.985 0.934 0.879 0.826 0.881 0.841 0.729
Table 8: Overlap@(500,10) for different methods.

Method / cutoff 1

5 10 20 100 200

BMW-LB+CF 0.267 0.253 0.259 0.252 0.275 0.292

BMA+CF 0.261 0.253 0.253 0.247 0.270 0.290

BMW-CS+CF 0.249 0.241 0.236 0.222 0.245 0.270

Priority+CF 0.261 0.242 0.239 0.243 0.252 0.269

BMW-LB 0.143 0.162 0.161 0.174 0.225 0.256

5k-3k+CF 0.266 0.256 0.256 0.246 0.269 0.289

2k-2k+CF 0.266 0.249 0.245 0.238 0.260 0.284

2k-5k+CF 0.254 0.250 0.241 0.228 0.242 0.261

Table 9: NDCG at various cutoff levels on Million09.

5.4 Varying Query Length and Candidates
Next, we test the impact of query length on speed and quality, using various configurations of our methods. Table 10 presents the average query processing time (ms) when varying the query length, whereas Table 11 reports the corresponding Overlap@(500, 10) for each configuration. As query length increases, the query processing time also increases, since more lookups into more structures are performed. On the other hand, quality decreases, because top results in longer queries tend to be deeper inside the impactsorted list (a basic fact in top-k query processing shown in Fagin's theoretical analysis of his algorithms [23]).

Setup 2

3

4

5  6 avg

2k-500 0.178 0.297 0.477 0.732 0.965 0.295

2k-2k 0.321 0.573 0.914 1.349 1.809 0.558

5k-3k 0.531 0.929 1.461 2.115 2.878 0.863

Table 10: Efficiency when varying query length on the Million09 queries with various setups, measured in ms.

Table 12 shows the impact of the number of candidates returned by our candidate generation algorithm (cutoff c) on Overlap@(c, 10) for the 5000-3000 setup. As the cutoff c increases, the quality increases, untill flattening around 500. This justifies our choice of c = 500 throughout the paper.

Setup 2

3

4

5 6

2k-500 0.803 0.702 0.632 0.589 0.612

2k-2k 0.885 0.832 0.737 0.726 0.729

5k-3k 0.908 0.883 0.824 0.835 0.81

Table 11: Effectiveness in Overlap@(500, 10) when we vary query length on Million09 queries in various setups.

c

100 200 300 400 500 800 1200

Overlap@(c, 10) 0.539 0.696 0.791 0.856 0.881 0.883 0.885

Table 12: Effectiveness when we vary candidates c.

6. CONCLUSIONS AND FUTURE WORK
In this paper, we have proposed a fast first-phase candidate generation approach for cascading ranking architectures. Our framework builds an auxiliary layer of index structures (single-term and pairwise structures) based on models for query term frequency and posting quality, which is then selectively accessed at query time based on a cost budget and using early termination techniques. The experimental evaluation shows that the proposed framework can find candidates about an order of magnitude faster than conjunctive or disjunctive top-k computations, with little loss in quality.
Future work includes the addition of specialized structures for phrases and proximity into our framework. We also expect some improvements from further optimization of the query processor and lookup mechanism, e.g., by adding bit vectors, or Bloom filters as in [4], or by using more complex rules to decide which lookups to perform (possibly based on ideas similar to [52]).
Acknowledgement
This research was supported by NSF Grant IIS-1117829 "Efficient Query Processing in Large Search Engines", and by a grant from Google.
7. REFERENCES
[1] D. Agarwal and M. Gurevich. Fast top-k retrieval for model based recommendation. In Proc. of the Fifth Int. Conf. on Web Search and Data Mining, pages 483­492, 2012.
[2] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proc. of the 29th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2006.
[3] N. Asadi. Multi-Stage Search Architectures for Streaming Documents. PhD thesis, University of Maryland, 2013.
[4] N. Asadi and J. Lin. Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters. In Proc. of the 21st ACM Conf. Information and Knowledge Management, 2012.
[5] N. Asadi and J. Lin. Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures. In Proc. of the 36th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2013.
[6] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval. ACM Press / Addison-Wesley, 1999.
[7] H. Bast, D. Majumdar, R. Schenkel, M. Theobald, and G. Weikum. IO-Top-K: Index-access optimized top-k query processing. In Proc. of the 32th Int. Conf. on Very Large Data Bases, 2006.
[8] R. Blanco and A. Barreiro. Probabilistic static pruning of inverted files. ACM Transactions on Information Systems, 28(1), Jan. 2010.
[9] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer Networks, 30(1-7):107­117, 1998.
[10] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Y. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. of the 12th ACM Conf. on Information and Knowledge Management, 2003.

303

[11] A. Broschart and R. Schenkel. High-performance processing of text queries with tunable pruned term and term pair indexes. ACM Trans. Inf. Syst., 30(1):5, 2012.
[12] S. Bu¨ttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In Proc. of the 15th ACM Conf. Information and Knowledge Management, 2006.
[13] B. B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. Early exit optimizations for additive machine learned ranking systems. In Proc. of the Third Int. Conf. on Web Search and Data Mining, 2010.
[14] K. Chakrabarti, S. Chaudhuri, and V. Ganti. Interval-based pruning for top-k processing over compressed lists. In Proc. of the 27th Int. Conf. on Data Engineering, 2011.
[15] S. Chaudhuri, K. W. Church, A. C. K¨onig, and L. Sui. Heavy-tailed distributions and multi-keyword queries. In Proc. of the 30th Annual Int. ACM SIGIR Conf, 2007.
[16] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5):441­465, 2011.
[17] V. Dang, M. Bendersky, and W. B. Croft. Two-stage learning to rank for information retrieval. In Proc. of the 35th European Conf. on Information Retrieval, 2013.
[18] V. Dang and W. B. Croft. Query reformulation using anchor text. In Proc. of the Third Int. Conf. on Web Search and Data Mining, pages 41­50, 2010.
[19] C. Dimopoulos, S. Nepomnyachiy, and T. Suel. A candidate filtering mechanism for fast top-k query processing on modern cpus. In Proc. of the 36th Int. ACM SIGIR Conf, 2013.
[20] C. Dimopoulos, S. Nepomnyachiy, and T. Suel. Optimizing top-k document retrieval strategies for block-max indexes. In Proc. of the Sixth Int. Conf. on Web Search and Data Mining, 2013.
[21] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In Proc. of the 34th Int. ACM SIGIR Conf, 2011.
[22] P. Donmez, K. M. Svore, and C. J. C. Burges. On the local optimality of lambdarank. In Proc. of the 32th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2009.
[23] R. Fagin. Combining fuzzy information: an overview. SIGMOD Record, 31:2002, 2002.
[24] R. Fagin, D. Carmel, D. Cohen, E. Farchi, M. Herscovici, Y. Maarek, and A. Soffer. Static index pruning for information retrieval systems. In Proc. of the 24th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001.
[25] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data. ACM Trans. Inf. Syst., 24(1):51­78, 2006.
[26] M. Fontoura, M. Gurevich, V. Josifovski, and S. Vassilvitskii. Efficiently encoding term co-occurrences in inverted indexes. In Proc. of the 20th ACM Conf. Information and Knowledge Management, 2011.
[27] S. Goel, J. Langford, and A. L. Strehl. Predictive indexing for fast search. In Proc. of the 22nd Annual Conf. on Neural Information Processing Systems, Vancouver, 2008.
[28] S. Hwang and K. C. Chang. Optimizing top-k queries for middleware access: A unified cost-based approach. ACM Trans. Database Syst., 32(1):5, 2007.
[29] I. F. Ilyas, G. Beskales, and M. A. Soliman. A survey of top-k query processing techniques in relational database systems. ACM Comput. Surv., 40(4), 2008.
[30] R. Kumar, K. Punera, T. Suel, and S. Vassilvitskii. Top-k aggregation using intersections of ranked inputs. In Proc. of the Second Int. Conf. on Web Search and Data Mining, 2009.
[31] H. Li. Learning to Rank for Information Retrieval and Natural Language Processing. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, 2011.
[32] T. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.
[33] X. Long and T. Suel. Optimized query execution in large search engines with global page ordering. In Proc. of the 29th Int. Conf. on Very Large Data Bases, 2003.
[34] X. Long and T. Suel. Three-level caching for efficient query processing in large web search engines. Proc. of the 15th Int. Conf. on World Wide Web, 9(4), 2006.
[35] C. Macdonald, R. L. T. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.

[36] D. Metzler. Automatic feature selection in the markov random field model for information retrieval. In Proc. of the 16th ACM Conf. Information and Knowledge Management, 2007.
[37] A. Ntoulas and J. Cho. Pruning policies for two-tiered inverted index with correctness guarantee. In Proc. of the 30th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2007.
[38] R. Ozcan, I. S. Alting¨ovde, B. B. Cambazoglu, F. P. Junqueira, and O¨ . Ulusoy. A five-level static cache architecture for web search engines. Inf. Process. Manage., 48(5):828­840, 2012.
[39] M. Persin, J. Zobel, and R. Sacks-davis. Filtered document retrieval with frequency-sorted indexes. Journal of the American Society for Information Science, 47:749­764, 1996.
[40] T. Qin, T. Liu, J. Xu, and H. Li. LETOR: A benchmark collection for research on learning to rank for information retrieval. Inf. Retr., 13(4):346­374, 2010.
[41] K. Risvik, Y. Aasheim, and M. Lidal. Multi-tier architecture for web search engines. In 1st LA Web Congress, 2003.
[42] K. M. Risvik, T. M. Chilimbi, H. Tan, K. Kalyanaraman, and C. Anderson. Maguro, a system for indexing and searching over very large text collections. In Proc. of the Sixth Int. Conf. on Web Search and Data Mining, 2013.
[43] C. Rossi, E. S. de Moura, A. L. Carvalho, and A. S. da Silva. Fast document-at-a-time query processing using two-tier indexes. In Proc. of the 36th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2013.
[44] R. Schenkel, A. Broschart, S. Hwang, M. Theobald, and G. Weikum. Efficient text proximity search. In Proc. of the 14th Int. Symposium on String Processing and Information Retrieval, 2007.
[45] F. Scholer, H. E. Williams, J. Yiannis, and J. Zobel. Compression of inverted indexes for fast query evaluation. In Proc. of the 25th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2002.
[46] G. Skobeltsyn, F. Junqueira, V. Plachouras, and R. A. Baeza-Yates. Resin: a combination of results caching and index pruning for high-performance web search engines. In Proc. of the 31th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2008.
[47] T. Strohman and W. B. Croft. Efficient document retrieval in main memory. In Proc. of the 30th Annual Int. ACM SIGIR Conf, 2007.
[48] T. Strohman, H. R. Turtle, and W. B. Croft. Optimization strategies for complex queries. In Proc. of the 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005.
[49] N. Tonellotto, C. Macdonald, and I. Ounis. Efficient and effective retrieval using selective pruning. In Proc. of the Sixth Int. Conf. on Web Search and Data Mining, 2013.
[50] H. R. Turtle and J. Flood. Query evaluation: Strategies and optimizations. Inf. Processing and Management, 31(6):831­850, 1995.
[51] L. Wang, J. J. Lin, and D. Metzler. A cascade ranking model for efficient ranked retrieval. In Proc. of the 34th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2011.
[52] H. Wu and H. Fang. Document prioritization for scalable query processing. In Proc. of the 23rd ACM Conf. Information and Knowledge Management, 2014.
[53] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Adapting boosting for information retrieval measures. Inf. Retr., 13(3):254­270, 2010.
[54] H. Yan, S. Shi, F. Zhang, T. Suel, and J. Wen. Efficient term proximity search with term-pair indexes. In Proc. of the 19th ACM Conf. Information and Knowledge Management, 2010.
[55] J. Zhang, X. Long, and T. Suel. Performance of compressed inverted list caching in search engines. In Proc. of the 17th Int. Conf. on World Wide Web, 2008.
[56] M. Zhu, S. Shi, M. Li, and J. Wen. Effective top-k computation in retrieving structured documents with term-proximity support. In Proc. of the 16th ACM Conf. Information and Knowledge Management, 2007.
[57] M. Zhu, S. Shi, N. Yu, and J. Wen. Can phrase indexing help to process non-phrase queries? In Proc. of the 17th ACM Conf. Information and Knowledge Management, 2008.
[58] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comput. Surveys, 38(2), 2006.
[59] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-scalar RAM-CPU cache compression. In Proc. of the 22th Int. Conf. on Data Engineering, 2006.

304

Understanding Information Need: an fMRI Study

Yashar Moshfeghi
School of Computing Science University of Glasgow Glasgow, UK
Yashar.Moshfeghi@ glasgow.ac.uk

Peter Triantafillou
School of Computing Science University of Glasgow Glasgow, UK
Peter.Triantafillou@ glasgow.ac.uk

Frank E. Pollick
School of Psychology University of Glasgow
Glasgow, UK
Frank.Pollick@ glasgow.ac.uk

ABSTRACT
The raison d'etre of IR is to satisfy human information need. But, do we really understand information need? Despite advances in the past few decades in both the IR and relevant scientific communities, this question is largely unanswered. We do not really understand how an information need emerges and how it is physically manifested. Information need refers to a complex concept: at the very initial state of the phenomenon (i.e. at a visceral level), even the searcher may not be aware of its existence. This renders the measuring of this concept (using traditional behaviour studies) nearly impossible. In this paper, we investigate the connection between an information need and brain activity. Using functional Magnetic Resonance Imaging (fMRI), we measured the brain activity of twenty four participants while they performed a Question Answering (Q/A) Task, where the questions were carefully selected and developed from TREC-8 and TREC 2001 Q/A Track. The results of this experiment revealed a distributed network of brain regions commonly associated with activities related to information need and retrieval and differing brain activity in processing scenarios when participants knew the answer to a given question and when they did not and needed to search. We believe our study and conclusions constitute an important step in unravelling the nature of information need and therefore better satisfying it.
Keywords: Anomalous States of Knowledge, Information Need, Information Retrieval, fMRI Study
1. INTRODUCTION
The main goal of Information Retrieval (IR) systems is to satisfy searchers' information need (IN). Given the core and fundamental role IN plays in an information seeking and retrieval process, over the last several decades much research has been dedicated to better understand this concept in both
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '16, July 17­21, 2016, Pisa, Italy.
c 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4069-4/16/07.. DOI: http://dx.doi.org/10.1145/2911451.2911534

information retrieval and other relevant scientific communities. As a result of such research, seminal theories, models, and findings have been published, shaping the foundations of current IR systems. A few examples of such influential works are Wilson's Information Seeking Behaviour model [46], Kuhlthau's Information Seeking Process (ISP) model [28], Ingwersen's Cognitive IR Theory [21], and Belkin's Anomalous States of Knowledge (ASK) model [8]. These works are mainly based on behavioural studies of searchers while they engaged in an Information Retrieval and Seeking process, mainly through questionnaires/interviews [28], or by observing and studying searchers interaction with IR systems via their submitted queries and their reformulation [26], or via their interaction with retrieved results [44]. Despite their invaluable contributions, they have all investigated the phenomenon of IN indirectly, via some sort of mediator. Therefore, important research questions remain unanswered, such as:
· RQ1: "What is the nature of IN from a neuropsycology perspective?";
· RQ2: "Is there a clear, detectable, physical manifestation (i.e. neural correlate) of IN in human brains?";
· RQ3: "Can such manifestations be identified in an early stage of an information seeking and retrieval process?"; and
· RQ4: "Do such manifestations differ when an anomaly in the user's state of knowledge has been experienced? i.e., when searchers choose not to engage in a search process (Don't know ­ Don't search Scenario), compared to a scenario where they engage in a search process (Don't know ­ Do Search Scenario)?".
An answer to these questions will definitely improve our understanding and lead to robust definitions of the IN concept. And, fundamentally, it will play a key role in opening new doors to the design and implementations of novel IR techniques which will be enabled to better (and even proactively) satisfy searchers' need. The research described in this paper represents our efforts towards this direction. In particular, in this paper we focus on discovering and mapping the brain mechanisms of IN realisation, within an information retrieval process performed by humans engaged in a Question Answering (Q/A) retrieval task. Our central aims are to identify: (i) the brain regions associated with an IN realisation (in its earliest state) and (ii) contrast the scenarios where individuals engage in search or simply state the

335

need to search. We are focusing on the time period in which the brain exhibits the highest activity with regards to the process of IN realisation, from the moment of observing a question, i.e. recognising an ASK. The identification of the brain regions involved in an IN realisation (in particular in an early state) and observing the period of time in which there is a clear association of the activation of brain regions where there exists an Information Need, can be the basis to detect an IN in a Q/A task.
Our experiments rely on measuring Blood Oxygenation Level Dependent (BOLD) signals and the fact that BOLD signals can be analysed to detect significant brain activities in the process of realising an IN in a Q/A task. In particular, we aim to identify the time frame in which significant activity appears in users' brains, while they are realising an IN. In this spirit, this paper reports the results of an fMRI based user study on Q/A search. In particular, we have monitored, by an MRI device, the brain activities of twenty four participants engaged in a Q/A search task for a predefined set of questions with respect to a predefined set of relevant and non-relevant documents. In order to do so, we devised an experiment consisting of collecting data via a 3T MRI scanner in a lab-based user study, and analysed the collected data off-line. Our hypothesis is that there are brain regions in which the BOLD signals would be different for No-IN scenarios (i.e., "know the answer") versus IN scenarios (i.e., "don't know an answer and need to search"). Moreover, this difference would be sensitive to whether individuals engage in search or simply state the need to search.
The remainder of the paper is organised as follows: Section 2 presents related work and the background. Section 3 discusses at length the experimental methodology. Section 4 presents and analyses our results. Finally, Section 5 presents our key conclusions.
2. RELATED WORK
2.1 IN Complexity and the IN-Query Gap
IN is an essential concept and at the core of the information retrieval processes. When searchers realise an information need, they experience an anomaly in their current state of knowledge (ASK) [8]. As a result, search processes are initiated: Searchers transform their IN into a query and submit it to an IR system. In turn, the IR system retrieves potentially relevant documents, aiming to satisfy the IN. Subsequently, searchers evaluate retrieved documents, accumulating relevant information which leads them to satisfy their IN. Often, however, searchers are not satisfied with the results obtained in response to their initial query formulation [42], and thus must engage in further interaction with the system to resolve their need. Therein lies the complexity associated with the concept of IN.
The complexity of IN rests in its paradoxical nature [12]: Unlike other primary human need (e.g., physiological ones), it is often unknown to the individual beforehand what information is required to satisfy the IN. Research indicates that this is because an IN is "intangible and visceral" and therefore "unknowable and non-specifiable" ­ thus, how can one express an IN using a query to an IR system? [8, 12] Hence, the very nature of IN inherently makes it nearly impossible for searchers to correctly map their IN to a IR query [8, 9, 20, 12, 40]. This produces a gap between how an IN is represented (i.e. the formulated query) and the actual IN,

because the formulated query is not guaranteed to provide an exact description required to retrieve the relevant documents [45]. In other words, expressing an IN using a set of query keywords is considered to be uncertain and noisy [41], as it can only vaguely approximate the actual IN [40]. The problem becomes even more pronounced when an IN is ill-defined : i.e., when the searcher only knows "fringes of a gap in [his/her] knowledge" [12] making it extremely difficult for the searcher to identify and describe the IN [7, 8, 11]. Therefore, it is possible that a given query may not sufficiently define the characteristics of relevant documents, or even any relevant information, since a searcher cannot form an appropriate initial state from which to form a query [13].
Taylor's [40] classic four-level theory of IN theorised the complex nature of IN as follows: (i) a search begins with a process based on an area of doubt or a gap in understanding, which is a compromised expression of the need (Taylor calls this the Q4 level). The searcher, as a result of searching, can subsequently frame an IN, borrowing generic knowledge frames from adjacent areas in memory; at a certain point, a transformational event of information use leads to the IN being actualised, resulting in the information search becoming focused. This transformation in the use of information causes the searcher's Q4 compromised level of need to access deeper levels of the need, i.e. the formal expression of the need (Taylor calls this the Q3 level), the conscious "within brain" description of the need (Taylor calls this the Q2 level) and finally the deepest level of the need is the "visceral" (i.e., instinctual) level (Taylor calls the Q1 level) [40, 12]. What is of particular interest to the current paper are the lower levels in Taylor's conceptualisation of IN, namely the unconscious, visceral information need (which the user cannot know and therefore cannot specify to the IR system) leading to the "within brain" description of the IN. Ingwersen [21] in his Cognitive theory of IR explains that, based on the perspective of cognitive science, theories of information and empirical evidence, an appropriate approach to understanding IR is first to examine the mental formation of the information need and use this as the starting point for IR interaction. This formulation is a central issue in IR. This paper takes substantial motivation from this vein of research, arguing that IR systems that can detect and understand INs, starting from their Q1 levels, can better satisfy them.
2.2 Closing the IN-Query Gap
Typically, IR systems rely on a progressive disambiguation of the user's information need through an interactive and iterative process known as the relevance feedback cycle. Relevance feedback is central to the IR system's efforts to construct a better representation of the users' IN. Relevance feedback may be gathered through explicit [27], implicit [43], and/or affective feedback [2]. Explicit feedback is viewed as a robust method to improve retrieval effectiveness [27]. However, it is not always applicable due to the cognitive burden that it places on users [44]. Enter implicit feedback, in which relevance is inferred from the interactional data in an indirect and unobtrusive manner [22]. For example, researchers try to understand how task [43], dwell time [25] and clickthrough [22] relate to relevance. However, a problem occurs when actions are taken as an indication of relevance without sufficient evidence to support their effectiveness [36]. For example, Kelly and Belkin [24] show that the implicit feedback

336

measures based on user interaction with the full content of documents can often be unreliable, and difficult to measure or interpret. Recently, affective feedback has been proposed [2] which relies on capturing facial expression [5], eye tracking [30], and physiological signals [3, 33] (such as skin temperature) and uses them as implicit relevance judgements. However, these methods can only help researchers to understand the concept of relevance to a certain level and are not considered to be very effective. Implicit feedback IR systems, albeit noisy, can collect several distinct signals of the user's interests through the analysis of both user's actions and user generated contents [44]. Recently, affective and physiological features have been considered as a valid ground to define implicit feedback techniques [2, 35, 33]. For example, Arapakis et al., studied the role of emotions in information retrieval, and introduced a number of models [2, 4]. In their subsequent work, the authors have shown that emotional features can be effectively included in building implicit feedback systems [4], and they can also be used to personalise search [2]. Similarly, the work by Moshfeghi et al. demonstrated that, in addition to emotional features [34], physiological features can also be used to model relevance and they can also be used to predict task types [33].
2.3 Neuropsychology and IR
Recent research has begun to examine IR from a neuroscience perspective, using techniques of brain imaging to reveal the brain activity related to the underlying neural activity of a user. One particular area of emphasis to this research has been to examine the concept of relevance. Results of Moshfeghi et al. [35] showed, using fMRI, that it is possible to identify brain regions activated by the process of judging relevance of an image. The brain regions they reported included the inferior parietal lobe, inferior temporal gyrus and superior frontal gyrus; the activation of these regions during relevance assessment was significantly higher when evaluating relevant items.
Another example is a study conducted by Eugster et al. [17] that used EEG to show that the frequency content of the EEG signal as well as Event Related Potentials (ERPs) can be used effectively as a set of features to decode the relevance of a text. Similarly, Allegretti et al. [1] reported on EEG results that indicated that within 500 ms EEG signals begin to appear that differentiate between viewing a relevant and a non relevant image. Likewise, Kauppi et al. [23] used magnetoencephalography (MEG) to show that the frequency content of the MEG signal, along with eye movement data can be used for decoding relevance of images. These studies have used the relative strength of the different measurement techniques to make great progress and to indicate where in the brain relevance judgments are happening and what the time course is of these neural processes that determine relevance.
However, the above studies have not investigated the wider view of how IN emerges. In this paper we take an important step to understanding the neural processes involved with primary stages when an IN emerges.
3. EXPERIMENTAL METHODOLOGY
3.1 Research Questions and Hypothesis
This paper studies the concept of information need from a neuropsychological perspective by investigating brain activ-

ity during periods in which an information need was induced. In particular, we implemented two different scenarios to create information need, in both scenarios participants were asked a multiple choice question, but in the first scenario when participants confronted an ASK they only had the option to acknowledge that they needed to search, they could not act on this IN. In the second scenario when participants confronted an ASK they were able to engage in a search process. Our hypothesis was that there exists brain regions for which the activation levels are different depending on the state of information need and that the regions found for these two scenarios would provide measurement data of brain states to complement our theoretical understanding of IN.
Related to our hypotheses there are several considerations on the design and analysis of fMRI data [35, 14]. Of particular importance are several factors that were critical in guiding our research plans. Firstly, the fMRI scanning environment is restrictive in that a participant must lay supine with their head kept still, and that only limited response/interactive devices can be in this scanning environment without causing signal or safety issues. This constraint led to the use of multiple choice questions for a task since it was possible to provide response using an MRI-compatible button box. Another constraint is that while fMRI provides the ability to localise activity to within millimetres, the temporal resolution of fMRI ( the time to take a single measurement of the entire brain) is on the order of around 2 seconds. This relatively slow rate of data acquisition is compounded by the fact that the Blood Oxygenation Level Dependent (BOLD) signal measured is related to the underlying neural signal in a complex way that introduces further delays [18]. To address these delays it was necessary to time the events of the different scenarios at a rate that was compatible with our fMRI measurements. To achieve a suitable design for our questions about IN we adapted the methods used in related work in problem solving [47] which examined neural correlates of insight by comparing brain activity when a multiple choice response showed insight, to brain activity when a multiple choice response did not show insight.
3.2 Design
A "within-subjects" design was used in this study. The independent variable was the information need (with two levels: information need, and no information need), which was controlled by responding to questions viewed on the screen. The set of questions were designed so that averaged across all participants there would be an equal number of responses expressing an answer and expressing a need for search. The dependent variable was brain activity revealed by the BOLD signal.
3.3 Task
Each participant completed two different search scenarios. Question-Response (QR) Task: In the first scenario, which we term Question-Response (QR), participants were first presented with a question for 4 seconds, then for 4 seconds four possible responses were provided while the question stayed on the screen (Figure 1). Participants could not make a response until after the 4 seconds of observing the possible responses. This was done so that brain activity related to the motor response of pressing the button would not be contained in the model of brain activity, which only

337

considered these first 8 seconds. After the 8 seconds participants were able to respond and in this QR scenario the experiment progressed directly on to the next trial. Of the four possible responses, one was always the correct answer and one of them was always "need to search". The position of the four alternatives was randomised for each trial and the response given by pressing one of the four buttons available on the button box that each participant had in their right hand. The time to respond was left free so that participants were not under time pressure to respond. The order of the questions was randomised for each participant.
Figure 1: A schematic representation of Scenario 1
Question-Response-Search (QRS) Task: In the second scenario, which we term Question-Response-Search (QRS), participants performed the same task as in the QR scenario except that if the answer "need to search" was provided then they entered an additional stage where they formulated a search query (and submitted it verbally into a noise-cancelling microphone), received a document and evaluated this document. In the present study we do not investigate brain activity during the search and document evaluation periods. Instead we focus only on the brain activity in response to the the presentation of the question and the possible responses. The difference between the two scenarios is that in Scenario 1 participants do not engage in a search process, while in the second scenario they do engage in a search process. This is important from the conceptual stance that in Scenario 1 stating the need to search is the endpoint of the entire process, while for Scenario 2 it is a transition to the start of a search.
3.4 Question Answering Dataset
In order to perform the two task scenarios mentioned in Section 3.3, we created a Question Answering dataset1. To develop this standard set of questions, we used previous runs of TREC Q/A Track, in particular we carefully selected a set of 80 questions from the TREC-8 and TREC-2001 Question Answering Tracks - Main Task2. We chose these two Tracks since they were the first and last tracks where the questions presented there were (i) independent from one another, in contrast to other Tracks that share a relationship, and (ii) they also provided the correct answer to the questions.
We then manually examined all the questions presented in these two tracks and selected a subset of questions that (i) were not longer than one line, and (ii) the correct answer to the question was not longer than 5 words. This constraint is due to the limitation of presenting the questions and options to the participants in an fMRI settings. An additional constraint was that there were at least two relevant and non-relevant answers in their QRel. We then
1The Question Answering dataset is available upon request. 2For more information please visit http://trec.nist.gov/data/qa/ t8 qadata.html and http://trec.nist.gov/data/qa/2001 qadata/main task.html

removed the questions that were ambiguous or were time dependent, e.g. Who is the president of Stanford University? (TREC-8, Topic 51), making the answers provided in the Track not appropriate. The answers of all these questions were then checked by current search engines to make sure that the answers are still valid and correct. We also created two wrong answers for each question that were in the domain of the question, e.g. "What is supernova?" (TREC2001, Topic 1067) the correct answer is "An exploding star" and we created two other wrong answers i.e. "A newborn star" and "A dead star". We also made sure that the questions covered a wide range of topics, e.g. history, politics, science, etc. This was done in order to reduce any bias that might occur from emphasis of a particular type of question.
Over this set of questions, two annotators separately judged the difficulty of the questions (i.e. hard or easy) and then selected a subset of 80 questions where both annotators agreed upon their difficulties, i.e. 40 of them were hard and 40 were easy questions. For Scenario 1, 40 questions out of these 80 questions were selected where 20 were easy and 20 were hard questions. The remainder of the questions were used for Scenario 2. Since Scenario 2 was divided into two runs, additional care was made to further divide the questions across the runs so that they both had 10 easy and 10 hard questions covering a variety of topics. The goal of this procedure was to control the set of questions such that on average there was an equal chance of experiencing ASK and knowing the answer.
Another extra step for Scenario 2 was to prepare the documents that were shown to the subjects once they engaged in a search process. This took the form of simulating a snippet answer that is returned by current search engine such as Google when a question is submitted. For this purpose we selected two relevant and two non-relevant documents from QRel. The length of the answers provided in TREC8 and TREC 2001 were incompatible. In order to keep the size of the results consistent, for those answers that were too short, we found the original source file and selected sentences around the answer so that all snippets had the same length. The average length of the answers shown to the participants for first and second run of Scenario 2 were 39.47 words (SD of 3.33) and 39.65 words (SD of 3.285) respectively. This was done in order to reduce any potential confounding effect of snippet size on the brain activity results.
3.5 Procedure
This section outlines the flow of the study, from beginning to end. Ethical permission for the study was obtained from the Ethics Committee of the College of Science and Engineering, University of Glasgow. Participants were recruited from the participant database at Centre for Cognitive Neuroimaging, University of Glasgow. Participants were instructed of the duration of the experiment, which included approximately 50 minutes to perform all tasks examining information need, and approximately 10 minutes to obtain a scan of their anatomical structure. They were informed that they could leave at any point in time during the experiment and would still receive payment (the payment rate was £6/hr). They were then asked to sign a consent form. Before participating, participants underwent a safety check to guarantee that they did not possess any metal items inside or outside of their body, or any other contraindications for scanning, such as certain tattoo inks. They were

338

then provided with gear (similar to a training suit) to wear for the duration of the experiment to avoid interference from any metal objects in their clothes with the fMRI signal.
Next, as a training process they were given an example task and a corresponding set of example questions in order to familiarise themselves with the procedure. Once they had successfully completed their training task, participants entered the fMRI machine and the experimenter adjusted the settings of the machine to maximise their comfort and vision. While being scanned, each participant first participated in one run of the QR scenario, which contained 40 questions. They then participated in two separate runs of the QRS scenario, with each run comprised of 20 questions. Two runs were chosen to give the participants a further break to relax during the scanning and to prevent fatigue on the QRS task, which consumed more time. After the functional runs were complete the anatomical data of each participant was obtained.
After completion of scanning participants were asked to fill out an exit questionnaire that provided further demographic and qualitative descriptions of their experience during the experiment. They also filled out the Edinburgh handedness questionnaire [37] which provides evaluation of whether the participant was right-, left- or mixed-handed. Handedness information was obtained since lateralization of brain function is influenced by handedness and we wished to ensure that our sample of participants approximated the general population.
Apparatus: The images were presented using Presentation R software3, and projected using a LCD projector onto a translucent screen, while participants watched them in an angled mirror in the MRI scanner.
fMRI Data Acquisition: All fMRI data was collected using a 3T Tim Trio Siemens scanner and 32-channel head coil at the Centre for Cognitive Neuroimaging, University of Glasgow. A functional T2*-weighted MRI run was acquired for the single run of the QR scenario and the two runs of the QRS scenario (TR 2000ms; TE 30ms; 32 Slices; 3mm3 voxel; FOV of 210, imaging matrix of 70 × 70).
An anatomical scan was performed at the end of the scanning session that comprised a high-resolution T1-weighted anatomical scan using a 3D magnetisation prepared rapid acquisition gradient echo (ADNI- MPRAGE) T1-weighted sequence (192 slices; 1mm3 voxel; Sagittal Slice; TR = 1900ms; TE = 2.52; 256 × 256 image resolution).
Questionnaires: At the end of the experiment, the participants were introduced to an exit questionnaire, which gathered background and demographic information. It also enquired about previous experience with fMRI type user studies as well as participants general comments for the user study. Finally, it also included questions to ascertain participants' subjective experience of performing the experiment.
Pilot Studies: Prior to running the actual user study, a pilot study was performed using two participants to confirm that the process worked correctly and smoothly. A number of changes were made to the experimental paradigm based on feedback from the pilot study. After the pilot, it was determined that the participants were able to complete the user study without problems and that the system was correctly logging participants' interaction data.
3Presentation R software (Neurobehavioral systems, Inc.), http:// www.neurobs.com.

Agreement Level

4. RESULTS
A study with the procedure explained in Section 3.5 was conducted over 15 days from 7 December, 2015 to 22 December, 2015. Participants consisted of 24 healthy individuals with 11 males and 13 females. All participants were under the age of 44, with the largest group between the ages of 18-23 (54.1%) followed by a group between the ages of 3035 (20.8%). The handedness survey indicated that 79.1% were right-handed, 12.5% were left-handed and 8.33% were mixed-handed. Participants tended to have a postgraduate degree (20.8%), bachelors (33.33%) or other qualifications (45.8%). They were primarily students (54.1%), though there were a number of individuals who were self-employed (20.8%), not employed (4.16%) or employed by a company or organisation (20.8%). Participants were primarily native speakers (79.1%) or had an advanced level of English (20.8%). They all had experience in searching, with an average of 11.66 years (SD of 3.58) experience.
Task Perception: At the end of the procedure an exit questionnaire was performed that included questions about participants' overall subjective experience of performing the tasks. These questions specifically addressed participants' perception of their performed tasks in terms of the difficulty of the task, the familiarity of the participant with the task and the extent to which they found the task stressful, clear, successful, and satisfactory. Namely, participants were given the following questions "The tasks we asked you to perform were [easy/stressful/familiar/clear/Satisfactory] (answer: 1: "Strongly Disagree", 2: "Disagree", 3: "Neutral", 4: "Agree", 5: "Strongly Agree")". Descriptive statistics of these responses are shown by box plots in Figure 2, which show five key statistics: the minimum, first, second (median), third, and maximum quartiles.4. These results indicate that participants found the tasks difficult (not easy) and stressful, familiar, clear, successful, and satisfactory.
The tasks perfomed were

5

4

3

2

1

Easy

Stressful Familiar

Clear Successful Satisfactory

Figure 2: Box plot of the task perception based on the information
gathered from the questionnaires of 24 participants. The red diamond
represents the mean value.

Log Analysis: The fMRI analysis for both Scenario 1 and Scenario 2 relied upon a participant's response of the question to code whether a trial was IN or No-IN. This raised two considerations: firstly, for examination of brain data within a scenario it is important to have approximately an equal response rate for IN and No-IN responses. Secondly, comparison across scenarios is strengthened by having similar response rates since this argues against the possibility
4Further information can be found in [31].

339

that any difference found was due to simply response bias. Thus, it was important to examine the average response rates for whether the number of IN and No-IN responses were approximately balanced for each scenario and whether there was a difference between scenarios. In Scenario 1 the average number of IN responses was 18.45 (SD of 4.59) and the average number of No-IN responses was 21.42 (SD of 4.41). A paired t-test revealed that there was no difference between the type of responses (p-value = 0.12). In Scenario 2 the average number of IN responses was 17.5 (SD of 5.91) and the average number of No-IN responses was 22.5 (SD of 5.91). A paired t-test revealed a marginal difference between the type of responses (p-value = 0.05). An examination of the response rates across scenarios using paired t-tests revealed no significant difference between scenarios for either IN responses (p-value = 0.24) or No-IN responses (p-value = 0.19).
fMRI Data Preprocessing: The fMRI data were preprocessed using Brain Voyager QX5. A standard pipeline of pre-processing of the data was performed for each participant [19]. This involved slice scan time correction using trilinear interpolation based on information about the TR and the order of slice scanning. Three-dimensional motion correction was performed to detect and correct for small head movements by spatial alignment of all the volumes of a participant to the first volume by rigid body transformations. In addition, linear trends in the data were removed and high pass filtering with a cutoff of 0.0025 Hz performed to reduce artefact from low frequency physiological noise. The functional data were then coregistered with the anatomic data and spatially normalised into the common Talairach space [39]. Finally, the functional data of each individual underwent spatial smoothing using a Gaussian kernel of 6mm to facilitate analysis of group data.
4.1 General Linear Model (GLM) Analysis
Analysis began with a first-level analysis on the data of individual participants using multiple linear regression of the BOLD-response time course in each voxel, using two predictors for the different Response Type (respond need to search, respond with answer). To achieve this, for each participant's data a BrainVoyager protocol file (PRT) was derived that represented the onset and duration of the 8s total time that the question (4s) and question and possible responses (4s) were available. Predictors' time courses were adjusted for the hemodynamic response delay by convolution with a hemodynamic response function. Group data were statistically tested with a second-level analysis using a random effects analysis of variance using Response Type as a within-participants factor. To address the issue of multiple statistical comparisons across all voxels, activations are reported using False Discovery Rate (FDR) at a threshold of q < 0.01 [10]. Using FDR we control for the number of false positive voxels among the subset of voxels labelled as significant.
Main Results: The key findings which emerged from the results are that the analysis of fMRI brain data revealed differences in brain activity due to whether participants experienced IN or not. These differences appeared sensitive to whether or not the IN was associated with actually making a search or simply deciding that a search would be necessary. Although several brain regions showed differential activity
5 http://www.BrainVoyager.com

with IN, our results point to a particular region of the brain known as the posterior cingulate which is known to be a critical hub area involved in coordinating brain activity between the internal and external environment.
Analysis of Scenario 1 (RQ): In Scenario 1 we investigated brain activity when participants were presented with questions and possible responses, and they needed to decide that they knew the answer already or would need to search. For all participants we contrasted brain activity when they provided an answer versus when they provided the response that they needed to search. We hypothesised that this contrast would reveal brain regions associated with successful memory retrieval and working memory when they responded with an answer, signifying No-IN. When the response was that there was a need to search we expected activity in regions associated with IN.
The results, based on all 24 participants, for the effect of the factor Type of Response are shown for Scenario 1 in Figure 3 plotted on an average brain and in Table 1. To evaluate whether the effect of the Type of Response indicated higher or lower activity when a search was requested the average beta weights for each cluster were obtained and these are presented in bar charts for each of the clusters. Results showed that 4 of the 5 clusters had higher activation for the No-IN condition. This included thalamus, extending into the left head of caudate, right head of caudate, the left caudate body and an extensive cluster in the right inferior frontal gyrus (also known as the dorsolateral prefrontal cortex). These regions are known to have anatomical connectivity [16] and have been implicated in processes of memory retrieval [32], working memory [6], and decision making [15]. The only cluster showing greater activation for the ASK condition was found in the the ventral aspect of the posterior cingulate cortex. This region has often been associated with what is known as the default mode network [38] which shows decreased activation when a task is performed and increased activity when mind-wandering. We will return to discussion of the posterior cingulate later in the paper.
An interesting finding is that the situation when an answer could be provided, and thus an information need did not exist, provided 4 of the 5 clusters. Moreover, these clusters can be considered a network involving memory retrieval, information accumulation and working memory, all functions we would expect when a participant can provide an answer to the question. The remaining cluster in ventral posterior cingulate provides us with a putative brain region where activity reflects IN. Activity in this region has been hypothesised [29] to reflect a narrow focus of attention on internal information, and this is consistent with the "need to search" response in Scenario 1. Here, the response can be generated by assessing internally only that it was not possible to provide an answer; for success in the task there is no need to modify behaviour or to broaden the focus of attention.
Analysis of Scenario 2 (RQS): In Scenario 2 we investigated brain activity when participants were presented with questions and possible responses and if they responded that a search was needed then they would subsequently engage in a search. For all participants we contrasted brain activity when they provided an answer versus when they provided the response that they needed to search. Again we hypothesised that this contrast would reveal brain regions associ-

340

Figure 3: The five activation clusters from Scenario 1 are projected onto the average anatomical structure for three transverse sections. Note
that the brains are in radiological format where the left side of the brain is on the right side of the image.
Table 1: Details of Scenario 1 activations, including their anatomic label, location, Brodmann Area (BA), effect size and volume.

Brain Area Caudate Head Thalmus ventral Posterior Cingulate Caudate Body Inferior Frontal Gyrus

Hemisphere Right Left Left
Left Left

Talairach Coordinates

X

Y

Z

BA

11 10 3

-

-1

-11 12

-

-4

-44 12

29

-19 -17 21

-

-46 31 6

46

Effect size

F(1,23) 42.91 52.39 33.09

p-value 0.000001 <0.000001 0.000007

Number of voxels mm3
427 2017 263

39.03 107.63

0.000002 282 <0.000001 8024

ated with successful memory retrieval and working memory when they responded with an answer, signifying No-IN. In contrast to Scenario 1, such a result was not obtained. Instead we only found regions where brain activity was greater for IN, including a region of the posterior cingulate known as the dorsal posterior cingulate.
The results, based on all 24 participants, for the effect of the factor Type of Response are shown for Scenario 2 in Figure 4 plotted on an average brain and in Table 2. Again, to help assess the direction of the effect, parameter estimates are displayed as bar charts for each cluster. All clusters including the fusiform gyrus, the dorsal posterior cingulate and the cuneus (extending into the precuneus) showed greater activation for the IN condition. As both the precuneus and posterior cingulate clusters have been associated with the default mode network, it is perhaps not surprising that deactivations are apparent for these regions.

The fusiform gyrus is often associated with high level visual processing and it is possible that this cluster reflects greater visual activity when participants were in the state of IN.
The fact that the posterior cingulate was again identified as a region where activity was greater for the IN condition further raises the possibility that monitoring activity in this area could provide a useful brain signal for identifying information need. The fact that it was the dorsal posterior cingulate is important from a theoretical perspective. It has been hypothesised that the dorsal posterior cingulate is involved when there is brain activity directed towards external sources and broad attention is used [29]. This is consistent with the need to search as the individual must transition from a state of accessing internal information to one where they are engaging with the collection of new information from external sources.

341

Figure 4: The three activation clusters from Scenario 2 are shown projected onto the average anatomical structure for three transverse sections.
Note that the brains are in radiological format where the left side of the brain is on the right side of the image.
Table 2: Details of Scenario 2 activations, including their anatomic label, location, Brodmann Area (BA), effect size and volume.

Brain Area Fusiform Gyrus Cuneus dorsal Posterior Cingulate

Hemisphere Right Left Left

Talairach Coordinates

X

Y

Z

BA

41 -50 -12

37

-7

-74 24

18

-10 -29 27

23

Effect size

F(1,23) 31.29 38.64 60.35

p-value 0.000011 0.000002 <0.000001

Number of voxels mm3
266 3453 2147

Significance of the Posterior Cingulate: As we have discussed in relation to the current results, both the greater activation during IN and the switch between ventral and dorsal regions when an actual search is performed provide a unique signature for IN. In this regard it is useful to review that the posterior cingulate is thought to be what is known as a "hub" area. Such areas are known to be densely connected with many different brain regions and to be involved in the coordination of large scale brain networks. One function of the posterior cingulate appears to be in the balance between directing brain activity towards either internal or external sources and this role resonates with the requirements of detecting IN in that detecting the switch between internal and external processing is synonymous with search.
5. DISCUSSION AND CONCLUSION
This paper investigated the concept of information need from a neuropsychological perspective by investigating brain activity during periods in which an information need was induced. The raison d'etre of IR is to satisfy human information needs. Despite advances in the past few decades in both the IR and relevant scientific communities, we do not really understand how an information need emerges and how it is physically manifested. Information need refers to a complex concept: at the very initial state of the phe-

nomenon (i.e. at a visceral level), even the searcher may not be aware of its existence. This renders the measuring of this concept (using traditional behaviour studies) nearly impossible. Using functional Magnetic Resonance Imaging (fMRI), we measured the brain activity of twenty four participants while they performed a Question Answering (Q/A) Task. In order to do so, we devised a "within-subjects" design experiment where the independent variable was the information need (with two levels: Information Need, and NoInformation Need), which was controlled by responding to questions viewed on the screen. A set of questions were designed for a typical participant to respond equally between expressing an information need to answer the question or expressing a need to search for more information to answer the question using TREC-8 and TREC 2001 Q/A Tracks. The dependent variable was brain activity revealed by the BOLD signal.
We implemented two different task scenarios to create information need, in both scenarios participants were asked a multiple choice question, but in the first scenario when participants confronted an ASK they only had the option to acknowledge that they needed to search, they could not act on this IN (i.e. QR Task). In the second scenario when participants confronted an ASK they were able to engage in a search process (i.e. QRS Task). Our hypothesis was that there exists brain regions for which the activation levels are

342

different depending on the state of information need and that the regions found for these two scenarios would provide measurement data of brain states to complement our theoretical understanding of IN.
The key findings which emerged from the results are that the analysis of fMRI brain data revealed differences in brain activity due to whether participants experienced IN or not (addressing RQ2). These differences were obtained from modelling brain activity during the presentation of the question and possible responses and thus precede the actual decision to search (addressing RQ3). These differences appeared sensitive to whether or not the IN was associated with actually making a search or simply deciding that a search would be necessary (addressing RQ4). Although several brain regions showed differential activity with IN, our results point to a particular region of the brain known as the posterior cingulate which is known to be a critical hub area involved in coordinating brain activity between the internal and external environment. We speculate that this hub nature of switching between large scale brain networks that involve either internal or external processing could be an essential component of IN (addressing some issues raised by RQ1).
The results for the QR task of Scenario 1 and the QRS task of Scenario 2 provided strikingly different results. In the QR task of Scenario 1 the contrast of brain activation between the IN and No-IN conditions revealed greater activity for the IN condition in one brain area (posterior cingulate) and greater activity for the No-IN condition in four of the five areas obtained. These four areas can be associated with recalling information and making a decision. Relating this to IR we see evidence for the neural substrate involved with successfully recovering internal knowledge, a situation that relieves the need for information. However, a primary interest of the current research is to explore what brain regions are associated with IN. Thus, the results of the QRS task of Scenario 2 are of interest since we see that all three regions reported had greater activity for the IN condition. In particular, the dorsal posterior cingulate provides results similar to that found for the ventral posterior cingulate found in Scenario 1.
These differences between dorsal and ventral activation of posterior cingulate can be related to theories of the posterior cingulate and its special role in brain function [29]. The posterior cingulate is known to be an area that is metabolically active, using substantial energy and serving as a hub that regulates cognitive activity. It is especially involved in regulating brain resources between engaging in internal or external processes and the Arousal, Balance and Breadth of Attention (ABBA) model provides an explanation of the differences between the IN conditions in Scenarios 1 and 2. The model holds that activation in ventral posterior cingulate is consistent with a narrow internal focus while activation in dorsal posterior cingulate is consistent with a broad external focus. Thus, in Scenario 1, when participants need only to focus on the fact that they need to search (not what they need to search or mechanisms of search) the pattern of results in posterior cingulate shows a narrow and internal focus. However, in Scenario 2, when participants need to engage in a subsequent search task the pattern of results in posterior cingulate shows a broad and external focus. The implication for IR is that the differential patterns of activation in the posterior cingulate provide a window into how

cognitive processes are being directed and switching state from a narrow internal focus to a broad external focus can be used as a sign of a searcher needing to gather information from external sources.
Regarding the pattern of activity in posterior cingulate several aspects deserve further investigation. One is whether this general difference of activity found between IN and noIN is unique, the posterior cingulate is a complicated and densely connected brain region and other differences in cognitive states might reveal similar patterns of BOLD activity. This could be explained by heterogeneity of function in the posterior cingulate, or possibly by some more basic, and yet undetermined, mechanism that is common to IN and other cognitive processes. One possible way forward to studying this would for more detailed analysis of the spatiotemporal pattern of activity in posterior cingulate to determine the encoding of IN. Research in these directions would potentially aid in ways to exploit this neural signal in retrieval systems.
While our present interpretation of brain activity provides a parsimonious account of how IN is represented in the brain, further study is needed to advance this interpretation. One possible way forward is to perform a more detailed analysis of the spatiotemporal pattern of activity in posterior cingulate to see whether multivariate techniques of machine learning could provide a means to decode IN directly from brain activity, rather than to infer it from univariate comparison of brain activity during IN and no-IN states. Such a result could pave the way, as brain measuring technology advances, to monitor the IN state of an individual and to exploit this knowledge in a search engine system. Prerequisites to achieve this would be to confirm that this pattern of activity in posterior cingulate is robust across different IN scenarios as well as being unique to IN.
Our present results indicating differences between the QR and QRS tasks show the importance of task on brain activation. Thus, a meaningful research direction to pursue would be to investigate similar research questions in the context of other information retrieval and interaction tasks. This could provide converging evidence and would advance our neurotheoretical understanding of IN leading to exploitation of such signals in functioning IR systems.
In conclusion, the results of this experiment revealed a distributed network of brain regions commonly associated with information retrieval and produced novel results about the neural bases of information need. These results have implications both for theories explaining information need and the potential to design systems that could detect information need. Finally, we believe our study and conclusions constitute an important step in unravelling the nature of information need and therefore better satisfying it.
Acknowledgement: This work was supported by the Eco-
nomic and Social Research Council [grant number ES/L011921/1].
6. REFERENCES
[1] M. Allegretti, Y. Moshfeghi, M. Hadjigeorgieva, F. E. Pollick, J. M. Jose, and G. Pasi. When relevance judgement is happening?: An eeg-based study. In SIGIR '15, New York, NY, USA, 2015. ACM.
[2] I. Arapakis, K. Athanasakos, and J. M. Jose. A comparison of general vs personalised affective models for the prediction of topical relevance. In SIGIR '10, pages 371­378, 2010.
[3] I. Arapakis, I. Konstas, and J. M. Jose. Using facial expressions and peripheral physiological signals as implicit

343

indicators of topical relevance. In MM ' 09, pages 461­470, 2009.
[4] I. Arapakis, Y. Moshfeghi, H. Joho, R. Ren, D. Hannah, and J. M. Jose. Enriching user profiling with affective features for the improvement of a multimodal recommender system. CIVR '09, 2009.
[5] I. Arapakis, Y. Moshfeghi, H. Joho, R. Ren, D. Hannah, and J. M. Jose. Integrating facial expressions into user profiling for the improvement of a multimodal recommender system. In ICME '09, pages 1440­1443, 2009.
[6] A. K. Barbey, M. Koenigs, and J. Grafman. Dorsolateral prefrontal contributions to human working memory. Cortex, 49(5):1195­1205, 2013.
[7] M. J. Bates. Indexing and access for digital libraries and the internet: Human, database, and domain factors. JASIST, 49(13):1185­1205, 1998.
[8] N. Belkin, R. Oddy, and H. Brooks. ASK for information retrieval: Part I. Background and theory. Journal of Documentation, 38(2):61­71, 1982.
[9] N. J. Belkin, R. N. Oddy, and H. M. Brooks. ASK for information retrieval: Part II. Results of a design study. Journal of documentation, 38(3):145­164, 1982.
[10] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), pages 289­300, 1995.
[11] C. L. Borgman. From Gutenberg to the global information infrastructure: access to information in the networked world. Mit Press, 2000.
[12] C. Cole. A theory of information need for information retrieval that connects information to knowledge. JASIST, 62(7):1216­1231, 2011.
[13] C. Cole, C.-A. Julien, and J. E. Leide. An associative index model for hypertext internet search based on vannevar bush^aA Z´s memex machine: An exploratory case study. Information Research, 15(3):15­3, 2010.
[14] A. Dimoka. How to conduct a functional magnetic resonance (fmri) study in social science research. MIS Quarterly, 36(3):811­840, 2012.
[15] L. Ding and J. I. Gold. Caudate encodes multiple computations for perceptual decisions. The Journal of Neuroscience, 30(47):15747­15759, 2010.
[16] B. Draganski, F. Kherif, S. Kl¨oppel, P. A. Cook, D. C. Alexander, G. J. Parker, R. Deichmann, J. Ashburner, and R. S. Frackowiak. Evidence for segregated and integrative connectivity patterns in the human basal ganglia. The Journal of Neuroscience, 28(28):7143­7152, 2008.
[17] M. J. Eugster, T. Ruotsalo, M. M. Spap´e, I. Kosunen, O. Barral, N. Ravaja, G. Jacucci, and S. Kaski. Predicting term-relevance from brain signals. In SIGIR'14, pages 425­434. ACM, 2014.
[18] K. Friston, C. Buechel, G. Fink, J. Morris, E. Rolls, and R. Dolan. Psychophysiological and modulatory interactions in neuroimaging. NeuroImage, 6:218­229, 2007.
[19] R. Goebel. BrainVoyager QX, Vers.2.1, Brain Innovation B.V. Maastricht, Netherlands.
[20] B. Hjørland. The foundation of the concept of relevance. JASIST, 61(2):217­237, 2010.
[21] P. Ingwersen. Cognitive perspectives of information retrieval interaction: elements of a cognitive ir theory. Journal of documentation, 52(1):3­50, 1996.
[22] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05, pages 154­161, 2005.
[23] J. P. Kauppi, M. Kandemir, V. M. Saarinen, L. Hirvenkari, L. Parkkonen, A. Klami, R. Hari, and S. Kaski. Towards brain-activity-controlled information retrieval: Decoding image relevance from MEG signals. NeuroImage, in press, 2015.
[24] D. Kelly and N. Belkin. A user modeling system for

personalized interaction and tailored retrieval in interactive ir. ASIST, 39(1):316­325, 2005.
[25] D. Kelly and N. J. Belkin. Display time as implicit feedback: understanding task effects. In SIGIR '04, pages 377­384, New York, NY, USA, 2004. ACM.
[26] D. Kelly and X. Fu. Eliciting better information need descriptions from users of information search systems. Inf. Process. Manage., 43(1):30­46, Jan. 2007.
[27] J. Koenemann and N. J. Belkin. A case for interaction: a study of interactive information retrieval behavior and effectiveness. In SIGCHI '96, pages 205­212. ACM, 1996.
[28] C. C. Kuhlthau. A principle of uncertainty for information seeking. Journal of Documentation, 49(4):339­355, 1993.
[29] R. Leech and D. J. Sharp. The role of the posterior cingulate cortex in cognition and disease. Brain, 137(1):12­32, 2014.
[30] L. Lorigo, M. Haridasan, H. Brynjarsd´ottir, L. Xia, T. Joachims, G. Gay, L. Granka, F. Pellacini, and B. Pan. Eye tracking and online search: Lessons learned and challenges ahead. JASIST, 59(7):1041­1052, 2008.
[31] R. McGill, J. W. Tukey, and W. A. Larsen. Variations of box plots. American Statistician, 32(1):12­16, 1978.
[32] A. S. Mitchell and S. Chakraborty. What does the mediodorsal thalamus do? Frontiers in systems neuroscience, 7, 2013.
[33] Y. Moshfeghi and J. M. Jose. An effective implicit relevance feedback technique using affective, physiological and behavioural features. In SIGIR '13, pages 133­142, 2013.
[34] Y. Moshfeghi and J. M. Jose. On cognition, emotion, and interaction aspects of search tasks with different search intentions. In WWW, pages 931­942, 2013.
[35] Y. Moshfeghi, L. R. Pinto, F. E. Pollick, and J. M. Jose. Understanding relevance: An fmri study. In ECIR, pages 14­25, 2013.
[36] D. M. Nichols. Implicit rating and filtering. In Proceedings of the Fifth DELOS Workshop on Filtering and Collaborative Filtering, pages 31­36, 1997.
[37] R. C. Oldfield. The assessment and analysis of handedness: the edinburgh inventory. Neuropsychologia, 9(1):97­113, 1971.
[38] M. E. Raichle, A. M. MacLeod, A. Z. Snyder, W. J. Powers, D. A. Gusnard, and G. L. Shulman. A default mode of brain function. Proceedings of the National Academy of Sciences, 98(2):676­682, 2001.
[39] J. Talairach and P. Tournoux. Co-planar stereotaxic atlas of the human brain, volume 147. Thieme New York:, 1988.
[40] R. S. Taylor. Question-negotiation and information seeking in libraries. College & research libraries, 29(3):178­194, 1968.
[41] P. I. und Kalervo J¨arvelin. The Turn: Integration of Information Seeking and Retrieval in Context. Springer, 2005. xiv, 448 S. ISBN 1-4020-3850-X, 2006.
[42] C. van Rijsbergen. (invited paper) A new theoretical framework for information retrieval. In SIGIR, pages 194­200. ACM, 1986.
[43] R. White and D. Kelly. A study on the effects of personalization and task information on implicit feedback performance. In CIKM, pages 297­306, 2006.
[44] R. W. White. Implicit Feedback for Interactive Information Retrieval. PhD thesis, University of Glasgow, 2004.
[45] R. W. White, J. M. Jose, and I. Ruthven. A task-oriented study on the influencing effects of query-biased summarisation in web searching. IPM '03, 39(5):707­733, 2003.
[46] T. D. Wilson. On user studies and information needs. Journal of documentation, 37(1):3­15, 1981.
[47] Q. Zhao, Z. Zhou, H. Xu, S. Chen, F. Xu, W. Fan, and L. Han. Dynamic neural network of insight: a functional magnetic resonance imaging study on solving chinese `chengyu' riddles. PloS one, 8(3):e59351, 2013.

344

Modeling Document Novelty with Neural Tensor Network for Search Result Diversification

Long Xia Jun Xu Yanyan Lan Jiafeng Guo Xueqi Cheng
CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences
xialong@software.ict.ac.cn, {junxu, lanyanyan, guojiafeng, cxq}@ict.ac.cn

ABSTRACT
Search result diversification has attracted considerable attention as a means to tackle the ambiguous or multi-faceted information needs of users. One of the key problems in search result diversification is novelty, that is, how to measure the novelty of a candidate document with respect to other documents. In the heuristic approaches, the predefined document similarity functions are directly utilized for defining the novelty. In the learning approaches, the novelty is characterized based on a set of handcrafted features. Both the similarity functions and the features are difficult to manually design in real world due to the complexity of modeling the document novelty. In this paper, we propose to model the novelty of a document with a neural tensor network. Instead of manually defining the similarity functions or features, the new method automatically learns a nonlinear novelty function based on the preliminary representation of the candidate document and other documents. New diverse learning to rank models can be derived under the relational learning to rank framework. To determine the model parameters, loss functions are constructed and optimized with stochastic gradient descent. Extensive experiments on three public TREC datasets show that the new derived algorithms can significantly outperform the baselines, including the state-of-the-art relational learning to rank models.
Keywords
search result diversification; neural tensor network; relational learning to rank
1. INTRODUCTION
In web search, it has been widely observed that a large fraction of queries are ambiguous or multi-faceted. Search result diversification has been proposed as a way to tackle this problem and diverse ranking is one of the central problems. The goal of diverse ranking is to develop a ranking
Corresponding author: Jun Xu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17­21, 2016, Pisa, Italy. c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911498

model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents. Thus, how to measure the novelty of a candidate document with respect to other documents becomes a key problem in the designing of the diverse ranking models.
Methods for search result diversification can be categorized into heuristic approaches and learning approaches. The heuristic approaches construct diverse rankings with heuristic rules [3, 8, 14, 24, 25, 26]. As a representative model, the maximal marginal relevance (MMR) [3] formulates the construction of a diverse ranking as a process of sequential document selection. At each iteration the document with the highest marginal relevance is selected. The marginal relevance consists of the relevance score and novelty score. The novelty score is calculated based on a predefined document similarity function. Thus, the selection of the document similarity function becomes a critical issue for MMR. Different choices of the similarity functions result in different ranking lists. Usually it is difficult to define an appropriate similarity function in a real application.
Recently, machine learning models have been proposed and applied to the task of search result diversification [17, 20, 23, 28, 32]. The basic idea is to automatically learn a diverse ranking model from the labeled training data. Relational learning to rank is one of the representative framework in this field. In relational learning to rank, the novelty of a document with respect to the previously selected documents is encoded as a set of handcrafted novelty features. Several algorithms have been developed under the framework and state-of-the-art performances have been achieved [28, 32]. However, it is still an unsolved problem to define a set of novelty features which can effectively capture the complex document relationship. Unlike the designing of relevance features in conventional learning to rank, it is much more difficult to extract novelty features for search result diversification. Currently, a very limited number of novelty features can be utilized when constructing a diverse ranking model. For example, in R-LTR [32] and PAMM [28], the novelty of a document is characterized with only seven novelty features. Most of the features are based on the cosine similarities of two documents represented with tf-idf vectors or topic vectors. Thus, it is very difficult, if not impossible, for users to handcraft an optimal set of novelty features for search result diversification.
To address above problems and inspired by the neural models for relation classification [27], we propose to model the document novelty for search result diversification using a neural tensor network (NTN). Unlike existing methods

395

which manually define the document similarity functions or novelty features, the method automatically learns a nonlinear document novelty function from the training data. It first generates the novelty signals with a nonlinear tensor layer, through interacting the candidate document with other documents. Then, a max-pooling operation is applied to select the most effective novelty signals. Finally, the selected signals are combined linearly to form the final document novelty score.
New diverse ranking models, then, can be proposed under the relational learning to rank framework. The marginal relevance in relational learning to rank, which is used for selecting the best document at each step, is calculated as a sum of the query-document relevance score and document novelty score. Modeling the document novelty score with the proposed neural tensor network, we can achieve new diverse ranking models. On the basis of existing relational learning to rank algorithms of R-LTR and PAMM, two new loss functions are constructed and optimized, achieving two novel diverse ranking algorithms of R-LTR-NTN and PAMM-NTN.
To evaluate the effectiveness of the proposed algorithms, we conducted extensive experiments on three public TREC benchmark datasets. The experimental results showed that our proposed algorithms, including R-LTR-NTN and PAMMNTN, can significantly outperform the state-of-the-art baselines including heuristic approaches of MMR, and learning approaches of SVM-DIV [29], R-LTR, and PAMM. Analysis showed that the proposed approaches achieved better results through learning better document dissimilarities in terms of distinguishing the documents with different subtopics. Thus, the proposed algorithms have the ability to improve the queries with high ambiguity.
Contributions of the paper include: 1) We proposed to model the document novelty with a neural tensor network, which enables us to get rid of the manually defined similarity functions or handcrafted novelty features in search result diversification; 2) Based on the new document novelty model, two diverse ranking algorithms were derived under the framework of relational learning to rank; 3) The effectiveness of the proposed algorithms were verified based on public benchmark datasets.
The rest of the paper is organized as follows. After a summary of related work in Section 2, we present the neural tensor network model for measuring document novelty in Section 3. Section 4 presents the two derived diverse ranking algorithms under the relational learning to rank framework. Experimental results and discussions are given in Section 5. Section 6 concludes the paper and gives future directions.
2. RELATED WORK
This paper concerns about the ranking models for search result diversification. Existing methods can be categorized into heuristic approaches and learning approaches. One of the central problems in both of these two approaches is novelty, that is, how to model the novelty information of a document with respect to other documents.
2.1 Heuristic approaches
It is a common practice to use heuristic rules to construct a diverse ranking list in search. Usually, the rules are created based on the observation that in diverse ranking a document's novelty depends on not only the document itself but also the documents ranked in previous positions. Carbonell

and Goldstein [3] proposed the maximal marginal relevance criterion to guide the design of diverse ranking models. The criterion is implemented with a process of iteratively selecting the documents from the candidate document set. At each iteration, the document with the highest marginal relevance score is selected, where the score is a linear combination of the query-document relevance and the maximum distance of the document to the documents in current result set, in another word, novelty. The marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. A number of methods have been developed under the criterion. PM-2 [8] treats the problem of finding a diverse search result as finding a proportional representation for the document ranking. xQuAD [26] directly models different aspects underlying the original query in the form of sub-queries, and estimates the relevance of the retrieved documents to each identified subquery. Hu et al. [14] proposed a diversification framework that explicitly leverages the hierarchical intents of queries and selects the documents that maximize diversity in the hierarchical structure. See also [2, 4, 10, 11, 12, 22]
All of these heuristic approaches rely on a predefined document similarity (or distance) function to measure the novelty of a document. Thus, the selection of the similarity function is critical for the ranking performances. Usually it is hard to design an optimal similarity function for a specific task. In this paper, we focus on the learning approaches to estimate the novelty scores of documents.
2.2 Learning approaches
Machine learning techniques have been applied to construct ranking models for search result diversification. In these approaches, the relevance features and novelty features are extracted for characterizing the relevance and novelty information of a document, respectively. The ranking score is usually a linear combination of these features and the parameters can be automatically estimated from the training data. Some promising results have been obtained. For example, Zhu et al. [32] proposed the relational learning to rank framework in which the diverse ranking is constructed with a process of sequential document selection. The training of a relational learning to rank model thus amounts to optimizing the object function based on the ground-truth rankings. With different definitions of the object functions and optimization techniques, different diverse ranking algorithms have been derived [28, 32]. Radlinski et al. [23] proposed online learning algorithms that directly learn a diverse ranking of documents based on users' clicking behaviors. More works please refer to [17, 20, 30].
Most learning approaches depend on a set of handcrafted novelty features to represent the novelty of a document. Construction of such features is usually difficult and time consuming in real applications. In real world, we have a very limited number of novelty features, which greatly limits the usability of these diverse ranking models. In this paper, we propose to automatically learn the novelty with a neural tensor network and enhance the usability of the diverse ranking algorithms.
3. MODELING DOCUMENT NOVELTY WITH NEURAL TENSOR NETWORK
Inspired by the neural models for relation classification, in

396

Tensor Layer

Linear Layer

µTR 2 Rz

tanh

+

+



tanh

eT1 WR[1:z] e2 + VR

e1 e2

+ bR

t 2 Rz

Figure 1: Visualization of the neural tensor network for relation classification. Each dashed box represents one slice of the tensor, in this case there are z = 2 slices.

this paper we propose to use neural tensor network to model the novelty of a document w.r.t. a set of other documents.

3.1 Neural tensor network
In deep learning literature, neural tensor networks (NTN) is originally proposed to reason the relationship between two entities in knowledge graph [27]. Given two entities (e1, e2) represented with le dimensional features, the goal of NTN is to predict whether they have a certain relationship R. Specifically, NTN computes a score of how likely it is that these two entities are in certain relationship R by the following function:

g(e1, R, e2) = µTR tanh eT1 WR[1:z]e2 + VR

e1 e2

+ bR ,

where e1, e2  Rle are the vector representations of two entities, WR[1:z]  Rle×le×z is a tensor and the bilinear tensor product eT1 WR[1:z]e2 results in a vector h  Rz, where each entry of h is computed by one slice i (i = 1, · · · , z) of the
tensor: hi = eT1 WR[i]e2. The other parameters for relation R are the standard form of a neural network: VR  Rz×2le , µR  Rz, and bR  Rz. Figure 1 illustrates the neural tensor network with two slices for entity relationship reasoning.

3.2 Modeling document novelty with neural tensor network
Intuitively, the neural tensor networks model the relationships between two entities with a bilinear tensor product. The idea can be naturally extended to model the novelty relation of a document with respect to other documents for search result diversification. That is, we can represent the novelty information of a candidate document as a bilinear tensor product of the document and other documents, as shown in Figure 2.
More specifically, suppose that we are given a set of M documents X = {dj}M j=1, where each document dj can be characterized with its preliminary representation vj  Rlv , e.g., the topic distribution [9, 13] of dj or the document vector generated with a doc2vec [15] model. Given a candidate document d  X with its preliminary presentation v, and a set of documents S  X \ {d} with their preliminary representations {v1, · · · , v|S|}, the novelty score of d with respect to the documents in S can be defined as a neural tensor network with z hidden slices:
gn(v, S) = µT max tanh vT W[1:z] v1, . . . , v|S| ,

where each column in matrix v1, . . . , v|S|  Rlv×|S| stands

Tensor Layer

Max-pooling Linear

Layer

Layer

µT 2 Rz
tanh

tanh

vT





W[1:z] v1, . . . , v|S|

H 2 Rz|S|

t 2 Rz

Figure 2: Visualization of the neural tensor network for modeling document novelty (z = 2).

for the preliminary representation vector of the corresponding document in S, W[1:z]  Rlv×lv×z is a tensor, and µ  Rz the weights correspond to the slices of the tensor. As shown in Figure 2, the neural tensor network consists of
a tensor layer, a max-pooling layer, and a linear layer.
Tensor Layer: The tensor layer takes the preliminary
representations of the documents as inputs. The interactions
between the document d and documents in S are represented
as a bilinear product followed by a nonlinear operation:

 hT1   tanh vT W[1] v1, . . . , v|S| 

H= 

...



=

 





...

 

,

(1)



hTz

tanh vT W[z] v1, . . . , v|S|

where hi  R|S| is computed by one slice of the tensor.

Compared with the original neural tensor network in Sec-

tion 3.1, the tensor in Equation (1) models the relationship

between one document and multiple documents simultane-

ously. Thus, the output of Equation (1) is a z × |S| matrix

rather than a z-dimensional vector. Also, since the number

of documents in S varies in different document selection it-

erations, the term VR

e1 e2

in the original tensor neural

network is ignored. Moreover, in ranking we cares about

the order of the documents rather than the ranking scores.

Thus, the bias term bR is also ignored.

Max-pooling Layer: In the max-pooling layer, the ma-

trix outputted by the tensor layer is mapped to a z-dimensional

vector with the max operation:

t = max(hT1 ), · · · , max(hTz ) T .

(2)

Intuitively, the pooling layer aggregates individual novelty signal learned at each tensor layer hTi . Max-pooling extracts the most significant signals among them. Thus, vector t can
be considered as a the z-dimensional novelty features and
each dimension is defined by one slice of the tensor.
Linear Layer: Finally, the novelty score of the document
is calculated as a linear combination of the novelty signals outputted by the max-pooling layer: µT t, where µ is an
z-dimensional parameter vector.

4. DIVERSE RANKING ALGORITHMS BASED ON NEURAL TENSOR NETWORK
New diverse ranking algorithms can be derived based on the proposed neural tensor network for modeling document novelty. In this paper, we propose two algorithms under the framework of relational learning to rank.

397

Algorithm 1 Ranking via maximizing marginal relevance
Input: documents X and novelty features R Output: ranking of documents Y 1: S0  empty set 2: for r = 1, · · · , M do 3: Y (r)  arg maxj:xj X\Sr-1 f (xj , Rj , Sr-1) 4: Sr  Sr-1  {xY (r)} 5: end for 6: return Y
4.1 Relational learning to rank
The relational learning to rank framework [32] formalizes the ranking of documents as a process of sequential document selection and defines the marginal relevance as linear combination of the relevance score and the novelty score. Formally, let X = {d1, · · · , dM } denotes the set of documents retrieved by a query q. For each query-document pair (q, di), relevance feature vector xi  Rlx is extracted. Let R  RM×M×K denotes a 3-way tensor representing relationships between the documents, where Rijk stands for the k-th feature of relationship between documents di and dj. Assuming that a set of documents S have been selected in the previous iterations, the marginal relevance of the i-th candidate document with respect to S, denoted as f (xi, Ri, S), is then defined as the combination of the relevance score and the novelty score:
f (xi, Ri, S) = rT xi + nT hS(Ri), xi  X\S, (3)
where rT xi stands for the relevance score and r is the relevance weight vector, nT hS(Ri) stands for the novelty score of the document with respect to S and n is the diversity weight vector, Ri stands for the matrix of relationships between document xi and other documents, and hS(Ri) stands for the aggregation function on Ri which aggregates the matrix Ri into a novelty feature vector. Usually, hS can be one of the operations of max, min, or average.
According to the maximal marginal relevance criterion, sequential document selection process can be used to create a diverse ranking, as shown in Algorithm 1. The algorithm initializes S0 as an empty set, and then iteratively selects the documents from the candidate set. At iteration r (r = 1, 2, · · · , M ), the document with the maximal marginal relevance score f (xj, Rj, Sr-1) is selected and ranked at position r. At the same time, the selected document is inserted into Sr-1.
Given a set of training instances which consist of queries, documents, and their relevance labels, the model parameters can be learned from the training data. The process amounts to optimizing an objective function based on the training data. Different definitions of the objective functions and optimization techniques lead to different relational learning to rank algorithms. For example, in algorithm RLTR [32], the likelihood of the training queries is maximized using stochastic gradient descent. In algorithm PAMM [28], the loss function upper bounding the diversity evaluation measure is constructed and optimized with structured Perceptron.
Relational learning to rank models depend on a set of handcrafted features for characterizing the novelty of a document. However, how to design the features that can effectively capture the complex document relationship is still an unsolved problem. Unlike the conventional learning to

Table 1: Novelty features used in R-LTR.

Name

Explanation

Subtopic diversity Text diversity
Title diversity Anchor text diversity ODP-Based diversity
Link-based diversity URL-based diversity

document distance based on PLSA [13] one minus cosine similarity of the tf-idf vectors on body text text novelty feature based on title text novelty feature based on anchor categorical distance based on ODP1 taxomony link similarity based on inlink/outlink whether the two URLs belong to the same domain/site

rank in which a large number effective relevance features have been developed [21], it is much harder to find novelty features for search result diversification. As a result, the relational learning to rank algorithms of R-LTR and PAMM utilized only seven features in their experiments, as have listed in Table 1. We can see that most of these features are calculated based on the predefined similarities of two documents (represented as tf-idf vectors or topic distributions), and respectively applied to the document fields of title, body, and anchor.
In real world applications, the performances of the ranking algorithms heavily depend on the effectiveness of these handcrafted features and different ranking tasks need different features. It is necessary to develop a method that can learn the document novelty automatically and release people from the handcrafted novelty features.

4.2 Relational learning to rank algorithms based on neural tensor network
In this subsection, based on the technique of modeling the document novelty with neural tensor network, we develop two new relational learning to rank algorithms that can learn the document novelty function automatically.

4.2.1 The ranking model
Following the notations used in Section 3.2 and Section 4.1,
let X = {d1, · · · , dM } denotes the set of documents retrieved by a query q. Each query-document pair (q, d) is represented with the relevance feature vector x  Rlx . Each document d  X is characterized with its preliminary representation vector v  Rlv . Assuming that at one iteration of the sequential document selection, a set of documents S have been
selected. We define the marginal relevance score of a candi-
date document d as:

f (d, S) =gr(x) + gn(v, S)

(4)

=T x + µT max tanh vT W[1:z] v1, . . . , v|S| ,

where gr(x) is the relevance of d w.r.t. query q, which is further defined as a linear combination of the relevance features; gn(v, S) is the novelty of d w.r.t. the documents in S, which is further defined as a neural tensor network, as have been shown in Section 3.2. The model parameters , µ, and W[1:z] can be learned with the training data.
In the online ranking, a diverse ranking can created with the sequential document selection process, similar to the procedure shown in Algorithm 1.

1http://www.dmoz.org

398

The main advantage of using neural tensor network to model document novelty is that the tensor can relate the candidate document and the selected documents multiplicatively, instead of only through a predefined similarity function (as that of in heuristic approaches) or through a linear combination of novelty features (as that of in learning approaches and shown in Equation (3)). Intuitively, the model can be explained that each slice of the tensor is responsible for one aspect or subtopic of a query. Each tensor slice settles the diversity relationship between the candidate document and the selected documents set differently. Thus, with multiple tensor slices, the model calculates the novelty scores based on multiple diversity aspects.

4.2.2 General loss function

The parameters of the ranking model can be determined

with supervised learning methods, which amounts to opti-

mizing the objective function built upon the labeled training

data.

In training procedure, given the labeled data with N queries as: (X(1), J (1)), (X(2), J (2)), · · · , (X(N), J (N)), where X(n) =

{dj(n)

}M (n)
j=1

,

where

M (n)

denotes

the

number

of

documents

related with the n-th query. Let x(jn)  Rlx denote the rele-

vance feature vector for the n-th query and document d(jn),

vj(n)  Rlv the preliminary representation of document d(jn),

and J(n) the human labels on documents which is in the

form of a binary matrix. Jj(sn) = 1 if document d(jn) con-

tains the s-th subtopic of the query and 0 otherwise2. The

learning process amounts to minimizing the total loss with

respect to the given training data:

N
min
f F n=1

 X(n), f , J (n) ,

where  X(n), f denotes the ranking generated by the
ranking model f in Equation (4), for the documents in X(n). The generated ranking  is then compared with the human labels J(n) by the loss function . Intuitively, the learning process can be interpreted as finding an optimal ranking model f from some functional space F so that for each training query the difference between the generated permutation  and the human labels J is minimal.
Different objective functions and optimization techniques lead to different algorithms. In this section, based on the relational learning to rank algorithms of R-LTR [32] and PAMM [28], we construct two novel algorithms in which the document novelty is modeled with a neural tensor network, referred to as R-LTR-NTN and PAMM-NTN, respectively.

4.2.3 R-LTR-NTN
Based on the loss function defined for R-LTR [32], we derive the loss function of R-LTR-NTN, which is a negative logarithm likelihood of the training queries:

N
LR-LTR-NTN(f ) = - log Pr Y (n)|X(n) ,
n=1
where Y (n) is the ground-truth ranking generated from the human label J(n). For any query, the probability Pr(Y |X)
2In this paper we assume that all labels are binary.

Algorithm 2 The R-LTR-NTN Algorithm

Input: training data {(X(n), J(n))}Nn=1 and learning rate  Output: model parameter (, µ, W[1:z])

1: initialize {, µ, W[1:z]}  random values in [0, 1]

2: repeat

3: Shuffle the training data

4: for n = 1, · · · , N do

5:

calculate (n), µ(n) and W[1:z](n)

{Equation (6), Equation (7), and Equation (8)}

6:

   -  × (n)

7:

µ  µ -  × µ(n)

8:

W[1:z]  W[1:z] -  × W[1:z](n)

9: end for

10: until convergence

11: return (, µ, W[1:z])

can be further defined as

Pr(Y |X) =Pr(dY (1)dY (2) · · · dY (M)|X)

M -1

=

Pr(dY (r)|X, Sr-1)

r=1

M -1
=
r=1

exp{f (dY (r), Sr-1)}

M k=r

exp{f

(dY

(k) ,

Sr-1

)}

,

(5)

where Y (r) denotes the index of the document ranked at the r-th position in Y , Sr-1 = {dY (k)}rk-=11 is the documents ranked at the top r - 1 positions in Y , f (dY (r), Sr-1) is the marginal relevance score of document dY (r) w.r.t. the selected documents in Sr-1, as defined in Equation (4), and
S0 is an empty set.
Stochastic gradient descent is adopted to conduct the op-
timization. Given a query q, the retrieved documents X = {dj}M j=1, and the ranking Y generated by the ground-truth labels, the gradient of the model parameters can be written
as

M -1
 =
r=1

M k=r

exp

f

dY (k), Sr-1

xY (k)

M k=r

exp{f

(dY

(k) ,

Sr-1 )}

- xY (r) , (6)

M -1
µ =
r=1

M k=r

exp

f

dY (k), Sr-1

tY (k)

M k=r

exp{f

(dY

(k) ,

Sr-1)}

- tY (r)

, (7)

M -1
W[i] =
r=1

M k=r

exp

f

dY (k), Sr-1

µiY (k)

M k=r

exp{f

(dY

(k) ,

Sr-1 )}

(8)

-µiY (r) ,

where t is defined in Equation (2), and
Y (r) = 1 - tanh2 vYT (r)W[i]vi vY (r)vTi , (9)
where   Rlv×lv and i(1  i  |S|) stands for the output of the max-pooling position for the i-th (1  i  z) tensor slice.
Algorithm 2 shows the pseudo code of the R-LTR-NTN.
4.2.4 PAMM-NTN
Based on the loss function defined for PAMM [28], we derive the loss function of PAMM-NTN, which is directly

399

defined over a diversity evaluation measure:

Algorithm 3 The PAMM-NTN algorithm

N
1 - E  X(n), f , J (n) ,

(10)

n=1

where E(·, ·)  [0, 1] is a diversity evaluation measure such as -NDCG or ERR-IA etc. It can be proved that the Equation (10) is upper bounded by

N
LPAMM-NTN(f ) =

Pr(Y +|X(n)) - Pr(Y -|X(n))

n=1 Y +Y(n)+; Y - Y(n)-

 E(Y +,J (n))-E(Y -,J (n)) .

where Y(n)+ and Y(n)- are the sets of positive and neg-

ative rankings generated from human labels J(n), respec-

tively. · is one if the condition is satisfied otherwise zero.

Pr(·|·) stands for the probability of the ranking, as defined

in Equation (5).

Also, stochastic gradient descent is adopted to conduct the

optimization. At each iteration, we are given a query q, the

retrieved documents X = {dj}M j=1, a positive ranking Y +, and a negative ranking Y -. For convenience of calculation,

we

resort

to

the

optimization

problem

of

max log

Pr(Y Pr(Y

+ |X ) - |X )

.

Thus, the gradients of the parameters can be written as

M -1
 =
r=1
-

M k=r

exp

f (dY +(k), Sr-1)

xY +(k)

M k=r

exp

f (dY +(k), Sr-1)

M k=r

exp

f (dY -(k), Sr-1)

xY -(k)

M k=r

exp

f (dY -(k), Sr-1)

(11)

-xY +(r) + xY -(r) ,

M -1
µ =
r=1
-

M k=r

exp

f (dY +(k), Sr-1)

tY +(k)

M k=r

exp

f (dY +(k), Sr-1)

M k=r

exp

f (dY -(k), Sr-1)

tY -(k)

M k=r

exp

f (dY -(k), Sr-1)

(12)

Input: training data {(X(n), J (n))}Nn=1, parameter: learning rate , diversity evaluation measure E,

number of positive/negative rankings per query  +/ -.

Output: model parameter (, µ, W[1:z])

1: for n = 1 to N do

2: P R(n)  Sample positive rankings {[28]}

3: N R(n)  Sample negative rankings {[28]}

4: end for

5: initialize (, µ, W[1:z])  random values in [0, 1]

6: repeat

7: for n = 1 to N do

8:

for all {Y +, Y -}  P R(n) × N R(n) do

9:

P  Pr(Y +|X(n)) - Pr(Y -|X(n))

{Pr(Y |X) is defined in Equation (5)}

10:

if P  E(Y +,J (n))-E(Y -,J (n)) then

11:

calculate , µ and W[1:z]

{Equation (11), Equation (12), and Equation (13)}

12:

   +  × 

13:

µ  µ +  × µ

14:

W[1:z]  W[1:z] +  × W[1:z]

15:

end if

16:

end for

17: end for

18: until convergence

19: return (, µ, W[1:z])

of order O(T ·N ·M 2 ·(lx +lv ·Z)), where T denotes the number of iterations, N the number of queries in training data, M the maximum number of documents per training query, lx the number of relevance features, lv the dimensions of the preliminary document representation, and Z the number of tensor slices. The learning process of PAMM-NTN (Algorithm 3) is of order O(T · N ·  + ·  - · M 2 · (lx + lv · Z)), where  + denotes the number of positive rankings per query and  - the number of negative rankings per query.The time complexity of online ranking prediction (Algorithm 1) is of order O(M · K · (lx + lv · Z)), where M is the number of candidate documents for the query and K denotes the number documents need to be ranked.
5. EXPERIMENTS
5.1 Experimental settings

-tY +(r) + tY -(r) ,

M -1
W[i] =
r=1
-

M k=r

exp

f (dY +(k), Sr-1)

µiY +(k)

M k=r

exp

f (dY +(k), Sr-1)

M k=r

exp

f (dY -(k), Sr-1)

µiY -(k)

M k=r

exp

f (dY -(k), Sr-1)

-µiY +(r) + µiY -(r) ,
(13) where t is defined in Equation (2), and  is defined in Equation (9). Algorithm 3 shows the pseudo code of the PAMMNTN algorithm.
4.2.5 Time complexities

We conducted experiments to test the performances of R-LTR-NTN and PAMM-NTN using three TREC benchmark datasets for diversity task: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), and TREC 2011 Web Track (WT2011). Each dataset consists of queries, corresponding retrieved documents, and human judged labels. Each query includes several subtopics identified by the TREC assessors. The document relevance labels were made at the subtopic level and the labels are binary3. Statistics on the datasets are given in Table 2.
All the experiments were carried out on the ClueWeb09 Category B data collection4, which comprises of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied to the documents as preprocessing. We conducted 5-fold cross-validation experiments on the three datasets. For each dataset, we randomly split the queries into five even subsets.

We analyzed time complexities of R-LTR-NTN and PAMMNTN. The learning process of R-LTR-NTN (Algorithm 2) is

3The graded judgements in WT2011 was treated as binary. 4http://boston.lti.cs.cmu.edu/data/clueweb09

400

Table 2: Statistics on WT2009, WT2010 and WT2011. Dataset #queries #labeled docs #subtopics per query

WT2009

50

WT2010

48

WT2011

50

5149 6554 5000

38 37 26

At each fold three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the five trials.
The TREC official evaluation metrics for the diversity task were used in the experiments, including the ERR-IA [5], NDCG [6], and NRBP [7]. They measure the diversity of a result list by explicitly rewarding novelty and penalizing redundancy observed at every rank. Following the default settings in official TREC evaluation program, the parameters  and  in these evaluation measures are set to 0.5. We also used traditional diversity measures of Precision-IA (denoted as "Pre-IA") [1], and Subtopic Recall (denoted as "Srecall") [31]. All of the measures are computed over the top-k search results (k = 20).
We compared R-LTR-NTN and PAMM-NTN with several types of baselines. The baselines include three heuristic approaches to search result diversification.
MMR [3] : a heuristic approach in which the document ranking is constructed via iteratively selecting the document with the maximal marginal relevance.
xQuAD [26] : a representative heuristic approach to search result diversification which explicitly accounts for the various aspects associated to an under-specified query.
PM-2 [8] : a method of optimizing proportionality for search result diversification.
Note that these baselines require a prior relevance function to implement their diversification steps. In our experiments, ListMLE [16, 18] was chosen as the relevance function.
The baselines also include state-of-the-art learning approaches to search result diversification.
SVM-DIV [29] : a learning approach in which structural SVMs was used to optimize the subtopic coverage.
R-LTR [32] : a state-of-the-art learning approach developed in the relational learning to rank framework.
PAMM [28] : another state-of-the-art learning algorithm that directly optimizes diversity evaluation measure.
Following the practice in [32], for the baseline of R-LTR, we used the results of R-LTRmin in which the relation function hS(R) was defined as the minimal distance of the candidate document to the selected documents.
For the baseline PAMM (and our approach PAMM-NTN), we configure them to directly optimize -NDCG@20 because it is one of the most widely used performance measures. Thus, the baseline of PAMM is denoted as PAMM(NDCG). Following the practice in [28], we set the number of sampled positive rankings per query  + = 5 and the number of sampled negative rankings per query  - = 20.
5.2 Relevance features and preliminary document representations
As for the relevance features, we adopted the features used in R-LTR experiments [21], including the typical weighting

Table 3: Relevance features used in the experiments.

Each of the first 4 features is applied to the fields of

body, anchor, title, URL, and the whole documents. [32]

Name Description

# Features

TF-IDF The tf-idf model

5

BM25 BM25 with default parameters

5

LMIR LMIR with Dirichlet smoothing

5

MRF [19] MRF with ordered/unordered phrase

10

PageRank PageRank score

1

#inlinks number of inlinks

1

#outlinks number of outlinks

1

models (e.g., TF-IDF, BM25, LM) and term dependency model [19]. Table 3 summarized the relevance features. For all the query-document matching features, they were applied in five fields: body, anchor, title, URL, and the whole document, resulting in 5 features in total. Note that the MRF feature has two variations: ordered phrase and unordered phrase [19]. Thus the total number of MRF features becomes 10.
The neural tensor network need preliminary representations of the documents as its inputs. In the experiments, we used the document vector generated by the topic model of probabilistic latent semantic analysis (PLSA) [13] or the deep learning model of doc2vec [15], both are trained on all of the documents in ClueWeb09 Category B data collection and the number of latent dimensions are set to 100. For training the doc2vec model, we used the distributed bag of words (DBOW) model5. In all of the experiments, the learning rate is set to 0.025 and the window size is set to 8.
Our approaches (R-LTR-NTN and PAMM-NTN) with the settings of using the PLSA or doc2vec as document representations are denoted with the corresponding subscripts. For example, the R-LTR-NTN that using PLSA as document representations is denoted as R-LTR-NTNplsa. Thus, in all of the experiments, our approaches include R-LTRNTNplsa, R-LTR-NTNdoc2vec, PAMM-NTN(-NDCG)plsa, and PAMM-NTN(-NDCG)doc2vec. Please note in all of the experiments, PAMM-NTN was configured to direct optimize the evaluation measure of -NDCG@20.
5.3 Experimental results
Table 4, Table 5, and Table 6 report the performances of the proposed methods and baselines in terms of 5 diversity metrics (ERR-IA@20, -NDCG@20, NRBP@20, PreIA@20, and S-recall@20) on the datasets of WT20096, WT2010, and WT2011, respectively. Boldface indicates the highest score among all runs. For all of our approaches, the number of tensor slices z is set to 7.
From the results we can see that, on all of the three datasets and in terms of the five diversity evaluation metrics, our approaches (R-LTR-NTNplsa, R-LTR-NTNdoc2vec, PAMM-NTN(-NDCG)plsa, and PAMM-NTN(-NDCG)doc2vec) can outperform all of the baselines. We conducted significant testing (t-test) on the improvements of our approaches over the baselines. The results indicate that the improvements of R-LTR-NTNplsa and R-LTR-NTNdoc2vec over RLTR are significant (p-value < 0.05), in terms of all of the
5http://radimrehurek.com/gensim/models/doc2vec.html 6The performances of XQuAD reported in Table 4 are different to that of reported in [26]. It may caused by the different splitting of the dataset in cross validation.

401

Method

Table 4: Performance comparison of all methods for WT2009.

ERR-IA@20

-NDCG@20 NRBP@20

Pre-IA@20

MMR xQuAD PM-2 SVM-DIV
R-LTR
R-LTR-NTNplsa R-LTR-NTNdoc2vec PAMM(-NDCG) PAMM-NTN(-NDCG)plsa PAMM-NTN(-NDCG)doc2vec

0.2022 0.2316 0.2294 0.2408
0.2714 0.3015 0.3117
0.2842 0.3081 0.3135

0.3083 0.3437 0.3369 0.3526
0.3964 0.4444 0.4503
0.4271 0.4377 0.4555

0.1715 0.1956 0.1788 0.2073 0.2339 0.2563 0.2578 0.2411 0.2642 0.2626

0.0918 0.0984 0.0949 0.1075
0.1233 0.1588 0.1670
0.1265 0.1661 0.1745

S-recall@20
0.4698 0.4931 0.4876 0.5101 0.5511 0.5743 0.5910
0.5612 0.5755 0.5772

Method

Table 5: Performance comparison of all methods for WT2010.

ERR-IA@20

-NDCG@20 NRBP@20

Pre-IA@20

MMR xQuAD PM-2 SVM-DIV
R-LTR
R-LTR-NTNplsa R-LTR-NTNdoc2vec PAMM(-NDCG) PAMM-NTN(-NDCG)plsa PAMM-NTN(-NDCG)doc2vec

0.2735 0.3278 0.3296 0.3331 0.3647 0.3876 0.3932
0.3802 0.3898 0.3901

0.4036 0.4445 0.4478 0.4593
0.4924 0.5311 0.5376
0.5249 0.5379 0.5407

0.2252 0.2872 0.2901 0.2934 0.3293 0.3333 0.3623
0.3431 0.3479 0.3553

0.1722 0.1883 0.1885 0.1925 0.2042 0.2341 0.2418
0.2111 0.2264 0.2386

S-recall@20
0.6444 0.6732 0.6749 0.6774 0.6893 0.6912 0.6994 0.6832 0.7006 0.7032

Method

Table 6: Performance comparison of all methods for WT2011.

ERR-IA@20

-NDCG@20 NRBP@20

Pre-IA@20

MMR xQuAD PM-2 SVM-DIV
R-LTR
R-LTR-NTNplsa R-LTR-NTNdoc2vec PAMM(-NDCG) PAMM-NTN(-NDCG)plsa PAMM-NTN(-NDCG)doc2vec

0.4284 0.4753 0.4873 0.4898
0.5389 0.5483 0.5538
0.5417 0.5496 0.5554

0.5302 0.5645 0.5786 0.5910
0.6297 0.6537 0.6555
0.6433 0.6469 0.6566

0.3913 0.4274 0.4318 0.4475 0.4982 0.5050 0.5223
0.5012 0.5111 0.5212

0.3176 0.3299 0.3405 0.3468
0.3921 0.4011 0.4125
0.3955 0.4169 0.4177

S-recall@20
0.7567 0.7683 0.7743 0.7750 0.8512 0.8543 0.8590
0.8518 0.8524 0.8533

performance measures. The results also indicate that the improvements of PAMM-NTN(-NDCG)plsa and PAMMNTN(-NDCG)doc2vec over all of the baselines are significant, in terms of all of the performance measures. The results indicate that the neural tensor network is effective for modeling the document novelty information, and thus can improve the performances.
5.4 Discussions
We conducted experiments to show the reasons that our approaches outperformed the baselines and impacts of different parameter settings, using the results of R-LTR-NTNplsa and R-LTR-NTNdoc2vec on WT2009 dataset as examples.
5.4.1 Ability to learn better document dissimilarities
We found that the learned neural tensor network can help to distinguish the relevant documents in terms of different subtopics, by learning a better dissimilarity (novelty) function for documents. That is one of the reasons why our approaches can outperform the baselines.
Specifically, the dissimilarities between two documents can be calculated based on the preliminary document representations, either using the Euclidean distance or using the learned neural tensor network (the novelty score of a document w.r.t. another document). That is, given two documents represented with the preliminary presentations vi and vj, the dissimilarity score can calculated either based

on the Euclidean distance:
de(vi, vj ) = vi - vj 2,
or based on the learned neural tensor network:
dn(vi, vj ) = gn(vi, {vj }) = µT tanh viT W[1:z]vj
where µ and W[1:z] are learned with the R-LTR-NTN algorithms. Here we can ignore the max operation because there is only one document vj at the righthand of W[1:z].
Suppose we are given a set of queries and the associated relevant documents. For each query, the relevant documents can be grouped into several clusters, each corresponds a subtopic of the query. Thus, all of the associated documents from all queries are grouped into different clusters, each corresponds to a subtopic. We calculated the ratio of average inter-cluster documents dissimilarities to average intra-cluster document dissimilarities. It is obvious that in search result diversification, a good document dissimilarity function would get large inter-cluster document dissimilarities and small intra-cluster document dissimilarities (large ratio value). This is because such a dissimilarity function could discriminate the subtopics well.
Table 7 shows the ratios calculated based on different dissimilarity definitions and different preliminary document representations. From the results, we can see that the ratio of "dn with PLSA" (documents represented with PLSA topics and dissimilarities are calculated with neural tensor

402

Table 7: Ratio of average inter-cluster documents dis-

similarities to average intra-cluster document dissimilar-

ities. The documents are grouped according to their as-

sociated subtopics. Method

average dissimilarity ratio

de with PLSA dn with PLSA de with doc2vec dn with doc2vec

1.65 2.73 2.10 4.32

network) is larger than the ratio of "de with PLSA" (documents represented with PLSA topics and dissimilarities are calculated as Euclidean distance), and the ratio of "dn with doc2vec" is larger than the ratio of "de with doc2vec". The results indicates that the dissimilarity functions learned by the tensor neural network are better than the Euclidean distances, in terms of discriminating the query subtopics.
The conclusion is quite intuitive and nature because the parameters of neural tensor network are determined based on the labeled data and thus can be adapted to the specific dataset and task, while the Euclidean distance is a predefined function for all datasets and tasks. Therefore, we can conclude that R-LTR-NTN (and also PAMM-NTN) can improve the performances through learning a better document dissimilarity function which distinguishes the documents with different subtopics effectively.
5.4.2 Ability to improve queries with high ambiguity
We also conducted experiments to show on which kinds of queries our approaches can perform well. Specifically, in each fold of the experiments on WT2009, we trained an R-LTR-NTNdoc2vec model, an R-LTR, and a PAMM(NDCG) model on the training data and tested them on the test data. We then grouped the queries in the test datasets according to the number of subtopics they associated. We compared the performances of these three models in terms of -NDCG@20 on each of the query groups and the results are shown in Figure 3. Boldface indicates the number of associated subtopics by the candidate documents, and the numbers in the parentheses indicate the proportion of queries in that group to the number of all queries. Please note that in Figure 3 some queries associated with only one or two subtopics while in Table 2 all queries have at least 3 subtopics associated. This is because we used the Indri7 toolkit to retrieve the top 1000 documents as the candidates. Some labeled documents may not be ranked at top 1000 and thus be eliminated from the candidate set.
From the results reported in Figure 3, we can see that for those queries that associated with only one or two subtopics, R-LTR-NTN performed worse than the baselines of R-LTR and PAMM(-NDCG). However, for those queries that associated with three or more subtopics (queries with high ambiguity), R-LTR-NTN outperformed the baselines. We also observed the trends that larger improvements R-LTRNTN can achieve on the queries with more subtopics. The results is also intuitive because the document relations are more complex for ambiguous queries and neural tensor network can model the complex document relationship better. Thus, we can conclude that R-LTR-NTN can improve the baselines through improving the high ambiguity queries.
7http://lemurproject.org/indri

0.44

-NDCG@20

0.42

0.4

R-LTR

PAMM(-NDCG)

0.38

R-LTR-NTNdoc2vec

0.36

0.34
1(4%) 2(6%) 3(22%)4(34%)5(24%) 6(1%) query grouped by the number of subtopics

Figure 3: Performances with respect to query groups with different number of subtopics. The numbers in the parentheses indicate the proportion of queries in that group to the number of all queries.

0.46

time (hours)

-NDCG@20

20 0.45
15
0.44 10

0.43 5
R-LTR-NTNdoc2vec
training time

0.42

0

1 3 5 7 9 11 13 15 17 19

the number of tensor slices z

Figure 4: Ranking accuracies and training time with respect to the number of tensor slices z.

5.4.3 Effects of the number of tensor slices
Finally, we conducted experiments to test if the proposed algorithms are sensitive to the model parameters. One of the most important parameters in the proposed method is the number of tensor slices z. Thus, in the experiments we tested if R-LTR-NTNdoc2vec is sensitive to different settings of z values. Specifically, we tuned z by varying the values of parameter z from 1 to 19, with step 2 and fixing other model parameters to the default or optimal values. Figure 4 shows the performances of R-LTR-NTNdoc2vec with respect to number of slices z, in terms of -NDCG@20. The training time (in hours) with respect to z are also shown in the figure.
From the results, we can see that the performances did not change much with different z values, which indicates R-LTR-NTNdoc2vec (and other proposed algorithms) are robust and not sensitive to the parameter settings. In all of the experiments the number of tensor slices was set to the optimal value 7.
One of the negative effects of increasing z values is that the training time increased dramatically with the creased z values, as shown in Figure 4. This is because much more operations are needed for training the model if z is increased. Please refer to Section 4.2.5 for the time complexities of the proposed algorithms.

403

6. CONCLUSIONS
How to model the novelty of a candidate document with respect to other documents is one of the key problems in search result diversification. Existing approaches have been hurt from the necessaries of predefining a document similarity function or a set of novelty features, which are usually hard in real applications. In this paper we proposed to model the novelty of a document with a neural tensor network, which enables us to automatically learns a nonlinear novelty function based on the preliminary representations of the candidate document and other documents. Under the framework of relational learning to rank, new diverse learning to rank models have been derived, by replacing the novelty term in the original objective function with the neural tensor network. Experimental results based on three benchmark datasets showed that the proposed models significantly outperformed the baseline methods, including the state-of-the-art relational learning to rank models. Experimental results also showed that the proposed algorithms can improve the baselines via learning a document dissimilarity function that matches well with the query subtopics. The results also showed that more improvements can be achieved on the queries with high ambiguity.
As future work, we would like to verify the effectiveness of the proposed algorithms on applications other than search result diversification such as multi-document summarization etc. We also want to study the approaches to learning the relevance features and novelty features simultaneously.
7. ACKNOWLEDGMENTS
The work was funded by the 973 Program of China under Grants No. 2014CB340401 and 2012CB316303, the 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China (NSFC) under Grants No. 61232010, 61472401, 61433014, 61425016, and 61203298, the Key Research Program of the Chinese Academy of Sciences under Grant No. KGZD-EW-T03-2, and the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102.
8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of ACM WSDM '09, pages 5­14, 2009.
[2] S. Bhatia. Multidimensional search result diversification: Diverse search results for diverse users. In Proceedings of ACM SIGIR '11, pages 1331­1332, 2011.
[3] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of ACM SIGIR '98, pages 335­336, 1998.
[4] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of ACM CIKM '09 pages 1287­1296, 2009.
[5] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of ACM CIKM '09, pages 621­630, 2009.
[6] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of ACM SIGIR '08, pages 659­666, 2008.
[7] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of ICTIR '09, pages 188­199, 2009.
[8] V. Dang and W. B. Croft. Diversity by proportionality: An election-based approach to search result diversification. In Proceedings of ACM SIGIR '12, pages 65­74, 2012.

[9] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. JASIS, 41(6):391­407, 1990.
[10] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In Proceedings ofWWW '09, pages 381­390, 2009.
[11] S. Guo and S. Sanner. Probabilistic latent maximal marginal relevance. In Proceedings ofACM SIGIR '10, pages 833­834, 2010.
[12] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proceedings of ACM SIGIR '12, pages 851­860, 2012.
[13] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR '99, pages 50­57, 1999.
[14] S. Hu, Z. Dou, X. Wang, T. Sakai, and J.-R. Wen. Search result diversification based on hierarchical intents. In Proceedings of ACM CIKM '15, pages 63­72, 2015.
[15] Q. V. Le and T. Mikolov. Distributed Representations of Sentences and Documents. ArXiv e-prints, May 2014.
[16] H. Li. Learning to rank for information retrieval and natural language processing; 2nd ed. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publ., San Rafael, CA, 2014.
[17] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of WWW '09, pages 71­80, 2009.
[18] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225­331, Mar. 2009.
[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of ACM SIGIR '05, pages 472­479, 2005.
[20] L. Mihalkova and R. Mooney. Learning to disambiguate search queries from short sessions. In W. Buntine, M. Grobelnik, D. MladeniA¨ G , and J. Shawe-Taylor, editors, Machine Learning and Knowledge Discovery in Databases, volume 5782 of Lecture Notes in Computer Science, pages 111­127. Springer Berlin Heidelberg, 2009.
[21] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A benchmark collection for research on learning to rank for information retrieval. Inf. Retr., 13(4):346­374, Aug. 2010.
[22] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proceedings of ACM SIGIR '06, pages 691­692, 2006.
[23] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of ACM ICML '08, pages 784­791, 2008.
[24] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of WWW '10, pages 781­790, 2010.
[25] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of ACM SIGKDD '12, pages 705­713, 2012.
[26] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of WWW '10, pages 881­890, 2010.
[27] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning with neural tensor networks for knowledge base completion. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 926­934. Curran Associates, Inc., 2013.
[28] L. Xia, J. Xu, Y. Lan, J. Guo, and X. Cheng. Learning maximal marginal relevance model via directly optimizing diversity evaluation measures. In Proceedings of ACM SIGIR '15, pages 113­122, 2015.
[29] Y. Yue and T. Joachims. Predicting diverse subsets using structural svms. In Proceedings of ACM ICML '08, pages 1224­1231, 2008.
[30] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of ACM ICML '09, pages 1201­1208, 2009.
[31] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of ACM SIGIR '03, pages 10­17, 2003.
[32] Y. Zhu, Y. Lan, J. Guo, X. Cheng, and S. Niu. Learning for search result diversification. In Proceedings of ACM SIGIR '14, pages 293­302, 2014.

404

Evaluating Search Result Diversity using Intent Hierarchies

Xiaojie Wang1,2, Zhicheng Dou,1,2, Tetsuya Sakai3, and Ji-Rong Wen1,2,4 1School of Information, Renmin University of China
2Beijing Key Laboratory of Big Data Management and Analysis Methods, China 3Department of Computer Science and Engineering, Waseda University
4Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China 2{wangxiaojie,dou}@ruc.edu.cn, 3tetsuyasakai@acm.org, 4jirong.wen@gmail.com

ABSTRACT
Search result diversification aims at returning diversified document lists to cover different user intents for ambiguous or broad queries. Existing diversity measures assume that user intents are independent or exclusive, and do not consider the relationships among the intents. In this paper, we introduce intent hierarchies to model the relationships among intents. Based on intent hierarchies, we propose several hierarchical measures that can consider the relationships among intents. We demonstrate the feasibility of hierarchical measures by using a new test collection based on TREC Web Track 2009-2013 diversity test collections. Our main experimental findings are: (1) Hierarchical measures are generally more discriminative and intuitive than existing measures using flat lists of intents; (2) When the queries have multilayer intent hierarchies, hierarchical measures are less correlated to existing measures, but can get more improvement in discriminative power; (3) Hierarchical measures are more intuitive in terms of diversity or relevance. The hierarchical measures using the whole intent hierarchies are more intuitive than only using the leaf nodes in terms of diversity and relevance.
Keywords
Ambiguity; Diversity; Evaluation; Novelty; Hierarchy
1. INTRODUCTION
Nowadays, people tend to meet their daily information needs by typing keywords into search engines like Google and Bing. However, these keywords, also known as queries, are often ambiguous or broad [14, 15, 28, 10]. The queries usually have several interpretations or aspects, also known as subtopics or user intents. When users submit the same query to retrieval systems, they may want different information returned to fulfill their information needs. This poses
Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17 - 21, 2016, Pisa, Italy c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911497

a challenge to search engines when the targeted user intent cannot be known in advance.
To tackle this problem, a wide range of search result diversification algorithms ([1, 2, 5, 13, 18, 26, 27, 31, 25, 12, 11]) have been proposed over the past years. They aim at returning a diversified ranked document list that covers different intents of the queries. In the meantime, some researchers have introduced a variety of diversity measures, such as I-rec [22], -nDCG [9], Intent-Aware measures [1], D-measures [24], etc. These measures evaluate ranked lists in terms of both diversity and relevance, and indicate which diversification algorithms are better to use. Existing diversity measures assume that the users' information need could be represented by a single layer of intents and these intents are either independent or exclusive. However, some of the intents are not independent and are related to each other.
We use the query "bobcat", which is a topic (No. 77) in Text Retrieval Conference(TREC) 2010 Web Track [8], as an example. This query is ambiguous because of the polysemy of "bobcat": one interpretation is a company called "bobcat company" whose core business is about tractors; another interpretation is a kind of wild animals called "wild bobcat." We show its official intents, marked by i1-i4, in Figure 1(a). The figure shows that except intent i2 that is about "wild bobcat," the remaining ones, i1, i3, and i4, are all about "bobcat company." This indicates that i1, i3, and i4 are more related to each other, but are less related to i2. Even within the three intents about "bobcat company," i1 and i3 are closer because they are about the trade involving tractors of the company, whereas i4 is about homepage the company. We argue that this kind of relationships among intents should be modeled when evaluating search result diversity. However, none of existing measures considers this.
Specifically, we find two submitted runs for the query, cmuFuTop10D and THUIR10DvNov, in TREC Web Track 2010 diversity task. cmuFuTop10D covers i1, i3, and i4, while THUIR10DvNov covers i1, i2, and i4 in their top ten ranks. Since i1, i3, and i4 are all about "bobcat company," cmuFuTop10D misses another interpretation of bobcat, i.e. "wild bobcat," but THUIR10DvNov covers both interpretations. In this sense, the latter is more diversified but I-rec [22] treats them as equally good because they cover the same number of intents. Some other existing measures also have similar problems, which will be illustrated in Section 3.3.1. We think that this is due to their lack of recognition of the relationships among intents.
In light of the above observation, we introduce intent hierarchies to represent the relationships among intents. We

415

(a) Official intents of the query "bobcat".

(b) LEFT: Intent Hierarchies OIH and EIH. OIH is comprised of the solid boxes, whereas EIH includes both solid

and dashed nodes. RIGHT: An example showing relevance assessments for the added nodes (under R in red) derived from relevance assessments for the official intents (under R in blue).

Figure 1: The official intents, original intent hierarchy (OIH), and extended intent hierarchy (EIH) of No. 77 query "bobcat" in TREC Web Track 2010.

design hierarchical measures using the intent hierarchies to solve the problems mentioned above. The main contributions of this paper are:
(1) To the best of our knowledge, this is the first work on modeling user intents as intent hierarchies and using the intent hierarchies for evaluating search result diversity.
(2) We propose hierarchical measures using intent hierarchies, including Layer-Aware measures, N-rec, LD-measures, LAD-measures, and HD-measures. We show several cases where hierarchical measures outperform existing measures in terms of discriminative power and intuitiveness.
(3) We present a method for creating intent hierarchies from existing diversity test collections, and reusing the relevance assessments. We create a new dataset based on the TREC Web Track 2009-2013 diversity test collections. The new dataset can be assessed online 1.
(4) We compare our measures with existing measures. We find that (i) Hierarchical measures are generally more discriminative and intuitive than existing measures, especially when using the intent hierarchies whose leaf nodes have the same depth; (ii) When the queries have multilayer intent hierarchies, hierarchical measures are less correlated to existing measures, but can get more improvement in discriminative power; (iii) The hierarchical measures using the whole intent hierarchies are more intuitive than only using the leaf nodes in terms of diversity and relevance.
The remainder of this paper is organized as follows. Section 2 describes some existing diversity measures and the methods for testing evaluation measures. In Section 3, we introduce intent hierarchies, and our method for creating a new test collection based on TREC Web Track 2009-2013 diversity test collections. We then propose several new diversity measures that can utilize the intent hierarchies. Section 4 describes experimental results and analysis. We conclude our work in Section 5.
2. RELATED WORK
Given a query q, most existing measures evaluate a ranked document list by modeling users' information need as a flat list of intents {i}. Some measures can handle intent probability P r(i|q) and graded relevance assessments but some cannot. In this section, we briefly summarize the previous work on designing and testing diversity measures.
1http://www.playbigdata.com/dou/heval/

2.1 Diversity Measures

2.1.1 Intent Recall

Intent recall(I-rec) [22], also known as subtopic recall [30]
is the proportion of intents covered by a ranking list. Let dr denote the document at rank r, and let I(dr) denote the set of intents to which document dr is relevant. Then, I-rec for a certain cutoff K can be expressed as:

I -rec@K

=

|

K
r=1

I (dr )|

(1)

|{i}|

Note that I-rec does not take the positions of relevant documents into account, and cannot handle intent probability and graded relevance assessments.

2.1.2 -nDCG

In order to balance both relevance and diversity of ranked lists, -nDCG [9] is defined as:

-nDCG@K =  K rK r==11NNGG((rr))//lloogg((rr++11))

(2)

N G(r) =

Ji(r)(1 - )Ci(r-1)

i{i}

where N G(r) is N G(r) in the ideal ranked list; Ji(r) is

1othifertwheised;ocCuim(re)nt=atrrka=n1kJri

is (k)

relevant to intent is the number of

i, and 0 relevant

documents to intent i within top r; and  is a parameter.

-nDCG tends to disregard unpopular intents and hence can

be counterintuitive sometimes [24].

2.1.3 Intent-Aware measures

Intent-Aware measures (IA measures) [1] is a general framework to evaluate ranked document lists. Assuming that i{i} P r(i|q) = 1, M -IA can be computed as:



M -IA@K =

P r(i|q)Mi@K

(3)

i{i}

where Mi is the per-intent version of measure M. Measure M can be nDCG [16], ERR [4], nERR [7], etc.

2.1.4 D-measures
D-measures [24] aim to boost intent recall, and to reward documents that are highly relevant to more popular intents. Assume that gi(r) is the gain value of the document at rank

416

r for intent i, and gi(r) is calculated based on per-intent

relevance assessments. Then the global gain at rank r is

given by:



GG(r) =

P r(i|q)gi(r)

(4)

i{i}

Let global

CGG(r) gain at

=

r
k=1

GG(k),

rank r. Further,

which is the cumulative let GG(r) and CGG(r)

denote the global gain and the cumulative global gain re-

spectively at rank r in the ideal ranked list. The ideal list is

obtained by listing up all relevant documents in descending

order of global gains. Let J(r) = 1 if the document at rank r

iLserteCle(vra)nt=toarkn=y1

of the J (k),

intents {i}, which is the

and J(r) = 0 otherwise. number of relevant doc-

uments within top r. D-nDCG and D-Q at document cutoff

K are defined as:

D-nDCG@K = K rK r==11GGGG((rr))//lloogg((rr++11))

(5)

1

 K

C(r) + CGG(r)

D-Q@K =

J (r)

(6)

min(K, R)

r + CGG(r)

r=1

where R is the number of judged relevant documents. Then D-measure is defined as:

D-measure@K = I-rec@K + (1 - )D-measure@K (7)

where D-measure can be D-nDCG or D-Q, and  is a parameter controlling the tradeoff between diversity and relevance. D-measures are free of the under-normalization problem of -nDCG and IA measures.
The diversity measures mentioned above are widely used in several tasks of TREC Web Track 2 or NII Testbeds and Community for Information access Research (NTCIR) 3, but they do not take the relationships among intents into consideration, which is what we aim to deal with in this paper.

2.2 Measure Evaluation
Given a certain significance level, discriminative power measures the stability of measures across queries and experiments based on significance tests, e.g. paired bootstrap test [20], Tukey's Honestly Significant Differences(HSD) [3] test, etc. Discriminative power can be used to estimate the performance difference required to achieve statistical significance between two retrieval systems [21].
Concordance test [21] is proposed to quantify the intuitiveness of diversity measures. In concordance test, one or more gold standard measures are chosen and assumed to truly represent intuitiveness. Given two diversity measures M1 and M2, the relative intuitiveness of M1 (or M2) is measured in terms of preference agreement with the gold standard measures. The preference agreement is that M1 (or M2) agrees with the gold standard measure(s) about which one of two ranked lists should be preferred.
Rank correlation compares two rankings, which are two ranked system lists in our case. Kendall's  [17] is a widelyused statistic to measure rank correlation. However  lacks the property of top heaviness, which means the exchanges near the top of a ranked list and those near the bottom are treated equally, even though the swaps near the top is generally more important in the context of IR evaluation. ap [29] is proposed to deal with the problem. Note that  is symmetric but ap is not. However, a symmetric ap can
2http://plg.uwaterloo.ca/~trecweb/ 3http://research.nii.ac.jp/ntcir/index-en.html

be obtained by averaging two ap values when each list is treated as the former one. Both  and ap range from -1, which implies two ranked lists perfectly disagree, to 1, which implies two ranked lists are identical.
In this paper, we use discriminative power, concordance test, and rank correlation to evaluate diversity measures.
3. PROPOSED METHODS
In this section, we define two types of intent hierarchies to represent the relationships among user intents and discuss their properties. We then introduce our method for creating such intent hierarchies and obtaining relevance assessments for the intent hierarchies based on TREC Web Track 2009-2013 diversity test collections. Last, we propose several diversity measures based on intent hierarchies, and demonstrate that in some cases, the new measures outperform their corresponding existing measures.
3.1 Intent Hierarchies
Given a query q, the users' information need is represented as a set of intents {i}. We assume these intents cannot be further subdivided, and refer to them as atomic intents. We aim to build an intent hierarchy based on the semantic similarity or relatedness of the intents. The intent hierarchy should possess some basic properties as follows:
Property 1. The intent hierarchy is in a tree structure, where every child has only one parent.
Property 2. The root of intent hierarchy is denoted by q itself, which stands for the user's information need as a whole. The root is a dummy node only for the completeness of the tree, and is not considered in our measures.
Property 3. When q is broad, the intent hierarchy is built in such a way that a parent node refers to a more general concept than its children, and a child node refers to one aspect of its parent. When q is ambiguous, each child node of the root is one interpretation of the query, and each of its subtrees is built in the same way as a broad query.
Property 4. These atomic intents, i.e. {i}, correspond one to one with leaves of the intent hierarchy. This means the number of leaves in the intent hierarchy is the same as the number of the atomic intents.
We call an intent hierarchy that satisfies the properties specified above is called an original intent hierarchy (OIH). OIH can be extended so as to satisfy an extra property as:
Property 5. These atomic intents are in the same layer of the intent hierarchy. In other words, all leaf nodes of the intent hierarchy have the same depth because the atomic intents correspond to leaves of the intent hierarchy (see Property 4).
An intent hierarchies that satisfies all five properties are called an extended intent hierarchy (EIH). If a query's OIH satisfies Property 5, then its EIH is the same as the OIH.
We consider the root of an intent hierarchy as the zeroth layer, the child nodes of the root as the first layer and so forth. If an intent hierarchy only has the zeroth layer and the first layer, the height of the intent hierarchy is one. In the paper, a single-layer intent hierarchy refers to an intent hierarchy whose height is one, while a multilayer intent hierarchy refers to that whose height is greater than one.

417

3.2 Creating Intent Hierarchies
In this paper, we create intent hierarchies based on TREC Web Track 2009-2013 diversity test collections. Note that for each query in TREC Web Track 2010-2013, the description of its first intent is the same as the description of the query itself. We find that although the descriptions are the same, if a query has several different interpretations, the first intent is just one of these interpretations. A query's first intent does not refer to a more general concept than the other intents. So we do not treat the first intent differently.
We use the official intents as atomic intents to avoid reassessing relevance of the documents. First we create original intent hierarchies (OIH) by manually grouping the official intents based on their semantic similarity or relatedness. Then, we extend them to extended intent hierarchies (EIH). Figure 1 illustrates how we create OIH and EIH for the query "bobcat" in TREC 2010 Web Track. It can be seen from Figure 1(a) that this query has four official intents and intent i1 and i3 are related to the trade involving tractors of the "bobcat company." So we create a new node n1 that stands for "bobcat tractors" as their parent node. Similarly, n1 and i4 are related to "bobcat company," hence we create another new node n2 representing "bobcat company" as their parent. Finally, since n2 ("bobcat company") and i2 ("wild bobcat") are two distinct interpretations of query "bobcat," they are considered as the child nodes of the root node. The resultant OIH is shown in solid boxes in the left of Figure 1(b). Further, we extend the OIH by adding more child nodes to i2 and i4 to make sure that all the leaf nodes have the same height. The resultant EIH is shown in solid boxes plus dashed boxes in the left of Figure 1(b).
For a leaf node, we use its original weight of the corresponding official intent as its initial weight. For an intermediate node, we set its original weight to the sum of its child node weights. We then normalize the weights for each layer to make sure that these weights sum to 1. For TREC Web Track 2009-2013 test collections, because of the lack of official intent weights, we assume that each official intent for a query is equally important.
As for the OIH or EIH shown in Figure 1(b): (1) It is in a tree structure (Property 1); (2) Its root is query "bobcat" itself (Property 2); (3) The query is ambiguous, so the child nodes of root are its two different interpretations, i.e. "bobcat company" and "wild bobcat." A parent node refers to a more general concept than its children (Property 3), e.g. "bobcat company" is more general than "bobcat company homepage;" (4) The leaf nodes are exactly the official intents of query "bobcat" (Property 4). Further, the depth of all the leaf nodes in EIH is three (Property 5).
Note that we only have document relevance assessments for the original intents appeared in TREC Web Track diversity test collections. In other words, for the intent hierarchies we create, document relevance judgments are just available for their leaf intents. We do not have document relevance assessments for intermediate intents. As assessing document relevance is usually very time-consuming, it is not desirable to reassess the documents for intermediate nodes of the intent hierarchies. Fortunately, according to Property 3, a parent node of an intent hierarchy stands for a more general concept than its child nodes. Hence it is reasonable to assume that if a document is relevant to a node, it would be relevant to the node's parent. This means that we can derive relevance assessments for the intermediate nodes

starting from the leaves. In this paper, we simply let:

Ld(n) = max Ld(c)

(8)

cC(n)

where Ld(n) is the relevance rating assigned to document d for node n, and C(n) is the set of child nodes of n.
We show an actual document (denoted by d in the following) from TREC Web Track 2010 diversity test collection in Figure 1(b). In the table, the officially provided relevance assessments are marked in blue, e.g. the relevance rating of d for i1 is 1. Firstly, node n1 has two child nodes, i1 and i3, and the relevance ratings of d for them are 1 and 0. According to Equation (8), the relevance rating of d for n1 is 1. Similarly, we can derive the relevance rating for n2 based on its child node i4 and n1. These derived relevance assessments are shown in red in the table of Figure 1(b).
To conclude, we create a new dataset containing intent hierarchies by manually grouping the official intents from TREC Web track test collections. The good news is that we do not need to reassess document relevance with regards to the intent hierarchies. We directly leverage document relevance assessments for the leaf intents, and automatically assign relevance ratings for the intermediate intents. This also implies that when we want to create hierarchical intents for evaluating diversity, we just need to assess document relevance for the leaf nodes or atomic intents.
The new test collection has 250 queries, and 105 topics have multilayer intent hierarchies. Most of the time of creating the new dataset is spent on grouping the original intents. On average, we spend about three minutes per query mainly in understanding the original intents with the assistance of search engines such as Bing and Google.

3.3 Hierarchical Measures

3.3.1 Layer-Aware measures
Given a query q and its intent hierarchy, our first proposal
for evaluating a ranked list is to first evaluate the ranked
list for each layer using existing measures, then combine the
evaluation scores. Let H denote the height of the intent hierarchy, and let
L = {l1, l2, ..., lH } denote its first layer to the last layer. We define Layer-Aware measures (LA measures) at document cutoff K as the follows.

 H

M -LA@K = wi  Mi@K

(9)

i=1

Here,

wi

is

the

weight

of

layer

li,

where

H
i=1

wi

=

1,

and Mi is the evaluation score of measure M by using the

intents of layer li. For example, ERR-IA-LA is computed as

follows: (1) For each layer, compute the per-layer scores of

ERR-IA; (2) Compute the weighted average of the per-layer

scores using Equation (9).

We find that the combination of measures over layers of

intent hierarchies could outperform the original measures

using a flat list of intents. We use the query "defender",

which is a topic (No. 20) in TREC Web Track 2009 [6], as

an example. We choose this query because it has a relatively

simple intent hierarchy. Its extended intent hierarchy (EIH)

is shown in the left of Figure 2(b). Suppose we have three

documents, d1-d3, and each of them can be viewed as a

ranked list containing only one document. Their relevance

assessments for the EIH are displayed in blue in the right of

418

(a) Official intents of the query "defender".

(b) LEFT: Intent Hierarchies OIH and EIH. OIH is comprised of the solid boxes, whereas EIH includes both solid and dashed nodes. RIGHT: 1st column: document IDs (d: the ideal one), each document is equal to a ranked list of length 1. 2nd to

5th column (R and R ): relevance assessments for the official intents (in red) and derived relevance assessments for added
nodes (in blue). 6th to 12th column (S): the measures are computed at rank 1 (subscript 1 means only using the first layer of EIH and subscript 2 means only using the second layer. subscript O means using OIH and subscript E means using EIH.), e.g. d2 get 0.35 when using D-nDCG on the first layer of EIH. Note that the original D-nDCG is equal to D-nDCG2 and the original I-rec is equal to I-rec2.

Figure 2: The official intents, original intent hierarchy (OIH), and extended intent hierarchy (EIH) of No. 20 query "defender" in TREC Web Track 2009. In the right table, if two documents have the same scores under a measure (in green below), it means that this measure cannot tell which one is better, e.g. D-nDCG1@1 treats d2 and d3 as equally good, but d3 is better because of its relevance to an extra intent i5.

Figure 2(b). Note that the nodes that receive no relevant documents within the documents are not displayed to save space. Assume d is the first document within the ideal rank list and it is relevant to every node displayed. In the right of Figure 2(b), D-nDCG1@1 is the evaluation score when only using the first layer of the EIH, D-nDCG2@1 means only using the second layer, and D-nDCG-LAE@1 is the average of D-nDCG1@1 and D-nDCG2@1. Note that the original D-nDCG is equal to D-nDCG2. We use the measures to score d1 to d3, which is equivalent of evaluating at document cutoff 1. We show the evaluation results in Figure 2(b), e.g. d2 gets 0.35 when using D-nDCG1@1.
We find that d1>d2=d3 in terms of D-nDCG1@1, d1=d2>d3 in terms of D-nDCG2@1, whereas d1>d2>d3 in terms of D-nDCG-LAE@1. Here, ">" means the former document is preferred compared with the latter when evaluating them at rank 1, and "=" means neither is preferred. The real preference should be d1>d2>d3. This is because (1) d1 is more diversified than d2 because d1 refers to two interpretations of query "defender", i.e. "windows defender" and "defender arcade game online," while d2 only refers to the former; (2) d2 is more diversified than d3 because d2 refers to two aspects of "windows defender", i.e. "windows defender homepage" and "windows defender reports" while d3 just refers to the former. Here, only D-nDCG-LAE@1 is consistent with the real preference. D-nDCG1@1 fails to tell the difference between d2 and d3, whereas D-nDCG2@1 fails to tell the difference between d1 and d2. This indicates that the combination over layers has higher potential to reflect real user satisfaction than the use of a flat list of intents in some cases.
3.3.2 Node Recall
Given a query q, let V denote the nodes in its intent hierarchy except for its root. Let dr denote the document at

rank r, and let N (dr) denote the set of nodes in V to which dr is relevant. Given a document cutoff K, we define node recall (N -rec) as:

N -rec@K

=

|

K
r=1

N

(dr

)|

|V |

(10)

which is the proportion of nodes in the hierarchy covered by the top K documents. N-rec is a natural generalization of I-rec when using the intent hierarchy rather than a flat list of intents. They both are rank-insensitive and cannot handle graded relevance assessments.
We use an example to show that N-rec is able to outperform I-rec in terms of discriminative power. In the right of Figure 2(b), I-rec1@1 means only using the first layer, I-rec2@1 means only using the second layer, and N-recE@1 means using the extended intent hierarchy (EIH) when computing N-rec. These measures are computed at rank 1. Note that the original I-rec is equal to I-rec2. We find that d1>d2=d3 according to I-rec1@1, d1=d2>d3 according to I-rec2@1, whereas d1>d2>d3 according to N-recE@1. As we discussed in Section 3.3.1, The real preference should be d1>d2>d3. I-rec1@1 fails to tell the difference between d2 and d3, while I-rec2@1 fails to distinguish between d1 and d2. Only N-recE@1 can tell the difference between the three documents, and thus is more discriminative than I-rec.
Another point worth noting is that the types of intent hierarchies are crucial to N-rec. In the right of Figure 2(b), N-recO@1 means using the original intent hierarchy (OIH) instead of EIH. We find that N-recO@1 cannot determine which one of d1 and d2 is better because they have exactly the same score. This indicates that using EIH has higher discriminative power than using OIH.
We aim to retrieve documents that cover as many nodes of intent hierarchies as possible. At the same time, we pre-

419

fer the documents that are highly relevant to more popular nodes and layers. N-rec mainly rewards wide coverage of different nodes of intent hierarchies in the top ranks. In the following, we discuss some measures to complement N-rec.

3.3.3 LD-measures
We use the leaf nodes of intent hierarchies to compute D-measures. Then, LD-measure is defined as:
LD-measure@K = N -rec@K + (1 - )D-measure@K (11)
where  is a parameter controlling the tradeoff between diversity and relevance. Since D-measures only use the leaves of intent hierarchies, LD-measures reward high relevance with more popular leaves, but do not reward high relevance with more popular intermediate nodes. Also, LD-measures cannot handle the weights of layers. To tackle these, we propose HD-measures and LAD-measures in the next section.

3.3.4 HD-measures and LAD-measures
Inspired by D-measures, we define the global gain for an intent hierarchy at rank r as:

 H GGh(r) = wi  GGi(r)
i=1

(12)

where wi is the gain for layer li

weight of layer at rank r. Let

li C

and GGh

(Gr)G=i(r) irks=t1hGeGghlo(bka),l

which is the cumulative global gain for the intent hierarchy

at rank r. Further, let GGh(r) and CGGh(r) denote the global gain and the cumulative global gain for the intent

hierarchy at rank r in the ideal ranked list. The ideal list

is obtained by listing up all the judged documents in de-

scending order of global gains for the intent hierarchy. Let

J(r) = 1 if the hierarchy, and

document at rank r J(r) = 0 otherwise.

iLserteClev(ra)nt=totrkh=e1inJt(ekn)t.

We define HD-nDCG and HD-Q at document cutoff K as:

H D-nDC G@K

=

K K r=1
r=1

GGh(r)/ GGh(r)/

log(r log(r

+ +

1) 1)

(13)

H D-Q@K

=

1 min(K, R)

 K
r=1

J

(r)

C

(r) + CGGh(r) r + CGGh(r)

(14)

where R is the number of judged documents relevant to the

intent hierarchy. We define HD-measure as:

HD-measure@K = N -rec@K +(1-)HD-measure@K (15)

where HD-measure can be HD-nDCG or HD-Q, and  is a parameter controlling the tradeoff between diversity and relevance. Besides, We define LAD-measure as:

LAD-measure@K = N -rec@K + (1 - )D-measure-LA@K

(16)

where  is a parameter balancing diversity with relevance,

and D-measure-LA is the LA version of D-measure.

To measure the relevance of ranked lists, HD-measures

use HD-measures, while LAD-measures use D-measures-LA.

HD-measures and D-measures-LA reward high relevance to

more popular nodes, and can handle layer weights. The

difference between them is what to combine over layers:

HD-measures combine the global gain for each layer while

D-measures-LA combine D-measures for each layer. Take

HD-nDCG and D-nDCG-LA as an example:

H D-nDC G@K

=

K Kr=1
r=1

[[ HiHi==11

wi wi

 GGi(r)]/ log2 (r + 1)  GGi (r)]/ log2 (r + 1)

 H D-nDCG-LA@K = wi  D-nDCGi@K

i=1

where GGi(r) is the global gain for layer li at rank r, and D-nDCGi means only using the nodes of layer li.

Figure 3:

Relationships of D-measures,

LD-measures, HD-measures, and LAD-measures.

3.3.5 Summarization
Since our measures use intent hierarchies, we call them hierarchical measures. Each of D-measures, LD-measures, HD-measures and LAD-measures is a linear combination of two measures: one measure mainly rewards the diversity of ranked lists, whereas another measure mainly rewards the relevance. We show their relationships in Figure 3. It can be seen that (1) To reward the diversity, LD-measures, HD-measures and LAD-measures use the whole intent hierarchy, whereas D-measures only use the leaf nodes; (2) To reward the relevance, HD-measures and LAD-measures use the whole intent hierarchy, whereas D-measures and LD-measures only use the leaf nodes.
4. EXPERIMENTS
4.1 Settings
We experiment with the proposed measures on the TREC Web Track 2009-2013 diversity test collections and the new test collection mentioned in Section 3.2. The new test collection has two types of intent hierarchies, i.e. the original intent hierarchies (OIH), and the extended intent hierarchies (EIH). The results of our measures using OIH are different from those using EIH. In the following, subscript O means using the OIH, while subscript E means using the EIH.
We use uniform probabilities for official intents like in TREC Web Track 2009-2013 diversity task. We use uniform layer weights when computing our measures, and leave the investigation of nonuniform weights to future. Unless stated otherwise, we use document cutoff K = 20 for all measures, and  = 0.5 in Equation (7, 11, 15, and 16).
4.2 Discriminative Power Results
Following the previous work [19, 20, 23, 24, 21], we use the paired bootstrap test and set B = 1, 000 (B is the number of bootstrap samples). When the queries have single-layer intent hierarchies: (1) LA measures are reduced to their corresponding existing measures. For example, D-measures-LA are reduced to D-measures; (2) LD-measures, HD-measures, and LAD-measures are reduced to D-measures. We conduct the experiments as follows: (1) Sampling 20 submitted runs every year (2009-2013), which produces 950 pairs of

420

Table 1: Discriminative power and performance  of diversity measures based on the paired bootstrap test at

 = 0.05. The leftmost column shows existing measures' results; the middle column shows their corresponding

LA measures' results using original intent hierarchies (denoted by subscript O); the rightmost column shows

their corresponding LA measures' results using extended intent hierarchies (denoted by subscript E). For

each row, the greatest value is in bold.

(a) 250 queries in TREC Web Track 2009-2013

existing measures

measures based on OIH

measures based on EIH

measure disc.power required 

measure disc.power required 

measure disc.power required 

I-rec -nDCG ERR-IA nDCG-IA
Q-IA D-nDCG
D-Q

49.1% 56.8% 52.0% 53.4% 43.1% 55.1% 52.7%

0.14

I-rec-LAO

48.5%

0.13

I-rec-LAE

0.11

-nDCG-LAO

56.4%

0.12

-nDCG-LAE

0.12

ERR-IA-LAO

51.1%

0.14

ERR-IA-LAE

0.07

nDCG-IA-LAO

52.4%

0.06

nDCG-IA-LAE

0.06

Q-IA-LAO

42.7%

0.06

Q-IA-LAE

0.09

D-nDCG-LAO

54.4%

0.10

D-nDCG-LAE

0.09

D-Q-LAO

52.7%

0.09

D-Q-LAE

(b) 105 queries that have multilayer intent hierarchies (out of 250)

49.7% 56.0% 52.1% 53.5% 43.5% 55.3% 53.7%

0.13 0.11 0.14 0.06 0.06 0.09 0.08

I-rec -nDCG ERR-IA nDCG-IA
Q-IA D-nDCG
D-Q

36.4% 38.7% 31.9% 29.9% 20.2% 38.9% 38.7%

0.23 0.22 0.19 0.11 0.14 0.17 0.17

I-rec-LAO -nDCG-LAO ERR-IA-LAO nDCG-IA-LAO
Q-IA-LAO D-nDCG-LAO
D-Q-LAO

33.4% 37.8% 31.2% 29.2% 20.3% 36.4% 36.3%

0.26 0.26 0.23 0.13 0.13 0.19 0.16

I-rec-LAE -nDCG-LAE ERR-IA-LAE nDCG-IA-LAE
Q-IA-LAE D-nDCG-LAE
D-Q-LAE

36.6% 37.9% 32.9% 32.3% 22.4% 39.7% 39.3%

0.26 0.18 0.21 0.13 0.14 0.18 0.16

Table 2: Discriminative power (shown in columns A)

and performance  (shown in columns B) of diver-

sity measures ranked by their discriminative power

based on the paired bootstrap test at  = 0.05. Baseline measures are marked by .

(a) 250 queries in TREC Web Track 2009-2013

measure A

B

measure A

B

HD-nDCGE 55.9% 0.11 HD-QE 53.4%

0.09

LD-nDCGO 55.5% 0.10 LAD-QO 53.4%

0.09

LD-nDCGE 55.3% 0.09 LD-QO 53.3%

0.09

LAD-nDCGE 55.2% 0.10 HD-QO 53.1% D-nDCG 55.1% 0.09 LAD-QE 52.9%

0.09 0.10

HD-nDCGO 54.7% 0.10 LAD-nDCGO 54.5% 0.10

LD-QE 52.7% D-Q 52.7%

0.09 0.09

(b) 105 queries that have multilayer intent hierarchies (out of 250)

HD-nDCGE LD-nDCGE LAD-nDCGE
 D-nDCG
LD-nDCGO LAD-nDCGO
HD-nDCGO

40.5% 40.0% 39.1% 38.9% 38.1% 36.8% 36.5%

0.16 0.17 0.19 0.17 0.19 0.21 0.17

LD-QE LAD-QE
HD-QE  D-Q
HD-QO LD-QO LAD-QO

39.4% 39.4% 38.9% 38.7% 38.1% 37.3% 37.1%

0.19 0.19 0.17 0.17 0.17 0.19 0.15

sampled runs in total; (2) With the 950 pairs of sampled runs, computing the discriminative power and performance  using all 250 queries in TREC Web Track 2009-2013 diversity test collections; (3) With the 950 pairs of sampled runs, computing the discriminative power and performance  using the 105 queries that have multilayer intent hierarchies. Performance  is the required value to achieve statistical significance, and is computed following [21]. The results are shown in Table 1 and Table 2.
By comparing the discriminative power scores of existing measures and their corresponding LA measures based on OIH or EIH in each row of Table 1, we find that: (1) Except -nDCG-LA, LA measures using EIH are more discriminative than their corresponding existing measures, especially in the case of IA measures. For example, when experimenting with 105 queries that have multilayer intent hierarchies, Q-IA-LAE (22.4%) outperforms Q-IA (20.2%) in terms of discriminative power; (2) The measures using OIH are less discriminative than the measures using EIH. For example, nDCG-IA-LAO is 29.2% while nDCG-IA-LAE is 32.3% when experimenting with 105 queries that have multilayer intent hierarchies.

By comparing the discriminative power results of D-measures, LD-measures, HD-measures, and LAD-measures (each block in Table 2), we find that: (1) The measures using EIH are generally more discriminative than the measures using OIH; (2) When using EIH, LD-measures, HD-measures and LAD-measures are better than (or at least as good as) D-measures in terms of discriminative power.
By comparing the results using all 250 queries in TREC Web Track 2009-2013 (shown in Table 1(a) or Table 2(a)) and the results only using the queries that have multilayer intent hierarchies (shown in Table 1(b) or Table 2(b)), we find that hierarchical measures have greater improvement of discriminative power than existing measures for queries that have multilayer intent hierarchies. This is reasonable because our measures have potential to recognize the difference between ranked lists by utilizing the hierarchies whereas existing measures cannot. Another justification is that our measures are equivalent to existing measures when the queries only have single-layer intent hierarchies.
The above observations suggest that it is preferable to use EIH when computing hierarchical measures. We think that the hierarchical measures using EIH have higher discriminative power than the hierarchical measures using OIH. For example, Figure 2(b) shows that d1 is more diversified than d2 because d1 refers to two interpretations of the query, while d2 only refers to one of them. N-recE agrees with this but N-recO cannot tell which one is more diversified.
4.3 Intuitiveness
4.3.1 Difference between using OIH and EIH
The hierarchical measures using EIH are more intuitive than using OIH in terms of diversity. Following the previous work [21], we use I-rec as the gold standard measure for the diversity because it does not depend on intent hierarchies OIH and EIH. Table 3 shows the intuitiveness when using all the queries in TREC Web Track 2009-2013 diversity test collections. We find that for a document cutoff K = 10, the hierarchical measures using EIH are more intuitive than using OIH. For a document cutoff K = 20, there is only one exception (LD-nDCG@20).
This is because the hierarchical measures using OIH may reward high relevance to some official intents, and fail to

421

Table 3: Intuitiveness based on preference agree-

ment with I-rec. For each measure pair (using OIH

or EIH), the higher score is shown in bold and

the numbers of disagreements between this pair are

shown in parentheses below.

(a) Document cutoff K = 10. Gold standard measure: I-rec

ERR-

nDCG-

Q-

LD-

HD-

IA-LA

IA-LA

IA-LA

nDCG

nDCG

OIH .663

.624

.653

.802

.025

EIH .724

.748

.696

.841

.999

(4973)

(5492)

(4660)

(2601)

(3421)

(b) Document cutoff K = 20. Gold standard measure: I-rec

OIH .692

.656

.677

.821

.823

EIH .732

.748

.729

.739

.837

(5362)

(6688)

(5273)

(2511)

(5329)

LADnDCG .753 .886 (4948)
.827 .840 (5645)

reward wide coverage of the official intents. Take the OIH in Figure 2 as an example. Since we assume that the documents that are relevant to a node are relevant to its parent node, the relevance assessments for intent i1 or i5 are reflected in the relevance assessments for their parent node n1. Though the first layer of the OIH excludes i1 or i5, it indirectly considers them through their parent node n1. By including the other four intents, the first layer considers all six official intents, but the second layer of the OIH only has i1 or i5. When combining the two layers, the relevance assessments for i1 or i5 are considered twice, once in the first layer and again in the second layer. However, the relevance assessments for the other four intents are only considered once in the first layer. This means that when using the OIH, hierarchical measures mainly reward higher relevance to i1 and i5 than other intents. The EIH in Figure 2 solves this problem by extending i2, i3, i4, and i6 to the second layer so that every official intent can be considered in each layer when evaluating the ranking quality.
In the remaining part of the section, we will only report experimental results using EIH due to space limitation. In most experiments, using EIH yields higher discriminative power and intuitiveness than OIH.
4.3.2 Intuitiveness of Hierarchical Measures
In Section 4.2, we show that LD-nDCGE, HD-nDCGE, and LAD-nDCGE are highly discriminative among hierarchical measures. In this section, we further compare their intuitiveness with some existing measures, including -nDCG, ERR-IA, and D-nDCG. We do the concordance test based on all the queries in TREC Web Track 2009-2013 diversity test collections, and show the results in Table 4. In Table 4(a) and Table 4(b), we use N-recE and Precision as the gold standard measure respectively, whereas in Table 4(c), both N-recE and Precision are used as the gold standard measures. We use N-recE as a gold standard measure in terms of the diversity because: (1) It is a simple binary measures; (2) It measures diversity better than I-rec, which is traditionally used as the gold standard measure for diversity.
Table 4 shows that (1) In terms of the diversity, LD-nDCGE, HD-nDCGE, and LAD-nDCGE are more intuitive than existing measures. This is expected because these hierarchical measures directly depend on N-recE by means of Equation (11) and the like; (2) In terms of diversity, LD-nDCGE is most intuitive; (3) In terms of relevance, HD-nDCGE is most intuitive; (4) In terms of both diversity and relevance, LAD-nDCGE is the most intuitive measure.
Table 4 shows that using the whole intent hierarchies instead of only using the leaf nodes can improve the intuitiveness of measures. HD-nDCGE and LAD-nDCGE use the

Table 4: Intuitiveness based on preference agree-

ment with gold standard measures. For each mea-

sure pair, the higher score is shown in bold and

the numbers of disagreements between this pair are

shown in parentheses below.

(a) Gold standard measure: N-recE ("diversity")

ERR-

D-

LD-

-nDCG

IA

nDCG

nDCGE

.988/.362 .661/.983 .656/.986

(14215)

(43908)

(44098)

ERR-IA

-

.577/.987 .573/.991

-

(56060)

(56011)

D-nDCG

-

-

.428/.612

-

-

(2124)

LD-nDCGE

-

-

-

-

-

-

HD-nDCGE -

-

-

-

-

-

(b) Gold standard measure: Precision ("relevance")

ERR-

D-

LD-

-nDCG

IA

nDCG

nDCGE

.749/.345 .359/.746 .358/.749

(14215)

(43908)

(44098)

ERR-IA

-

.348/.754 .346/.756

-

(56060)

(56011)

D-nDCG

-

-

.488/.592

-

-

(2124)

LD-nDCGE

-

-

-

-

-

-

HD-nDCGE -

-

-

-

-

-

(c) Gold standard measures: N-recE and Precision

ERR-

D-

LD-

-nDCG

IA

nDCG

nDCGE

.738/.085 .156/.731 .154/.735

(14215)

(43908)

(44098)

ERR-IA

-

.126/.742 .124/.747

-

(56060)

(56011)

D-nDCG

-

-

.036/.217

-

-

(2124)

LD-nDCGE

-

-

-

-

-

-

HD-nDCGE -

-

-

-

-

-

HDnDCGE .663/.984 (44522) .578/.990 (56245) .700/.741 (3822) .898/.799 (2356) -
HDnDCGE .358/.751 (44522) .345/.758 (56245) .502/.625 (3822) .523/.629 (2356) -
HDnDCGE .159/.734 (44522) .127/.748 (56245) .267/.371 (3822) .432/.438 (2356) -

LADnDCGE .661/.984 (44444) .577/.990 (56209) .677/.738 (3586) .895/.811 (2026) .724/.915 (330)
LADnDCGE .357/.751 (44444) .345/.758 (56209) .499/.625 (3586) .518/.633 (2026) .603/.552 (330)
LADnDCGE .157/.735 (44444) .127/.749 (56209) .247/.368 (3586) .424/.449 (2026) .370/.476 (330)

whole intent hierarchy to measure both diversity and relevance of ranked lists. LD-nDCGE uses the whole intent hierarchy to measure the diversity but only uses the leaf nodes to measure the relevance. D-nDCG only uses the leaf nodes to measure the diversity and relevance. Table 4 shows that LD-nDCGE, HD-nDCGE and LAD-nDCGE are more intuitive than D-nDCG in terms of diversity. HD-nDCGE and LAD-nDCGE are more intuitive than D-nDCG and LD-nDCGE in terms of relevance. We get the same result when both diversity and relevance are considered.
4.3.3 Case Studies
D-nDCG, LD-nDCGE, HD-nDCGE, and LAD-nDCGE are closely related (shown in Section 3.3.5 and Section 4.4). We examine their differences in terms of intuitiveness by looking at some real examples from the submitted runs in TREC Web Track 2009-2013 diversity task.
Specifically, we select five pairs of real ranked lists from TREC Web Track diversity runs in Table 5, and refer to them as Case A-E. For example, Case A stands for two runs cmuFuTop10D and THUIR10DvNov for No. 77 query; The middle column shows the relevance assessments of the top ten documents in each run (e.g. the first document retrieved by cmuFuTop10D is relevant to intent i4 with a relevance rating 1); The last four columns show the 's for each query (e.g. score of cmuFuTop10D minus that of THUIR10DvNov) where arrows indicate which run has higher score under each measure. Note that in this section, the measures are computed for a document cutoff K = 10 because we only have space to show top 10 documents in Table 5. We categorize five cases into two classes from the viewpoint of diversity (Case A-C) or relevance (Case D-E).

422

Table 5: Five ranked list pairs from TREC Web Track 2009-2013 diversity test collections, document cutoff

K = 10. 1st column: case IDs (query IDs). 2nd column: run IDs. 3rd column: number of official intents

covered by each run. 4th column: number of nodes in extended intent hierarchies covered by each run. 5th

column: relevance ratings for each intent at ranks 1-10. The rightmost column: performance differences

using each measure and arrows point to its preferred run.

A (77) B (77) C (77)

cmuFuTop10D THUIR10DvNov THUIR10DvQEW UAMSD10aSRfu msrsv2div qirdcsuog3

D

qutir11a

(117)

uwBBadhoc

E

2011SiftR2

(128)

UWatMDSdm

1
3 6 i4L1 3 8 i4L1 2 5 i4L1 26
3 8 i4L1 3 7 i3L1
3 5 i1L1 i2 L1
3 5 i1L3 i2 L2 i3 L3
3 5 i1L2 i2 L2 i3 L1
3 5 i1L1 i2 L1

2
i2 L1 i3 L1 i1 L1

3
i2 L1 i3 L1

Document rank

(i: official intents)

4

5

6

7

i2 L1 i1 L1 i3 L1
i1 L1 i3 L1

i3 L1
i2 L1 i1 L1
i1 L2 i2 L1 i3 L1

i1 L1 i3 L1 i4 L1 i2 L1
i2 L1

i1 L1 i3 L1
i1 L2 i2 L2

i1 L2 i3 L2
i1 L1 i3 L1

i1 L1

i1 L2 i2 L2

8
i1 L1 i1 L3 i2 L1

9
i2 L1 i3 L1 i2 L1 i2 L1

10 i1 L1 i2 L1
i2 L1
i2 L1
i1 L1 i2 L2 i3 L1
i1 L2 i2 L2

 in DnDCG 0.0013  0.0300  -0.0329 
-0.0030 
0.0087 

 in LDnDCGE
-0.1098  -0.0256  0.0226 

 in HDnDCGE
-0.0977  0.0011  -0.0115 

 in LADnDCGE
-0.0988  -0.0019  -0.0085 

-0.0030 

0.0171 

0.0148 

0.0087 

-0.0005 

0.0004 

In Case A, we argue that D-nDCG is less intuitive than the other three. THUIR10DvNov covers both "bobcat company" and "wild bobcat" while cmuFuTop10D only covers the former (Please refer to the detailed description for the official intents of No. 77 query shown in Figure 1) although both runs cover three leaf intents. In this sense, THUIR10DvNov is more diversified than cmuFuTop10D and should be preferred. Note that this is also a case where I-rec cannot tell which run is better but N-recE can. The rightmost column of Table 5 shows that only D-nDCG disagrees with this intuition. In Case B, we argue that D-nDCG and HD-nDCGE are less intuitive than the other two. Similar to Case A, UAMSD10aSRfu covers both "bobcat company" and "wild bobcat," whereas THUIR10DvQEW fails to cover the latter. So UAMSD10aSRfu should be preferred, and only LAD-nDCGE and LD-nDCGE agree with this. In Case C, we argue that LD-nDCGE is the most intuitive among the four measures. In this case, both msrsv2div and qirdcsuog3 cover "bobcat company" and "wild bobcat". However, Figure 1 shows that msrsv2div covers both "bobcat tractors" and "bobcat company homepage," which are sub intents of "bobcat company," while qirdcsuog3 does not cover "bobcat company homepage." Because of this, msrsv2div should be preferred and only LD-nDCGE agrees with this.
In summary, from the viewpoint of diversity, LD-nDCGE is the most intuitive measure. HD-nDCGE is less intuitive than LAD-nDCGE, but is more intuitive than D-nDCG.
The two runs in Case D and in Case E have the same I-rec and N-recE, hence the measures' preference is determined by their Precision part (e.g. D-nDCG if it is D-nDCG, and HD-nDCGE if it is HD-nDCGE). In Case D, we argue that D-nDCG and LD-nDCGE are less intuitive than the other two. No matter whether measuring by I-rec or by N-recE, qutir11a and uwBBadhoc are equally good in terms of diversity. However, qutir11a should be preferred because its top ten documents are all relevant, whereas uwBBadhoc only has three. From the rightmost column of Table 5, we find that D-nDCG and LD-nDCGE fail to reflect this. In Case E, we argue that HD-nDCGE is the most intuitive among the four measures. UWatMDSdm should be preferred because it returns much more relevant documents than 2011SiftR2. In this case, only HD-nDCGE successfully recognizes this.

Table 6: Kendall's  / Symmetric ap by averaging over TREC Web track 2009-2013. Values greater

than .950 are shown in bold.

(a) 250 queries in TREC Web Track 2009-2013

ERR-

D-

LD-

HD-

LAD-

-nDCG

IA

nDCG

nDCGE

nDCGE

nDCGE

.923/.870 .840/.796 .845/.796 .843/.792 .844/.793

ERR-IA

-

.772/.699 .780/.706 .779/.704 .779/.706

D-nDCG

-

-

.976/.959 .976/.957 .977/.960

LD-nDCGE

-

-

-

.991/.988 .995/.993

HD-nDCGE -

-

-

-

.995/.994

(b) 105 queries that have multilayer intent hierarchies (out of 250)

ERR-

D-

LD-

HD-

LAD-

-nDCG

IA

nDCG

nDCGE

nDCGE

nDCGE

.872/.802 .812/.747 .821/.755 .821/.758 .822/.759

ERR-IA

-

.701/.609 .714/.624 .712/.626 .714/.628

D-nDCG

-

-

.964/.941 .959/.933 .958/.932

LD-nDCGE

-

-

-

.984/.977 .986/.981

HD-nDCGE -

-

-

-

.996/.995

Generally, from the viewpoint of relevance, LAD-nDCGE is more intuitive than LD-nDCGE. LAD-nDCGE is able to measure the relevance of ranked lists more accurately by considering the whole intent hierarchy, and thus make the measures more consistent with Precision than LD-nDCGE.
4.4 Rank Correlation Results
We compute Kendall's  and ap for different pairs of measures to check the correlation between these measures. Results are shown in Table 6. The table shows that: (1) LD-nDCGE, HD-nDCGE and LAD-nDCGE are less correlated to existing measures, especially when only using the queries that have multilayer intent hierarchies. This is because our measures are able to recognize the subtle difference between ranked lists when multilayer intent hierarchies are used, whereas the existing measures may not. This indicates that our measures are useful and could be supplementary to the existing measures; (2) LD-nDCGE, HD-nDCGE, as well as LAD-nDCGE are more correlated to D-nDCG than -nDCG and ERR-IA. This is because they are different kinds of extensions of D-nDCG. Similar to D-nDCG, they model diversity and relevance in different components separately. They yield the same evaluation results when the queries only have single-layer intent hierarchies. (3) LD-nDCGE and HD-nDCGE are less correlated. As discussed in 4.3, LD-nDCGE prefers highly diversified ranked lists, whereas HD-nDCGE prefers highly relevant ranked lists.

423

5. CONCLUSIONS AND FUTURE WORK
In this paper, we argued that user intents of a query could be hierarchical. We described the concept of hierarchical intents and proposed hierarchical measures that could work with intent hierarchies. We created a new test collection containing intent hierarchies based on the existing TREC Web Track 2009-2013 diversity test collections by grouping the official intents into original intent hierarchies and extending them to extended intent hierarchies. Our experimental results showed that our proposed hierarchical measures can be more discriminative than existing measures which use a flat list of intents and assume the independence among intents. We revealed that LD-nDCG should be used when the diversity of search results is more valued than the relevance, whereas HD-nDCG should be used when the relevance is more important. LAD-nDCG is a better choice when both diversity and relevance are important.
In this paper, we simply assume that the official intents provided in TREC Web Track 2009-2013 diversity test collections are atomic intents. It is possible that some of these intents can be further divided into sub intents. We will investigate this in the future.
6. ACKNOWLEDGMENTS
This work was supported by the National Key Basic Research Program (973 Program) of China under grant No. 2014CB340403, and the Fundamental Research Funds for the Central Universities, the Research Funds of Renmin University of China No. 15XNLF03, the National Natural Science Foundation of China (Grant No. 61502501, 61502502, and 61502503)
7. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM, 2009.
[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 1998.
[3] B. A. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. TOIS, 2012.
[4] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM, 2009.
[5] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR, 2006.
[6] C. L. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2009 web track. In TREC, 2009.
[7] C. L. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In WSDM, 2011.
[8] C. L. Clarke, N. Craswell, I. Soboroff, and G. V. Cormack. Overview of the trec 2010 web track. In TREC, 2010.
[9] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR, 2008.
[10] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In ICTIR, 2009.

[11] V. Dang and B. W. Croft. Term level search result diversification. In SIGIR, 2013.
[12] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In SIGIR, 2012.
[13] Z. Dou, S. Hu, K. Chen, R. Song, and J.-R. Wen. Multi-dimensional search result diversification. In WSDM, 2011.
[14] Z. Dou, R. Song, and J.-R. Wen. A large-scale evaluation and analysis of personalized search strategies. In WWW, 2007.
[15] B. J. Jansen, A. Spink, and T. Saracevic. Real life, real users, and real needs: a study and analysis of user queries on the web. Information Processing & Management, 2000.
[16] K. J¨arvelin and J. Kek¨al¨ainen. Ir evaluation methods for retrieving highly relevant documents. In SIGIR, 2000.
[17] M. G. Kendall. A new measure of rank correlation. Biometrika, 1938.
[18] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In SIGIR, 2006.
[19] T. Sakai. Bootstrap-based comparisons of ir metrics for finding one relevant document. In AIRS, 2006.
[20] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In SIGIR, 2006.
[21] T. Sakai. Evaluation with informational and navigational intents. In WWW, 2012.
[22] T. Sakai, N. Craswell, R. Song, S. Robertson, Z. Dou, and C.-Y. Lin. Simple evaluation metrics for diversified search results. In EVIA, 2010.
[23] T. Sakai and S. Robertson. Modelling a user population for designing information retrieval metrics. In EVIA, 2008.
[24] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In SIGIR, 2011.
[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In WWW, 2010.
[26] R. L. Santos, C. Macdonald, and I. Ounis. Selectively diversifying web search results. In CIKM, 2010.
[27] R. L. Santos, C. Macdonald, and I. Ounis. Intent-aware search result diversification. In SIGIR, 2011.
[28] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz. Analysis of a very large web search engine query log. SIGIR Forum, 1999.
[29] E. Yilmaz, J. A. Aslam, and S. Robertson. A new rank correlation coefficient for information retrieval. In SIGIR, 2008.
[30] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR, 2003.
[31] X. Zhu, A. B. Goldberg, J. Van Gael, and D. Andrzejewski. Improving diversity in ranking using absorbing random walks. In HLT-NAACL, 2007.

424

When does Relevance Mean Usefulness and User Satisfaction in Web Search?

Jiaxin Mao, Yiqun Liu, Ke Zhou , Jian-Yun Nie#, Jingtao Song, Min Zhang,
Shaoping Ma, Jiashen Sun, Hengliang Luo
Tsinghua National Laboratory for Information Science and Technology, Department of Computer Science & Technology, Tsinghua University, Beijing, China
Yahoo! Research, London, U.K. #Université de Montréal
Samsung R&D Institute China - Beijing
yiqunliu@tsinghua.edu.cn

ABSTRACT
Relevance is a fundamental concept in information retrieval (IR) studies. It is however often observed that relevance as annotated by secondary assessors may not necessarily mean usefulness and satisfaction perceived by users. In this study, we confirm the difference by a laboratory study in which we collect relevance annotations by external assessors, usefulness and user satisfaction information by users, for a set of search tasks. We also find that a measure based on usefulness rather than relevance annotated has a better correlation with user satisfaction. However, we show that external assessors are capable of annotating usefulness when provided with more search context information. In addition, we also show that it is possible to generate automatically usefulness labels when some training data is available. Our findings explain why traditional system-centric evaluation metrics are not well aligned with user satisfaction and suggest that a usefulness-based evaluation method can be defined to better reflect the quality of search systems perceived by the users.
Keywords
Relevance; Usefulness; User satisfaction; Evaluation
1. INTRODUCTION
Relevance, which "expresses a criterion for assessing effectiveness in retrieval of information, or of objects potentially conveying information" [37], is a central concept in IR and plays an important role in search engine evaluation. It is well known that this notion involves multiple aspects. In the traditional system evaluation paradigm [10, 43], in order to compare the performances of different search systems, we typically rely on a test collection that consists of a document corpus, a set of predefined statements of information needs, and a set of relevance judgements. Based on the relevance judgements of query document pairs, evaluation metrics, such as MAP, NDCG [21], and ERR [7], are computed for the ranked lists returned by different systems. Each of these measures is defined according to a different user model, which describes how the user interacts with
Yiqun Liu is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911507

the ranked list [33], and links the document-level relevance judgments with an estimation of the query-level user satisfaction [1, 28].
Conceptually, the relevance judgements are expected to represent users' opinions about whether the retrieved documents are relevant and meet users' information needs [43, 44] and should be made by the users themselves. However in practice, it is usually hard to collect relevance feedbacks directly from actual search users, especially in Web search. We therefore ask external (secondary) assessors to make the relevance judgements instead. In this case, there is a high risk that the collected relevance judgments may not necessarily reflect the user-perceived usefulness of retrieved documents. This is due to several reasons. On the one hand, in general, the assessors do not originate the information needs themselves and thus may not fully understand what the user actually wants. It has been indeed questioned whether the search intent can always be captured by the assessors [42]. On the other hand, conventionally the relevance judgments are made in a much simplified environment in which the assessor is asked to judge the relevance relation between each query-document pair independently. The assessor does not have access to much contextual information that may affect relevance judgment, such as the queries the user issued previously in the session, the documents examined or clicked by the user, and so on. In addition, the assessor is only provided with a single short query, which may hardly describe accurately the user's information need. In reality, the Web search engine users often issue multiple queries in a search session [38], especially for exploratory and struggling search tasks [19]. It has been well documented that there are dependency and redundancy among the result documents [5, 9] . When all these contextual factors are ignored, it is very difficult for the assessor to put herself in the shoe of the user to make correct relevance judgment.
The lack of contextual information and accurate description of the information need often leads the assessor to limit herself to judge the topical aspect of relevance, which is different from the highly situational, subjective, user-perceived usefulness. This difference can be easily observed when the relevance judgment of the assessor is compared to the usefulness feedback from the user. Table 1 shows a search session collected in our experimental study in which we collect assessor's relevance judgments and user's usefulness feedback (see Section 3 for more details). Given a search task, a user issued two queries and viewed several results. For the first query, baggage restrictions, the user clicked on two results in order. The contents of these two documents are very similar and both are topically related to the query. The assessor judged both document to be "highly relevant". However, the user judged the first document to be more useful than the second. This may be due to the fact that the second result does not contain

463

Table 1: An example session showing the difference between

user's feedbacks on usefulness and assessor's relevance labels.

Search Task:

You are going to US by air, so you want to know what restrictions there are for both checked and carry-on baggage during air travel.

Query Logs:

Query #1 Click #1
Click #2

baggage restrictions

Checked baggage policy - American Airlines

Relevance: 4(Highly)

Usefulness: 3(Fairly)

Air Canada - BaggageInformation

Relevance: 4(Highly)

Usefulness: 2(Somewhat)

Query #2 carry-on baggage liquids ... Click #3 The Best Way to Pack a Suitcase

Relevance: 2(Somewhat) Usefulness: 4(Very)

much novel information after reading the first one. As for the second query, the user clicked on the result titled The Best Way to Pack a Suitcase. From the assessor's point of view, this document is not so relevant to the query carry-on baggage liquids, but the user finds it very useful when he or she is preparing for an air trip (this is part of the task specification missing in the short query). In these examples, we clearly spot that the usefulness of a document is dependent on previously read contents and on the accurate specification of the search task, and therefore, is different from its topical relevance. In this paper, the difference on relevance judgments between the user and the assessors will be further analyzed.
A number of existing studies have noticed the differences between users' and external assessors' relevance labels. Vakkari and Sormunen found that the relevance criterion of some users is more liberal than that used by TREC assessors [41]. Al-Maskari et al. [2] also compared the differences in relevance labeling process between users and TREC assessors, and observed that various factors, such as the number of retrieved relevant documents and the ranking of relevant documents (i.e. context of the current document), contribute to the differences. Although these previous studies show that users' judgements are different from the assessors', they do not attempt to propose a new way to evaluate systems to better correspond to user's perception. Yilmaz et al. [47] compared document usefulness for users (called utility in their paper) with relevance annotation by assessors, which is in line with our work. They come to an interesting conclusion that some of the differences between user's usefulness and assessor's relevance are caused by the amount of effort required to find the relevant information in a document. They used dwell time as a sign of usefulness. In our work, we collect users' explicit feedback information, which is expected to be more reliable than implicit behavior signals. The idea of replacing relevance-based measurements with usefulness-based ones is also proposed by Belkin et al. [3] and Cole et al. [13]. However, the idea has not been implemented and no experimental study has been carried out so far on realistic data.
In this paper, we examine the relationship between relevance, usefulness and user satisfaction in a realistic Web search setting. In particular, we design a protocol to collect data 1 containing both (1) user's explicit feedbacks on document usefulness and user satisfaction, which will be considered as ground truth; and (2) external assessor's relevance judgments. Based on the collected data, we examine the following research questions in order to see if usefulness should be used for system evaluation rather than annotated relevance: RQ1 What is the difference between user's perceived usefulness
and the external assessor's relevance annotation? RQ2 How do document's usefulness perceived by the user and
relevance annotated by the assessors correlate with user's satisfaction?

1This dataset and the detailed instructions used to construct the dataset are publicly available on https://github.com/THUIR/UsefulnessUserStudyData

However, in a practical Web search setting, it is impossible to ask users to provide explicit feedback on usefulness. We have to resort to an alternative approach. This motivates us to examine the following two additional research questions: RQ3 Can we rely on external assessors to make reliable and valid
assessments for the document-level usefulness? RQ4 Can we automatically generate usefulness labels based on
user behavior and search context features? Regarding RQ3 and RQ4, we propose two approaches that can collect usefulness labels in practical Web search settings. The first approach relies on manual labeling by external assessors who are provided with more information about the search task and the search context. We will show that the usefulness annotation in this case is reasonably close to estimate user-perceived usefulness. The second approach goes a step further by utilizing machine learning techniques based on user behavior data to automatically generate usefulness labels. We show that this approach is feasible when a small amount of training data is available. Such an automatic usefulness labeling approach can help save the tedious work of manual labeling. By answering these research questions, we aim to propose a new evaluation framework in which usefulness, instead of the current simplified relevance, is used. Based on manually labeled or automatically generated usefulness labels, evaluation metrics in this new framework are expected to better correlate with users' feelings of satisfaction, which will be confirmed in our experiments. We do not hope to fully replace the current practice of relevance judgment with usefulness assessment in all situations. Instead, we hope to show that in certain circumstances where usefulness information can be collected or deduced, evaluation based on usefulness assessment can better reflect users' opinions. The rest of this paper is organized as follows: Related studies are discussed in Section 2. In Section 3, we describe the experiment design and the data collecting procedure. In Section 4, we compare user's usefulness feedback with assessor's relevance annotation to answer RQ1. In Section 5, we characterize the relationship between document-level measures and user satisfaction, and answer RQ2. To answer RQ3 and RQ4, we propose and test two approaches for acquiring usefulness labels, manually or automatically, in Section 6. Finally we draw conclusions and provide future work directions in Section 7.
2. RELATED WORK
Our work is related to a broad range of IR evaluation studies. In the traditional system-centric Cranfield-style evaluation [10, 43], most evaluation metrics are based on an implicit user model describing how the user interacts with a SERP [33]. They assess and summarize the effectiveness at query level. Recent studies extend the Cranfield-style evaluation paradigms to (1) cope with the redundancy and diversity of documents [9, 35]; (2) assess the overall effectiveness in a search session [6, 22, 25]. On the other hand, the user-centric evaluation draws more and more attention along with the emergence of Web search engines. It has been argued for a long time that instead of relevance, usefulness (or utility) should be used as a measure of retrieval effectiveness [13, 14]. Using the user behavior information that can be implicitly collected at a large scale, the utility or usefulness of a document (sometimes referred to as click satisfaction or intrinsic relevance) are estimated [4, 8, 24, 46]. These studies are based on the assumption that there are correlations between user behaviors and usefulness of documents. We will further investigate these correlations in Section 6.2. Our work is complementary to the existing work in the following ways: (1) instead of relying on natural log data from a search engine, we collect explicit usefulness feedbacks as well as comprehensive user behavior and search context information in a laboratory user study, which allow us to investigate the correlations between different notions. (2) Different from the studies that focus on predicting user satisfaction and search success at query- or

464

Table 2: Descriptions of major measures used in this work.

Concepts Measures

Descriptions

Relevance Relevance

4-level graded relevance annotations

annotation (R)

made by external assessors in Stage II.1

(see Figure 1 and Section 3.2).

Usefulness

4-level graded usefulness feedbacks

Usefulness feedbacks (Uu)

collected in Stage I.4 (see Figure 1 and Section 3.2). We use them as the ground

truth labels for usefulness.

Usefulness

4-level graded usefulness annotations

annotation (Ua)

made by external assessors reviewing augmented search logs in Stage II.2 (see

Figure 1 and Section 3.2).

Usefulness

Predicted usefulness labels. We utilize

prediction

or a machine learning method and different

predicted usefulness combinations of features to predict

(UQ, UQ+S, etc.) Query-level Query-level

usefulness (See Section 6.2). 5-level graded satisfaction feedbacks for

Satisfaction satisfaction

each issued query collected in Stage I.4

feedbacks (QSATu) Query-level

(see Figure 1 and Section 3.2). 5-level graded satisfaction annotations

satisfaction

made by external assessors in Stage II.2

Task-level

annotation (QSATa) (see Figure 1 and Section 3.2).

Task-level

5-level graded satisfaction feedbacks for

Satisfaction satisfaction

each search task collected in Stage I.4

feedbacks (T SATu) Task-level

(see Figure 1 and Section 3.2). . 5-level graded satisfaction annotations

satisfaction

made by external assessors in Stage II.2

annotation (T SATa) (see Figure 1 and Section 3.2).

task-level in Web search [18, 23, 32, 34], our primary goal in this study is to exploit the possible correlations between document-level measures and query- and task-level user satisfaction so that (1) we can understand the relationship between document utility and user satisfaction; and (2) we can derive appropriate usefulness measures.
Some existing studies investigated the relation between the system-centric and user-centric evaluation by comparing the system-centric evaluation metrics, usually based on relevance, with user performance, satisfaction, or preference [1, 20, 28, 36, 40]. In a recent work, Yilmaz et al. [47] compared document utility with document relevance. They showed that the required effort plays an important role in the degree of document utility perceived by a real search user. Our work also extensively compares document usefulness with its annotated relevance, and extends their work as follows: (1) instead of relying on the dwell time as a surrogate for utility, we collect user's explicit feedbacks of usefulness; (2) in addition to the required effort and dwell time, we also consider other factors, such as the the current search task and the redundancy with previous documents read by the user, as they have been shown to influence document usefulness perceived by the user.
Usefulness and satisfaction are both subjective. Therefore, our work is also indirectly related to the field of personalized search [15, 39]. As personalized search aims to take into account the diverse and volatile information needs from different users, to make our study more reliable, we control this variability by designing predefined and clearly stated search tasks for the participants, which differentiate our work from most personalized search studies.
3. DATA COLLECTION
As shown in Figure 1, the data collection procedure consists of two parts: I. User Study and II. Data Annotation. The first part is collected in a laboratory environment. We collected users' behavior logs and their explicit feedbacks for both usefulness and satisfaction. In the second step, we hired external assessors to generate corresponding relevance annotations. To investigate RQ3 and RQ4, we also asked the assessors to provide their usefulness and satisfaction annotations. We use these feedbacks and annotations as measures for relevance, usefulness or satisfaction. Table 2 provides a summary of these measures.

Table 3: Examples of search tasks. The TREC topic indexes

are given in parentheses ().

Init. Query

Description

baggage

You are going to US by air, so you want to know what

restrictions

restrictions there are for both checked and carry-on baggage

during air travel. (2010-7)

long-term care You just learned about the existence of long-term care

insurance

insurance and want to know about it: costs / premiums,

companies that offer it, types of policies, people's opinion

about long term care insurance; what are the differences

between long term care and health insurance? (2013-8)

quit smoking

Your friend would like to quit smoking. You would like to

provide him with relevant information about: the different

ways to quit smoking, benefits of quitting smoking, second

effects of quitting smoking. (2013-12)

3.1 User Study Design
In the laboratory user study, each participant was asked to complete 12 search tasks using an experiment search engine system. Compared with collecting data from real search logs [23, 34], or by browser plugins [16, 45], the laboratory user study had a smaller scale, but enabled us to fully control the variabilities in search tasks and information needs as well as to collect explicitly the information needed.
To simulate a real Web search environment, we built an experimental search engine that can access the open Web. As shown in Figure 1(I.3), this experimental search engine has an interface similar to common Web search engines, and supports query reformulation and pagination. When the user issues a query, or clicks a pagination link, the experimental search system will forward the request to a commercial search engine in real time, and retrieve the corresponding search engine result page (SERP). To control the variability in presentation styles, all query suggestions, ads, sponsor search results, and vertical results in the retrieved SERP are removed, only the remaining organic results are returned to the user. We also store these organic results in our system, not only for further annotation and analysis, but also to make sure that if another participant issues the same query, he or she will be shown the same SERP. A javascript plugin was injected into the returned SERP to log users' search behaviors including query reformulation, click, scrolling, tab switching and mouse movement.
12 search tasks were selected from the topics of TREC Session Track 2010-20142. Several criteria were considered when selecting the search tasks. Firstly, a search task should be clearly stated so that different participants could interpret the task description in the same way. Secondly, the difficulty and complexity of a search task should be appropriate. The search task should be neither too time-consuming, nor so easy that only requires one query and a few clicks on top results to complete. A pilot experiment was conducted to test whether these criteria were met. Based on the result of the pilot experiment, we made necessary modifications to the original TREC task descriptions to adjust the difficulties and complexities. We further provided an initial query for each search task. While providing initial query might threaten the ecological validity of our experiment, it can make sure that all participants will see the same initial SERP for each search task, and thus effectively prevent potential topic drifts. Table 3 shows some examples of the selected search tasks.
We recruited 29 undergraduate students, via emails and online social networks, to take part in the user study. 15 participants were female and 14 were male. The ages of participants range from 18 to 26. The distribution of their major is: 15 in engineering, 10 in humanities and social sciences, and 4 in designs and arts. All the participants were familiar with basic usage of Web search engines, and most of them reported using Web search engines daily.
Each participant was asked to complete all of the 12 search tasks in a random order. As shown in Figure 1(I), to make sure that every participant was familiar with the experiment procedure,

2http://trec.nist.gov/data/session.html

465

I.  User Study
I.1  Pre--experiment Training
I.2  Task Description Reading and Rehearsal

II.  Data Annotation

24 assessors

II.1  Relevance   Annotation

I.3  Task  Completion  with   the   Experimental  Search  Engine

II.2  Satisfaction  &   Usefulness  Annotation

I.4  Satisfaction  and   Usefulness   Feedback

I.5  Post--experiment Question

29 participants

Figure 1: Data collection procedure. With enrolled participants, we collected behavior logs and feedback data in I. User Study. With hired external assessors, we generated relevance, usefulness and satisfaction annotation data in II. Data Annotation.

an example task was used for demonstration in the Pre-experiment Training stage (I.1). For each search task, the participant had to go through 4 different stages (I.2-I.5). Firstly, the participant should read and memorize the task description (note that the complete task description is provided to the participant). After that, s/he was required to re-input the task description without viewing it again during searching (I.2). Then s/he would be redirected to the SERP of the initial query, and start completing the search task (I.3). The participant could click on the results and submit new queries freely, just like using a normal Web search engine. While no task time limits were imposed, s/he could stop searching and click the finish button when s/he thought the task was completed, or no more helpful information would be found. After the task completion stage, the participant was required to review the search process and provide explicit feedbacks (I.4). Figure 1(I.4) shows the interface for collecting usefulness and query-level satisfaction feedbacks. We used a 4-level graded usefulness feedback (Uu, 1: not useful at all; 2: somewhat useful; 3: fairly useful; 4: very useful) since we aim to compare it against 4-level graded relevance annotation [26]. We used a 5-level graded query-level satisfaction feedback. The 5-level satisfaction scale and instructions are in accordance with those introduced by Liu et al. [32]. We only collected usefulness feedbacks for documents that were clicked by that particular participant in the task completion stage. After reviewing all issued queries, the participant would further submit a 5-level task-level satisfaction feedback (T SATu). The explicit feedback stage was immediately after, but did not interfere with, the search process. We believe such an experiment design could collect most accurate feedbacks while introduce a minimal interference to users' search behavior. A question answering stage was put at the end of each search task (I.5). The participant must answer a question related to the search task (e.g. "Please provide three suggestions for quitting smoking." for the task "quit smoking") in voice. In the pilot test we found that the voice question answering introduces much less cognitive cost than requiring the participants to use keyboard to input the answers, and it can effectively ensure that the participants indeed put some effort in finishing the search task.
3.2 Data Annotation
After collecting the search behavior logs and user feedbacks in the user study, we hired external assessors to generate: (1) relevance annotations (R) for all the documents that were clicked

Annotation Instructions: Search Task: You are going to US by air, so you want to know what restrictions there are for both checked and carry-on baggage during air travel. The left part shows the issued queries and clicked documents when a user is doing the search task via a search engine, you need to complete the following 3step annotation: STEP1: Annotate the usefulness of each clicked document for accomplishing the search task:
1 star: Not useful at all; 2 stars: Somewhat useful; 3 stars: Fairly useful; 4 stars: Very useful. STEP2: Annotate query-level satisfaction for each query (1 star: Most unsatisfied - 5 stars: Most satisfied) STEP3: Finally, please annotate the task-level satisfaction (1 star: Most unsatisfied - 5 stars: Most satisfied) Completed units/all units0/29
Figure 2: Annotation instructions shown to assessors.
by users or shown in the top 5 positions of a SERP; (2) usefulness annotations (Ua) for all clicked documents; (3) query-level satisfaction annotations (QSATa) for all issued queries; and (4) task-level satisfactions (T SATa) for all search sessions.
Figure 1(II.1) shows the interface for relevance annotation. A relevance annotation unit consists of a query-document pair. For each unit, we showed the short query and the snippet of the document, in a single page, to the assessors. The assessors were required to click and examine the document and make a 4-level graded relevance judgment (1: irrelevant; 2: somewhat relevant; 3: fairly relevant; 4: highly relevant). The relevance scale and annotation instructions are similar to those introduced by Kekäläinen et al. [26] and are also consistent with the current practice in Web search. Some documents were not accessible during the annotation process because the page had been removed or deleted. So we asked the assessor to check the Invalid document? checkbox when s/he could not access the document.
The annotation unit of usefulness and satisfaction annotation is a search session in which a participant completed a single search task (with full task description). As shown in Figure 1(II.2), for each annotation unit, we showed an augmented search log, along with the instruction and the search task description (see Figure 2), to the assessor. All queries and clicked documents in the log were presented in the same order as when the participant issued and clicked them in the user study. To imitate the search process and reproduce the search context, the assessors were instructed to inspect the search session and judge the document-level

466

Table 4: Statistics of behavior logs.

#tasks #participants #sessions #queries #clicks

9

25

225

935

1,512

Table 5: Statistics of annotation data.

Rnc

Rc

Ua

QSATa T SATa

#Annotations 1,944 1,161 1,512 935

225

Weighted  0.344 0.413 0.530 0.535 0.274

usefulness, query-level satisfaction and task-level satisfaction sequentially. Similar to the laboratory user study, we used the same 4-level graded scale for usefulness, and 5-level graded scale for satisfaction. We also showed behavioral information including the query dwell time, click dwell time and the ranks of clicked documents to the assessors. The above annotation approaches are consistent with the existing studies [23, 29, 32] on user satisfaction.
24 assessors were enrolled in the data annotation tasks. They were all graduate, or senior undergraduate students. We randomly assigned 9 of them to complete the relevance annotation task, and 15 of them the usefulness and satisfaction annotation task.
3.3 Quality Control and Data Filtering
To make sure the data annotations are reliable, we ensured that each unit was judged by at least 3 different assessors. As the annotations are ordinal, we applied Cohen's Weighted  [11] to assess the inter-assessor agreements. This requires a weight matrix W to indicate how severe a disagreement is. We chose to use the difference on the ordinal scale as the values in W .
After a careful inspection of the annotation data, we filtered out three search tasks: one search task which contained a considerable number of invalid documents; and two other search tasks for which there were many documents with commercial intents, and the assessors had difficulties in determining whether they were spams or not. While these tasks represent real search situations, we judge that the collected judgments are not reliable enough to serve as ground truth, so they are discarded. We also examined the search log collected in the user study, and removed the data generated by 4 participants who did not put sufficient effort in search tasks. They completed search tasks in a significantly shorter time than other participants, and gave very vague answers in the question answering stage.
Summary
Through the user study, data annotation and filtering, we collected user behavior logs, users' explicit feedbacks for usefulness and satisfaction, and a set of corresponding annotation data from external assessors. The statistics of the behavior logs are shown in Table 4. The number of collected relevance, usefulness and satisfaction annotations are shown in Table 5. We separate the assessor's relevance annotations R into two groups: Rc and Rnc. Rc are the relevance annotations for clicked documents, which will be compared with usefulness measures. Rnc are the relevance annotations for the documents that were among the top 5 results of a query, but never clicked by a user. We also list the average Weighted  for each kind of annotations. According to Landis et al. [30] 3 , fair inter-assessor agreements between assessors are reached for Rnc and T SATa, and moderate agreements are reached for Rc, Ua, and QSATa , which indicates the annotation data are of reasonable quality.
4. USEFULNESS V.S. RELEVANCE
Based on the data collected, we first investigate the difference and relationship between assessor's relevance and user's usefulness to answer RQ1. In this work, we use usefulness feedbacks (Uu) as the ground truth labels for usefulness, to which the relevance annotation (R) for each clicked document, will be compared.
3Landis et al. [30] characterize  values < 0 as no agreement, 0 - 0.20 as slight, 0.21 - 0.40 as fair, 0.41 - 0.60 as moderate, 0.61 - 0.80 as substantial, and 0.81 - 1 as almost perfect agreement.

Figure 3: Marginal distributions of the relevance annotations (R), usefulness feedbacks (Uu) and usefulness annotations (Ua) for the clicked documents. The values from 1 to 4 mean respectively not relevant/useful, somewhat relevant/useful, fairly relevant/useful, very relevant/useful.
Figure 4: Joint distributions of document-level measures for clicked documents. Darker color indicates a higher frequency. (a) joint distribution of relevance annotations (R) and usefulness feedbacks (Uu); (b) joint distribution of usefulness annotations (Ua) and usefulness feedbacks (Uu).
The marginal distribution of R and Uu are shown in Figure 3. Note that the distributions are computed per click, so only the relevance annotations of clicked documents (Rc in Section 3.3) are used. We can see an obvious difference between these two distributions (Chi-Square test, 2(3, N = 1, 512) = 874, p < 0.001). For relevance R, nearly 50% of clicked documents are annotated as fairly relevant (R = 3). This is not surprising because all these documents ranked in high positions by a commercial search engine are topically related to the short query. Meanwhile, for usefulness feedbacks Uu, we spot a nearly uniform distribution with a little more clicks with Uu = 1, which implies that the user knows clearly whether an examined document is useful or not. As there are only a few clicks on the document with R = 1 (4.3%), and a considerable number of clicks (32.3%) are reported as Uu = 1, we can conclude that a large proportion of the documents considered relevant by the assessors may not be useful to users.
To study the correlation between Uu and R, we compute Pearson's correlation coefficient r and Cohen's Weighted  between these two document-level measures. A moderate positive correlation is detected, r(1, 510) = 0.3324, p < 0.001, two tails. The computed Weighted  is 0.209 ( = 0.017), just reaching a fair agreement level [30]. We plot the heat map for the joint distribution of Uu and R in Figure 4 (a) and find that R and Uu are not aligned well. Except for the document with perfect relevance (R = 4), other documents are likely to be not useful at all (Uu = 1). Even for the clicks on documents with fair relevancy (R = 3), 29.3% are not useful from the users' perspective. However, only a few clicked documents have low relevance (R  2) and high usefulness (Uu  3), suggesting that high relevance is a necessary condition for high usefulness. This finding may explain why some implicit signals for high usefulness (e.g. long dwell time [4, 46], and last click [8, 24]) could be used as positive implicit relevance feedbacks in previous studies.
We are now interested in understanding why Uu and R are not aligned. We manually inspected the clicks with low relevance
4The degree of freedom is given by #clicks - 2

467

Table 6: Correlations with query-level satisfaction feedback QSATu.
All correlations (measured in Pearson's r) are significant at p < 0.001 . (or )

indicates the difference is significant at p < 0.05(p < 0.01), comparing to the same

metric based on relevance annotation R.

All Queries

(d f = 933)

cCG cDCG cMAX

Uu 0.572 0.724 0.751

R 0.425 0.498 0.563

cCG/#clicks

0.733 0.551

Queries with only top

5 clicks (d f = 635)

Uu 0.647 0.747 0.759

R 0.499 0.535 0.599

0.751 0.587

MAP@5

-

DCG@5

-

E RR@5

-

Weighted Rel. [20] -

0.192 0.295 0.258 0.229 -

0.255 0.363 0.332 0.273

(R  2) and high usefulness (Uu  3), and the clicks with high relevance (R  3) and low usefulness (Uu  2). We find that the major reason for the users reporting that a document with low relevance is actually very useful, is that the document is useful for the overall search task but not so relevant to the current issued query (e.g. Click 3 that we showed in Table 1). On the other hand, the users will report low usefulness for some relevant documents because (1) the document is redundant in content with previously seen documents in the search session [5]; (2) the dwell time on the document is short, the user might not read it as carefully as the assessors did in relevance annotation process [47]. These observations confirm once again that the query-level relevance judgments are unable to fully capture user's perceived usefulness.
Summary
To summarize, regarding RQ1, we find that although there is a moderate positive correlation between assessor's relevance annotation R and user's usefulness feedback Uu, there is a significant gap between these two document-level measures in our dataset. High relevance seems to be a necessary but not sufficient condition for high usefulness, which explains the success of the previous approaches using positive usefulness feedback as positive relevance feedback. The differences observed between assessor's relevance annotations and user's usefulness judgments also suggest that a system evaluation directly based on usefulness may be more appropriate.
5. RELEVANCE, USEFULNESS AND USER
SATISFACTION
As stated by Kelly [27], "satisfaction can be understood as the fulfillment of a specified desire or goal". Satisfaction attempts to gauge users' actual feelings about the system. It is becoming an important criterion in the user-centric evaluation for Web search engines [1, 20]. As we observed in Section 4 that at document -level, user reported usefulness Uu is not well aligned with annotator's relevance annotation R, we further investigate their correlations with query-level and task-level user satisfaction (QSATu and T SATu) to answer RQ2.
To do this, first we need to introduce some evaluation metrics to link document-level measures with query-level and task-level user satisfaction. In traditional batch evaluation paradigm, evaluation metrics, such as NDCG, MAP, and ERR, are used to summarize document-level relevance annotations to estimate query-level satisfactions. We refer to these classic metrics as rank-based metrics. On the other hand, the click-sequence-based metrics are computed based on the click sequences and document-level measures (i.e. usefulness or relevance) of clicked documents. We believe that this latter type of measure can better capture user satisfaction.
5.1 Correlation with Query-level Satisfaction
For query-level satisfaction, we use four click-sequence-based metrics: cCG, cDCG, cMAX, and cCG/#clicks. Click cumulated

gain (cCG) for a query measures the total information gain, or

utility, after submitting the query and viewing all the clicked

documents in sequence. It is computed by summing up the

document-level measures for all clicks under that query [23, 32]:
|CS|

cCG(CS, M) =  M(di) i=1
Here, CS = (d1, d2, . . . , d|CS|) is the click sequence in which each

element di is a clicked document. M(di) is the document-level measure for document di. In this section, M can be either relevance annotation R or usefulness feedback Uu. cCG/#clicks is the average gain per click. Click discounted cumulative gain

(cDCG) is defined as:

|CS|
cDCG(CS, M) =

M(di)

i=1 log2(i + 1)

cMAX assumes that the user's satisfaction is largely dependent on

the most relevant or useful document s/he finds. It is given by:

cMAX(CS, M) = max(M(d1), M(d2), . . . , M(d|CS|))

We also use four rank-based metrics: MAP@5, DCG@5, ERR@5 and Weighted Relevance introduced by Huffman et al. [20]. All these metrics use cut-off at rank 5, because we only collected relevance annotations for top 5 documents in the relevance annotation stage (see Section 3.2). We do not use nDCG [21] here, because the computation of ideal DCG is biased when we do not have an exhaustive list of relevant documents.
As we only have usefulness measures for clicked document, we compare the click-sequence-based metrics based on usefulness feedback Uu with those based on relevance annotation R. We compute their correlations with query-level satisfaction QSATu, and use the rank-based metrics based on relevance annotation R as references. In order to compare two correlation coefficients (rs), we construct a t-statistic to test the significance of the difference between dependent r's [12]. As the cut-off of 5 for rank-based metrics may affect their correlations with satisfaction, especially when the user goes deeper than rank 5, we further compute and report the correlations for 637 queries that only has clicks among top 5 results.
The correlations are shown in Table 6. First, we can see that the correlations between QSATu and the click-sequence-based metrics (shown in upper part of Table 6) are stronger than those between QSATu and rank-based metrics (shown in the lower part of Table 6). The best rank-based metric is DCG@5 with r(933) = 0.295 (the degrees of freedom is given by #queries - 2). However, all the click-sequence-based metrics are more positively correlated with QSATu than rank-based metrics, with all differences being significant at p < 0.001, two-tailed. Second, the click-sequence -based metrics based on Uu are more correlated with QSATu than those based on R, with all the differences between two counterparts being significant at p < 0.01, two-tailed. cCG(Uu), cDCG(Uu) and cCG(Uu) are strongly correlated with QSATu, with r(933) > 0.7. This result shows that user usefulness feedback is a much better indicator of user satisfaction than assessor's relevance annotations. Third, for the queries with only top 5 results clicked, the correlations between QSATu and the rank-based metrics are slightly stronger than those for all queries; but they are still much weaker than those between click-sequence-based metrics and QSATu, with all differences being significant at p < 0.01, two-tailed. This suggests that click-based metrics can better capture user perceived satisfaction.
5.2 Correlation with Task-level Satisfaction

For task-level satisfaction, we only use four click-sequence-based metrics: sCG, sCG/#queries, sCG/#clicks, and sDCG. sCG is defined as the sum of each query's gain [22]. We use cCG to measure a query q j's gain. So sCG is computed by:

n

n

sCG(M) =  gain(q j) =  cCG(CS j, M)

j=1

j=1

468

Table 7: Correlations with task-level satisfaction feedback

T SATu.

Measured in Pearson's r(d f = 223). The darker and lighter shadings indicate

the correlation is significant at p < 0.01 and 0.05. (or ) indicates the difference

is significant at p < 0.05(p < 0.01), comparing to the same metric based on

relevance annotation R.
sCG sCG/#queries sCG/#clicks sDCG

Uu 0.110
0.437
0.525 0.317

R -0.046 0.330 0.320 0.142

Here n is the number of queries in the session. CS j is the click sequence for q j. sCG/#queries and sCG/ #clicks measure average gain per query and per click. sDCG [22] discounts the gains for
later queries in a search session:

sDCG(M)

=

n

j=1

gain(q j) 1 + log( j)

=

n

j=1

cCG(CS j, M) 1 + log( j)

The correlations between these click-sequence-based metrics and the task-level satisfaction feedbacks T SATu are shown in Table 7. Except for sCG, the other metrics significantly correlate with T SATu. The click-sequence-based metrics based on Uu are significantly more correlated with T SATu than their counterparts based on R (with p < 0.01, two-tailed). sCG(Uu)/#clicks is moderately correlated with task-level satisfaction T SATu, with r(223) = 0.525.
Summary
In this section, regarding RQ2, we compare a variety of evaluation metrics based on either user's usefulness feedbacks Uu or assessor's relevance annotation R with query-level satisfaction feedbacks QSATu and task-level satisfaction feedbacks T SATu. Comparing to the rank-based metrics, the click-sequence-based metrics are more related to users' query-level satisfaction feedbacks. Comparing to relevance, usefulness has a stronger correlation with user satisfaction in all metrics. These empirical results further suggest that: (1) when the click sequence is known, we can exploit click-sequence-based metrics to make a better user-oriented evaluation; (2) usefulness can better reflect user's real feelings in Web search than assessor's relevance.
6. COLLECTING USEFULNESS LABELS
In Section 4 and 5, we showed that there is a significant difference between assessor's relevance and user's usefulness. Although usefulness may be more suited for evaluating the Web search engine, it is unrealistic to collect explicit usefulness feedback from users. We have to resort to alternative approaches to assess and acquire document-level usefulness labels. In this section, with regard to RQ3 and RQ4, we test two such approaches. The first one is to rely on external assessors to review augmented search logs and make document-level usefulness annotations. The second one uses a machine learning method and features extracted from behavior logs to estimate usefulness.
We evaluate these two usefulness estimation approaches in terms of their reliability and validity. As stated by Kelly [27] (p. 176), reliability is "the extent to which the method and measures yield consistent findings", and validity is "the extent to which methods and measures allow a researcher to get at the essence of whatever it is that is being studied". Reliability is a necessary condition for validity, and when combined together, these two criteria measure the extent to which the usefulness labels produced by theses two approaches can reflect the user-perceived usefulness of documents.
For usefulness annotation approach, we assess its reliability by calculating the inter-assessor agreement, and its validity by comparing usefulness annotations Ua with usefulness feedbacks from users Uu and correlating them with query-level satisfaction feedbacks QSATu. For usefulness prediction approach, we also

Table 8: Correlations with usefulness feedbacks Uu.
(or ) indicates difference is significant at p < 0.05(p < 0.01), comparing to the

same metric related to R Pearson's r
Ua 0.413
R 0.332

MSE 1.51
1.79

MAE 0.852
1.020

Weighted  0.321 0.209

Table 9: Correlations with query-level satisfactions QSATu.
 (or ) indicates the difference between Ua and R is significant at p < 0.05(p <
0.01). (or ) indicates the difference between Ua and Uu is significant at
p < 0.05(p < 0.01). The darker and lighter shadings indicate the difference

between Ua and QSATa is significant at p < 0.01 and 0.05.

Pearson's r(d f = 933)

Pref. agreement ratio

Ua

Uu R

Ua

Uu R

cCG

.466 / .572 .425 .701 / .751 .669

cDCG

.518 / .724 .498 .742 / .826 .698

cMAX

.580 / .751 .563 .681 / .779 .632

cCG/#clicks .548

.733 .551 .716 / .807 .689

QSATa

.508

.584

assess its validity by comparing the predicted usefulness scores with Uu and QSATu, and we use cross-validations and significance tests to ensure the results are reliable.
6.1 Usefulness Annotation
The detailed procedure of usefulness annotation is described in Section 3.2. So here we only describe and discuss the reliability and validity of collected usefulness annotations Ua.
To measure the reliability of usefulness annotation, we use Cohen's Weighted  to assess the agreement between different assessors. As shown in Table 5, the  for Ua (Ua = 0.530, Ua = 0.008) is larger than those for Rc (Rc = 0.413, Rc = 0.010) and Rnc (Rnc = 0.344, Rnc = 0.008). The standard error of weighted s are computed by the method introduced by Cohen [11]. The difference between Ua and Rc and the difference between Ua and Rnc are both significant at p < 0.001 (two-tailed independent t-tests). These results suggest that, measuring at the inter-assessor agreement level, the usefulness annotations are more reliable than the conventional relevance annotations. The possible reason is that providing search context and behavioral information (e.g. the full search task, search session and dwell times) to assessors may help them make judgments more consistently with the users. This is corroborated to some extent by the marginal distribution of Ua shown in Figure 3: unlike the relevance distribution concentrated on R = 3, the distribution of Ua is a more similar to Uu than R, which indicates that, with the help of search context and user behavior information, the assessors can detect low usefulness clicks and make more discriminative judgements.
To assess the validity of usefulness annotation, we first compare Ua with the usefulness feedbacks Uu, which are used as the ground truth labels for usefulness. The correlations are measured in Pearson's r, Mean Squared Error (MSE), Mean Absolute Error (MAE), and Cohen's Weighted . The results are shown in Table 8. A moderate positive correlation (r(1, 510) = 0.412, p < 0.001, two tailed) and a fair agreement ( = 0.321,  = 0.017) between Uu and Ua are detected. The correlation between Ua and Uu is significantly stronger than that between R and Uu. We also show the joint distribution of Uu and Ua in Figure 4(b). The diagonal blocks are the darkest block in almost every rows and columns, showing a fair agreement between Uu and Ua. From the correlation metrics and the joint distribution we can see that although Uu and Ua are not perfectly aligned, comparing to relevance annotation, usefulness annotation can better reflect the user-perceived usefulness.
As shown in Section 5.1, a strong correlation exists between usefulness feedbacks and query-level satisfaction. Therefore, a valid assessment of usefulness should also correlate well with query-level satisfaction feedbacks QSATu. We use usefulness

469

annotations Ua to compute four click-sequence-based metrics defined in Section 5.1: cCG, cDCG, cMAX, and cCG/#clicks, and correlate them with QSATu. Beside computing the Pearson's r for these correlations, we also conduct a naturalistic pairwise preference test. In the preference test, we extract 1,455 query pairs (qi, q j), where qi and q j belong to the same search session, and QSATu(qi) > QSATu(q j). For each query pair, if an evaluation metric also indicates the same relative preference, then we say the evaluation metric agrees with QSATu on that query pair. A similar method is used by Sanderson et al. [36]. As we only extract query pairs from the same search sessions, the preference test can effectively reduce the variabilities introduced by different users and different search tasks.
We report the correlations with QSATu, measured in Pearson's r and the agreement ratios in the preference test, in Table 9. We compare the correlations related to Ua to those related to relevance R (baseline) and usefulness feedbacks Uu (oracle performance). We also use the query-level satisfaction annotation from external assessors (QSATa) as another baseline. The results show that, although usefulness annotations Ua do not correlate with query-level satisfaction feedbacks QSATu as well as usefulness feedbacks Uu from users (all the differences are significant at p < 0.01), most click-sequence-based metrics based on Ua outperform their counterparts based on R, in terms of correlation with QSATu. It is also interesting to observe that query-level satisfaction annotations from external assessors (QSATa) are quite different from query-level satisfaction feedbacks from users (QSATu), which is also observed by Liu et al. [32]. Some of click-sequence-based metrics based on Ua are significantly better than satisfaction annotations (QSATa), which suggests that document-level usefulness annotation may be more valid than query-level satisfaction annotation.
6.2 Usefulness Prediction
As previous studies show that there are substantial correlations between the user behavior signals (e.g. long dwell time [4, 31, 46], last click in a query [8, 24], and query position and reformulation types [34]) and evaluation-related measures like document relevance, search success, and user satisfaction, we attempt to use a regression model based on user behavior features and search context features to (1) automatically generate document-level usefulness labels, and (2) improve and enhance the document-level annotations (both R and Ua) so as to make them more aligned to users' usefulness feedbacks Uu.
Features
We list the features extracted from behavior logs in Table 10. We categorize these features into three groups: Query features (Q), Session features (S) and User features (U). Query features are the features related to a single query. With user behavior features, such as click numbers and dwell time included, they mainly describe how the user interacted with the search engine. Session features depend on the whole search session, and include short-term search context features like query position and query reformulation types. To compute User features, we need the long-term search history of that user. For (1) automatic usefulness label generation, only query features, session features and user features are involved (Q+S+U or referred to as All for simplicity). For (2) annotation enhancement, we extract relevance annotation features (R) and usefulness annotation features (A) from the annotation data. In particular, we use the document-level annotation itself, and the four interactive evaluation metrics computed by the document-level annotations, as relevance annotation features (R) and usefulness annotation features (A).
Prediction Models
We frame the usefulness prediction as a supervised regression problem, and use usefulness feedbacks (Uu) for clicked documents as the target value of the regression model. We perform five-fold

Table 10: Features to predict usefulness, extracted from the

behavior logs.

Query features(Q)

rank

The rank of clicked document in result list

#clicks

The number of clicks in the query

query length

The length of the query, in words and in characters

click position

Whether the click is the first/last/intermediate click in a

query with more than one click, and whether the query

has only one click

dwell time

click dwell time and query dwell time

Session features(S)

#queries

The number of queries in the search session

#queries w/o click The number of queries without click in session

query position

Whether the query is the first/last/intermediate query in

a session with more than one query, and whether the

session has only one query

time to completion The total time spent on this search session

query reformulation Whether the query is generated from a specification/

generalization/ parallel reformulation, and whether the

query leads to a specification/ generalization/ parallel

reformulation

User features(U)

user #clicks

The average/max/min/standard deviation of #clicks per

query of the user

user #queries

The average/max/min/standard deviation of #queries per

session of the user

user #dwell time

The average/max/min/standard deviation of query/click

dwell time of the user

cross-validation over search sessions to ensure the results are reliable. All the user features are computed on the training set. Since the cross-validation are performed over sessions, each session belongs to either the training set or the test set, the query and session features for a test document will not be present in the training set. We use a Gradient Boosting Regression Tree (GBRT) [17] as our regression model, because it can naturally handles mixed types of features, has a good predictive power, and is robust to outliers. A variety of feature combinations are tested. Similar to usefulness annotation studied in Section 6.1, we evaluate the validity of usefulness predictions in terms of their correlations with usefulness feedbacks (Uu), and their correlations with query-level satisfaction feedbacks (QSATu).
Prediction Results

We measure the correlations between predicted usefulness scores and usefulness feedbacks from users (Uu) in Pearson's r, MSE and MAE. The results are shown in Table 11. We use subscripts to
indicate the feature groups used in usefulness prediction, for examples, UQ refers to the predicted usefulness based on the query features and UAll refers to the predictions based on all the features extracted from the behavior logs (i.e. Q+S+U). Both relevance annotation R and usefulness annotation Ua are used as baselines.
The results show that, as we add more behavior features, the
performance of usefulness prediction increases, which proves that
search context features (Q) and user-specific features (U) are useful in usefulness prediction. Comparing to R, all the predicted usefulness scores U(·) are significantly more correlated with users' usefulness feedbacks, which once again demonstrates the gap
between relevance and user-perceived usefulness. When we
combine all the features extracted from the behavior logs, the resulting UAll establishes a comparable or stronger correlation with Uu, than usefulness annotation Ua does. This result suggests that when some usefulness feedbacks from users Uu are available for training, instead of relying on external assessors to generate usefulness annotation Ua, we can automatically generate document-level usefulness labels UAll of at least equal validity to Ua, based on the features that can be implicitly collected from behavior logs.
On the other hand, for (2) annotation enhancement, when we
combined behavior features (All) with document-level annotations
(A or R), significant improvements over the annotation-based baselines (Ua and R) are found. While it is not surprising to see UAll+A+R achieving the best performance, it is interesting to observe that UAll+R is better than UAll+A. A possible reason for

470

Table 11: Results for usefulness prediction.

Measured in the correlations with usefulness feedback Uu. (or ) indicates

the difference between U(·) and R is significant at p < 0.05(p < 0.01). The

darker / lighter shadings indicates the difference between U(·) and Ua is significant at p < 0.05/0.01.

UQ UQ+S UAll
UAll+A UAll+R UAll+A+R

Pearson's r 0.398 0.410 0.461
0.467 0.519 0.521

MSE 1.198 1.186 1.103
1.105 1.021 1.023

MAE 0.894 0.889 0.851
0.845 0.815 0.803

Ua

0.413

1.512 0.852

R

0.332

1.786 1.020

Table 12: Correlations with query-level satisfactions QSATu.
Measured in Pearson's r(d f = 933). (or ) indicates the difference between

U(·) and Ua is significant at p < 0.05(p < 0.01). (or ) indicates the difference

between U(·) and Uu is significant at p < 0.05(p < 0.01). The darker and lighter

shadings indicate the difference between U(·) and Jiang et al. [23] is significant at

p < 0.01 and 0.05.

cCG

UAll 0.459

UAll+A+R 0.490/

Ua

Uu

0.466 0.572

cDCG

0.580/ 0.612/

0.518 0.724

cMAX

0.601

0.635/ 0.580 0.751

cCG/#clicks 0.571

0.608/ 0.548 0.733

QSATa Jiang et al. [23]

0.508 0.539

this is that while usefulness annotations inevitably depend on some behavior features like dwell time, the relevance annotations of documents are in some sense more complementary to the behavior features than usefulness annotations, thus UAll+R has a broader coverage of useful features than UAll+A.
Correlations with Query-level Satisfaction
We further demonstrate the validity of usefulness prediction approach by showing the correlations between predicted usefulness labels and query-level satisfaction feedbacks QSATu. Due to the lack of space, we only show the correlations in Pearson's r, since the preference test gives similar results. Usefulness annotation Ua and usefulness feedback Uu are used as document-level baselines; query-level satisfaction annotation QSATa and a graded satisfaction prediction method based on user behavior features developed by Jiang et al. [23] are used as query-level baselines. Although our goal is not to predict satisfaction, we use this method as a baseline because we share similar behavior features, and the performance of a recently proposed satisfaction prediction model sets up a relatively high standard about how well one can predict satisfaction based on these features.
The results (Table 12) show the following facts: Firstly, because the correlations related to UAll are comparable or stronger when compared to those related to Ua, the usefulness predictions based on behavior features are at least as valid as usefulness annotations. Secondly, the usefulness predictions based on both behavior features and annotation features UAll+A+R are significantly better than Ua (therefore better than R). We can thus use behavior features to enhance usefulness and relevance annotation. Finally, although there is still a significant gap between usefulness predictions (UAll and UAll+A+R) and usefulness feedbacks (Uu), the click-sequence-based metrics based on document-level usefulness predictions outperforms the query-level satisfaction prediction baselines in terms of correlations with QSATu, which indirectly proves that these usefulness predictions indeed reflect users' opinions and perception to some extent.

Summary
In this section, we proposed two usefulness labeling methods: usefulness annotation and automatic usefulness prediction, and conducted analyses to demonstrate their reliability ad validity. With regards to RQ3, we find that usefulness annotations are more reliable than conventional relevance annotations. The assessors in usefulness annotation process can detect low usefulness clicks effectively. The usefulness annotations collected in this process are shown to be valid due to their consistence with usefulness feedbacks and query-level satisfaction feedbacks from users. With regards to RQ4, we show that using behavior features, we can automatically generate valid usefulness labels, and improve existing document-level annotations so as to make them more aligned to usefulness feedbacks.
To summarize, we can collect reliable and valid usefulness labels by different approaches. When there is no usefulness feedback from any users at all, we can hire external assessors to generate usefulness annotation when provided with sufficient search context information. When there are some usefulness feedbacks from real users, we can use machine learning techniques and features extracted from behavior logs, to generate usefulness labels for other search sessions. We can also combine manual annotations from assessors and features from behavior logs to better estimate usefulness. In this case, if the cost of the additional annotations is taken into account, it is better to ask the assessors to make relevance judgments instead of usefulness annotations.
7. CONCLUSIONS AND DISCUSSIONS
In this work, through a carefully designed user study and dedicated annotation processes, we collected a comprehensive dataset that consists of behavior logs, user feedback data, and corresponding annotation data. Based on this dataset, we first investigated the difference and relationship between two document-level measures: the system-centric, highly-independent, relevance assessed by assessors, and the user-centric, situational, and subjective usefulness assessed by users. The results suggest that high relevance by assessors is a necessary but not sufficient condition for high usefulness for users, thus, in general, these two document-level measures are not aligned well. We further studied the correlations between relevance, usefulness, and user satisfaction, and found that usefulness is potentially of a great value in the evaluation of Web search engines since it is highly correlated with query-level satisfaction feedbacks. These findings partially explain why traditional system-centric evaluation metrics are not well aligned with user satisfaction. Finally, we proposed two approaches to collect usefulness labels in practical Web search settings, and evaluate them in terms of their reliability and validity.
Our findings and conclusions are based on a laboratory user study in which a set of predefined tasks are used and 29 participants are treated as real search users. Compared to a naturalist study based on real search logs, a laboratory user study has its limitations in its scale and the ecological validity of the collected data could be questioned. However, the laboratory study has the advantage to be able to control the variabilities that lie in the different information needs from different users. To enhance the ecological validity and ensure our findings can generalize, we carefully chose the search tasks and designed the experimental search system to simulate practical Web search scenarios.
The main focus of this paper is contrasting usefulness perceived by users with relevance annotations by assessors. Although we showed that it may be better to use the former in system evaluation rather than the latter, we do not expect the replacement can be done in all situations. Traditional relevance annotations have the advantage to be reusable, thus can be used to evaluate the system in prior to its deployment; while usefulness is suited in a more user-centric post hoc evaluation.

471

Our study makes a first step towards a new user-centric evaluation framework. A variety of click-sequence-based evaluation metrics (e.g. cCG and cDCG) are shown to be better suited for user-centric evaluations in this work. Their properties, and the assumptions and user models behind them are worth being investigated in the future. To fully establish a new evaluation framework based on usefulness and these metrics, more user studies that involve multiple search systems and more users are required in the future.
8. ACKNOWLEDGMENTS
This work was supported by Tsinghua University Initiative Scientific Research Program(2014Z21032), National Key Basic Research Program (2015CB358700), Natural Science Foundation (61532011, 61472206) of China and Tsinghua-Samsung Joint Laboratory for Intelligent Media Computing.
9. REFERENCES
[1] A. Al-Maskari, M. Sanderson, and P. Clough. The relationship between ir effectiveness measures and user satisfaction. In Proc. SIGIR '07, pages 773­774, New York, NY, USA, 2007. ACM.
[2] A. Al-Maskari, M. Sanderson, and P. Clough. Relevance judgments between trec and non-trec assessors. In Proc. SIGIR '08, pages 683­684, New York, NY, USA, 2008. ACM.
[3] N. J. Belkin, M. Cole, and R. Bierig. Is relevance the right criterion for evaluating interactive information retrieval. In Proc. SIGIR '08 Workshop on Beyond Binary Relevance: Preferences, Diversity, and Set-Level Judgments., 2008.
[4] G. Buscher, L. van Elst, and A. Dengel. Segment-level display time as implicit feedback: A comparison to eye tracking. In Proc. SIGIR '09, pages 67­74, New York, NY, USA, 2009. ACM.
[5] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proc. SIGIR '98, pages 335­336, New York, NY, USA, 1998. ACM.
[6] B. Carterette, E. Kanoulas, M. Hall, and P. Clough. Overview of the trec 2014 session track. 2013.
[7] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc. CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.
[8] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In Proc. WWW '09, pages 1­10, 2009.
[9] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Büttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proc. SIGIR '08, pages 659­666, 2008.
[10] C. Cleverdon. The cranfield tests on index language devices. In Aslib proceedings, volume 19, pages 173­194. MCB UP Ltd, 1967.
[11] J. Cohen. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213, 1968.
[12] J. Cohen and P. Cohen. Applied multiple regression/correlation analysis for the behavioral sciences, chapter 2, pages 53­54. Lawrence Erlbaum Associates, 1975.
[13] M. Cole, J. Liu, N. Belkin, R. Bierig, J. Gwizdka, C. Liu, J. Zhang, and X. Zhang. Usefulness as the criterion for evaluation of interactive information retrieval. Proc. HCIR, pages 1­4, 2009.
[14] W. S. Cooper. On selecting a measure of retrieval effectiveness. JASIS, 24(2):87­100, 1973.
[15] Z. Dou, R. Song, and J.-R. Wen. A large-scale evaluation and analysis of personalized search strategies. In Proc. WWW '07, pages 581­590, New York, NY, USA, 2007. ACM.
[16] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM TOIS, 23(2):147­168, 2005.
[17] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189­1232, 2001.
[18] A. Hassan, R. Jones, and K. Klinkner. Beyond dcg: User behavior as a predictor of a successful search. In Proc. WSDM '10, pages 221­230, 2010.
[19] A. Hassan, R. W. White, S. T. Dumais, and Y.-M. Wang. Struggling or exploring?: Disambiguating long search sessions. In Proc. WSDM '14, pages 53­62, New York, NY, USA, 2014. ACM.
[20] S. Huffman and M. Hochster. How well does result relevance predict session satisfaction? In Proc. SIGIR '07, pages 567­574, 2007.
[21] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM TOIS, 20(4):422­446, Oct. 2002.

[22] K. Järvelin, S. L. Price, L. M. Delcambre, and M. L. Nielsen. Discounted cumulated gain based evaluation of multiple-query ir sessions. In Advances in Information Retrieval, pages 4­15. 2008.
[23] J. Jiang, A. Hassan Awadallah, X. Shi, and R. W. White. Understanding and predicting graded search satisfaction. In Proc. WSDM '15, pages 57­66, New York, NY, USA, 2015. ACM.
[24] S. Jung, J. L. Herlocker, and J. Webster. Click data as implicit relevance feedback in web search. Information Processing & Management, 43(3):791­807, 2007.
[25] E. Kanoulas, B. Carterette, P. Clough, and M. Sanderson. Evaluating multi-query sessions. In Proc. SIGIR '11, pages 1053­1062, 2011.
[26] J. Kekäläinen and K. Järvelin. Using graded relevance assessments in ir evaluation. JASIST, 53(13):1120­1129, 2002.
[27] D. Kelly. Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Information Retrieval, 3(1--2):1­224, 2009.
[28] D. Kelly, X. Fu, and C. Shah. Effects of rank and precision of search results on users' evaluations of system performance. University of North Carolina, 2007.
[29] Y. Kim, A. Hassan, R. W. White, and I. Zitouni. Modeling dwell time to predict click-level satisfaction. In Proc. WSDM '14, pages 193­202, New York, NY, USA, 2014. ACM.
[30] J. R. Landis and G. G. Koch. The measurement of observer agreement for categorical data. biometrics, pages 159­174, 1977.
[31] C. Liu, J. Liu, N. Belkin, M. Cole, and J. Gwizdka. Using dwell time as an implicit measure of usefulness in different task types. Proc. ASIST, 48(1):1­4, 2011.
[32] Y. Liu, Y. Chen, and et. al. Different users, different opinions: Predicting search satisfaction with mouse movement information. In Proc. SIGIR '15, pages 493­502, 2015.
[33] A. Moffat, P. Thomas, and F. Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. CIKM '13, pages 659­668, New York, NY, USA, 2013. ACM.
[34] D. Odijk, R. W. White, A. Hassan Awadallah, and S. T. Dumais. Struggling and success in web search. In Proc. CIKM '15, pages 1551­1560, New York, NY, USA, 2015. ACM.
[35] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proc. SIGIR '11, pages 1043­1052.
[36] M. Sanderson, M. L. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? In Proc. SIGIR '10, pages 555­562, New York, NY, USA, 2010. ACM.
[37] T. Saracevic. Relevance reconsidered. In the Second Conference on Conceptions of Library and Information Science, volume 1, pages 201­218, 1996.
[38] M. Shokouhi, R. W. White, P. Bennett, and F. Radlinski. Fighting search engine amnesia: Reranking repeated results. In Proc. SIGIR '13, pages 273­282, New York, NY, USA, 2013. ACM.
[39] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing search via automated analysis of interests and activities. In Proc. SIGIR '05, pages 449­456, New York, NY, USA, 2005. ACM.
[40] A. Turpin and F. Scholer. User performance versus precision measures for simple search tasks. In Proc. SIGIR '06, pages 11­18, 2006.
[41] P. Vakkari and E. Sormunen. The influence of relevance levels on the effectiveness of interactive information retrieval. JASIST, 55(11):963­969, 2004.
[42] S. Verberne, M. Heijden, M. Hinne, M. Sappelli, S. Koldijk, E. Hoenkamp, and W. Kraaij. Reliability and validity of query intent assessments. JASIST, 64(11):2224­2237, 2013.
[43] E. M. Voorhees. The philosophy of information retrieval evaluation. In the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, CLEF '01, pages 355­370, 2002.
[44] E. M. Voorhees and D. Harman. Overview of trec 2001. In Trec, 2001.
[45] R. W. White and S. M. Drucker. Investigating behavioral variability in web search. In Proc. WWW '07, pages 21­30, 2007.
[46] R. W. White and D. Kelly. A study on the effects of personalization and task information on implicit feedback performance. In Proc. CIKM '06, pages 297­306, New York, NY, USA, 2006. ACM.
[47] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and effort: An analysis of document utility. In Proc. CIKM '14, pages 91­100, New York, NY, USA, 2014. ACM.

472

Risk-Sensitive Evaluation and Learning to Rank using Multiple Baselines

B. Taner Dinçer1, Craig Macdonald2, Iadh Ounis2
1 Sitki Kocman University of Mugla, Mugla, Turkey 2 University of Glasgow, Glasgow, UK
dtaner@mu.edu.tr1,{craig.macdonald, iadh.ounis}@glasgow.ac.uk2

ABSTRACT
A robust retrieval system ensures that user experience is not damaged by the presence of poorly-performing queries. Such robustness can be measured by risk-sensitive evaluation measures, which assess the extent to which a system performs worse than a given baseline system. However, using a particular, single system as the baseline suffers from the fact that retrieval performance highly varies among IR systems across topics. Thus, a single system would in general fail in providing enough information about the real baseline performance for every topic under consideration, and hence it would in general fail in measuring the real risk associated with any given system. Based upon the Chi-squared statistic, we propose a new measure ZRisk that exhibits more promise since it takes into account multiple baselines when measuring risk, and a derivative measure called GeoRisk, which enhances ZRisk by also taking into account the overall magnitude of effectiveness. This paper demonstrates the benefits of ZRisk and GeoRisk upon TREC data, and how to exploit GeoRisk for risk-sensitive learning to rank, thereby making use of multiple baselines within the learning objective function to obtain effective yet risk-averse/robust ranking systems. Experiments using 10,000 topics from the MSLR learning to rank dataset demonstrate the efficacy of the proposed Chi-square statistic-based objective function.
1. INTRODUCTION
The classical evaluation of information retrieval (IR) systems has focused upon the arithmetic mean of their effectiveness upon a sample of queries. However, this does not address the robustness of the system, i.e. its effectiveness upon the worst performing queries. For example, while some retrieval techniques (e.g. query expansion [2, 8]) perform effectively for some queries, they can orthogonally cause a decrease in effectiveness for other queries. To address this, various research into robust and risk-sensitive measures has taken place. For instance, in the TREC Robust track, systems were measured by geometric mean average precision [23, 25] to determine the extent to which they perform well on all queries. More recently, the notion of risksensitivity has been introduced, in that an evaluation mea-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17­21, 2016, Pisa, Italy.
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911511

sure should consider per-query losses and gains compared to a particular baseline technique [11]. Within this framework, measures such as URisk [27] and TRisk [13] have been proposed. Both measures can be adapted to integrate with the state-of-the-art LambdaMART learning to rank technique.
Since risk-sensitive measures compare to a specific baseline, such measures are most naturally applied in experiments using a before-and-after design, where different treatments are applied to a particular baseline system, e.g. query expansion. However, when simply considering a single baseline, a full knowledge of the difficulty of a particular query cannot be obtained. For instance, a single baseline system may perform lowly for a query that other systems typically perform well. For this reason, the inference of risk based upon a population of baseline systems is attractive. One can easily draw an analogy with the building of ranking methods that combine multiple weighting models, such as data fusion or learning to rank, to obtain a more effective final ranking. Moreover, the use of multiple baselines permits a deployed search engine to evaluate the risk of an alternative retrieval approach not only with respect to its own baseline, but also to other competitor systems.
In this paper, we show how a risk-sensitive evaluation based on the Chi-square test statistic permits the consideration of multiple baselines, unlike the existing measures URisk & TRisk which can only consider a single baseline. In doing so, we argue that a robust system should not be less effective for a given topic than an expectation of performance given a population of other (baseline) systems upon that topic. In particular, this paper contributes: a new risksensitive evaluation measure, namely ZRisk, based on Chisquare test statistic, and a derivative called GeoRisk that enhances ZRisk by also taking into account the overall magnitude of effectiveness; Moreover, we demonstrate the use of ZRisk and GeoRisk upon a TREC comparative evaluation of Web retrieval systems; Finally, we show how to directly and effectively integrate GeoRisk within the state-of-the-art LambdaMART learning to rank technique.
This paper is organised as follows: Section 2 provides a background on robust and risk-sensitive evaluation; Section 3 defines ZRisk based upon Chi-squared statistic, as well as the GeoRisk derivative; Section 4 & Section 5 demonstrate the proposed measures upon synthetic & real TREC data, while Section 6 shows the integration of GeoRisk within the LambdaMART learning to rank technique; Related work and concluding remarks follow in Sections 7 & 8.
2. RISK-SENSITIVE EVALUATION
Risk-sensitive evaluation [11] aims at quantifying the tradeoff between risk and reward for any given retrieval strat-

483

egy. Information retrieval performance, which is usually measured by a given retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [9]) over a set of topics Q, can be expressed in terms of risk and reward as a risk function. Such a risk function takes into account the downside-risk of a new system s with respect to a given baseline system b (i.e. a loss: performing a topic q worse than the baseline according to the effectiveness measure, sq < bq) and an orthogonal reward function that takes into account the upside-risk (i.e. a win: performing a topic better than the baseline, sq > bq).
A single measure, URisk [27], which allows the tradeoff between risk and reward to be adjusted, is defined as:





1

URisk

=

 c

q + (1 + )

q , (1)

qQ+

qQ-

where c = |Q| and q = sq - bq. The left summand in the square brackets, which is the sum of the score differences q for all q where sq > bq (i.e. q  Q+), gives the total win (or upside-risk) with respect to the baseline. On the other hand, the right summand, which is the sum of the score differences q for all q where sq < bq, gives the total loss (or downsiderisk). The risk sensitivity parameter   0 controls the tradeoff between reward and risk (or win and loss):  = 0 calculates the average change in effectiveness between s and b, while for higher , the penalty for under-performing with respect to the baseline is increased: typically  = 1, 5, 10 [12] to penalise risky systems, where  = 1 doubles the emphasis of down-side risk compared to  = 0.
Recently, Din¸cer et al. [13] introduced a statistically-grounded risk-reward tradeoff measure, TRisk, as a generalisation of URisk, for the purposes of hypothesis testing:

TRisk

=

URisk , S E (URisk )

(2)

where SE(URisk) is the standard error in the risk-reward tradeoff score URisk. Here, TRisk is a linear monotonic transformation of URisk. This transformation is called studentisation in statistics (c.f., t-scores) [16], and TRisk can be used as the test statistic of the Student's t-test. Moreover, the aforementioned work shows that TRisk permits a state-of-the-art learning to rank algorithm (LambdaMART) to focus on those topics that lead to a significant level of risk in order to learn effective yet risk-averse ranking systems.
On the other hand, the comparative risk-sensitive evaluation of different IR systems is challenging, as the systems may be based upon a variety of different (base) retrieval models ­ such as learning to rank or language models ­ or upon different IR platforms (Indri, Terrier etc.). It has been shown that using a particular system as the baseline in a comparative risk-sensitive evaluation of a set of diverse IR systems ­ as attempted by the TREC 2013 and 2014 Web track ­ yields biased risk-reward tradeoff measurements [14], especially when the systems under evaluation are not variations of the provided baseline system. To address this, the use of the within-topic mean system performance was proposed as an unbiased baseline (as well as the within-topic median system performance and the within-topic maximum system performance). Given a particular topic q and a set of r systems, the arithmetic mean of the r performance scores according to an evaluation measure observed on q is the unbiased baseline score:

1r

Meanq = r si(q),

(3)

i=1

where si(q) is the performance score of system i on topic q measured by a given evaluation measure (e.g. ERR@20) for i = 1, 2, . . . , r. Since the arithmetic mean gives equal weight to every retrieval strategy in determining the within-topic mean system performance, a baseline system that is determined by the Meanq scores will be unbiased with respect to the retrieval strategies yielding the r system scores.
However, as shown in [14], the use of Meanq exposes a problem about the validity of the comparative risk-sensitive evaluation of different IR systems. This issue is related to the risk-based rankings of the systems obtained using Meanq. Indeed, such a comparison of the risk-sensitive performances of different IR systems actually implies the comparison of the retrieval effectiveness of the individual systems based on the underlying effectiveness measure, i.e. ERR@20 [14]. That is, the ranking of the systems obtained by using the underlying effectiveness measure will be the same as the risk-based ranking of the systems obtained using the unbiased baseline Meanq, irrespective of the value of the risk sensitivity parameter .
Most importantly, the previously proposed risk measures are only sensitive to the mean and the variance of the observed losses and wins, i.e. URisk is sensitive to mean and TRisk is sensitive to mean and variance (c.f. SE(URisk)). However in a comparative risk-sensitive evaluation, we argue that it is necessary to be sensitive to the shape of the score distributions, as well as the mean and the variance. As such, in the next section, we propose the ZRisk measure, which satisfies the aforementioned variance and shape requirements of a comparative risk-sensitive IR evaluation, while the derivative GeoRisk measure enhances ZRisk by naturally incorporating the overall effectiveness of the considered system.
3. MEASURES OF RISK FROM CHI-SQUARE
Each existing robust and risk-sensitive evaluation measure each encodes properties about what a good (or bad) IR system should exhibit. Firstly, the classical mean measure (e.g. MAP or mean NDCG) stipulates that a good system should perform well on a population of topics on average; The geometric mean (e.g. as proposed in [24] for Mean Average Precision as GMAP) says that a good system should avoid performing lowly on any topics, while comparing GMAP values permits identifying improvements in low performing topics, in contrast to mean, which gives equal weight to absolute changes in per-topic scores, regardless of the relative size of the change [4]. Risk-sensitive evaluation measures such as URisk and TRisk use the notion of a baseline - a good system should perform well, but preferably no worse than the given baseline. Hence URisk responds to changes in the mean effectiveness of the system, but emphasises those worse than the baseline. Building upon URisk, TRisk is also sensitive to the variance exhibited by a system across the population of topics. These attributes are highlighted in Table 1.
In this section, we argue for a risk measure that considers the `shape' of a system's performance across topics. In particular, we consider that the distribution of the effectiveness scores of a set of baseline systems across the topics, mapped to the same overall mean effectiveness as the system at hand, represents an expected performance for each topic that the system should not underperform. In other words, we calculate the expectation of the system's performance for each topic, by considering the overall performance of the current system and the observed performances of other baseline systems. This allows to determine topics that the system should be performing better on. It follows that our proposal encap-

484

Measure
Mean AP Geo. MAP
URisk TRisk ZRisk GeoRisk

Baseline
None None
Single Single Multiple Multiple

Penalty of low topics None Focus on lowest topics 1+ 1+ 1+ 1+

Sensitive to:

Mean Var. Shape



































Table 1: Comparison of existing and proposed robustness/risk-sensitive measures.

Topics

Systems t1 t2 t3 . . . tc Total

s1

x11 x12 x13 . . . x1c S1

s2

x21 x22 x23 . . . x2c S2

X=

s3

x31 x32 x33 . . . x3c S3

...

...

...

...

...

...

...

sr

xr1 xr2 xr3 . . . xrc Sr

Total T1 T2 T3 . . . Tc N

Table 2: Data matrix for an IR experiment.

sulates two separate measures: ZRisk, introduced in Section 3.1, which measures the shape of the system's performance irrespective of the overall magnitude of effectiveness; and later in Section 3.2 we show how to create a risk-measure responsive to mean effectiveness called GeoRisk. The subsequent Section 4 & Section 5 demonstrate the ZRisk and GeoRisk measures upon artificial and TREC data.
The first measure, ZRisk, is inspired by the Chi-square statistic used in the Chi-square test for goodness-of-fit, which is one of the well-established nonparametric hypothesis tests in categorical data analysis [1]. In statistics, goodness-of-fit tests are used to decide whether two distributions are significantly different from each other in shape/form. In relation to risk-sensitive evaluation, this means that, given a sample of topics, a risk measure based on Chi-square statistic permits quantifying the difference in the performance profiles of two IR systems across the topics. As mentioned above, none of the previously proposed risk measures are sensitive to the score distributions of IR systems on topics. However, risksensitive evaluation, by nature, should take into account all of shape, mean and variance, while ZRisk is independent of overall mean effectiveness. Hence, building upon ZRisk, we propose the GeoRisk measure, which covers all of the aforementioned aspects including the overall mean effectiveness of the system at hand, as highlighted in Table 1.

3.1 The Chi-square Statistics & ZRisk

ZRisk is best explained by deriving it directly from the

Chi-square statistic used in the Chi-square test for goodness-

of-fit. In particular, the Chi-square statistic is calculated

over a data matrix of r × c cells, called the contingency ta-

ble. The result of an IR experiment involving r systems

and c topics can be represented by a r × c data matrix X,

whose rows and columns correspond respectively to the r

systems and c topics, where the cells xij (for i = 1, 2, . . . , r

and j = 1, 2, . . . , c) contain the observed performances of

the corresponding systems for the associated topics, mea-

sured by an effectiveness measure such as ERR@20. Table 2

provides a graphical portrayal of data matrix X.

For such a data matrix, the row and the column marginal

totals are given by Si =

c j=1

xij

and

Tj

=

r i=1

xij

respec-

tively, and the grand total is given by N =

r i=1

c j=1

xij

.

The average effectiveness of a system i over c topics is given

by Si/c and similarly, the within-topic mean system effec-
tiveness is given by Tj/r. Given a data matrix X, the Chi-square statistic, G2, can
be expressed as

r
G2 =

c (xij - eij )2 ,

(4)

i=1 j=1

eij

where the expected value for cell (i, j), eij, is given by

eij

=

Si × Tj N

= Si ×

Tj N

= Si × pj .

(5)

In Equation (5), pj

=

Tj N

can be described as the density

or mass of column j, for j = 1, 2, . . . , c. If a row total Si is

distributed on columns proportional to the column masses

pj, then the Chi-squared differences of the associated cell

values from the corresponding expected values will sum up to

zero, i.e.

c j=1

(xij

-

eij

)2

=

0.

Note that

eij

=

xij

when

r

=

1, where pj = xij/Si since N = Si. Intuitively, when there

is only one IR system, the expected system performance for

any topic j will be equal to the score observed for that sys-

tem. When r = 1, G2 = 0, meaning that the observed score

distribution of the system across topics is perfectly fit to it-

self. Thus, G2 values that are greater than zero indicate a

discordance between two distributions, above or below ex-

pectations. This makes G2 not directly applicable as a risk-

sensitive evaluation measure, since it equally and uniformly

penalises both downside (losses) and upside risk (wins). In

contrast, risk-sensitive measures should favour wins and or-

thogonally penalise losses. Hence, we propose below a mea-

sure derived from G2 that addresses this limitation.

For large samples, the Pearson's Chi-square statistic G2 in

Eq. (4) follows a Chi-square distribution with (r - 1)(c - 1)

degrees of freedom and the observed cell values xij follow a

Poisson distribution with mean eij and variance eij [1]. This

means that the Chi-square statistic can also be expressed as

the sum of the square of standard normal deviates [1]:

rc

G2 =

zi2j

i=1 j=1

where

zij = xij -eijeij .

The square root of the components of Chi-square statistic, zij, gives the standardised deviation in cell (i, j) from the expected value eij (i.e. z-scores). Thus, for large samples, the distribution of zij values on the population can be approximated by the standard normal distribution with zero mean and unit variance.
It follows that a risk-reward tradeoff measure can be expressed in terms of the standard normal deviates from the expected effectiveness, as given by:





ZRisk = 

ziq + (1 + )

ziq ,

(6)

qQ+

qQ-

for any system i, i = 1, 2, . . . , r. Q+ (Q-) is the set of queries where ziq > 0 (ziq < 0, respectively), determined by whether system i outperforms its expectation on topic j (c.f. xij - eij ).
ZRisk takes the classical form of a risk-sensitive evaluation measure, in that upside risk is rewarded and the effectiveness penalty of downside risk is amplified by  - i.e. the higher the ZRisk, the more safe and less risky a system is. In addition, ZRisk calculates the risk of a system in relation to the shape of effectiveness across topics exhibited by multiple baselines. In this way, ZRisk brings a new dimension to the measurement of robustness, originally defined by

485

Voorhees [24] as "the ability of the system to return reasonable results for every topic", in that for ZRisk, robustness is measured compared to a per-topic expectation calculated from a population of baseline systems.

3.2 GeoRisk
As noted before, a limitation of ZRisk is that it measures robustness irrespective of the mean effectiveness of IR systems. Indeed, one may consider that the baseline for any given system i is composed of the expected per-topic scores of the system, eij, such that the sum of expected per-topic scores is equal to the sum of the observed per-topic scores of the system, i.e. j eij = Si. This means that ZRisk measures robustness using individual baselines for every system, each of which is derived on the basis of the observed total effectiveness of the system (i.e. Si) and the observed topic masses (i.e. Tj). This makes the robustness/risk measurements of ZRisk independent of the observed mean effectiveness of the systems, i.e. j xij = j eij for i = 1, 2, . . . , r.
On the other hand, for the purposes of the comparative risk-sensitive evaluation of different IR systems, we combine the risk measure with the effectiveness measure in use, ZRisk and ERR@20 for example, into a final measure. A natural method for such a combination is the geometric mean, which is expressed as the nth root of the product of n numbers. The geometric mean is a type of average, like arithmetic mean, that represents the central tendency in a given set of numbers. In contrast to the arithmetic mean, the geometric mean normalises the ranges of the variables, so that each datum has an equal impact on the resulting geometric mean. Hence, the geometric mean of the ERR@20 scores and the ZRisk scores represents, evenly, both the effectiveness and the robustness of system si under evaluation:

GeoRisk (si) = Si/c × (ZRisk/c),

(7)

where 0  ()  1 is the cumulative distribution function of the standard normal distribution. In this way, we use () to normalise ZRisk into [0,1], because -  ZRisk/c   .

4. DEMONSTRATION
To illustrate ZRisk and GeoRisk introduced in Section 3, Table 3 presents an example data matrix X composed of 8 systems and 5 topics. The effectiveness scores of the example systems are artificially determined so that the resulting performance profiles of the systems across the topics serve as a basis to exemplify some potential differences in performance profiles of IR systems in relation to their mean effectiveness. Figure 1 shows the performance profiles of the 8 systems, which can be characterised as follows:
· Systems s1 and s2 have the same mean effectiveness over the 5 topics (i.e. 0.3000) but the scores of s1 are monotonically increasing in magnitude across the topics, whereas, the scores of s2 are monotonically decreasing. That is, s1 and s2 have contrasting performance profiles across the topics, with respect to the same mean effectiveness score of 0.3000.
· Systems s3 and s4 have constant scores across the topics that are equal to their respective mean effectiveness scores. In other words, these systems have constant performance profiles, while system s3 has the same mean effectiveness as both s1 and s2.
· Systems s5 and s6 again have the same mean effectiveness as systems s1 and s2, but have alternating scores across the topics, such that one has a higher score in magnitude

s1 s2 s3 s4 s5 s6 s7 s8 Tj Mean

t1 0.0500 0.4000 0.3000 0.2500 0.4000 0.2000 0.2542 0.2918 2.1460 0.2683

t2 0.1500 0.3500 0.3000 0.2500 0.1500 0.4500 0.2629 0.2994 2.2123 0.2765

t3 0.3000 0.3000 0.3000 0.2500 0.4000 0.2000 0.2802 0.3147 2.3449 0.2931

t4 0.4500 0.2500 0.3000 0.2500 0.1500 0.4500 0.2975 0.3301 2.4776 0.3097

t5 0.5500 0.2000 0.3000 0.2500 0.4000 0.2000 0.3061 0.3378 2.5440 0.3180

Si 1.5000 1.5000 1.5000 1.2500 1.5000 1.5000 1.4009 1.5738 11.7248

Mean 0.3000 0.3000 0.3000 0.2500 0.3000 0.3000 0.2802 0.3148

Table 3: Example data matrix X.

Score

0.5

0.4

S2

S5

0.3

S3 S8

S7

0.2

S6

S4

0.1 S1
01

s1 s2 s3 s4 s5 s6 s7 s8

2

3

4

5

Topics

Figure 1: Example systems' performance profiles.

than the other for one topic and vice versa for the next topic. We describe such systems as having alternating performance profiles across the topics.
· Systems s7 and s8 have different mean effectiveness scores from each other and also from that of the other systems. Their performance profiles are visually parallel to each other, and concordant with the profile of the mean topic scores, i.e. the row "Mean" of Table 3.

4.1 Single Baseline
Measuring the level of risk associated with a given IR system s with respect to a particular single baseline system b means that, in total, there are two systems under consideration, i.e. r = 2. For such a risk-sensitive evaluation, the Chi-square statistic G2 is given by

c
G2 =

(xsj - esj )2 + (xbj - ebj )2 ,

j=1

esj

ebj

and, under the null hypothesis that the observed score distri-
butions of both systems follow a common distribution with mean µ and variance 2, it can be expressed as

c (xsj - xbj )2 ,

(8)

j=1 xsj + xbj

where xsj is the observed score of the system s for topic j, and xbj is the observed score of the baseline system b. Note that, when there are only two systems, Tj = xsj + xbj, and hence xbj = Tj - xsj and xsj = Tj - xbj . Here,

esj

=

Ss × Tj N

=

Ss N

× (xsj

+ xbj )

ebj

=

Sb × Tj N

=

N - Ss N

× (xsj + xbj )

where N = Ss + Sb. In fact, given two IR systems, the level of risk associated
with any one of the two systems can be measured by taking the other system as the baseline, as implied by Eq. (8). Most importantly, Eq. (8) suggests, in this respect, that, if the

486

=0
s2 vs. s1 s1 vs. s2 s3 vs. s1 s1 vs. s3 s4 vs. s1 s1 vs. s4 s5 vs. s1 s1 vs. s5 s6 vs. s1 s1 vs. s6 s7 vs. s1 s1 vs. s7 s8 vs. s1 s1 vs. s8

t1 0.3689 -0.3689 0.2988 -0.2988 0.3077 -0.2809 0.3689 -0.3689 0.2121 -0.2121 0.2799 -0.2705 0.2792 -0.2860

t2 0.2000 -0.2000 0.1581 -0.1581 0.1599 -0.1460 0.0000 0.0000 0.2739 -0.2739 0.1422 -0.1374 0.1445 -0.1480

t3 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0845 -0.0845 -0.1000 0.1000 0.0000 0.0000 -0.0001 0.0001

t4 -0.1690 0.1690 -0.1225 0.1225 -0.1209 0.1103 -0.2739 0.2739 0.0000 0.0000 -0.1057 0.1021 -0.1097 0.1123

t5 -0.2858 0.2858 -0.1917 0.1917 -0.1884 0.1720 -0.1088 0.1088 -0.2858 0.2858 -0.1669 0.1613 -0.1732 0.1774

ZRisk 0.1141 -0.1141 0.1427 -0.1427 0.1583 -0.1445 0.0708 -0.0708 0.1002 -0.1002 0.1496 -0.1446 0.1408 -0.1442

Table 4: Single baseline example.

systems show an equal mean performance over a given set of c topics (i.e. Ss = Sb), the measured level of risk will be the same for both systems. In risk-sensitive evaluations, a baseline system defines what is a robust system, so that risk can be quantified as the degree of divergence from that baseline. However, given a set of IR systems, taking every system as a baseline, actually contributes information for the qualification of a robust (i.e. not `risky' or safe) system on the population of topics. In this regard, multiple baselines can provide more information about the real level of risk associated with any IR system.
Let system s1 in Table 3 be the baseline system. The level of risk associated with system s2, which has the same mean performance with s1, is ZRisk = 0.1141, while the level of risk associated with s1 for baseline s2 is the same in magnitude but different in sign, i.e. -0.1141. The sign of the ZRisk scores indicates the direction of the observed level of riskreward tradeoff, where minus indicates down-side risk and plus indicates up-side risk. Table 4 shows the calculated values of ZRisk at  = 0 for each system i = 2, 3, . . . , 8. As can be seen, for those systems whose mean performances are equal to the mean performance of s1, only the sign of the calculated ZRisk values changes when the baseline is swapped.
Based on the calculated ZRisk values when the baseline is s1, system s4 is the least `risky' system among the 8 example runs with the highest ZRisk value of 0.1583 (i.e. the s4 vs. s1 row of the table). However, as can be seen in Figure 1, s3, s7, or s8 are relatively less `risky' than s4. That is, those three systems have performance profiles that are concordant/parallel with that of s1 and also they have relatively higher mean effectiveness scores than s4: thus, s4 could not be considered less "risky" than s3, s7, or s8. The reason behind this counter-intuitive result is two-fold. Firstly, baseline system s1 has performance scores that are monotonically increasing in magnitude across the topics. Thus, as a baseline, it suggests that the expected system performance on the population of topics that is represented by the sample topic t1 would be low, and for the population of topics represented by t2 it would be relatively higher than that of t1, and so on. However, as seen from Figure 1, considering the observed scores of the other systems, it would appear that the expected per-topic system performances are in general different from those that the system s1 suggests, i.e. the `mean' row of Table 3. Secondly, the risk that is measured by ZRisk is related to the distribution of the total system performance Si on topics with respect to the expected per-topic system performances, and is not dependent on the magnitude of the mean performance of the systems across topics.
These two issues explain the above counter-intuitive result that s4 is declared as the least `risky' system. Indeed, the former issue can be resolved by employing multiple baselines

=0

=1

=5

 = 10

Mean ZRisk Geo ZRisk Geo ZRisk Geo ZRisk Geo

s1 0.300 -0.049 0.386 -0.727 0.364 -3.442 0.271 -6.835 0.160

s2 0.300 0.026 0.388 -0.312 0.378 -1.668 0.333 -3.362 0.274

s3 0.300 0.006 0.387 -0.069 0.385 -0.368 0.376 -0.742 0.364

s4 0.250 0.005 0.354 -0.063 0.352 -0.336 0.344 -0.677 0.334

s5 0.300 0.006 0.387 -0.541 0.370 -2.727 0.296 -5.460 0.203

s6 0.300 0.005 0.387 -0.539 0.370 -2.718 0.297 -5.442 0.204

s7 0.280 -0.001 0.374 -0.008 0.374 -0.036 0.373 -0.072 0.372

s8 0.315 0.001 0.397 -0.010 0.396 -0.052 0.395 -0.106 0.393

Table 5: ZRisk and GeoRisk for the example systems.

as shown in the following Section 4.2, and the latter issue of independence from the magnitude of mean effectiveness can be resolved as shown in Section 4.3, where the risk measure ZRisk and the measure of effectiveness are combined into a single measure of effectiveness, GeoRisk.

4.2 Multiple Baselines
Chi-square statistic allows the use of all systems in data matrix X as multiple baselines for risk-reward tradeoff measurements using ZRisk. Recall that the expected value for cell (i, j), eij, is given by

eij

= Si ×

Tj N

= Si × pj .

For the case of a single baseline system b, given a particular system s, to calculate the mass or density pj of topic j, the within topic total performance score Tj is taken as xsj + xbj, i.e. pj = (xsj + xbj)/N . Similarly, given a set of baselines, the topic masses can be calculated as

1r

pj = N

xij ,

i=1

for each topic j = 1, 2, . . . , c. Intuitively this means that, given a set of r systems, the level of risk associated with every system is measured by taking the remaining (r - 1) systems as the baseline. Compared to the case of taking a particular system as the baseline, as the number of baseline systems increases, the accuracy of the estimates of expected system performance for each topic increases, and hence the accuracy of the estimates of real risk increases.
Table 5 shows the calculated ZRisk values for each of the 8 example runs at  = 0, 1, 5, 10. We observe from the table that, as the risk sensitivity parameter  increases, example systems s7 and s8 exhibit the lowest levels of risk relative to the other systems, (i.e.  = 1, 5, 10), while s1 exhibits the highest level of risk (ZRisk = -6.835 at  = 10). As can be seen, using multiple baselines resolves the effect of the lack of information about the expected per-topic system performance in assessing the risk levels of systems, i.e. s4 vs. s7 and s4 vs s8. In the following section, we show how to combine ZRisk with mean system effectiveness in order to solve the last issue about ZRisk, i.e. the counter-intuitive case of s4 vs. s3, where the measured level of risk for s3 is higher than that of s4 (e.g. the ZRisk score of s3 is -0.368 and it is -0.336 for s4 at  = 5), while s3 has higher effectiveness score than s4 (i.e. 0.300 vs. 0.250) and it has also a performance profile concordant with that of s4.

4.3 Effectiveness vs. Risk
Table 5 shows the calculated GeoRisk values for each of the 8 example runs. As can be seen, for the case of s4 vs. s3, the issue of the independence of ZRisk measurements from the magnitude of the mean effectiveness of IR systems is solved. The example system s3 is now measured as less

487

GeoRisk

0.45 S 8
0.4

0.35

0.3

s 1

s

0.25

2

s 3

0.2

s

4

0.15

s 5

s

0.1

6

s 7

0.05

s

8

S 4

S 3

S

2

S

6

S

5

S 1

0

0

1

2

3

4

5

6

7

8

9

10



Figure 2: Example systems' GeoRisk as 0    10.

GeoRisk

0.45

TREC 2012 Runs

0.4

uogTrA44xu

srchvrs12c00

0.35

DFalah121A

0.3

QUTparaBline

utw2012c1

0.25

ICTNET12ADR2

indriCASP

0.2

autoSTA

irra12c

0.15

0.1

0.05

0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

Figure 3: GeoRisk plot for 8 TREC 2012 runs.

`risky' than s4, as suggested by the magnitude of the observed mean effectiveness scores.
Figure 2 shows the plot of GeoRisk scores for each example system for  = 0, 1, 2, . . . , 10, where the systems with lines sloping downward along the increasing values of  (i.e. xaxis) are those that exhibit a risk of abject failure, (i.e. s1, s2, s5, and s6) while, in contrast, the robust systems such as s3, s4, s7 and s8 have nearly a straight, horizontal lines.
In summary, the GeoRisk measure takes into account both the mean effectiveness of IR systems and the difference in the shapes of their performance profiles. As a result, GeoRisk is sensitive to mean (i.e. the component Si/c), variance and the shape of the observed effectiveness scores across topics (i.e. the ZRisk component).

5. ANALYSIS OF TREC DATA
In this section, we demonstrate the use of the risk-reward tradeoff measure derived from the Chi-square statistic, ZRisk, and the aggregate measure GeoRisk, on real systems submitted to the TREC 2012 Web track [10]1, in comparison with the existing measures URisk and TRisk. In the subsequent year of the Web track [12], a standard baseline called indriCASP and based on the Indri retrieval platform was provided. Similar to [13, 14], we use the same indriCASP system as the nominal single baseline on the 2012 Web track topics.
In particular, out of the 48 runs submitted to TREC 2012, we select the top runs of the highest 8 performing groups, based on the mean ERR@20 score, While we omit other submitted runs for brevity, the following analysis would be equally applicable to them. For each run, we report the riskreward tradeoff scores obtained using the official TREC 2012 evaluation measure, ERR@20.
1Although our analysis is equally applicable to the TREC 2013 Web track, due to the lack of space, we report results from TREC 2012, which are also directly comparable to that of previous works [13, 14].

Table 6 lists the URisk, TRisk, TRisk, ZRisk and GeoRisk risk-reward tradeoff scores for the 8 runs. For the measures URisk and TRisk, the baseline run is indriCASP ; for the measures ZRisk and GeoRisk, we use as multiple baselines all 48+1 TREC 2012 runs including indriCASP ; TRisk denotes TRisk calculated using the per-topic mean effectiveness of the 49 runs as the baseline, i.e. Meanq in Eq. (3). Note that Dinc¸er et al. [13] showed that TRisk is inferential, i.e. the TRisk scores correspond to scores of the Student's t statistic. For this reason, for the URisk scores in TREC 2012 in Table 6 (where c = 50), TRisk > ±2 indicates that the observed URisk score exhibits a significant level of risk.
Table 6 shows in general that the notion of risk quantified by the Chi-square statistic-based risk measure ZRisk differs from that of the URisk, TRisk and TRisk measures, as illustrated by the contrasting systems' rankings (the column R next to each measure) for ZRisk. In particular, at  = 0, URisk and TRisk agree with the effectiveness measure ERR@20 on the rankings of the 8 TREC 2012 runs. However, at  = 5, although URisk and TRisk still agree with each other, they both diverge from the agreement with ERR@20. On the other hand, the risk measure ZRisk agrees neither with ERR@20 nor with the risk measures URisk and TRisk. Note that, except for the determination of baselines, the three risk measures URisk, TRisk, and ZRisk rely on the same notion of risk and reward, i.e. down-side risk and upside risk. Thus, comparing ZRisk with URisk and TRisk, it follows that multiple baselines (i.e. 49 TREC 2012 runs) provide information that is different from the information provided by the single baseline system indriCASP.
According to ZRisk, the most robust run is "uogTrA44xu" with a ZRisk value of 0.962 at  = 0, and the next is "irra12c" with ZRisk = 0.265, and so on, given the expected pertopic performance scores representing the baselines for each system. Based on the definition of ZRisk, it is expected that "uogTrA44xu" would perform any given topic with an ERR@20 score that is better than or equal to the expected score for that topic on a population of systems with mean ERR@20 scores equal to 0.3406. Conversely, the least robust or most `risky' run is "srchvrs12c00" with a ZRisk = -0.912.
Recall that ZRisk is independent of the observed mean effectiveness scores of the systems, which is, by definition, inappropriate for the purpose of a comparative IR evaluation. Thus, as an aggregate measure, GeoRisk, the geometric mean of ZRisk and ERR@20, can be used to tackle this challenge. As can be seen in Table 6, GeoRisk agrees with ERR@20 at  = 0 on the rankings of the 8 TREC 2012 runs. Here, GeoRisk gives equal weights to ERR@20 and ZRisk, and similarly, at  = 0, ZRisk gives equal weights to downside risk and up-side risk. Thus, the observed agreement between GeoRisk and ERR@20 implies that the measured ZRisk scores for each of the 8 TREC 2012 runs at  = 0 are negligible compared to the observed differences in effectiveness between the runs. In other words, every TREC 2012 run exhibits risk, to a certain extent, but none of the measured risk levels are high enough to compensate for the observed difference in mean effectiveness between two systems, so that a swap between risk and reward for a given topic is likely to occur for two systems on the population of topics. Note that the agreements of TRisk, as well as URisk, at  = 0, with ERR@20 also give support in favour of the same conclusion, i.e. the practical insignificance of the measured levels of risk at  = 0.
On the other hand, as  increases (i.e. as the emphasis of down-side risk increases in ZRisk measurements), GeoRisk

488

uogTrA44xu

=0

=5

 = 20

ERR URisk TRisk TRisk ZRisk R Geo R URisk R TRisk R TRisk R ZRisk R Geo R Geo R

0.3406 0.146 2.822 4.833 0.962 1 0.4158 1 -0.130 2 -0.798 2 2.767 1 -29.255 1 0.308 1 0.053 1

srchvrs12c00 0.3067 0.112 2.332 2.985 -0.912 9 0.3887 2 -0.100 1 -0.673 1 -0.678 2 -37.069 7 0.265 4 0.024 7

DFalah121A 0.2920 0.097 2.290 2.895 -0.328 7 0.3811 3 -0.156 3 -0.981 3 -0.861 3 -32.635 5 0.274 3 0.037 4

QUTparaBline 0.2901 0.095 2.130 2.870 0.004 4 0.3809 4 -0.189 4 -1.112 4 -1.006 4 -30.966 4 0.279 2 0.044 3

utw2012c1

0.2203 0.026 0.561 0.917 -0.172 6 0.3314 5 -0.388 6 -2.046 6 -2.987 5 -29.807 2 0.246 5 0.044 2

ICT. . . DR2 0.2149 0.020 0.487 0.569 0.233 3 0.3284 6 -0.329 5 -1.994 5 -3.238 6 -32.887 6 0.234 6 0.030 6

indriCASP

0.1947 *

* -0.120 -0.339 8 0.3111 7 * * *

* * -38.619 9 0.207 8 0.014 8

autoSTA

0.1735 -0.021 -0.498 -0.968 -0.143 5 0.2942 8 -0.509 8 -2.518 7 -4.195 7 -38.215 8 0.196 9 0.014 9

irra12c

0.1723 -0.022 -0.545 -1.214 0.265 2 0.2942 9 -0.501 7 -2.634 8 -4.510 8 -30.410 3 0.216 7 0.035 5

Table 6: URisk, TRisk, TRisk and ZRisk risk-reward tradeoff scores for the top 8 TREC 2012 runs, along with GeoRisk at  = 0, 1, 5, 10, 20. For URisk and TRisk the baseline is indriCASP, and for TRisk it is Meanq in Eq. (3) over all 48 + 1 TREC 2012 runs including indriCASP, and for ZRisk and GeoRisk, the baselines are estimated for the 8 runs over the same set of 49 runs and 50 Web track topics. The underlined TRisk scores correspond to those URisk scores for which a two-tailed paired t-test gives significance with p-value < 0.05 - i.e. TRisk > ±2.

diverges from ERR@20 and ranks the 8 TREC 2012 runs in a way that is different from all of the risk measures under consideration including ZRisk (for example,  = 5), and at a high value of , it converges to an agreement with ZRisk, e.g. the systems' rankings in Table 6 at  = 20 for GeoRisk and at  = 5 for ZRisk. The tendency of GeoRisk towards ZRisk as  increases is expected from the definition of GeoRisk.
Figure 3 plots the GeoRisk scores calculated for each run at  = 0, 1, . . . , 20. As can be seen, each of the 8 TREC 2012 runs has a decreasing GeoRisk score in magnitude, as the risk sensitivity parameter  increases. This means in general that - to a varying extent - every run under evaluation is subject to the risk of committing an abject failure, as the importance of getting a reasonable result for every topic increases. In particular, the runs "uogTrA44xu" and "ICTNET12ADR2" keep their relative positions in the observed rankings across all  values, while the ranks of the other runs considerably change. For example, the rank of "srchvrs12c00" changes from 2 at  = 0 to 7 at  = 5. At  = 0, the run with rank 7 is indriCASP. However, the calculated risk for "srchvrs12c00" at any level of  cannot be considered as empirical evidence to favour indriCASP over "srchvrs12c00" for any given topic from the population, since the mean effectiveness of "srchvrs12c00" is significantly higher than the mean effectiveness of indriCASP (p < 0.0239, paired t-test).
Note that, for two IR systems whose mean effectiveness scores are significantly different from each other, a measured risk level could have no particular meaning from a user perspective. This is because the system with higher mean effectiveness would be the one that can fulfil the users' information needs better than the other on average, no matter what level of risk is associated with it. The system with significantly low mean effectiveness would, on average, fail to return a "reasonable" result for any given topic, compared to the other system's effectiveness. For a declared significance with a p-value of 0.05, a swap in scores between the two systems for a topic (i.e. a transition from risk to reward or vice versa between the systems) is likely to occur 5% of the time on average [26].
Nevertheless, the same case is not true for runs "DFalah121A" and "QUTparaBline", whose observed mean effectiveness scores are not significantly different from the mean effectiveness of "srchvrs12c00". The paired t-test, which is performed at a significance level of 0.05, fails to give significance to the observed difference in mean effectiveness between "DFalah121A" and "srchvrs12c00" with a p-value of 0.7592, and similarly for "QUTparaBline" with a p-value of 0.7003. This means that, for a given topic, a transition from risk to reward, or vice versa, between the runs "DFalah121A" and "srchvrs12c00", or between runs "QUTparaBline" and

"srchvrs12c00", is highly likely to occur on the population of topics. Thus, both systems can be considered less "risky" or more robust than "srchvrs12c00".
In summary, this analysis of the TREC 2012 Web track runs demonstrates the suitability of GeoRisk for balancing risk-sensitivity and overall effectiveness, and the importance of using multiple baselines within the appropriate statistical framework represented by ZRisk. The analysis performed for the 8 TREC runs shows overall that, in a comparative IR evaluation effort, relying only on the observed mean effectiveness of the systems may be misleading, even for a best performer system like "srchvrs12c00", where the risk associated with such a system is high enough that it can cause an over-estimation of the real performance of the system. However, we showed that GeoRisk provides a solution for identifying systems exhibiting such risks.
6. RISK-SENSITIVE OPTIMISATION
In this section, we show how GeoRisk can be integrated within the state-of-the-art LambdaMART learning to rank technique [7, 28]. Indeed, Wang et al. [27] showed how their URisk measure could be integrated within LambdaMART. Similarly, Din¸cer et al. [13] proposed variants of TRisk that resulted in learned models that exhibited less risk.
The method of integration of risk-sensitive measures into LambdaMART requires adaptation of its objective function. In short, LambdaMART's objective function is a product of (i) the derivative of a cross-entropy that was originally defined in the RankNet learning to rank technique [6], based on the scores of two documents a and b, and (ii) the absolute change Mab in an evaluation measure M due to the swapping of documents a and b [28]. Various IR measures (e.g. NDCG) can be used as M , as long as the measure is consistent: for each pair of documents a and b with differing relevance labels, making an "improving swap" (moving the higher labelled document above the lesser) must result in Mab  0, and orthogonally for "degrading swaps".
In adapting LambdaMART to be more robust within a risk-sensitive setting, the M is replaced by a variant M that considers the change in risk observed by swapping documents a and b, according to the underlying evaluation measure M , e.g. NDCG. In the following, we summarise existing instantiations of M arising from URisk and TRisk (called U-CRO, T-SARO and T-FARO), followed by our proposed instantiation of GeoRisk within LambdaMART. U-CRO: Constant Risk Optimisation based upon the URisk measure [27] (U-CRO) maintains a constant risk-level, regardless of the topic. In particular, let Mm define the effectiveness of the learned model m according to measure M , and let Mb define the effectiveness of the baseline b. Corre-

489

spondingly, Mm(j) (Mb(j)) is the effectiveness of the learned model (baseline) for query j, then:

M =

M

if Mm(j) + M  Mb(j);

M · (1 + ) otherwise.

(9)

T-SARO & T-FARO: Adaptive Risk-sensitive Optimisation makes use of the fact that TRisk can identify queries that exhibit real (significant) levels of risk [13] compared to the baseline b. Dinc¸er et al. [13] proposed two Adaptive Risk-sensitive Optimisation adaptations of LambdaMART, namely T-SARO and T-FARO, which use this observation to focus on improving those risky queries. Indeed, in U-CRO, M is multiplied by (1 + ), for a static   0. In T-SARO and T-FARO,  is replaced by a variable  , which varies according to the probability of observing a query with a risk-reward score greater than that observed. By modelling this probability using the standard normal cumulative distribution function P r Z  TRj  1 -  TRj , T-SARO replaces the original  in Eq. (9) with  as:

 = [1 -  TRj ] · ,

(10)

where TRj = j/SE(URisk) determines the level of risk exhibited by topic j. T-SARO and T-FARO contrast on the topics for which  is adjusted ­ while T-SARO only adjusts topics with downside risk as per Eq. (9), T-FARO adjusts all topics:

M = M · (1 +  )

The experiments in [13] found that T-FARO exhibited higher effectiveness than T-SARO, thus we compare GeoRisk to only U-CRO and T-FARO in our experiments. GeoRisk: Our adaptation of M for the newly proposed GeoRisk is more straightforward than the TRisk Adaptive Risk-sensitive Optimisations, in that we use GeoRisk directly as the measure within LambdaMART.

M = GeoRisk(Mm + M ) - GeoRisk(M ) = (Si + M )/c × (ZRisk/c) - GeoRisk(M )

Indeed, GeoRisk is suitable for LambdaMART as it exhibits the consistency property: an improving `swap' will increase both the left factor (Si/c) and the right factor ZRisk and therefore the value of GeoRisk for Mm +M will likewise increase. Moreover, as M is calculated repeatedly during the learning process, the speed of the implementation is critical for efficient learning. In this respect, it is important to note that retaining the values of the separate zij summands for each query in the ZRisk calculation (see Equation (6)) allows the new ZRisk value to be calculated by only recomputing the zij for the query affected, then recalculating GeoRisk.
Recall from Section 3 that zij encapsulates differential weighting of downside and upside risks, but with respect to the expected performance on the topic. Hence, by using GeoRisk, the objective function of LambdaMART will favour learned models that make improving swaps of documents on topics where the learned model performs below expectation as calculated on the set of baselines.
Naturally, the instantiation of GeoRisk within a learning to rank setting depends on the set of baselines X, to allow the estimation of the topic expectations eij (see Eq. (5)). The choice of baselines to provide for learning can impact upon which topics the learner aims to improve. Previous works on risk-sensitive learning [13, 27] have used the performance of a single BM25 retrieval feature as the baseline. Indeed, single weighting models such as BM25 are typically

used to identify the initial set of documents, which are then re-ranked by the learned model [19, 20], and hence it is a baseline that the learner should outperform. However, it does not represent the typical performance of a learned approach upon the queries, as it cannot encapsulate the effectiveness of refined ranking models using many other features.
Hence, instead of using GeoRisk to learn a model more effective than a set of BM25-like baselines, we argue for the use of state-of-the-art baselines, which portray representative estimations of query difficulty to the learner. Such baselines are more akin to the systems submitted to TREC (which themselves have been trained on previous datasets), rather than a single weighting model feature. In a deployed search engine, such state-of-the-art-baselines could represent the effectiveness of the currently deployed search engine, or other deployed search engines for the same query. In an experimental setting, such as in this paper, we use held-out data to pre-learn several learned models before conducting the main comparative experiments. Finally, for comparison purposes, we also deploy T*-FARO in our experiments, where the mean performance of the state-of-the-art baselines for each topic is used as the single learning baseline.
6.1 Experimental Setup
Our experiments use the open source Jforests learning to rank tool [15]2, which implements U-CRO, and T-FARO, as well as plain LambdaMART. Our implementations of T*FARO & GeoRisk are also built upon Jforests3. As baselines, in addition to LambdaMART, we also deploy a plain gradient boosted regression trees learner (also implemented by Jforests), and two linear learned models, Automatic Feature Selection (AFS) [21] & AdaRank [19, Ch. 4].
We conduct experiments using the MSLR-Web10k dataset4. This learning to rank dataset contains 136 query-dependent and query-independent feature values for documents retrieved for 10,000 queries, along with corresponding relevance labels. As highlighted above, our baselines require separate training. For this reason, we hold out 2000 queries for initial training (two thirds) and validation (one third). The remaining 8000 queries are then split into 5 folds, each with a balance of 60% queries for training, 20% for validation, and 20% for testing. Hence, our reported results are not comparable to previous works using all 10000 queries of the MSLR dataset, but instead performances for LambdaMART, UCRO & T-FARO are presented on the 8000 queries. The underlying evaluation measure M in each learner is NDCG.
For GeoRisk & T*-FARO, the multiple baselines are evaluated for each query in the main 5 folds, which represent `unseen' queries for those systems. For U-CRO, T-SARO and T-FARO, the baseline is depicted by the performance of the BM25.wholedocument feature.
Finally, we note that LambdaMART has several hyperparameters, namely the minimum number of documents in each leaf m, the number of leaves l, the number of trees in the ensemble nt and the learning rate r. Our experiments use a uniform setting for all parameters across all folds, namely m = 1500, l = 50, nt = 500, l = 0.1, which are similar to those reported in [27] for the same dataset.
6.2 Results
In Table 7, we report the NDCG@10 effectiveness and robustness for LambdaMART, U-CRO, T-FARO, T*-FARO
2https://github.com/yasserg/jforests 3We have contributed GeoRisk as open source to Jforests. 4http://research.microsoft.com/en-us/projects/mslr/

490

and GeoRisk for  = {1, 5}5. The table is split into two halves: comparison to the effectiveness of the single BM25 baseline, and comparison viz. the 4 baseline learned models. For each of the baselines, we report the reward to risk ratio (denoted "Reward/Risk"), which measures the gain over the effectiveness of the baseline. Similarly, the win to loss ratio (denoted "W/L") measures the number of queries that the risk-sensitive optimisation contributed to reward against risk. Finally, the number of queries that each model wins or looses relative to the baseline are also shown for each  value, along with the number of queries that experience a relative loss greater than 20% NDCG@10. For clarity, the header of Table 7 denote arrows to show the favourable direction of each measure, e.g.  means that higher is better.
On analysis of the top half of Table 7, we observe that GeoRisk generates the highest mean NDCG effectiveness, marginally improving over LambdaMART. This is also observable for the Reward measure, in comparison to the BM25 baseline. However, for the risk measures, T-FARO and UCRO obtain the best values. For ,  = 1 is deemed the appropriate setting across all risk-sensitive learners, which has the effect of doubling the penalty of a query underperforming the corresponding baseline for that learner.
In the bottom half of Table 7, we examine the performance profiles of the different learners compared to the effectiveness of the 4 learned model baselines - the measures reported are the macro-average, i.e. the mean when each measure is calculated with respect to each learned baseline in turn (rather than compared to the micro-averaged effectiveness of the 4 learned baselines). In this half of the table, we note that, for  = 1, GeoRisk demonstrates the highest Reward and number of Wins and lowest Losses (and the resulting best Reward/Risk & Win/Loss ratios (the latter is a 2.7% improvement over LambdaMART). These improvements in the risk profile of the systems are achieved while still attaining the highest mean NDCG effectiveness among the systems. All differences are statistically significant (n = 8000 queries).
Finally, we note that the effectiveness and risk profiles attained by T*-FARO are markedly different, with T*-FARO attaining the lowest Reward/Risk & Win/Loss ratios. This verifies that the use of expected topic performance by GeoRisk rather than a mean per-topic performance (as used by T*-FARO) results in a learned model more attuned to the normal performances of state-of-the-art baseline systems.
Overall, this empirical evidence confirms our claim that the new risk-sensitive objective function GeoRisk for the LambdaMART learning to rank technique allows effective yet robust learned models to be obtained using multiple baselines. Moreover, we would highlight the more impressive risk-profiles attained in the bottom half of Table 7, which demonstrate that given a set of state-of-the-art baselines, using GeoRisk can generate an effective model that is as effective as LambdaMART with better risk profiles, and allows learning-to-rank to benefit from natural incremental improvements as practiced in real deployment settings.

vestigated the use of the Student's t-test for risk-sensitive evaluation, this is the first work to investigate the use of Pearson's Chi-square statistic for risk-sensitive evaluation, thereby facilitating the use of multiple baselines. Instead, previous usages of the Chi-square statistic has encompassed index term weighing [18] and document classification [22].
In IR experimentation, Armstrong et al. [3] noted that many papers appeared to show improvement upon older, weaker baselines. More recent work by Kharazmi et al. [17] showed that testing upon state-of-the-art baselines is necessary to demonstrate an advance. Similarly, this paper advocates the use of multiple state-of-the-art baselines, both in experimental and learning settings.
We also note several attempts to develop robust learning to rank techniques: of note, the AdaRank technique [19, Ch. 4] focuses on improving hard queries using boosting. Since then, risk-sensitive optimisation techniques such as UCRO [27], T-SARO & T-FARO [13] have aimed to adapt the LambdaMART technique by identifying risky topics with respect to a single baseline. Our work goes further by making use of multiple state-of-the-art baseline systems when calculating risk-estimation in the learning to rank objective function. Finally, Bennett et al.[5] take a different route, by developing personalised risk-averse ranking strategies upon the LambdaMART technique. As they build upon U-CRO, it may be possible to combine both approaches in the future.
8. CONCLUSIONS
This is the first paper that thoroughly investigated the use of multiple baselines in risk-sensitive evaluation. It argued for a new definition of risk-sensitivity related to the expected performance upon a given topic, calculated from a population of existing baseline systems. In particular, the paper introduced two new risk-sensitive evaluation measures, ZRisk and GeoRisk that are based upon the Chi-square statistic. Moreover, while ZRisk estimates risk independent of the overall mean retrieval effectiveness, GeoRisk enhances ZRisk by additionally accounting for overall effectiveness.
Our new measures were demonstrated on the results of a comparative system evaluation from the TREC Web track. Finally, the paper showed how the proposed GeoRisk measure can be directly integrated within the objective function of the state-of-the-art LambdaMART learning to rank technique. Experiments upon 8000 queries from a learning to rank dataset showed that the resulting learned models were as effective as LambdaMART, but also more risk-averse when compared to four learned baselines.
9. ACKNOWLEDGMENTS
This work is partially supported by TUBITAK, The Scientific and Technological Research Council of Turkey (Project No: 114E558). All opinions are that of the authors.

7. RELATED WORK
One aspect of this paper is the assessment of the robustness of IR systems, initiated first by the TREC Robust track [24] based on the geometric mean average precision measure [23, 25], and developed further by the introduction of new measures of risk/robustness, such as URisk [27] and TRisk [13]. In this respect, while Dinc¸er et al. [13] in-

10. REFERENCES
[1] A. Agresti. Categorical Data Analysis. Wiley, 2002. [2] G. Amati, C. Carpineto, and G. Romano. Query
difficulty, robustness, and selective application of query expansion. In Proceedings of ECIR, 2004. [3] T. Armstrong, A. Moffat, W. Webber, and J. Zobel. Improvements that don't add up: ad-hoc retrieval results since 1998. In Proceedings of ACM CIKM, 2009.

5=0 is equivalent to the normal LambdaMART algorithm.

[4] S. Beitzel, E. Jensen, and O. Frieder. GMAP. In

L. Liu and M. O¨ zsu, eds., Encyclopedia of Database

491

Systems, pp 1256­1256, 2009.

 NDCG Reward Risk Reward/Risk Wins Losses W/L L > 20%

Compared to BM25 Baseline

L'MART 0 0.4578 0.195 0.036

5.401

5892 1715 3.44

982

U-CRO 1 0.4558 0.192 0.035

5.472

5913 1684 3.51

965

U-CRO 5 0.4461 0.180 0.032

5.552

5913 1666 3.55

880

T-FARO 1 0.4576 0.194 0.035

5.568

5928 1671 3.55

963

T-FARO 5 0.4569 0.194 0.036

5.435

5889 1719 3.43

995

T*-FARO 1 0.4557 0.194 0.037

5.25

5857 1753 3.34

1016

T*-FARO 5 0.4562 0.194 0.037

5.26

5861 1737 3.37

999

GeoRisk 1 0.4581 0.196 0.037

5.327

5868 1732 3.39

992

GeoRisk 5 0.4557 0.195 0.038

5.179

5876 1728 3.40

1037

Compared to 4 Learned Models Baselines

L'MART 0 0.4578 0.061 0.053

1.138

3978.3 3669.0 1.12 1722.5

U-CRO 1 0.4558 0.059 0.053

1.103

3954.5 3690.0 1.11 1718.8

U-CRO 5 0.4461 0.051 0.056

0.941

3748.8 3887.3 1.01 1825.8

T-FARO 1 0.4576 0.059 0.052

1.137

3965.5 3674.3 1.12 1677.0

T-FARO 5 0.4569 0.059 0.052

1.121

3967.3 3676.5 1.12 1692.0

T*-FARO 1 0.4557 0.042 0.051

0.820

3562.0 4079.5 0.87 1821.5

T*-FARO 5 0.4562 0.041 0.053

0.823

3539.0 4099.5 0.86 1783.5

GeoRisk 1 0.4581 0.061 0.054

1.142

4017.0 3628.8 1.15 1709.8

GeoRisk 5 0.4557 0.060 0.055

1.095

3974.0 3674.0 1.12 1756.5

Table 7: Learning to rank results, with risk results calculated w.r.t. BM25 & the 4 learned models. All differences are statistically significant over the n = 8000 queries.

[5] P. N. Bennett, M. Shokouhi, and R. Caruana. Implicit preference labels for learning highly selective personalized rankers. In Proceedings of ACM ICTIR, 2015.
[6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of ICML, 2005.
[7] C. J. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.
[8] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query refinement using lexical affinities with maximal information gain. In Proceedings of ACM SIGIR, 2002.
[9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of ACM CIKM, 2009.
[10] C. L. A. Clarke, N. Craswell, and E. Voorhees. Overview of the TREC 2012 Web track. In Proceedings of TREC, 2012.
[11] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In Proceedings of ACM CIKM, 2009.
[12] K. Collins-Thompson, P. Bennett, F. Diaz, C. Clarke, and E. M. Voorhees. Overview of the TREC 2013 Web track. In Proceedings of TREC, 2013.
[13] B. T. Din¸cer, C. Macdonald, and I. Ounis. Hypothesis testing for the risk-sensitive evaluation of retrieval systems. In Proceedings of ACM SIGIR, 2014.
[14] B. T. Din¸cer, I. Ounis, and C. Macdonald. Tackling biased baselines in the risk-sensitive evaluation of retrieval systems. In Proceedings of ECIR, 2014.
[15] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of ACM SIGIR, 2011.
[16] D. Hoaglin, F. Mosteller, and J. Tukey, eds. Understanding robust & exploratory data analysis. Wiley, 1983.

[17] S. Kharazmi, F. Scholer, D. Vallet and M. Sanderson. Examining Additivity and Weak Baselines. TOIS, to appear, 2016.
[18] I. Kocaba¸s, B. T. Din¸cer, and B. Karaoglan. A nonparametric term weighting method for information retrieval based on measuring the divergence from independence. Information Retrieval, 17(2):153­176, 2014.
[19] T.-Y. Liu. Learning to rank for information retrieval. Foundation and Trends in Information Retrieval, 3(3):225­331, 2009.
[20] C. Macdonald, R. L. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Information Retrieval., 16(5):584­628, 2013.
[21] D. A. Metzler. Automatic feature selection in the markov random field model for information retrieval. In Proceedings of ACM CIKM, 2007.
[22] M. Oakes, R. Gaaizauskas, H. Fowkes, A. Jonsson, V. Wan, and M. Beaulieu. A method based on the chi-square test for document classification. In Proceedings of ACM SIGIR, 2001.
[23] S. Robertson. On GMAP - and other transformations. In Proceedings of ACM CIKM, 2006.
[24] E. M. Voorhees. Overview of the TREC 2003 Robust retrieval track. In Proceedings of TREC, 2003.
[25] E. M. Voorhees. The TREC Robust retrieval track. SIGIR Forum, 39(1):11­20, June 2005.
[26] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proceedings of ACM SIGIR, 2002.
[27] L. Wang, P. N. Bennett, and K. Collins-Thompson. Robust ranking models via risk-sensitive optimization. In Proceedings of ACM SIGIR, 2012.
[28] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Ranking, boosting, and model adaptation. Technical Report MSR-TR-2008-109, Microsoft, 2008.

492

Document Retrieval Using Entity-Based Language Models

Hadas Raviv

Oren Kurland

David Carmel

Technion, Israel

Technion, Israel

Yahoo Research, Israel

hadasrv@tx.technion.ac.il kurland@ie.technion.ac.il david.carmel@ymail.com

ABSTRACT
We address the ad hoc document retrieval task by devising novel types of entity-based language models. The models utilize information about single terms in the query and documents as well as term sequences marked as entities by some entity-linking tool. The key principle of the language models is accounting, simultaneously, for the uncertainty inherent in the entity-markup process and the balance between using entity-based and term-based information. Empirical evaluation demonstrates the merits of using the language models for retrieval. For example, the performance transcends that of a state-of-the-art term proximity method. We also show that the language models can be effectively used for clusterbased document retrieval and query expansion.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: document retrieval; entity-based language models
1. INTRODUCTION
Most ad hoc document retrieval methods compare query and document representations. To address the potential vocabulary mismatch between a short query and documents relevant to the query, various semantic document-query similarity measures have been proposed [28].
Specifically, there is a growing body of work on retrieval methods that utilize information about entities in a repository (e.g., Wikipedia or Freebase) which appear in queries and documents (e.g., [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]). Most of these methods expand the query with terms or entities related to those appearing (or marked) in it [46, 35, 39, 7, 13, 31, 45, 29]; other methods project queries and documents onto a latent or explicit entity space [14, 33, 44].
In this paper we take a step back, and address a more fundamental challenge regarding the use of entity-based information for document retrieval. We study whether using surface level entity-based query and document representations can help to improve retrieval effectiveness. By "surface level" we refer to representations based only on terms
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911508

in the text and markups of entities in it, along with raw corpus-based occurrence statistics. This is in contrast to expansion-based and projection-based representations that utilize also terms and entities related to those (marked) in the text and which often use auxiliary information about entities from the entity repository; e.g., textual descriptions of entities, entities' categories and inter-entity relations [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]. Put in simpler words, the question we address is whether the markup of entities in a query and documents is, by itself, sufficient information for
improving retrieval effectiveness. The reason for addressing the question just posed is two
fold. First, it will shed light on the effectiveness of using entities in their most basic capacity; that is, special tokens marked in queries and documents. Indeed, findings in past work on ad hoc retrieval regarding the merits of using surface level entity-based representations are inconclusive [16, 42, 47, 3, 14]. Second, such representations can be naturally used in existing retrieval approaches and tasks to improve performance; e.g., query expansion and cluster-based document retrieval as we show in this paper.
There are various potential merits in using surface level entity-based representations. For example, these can help to cope with the vocabulary mismatch problem; e.g., the entity United States of America can have different expressions in the text, including, "U.S.", "USA", "United States" and more. Furthermore, expressions of entities in the text are variable-length n-grams that bear semantic meaning. Thus, entities can be used for effective modeling of term proximity information which goes beyond using fixed-length n-grams.
An important challenge in inducing entity-based representations is accounting for the uncertainty inherent in the entity-markup process (a.k.a. entity linking); that is, associating term sequences with entities in a repository. Specifically, a term sequence can potentially be associated with multiple entities; e.g., the term "Lincoln" can be associated with the U.S. president, the car, the 2012 movie, etc. The uncertainty in entity linking has significant impact on retrieval effectiveness as we show in this paper.
We present novel types of entity-based language models which consider both single terms in the text as well as term sequences marked as entities by an existing entity-linking tool. These language models are induced from the query and documents in the corpus and serve for retrieval in the language modeling framework. The main novelty of these language models is accounting, simultaneously, for (i) the uncertainty in entity linking -- specifically, the confidence levels of entity markups; and, (ii) the balance between using

65

term-based and entity-based information. We demonstrate the importance of accounting for the mutual effects of these two aspects. For example, we show that using high recall entity markup, which is quite noisy, can help to significantly improve retrieval effectiveness if the noise is "balanced" by sufficient utilization of term-based information.
Empirical evaluation demonstrates the merits of using our entity-based language models for retrieval. The performance significantly transcends that of a state-of-the-art term proximity method: the sequential dependence model (SDM) [36, 19]. Integrating the language models with SDM yields further performance improvements. The language models are also effective for two additional retrieval paradigms: clusterbased document retrieval and query expansion.
2. RELATED WORK
The work most related to ours is that on devising surface level entity-based document and query representations for document retrieval [21, 16, 42, 47, 3, 41, 14]. The findings about the merits of these representations have been inconclusive. The few cases where the representations were shown to be somewhat effective for retrieval were when entity markups were devised in extreme care and were of very high quality [47, 3, 14]. In contrast to this past work that focused on vector space models, we demonstrate the clear merits of using our entity-based language models for retrieval. Also, in contrast to previously proposed representations [21, 16, 42, 47, 3, 41, 14], our language models account simultaneously for the uncertainty in the entity-markup process, and the balance between using term-based and entity-based information. Consequently, a highly important aspect that further differentiates our approach from past work is the effective utilization of high recall, noisy, entity markups.
There is work on query expansion using entity-based information [43, 34, 30, 40, 8, 10, 18, 46, 6, 20, 7, 35, 39, 13, 31, 29, 45] and on projecting queries and documents onto an entity space to compare them [14, 33, 44]. There are two fundamental differences between all this past work and ours which focuses on surface level entity-based query and document representations. First, in these past methods, queries and documents are represented by external terms and entities which they do not contain1. Our surface level representations do not utilize such expansions. Second, auxiliary information about entities from the entity repository (e.g., textual descriptions of entities and their interrelations) is utilized in this past work, but not in our representations2.
We show that our entity-based language models can be used to create effective expanded query forms by "plugging" them into an existing query expansion method: the relevance model [26, 1]. The resultant approach, which simultaneously expands the query with both terms and entities, is conceptually reminiscent of some methods recently proposed by Dalton et al. [13]. In their work, queries are expanded, independently, using terms and entities. The retrieval scores at-
1Xiong and Callan [44] found that representing queries using only entities marked in them is of merit for their learningto-rank approach. However, features describing the queryentities relations rely on auxiliary information from the entity repository that is not used by our methods. 2The entity-linking process could use auxiliary information from the entity repository. However, our proposed representations utilize the entity markups simply as tokens with confidence levels, and do not use auxiliary information.

tained by using multiple term-only and entity-only expanded query forms are fused using a learning-to-rank method [13]. We show that our language models can be used to further improve the effectiveness of such expansion-based approaches by improving the quality of the pseudo relevant document list used for query expansion.
We also demonstrate the merits of using our language models for cluster-based document retrieval. Using entitybased representations for this task is novel to this study.
In some studies, concepts (entities) in verbose queries were automatically weighted [2, 22, 4, 5]. In contrast to our approach, weights (confidence levels) of entities in documents were not accounted for. We demonstrate the importance of accounting for the confidence level of entity markups in both queries and documents. Further tuning of entities' weights in our proposed language models, using some of these approaches [2, 22, 4, 5], is interesting future work.
There are language models that integrate word phrases and named entities based on their association with predefined classes [27, 23]. In contrast to our language models, which are not based on such classes, these language models were not designed and used for document retrieval.

3. RETRIEVAL FRAMEWORK
In what follows we present ad hoc document retrieval methods that rank documents in a corpus D in response to query q. The methods utilize information about entities mentioned in the query and in documents.
To mark entities in texts, we use some entity-linking tool that utilizes a repository (e.g., Wikipedia or Freebase) where entities have unique IDs. The entity-linking tool takes as input a text, query or document in our case, and marks variable length sequences of terms as potential entities in the repository. The entity markup of a term sequence is composed of entity ID and a confidence level in [0, 1]. The confidence level reflects the likelihood that the term sequence corresponds to the entity. The confidence level relies on the term sequence and its context; e.g., its neighboring terms or other term sequences marked as entities [15, 38]. Using high confidence level results in high precision entity markup while low confidence level results in high recall.
We assume that each position in a given text can be part of at most a single term sequence that is marked as an entity; i.e., the entity markups do not overlap. A specific occurrence of a term sequence in a text cannot be marked with more than one entity. Yet, a term sequence can appear several times in a text with different entity markups as the markups depend on the context of the sequence. Details of the entity linking tools we use are provided in Section 4.1.
The retrieval methods we present in Section 3.2 use entitybased query and document language models. We now turn to define these language models.

3.1 Entity-based language models
We define unigram entity-based language models over a token space T ; i.e., tokens are generated by the language model independently of each other. The token space,

T d=ef V  E

(1)

is composed of the set V of all terms in the corpus D and the set E of entities in the entity repository which were marked at least once in a document in D with any confidence level.

66

The language models we devise rely on a definition of pseudo counts for tokens. Two definitions of pseudo counts will be presented in Sections 3.1.1 and 3.1.2. Let pc(t, x) be the pseudo count of token t ( T ) in the text or text collection x. We define the pseudo length of x as:

pl(x) d=ef

X pc(t, x).

tT :pc(t,x)>0

The maximum likelihood estimate (MLE) of token t ( T ) with respect to x is:

xMLE (t)

d=ef

pc(t, x) pl(x)

.

(2)

The MLE can be smoothed using Dirichlet priors [49]:

xDir(t) d=ef

pc(t,

x) + µDMLE pl(x) + µ

(t)

;

(3)

µ is a smoothing parameter. We next describe two types of language models defined
over T and induced using Equations 2 and 3. The language models differ by the definition of pseudo counts for tokens.

3.1.1 Hard confidence-level thresholding
The hard confidence-level thresholding language model, HTLM in short, is based on fixing a threshold  ( [0, 1]) for entity markups. Entity-based information is used only for entity markups with confidence level   . In contrast, every term occurrence in a text, including those in entity markups with a confidence level <  , is accounted for.
To formally define a HTLM using Equations 2 and 3, we have to define pseudo counts for tokens from T in a text or text collection x. To that end, we lay down a few definitions. If t ( T ) is a term, then cterm(t, x) is the number of occurrences of t in x. Let M(x) denote the set of all entity markups in x; i.e., all occurrences of term sequences in x that were marked as entities with some confidence level. For a markup m ( M(x)), E(m) is the entity and (m) is the confidence level. The equivalence relation t1  t2 holds iff the entity tokens t1 and t2 are identical (i.e., have the same ID). The pseudo count of t ( T ) in x is based on (i) the raw count of t in x if t is a term; and, (ii) the number of entity markups of t in x with a confidence level   if t is an entity. Formally,

pcHT LM; (t, x) d=ef

( cterm(t, x)

(1

-

)

P
mM(x):E(m)t

[(m)



]

(4)
if t  V; if t  E;

 ( [0, 1]) is a free parameter which controls the relative importance attributed to term and entity tokens;  is Kronecker's delta function: for statement s, [s] = 1 if s is true and [s] = 0 otherwise.
We note that using a Dirichlet smoothed HTLM (i.e., using Equation 4 in Equation 3) can still result in assigning zero probability to some tokens in T . These are entities with no corresponding markup of a term sequence in the corpus with confidence level   . We re-visit this point below.
If we set  = 1 in Equation 4, then the resultant HTLM reduces to a standard unigram term-based language model. Setting  = 0 results in HTEntLM which is a unigram language model that assigns non-zero probability only to entities: if the MLE from Equation 2 is used, then these are

the entities with at least one markup in x with a confidence level   ; if the Dirichlet smoothed language model is used (Equation 3), then these are the entities with at least one markup in the corpus with a confidence level   .

3.1.2 Soft confidence-level thresholding

A potential drawback of HTLM is committing to a specific threshold  for entity markups. That is, information about entity markups with confidence level lower than  is ignored. Furthermore, all entity markups with confidence level   are counted equally as their confidence levels are ignored.
Thus, we now turn to present a soft confidence-level thresholding language model, STLM. STLM accounts for all markups of an entity and weighs them by the corresponding confidence levels. Specifically, the pseudo count of t ( T ) in the text or text collection x is defined as:

(

pcST LM (t, x) d=ef

cterm(t, x)

(1

-

)

P
mM(x):E(m)t

(m)

if t  V; if t  E;

(5)

 ( [0, 1]) is a free parameter that, as in HTLM, controls

the relative importance attributed to term and entity to-

kens. Thus, STLM addresses the uncertainty inherent in

the entity linking process by using expected entity occur-

rence counts; the corresponding confidence levels serve for

occurrence probabilities. These expected counts are then

integrated with deterministic term counts.

If we set  = 1 in Equation 5, then STLM reduces to

a standard unigram term-based language model as was the

case for HTLM. Setting  = 0 results in STEntLM. This

language model assigns a non-zero probability only to en-

tities that have at least one markup (with any confidence

level) in x when using the MLE (Equation 2) or in the corpus

when using the Dirichlet smoothed language model (Equa-

tion 3). We note that in contrast to the case for HTLM,

there is no token in T that is assigned a zero probability by

a Dirichlet smoothed STLM.

3.2 Retrieval models
We rank document d by the cross entropy between the language models induced from the query (q) and d [25]:

X

CE (q || d) = - q(t) log d(t);

(6)

tT

higher values correspond to decreased similarity. Equation 6 is instantiated using the entity-based language
models described in Section 3.1. Following common practice [48], we use an unsmoothed maximum likelihood estimate for the query language model (Equation 2) and a Dirichlet smoothed document language model (Equation 3). We obtain four retrieval methods : HT3, HTOEnt, ST and STOEnt4, which utilize the HTLM, HTEntLM, STLM and

3In HT, the same confidence-level threshold, d, is used for all documents; the query threshold, q, can be different from d. Hence, an entity token assigned a non-zero probability by q could be assigned a zero probability by d; e.g., an entity marked in q with a confidence level  q but with no markup in the corpus with confidence level  d. In these cases, we zero the probability assigned to the entity token by q to avoid a log 0 in Equation 6. This is common practice in addressing term tokens that appear in a query but not in any document in the corpus. 4HTOEnt and STOEnt rely only on entity tokens. If all entities in E are assigned a zero probability by the unsmoothed

67

Table 1: TREC data used for experiments.

corpus

# of docs

data

queries

AP
ROBUST
WT10G GOV2 ClueB ClueBF

242, 918 528, 155 1, 692, 096 25, 205, 179 50, 220, 423

Disks 1-3 Disks 4-5 (-CR)
WT10g GOV2 ClueWeb09 (Cat. B)

51 - 150 301 - 450, 601 - 700 451 - 550 701 - 850
1 - 200

STEntLM language models, respectively. HT and ST utilize entity and term tokens, while HTOEnt and STOEnt utilize only entity tokens, hence the "O" in the methods names.
3.2.1 Score-based fusion
The HTLM and STLM language models integrate termbased and entity-based information at the language model level. Hence, the query-document comparison in Equation 6 simultaneously accounts for the appearance of the query terms and entities in a document.
An alternative approach is integrating term and entity information at the retrieval score level. Inspired by approaches in the vector-space model [42], and in work on using a latent entity space [33], we consider a method that fuses document retrieval scores produced by utilizing, independently, termonly (xterm) and entity-only (xent) language models induced from text x. Document d is scored by:
CE `qterm || dterm´ + (1 - )CE `qent || dent´ ; (7)
the  parameter balances the score fusion5. The query language models are unsmoothed maximum likelihood estimates (Equation 2) and the document language models are Dirichlet smoothed (Equation 3).
Instantiating Equation 7 with an entity-only language model, HTEntLM or STEntLM, and with a standard unigram termbased language model yields the F-HT and F-ST methods, respectively. These are conceptually highly similar to the HT and ST methods which integrate term-based and entitybased information at the language-model level. However, HT and ST use a single smoothing parameter for both term and entity tokens (see Equation 3) while F-HT and F-ST can use a different smoothing parameter for each as they utilize separately term-only and entity-only language models. We could have used different smoothing parameters for entity and term tokens under the same language model, e.g., by applying term-specific smoothing [17], but we leave this exploration for future work.
4. EVALUATION
4.1 Experimental setup
Experiments were conducted using the TREC datasets specified in Table 1. AP and ROBUST are mostly composed of news articles. WT10G is a small, noisy, Web collection. GOV2 is a much larger Web collection composed of high quality pages crawled from the .gov domain. ClueB is the
query language model, then no documents are retrieved. This can happen for example when inducing HTEntLM from the query with a high confidence-level threshold or inducing a STEntLM from a query which has no entity markups. 5The  in the score-based fusion model has a conceptually similar role to that of  in STLM and HTLM: balancing the use of term-based and entity-based information.

English part of the Category B of the ClueWeb 2009 Web collection. ClueBF was created from ClueB by filtering from rankings suspected spam documents: those assigned a score below 50 by Waterloo's spam classifier [11].
Data processing. Titles of TREC topics served for queries.
Tokenization and Porter stemming were applied using the Lucene toolkit (lucene.apache.org) which was used for experiments. Stopwords on the INQUERY list were removed from queries but not from documents.
Unless otherwise specified, the TagMe entity-linking tool (tagme.di.unipi.it) is used to annotate queries and documents. TagMe uses Wikipedia (a July 2014 dump) as the entity repository, and was shown to be highly effective and efficient in comparison to other publicly available entity-linking systems [12]. In Section 4.2.1 we also show the effectiveness of our methods using the Wikifier entity-linking tool6 [9, 12]. Wikifier was applied with an efficient configuration claimed to yield baseline entity linking effectiveness.
TagMe and Wikifier cannot process very long texts. Thus, we split documents into non-overlapping term-window passages. We terminate a passage at the first space that appears at least 500 characters after the beginning of the previous passage. We let the tools mark the passages independently. The tools are applied on the non-stemmed and non-stopped queries and documents. Entity markup of a term sequence includes an entity ID and a confidence level (in [0, 1]). We scan each text left to right and remove overlapping entity markups so that each position can be part of at most a single markup. If two markups overlap, we select the one with the higher confidence level. We break ties of confidence levels by selecting the markup which starts at the leftmost position.
Baselines. We use standard term-based unigram language
model retrieval [25], denoted TermsLM, for reference. This is a special case of the HT, ST, F-HT and F-ST methods with  = 1. Documents are ranked by the cross entropy between the unsmoothed (MLE) query language model and Dirichlet smoothed document language models.
The HTCon method is a special case of HT with  = 0.5 and q = d = 0 (q and d are the query and document thresholds, respectively). HTCon accounts uniformly for all entity mentions, and attributes the same importance to term and entity tokens. HTCon is conceptually reminiscent of methods representing documents and queries using concepts (e.g., from Wordnet) by concatenating with equal weights term-based and concept-based vector-space representations [41, 16, 42]. Accordingly, we consider F-HTCon: a special case of F-HT with  = 0.5 and q = d = 0.
Additional baseline is the state-of-the-art sequential dependence model, SDM, from the Markov Random Field framework which utilizes term proximities [36, 19]. The comparison with SDM, and its integration with our STLM is presented in Section 4.2.3.
Evaluation measures and free-parameters. Mean aver-
age precision at cutoff 1000 (MAP), precision of the top 10 documents (p@10) and NDCG@10 (NDCG) serve as evaluation measures. Statistically significant performance differences are determined using the two-tailed paired t-test with a 95% confidence level.
6cogcomp.cs.illinois.edu/page/demo view/Wikifier

68

Table 2: Comparison of methods instantiated from Equation 6 using term-only (TermsLM) and entitybased language models. Bold: the best result in a row. 't', 'h', 'o', 'c' and 's' mark statistically significant differences with TermsLM, HT, HTOEnt, HTCon and ST, respectively.

TermsLM HT HTOEnt HTCon ST STOEnt

MAP

AP

p@10

NDCG

MAP ROBUST p@10

NDCG

MAP WT10G p@10

NDCG

GOV2

MAP p@10

NDCG

ClueB

MAP p@10 NDCG

ClueBF

MAP p@10

NDCG

20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8 17.1 22.7 16.5 18.8 33.6 24.3

23.1t 44 2t
. 45.3t 28 1t
. 45 5t
. 47 1t
.
21.9t 30.4t
32.7 32.1t 57.3t 47.4t 18.7t 25.9t 18.7t 20 5t
. 37.9t 28 4t
.

15.6t,h 36.0h 37.6h 19.1t,h 35.7t,h 36.9t,h 13.3t,h 21.6t,h 21.2t,h 18.0t,h 39.4t,h 32.7t,h 14.0t,h
23.9 18.3 14.4t,h 29.2h 22.2h

22.5o 43.4to 44.7to 27.4to 45.0to 46.3to 21.4to 30.5to 32.1o 30.6ho 56.8o
46.9o
18.5o 26.7to 19.2t
19.9o 38.2to 28.4to

23.5to,c 17.5to,,hc,s

43.8to 38.3hc,s

45.5to 39.6hc,s

28.1to,c 21.4to,,hc,s

45.3to 38.0to,,hc,s

46.9to 39.2to,,hc,s

22.9to,,hc 16.7ho,c,s

31.6to 25.3ho,c,s

34.3to,c 25.4ho,c,s

32.2to,c 20.7to,,hc,s

57.7to 44.0to,,hc,s

47.9to 35.7to,,hc,s

19.5to 14.0tc,,hs

27 4t .

24.1

19 3t .

17.5

20.3to 14.4tc,,hs

37.9to 30.6hc,s

27.5to 22.8hc,s

The free parameter values of all retrieval methods are set using 10-fold cross validation performed over the queries in a dataset. Query IDs are used to create the folds. The optimal parameter values for each of the 10 train sets are determined using a simple grid search applied to optimize MAP. The learned parameter values are then used for the queries in the corresponding test fold.
The value of the Dirichlet smoothing parameter, µ, is selected from {100, 500, 1000, 1500, 2000, 2500, 3000}. The parameter , used in HTLM, STLM, F-HT and F-ST, is set to values in {0, 0.1, . . . , 1}. The document (d) and query (q) entity-markup confidence level thresholds, used in HT, HTOEnt and F-HT, are set to values in {0, 0.1, . . . , 0.9}.
4.2 Experimental results
4.2.1 Entity-based language models
Table 2 presents the performance of the methods that use entity-based language models to instantiate Equation 6. Our first observation is that the HT and ST methods outperform the standard term-based language-model retrieval, TermsLM, in all relevant comparisons (6 corpora × 3 evaluation measures); most improvements are substantial and statistically significant. Furthermore, HT and ST outperform to a substantial and statistically significant degree their special cases which use only entity tokens: HTOEnt and STOEnt, respectively. These findings attest to the merits of using our proposed language models, HTLM and STLM, which integrate term-based and entity-based information.
We also see in Table 2 that HT and ST outperform HTCon in most relevant comparisons; most MAP improvements for ST are statistically significant. Recall from Section 4.1 that HTCon represents past practice of concept-based representations: accounting uniformly for all entity mentions and attributing equal importance to entity and term tokens. Below we further study the importance of accounting for the

MAP

MAP

MAP

24.0 23.0 22.0 21.0 20.0 19.0 18.0 17.0 16.0 15.0
0
24.0

AP
HT ST
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
 WT10G

22.0

20.0

18.0

16.0

14.0 12.0
0
20.0

HT ST
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

ClueB

19.0

18.0

17.0

16.0

15.0 14.00

HT ST
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


MAP

MAP

MAP

30.0

ROBUST

28.0

26.0

24.0

22.0

20.0
18.0 0
34.0 32.0 30.0 28.0 26.0 24.0 22.0 20.0 18.0 16.0
0
22.0 21.0 20.0 19.0 18.0 17.0 16.0 15.0 14.00

HT ST
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
 GOV2
HT ST
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
 ClueBF
HT ST
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


Figure 1: The effect of varying  on the MAP of HT and ST. For  = 1, the methods amount to TermsLM (term-based language model retrieval). For  = 0, the methods use only entity tokens. The performance is reported for the test folds (i.e., all queries in a dataset) when fixing the value of  and using cross validation to set the values of all other free parameters. Note: figures are not to the same scale.

confidence level of entity markups, and attributing different weights to term and entity tokens as in HT and ST.
Table 2 shows that ST outperforms HT in most relevant comparisons, although rarely to a statistically significant degree. In addition, ST posts more statistically significant improvements over HTCon than HT. We note that HT depends on four free parameters (, q, d and µ) while ST depends only on two ( and µ). Furthermore, the values learned for q and d in HT using the training folds are very low, attesting to the merits of using high recall entity markup. (We revisit this point below.) Overall, these findings attest to the potential merits of using a soft-thresholding approach for the confidence level of entity markups (STLM) with respect to a hard-thresholding approach (HTLM); i.e., accounting for all entity markups in a text and weighing their impact by their confidence levels is superior to accounting, uniformly, for entity markups with a confidence level above a threshold.
Terms vs. entities. Figure 1 depicts the MAP performance
of HT and ST as a function of . Low and high values of  result in more importance attributed to entity-based and term-based information, respectively. For  = 1, the two methods amount to TermsLM -- i.e., standard termbased language model retrieval. For  = 0, the methods use only entity-based information; specifically, HT reduces to HTOEnt and ST reduces to STOEnt.
We see in Figure 1 that optimal performance is always attained for   {0, 1}. This finding echoes those based on Table 2. That is, HT and ST outperform TermsLM,

69

MAP MAP

35.0 30.0

AP ROBUST

HT
WT10G GOV2

CLUEB09 CLUEB09F

35.0 30.0

AP ROBUST

HT
WT10G GOV2

CLUEB09 CLUEB09F

25.0

25.0

20.0

20.0

15.0 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
q

15.0 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
d

Figure 2: The effect of varying q and d on the MAP performance of HT. The values of free parameters, except for that in the x-axis, are set using cross validation as in Figure 1.

and HTOEnt and STOEnt, respectively. Thus, we find that there is much merit in integrating term-based and entitybased information for representing queries and documents.
Figure 1 shows that the optimal value of  for HT is often higher than for ST. This can be attributed to the fact that HTLM, used to represent the query and documents in HT, uses a single confidence-level threshold for entity markups. Thus, potentially valuable information about entities is not utilized. As a result, HT calls for more reliance on termbased information to "compensate" for this potential information loss. In contrast, ST accounts for all entity markups, weighing their impact by their confidence levels. Hence, the "risk" in relying on entity-based information is lower7.
To further explore the effect of using a hard threshold for the confidence level of entity markups in HT, we present in Figure 2 its MAP performance as a function of q and d -- the query and document thresholds, respectively. Recall that low threshold corresponds to high recall markup. Figure 2 shows that low values of q and d lead to improved performance. This finding can be attributed to the fact that increasing the confidence-level threshold amounts to loosing potentially valuable information about appearances of entities in the query and documents. To compensate for the lower precision (i.e., noisier) markup caused by using a low threshold, more weight is put on term-based information as is evident in the relatively high optimal values of  presented in Figure 1. Specifically, we note that the learned values of , d, and q, averaged over the train folds, for AP, ROBUST, WT10G, GOV2, ClueB and ClueBF are (0.6, 0.01, 0.11), (0.7,0.1,0.01), (0.55,0.1,0.2), (0.77,0.1,0.01), (0.7,0.15,0), and (0.81, 0.17, 0) respectively; namely, relatively high values of  and low values of d and q lead to improved performance.
Entity linking. Our main evaluation is based on using TagMe
for entity linking. In Table 3 we compare the retrieval performance when using the entity markups of TagMe and Wikifier. Having Wikifier annotate large-scale collections is a challenging computational task. Thus, we present results only for AP, ROBUST and WT10G. We report MAP and NDCG; the performance patterns for p@10 are the same.
Table 3 shows that using ST, our best performing method from above, with Wikifier, results in performance that transcends (often, significantly) that of the standard term-based language model (TermsLM) when using all queries in a dataset
7Setting  on a per-query basis, in the spirit of work on fusing term-only-based and latent-entity-space-based retrieval scores [33], is a future direction we intend to explore.

Table 3: Comparing entity-linking tools. Either all queries in a dataset are used ("All Queries"), or only those marked with at least one entity by both TagMe and Wikifier ("Marked Queries"). Bold: best result in a column in a block; 't', 's', 'w' and 'e': statistically significant differences with TermsLM, TagMeST, Wikifier-ST and TagMe-STOEnt, respectively.

AP

ROBUST

WT10G

MAP NDCG MAP NDCG MAP NDCG

All Queries

TermsLM 20.9 40.4

TagMe ST

23 5t 45 5t

.

.

Wikifier ST

23.3t 43.6

25.0 28 1t
. 27.2t

43.5 46 9t
. 45.6t

19.1 22 9t
. 19.7t,s

30.3 34 3t
. 30.9s

Marked Queries

TermsLM 22.2

TagMe ST

25 1t .

Wikifier ST

25 1t .

TagMe STOEnt 18.5tw,s

Wikifier STOEnt 17.5tw,s

41.7 48 4t
.
46.2t 41.4s 39.1sw

25.4 43.9 21.4 34.2

28 8t 47 3t 24 8t 36 2

.

.

.

.

28.0t 46.4t 21.9s 34.0

22.9tw,s 41.1sw 18.1s 28.1s

19.4tw,s,e 34.8tw,s,e 12.6tw,s,e 21.8tw,s

(the "All Queries" block). However, the performance of using TagMe is consistently better.
TagMe marks more queries with at least one entity than Wikifier: for AP, ROBUST and WT10G, Wikifier marked no entities in 17, 34 and 26 queries, respectively; TagMe did not mark entities in 0, 1 and 3 queries. (For GOV2 TagMe marked all queries with entities and for ClueB/ClueBF all queries except for one.) Recall that for queries with no marked entities, ST relies only on term-based information.
To refine the comparison of TagMe and Wikifier, we report the performance of ST and STOEnt8 -- the latter relies only on entity tokens -- with these two tools over only queries in which both marked at least one entity. As can be seen in the "Marked Queries" block in Table 3, TagMe still outperforms Wikifier in almost all relevant comparisons; for STOEnt, several improvements are statistically significant.
TagMe's superiority can be partially attributed to marking more entities (with confidence level > 0) on average than Wikifier: (2.4, 1.8, 2.0) with respect to (1.7, 1.2, 1.0) in queries over AP, ROBUST and WT10G; and, (157.2, 158.7, 207.0) with respect to (58.4, 50.5, 61.7) in documents.
To conclude, our methods are effective with both TagMe and Wikifier. Using TagMe yields better performance that can be partially attributed to higher recall entity markup.
4.2.2 The score-based fusion methods
Table 4 presents the performance of the F-HT and FST methods from Section 3.2.1 that perform score fusion of term-only-based and entity-only-based retrieval scores. The performance of TermsLM (term-only language model), HT and ST that integrate term and entity information at the language model level, and that of F-HTCon which is a special case of F-HT (see Section 4.1), is presented for reference. We see that F-HT and F-ST substantially outperform TermsLM. (F-ST posts the best performance in most relevant comparisons in Table 4.) Both methods also outperform F-HTCon in most relevant comparisons.
8For queries for which a tool does not mark any entities, no documents are retrieved with STOEnt. Thus, we do not report the performance of STOEnt using all queries as the results are inherently biased in favor of TagMe which marks many more queries with entities than Wikifier.

70

Table 4: Score-based fusion ("F-" methods). Bold: best result in a row; 't', 'h', 's', 'f ' and 'c': statistically significant differences with TermsLM, HT, ST, F-HT and F-HTCon, respectively.

TermsLM HT ST F-HT F-HTCon F-ST

MAP

AP

p@10

NDCG

MAP ROBUST p@10

NDCG

MAP WT10G p@10

NDCG

GOV2

MAP p@10

NDCG

ClueB

MAP p@10

NDCG

ClueBF

MAP p@10

NDCG

20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8
17.1
22.7 16.5 18.8 33.6 24.3

23.1t 23.5t 23.1t

44.2t 43.8t

44 5t .

45.3t 45.5t

46 2t .

28.1t 28.1t 28.1t 45.5t 45.3t 45.7t

47.1t 46.9t 47.3t

21.9t

22 9t,h .

22.2t

30.4t 31.6t 30.0

32.7

34 3t .

32.7

32.1t
57.3t 47.4t

32.2t
57.7t 47.9t

33.5ts,h 58 6t
. 48 7t
.

18.7t 19.5t 19.6t,h

25.9t 27.4t 26.4t

18.7t 19.3t 19.1t

20.5t 20.3t 21.3t,h

37.9t 37.9t

39 6t .

28.4t 27.5t 29.5ts

22.5s 43.5t 45.1t 27.7t 45.2t 46.6t 21.6ts 30.4t
33.1 30.6hs,f
57.0
46.6 19.3t
27.5t 19.9t
19.7f
36.5f 27.6

23 9t,h . f ,c
44.2t 45.8t
28.4tc 46.7ts,c 47.8tc
22.9tc 31 8t
.
33.7t
33.3ts,,hc 58.0t 48.2t
20 8t,h . s,f ,c
28 8t,h .f
20 5t,h .f
21.8ts,,hc 39.4tc 29.2ts

Table 5: Comparison and integration with SDM [36]. Bold: the best result in a row. 't', 's', 'f ' and 'm' mark statistically significant differences with TermsLM, ST, F-ST and SDM, respectively.

AP ROBUST WT10G GOV2 ClueB ClueBF

TermsLM ST F-ST SDM SDM+STLM

MAP p@10 NDCG MAP p@10 NDCG MAP p@10 NDCG MAP p@10 NDCG
MAP p@10 NDCG MAP p@10 NDCG

20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8
17.1 22.7 16.5 18.8 33.6 24.3

23.5t
43.8t 45.5t

23 9t .
44 2t .
45 8t .

21.6sf 40.6f 42.3f

28.1t

28 4t .

25.7tf,s

45.3t

46 7t,s .

43.9tf

46.9t

47 8t .

44.8tf,s

22.9t 31.6t 34 3t
.

22.9t 31 8t
. 33.7t

20.2sf 27.7sf 30.7sf

32.2t 33.3t,s 32.1t

57.7t 58.0t 58.3t

47.9t 48.2t 48.4t

19.5t 27.4t 19.3t

20.8t,s 28.8t 20.5t

18.2tf,s 23.8sf 16.9sf

20.3t 21.8t,s 20.2tf

37.9t 39.4t 35.8tf

27.5t 29.2t,s 25.9tf

23.9tm 44.2tm 45.8tm
28.3tm 45.7tf,m 47.1tf,m 23.1tm 31.6tm 34.0tm 34 7t,s
. f ,m 61 4t,s
. f ,m 50 6t,s
. f ,m 21.5tm,s
30 8t,s . f ,m
21.9tm,s
22 7t,s . f ,m
42 8t,s . f ,m
32 2t,s . f ,m

In most relevant comparisons, F-HT outperforms HT and F-ST outperforms ST, although most performance differences are not statistically significant. The improvements can be attributed to the fact that F-HT and F-ST use a different smoothing parameter value for terms and entities while HT and ST use a joint one. (See Section 3.2.1 for details.)
The potential effectiveness of using different smoothing parameters for term and entity tokens stems from the different number of terms and entity markups in a document. The average number of terms in a document for AP, ROBUST, WT10G, GOV2, and ClueB (ClueBF) is 455.4, 474.8, 588.2, 904.7 and 813.6, respectively. The average number of entity markups with a confidence level > 0 is much lower: 157.2, 158.7, 207.0, 291.9 and 307.8.
4.2.3 Comparison and integration with SDM
We next compare our entity-based approach with the sequential dependence model (SDM) [36] which scores d by:
SSDM (d; q) d=ef SSimS(d, q)+OSimO(d, q)+U SimU (d, q);
the sum of the S, O and U parameters is 1; SimS(d, q), SimO(d, q) and SimU (d, q) are cross-entropy based similarity estimates of the document to the query, utilizing information about occurrences of unigram, ordered bigrams, and unordered bigrams, respectively, of q's terms in d; un-ordered bigrams are confined to 8-terms windows in documents.
Using entity tokens in our methods amounts to utilizing information about the occurrences of only some ordered variable-length n-grams of query terms in documents -- i.e., n-grams which constitute entities. Thus, in contrast to SDM, our methods do not utilize proximity information for query terms which are not in entity markups nor proximity information for unordered n-grams of query terms.
In addition, we study the merit of integrating entity-based information, specifically, our soft-thresholding language model STLM, with SDM. To that end, we augment the SDM scoring function with an entity-based document-query similar-

ity estimate, SimE(d, q). For this estimate, we use the score assigned to d by the STOEnt method; i.e., we use an entity-only language model since term-based information is accounted for in SimS(d, q). The resultant method, SDM+STLM, scores d by (S + O + U + E = 1):
SSDM+ST LM (d; q) d=ef S SimS (d, q) + OSimO(d, q)+
U SimU (d, q) + ESimE(d, q).
SDM+STLM can be viewed as a novel instantiation of a weighted dependence model (WSDM) [4] with a novel concept type (i.e., entity). If O = U = 0, SDM+STLM amounts to our F-ST method (see Section 3.2.1).
All free parameters of SDM and SDM+STLM: S, O, U , E and the Dirichlet smoothing parameter, µ, are set using cross validation as described in Section 4.1; S, O, U , and E are selected from {0, 0.1, . . . , 1} and µ is set to values in {100, 500, 1000, 1500, 2000, 2500, 3000}.
Table 5 shows that ST and F-ST outperform SDM, often statistically significantly, in most relevant comparisons (6 corpora × 3 evaluation measures). This implies that using variable length n-grams which potentially bear semantic meaning (entities) can yield better performance than using ordered and unordered bigrams which do not necessarily have semantic meaning. Recall that in contrast to SDM, ST and F-ST do not account for proximities between terms which do not constitute entities and for unordered bigrams.
In most relevant comparisons, SDM+STLM outperforms SDM and ST (which utilizes STLM) and is as effective as, and often posts statistically significant improvements over, F-ST -- its special case that fuses unigram term-only and entity-only retrieval scores. The few cases where F-ST outperforms SDM+STLM could be attributed to potential overfitting effects due to the high number of free parameters of SDM+STLM and the relatively low number of queries.
We also found that effective weights assigned to entityonly similarities in SDM+STLM (E) are much higher than those assigned to ordered (O) and un-ordered (U ) bigram

71

Table 6: Robustness analysis. Number of queries for which ST hurts (-) and improves (+) AP performance with respect to TermsLM and SDM.

AP ROBUST WT10G GOV2 ClueB ClueBF

-+- + - + - + - + - +

ST vs. TermsLM 38 61 75 173 31 63 50 99 54 137 75 112

ST vs. SDM

35 64 87 161 33 60 74 75 79 112 89 97

term-based similarities. Furthermore, effective values of O and U are lower and higher, respectively, for SDM+STLM than for SDM. These findings further attest to the merits of using entity-based similarities with respect to (ordered and un-ordered) bigram similarities, and show that un-ordered bigram, in contrast to ordered bigram, similarities could be complementary to entity-based similarities.
4.2.4 Further analysis
We now turn to further analyze merits, and shortcomings, of using entity-based query and document representations. To that end, we focus on the ST method that utilizes STLM.
Table 6 presents performance robustness analysis: the number of queries for which ST improves or hurts average precision (AP) over TermsLM and SDM. In both cases, ST improves AP for more queries than it hurts; naturally, the differences with SDM are smaller than those with TermsLM.
One advantage of STLM is that it represents the query and documents using entities which constitute variable length n-grams with semantic meaning. A case in point, query #41 in ClueWeb, "orange county convention center", refers to the primary public convention center for the Central Florida region. TermsLM, SDM and ST ranked the Web home page for this entity second. However, at the third rank in the lists retrieved by TermsLM and SDM appears a Wikipedia page titled "list of convention and exhibition centers", which is not specific to the entity of concern. The average precision (AP) of TermsLM, SDM and ST for the query in the ClueB dataset was 9, 13, and 30, respectively, attesting to the merit of the correct identification of the entity in the query and its utilization by ST.
The ST method can suffer from incorrect entity identification in queries. For example, query #407 in ROBUST, "poaching, wildlife preserves", targets information about the impact of poaching on the world's various wildlife preserves. The entities identified by TagMe are "poaching", "wildlife" and "preserves"; the latter refers to fruit preserves instead of nature preserves. Such erroneous entity identification can be attributed to the little context short queries provide. Consequently, the AP of ST for this query is only 8 while that of TermsLM and SDM is 31.4 and 30.0, respectively.
4.3 Using entity-based language models in additional retrieval paradigms
We next explore the effectiveness of using our entity-based language models in two additional retrieval paradigms: clusterbased document retrieval and query expansion.
4.3.1 Cluster-based document retrieval
Let Dinit denote the list of top-n documents retrieved by TermsLM (standard language-model-based retrieval). Following common practice in work on cluster-based document retrieval [32, 24], we re-rank Dinit using information induced from nearest-neighbor clusters of documents in Dinit.

Table 7: Cluster-based document re-ranking. Bold:

the best result in a row; 't', 's', '' and '' mark sta-

tistically significant differences with TermsLM, ST,

C-Term-Term and C-Term-Ent, respectively.

TermsLM ST C-Term-TermC-Term-EntC-Ent-Ent

AP

p@10 NDCG

ROBUST p@10 NDCG

WT10G

p@10 NDCG

GOV2

p@10 NDCG

ClueB

p@10 NDCG

ClueBF

p@10 NDCG

39.6 40.8 42.2
43.5
28.6 31.2 53.4 45.0 23.7 17.2 32.1 22.9

42.5 44.8t 44.3t
45.5t
30.6
33.4 57.0t
46.8 27.1t
19.1 36.9t 27.8t

43.2t 44.2t
43.1
44.2
30.2 32.1 55.1 45.8 23.7 17.2 31.2s 23.1s

44.3t
44.9
46.0t 47.5t 3335.7.4tt,s 58 3t
. 48 9t
.
33 0t,s 24.9t,s
. 3308.53tt
.

46 5t,s .
46 8t .
47 7t,s . ,
49 1t,s . ,
34 8t,s 36.3t,s
. 57.9t 47.8t 3212..59tt,,ss 39 0t 29..6t

We use Sim(x, y) d=ef exp(-CE `xMLE || yDir´) to measure the similarity between texts x and y [24]; xMLE is an unsmoothed MLE induced from x and yDir is a Dirichlet smoothed language model induced from y. Each document

d ( Dinit) and the k - 1 documents d (d = d) in Dinit that yield the highest Sim(d, d) constitute a cluster.

We rank the (overlapping) clusters c, each contains k doc-

q

uments, by:

k

Q
dc

S

im(q,

d)

[32].

This is a highly effective

simple cluster ranking method [24]. To induce document

ranking, each cluster is replaced with its constituent docu-

ments omitting repeats; documents in a cluster are ordered

by their query similarity: Sim(q, d).

The document (re-)ranking procedure just described re-

lies on the choice of the document language models used to

induce clusters (i.e., in Sim(d, d)) and the choice of docu-

ment and query language models used to induce document-

query similarities (Sim(q, d)); the latter are used for rank-

ing both clusters and documents within the clusters. We

use C-Term-Term to denote the standard method that

uses term-only language models for inducing clusters and

document-query similarities [32, 24]. The C-Term-Ent

method utilizes the same clusters used by C-Term-Term, but

uses our entity-based language model, STLM, for inducing

document-query similarities to rank clusters and documents

in them. In the C-Ent-Ent method, STLM is used to both

create clusters and induce document-query similarities. As a

reference comparison, we re-rank Dinit using the ST method

that uses STLM but does not utilize clusters.

As the main goal of cluster-based re-ranking is improv-

ing precision at top ranks [32, 24], we report p@10 and

NDCG@10 (NDCG). Free-parameter values are set using

cross validation; NDCG is the optimization criterion. Specif-

ically, n is selected from {50, 100}; k is in {5, 10}; and, 

(used in STLM) is in {0, 0.1, . . . , 1}; the Dirichlet smooth-

ing parameter is set to 1000. Table 7 presents the results.

We see that all cluster-based methods (denoted "C-X-Y")

almost always outperform the initial term-based document

ranking, TermsLM. C-Term-Ent substantially outperforms

C-Term-Term. This attests to the merits of using STLM

for inducing cluster ranking and within cluster document

ranking. In most relevant comparisons, C-Ent-Ent outper-

forms (and is never statistically significantly outperformed

by) C-Term-Ent, attesting to the potential merits of using

72

Table 8: Query expansion. Bold: the best result

in a row. 't', 's', 'r', 'w', 'm' and 'n' mark sta-

tistically significant differences with TermsLM, ST,

RM3, WikiRM, SDM-RM and RMST, respectively.

TermsLM ST RM3 WikiRMSDM-RM RMST RMST-ST

MAP AP p@10
NDCG MAP
ROBU p@10
ST NDCG MAP
WT p@10 10G NDCG
MAP GOV2 p@10
NDCG MAP ClueB p@10 NDCG MAP Clue p@10 BF NDCG

20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8 17.1 22.7 16.5 18.8 33.6 24.3

23.5t 24.1t 24.0t

24.9t

24.6t 27.4tw,s,m ,r,n

43.8t 42.5t 46.2t

43.9t

44.8t

46 8t,r .

45.5t

43.2

48 2t,r .

45.6t

45.0t

47.4t,r

28.1t 28.3t 27.8t

28.4t

29.0t 30.5tw,s,m ,r,n

45.3t 43.6 44.6t

43.2 45.9tm,r 47.1tw,s,m ,r

46.9t 43.8s 46.1t,r 43.6sw 46.5tm,r 47.2tm,r

22 9t .

19.6s

21.9t,r

20.0s

22.7tm,r

22.8tm,r

31.6t 28.0s

34 2t,r .

28.6w

31.7tm,r

34 3t .

30.1s

34 3t,r .

30.5sw

32.9

31.1t 31.8s

32.2t 32.4t 32.1t

33.7tw

33.1t

33 7t,s .

57.7t 58.1t

60 1t .

58.0t

59.6t

58.5t

47.9t 48.0t

50 6t .

47.6

49.4t

48.8t

19.5t 19.3t 21.9t,s,r 20.9t,r 20.7t,s,r 22.1tn,s,r

27.4t

30.6t

35 3t,s,r .

32.2tw,s

32.2tw,s 34.9tn,s,r

19.3t 22.6t,s 26.1t,s,r 24.3t,s 25.1t,s,r 27.1tn,s,r

20.3t 20.4t 21.0t 21.8t,s,r 20.8t 21.9tn,s

37.9t 37.9t 38.5t

39 7t,r .

38.2t

38.4t

27.5t 28.1t 28.2t

29.8t,r

28.5t

30 3t,s .

entity-based information to also create clusters. However, only two improvements are statistically significant.
Finally, Table 7 shows that in almost all relevant comparisons, ST outperforms TermsLM (often, statistically significantly) and C-Term-Term and is outperformed by C-TermEnt and C-Ent-Ent. This shows that while there is merit in using STLM for direct ranking of documents as shown in Section 4.2.1, the performance can be further improved by using STLM for cluster-based document ranking.

4.3.2 Query expansion
As noted in Section 2, there is much work on expanding queries with terms and entities using entity-based information. In contrast, our entity-based language models, when induced from the query, utilize only query terms and entities marked in the query. Hence, we study the effectiveness of using our language models to perform query expansion.
We use the relevance model (RM3) [1] as a basis for instantiating expanded query forms. The probability assigned to token t by a relevance model RM is:

RM (t)

d=ef

qMLE (t)

+

(1

-

)

X
dL

dDir

(t)

S(d; q)

P
d L

S(d

;

q)

;

(8)

 is a free parameter; L is a list of top-retrieved documents

used to construct RM ; S(d; q) is d's score. Due to computa-

tional considerations, as in work on entity-based query ex-

pansion [13, 45] we use RM to re-rank an initially retrieved

document list; CE `RM || dDir´ serves for re-ranking. Using only terms as tokens, and applying standard language-

model-based retrieval (TermsLM) over the corpus to create

L, yields the standard RM3 [1]. Creating L by applying

TermsLM over Wikipedia results in WikiRM [46], an ex-

ternal corpus expansion approach also used in [13, 45]. RM3

and WikiRM re-rank a document list retrieved by TermsLM.

(WikiRM is the only model where the list from which RM is

constructed, L, is not a sub-set of the list to be re-ranked.)

In both methods, S(d; q) d=ef exp(-CE `qMLE || dDir´). The SDM-RM model [13] is constructed from, and used

to re-rank, lists retrieved by the sequential dependence model

(SDM) [36]. dDir, and the resultant relevance model constructed by setting  = 0 in Equation 8, are term-based unigram language models; S(d; q) is the exponent of the score assigned to d by SDM. Re-ranking is performed by linear interpolation of the SDM score assigned to d and CE `RM || dDir´, using a parameter . SDM-RM is, in fact, the highly effective Latent Concept Expansion method [37] without IDF-based weighting of expansion terms.
The next two relevance models, defined over T (the termentity token space from Equation 1), are novel to this study. They utilize our STLM language model which integrates terms and entities at the language model level. RMST is inspired by methods proposed by Dalton et al. [13]9 by the virtue of using both terms and entities for query expansion. qMLE and dDir are our STLM language models. S(d; q) d=ef exp(-CE `qMLE || dDir´). The TermsLM method is applied over the corpus to create the initial list to be re-ranked (cf. [45]) and from which L is derived.
RMST-ST is constructed as RMST using STLM. The difference is that our entity-based ST method, rather than TermsLM, is used to create the initial list to be re-ranked and from which L is derived. The formal ease of using STLM in the relevance model (Equation 8), yielding RMST and RMST-ST, attests to the merits of using a single language model defined over terms and entities with respect to the alternative score-based fusion approach from Section 3.2.1.
The free parameters of all methods are set using cross validation. The number of expansion terms (i.e., those assigned the highest probability by RM ), the number of documents in L, and  are set to values in {10, 30, 50, 100}, {50, 100} and {0, 0.1, . . . , 1}, respectively. (Only for WikiRM, the number of documents in L is selected from {1, 5, 10, 30, 50, 100} following [46].) All lists that are re-ranked contain 1000 documents. The values of the free parameters of ST and SDM are selected from the ranges specified in Section 4.1. The Dirichlet smoothing parameter, µ, is selected from {100, 500, 1000, 1500, 2000, 2500, 3000}; for relevance model construction (Equation 8) the value 0 is also used (yielding unsmoothed MLE). To reduce the number of free-parameter values configurations, we use the same value of µ for creating L, for re-ranking and for constructing the relevance model, unless 0 is used for relevance model construction.
Table 8 presents the performance. Our ST method, which does not perform query expansion, is competitive with the term-based relevance model (RM3). We also see that RMST is an effective expansion method which often outperforms RM3 and SDM-RM. This finding echoes those from past work [13, 45] about the merits of using both terms and entities for query expansion. The best performing method in most relevant comparisons is RMST-ST which uses STLM to (i) create an effective initial list for re-ranking; (ii) create an effective list, L, for relevance model construction; and, (iii) induce ranking using the entity-based relevance model as in RMST. We conclude that our STLM language model can play different important roles in query expansion.
Table 8 shows that expansion using Wikipedia as an external corpus (WikiRM) is effective. Our RMST and RMSTST expansion methods (as well as ST) utilize entity tokens marked by TagMe (i.e., Wikipedia concepts), but do no use
9Various expansion methods, which utilize also auxiliary information about entities from the entity repository, were integrated in [13]. We do not use such auxiliary information.

73

the text on their Wikipedia pages in contrast to WikiRM. Thus, integrating WikiRM with our methods, e.g., using score-based integration [13], is interesting future direction.
5. CONCLUSIONS
We presented novel entity-based language models induced using an entity linking tool. The models simultaneously account for the uncertainty in the entity-linking process and the balance between using term-based and entity-based information. We showed the merits of using the language models for document retrieval in several retrieval paradigms.
Acknowledgments. We thank the reviewers for their comments. This paper is based upon work supported in part by a Yahoo! faculty research and engagement award.
6. REFERENCES
[1] N. Abdul-jaleel, J. Allan, W. B. Croft, O. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. Umass at TREC 2004: Novelty and hard. In Proc. of TREC-13, 2004.
[2] J. Allan, J. P. Callan, W. B. Croft, L. Ballesteros, J. Broglio, J. Xu, and H. Shu. Inquery at TREC-5. In Proc. of TREC-5, pages 119­132, 1996.
[3] A. R. Aronson, T. C. Rindflesch, and A. C. Browne. Exploiting a large thesaurus for information retrieval. In Proc. of RIAO, volume 94, pages 197­216, 1994.
[4] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proc. of WSDM, pages 31­40, 2010.
[5] M. Bendersky, D. Metzler, and W. B. Croft. Parameterized concept weighting in verbose queries. In Proc. of SIGIR, pages 605­614, 2011.
[6] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proc. of WSDM, pages 443­452, 2012.
[7] W. C. Brand~ao, R. L. T. Santos, N. Ziviani, E. S. de Moura, and A. S. da Silva. Learning to expand queries using entities. JASIST, 65(9):1870­1883, 2014.
[8] G. Cao, J. Nie, and J. Bai. Integrating word relationships into language models. In Proc. of SIGIR, pages 298­305, 2005.
[9] X. Cheng and D. Roth. Relational inference for wikification. In Proc. of EMNLP, pages 1787­1796, 2013.
[10] K. Collins-Thompson and J. Callan. Query expansion using random walk models. In Proc. of CIKM, pages 704­711, 2005.
[11] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.
[12] M. Cornolti, P. Ferragina, and M. Ciaramita. A framework for benchmarking entity-annotation systems. In Proc. of WWW, pages 249­260, 2013.
[13] J. Dalton, L. Dietz, and J. Allan. Entity query feature expansion using knowledge base links. In Proc. of SIGIR, pages 365­374, 2014.
[14] O. Egozi, S. Markovitch, and E. Gabrilovich. Concept-based information retrieval using explicit semantic analysis. ACM Transactions on Information Systems (TOIS), 29(2):8, 2011.
[15] P. Ferragina and U. Scaiella. Tagme: On-the-fly annotation of short text fragments (by Wikipedia entities). In Proc. of CIKM, pages 1625­1628, 2010.
[16] W. R. Hersh, D. H. Hickam, and T. Leone. Words, concepts, or both: optimal indexing units for automated information retrieval. In Proc. of SCAMC, page 644, 1992.
[17] D. Hiemstra. Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term. In Proc. of SIGIR, pages 35­41, 2002.
[18] M. Hsu, M. Tsai, and H. Chen. Combining wordnet and conceptnet for automatic query expansion: A learning approach. In Proc. of AIRS, pages 213­224, 2008.
[19] S. Huston and W. B. Croft. A comparison of retrieval models using term dependencies. In Proc. of CIKM, pages 111­120, 2014.
[20] A. Kotov and C. Zhai. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries. In Proc. of WSDM, pages 403­412, 2012.

[21] R. Krovetz and W. B. Croft. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems (TOIS), 10(2):115­141, 1992.
[22] G. Kumaran and J. Allan. A case for shorter queries, and helping users create them. In Proc. of NAACL, pages 220­227, 2007.
[23] H.-K. J. Kuo and W. Reichl. Phrase-based language models for speech recognition. In Proc. of EUROSPEECH, 1999.
[24] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research (JAIR), 41:367­395, 2011.
[25] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proc. of SIGIR, pages 111­119, 2001.
[26] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.
[27] M. Levit, S. Parthasarathy, S. Chang, A. Stolcke, and B. Dumoulin. Word-phrase-entity language models: getting more mileage out of n-grams. In Proc. of INTERSPEECH, pages 666­670, 2014.
[28] H. Li and J. Xu. Semantic matching in search. Foundations and Trends in Information Retrieval, 7(5):343­469, 2014.
[29] R. Li, L. Hao, P. Zhang, D. Song, and Y. Hou. A query expansion approach using entity distribution based on markov random fields. In Proc. of AIRS, 2015.
[30] S. Liu, F. Liu, C. T. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In Proc. of SIGIR, pages 266­272, 2004.
[31] X. Liu, F. Chen, H. Fang, and M. Wang. Exploiting entity relationship for query expansion in enterprise search. Information Retrieval Journal, 17(3):265­294, 2014.
[32] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proc. of ECIR, pages 454­462, 2008.
[33] X. Liu and H. Fang. Latent entity space: a novel retrieval approach for entity-bearing queries. Information Retrieval Journal, 18(6):473­503, December 2015.
[34] R. Mandala, T. Tokunaga, and H. Tanaka. Combining multiple evidence from different types of thesaurus for query expansion. In Proc. of SIGIR, pages 191­197, 1999.
[35] E. Meij, D. Trieschnigg, M. de Rijke, and W. Kraaij. Conceptual language models for domain-specific retrieval. Information Processing & Management, 46(4):448­469, 2010.
[36] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.
[37] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In Proc. of SIGIR, pages 311­318, 2007.
[38] D. Milne and I. H. Witten. Learning to link with Wikipedia. In Proc. of CIKM, pages 509­518, 2008.
[39] D. Pan, P. Zhang, J. Li, D. Song, J. Wen, Y. Hou, B. Hu, Y. Jia, and A. N. D. Roeck. Using Dempster-Shafer's evidence theory for query expansion based on freebase knowledge. In Proc. of AIRS, pages 121­132, 2013.
[40] C. Shah and W. B. Croft. Evaluating high accuracy retrieval techniques. In Proc. of SIGIR, pages 2­9, 2004.
[41] P. Srinivasan. Query expansion and medline. Information Processing & Management, 32(4):431­443, 1996.
[42] E. M. Voorhees. Using wordnet to disambiguate word senses for text retrieval. In Proc. of SIGIR, pages 171­180, 1993.
[43] E. M. Voorhees. Query expansion using lexical-semantic relations. In Proc. of SIGIR, pages 61­69, 1994.
[44] C. Xiong and J. Callan. EsdRank: Connecting query and documents through external semi-structured data. In Proc. of CIKM, pages 951­960, 2015.
[45] C. Xiong and J. Callan. Query expansion with Freebase. In Proc. of ICTIR, pages 111­120, 2015.
[46] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on Wikipedia. In Proc. of SIGIR, pages 59­66, 2009.
[47] Y. Yang and C. G. Chute. Words or concepts: the features of indexing units and their optimal use in information retrieval. In Proc. of SCAMC, page 685, 1993.
[48] C. Zhai. Statistical language models for information retrieval: A critical review. Foundations and Trends in Information Retrieval, 2(3):137­213, 2008.
[49] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.

74

A Test Collection for Matching Patients to Clinical Trials
Bevan Koopman1,2, Guido Zuccon2
1Australian e-Health Research Centre, CSIRO, Brisbane, Australia 2Queensland University of Technology (QUT), Brisbane, Australia
bevan.koopman@csiro.au, g.zuccon@qut.edu.au

ABSTRACT
We present a test collection to study the use of search engines for matching eligible patients (the query) to clinical trials (the document). Clinical trials are experiments conducted in the development of new medical treatments, drugs or devices. Recruiting candidates for a trial is often a timeconsuming and resource intensive effort, and imposes delays or even the cancellation of trials.
The collection described in this paper provides: i) a large corpus of clinical trials; ii) 60 patient case reports used as topics; iii) multiple query representations for a single topic (long, short and ad-hoc); iv) a user provided estimate of how many trials they expect each patient topic would be eligible for; and v) relevance assessments by medical professionals. The availability of such a collection allows researchers to investigate, among other questions: i) the effectiveness of retrieval methods for this task, ii) how multiple representations of an information affect retrieval iii) what influences relevance assessments in this context, iv) whether automated matching of patients to trials improves patient recruitment. The collection is available at http://doi.org/10.4225/08/5714557510C17.
CCS Concepts
·Information systems  Information retrieval; Test collections;
Keywords
Information Retrieval, Clinical Trials, Test Collections
1. INTRODUCTION
Clinical trials are experiments done in the development of new treatments, drugs or medical devices. They are a critical step for medical advancement and are a regulatory requirement before new medical advances can be used in practise. However, recruiting a sufficient number of eligible patients to participate in a trial can be a major obstacle [9]. If suitable patients cannot be found then trials may be cancelled or significantly delayed. Even if sufficient patients are
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
© 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914672

found, the recruitment process can be time consuming and resource intensive. Automating and improving this difficult manual process has the potential to improve the running of a clinical trial. In addition, certain patients can benefit from finding and being included into specific trials, e.g., to have access to potentially life-saving treatment options. However, often treating doctors are not aware of trials that may benefit specific patients.
Large collections of clinical trials are published online (e.g., ClinicalTrials.gov contained approx. 200,000 trails in 2015), with details of the inclusion and exclusion criteria of eligible patients. At the same time, a patient's conditions are also documented in electronic form (for example, in electronic patient records). Matching patients to clinical trials is essentially an information retrieval task: the query is the patient details (either in the form of electronic patient records or ad-hoc queries) and the documents are the clinical trials currently recruiting patients.
While research exists on automated matching of patients to trials [10], much of the evaluation is done on small, private datasets and on specific diseases. This paper aims to address this gap by developing a large-scale, heterogeneous test collection of clinical trial documents and associated patient queries. The availability of such a collection allows researchers to investigate: i) the effectiveness of retrieval methods for this task, ii) how multiple representations of an information affect retrieval iii) what influences relevance assessments in this context, iv) whether automated matching of patients to trials improves patient recruitment.
2. RELATED WORK
Sustained focus on medical information retrieval has led to the development of a number of other relevant test collections. Within TREC, there have been two medical related tracks relevant to this work: the Medical Records Track (MedTrack) and the Clinical Decision Support (CDS Track).
The MedTrack task involved searching a collection of electronic patient records for patients that meet a certain criteria (the query) [12]. One use case was that the query indicated the inclusion criteria for a clinical trial, while the documents were patients to be retrieved that matched that criteria. Thus, Medtrack could be viewed as the opposite, trial-centric (trial is the query and patient is the document) to the patient-centric task considered here. Another important difference that sets this work apart is that real clinical trials were used; instead Medtrack used only ad-hoc queries to describe the patient (e.g., topic# 115 "Adult patients who are admitted with an asthma exacerbation").
In the TREC CDS task the topics were patient case re-

669

A 51-year-old woman is seen in clinic for advice on osteoporosis. She has a past medical history of significant hypertension and diet-controlled diabetes mellitus. She currently smokes 1 pack of cigarettes per day. She was documented by previous LH and FSH levels to be in menopause within the last year. She is concerned about breaking her hip as she gets older and is seeking advice on osteoporosis prevention.
Figure 1: Example patient case (topic# 201429).
ports and were used to search for medical journal articles that would help uncover the diseases, tests and treatments relevant to the patient case [11]. The patient case reports were verbose: on average 78 words per topic (an example report is shown in Figure 1). TREC CDS is intended for searching medical literature for clinical decision support; however, the patient case reports are general descriptions of a patient past and current medical history. The patient case reports can, therefore, also be used to search for clinical trials. For this reason, we use the same patient case reports from TREC CDS as our topics in this test collection to search for clinical trials. This also has the added advantage of being able to link a patient with both clinical trials from this collection and associated medical literature from the TREC CDS collection.
Other medical collections do exist in the TREC Genomics Track and in the CLEF eHealth Lab; however, these are focused on genomic search and consumer health search and, therefore, not detailed here.
3. CREATION OF THE COLLECTION
3.1 Document Collection
A collection of 204,855 publicly available clinical trails was crawled from ClinicalTrials.gov.1 Trials are made available in a specific XML format, however, large portions (including the inclusion and exclusion criteria) are free-text.2 These represent the documents to be searched.
3.2 Query Topics
As query topics, we adopted the topics previously used by the TREC CDS [11], comprising 60 patient case reports (30 from 2014 and 30 from 2015). Each topic describes a patient with certain conditions and observations. Each patient case topic had two forms: a description (on average 78 words) and a shorter summary (on average 22 words).
As noted above, the topics were verbose patient case reports. Automatically matching these case reports to clinical trials was the first use case -- here the user simply supplies the case report and does not author a query. However, an alternative use case exists as a traditional ad-hoc retrieval scenario where the user authors a short keyword query. To cover this second use case we showed four medical assessors each patient case report and asked them to provide ad-hoc keyword queries that they would issue to a search engine to find clinical trials for the given patient. A total of 489 unique queries were produced, on average 8.2 (sd=3.2) keyword queries per topic. In addition, assessors were asked the following question for each topic: "How many clinical trials do you expect this patient would be eligible for?" The
1This represents all the trials available on 16th Dec., 2015. 2More details on the format and download options can be found at: https://clinicaltrials.gov/ct2/resources/download.

answer to this was recorded and was used in the INST evaluation measure [5]3 we will detail in Section 3.4.
3.3 Pooling and Judging
A number of baseline retrieval models were run to form the pool. These included: BM25, Language Model (Direchlet and Jelinek-Mercer), Divergence From Randomness (BB2 and DLH) and TF-IDF.4 While this was only a small number of systems, we note that Moffat et al. found that query variations are as strong as system variations in producing a diverse document pool [6]; thus, we overcame the limit of having a small number of systems by including a large number of query variations. Specifically, each baseline system listed above was run with the following queries for each topic: i) the patient case report description; ii) the patient case report summary; and iii) the ad-hoc keyword queries provided by our medical assessors (8.2 queries on average). This equates to an average of 61 runs per topic (10.2 queries per topic * 6 baseline methods). This provided a diverse set of retrieved documents to form the pool.
To maximise the time and minimise costs associated with employing medical assessors it was important to maximise the chance of sampling important documents for assessment. A standard approach to form the pool is to include all documents that are highly ranked by participating systems. However, Moffat et al. [7] noted that not all documents provide the same benefit and instead propose an alternative method based on the Ranked Biased Precision (RBP) evaluation measure. Documents were ranked according to RBP across all queries; documents that were retrieved by multiple, different systems in top-ranked positions would appear higher in the RBP ranking. The pool was then formed based on the available assessment budget by setting a cut-off point of 4,000 documents in the RBP ranking -- documents above the cut-off were included in the pool.
The documents and queries were uploaded to the Relevation! relevance assessment system [2] and four medical assessors were engaged to conduct the relevance assessment according to a three-point scale: 0) Would not refer this patient for this clinical trial ; 1) Would consider referring this patient to this clinical trial upon further investigation; and 2) Highly likely to refer this patient for this clinical trial. Queries were divided amongst the four assessors; a control query (topic #20158) was used to familiarise assessors with the task and to record inter-coder reliability (agreement found to be 70%). This highlights the difficulty intrinsic in judging relevance in the medical domain, as identified by other studies [3]. Reasons for assessors disagreement will be investigated in future work.
3.4 The Task and Evaluation Measures
The task of matching patients to trials has three specific use cases; we use these to set the evaluation measures.
The first use case is in a General Practitioner (GP) setting where the GP opens a patient's record as part of a consultation and a search is automatically initiated to find relevant clinical trials that the GP may refer the patient to. In this scenario the GP is time-pressured and would likely only review a small number of results, stopping when a sin-
3We are thankful to the authors of [5] for sharing their sample implementation of INST. 4The Terrier IR system was used for all models and parameters left to Terrier defaults [4].

670

Number of queries Topic, 201401 ... 201530

16

Assessor

Assessor A 12
Assessor B 8
Assessor C

4

Assessor D

0

Topic, 201401 ... 201530
Figure 2: Number of queries supplied by each assessors for each topic.

gle relevant trial is found. Thus for this scenario we adopted Mean Reciprocal Rank (MRR) as the evaluation measure.
The second use case is also set within a general medical professional (GP or other) but where the user is specifically searching for clinical trials and may dedicate more time and effort to the task. In this case they may issue an ad-hoc query themselves and be willing to evaluate a few more results. For this scenario we adopted Precision at 5 (P@5) as the evaluation measure.
The final use case is for medical specialists or patients themselves searching for trials. Here both types of users may conduct longer search sessions and review far more results. They may use both short ad-hoc queries and more verbose patient case reports. In addition, both users would have an expectation about how many clinical trials they would be eligible for. This would influence their search behaviour: for rare diseases, they may expect to find a very small number of trials and would therefore not persist in examining results at greater rank depths. In contrast, for common diseases, they would expect to find many relevant trials and would therefore persist to greater rank depths. This notion of expected number of (relevant) results is directly modelled by T in the INST evaluation measure [5]; thus we adopted INST for this scenario. INST is a weighted precision metric where the likelihood of the user assessing a document at a specific rank depends on the rank position, the expected number of relevant documents, and the actual number of relevant documents encountered up to that rank. According to INST, the expected depth at which the user would stop viewing documents falls between approximately T +0.25 (all encountered documents are relevant) and 2T + 0.5 (no encountered documents are relevant) [5].
4. ANALYSIS OF THE COLLECTION
4.1 Test Collection Statistics
The collection contains 204,855 clinical trial documents. There are 60 topics made up of three types: patient case descriptions, patient case summaries and assessor provided adhoc queries, totalling an average of 10.2 queries per topic. A total of 4,000 documents were judged (67 per topic, min=13, max=153, mean=63, sd=27).
The number of ad-hoc queries provided by the assessors differed per topic and per assessor, as shown in Figure 2. Some assessors entered multiple short queries, while others preferred single longer queries. The average query length was 4.5 words, sd=2.5 words.
Assessors were also asked how many clinical trials they expected a patient would be eligible for. This was represented as T in the INST evaluation measure. The values of T for each topic, across the four assessors, is shown in Figure 3. Values of T varied across topics, thus indicating the different information needs assessors derived from different patients. Although T varied across topics, individual assessors displayed similar trends across topics; e.g., assessor D typically chose lower values of T and assessor C displayed

Assessor:  A B C D

 
   
   
 
 
 
 
 


C=80 C=80

 
 
 
  

 
 

   
       

 
 
 


B=100

0

20

40

60

T, expected number of trials*

*x-axis truncated at T = 60 excluding outliners T = 80, 80, 100.

Figure 3: T , the users' expected number of clinical trials for a patient topic.
higher values of T . This resulted in values of T that varied across assessors for a single topic. Qualitative feedback from assessors indicated estimating T was challenging and subjective. The assessors were asked about their rationale for determining values of T . We found that regardless of the value of T , assessors indicated that the main rationale was how rare or common the patient's medical condition was; secondary to that was the likelihood that clinical trails were currently being conducted on the patient's condition.

4.2 Retrieval Results Analysis
The relevance assessments we collected were used to evaluate six standard baselines. The purpose of the evaluation was twofold. On one hand, the retrieval systems were used to form the pool for assessments, thus the evaluation reports how effective the systems that contributed to the pool were. On the other hand, this evaluation serves to demonstrate the type of research questions this collection can contribute to investigate, e.g., what type of queries (verbose, summaries, ad-hoc) are most effective for searching for clinical trials.
For each system, runs were created using three different topic types: i) verbose patient case report descriptions; ii) shorter patient case report summaries; and iii) short ad-hoc keyword queries. Note that ad-hoc queries generated more than one run per topic per system, i.e., on average each system generated 8.2 runs per topic. We therefore averaged the effectiveness of a system over all ad-hoc queries for a topic.

671

Topic type: Ad-hoc Description Summary

INST 0.06

P@5 0.3

MRR

0.04

0.15

0.2

0.10

0.02

0.05

0.1

0.00 BDMF2R5-DLBFMBLR-2M-JD-eDLlinHireickh-lMetercTeFr-IDF

0.00 BDMF2R5-DLBFMBLR-2M-JD-eDLlinHireickh-lMetercTeFr-IDF

0.0 BDMF2R5-DLBFMBLR-2M-JD-eDLlinHireickh-lMetercTeFr-IDF

Figure 4: Retrieval results for different baselines and

topic representations.

Retrieval results are shown in Figure 4. We firstly observe that there was high variability of performance across the topic types. The assessor-provided ad-hoc queries proved most effective overall, followed by summary patient case reports and finally the full description patient case reports were the least effective topic type.
There was also variability across different baseline methods. The best method for ad-hoc queries was clearly the Jelinek-Mercer language model. For the longer summaries and descriptions the best method varied: TF-IDF proving effective for summaries, while no method clearly stood out for descriptions. This observation suggests that different baseline methods are best suited to different use cases.
Overall, we note that there was more variability across topic types than across baseline methods. This is inline with the results of Moffat et al. [6] that found query variability was as significant as system variability.
5. DISCUSSION
The test collection described in this paper is clearly aimed at focusing research on matching eligible patients to clinical trails; however, it also provides the basis for exploring a number of other research aspects: Query representations and variations. The collection provides multiple representations for a single topics: descriptions, summaries and ad-hoc queries (on average 8.2 per topic). The availability of multiple representations makes it possible to investigate whether specific retrieval methods are more suitable to different representations, e.g., ad-hoc vs verbose. The ad-hoc queries themselves expose different ways of formulating the same information need, each leading to different effectiveness for the same retrieval method. Along with the TREC-8 Query Track [1] and the CLEF 2015 eHealth [8], our collection is a rare example of a test collection with multiple query variations. Expected number of relevant results (T). Assessors indicated how many trials they expected the patient would have been eligible for. This data can serve the evaluation (e.g., through INST) but also allows exploring the perception about the results assessors expected to obtain. In particular, we observed that T greatly varied across topics and across assessors (this latter result was in contrast to previous studies [6]). Finally, this is the first collection that provides a user's estimate of the number of relevant documents they believe are required for each topic. What makes a clinical trial relevant. The collection makes available data to understand what characterises relevance when judging the eligibility of a patient to a clinical trial. It also provides evidence to the fact that judging the eligibility of a clinical trial is often challenging when only summary information about patients is available.

6. CONCLUSIONS
This paper presented a test collection aimed at helping the development of systems to automatically match eligible patients to clinical trials. The collection can be used to discriminate between different systems on this task. The limited number of assessments may make the evaluation less reliable for new systems that greatly differ from those used to form the pool. Nevertheless, the value of the collection is the insights it provides into research questions related to, e.g., the effectiveness of different query representations (and variations), how assessors judge relevance of patients to clinical trials, etc. The collection presents several original aspects. This is the first publicly available, large scale collection for matching patients to clinical trials -- an important task for medical advancement. The collection is also the first that provides estimates of the number of expected relevant documents for each query topic (T ), and is one of the few that provides multiple query representations and variations.
Future work will consider increasing the number of assessed documents, including increasing the number of systems used to form the pool, especially when specialised systems to search clinical trials become available. We also plan to expand the analysis of query variations and the effect query representations have on system effectiveness. Finally, another line of future research will consider the analysis of assessor disagreement (both for relevance and for the value of T ) to gain further insights about how users perceive relevance for this task. The collection is available at http://doi.org/10.4225/08/5714557510C17 and our INST implementation at https://github.com/ielab/inst eval.
7. REFERENCES
[1] C. Buckley and J. A. Walz. The TREC-8 Query Track. In TREC, 1999.
[2] B. Koopman and G. Zuccon. Relevation!: An open source system for information retrieval relevance assessment. In SIGIR, Gold Coast, Australia, July 2014.
[3] B. Koopman and G. Zuccon. Why assessing relevance in medical IR is demanding. In MedIR at SIGIR, 2014.
[4] C. Macdonald, R. McCreadie, R. L. Santos, and I. Ounis. From puppy to maturity: Experiences in developing terrier. Proc. of OSIR at SIGIR, pages 60­63, 2012.
[5] A. Moffat, P. Bailey, F. Scholer, and P. Thomas. INST: An adaptive metric for information retrieval evaluation. In ADCS, Sydney, Australia, 2015.
[6] A. Moffat, F. Scholer, P. Thomas, and P. Bailey. Pooled evaluation over query variations: Users are as diverse as systems. In CIKM, 2015.
[7] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In SIGIR, pages 375­382. ACM, 2007.
[8] J. Palotti, G. Zuccon, L. Goeuriot, L. Kelly, A. Hanbury, G. J. Jones, M. Lupu, and P. Pecina. Clef ehealth evaluation lab 2015, task 2: Retrieving information about medical symptoms. CLEF, 2015.
[9] L. T. Penberthy, B. A. Dahman, V. I. Petkov, and J. P. DeShazo. Effort required in eligibility screening for clinical trials. Journal of Oncology Practice, 8(6):365­370, 2012.
[10] T. R. Pressler, P.-Y. Yen, J. Ding, J. Liu, P. J. Embi, and P. R. O. Payne. Computational challenges and human factors influencing the design and use of clinical research participant eligibility pre-screening tools. BMC Medical Informatics & Decision Making, 12:47­47, 2012.
[11] M. S. Simpson, E. M. Voorhees, and W. Hersh. Overview of the TREC clinical decision support track. In TREC, 2014.
[12] E. M. Voorhees and W. R. Hersh. Overview of the TREC 2012 Medical Records Track. In TREC, 2012.

672

Evaluating Retrieval over Sessions: The TREC Session Track 2011­2014

Ben Carterette
University of Delaware, Newark, DE, USA
carteret@cis.udel.edu

Paul Clough

Mark Hall

University of Sheffield,

Edge Hill University, Ormskirk,

Sheffield, UK

UK

p.d.clough@sheffield.ac.uk hallmark@edgehill.ac.uk

Evangelos Kanoulas
University of Amsterdam, Amsterdam, The Netherlands
e.kanoulas@uva.nl

Mark Sanderson
RMIT University, Melbourne, Australia
mark.sanderson@rmit.edu.au

ABSTRACT
Information Retrieval (IR) research has traditionally focused on serving the best results for a single query-- so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. This paper describes the TREC Session Track, which ran from 2010 through to 2014, which focussed on forming test collections that included various forms of implicit feedback. We describe the test collections; a brief analysis of the differences between datasets over the years; and the evaluation results that demonstrate that the use of user session data significantly improved effectiveness.
1. INTRODUCTION
One of the commonest IR system evaluation methodologies is the Cranfield approach [4] using test collections to conduct controlled, systematic, and repeatable evaluations [7]. The focus of such evaluation is on how well an IR system can locate and rank relevant documents from a single query. In practice, however, users typically reformulate queries in response to search results or as their information need alters over time [9]. Retrieval evaluation should compute system success over multiple query-response interactions [10].
The TREC Session Track1 was an attempt to evaluate IR systems over multi-query sessions. In 2010, the track produced test collections and evaluation measures for studying retrieval over sessions [11]; from 2011 on [12, 13, 1, 2], the track focused more on providing participants with user data with which to improve retrieval. The resulting collections consist of document collections, topics, and relevance assessments, as well as log data from user sessions.
1http://ir.cis.udel.edu/sessions/
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914675

The track's test collections are described here and compared: including studying the effects of the search engines used to build the collections, user variability, and topic analysis. Participant results indicate that certain types of search benefit significantly from exploiting session information.
2. SESSION TRACK OVERVIEW
The aim of the track was to test if the retrieval effectiveness of a query could be improved by using previous queries, ranked results, and user interactions. We constructed four test collections comprising N sessions of varying length, each the result of a user attempting to satisfy one of T pre-defined topics. Each session numbered 1..i..N consisted of:
· mi blocks of user interactions (the session's length); · the current query qmi in the session; · mi - 1 blocks of interactions in the session prior to the
current query, composed of: 1. the user queries in the session, q1, q2, ..., qmi-1; 2. the ranked list of URLs seen by the user for each of those queries; 3. the set of clicked URLs/snippets.
Ranking algorithms were evaluated on the current query under two conditions: A one-off ad hoc query; or a query using some or all of the prior logged data. The latter condition had several different sub-conditions that varied year to year: ("RL" refers to Ranked List):
· RL1: The baseline condition: an ad hoc query · RL2-1: RL1 plus previous session queries · RL2-2: RL2-1 plus rankings (URLs, titles, snippets) · RL2-3: RL2-2 plus user data (clicks, dwell times) · RL3: Using all data in the session log (in particular,
other sessions on the same topic)
The focus of the track was on the degree to which a group improved their retrieval system's baseline effectiveness (RL1) by incorporating some or all of the additional log data.
3. TEST COLLECTIONS
Table 1 shows statistics of the Session track collections. The ClueWeb09 collection was used in 2011 and 2012, and the ClueWeb12 collection in 2013 and 2014.
Topics: While not a part of a true log of user search activity, we felt it was important to define topic descriptions for overall sessions so as to make relevance assessing

685

Table 1: Four years of TREC Session Track test collections and evaluations

2011

2012

2013

2014

collection topic properties
topic set size topic cat. dist.

ClueWeb09
62 known-item

session properties user population
search engine total sessions sessions per topic mean length (in queries) median time between queries relevance judgments topics judged total judgments evaluation by nDCG@10
mean RL1 mean RL2-1 mean RL2-2 mean RL2-3
mean RL3 max RL* - RL1

U. Sheffield
BOSS+CW09 filter 76 1.2 3.7
68.5s
62 19,413
0.3015 0.3083 0.2941 0.3077
­§ 0.1800

ClueWeb09
48 10 exploratory, 6 interpretive, 20 known-item, 12 known-subj
U. Sheffield
BOSS+CW09 filter 98 2.0 3.0
66.7s
48 17,861
0.1847 0.1950 0.2140 0.2303
­§ 0.1770

ClueWeb12
61 10 exploratory, 9 interpretive, 32 known-item, 10 known-subj
U. Sheffield + IR researchers indri 133 2.2 3.7 72.2s
49 13,132
0.1373 ­§ ­§
0.1832 0.1834 0.1230

ClueWeb12
60 15 exploratory, 15 interpretive, 15 known-item, 15 known-subj
MTurk
indri 1,257 21.0
3.7 25.6s
51 16,949
0.1719 ­§ ­§
0.1885 0.2002 0.1507

 2011 topics were not categorized, but a retrospective analysis suggests most of them fit the "known-item" label best.  2014 topics were reused 2012 and 2013 topics. § The RL2-1 and RL2-2 conditions were eliminated for 2013 and 2014; the RL3 condition was introduced in 2013.

simpler. The challenge was to construct topics that were likely to require multiple query reformulations. In 2011, we did this by adapting multi-faceted TREC 2007 Question Answering track topics. Because of the nature of the QA track, many topics modelled "fact-finding" tasks answerable by a single document. In 2012-2013, we developed topics according to a task categorization scheme [15] with four classes: known-item; known-subject; interpretive; and exploratory. In 2014, we reused topics from 2012-2013 selecting fifteen topics from the four categories, biasing selection to topics that had longer user sessions and more clicks.
Sessions: Assessing the impact of session data on retrieval effectiveness required capturing user-system interactions, including queries, rankings, and clicks. We describe the users and search engines employed to generate the data.
Users: In 2011-2013, the primary user group were staff and students at the University of Sheffield. Using a universitywide email, we invited participants to search on as many topics as they had time for. In 2013 we solicited additional participants from the Session Track and SIG-IRList mailing lists. In 2014 we used a crowdsourcing platform (Mechanical Turk) taking a similar approach to past work for crowdsourcing interactions [18].
Search process: Users were shown a topic description, a search box for entering queries, and a list of ten ranked results with a pagination control to navigate to further results. Each retrieved item was represented by its title, URL, and snippet. Additionally, there was a "Save" button that users were instructed to use to collect those documents that helped them satisfy their information need. We experimented with additional components, such as a list of queries issued, but did not observe a difference in users' behaviour.
Search engine: In 2011-2012 we used Yahoo!'s BOSS (Build your Own Search System) API to search the live web. We fil-

tered URLs returned by BOSS against those in the ClueWeb09 collection so that users would only see pages that were present in the publicly-available corpus. A large number of pages returned by BOSS did not match any URL in ClueWeb09. In 2013-2014, we switched to indri search with a homebuilt index of ClueWeb12. The indri index included each of the twenty ClueWeb12 segments (ClueWeb12 00 through ClueWeb12 19) indexed using the Krovetz stemmer and no stopword list. The indexes searched contained only text from title fields, anchor text from incoming links ("inlink" text), and page URLs. Each query was incorporated into an indri structured query language template and a retrieval score was computed from a query-likelihood model for the full document representation and three weighted combinations of query-likelihood field models with unordered-window within-field models. The "inlink" model was weighted 50 times higher than the title model, and 100 times higher than the URL model. This query template is the product of manual search and investigation of retrieved results.
The system logged all interactions with the user, including the queries issued, which documents were ranked (including URL, title, and snippet), which documents the user viewed, and which they saved as relevant to the task (note however that the latter are not the relevance judgments). This log data was then used to create the sessions.
4. EVALUATION
We used topical relevance judgments in order to compute measures of effectiveness like nDCG@10 for each topic. Since the Session Track examines whether session log data can be exploited, the evaluation examined the change in effectiveness from the baseline (RL1) to using some data (RL2) to using a full query log (RL3). In addition, since each topic may be the subject of more than one session, and

686

X20X13XXXnav2X0-1XX44

4 1

3 0

2 0

1 0

0 0

-2 0

key - 3 0 1 2 7 4 0

hi - 2 0 4 28 52 14 2

rel - 1 1 12 75 89 64 0

not - 0 4 5 50 161 337 11

junk -2 0 0 0 0 4 5

Table 2: Agreement on relevance grades

RL1 nDCG@10 0.1 0.2 0.3 0.4

1 2011 6 2012 1 2013 6 2014

0

20

40

60

80

100

run number

Figure 1: Mean nDCG@10 (with error bars showing ±2 standard error) for all 108 submitted runs' RL1 baseline.

each session may use different queries, the evaluation was over sessions rather than over topics.
Documents were selected for judging by pooling the top-10 results from all the submitted RLs along with all documents that were retrieved and viewed by the users. TREC NIST assessors (not the original users) judged each pooled document with respect to the topic description. All original user actions were invisible to the assessors; judgments were made solely on the topical similarity to the topic description on a 6-grade scale. Over four years, 66,548 relevance judgments were made to 60,500 unique pages identified by URL: 33,686 pages from ClueWeb09 ; 26,814 from ClueWeb12. A total of 19,179 (29%) documents were judged relevant (grade 1 or higher) and 47,369 (71%) judged nonrelevant.
Since the topics for 2014 were taken from the 2012 & 2013 Session Tracks and in the last two years the document collection was ClueWeb12, we have documents with multiple assessments. Table 2 shows assessor agreement. Assessors were much more likely to say a document judged non-relevant in 2013 was relevant in 2014 than vice versa.
Results: Figure 1 shows nDCG@10 for all groups' baseline RL1 submissions, sorted by nDCG@10 and coded by year. It is evident that 2011 had the best baseline effectiveness (average nDCG@10 of 0.30), followed by 2012 (0.18), then 2014 (0.17), and finally 2013 (0.14) had the lowest baseline effectiveness. The change from 2011 to 2012 reflects a shift to more difficult topics: the 2012 known-subject and interpretive topic categories proved to be significantly more difficult than the 2011 known-item topics. The change from 2012 to 2013 reflects a change in the underlying search technology from Yahoo! BOSS to the Indri-based system.
Figure 2 shows the improvement over each submitted run's RL1 baseline sorted by that improvement. Improvement from the RL1 baseline does not show any trend by year-- for 2011, the average improvement was 0.04, for 2012 it was 0.05, for 2013 it was 0.05, and for 2014 it dropped to 0.02.

max change in nDCG@10 from RL1 baseline

0.0

0.1

0.2

-0.1

2011 2012 2013 2014

0

20

40

60

80

100

run number

Figure 2: Largest measured improvement in nDCG@10 from RL1 to any other condition for all 108 submitted runs, with error bars showing ±2 standard errors.

From these results, we conclude that it is possible to use session history to improve effectiveness over basic ad hoc retrieval, and moreover that it does not take a lot of session history to do so. Further evidence is offered in [6, 5, 17]. A study of particular interest due to the fact that it was conducted both over a Session track collection and a commercial search engine proprietary collection is that by Raman et al. [16]; the session collection enabled them to demonstrate the effectiveness of their algorithm in accordance to the proprietary test collection.

5. ANALYSIS
In this section we perform some basic analysis of the Session Track collections and evaluation results.
Topic categories: We investigated the degree to which systems were able to improve effectiveness for each of our four topic classes. We look at the average improvement from the RL1 baseline, and find the maximum average improvement to any other RL condition for each run.
The overall mean improvements are 0.04, 0.07, 0.04, and 0.05 for known-item, known-subject, exploratory, and interpretive respectively, though only the differences between known-subject and the others were statistically significant.This suggests that known-subject topics benefit most from access to session history, but the details are more subtle. Exploratory topics tend to have the largest improvements for individual systems: the five largest improvements in exploratory topics are 5­10% larger than the five largest in known-subject topics. Exploratory topics also show the greatest benefit from the use of more log data: from RL1 to RL2, exploratory topics only increase an average of 0.03 (compare to 0.05 for known-subject topics, the largest improvement), but from RL2 to RL3 they increase by 0.05 (compare to 0.04 for known-subject topics, the second-largest improvement).
Topic variability: Most IR test collections have only one instantiation of a topic (an exception is the TREC Query track). Since we may have multiple sessions for any given topic, the Session Track gives us a chance to analyze variability in effectiveness within topics.
Figure 3 shows how much effectiveness varies over the different user sessions of a single topic. Each plot on the x-axis is a topic, the y-axis is a boxplot of the range in nDCG@10 changes from RL1 to any other RL. A taller box means more variability. A point or box plotted further up the y-axis indicates higher average change in nDCG@10 across the sessions

687

difference in nDCG@10 over sessions

0.0

0.5

1.0

1.5

20122-011022-041742-041032-01142-22081220-1432-02144-24061220-1642-051242-031942-021642-01134-24071220-1542-04143-21021210-1722-031212-031042-051632-021112-021022-031432-041942-011522-011142-021442-031542-011022-021342-031012-051232-02182-24
topic (ordered by median)
Figure 3: Variability over sessions and system effectiveness for selected topics.
of a topic. An extreme case is topic 24 from 2012 (the right most topic on the plot). There were five sessions recorded for this topic (numbers 48­52); one group improved from 0.00 in RL1 to 0.91 in RL3 on one session, but fell from 0.81 in RL1 to 0.00 in RL2 on another. Many other groups had similarly large differences across sessions on this topic.
The result indiactes that there is a substantial variability in topics, separate from the variability in system effectiveness, due to the way the users performs their search and formulates their query. Previous user study showed this as well [14]. It may be beneficial to include multiple versions of the same topic in standard test collections, so as to better capture interactions between topic and system variability.
6. CONCLUSION
This paper describes the four test collections produced for the TREC Session Track that have been used to assess the use of implicit feedback on retrieval performance within sessions. The key result from the track is that aggregate data from all participant submissions shows that retrieval effectiveness was improved for ad hoc retrieval using data based on session history data. It also appears that the more detailed the session data, the greater the improvement.
Through analyzing aspects of the test collections, such as topic categories and variability, we demonstrate how the resources can be used to investigate implicit feedback and offer reusable and publicly-accessible resources for evaluating IR systems across sessions.
7. ACKNOWLEDGEMENTS
This work was supported in part by the Australian Research Council's Discovery Projects scheme (DP130104007), the National Science Foundation (NSF) under grant number IIS-1350799, and the Google Faculty Research Award scheme. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.
8. REFERENCES
[1] B. Carterette, E. Kanoulas, A. Bah, M. Hall, and P. D. Clough. Overview of the TREC 2013 Session track. In Proceedings of TREC, 2013.
[2] B. Carterette, E. Kanoulas, A. Bah, M. Hall, and P. D. Clough. Overview of the TREC 2014 Session track (notebook version). In Proceedings of TREC, 2014.

[3] B. Carterette, E. Kanoulas, P. D. Clough, and M. Sanderson, editors. Proceedings of the ECIR 2011 Workshop on Information Retrieval Over Query Sessions, Available at http://ir.cis.udel.edu/ECIR11Sessions.
[4] C. W. Cleverdon. The significance of the cranfield tests on index languages. In Proceedings of SIGIR, pages 3­12, 1991.
[5] D. Guan and H. Yang. Is the first query the most important: An evaluation of query aggregation schemes in session search. In Proceedings of AIRS, pages 86­99. 2014.
[6] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In Proceedings of SIGIR, pages 453­462, 2013.
[7] D. Harman. Information Retrieval Evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2011.
[8] P. Ingwersen and K. J¨arvelin. The Turn: Integration of Information Seeking and Retrieval in Context (The Information Retrieval Series). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.
[9] B. J. Jansen, D. L. Booth, and A. Spink. Patterns of query reformulation during Web searching. J. Am. Soc. Inf. Sci. Technol., 60(7):1358­1371, July 2009.
[10] K. J¨arvelin. Explaining user performance in information retrieval: Challenges to ir evaluation. In Proceedings of ICTIR, pages 289­296, 2009.
[11] E. Kanoulas, B. Carterette, P. Clough, and M. Sanderson. Session track overview. In Proceedings of the 19th Text REtreival Conference (TREC), 2010.
[12] E. Kanoulas, B. Carterette, M. Hall, P. D. Clough, and M. Sanderson. Overview of the TREC 2011 Session track. In Proceedings of TREC, 2011.
[13] E. Kanoulas, B. Carterette, M. Hall, P. D. Clough, and M. Sanderson. Overview of the TREC 2012 Session track. In Proceedings of TREC, 2012.
[14] K. S. Kim. Information-seeking on the web: Effects of user and task variables. Library and Information Science Research, 23(3), 2011.
[15] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, Nov. 2008.
[16] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: Exploring intrinsic diversity in web search. In Proceedings of SIGIR, pages 463­472, 2013.
[17] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In Proceedings of SIGIR, pages 821­824, 2013.
[18] G. Zuccon, T. Leelanupab, S. Whiting, E. Yilmaz, J. Jose, and L. Azzopardi. Crowdsourcing interactions: Capturing query sessions through crowdsourcing. In Proceedings of ECIR, 2011.

688

New Collection Announcement: Focused Retrieval Over the Web

Ivan Habernal
habernal@ukp.informatik.tudarmstadt.de

Maria Sukhareva
sukhareva@ukp.informatik.tudarmsdtadt.de

Fiana Raiber
fiana@tx.technion.ac.il

Anna Shtok
annabel@tx.technion.ac.il

Oren Kurland
kurland@ie.technion.ac.il

Hadar Ronen
hadarg@gmail.com

Judit Bar-Ilan
Judit.Bar-Ilan@biu.ac.il

Iryna Gurevych
gurevych@ukp.informatik.tudarmsdtadt.de

UKP Lab, Technische Universität Darmstadt, Germany

Faculty of Industrial Engineering and Management, Technion, Israel

Department of Information Science, Bar-Ilan University, Israel

ABSTRACT
Focused retrieval (a.k.a., passage retrieval) is important at its own right and as an intermediate step in question answering systems. We present a new Web-based collection for focused retrieval. The document corpus is the Category A of the ClueWeb12 collection. Forty-nine queries from the educational domain were created. The 100 documents most highly ranked for each query by a highly effective learningto-rank method were judged for relevance using crowdsourcing. All sentences in the relevant documents were judged for relevance.
Categories and Subject Descriptors: H.3.0 [Information Storage and Retrieval] General
Keywords: focused retrieval
1. INTRODUCTION
Many retrieval applications and tasks rely on passage retrieval; that is, retrieving document parts (passages), rather than whole documents, in response to expressions of information needs. Question answering systems, for example, often apply passage retrieval in response to the question at hand [7, 1]. Then, an answer is compiled from the top retrieved passages. In focused retrieval systems, the result list retrieved for a query is composed of sentences [10, 25, 24] or more generally passages [2, 3, 4, 6, 9].
To advance the development of passage retrieval methods -- e.g., in light of the recent resurgence of interest in Web question answering [1] -- collections for evaluating passage
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914682

retrieval effectiveness are called for. In this paper we describe a new such Web-based collection.
The document corpus is category A of the English ClueWeb12 collection which contains about 733 million documents. Forty-nine short keyword queries, accompanied by descriptions of the information need, were created based on questions posted on community question answering sites and questionnaires. The queries are from the education domain and are of topical nature. They represent various information needs of parents (henceforth, the target group). Furthermore, educational topics are of interest to a wide range of additional users, such as education experts, journalists, policy makers, and students.
For each query, a document list was retrieved using a highly effective learning-to-rank method. All sentences in documents in the list were judged for relevance using crowdsourcing. The final collection as well as the data processing pipeline are publicly available.1
2. RELATED COLLECTIONS
The Novelty tracks of TREC [10, 25, 24] used relevance judgments for sentences and the HARD (High Accuracy Retrieval from Documents) tracks [2, 3, 4] used relevance judgments for passages. These tracks rely on old (mainly) newswire TREC document corpora and are rarely used nowadays. In contrast, our dataset is based on the newest Web collection of TREC (ClueWeb12).
The task in the TREC Question Answering (QA) track was to provide a short, concise answer to a factoid question [26]. In contrast, our queries are opinion-seeking and cannot necessarily be answered by a short string. The annotators in the TREC QA track evaluated the short answer string, while our annotators were asked to determine the relevance to a query of each sentence in the top retrieved documents.
1The dataset can be downloaded at: http://ie.technion. ac.il/~kurland/dip2016corpus/; the code used to process the data can be found at: https://github.com/UKPLab/ sigir2016-collection-for-focused-retrieval.

701

There is a Wikipedia-based INEX (Initiative for the Evaluation of XML Retrieval) collection with relevance judgments provided for parts of documents with respect to queries [6, 9]. The relevance judgment regime that was found to be the most robust with respect to inter-annotator agreement rates was highlighting all and only relevant text in each document [21]. We adopt a similar (binary) relevance judgment regime, but use crowdsourcing to produce judgments rather than trained annotators as was the case in INEX. Furthermore, we use a noisy Web collection (ClueWeb12) rather than the well edited Wikipedia collection.
A recently introduced collection provides relevance judgments for "answer passages" [13]: passages in the documents most highly ranked in response to a query are judged with respect to TREC topics (specifically, their descriptions) that are of a question form/type; the document collection is TREC's GOV2. In contrast, the queries we have developed are not necessarily of question form (e.g., many of the queries are of topical nature), and we use the much larger and noisier ClueWeb12 collection.
3. COLLECTION DESCRIPTION
3.1 Queries
To ensure high variability of queries, as well as their pertinence to the target group (parents), we combined two approaches for query compilation. The first approach relied on exploiting existing Question-Answering Web portals and the second approach utilized a user questionnaire.
To sample users' information needs from QA sites, we used Yahoo L6 Question Answering Collection2 [23] and selected about 150k questions from the Education section which contains nine categories. Questions from each category were projected onto a latent 300-dimensional semantic space using word2vec [18] and independently clustered into k/10 clusters (where k is a number of question in the category) using the CLUTO software package3 with the Repeated Bisection clustering method [27]. We randomly sampled 10 questions from each of the 5 largest clusters, and conducted a survey with eight participants whose task was to formulate a query/queries that would best represent the information needs reflected by that cluster. This process yielded roughly 200 queries in total, from which 39 queries were randomly selected. The selected queries were enriched with a specification of what relevant information is to be expected from the system; these specifications are similar to the narratives used in TREC.
Although the Yahoo QA set is very rich, it usually covers US-specific topics. We thus created an additional Google Forms based online questionnaire of 20 parental issues and distributed it among 51 non-US parents of children aged 121. The instructions were to rank each issue according to the level of interest that causes participants to search for online information, on a scale from 1 (not interested at all) to 5 (very interested). For the 10 top ranked queries, a description was added.
The final query set (examples of query titles are presented in Table 3) consists of 49 queries combined from the two
2Provided by Yahoo! Webscope, http://research.yahoo. com/Academic Relations 3http://www.cs.umn.edu/~karypis/cluto

Query ID: 1017, Query Title: student loans

Relevant info

Irrelevant info

· Eligibility criteria for · Completely unrelated topics.

student loans.

· Information on loans which are

· Categories of student not for educational purposes.

loans.

· Information about financial aid

· Where to apply for options which are not loans.

student loans.

· Information on the effect of stu-

· Conditions for stu- dent loans on higher education

dent loans.

prices and global economy.

Table 1: Example of a query.

aforementioned approaches.4 Following common practice, a query is composed of a short title and information need description. To make the annotation task easier for non-expert judges, we presented the information need description in a table. The table provides context that cannot be inferred from the title alone, and examples of relevant and irrelevant information that the annotator may encounter (see an example in Table 1).
3.2 Document collection
The full ClueWeb12 dataset (category A), which contains about 733 million documents, was used for creating the collection. For the retrieval stage, the documents and the queries were stemmed using the Krovetz stemmer via the Indri toolkit5 which was also used for retrieval. The 100 documents most highly ranked with respect to a query were annotated. (Details of the retrieval method are provided below.) Some of the (Web) documents are noisy and hard to read. Thus, we pre-processed all documents before the annotation. Specifically, we applied a state-of-the-art boilerplate removal tool [22] and a sentence splitter [16]. As a result of applying the boilerplate, a few documents became empty and were removed from the lists to be annotated.
3.3 Data annotation
3.3.1 Initial document retrieval
We first ranked all the documents in the dataset with respect to a query using the negative cross entropy between the unsmoothed unigram language model induced from the query and the Dirichlet-smoothed unigram language model induced from each of the documents [15]. Following common practice [8], documents assigned with a score below 70 by Waterloo's spam classifier were filtered out from this initial ranking top down until 1000 presumably non-spam documents were accumulated. The remaining documents were then re-ranked to produce the final document ranking.
To re-rank the documents, a learning-to-rank approach was applied with 130 features. Most of these features were used in Microsoft's learning-to-rank datasets6 with the following exceptions. Instead of the two quality features (QualityScore and QualityScore2), which are not available for the ClueWeb12 dataset, we used query-independent document quality measures that were shown to be highly effective
4We removed one query from the 50 selected queries as it asked for a site containing only links, which has a very different nature from the topical queries in the rest of the corpus. 5www.lemurproject.org/indri 6www.research.microsoft.com/en-us/projects/mslr

702

Documents Sentences Workers Sentences/HIT HITs Average duration (sec)

1...40 2199 67

41...80 1800 87

4,820 628,026
2,041 81...120 4,130 150

Table 2: Annotation details.

for spam classification [20] and Web retrieval [5]. Specifically, we used as features the ratio between the number of stopwords and non-stopwords in a document, the percentage of stopwords in a stopwords list that appear in the document and the entropy of the term distribution in a document. As is the case with the features used in Microsoft's datasets, these quality measures were computed for the entire document, its body, title, URL and anchor text. We used the score assigned to a document by Waterloo's spam classifier [8] as an additional quality measure. Hence, Waterloo's spam classifier served both for filtering out documents from the initial document ranking and as a feature in the learning-to-rank model. Additional features used in Microsoft's datasets that were not considered here are the Boolean Model, Vector Space Model, LMIR.ABS, Outlink number, SiteRank, Query-URL click count, URL click count, and URL dwell time.
To integrate the features we used SVMrank [12] applied with a linear kernel and default free-parameter values. The titles of topics 201-250 from TREC 2013 were used as queries to train the model. The Dirichlet smoothing parameter in LMIR.DIR, which served both for creating the initial ranking and as a feature in the learning-to-rank model, was set to µ = 1000. We used LMIR.JM with  = 0.1; for BM25, we set k1 = 1 and b = 0.5. The INQUERY list was used for computing the two stopword-based quality measures.
3.3.2 Crowdsourcing document annotations
Data preparation.
Although the majority of documents (74%) are no longer than 120 sentences, the document length distribution is heavytailed: 73% of the sentences are in documents containing over 120 sentences (e.g., the longest document contains over 4500 sentences). It has been frequently pointed out in previous work on crowdsourcing that unlike traditional annotation approaches, a so-called "fun factor" can strongly affect the annotation quality and workers' response. In some cases it played even a more important role than the size of the reward [19, 14]. Thus, during a pilot study, three experts were instructed to provide their feedback on how many sentences they can annotate without significant loss of concentration. Based on their observations, the length of a single Human Intelligence Task (HIT) was limited to 120 sentences. If a document contained more than 120 sentences, it was split on paragraph borders so that each split segment would have no more than 120 sentences. If a document or a split segment contained less than 80 or less than 40 sentences, it was grouped in medium and short HIT groups, respectively. Table 2 sums up the annotation setup details.
Annotation setup.

We performed crowdsourcing using the Amazon Mechanical Turk platform. The task was designed as follows: the workers were invited to read a document from top to bottom. Each document was split into sentences and no paragraph marking was preserved. The workers were to judge each sentence individually as relevant or not to a given query. The instructions asked workers to base their decision on two lists of relevant and irrelevant examples (Table 1). If a worker decided that a sentence or several sentences were relevant then they should highlight it by either clicking on it or dragging the mouse over the relevant sentences.7 We also provided a link to guidelines8 with an extended definition of what should be considered relevant. According to the guidelines, a sentence can be either:
· Clearly relevant, if it is informative and relevant regardless of its context.
· Relevant in context, if it is only relevant to a query in the context of a clearly informative sentence that either precedes or succeeds it.
· Clearly irrelevant, if it does not fall into any of the two aforementioned categories.
Workers were asked to highlight only sentences that are clearly relevant and relevant in context.
Quality control.
In order to be allowed to work on a HIT, workers were required to have two Amazon Mechanical Turk Qualifications: 1. They must be based in the US. 2. They must have acceptance rate higher than 95%. Rather than generating the gold data through a majority vote over five workers' decisions, we integrated MACE, an unsupervised tool for quality prediction of non-expert annotations [11], in our publicly available pipeline and extended the guideline by a warning about automatic cheater detection. Later on, 64 workers were blocked from working on the HITs based on the competence scores assigned by MACE.
Results.
Table 2 provides a summary of the resulting dataset. The dataset includes 4820 annotated documents and over 600k annotated sentences (5 assignments per sentence) for 49 queries. The total cost of the annotation was 3880 US Dollars. The workers' response varied depending on the length of the documents. The best response was observed for short HITs with an average annotation speed of 288 HITs a day, while long HITs were annotated with an average speed of 174 HITs a day. Although we did not explicitly instruct workers to submit any qualitative user feedback, the annotation interface had a comment field which was mostly designed to provide a convenient way for workers to report technical problems. Interestingly, we received over 2000 commented HITs (10% of the total) with multiple positive feedback about the content of the annotated documents. Many workers pointed out that they found articles useful and educational and, hence, enjoyed working on the task despite a modest reward. This demonstrates, again, the importance of the entertainment component for the success of a crowdsourcing annotation task, as it is very likely that the high agreement between annotators (see Section 3.4) is a direct
7http://tinyurl.com/jdxuyyl 8http://tinyurl.com/zvwjm2p

703

consequence of the fact that many workers were attentively reading documents because of their personal interest in the documents' content.
3.4 Collection statistics
For each annotated document, we computed observed annotation agreement using the DKPro Agreement package [17].9 The minimal units for agreement computation were sentences, each with two categories per annotator (relevant or non-relevant). Average agreement over all documents judged is 0.725 and standard deviation is 0.175 (where agreement ranges between 0 and 1). Interestingly, we found that the average (over sentences) agreement on non-relevant documents is statistically significantly higher than that on relevant documents: 0.880 vs 0.706 respectively10. Presumably, when the document is relevant the annotators may comprehend it differently as the task of marking relevant sentences is challenging; in contrast, it is easier to agree on irrelevant documents, especially if these are off topic.
Overall, the documents retrieved for 49 queries were annotated. Per query, about 98 documents on average were annotated on a sentence level. On average, about 87 documents and about 4618 sentences per query were judged relevant. Overall, about 89% of the annotated documents are relevant and about 36% of the sentences are relevant. The Normalized Discounted Cumulative Gain (NDCG) at top 30 documents is 0.924 and the precision at top 5 ranks is 0.943; these performance numbers attest to the high effectiveness of the retrieval.

Query ID 1001 1002 1003 1004 1005 1006 1007 1008 1010

Query Title Alternative ADHD treatments Cellphone for 12 years old kids Dealing with kids that skip classes Child depression Discipline for 8 year old daughter Discipline issues in elementary school Getting rid of childhood phobia Handling conflicts between children Homeschooling legal issues

Table 3: Examples of the queries used.

4. SUMMARY
We presented a novel Web-based collection for query-based focused retrieval (a.k.a. passage retrieval) with sentencelevel relevance judgments. The document corpus is the Category A of the ClueWeb12 collection; forty-nine queries from the educational domain are used.
Acknowledgments. We thank the reviewers for their comments. This paper is based upon work supported in part by the German Research Foundation (DFG) via the GermanIsraeli Project Cooperation (DIP, grant DA 1600/1-1), the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No I/82806, and the TechnionMicrosoft Electronic Commerce Research Center.
9The dynamic nature of assigning HITs to workers in crowdsourcing as well as splitting long documents into several HITs do not allow us to compute traditional inter-annotator statistics like Cohen's , Krippendorff's  or Fleiss' , as these measures expect a fixed set of the same annotators over the entire data. 10We used two tailed permutation test at 95% confidence level to test the difference.

5. REFERENCES
[1] E. Agichtein, D. Carmel, C. L. A. Clarke, P. Paritosh, D. Pelleg, and I. Szpektor. Web question answering: Beyond factoids: SIGIR 2015 workshop. In Proc. of SIGIR, page 1143, 2015.
[2] J. Allan. HARD track overview in TREC 2003: High accuracy retrieval from documents. In Proc. of TREC, pages 24­37, 2003.
[3] J. Allan. HARD track overview in TREC 2004 - high accuracy retrieval from documents. In Proc. of TREC, 2004.
[4] J. Allan. HARD track overview in TREC 2005 high accuracy retrieval from documents. In Proc. of TREC, 2005.
[5] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.
[6] T. Chappell and S. Geva. Overview of the INEX 2010 focused relevance feedback track. In Proc. of INEX, pages 303­312, 2010.
[7] K. Collins-Thompson, J. Callan, E. Terra, and C. L. Clarke. The effect of document retrieval quality on factoid question answering performance. In Proc. of SIGIR, pages 574­575, 2004.
[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Journal of Information Retrieval, 14(5):441­465, 2011.
[9] S. Geva, J. Kamps, and R. Schenkel, editors. Focused Retrieval of Content and Structure, INEX 2011, volume 7424 of Lecture Notes in Computer Science. Springer, 2012.
[10] D. Harman. Overview of the TREC 2002 novelty track. In Proc. of TREC, 2002.
[11] D. Hovy, T. Berg-Kirkpatrick, A. Vaswani, and E. Hovy. Learning whom to trust with mace. In Proc. NAACL-HLT, pages 1120­1130, 2013.
[12] T. Joachims. Training linear svms in linear time. In Proc. of KDD, pages 217­226, 2006.
[13] M. Keikha, J. H. Park, and W. B. Croft. Evaluating answer passages using summarization measures. In Proceedings of SIGIR, pages 963­966, 2014.
[14] A. Kumaran, M. Densmore, and S. Kumar. Online gaming for crowd-sourcing phrase-equivalents. In Proc. of COLING, pages 1238­1247, 2014.
[15] J. D. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proc. of SIGIR, pages 111­119, 2001.
[16] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. The Stanford CoreNLP natural language processing toolkit. In Proc. of ACL, pages 55­60, 2014.
[17] C. M. Meyer, M. Mieskes, C. Stab, and I. Gurevych. DKPro Agreement: An Open-Source Java Library for Measuring Inter-Rater Agreement. In Proc. of COLING: System Demonstrations, pages 105­109, Dublin, Ireland, 2014.
[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS 26, pages 3111­3119. 2013.
[19] Z. Neverilova´. Annotation game for textual entailment evaluation. In Proc. of CICLing 2014, pages 340­350. Springer Berlin Heidelberg, 2014.
[20] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. Detecting spam web pages through content analysis. In Proc. of WWW, pages 83­92, 2006.
[21] B. Piwowarski, A. Trotman, and M. Lalmas. Sound and complete relevance assessment for xml retrieval. ACM Trans. Inf. Syst., 27(1):1:1­1:37, 2008.
[22] J. Pomika´lek. Removing boilerplate and duplicate content from web corpora. PhD thesis, Masaryk university, Faculty of informatics, Brno, Czech Republic, 2011.
[23] C. Shah and J. Pomerantz. Evaluating and predicting answer quality in community QA. In Proc. of SIGIR, pages 411­418, 2010.
[24] I. Soboroff. Overview of the TREC 2004 novelty track. In Proc. of TREC, 2004.
[25] I. Soboroff and D. Harman. Overview of the TREC 2003 novelty track. In Proc. of TREC, pages 38­53, 2003.
[26] E. Voorhees and D. Tice. Building a Question Answering Test Collection. In Proc. of SIGIR, 2000.
[27] Y. Zhao and G. Karypis. Criterion functions for document clustering: Experiments and analysis. Technical report, Department of Computer Science, University of Minnesota, Minneapolis, 2002.

704

UQV100: A Test Collection with Query Variability

Peter Bailey
Microsoft, Australia pbailey@microsoft.com
Falk Scholer
RMIT University, Australia
falk.scholer@rmit.edu.au
ABSTRACT
We describe the UQV100 test collection, designed to incorporate variability from users. Information need "backstories" were written for 100 topics (or sub-topics) from the TREC 2013 and 2014 Web Tracks. Crowd workers were asked to read the backstories, and provide the queries they would use; plus effort estimates of how many useful documents they would have to read to satisfy the need. A total of 10,835 queries were collected from 263 workers. After normalization and spell-correction, 5,764 unique variations remained; these were then used to construct a document pool via Indri-BM25 over the ClueWeb12-B corpus. Qualified crowd workers made relevance judgments relative to the backstories, using a relevance scale similar to the original TREC approach; first to a pool depth of ten per query, then deeper on a set of targeted documents.
The backstories, query variations, normalized and spell-corrected queries, effort estimates, run outputs, and relevance judgments are made available collectively as the UQV100 test collection. We also make available the judging guidelines and the gold hits we used for crowd-worker qualification and spam detection.
We believe this test collection will unlock new opportunities for novel investigations and analysis, including for problems such as task-intent retrieval performance and consistency (independent of query variation), query clustering, query difficulty prediction, and relevance feedback, among others.
1. INTRODUCTION
Test collection-based evaluation is the most widely used methodology for measuring the effectiveness of information retrieval systems. A typical test collection consists of a set of queries, a collection of documents to search over, and a set of relevance judgments that indicate, for query-document pairs, whether the document was a topically related resource for that query. To evaluate a search system, a ranked answer list is generated for each query, and the relevance of each item is determined with reference to the available judgments [3]. Finally, the relevance information is condensed into
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '16, July 17­21, 2016, Pisa, Italy Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-4069-4/16/07 . . . $15.00. http://dx.doi.org/10.1145/2911451.2914671.

Alistair Moffat
The University of Melbourne, Australia
ammoffat@unimelb.edu.au
Paul Thomas
Microsoft, Australia pathom@microsoft.com
a single number, based on a chosen effectiveness metric, which may take into account features such as the number of relevant answers that were retrieved, at what positions in the ranked list the relevant answers were located, and so on.
Collection-based evaluation has several advantages: it supports reproducible experimentation; and, while constructing a test collection is typically resource-intensive in terms of time and labor ­ particularly the creation of relevance judgments ­ it is then inexpensive to run any number of further experiments using the same framework. However, there are also limitations in terms of the realism of the evaluation. For example, the user is almost entirely removed from the evaluation, and is represented via a single search query that instantiates the underlying information need.
The TREC Query Track studied the impact on effectiveness evaluation of multiple user-generated search queries, all aiming to resolve the same underlying information need. Analysis concluded that "topics are extremely variable; queries dealing with the same topic are extremely variable" while "systems were only somewhat variable" [4]. A specific concern is the coverage that existing collections offer when widely varying queries are admitted, even for a single topic. Moffat et al. [10] examine the adequacy of relevance judgments in a standard single-query test collection in the face of such query variations; their results demonstrate that a large proportion of documents retrieved for the variable queries are unjudged, including many near the top of their rankings. Our purpose in this work is to address the challenge posed by this earlier work, and develop a test collection that explicitly includes query variability as a factor.
2. THE COLLECTION
Corpus, Topics, and Backstories ClueWeb12-B [11] was used as an underlying corpus due to its wide availability, scale, coverage of modern Web documents, and additional annotations. One hundred topics from the 2013 and 2014 TREC Web tracks [5, 6] were taken as the basis for information-need statements (background stories, or "backstories", written by us), in a manner similar to that described by Bailey et al. [2]. Topic numbers 201­300 were used; where a topic contained subtopics, one of them was selected as the focus of the backstory, and the others were ignored. An example backstory is shown below, for topic 215 (maryland department of natural resources), subtopic 2 (How do you get a Maryland fishing license?):
Having heard of the pristine environment in Maryland, you have long dreamed of taking a fishing holiday there. However, you think that you may need a fishing license in Maryland. How do you get one?

725

Mean Min. Max.

Raw queries per worker

41.2 1 100

Raw queries per backstory

72.5 35 106

Normalized queries per backstory

61.0 22 101

Spell corrected queries per backstory 57.7 19 101

Table 1: Query counts through the data simplification process.

Each backstory provides a brief motivating context, hopefully with some degree of realism, that helps individuals imagine themselves in a similar information-seeking situation and informs their query and effort responses [2]. The wording makes use of anaphora (coreferencing entities via pronouns) to avoid offering obvious queries.
Query Variations and Effort Estimates We developed two crowd worker interfaces. Both presented the backstory, and then asked the worker to enter the first query they would use to access information via a search engine in response to the backstory, and for estimates of the effort (in terms of number of useful documents, and number of queries) that they anticipated needing to satisfy the information need. We varied from the radio-button interface described by Bailey et al. [2]. In one interface we asked for effort estimates using graphical slider widgets ranging from 0 to 101 for the estimate of the number of useful documents required, and 1 to 11 for the number of queries that would need to be issued. In the other interface, we provided text entry fields requiring integer non-negative numbers. These two interfaces were released to two different English-speaking crowds.
As is often the case with crowds, a number of low quality workers participated. A mixture of methods was used to identify suspicious data, including noting workers who entered the same query text for multiple responses, or who provided undeviating effort estimates regardless of the backstory. Data from these workers was removed. At the conclusion of the cleaning process we had data from 263 individuals, spread across the 100 backstories. The count of workers per backstory averaged 108 (min: 105, max: 113) and there were a total of 10,835 individual queries and effort estimates provided. Each query was normalized to lowercase, with extraneous whitespace and trailing punctuation removed, and passed through the Bing search engine's spelling service to generate a final canonical form of each worker's query. This was done to avoid differences arising in how systems might handle such basic query normalization and spell correction of the query variations, and to reflect how queries would be pre-processed in a live system. It also reduced the total number of unique query variations (Table 1).
For example, for the Maryland fishing license backstory listed earlier, there were 53 unique spell-corrected query variations obtained. The average effort estimate in terms of useful documents required was 2.7 (which lies in the lowest decile of the 100 backstories). There were 13 variations which occurred more than once, of which 7 occurred only twice. The most popular query was "maryland fishing license" which occurred 14 times. Many of the single occurrence variations are expressed in more natural language forms, such as "how do i get a fishing license in maryland", "who can get a fishing license in maryland", and "is a fishing license needed in maryland". Only 8% of occurrences were identical to the corresponding TREC title query. In these regards, our collection process provided similar query diversity as is reported by Bailey et al. [2].
Relevance Judgments The 2013 and 2014 TREC Web collections include NIST-generated relevance judgments covering the 100 topic/subtopic pairs, based on the sets of 61 and 30 participating system runs, many of which will have run the nominal "title" query

associated with the topic. The TREC judgments use a six-category scale with four ordinal relevance levels for informational tasks; a category for navigational tasks; and a final category for spam [6]. Analysis of the NIST relevance judgments indicates that these definitions were not strictly followed. For example, topic 298 had approximately 50% of the documents given the Home Page label for the entire collection, all of which came from pages on the official website of the Jehovah's Witnesses; yet many of these pages are only of marginal relevance. We adapted the TREC category names and descriptions to provide slightly clearer expression of what each category label should capture, and added more information about what a searcher might do after reading a document in this category. Our assessors would receive little training, and so the judging guidelines and qualification test had to suffice. The new rating categories were: Essential; Very Useful; Mostly Useful; Slightly Useful; Not Useful; and Junk. The lowest, rating Slightly Useful, includes goodquality pages containing links to documents that would probably have useful information, even if they did not include the requisite information themselves. We believe this reflects real user behavior in web search activities. The full description of each rating is available in the judging guidelines provided as part of the collection.
For effectiveness metrics that use ordinal (graded) relevance, the Junk category should be merged with the Not Useful category, after which the scale can be applied directly as a five-level ordinal relevance scale. For metrics that require binary relevance judgments, our recommendation is to fold the Junk and Not Useful categories into a single Not Relevant category, and to fold the remaining levels into a single Relevant category.
Quality Control A total of 120 documents were randomly selected from those topic/subtopic-document pairs available in the original TREC qrels files that did not appear in our top-10 pools. These were stratified-sampled to obtain an even spread of TREC relevance labels. Each of the topic/subtopics was replaced by the corresponding backstory, and then each document was judged by two of the authors with respect to the judging guidelines. Any disagreements were discussed, and a final label agreed. Five of the pairs were subsequently discarded, due to genuine disagreement and potential for confusion, leaving a set of 115 "gold" judgments to draw from.
Judgments were then sourced via a crowd worker platform. From the gold set of backstory-document labels, a set of 27 were selected, again attempting to stratify as evenly as possible across the label categories. Before being asked to provide ratings, workers had to correctly rate documents from at least four of seven randomly chosen test questions (from the set of 27), and had up to three attempts. Additional random injections from the remaining gold judgments were used by the crowd-sourcing system to calibrate ongoing worker quality levels, and eliminate workers failing to meet standards without incurring unnecessary costs. There was no intentional overlap between the workers who provided the original query variations and effort estimates and those carrying out judging.
Aptness of Relevance Judgments A critical question that arises is that of how to assess what we denote as aptness, the extent to which a set of relevance judgments is fit for scoring a set of runs generated by systems and/or queries. Moffat and Zobel [7] introduce the notion of a residual, a numeric quantification of the extent of the (upward) uncertainty of a score that arises from the presence of unjudged documents. Residuals can be calculated for any weightedprecision effectiveness metric, including Reciprocal Rank (RR), by taking the difference between the "all unjudged documents are nonrelevant" and the "all unjudged documents are maximally relevant" run scores. The greater the residual, the less apt the judgments.
Figure 1a illustrates this approach. To construct the graph, the

726

UQV100 qrels

NIST qrels

1.0

1.0

NIST qrels

INST

RBP0.85

0.5

RR

0.5

0.0

0.0

INST RBP0.85 RR

0

1000

2000

3000

trec13 systems/queries

0

5000

10000

user queries

(a) system variants, NIST pool (b) query variants, NIST pool

1.0

0.5

INST (r1)

INST (r1+2)

RBP0.85

RR

0.0

0

1000

2000

3000

trec13 systems/queries

(c) system variants, UQV pool

UQV100 qrels

1.0 INST (r1)

INST (r1+2)

RBP0.85

0.5

RR

0.0

0

5000

10000

user queries

(d) query variants, UQV pool

Figure 1: Residuals for (left column) 50 × 61 = 3,050 TREC runs from 2013, and for (right column) 10,835 user-generated queries including repeats. Residual scores for three metrics are shown in each pane, computed using (top row) the NIST qrels selecting only the query subtopic used to generate the corresponding backstory, and using (bottom row) the new UQV100 qrels. All runs are relative to the ClueWeb12-B collection, and all metrics were evaluated over the top 200 documents retrieved; with Indri/BM25 used to construct runs for the user query variants in the right column. In the bottom row, the additional INST line shows the first round of judgments; both rounds were used for RBP and RR.

50 × 61 = 3,050 combinations between TREC 2013 contributing systems and topics were scored using the NIST-provided qrels file and four different metrics, including the INST mechanism of Bailey et al. [2], and residuals computed. Those residual were then independently sorted for each metric, and plotted by ascending value. The relatively low residuals shown in Figure 1a for the RR and RBP0.85 provides evidence of the aptness of the judgments for the evaluation of the contributing runs. But when using the user-supplied T values (which average at 4.7), INST has an expected search depth of 8.6, and the higher residuals show that the available judgments are not such a good fit. Moffat et al. [9] discuss INST in detail, including the relationship between T, expected search depth, and residual.
Use of NIST Judgments for Query Variants Figure 1b was generated using the same methodology as Figure 1a, but using the 10,385 user queries (5,764 distinct). The NIST-supplied qrels are again used; what is clear is that the judgments are no longer apt, and that none of these three metrics, not even RR, should be used to generate effectiveness scores for these runs. Indeed, residuals of over 0.5 arise for more than half of the queries. Even if by chance the lowerend scores for some metric displayed some particular attribute (for example, gave rise to a statistically significant system comparison), it would be risky to trust such a conclusion.
Working With INST We are interested in exploring the implications of using Bailey et al.'s INST metric when forming a test collection. INST is designed to be sensitive to the user's search goal, and is parameterized by a value T, the expected number of useful documents that will be required. For example, when T = 1, the

Depth
1 2 5 10 20 50 100 200

Total pool
2,741 5,157 11,755 21,895 40,478 91,556 170,166 316,171

Per run
0.25 0.48 1.08 2.02 3.74 8.45 15.71 29.18

Per document
0.25 0.24 0.22 0.20 0.19 0.17 0.16 0.15

Table 2: Pool sizes over 10,835 topic-query combinations. The final two columns give per run and per retrieved document averages.

user is anticipating needing to retrieve one relevant document, and the information need may well be navigational or factoid in nature. Higher values of T correspond to richer information needs.
The effort-influenced variability embedded in INST means that it is desirable to judge different topics to different depths, rather than use a uniform pool. Moreover, INST is an adaptive metric and becomes increasingly top-weighted as relevant documents are encountered. In combination, these two factors suggest a two-stage judgment process: first, uniform pooling to a relatively shallow depth, followed by a gap analysis to determine the topics, and document within topics, where further judgment effort should be applied in order to ensure that all residuals were broadly comparable to within the limits of the judgment budget. In doing so, we are in part implementing the mechanism described by Moffat et al. [8].
Collecting New Judgments: Round 1 Table 2 lists the uniform pool sizes that were generated from the user queries, with the final two columns normalized first on a per topic-query basis, and then second on a per-document retrieved basis. Despite the fact that there are nominally only 100 different information needs covered by these queries, and despite the fact that on average each query occurs twice, even at depth 10 fully 20% of the documents retrieved must be judged in order to cover the pool.
A first round of crowd-worker judgments was collected using a uniform pool depth of 10 and three-way overlap judging, based on the use of Indri/BM25 to construct runs against the ClueWeb12-B corpus for each of the 10,385 spell-corrected query variants (5,764 distinct). A median label from the (in almost all cases) three individual judge labels (after removing ratings that indicated a judge was unable to provide a label due to page load failure or other reasons) was assigned. Results are reported using this median label; in the collection data resources we also provide a re-estimated label based on the Community BCC algorithm described by Venanzi et al. [12].
Collecting New Judgments: Round 2 We then scored each run using INST, and for each document-topic combination, computed the sum over the runs of the residuals associated with that document [8]. That list of document-topic pairs was then sorted into decreasing order, and a further 5,501 documents taken from the front of it and judged, with aggregate per-document residuals of between 1.653 and 0.051. This targeted process had the effect of applying deeper pooling on topics with higher T values, and on topics where there were comparatively few relevant documents identified. Note that the adaptive and goal-sensitive nature of INST means that we are unable to determine which additional judgments were required without the initial round of judgments being completed.
Figure 1c and 1d show the application of the new qrels, with two lines plotted for INST showing the further improvement in average residual delivered by the Round 2 judgments. Using the full set of

727

UQV100 judgments, more precise measurement of effectiveness for the query variants can be achieved (Figure 1d), because that facet is the basis of the pooling. But the ability to accurately evaluate different retrieval systems has been significantly compromised (Figure 1c) compared to the NIST judgments arising from pooling across systems. Determining how best to cater to the cross-product of these competing requirements is a clear direction for future work.
As a future extension we plan to incorporate further variability by using one or more systems that vary considerably from the Indri/BM25 system we have used here. We will update the collection progressively as further labels become available.
Observations on Judgments The final set of 27,396 judgments represent all 100 topics, and reflect contributions from 179 judges, after qualification tests and anti-spam measures were applied. Each topic was judged by 42 judges on average (range 17­68). With each topic/document pair having been judged (for the most part) three times, we computed an inter-assessor agreement as Krippendorff's  = 0.26, or 0.24 with binary labels.
We emphasize that the TREC and UQV100 judgments are different, should not be aggregated, and are not interchangeable. TREC judgments were created by well-qualified, specialist analysts against TREC judging guidelines and topic descriptions; each judge would read hundreds of documents per topic. UQV100 judgments were made by judges from a more diverse population, working under very different constraints, against UQV100 judging guidelines and backstories ­ which may incur some topic intent drift ­ and judges would read many fewer documents per topic on average. UQV100 judgments were also subject to automated quality control tests as well as post-hoc data cleaning.
3. POTENTIAL USES
We present three possible uses of UQV100, beyond the obvious example of using it to investigate system performance on a per query or per backstory basis. When assessing the latter, we recommend averaging the query-level performance across all query variations belonging to a specific backstory first, and then averaging these across the 100 backstories (double-averaging).
Query Clustering While commercial web search engines have large query logs and can use these to examine query­query relationships via within-session re-formulations or co-clicks on the web graph, there is no equivalent in current test collections available to the broader IR community. The UQV100 collection supports some new investigations in query clustering by having so many query variations per backstory. As observed in an extensive investigation of query clustering [1], there are various practical applications of improved query clustering abilities. Even in commercial web search engines, there are difficulties in finding many examples of queries to cluster for rare queries; however this dataset contains many rare queries in each backstory cluster. Either the raw or spell-corrected query variations could be used in this application.
Query Transformations and Difficulty Prediction Performance has been shown to vary widely by the query variation [2], yet the information seeking task remains the same. With many different query variations, each with performance scores, it is now possible to examine which query variations within a cluster have the best performance, and hence investigate what query transformations on low-performing queries lead to better results. An alternative to this investigation would be to use the collection for learning to predict query difficulty [14], given each cluster has a range of scores.
Relevance Feedback Blind relevance feedback approaches have been well studied [13]. The UQV100 collection can support new

analysis of such algorithms, given the large number of query variations per cluster. The analysis can examine either individual queries, or the entire set of variations, and may provide additional opportunities for exploring how structural or syntactic elements of query expression (for example, presence of natural language structures, stop-words, term count, and so on) can lead to altered performance of relevance feedback.
4. CONCLUSIONS
We have constructed a substantial new information retrieval test collection (UQV100), using the ClueWeb12-B corpus as the underlying set of documents. The additional resources for the test collection consist of: 100 backstories and their paired TREC topic/subtopic id; corresponding normalized and spell-corrected query variations (an average of 58 per backstory) and effort estimates; judging guidelines; gold hits for qualification and quality control for future judgments of unjudged documents; top-k document ID rankings for all queries using Indri/BM25 against ClueWeb12-B; and relevance judgments (as per judging guidelines) for as a minimum the top-10 pooled documents for each query variation relative to the corresponding backstory. These resources are all freely available from http://dx.doi.org/10.4225/49/5726E597B8376.
Acknowledgment This work was supported by the Australian Research Council's Discovery Projects Scheme (project DP140102655). Xiaolu Lu (RMIT University) provided helpful assistance.
References
[1] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Improving search engines by query clustering. JASIST, 58(12):1793­1804, 2007.
[2] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. User variability and IR system evaluation. In Proc. SIGIR, pages 625­634, 2015.
[3] C. Buckley and E. M. Voorhees. Retrieval system evaluation. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, chapter 3, pages 53­75. MIT Press, 2005.
[4] C. Buckley and J. Walz. The TREC-8 query track. In Proc. TREC, 1999. NIST Special Publication 500-246.
[5] K. Collins-Thompson, P. N. Bennett, F. Diaz, C. L. A. Clarke, and E. M. Voorhees. TREC 2013 web track overview. In Proc. TREC, 2013. NIST Special Publication 500-302.
[6] K. Collins-Thompson, C. Macdonald, P. N. Bennett, F. Diaz, and E. M. Voorhees. TREC 2014 web track overview. In Proc. TREC, 2014. NIST Special Publication 500-308.
[7] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Sys., 27(1):2.1­2.27, 2008.
[8] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In Proc. SIGIR, pages 375­382, 2007.
[9] A. Moffat, P. Bailey, F. Scholer, and P. Thomas. INST: An adaptive metric for information retrieval evaluation. In Proc. Aust. Doc. Comp. Symp., pages 5:1­5:4, 2015.
[10] A. Moffat, F. Scholer, P. Thomas, and P. Bailey. Pooled evaluation over query variations: Users are as diverse as systems. In Proc. CIKM, pages 1759­1762, 2015.
[11] The Lemur Project. The ClueWeb12 Dataset, 2012. URL www. lemurproject.org/clueweb12.php/.
[12] M. Venanzi, J. Guiver, G. Kazai, P. Kohli, and M. Shokouhi. Community-based Bayesian aggregation models for crowdsourcing. In Proc. WWW, pages 155­164, 2014.
[13] J. Xu and W. B. Croft. Query expansion using local and global document analysis. In Proc. SIGIR, pages 4­11, 1996.
[14] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: Including applications to missing content detection and distributed information retrieval. In Proc. SIGIR, pages 512­519, 2005.

728

A Simple Enhancement for Ad-hoc Information Retrieval via Topic Modelling

Fanghong Jian1, Jimmy Xiangji Huang, Jiashu Zhao2, Tingting He3 and Po Hu3
Information Retrieval and Knowledge Management Research Lab 1National Engineering Research Center for E-Learning, 3School of Computer Science, Central China Normal University, Wuhan, China; 2School of Information Technology, York University, Toronto, Canada
jhuang@yorku.ca, jfhrecoba@mails.ccnu.edu.cn, jessie@cse.yorku.ca

ABSTRACT
Traditional information retrieval (IR) models, in which a document is normally represented as a bag of words and their frequencies, capture the term-level and document-level information. Topic models, on the other hand, discover semantic topic-based information among words. In this paper, we consider term-based information and semantic information as two features of query terms and propose a simple enhancement for ad-hoc IR via topic modeling. In particular, three topic-based hybrid models, LDA-BM25, LDA-MATF and LDA-LM, are proposed. A series of experiments on eight standard datasets show that our proposed models can always outperform significantly the corresponding strong baselines over all datasets in terms of MAP and most of datasets in terms of P@5 and P@20. A direct comparison on eight standard datasets also indicates our proposed models are at least comparable to the state-of-the-art approaches.
Keywords
Probabilistic Model; Dirichlet Language Model; LDA
1. INTRODUCTION
Many traditional IR models are based on the assumption that query terms are independent of each other, where a document is represented as a bag of words. Nevertheless this assumption may not hold in practice. Each document may contain several different topics and terms appeared in the document might belong to different topics, which represent different semantic information. Many researchers have been working on term topic information in IR [1, 10, 15, 16]. However, the nature of the associations among query terms still awaits further study. Some cluster-based approaches consider each document has only one topic [10], which is not reasonable to model large collection of documents. Topicbased document representation is effective in the language modeling (LM) framework [1, 15, 16]. But there is no generality in BM25 [2, 6, 20] and MATF (Multi Aspect TF) [13] based frameworks.
In this paper, we present three hybrid models for enhanc-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914750

ing traditional IR model via topic modelling. In our proposed approach, term-based information and semantic information are considered as two features of query terms. Latent Dirichlet Allocation (LDA) [3] is utilized to combine these two features and enhance three well-known traditional IR models BM25 [2], MATF [13] and Dirichlet LM [18]. In particular, three hybrid models, denoted as LDA-BM25, LDA-MATF and LDA-LM, are proposed respectively. The main contributions of this paper are as follows. First we propose three simple but effective IR models by combining traditional IR models with topic model. Second we conduct extensive experiments to confirm the effectiveness of our proposed models.
The remainder of this paper is organized as follows. We describe the related work and propose three topic-based hybrid models for ad-hoc IR in Section 2 and 3 respectively. In Section 4, we set up our experimental environment on eight TREC collections. In Section 5, the experimental results are presented and discussed. Finally, we conclude our work briefly and present future research directions in Section 6.
2. RELATED WORK
Since the 1990s, researchers started to investigate how to integrate term association into IR models [8, 12, 16, 19, 20]. The query-term associations have been modeled by different approaches according to the distance of the query terms in documents. For example, Buttcher et al. (2006) [4] used a proximity accumulator to associate each query term. Lv and Zhai (2009) [11] proposed a positional language model (PLM) that incorporated the term proximity in a modelbased approach using term-propagation functions. Metzler et al. (2005) [12] proposed a Markov Random Fields (MRF) model which modeled the joint distribution over queries and documents. Song et al. (2011) [14] proposed Proximity Probabilistic Model (PPM) which used a position-dependent term count to represent both the number of occurrences of a term and the term counts propagated from other terms. Recently, topic models have been widely used to explore latent term association in knowledge discovery and other related area. Liu and Croft (2004) [10] proposed cluster-based retrieval models under the language modeling framework, which were used to smooth the probabilities in the document model. In their approach, a document is supposed to contain only one topic, which is not reasonable to model large collection of documents. Azzopardi et al. (2004) [1] showed that it was effective to use the LDA model [3] to smooth the probabilities in the document model on several small collections. Wei and Croft (2006) [15] also discussed the applications of LDA in large collections, and presented a detailed evalua-

733

tion of the effectiveness. Yi and Allan (2009) [17] explored the utility of Mixture of Unigrams (MU) model, LDA and Pachinko Allocation Model (PAM) [9] for IR. They showed that topic models were effective for document smoothing. More rigorous topic models like LDA provided gains over cluster-based models and more elaborate topic models that capture topic dependencies provided no additional gains. Although it is effective to integrate topic models into the language modeling framework, how to integrate topical information into other traditional IR models is not clear.

3. OUR APPROACH

For enhancing performance, topic model is integrated into

traditional retrieval models. First, the latent semantic in-

formation of query terms in a document is extracted via

topic modeling. Then, the term-based information is ob-

tained through traditional retrieval models. The documents

that are more related to the query according to both seman-

tic topic-based information and term-based information are

boosted in the ranking process. For clarification, Table 1

outlines the notations used throughout the paper.

Notations

c

d

q

qi dl

avdl

N

n

tf

qtf

z

Kt

p, w ,

wpm  ,l

w

b, k1 , k3 µ

, 

Table 1: Notations
Description
collection document query query term length of document average document length number of indexed documents in collection number of indexed documents containing a term within-document term frequency within-query term frequency topic number of topics probability function
weighting function parameter in BM25 Dirichlet prior in Dirichlet LM hyperparameter in LDA

3.1 Topic-based Hybrid Model

Traditional retrieval models only capture term-based in-

formation. On the other hand, topic models acquire seman-

tic information between words. In this paper, we propose

enhanced retrieval models that consider not only term fre-

quency, document frequency and document length, but also

term topics information. We treat term-based information

and semantic topic-based information as two features for

query terms. The enhanced retrieval models combine these

two features.

Given a query q, for each term qi in query q, w(qi, d) is the

enhanced weight for document d. In order to capture the two

kinds of information, we use a parameter  to balance their

importance. So the weight of a query term for a document

is as follows.

w(qi, d) = (1 - ) · w(qi, d) +  · w(qi, d)

(1)

where w(qi, d) represents the explicit term-based related information in traditional retrieval model for document d, w(qi, d) is the implicit semantic information in topic model. Finally, a document's weight for a query is given by the sum of its weight for each term in the query. When  equals to 0, the hybrid models become traditional IR models such as BM25 and LM. When  equals to 1, the hybrid models become topic models. Because traditional IR models and topic models are normalized independently, the value of  changes with different combinations. It is well known that BM25, MATF and Dirichlet LM are the state-of-the-art traditional IR models and LDA is a simple but effective topic model. Therefore, we use BM25, MATF and Dirichlet LM as the traditional models and we use LDA as the topic model.

3.2 Topic Model

In general, topic model is used to capture latent seman-

tic information of terms in document. There are a lot of

topic models, such as probabilistic Latent Semantic Index-

ing (pLSI) [7], LDA [3] and PAM [9]. LDA is a simple and

effective topic model, and is broadly used. In this paper, we

use LDA as our topic model.

LDA model can generate the probability of topics in a doc-

ument and the probability of words in a topic, which can ob-

tain the generated probability of words in a document. We

take the probability of a query term in a document as its

implicit semantic information in the document. The proba-

bility is larger, the term is more related with the document.

In order to be the same magnitude with weights in tradi-

tional models, the weight of a query term for a document in

LDA uses log value of the generated probability as follows.

 Kt



w(qi, d) = log p(qi|d) = log  p(qi|z)p(z|d)

(2)

z=1

The LDA model can not be solved by exact inference and use Gibbs Sampling for parameter estimation like in [5].

3.3 Traditional Information Retrieval Models
Traditional information retrieval models are mainly classified into classic probabilistic model, vector space model and statistical language model. There are several well-known strong baselines in each class, considering BM25, MATF and Dirichlet LM respectively.
In BM25, the weight of a query term is related to its within-document term frequency and query term frequency. The corresponding weighting function is as follows.

w(qi, d) =

(k1 + 1)  tf K + tf

 log (N - n + 0.5)  (k3 + 1 )  qtf

(n - 0.5)

k3 + qtf

(3)

where w is the weight of a query term, the kis are tuning constants and K equals to k1  ((1 - b) + b  dl/avdl).
In 2013, Jiaul H. Paik [13] proposed a novel TF-IDF term weighting scheme MATF that employed two different within document term frequency normalizations to capture two different aspects of term saliency. One component of the term frequency is effective for short queries, while the other performs better on long queries. The final weight is measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query. Experiments carried out on a set of news and web datasets show that MATF outperforms several wellknown state-of-the-art TF-IDF baselines with significantly large margin.
Dirichlet LM presented by Zhai and Lafferty in 2001 [18] used the likelihood probability of query terms in a document to rank relevance between query and document. In order to better computing, the weight of a query term uses the log value of the probability as follows.

w (qi , d) = log p(qi |d)= log

dl

µ

dl + µ pml (qi |d) + dl + µ pml (qi |c)

(4)

4. EXPERIMENTAL SETTING
We conduct experiments on eight standard collections, which include AP88-89 with queries 51-100, AP88-90 with queries 51-150, FBIS with queries 351-450, FT(91-94) with queries 301-400, LA with queries 301-400, SJMN(1991) with queries 51-150, WSJ(87-92) with queries 151-200 and WT2G with queries 401-450. These datasets are different in size and genre [15, 19]. Queries without judgments are removed.

734

BM25 LDA-BM25
MATF LDA-MATF
LM LDA-LM

Eval Metric MAP P@5 P@20 MAP
P@5
P@20
MAP P@5 P@20 MAP
P@5
P@20
MAP P@5 P@20 MAP
P@5
P@20

AP88-89
0.2710 0.4360 0.3860 0.3021* (+11.476%) 0.5020 (+15.138%) 0.4388* (+13.679%) 0.2771 0.4531 0.3980 0.3041* (+9.744%) 0.4898 (+8.100%) 0.4378* (+10.000%) 0.2672 0.4571 0.4041 0.2980* (+11.527%) 0.5102* (+11.617%) 0.4276* (+5.815%)

AP88-90
0.2198 0.4566 0.3894 0.2617* (+19.064%) 0.5232* (+14.595%) 0.4505* (+15.693%) 0.2238 0.4707 0.4086 0.2617* (+16.935%) 0.5131 (+9.008%) 0.4465* (+9.276%) 0.2157 0.4465 0.4146 0.2560* (+18.683%) 0.5010* (+12.206%) 0.4414* (+6.464%)

FBIS
0.2606 0.3735
0.2685
0.2661
(+2.111%) 0.3679
(-1.499%) 0.2691
(+0.223%) 0.2553 0.3605 0.2673 0.2634*
(+3.173%) 0.3580
(-0.693%) 0.2784
(+4.153%) 0.2525 0.3506 0.2500 0.2628*
(+4.079%) 0.3630
(+3.537%) 0.2599*
(+3.960%)

FT
0.2600 0.3726 0.2389 0.2769* (+6.500%) 0.3621 (-2.818%) 0.2416 (+1.130%) 0.2660 0.3789
0.2426 0.2781* (+4.549%)
0.3621 (-4.434%)
0.2453
(+1.113%) 0.2571 0.3684 0.2311 0.2774*
(+7.896%) 0.3600
(-2.280%) 0.2426*
(+4.976%)

LA
0.2490 0.3571 0.2194 0.2592* (+4.96%) 0.3673 (+2.856%) 0.2291* (+5.105%) 0.2502 0.3571 0.2240 0.2586* (+3.357%) 0.3694
(+3.444%) 0.2337*
(+4.330%) 0.2427 0.3429 0.2235
0.2603* (+7.252%)
0.3694* (+7.728%)
0.2286 (+2.282%)

SJMN
0.1965 0.3404 0.2564 0.2297* (+16.902%) 0.3809 (+11.889%) 0.2915 (+13.697%) 0.2095 0.3723 0.2809 0.2309* (+10.215%) 0.3915
(+5.157%) 0.2989
(+6.408%) 0.2009 0.3532 0.2697 0.2254*
(+12.195%) 0.3830*
(+8.437%) 0.2904*
(+7.675%)

WSJ
0.3156 0.5240 0.4410 0.3471* (+9.981%) 0.5520
(+5.344%) 0.4640*
(+5.215%) 0.3029 0.5240 0.3950 0.3343*
(+10.366%) 0.5200
(-0.763%) 0.4300*
(+8.861%) 0.3047 0.5120 0.3910 0.3344*
(+9.747%) 0.5200
(+1.563%) 0.4320*
(+10.486%)

WT2G
0.3156 0.5280 0.3930 0.3230 (+2.345%) 0.5360
(+1.515%) 0.4030
(+2.545%) 0.3340 0.5240 0.4110 0.3393
(+1.587%) 0.5360
(+2.290%) 0.4150
(+0.973%) 0.3118 0.5000 0.3920 0.3165*
(+1.507%) 0.5080
(+1.600%) 0.3950
(+0.765%)

Table 2: Comparison with baselines. The best result obtained on each dataset is in bold. "*" denotes statistically significant improvements over corresponding baselines (Wilcoxon signed-rank test with p < 0.05). The percentages below are the percentage improvement of proposed models over corresponding baselines.

For all test collections used, each term is stemmed by using Porter's English stemmer. Standard English stopwords are removed. The official TREC evaluation measure is used in our experiments, namely Mean Average Precision (MAP). To investigate top retrieved documents, P@5 and P@20 are also used for evaluation. All statistical tests are based on Wilcoxon Matched-pairs Signed-rank test.
For fair comparisons, we use the following parameter settings for both the baselines and our proposed models, which are popular in the IR domain for building strong baselines. First, in BM25, setting k1, k3 and b to 1.2, 8 and 0.35 respectively gave the best MAP for most datasets in [20]. Second, in Dirichlet LM, µ = 1000 was shown in [15] to achieve best MAP for most datasets. Finally, in LDA model, we use symmetric Dirichlet priors with  = 50/Kt and  = 0.01, which are common settings in the literature and shown in [15] that retrieval results were not very sensitive to the values of these parameters. The number of topics Kt is set to be 400 as recommended in [15].
5. EXPERIMENTAL RESULTS
5.1 Comparison with Baselines
We first investigate the performance of our proposed topicbased models compared with the corresponding strong baselines BM25, MATF and Dirichlet LM. The experimental results are presented in Table 2. As shown by the results, our proposed models outperform the corresponding baselines on almost all datasets in terms of MAP, P@5 and P@20. Statistically significant improvement can be observed on most of datasets in terms of MAP and P@20. According to the results in Table 2, each hybrid model has its advantage on some aspects. However, there is no single hybird model that can achieve the best performance on all the datasets.
5.2 Parameter Sensitivity
An important issue that may affect the robustness of our proposed models is the sensitivity of their parameter  to retrieval performance. Since the weights of query terms in traditional retrieval models and topic model are normalized independently, the value of  reflects the influence of using topic-based model. Figure 1 plots the evaluation metrics MAP obtained by the proposed hybrid models over  values ranging from 0 to 1 on all the datasets. It is clear that hybrid models perform better than either traditional models

or topic model on all data sets. As we can see from Figure 1, our proposed models LDA-BM25, LDA-MATF and LDALM generally perform well over different datasets when  has a smaller value.
We also study the performance of our proposed topicbased models with different number of topics compared with the corresponding baselines in terms of MAP. In Figure 2, the traditional models are shown as straight lines since the performance does not change over the number of topics. All the results are presented in Figure 2, which shows that our proposed models with different number of topics outperform corresponding baselines in terms of MAP over all datasets. Figure 2 shows that the proposed hybrid models tend to perform better when the number of topics increases. When the number of topics reaches a certain value, the retrieval performance tends to become more stable. The performance tendency of our proposed models with different number of topics is surprisingly consistent on all the datasets. Similar trends for  and with different number of topics can also be observed in terms of P@5 and P@20.

5.3 Comparison with CRTER2 and LBDM
In addition, we compare our proposed models with two

state-of-the-art approaches. Zhao etc. [19, 20] showed that

bigram cross term model (CRTER2) is at least comparable to

major probabilistic proximity models PPM [14] and BM25TP

[4] in BM25-based framework. Xing and Allan [17], which

is most close to our proposed model LDA-LM, showed their

LDA-based model (LBDM) [15] achieved the best performance

in topic-based LM framework. So we make a direct compar-

ison with CRTER2 and LBDM. The results in terms of MAP

are presented in Table 3. "" denotes LDA-BM25 outper-

forms CRTER2, while "" denotes LDA-LM outperforms LBDM.

Among eight datasets, LDA-BM25 wins five times and LDA-

LM wins four times. By comparison, we can conclude that

our proposed models LDA-BM25 and LDA-LM are at least

comparable to the state-of-the-art models CRTER2 and LBDM.

Table 3: Comparison with CRTER2 and LBDM

AP88-89 AP88-90
FBIS FT LA
SJMN WSJ WT2G

CRTER2 0.2789 0.2268 0.2738 0.2717 0.2604 0.2095 0.3406 0.3359

LDA-BM25 0.3021 0.2617 0.2661 0.2769 0.2592 0.2297 0.3471 0.3230

LBDM 0.3051 0.2535 0.2636 0.2750 0.2630 0.2234 0.3359 0.3108

LDA-LM 0.2980 0.2560 0.2628 0.2774 0.2603 0.2254 0.3344 0.3165

735

MAP

MAP

0.32

AP88-89

0.3

0.28

0.26

0.24

0.22 0.2
0.18 0
0.28

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



LA

0.26

0.24

0.22

0.2

0.18

0.16 0.14 0.12
0

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



MAP

MAP

0.28

AP88-90

0.26

0.24

0.22

0.2

0.18
0.16 0
0.24 0.23 0.22 0.21
0.2 0.19 0.18 0.17 0.16 0.15
0

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



SJMN

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



MAP

MAP

0.28 0.26 0.24 0.22
0.2 0.18 0.16 0.14 0.12
0.1 0.08
0
0.34 0.32
0.3 0.28 0.26 0.24 0.22
0.2 0.18 0.16
0

FBIS

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



WSJ

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



MAP

MAP

0.28 0.26 0.24 0.22
0.2 0.18 0.16 0.14 0.12
0.1 0

FT

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



WT2G

0.3

0.25

0.2

0.15 0.1
0.05 0

LDA-BM25 LDA-MATF LDA-LM

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0



Figure 1: Parameter sensitivity of  on all data sets

0.315 0.31
0.305 0.3
0.295 0.29
0.285 0.28
0.275 0.27
0.265 100

200

300

0.265 0.26
0.255

LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

AP88-89

400

500

Number of topics

LA

600

LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

700

800

0.25

0.245

0.24 100

200

300

400

500

Number of topics

600

700

800

MAP

MAP

0.27

AP88-90

0.26

0.25 0.24 0.23

LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

0.22

0.21 100
0.245 0.24
0.235 0.23
0.225 0.22
0.215 0.21
0.205 0.2
0.195 100

200

300

LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

200

300

400

500

Number of topics

SJMN

600

400

500

Number of topics

600

700 700

800 800

MAP

MAP

0.275
0.27
0.265
0.26
0.255
0.25 100
0.35 0.345
0.34 0.335
0.33 0.325
0.32 0.315
0.31 0.305
0.3 100

200 200

FBIS
LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

300

400

500

Number of topics

WSJ

600

LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

300

400

500

Number of topics

600

700 700

800 800

MAP

MAP

0.285 0.28
0.275 0.27
0.265 0.26
0.255 100
0.345 0.34
0.335 0.33
0.325 0.32
0.315 0.31 100

FT
LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

200

300

400

500

Number of topics

WT2G

600

LDA-BM25 LDA-MATF LDA-LM BM25 MATF LM

200

300

400

500

Number of topics

600

700 700

800 800

Figure 2: Parameter sensitivity of the number of topics on all data sets

MAP

MAP

6. CONCLUSIONS AND FUTURE WORK
In this paper, a simple enhancement for ad-hoc IR is proposed by combining traditional retrieval model and topic model. Specifically, we present three hybrid models LDABM25, LDA-MATF and LDA-LM for enhancing traditional IR models via topic modeling. These three models capture both term-based information and latent semantic topicbased information at the same time. Experimental results on eight standard datasets show that the proposed models are effective, and outperform the corresponding strong baselines on most of datasets in terms of MAP, P@5 and P@20. Meanwhile, our proposed models are at least comparable to the state-of-the-art CRTER2 and topic-based model LBDM. Additionally, we carefully analyze the influence of  to our proposed models and the performance of our proposed models with different number of topics.
There are several interesting future research directions to further explore. We would like to study the optimal topic number on each dataset. It is also interesting to conduct an in-depth study on the combination traditional IR model with topic model and find the best combination. We also plan to evaluate our models on more datasets including some real datasets and apply our models into real world applications.
7. ACKNOWLEDGMENTS
This research is supported by a Discovery grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada, an NSERC CREATE award and also supported by the National Natural Science Foundation of China. We thank anonymous reviewers for their thorough comments.

8. REFERENCES

[1]
[2] [3] [4]
[5] [6]
[7] [8]
[9] [10] [11] [12] [13] [14] [15] [16]
[17]
[18] [19] [20]

L. Azzopardi, M. Girolami, and C. J. Van Rijsbergen. Topic Based Language Models for ad hoc Information Retrieval. In Proceedings of the International Joint Conference on Neural Networks, pages 3281­3286, 2004.
M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. Williams. Okapi at TREC-5. In Proc. of TREC, pages 143­166, 1996.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993­1022, 2003.
S. Buttcher, C. L. A. Clarke, and B. Lushman. Term Proximity Scoring for Ad-hoc Retrieval on Very Large Text Collections. In Proceedings of the 29th ACM SIGIR, pages 621 ­ 622, 2006.
T. L. Griffiths and M. Steyvers. Finding Scientific Topics. In Proceeding of the National Academy of Sciences, pages 5228­5235, 2004.
B. He, J. X. Huang, and X. Zhou. Modeling term proximity for probabilistic information retrieval models. Information Sciences, 181(14):3017­3031, 2011.
T. Hofmann. Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd ACM SIGIR, pages 50­57, 1999.
Q. Hu, J. X. Huang, and X. Hu. Modeling and Mining Term Association for Improving Biomedical Information Retrieval Performance. BMC Bioinformatics, 13(2):18 pages, 2012.
W. Li and A. McCallum. Pachinko Allocation: DAG-Structured Mixture Models of Topic Correlations. In Proc. of ICML, pages 577­584, 2006.
X. Liu and W. B. Croft. Cluster-Based Retrieval Using Language Models. In Proceedings of the 27th ACM SIGIR, pages 186­193, 2004.
Y. Lv and C. Zhai. Positional Language Models for Information Retrieval. In Proceedings of the 32nd ACM SIGIR, pages 299­306, 2009.
D. Metzler and W. B. Croft. A Markov Random Field Model for Term Dependencies. In Proceedings of the 28th ACM SIGIR, pages 472­479, 2005.
J. H. Paik. A Novel TF-IDF Weighting Scheme for Effective Ranking. In Proc. of the 36th ACM SIGIR, pages 343­352, 2013.
R. Song, L. Yu, J. R. Wen, and H. W. Hon. A Proximity Probabilistic Model for Information Retrieval. Tech. Rep., Microsoft Research, 2011.
X. Wei and W. B. Croft. LDA-Based Document Models for Ad-hoc Retrieval. In Proc. of the 29th ACM SIGIR, pages 178­185, 2006.
X. Wei and W. B. Croft. Modeling Term Associations for Ad-Hoc Retrieval Performance within Language Modeling Framework. In Proceedings of the 29th European Conference on IR research, pages 52­63, 2007.
X. Yi and J. Allan. A Comparative Study of Utilizing Topic Models for Information Retrieval. In Proceedings of the 31st European Conference on IR Research on Advances in Information Retrieval (ECIR'09), pages 29­41, 2009.
C. Zhai and J. Lafferty. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM TOIS, 22(2):179­214, 2004.
J. Zhao, J. X. Huang, and B. He. CRTER: Using Cross Terms to Enhance Probabilistic IR. In Proc. of the 34th ACM SIGIR, pages 155­164, 2011.
J. Zhao, J. X. Huang, and Z. Ye. Modeling Term Associations for Probabilistic Information Retrieval. ACM Trans. Inf. Syst., 32(2):1­47, 2014.

736

An Exploration of Evaluation Metrics for Mobile Push Notifications

Luchen Tan, Adam Roegiest, Jimmy Lin, and Charles L. A. Clarke
David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada
{luchen.tan, aroegies, jimmylin}@uwaterloo.ca, claclark@gmail.com

ABSTRACT
How do we evaluate systems that filter social media streams and send users updates via push notifications on their mobile phones? Such notifications must be relevant, timely, and novel. In this paper, we explore various evaluation metrics for this task, focusing specifically on measuring relevance. We begin with an analysis of metrics deployed at the TREC 2015 Microblog evaluations. A simple change to the metrics, reflecting a different assumption, dramatically alters system rankings. Applying another metric, previously used in the TREC Microblog evaluations, again yields different system rankings. We find little correlation between a number of "reasonable" evaluation metrics, which suggests that system effectiveness depends on how you measure it--an undesirable state in IR evaluation. However, we argue that existing evaluation metrics can be generalized into a framework that uses the same underlying contingency table, but places different weights and penalties. Although we stop short of proposing the "one true metric", this framework can guide the future development of a family of metrics that more accurately models user needs.
1. INTRODUCTION
This paper explores the problem of evaluating push notification techniques on social media streams in a filtering application. We assume an infinite stream of social media posts such as Twitter, against which the user issues an arbitrary number of standing queries representing "interest profiles", analogous to topics in traditional ad hoc retrieval. For example, the user might be interested in poll results for the 2016 U.S. presidential elections and wishes to be notified whenever new results are published. The system's task is to identify relevant tweets from the stream and send those updates directly to the user's mobile phone via push notifications. Since such notifications are often associated with an auditory or visual cue upon arrival, each imposes a nontrivial cognitive burden on the user (even if ignored). Thus, careful control of the volume of notifications is critical to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914694

successful push strategies. This paper explores evaluation metrics for such a task.
At a high level, push notifications should be relevant, timely (provide updates as soon after the actual event occurrence as possible), and novel (users should not be pushed multiple notifications that say the same thing). Accordingly, an evaluation metric should reward systems for updates that satisfy these three main criteria. As the design space is vast, in this short paper we focus on relevance, adopting existing notions of novelty and timeliness.
We start with an analysis of metrics from the TREC 2015 Microblog track, which operationalized such a push notification task, and then re-assess submitted runs after making a minor tweak to reflect a different assumption about the user model. We then re-assess submitted runs using variants of a metric that has been applied in previous iterations of the same evaluation. Using score and rank correlations, we compare system effectiveness as measured by each metric. Our results are surprising: we find little correlation between the different metrics. This means that the answer to "which system is better" depends on how you measure it, which is undesirable from an evaluation perspective.
The contribution of this paper is twofold. First, we present the novel and surprising finding discussed above: any number of reasonable evaluation metrics give rise to significantly different system rankings. We discuss and analyze why, tracing the issue to the handling of days for which there are no relevant tweets. Second, we argue that the different existing evaluation metrics we applied can be generalized into a framework that uses the same underlying contingency table, but places different weights and penalties. Although we do not propose the "one true metric", we believe this framework can guide the future development of an evaluation metric that more accurately models user needs.
2. BACKGROUND
The application described in the introduction was operationalized in the TREC 2015 Microblog track as the so-called "scenario A" variant of the real-time filtering task [3]. Over the official evaluation period, which spanned ten days during July 2015, participating systems "listened" to Twitter's live tweet sample stream to identify relevant tweets with respect to 225 topics, 51 of which were later assessed. Each system identified up to ten tweets per day, which were putatively delivered to hypothetical users. In total, 14 groups submitted 37 runs to the evaluation. Data from this evaluation provides the starting point for our analysis.
The assessment workflow for the track was as follows:

741

0.35 0.30 0.25 0.20 0.15 0.10 0.05

ELG-1 vs. nCG-1 R2 =0.7166

0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02

ELG-1 vs. ELG-0 R2 =0.0007

nCG-1 vs. nCG-0 R2 =0.003
0.20 0.15 0.10 0.05

nCG-1 ELG-0 nCG-0

0.00 0.00

0.05

0.10

0.15 0.20 ELG-1

0.25

0.30

0.00 0.00

0.05

0.10

0.15 0.20 ELG-1

0.25

0.30

0.00 0.00

0.05

0.10

0.15 0.20 nCG-1

0.25

0.30

0.35

Figure 1: ELG-1 vs. nCG-1 (left), ELG-1 vs. ELG-0 (middle), and nCG-1 vs. nCG-0 (right) for all submitted runs. Plots show that the treatment of "silent days" has a large impact on system scores.

first, tweets returned by the systems were assessed for relevance using a traditional pooling process. Relevant documents were then semantically clustered into groups containing tweets that share substantively similar information. We refer the reader to previous papers for more details [5].
The two metrics used to evaluate system runs were expected latency-discounted gain (ELG) and normalized cumulative gain (nCG). These two metrics are computed for each topic for each day in the evaluation period (explained in detail below). The score for a topic is the average of the daily scores in the evaluation period. The score of a system run is the average of the scores across all topics.
Expected latency-discounted gain (ELG) was adapted from the TREC Temporal Summarization track [2]:

1 G(t)

(1)

N

where N is the number of tweets returned and G(t) is the gain of each tweet: non-relevant tweets receive a gain of 0, relevant tweets receive a gain of 0.5, and highly-relevant tweets receive a gain of 1.0.
A key aspect of this metric is its handling of redundancy and timeliness: a system only receives credit for returning one tweet from each cluster. Furthermore, a latency penalty is applied to all tweets, computed as MAX(0, (100-d)/100), where the delay d is the time elapsed (in minutes, rounded down) between the tweet creation time and the putative time the tweet was delivered. That is, if the system delivers a relevant tweet within a minute of the tweet being posted, the system receives full credit. Otherwise, credit decays linearly such that after 100 minutes, the system receives no credit even if the tweet was relevant.
The second metric is normalized cumulative gain (nCG):

1

G(t)

(2)

Z

where Z is the maximum possible gain (given the ten tweet per day limit). The gain of each individual tweet is computed as above (with the latency penalty). Note that gain is not discounted (as in nDCG) because the notion of document ranks is not meaningful in this context.
Due to the setup of the task and the nature of interest profiles, it is possible (and indeed observed empirically) that for some days, no relevant tweets appear in the judgment pool. In terms of evaluation metrics, a system should be rewarded for correctly identifying these cases and not pushing nonrelevant content. If there are no relevant tweets for a particular day and the system returns zero tweets, it receives a

score of one (i.e., perfect score) for that day; otherwise, the system receives a score of zero for that day. This applies to both ELG and nCG.
It is worth mentioning that despite superficial similarities, our task is very different from document filtering in the context of topic detection and tracking (TDT) [1]. TDT is concerned with identifying all documents related to a particular event--with an intelligence analyst in mind--which requires keeping track of false alarms and missed detections. In contrast, we are focused on identifying a small set of the most relevant updates to push to users, grounded in interactions with mobile devices. Furthermore, in TDT, systems must make online decisions as soon as documents arrive, whereas in our case systems can choose to push older content (subjected to the latency penalty), thus giving rise to the possibility of algorithms operating on bounded buffers. For these various reasons, TDT evaluation tools such as the decision error tradeoff (DET) curve and derivative metrics provide inspiration, but are not directly applicable.
3. ANALYSIS OF "SILENT DAYS"
In Figure 1 (left), we show a scatterplot of the official ELG scores (which we call ELG-1 for reasons that will become clear shortly) vs. nCG (specifically, nCG-1, for the same reasons). Although there is an overall correlation between ELG-1 and nCG-1 across all submitted runs, we do note that in particular cases ELG-1 and nCG-1 are capturing different aspects of effectiveness: for example, the top three runs in terms of ELG-1 (circled in blue) exhibit relatively large differences in nCG-1. There are also cases in which systems achieve high nCG-1 relative to their ELG-1 scores (the runs circled in red).
One interesting aspect of ELG-1 and nCG-1 is their handling of days in which there are no relevant documents: for rhetorical convenience, we call days in which there are no relevant tweets for a particular topic (in the pool) "silent days", in contrast to "eventful days" (where there are relevant tweets). In both ELG-1 and nCG-1, for a "silent day", the only two possible scores are one (if the system remained silent) or zero (if the system pushed any tweet). This means that an empty run (a system that never returns anything) may have a non-zero score based on how many silent days there are in each topic. As it turns out, an empty run will score 0.2471 in ELG-1 and nCG-1, shown as dotted lines in Figure 1 (left). Since this was the first year of this TREC evaluation, systems achieved high scores by simply returning few results, in many cases for totally idiosyncratic reasons-- for example, the misconfiguration of a score threshold.

742

ELG-1 vs. Silence Precision
1.0
R2 =0.02
0.8

ELG-1 vs. Silence Recall
1.0
R2 =0.6132
0.8

Silence Precision vs. Silence Recall
1.0
R2 =0.0002
0.8

Silence Precision Silence Recall
Silence Precision

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.0 0.00

0.05

0.10

0.15 0.20 ELG-1

0.25

0.30

0.0 0.00

0.05

0.10

0.15 0.20 ELG-1

0.25

0.30

0.0

0.0

0.2

0.4

0.6

0.8

1.0

Silence Recall

Figure 2: Characterizing the effects of "silent days": silence precision vs. ELG-1 (left), silence recall vs. ELG-1 (middle), and silence precision vs. silence recall (right). Systems score well by learning when to "shut up".

As an alternative, what if we did not reward systems for remaining silent? That is, on a silent day, all systems receive a zero score, no matter what they did. We call these variant metrics ELG-0 and nCG-0 (in contrast to ELG-1 and nCG-1). We can justify this from the user perspective in that for a silent day, the user does not obtain any relevant information regardless of system output (since there are no relevant documents). In this case, how would the user know to "reward" a system for remaining silent? That is, properly determining a silent day requires global knowledge (e.g., from pooling), which no individual user has access to.
In Figure 1, we show scatterplots of ELG-1 vs. ELG-0 (middle) and nCG-1 vs. nCG-0 (right). We see no discernible relationship between each pair of metrics, which suggests that the handling of silent days is the most critical part of each metric, in that different (reasonable) formulations yield dramatically different results and system rankings. In fact, we would go as far as saying that effectiveness under ELG-1 and nCG-1 is primarily dominated by a system's ability to identify the silent days. Under both metrics, systems do well by learning when to "shut up".
This observation is further illustrated by the scatterplots in Figure 2, where we show silence precision vs. ELG-1 (left), silence recall vs. ELG-1 (middle), and silence precision vs. silence recall (right) for all runs. Silence precision and recall follow the usual definitions of precision and recall, but with respect to identifying the silent days. Since each topic has an equal number of days, there is no difference between micro- and macro-averaging. Across all the topics, 24.7% of all days are completely silent, while another 6.7% have relevant but redundant material. We see that there is a slightly positive (but very weak) correlation between ELG-1 and silence precision. The middle graph, in effect, shows that systems achieve a high ELG-1 score by achieving a high silence recall--i.e., getting a good score is dominated by a system knowing when to "shut up". Although systems with comparable silence recall can differ substantially in ELG-1, we were surprised by how much of the variance in the official metric can be explained by silence recall alone.
The right graph in Figure 2 shows the tradeoffs systems make with respect to precision and recall. On the right edge of the plot are cases where the systems are almost always quiet, achieving nearly perfect recall; in the left lower corner is a system that never "shuts up", and hence its precision and recall are both zero. It is interesting to note that some systems perform poorly in both precision and recall--they don't push content when there's relevant content and don't "shut up" when there's no relevant content.

To our knowledge, we are the first to make this observation about the huge impact of silent vs. eventful days in the current evaluation of push notification. However, we withhold judgment as to whether the current TREC metrics represent the "right" approach: from the user perspective, since push notifications are associated with high cognitive effort (because they may interrupt the user), perhaps we should force systems to focus on learning when to "shut up". On the other hand, having such highly binarized scores on the silent days creates many issues for system tuning, since it creates discontinuities in the objective. We observe similar issues when trying to optimize a metric such as precision at rank one for question answering.

4. GAIN AND PAIN
What are other reasonable ways in which we can evaluate the push notification task? A simple and intuitive utility-based metric would be to reward "gain" based on delivery of relevant information and to deduct "pain" based on delivery of non-relevant information. In fact, the TREC 2012 Microblog track employed exactly such a metric, called T11U [4], itself derived from the linear utility metrics used in the TREC filtering tracks.
We adopt a slightly different but mathematically equivalent formulation as follows:

T11U =  · G - (1 - ) · Nx

(3)

where G is total gain, Nx is the number of non-relevant documents pushed, and  controls the relative weight of gain vs. pain. Note that in T11U, the total gain factors in different relevance grades, the latency penalty, and the treatment of redundant tweets in exactly the same way as ELG.
In Figure 3, we show a scatterplot of ELG-1 vs. T11U with  = 0.66, which was the value used in TREC 2012. This value can be understood as setting the gain of a relevant notification (highest relevance grade, no temporal penalty, not redundant) equal to the pain of returning two non-relevant updates. With this setting, we do see reasonable positive correlation between ELG-1 and T11U overall, but this correlation is highly misleading. According to T11U, very few systems achieve positive utility overall--that is, the gain from pushing relevant content is not sufficient to offset the pain from pushing non-relevant content. Furthermore, the relatively large cluster of runs which score around zero in T11U vary widely in ELG-1, from around 0.2 to over 0.3, with the highest T11U score being somewhere in the middle of this band. In other words, poorly-performing systems

743

T11U

ELG-1 vs. T11U
1
R2 =0.6913
0

-1

-2

-3

-4

-5

-6 0.00

0.05

0.10

0.15 0.20 ELG-1

0.25

0.30

Figure 3: ELG-1 vs. T11U.

Kendall's ¿ between T11U and ELG-1
0.5

0.4

0.3

Kendall's ¿

0.2

0.1

0.0

-0.1

-0.2

0.0

0.2

0.4

0.6

0.8

1.0

®

Figure 4: Kendall's  between T11U and ELG-1 for different values of .

score low in both ELG-1 and T11U, but beyond that, T11U and ELG-1 exhibit a weak correlation at best.
Of course, absolute scores and relative system rankings depend on the  parameter that balances gain vs. pain, and the setting of  = 0.66 was arbitrary. What would the evaluation results look like for different settings of ? The answer is shown in Figure 4, where we sweep the  parameter and compute Kendall's  with respect to ELG-1 for each setting. The results show that Kendall's  varies substantially, ranging from moderate correlation to non-existent and even slightly negative correlation. We have shown above that high correlations can be misleading, and this plot shows that  = 0.66 is around the highest Kendall's  we can obtain regardless. These results suggest that T11U and ELG-1 are measuring quite different aspects of effectiveness.

5. TOWARD A GENERAL FRAMEWORK
Let us take stock of our findings so far: we have evaluated runs from the TREC 2015 Microblog track using official as well as alternative metrics (ELG-1, ELG-0, nCG-1, nCG-0, and T11U). In comparing the metrics, we observe many inconsistencies in terms of both system scores and relative rankings. In other words, "which system is better" depends on what measure we use. From an evaluation perspective, this is not desirable because researchers lack consistent guidance for algorithm development.
To address this issue, we propose a general evaluation framework built around the contingency table shown in Ta-

System action
Pushed relevant Pushed not-relevant Stayed silent

"eventful days"
+GE -PE -SE

"silent days"
-P0 +S0

Table 1: The contingency table for a general evaluation framework for push notifications.

ble 1. At the core, our framework is utility-based in that gain is rewarded for pushing relevant content (+GE) and pain is deducted for pushing non-relevant content (-PE and -P0). However, a key insight is the explicit separation of eventful and silent days, which our ELG-1 vs. ELG-0 experiments have shown to be critical in system evaluations.
Our evaluation framework is general in that the metrics we have examined in this paper can be viewed as specific instantiations of the parameters in Table 1. For example, T11U sets GE and PE (based on ) but ignores the final row, and furthermore does not make the distinction between eventful and silent days. ELG-1 and ELG-0 make different choices on S0, how systems should be rewarded for staying silent on silent days, but both set PE and P0 to zero. That is, no pain is deducted for pushing non-relevant content.
Using the framework presented in Table 1 as a guide, we can imagine a family of metrics beyond those already presented. For example, we might augment T11U by creating a distinction between eventful and silent days, thus arriving at a metric that is closer to ELG-1 or ELG-0. We might set PE differently from P0 to create more nuanced distinctions in a T11U-like metric. Different ratios between these weights also give rise to emphasis on different aspects of the push notification problem.
The question remains on how to properly set the gain and pain weights in the contingency table--and we presently provide no concrete answer, expect to say that further studies in user modeling are necessary. For example, we have presented two plausible scenarios (ELG-1 and ELG-0) for the treatment of silent systems on silent days: a user study is necessary to decide which alternative (or neither) matches user preferences. Although the development of the "one true metric" is beyond the limited scope of this short paper, our framework contributes to a step toward that goal.
Acknowledgments. This work was supported in part by the U.S. National Science Foundation under awards IIS1218043 and CNS-1405688 and the Natural Sciences and Engineering Research Council of Canada (NSERC). Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.

6. REFERENCES
[1] J. Allan. Topic Detection and Tracking: Event-Based Information Organization. Kluwer, 2002.
[2] J. Aslam, M. Ekstrand-Abueg, V. Pavlu, F. Diaz, R. McCreadie, and T. Sakai. TREC 2014 Temporal Summarization Track overview. TREC, 2014.
[3] J. Lin, M. Efron, Y. Wang, G. Sherman, and E. Voorhees. Overview of the TREC-2015 Microblog Track. TREC, 2015.
[4] I. Soboroff, I. Ounis, C. Macdonald, and J. Lin. Overview of the TREC-2012 Microblog Track. TREC, 2012.
[5] Y. Wang, G. Sherman, J. Lin, and M. Efron. Assessor differences and user preferences in tweet timeline generation. SIGIR, 2015.

744

Engineering Quality and Reliability in Technology-Assisted Review

Gordon V. Cormack
University of Waterloo
gvcormac@uwaterloo.ca

Maura R. Grossman
Wachtell, Lipton, Rosen & Katz
mrgrossman@wlrk.com

ABSTRACT
The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.
Keywords: Technology-assisted review; predictive coding; electronic discovery; e-discovery; test collections; relevance feedback; continuous active learning; reliability; quality; systematic review.
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '16 July 17-21, 2016, Pisa, Italy c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4069-4/16/07. DOI: http://dx.doi.org/10.1145/2911451.2911510

1. INTRODUCTION
A vexing question that has plagued the use of technologyassisted review ("TAR") is "when to stop"; that is, knowing when as much relevant information as possible has been found, with reasonable effort. We present a provably reliable method to achieve high recall using any search strategy that repeatedly retrieves documents and receives relevance feedback, continuing indefinitely until a decision is made to discontinue the review process. Amenable search strategies include traditional ranked retrieval,1 interactive searching and judging [8], move-to-front pooling [8], and continuos active learning ("CAL") [5].
For the particular implementation of CAL supplied as the baseline model implementation ("BMI") [7] for the TREC 2015 Total Recall Track [13], we present two stopping procedures that achieve superior empirical reliability for comparable effort, and comparable empirical reliability for less effort, relative to our provably reliable method.
Our primary motivation is to provide quality assurance for TAR applications, including electronic discovery ("eDiscovery") in legal matters [5], systematic review in evidencebased medicine [10], and the creation of test collections for information retrieval ("IR") evaluation [14]. Since these applications generally require that a human review each relevant document, we assume for this study that the effort to provide relevance feedback for relevant documents is a sunk cost. On the other hand, the effort to assess and provide relevance feedback for non-relevant documents is wasted. We measure review effort in terms of the total number of documents reviewed, whether relevant or not. An ideal search would find all of the relevant documents with effort equal to precisely that number. An acceptable search would find most of the relevant documents with minimal wasted effort.
A reliable search method would achieve an acceptable search most of the time. More formally, if S is a random variable representing a search, and acceptable(s) is an indicator function denoting whether a particular search s has an acceptable result, we define:
reliability =def Pr[acceptable(S) = 1] .
To this end, we define recall(s) and effort(s) to be the recall and effort associated with s. For simplicity, our primary
1To be amenable, a retrieval method must be able to rank the entire collection. Incomplete rankings or set-based results may be extended by adding the remaining documents in any order.

75

Collection
At Home At Home At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05

Source
TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall Reuters TREC 2012 Filtering TREC 2012 Filtering TREC 2004 Robust TREC 2005 Robust

Description
Jeb Bush public email Hacker forums Local news Tim Kaine non-public email MIMIC II Clinical Database News subject categories NIST topics Conjunction of RCV1-v2 subject pairs Amalgam of TREC ad-hoc topics 50 legacy topics, new dataset

# Docs
290,000 465,147 902,434 401,953
31,538 804,414 804,414 804,414 528,256 1,033,461

# Topics
10 10 10 4 19 103 50 50 249 50

# Rel (R)
227-17,135 179-9,517 23-2,094 14,341-166,118 180-19,182 5-381,327
12-610 21-349 4-161 17-376

Table 1: Ten Evaluation Datasets. In our experiments, the three At Home datasets are treated as a single test collection, for a total of eight test collections.

results use a goal-post definition [18] of acceptability:
acceptable(s) = 1 (recall(s)  0.70) . 0 (recall(s) < 0.70)
Our primary results further assume that 95% reliability is sufficiently high.
The methods and results detailed in this work are:
· The target method : a provably reliable method that chooses ten random relevant documents as a target, and employs an independent search method to retrieve documents without knowledge of the target set, until each document in the target set has been retrieved. We prove that this method achieves 95% reliability for a minimum threshold recall of 70%.
· The knee method : a geometric stopping procedure, based on the shape of the gain (i.e., recall versus effort) curve, that augments BMI to achieve similar empirical reliability to the target method, with substantially less effort. The knee method, in contrast to the target method, is practical regardless of the number of relevant documents in the collection.
· The budget method: a variant of the knee method that achieves superior empirical reliability to the target method, with similar effort.
· Empirical validation: we assess the effectiveness and reliability of our methods on eight archival test collections consisting of 555 topics and 4.5 million documents (see Table 1).
· Quality evaluation: As an alternative to binary relevance and fixed recall and reliability thresholds, we argue and provide evidence that quality loss functions [18] provide more nuanced measures that better reflect the tensions among consistency, effectiveness, and efficiency.
2. BACKGROUND
The modern literature on the effectiveness and reliability of high-recall retrieval is largely confined to the problem of constructing test collections for IR evaluation, and eDiscovery in legal matters. A 1985 study by Blair and Maron [2] showed that teams of lawyers and paralegals, using iterative Boolean searches, believed they had achieved 75% recall, when in fact they had achieved 20%. Blair [3] later

described the difficulty of measuring high recall in general, and the use of targeted searching, systematically constructed Boolean queries, and stratified sampling to estimate recall for the Blair and Maron study.
The Text Retrieval Conference ("TREC") [21] first addressed the problem of IR evaluation for "large" datasets, which at the time of TREC's inception in 1992, contained on the order of 500,000 documents. TREC follows the Cranfield paradigm [20], which evaluates the results of subject systems against a gold standard that identifies every relevant document. For large datasets, the effort to render a human assessment for each document is prohibitive, thus occasioning the use of automated or semi-automated methods to limit the human review effort required to label the dataset. TREC saw the introduction of the "pooling method," which selects the top-ranked documents from a number of independent retrieval efforts for assessment, and deems all other documents to be non-relevant. A number of studies (see [19]) indicate that this method fails to identify a substantial number of documents, but even so, the resulting gold standard yields a stable evaluation of the relative effectiveness of candidate systems, as measured by Kendall's  rank correlation. We are unaware of any studies that address the effectiveness of pool-based gold standards for evaluating high-recall retrieval, or for simulating interactive relevance feedback. Studies suggest that greedy or machine-learning methods to select the pool yield a more nearly complete gold standard [8, 14].
Interactive searching and judging ("ISJ"), in which a searcher repeatedly formulates queries and examines the top results from a relevance-ranking search engine, has been shown to yield gold standards with comparable quality to the pooling method, with considerably less effort [8]. Continuous active learning ("CAL") [5] is essentially the same as ISJ, but uses machine learning instead of, or in addition to, manually formulated queries to rank the documents for review. An approach similar to CAL was used in the TREC 2012 Filtering Track (see [17]) to construct the gold standard that was used for evaluation, and also to simulate relevance feedback. A subsequent study based on pooling showed that the CAL-like approach had achieved high recall, and high effectiveness, as measured by Kendall's  [17]. CAL achieved superior results at the TREC 2009 Legal Track [4], and remains state of the art for eDiscovery.
The TREC 2015 Total Recall Track [13] represents the first study of high-recall human-in-the-loop retrieval in which all aspects of human intervention are simulated, and

76

hence controlled. Fully automated or semi-automated retrieval systems were tested through their interaction with an evaluation server. At the outset, the evaluation server provided a document collection and a topic description, after which the system under test submitted potentially relevant documents from the collection to the evaluation server. In response, the evaluation server provided an assessment (derived from a pre-computed gold standard) for the submitted documents, and the process continued until the documents were exhausted or the system chose to stop.
Participants in the Total Recall Track were supplied with a CAL baseline model implementation2 ("BMI") that, when connected to the evaluation server, performed all aspects of the task--other than deciding when to stop--without human intervention. Participating systems were allowed to run indefinitely, and were evaluated (primarily) on the quality of the ranking determined by the order in which the system presented documents to the server. Instead of actually terminating when they thought an acceptable result had been achieved, participants were invited to "call their shot" by indicating, in real time, when they would have stopped, had they been required to balance benefit with cost. The current study considers the addition of a call-your-shot mechanism to BMI, and, more generally, to any ranking system.
The TREC 2015 Total Recall Track contributed five fully labeled archival datasets. The Jeb Bush, Hacker Forums, and Local News datasets were used for the At Home task, in which participants ran their systems on their own platforms, connecting via the internet to the evaluation server, which was run by the track coordinators. The Kaine and MIMIC II datasets were used for the Sandbox task, in which participants encapsulated their systems as a virtual machine, which was run by the track coordinators, along with the evaluation server, isolated from the internet.
The reliability of methods for constructing gold standards for IR evaluation has typically been evaluated by how well the resulting gold standard ranks the relative effectiveness of precision-oriented retrieval systems, where the objective is to find as much relevant information as possible at low rank. For this purpose, a calibrated estimate is not required; it is sufficient to determine whether one system achieves higher recall than another, and the actual numerical value is ascribed little meaning. A number of studies (see [23]) eschew recall altogether, assuming that the user's information need will be satisfied by a tiny fraction of a vast sea of relevant documents. Zobel et al. [23] suggest that recall is a poor effectiveness measure, even for the "high-recall applications" where the user seeks "total recall," and that only an extensive ad-hoc effort using multiple queries and tools will satisfy the user that their information need has been met.
The reliability and effectiveness of TAR (also known as "predictive coding") is the subject of much interest in the legal community [9, 16]. A number of approaches to TAR, to deciding when to stop, and to quality assurance have been advanced, but no stopping procedure has previously been shown to be mathematically or empirically reliable. Perhaps the most commonly used approach to TAR involves the use of a supervised machine-learning algorithm trained using a set of documents from the collection (typically referred to as a "seed set") to partition the collection into a "review set," which is subject to human review, and a "null set," which
2See http://plg.uwaterloo.ca/~gvcormac/trecvm/.

is not. This approach is referred to as either simple passive learning ("SPL") or simple active learning ("SAL"), depending on whether or not the learning algorithm is involved in selecting the training documents [5]. Recently, CAL has been advanced as a superior alternative [5, 7].
Regardless of the TAR method used, the question remains of when to stop. For SPL and SAL, two questions must be answered: when to stop training; and how many documents should be included in the review set. For CAL, the sole question is when to stop. One approach that has been advanced is to draw a random hold-out set (referred to as a "control set") that is used to measure the effectiveness of the classifier, in order to determine when to stop training, and then to measure recall, so as to determine how many documents should comprise the review set. The control set must be large enough to contain a sufficient number of relevant documents to yield a precise estimate. Bagdouri et al. [1] note that the use of a control set constitutes sequential sampling, with the net effect that it yields a biased estimate of recall, and cannot be used for quality assurance. As an alternative, they propose "certified text classification," in which part of the review budget is set aside to conduct a frequentist acceptance test that will accept or reject the classifier. Bagdouri et al. are concerned with the problem of testing whether the classifier has achieved a threshold level of F1; they do not consider recall, or how to proceed in the event that the classifier is rejected by the test.
The limitations of binary relevance may be of particular importance in evaluating the effectiveness and reliability of TAR systems. Binary relevance does not account for the differential importance of relevant documents, and there will necessarily be documents near the threshold about which competent assessors will disagree (see [19]). In evaluating the recall of a system against a gold standard, there will necessarily be uncertainty for some documents as to whether the system is correct, the gold standard is correct, or reasonable minds could disagree. If a system fails to meet a target recall threshold, is it because the system has missed important documents, because it has missed marginal documents about which reasonable minds could differ, or because it has missed documents that are incorrectly coded relevant in the gold standard? And, is the effort to remedy the shortfall proportionate to the importance of the missed documents?
Binary relevance and fixed recall targets are examples of traditional goal-post methods in quality engineering ([18]), where success or failure is a binary quantity, and reliability is the probability of success. In quality engineering, a quadratic loss function blends reliability and effectiveness into a single quality measure, with targets, but no arbitrary thresholds [18].
3. GUARANTEED RELIABILITY
Our target method involves drawing a target set T of k random relevant documents from the collection; for simplicity we fix k = 10, but a different number could be chosen. In order to draw T , we retrieve and review documents selected at random until k relevant documents are found, or the collection is exhausted. The underlying search strategy retrieves documents for review without knowledge of T , until every document in T has been found. This method achieves 95% reliability, as shown below.
Consider a document collection C and a function rel(d) indicating binary relevance. The number of relevant docu-

77

ments in the collection is R = |{d  C|rel(d)}|. A search strategy is a ranking on C where 1  rank(d)  |C| denotes the position of d in the ranking. It is important to note that the following argument holds for any such ranking, provided it is independent of T .
The retrieved set of the target method is the shortest prefix P of the ranking that contains T :

P = {d|rank(d)  max rank(d )} .
d T

Now consider the ranking relrank(d) of only relevant documents:

relrank(d) = d  C|rel(d)  rank(d )  rank(d) .

The last retrieved document dlast is necessarily in T :

dlast = arg max rank(d) = arg max relrank(d) .

dT

dT

The recall of our method is:

recall

=

relrank(dlast) R

.

Taking T to be a random variable, the method is reliable if:

R



10



Pr[

relrank(dlast R

)



0.7]



0.95 .

Assuming large R, consider the problem of determining a cutoff c such that:

Pr[

R

-

relrank(dlast) R

>

c]

=

0.05

(1)

Pr[R - relrank(dlast) > cR] = 0.05

(2)

For the condition in Equation 2 to hold, it must be the case that the [numerically] top-ranked cR documents are absent from T , which occurs with probability

1-

10 R

cR
= 0.05 .

It follows that: For all R > 10,

c

=

log 0.05

R log

1

-

10 R

.

c

<

lim
R

R

log 0.05

log

1

-

10 R

= 0.299573 < 0.3 .

(3)

Combining (1) and (3), we have:

Pr[

R

-

relrank(dlast) R

>

0.3]

<

0.05

R



10



Pr[

relrank(dlast) R



0.7]



0.95

.

Reliability is obtained at the cost of supplemental review

effort inversely proportional to R, the number of relevant

documents. The number of randomly selected documents

that

need

to

be

reviewed

to

find

k

relevant

ones

is

k

|C| R

,

on

average for R

|C |.

For k

= 10

and prevalence

R |C|

 1%,

the target method entails a review overhead of about 1,000

additional documents. Lower prevalence entails substan-

tially more overhead, while higher prevalence entails less.

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

0 0 0.1 0.2 0.3

(a)
Fig. 2: KnFeiegdulerea1lg: oKrintheme Dfoetrecotniloinne[1k5n].ee detection. (a) depict

4pro.etrapteeEndMd4ic5PuIldaRergIrdCeiesAsta.LnTcRheeEfmrLoamIgAnyBituI=dLeIxTofYwthiethsethbearsmcaoxrirmesupmonddisttoanthcee

vaOluuerskannede tmheethcoodrrreeslpieosnodnintghetharsessuhmopldtiovnaltuheast(CwAitLh,Sin= 1). The kn

aiinsccrsoairnmdkaipnnclgyemwaoitrfheu-ltnihkceetliyporrnoebloeavbfailntihttyed-rolacenunkmginetgnhtspsrobinefcfiotprhleee,lesssuisdc-lceiekseedloysf the triangl

rwelietvhantthedopcouminetnstsa.sAvseartciconesse.qHueonwcee, vtheer,gaasinwcuersvhe opwloti-n Section IV

twawdinnirthdaghiwnlreheenaicgraMfh-lzrl eeosvrnlmoeogrpsseeluiordsp(eicer.aaelooln.n,kscc"eeomilsnnyaeatriasagnsripulnuypmaoalreuoldplsxrrteeifomclueisvnbaiaoectnnettgi"soe)dnncoaecustru,armtvlilhtyaeentdcutooosruneehtvssaeefvxtone,,rotofwfloinrke

dat we

bfoeerntrheetrinevoeids.y online data sets typical of computing systems.

poAinEntWicdoerMarel sAgpao.inndTcuihnrgevetowEtoWhueldMrahnAakveaastlpowppherioc1ha.0cauhllndtiuolcsauenmsinenfltetesccthhioanndiques simila

btoeenthroetsreieveemd,palondyesdlopbey0.B0 othlleirnegafeterr.BaAnndsact[u1a5l ]gaainnd Geometri

cisMantulargonpvodvreoraiibtnrtydahgpb.miiclSAiatultyvlphyeparraodtansigvewkeeitrnehggeaa,ustlsgrtfeaorhnoreidmisotrhembttmhraiefsesaevciaddtfloeoaromrsln,edctauhthnheoadedntoagwmeelneirmoetdihisateyoabtdtelgeiocoonttlldioosogny

[16]. Th describe

abcyhieAvelb7r0e%chrteceatll aaln.din70%thepirrecwisioonrkatosnompearrtainakl rb,aarsriiesrs [3], whic

tbdyyepriecixvahleasfuosrftrimvoeomdmerpannruecavlalisorseiufivseierwsw[o1[11r9k],].oorTnahseMmsOliogpNhetE,bTuepa[ct1oh7iet]vh.eadEt WMA is a roannlkin(seloapleg<orr)i,twhomuldthbaet 0u.s7e, santdwtoheexslpopoenaefntetiratlhlyatwraenikghted movin

(asvloeprea>gre)sw. ouTlhdeapfiprrosatchE, WbutMnoAt,eqcuaalll0e.dFoarrsrm,alilsvauluseesd to smoot wotshfeoBeRcuaoldsineneddpx|CopuEnet|,cWotwdutMerahteewaAx"o,ps,uelwloradpierheenrixcrcvpeaheatwicortii"t,sshlkonv=epoieenepssw-llpoosppue0eetbd< >.r0larri,ccaadksnad1tohaff.osoesrtthtsa,elalwarRervioevwbrae-algteimdeevs.iatTioh

sferrovmed athrart,foarndR is 1a0n0,estima6t.0e (owfitthhesuivtaabrileanscmeooitnh-arrival times

irtFtnaehiglrring)aeeabwstlihallmiysoty,ealtdtthghhoooeadostf.decaWiontrmwedripfoco+aarrvtmeo4adreld·uofaaftevhhrsoeirrgaahvhrbyaerlpyerouc,ttasholwele,dtshhiasaictcohtthhiaeacvctorhientmihepgveprrseeeuedscttaebehlnylraetatssnhmhdet-haeximmauxmimwuam

oalmdsowuenret uonfivetrimsale; thtoat wthaeistamfoerthtrheeshonldexwtoupldoiwnotrktoforarrive. If th aqpuoweinindttleyavurasrreiidvetefyosrdaoafuttareseermtst,phiiirnsiccaltulhderivenasglhutaohtleidon,teo(nsretehthaTteabwtleehr1se)u.sbhseo-ld is reache 4wim.1itphoorNutatonistseeeaitAntrgbibatuthteeemnoeefnxttthiasrraivlgaol,riEthWmMisAthdaetclEarWesMaAkndeoee.sOnno edxiiIrsfetwsceatlnwyienrrfleeeptcootisrottnopwpoahitnettrhe1e<tmhiien<imksnusmeuecrhapntohkianst,tsiusc--h6,tiwhtaetowtnohlueyrldedetermines i aalmkonsetecehrtaasinblyeestnoppapsresmedat.uAreslyadureestuolcth, aEnWce.MMAoreiosvoern, ly applicabl

teihnffiosratnnaa¨isovnealaifnupnpecrotsiaeocnthtionwfgot.uhled

entail size of

quadratic computational the collection. To avoid

both eventualities, we evaluate  only at values of s arising

forfombattchheebsaitschpersopofodrtIoiIcoIun.maleKtnotNslEosegEle|DCctL|e,EdabsAytLhBeGMvOIa.RlTuIehTseHonfMums baerer

sfeewpKaroanfteetedhdeblyceaaninsdeibxdpaatoseneedvnatoliuanellsythfionercrnseoawstiiinollgnbintethevarivtaabtlh.leeR, eeplvaoetininvetblsyy of maximum cchuarnvcae.tuArenyinreasidduaatlasesqeut--enttihale-teksntienegsb--iasarise oaffpspetrobxyima ately the se coofnFpsoerorevinaactthisvevinaclhuaoeiccouef rsov,fewthetrheeavsahtloaulardetefolor cata. lonmlyaoxniemia, cihfotshene ucsu- rve is rotate ingdeaggreeoemsetcrliocc"kknweies-deeatebcotiuont "(xalmgoinri,tyhmmin[1)5t],hriloluusgtrhattehde line forme by the points (xmin, ymin) and (xmax, ymax). We choose this lin because we want to preserve the overall behavior of the dat

78 set--using a line of best fit, for example, risks cutting off th end points due to a higher concentration of points in the middl

of the curve. After rotating about this line, the local maxima--

Collection
At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05

Target Method

Reliability Recall Effort

1.00

0.91

44,079

1.00

0.92 119,644

1.00

0.89

14,440

0.96

0.88

83,412

0.98

0.93 133,788

1.00

0.92 174,415

0.89

0.94 169,907

1.00

0.92 155,405

Knee Method

Reliability Recall Effort

0.93

0.93

5,244

1.00

0.98 172,774

1.00

0.97 19,387

0.99

0.94 60,645

1.00

0.99

3,857

0.98

0.96 153,638

1.00

1.00

7,575

1.00

0.96

8,444

Budget Method
Reliability Recall Effort
0.97 0.97 43,896 1.00 0.98 172,774 1.00 0.97 19,418 0.99 0.94 70,601 1.00 1.00 143,798 1.00 0.99 190,671 1.00 1.00 162,673 1.00 1.00 134,719

Table 2: Reliability, Mean Recall, and Mean Effort for the Target, Knee, and Budget Methods.

in Figure 1. We draw a line from the origin to the recall achieved at rank s, and compute the maximum perpendicular distance from this line to the gain curve. Our candidate value of i is the projection to the x-axis of the intersection between the perpendicular and the gain curve. Our rationale in choosing this point was that it would correctly choose the inflection point for an ideal curve, and would avoid anomalies associated with points very close to the origin or to rank s, while capturing our intuitive notion of a genuine tipping point.
We calculated the slope ratio as:

|{d|rank(d)irel(d)}|

=

i 1+|{d|i<rank(d)s}|

.

s-i

Smoothing was accomplished by adding 1 to the number of relevant documents beyond the knee. This choice avoided the singularity of no relevant documents beyond the knee, and generally penalized situations in which the chosen inflection point was close to s. No smoothing was applied to the numerator, as we were not concerned with occasional underestimates.

4.2 Adjustment for Low Prevalence

The case of R 100 is more problematic. Any correction for small R faces a dual problem:

1. the stopping procedure has no knowledge of the value of R, other than what can be estimated through relevance feedback from retrieved documents; and,

2. even if it were known that R was small, the sparsity of relevant documents compromises the reliability of our slope-ratio calculation.

The knee method relies entirely on the slope-ratio test, adjusted to compensate for low R. Initial tuning on the training collections from the TREC 2015 Total Recall Track indicated that a fixed lower bound  on the rank at which to stop, might be effective. For our submission to the TREC 2015 Total Recall At Home task, we conducted a parameter sweep of six combinations of   {100, 1000} and   {3, 6, 10}. Our results showed that combinations involving  = 100 or  = 3 were unreliable, and we eliminated them from further consideration. Unsurprisingly, the combination of  = 1000,  = 10 proved most reliable, achieving the recall target for 29 of 30 topics (reliability = 0.97 [0.830 - 0.999 95% c.i.]).
We observed that recall and reliability appeared to be lower for smaller R, while effort (especially for  = 10) appeared to be disproportionately higher for large R. This observation led us to seek more reliable methods for small R,

and to choose  = 6 for large R. To aid in this endeavor, we used a non-public dataset consisting of about 300,000 documents reviewed by attorneys and labeled according to 63 criteria, with R ranging from 5 to 164, 000 (median 431). Based on tuning experiments using this dataset, we calibrated the slope-ratio cutoff as a function of relret, the number of relevant documents retrieved at any given rank:

 = 156 - min(relret, 150) .

In other words, we set the threshold for the slope ratio to be 150 when no relevant documents have been retrieved, 6 whenever at least 150 relevant documents have been retrieved, and use linear interpolation between these values.
We further observed that with this adjustment, the choice of  = 100 versus  = 1000 became less critical. The lower value occasionally achieved lower effort than the higher value, and occasionally failed when the higher value did not. We chose to retain the value of  = 1000 from our earlier experiments.

4.3 Effort Adjustment

A variant of our knee method--the budget method --

adjusts for small R by stopping only when a review budget

comparable to that of the target method has been expended,

and the slope-ratio test   6 is also satisfied. This ad-

justment substantially delays termination for small R, thus

ensuring reliability.

The approach is predicated on the hypothesis that the

supplemental review effort entailed by the target method

would be better spent reviewing more documents retrieved

by CAL. The target method entails the supplemental review

of

about

10|C| R

documents

in

order

to

find

10

relevant

ones.

According to the probability-ranking principle, we would ex-

pect CAL to find more relevant documents than random

selection,

for

any

level

of

effort,

up

to

and

beyond

10|C| R

.

While the supplemental documents retrieved by the target

method provide a statistical estimate of R, the documents

retrieved by CAL provide a lower bound for R, and therefore

an upper bound for the expected effort entailed by the target

method. At the outset, this upper bound is loose, but as the

review progresses, it tightens. The budget method retrieves

documents using CAL until review effort exceeds this upper

bound and   6.0, or until 0.75|C| documents are retrieved.

For small R, the budget determines the stopping point.

For large R, enough relevant documents will likely be discov-

ered to bound the review budget to an insubstantial fraction

of R, and the slope-ratio test will determine when to stop.

In any event, the review stops at 0.75|C|. This final cutoff

is predicated on the probability-ranking principle: random

79

TREC 2015 Total Recall Track -- Athome Tasks
1000000 1

Effort (Documents Reviewed)

0.8 100000
0.6

Recall

0.4 10000
0.2

0

1000

23 26 66 76 113 179 182 227 252 255 265 506 589 629 661 1111 1256 1624 2036 2094 2299 2375 2375 3635 4542 4805 5725 5836 9517 17135

Number of Relevant Documents in Collection
Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method
TREC 2015 Total Recall Track -- Athome Tasks 1000000
1

Effort (Documents Reviewed)

0.8 100000
0.6

Recall

0.4 10000
0.2

0

1000

23 26 66 76 113 179 182 227 252 255 265 506 589 629 661 1111 1256 1624 2036 2094 2299 2375 2375 3635 4542 4805 5725 5836 9517 17135

Number of Relevant Documents in Collection
Effort for Budget Method Effort for Target Method Recall for Budget Method Recall for Target Method

Figure 2: TREC 2015 Total Recall At Home Collection.

selection of 75% of the collection would, with high probability, achieve 70% recall; the top-ranked 75% should achieve even higher recall.
5. EXPERIMENTS
Testing the reliability of our stopping methods occasioned the use of "fully assessed" test collections, with a large number of topics and documents, where by "fully assessed," we mean that the pooling method, ISJ, or a rule base was used, and the resulting documents were labeled by a human assessor. From the limited number of collections that met these criteria, we selected the TREC 2015 Total Recall Track collections, the Reuters RCV1-v2 news dataset, the TREC 2002 Filtering Track collections, and the TREC 2004 and 2005 Robust Track collections, as detailed in Table 1. We used our Total Recall At Home participation to conduct an initial parameter sweep with six combinations, as well as final testing; the other datasets were used solely for testing.
The first phase of our experiments took place within the context of the TREC 2015 Total Recall Track, which had three distinct phases: training, At Home, and Sandbox. We conducted our initial development and tuning during the training phase, and submitted the knee method for evaluation in the At Home phase, but not the Sandbox phase. We captured the sequence of documents retrieved by BMI in both the At Home and Sandbox phases, and later used them

Effort (Documents Reviewed)

Recall

TREC 2015 Total Recall Track -- Kaine Collection

1

1000000

0.8
100000 0.6

0.4 10000
0.2

0

1000

14341 20083 131698 166118

Number of Relevant Documents in Collection
Surplus Effort for Knee Method Surplus Effort for Target Method
Recall for Knee Method Recall for Target Method

TREC 2015 Total Recall Track -- ICD-9 Codes

1

1000000

0.8
100000 0.6

Effort (Documents Reviewed)

Recall

0.4 10000
0.2

0

1000

179 2141 2575 3452 3851 5066 5140 5867 6113 6815 7806 8025 8678 8724 11081 11222 15046 16780 19095

Number of Relevant Documents in Collection
Surplus Effort for Knee Method Surplus Effort for Target Method
Recall for Knee Method Recall for Target Method

Figure 3: TREC 2015 Total Recall Sandbox Collections.

to simulate the effect of the stopping methods whose results are presented here. After conducting further tuning on our non-public collection of 300,000 documents with 63 topics, we froze all parameters, and ran BMI on the other evaluation datasets, capturing the order in which the documents were retrieved. We then simulated our stopping methods by applying them to the ranking.
Summary results showing reliability, average recall, and average effort for all collections are shown in Table 2. The overall reliability of the target method, the knee method, and the budget method are substantially higher than the target of 0.95. Considering reliability, alone, there is little to choose among the methods; but the recall achieved by the knee and budget methods is substantially higher, while the effort expended by the knee method is, for some datasets, dramatically lower.
As illustrated in Figures 2 through 6, R (the number of relevant documents) appears to be the principal determinant of effort. For small R, effort for the target and budget methods approaches the size of the collection, while effort for the knee method, with one notable exception, generally diminishes with R, approaching the floor of  = 1000 that we chose for this study. On the other hand, for large R, the effort for all methods appears proportional to R.
The top panel of Figure 2 compares recall and effort for the knee and target methods, for each topic in the At Home col-

80

Effort (Documents Reviewed)

Recall

Reuters RCV1-v2 Subject Categories

1

1000000

0.8
100000 0.6

0.4 10000
0.2

0

1000

5 844 2107 2636 4835 7406 11878 21280 32153 47708 204820

Number of Relevant Documents in Collection Surplus Effort for Knee Method
Surplus Effort for Target Method Recall for Knee Method
Recall for Target Method
Figure 4: Reuters RCV1-v2 Subject Codes.

lection, ordered by R. We see that 28 of the 30 recall points for the knee method (shown by the green curve) fall above 0.7, indicating reliability of 0.93, while all of the points for the target method (shown by the red curve) fall above 0.7, indicating reliability of 1.00 for this collection. We also see that the most of the recall points for the knee method fall above those for the target method, indicating higher median recall, and the (signed) area between the curves is positive, indicating higher mean recall. Per-topic effort is shown as a bar graph on a logarithmic scale spanning three orders of magnitude. For small R, the knee method entails about 100 times less effort than the target method, while for large R, the effort is comparable.
The bottom panel of Figure 2 follows the same format, comparing the budget method (shown in blue) to the target method (shown in red). While the budget method achieves higher recall than the target method for nearly all topics, that superiority is not reflected in higher reliability. Effort for the two methods is very similar. The same observations apply to the results for the other collections: For low R, recall for the budget method exceeds that of the target method, while effort is indistinguishable; for large R, recall and effort are indistinguishable from the knee method. Both methods are reliable.
For brevity, we show graphical results comparing only the knee and target methods for the other collections. Tabular results for all methods are presented in Table 2.
Figure 3 shows results for the Sandbox task of the TREC 2015 Total Recall Track, which was notable in that participants had no prior access to the datasets or the topics, and their retrieval systems had to run fully autonomously. The top panel shows our results for the Kaine collection, which consisted of about 400,000 documents from Tim Kaine's eight-year tenure as Governor of Virginia. These documents had been previously reviewed and labeled by the archivist at the Library of Virginia according to four statutory categories: "record" (versus "non-record"), "open record," "restricted record," and "pertaining to the Virginia Tech shooting." Two of the topics had moderately high R  104, and two had very high R  105. For all topics, the knee method achieved higher recall at the expense of somewhat higher effort. The bottom panel shows our results for the MIMIC II collection, which consisted of about 30,000 medical records

Recall 12 23 33 49 59 72 91 112 168 321 610
Effort (Documents Reviewed)

TREC 2002 Filtering Track -- Assessor Topics
1000000 1

0.8 100000
0.6

0.4 10000
0.2

0

1000

Number of Relevant Documents in Collection
Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method
TREC 2002 Filtering Track -- Intersection Topics 1000000
1

0.8 100000
0.6

0.4 10000
0.2

0

1000

Number of Relevant Documents in Collection
Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method

Figure 5: TREC 2002 Filtering Track Collections.

Recall 21 28 39 45 50 59 78 89 137 218 349
Effort (Documents Reviewed)

collected from a hospital intensive care unit. The documents consisted of nurses' notes, radiology reports, and discharge summaries. The "topics" consisted of ICD-9 diagnostic codes extracted from non-textual database records. With one exception (R = 179), all topics had moderately high R. The knee method generally achieved higher recall than the target method, at the expense of somewhat higher effort for most topics.
Figure 4 shows the results for the RCV1-v2 dataset, using the subject categories and descriptions published with the dataset as topics [11]. Over a very wide range 101 R 105, we observe a familiar pattern: The knee method has somewhat higher recall and lower variance, with dramatically lower effort, for small R.
Figure 5 shows results for two sets of topics created for the TREC 2002 Filtering Track. The top panel shows results for topics that were created and assessed by NIST for the track. All topics had low R 610; the majority had very low R  100. For all topics, including those with the lowest R 100, the knee method achieved near-perfect recall. Recall for the target method showed much higher variance, suggesting that its reliability is actually lower. The knee method entails order(s) of magnitude less effort. The lower panel shows results for intersection topics, each of which was the conjunction of two RCV1-v2 subject categories. If rel1(d) and rel2(d) indicate relevance for two RCV1-v2 topics, rel1(d)  rel2(d) indicates relevance for the intersection

81

TREC 2004 Robust Track

1

1000000

0.8
100000 0.6

Recall 6 9 11 14 17 19 22 27 28 33 35 39 46 50 57 66 71 80 94 113 130 161 194 254
Effort (Documents Reviewed)

0.4 10000
0.2

0

1000

Number of Relevant Documents in Collection
Surplus Effort for Knee Method Surplus Effort for Target Method
Recall for Knee Method Recall for Target Method
TREC 2005 Robust Track 1000000
1

Recall 9 21 32 42 52 60 65 71 83 88 97 109 111 121 127 151 153 163 165 177 183 232 242 280 356
Effort (Documents Reviewed)

0.8 100000
0.6

0.4 10000
0.2

0

1000

Number of Relevant Documents in Collection
Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method

Figure 6: TREC 2004-2005 Robust Track Collections.

topic. The intersection topics were reported as a failed experiment [17], since no system achieved reasonable results on them. The results show that, while the effort to achieve high recall for these anomalous topics is inordinately large, our stopping methods are reliable.
Figure 6 shows results for the TREC 2004 and 2005 Robust tracks. In 2004, the Robust Track aggregated 150 topics developed for the TREC 6, TREC 7, and TREC 8 Ad-Hoc tasks, 50 topics developed for the 2003 Robust Track, and 49 new topics, for a total of 249 topics. For 2005, 50 of these topics--those deemed to be "difficult"--were reprised with a new dataset. The top panel reports our results for 2004; the bottom for 2005. The results further confirm that the target and knee methods both achieve high reliability, while the knee method entails dramatically less effort.
6. DIMINISHING LOSS
As evidenced by the results above, reliability does not capture certain important aspects of effectiveness or efficiency. Moreover, empirical measurements of reliability lack statistical power, while parametric estimates depend on assumptions regarding the distribution of recall values. Since the choices of acceptable recall and acceptable reliability are both somewhat arbitrary, bias due to incorrect distributional assumptions may be of little consequence. We suggest that reporting the mean µ and standard deviation  of recall

conveys more useful information, if not a provably accurate estimate of reliability. Such an estimate would have to be compared to one or more tacit thresholds to determine the reliability of the method; for example, assuming normality, any pair of µ and  such that µ - 1.64  0.70 would be 95% reliable. More generally, the value of Q = µ - 1.64 is a quantitative measure of quality, which may be used to determine the threshold level of acceptable recall for which 95% reliability may be obtained. Alternatively, by substituting the appropriate z-score in place of 1.64, a threshold of reliability different from 95% may be tested.
We suggest that reliability and recall should be supplanted by quality estimates based on loss functions, of which recall and reliability are special cases. We define Q = 1 - loss, where loss is the mean value of a loss function over all topics. If
loss = 1 - recall , Q = recall ; if,

loss = 0 (recall  0.7) , Q = reliability . 1 (recall < 0.7)

A quadratic loss function such as:

lossr = (1 - recall)2

(4)

captures the desirability of consistently high recall, subsuming the roles of µ and  in the previous discussion. Our aspirational goal is to achieve 100% recall. Any shortfall is penalized, and larger shortfalls are penalized more heavily.
Quadratic loss further generalizes to other aspects of quality, such as graded relevance, facet relevance [6], and efficiency. For example, let a1, a2, . . ., an be categories of relevance, and rela(d) be the indicator function for category a. Define:

recalla

=

|{d



C|relret(d)  rela(d)}| |{d  C|rela(d)}|

lossa = (1 - recalla)2

n

n

loss = ilossai , where 1 = i .

(5)

i=i

i=1

The

choice

of

weights

i

is

not

critical;

the

value

i

=

1 n

for

all i will often suffice, as it rewards consistent recall over

all categories, with the effect that documents belonging to

rarer categories are afforded more influence.

Review effort may also be modeled as a category of loss,

thus quantifying the notion of "reasonable effort." For the

problem as we have framed it, an ideal method would entail

effort = R. From the presentation of results in the TREC

2015 Total Recall Track Overview [13], we observe that a

reasonable effort might entail effort = aR + b, where a  1

represents effort proportional to sunk review cost, and b 

0 represents fixed overhead. We suggest that a  2 and

b  1000 represent reasonable effort to achieve recall  0.70

with 95% reliability. The use of a quadratic loss replaces the

a and b thresholds by a soft target:

losse =

b2 |C |

ef f ort R+b

2
.

(6)

losse may be used to measure efficiency in its own right, or treated as a category loss in (5).

82

Collection
At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05

Target Method

lossr
0.0132 0.0815 0.1229 0.1475 0.1011 0.1057 0.0870 0.1141

losse 0.0090 0.0016
0.0734 0.0883 0.2110 0.2499 0.4136 0.2368

lossre 0.0111 0.0577 0.1012 0.1216 0.1654 0.1919 0.2989 0.1858

Knee Method

lossr 0.0197 0.0252
0.0516 0.0947 0.0181 0.0818 0.0430 0.0570

losse 0.0000 0.0025 0.0862 0.0154 0.0079 0.2740 0.0481 0.0265

lossre 0.0099 0.0179 0.0710 0.0678 0.0140 0.2022 0.0456 0.0445

Budget Method

lossr 0.0056 0.0252 0.0516 0.0824 0.0015 0.0159 0.0025 0.0087

losse
0.0108 0.0025 0.0866 0.0795 0.2278 0.2947 0.3865 0.1843

lossre
0.0082
0.0179 0.0712 0.0809 0.1611 0.2087 0.2733 0.1305

Table 3: Root Mean Loss for Relevance, Effort, and Combined.

Target Method

lossr

lossh

0.0837 0.0504

Knee Method

lossr

lossh

0.0134 0.0021

Budget Method

lossr 0.0007

lossh 0.0011

Table 4: Root Mean Loss for Relevance and High Relevance.

In Table 3, we report, for each collection, the root mean
loss ( loss) over all topics for relevance loss as defined in (4); effort loss as defined in (6); as well as their unweighted average, lossre = 0.5 · lossr + 0.5 · losse. The results show conclusively the superiority of the budget method in terms of lossr. They show the general superiority of the knee method in terms of losse, while calling to our attention three collections where the target method is more efficient. On inspection, we see that two of the three collections have exclusively or nearly exclusively topics with high prevalence. We further see that that the the target method's narrow margin of superiority in terms of lossr is offset by a wide margin of inferiority in losse, as reflected in lossre. For the intersection collection, no system achieved acceptable losse.
The bottom line is that the quality loss results support our qualitative observation that the knee method affords the best balance between consistently high recall and consistently low effort; the budget method provides consistently higher recall at the expense of disproportionate effort for topics with few relevant documents; the target method, while provably reliable, yields empirical results that are generally inferior to the knee and budget methods.
To illustrate the use of quality loss for graded relevance, we used a subset of 84 topics from Robust-04, for which relevance assessments were available for the categories "highly
relevant" and "relevant." Table 4 shows lossr and lossh for these categories, respectively. The knee and target methods have lower lossh, than lossr, indicating they retrieve highly relevant documents more consistently than merely relevant documents. The budget method shows the opposite effect, but even so, is markedly superior to the target and knee methods. While we cannot draw any firm conclusions from this small experiment, the results do not support the proposition that TAR methods achieve high recall by "bulking up" on marginal documents at the expense of important ones (cf. [12]).
7. DISCUSSION
To our knowledge, the target method is the first provably reliable method for TAR. The commonly used frequentist acceptance test (see [1, 9]) offers a p-value or confidence level which is a measure of the reliability of the test, not

the reliability of the TAR method, not the probability that a given result is acceptable, and not the probability that a TAR method will pass the acceptance test. In eDiscovery, it is common to calculate a frequentist recall estimate, with a 5% margin of error and 95% confidence, and deem the result acceptable if the estimate exceeds 75%. Calculating such an estimate requires a sample of about 385 random relevant documents, entailing 38.5 times as much surplus effort as the target method.
Our proof of reliability does not require that the target sample T be chosen at the outset, as long as it is independent of the retrieval method. The target method could be used as an acceptance test, such that the consequence of failing the test would be to continue to retrieve documents without knowledge of T , until all the documents in T are retrieved.
Over test collections like the ones used in this study, there can be little doubt (p  0.00) that the knee and budget methods are reliable, that the budget method is more reliable than both the knee and target methods, and that the knee method is the most efficient. As with any empirical work, the test collections constitute convenience samples and ongoing research is necessary to characterize the scope of TAR tasks to which our results may be generalized.
The target method is reliable regardless of the underlying review method; however, if the underlying method uses a human in the loop to formulate queries or to influence the selection of documents in any way, that human must be isolated from any knowledge of T . The simplest approach to accomplish this goal might be to complete all such human intervention before drawing T , and to rely on fully automated document selection thereafter. An alternative would be to establish an "information barrier" between those who draw T and those who conduct the search.
This work establishes the reliability of the knee and budget methods as applied to BMI. It remains to be determined how well these approaches would work--possibly with different tuning parameters--for other CAL methods, including hybrid systems in which a human is afforded influence in the selection of documents for review. It is not obvious how to adapt the knee or budget method to SPL or SAL, for which an essential question is when to stop training.
The target method, by design, targets less than 100% recall. It could be modified to continue past the point at which

83

the last document in T is retrieved, thereby expending additional effort to increase the probability of achieving 100% recall. One might, for example, extrapolate from the distribution of rank(d  T ). The knee method, on the other hand, does target 100% recall, and only incidentally optimizes reliability. It appears that loss functions better characterize the tension among consistency, effectiveness, and efficiency, as compared to goal-post methods. Regardless of which measure is chosen for evaluation, systems should be tuned to optimize their suitability for their intended purpose, not the measure itself (cf. [22]).
8. CONCLUSIONS
Reservations about the effectiveness and reliability of TAR have impeded its adoption for eDiscovery and other highrecall retrieval tasks. A primary area of concern has centered on the issue of "when to stop," or knowing with reasonable certainty--and being able to show an adversary or the court--that a particular TAR effort has identified an acceptable amount of relevant information. Many approaches to validation in common use today are simply invalid, or require disproportionate effort compared to the information they yield, and are often misunderstood and misapplied [9, 16].
We offer a method to determine when to stop that is guaranteed to be reliable, for the price of reviewing a number of random documents that is an order of magnitude less than acceptance tests that estimate recall, but neither determine when to stop nor guarantee reliability. We provide two other methods that entail no effort beyond that required by the underlying TAR method and, while not providing a guarantee of reliability, consistently demonstrate better reliability, and better recall, when evaluated on eight test collections, comprising 555 topics and 4.5M documents. Of particular interest is the knee method which, in contrast to the other methods, is demonstrated to be reliable and efficient when the collection contains few relevant documents.
While our primary results are demonstrated using measures derived from traditional goal-post methods--binary relevance, a recall threshold, and a reliability floor--we describe how loss functions may be formulated to capture the tension among consistency, degrees of relevance, facets of relevance, and efficiency. We apply these formulae to show insights into our results that might not have been readily apparent from the goal-post measures.
9. REFERENCES
[1] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. Oard. Towards minimizing the annotation cost of certified text classification. In SIGIR 2013.
[2] D. Blair and M. E. Maron. An evaluation of retrieval effectiveness for a full-text document-retrieval system. Commun. ACM, 28(3):289­299, 1985.
[3] D. C. Blair. Stairs redux: Thoughts on the stairs evaluation, ten years after. J. Am. Soc. Inf. Sci., 47(1):4­22, Jan. 1996.
[4] G. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In TREC 2009.

[5] G. V. Cormack and M. R. Grossman. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In SIGIR 2014.
[6] G. V. Cormack and M. R. Grossman. Multi-faceted recall of continuous active learning for technology-assisted review. In SIGIR 2015.
[7] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv:1504.06868, 2015.
[8] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR 1998.
[9] M. R. Grossman and G. V. Cormack. Comments on "The implications of Rule 26(g) on the use of technology-assisted review". Fed. Cts. L. Rev., 7:285­312, 2014.
[10] C. Lefebvre, E. Manheimer, and J. Glanville. Searching for studies. Cochrane Handbook for Systematic Reviews of Interventions, 2008.
[11] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361­397, 2004.
[12] D. Remus and F. S. Levy. Can robots be lawyers? Computers, lawyers, and the practice of law. http://dx.doi.org/10.2139/ssrn.2701092, 2015.
[13] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. Notebook Draft TREC 2015 Total Recall Track Overview. In TREC 2015.
[14] M. Sanderson and H. Joho. Forming test collections with no system pooling. In SIGIR 2004.
[15] V. Satop¨a¨a, J. Albrecht, D. Irwin, and B. Raghavan. Finding a "kneedle" in a haystack: Detecting knee points in system behavior. In ICDCSW 2011.
[16] K. Schieneman and T. Gricks. The implications of Rule 26(g) on the use of technology-assisted review. Fed. Cts. L. Rev., 7:239­274, 2013.
[17] I. Soboroff and S. Robertson. Building a filtering test collection for TREC 2002. In SIGIR 2003.
[18] G. Taguchi. Introduction to Quality Engineering: Designing Quality Into Products and Processes. 1986.
[19] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, 2000.
[20] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, pages 143­170. Springer, 2002.
[21] E. M. Voorhees and D. K. Harman. The Text REtrieval Conference. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, pages 3­19. MIT Press, 2005.
[22] E. Yilmaz and S. Robertson. On the choice of effectiveness measures for learning to rank. Information Retrieval, 13(3):271­290, 2010.
[23] J. Zobel, A. Moffat, and L. A. Park. Against recall: Is it persistence, cardinality, density, coverage, or totality? In ACM SIGIR Forum, volume 43, pages 3­8. ACM, 2009.

84

Axiomatic Analysis for Improving the Log-Logistic Feedback Model

Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery

School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Iran

Center for Intelligent Information Retrieval, College of Information and Computer Sciences,

University of Massachusetts Amherst, MA 01003

{ali.montazer,shakery}@ut.ac.ir

zamani@cs.umass.edu

ABSTRACT
Pseudo-relevance feedback (PRF) has been proven to be an effective query expansion strategy to improve retrieval performance. Several PRF methods have so far been proposed for many retrieval models. Recent theoretical studies of PRF methods show that most of the PRF methods do not satisfy all necessary constraints. Among all, the log-logistic model has been shown to be an effective method that satisfies most of the PRF constraints. In this paper, we first introduce two new PRF constraints. We further analyze the log-logistic feedback model and show that it does not satisfy these two constraints as well as the previously proposed "relevance effect" constraint. We then modify the log-logistic formulation to satisfy all these constraints. Experiments on three TREC newswire and web collections demonstrate that the proposed modification significantly outperforms the original log-logistic model, in all collections.
CCS Concepts
·Information systems  Query representation; Query reformulation;
Keywords
Pseudo-relevance feedback; axiomatic analysis; theoretical analysis; query expansion; semantic similarity
1. INTRODUCTION
Pseudo-relevance feedback (PRF) refers to a query expansion strategy to address the vocabulary mismatch problem in information retrieval (IR). PRF assumes that a number of top-retrieved documents are relevant to the initial query. Based on this assumption, it updates the query model using these pseudo-relevant documents to improve the retrieval performance. PRF has been shown to be highly effective in many retrieval models [1, 5, 8, 10].
Several PRF models with different assumptions and formulations have so far been proposed. Clinchant and Gaussi-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914768

er [2] theoretically analyzed a number of effective PRF models. To this end, they proposed five constraints (axioms) for PRF models and showed that the log-logistic feedback model [1] is the only PRF model (among the studied ones) that satisfies all the constraints. They also showed that its performance is superior to the other PRF methods, including the mixture model [10] and the geometric relevance model [9]. Effectiveness of the log-logistic model motivates us, in this paper, to study this state-of-the-art PRF model.
Recently, Pal et al. [8] proposed a sixth constraint for PRF models to improve the PRF performance in the divergence from randomness framework. This constraint, which is called "relevance effect", indicates that the terms in the feedback documents with high relevance scores (i.e., relevance of document to the initial query) should have higher weights in the feedback model compared to those with exactly similar statistics, but appear in the documents with lower relevance scores. Formally writing, if a term w occurs in two documents d1, d2  F (F denotes the set of feedback documents) such that d1 is more relevant to the initial query than d2. Then, we can say that the feedback weight of w given the F - {d1} feedback documents is lower than the weight of the same word in the F - {d2} feedback documents [8]. It can be shown that the log-logistic feedback model does not satisfy the relevance effect constraint.
In this paper, we propose two additional constraints for PRF models. The first constraint considers the semantic similarity of feedback terms to the initial query. Although previous work, such as [4], proposed similar constraints for retrieval models, to the best of our knowledge, it is the first time to study a semantic-related constraint for the PRF task. The second constraint indicates that the weight of each term w in the feedback model not only depends on the distribution of w in the feedback documents, but is also related to the distribution of the other terms in those documents. We further show that the log-logistic model does not satisfy the two proposed constraints. We then propose a modification to the log-logistic feedback model to satisfy the proposed constraints as well as the relevance effect constraint [8].
We evaluate the modified log-logistic model using three standard TREC collections: AP (Associated Press 1988-89), Robust (TREC 2004 Robust track), and WT10g (TREC 910 Web track). The experimental results demonstrate that the proposed method significantly outperforms the original log-logistic feedback model in all collections. The proposed method is also shown to be more robust than the original log-logistic model, especially in the web collection.

765

2. METHODOLOGY
In this section, we introduce two constraints that (pseudo) relevance feedback methods should satisfy (in addition to those proposed in [2, 8]). We further analyze the log-logistic model, a state-of-the-art feedback model, and figure out that this model does not satisfy the proposed constraints as well as the "relevance effect" constraint introduced in [8]. Based on these observations, we modify the log-logistic feedback model to satisfy all the constraints.
We first introduce our notation. Let F W (w; F, Pw, Q) be the feedback weight function that assigns a real-value weight to each feedback term w for a given query Q. F and Pw respectively denote the set of feedback documents for the query Q and a set of term-dependent parameters. For simplicity, we henceforth use F W (w). In the following equations, T F and IDF denote term frequency and inverse document frequency, respectively. The notation | · | is also used for query/document length or size of a given set.

2.1 Constraints
In this subsection, we introduce two constraints for feedback models.
[Semantic effect] Let Q be a single-term query (i.e., Q = {q}), w1 and w2 be two terms such that IDF (w1) = IDF (w2), D  F : T F (w1, D) = T F (w2, D), and
sem(q, w1) < sem(q, w2)
where sem(·, ·) denotes the semantic similarity of the given terms. Then, we can say:

F W (w1) < F W (w2)
The intuition behind this constraint is that the feedback terms should be semantically similar to the initial query.
[Distribution effect] Let w1 and w2 be two vocabulary terms such that T F (w1, D1) = T F (w2, D2), T F (w1, D2) = T F (w2, D1) = 0, and |D1| = |D2|, where D1 and D2 are two documents in the feedback set F . Also, assume that w1 and w2 do not occur in other feedback documents, and
U niqueT erms(D1) < U niqueT erms(D2)
where U niqueT erms(·) denotes the number of unique terms in the given document. Then, we can say:1

F W (w1) < F W (w2)
In other words, this constraint implies that for computing the feedback weight of a term w, the distribution of other terms in the feedback documents should also be considered.

2.2 Modifying the Log-Logistic Model
The feedback weight of each term w in the log-logistic feedback model [1] is computed as follows:

F W (w)

=

1 |F |

DF

F W (w, D)

=

1 |F |

DF

log(

t(w,

D) w

+

w

)

(1)

where w

=

Nw N

(Nw

and N

denote the number of docu-

ments in the collection that contain w and the total number

of documents in the collection, respectively), and t(w, D) =

T F (w,

D)

log(1

+

c

avgl |D|

)

(avgl

denotes

the

average

document

length and c is a free hyper-parameter). It is shown that

1The intuition behind this constraint comes from the definition of information in information theory literature.

the log-logistic model satisfies all the PRF constraints introduced in [2]. It can be easily proved that this model cannot satisfy the constraints proposed in this paper. In more detail, there is no semantic-related or relevance-related components in the log-logistic formulation and thus it cannot satisfy the proposed "semantic effect" and the "relevance effect" [8] constraints. In addition, the log-logistic formula does not consider the distribution of other terms in computing the weight of each term w, and thus it does not satisfy the "distribution effect" constraint.
To satisfy the "semantic effect" constraint, we modify the log-logistic feedback weight function as follows:

F Wsem(w)

=

F W (w)







1 |Q|

s(w, q) s(q, q)

(2)

qQ

where s(·, ·) denotes the semantic similarity between the given two terms. The parameter  controls the effect of semantic similarity in the feedback weight function. The semantic weighting component comes from the query-growth function, which was previously proposed by Fang and Zhai [4]. Note that in Equation (2), we can ignore the 1/|Q| term and the  parameter, since they are equal for all terms and the feedback weighting function will be normalized. Several methods have so far been proposed to incorporate semantic similarity of terms in various retrieval tasks. In this paper, we consider the mutual information as a basic semantic similarity metric to compute s(·, ·). The mutual information (MI) of two terms w and w is computed as follows:

I (Xw ,

Xw )

=

Xw ,Xw {0,1}

p(Xw ,

Xw )

log

p(Xw, Xw ) p(Xw)p(Xw )

where Xw and Xw are two binary random variables that represent the presence or absence of the terms w and w in each document. A simple way to compute the mutual information is to consider the whole collection; but, this choice may not be ideal for ambiguous terms. Another way is to compute the mutual information from the pseudo-relevant documents. However, the top-retrieved documents could be a biased corpus for this goal. Therefore, similar to [4], we extract the mutual information from a corpus containing the top m retrieved documents and r × m documents randomly selected from the collection, where r is a free parameter that controls the generality of mutual information scores.
To satisfy the "distribution effect" constraint, we re-define the function t(w, D) as follows:

t(w, D)

=

t(w, D)

log(

|D| ut(D)

)

(3)

where ut(D) denotes the number of unique terms in the document D. A similar approach for modifying the raw TF formula was previously used in [7].
To satisfy the "relevance effect" constraint, we re-define the function F W (w, D) (see Equation (1)) as follows:

F W (w, D) = F W (w, D)  RS(Q, D)

(4)

where RS(Q, D) denotes the relevance score of the document D to the query Q. This function can be computed using the relevance score of D in the first ranking phase in PRF. A similar idea was previously proposed by Lavrenko and Croft [5]. They used the query likelihood similarity as a posterior probability in the relevance models. Lv and

766

ID AP Robust
WT10g

Collection
Associated Press 88-89 TREC Disks 4 & 5 minus
Congressional Record
TREC Web Collection

Table 1: Collections statistics.
Queries (title only) TREC 1-3 Ad Hoc Track, topics 51-200
TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 9-10 Web Track, topics 451-550

#docs 165k 528k
1692k

doc length 287 254
399

#qrels 15,838 17,412
5931

Table 2: Performance of the proposed modifications and the baselines. Superscripts 0/1 denote that the MAP improvements over NoPRF/LL are statistically significant. The highest value in each column is marked in bold.

Method

AP MAP P@10 RI

Robust MAP P@10 RI

WT10g MAP P@10 RI

NoPRF LL
LL+Sem LL+Rel LL+Dis

0.2642
0.3385
0.34220 0.34250 0.33860

0.4260 0.4622
0.4702 0.4681 0.4671

­ 0.15
0.18 0.20 0.16

0.2490
0.2829
0.294001 0.289701 0.28310

0.4237 ­ 0.4393 0.33
0.4474 0.31 0.4490 0.35 0.4401 0.32

0.2080 0.2127
0.2247 0.228901 0.2194

0.3030 ­ 0.3187 0.08
0.3188 0.10 0.3289 0.17 0.3207 0.13

LL+All 0.344501 0.4722 0.20 0.297901 0.4486 0.36 0.230001 0.3177 0.19

Zhai [6] also used a similar technique to improve the divergence minimization feedback model [10].
Considering the aforementioned modifications, we can rewrite the log-logistic feedback weighting formula as follows:

F W (w)

=

1 |F |

DF

log(

t(w,

D) w

+

w

)



RS

(Q,

D)



s(w, q) s(q, q)

(5)

qQ

3. EXPERIMENTS

3.1 Experimental Setup
We used three standard TREC collections in our experiments: AP (Associated Press 1988-89), Robust (TREC Robust Track 2004 collection), and WT10g (TREC Web Track 2001-2002). The first two collections are newswire collections, and the third collection is a web collection with more noisy documents. The statistics of these datasets are reported in Table 1. We consider the title of topics as queries. All documents are stemmed using the Porter stemmer. Stopwords are removed in all the experiments. We used the standard INQUERY stopword list. All experiments were carried out using the Lemur toolkit2.
3.1.1 Parameter Setting
The number of feedback documents, the number of feedback terms, the feedback coefficient and the parameter that controls the generally of mutual information scores (parameter r) are set using 2-fold cross validation over each collection. We sweep the number of feedback documents and feedback terms between {10, 25, 50, 75, 100}, the feedback coefficient between {0, 0.1, · · · , 1}, and the parameter r between {2, 4, 6, 8, 10}.
3.1.2 Evaluation Metrics
To evaluate retrieval effectiveness, we use mean average precision (MAP) of the top-ranked 1000 documents as the
2http://lemurproject.org/

main evaluation metric. In addition, we also report the pre-

cision of the top 10 retrieved documents (P@10). Statisti-

cally significant differences of performance are determined

using the two-tailed paired t-test computed at a 95% confi-

dence level over average precision per query.

To evaluate the robustness of methods, we consider the ro-

bustness

index

(RI)

[3]

which

is

defined

as

N+ -N- N

,

where

N

denotes the number of queries and N+/N- shows the num-

ber of queries improved/decreased by the feedback method.3

The RI value is always in the [-1, 1] interval and the method

with higher value is more robust.

3.2 Results and Discussion
In this subsection, we first evaluate the proposed modifications to the log-logistic model. We further study the sensitivity of the proposed method to the free parameters.

3.2.1 Evaluating the Modified Log-Logistic Model
We consider two baselines: (1) the document retrieval method without feedback (NoPRF), and (2) the original loglogistic feedback model (LL). Although several other PRF methods have already been proposed, since in this paper, we propose a modification of the log-logistic model, we do not compare the proposed method with other existing PRF models.
To study the effect of each constraint in the retrieval performance, we modify the log-logistic model based on each constraint, separately. LL+Sem, LL+Rel, and LL+Dis denote the modified log-logistic model based on the "semantic effect", the "relevance effect", and the "distribution effect" constraints, respectively. We also modify the log-logistic model by considering all of these constraints (called LL+All) as introduced in Equation (5). The results obtained by the baselines and those achieved by the proposed modifications are reported in Table 2. According to this table, LL outperforms the NoPRF baseline in all cases, which shows the effectiveness of the log-logistic model. The improvements on the WT10g collection is lower than those on the AP and the Robust collections. This observation demonstrates that the
3To avoid the influence of very small average precision changes in the RI value, we only consider the improvements/losses higher than 10% (relatively).

767

0.32  Robust WT10g

0.32  Robust WT10g

0.30















0.30 









0.28

0.28

MAP MAP

0.26

0.26

0.24

0.24

0.22

0.22

25 50 75 100 125 150 175 200

2

4

6

8

# of feedback terms

r

Figure 1: Sensitivity of the proposed method to the number of feedback terms and the parameter r.

log-logistic model is less effective and robust in improving

the retrieval performance in the web collection, compared

to the newswire collections. LL+Sem and LL+Rel perform

better than LL in terms of MAP and P@10, in all collec-

tions. The MAP improvements are statistically significant

in many cases, especially in the LL+Rel method. Except

in one case (i.e., LL+Sem in Robust), both LL+Sem and

LL+Rel models are shown to be more robust than the LL

baseline. It is worth noting that we use very simple modifi-

cations to satisfy these two constraints, and thus using more

accurate methods to satisfy these constraints can potentially

improve the performance. LL+Dis method in general per-

forms comparable to or sometimes slightly better than the

LL baseline. The results achieved on the WT10g collection

shows that LL+Dis can be more effective in noisy conditions,

such as web collections. Overall, although the theoretical

analysis shows that PRF methods should satisfy the "distri-

bution effect" constraint, it does not substantially affect the

retrieval performance in the AP and the Robust collections.

The

reason

is

that

the

values

of

|D| ut(D)

(see

Equation

(3))

are

very close to each other for different documents, especially

in newswire collections. Thus, our modification to the log-

logistic regarding the "distribution effect" constraint cannot

substantially affect the retrieval performance.

As shown in Table 2, the LL+All method, which is our fi-

nal modification to the log-logistic model, outperforms both

baselines in all collections in terms of MAP and P@10. The

MAP improvements are always statistically significant. The

LL+All method is also shown to be more robust than the

LL method, in particular in the WT10g collection.

3.2.2 Parameter Sensitivity
In this set of experiments, we fix one of the parameters r (the generality control parameter for mutual information) and n (the number of feedback terms), and then sweep the other one to show the sensitivity of the method to the input parameters. The results are reported in Figure 1.4 According to this figure, the method is quite stable w.r.t. the changes in the values of these two parameters, especially for the parameter r. The results also indicate that by increasing the number of feedback terms, performance in the Robust collection generally increases, but in the WT10g collection it is not the case. The reason could be related to the noisy nature of this collection compared to the newswire collections.

4For the sake of visualization, we only report the results for the Robust and the WT10g collections. The behaviour of the method in AP is similar to the Robust collection.

4. CONCLUSIONS AND FUTURE WORK
In this paper, we proposed two new constraints for pseudorelevance feedback models. The first constraint considers semantic similarity of the feedback terms to the initial query. The second constraint focuses on the effect of distribution of all terms in the feedback documents on each term. We further studied the log-logistic model, a state-of-the-art feedback model, and showed that this model does not satisfy the proposed constraints as well as the previously proposed "relevance effect" constraint [8]. We then modified the loglogistic model to satisfy all of these constraints. The proposed modification was evaluated using three TREC newswire and web collections. Experimental results suggest that the modified model significantly outperforms the original log-logistic model, in all collections.
An interesting future direction is to study other feedback methods, such as the language model-based feedback methods, and modify them in order to satisfy the constraints. In this paper, we only consider simple approaches to satisfy the constraints, such as using mutual information for capturing semantic similarities. Future work can focus on more complex and accurate approaches to improve the retrieval performance.
5. ACKNOWLEDGEMENTS
This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
6. REFERENCES
[1] S. Clinchant and E. Gaussier. Information-based Models for Ad Hoc IR. In SIGIR '10, pages 234­241, 2010.
[2] S. Clinchant and E. Gaussier. A Theoretical Analysis of Pseudo-Relevance Feedback Models. In ICTIR '13, pages 6­13, 2013.
[3] K. Collins-Thompson. Reducing the Risk of Query Expansion via Robust Constrained Optimization. In CIKM '09, pages 837­846, 2009.
[4] H. Fang and C. Zhai. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In SIGIR '06, pages 115­122, 2006.
[5] V. Lavrenko and W. B. Croft. Relevance Based Language Models. In SIGIR '01, pages 120­127, 2001.
[6] Y. Lv and C. Zhai. Revisiting the Divergence Minimization Feedback Model. In CIKM '14, pages 1863­1866, 2014.
[7] J. H. Paik. A Novel TF-IDF Weighting Scheme for Effective Ranking. In SIGIR '13, pages 343­352, 2013.
[8] D. Pal, M. Mitra, and S. Bhattacharya. Improving Pseudo Relevance Feedback in the Divergence from Randomness Model. In ICTIR '15, pages 325­328, 2015.
[9] J. Seo and W. B. Croft. Geometric Representations for Multiple Documents. In SIGIR '10, pages 251­258, 2010.
[10] C. Zhai and J. Lafferty. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01, pages 403­410, 2001.

768

Balancing Relevance Criteria through Multi-Objective Optimization

Joost van Doorn1

Daan Odijk1

joost.vandoorn@student.uva.nl d.odijk@uva.nl

Diederik M. Roijers1,2

Maarten de Rijke1

diederik.roijers@cs.ox.ac.uk derijke@uva.nl

1Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands 2Department of Computer Science, University of Oxford, Oxford, United Kingdom

ABSTRACT
Offline evaluation of information retrieval systems typically focuses on a single effectiveness measure that models the utility for a typical user. Such a measure usually combines a behavior-based rank discount with a notion of document utility that captures the single relevance criterion of topicality. However, for individual users relevance criteria such as credibility, reputability or readability can strongly impact the utility. Also, for different information needs the utility can be a different mixture of these criteria. Because of the focus on single metrics, offline optimization of IR systems does not account for different preferences in balancing relevance criteria.
We propose to mitigate this by viewing multiple relevance criteria as objectives and learning a set of rankers that provide different tradeoffs w.r.t. these objectives. We model document utility within a gainbased evaluation framework as a weighted combination of relevance criteria. Using the learned set, we are able to make an informed decision based on the values of the rankers and a preference w.r.t. the relevance criteria. On a dataset annotated for readability and a web search dataset annotated for sub-topic relevance we demonstrate how trade-offs between can be made explicit. We show that there are different available trade-offs between relevance criteria.
Keywords
Multi-objective optimization; Learning to rank
1. INTRODUCTION
The primary goal of information retrieval (IR) systems is to satisfy the information need of a user. Search engines today are fairly successful in finding topically relevant pages. To achieve this most search engines are optimized to rank documents based on their topical relevance to the query. In an offline optimization setting relevance is typically determined by experts, and evaluated with utility-based metrics such as nDCG, which tends to focus on optimizing a single aspect of utility. In online optimization, feedback is collected implicitly for all relevance criteria. However, this approach may ignore differences between individual users and information
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914708

needs by aggregating across all users and queries. Often aggregation works well, but not always. E.g., users that have limited vocabularies (e.g., children) can benefit from search results optimized for their reading level. When people look for medical information on the web they would benefit from accurate and reliable information, more so than when looking for information on a Star Wars movie.
Utility depends on many factors aside from topicality; criteria such as credibility, reputability and readability are also important [14]. While their importance is typically secondary to topicality, there clearly is a benefit in many use cases. A learning to rank approach [16] can be used to learn an optimal ranker for a specified weighted preference over criteria. Similarly, data fusion techniques can combine ranked lists that are optimized for a certain notion of utility. But what if we want to optimize for multiple criteria, without knowing their relative importance beforehand? For instance, how should we balance relevance and readability if we do not know who our user will be? Or relevance and sub-topic relevance?
We draw inspiration from multi-objective optimization techniques to answer these questions, i.e., to find a set of rankers for which each solution is optimal for a different trade-off in the relevance criteria. We combine the multi-objective technique Optimistic Linear Support (OLS) [12] with multiple utility-based metrics in a learning-to-rank setting. We consider two scenarios with two relevance criteria for which we optimize a set of rankers. We evaluate our approach on two datasets, one annotated for relevance and readability, and one annotated for relevance and diversity. To learn our rankers we apply dueling bandit gradient descent with a point-wise ranking function. To optimize for diversity we subsequently apply MMR and cluster-based ranking.
2. BACKGROUND
The concept of relevance is core to IR and a much debated subject. Park [9] gives an extensive analysis on the nature of relevance in IR, and argues that relevance is intrinsically related to the selection process by the user. Cooper [4] states that each query could represent a set of specific questions as part of the information need, where documents are relevant if they provide an answer to one of these specific questions. Schamber [14] identifies major criterion groups for relevance: aboutness, currency, availability, clarity and credibility. There is a general trend that relevance cannot be attributed to just one factor such as topicality, but is multi-factored [9].
Many metrics have been proposed to measure the effectiveness of an IR system; we focus on metrics based on the concept of utility [2, 4]. The utility of an IR system depends on all factors that determine the usefulness for each specific user. Cooper [4] defines utility as "A catch all concept involving not only topic relatedness

769

Convex coverage set

Value of objective 2 Scalarized value

Dominated solutions

Value of objective 1

0

w1

10

w1

10

w1

1

Figure 1: The points on the line Figure 2: Three steps of OLS on a two-objective problem. X-axis is weight of one objective (note:

represent solutions in the cover- w2 = 1 - w1), y-axis the scalarized value. Blue area highlights difference between upper bound

age set, the others are dominated. and convex value. As more solutions are added to the CCS, the difference is iteratively reduced.

but also quality, novelty, importance, credibility, and many other

things." Utility-based metrics combine a notion of utility with spe-

cific assumptions about user behavior [2]. Each document has a

specific numerical utility value for an information need. Addition-

ally, a discount function is used on the document's rank, under the

assumption that more effort is needed to reach lower ranked doc-

uments, and it is less likely for the user to reach these documents.

Many metrics, therefore, boil down to the same basic formula to

estimate the utility of the ranked list, composed of a sum of the

product of document utility and a discount factor [3]:

M=

K k=1

gain

(relk

)

×

discount

(k)

(1)

Extensions focus on multiple criteria. E.g., Dai et al. [5] present

an extension of nDCG for freshness and relevance. Zuccon [17]

proposes on an extension of rank-based precision for readability.

Similarly, diversity and novelty metrics also take into account

multiple criteria in the form of subtopic relevance [3]. The underly-

ing assumption is that there are multiple subtopics (or categories)

for each query, and each user will be interested in results for at least

one of these subtopics. Relevance assessments are provided for

each of the subtopics belonging to a query separately. These are

combined based on the probability p(i|q) of intent i being intended

by the user for query q [1]. -nDCG extends (1) with a weighted

sum over subtopics, given pi as the probability of each subtopic:

-nDCG

=

1 N

M i=1

pi

K k=1

gain

k i

×

discount (k)

(2)

While there has been previous work that combines multiple rel-

evance criteria in the utility-based evaluation framework, to the

best of our knowledge, no previous work uses multi-objective opti-

mization techniques on information retrieval problems, i.e., existing

methods do not return a set of alternative rankers with different

available trade-offs with respect to the different relevance criteria.

3. MULTI-OBJECTIVE OPTIMIZATION

Scalarization function. We assume that a ranker has a value for

each different relevance criterion, i.e., each ranker has an associated

value vector V, with a value, Vi in each criterion i. We follow [11]

and assume that the preference of an individual user with respect

to these criteria can be expressed in terms of an unknown scalar-

ization function f , that collapses the value vector to a scalar utility:

f (V, w), where w is a vector that parameterizes f . We are unable

to observe this function directly. Instead, we aim to find a cover-

age set [11] of rankers, that contains an optimal trade-off for each

possible preference (i.e., f and w) that a user might have, see Fig. 1.

We assume that f is linear (where weighted means:

C i

wi

=

1):

f (V, w) = wT V,

(3)

i.e., the utility for the user is a weighted sum over relevance criteria.

Metrics as objectives. To formulate our own scalarization function

we can combine (1) with (3):

M=

C i=1

wi

K k=1

gain

i(dock

)

×

discount

(k)

(4)

This definition is similar to the definition of -nDCG of (2), where

instead of a sum over topics we have a sum over C relevance criteria.

In -nDCG, the metric M would subsequently be normalized. It

is, however, not desirable to normalize the linear scalarized value

function as this would remove the convex property of the value

vector that is required for efficient optimization using OLS. The

linear scalarization function does require values that are comparable

in their magnitude, therefore, the individual value functions are

normalized with normalization value Ni instead, giving:

Vi

=

1 Ni

K k=1

gain

i

(dock

)

×

discount

(k)

(5)

Convex coverage set. Because each criterion contributes positively to the scalarization function, and we are interested in the relative importance of each criterion, we can assume that w is a positive vector that sums to 1 in order to determine a coverage set. A coverage set that covers all possible linear scalarizations is called a convex coverage set (CCS) [11]. To compute the (approximate) CCS, we build on the Optimistic Linear Support (OLS) framework for multi-objective optimization [12]. Fig. 2 illustrates the OLS procedure; OLS computes a CCS by solving a multi-objective problem as a series of single-objective problems, i.e., problems that are scalarized using different w. At each iteration, OLS tries to find a new ranker, thereby incrementally building a CCS. We can use existing single-objective optimization techniques to find rankers for a given w. We use Dueling Bandit Gradient Descent (DBGD) [16].
Each ranker found in an iteration of OLS has an associated value vector V. For each w, the scalarized value of a ranker is Vw = w·V. Given a partial CCS, i.e., the set S of rankers and associated V found so far, we define the scalarized value function as a function of w:
VS(w) = max w · V,
VS
i.e., the scalarized value function is the convex upper surface of the vectors in Fig. 2. OLS selects the next w from the corner weights of VS(w), i.e., those w where VS(w) changes slope. In Fig. 2 the corner weights evaluated in that iteration are indicated by the blue vertical lines. The maximal possible improvement in scalarized value on the partial CCS is indicated by the dashed lines above VS(w). Once it reaches a corner weight, OLS is provably optimal as long as the single-objective method it employs to solve the scalarized problems is exact [12]. In practice, exact singleobjective subroutines are not required; we can safely use DBGD, but with lesser guarantees of the optimality of the solution [13].

Reuse and iterated search scheme. A limitation of applying standard OLS is that for every corner weight DBGD needs to be run. This can be expensive, depending on the size of the dataset. However, this can be mitigated by hot-starting the single-objective optimization algorithm with parts of previously found solutions (following [13]). For each new corner weight, we multi-start DBGD, starting from the rankers that were found at the 3 closest corner weights so far. It is possible that DBGD does not find a new best

770

solution, even though such a solution might still exist. If this is the case for a number of iterations, we take a random pertubation step. DBGD is stopped automatically after 40,000 iterations, or if no improvement has been found after 20 random pertubations. To our knowledge, this is the first time such a Multi-Start/Iterated Local Search scheme [7] has been combined with OLS.

4. EXPERIMENTAL SET-UP
To demonstrate how multi-objective optimization for balancing multiple relevance criteria works in practice, we perform experiments on two datasets: (i) balancing readability and topical relevance in a health setting (CLEF eHealth 2015 task 2 [8]), and (ii) balancing diversity and topical relevance in a web search dataset annotated for sub-topic relevance (TREC 2012 Web Track diversity task). While our runs are competitive, our main goal is to find multiple solutions that balance different relevance criteria, which we report in the form of a CCS.

CLEF eHealth. CLEF eHealth 2015 task 2 provides annotations for two objectives. It is composed of 5 training queries and 67 test queries; annotations are provided for relevance only for the training queries, and both relevance and understandability for the test queries. As the extra understandability annotations are required to optimize for both relevance and understandability at the same time we only use the test set queries for optimization. To measure readability document text is extracted using boilerpipe [6]. Since simple readability metrics do not correlate well with actual readability in the medical domain [15], another approach to readability is required. We compiled a list of medical terms and counted their occurrences. This list was taken from an English Wikipedia page, for which all words contained in the 20k English word list from the Google Trillion Word Corpus were filtered out.1 Using this feature, and, additionally, the Coleman-Liau index, Gunning fog index, and document length, we trained an SVM to predict the understandability score. For the CLEF eHealth 2015 task 2 the usual metrics are RBP, uRBP and uRBPgr. In preliminary experiments we found a strong correlation between RBP and uRBP, like [17]. Hence, we optimize for nDCG using relevance annotations (nDCGrel), and also for nDCG using understandability annotations (nDCGread). We normalize nDCGread so that the value is in the same range as nDCGrel.

TREC Web Diversity. The TREC 2012 Web Track diversity

task comes with 50 queries, with sub topic relevance assessments

provided for the first 20 documents produced by the participating

systems. TREC 2010 and 2011 Web Track diversity task queries

were used for training. For diversity we use MMR and cluster-based

ranking [10] with cosine similarity on TF-IDF vectors. We only

apply MMR on the first T clusters. Documents are first scored

based on relevance, subsequently, MMR and cluster-based ranking

rerank the documents for diversity, which produces a rank i for each

document i. Using rank i, the final ranking is determined based on

(1

-

wd )scorei

+

wd

1 rank

i

,

where

wd

is

a

parameter

that

balances

diversity and relevance. The usual metrics reported for the TREC

Diversity task are nDCG and -nDCG. For optimization we use both

nDCG and -nDCG to optimize for both relevance and diversity.

As clustering introduces a lot of randomness, we average over 5

runs of DBGD. For value functions Vrel and Vdiv, we normalize the

values of nDCG and -nDCG, respectively, to [0, 1].

5. RESULTS
CLEF eHealth 2015 task 2. For this task, we simultaneously optimize for readability and relevance, using nDCG for both metrics.

1For this list see: github.com/JoostvDoorn/moo-sigir2016

0.40

0.35

Scalarized value

0.30

0.25

0.20

0.15

0.0

0.2

0.4

0.6

0.8

1.0

Weight for Readabilit y

Figure 3: The CCS found on CLEF eHealth 2015, with a scalarized value based on relevance and readability. Absolute left having maximum weight on relevance, and absolute right maximum weight on readability.

The scalarized value of a solution was calculated using a weighted interpolation between value functions Vrel and Vread (Eq. 3).
The convex coverage set (CCS), constructed using OLS, is shown in Fig. 3. OLS finds eight solutions of which six are not dominated. The set of solutions is reported in Table 1 with their RBP and uRBP scores. We note that our best uRBP score is above the second run for the original task and the best nDCGrel is in the top-5 out of 110 runs. We observe that we are able to find solutions that optimally combine the nDCGrel and nDCGread objectives given different preferences for readability. E.g., with a wread of 0.626, we obtain a 5% increase in nDCGread, with an 8% loss compared the best solution in terms of nDCGrel (wread = 0.197). uRBP combines both objectives, and is highly correlated to RBP [17]. Due to this correlation, using RBP with uRBP would not find all rankers that offer the best available trade-offs between relevance and readability; the solution would be biased toward relevance. We therefore conclude that uRBP is not suitable for all possible preferences that a user might have.
Table 1: Evaluation of the solutions from the CCS on eHealth. wread nDCGrel nDCGread RBP uRBP uRBPgr 0.000 0.350 0.783 0.392 0.342 0.337 0.197 0.364 0.777 0.397 0.340 0.339 0.448 0.344 0.807 0.371 0.327 0.324 0.514 0.343 0.804 0.372 0.327 0.324 0.626 0.335 0.814 0.369 0.326 0.324 0.771 0.298 0.832 0.333 0.294 0.294 0.944 0.266 0.840 0.304 0.270 0.269 1.000 0.157 0.840 0.189 0.160 0.159
To further analyze the effect of different weights for the readability objective, we analyze the annotations at each position in the ranking averaged over topics. Fig. 4 shows for three solutions in the CCS. The ranker optimized for readability does not show documents with higher relevance annotations in the top positions, whereas the other rankers are able to place more relevant documents at the top (similarly for readability). We observe that each ranker is suitable for their specific scalarization function, and as such our method is effective in balancing different relevance criteria.
TREC 2012 diversity task. For this task, we simultaneously optimize for overall relevance and sub-topic relevance by linearly combining value functions based on nDCG and -nDCG. The CCS from OLS is shown in Fig. 5. The results from the points in the CCS on the TREC 2012 diversity task are shown in Table 2. Fewer solutions were found for the CCS compared to the readability task,

771

1

2

3

4

5

6

7

wread = 0.197

8 9 10

wread = 0.626 wread = 1.00

2.6 2.4 2.2 2.0 1.8 1.6 0.0 0.2 0.4 0.6 0.8 1.0

Readability

Relevance

Figure 4: Average readability and relevance annotations on each rank for three different solutions in the CCS.

1.00

0.95

Scalarized value

0.90

0.85

0.80

0.75

0.70

0.65

0.60

0.0

0.2

0.4

0.6

0.8

1.0

Weight for Diversity

Figure 5: The CCS found on the TREC 2010 and 2011 datasets, with a scalarized value based on relevance and diversity. Absolute left having maximum weight on relevance, and absolute right maximum weight on diversity.

furthermore the differences between the values in Table 5 are also

quite small, suggesting only a small trade-off between the objectives.

As such this setting seems less suitable for our method. In terms of -nDCG, the solutions on the test set (TREC 2012) are midperformers compared to the original participants (the best solution is above the fourth of nine participants). The overall nDCG score

Table 2: Evaluation of the solutions from the CCS on the TREC 2012 dataset.
wdiv nDCG -nDCG 0.000 0.236 0.489 0.808 0.229 0.493 1.000 0.204 0.480

would have ranked second. During training (TREC 2010­2011),

the intermediate solution that OLS found obtains the same -nDCG

score with an increase in nDCG, compared to the solution optimized

only for -nDCG, see Fig. 5. We therefore conclude that our ap-

proach finds more balanced and better solutions, than if we would

optimize for a single objective.

6. DISCUSSION
We demonstrated how to optimize rankings for multiple objectives by proposing a multi-objective approach based on optimistic linear support and DBGD for learning to rank. Because DBGD may get stuck in a local minimum we proposed an iterated local search schema for DBGD, and reuse of rankers inside OLS in order to make our algorithm more efficient. Using this approach, we have found multiple optimal rankers on the CLEF eHealth 2015 task 2 and on the TREC diversity task that offer different trade-offs w.r.t. different relevance criteria. These multiple optimal rankers are more flexible than a one-size-fits-all ranker produced by a standard learning to rank approach, and our work therefore forms an important step for flexibly optimizing search when multiple criteria are in play.
As to future work, one important issue is exposing different solutions to the user, or using different solutions to select the desired one. Exposing the user to multiple solutions can be done using additional

user interface elements, or based on profiling of the user or adapting per query. The number of user interface controls provided in generic search engines is very minimal; specialized search engines are more likely to benefit from optimizing their controls based on these multiobjective criteria. Future work may also investigate what a good scalarization function is, as others may exist and be more suited, and which metrics are more suitable for linear combination. Many current evaluation metrics are highly correlated with relevance and as such may not always provide the flexibility to get a large CCS.
Acknowledgments. This research was supported by Ahold, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOXPol), the ESF Research Network Program ELIAS, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. program, the Netherlands eScience Center under project number 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.001.109, 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty Research and Engagement Program, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM'09, pages 5­14. ACM, 2009.
[2] B. Carterette. System effectiveness, user models, and user utility: a conceptual framework for investigation. In SIGIR'11, pages 903­912. ACM, 2011.
[3] C. L. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In WSDM'11, pages 75­84. ACM, 2011.
[4] W. S. Cooper. A definition of relevance for information retrieval. Information Storage and Retrieval, 7(1):19­37, 1971.
[5] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR'11, pages 95­104. ACM, 2011.
[6] C. Kohlschütter, P. Fankhauser, and W. Nejdl. Boilerplate detection using shallow text features. In WSDM'10, pages 441­450. ACM, 2010.
[7] H. R. Lourenço, O. C. Martin, and T. Stützle. Iterated local search. Springer, 2003.
[8] J. Palotti, G. Zuccon, L. Goeuriot, L. Kelly, A. Hanbury, G. J. Jones, M. Lupu, and P. Pecina. CLEF eHealth evaluation lab 2015, task 2: Retrieving information about medical symptoms. In CLEF '15. Springer, 2015.
[9] T. K. Park. The nature of relevance in information retrieval: An empirical study. The Library Quarterly, 63(3):318­351, 1993.
[10] F. Raiber and O. Kurland. The Technion at TREC 2013 web track: Cluster-based document retrieval. Technical report, Technion, Israel, 2013.
[11] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley. A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research, 48:67­113, 2013.
[12] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Computing convex coverage sets for faster multi-objective coordination. Journal of Artificial Intelligence Research, 52:399­443, 2015.
[13] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Point-based planning for multi-objective POMDPs. In IJCAI'15, 2015.
[14] L. Schamber and J. Bateman. User criteria in relevance evaluation: Toward development of a measurement scale. In ASIS'96, volume 33, pages 218­25. ERIC, 1996.
[15] X. Yan, D. Song, and X. Li. Concept-based document readability in domain specific information retrieval. In CIKM'06, pages 540­549. ACM, 2006.
[16] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML'09, pages 1201­1208. ACM, 2009.
[17] G. Zuccon. Understandability biased evaluation for information retrieval. In ECIR'16. Springer, 2016.

772

Burst Detection in Social Media Streams for Tracking Interest Profiles in Real Time

Cody Buntain
Dept. of Computer Science University of Maryland
College Park, Maryland, USA
cbuntain@cs.umd.edu

Jimmy Lin
David R. Cheriton School of Computer Science University of Waterloo
Waterloo, Ontario, Canada
jimmylin@uwaterloo.ca

ABSTRACT
This work presents RTTBurst, an end-to-end system for ingesting descriptions of user interest profiles and discovering new and relevant tweets based on those interest profiles using a simple model for identifying bursts in token usage. Our approach differs from standard retrieval-based techniques in that it primarily focuses on identifying noteworthy moments in the tweet stream, and "summarizes" those moments using selected tweets. We lay out the architecture of RTTBurst, our participation in and performance at the TREC 2015 Microblog track, and a method for combining and potentially improving existing TREC systems. Official results and post hoc experiments show that our simple targeted burst detection technique is competitive with existing systems. Furthermore, we demonstrate that our burst detection mechanism can be used to improve the performance of other systems for the same task.
CCS Concepts
·Information systems  Summarization; Social tagging systems; ·Human-centered computing  Social networking sites;
Keywords
burst detection, real-time tracking, twitter
1. INTRODUCTION
A significant power of social media is the velocity with which new information is posted and shared. If a user is interested in recent posts about a particular item, event, or topic, she can search for a few relevant keywords in a social network and track the newest developments. For instance, one can track tweets mentioning "goal" on Twitter during the 2014 World Cup to identify when goals are scored [4]. If a user wants to track these interesting events on current social media platforms, however, she must remain online and manually filter through many duplicate posts. Many approaches have been proposed to address this need [3, 7, 8, 9], as explored at the 2015 Text Retrieval Conference (TREC) [6] organized by the National Institute of Standards and Technology (NIST).
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914733

This paper describes a simple scoring method that uses burst detection to address this social media tracking problem. By identifying rapid increases (i.e., "bursts") in relevant social media posts, one can theoretically rely on the social network to determine interesting data for a given set of interests. We describe this approach, which we call RTTBurst, and its use of real-time burst detection on Twitter's social media stream. Besides documenting our techniques, RTTBurst was one of the systems participating in the TREC 2015 Microblog track, and we discuss its performance relative to similarly purposed systems. Lastly, RTTBurst's architecture is quite different from the other TREC systems, allowing us to demonstrate its use as an additional filtering step to increase other systems' performance.
This work makes the following contributions:
· Presents a real-time streaming algorithm for discovering and summarizing relevant moments on Twitter,
· Details RTTBurst's performance relative to similar real-time systems, and
· Demonstrates that burst detection can enhance other realtime tracking systems.
2. RELATED WORK
Identifying important events from the ever-growing body of digital media has fascinated researchers for over twenty years, starting from digital newsprint to blogs and now social media [1]. Early event detection research followed the work of Kleinberg [5] by identifying bursty keywords from digital newspapers and clustering those keywords to identify bursty events. These works inspired our exploration of burst detection, but they often used complex models, were not designed for big datasets, and were not designed for real-time use.
A recently published survey by Atefeh and Khreich [2] explores many of the avenues used for modern event detection in social media. They laid out many of the issues in analyzing Twitter (e.g., high levels of noise, mixed language, spelling/grammar mistakes, etc.) and presented a classification of event detection techniques. The classes cover three dimensions: unspecified vs. specified event information, new vs. retrospective detection, and unsupervised vs. supervised learning methods. Our work falls in the new-eventdetection, unsupervised learning classes but represents a hybrid in the unspecified vs. specified dimension. RTTBurst was originally developed on an open-domain model and was adapted to the interest tracking domain. It allows the user to pre-specify a topic of interest but leverages temporal signatures to identify new, unanticipated events related to that topic. This hybridization is well-suited for the TREC 2015 Microblog track, which focused on identifying new, topically relevant information on Twitter in real time.

777

As mentioned in the track's 2015 overview paper by Lin et al. [6], this filtering task's goal was to identify new tweets relevant to a set of given interest profiles, each of which was comprised of an identifier, title, a brief description, and a narrative describing the topic of interest. The evaluation occurred in July of 2015 over ten days and was broken across two tasks: a mobile notification task that enforced a limit of 10 tweets per topic per day and penalized tweets based on the delay between posting and reporting (Scenario A), and a daily digest task with the relaxed constraint of 100 messages per day and no temporal penalty (Scenario B). For the evaluation, NIST created 225 topics, 51 of which were later assessed.
3. METHODS
RTTBurst's high-level pipeline is composed of several stages, from collecting the Twitter stream, to finding bursty tokens, to using these tokens to extract the most interesting tweets. Each of these stages is described below.
Processing the Twitter Stream. For input, RTTBurst used Twitter's unfiltered public sample stream, corresponding to approximately 1% of the full stream (though larger samples should also work), and the user's interest profiles. After extracting search keywords k  Pi from the set of interest profile titles P , RTTBurst leveraged Apache Spark's1 Twitter receiver to collect all tweets from the public sample stream and tokenized them using CMU's ARK TweetNLP tokenizer.2 We then applied a series of filters to remove non-English tweets and low-quality tweets based on the number of hashtags, web links, token counts, and whether the tweet contained the string "follow" (motivated by the large amount of "follow-me" spam on Twitter).
After this first round of quality-based pruning, RTTBurst then calculated the intersection between each tweet's token set and the set of all search keywords iPi and kept only those tweets with a non-empty intersection (i.e., only those tweets that contained at least one keyword from at least one interest profile). These tokenized tweets were then converted into a time-stamped inverted index matching tokens to the users who tweeted them.
Identifying Bursty Tokens. This time-stamped inverted index allowed us to capture changes in a token's usage over time. We maintained a sliding window over all tweets generated by the Twitter streaming API within the past two minutes and incremented the window by 60-second time slices. Each window therefore overlapped with the previous 60 seconds to smooth the input.
For each two-minute window, we calculated the number of users tweeting with each token and stored this frequency over the previous N windows. We normalized these frequencies by the number of unique tokens in the past N windows and used add-one additive smoothing to correct for tokens with zero occurrences in a single window. Following the features set forth in the paper by Buntain et al. [3], we then used linear regression to fit a line to the natural logarithm of this frequency data. By transforming this frequency data to logarithmic space, exponential curves will appear linear, simplifying the linear regression step, and the steeper the slope of the best-fit line, the steeper the exponential growth of the token's usage. Based on this fit, we then scored each token by the product of the slope of the best-fit line and its coefficient of determination R2. Since R2 coefficient is in the range [0, 1], this product reduced scores for highly deviant frequency curves. In this manner, tokens experiencing large bursts in usage, which we would expect to exhibit exponential growth, were scored highly. We then discarded all
1https://spark.apache.org 2http://www.cs.cmu.edu/~ark/TweetNLP/

tokens with scores below a burst threshold  and any token whose length is less than four characters.
Moment Summarization. Every sixty seconds, RTTBurst identified a new (possibly empty) set of bursty tokens, which corresponded to noteworthy moments in the relevant interest profile. For the TREC Microblog track, however, returning these bursty tokens was not sufficient for summarizing the moment, since the evaluation was based on judgments over individual tweets. Rather, our system used tweets to summarize these moments, similar to the ReDites system [7].
To this end, every sixty seconds, RTTBurst parsed all tweets in the previous N windows to create a subset of tweets containing these bursty tokens. We then calculated a Jaccard similarity score for each tweet in this subset by comparing the tweet to tweets returned to the user in previous windows. Any new tweet whose Jaccard similarity was above our threshold Jt = 0.7 was discarded, and the remaining tweets were sorted by their similarity scores in decreasing order. Finally, the top M least similar tweets containing bursty tokens from the past N windows were assigned to the relevant interest profiles and stored.
Before pushing a tweet to the user, however, RTTBurst performed one last pass through the tweets to select those that were most relevant to the given interest profile. For each candidate tweet stored up to this point, RTTBurst then selected only those tweets that contained at least X tokens from the relevant interest profile. All other tweets were then discarded.
In summary, for Scenario A of the TREC Microblog track, the top 10 most dissimilar tweets containing bursty tokens and at least two tokens from the relevant interest profile were returned to the user per day. Scenario B followed the same pipeline with the additional relaxation of returning the top 100 most dissimilar tweets.
3.1 Ensembles with RTTBurst
While analyzing results after the official TREC 2015 evaluation, we noticed a significant dissimilarity between the tweets returned by RTTBurst and those returned by the other systems. This observation led to an interesting question: If we apply the burst detection approach of RTTBurst to the output of another more traditional information retrieval system, could we increase the system's performance? To explore this question, we designed a simple gating mechanism that, given a set of tweets returned by system A, used RTTBurst to keep only those tweets that contained a bursty token.
Following from this question of using RTTBurst to filter other systems, we also investigated whether RTTBurst could be used to create ensembles of these information tracking and summarization systems. That is, given the output of two TREC systems A and B, would applying RTTBurst to their combined output yield higher scores? For this investigation, we constructed a simple system that takes the union of any two systems' returned tweets and then applies RTTBurst's gating mechanism to filter the results. To ensure that RTTBurst did not benefit simply from combining multiple systems, we also conducted an experiment that scored the outputs of each pair of systems, without any gating by RTTBurst. Duplicate tweets were removed from this paired output, the output was ordered by delivery time, and only the first tweets within the scenario A daily limits were scored.
4. RESULTS
We divide our results into two sets: The first covers RTTBurst's relative performance results from the real-time Microblog track tasks as scored by NIST (including some post hoc testing), and the second covers results from our ensemble experiments.

778

Window Size (N )
 30 37 18 37

Table 1: Optimized Parameters, Tweets Delivered to Users, and Scores (Best in Bold)

Parameters

Top M Burst Tweets Threshold 

10

0.07

13

0.036854

34

0.138824

48

0.067306

Delivered Tweets
1 29 15 6

Scenario A

Unjudged

ELG

Tweets

0

0.2471

15

0.2549

7

0.2525

1

0.2506

nCG
0.2471 0.2464 0.2494 0.2479

Delivered Tweets
1 29 15 6

Scenario B Unjudged Tweets
0 15 7 1

 ­ Parameters used for TREC 2015 evaluation

nDCG
0.2471 0.2420 0.2479 0.2489

4.1 RTTBurst Performance
RTTBurst's TREC evaluation version originally lacked several tweet quality metrics (i.e., it did not filter out tweets with many hashtags, many links, or few tokens) and did not include mechanisms for preventing duplicate tweet content from being reported to the user. This official run crystallized the need for these quality metrics as our system caught a significant amount of spam in this early run. For example, while the original RTTBurst implementation did prevent the same tweet ID from being reported twice, two different tweets with the same content could still be reported, and many Twitter bots spammed the same tweet content with only slight differences (one token at the end of the tweet might differ from one spam tweet to the next).
Following the TREC evaluation period and the release of the NIST-judged tweets, we implemented these quality metrics and performed a series of post hoc parameter optimization experiments. Parameter optimization used a randomized parameter search over window size N  [7, 43], maximum tweets delivered per minute N  [10, 50], and burst thresholds   [0.015, 0.18]. For each parameter set, we recorded the number of tweets RTTBurst flagged for delivery to the user (across all topics), the number of these tweets that did not have associated relevance judgments from NIST (unjudged tweets), and their scores. Table 1 shows the top-scoring sets for both scenarios from the official run (indicated by the ) and our parameter optimization (see the track overview paper [6] for details on the scoring methodology). Official scores placed RTTBurst 11th out of 32 automatic runs in Scenario A (ranked by ELG) and 4th out of 38 in Scenario B. After parameter optimization, RTTBurst would move up one rank in Scenario A and would remain in fourth in Scenario B. Note that randomized parameter optimization produced more scored tweets than the official run, which was essentially silent. It is worthwhile to note that RTTBurst is exceedingly conservative in the emission of tweets, and that this approach occupies a completely different point in the tradeoff space compared to standard retrieval-based systems.
4.2 Gating with RTTBurst
Applying RTTBurst's gating mechanism to a single Scenario A system resulted in an average increase in ELG and nCG by 17% and 13% respectively but decreased the ELG of the best-performing system [10] by about 19%. A two-sided t-test on the original scores and the gated scores determined this increase in ELG was statistically significant (t(33) = 3.28, p < 0.01). In total, RTTBurst increased the performance of 22 systems and decreased the performance of 13 systems, as shown in Figure 1a. For Scenario B, gating with RTTBurst resulted in a 9% decrease in nDCG@10.
For system pairs, comparing an individual system with its highest-scoring pair (that is, pairing it to all other systems and taking the one that achieves the highest ELG) yielded an 11% average ELG increase. Only three systems achieved higher scores without pairing. Using RTTBurst to gate these pairs yielded a 24%

increase in ELG over the individual, ungated systems, and five systems performed worse than their unpaired, ungated counterparts. Differences in single system ELG and paired, gated system scores are shown in Figure 1b.
For completeness, we also compared the best pairs' ELG to a silent system (Figure 2a) and the best gated pairs of systems (Figure 2b). Note that these figures show absolute scores as opposed to score differences. We see that the best pairs of systems did not perform as well as a silent system, but applying RTTBurst as an additional gating filter raised all pairs up to or above the score for a silent system.
5. DISCUSSION
Results from our experiments and the official Microblog track exhibited a correlation between higher scores and fewer reported tweets. This link was first apparent given the score for a system that returns no tweets at all: an ELG, nCG, and nDCG@10 of 0.2471, which placed in the upper third of rankings in both TREC scenarios. During our parameter optimization experiments, we saw more evidence of this trend in a strongly negative, nearly linear correlation (R2 = 0.8172) between the more tweets RTTBurst returned and the score produced by the TREC evaluations. This preference towards silence might explain why gating with RTTBurst increased the average score in Scenario A: Summed across all topics, gating reduced the average number of tweets delivered by two orders of magnitude (from 1,600 tweets to a mere 57).
Such a significant reduction in the number of delivered tweets suggested another issue regarding similarity of results returned by the original systems and their gated counterparts. From Figure 2, all systems' scores tended to converge to the same value; this convergence would be easily explained if all systems were converging to the same set of tweets. To examine this potential issue, we calculated the Jaccard similarity among the returned tweets for each system and then among the gated systems: For the original systems, the average similarity across all systems was 0.045, and for our gated systems, average similarity was 0.55. Therefore, gains made from gating with RTTBurst are not the result of reducing all output to a common set of tweets. This result suggests bursts provide a valuable relevance signal.
While this convergence is a positive effect for many systems, we must address why RTTBurst decreases the top performing run [10] by 19%. One possibility is the absence of query expansion techniques. RTTBurst was originally designed as an open-domain system without tracking capabilities, and the modifications to track interest profiles did not include data-driven synonyms or identify related keywords that could expand the filtered data. RTTBurst therefore potentially discarded many relevant tweets, something that future versions of the system should address. Another possibility, however, is an imbalance in the "bursty-ness" of some topics; thresholds for bursts about celebrities may be too high for more esoteric topics.

779

¢ ELG

0.20 0.15 0.10 0.05 0.00 -0.05 -0.10
0

0.20

0.15

0.10

¢ ELG

0.05

0.00

5

10

15

20

25

30

35

System Index

-0.05 0

5

10

15

20

25

30

35

System Index

(a) Single System vs. Gated System

(b) Single System vs. Paired, Gated System

Figure 1: Performance Differences in ELG. Systems arranged alphabetically.

ELG

0.34 0.32

Silent Mean

0.34 0.32

Silent Mean

0.30

0.30

ELG

0.28

0.28

0.26

0.26

0.24

0.24

0.22

0.22

0.20 0

5

10

15

20

25

30

35

System Index

0.20 0

5

10

15

20

25

30

35

System Index

(a) ELG of Best System Pairs vs. Silent System

(b) ELG of Best Gated System Pairs vs. Silent System

Figure 2: Average ELG vs. Silent System. Systems arranged alphabetically.

This work is also limited by unjudged tweets in the returned tweet sets, which makes a true performance comparison between official and post hoc runs difficult. That is, while the NIST assessors provided relevance judgments for approximately 94k tweets, the Twitter sample stream over the TREC evaluation period contains around 40 million tweets, so it is highly likely post hoc runs of RTTBurst may return tweets without these judgments. This limitation may be the driving force behind the connection between returned tweet set size and low scores. Going forward, we need to explore better methods for scoring these unjudged tweets or comparing judged and unjudged tweets and scores via similarity propagation, self-learning, or a similar method.
6. CONCLUSIONS
RTTBurst is a hybrid end-to-end system that uses a simple burstdetection technique to identify tweets a user may find interesting. This paper laid out RTTBurst's architecture, our participation in and performance at the TREC 2015 Microblog track, and a method for combining and potentially improving the performance of existing TREC systems. While not as effective as the best systems, RTTBurst did perform well and shows potential in hybrid or combined approaches. Further steps could be taken to integrate modern information retrieval techniques like query expansion and spam detection to increase RTTBurst's performance. Given RTTBurst's simple model and its stream-oriented processing, it is at least a useful tool that can be easily integrated into other approaches.
Acknowledgments. This work was supported in part by the National Science Foundation under awards IIS-1218043 and CNS1405688. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.

7. REFERENCES
[1] J. Allan, R. Papka, and V. Lavrenko. On-line new event detection and tracking. SIGIR, 1998.
[2] F. Atefeh and W. Khreich. A survey of techniques for event detection in Twitter. Computational Intelligence, 31(1):132­164, 2015.
[3] C. Buntain, J. Lin, and J. Golbeck. Discovering Key Moments in Social Media Streams. CCNC, 2016.
[4] L. Cipriani. Goal! Detecting the most important World Cup moments. Technical report, Twitter, 2014.
[5] J. Kleinberg. Bursty and hierarchical structure in streams. KDD, 2002.
[6] J. Lin, M. Efron, Y. Wang, G. Sherman, and E. Voorhees. Overview of the TREC-2015 Microblog Track. TREC, 2015.
[7] M. Osborne, S. Moran, R. McCreadie, A. Von Lunen, M. Sykora, E. Cano, N. Ireson, C. Macdonald, I. Ounis, Y. He, and Others. Real-Time Detection, Tracking, and Monitoring of Automatically Discovered Events in Social Media. ACL, 2014.
[8] J. Rogstadius, M. Vukovic, C. A. Teixeira, V. Kostakos, E. Karapanos, and J. A. Laredo. Crisistracker: Crowdsourced social media curation for disaster awareness. IBM Journal of Research and Development, 57(5), 2013.
[9] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes Twitter users: real-time event detection by social sensors. WWW, 2010.
[10] L. Tan, A. Roegiest, and C. L. A. Clarke. University of Waterloo at TREC 2015 Microblog track. TREC, 2015.

780

Exploiting Semantic Coherence Features for Information Retrieval
Xinhui Tu1 Jimmy Xiangji Huang1,2 Jing Luo3 Tingting He1
1School of Computer Science, Central China Normal University, Wuhan, China 2School of Information Technology, York University, Toronto, Canada
3School of Computer Science, Wuhan University of Science and Technology, Wuhan, China
tuxinhui@mail.ccnu.edu.cn jhuang@yorku.ca luojing@wust.edu.cn tthe@mail.ccnu.edu.cn

ABSTRACT
Most of the existing information retrieval models assume that the terms of a text document are independent of each other. These retrieval models integrate three major variables to determine the degree of importance of a term for a document: within document term frequency, document length and the specificity of the term in the collection. Intuitively, the importance of a term for a document is not only dependent on the three aspects mentioned above, but also dependent on the degree of semantic coherence between the term and the document. In this paper, we propose a heuristic approach, in which the degree of semantic coherence of the query terms with a document is adopted to improve the information retrieval performance. Experimental results on standard TREC collections show the proposed models consistently outperform the state-of-the-art models.
Keywords
Document ranking; Retrieval model; Term weighting
1. INTRODUCTION AND MOTIVATION
Most of the existing information retrieval (IR) models assume that the terms of a text document are independent of each other. Generally, these retrieval models integrate three major variables to determine the degree of importance of a term for a document: within document term frequency, document length and the specificity of the term in the collection [4]. Though these approaches are reasonably simple and easy-to-use, the coherence aspect of a term's saliency in a document cannot be taken into account by the term independence assumption.
The terms in a document can generally be classified into two groups: topical term and non-topical term. The topical terms will be highly associated with each other, while the non-topical terms will have very low association with the other terms within the document. Let us consider two arbitrary documents and as follows:
: So let's say you're out for a walk in the woods, with your iPhone handy, and you run into a grizzly bear ......
: Currently, Apple company has had to allocate massive resources to its iTunes and iPhone ......
The first document is concerned about "grizzly bear" and the second document is related to "Apple Company". Suppose that the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '16, July 17­21, 2016, Pisa, Italy. © 2016 ACM. ISBN 978-1-4503-4069-4/16/07...$15.00. DOI: http://dx.doi.org/10.1145/2911451.2914691

two documents are equal in length. When we use "iPhone" as query, all the retrieval models mentioned above will give approximately the same score for the two documents. However, the word "iPhone" is a topical term in document and a nontopical term in document . Apparently, the document is more relevant to the query than document . As we can see from the example, the degree of importance of a term for a document is dependent not only on the three aspects mentioned before, but also dependent on the degree of semantic coherence between the term and the document.
Humans can easily identify the topical terms and the non-topical terms of a document, because they have the knowledge of the "word association". In , the term "iPhone" appears with many other terms such as "Apple", "iTunes", which are associated with the term "iPhone". In , however, the term "iPhone" seems to be irrelevant to the other terms such as "woods"and "bear"'. For computational purposes, this knowledge can be discovered by analyzing the corpus. Semantic association measures based on corpus analysis have served this purpose for many applications related to natural language processing and IR [6].
In this paper, we study how to efficiently use semantic coherence features to improve the information retrieval performance. We propose an approach to use the degree of semantic coherence between term and document for document ranking. First, each document is represented as a graph-of-word that corresponds to a weighted directed graph whose vertices represent unique terms, whose edges represent the translation probability between two terms. Then, a graph-based algorithm is adopted to calculate coherence-based weighting score of a term within a document. The coherence-based weight score of a term for a document is determined by the degree of semantic relatedness between the term and the other terms within the document. Finally, the coherence-based document score and the frequency-based score obtained by the existing retrieval function are integrated into a linear feature-based model for document ranking. Experimental results on standard TREC collections show the proposed model consistently outperform the state-of-the-art models.
2. RELATED WORK
Over the decades, many different retrieval models have been proposed and studied, including the vector space model [16, 17], the classic probabilistic model [7, 13, 14] and the language modeling approach [12, 19]. Most of the existing retrieval models assume a "bag-of-words" representation of both documents and queries. A typical effective retrieval function involves a TF part, an IDF part, and a document length normalization part [4]. The TF part intents to give a higher score to a document that has more occurrences of a query term, while the IDF part is to penalize words that are popular in the whole collection. The document length normalization is to avoid favoring long documents.

837

Most of the traditional retrieval models employ a single term frequency normalization mechanism that does not take into account various aspects of a term saliency in a document [11, 18]. Paik [11] proposes a novel TF-IDF term weighting schema (MATF) that employs two different within document term frequency normalizations to capture two different aspects of term saliency. The experimental results show that MATF achieves significantly better precision than the start-of-the-art models, such as BM25 [14] and LMDir (the language modeling approach with Dirichlet prior smoothing) [19].
Intuitively, the degree of semantic coherence between a term and a document is an important factor to determine the importance of the term in the document. However, little work has been done around coherent-based term weighting. To the best of our knowledge, the model proposed in [6] is the only work closely related to ours. In the paper, the authors propose a neighborhood based document smoothing model by exploiting lexical association between terms. Different from their work, we adopt a linear feature-based model for final document ranking instead of modifying the traditional retrieval functions.
Inspired by the success of graph-based ranking algorithms like TextRank [9], some researches attempt to adopt graph-based document representation for improving information retrieval performance [1, 15]. Generally, a graph based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph rather than relying only on local vertexspecific information. Our work is different from the existing graph-based retrieval methods in two aspects: (1) Each edge is weighted by the mutual information between the two terms, instead of by the number of co-occurrences of the two term in sliding windows; (2) The final ranking score of a document is determined by the combination of the graph-based term weighting score and the frequency-based score instead of by the graph-based term weighting score alone.
3. PROPOSED APPROACH
In this section, we introduce a graph-based model to calculate weighting score for terms and then use these score for final document ranking.
3.1 Coherence-based Term Weighting
The coherence-based weight score of a term for a document is determined by the degree of semantic relatedness between the term and the other terms within the document. Let us consider the example mentioned in section 1 again. Once the semantic relatedness has been calculated, "iPhone" will have a higher relatedness with the terms such as "Apple, iTunes, company" than the terms such as "bear, woods, walk". Therefore, "iPhone" will get a higher coherence-based weight score in document d2 than in document d1. When we use iPhone as query, document d2 will get a higher coherence-based score.
In this paper, we adopt a graph-based ranking algorithm to calculate the coherence-based weight score of a term within a document. Each textual document is represented as a graph-ofword that corresponds to a weighted directed graph whose vertices represent unique terms, whose edges represent the translation probability between two terms. We prefer to use term rather than word because tokenization and feature selection algorithms (such as stop word removal) have already been applied. The coherence-based weight score of a term within a document is determined by the votes that are casting for it and the weight of the terms casting that votes. The weight of term t within

document d is initially set to 1 and the following PageRank function is run for several iterations

1





,

1

,

where is a damping factor that can be set between 0 and 1,

which has the role of integrating into the model the probability of

jumping from a given vertex to another random vertex in the

graph. In the context of the Web, this graph based ranking

algorithm implements a random walk model, where a user clicks

on links at random with a probability d, and jumps to a completely

new page with probability 1

[10]. We set the damping

factor to 0.85, the convergence threshold to 0.0001, following [9,

10]. Our experiments showed that only a small number of

iterations (< 50) is required to obtain convergence.

In formula (1),

, is the translation probability from term

to term in the graph of document . A simple way to

estimate

, is as follows:

where

,

,



,

2

, is the strength of association between two term

and .

To calculate

, in formula (2), we adopted a pairwise

term similarity score, which is a combination of mutual information and a distance factor. The distance factor exponentially decreased as the distance between terms increased. The assumption behind this work is that semantically related words are usually located in proximity, and the distance between two words could indicate the strength of their association. The
pairwise term similarity score of terms and can be calculate

as follows:

,

,

,

3

,



,

4

where

, is the average distance between terms and

in all the documents in the collection; is the decaying rate for the exponential function. In our experiments, the decaying rate is set to 0.8, following [5].

In formula (3),

, denotes the mutual information between

term and , which can be calculated as follows:

,

,

,

5

,

,

where and are binary variables indicating whether term or

is present or absent. The probabilities are estimated as follows:

1

1

0 1

1

1

1

01

1

1, 1

1, 1

1, 0

1

1, 1

1, 0

1

1, 1

0, 0 1

1, 1

0, 1

1, 0

838

where

1 and

1 are the numbers of documents

containing term or , respectively,

1, 1 is the

number of documents that contain both or , and in the total

number of documents in the collection.

3.2 Retrieval Function

Coherence-based term weighting score itself may be not effective

to be used as the only scoring function for retrieval. It's a

potential way to incorporate coherence-based term weighting

score into the traditional retrieval functions for document ranking.

However, it is not always possible to easily incorporate new

feature into the traditional retrieval functions, which are built

upon some underlying theoretical framework [8]. For example, it

proved non-trivial to include query independent features such as

PageRank into BM25 ranking formula [2]. Therefore we adopt a

linear feature-based model for final document ranking, instead of

modifying the traditional retrieval functions. The linear feature-

based model has been proven to be an effective way to

incorporate different features into a united retrieval model [8].

The final retrieval function can be described as follows:

,

1

, ,

1  1

, ,

6

where , is the document score calculate by the existing

retrieval functions, such as BM25, LMDir, and MATF;

,

is the coherence-based weight scores of all query's term within a

document, which can be calculate as follows:

,

St  t

7



where

can be calculated by formula (1);

inverse document frequency of term 

is the

4. EXPERIMENTS 4.1 Test Collections and Baseline Models
Table 1 summarizes that statistics on test collections used in our experiments. These collections are different in size and genre. Each document is processed in a standard way for indexing. Words are stemmed (using porter-stemmer), and stop words are removed. In the experiments, we only use title of the queries. In order to evaluate our model and compare it to other models, we use the MAP and P@10 measure, which are widely accepted measure for evaluating effectiveness of ranked retrieval systems. The methods used in our experiments are as listed in Table 2.

4.2 Parameter Sensitivity Study
An important issue that may affect the robustness of the proposed models is the sensitivity of their parameter (in Equation 6). The parameter controls weight of coherence-based weighting score in the final ranking function. In this section, we study how sensitive the parameter is to MAP measure. At the current stage of our work, the parameter is selected through grid search.
In order to find the optimal value of parameter , we sweep the values from 0 to 1 with an interval of 0.1. When  equals to 0, we reach the baseline. When  equals to 1, the retrieval model use coherence-based weighting score only. The value of  controls the influence of coherence-based weighting score. Figure 1 shows the sensitivity of the value of parameter according to MAP measure. The experimental results show that parameter can greatly impact the performance of the proposed models. Generally, the performance of all models increases at the beginning when the value of grows up. The performance of each model starts to continually drop after a peak. However, this is no unique optimal

Collection AP88-89 TREC8
WT2G WT10G

Table 1. Test collection statistics

# of Docs

Topics

164,597

51-100

528,155

301-450

247,491

401-450

1,692,096

451-550

# of Topics 50 50 50
100

Table 2. The retrieval models used in the experiments

Model BM25 LMDir
MATF
ConRank-BM25
ConRank-LMDir
ConRank-MATF

Description
The classical probabilistic model [14]
The language modeling approach with Dirichlet prior smoothing [19]
A novel TF-IDF model with excellent retrieval performance [11].
The proposed model using BM25 function to calculate ,
The proposed model using LMDir function to calculate , .
The proposed model using MATF function to calculate ,

value of for all of them. For example, the best values on collection AP88-89 are 0.3, 0.4, and 0.3 for ConRank-BM25, ConRank-LMDir, and ConRank-MATF, respectively. For all models on all datasets, we can gain the best retrieval performance
when 0.2   0.4. The proposed models constantly perform
better than the corresponding baseline while range from 0.1 to 0.6. When the value of is too large in the linear feature-based model, the performance is downward and even worse than that of the baseline.
4.3 Comparing with the State-of-the-art Models
In order to make the comparison fair, we need to carefully choose suitable values for the parameters in all the models. To build strong baselines, we adopt a method proposed in [3] to find the optimal parameter settings for BM25 and LMDir. All parameters in BM25 and LMDir are respectively set to the optimal values in our experiments. As a parameter free model, MATF does not need parameter tuning. In the proposed models, parameter and k are respectively set to 0.8 and 0.85 following [5, 10]. The previous research work [20] has demonstrated that when a new feature is integrated into a traditional retrieval function under linear interpolation, the best retrieval performance can be obtained by assigning a relatively small weight (0.1-0.2) to the new feature. As we can see from the experiments presented in section 4.2, good performance can be archived with parameter are set to 0.3. Therefore, the parameter in the proposed models is set to 0.3 in the following experiments.
Table 3 present the retrieval results of the six retrieval models on the four collections respectively. The results indicate that the proposed models constantly outperform the corresponding baseline on all collections. In most of the cases, the improvement of MAP and P@10 was statistically significant. The maximum average improvement is as high as 7.81% and 7.32% in terms of MAP and P@10, respectively. It is worth noting that MATF has obtained significant improvements over BM25 and LMDir, and is therefore a strong baseline. The significant performance improvements from such a strong baseline are very encouraging. The results confirm that semantic coherent features can be used to improve the retrieval performance.

839

Figure 1. Sensitivity of parameter to MAP measure

Table 3. Comparison of the performance of the six models in terms of MAP and P@10 (The values in the parentheses are the improvement over the model in the previous row in the table; * indicates a statistically
improvements over the model in the previous row according to the Wilcoxon signed-rank test at the 0.05 level)

Model

AP88-89

P@10

MAP

TREC8

P@10

MAP

WT2G

P@10

MAP

WT10G

P@10

MAP

BM25 ConRank-BM25

0.4216
0.4489* (+6.48%)

0.2702
0.2913* (+7.81%)

0.4645
0.4853* (+4.48%)

0.2552
0.2693* (+5.53%)

0.4957
0.5320* (+7.32%)

0.3128
0.3349* (+7.07%)

0.3626
0.3747* (+3.34%)

0.2107
0.2184* (+3.65%)

LMDir ConRank-LMDir

0.4416
0.4595* (+4.05%)

0.2768
0.2936* (+6.07%)

0.4753
0.4918* (+3.47%)

0.2509
0.2615* (+4.22%)

0.5063
0.5409* (+6.83%)

0.3057
0.3211* (+5.04%)

0.3108
0.3154 (+1.48%)

0.2094
0.2128 (+1.62%)

MATF ConRank-MATF

0.4679
0.4872* (+4.12%)

0.2994
0.3186* (+6.41%)

0.4905
0.5097* (+3.91%)

0.2671
0.2752* (+3.03%)

0.5481
0.5594* (+2.06%)

0.3241
0.3392* (+4.66%)

0.3283
0.3327 (+1.34%)

0.2226
0.2301* (+3.37%)

5. CONCLUSIONS AND FUTURE WORK
In this paper, a new term weighting approach is proposed to calculate the degree of semantic coherence of the query terms with a document. The coherence-based term weighting score can further be used in combined with the existing retrieval functions for document ranking. Experimental results on standard TREC collections show the proposed retrieval methods consistently outperform the corresponding strong baselines. The results confirm that semantic coherent features can be used to improve the retrieval performance. At the current stage of our work, only one type of distance measure is adopted to calculate semantic similarity between terms, and the values of the parameters in the proposed models are chosen empirically. For future work, we will investigate the effectiveness of other distance measures and study how to find the optimal parameter setting for further improving the retrieval performance.
6. ACKNOWLEDGMENTS
This work was partially supported by the National Science Foundation of China under the grant number 61572223, a Discovery grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada and an NSERC CREATE award. We thank anonymous reviewers for their thorough review comments.
7. REFERENCES
[1] Blanco R. and Lioma C. Graph-based term weighting for information retrieval. Information Retrieval, 15(1), 54­92, 2012.
[2] Craswell N., Robertson S. and Zaragoza H. Relevance weighting for query independent evidence. ACM SIGIR, 416-423, 2005.
[3] Diaz, F. and Metzler, D. Improving the estimation of relevance models using large external corpora. ACM SIGIR, 154-161, 2006.
[4] Fang, H., Tao, T. and Zhai, C. A formal study of information retrieval heuristics. ACM SIGIR, 49­56, 2004.
[5] Gao, J., Zhou, M., Nie, J. Y., He, H., and Chen, W. Resolving query translation ambiguity using a decaying co-occurrence model and syntactic dependence relations. ACM SIGIR, 183-190, 2002.

[6] Goyal, P., Behera, L. and McGinnity, T. M. A novel neighborhood based document smoothing model for information retrieval. Information retrieval, 16(3), 391-425, 2013
[7] Jones, K. S., Walker, S. and Robertson, S. E. A probabilistic model of information retrieval: development and comparative experiments part 1. Information Processing Management, 36(6), 779­808, 2000.
[8] Metzler, D. and Croft, W. B. Linear feature-based models for information retrieval. Information Retrieval, 10(3), 257-274, 2007.
[9] Mihalcea, R. and Tarau, P. TextRank: bringing order into texts. EMNLP, 2004.
[10] Page, L., Brin, S., Motwani, R. and Winograd, T. The PageRank citation ranking: bringing order to the Web. Technical report, Stanford Digital Library Technologies Project, 1998.
[11] Paik, J. H. A novel TF-IDF weighting scheme for effective ranking. ACM SIGIR, 343-352, 2013.
[12] Ponte, J. M. and Croft, W. B. A language modeling approach to information retrieval. ACM SIGIR, 275­281, 1998.
[13] Robertson, S. E. Readings in information retrieval. Chapter The probability ranking principle in IR, pages 281­286. Morgan Kaufmann Publishers Inc., 1997.
[14] Robertson, S. and Zaragoza, H. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3(4), 333­389, 2009.
[15] Rousseau, F. and Vazirgiannis, M. Graph-of-word and TW-IDF: new approach to ad hoc IR. ACM CIKM, 59-68, 2013.
[16] Salton, G., Wong, A., and Yang. C. S. A vector space model for automatic indexing. Communications of the ACM, 18(11), 613­620, 1975.
[17] Salton, G. McGill, M. J. Introduction to modern information retrieval. McGraw-Hill, Inc., 1986.
[18] Ye, Z. and Huang, J. X. A simple term frequency transformation model for effective pseudo relevance feedback. ACM SIGIR, 323-332, 2014.
[19] Zhai, C. and Lafferty, J. A study of smoothing methods for language models applied to information retrieval. ACM Transaction on Information System, 22(2), 179­214, 2004.
[20] Zhao, J., Huang, J. X., and Ye, Z. Modeling term associations for probabilistic information retrieval. ACM Transactions on Information Systems, 32(2), 7, 2014.

840

First Story Detection using Multiple Nearest Neighbors

Jeroen B. P. Vuurens
The Hague University of Applied Science Delft University of Technology, The Netherlands
j.b.p.vuurens@tudelft.nl

Arjen P. de Vries
Radboud University Nijmegen Institute for Computing and Information Sciences, Nijmegen, The Netherlands
arjen@acm.org

ABSTRACT
First Story Detection (FSD) systems aim to identify those news articles that discuss an event that was not reported before. Recent work on FSD has focussed almost exclusively on efficiently detecting documents that are dissimilar from their nearest neighbor. We propose a novel FSD approach that is more effective, by adapting a recently proposed method for news summarization based on 3-nearest neighbor clustering. We show that this approach is more effective than a baseline that uses dissimilarity of an individual document from its nearest neighbor.
1. INTRODUCTION
Internet users are turning more frequently to online news as a replacement for traditional media sources such as newspapers or television. For the user, the news stream is a source to both track topics of interest and to become informed about important new events the user was not yet aware of. Automated detection of new events can save to user a great deal of time, for instance by notifying users about new events, which is especially interesting to users and organizations for whom the information is time-critical and who need to act on that information.
FSD systems aim to identify those news articles that discuss an event that was not reported before in earlier stories, without knowledge of what events will happen in the news [2]. Recently, FSD has been suggested as a useful tool to monitor the Twitter feed [7], and while previous work has addressed the efficiency that is required for this purpose, there has been little work on improving the effectiveness in over a decade [7, 8].
In this study, we propose a novel approach that is more effective that the widely used function proposed by Allen et al. that declares a story new if it is dissimilar to its nearest neighbor [1].
2. RELATED WORK
The task of detecting events can be automated using information about the events published online. For this purpose, the Topic Detection and Tracking (TDT) program was initiated to discuss applications and techniques to organize broadcast news stories by the real world events that they discuss in real-time. News stories are
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914761

gathered from several sources in parallel to create a single stream of constantly arriving news. The problem of first story detection is to identify the stories in a stream of news that contain discussion of a new topic, i.e. whose event has not been previously reported [6].
FSD has been recognized as the most difficult task in the research area of TDT [11]. In early work, Allen et al. detect first stories as news articles whose cosine similarity over tf-idf vectors to its nearest neighbor is less than a threshold, an effective approach that outperforms complex language model approaches in most cases [1]. This baseline is still used for FSD in recent work, in which more focus is put on efficiency than to improve effectiveness [3, 5, 4].
Papka and Allen, argue that a side-effect of the timely nature of broadcast news is that stories closer together on the news stream are more likely to discuss related topics than stories farther apart on the stream. When a significant new event occurs, there are usually several stories per day discussing it; over time, coverage of old events is displaced by more recent events. They use temporal proximity as a distinguishing feature to incorporate the salient properties of broadcast news [2, 6].
In recent work, Vuurens et al. proposed a novel 3-nearest neighbor clustering (3NN) approach to retrieve sentences from news articles that contain novel and useful news facts. In this approach every text is linked to its three nearest neighbors that must be from a different domain [10]. The so-called `2-degenerate cores' constructed by the algorithm correspond to highly similar texts from three different sources. Their existence indicates the importance or salience of the information contained. Temporal proximity is incorporated in the model by weighting the time between news articles in the similarity function used. In [9] normalized information gain is shown to be more effective than cosine similarity for the task of clustering news articles that are topically related.
3. METHOD
In this work, we adapt the 3NN clustering approach to First Story Detection, by clustering news articles rather than sentences, and using a similarity function based on normalized information gain to promote the clustering of news articles that are likely to be topically related.
3.1 Single Linkage
We compare our efforts to the approach described by Allen et al. [1], which is considered a state-of-the-art approach in recent studies on First Story Detection, e.g. [7, 4]. In this approach, documents are represented as tf-idf weighted vectors, and the novelty of a document d is estimated by the cosine similarity to its nearest neighbor n in the collection C [1]:

845

novelty(d) = 1 - max cos(d, n)

(1)

nC

Then, a news article is marked as a first story when its novelty is below a threshold   [0, 1].

3.2 3NN First Story Detection
In this study, we propose a novel approach that is based on 3nearest neighbor clustering (3NN), using the existing open source implementation [10]. In 3NN clustering, every node is assigned to its three nearest neighbors, not allowing links between nodes from the same news domain, and based on temporal proximity between publication dates which allows the clustering to be continuously updated in near real-time. 2-generate cluster cores are formed when three nodes each link to the other two as a one of its 3 nearest neighbors. These clusters contain information that is locally most central and therefore likely to be salient information [10]. The key idea for First Story Detection, is that acting on formed 3NN clusters rather than individual documents is less likely to return false positives. However, instead of truly detecting the first story as was the objective in the TDT program, here we aim to improve detection performance at the expense of slightly delayed detection. It may also be that the story detected as the first of a new event is more central to the information, and therefore more suitable as a seed to start tracking a topic, however, this hypothesis is outside the scope of this study and left for future work.
In [9], news sentences were fitted into a hierarchy that distinguishes between different events and topics by forming clusters of topically related the news articles, for which normalized information gain was shown to be more effective than cosine similarity. Therefore, to promote 3NN clusters to be formed around topically related news articles we use a similarity function based on normalized information gain. In Equation 2, the normalized information gain between two documents d and d results in a score of 0 between identical documents and a score of 1 between disjoint documents, by dividing the information gain IG between the documents by an upper bound of the information gain IGmax that would be obtained if these documents have the same internal distributions over terms but are completely disjoint. For the remainder of this paper we use IGsim as defined in Equation 3 as a similarity function between two documents d, d based on IGnorm.

IG(d, d )

IGnorm(d, d ) = IGmax(d, d )

(2)

IGsim(d, d ) = 1 - IGnorm(d, d )

(3)

From the obtained 3NN clustering, the newly formed 2-degenerate cores are inspected for first stories. Similar to the Single Linkage baseline, first stories are detected when a newly formed cluster core is dissimilar from news articles seen recently. In 3NN every news article is linked to its three nearest neighbors, therefore the members of a newly formed 2-degenerate core that contains a first story each have two links to the other core members and the third link links to a dissimilar news article. The most similar non-core news article that a core member links to, is then used to estimate the novelty of that cluster core. Formally, in Equation 4 a cluster core A is declared novel when the similarity between a news article d  A and a news article n in the remainder of the collection C is below a threshold novelty.

novelty(A)

=

max
dA,nC-A

I

Gsim(d,

n)

<

novelt

y

(4)

Lastly, we add a threshold to filter out newly formed clusters that are less likely to be topically related to each other. Vuurens et al. show that news articles that have a high normalized information gain are rarely topically related [9]. Following their findings, we filter out clusters that fail the coherence criterium in Equation 5, that enforces that the similarity between all nodes d, d that are members of the same 2-degenerate core A exceeds a threshold coherence, for which different settings are tried to examine the sensitivity and impact on effectiveness.

coherence(A) = min IGsim(d, d ) >  coherence (5)
dA,d A-{d}

3.3 Test set
For the evaluation, we use the TREC Temporal Summarization test sets of 2013 and 2014. The corpus for these test sets is the 2013 TREC KBA Streaming corpus, which contains approx. 150M news articles that are processed in a strict online setting. Table 1 shows the topics from the test sets, which are all types of a crisis that received continuous updates in the media over time. Arguably, the news regarding a single topic could be considered to be all part of the same story, or in some cases be regarded as separate stories within a topic. Here we regard all news articles that are matched to the same topic as part of one news story, for which ideally only the first article should be returned. TREC assessors annotated the sentences that TREC participants retrieved as relevant if they contain a news fact relevant to the topic.
The basis for the evaluation of the FSD systems is a list per topic of all documents that contain relevant news facts according to the TREC ground truth or the online published extended lists that contain duplicate sentences found in the collection. For the combined 23 topics, there are 65,358 documents that were annotated as containing relevant information. For this task, a returned news article is considered as a first for a topic when it is the first relevant article returned by the system, and a false alarm when another relevant article for the same topic was returned earlier. News articles that are not marked as relevant to the topic are ignored in the evaluation.

3.4 Experiment setup and evaluation metrics

The effectiveness of First Story Detection systems is measured

by the miss rate, false alarm rate, recall and precision, which we

explain using the contingencies in Table 2. For any topic, we only

consider articles that are annotated as relevant for the topic, thus if

T is the number of documents annotated as relevant for the topic,

then T P + FN + FP + T N = T . Since there can only be one first

story per topic per system, T P + FN = 1 and FP + T N = T - 1.

A miss occurs when the system fails to detect a new event, i.e.

miss

rate =

T

FN P+F

N

.

A

false

alarm

occurs

when

the

system

emits

a news article when a first story was already emitted for that topic,

i.e.

f alse

alarm

rate

=

F

FP P+T

N

.

Recall is the fraction of topics

for

which

a

first

story

was

detected

Recall

=

T

TP P+F N

,

and

Preci-

sion is the fraction of retrieved news articles that is a fist story

Precision

=

T

TP P+F P

,

which

here

only

considers

the

news

articles

that are relevant to the topic.

Table 2: Contingency table for evaluation metrics

Retrieved Not retrieved

First story TP

FN

Not first story FP

TN

846

Table 1: Topics for the 2013 and 2014 TREC TS track

Topic 1 2 3 4 5 6 8 9 10 12 13 14 15 16 17 18 19 20 21 22 23 24 25

Title 2012 Buenos Aires Rail Disaster 2012 Pakistan garment factory fires 2012 Aurora shooting Wisconsin Sikh temple shooting Hurricane Isaac (2012) Hurricane Sandy Typhoon Bopha 2012 Guatemala earthquake 2012 Tel Aviv bus bombing Early 2012 European cold wave 2013 Eastern Australia floods Boston Marathon bombings Port Said Stadium riot 2012 Afghanistan Quran burning protests In Amenas hostage crisis 2011-13 Russian protests 2012 Romanian protests 2012-13 Egyptian protests Chelyabinsk meteor 2013 Bulgarian protests against the Borisov cabinet 2013 Shahbag protests February 2013 nor'easter Christopher Dorner shootings and manhunt

4. RESULTS
In this Section, we compare the effectiveness of first story detection using Single Linkage (SL) to FSD using 3NN.
4.1 Effectiveness
In Figure 1, a DET curve shows the relationship between miss rate and false alarm rates. Overall, the 3NN runs perform better than SL, regardless of the setting used for coherence. In Figure 2, we show a tradeoff between recall and precision, which further supports that 3NN is consistently more effective than Single Linkage. Table 3 gives the precision and false alarm rate when the novelty thresholds for both systems are set to the highest precision that can be obtained at recall = 1. When  = 0.48 and novelty = 0.6 are set to allow for the lowest false alarm rate at a missed rate of 0 (i.e. recall=1), precision is respectively 0.0149 for SL and 0.0618 for 3NN, meaning that SL more redundantly retrieves 4 times more news articles for the same event.
Table 3: Optimal effectiveness at recall=1.
Precision false alarm rate Single Linkage  = 0.48 0.0149 0.0195
3NN novelty = 0.60 0.0618 0.0053

4.2 Timeliness
In Figure 3, the y-axis shows the aggregated number of relevant news articles per hour over time on the x-axis. In this Figure, we can visually compare the moment a first story was detected against the volume of published news articles. We can see that the systems occasionally missed early detection, e.g. 3NN for topic 3, and Single Linkage for topic 9. On topic 12, detection may be late for 3NN, but there is a difficult tradeoff between early detection and a lower false alarm rate.

1

missed rate

0.1

Single Linkage

3NN coherence = 0.5 3NN coherence = 0.4 3NN coherence = 0.6

0.0001

0.001

0.01

false alarm rate

Figure 1: Detection Error Tradeoff curve, closer to the origin is better.

1

0.8

Recall

0.6

0.4 Single Linkage

3NN coherence = 0.5

0.2

3NN coherence = 0.4

3NN coherence = 0.6

0

0.01

0.1

1

Precision

Figure 2: Plotted point show the Recall/Precision that correspond to the systems' effectiveness at the given threshold.

Some topics are related to an incident that is followed by a quick burst (e.g. topic 1), while other topics initially have a phase of little media attention and have intervals of increased interest later in time (e.g. topic 16). An interesting case is topic 18, which concerns the demonstrations that followed the Russian elections. For this topic, the news slowly shifted over the cause of days from a focus on the election itself to the steadily increasing demonstrations. This gradual shift towards a new topic is relatively difficult to detect for the approaches used in this study. The effective detection of these types of event may require a novel FSD approach that is not solely based on dissimilarity.
An inspection on the timeliness of the first stories detected reveals weaknesses in both approaches, and potentially an important aspect that should be taken into consideration in attempts to improve FSD. Timeliness of the detection is currently not addressed by the traditional evaluations that use a DET-curve and the tradeoff between recall and precision. To evaluate future work that ad-

847

dresses this issue, an additional metric to compare the timeliness of FSD approaches is required.
5. CONCLUSION
In this study, we propose a novel approach for the task of First Story Detection based on clustering news articles that are likely to be topically related, and estimating the novelty of newly formed clusters by comparison to previously seen news articles. We compared this approach to a baseline that estimates the novelty of a single news article by the cosine similarity to its nearest neighbor. The evaluation shows that the proposed model outperforms the existing baseline both in tradeoff between missed first stories and false positives, and in tradeoff between recall and precision. An analysis of the timeliness of the first story detections revealed that both systems missed early detection on some cases, and that there are specific cases such as evolving events that are particularly hard to detect.
Acknowledgment
This work was carried out with the support of SURF Foundation.
References
[1] J. Allan, V. Lavrenko, D. Malin, and R. Swan. Detections, bounds, and timelines: Umass and TDT-3. In Proceedings of TDT-3 Workshop, pages 167­174, 2000.
[2] J. Allan, R. Papka, and V. Lavrenko. On-line new event detection and tracking. In Proceedings of SIGIR 1998, pages 37­45. ACM, 1998.
[3] M. Karkali, F. Rousseau, A. Ntoulas, and M. Vazirgiannis. Efficient online novelty detection in news streams. In WISE 2013, pages 57­71. Springer, 2013.
[4] R. McCreadie, C. Macdonald, I. Ounis, M. Osborne, and S. Petrovic. Scalable distributed event detection for Twitter. In IEEE Big Data, pages 543­549. IEEE, 2013.
[5] M. Osborne, S. Petrovic, R. McCreadie, C. Macdonald, and I. Ounis. Bieber no more: First story detection using twitter and wikipedia. In SIGIR 2012 TAIA Workshop, 2012.
[6] R. Papka and J. Allan. Topic detection and tracking: Event clustering as a basis for first story detection. In Advances in Information Retrieval, pages 97­126. Springer, 2002.
[7] S. Petrovic´, M. Osborne, and V. Lavrenko. Streaming first story detection with application to twitter. In Proceedings of NAACL 2010, pages 181­189. ACL, 2010.
[8] S. Petrovic´, M. Osborne, and V. Lavrenko. Using paraphrases for improving first story detection in news and twitter. In Proceedings of NAACL 2012, pages 338­346, 2012.
[9] J. B. Vuurens, A. P. de Vries, R. Blanco, and P. Mika. Hierarchy construction for news summarizations. In Proceedings of SIGIR 2015 TAIA Workshop, 2015.
[10] J. B. Vuurens, A. P. de Vries, R. Blanco, and P. Mika. Online news tracking for ad-hoc information needs. In Proceedings of ICTIR 2015, pages 221­230. ACM, 2015.
[11] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topic-conditioned novelty detection. In Proceedings of SIGKDD 2002, pages 688­693. ACM, 2002.

80 60

VV

40

20

0

2 days

4 days

6 days

topic:1 8 days

200

VV

150

100

50

0

2 days

4 days

6 days

topic:2 8 days

500 400

V

300

200

100

0

V

topic:3

2 days

4 days

6 days

8 days

300

V

V

200

topic:4

100

0

2 days

4 days

6 days

8 days

400

300

V

V

200

100

0

topic:6

2 days

4 days

6 days

8 days

40

VV

topic:8

20

0

2 days

4 days

6 days

8 days

100 80

V

60

40

20

0

V

2 days

4 days

topic:9

6 days

8 days

15

10

V

5

0

20 days

V

topic:12

40 days

60 days

80 days

60

V

topic:13

40

20

0

20 days 40 days 60 days 80 days 100 days 120 days 140 days

15

VV

topic:15

10

5

0

20 days

40 days

60 days

80 days

30

20

V

V topic:16

10

0

20 days

40 days

60 days

12

10 8

V

V

6

4

2

0

20 days

40 days

topic:18

60 days

80 days

12

10 8

V

6

4

2

0

topic:19

20 days

40 days

60 days

300

VV

200

100

0

2 days

4 days

6 days

topic:21 8 days

30

V

20

10

0

V topic:22

2 days

4 days

6 days

8 days

10 days

15

V

V

10

5

0

topic:23

5 days

10 days

15 days

Figure 3: On the y-axis is the number of relevant news articles for the topic per hour, over time on the x-axis. A red V indicates when a fist story is detected by 3NN coherence = 0.5, and a black V indicates when a first story is detected by Single Linkage, both at the `optimal' novelty threshold that obtained recall=1 and the highest precision.

848

Identifying Careless Workers in Crowdsourcing Platforms: A Game Theory Approach

Yashar Moshfeghi
School of Computing Science University of Glasgow Glasgow, UK
Yashar.Moshfeghi@ glasgow.ac.uk

Alvaro F. Huertas-Rosero
School of Computing Science University of Glasgow Glasgow, UK
Alvaro.Huertas@ glasgow.ac.uk

Joemon M. Jose
School of Computing Science University of Glasgow Glasgow, UK
Joemon.Jose@ glasgow.ac.uk

ABSTRACT
In this paper we introduce a game scenario for crowdsourcing (CS) using incentives as a bait for careless (gambler) workers, who respond to them in a characteristic way. We hypothesise that careless workers are risk-inclined and can be detected in the game scenario by their use of time, and test this hypothesis in two steps: first, we formulate and prove a theorem stating that a risk-inclined worker will react to competition with shorter Task Completion Time (TCT) than a risk-neutral or risk-averse worker. Second, we check if the game scenario introduces a link between TCT and performance, by performing a crowdsourced evaluation using 35 topics from the TREC-8 collection. Experimental evidence confirms our hypothesis, showing that TCT can be used as a powerful discrimination factor to detect careless workers. This is a valuable result in the quest for quality assurance in CS-based micro tasks such as relevance assessment.
Keywords: Game Theory, Crowdsourcing, Relevance Assessment, Chicken Game
1. INTRODUCTION
Crowdsourcing (CS) platforms offer new ways of collecting relevance assessments for IR evaluation and are already used widely [4]. The advantages associated with this platform, i.e. low monetary cost and high task completion speed are, however, entangled with a mixed output quality, making the effective use of CS a non-trivial problem. Past studies have shown the importance of careful quality assurance [8], particularly when the task is `micro', such as relevance assessment (RA), and involves financial incentives [6].
Research in quality assurance for CS has been largely heuristic and non-generalisable. Zhao reports in 2012 that only 9 out of 55 studies of crowdsourcing include any kind of theoretical bases [14]. Quality is usually assured with the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914756

use of "gold standard" data, multiple answers [5], qualification tests, honey pots (i.e. questions whose answers are known in advance) or a combination of the above. Such quality assurance techniques can potentially increase the general experiment cost, task completion time, and experimenters' intervention [2].
We propose a systematic approach to the problem based on spotting workers who undertake the task as gambling. A worker taking insufficient time to complete a task is knowingly taking the risk of getting a wrong result [9], so we will assume that careless workers (gamblers) are risk-inclined [10]. Therefore we formulate our research question as: "Is workers' inclination towards risk, reflected in low Task Completion Times (TCT) in a competition scenario, a good predictor of poor performance?"
We propose a competitive game where we motivate workers to finish their RA task correctly (pushing towards longer time) and fast (pushing towards shorter time). As a result of these two opposing forces, TCTs will become sensitive to the worker's attitudes towards risk and their perceptions of the task and payoff, i.e. workers adjust the time they take to perform a task to be longer or shorter according to whether they perceive the risk of not taking enough time as more or less important than being fast.
The rest of the paper is organised as follows: our game theoretical model is described in section 2, the theorem linking risk inclination with TCT is stated and proved in Section 3. Section 4 describes the empirical experiment where we link TCT and performance, results and discussion are in Section 5. Finally we conclude in Section 6.
2. THEORETICAL MODEL
We start from a theoretical model based on Game Theory, which builds on a few assumptions and leads us to our first result, linking TCT with inclination to risk. This result motivates a CS experiment where we check the correlation between TCT and performance, completing our argument for detecting careless workers by their TCT under competition conditions.
Best scenarios, worst scenarios and risk: Attitudes towards risk can be viewed as ways of perceiving utility [10], relative to the probabilistic average. A risk-inclined player perceives uncertain utility as higher than the average, a riskneutral player perceives it exactly like the average, and a risk-averse player perceives it as lower than the average. Fig. 1 illustrates these pessimistic and optimistic views.

857

PERCEIVED PROBABILITY OF BEST OUTCOME
1
Riskinclined

Perceived

0 0

Riskaverse

Objective

1

Figure 1: Perceived probability of getting the highest utility for
different attitudes towards risk. Risk-inclined players will perceive a higher probability in uncertain situations (convex, upper curve), risk-neutral players will perceive a probabilistic mean utility (linear, middle curve) and risk-averse players will perceive a lower probability in uncertain situations (concave, lower curve).

Risk inclined workers are more sensitive to incentives to compete, because they perceive an uncertain outcome as more profitable. Risk averse workers, on the other hand, are more interested in certain aspects of the game, e.g. noncompetitive incentives that do not depend on other workers.
The Mathematical Concept of Game: A game, in the context of mathematical game theory, consists of three components: (i) several agents (players), (ii) a set of decisions (actions) they take based on a set of strategies, and (iii) an outcome (payoff) which is aimed to be maximised by considering their own and other players' strategies.
It is important to note that a mathematically defined game differs from gamification scenarios, even though it shares common elements. Gamification involve managing affective incentives, as is the case of image labeling games (called ESP) or Games With a Purpose [12]. These are not necessarily game-theoretical approaches, but they do involve quantifying and managing entertainment rewards in order to improve the work obtained from volunteers. Gamification has been used for RA tasks in [3], where individuals were persuaded to assess relevance as entertainment. Our approach relies entirely on monetary rewards, and models the motivation of workers as a simple rational principle of maximising monetary gain.
Chicken Game for RA: The Chicken game, also known as Hawk-Dove game [11] is a particular game setting based on time choices, whose characteristics are determined mostly by competition. In the n-player version, a group of teenagers compete with each other for the prestige of being the bravest. They engage in a racing game towards a cliff in their cars as fast as they can, and the player who jumps out of the car the last without falling off the cliff is the winner. Each player chooses the time for an action, and the outcome depends on the times other players chose. A rational player will aim to an optimal trade-off between the two opposing forces of the game, i.e. one preventing them from jumping too quick (competition) and the other pushing them to jump before it is too late (survival). When each player is uncertain about the choices of the other players (i.e. imperfect information), the chicken game can present a Bayesian Nash Equilibrium (BE), which is an important solution concept in game theory. In a BE the strategy of a player is optimal according to their available private knowledge about what the other players would do. Their expected payoff cannot increase by changing their strategy.
We propose a game in which an assessor is paid only if he or she produces a correct RA result. If no other assessor

produced a correct result quicker, he or she also gets an extra bonus. This is a similar but inverse version of the Chicken game: in our setting a player needs to choose a time longer than the minimum, but shorter than the other players, while in the Chicken game a player needs to choose a time shorter than a limit (i.e. when they reach the cliff), but longer than the other players.

TASK

PROBABILISTIC MODEL

yes
OTHER FACTORS
PF

ENOUGH TIME?
no

( 1 - PF )

TOTALLY RANDOM

PR
RIGHT

( 1 - PR )
WRONG

Figure 2: Graph representation of a probabilistic model of an as-
sessment. The rhomboid represents the condition of taking enough time, and ellipses represent random processes: left for hidden but de-
terministic factors, and right for randomness. The arrows with their probabilities, represent the possible outcomes of each subprocess.

In our model, time is related to payoff through the probability of a correct assessment. If a worker takes an insufficient time, their result will be completely random. In Fig. 2 we represent our model as a probability graph: the condition of taking enough time determines whether the result is random (with probability PR of being correct). If the worker takes enough time to complete an assessment, then other processes involving knowledge, difficulty, etc. determine whether the worker gets the correct result (with probability PF ).
The minimum time is uncertain, with a probability distribution D(tmin). The probability of getting the answer right when taking time t will be a monotonically increasing function:
t
Pcorrect(t) = PR + (PF - PR) D(tmin)dtmin (1)
0

Similarly, given a distribution of times when the quickest worker will finish the task Q(t):

t

Pquickest(t) = 1 - Q(t)dt

(2)

0

Expected utility is then:

U (t) = KRPcorrect(t)Pquickest(t) - KP (1 - Pcorrect(t)) (3)
where KR is the value of the quickness reward and KP the incorrectness punishment. Note that the term with KP is monotonically increasing, while that with KR has a maximum. This means that the position of the maximum will only change notably with these paremeters when KP > KR (small incentive for competing).

3. TCT AND INCLINATION TOWARDS RISK
The following theorem links risk inclination to short TCTs in the game scenario, assuming rationality and condition of BE:
Theorem 1. In the BE of our game, optimal times for risk-inclined players will be shorter than those for risk-neutral or risk-averse players.

858

Proof. A situation of BE is defined by maximal values of U (t), i.e. dU (t)/dt = 0. From Eq. 3:
KR(Pquick(dPcorrect/dt) + Pcorrect(dPquick/dt)) + KP (dPcorrect/dt) = 0 (4)
From explicit expressions for Pcorrect(t) and Pquick we obtain the condition for an optimal time top:
KR(PquickD(top) - PcorrectQ(top)) + KP D(top) = 0 (5)

negative derivative

Figure 3: Utility as a function of TCT with objective probabilities
(a) and perceived by a risk-inclined player (b). Optimal time top and gambler's perceived optimal time tg-op are marked with dotted lines.

A gambler over-estimates propitious probabilities, which we represent with a convex function P  = F(P ) and underestimate adverse probabilities, which we represent as a concave function P  = F(P ). which we represented as a concave function P  = F(P ). These are the monotonic transforms depicted in Fig. 1. Optimal time for gamblers tg-op is calculated wit These monotonic transforms are depicted in Fig. 1. Using the chain formula dF (P )/dt = (dF/dP )(dP/dt) we obtain a gambler's optimal time:

KR (dF /dPcorrect)Pquick D(tg-op )

-KR (dF /dPquick )Pcorrect Q(tg-op )

(6)

+KP (dF/dPcorrectD(tg-op))

=0

Since Eq 6 correspond to the maximal gambler utility U (tg-op), the derivative will be positive for shorter times t < tg-op and negative for longer times t > tg-op.
We know that both F and F are monotonously increasing, so their derivatives are always positive. We also know that in the region of high probabilities of being correct and quickest (where we expect optimal times) a convex function will have a lower derivative than a concave function, so dF/dP < dF/dP .
This means that the negative term in Eq 6 has become larger than that in Eq 5
KR(dF/dPquick)PcorrectQ(top) < KRPcorrectQ(top) (7)

The positive term in Eq 6, on the contrary, has become smaller than that in Eq 5

(dF/dPcorrect)(KRPquickD(top) + KP D(top)) < (KRPquickD(top) + KP D(top)) (8)

This means that overall, the derivative will be negative, and therefore top > tg-op. (this argument is depicted in Fig. 3)

4. EXPERIMENTAL METHODOLOGY
In Section 3 we proved that a risk-averse worker would tend to take shorter TCT than others in the game scenario, because of their perception of the competition incentive. In this section, we devise an between-group study to investigate the relation between TCT and precision in the competitive scenario where the independent variable to be competition setting (with two levels: "Base" and "Game"), differing in the description and pay conditions given to the participants. The dependent variables are the accuracy of gathered labels and corresponding TCTs. We make use of Amazon's Mechanical Turk (M-Turk), as our crowdsourcing platform. It provides a convenient participant pool to draw upon to carry out many tasks, especially relevance assessment labelling.
Data Collection: We used TREC-8, in particular the LA Times, FT, FR94 and FBIS sub-collections as our test collection. We randomly selected 35 out of 50 topics in the TREC-8 test set and used the judged articles as a gold standard set, following previous research in the domain, e.g. [1].
Task: We define two tasks, i.e. Base and Game. In both tasks the participants were asked to evaluate the relevance of a set of document-topic pairs. We used plain text to indicate the task rather than the original TREC instructions, according to the suggestions given in [7] for M-Turk crowds. We adopt the concept of relevance used in [13] which has been used in previous work in this domain [1]. We followed the guidelines provided in [1] to instruct our workers, e.g. asking for their consistency with their judgement, etc.
Base Task: We asked participants to perform the relevance judgment task for ten document-topic pairs. In order to ensure quality we highlighted that random assessment will not work, because we know the answer of a few of the pairs. If they fail to assess them correctly, their HIT will not be approved. We also highlighted that presence of topic terms in the document does not necessary mean that the document is relevant to the given topic. They need to make a decision by considering more than solely this criteria.
Game Task: For the game task, in addition to above, we instructed the worker that they would be competing with a few other M-Turk workers in completing the task. In order to create the conditions of the game, we highlighted that they need to complete the assessment correctly and quicker than the other workers. We also mentioned that the winner will be with rewarded with a $2.0 additional bonus.
Relevance Assessment System: For the completion of the tasks we used a custom-made relevance assessment environment designed to gather the workers' judgement, while retaining a minimum of graphical elements and distractions. The back-end layer of the system created a set of random ten topic-document pairs from the QRel file and fetched the associated document and topic information from collections and topic files, respectively. It also ensured that workers judge a topic-document pair only once, while each topicdocument pair is judged by no fewer than three workers and no more than five. The documents were presented without their title in order to avoid it being used as the only criterion. The user Interface (UI) contained a topic, document and closed question (binary relevance; yes/no). Workers were required to log in to the system in order to capture their Worker ID. After workers log in, the UI layer gets a set of ten topic-document pairs from the back-end layer and presents them, one at a time, in the same exact order as

859

they were obtained. The interface also prevents them from skipping difficult assessments.
Workers' actions were monitored and logged by the search interface, including the log-in and log-out time to the system, workers' as well as their judgement of each documenttopic pair. The length of time workers spent to judge a pair was calculated on client side to avoid interference from network delay: it is computed as the time difference between the moment a topic-document pair was shown until the assessment was submitted.
Procedure: At the beginning of the experiment, participants were instructed that the experiment would take approximately 10 minutes to complete, though they would be given 30 minutes between the time they accepted and submitted the HIT assignment. They were informed that they could participate in each task more than once.Workers could only accept the HIT if they agreed to a consent form. Subsequently, participants were assigned to one of two external surveys ("base" or "game"). At the beginning of the survey, we describe that all the collected data will be treated as confidential and anonymous. Payment for HIT completion was $1.0. The total cost of the evaluation was $270, including the cost of the pilot studies and some of the rejected participants, which we consider to be cost-effective.
5. RESULTS AND DISCUSSION
This section presents the experimental findings of our study. To fulfil the conditions of a between-group study, we eliminated participants whom we found had taken part in the experiment under different conditions (e.g. taken part in the Game task after taking part in the Base task, or vice versa). Out of 235 workers who started the tasks, 200 workers completed them successfully (100 workers per group).
Figure 4: Distributions of TCT (in minutes) for four ranges of pre-
cision, in a non-competitive base scenario (a) and game scenario (b). Bars represent the quartiles of the distribution, and the red line connects the mean values.
Figure 4 shows the box plot corresponding to the TCTs of each of four categories according to their fraction of correct results (precision): 0.2 to 0.4, 0.4 to 0.6, 0.6 to 0.8 and 0.8 to 1.0, and show the distribution of TCT in each category by their minimum, first, second (median) third and maximum quartiles. across data collections and tasks. We found that distributions of task completion time changed notably between the Base and Game tasks. As shown in Fig. 4 (b), we can see that there is a clear tendency of the lowest performance categories to low values of TCT in the game (competitive) scenario. In the base task scenario,

however, the values of task completion time did not seem to bear any relation to performance, as shown in Fig. 4 (a). Therefore, we confirmed that lower performing players react to our game competing scenario by lowering their TCT and hence turning TCT into a powerful discriminative factor.
Normally in settings like the base task, extra information about users (previous performance, background, etc.) or hints about their current performances (previously known judgments, test questions) are used to detect untrustworthy workers. These strategies, however, require settings that are potentially more expensive and/or discouraging for workers.
6. CONCLUSION
In this paper we hypothesised that it is possible to design a mechanism based on a game theory model that makes taskcompletion time an effective discriminative factor between careless (i.e. risk-inclined) and other (i.e. risk-neutral or risk-averse) workers. We therefore designed a CS relevance assessment task scenario inspired by an n-player Chicken game model, where there are two opposing forces: one pushing towards quickness and another pushing towards correctness. We tested our hypotheses with a crowdsourced evaluation using 35 topics from TREC-8 collection. Our findings show that the proposed framework allows to use task completion time as a powerful discriminative factor to identify different types of workers. An obvious direction to continue the research on this promising approach is to explore postfiltering methodologies based in risk attitudes inferred from TCT and other indirect data.
7. REFERENCES
[1] O. Alonso. Implementing crowdsourcing-based relevance experimentation: an industrial perspective. Inf. Retr., pages 1­20, 2013.
[2] B. Carterette and I. Soboroff. The Effect of Assessor Error on IR System Evaluation. In SIGIR '10, pages 539­546, 2010.
[3] C. Eickhoff, C. G. Harris, A. P. de Vries, and P. Srinivasan. Quality through flow and immersion: gamifying crowdsourced relevance assessments. In SIGIR '12, pages 871­880, 2012.
[4] C. Grady and M. Lease. Crowdsourcing document relevance assessment with Mechanical Turk. In CSLDAMT '10, pages 172­179, 2010.
[5] P. G. Ipeirotis, F. Provost, and J. Wang. [6] G. Kazai, J. Kamps, and N. Milic-Frayling. The face of quality
in crowdsourcing relevance labels: Demographics, personality and labeling accuracy. In CIKM '12, pages 2583­2586, 2012. [7] G. Kazai, J. Kamps, and N. Milic-Frayling. An Analysis of Human Factors and Label Accuracy in Crowdsourcing Relevance Judgments. Inf. Retr., 16(2):138­178, Apr. 2013. [8] J. Le, A. Edmonds, V. Hester, and L. Biewald. Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution. In SIGIR 2010 workshop on crowdsourcing for search evaluation, pages 21­26, 2010. [9] Y. Moshfeghi, A. F. H. Rosero, and J. M. Jose. A Game-Theory Approach for Effective Crowdsource-Based Relevance Assessment. ACM Trans. Intell. Syst. Technol., 7(4):55:1­55:25, Mar. 2016. [10] T. Straub, H. Gimpel, and F. Teschner. The negative effect of feedback on performance in crowd labor tournaments. Collective Intelligence 2014, 2014. [11] M. Szilagyi. Agent-Based Simulation of the N-Person Chicken Game. In Advances in Dynamic Game Theory, volume 9, pages 696­703. Birkh¨auser Boston, 2007. [12] L. Von Ahn and L. Dabbish. Designing games with a purpose. Communications of the ACM, 51(8):58­67, 2008. [13] E. Voorhees, D. Harman, N. I. of Standards, and T. (US). TREC: Experiment and evaluation in information retrieval, volume 63. MIT press Cambridge, 2005. [14] Y. Zhao and Q. Zhu. Evaluation on crowdsourcing research: Current status and future direction. Information Systems Frontiers, pages 1­18, 2012.

860

Impact of Review-Set Selection on Human Assessment for Text Classification

Adam Roegiest
University of Waterloo

Gordon V. Cormack
University of Waterloo

ABSTRACT
In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p<0.0002] across a population of documents including both relevant and non-relevant documents, for several definitions of ground truth. This result is in accord with Smucker and Jethani's SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is malleable, and that one should take care in assuming any labeling to be ground truth, whether for training, tuning, or evaluating text classifiers.
1. INTRODUCTION
In supervised learning for text classification, each of a set of training documents is labeled as relevant or non-relevant by a human assessor. The training documents and their labels are used to induce a classifier to predict the relevance or non-relevance of the remaining documents in the population from which the training documents were drawn. Training documents may be selected using random sampling, or using active learning methods such as uncertainty sampling or relevance sampling [8, 7]. Simulation studies comparing these approaches typically rely on the assumption that the sampling strategy does not influence how the assessor will label a particular document. We show, in a study involving 36 paid assessors recruited from a university community, that the same documents are much more likely to be judged relevant when embedded in a set selected by random sampling or uncertainty sampling, than one selected by relevance sampling.
The influence of sampling strategy on labeling has impact beyond the selection of training sets for inducing classifiers. In technology-assisted review, where every document
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '16 July 17-21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4069-4/16/07. DOI: http://dx.doi.org/10.1145/2911451.2914709

with a positive classification is assessed and labeled, a superior (higher precision) classifier might yield fewer labeledrelevant documents than an inferior (lower precision) classifier. In the Cranfield approach to IR evaluation (see [13]), a set of relevance assessments based on a pool of likely relevant documents might yield substantially different results from one based a sample of the whole. Our result calls into question the common practice of estimating measures like recall and precision from statistical samples, especially those employing non-uniform inclusion probabilities.
A number of studies [11, 12, 6, 10, 5] have shown that the proportion, as well as the order of presentation of relevant and non-relevant documents, can affect user assessment behaviour. Taken together, the results suggest that assessors are less likely to label documents relevant once they have seen a number of relevant documents, either due to presentation order or due to the overall proportion of relevant documents. This observation forms the basis of the question we addressed: Since relevance sampling produces a higher proportion of relevant documents than uncertainty sampling or random sampling, does it suppress the assessor's propensity to judge a document relevant? We further addressed the question of whether this effect was conditioned on ground truth, for several definitions of ground truth.
2. EXPERIMENTAL DESIGN
Our design specified that assessors would review batches of 100 documents for relevance to several topics, where the batches for each topic contained the same 12 known documents, and 88 documents selected by one of random sampling, uncertainty sampling, or relevance sampling.1 There were 9 topics, and hence 36 batches in total; each batch was assessed by three different assessors.
Table 1 provides a glossary of terms specific to our experimental design.
2.1 Documents and Labels
Documents were selected from the TREC-6 Ad Hoc collection, which has been the subject of previous relevance assessment studies [14, 9, 4], and has two independent sets of relevance assessments: the official NIST binary relevance assessments ("relevant," and "not relevant") created using the pooling method, and a set of graded relevance assess-
1Due to an error in our setup that went undetected until the assessments were complete, for some topics, the batches contained only 10 or 11 of the same known documents. The corresponding shortfall reduced the statistical power of our experiment, but does not affect its validity.

861

Batch
Known documents Context
NIST assessments Waterloo assessments Rel
CAL
SAL
SPL

100 documents presented to assessors for review, consisting of known documents, and context documents 12 common documents presented for assessment with different contexts The manner in which the documents other than the known documents in a batch are selected Relevance assessments rendered by NIST for TREC 6 Relevance assessments rendered by the University of Waterloo at TREC 6 The relevance class of a document, as determined by some combination NIST and/or Waterloo assessments Context in which documents are selected iteratively using relevance sampling [2] Context in which documents are selected iteratively using uncertainty sampling [2] Context in which documents are selected at random [2]

Table 1: Glossary of terms used throughout this work.

ments ("relevant," "iffy," and "not relevant") constructed by the University of Waterloo using interactive search and judging [1]. We augmented each set of assessments with an additional category "unjudged" for documents that were excluded from the pool. The net effect is that each document has one of 12 combinations of three NIST and four Waterloo assessment categories. We chose at random one document with each combination of assessments as the 12 known documents for each of 9 topics.
Our selection of topics was predicated on the fact that, of the 50 topics in the collection, only the nine we chose had at least one document labeled with each of the 12 combinations of assessment categories. Using each sampling method, as discussed below, we selected a list of 90 documents as detailed below, and inserted the known documents at fixed positions, chosen at random. Surplus documents (at positions beyond 100) were discarded.
For random sampling, the list of 100 documents was a uniform random sample of the 560,000-document TREC corpus, in random order. Following Cormack and Grossman [2], we label this protocol, simple passive learning ("SPL"). For uncertainty sampling and relevance sampling, we employed the simple active learning ("SAL") and continuous active learning ("CAL") methods [2, 3], training a classifier to retrieve 10 documents, adding those documents to the training set, and repeating the process nine times. Sofia-ML was used as the base classifier, configured to minimize logistic loss, and applied to a tf-idf representation of the documents. The initial training set consisted of a positively labeled pseudo-document consisting of the topic description, plus 100 negatively labeled documents selected at random without regard to their relevance. The training set was used to train the classifier, which was used to compute the likelihood of relevance for each document in the collection. For SAL, the 10 documents with likelihood closest to 0.5 were selected and added to the training set; for CAL, the 10 docu-

ments with greatest likelihood were selected. Training labels were derived from the Waterloo assessments: "relevant" and "iffy" were labeled positive; "non-relevant" and "unjudged" were labeled negative.
Our list of 90 context documents consisted of the documents retrieved by these nine iterations, in the order retrieved. Table 2 depicts the prevalence of positive (Waterloo "relevant" or "iffy") documents in the corpus, as well as the number in each batch, including known documents.

Topic
301 304 306 307 319 324 332 337 343

Corpus Prevalence (%)
0.24 0.13 0.11 0.14 0.17 0.09 0.08 0.11 0.12

Context Count

CAL SAL SPL

40

6

5

26

4

4

41

6

6

65

5

5

63

6

4

74

5

5

41

5

5

83

6

6

33

5

6

Table 2: Corpus prevalence and number of positive documents for each context, where Waterloo "relevant" and "iffy" assessments are considered positive. The counts for each batch include known documents.

2.2 Assessment Protocol
Following approval from the ethics review panel, we recruited 36 participants at large from University of Waterloo, including undergraduate students, graduate students, and faculty. At the outset, a participant was assigned one of the methods and topics at random, and was told that they would be remunerated $20 for reviewing all 100 documents. If the participant took 2 hours or less to judge the documents and achieved at least 25% recall and 25% precision with respect to the NIST assessments for the 100 documents, they were paid a bonus of $10 and offered the opportunity to assess up to two additional batches. For each subsequent batch, a new context and topic were selected at random, such that no participant saw the same context or the same topic more than once. Participants whose assessments did not meet the criteria were paid $20 and not invited to continue. The criteria and bonus were used to encourage participants to perform to the best of their ability. The recall and precision cutoffs were chosen with the intention of ensuring quality but not forcing users to be NIST-quality. 19 participants assessed 3 batches, 7 participants completed 2 batches, and 10 participants completed 1 batch. In total, three batches representing each context and each topic were assessed. The the continuation criterion was intended to limit the impact of poor assessments, while the random context and topic assignment without repetition was intended to mitigate learning effects.
2.3 User Interface
Participants conducted their assessments using a full-screen HTML interface, that displayed panels containing:
· The topic title and description.
· The document, with topic title words highlighted.

862

· Progress information, including the number of documents reviewed, elapsed time for the current document, cumulative time per document, and target time per document.
· Single-click action buttons to render an assessment and proceed to the next document.
2.4 Evaluation
The outcome of general interest is the probability Pr[User+] that an assessor will render a positive judgement. The primary predictor variable is the context within which the document is assessed. Accordingly, we wish to measure the conditional probability Pr[User+|Context] in order to test the hypothesis that Pr[User+|CAL] < Pr[User+|SALSPL]. Assuming this hypothesis to be supported, we wish to test whether, individually, Pr[User+|CAL] < Pr[User+|SAL] and Pr[User+|CAL] < Pr[User+|SPL].
A second predictor variable is the relevance class Rel of the document, as determined by some combination of Waterloo and NIST assessments. To preserve the statistical power of our experiment, we restrict our consideration to the class W-RI, and its complement W-NU, where W-RI denotes any combination of assessments for which the Waterloo assessment is either "relevant" or "iffy." We assumed that the hypothesis Pr[User+|W-RI] > Pr[User+|W-NU] was extremely unlikely to be rejected, and concerned ourselves instead with the two hypotheses:
(1)Pr[User+|CAL  W-RI] < Pr[User+|(SAL  SPL)  W-RI]
(2)Pr[User+|CALW-NU] < Pr[User+|(SALSPL)W-NU]
To estimate the conditional probabilities, we computed the fraction of positive assessments for documents satisfying the specified predictors. To evaluate the significance of hypothesized differences, we applied a paired binomial test, where possible, to corresponding batches. There are an equal number of batches for each context, and for each of the relevance classes W-RI and W-NU. Across these sets, batches may be matched by topic and by the specific combination of relevance assessments, leaving us to match within corresponding triples of batches, each reviewed by a different assessor. We matched the members of these triples by the assessor's experience: As far as possible, the first batch reviewed by one assessor was matched to the first batch reviewed by another assessor; the second batch reviewed by one assessor was matched to the second match reviewed by another assessor; and so on.

Predictor Context: CAL Context: SAL Context: SPL
Rel: W-RI Rel: W-NU

Pr[User+|Predictor] 0.42 (0.36,0.48) 0.54 (0.48,0.59) 0.54 (0.48,0.60) 0.63 (0.58,0.68) 0.39 (0.34,0.43)

p-value -
0.0002 0.0002
< 0.0001

Table 3: Probability of a study participant making a positive assessment, with 95% confidence intervals, for the primary predictors. For context, p-values were computed using a two-tailed paired binomial test; for relevance, p-values were computed using a z-test for difference in proportions. the CAL context.

Predictor CAL and W-RI SAL and W-RI SPL and W-RI CAL and W-NU SAL and W-NU SPL and W-NU

Pr[User+|Predictor] 0.52 (0.43,0.61) 0.67 (0.58,0.75) 0.70 (0.62,0.78) 0.33 (0.26,0.41) 0.42 (0.34,0.50) 0.40 (0.32,0.48)

p-value -
0.0037 0.0005
0.0288 0.1214

Table 4: Probability of a study participant making a positive assessment, with 95% confidence intervals, for combined predictors. p-values were computed relative to CAL, using a two-tailed paired binomial test.

1.0
CAL

SAL

0.8

SPL

0.6

Pr[User + ]

0.4

0.2

0.0 UU NU UN IU NN NR IN RU RN IR UR RR Category
Figure 1: Probability of positive assessment given a context and elementary relevance class. Relevance classes are denoted xy where x  R, I, N, U denotes Waterloo relevant, iffy, non-relevant and unjudged, and x  R, N, U denotes NIST relevant, non-relevant, and unjudged.
3. RESULTS
Table 3 shows the results of our primary hypotheses, that context and relevance class separately influence the probability of a positive assessment. Separately, SAL and SPL both yield a substantially and significantly higher probability of positive assessment than CAL; W-RI yields a substantially and significantly higher probability of positive assessment than W-NU.
Table 4 shows the combined effect of context and relevance class. With respect to the W-NU relevance class, SAL and SPL separately yield a substantially and significantly higher probability of positive assessment than CAL. With respect to W-RI, the difference appears to be substantive, but only the difference between CAL and SAL appears to be significant, and even so would not be significant under Bonferroni correction for multiple hypothesis testing. The difference Pr[User+|CAL  W-RI] - Pr[User+|(SAL  SPL)  W - R)] is significant (p 0.0288), by the following argument: for the null hypothesis to be true, it would be necessary that Pr[User+|CAL  W-RI]  Pr[User+|SAL  W-RI)] and Pr[User+|CAL  W-RI]  Pr[User+|SPL  W-RI)]. The probability of both of these occurring by chance cannot exceed the probability of either one, which implies

863

CAL

SAL

40

SPL

Time Taken (in seconds)

30

20

10

0 UU NU UN IU NN NR IN RU RN IR UR RR Category
Figure 2: Average time for assessment given a context and elementary relevance class. Relevance classes are denoted xy where x  R, I, N, U denotes Waterloo relevant, iffy, non-relevant and unjudged, and x  R, N, U denotes NIST relevant, non-relevant, and unjudged
p < min(0.0288, 0.1214) = 0.0288. For each combination of Waterloo and NIST assessments, Figure 1 plots the probability of a positive user assessment. Consistent with our statistical findings, the curves for SAL and SPL are generally superior to the curve for CAL. It appears that for cases where one of the Waterloo or NIST assessments is "relevant" and is not discordant with the other (i.e., the other is "relevant" or "unjudged") there may be an insubstantial difference between CAL and the other contexts. Whether this observation reflects chance or an effect is a subject for future research.
4. ASSESSMENT TIME
We collected timing information in the course of implementing our participant retention criteria. Table 5 indicates that neither the context nor the relevance class has a substantial or significant effect on the time taken by participants to review documents. Figure 2, which plots assessment time taken against elementary relevance classes, suggests that non-relevant documents that were included in both the Waterloo and NIST judging pools may take longer to review. An interesting avenue of research would be to investigate whether this observation is a manifestation of the observation by Smucker and Jethani [12], who found that assessors took longer to make incorrect assessments. Namely, we are interested in answering the following question: Are the long prediction times observed for the NN relevance class the result of false positives?
5. CONCLUSIONS
Our results show clearly that assessors are less likely to judge documents relevant when they are presented within the context of documents selected using relevance sampling, than when they are presented within the context of documents selected using uncertainty sampling or random sampling. The effect holds for both relevant and non-relevant

Predictor Rel: W-RI Rel: W-NU Context: CAL Context: SAL Context: SPL

Time Taken Per Doc 26.56 (23.86,29.27) 23.92 (21.19,26.64) 26.44 (22.69, 30.19) 23.53 (20.58, 26.48) 25.46 (22.21, 28.72)

p-value
0.1775
0.1984 0.6781

Table 5: Average time, in seconds, taken to assess documents under each condition with 95% confidence intervals for both all documents and known documents only. For context, p-values are with respect to a paired two-tailed t-test against the CAL predictor. For relevance, p-values are from Welch's t-test.

documents, as determined by archived assessments rendered by the University of Waterloo. Whether this effect applies to all relevant documents, or only to marginally relevant documents, remains an open question.
Our results call into question the practice of deeming the assessments of one individual to be authoritative [15], or of assuming that validation based on sampling is equivalent to validation using the pooling method.
6. REFERENCES
[1] G. V. Cormack, C. L. A. Clarke, C. R. Palmer, and S. S. L. To. Passage-Based Refinement (MultiText Experiments for TREC-6). In Proc. TREC-6, 1997.
[2] G. V. Cormack and M. R. Grossman. Evaluation of Machine-learning Protocols for Technology-assisted Review in Electronic Discovery. In Proc. SIGIR 2014, 2014.
[3] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv Preprint, 2015.
[4] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In Proc. SIGIR 1998, 1998.
[5] M. Eisenberg and C. Barry. Order effects: A study of the possible influence of presentation order on user judgments of document relevance. J. Amer. Soc. Info. Sci., 1988.
[6] M.-h. Huang and H.-y. Wang. The influence of document presentation order and number of documents judged on users' judgments of relevance. J. Amer. Soc. Info. Sci., 55(11), 2004.
[7] D. D. Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. SIGIR Forum, 29(2), Sept. 1995.
[8] D. D. Lewis and W. A. Gale. A sequential algorithm for training text classifiers. In Proc. SIGIR 1994, 1994.
[9] A. Roegiest, G. V. Cormack, C. L. A. Clarke, and M. R. Grossman. Impact of surrogate assessments on high-recall retrieval. In Proc. SIGIR 2015, 2015.
[10] F. Scholer, D. Kelly, W.-C. Wu, H. S. Lee, and W. Webber. The effect of threshold priming and need for cognition on relevance calibration and assessment. In Proc. SIGIR 2013, 2013.
[11] M. D. Smucker and C. P. Jethani. Human performance and retrieval precision revisited. In Proc. SIGIR 2010, 2010.
[12] M. D. Smucker and C. P. Jethani. Time to judge relevance as an indicator of assessor error. In Proc. SIGIR 2012, 2012.
[13] E. M. Voorhees. The philosophy of information retrieval evaluation. In Proc. CLEF 2001.
[14] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. J. Info. Proc. & Man., 36(5), 2000.
[15] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In Proc. CIKM 2010.

864

Improving Language Estimation with the Paragraph Vector Model for Ad-hoc Retrieval

Qingyao Ai1, Liu Yang1, Jiafeng Guo2, W. Bruce Croft1
1College of Information and Computer Sciences,
University of Massachusetts Amherst, Amherst, MA, USA
{aiqy, lyang, croft}@cs.umass.edu
2CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology,
Chinese Academy of Sciences, China
guojiafeng@ict.ac.cn

ABSTRACT
Incorporating topic level estimation into language models has been shown to be beneficial for information retrieval (IR) models such as cluster-based retrieval and LDA-based document representation. Neural embedding models, such as paragraph vector (PV) models, on the other hand have shown their effectiveness and efficiency in learning semantic representations of documents and words in multiple Natural Language Processing (NLP) tasks. However, their effectiveness in information retrieval is mostly unknown. In this paper, we study how to effectively use the PV model to improve ad-hoc retrieval. We propose three major improvements over the original PV model to adapt it for the IR scenario: (1) we use a document frequency-based rather than the corpus frequency-based negative sampling strategy so that the importance of frequent words will not be suppressed excessively; (2) we introduce regularization over the document representation to prevent the model overfitting short documents along with the learning iterations; and (3) we employ a joint learning objective which considers both the document-word and word-context associations to produce better word probability estimation. By incorporating this enhanced PV model into the language modeling framework, we show that it can significantly outperform the stateof-the-art topic enhanced language models.
Keywords
Retrieval Model; Language Model; Paragraph Vector
1. INTRODUCTION
Language models have been successfully applied to IR tasks[8, 14]. The core of this approach is to estimate a language model for each document and rank documents according to the likelihood of observing a query given the estimated model. The simple language model approach rep-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914688

resents documents and queries under the bag-of-words assumption. This approach fails when query words are not observed in a document. A typical solution to this issue is to apply smoothing techniques by incorporating a corpus language model for "unseen" words, such as the Jelinek-Mercer method, absolute discounting, and Bayesian smoothing using Dirichlet priors [14]. However, smoothing every document with the same corpus language model is intuitively not optimal since we essentially assume that all the unseen words in different documents would have similar probabilities [13].
One way to improve the smoothing techniques is to introduce document dependent smoothing that can reflect the content of the document, for example by representing documents and queries in a latent topic space and estimating the generation probability accordingly. By incorporating topic level estimation into language model approaches, previous work such as the cluster-based retrieval model [6] and the LDA-based retrieval model [12] obtained consistent improvements over the basic language models. Nonetheless, the existing topic model based approaches have several drawbacks. Firstly, the model estimation relies on the predefined number of topics. Secondly, the topic models typically assign high probabilities to frequent words. Finally, the learning cost (of the LDA model) is expensive on a large corpus.
Recent advances in Natural Language Processing (NLP) have shown that semantically meaningful representations of words and documents can be efficiently acquired by neural embedding models. In particular, a paragraph vector (PV) model [4] has been proposed to jointly learn word and document embeddings by directly optimizing the generative probabilities of each word given the document. In contrast to existing topic models, PV can automatically cluster topic related words and documents without explicitly defining the number of topics a priori. The negative sampling based optimization strategy makes PV assign high probabilities to discriminative words rather than frequent words. Moreover, the online learning algorithm enables PV to learn over a large-scale corpus efficiently. Existing work has shown that PV can outperform the LDA model on several linguistic tasks [1], but its effectiveness for IR remains mostly unknown.
In this paper, we study how to effectively use the PV model in the language model framework to improve ad-hoc retrieval. Specifically, we use the Distributed Bag of Words version of PV (PV-DBOW) because it naturally constructs a

869

document language model that fits the framework of the language modeling approach. However, the original PV-DBOW model is not designed for IR, and we find there are three inherent problems make the original PV-DBOW less effective for ad-hoc retrieval. Firstly, the learning objective of PV-DBOW makes it suppress the importance of frequent words excessively. Secondly, PV-DBOW is prone to over-fit short documents during the training iterations. Finally, PVDBOW does not model word-context associations, making it difficult to capture word substitution relationships that are important in IR. To address these problems, we proposed three modifications to enhance PV-DBOW model for ad-hoc retrieval, including document-frequency based negative sampling, document regularization and a joint learning objective. Empirical results show that consistent and significant improvements over baselines can be obtained with our enhanced PV model.

2. RELATED WORK
Previous work has shown that generative topic models are beneficial for language model estimation. For example, Liu and Croft [6] showed that document clustering can significantly improve retrieval effectiveness when incorporated in language smoothing. The cluster model, also known as the mixture of unigrams model, groups documents into a finite set of clusters (topics) and associates each cluster with a multinomial distribution over the vocabulary. Later, Wei and Croft [12] proposed an LDA-based retrieval model by combining language estimation based on LDA with query likelihood model. Their results showed that the LDA-based retrieval model can consistently outperform the clustering based model.
Recently, there have been several studies exploring the application of word embeddings in the IR scenario. For example, Vuli´c and Moens [11] construct dense representations for queries and documents by aggregating word vectors and rank results based on the fusion of cosine similarities and query likelihood scores. Ganguly et al. [2] proposed a generalized language model based on word embeddings by considering three term transformation processes. In contract to these studies that construct retrieval models based on bag of word embeddings, our work mainly focuses on how to effectively use the paragraph vector model to improve estimation in the language model approach.

3. ENHANCED PARAGRAPH VECTOR
In this section, we describe the details of how we enhance the PV model for language estimation in ad-hoc retrieval.

3.1 PV-DBOW
PV-DBOW maps words and documents into low-dimension dense vectors. Each document vector is trained to predict the words it contains. Under the bag-of-words assumption, the generative probability of word w in document d is obtained through a softmax function over the vocabulary:

exp(w · d)

PP V (w|d) =

(1)

w Vw exp(w · d)

where w and d are vector representations for w and d; and Vw is the vocabulary of the training collections.
In training, negative sampling is used to approximate the softmax function in Equation (1). Formally, the local objec-

tive function for each (w, d) pair in PV-DBOW with negative sampling is

= log((w · d)) + k · EwN Pn [log (-wN · d)] (2)

where (x) = 1/(1 + exp(-x)), k denotes the number of negative samples, wN denotes the sampled word, and Pn(w) denotes the distribution of negative samples. In [7], Pn(w) is defined as the unigram distribution raised to the power 0.75:

#(w)0.75

Pn(w) = |C|

(3)

where #(w) denotes the corpus frequency of w and |C| = w Vw #(w )0.75.

3.2 PV-DBOW based Retrieval Model
From the learning objective of PV-DBOW, we can see that it can be naturally applied in the probabilistic language model framework for IR. With the learned word and document embeddings, we can directly estimate the generative probability of each word given the document in a latent semantic space. Therefore, we can incorporate the language estimation of PV-DBOW into the query likelihood model as a document dependent smoothing technique:

P (w|d) = (1 - )PQL(w|d) + PP V (w|d)

(4)

where PQL(w|d) and PP V (w|d) represent the word probability estimated with QL and PV-DBOW respectively.  is the parameter that controls the weights of QL and PV-DBOW.

3.3 Adaptation for IR

Now we describe in detail the major problems of the original PV-DBOW model that makes it less effective for IR, as well as the techniques we employ to solve these issues.
Document Frequency Based Negative Sampling. Following the idea in [5], we can see that PV-DBOW with negative sampling is implicitly factorizing a shifted matrix of point-wise mutual information between words and documents:

#(w, d) |C|

w · d = log(

·

) - log(k)

#(d) #(w)

(5)

where #(w, d) is the term frequency of w in d; #(d) is the length of d and k is the number of negative instances. From Equation (5), we can see that the original PV-DBOW model implicitly weights words according to inverse corpus frequencies (ICF). However, previous studies have shown that term weighting with ICF may over-penalize frequent words, and often performs worse than term weighting with inverse document frequency (IDF) [9]. Inspired by this, we propose a novel document-frequency based negative sampling strategy for PV-DBOW to better fit the IR scenario. More formally, we replace Pn(w) with a new sample distribution:

#D(w)

Pn(w) = w Vw #D(w )

(6)

where #D(w) represents the document frequency of w. We can find that the new learning objective of PV-DBOW with document-frequency based negative sampling is equal to the following factorization:

w · d = log( #(w, d) ·

w

Vw

#D(w

) )

-

log(k)

(7)

#(d)

#D(w)

870

Since k and w Vw #D(w ) are constants, the training process of PV-DBOW with document-frequency based negative

sampling is actually factorizing a shifted tf-idf matrix.

In practice, the exact value of the inverse document fre-

quency is too aggressive for tf-idf weighting and its logarith-

mic version is more widely used. To achieve similar effects,

we adapt a power version of document frequency that uses #D(w) (  1) instead of #D(w) .

Document Regularization. The original PV-DBOW

does not handle the varied lengths of documents, making

it prone to over-fit short documents during the training it-

erations. Specifically, through the training process of PV-

DBOW, vector norms of long documents remain roughly

the same while vector norms of short documents keep grow-

ing. Increasing vector norms affect the dot product value in

Equation (1) and make the language estimation concentrate

on the observed words. This in turn significantly decreases

the smoothing power of the PV-DBOW model on short doc-

uments. To solve this problem, we propose to introduce

document regularization into the learning objective to avoid

the ever-growing norm of short documents. Specifically, we

add an L2 constraint on the document norm to the learning

objective of PV-DBOW:

=

log((w

·

d))+k ·EwNPn

[log

(-wN

·d)]-

 #(d)

||d||2

(8)

where #(d) is the number of words in d, ||d|| is the norm of
vector d and  is a hyper-parameter that control the strength of regularization. Each iteration of the stochastic gradient descent in PV-DBOW goes through each word exactly once, so we use the document length 1/#(d) to ensure equal regularizations over long and short documents.
Joint Objective. The original PV-DBOW model learns over the word-document co-occurrence information as shown in Equation (2), making it focus on capturing syntagmatic relations between words (i.e., words that frequently co-occur in same documents). It lacks the modeling of paradigmatic relations between words (e.g. "car" and "vehicle") since no word-context information is leveraged in its learning process. As suggested by [1, 10], by modeling both word-document and word-context information, one can usually obtain better word and document vectors for NLP tasks. Following the same idea as [10], we introduce a joint learning objective to the PV-DBOW model. Specifically, we apply a two-layer structure that first uses the document to predict the target word and then uses the target word to predict its context. The new objective function is as follows:

= log((wi · d)) + k · EwNPn [log (-wN · d)]
i+L
+ log((wi · cj )) + k · EcNPn [log (-wi · cN )] (9)
j=i-L j=i
where cj is the context vector for word wj, cN denotes the sampled context and L represents the context window size.

4. EXPERIMENTS
Experimental Setup. We evaluate three baselines: query likelihood model (QL), LDA-based retrieval model (LDALM) and original PV-DBOW model (PV-LM). We add document frequency based negative sampling (D), document regularization (R), and joint objective (J) to PV-DBOW

one by one, and refer to the enhanced PV based retrieval model as EPV-D-LM, EPV-DR-LM, and EPV-DRJ-LM respectively. We use two TREC collections, Robust04 and GOV2. We report the results of different versions of enhanced PV based retrieval models on Robust04, but only the full model (EPV-DRJ-LM) on GOV2 due to the space limitation. We use the Galago search engine1 to index the corpus and report results for both the title and description of each TREC topic (stop words removed). Queries and documents are stemmed with the Krovetz stemmer. For test efficiency, we adopt a re-ranking strategy. An initial retrieval is performed with QL to obtain 2,000 candidate documents, and then a re-ranking is performed with both LDA-LM and EPV based retrieval models. The final evaluation is based on the top 1,000 results. We use a 5-fold cross validation in the same way as [3]: 4 folds are used to tune  in smoothing process and 1 fold is used to test retrieval performance. We includes three evaluation metrics: mean average precision (MAP), normalized discounted cumulative gain at 20 (nDCG@20) and precision at 20 (P@20).
Parameter Settings. We train both LDA and EPV on the whole Robust04 collection. However, for the GOV2 collection, due to the prohibitive training time, we train both LDA and EPV on a randomly sampled subset with 500k documents for fair comparison. The topic number (K) in LDA and the vector dimension in PV-DBOW/EPV are empirically set as 300. For LDA, we set the hyper-parameters  and  to 50/K and 0.01 as described in [12]. For EPV, we tuned  from 1 to 100 ( 1, 10 and 100), and  from 0.1 to 0.9 (0.1 per step). The final value for  is 10 (Robust04/GOV2), for  is 0.1 (Robust04) and 0.2 (GOV2).
Results. The results on Robust04 are shown in the top part of Table 1. As we can see, by incorporating topic level estimation, LDA-LM can outperform the QL model on both topic titles and descriptions. Meanwhile, by estimating the language model using the original PV-DBOW model, PVLM obtains very similar results as LDA-LM. By adding the proposed techniques one by one to enhance the PV-DBOW model for IR, we obtain better and better retrieval performance. The results indicate the effectiveness of the proposed techniques for the PV based retrieval model. Finally, the full enhanced model EPV-DRJ-LM can outperform both QL and LDA-LM significantly on both topic titles and descriptions. For example, the relative MAP improvement of EPV-DRJ-LM over QL and LDA-LM in Robust04 is 5.5% and 3.5% on titles, 2.5% and 2.4% on descriptions, respectively.
From the results on GOV2, however, we find that the incorporation of the LDA model may even hurt the retrieval performance in most cases. A major reason is that GOV2 is a large Web collection with many diverse and noisy topics. By using only 300 topics, the learned topics in LDA might be too coarse and noisy, which can hurt the language model estimation. Therefore, one may observe better performance with LDA-LM by increasing the number of topics (with correspondingly lower efficiency). On the other hand, although the vector dimension of our enhanced PV model is also 300, the potential number of topics is not limited to that number. Therefore, EPV-DRJ-LM can capture much finer topic relations between words and documents, and produce better language estimation in the latent semantic space. We
1http://www.lemurproject.org/galago.php

871

Table 1: Comparison of different models over Robust04 and GOV2 collection. , + means significant difference

over QL, LDA-LM respectively at 0.05 significance level measured by Fisher randomization test.

Robust04 collection

Topic titles

Topic descriptions

Method

MAP

nDCG@20 P@20

MAP

nDCG@20 P@20

QL LDA-LM PV-LM EPV-D-LM EPV-DR-LM EPV-DRJ-LM

0.253 0.258 0.259 0.260 0.262 0.267+

0.415 0.421 0.418 0.417 0.418 0.425

0.369 0.374 0.371 0.371
0.368 0.376

0.246
0.247
0.247 0.251 0.252+ 0.253+

0.391
0.392
0.392 0.397 0.397 0.404+

0.334
0.336
0.335 0.340 0.338 0.347+

GOV2 collection

Topic titles

Topic descriptions

Method

MAP

nDCG@20 P@20

MAP

nDCG@20 P@20

QL LDA-LM EPV-DRJ-LM

0.295
0.292 0.297+

0.409
0.405 0.415+

0.510
0.504 0.519+

0.249

0.371

0.244

0.375

0.252+ 0.371

0.470 0.467 0.472

observe much better performance with EPV-DRJ-LM compared with both QL and LDA-LM.
The results in Table 1 also show that the topic level smoothing is more effective on short queries (topic titles) than long queries (topic descriptions). For example, the relative improvement of LDA-LM over QL is 2.0% on titles and 0.4% on descriptions in terms of MAP respectively; while the relative improvement of EPV-DRJ-LM over QL is 5.5% on titles and 2.5% on descriptions in terms of MAP. With fewer words in a query, the language model estimation would be more difficult based on exact matching. Therefore, by involving topic level estimation, the smoothing technique can bring larger benefits by alleviating the vocabulary mismatch problem.
5. CONCLUSION
In this paper, we study how to effectively use the PV model to improve ad-hoc retrieval. We identify several problems that make the original PV-DBOW model less effective for the IR scenario. To solve these issues, we proposed three techniques to enhance the original PV model. The experimental results demonstrate the effectiveness of our enhanced PV based retrieval model compared with the state-of-the-art topic enhanced language models. This is also the first study to show that a PV model can work better than a topic model on language model estimation for IR.
6. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF IIS-1160894, and in part by NSF grant IIS-1419693. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
7. REFERENCES
[1] A. M. Dai, C. Olah, Q. V. Le, and G. S. Corrado. Document embedding with paragraph vectors. In NIPS Deep Learning Workshop, 2014.
[2] D. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word embedding based generalized language model for information retrieval. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 795­798. ACM, 2015.

[3] S. Huston and W. B. Croft. A comparison of retrieval models using term dependencies. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 111­120. ACM, 2014.
[4] Q. Le and T. Mikolov. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188­1196, 2014.
[5] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2177­2185. Curran Associates, Inc., 2014.
[6] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186­193. ACM, 2004.
[7] T. Mikolov, I. Sutskever, K. Chen, G. S. CJorrado, and M. I. Dean, Jeffdan. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111­3119, 2013.
[8] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275­281. ACM, 1998.
[9] S. Robertson. Understanding inverse document frequency: on theoretical arguments for idf. Journal of Documentation, 60(5):503­520, 2004.
[10] F. Sun, J. Guo, Y. Lan, J. Xu, and X. Cheng. Learning word representations by jointly modeling syntagmatic and paradigmatic relations. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 136­145, Beijing, China, 2015.
[11] I. Vuli´c and M.-F. Moens. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 363­372. ACM, 2015.
[12] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '06, pages 178­185, New York, NY, USA, 2006. ACM.
[13] C. Zhai. Statistical language models for information retrieval. Synthesis Lectures on Human Language Technologies, 1(1):1­141, 2008.
[14] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334­342. ACM, 2001.

872

Generalized BROOF-L2R: A General Framework for Learning to Rank Based on Boosting and Random Forests 

Clebson C. A. de Sá Marcos A. Gonçalves Daniel X. Sousa Thiago Salles
Federal University of Minas Gerais Computer Science Department Belo Horizonte, Brazil
{clebsonc, mgoncalv, danielxs, tsalles}@dcc.ufmg.br

ABSTRACT
The task of retrieving information that really matters to the users is considered hard when taking into consideration the current and increasingly amount of available information. To improve the effectiveness of this information seeking task, systems have relied on the combination of many predictors by means of machine learning methods, a task also known as learning to rank (L2R). The most effective learning methods for this task are based on ensembles of tress (e.g., Random Forests) and/or boosting techniques (e.g., RankBoost, MART, LambdaMART). In this paper, we propose a general framework that smoothly combines ensembles of additive trees, specifically Random Forests, with Boosting in a original way for the task of L2R. In particular, we exploit out-of-bag samples as well as a selective weight updating strategy (according to the out-of-bag samples) to effectively enhance the ranking performance. We instantiate such a general framework by considering different loss functions, different ways of weighting the weak learners as well as different types of weak learners. In our experiments our rankers were able to outperform all state-of-the-art baselines in all considered datasets, using just a small percentage of the original training set and faster convergence rates.
Keywords
Learning to Ranking; Random Forests; Boosting
1. INTRODUCTION
Today, we live in an era of massive available information, with a never-seen-before (and increasing) rate of information production. It is not surprising that such a scenario imposes hard to tackle challenges. For example, the availability of massive amounts of data is not of great help if one is not
This work was partially funded by projects InWeb (grant MCT/CNPq 573871/2008- 6) and MASWeb (grant FAPEMIG/PRONEX APQ-01400-14), and by the authors' individual grants from CNPq, FAPEMIG, Capes and Google Inc.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17­21, 2016, Pisa, Italy.
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911540

able to effectively access relevant information that satisfies her information needs. Retrieval systems, such as search engines, question and answer, and expert search systems serve exactly this purpose: given an information need, expressed in the form of a query, and a set of possible information units (e.g., documents), the main goal is to provide an ordered list of information units according to their relevance with relation to the query. The desideratum is to increase the likelihood of satisfying an user's information need in an effective manner, which translates to maintain the truly relevant results on top of the less relevant ones.
One of the key aspects that influence retrieval systems is how they determine the relative relevance among candidate results in order to produce a ranked list based on their relevance with regard to some information need, posed in the form of a query. The quality of those rankings is thus paramount to guarantee efficient and effective access to relevant information (and, hopefully, the satisfaction of the user's information needs). Several approaches to generate such ranked lists do exist, being traditionally performed by the specification of a function that is able to relate some user's query to the set of known (indexed) information units. Usually, ranking functions consider several features, such as those that rely on the relatedness between query and possible results (e.g., BM25, edit distance, similarities in vector space models) or on link analysis information (e.g., PageRank, HITS). Such features must be somehow combined to provide accurate relevance scores (and, thus, a properly ranked list of results).
Unfortunately, to specify and tune ranking functions turns out to be a major problem, specially when the number of features becomes large, with non-trivial interactions. This motivates the use of supervised machine learning techniques to devise such functions, since machine learning techniques are effective to combine multiple pieces of evidence towards optimizing some goal. This is the direction pursued by Learning to Rank (L2R) techniques, the primary focus of this work.
More specifically, based on a set of query-document pairs with known relevance judgments, the goal is to learn a function f (d, q) that is able to accurately devise the relevance scores for a document d, with respect to a query q. Due to its importance, several approaches for L2R have been proposed in the literature. Ensemble methods, such as RankBoost [7], AdaRank [32] and Random Forests [1] (and the variations thereof, such as [11]), are deemed to be the techniques of choice for L2R, achieving higher effectiveness in published benchmarks when compared to other algorithms [11, 3]. Both RankBoost and AdaBoost are based on boosting [26], an

95

iterative meta-algorithm that combines a series of weaklearners in order to come up with a strong final learner, focusing on hard-to-classify regions of the input space as the iterations go by. The strategies based on Random Forest rely on the combination of several decision trees, learned using bootstrapped samples of the training set, together with additional sources of randomization (such as random feature selection) to produce decorrelated-correlated trees--a requirement to guarantee its effectiveness.
In this work, we propose a general framework for L2R, named Generalized BROOF-L2R that explores the advantages of boosting and Random Forests, by combining them in a non-trivial fashion. More specifically, at each iteration of the boosting algorithm, a Random Forest model is learned, considering training examples sampled according to a probability distribution. Such probability distribution is updated at the end of each iteration, in order to force the subsequent learners to focus on hard to classify regions of the input space. In particular, the use of RF models as weak learners has its own advantages, since it is capable of providing robust estimates of expected error through out of bag error estimates and, by means of selectively updating the weights of out of bag samples, one can effectively slow down the tendency of boosting strategies to overfit (a well known phenomenon that becomes critical as the noise level of the dataset being analyzed increases).
As we shall detail in the next sections, the key aspects of the proposed Generalized BROOF-L2R have to do with how to update the probability distribution and how such update should be performed, as well as the underlying ranker to be used to produce the final set of results. In this work, we discuss a set of possible instances of the proposed general framework, in order to highlight the behavior and potential of the proposed L2R solution. In fact, the instances that makes use of out-of-bag samples and optimizes through gradient descent [12] over the residues is able to achieve the strongest results, in terms of Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), with significant improvements over the explored adversary algorithms, considering 5 traditional benchmark datasets. Our alternative instances were also able to achieve competitive (or superior) results when compared to the baselines. Moreover, as our experimental evaluation shows, our approaches based on the proposed general framework are able to produce topnotch results with substantially less training samples when compared to the baselines. Such data efficiency is key to guarantee practical feasibility as obtaining labeled data is still a costly process.
To summarize, the contributions of this work are threefold. We provide a general framework for L2R that is able to combine two strong methods (boosting and Random Forests) in an original way, which can be specialized in several ways and produce highly effective L2R solutions. We propose and discuss a set of alternative instantiations of such a framework, in order to highlight the behavior and effectiveness of each possible choice. Finally, we advance the state of the art in L2R by means of some instantiations of our proposed framework that are able to outperform top-notch solutions, according to an extensive benchmark evaluation considering five datasets and seven L2R baseline algorithms.
Roadmap: Section 2 discusses related work. Section 3 presents our proposed Generalized BROOF-L2R framework, as well as outlines our proposed set of possible instantiations

of the proposed framework. We clarify our experimental setup and discuss the obtained results in Section 4. Finally, Section 5 concludes and highlights some future work.
2. RELATED WORK
Learn to Rank (L2R) [17] is the focus of active developments due to its cross-industry and society importance. Here, we review some relevant work on this topic, positioning our work in the literature.
L2R attempts to improve traditional strategies for ranking query results according to some relevance criteria, by exploring supervised machine learning algorithms to combine various relevance related features into a more effective ranking function, based on a set of queries and associated documents with relevance judgments. L2R have been successfully applied to a variety of tasks, such as Question and Answer [28], Recommender [29, 16] and Document Retrieval [14] systems.
Solutions specifically tailored to improve document retrieval have been extensively studied in the past years [4, 19]. In general, there are three major L2R approaches: the pointwise, pairwise and listwise approaches. Pointwise L2R algorithms are probably the simplest (yet successful) approaches, directly translating the ranking problem to a classification/regression one. In this case, the training set for the supervised learning algorithm consists of pairs qi, (xi,j, yi,j) of queries qi and a list of associated documents xi,j, each one with its relevance judgment yi,j. In this case, each triple qi, xi,j, yi,j is considered to be a single training example. The goal is to learn a classifier/regressor model capable of accurately predicting the relevance score of a document x , with relation to a query qi, thus producing a partial ordering over documents. Pairwise algorithms, on the other hand, transform the ranking problem into a pairwise classification/regression problem. In this case, learning algorithms are used to predict orders of document pairs, thus exploring more ground-truth information than the pointwise approaches. Unlike both mentioned strategies, the listwise approaches essentially treat qi, (xi,j, yi,j)j as a single training instance (that is, considering a ranked list of documents for a query qi as a single training example), capturing more information from the training set (namely, group structure) than the previous alternatives. Of course, being able to better capture training data information when learning a ranking function comes with a price: usually, pairwise and mainly listwise approaches are harder to train, since they require more sophisticated (e.g. query-level) loss functions [17].
In terms of the state-of-the-art in L2R, methods based on Random Forests (RFs) and boosting were shown to be strong solutions according to already published benchmarks [22, 11, 3]. More specifically, RFs (and the variations thereof [11]) as well as boosting algorithms such as Gradient Boosted Regression Trees (GBRT) [9] and LambdaMART [33], are considered by many [3, 22, 18] to be the state of the art in L2R tasks. This work is based on both RFs and boosting strategies. Thus, in the following we briefly review some previous literature on them.
The RF algorithm was proposed in [1] as a variation of bagging of low-correlated decision/regression trees, built with a series of random procedures, such as bootstrapping of training data and random attribute selection. The popularity of RFs is highlighted by their successful application in several domains, such as tag recommendation [3], object segmentation [27], human pose recognition [30] and L2R [3, 22],

96

to name a few. Thus, it is natural to expect several extensions to it, in order to improve its effectiveness even more. One such extension is the extremely randomized trees (ERT) model [10] and its application to L2R [11]. The ultimate goal of ERTs is to reduce the correlation between the trees composing the ensemble, a requirement to guarantee high effectiveness of RF models. This is achieved by modifying the RF algorithm in, essentially, two aspects: each tree is learned considering the entire training set, instead of bootstrapped samples. Furthermore, in order to determine the decision splits after the random attribute selection, instead of selecting a cut-point that optimizes node purity, ERTs simply select a random cut-point threshold. This ultimately reduces tree correlation, potentially improving generalization capability of the learned model. As a final remark, such RF based models can be regarded as nonlinear pointwise approaches for L2R.
Boosting strategies have also been shown to produce state of the art results on L2R tasks, with GBRT [9] (a.k.a, MART1 (Multiple Additive Regression Trees) and Lambda-MART [33] as the two perhaps most widely used strategies. Both algorithms are additive ensembles of regression trees. GBRT learns a ranking function by approximating the root mean squared error (RMSE) on the training set through gradient descent. As with typical boosting algorithms, the goal of GBRT is to focus on regions of the input space where predicting the correct relevance score is a hard task. Since this algorithm aims at approximating the RMSE on the training data, it can be regarded as a pointwise approach. The Lambda-MART algorithm, on the other hand, is a listwise approach that directly optimizes the ranked list of documents according to some retrieval measure, such as NDCG (instead of simply approximating the RMSE of the training documents relevance scores in isolation). To this end, Lambda-MART learns a ranking function that generates a list of relevant documents to a query that is as close as possible to the correct rank. As GBRT, it is based on gradient descent to optimize such metric.
Due to the successful application of RFs and boosting in machine learning tasks (such as classification and L2R), some authors propose to use both strategies in order to come up with better learned models. For example, in [22] GBRTs and RFs are independently explored in order to learn better ranking functions. More specifically, the GBRT model is initialized with the residues of the RF algorithm, followed by the traditional iterations of a GBRT model. The main motivation behind this approach is that RFs are less prone to overfitting, being ideal to initialize the GBRT algorithm instead of the usual uniform initialization. According to the reported benchmark, such strategy was shown to be superior to the GBRT algorithm.
Unlike [22], in [21] the authors propose an enhanced RF model for classification by boosting the decision trees composing the ensemble. In this case, each tree is learned with training examples weighted by wi, resembling boosting by re-weighting. In particular, training instances with higher weights influence more when determining the decision nodes (and cut-point threshold definition). Furthermore, each tree is evaluated according to this weighted training set, which enables the ensemble to focus on hard-to-predict regions. The observed effect of such combination is the ability to
1From now on, we will use MART and GBRT as synonyms.

come up with high quality models with substantially reduced training sets. As we shall detail, our proposed framework is tailored for the L2R task and, instead of introducing boosting into random forests, we apply boosting to several RF models, which act as weak learners.
Differently from the aforementioned previous work, we base ourselves in a recent development for text classification, namely, the BROOF algorithm [25]. In BROOF algorithm, RF and boosting strategies are tightly coupled in order to exploit their unique advantages: by exploiting out of bag error estimates as well as selectively updating training weights according to out of bag samples, the BROOF model is able to focus on hard-to-classify regions of the input space, without being compromised by the boosting tendency to overfit. This ultimately leads to competitive results when compared to state of the art algorithms. In here, we generalize such approach specifically for L2R tasks in order to come up with better ranking functions: the Generalized BROOF-L2R. As we shall see, this general framework is flexible enough so that it can be instantiated in several ways, exploiting distinct characteristics of the ranking tasks being addressed. In special, with this general framework we are able to achieve state of the art results, with rankers superior to the top notch algorithms proposed so far in all evaluated cases.
3. GENERALIZED BROOF-L2R
In this section, we detail our proposed Generalized BROOFL2R framework. Briefly speaking, this framework allows the definition of learners based on the combination of Random Forests and the Boosting meta-algorithm, in a non-trivial fashion. As we shall see, this framework establishes a set of operations to be performed during the boosting iterations, in a well defined order of application. The goal is to drive the weak learners towards hard to predict regions of the underlying data representation, in order to come up with an optimized additive combination of weak learners to form the final predictor. The extension points of the proposed framework can produce a heterogeneous set of instantiations that typically produces very competitive results for L2R. In the following, we present the generalized framework for L2R, as well as some pointwise instantiations. We stress that the set of instantiations discussed here is far from exhaustive, being possible to elaborate even better possibilities in future work.
3.1 Framework Description
Based on the BROOF algorithm, proposed in [25] to solve text classification tasks, we here extend the proposed ideas in order to exploit the combination of Random Forests and Boosting for the specific task of L2R. However, instead of directly adapting the original algorithm to a single L2R method, we here generalize it into an extensible framework that is flexible enough to permit a series of possible instantiations. The proposed framework, named Generalized BROOF-L2R is an additive model composed of several Random Forest models, which act as weak-learners. Each fitted model influences the final decision proportionally to its accuracy, focusing -- as the boosting iterations go by -- on ever more complex regions of the input space, in order to drive down the expected error. As usual in a boosting strategy, two aspects play a key role: (i) the influence t of each learner in the fitted additive model, and (ii) the strategy to update the sample distribution wi,j in each iteration t of the boosting meta-algorithm.

97

The basic structure of the framework is outlined in Algorithm 1, together with a brief explanation of what we call its extension points--the general functions exploited by the framework to determine how the optimization process works. There are 5 general functions whose purpose is to specify the weight distribution update process, the error estimation and the underlying input representation. Particularly, the use of the Random Forest classifier as a weak learner extends the range of possible instantiations of the framework, since it enables us to come up with better error rate estimates and a more selective approach to update the examples' weights, through the use of the so-called out-of-bag samples.
Let Qtrn = {(qi, {xi,j , yi,j })|m j=i1} be the training set, descriptively the set of documents xi,j, with associated graded relevance judgment yi,j with relation to a query qi. Initially, associate a weight wi,j with each training example xi,j according to the general function InitializeWeights. For each boosting iteration t, the input data representation may be updated, through the general function UpdateExamples. This general function can considerably extend the range of possible implementations of the framework, allowing us for example, to instantiate a Gradient Boosting Machine algorithm [20, 12]. Then, a Random Forest regressor model RFt is learned considering this data representation.
In order to evaluate the generalization capabilities of RFt, predict y^ for a set of training documents given by ValidationSet. The output of this step is paramount to guide the optimization process towards hard to classify regions of the input space. Although being of great importance to boosting effectiveness, this focus on hard to classify regions of the input space may also be harmful to the optimization process, specially when dealing with noisy data. As noted by [8, 13], boosting tends to increase the weights of few hard-to-classify examples (e.g., noisy ones). Thus, the decision boundary may only be suitable for those noisy regions of the input space while not necessarily general enough for general examples. In order to offer a greater robustness against such a drawback, our framework exposes an intermediary step related to how the examples weights get updates as the boosting iterations go by. The general function ValidationSet serves the purpose of specifying which training examples should be used during error estimation and weights update. The main goal here is to provide some mechanism to slowdown overfitting as well as provide more robust estimates of error weight (to capture the generalization power of each weak learner and to determine how they should influence the final predictor).
The selected training examples are then used to compute both the error rate of the model and the influence t of the weak learner on the final model, through ComputeLearnerWeights. Finally, the training examples' weight distribution is updated by UpdateExampleWeights. This update process should, ideally, take into account the generalization capability of the current weak learner RFt, as well as how hard is to correctly predict the ranked lists of the validation examples. Validation examples whose outcome is hard to predict by an accurate learner should influence more in the following boosting iterations. An early stopping strategy is adopted, terminating the boosting iterations if the current learner has an estimated error rate greater than 0.5. The final prediction rule is then given by an additive combination of the weak-learners RFt, weighted by t.

Instantiation BROOFabsolute BROOFmedian BROOFheight BROOFgradient

Description
Extension Point
InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights
InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights
InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights
InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights

Variation
Uniform Identity OOB Absolute OOB
Uniform Identity OOB Median OOB
Uniform Identity OOB Height OOB
Uniform Residue OOB Constant Constant

Table 1: Generalized BROOF-L2R: Possible instantiations.
3.2 Possible Instantiations
In this section, we describe a set of possible instantiations of the proposed framework. Due to space limitations, we here focus on four possible instantiations, stressing that this is far from being an exhaustive list of possibilities. In fact, we consider some representative alternatives that highlight the flexibility of the proposed framework to produce L2R solutions that typically produces very competitive results.
In order to induce a L2R algorithm based on the Generalized BROOF-L2R framework, one needs to specify the 5 generic functions discussed earlier. Our proposed instantiations can be found in Table 1. In that table, we specify which alternative was chosen for each generic function, providing details on how they are implemented.
As it can be observed, BROOFabsolute, BROOFmedian and BROOFheight rely on out-of-bag samples in order to drive the boosting meta-algorithm further on hard to predict regions of the input space. Such samples are explored when estimating the weak-learner's error rate through out-of-bag estimates. Recall that in boosting, the usual way of assessing the errors is to use the training to measure the error. This is too optimistic, since the same data that was used to train the model is used as a measure of error. By using the out-of-bag samples we are able to produce better error estimates, since the out-of-bag are an independent set of samples that was left apart during the construction of the model. Thus, it is able to better approximate the expected error rate of the learner and is a more reliable measure than the usual training error rate [1].
In addition, the out-of-bag errors estimates are used to identify the weights' distribution that should be applied on following iterations of the boosting procedure; allowing the model to focus on hard to predict regions of the input space. We hypothesize that such selective update strategy can slowdown the algorithm's tendency to overfit. The major difference between them relates on how each weaklearner influence on the final predictor. The proposed instantiations can be found outlined in Algorithms 2 to 4. More specifically, we considered the absolute regression loss, |yi,j - y^i,j|, computed for the out-of-bag samples. We call

98

Algorithm 1 Generalized BROOF-L2R: Pseudocode

1: function Fit(Qtrn = {(qi, {xi,j , yi,j }|m j=i1}, max iter=M , num trees=N , shrinkage=)

2: wi,j =InitializeWeights(Qtrn )

3:

xi,j  yi,j

4: for each t = 1 to M do

5:

xi,j UpdateExamples(Qtrn , xi,j )

6:

RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )

7:

{(y^it,j , yit,j )} ValidationSet(RFt, Qtrn )

8:

eti,j , t ComputeLearnerWeights(RFt, {(y^it,j , yit,j )})

9:

if i,j eti,j wi,j  0.5 then

10:

break

11:

end if

12:

wi,j UpdateExampleWeights(eti,j , t, {(y^it,j , yit,j )})

13: end for

14: return {(RFt, t)}|M t=1 15: end function

Function

Description

InitializeWeights UpdateExamples

Initial weights associated to each example, ressambling boosting by re-weighting.

Uniform: Equal weights for each example, wi,j =

1 i,j mi,j

.

Random: Randomly initialized weights, wi,j =Random(), 0  wi,j  1.

Determines the underlying representation of the input data, directly defining what the algorithm should

optimize for.

Identity: Maintains the original representation of input data, xi,j = xi,j .

Residue: Optimizes for the residues: xi,j =

xi,j - y^it,-j1 if t > 1 , where  is a shrinkage factor. yi,j otherwise

ValidationSet ComputeLearnerWeights

Determines which training data will be considered during weight update and error rate estimation, with direct influence on the algorithm robustness to overfitting. OOB: The set of out of bag examples OOBt related to RFt. Train: The entire training set Qtrn .

Determines how to compute the influence of the current weak learner on the final predictor.

Absolute: t =  1- , where = i,j eti,j wi,j , eti,j = |yi,j - y^i,j | and  is a shrinkage factor. Median: Similarly to the above variant, t =  1- and = i,j eti,j wi,j . However, the errors are given by eti,j = |Median(Ry^i,j ) - y^i,j | where Ri denotes the list of predictions y^i,j associated to documents

whose real relevance score is i.

Height: Similarly to the variants above, both t =  1- and = i,j eti,j wi,j . Unlike them, eti,j =

# irrelevant documents above xi,j # relevant documents below xi,j

if xi,j is relevant otherwise

, in the ordered list of results.

Constant: Produces constant coefficients, t = .

UpdateExampleWeights

Specifies how to update the training examples weights to be used in the next iteration.
OOB: Updates the weights associated to the out of bag samples according to t and the difficulty involved in predicting the samples' outcomes. More specifically, wi,j = wi,j t1-eti,j Train: Updates the weights associated to the entire training set. Similarly to the above variant, the update strategy considers both the coefficient t and the error eti,j .
Constant: Keeps the same weights during the boosting iterations, wi,j = wi,j .

this variant BROOFabsolute. We also considered two other alternatives, that rely on the position of documents in the predicted ranked lists. One alternative, named BROOFL2Rmedian, relies on the intuition that documents with the same relevance judgment should be as nearer as possible to each other on the current ranked list. We thus consider as loss |Median(Ry^i,j ) - y^i,j | where Ri denotes the list of predictions y^i,j associated to documents whose real relevance score is i. The second alternative, named BROOFheight, is inspired on ideas of [5]. We define the height of a document xi,j as the total number of irrelevant documents ranked higher then xi,j if xi,j is relevant, or the total number of relevant documents ranked below xi,j if it is an irrelevant one.
Finally, in order to illustrate the generality of our proposed framework, we provide a fourth instantiation, BROOFgradient, that resembles the gradient boosting machines (GBM), that optimizes through gradient descent [12] over the residues. More specifically, by a suitable combination of alternative implementations for each general function outlined in Algorithm 1, one can come up with an algorithm that could be named Gradient Boosted Random Forests (GBRF). This

is achieved by considering an alternative representation of input data, that optimizes for the residues, such as y - y^, instead of the original input representation, updating them according to the negative gradient of the cost function (in this case, RMSE). Such alternative is outlined in Algorithm 5.
As we shall see in our experimental evaluation (Section 5), our proposed instantiations achieve very strong results compared to seven state-of-the-art baselines algorithms in five representative datasets. In particular, BROOFabsolute and BROOFgradient were shown to be the strongest algorithms, obtaining significant improvements over the best baselines.
4. EXPERIMENTAL EVALUATION
We conducted extensive experiments in well-known L2R benchmarks. In the following, we describe the characteristics of the used datasets, the baseline algorithms, the experimental protocol/setup and the experimental results.
4.1 Datasets
The corpus we use are freely available online for scientific purposes. Such datasets can be divided into two groups

99

Algorithm 2 BROOFabsolute: Pseudocode

1: function Fit({(qi, {xi,j , yi,j }|m j=i1}, M , N , )

2:

wi,j =

1 i,j mi,j

3:

xi,j  yi,j

4: for each t = 1 to M do

5:

xi,j  xi,j

6:

RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )

7:

{(y^it,j , yit,j )}  RFt.OOB

8:

eti,j  |yi,j - y^i,j |

9:

 i,j eti,j wi,j

10:

t   1-

11:

if  0.5 break

12:

wi,j  wi,j t1-eti,j

13: end for

14: return {(RFt, t)}|M t=1 15: end function

Algorithm 3 BROOFmedian: Pseudocode

1: function Fit({(qi, {xi,j , yi,j }|m j=i1}, M , N , )

2:

wi,j =

1 i,j mi,j

3:

xi,j  yi,j

4: for each t = 1 to M do

5:

xi,j  xi,j

6:

RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )

7:

{(y^it,j , yit,j )}  RFt.OOB

8:

eti,j  Median(pos(yi,j ) - pos(y^i,j ))

9:

 i,j eti,j wi,j

10:

t   1-

11:

if  0.5 break

12:

wi,j  wi,j t1-eti,j

13: end for

14: return {(RFt, t)}|M t=1 15: end function

considering the relevance judgments and their sizes. The two largest datasets contain query, document pairs with five relevance levels, ranging from 0 (completely irrelevant) to 4 (highly relevant). In this group we have one dataset from the "YAHOO! Webscope Learning to Rank Challenge", divided into three partitions for training, validation and test. The second largest dataset, WEB10K, consists of 10, 000 queries released by Microsoft. In contrast to the YAHOO! datasets, the Microsoft dataset is partitioned into 5 folds for crossvalidation purposes, with 3 partitions used for training, 1 for validation and 1 for test.
The second group of datasets corresponds to well-known LETOR 3.0 Topic distillation tasks, TD2003 and TD2004 (a.k.a., informational queries), of the Web track of the Text Retrieval Conference 2003 and 2004. These datasets contain binary relevance judgments. Similarly to the WEB10K benchmark, these datasets are partitioned into 5 folds to be used in a folded cross-validation procedure.
For comparative purposes, considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure, we applied this same strategy to the YAHOO! dataset by merging the original partitions into a single set, and splitting the sorted queries into 5 folds, distributed using the same proportions: 3 folds for training, 1 for validation and 1 for test. We report results for both splits: the original one (called YAHOOV1S2) and the new 5-fold split (called YAHOOV1S2-F5).
4.2 Baselines
In our experiments we consider as baselines freely avail-

Algorithm 4 BROOF-L2Rheight: Pseudocode

1: function Fit({(qi, {xi,j , yi,j }|m j=i1}, M , N , )

2:

wi,j =

1 i,j mi,j

3:

xi,j  yi,j

4: for each t = 1 to M do

5:

xi,j  xi,j

6:

RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )

7:

{(y^it,j , yit,j )}  RFt.OOB

8:

eti,j 

# irrelevant documents above xi,j # relevant documents below xi,j

9:

 i,j eti,j wi,j

10:

t   1-

11:

if  0.5 break

12: 13:

wi,j  wi,j t1-eti,j end for

14: 15:

return {(RFt, t)}|M t=1 end function

if xi,j is relevant otherwise

Algorithm 5 BROOFgradient: Pseudocode

1: function Fit({(qi, {xi,j , yi,j }|m j=i1}, M , N , )

2:

wi,j =

1 i,j mi,j

3:

xi,j  yi,j

4: for each t = 1 to M do

5:

xi,j 

xi,j - y^it,-j1 if t > 1 yi,j otherwise

6:

RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )

7:

{(y^it,j , yit,j )}  RFt.OOB

8:

eti,j  |yi,j - y^i,j |

9:

 i,j eti,j wi,j

10:

t  

11:

if  0.5 break

12:

wi,j  wi,j

13: end for

14: 15:

return {(RFt, t)}|M t=1 end function

able implementations of state-of-the-art L2R methods, including AdaRank (with MAP and NDCG as loss functions), Random Forests, SVMrank, MART, LambdaMART and RankBoost. We used the RankLib2 (under the Lemur project) implementations of RankBoost, MART and LambdaMART. For AdaRank we used the implementation freely available at Microsoft Research3. For SVMrank, we used the original implementation of [15]4. Finally, for Random Forests, we used the implementation available in Scikit-Learn[24] library, which is also the basis of our implementations.
4.3 Experimental Protocol and Setup
To validate the performance of our approaches, we use two statistical tests to assess the statistical significance of our results, namely, the Wilcoxon signed-rank test and the paired Student's t-test. We consider the Wilcoxon signedrank test since it is a non-parametric statistical hypothesis testing procedure that requires no previous knowledge of the samples distribution. In fact, some authors believe that it is one of the best choices for the analysis of two independent samples [6]. However, there is also some discussion in the literature favoring the Student's t-test when comparing L2R methods [23]. Due to the lack of consensus, we perform our
2http://sourceforge.net/p/lemur/wiki/RankLib/ 3http://research.microsoft.com/en-us/downloads/ 0eae7224-8c9b-4f1e-b515-515c71675d5c/ 4https://www.cs.cornell.edu/people/tj/svm light/svm rank.html

100

analysis with both tests, considering a two-sided hypothesis with significance level of 0.95% in both tests.
The statistical tests are computed over the values for Mean Average Precision (MAP) and the Normalized Discounted Cumulative Gain at the top 10 retrieved documents (hereafter, NDCG@10), the two most important and frequently used performance metrics to evaluate a given permutation of a ranked list using binary and multi-relevance order [31]. To compute these metrics we used the standard evaluation tool available for the LETOR 3.0 benchmark (for binary datasets), as well the tool available for the Microsoft dataset for all multi-label relevance judgment datasets 5. For MAP, let Q be the set of all queries. These tools simply compute

M AP = AveragePrecision(q) . |Q|
qQ

Regarding NDCG, we assume that NDCG@p is 0 (zero) for empty queries, i.e., queries with no relevant documents. Some of the available evaluations tools (e.g., the one from YAHOO!) assume the value of 1 for these cases, which may lead to higher values of NDCG [2]. We chose to standardize this issue, using the same criterion used by most evaluation tools, e.g., those available for the Letor (3.0 and 4.0) and Microsoft datasets, in order to allow fairer comparisons. Accordingly, let IDCGp be the maximum possible discounted cumulative gain for a given query. These tools implement NDCG@p as follows:

N DCG@p

=

DCGp , where I DC Gp

DC Gp

=

p i=1

2reli - 1 .
log2(i + 1)

In terms of algorithm tuning, we follow the usual procedure of tuning the hyper-parameters using training and validation sets. Considering the Random Forest based approaches we vary the number of trees ranging from 10 to 1000. We achieved convergence around 300 trees, We also optimized the percentage of features to be considered as candidates during node splitting, as well as the maximum allowed number of leaf nodes. The optimal values were 0.3 and 100, respectively.
For BROOFabsolute, BROOFmedian and BROOFheight, we limited the number of iterations to 500, reminding that the algorithms have an early stopping criterion that prevents further boosting iterations when the error rate exceeds 0.5. On average, our strategies converge at about 15 iterations on the LETOR datasets, and around 5 to 10 iterations on the multi-relevance judgment datasets. An exception was BROOFgradient which converged at about 100 iterations for the largest datasets.
Concerning the SVMrank baseline, we favored the use of a linear kernel considering the fact that we verified in our analysis that a polynomial kernel is infeasible on large scale benchmarks such as WEB10K. The cost parameter C was calibrated using the training and validation sets with the explored values: 0.001, 0.01, 0.1, 1, 10, 100 and 1000. For the boosting methods Mart and LambdaMART, we tuned, always considering the validation set, the number of iterations ranging from one to a hundred, with a step of 1, and then scaling it up to 1000 iterations, with steps of 100. For the shrinkage factor of the predictive models, we tested the

5Reminding that, at the time of the writing of this paper, the evaluation tool used in the YAHOO! competition was not available online.

values of 0.025, 0.05, 0.075 and 0.1. The best found values for the MART and LambdaMART were ensembles of 1000 trees with shrinkage factor  of 0.1. For the AdaRankMAP , AdaRankNDCG@5 and for the RankBoost algorithm, similar procedures were performed in the validation set to configure the number of iterations.
Finally, we performed 5, 10 and 30 runs of the 5-fold cross validation procedure for WEB10K, YAHOO! and LETOR datasets, respectively. The differences in the number of repetitions are due to the size of the datasets and the need to properly address the variance of the results. The reported results on Tables 2 and 3 are the average of all these runs, being the statistical tests applied to these results.
4.4 Results
In this section we analyze our proposals in terms of effectiveness, comparing them to the 7 explored baseline algorithms on the 5 described datasets. The results are reported on Tables 2 and 3.
We start by considering the MAP metric (Table 2). Briefly, the MAP results show that, overall, our proposed framework outperforms or ties with the strongest baselines in all cases. More specifically, with the TD2003 dataset, BROOFheight outperformed the strongest baseline (RF) considering both statistical tests, with BROOFabsolute and BROOFmedian as the winners according to at least one statistical test. In this dataset, BROOFgradient was statistically tied with the best baseline. Considering TD2004, BROOFabsolute was considered the top performer amongst the proposed solutions, being tied with the strongest baseline ­ RankBoost ­ in this dataset. Regarding the WEB10K dataset, we can see that BROOFgradient was the top performer, according to both statistical tests, being superior to MART, the strongest baseline. Finally, in the YAHOOV1S2 dataset all four proposed algorithms were statistically superior to the strongest baseline (RF) according to both statistical tests, whereas in the YAHOOV1S2-F5 dataset BROOFgradient was the best approach. In sum, according to the MAP metric, our results clearly show that the proposed instantiations of the Generalized BROOF framework produced very competitive results as the best algorithm, being superior in the majority of the cases (and tying in the others) ­ a very significant result.
Turning our attention to the NDCG results, reported on Table 3, a similar behavior can be observed: our proposed instantiations are no worse than the strongest baselines in all cases, being superior in the majority of cases. Considering the TD2003 and TD2004 datasets, our solutions were no worse than any baseline, being statistically tied with the strongest one (RF, in both cases). BROOFgradient was the best algorithm in the three remaining datasets, according to both employed statistical tests. Furthermore, BROOFmedian was also superior to the best baseline (MART) in the YAHOOV1S2 dataset (according to the Student's t-test), with BROOFabsolute and BROOFheight tied with the MART algorithm. Again, this set of results highlights the effectiveness of the proposed approaches.
We now turn our attention to some behavioral aspects of our algorithms, namely, convergence and learning efficiency. In order to better understand the convergence rate of our proposals, we provide an empirical evaluation of our most effective solution (i.e., BROOFgradient), by analyzing the obtained NDCG@10 as we vary the number of boosting iterations, contrasting these results with the boosting

101

Baselines

Algorithm
Mart LambdaMart RF RankBoost AdaRank-MAP Adarank-NDCG SVM-Rank
BROOFabsolute BROOFmedian BROOFheight BROOFgradient

TD2003
0.192633 0.165181 0.278644 0.235189 0.2003 0.121672 0.257490
0.288039+ 0.282427 0.285937+ 0.280634

TD2004
0.193744 0.169605 0.2522 0.255467 0.196801 0.132435 0.220392
0.263288 0.259941 0.259058 0.252342

Datasets

WEB10K YAHOOV1S2

0.352491 0.350263 0.337702 0.316201 0.294792 0.304359 0.324552
0.342437 0.347665 0.340340 0.36251+

0.559721 0.5545
0.563355 0.544887 0.413846 0.540243 0.544887
0.565486+ 0.567696+ 0.564774+ 0.572918+

YAHOOV1S2-5F
0.568821 0.563694 0.559019 0.547524 0.480190 0.538514 0.551333
0.563729 0.567284 0.557727 0.57656+

BROOF

`': better than the strongest baseline, with statistical significance according to Wilcoxon Test `+': better than the strongest baseline with statistical significance according to Student's t-test `n': statistically tied results considering both tests

Table 2: Mean Average Precision (MAP): Obtained results.

Baselines

Algorithm
Mart LambdaMart RF RankBoost AdaRank-MAP AdaRank-NDCG SVM-Rank
BROOFabsolute BROOFmedian BROOFheight BROOFgradient

TD2003
0.271274 0.224536 0.36346 0.31613 0.271921 0.166241 0.344177
0.360802 0.36798 0.368195 0.368695

TD2004
0.263926 0.237338 0.350582 0.33399 0.281035 0.182031 0.303471
0.358146 0.350466 0.355356 0.348532

Datasets

WEB10K YAHOOV1S2

0.4404 0.445437 0.424498 0.397071 0.35732 0.385761 0.399902

0.703757 0.69619 0.703139 0.682478 0.51767 0.66309 0.682478

0.434964
0.436284
0.42882 0.456081+

0.70633
0.708538+ 0.70383
0.717271+

YAHOOV1S2-5F
0.714763 0.706287 0.702384 0.681796 0.607867 0.664115 0.691064
0.706954 0.709148 0.701985 0.725129+

BROOF

`': better than the strongest baseline, with statistical significance according to Wilcoxon Test `+': better than the strongest baseline, with statistical significance according to Student's t-test `n': statistically tied results considering both tests

Table 3: Normalized Discounted Cumulative Gain (NDCG@10): Obtained results.

baselines. We here focus on the three largest datasets: YAHOOV1S2, YAHOOV1S2-F5 and WEB10K. Results can be found on Figure 1. As it can be observed, BROOFgradient share similar behavior with three explored boosting algorithms, namely, MART, RankBoost and AdaRank-NDCG: the four algorithms show fast convergence rates. The two key differences are: (i) our approach is able to achieve significantly better results at the initial boosting iterations and (ii ) BROOFgradient converges to a higher asymptote than the other algorithms. On the other hand, the convergence rate of LambdaMART was significantly slower than the convergence rate of the mentioned algorithms. In sum, BROOFgradient enjoys faster convergence rates, with higher NDCG values at the initial boosting iterations and higher asymptote. This is paramount to guarantee practical feasibility of our solution: although high effectiveness is a requirement, achieving such high effectiveness with just a few boosting iterations is key to minimize running time.
Another aspect of direct impact on the practical feasibility of the solutions is to what extent the algorithms are "data efficient". That is, to what extent each algorithm is capable of delivering highly effective rankings with reduced training sets. We evaluate the solutions under this dimension by analyzing each algorithm's learning curve. To this end, we measure the effectiveness of each algorithm as we vary training set size. We randomly sample s% examples from the training set, selected at random. We vary s from 10% to 100%, with steps of 10%. The obtained results can be found

on Figure 2. Considering the WEB10K dataset, we can observe a surprising result: BROOFgradient is able to outperform all algorithms with just 20% of the training set, even considering the other algorithms trained with larger training sets (including the entire training set). Also, it can be noted that BROOFabsolute is no worse than the baseline algorithms, even with 10% of the training set. In fact, with about 40% of the training set BROOFgradient is able to achieve its maximum effectiveness, whereas for BROOFabsolute 10% is enough. For the YAHOO datasets, a similar behavior was observed: with about 20% to 30% of the training set our approaches were able to outperform the baseline algorithms (or match, in the case of BROOFabsolute), even considering the baseline algorithms trained with the entire training set. In these datasets, our algorithms were able to achieve maximum effectiveness at 50% to 80% of the training set. Considering the TD2003 and TD2004 datasets, the RF baseline was a bit more competitive to our approaches, exhibiting a similar behavior in terms of effectiveness as the training set size varies. In these datasets, 50% to 60% of the training set was enough to produce the best effectiveness on the TD2003, while 40% was enough to surpass all baselines on TD2004. These findings have also a direct influence on the practical feasibility of our solutions. First, smaller training sets translates to smaller runtimes. Second, obtaining labeled data is critical but also costly. Clearly, being able to produce highly effective models from reduced training sets is an important characteristic of a successful approach.

102

Figure 1: Convergence analysis: NDCG as the number of boosting iterations increases.

Figure 2: Learning curve analysis for the boosting algorithms.

Finally, we turn our attention to the effect of the use of out-of-bag samples by our approaches. Due to space restrictions, we here focus on BROOFgradient, considering the WEB10K dataset. We analyze the effect of weak-learner error rate estimation through out-of-bag samples by contrasting it with a variant whose generic function ValidationSet equals to Train. The effectiveness of BROOFgradient and the mentioned variation, as the boosting iterations go by, can be found on Figure 3. From that figure, it is clear that the out-of-bag error estimation produces more effective results than the simple training error estimate. In fact, for all boosting iterations, the BROOFgradient variation with ValidationSet set to OOB produces better results than the results obtained with ValidationSet set to Train. This highlights the importance of exploiting the out-of-bag error estimates in our proposed framework instantiations. As a final remark, as it can be observed in Figure 3, even the variant that uses the training error rate is able to outperform the explored baselines. This is also an important aspect that highlights the quality of the proposed framework.
5. CONCLUSIONS AND FUTURE WORK
In this work, we propose an extensible framework for L2R, called Generalized BROOF-L2R, which smoothly combines two successful strategies for Learning to Rank, namely, Ran-

Figure 3: BROOFgradient: Effect of out-of-bag samples versus entire training set.
dom Forests and Boosting. Such combination, that uses Random Forests models as weak-learners for the boosting algorithm, relies on the use of the out of bag samples produced by the Random Forests to (i) determine the influence of each weak-learner in the final additive model and (ii) update the sample distribution weights by means of a more reliable error rate estimate. In fact, the framework is general enough to provide a rather heterogeneous set of instantiations that, according to our empirical evaluation, are able to

103

achieve competitive results compared to state-of-the-art algorithms for L2R. We proposed four different instantiations. Three instantiations closely follows the ideas of a recently proposed algorithm for text classification, namely, BROOF. The fourth instantiation is based on gradient descent optimization, resembling gradient boosting machines. In fact, such instantiation can be seen as a gradient boosted random forests model. As our results show, despite the fact that all the four algorithms provide very competitive results, two of them are consistently the top-performers, highlighting the quality and effectiveness of our proposed framework. Also, our proposals have two properties that are paramount to guarantee their practical feasibility, namely, data efficiency and fast convergence rates.
The space of possible instantiations of the proposed general framework for L2R is rather large. This clearly makes room for further investigations regarding such possibilities. In fact, one can come up with improved instantiations of the framework, by means of extending the set of possible implementations for each generic function composing the framework. This is under investigation. We also plan to study a more comprehensive set of instantiations, in order to build a substantially larger catalog of algorithms based on the Generalized BROOF-L2R to better understand the effects of each choice on model effectiveness.
References
[1] L. Breiman. Random forests. Mach. Learn., 45(1):5­32, 2001.
[2] R. Busa-Fekete, B. K´egl, T. E´lteto, and G. Szarvas. Tune and mix: learning to rank using ensembles of calibrated multi-class classifiers. Machine Learning, 93(2):261­292, 2013.
[3] S. D. Canuto, F. M. Bel´em, J. M. Almeida, and M. A. Gonc¸alves. A comparative study of learning-to-rank techniques for tag recommendation. JIDM, 4(3):453­468, 2013.
[4] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. JMLR - Proceedings Track, 14:1­24, 2011.
[5] K. Christakopoulou and A. Banerjee. Collaborative ranking with a push at the top. In WWW, pages 205­215, 2015.
[6] J. Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7:1­30, 2006.
[7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4:933­969, 2003.
[8] Y. Freund and R. E. Schapire. Experiments with a New Boosting Algorithm. In ICML, pages 148­156, 1996.
[9] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2000.
[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely randomized trees. Mach. Learn., 63(1):3­42, 2006.
[11] P. Geurts and G. Louppe. Learning to rank with extremely randomized trees. In Proc. of the Yahoo! L2R Challenge, held at ICML 2010, Haifa, Israel, June 25, 2010, volume 14 of JMLR Proceedings Track, pages 49­61, 2011.
[12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, 2009.
[13] R. Jin, Y. Liu, L. Si, J. Carbonell, and A. G. Hauptmann. A new boosting algorithm using input-dependent regularizer. In ICML, 2003.

[14] T. Joachims. Optimizing search engines using clickthrough data. In ACM SIGKDD, pages 133­142, 2002.
[15] T. Joachims. Training linear svms in linear time. In ACM SIGKDD, pages 217­226, 2006.
[16] A. Karatzoglou, L. Baltrunas, and Y. Shi. Learning to rank for recommender systems. In ACM RecSys, pages 493­494, 2013.
[17] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225­331, 2009.
[18] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonellotto, and R. Venturini. Quickscorer: A fast algorithm to rank documents with additive ensembles of regression trees. In SIGIR, pages 73­82, 2015.
[19] C. Macdonald, R. L. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.
[20] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In In Advances in Neural Information Processing Systems, pages 512­518, 2000.
[21] Y. Mishina, R. Murata, Y. Yamauchi, T. Yamashita, and H. Fujiyoshi. Boosted random forests. IEICE Transactions, 98-D(9):1630­1636, 2015.
[22] A. Mohan, Z. Chen, and K. Weinberger. Web-search ranking with initialized gradient boosted regression trees. JMLR Workshop and Conference Proceedings: Proceedings of the Yahoo! Learning to Rank Challenge, 14:77­89, 2011.
[23] L. a. F. Park. Confidence Intervals for Information Retrieval Evaluation. Australiasian Document Computing Symposium, 2010.
[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res., 12:2825­2830, 2011.
[25] T. Salles, M. Gonc¸alves, V. Rodrigues, and L. Rocha. Broof: Exploiting out-of-bag errors, boosting and random forests for effective automated classification. In SIGIR, pages 353­362, 2015.
[26] R. E. Schapire and Y. Freund. Boosting: Foundations and Algorithms. 2012.
[27] F. Schroff, A. Criminisi, and A. Zisserman. Object class segmentation using random forests. In British Machine Vision Conf., pages 1­10, 2008.
[28] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In ACM SIGIR, pages 373­382, 2015.
[29] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning to rank with matrix factorization for collaborative filtering. In ACM RecSys, pages 269­272, 2010.
[30] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In CVPR, pages 1297­1304, 2011.
[31] N. Tax, S. Bockting, and D. Hiemstra. A cross-benchmark comparison of 87 learning to rank. Information Processing & Management, 51(6):757­772, 2015.
[32] J. Xu and H. Li. Adarank: A boosting algorithm for information retrieval. In ACM SIGIR, pages 391­398, 2007.
[33] Z. E. Xu, K. Q. Weinberger, and O. Chapelle. The greedy miser: Learning under test-time budgets. In ICML, 2012.

104

Quit While Ahead: Evaluating Truncated Rankings

Fei Liu, Alistair Moffat, Timothy Baldwin
The University of Melbourne Melbourne, Australia
fliu3@student.unimelb.edu.au, ammoffat@unimelb.edu.au, tb@ldwin.net

Xiuzhen Zhang
RMIT University Melbourne, Australia
xiuzhen.zhang@rmit.edu.au

ABSTRACT
Many types of search tasks are answered through the computation of a ranked list of suggested answers. We re-examine the usual assumption that answer lists should be as long as possible, and suggest that when the number of matching items is potentially small ­ perhaps even zero ­ it may be more helpful to "quit while ahead", that is, to truncate the answer ranking earlier rather than later. To capture this effect, metrics are required which are attuned to the length of the ranking, and can handle cases in which there are no relevant documents. In this work we explore a generalized approach for representing truncated result sets, and propose modifications to a number of popular evaluation metrics.
1. INTRODUCTION AND BACKGROUND
Ranked answer lists are a staple of search; and mechanisms for generating and evaluating them are widely known [1]. In most experimentation, ranked lists are taken to be of arbitrary length, that is, potentially spanning every item in the underlying collection; or to be of some fixed but large length, perhaps to depth d = 1,000. But there are also situations in which there is only a small number of relevant answers ("find the home page of . . . ") or no relevant answers to date ("how do I get LATEX to . . . "), for which generating a long list of unhelpful results is counter-productive. When confronted with such questions, an effective retrieval system might truncate its ranking after just a few suggestions, or even offer no answers at all, choosing to "quit while ahead"; assuming, of course, that the user understands the message being conveyed when a truncated ranking is generated by a system. Here we consider how to compute an effectiveness score for rankings that are of variable ­ and possibly zero ­ length, based on which we propose modifications to a range of popular evaluation metrics.
Effectiveness Metrics for Extended Rankings A large number of effectiveness metrics for ranked lists have been described, covering both binary relevance (the gain ri associated with position i in the ranking is either zero or one), and graded relevance (ri may take on arbitrary non-negative values). These include precision-focused metrics such as Precision@k and Reciprocal Rank (RR), which is
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17­21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914737

the precision at the first relevant document in the ranking. Other

metrics add a recall component, such as Average Precision (AP).

Järvelin and Kekäläinen [2] describe a top-weighted evaluation

metric they call discounted cumulative gain (DCG). A key de-

velopment in this metric is that items near the top of the ranking

are explicitly given a greater influence on the final score than are

items later in the ranking. The formulation usually used is given

by DCG@d =

d i=1

(ri/

log2

(1

+

i)),

where

d

is

the

chosen

eval-

uation depth. An issue with DCG is that the values generated are

unbounded; to address this, Järvelin and Kekäläinen also introduce

a normalized version (NDCG), defined as the DCG score at that

depth divided by the DCG of a permuted ideal ranking in which

all relevant documents are returned at the head of the answer list:

NDCG@d = DCG@d / DCGI @d. An NDCG@d score of 1.0

indicates that, down to depth d, the ranking is as good as would

have been attained by an omniscient system. Note, however, that

the DCG score of a ranking in which there are no relevant answers

is zero; and hence that NDCG is undefined on nil-answer queries.

Other recall-based metrics, including Average Precision and the Q-

measure [5], face the same challenge.

Moffat and Zobel [3] proposed an alternative top-weighted ap-

proach that avoids the need for the normalizing step. Their Rank-

Biased Precision (RBP) metric is based on a simple user model,

assuming that the user always looks at the first returned document,

and then continues from one depth i in the ranking to the next

depth i + 1 with a fixed probability p, their persistence. The ex-

pected per-document rate at which gain is accrued is then given by

RBP = (1 - p) ·

 i=1

ri

· pi-1.

Rank-biased precision

assigns

a score of zero to an empty ranking list, regardless of whether the

query that led to the ranking has answers or not.

Effectiveness Metrics for Truncated Rankings Peñas and Rodrigo [4] note that in some question-answering (QA) scenarios, not responding is preferable to responding incorrectly, and propose a metric they denote c@1. Scores are based on having correct answers at the head of the ranked list, together with a component that is extrapolated for empty lists: c@1 = nac/n + (nac/n) · (nu/n), where nac is the number of correctly answers across a set of n questions, and nu is the number of unanswered questions. However, c@1 is only applicable in cases where each question has a single correct answer, such as reading comprehension tests.
Another option for adding nil-answer assessment to an evaluation is to treat questions for which there are no answers differently from the has-answer queries. This may be appropriate if the distribution for the two classes of questions is imbalanced and nil-answer questions account for a small fraction of queries; the evaluation can then be one of correct classification between the two classes, followed by a standard evaluation within the has-answer class. For example, in the TREC 2001 QA track, there are 49 nil-answer ques-

953

tions, out of 492 test questions. Similar statistics arise in the TREC 2002­2007 QA tracks. But note also that there are cases where nilanswer queries dominate. For example, in duplicate question detection for community question answering, the expectation is that most new questions will not have previously been asked.
Sakai [5] proposed that NIL be regarded as a valid answer list of length one with positive gain, and showed that under this interpretation the Q-Measure (and other recall-based approaches) can be used to evaluate nil-answer questions. A similar approach was also used in the 2001 TREC QA track [6], where systems were permitted to return NIL in their answer lists. Any NIL's that appeared were assigned a gain of ri = 1.0 if and only if there were no "actual" answers to that query, and a gain of ri = 0 otherwise. Systems were free to continue listing documents after the NIL, meaning that a simple hedging strategy is to prefix NIL to every returned list; another, to insert NIL part way through every answer list. We explore the implications of this approach in more detail in Section 3.
2. EVALUATING ARBITRARY RANKINGS
All Rankings Are Different We propose that a system always be viewed as returning a ranking of documents, and that the length of that ranking always be regarded as having been determined by the system in response to the query. We then require that the evaluation process employed should be applicable to all rankings, including those of zero length.
As a motivating example, consider the case of a query for which there are known to be R = 3 relevant answers. For this query the five-document ranking (reading ri values from left to right, with "1" representing relevant, and "0" denoting non-relevant) "10100" is almost certainly superior to the ranking "01001", a relativity supported by all of RR, AP, NDCG, and RBP. Now consider the threeelement ranking "101". It seems clear that "101" must be regarded as superior (or, at the very least, not inferior) to "10100", since it has the relevant documents in the same positions, and fewer nonrelevant documents. Next, consider the ranking "011". Where does it fit in relation to the other three rankings? Most metrics would assess it as being inferior to "101" and better than "01001", but what about in comparison to "10100"? That is, is: "101" > "10100" > "011" > "01001" the preferred ordering from a user's point of view, where > is used as an abbreviation for numeric order, based on score? Or is: "101" > "011" > "10100" > "01001" the preferred relationship? And, what about the ranking "1" ­ is one correct answer and no non-relevant answers better, or worse, than the rankings shown, all of which contain two correct answers? Finally, do any of these relativities change if instead of R = 3 relevant documents, there are known to be R = 5, or R = 10?
In the proposed new framework, in which ranking length is also regarded as being a factor that affects the score, dealing with nilanswer queries becomes a natural extension. If a query has no answers, then we would expect the evaluation metric to tell us that "" > "0" > "00" > "000" , and so on. Indeed, if a query has no answers, and a system returns a ranking containing no documents, would we not wish the score of that ranking to be 1.0, representing "fully correct system response, and cannot be improved on"?
Depth-Sensitive Evaluation To allow ranking length to influence assessed effectiveness, we modify every ranking to add a nominal terminal document at the first rank position after the last one supplied by the retrieval system. For example the ranking "011" is extended to make a new ranking "011t", where "t" represents the terminal document, and reflects that the system declined to provide an answer document in that or any subsequent position. Provided

that a corresponding gain value rt is also assumed, any weightedprecision effectiveness metric, such as RR, Precision@k, or RBP,

can then be used to score the ranking.

The key to making this approach work is selecting a value for

rt, the gain value associated with the terminal document. In the

2001 TREC QA Track, and in the example presented by Sakai [5],

rt = 1.0 iff the question is a nil-answer one, and rt = 0.0 if not.

We propose a more gradual approach. Suppose that the total gain

pool for the query is R  0. Then at depth d  0 in any given

ranking the fraction of the available gain that has been accrued is

given by

d i=1

ri/R.

On

this

basis,

we

define:

1

if R = 0

rt =

d i=1

ri

/R

if R > 0 .

(1)

To understand the implications of this definition, consider the metric RR, defined for binary gain values as the reciprocal of the first rank at which a relevant document appears. If a ranking of length d contains a relevant answer, then RR has the same value as it always does, since the terminal document at depth d + 1 has no bearing. If a ranking of length d does not contain a relevant answer, and if R > 0, then rt = 0 and hence the value of RR is zero, as it should be ­ the system failed to return an answer that exists. But if R = 0, then rt = 1, and the value of RR is given by 1/(d + 1). That is, an empty ranking will be given a score of 1.0 if there are no relevant documents in the collection; the ranking "0" will be given a score of 0.5 when R = 0, and so on. Overall, the adjusted RR computation that takes the terminal document into account smoothly adapts its score on nil-answer queries, as required; and has its previous behavior on has-answer queries.
In the case of RBP, rt is used in a slightly different way. Since RBP computes an infinite weighted sum over a geometric sequence of weights, it is appropriate to presume an arbitrary number of answers past the d th one, all with gain rt. That is, the finite truncated gain vector r1, r2, · · · , rd is treated as an infinite one, r1, r2, · · · , rd, rt, rt, rt, · · · , and the RBP score computed as normal. This has the same effect as taking the RBP residual at depth d, which is given by pd, and multiplying it by rt. That is, we define the adjusted RBP as

d
RBP = RBP @d = (1 - p) · ri · pi-1 + rt · pd . (2)
i=1
As a third example, consider NDCG. To adjust this metric to handle truncated lists, we add rt as a (d + 1) th gain value, as for RR, and then use the usual scoring approach to depth d + 1 rather than to depth d:

NDCG = NDCG @d = DCG@(d + 1) r1, r2, · · · , rd, rt . DCGI @(d + 1) (3)
Note that this approach also means that d is no longer a parameter of the metric and is instead the length of the ranking supplied by the system; note also that the ideal (d + 1)-element ranking used in the denominator includes an extra gain of 1.0 in the first zero-gain position only if there are fewer than d + 1 full- or part-gain answers for the query. For example, if R = 3, and all gain values are binary, then the ranking "101" leads to rt = 2/3, and is scored as:
NDCG = 1/ log 2 + 1/ log 4 + (2/3)/ log 5 = 0.698 , 1/ log 2 + 1/ log 3 + 1/ log 4 + 1/ log 5

where the final term in the denominator arises because in an ideal ranking of d = 3 documents, the corresponding ideal rt value placed in the fourth position of the ranking would be 1.0.

954

Ranking R

"00" "000"

R=0 R=0

"111" "11" "11100" "101" "1" "10100" "011" "01001"

R=3 R=3 R=3 R=3 R=3 R=3 R=3 R=3

rt
1.000 1.000
1.000 0.667 1.000 0.667 0.333 0.667 0.667 0.667

RR
0.333 0.250
1.000 1.000 1.000 1.000 1.000 1.000 0.500 0.500

RBP
0.250 0.125
1.000 0.917 0.906 0.708 0.667 0.646 0.458 0.302

NDCG
0.500 0.431
1.000 0.922 0.971 0.698 0.742 0.678 0.554 0.490

AP
0.333 0.250
1.000 0.648 0.917 0.528 0.306 0.491 0.403 0.299

Table 1: Example truncated answer rankings and their modified scores, for two different queries, one with R = 0 and one with R = 3. The parameter p = 0.5 is assumed for the RBP computation. Within each group, the results are sorted by RBP , which (by chance, for these examples) also corresponds to RR -order.

Average precision (AP) is handled similarly, by defining rd+1 = rt, and then scoring the resulting extended-by-one ranking:

1 d+1

AP

= R+1

ri

i j=1

rj

.

i

(4)

i=1

As is also the case with NDCG , the reference ranking used by AP contains R instances of ri = 1, followed by a nominal terminating document with a gain of 1.0, that is, R + 1 values in total.
Table 1 shows scores computed for a range of rankings using the modified versions of RR, RBP, NDCG, and AP. The different adjusted metrics place different emphases on the tradeoff between recall and precision. All of the metrics respect the strict pairwise orderings noted earlier, for example, that "101"  "10100"; but they vary in their response to other relativities, such as the question as to whether "1" is better or worse than "101". Note how the different metrics place different emphases on the rankings, resulting in variations in their score orderings.

3. EXPERIMENTS AND RESULTS
Tasks and Test Collections To explore the ramifications of the proposed approach, we employ the runs submitted to the main task of the TREC 2001 QA track. Participants were invited to submit a ranked list of [doc-id, answer-str] pairs of length up to five for each question; and for questions deemed to have no answer, were permitted to return "NIL" rather than one of the pairs. Overall, 36 groups contributed a total of 67 submissions to the QA main task; 47 of them are available for download.1 The question set consists of 492 queries, 49 of which are nil-answer queries. The 443 hasanswer questions have on average 25.7 relevant answers each.
Interpretation of Truncation To evaluate the proposed approach, we transform each individual run using the rules shown in Table 2, so that we accurately capture any evidence of deliberate truncation. The first two rules, covering cases where fewer than five results are provided, or where an explicit "NIL" is provided, are evidence of system-initiated truncation, and are processed as such in our comparison; in the third case we cannot infer truncation, and those runs are retained intact and scored in the original manner by the unmodified metrics throughout our experimentation.
1http://trec.nist.gov/results/trec10/qa_main_input.html

aNIL n
i 5 -1 < 5 -1 = 5

modified run
a1, · · · , ai-1, t a1, · · · , an, t a1, · · · , a5

Table 2: Transformation of a run a1, · · · , aNIL, · · · , an , where aNIL is the rank of an explicit NIL document (either rank i  [1, n], or -1 indicating not present) to a new ranked list.

#

20,000 15,000

Original Transformed

10,000

5,000

0

0

1

2

3

4

5

n-answer response

Figure 1: Distribution of lengths of 23,124 query responses.

Figure 1 shows the distribution before and after transformation of the 23,124 runs submitted for the 492 queries by the 47 participants. The number of five-answer lists is reduced from around 20k to 16k, generating a total of approximately 7k truncated answer lists post-transformation. The number of zero-answer lists is zero before the transformation, because even when a system believes a query is a nil-answer question, it must return a "NIL" to indicate so. This also accounts for the decline in the number of single-answer responses post-transformation.
Results and Analysis We first compare the TREC QA systems against each other using the TREC methodology (that is, with NIL in runs given a gain of 1.0 iff a query is nil-answer and otherwise given a gain of 0.0, and with metrics then applied in their standard form), and using our proposed modified approach applied to the transformed version of each run. Four different effectiveness metrics were explored, with the goal of determining the extent to which systems are affected by the proposed alteration in methodology. Each run for each system was scored using the two different approaches, and then system averages computed. In all of these evaluations, a [doc-id, answer-str] pair is considered correct iff the answer-str contains an answer to the question and is supported by the document specified by the doc-id.
Table 3 compares the system orderings generated by the four pairs of original/modified metrics using Kendall's , which computes a correlation coefficient between pairs of ordered lists over the same domain. Three evaluation metric pairs give rise to  scores greater than 0.9, indicating strong agreement between the system ordering induced by the original metric and the system ordering generated by its modified version. The strong agreement between RR and RR was expected, because scores are primarily derived from just one relevant document, and because only a minority of the runs had explicit NIL markers. The similarly strong agreement between NDCG and NDCG was more surprising. At the other end of the scale, the pair AP/AP has the lowest  among the four metrics, but they are still strongly correlated.

955

RR Score

0.6

0.4

B

A 0.2

0

0

0.2

0.4

0.6

RR

Figure 2: Relationship between RR and RR scores for 47 systems, with each system's score the mean over 492 queries.

Metric Pair
RR/RR NDCG/NDCG RBP@0.5/RBP @0.5 AP/AP

Kendall's 
0.960 0.958 0.916 0.870

Table 3: Kendall's  correlation coefficient calculated from the system orderings generated by pairs of original and modified metrics.

Figure 2 provides details of the relationship between the RR and RR scores for the set of systems. Overall, RR and RR are in high agreement in regard to both system ordering (Table 3) and in terms of the actual scores assigned. However, there are also inverted pairs, where a system is ranked higher by the original metric but has inferior score in the modified. For example, the system marked with "A" has a slightly higher score than does "B" for RR, but is ranked lower than "B" by RR because of "B"s aggressive (and effective) truncation strategy.
We also investigated the impact of truncation on performance of individual systems. The horizontal axis in Figure 3 (% truncation) is the fraction of answer lists of length less than five, including NIL, but excluding terminal documents. Both of the top two systems receive a boost in score when truncation is taken into consideration. In the [0.15, 0.3] score range, despite the aggressive truncation, there are systems that obtain little improvement, in part due to their placement of a NIL at the end of every run. In addition, much of the truncation is a consequence of the system's inability to find a correct answer, rather than intentionally terminating the answer list. In such cases, even though there is explicit truncation, the system is not rewarded as there is no relevant document in the truncated answer list. Some systems sometimes prematurely truncate an answer list by placing a NIL before relevant documents. This causes the performance to drop when the modified metrics are employed. Two of the systems generated a NIL in the fifth position of all of their answer lists.
4. CONCLUSIONS AND FUTURE WORK
We have identified an opportunity to refine the way in which truncated rankings are evaluated, and at the same time deal seam-

0.5

RBP

0.4

RBP

0.3

0.2

0.1 0

20 40 60 80 100 % Truncation

Figure 3: Impact of proposed methodology on effectiveness scores of the top 20 systems. Percentage truncation (horizontal axis) is the fraction of truncated answers (length of answer list < 5, excluding the terminal document), with the two points marking pre- and post transformation scores. The RBP parameter is 0.5 throughout.

lessly with a well-known shortcoming of recall-based evaluation metrics, namely, their inability to cope with queries with no relevant documents. By providing modified effectiveness approaches that provide subtle differentiation between runs of different lengths (for example, because "110" < "11" in our mechanism, but not in previous approaches to the problem) we are better able to nuance system evaluations. The approach we employ ­ the appending of a terminal document to every ranking, to indicate the truncation point, and modifications to a range of standard evaluation metrics including RR, RBP, NDCG and AP ­ is both intuitive, and also easy to implement and apply. In retrieval experiments over a large QA dataset, containing a non-trivial fraction of nil-answer queries, we illustrated the effectiveness of the modified metrics, and demonstrated that a refined evaluation of truncated document rankings can help differentiate system orderings.
The obvious next step in our project is the development of methods for taking long document rankings and identifying, relative to the truncation-sensitive metrics, the point in each at which truncation is appropriate. One possible way of approaching this problem would be through analysis of the distribution of document scores in the ranking, in both relative and absolute terms. Query analysis could also be performed to predict the R value for a given query, for incorporation into the truncation process. We leave this exploration to future work.
Acknowledgments The authors thank MACE Engineering Group for their early support of this work. The third author was supported by ARC grant FT120100658.
References
[1] C. Buckley and E. M. Voorhees. Retrieval system evaluation. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, chapter 3, pages 53­75. MIT Press, 2005.
[2] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Sys., 20(4):422­446, 2002.
[3] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Sys., 27(1):2:1­2:27, 2008.
[4] A. Peñas and A. Rodrigo. A simple measure to assess non-response. In Proc. ACL/HLT, pages 1415­1424, 2011.
[5] T. Sakai. New performance metrics based on multigrade relevance: Their application to question answering. In Proc. NTCIR, 2004.
[6] E. M. Voorhees. Overview of the TREC 2001 question answering track. In Proc. TREC, pages 42­51, 2002.

956

Ranking Documents Through Stochastic Sampling on Bayesian Network-based Models: A Pilot Study

Xing Tan1, Jimmy Xiangji Huang1 and Aijun An2
Information Retrieval and Knowledge Management Research Lab 1School of Information Technology, 2Department of Computer Science & Engineering
York University, Toronto, Canada
{xtan, jhuang}@yorku.ca, aan@cse.yorku.ca

ABSTRACT
Using approximate inference techniques, we investigate in this paper the applicability of Bayesian Networks to the problem of ranking a large set of documents. Topology of the network is a bipartite. Network parameters (conditional probability distributions) are determined through an adoption of the weighting scheme tf -idf . Rank of a document with respect to a given query is defined as the corresponding posterior probability, which is estimated through performing Rejection Sampling. Experimental results suggest that performance of the model is at least comparable to the baseline ones such as BM 25. The framework of this model potentially offers new and novel ways in weighting documents. Integrating the model with other ranking algorithms, meanwhile, is expected to bring in performance improvement in document ranking.
Keywords
Info. Retrieval; Bayesian Networks; Stochastic Sampling
1. INTRODUCTION
Probabilistic Graphical Models [10] in the form of Bayesian Networks (BN) [6] are widely used to represent knowledge with uncertainties. In the recent years, computational technologies and tools for BN-based models are becoming increasingly powerful. That being the case, modeling problems in Information Retrieval (IR), in particular for document ranking, as probabilistic inference problems in BNbased models has achieved only limited success to date. Major reasons for such relatively small progress in BN-based approaches for document ranking are, for one, conceptually it is challenging to appropriately identify causalities in document ranking (i.e., deciding network topology) and then to accurately capture the uncertainties (i.e., deciding network parameters) in order to construct a BN model for IR; and two, computationally exact inference algorithms associated with the model is bound to be intractable as practically the size of the BN model in terms of nodes representing both the number of documents and vocabulary size in words, can be easily in a few millions.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914750

In this paper, we investigate the applicability of BNs to the problem of ranking a large set of documents. We propose a model, which specifically takes into considerations both the appropriateness in the semantics of causalities, and the computational tractability of probabilistic inferences. Topology of the network is a bipartite. Conditional probability distributions are determined through adopting the weighting scheme tf -idf . Experimental results, obtained from working on both computer-generated and standard document sets suggest that performance of the model is at least comparable to the baseline ones such as BM 25 ([2, 11]).
The remainder of this paper is organized as follows. Section 2 presents the background and preliminaries. Section 3 introduces the model. Experimental results are reported and analyzed in Section 4. Section 5 concludes the paper.
2. PRELIMINARIES
In this section, document ranking in IR, and BN, are briefly reviewed.
2.1 Document Ranking in IR
One of the fundamental tasks in IR is document-ranking: Given a set of documents D such that D = {d1, . . . , dM } and |D| = M , a set of terms T = {t1, . . . , tN } and |T | = N , and a collection of query terms q such that q  T , documents in D need to be ranked in a complete order according to their respective relevance to q (other criteria such as "diversity" might also be considered for ranking). To do this, a typical approach is to define a score function score(q, di) that returns a numeric score for each document di  D with respect to q. Documents can then be ranked on their scores in descending order. The top S elements will be selected to construct the set S, where |S| = S.
Let tft,d denote term frequency, the number of occurrences of term t in document d; dft denotes document frequency, the number of documents in D that contain the term t; and idft = log (M/dft) denotes inverse document frequency. Summing up on tft,d × idft for each term t  q with respect to d defines a baseline score function for document ranking: score(q, d) = tq(tft,d ×idft). Definitions for variant score functions of tf -idf such as BM25, can be found in [11]. In addition, collection frequency in D and its subset S, are defined as the total number of occurrences of t in D and in S, and denoted by cft and sft, respectively.
2.2 Bayesian Networks
A Bayesian Network is a directed acyclic graph where nodes correspond to random variables [6]. Pairs of nodes in the graph might be connected by a directed edge. For example, given two nodes X and Y in the graph, if X enters Y , it is said that X is a parent of Y . Effect of the par-

961

ents of a node X in the graph is quantified by a conditional probability distribution P (X|P arents(X)).
BNs are often used to carry out probabilistic inference: computing posterior distribution of a set of variables given a set of evidence variables ­ variables whose values are observed or assigned. Consider a simple example of Baby World, which consists of four random variables H, T , C, and R, corresponding to the variable facts that the baby is Hungary, Tired, Crying, and scReaming, respectively. A BN for this world is shown in Figure 1. After all four conditional probability distributions as listed in the figure are specified, we could compute, for example, the probability that the baby is hungry if we observe that it is crying but not screaming: P (H is true | C is true, and R is false).

P (H) Hungry? H

P (T ) T Tired?

P (C|H, T ) C Crying?

P (R|H, T ) scReaming? R

Figure 1: A Bayesian Network: Baby World.
3. MODEL
This section presents the model, explains how samplings are performed, and justifies the merits of our model.

3.1 Network Topology and Conditional Probabilities
Suppose a user specifies a set of terms as a query q for a set of documents D, a subset S  D of documents need to be retrieved and ranked in a complete order. To be explained in this section, we formulate this problem into the problems of calculating posterior probability values in a BN-based model where the original query is treated as the observed evidence.
In our model, the probability space is induced accordingly by two sets of random variables D (document random variables) and T (term random variables), where Di  D for 1  i  M , and Tj  T for 1  j  N . Each document Di takes two values: V al(Di) = {d1i , d0i }, which represents the values of "Di selected with respect to a query" (d1i ) or not (d0i ); Similarly, V al(Tj) = {t1j , t0j }, which represents the values of term "Tj is a query term" (t1j ) or not (t0j ). The BN, as shown in Figure 2, is a two-layer directed graph, which contains a node in the top layer for each document variable Di and a node in the bottom layer for each term variable Tj. In the graph, an edge from Di to Tj represents that term Tj appears in the document Di. We assume no edges between document variables in D, and no edges between term variables in T . In addition to the graph, two types of

D

D1 D2

... ...

DM

T

T1

T2

... ...

... ...

TN

Figure 2: A BN graph for document ranking.
probabilities need to be specified to capture the nature of the dependence of variables: P (Di), the prior distribution over a document Di, and P (Tj|D1, . . . , DM ), the conditional probability distribution of Tj given D1, D2, . . . , and DM . In our model, P (Di) represents the distribution that Di is selected in S or not, hence it is reasonable to define, for any

document Di in D, the probability that Di is eventually selected into S equals the ratio of the size of S to the size of D, i.e., P (d1i ) = S/M .
The conditional distribution P (Tj|D1, . . . , DM ) specifies the distribution over the term Tj, which depends on the actual content of S, the set of documents selected. That is to say, specifically, for each subset of D (totally 2M of them), we need to specify a distribution for t1j , the event that tj is actually a query term. Since the number of parents of a term in the network is not bounded by a constant, we know that exact inference here has exponential time complexity in worst cases. Nevertheless what we really need is to calculate, for any document variable Di where 1  i  M , the relevance of Di to evidence q~, i.e., the posterior probability of P (d1i |q~), the value of which can be estimated through stochastic sampling. For simplicity in explanation, we assume that q~ contains only one term t, without loss of generality.
3.2 Estimating Posteriors Through Sampling

General reasoning problems of probabilistic inference in

BNs, and even their corresponding approximate versions,

are NP-hard ([4], [5]). Due to this computational intractabil-

ity, one often turns to randomized sampling methods (e.g.,

Rejection Sampling [12], which is used in our current re-

search) to approximate posterior probabilities. Asymptoti-

cally accuracy of these sampling methods would usually be

improved as the number of samples increases.

Given a BN and its specified prior distributions for all n

variables {X1, X2, . . . , Xn}, where Xi  X for 1  i  n,

forward sampling samples all nodes in X in an order consis-

tent with the topological structure of the BN, and the prob-

ability of a specific event, written as (x1, . . . , xn)1, generated

from forward sampling equals to

n i=1

P

(xi|

parents(Xi)),

which in turn equals to the joint distribution P (x1, . . . , xn).

Suppose totally N samples are taken, and the number of oc-

currences of an event (x1, . . . , xn) equals to N(x1,...,xn), then the ratio N(x1,...,xn)/N is an approximation to P (x1, . . . , xn). With observation of evidence e for E, where E  X , the con-

ditional probability P (X = x|e) can be further estimated

through Rejection: first, all samples that do not match e

are rejected in N , to obtain N1; second, all samples in N1

and compatible with X = x are put into N2; third, N2/N1

is an estimate to P (X = x|e).

Consider our model again and suppose there are P sam-

ples where |P| = P . During sampling, we dynamically main-

tain a vector of counters C, where |C| = M . All counters

in C are initialized to zero. Our sampling strategy say for

the jth sample P j where 1  j  P : Step 1, each docu-

ment variable is sampled according to the distribution S/M ;

Those selected document variables are put into Sj, thus the

expected value of |Sj| is S. Step 2, we accept this sample

if and only if the collection frequency of Sj with respect to

term t proportionally exceeds the one of D. Formally, the

sample is accepted iff

sftj cft

>

S M

.

If sample P j

is accepted

and Di  Sj, ci  C, which is the corresponding counter for

Di, would be increased by one.

After completion of sampling, the set P would be mutual-

exclusively partitioned into two sets, the set of accepted sam-

ples Paccepted, and the set of rejected ones Prejected. The

vector C would be updated for |Paccepted| times. Values

1The term (x1, . . . , xn) is an abbreviation for (X1 = x1, . . . , Xn = xn).

962

stored in the counters of C, are actually scores for their corresponding documents with respect to the query term t. The documents can thus be ranked according to their scores.
3.3 Justification of Methodology
In the literature, considerable research in investigating potential linkages between BNs and IR in general, has been reported (most notably [1], [7], [13], [14]). The originality and value of our research contribution lies in the following facts.
Model Semantics. The model defines 2(M+N) different states, for different combinations of truth assignments to all random variables in D  T . A state specifies an instance of which random variables are true and which are false. For each state, its joint probability distribution theoretically can be calculated (although computationally it might be impractical). We are only interested in those states where statistically S out of D variables are true, since we only concern about the problem of selecting S out of D documents related to a given query.
Causality. In the model, document variables in D are designed, in consistence with common perceptions, to have direct causal influence on term in T . For example, causal relation "Di  Tj" is interpreted as: If Di is selected to be a member of S (i.e., Di  S) then the term Ti should be of interest to the user.
Scalability. The size of the problem of document ranking in practise is often in the magnitude of a few millions, if not more. A network built-up from these problems is large in size and multiply connected, making it dauntingly challenging to perform exact probabilistic inference. Consequently, it can be seen from the literature that experiments in earlier work (e.g., [7] and [1]), are restricted to cases with maximal a few thousand documents only. The development of BNs however has reached the point where approximate inference algorithms, such as randomized sampling, loopy propagation, or variational approximation, can make a practical difference. We adopt a direct sampling method in this research.
Bipartite Network Structure. The underlying undirected graph of the network is a bipartite: nodes are grouped into two types, only connections between nodes from two different types are allowed. Recently, Bayesian models on bipartite graphs have found their ways to modeling real-world applications in social networks, with appealing properties demonstrated [3]. It remains to be investigated how these results can be utilized into our own framework of BN for IR. Nevertheless, due to this simplicity of topological structure, additional features (e.g., ontological/relational/logical representation and reasoning: see [8] for a mosaic of such proposals) can be incorporated into the current model.
4. EXPERIMENTAL RESULTS
In this section we compare the performance of our proposed model with the ones of tf -idf , BM 25 and Golden (to be explained in Section 4.1). We first work on a set of computer-generated random documents (DR1 , DR2 , DR3 and DR4 ) and then on D1 and D2, which are two subsets of WT2G, a standard TREC test collection.
4.1 Documents Generation
To simplify matters, we assume that all generated documents in DR are with same document length, which equals to the size of the vocabulary TR. For any document Dr  DR, occurrences of terms in Dr follow a Normal Distribution N (µ, ), where the values of mean µ and the standard de-

Term Frequency Term Frequency

An Example Document
3000

Center Shifted
3000

2500

2500

2000

|D| = 100000

2000

|D| = 100000

µ = 50

µ = 18

 = 17

 = 17

1500

1500

1000

1000

500

500

0 0 10 20 30 40 50 60 70 80 90 100
Terms

0 0 10 20 30 40 50 60 70 80 90 100
Terms

Figure 3: An example document (created according

to a Normal Distribution with µ = 50 and  = 17),

and its variant with the center shifted to 18 (µ = 18).

viation  can be adjusted. However all documents in DR share the same µ and . Note that the smaller the value of , the terms cluster more closely to µ. The center of a given document is shifted to a random center before being stored into a vector. To illustrate the idea, an example document with µ = 50,  = 17, |DR| = 10000, and |TR| = 100, and its variant with shifted center (now, µ = 18) is respectively shown in the left subplot (and the right one) of Figure 3.
Accordingly we introduce the baseline ranking method Golden. That is, given an input query q and a document d, the score(q, d) is defined as the difference between the center of d, and q. When the value of score(q, d) is smaller (greater), it means document d is more (less) related to the term q. Golden is used as one of the three methods for comparisons in Figure 4.
4.2 Experiments
We first work on four sets of randomly generated documents: DR1 , where  = 1250, DR2 ,  = 1000, DR3 ,  = 833, DR4 ,  = 416. Between these sets from DR1 up to DR4 , documents are more closely clustered around their means, thus intuitively more reliable in the sense of IR. Collection size, document length, and vocabulary size, are set to be all equal: |DR| = |TR| = 10000. Experiments related to a specific set is pictorially summarized in the corresponding sub-figure in Figure 4.
Consider Figure 4.a, for example, a term is queried against DR1 , three different ranking methods, i.e., Golden, BM25, and Rejection, return three different completely ordered sequences of 500 elements (the number 500 is obtained from |DR| × 0.05, where 0.05 is the pre-specified ratio, i.e., portion of all documents in DR1 need to be ranked). Results of pair-wise comparisons between these three methods are reported in Figure 4.a, where x-axis values indicate sample sizes and y-axis values indicate how many documents out of the 500 ranked ones are actually agreed between two given methods. For example, the red dot pointed by the arrow in Figure 4.a refers to the fact that totally 336 documents are shared by the 500 documents retrieved from applying BM25 on DR1 , and the 500 elements obtained from performing Rejection Sampling on DR1 (with the sample size equaling to 0.1 Million).
Documents in both D1 and D2 (Figure 5) are drawn from dataset collection WT2G where |D1| = |D2| = 2500, |T1| = 50961 and |T2| = 127487. First 100 elements obtained from three different ranking methods, tf -idf , BM 25, and Rejection are pair-wise compared in Figure 5.
4.3 Brief Discussion
This section draws the major observations from the present experimental study, and discusses some implications.

963

Num. of Agreed Documents Num. of Agreed Documents

(a) DR1 and  = 1250

500

Golden-BM25

450

Golden-Rejection

BM25-Rejection

400

350

300 250



200

150

100

50

0

0

1K

10K 50K 0.1M 0.5M 1M 1.5M 2M

Sample Size

(c) DR3 and  = 833

500

Golden-BM25

450

Golden-Rejection

BM25-Rejection

400

350

300

250

200

150

100

50

0

0

1K

10K 50K 0.1M 0.5M 1M 1.5M 2M

Sample Size

(b) DR2 and  = 1000

500

Golden-BM25

450

Golden-Rejection

BM25-Rejection

400

350

300

250

200

150

100

50

0

0

1K

10K 50K 0.1M 0.5M 1M 1.5M 2M

Sample Size

(d) DR4 and  = 416

500

Golden-BM25

450

Golden-Rejection

BM25-Rejection

400

350

300

250

200

150

100

50

0

0

1K

10K 50K 0.1M 0.5M 1M 1.5M 2M

Sample Size

Figure 4: Pair-wise comparisons among the ranking methods Golden, BM25, and Rejection on data-sets with different standard deviations.

Num. of Agreed Documents

(a) Data Set: D1
100

90

80

70

60

50

40

30

20

TfIdf-BM25

10

TfIdf-Rejection

BM25-Rejection

0

0

1K

5k

7.5k 10K 50K 0.1M 0.5M 1M

Sample Size

Num. of Agreed Documents

(b) Data Set: D2
100

90

80

70

60

50

40

30

20

TfIdf-BM25

10

TfIdf-Rejection

BM25-Rejection

0

0

1K

5k

7.5k 10K 50K 0.1M 0.5M 1M

Sample Size

Figure 5: Pair-wise comparisons among the ranking methods tf-idf, BM25, and Rejection on two datasets (2500 documents each) from WT2G.

For more closely clustered documents, in essence all methods agree more on their rankings (as shown in Figure 4, they agree most on the set DR4 , but least on DR1 ); Following the same argument, we should claim that D1 is more clustered than D2.
In our experimental settings, increasing sample size initially improves the performance sharply, but it tends to be leveling off after sample size is greater than certain value (e.g., 0.5M in the subplots of Figure 4). As the sample size increases, asymptotically the Rejection method agrees at least 80% with BM25 for almost all sets except for D2 (around 60% only). It seems that we should conclude that the proposed BN-based model can achieve competitive performance levels at relatively low cost in sampling.
Since in Rejection, estimating posterior probabilities are based on the tf -idf ranking scheme, Rejection is a stochastic variant to the tf -idf ranking method. BM 25, meanwhile, can be deemed as a refinement to tf -idf . Hence, it is not a surprise that BM 25 agrees largely with Rejection. The more intriguing observation is that, with standard data-sets, the two methods disagree at least on 20% of their rankings. The reason as we speculate is either Rejection behaves somewhat differently from BM 25 in practise, or sampling with the current settings can get trapped and do not converge. In order to unravel this intricacy, a further investigation is necessary and desirable.

Num. of Agreed Documents Num. of Agreed Documents

5. SUMMARY & FUTURE WORK
In this paper, document ranking in IR is transformed into the problems of estimating posterior probabilities through stochastic sampling on a BN designed for IR. Experimental results from this pilot study is quite encouraging in the sense that, with moderate sampling efforts, the model demonstrates its ranking capability comparable to BM 25. Additionally, graph-based structure and probability-based parameters of the model, together with other considerations in the model, suggest that new and novel weighing schemes for document ranking are conjecturally within a reach.
Among many possible avenues, our direct future research includes 1) further evaluating performance of the model on WT2G, WT10G, and other standard dataset collections; 2) testing on parametric settings other than the current one that is based on term-frequency; 3) testing other sampling strategies (e.g., Gibbs Sampling [10], and the most recent ones [9]) to improve sampling efficiency and performance.
Feedback to this work we have received encourages us to investigate in a broader context the relationships between the proposed BN model and other term weighting models ([15, 16]). With ease, most existing probability theory-based models in IR can actually be derived in this BN-based framework. It is thus rather promising to exploit this framework for a deeper understanding of existing term weighting models, and for the developments of new and better models in Information Retrieval.

6. ACKNOWLEDGEMENTS
We gratefully acknowledge the anonymous reviewers for their insightful comments and suggestions. This research is supported by a Discovery grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), an NSERC CREATE award in ADERSIM2 and an ORF-RE (Ontario Research Fund - Research Excellence) award in BRAIN Alliance3.

7. REFERENCES

[1]
[2] [3]
[4] [5] [6] [7] [8] [9]
[10] [11] [12] [13] [14] [15] [16]

S. Acid, L. M. de Campos, J. M. Fern´andez-Luna, and J. F. Huete. An information retrieval model based on simple Bayesian networks. Int. J. Intell. Syst., 18(2):251­265, 2003.
M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. Williams. Okapi at TREC-5. In Proc. of TREC, pages 143­166, 1996.
F. Caron. Bayesian nonparametric models for bipartite graphs. In Advances in Neural Information Processing Systems 25, pages 2051­2059. Curran Associates, Inc., 2012.
G. F. Cooper. The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42(2):393 ­ 405, 1990.
P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial Intelligence, 60(1):141 ­ 153, 1993.
P. A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press, New York, NY, USA, 1st edition, 2009.
R. Fung and B. Del Favero. Applying Bayesian networks to information retrieval. Commun. ACM, 38(3):42­ff., Mar. 1995.
L. Getoor and B. Taskar. Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning). The MIT Press, 2007.
K. Kandasamy, J. G. Schneider, and B. P´oczos. Bayesian active learning for posterior estimation - IJCAI-15 distinguished paper. In Proc. of the 24th IJCAI, pages 3605­3611, 2015.
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. The MIT Press, 2009.
C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.
S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press, Upper Saddle River, NJ, USA, 3rd edition, 2009.
H. Turtle and W. B. Croft. Inference networks for document retrieval. In Proc. of the 13th ACM SIGIR, pages 1­24, New York, NY, USA, 1990.
H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187­222, July 1991.
J. Zhao, J. X. Huang, and B. He. CRTER: Using Cross Terms to Enhance Probabilistic IR. In Proc. of the 34th ACM SIGIR, pages 155­164, 2011.
J. Zhao, J. X. Huang, and Z. Ye. Modeling Term Associations for Probabilistic Information Retrieval. ACM Trans. Inf. Syst., 32(2):1­47, 2014.

2http://www.yorku.ca/adersim 3http://brainalliance.ca

964

Sampling Strategies and Active Learning for Volume Estimation

Haotian Zhang, Jimmy Lin, Gordon V. Cormack, and Mark D. Smucker
University of Waterloo, Ontario, Canada {haotian.zhang, jimmylin, gvcormac, mark.smucker}@uwaterloo.ca

ABSTRACT
This paper tackles the challenge of accurately and efficiently estimating the number of relevant documents in a collection for a particular topic. One real-world application is estimating the volume of social media posts (e.g., tweets) pertaining to a topic, which is fundamental to tracking the popularity of politicians and brands, the potential sales of a product, etc. Our insight is to leverage active learning techniques to find all the "easy" documents, and then to use sampling techniques to infer the number of relevant documents in the residual collection. We propose a simple yet effective technique for determining this "switchover" point, which intuitively can be understood as the "knee" in an effort vs. recall gain curve, as well as alternative sampling strategies beyond the knee. We show on several TREC datasets and a collection of tweets that our best technique yields more accurate estimates (with the same effort) than several alternatives.
1. INTRODUCTION
Suppose we would like to estimate the number of relevant documents in a collection for a particular topic. We refer to this as the volume estimation problem. How would we go about doing this both accurately, such that our estimate is as close as possible to the actual value, and efficiently, with as little effort as possible? This problem presents an interesting twist on the problem of high-recall retrieval. For example, in electronic discovery [6], the litigants are interested in the actual documents, whereas we just want the volume of the relevant documents. Of course, if we can find all the relevant documents, we can just count them--so existing active learning techniques for high-recall retrieval provide a baseline. Alternatively, we could just randomly sample from the collection to estimate the prevalence and infer the volume. The question is: can we do better than either approach?
Even assuming we can, who cares? Under what circumstances would we like to know the number of relevant documents without identifying the actual relevant documents? The concrete instantiation of our problem is estimating the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914685

volume of social media posts (e.g., tweets) pertaining to a topic. Volume estimation in this case is fundamental to several real-world applications: tracking the popularity of politicians and brands, the box-office appeal of movies, the audience for a sporting event, the potential sales of a product, and so on. The number of tweets pertaining to unfolding events is an often-quoted statistic in media coverage. Thus, volume estimation is not only academically interesting, but has significant real-world value.
The contribution of this paper is the development and evaluation of a technique for volume estimation based on active learning and sampling. Consider an active learning approach that can be characterized by a gain curve. The "knee" of that curve corresponds to the point where all the "easy" documents have been found. Our idea is to take advantage of active learning until the knee, and then use sampling techniques to extrapolate on the remainder of the collection. We present a simple technique for finding the knee that works well in practice and explore three different sampling approaches past the knee. On several TREC datasets and a collection of tweets, we show that our stratified sampling technique yields the most accurate estimates compared to other techniques with the same amount of effort.
2. BACKGROUND AND RELATED WORK
The volume estimation problem is related to retrieval techniques that focus on achieving very high recall--motivating application domains include legal eDiscovery, systematic reviews in evidence-based medicine, and locating prior art in patent search. The starting point of our work is the socalled baseline model implementation (BMI) that was provided to participants of the TREC 2015 Total Recall track, whose principal purpose was to evaluate, through a controlled simulation, methods to achieve very high recall--as close as practicable to 100%--with a human assessor in the loop. BMI is based on the "AutoTAR" technique of Cormack and Grossman [3]. Starting with a query, AutoTAR applies continuous active learning using relevance feedback to prioritize documents for human assessment. As in standard active learning, relevance judgments incrementally refine an underlying relevance model--but unlike the standard formulation of active learning, the point of AutoTAR is not to arrive at the best decision boundary between relevant and non-relevant documents, but rather to find all the relevant documents within a finite collection.
The effectiveness of techniques for high-recall retrieval is typically characterized by gain curves plotting effort (x-axis) vs. recall (y-axis). Most techniques, including the BMI and

981

even manual approaches, exhibit gain curves that begin with a "ramp up" phase, followed by a period where gain rises steadily, indicating the continuous discovery of "easy to find" relevant documents, and end with a region where the gain curves level off as the remaining relevant documents become more difficult to find. The "knee" is where the gain curve levels off, and this is a feature we exploit in our solution. In this work, we use the BMI from the TREC 2015 Total Recall track as-is, with the enhancement of a stopping criterion.
3. APPROACH
Suppose we would like to estimate the number of relevant documents in a particular collection consisting of D documents. How might we go about doing this?
A na¨ive approach might be to randomly sample (without replacement) documents from the collection and assess them for relevance. We can approximate this as a Bernoulli process, for which the volume estimate RT is D ¨ RE{E, where we find RE relevant documents after examining a total of E documents. Note that this does not form a complete algorithm because we still need to know how many samples to draw. We return to this issue later when explaining our paired experimental methodology.
An alternative starting point might be to leverage an existing high-recall retrieval technique such as the BMI (Section 2). That is, we simply find all the relevant documents, which is a trivial way to determine the count. The problem, however, is that the BMI also does not come with a stopping criterion. Although various researchers have tackled this and related issues [7, 1], it is by no means solved.
Regardless, let's say we apply BMI to judge A documents. During this process, we will have explored some fraction of the collection that is more likely to contain relevant documents, and say we discover RA documents. This value, of course, provides a lower bound on the number of relevant documents in the collection. But the problem is that we don't know how many relevant documents there are in the documents we didn't look at (what we refer to as the residual collection). Nevertheless, we can estimate an upper bound using the rate at which we're finding relevant documents just before we stopped, and extrapolate to the remainder of the collection. However, this makes a terrible assumption because any active learning technique will prioritize documents more likely to be relevant before documents less likely to be relevant, and so such an extrapolation would yield an unrealistically large overestimate.
One solution is to sample the residual collection. This requires answering two questions: First, how do we find the knee (i.e., the value of A)? Second, how do we sample after the knee? We tackle these two questions in turn.
3.1 Find the Knee
In our approach, we employ the BMI augmented by the following the knee-finding method proposed by Cormack and Grossman [4]. In each iteration, the BMI selects exponentially larger batches of documents for human judgment. After receiving feedback for each batch, we can trace the gain curve described in Section 2; to be precise, the y axis is now the number of relevant documents found and not recall.
At the end of each iteration, we have a point that corresponds to the total number of relevant documents found and the total number of documents judged thus far. We propose a candidate knee point as follows: find a point on

the gain curve with maximum perpendicular distance from

a line between the origin and the current (i.e., last) point

of the curve. Let p0 be the slope of the line from the ori-

gin to the candidate knee, p1 be the slope of the line from

the candidate knee to the last point, and the slope ratio

"

p0 p1

.

We accept the candidate knee (and terminate) if:

(1) the number of documents examined exceeds 1000 (to en-

sure that the active learning process has gotten beyond the

"ramp up" phase), and (2)   6, if at least 150 relevant

documents have been retrieved, or   156 ´ r, if r  150

relevant documents have been retrieved. The second clause

in (2) is a special case for handling topics with few relevant

documents. The parameters for this technique were tuned

on a private dataset.

Note that to be precise, this technique doesn't actually

discover the knee until we've "passed" the knee, but the intu-

ition nevertheless holds. The actual switchover point where

we stop active learning and begin sampling corresponds to

the point where we discovered the knee. However, for ex-

pository convenience we still refer to "stopping at the knee".

3.2 Sampling Strategies
Based on the knee-finding algorithm described in the previous section, we apply the BMI until the stopping criterion is met. At that point, we have judged A documents and found RA relevant documents. The next question is: what do we do with the residual collection that we have not yet examined? We present three sampling strategies:
Negative Binomial Sampling. In this approach, we sample from the residual collection until we encounter M relevant documents; let's say this process requires us to examine S documents. Each sample can be modeled as a Bernoulli trial, and thus the sampling process can be characterized by a negative binomial distribution. Under this interpretation, the minimum variance unbiased estimator for p^, the probability of success (i.e., probability of a document being relevant) is given as p^ " pr ´ 1q{pr ` k ´ 1q, where r is the number of successes (relevant documents, which we set to M ) and k is the number of failures (non-relevant documents) in our sequence of observations [5].
From this, our estimate of the total number of relevant documents, RT , is as follows (note S " r ` k):

RT

"

RA

`

pD

´

Aq

pM ´ 1q pS ´ 1q

,

for M

 1.

(1)

In our experiments, we tried setting M P t2, 4, 8u. Naturally, higher values of M reduce the variance, but at the cost of requiring more assessment effort. The total effort required with this approach is A ` S, where S is the number of documents we must assess to find M relevant documents.
The Horvitz-Thompson Estimator. The downside of the negative binomial sampling strategy is that for cases where the prevalence of relevant documents in the residual collection is low, it might require a lot of effort to find M relevant documents. An alternative is to use the BMI to score all documents in the residual collection at the point when it terminates, thus ordering all remaining documents in decreasing probability of relevance.
From here, we can apply a standard sampling technique called the Horvitz-Thompson Estimator (HT estimator) [8]: we compute a distribution over all documents in the residual collection such that its probability of being sampled is

982

proportional to its probability of relevance (as estimated by the BMI). This renormalized distribution is referred to as the inclusion probability, i.e., i refers to the probability that document i will be sampled. The Horvitz-Thompson estimate of the number of relevant documents is:

n

RT " RA ` ÿ i´1Yi

(2)

i"1

where Yi is an indicator variable for relevance in each of the n sampled documents. Note that this does not form a complete algorithm because we are missing a stopping criterion. Once again, we address this issue in our paired experimental methodology below.

Stratified Sampling. We propose a novel stratified sam-
pling strategy, also based on a relevance ranking of the resid-
ual collection at the point when the BMI terminates. This
approach proceeds in iterations: at the i-th iteration, we randomly sample KS documents (" 1000) from the next top ranking K documents (" 10000) and judge those documents. Suppose we find Ri relevant documents: we can then estimate that there are K ¨ pRi{KSq in the top K hits. We then proceed to the next iteration and sample KS doc-
uments from the next K top ranking documents, repeating
as long as we find at least one relevant document. The total
number of relevant documents can then be computed as:

n

RT " RA ` ÿ K ¨ pRi{KSq

(3)

i"1

where n is the number of iterations. The total effort expended is A ` n ¨ KS.

4. EXPERIMENTAL SETUP
To evaluate our volume estimation techniques, we used test collections from the TREC 2015 Total Recall Track [7]. In particular, we used three collections: the (redacted) Jeb Bush Emails (called "Athome1"), consisting of 290k emails from Jeb Bush's eight-year tenure as the governor of Florida (10 topics); the Illicit Goods dataset (called "Athome2") collected for the TREC 2015 Dynamic Domain Track, consisting of 465k documents from a web crawl (10 topics); and the Local Politics dataset (called "Athome3") collected for the TREC 2015 Dynamic Domain Track, consisting of 902k documents from various news sources (10 topics). The relevance assessment process is described in the track overview paper [7], but for the purposes of our study, it suffices to say that the evaluation methodology has been sufficiently validated for assessing the effectiveness of high-recall tasks (and thus these collections are suitable for our volume estimation problem). Finally, as a validation set, we evaluated our techniques on the Twitter collection described in Bommannavar et al. [2], who exhaustively annotated approximately 800k tweets from one day in August 2012 with respect to four topics: Apple (the technology company), Mars (the planet), Obama, and the Olympics. This dataset exactly matches our motivating application: how much "buzz" is there on social media about a particular topic?
Note that random sampling and the HT estimator approaches are not complete estimation algorithms since they lack a stopping criterion. Therefore, they are compared to negative binomial sampling and stratified sampling in a paired setup, where we evaluate how the techniques compare at the same level of effort. This models an A/B test-

Average relative error

0.20

Avg Effort: 23488.36

±1271.44

0.15

Avg Effort: 38094.33 ±1691.9

Avg Effort: 67327.79 ±2193.78

Avg Effort: 12055.9 ±137.35

0.10

0.05

0.00 N.B. (M=2) Random N.B. (M=4) Random N.B. (M=8) Random Stratified HTEstimator Random
Figure 1: Box-and-whiskers plot characterizing 50 trials of each of our techniques on the Athome1 collection.
ing scenario where we have two parallel efforts proceeding at exactly the same pace assessing documents. When one technique terminates, we also stop the other. At that point, we ask: how do the two estimates compare?
Our experimental procedure is as follows: for each topic in a collection, we ran our estimation technique (either negative binomial sampling or stratified sampling) and recorded the total effort. We then ran a paired experiment with either random sampling or the HT estimator (or both) using exactly the same level of effort. We recorded the estimated volume for all techniques. For each collection, we report the average (relative) error across all topics and the root mean square error. The Athome1 collection was used as our training set, on which we ran 50 trials of the above procedure to characterize the variability of estimates. The Athome2, Athome3, and Twitter collections were used as held-out test sets--we report the results of a single trial.
5. RESULTS
The results of 50 trials of our experimental procedure are shown in Figure 1, where the average relative error across the trials is characterized by a standard box-and-whiskers plot. We compared negative binomial sampling, M " t2, 4, 8u, with random sampling using the paired approach described above. We compared stratified sampling with the HT estimator and random sampling using exactly the same procedure. Each of these comparisons is shown by grouped bars (separated by dashed lines) in the figure.
As expected, the negative binomial sampling approach becomes more accurate with increasing values of M (but requires correspondingly more effort). For reference, the entire collection contains 290k documents, so with M " 8, on average we must examine nearly a quarter of the collection. However, we see that negative binomial sampling is more accurate than random sampling at the same level of effort. It is clear that our stratified sampling approach is superior to all other techniques. On average, stratified sampling requires about half as much effort as negative binomial sampling with M " 2 but gives much more accurate estimates. In fact, stratified sampling provides more accurate estimates than negative binomial sampling with M " 8, at about a fifth of the effort. Stratified sampling also beats both the HT estimator and random sampling at the same level of effort.
Results for Athome2 and Athome3 are shown in Table 1.

983

Measure
Neg. Binomial (M " 2) = sample Neg. Binomial (M " 4) = sample Neg. Binomial (M " 8) = sample Stratified = HTEstimator = sample
Neg. Binomial (M " 2) = sample Neg. Binomial (M " 4) = sample Neg. Binomial (M " 8) = sample Stratified = HTEstimator = sample
Neg. Binomial (M " 2) = sample Neg. Binomial (M " 4) = sample Neg. Binomial (M " 8) = sample Stratified = HTEstimator = sample

Avg Avg Relative

Effort

Error

Athome2

80925 80925

0.016 0.094

122527 122527

0.014 0.052

181407 181407

0.015 0.045

8363 8363 8363

0.026 0.051 0.410

Athome3

482237 482237

0.041 0.045

546379 546379

0.011 0.042

597489 597489

0.023 0.032

3168 3168 3168

0.053 0.100 0.867

Twitter

24160 24160

0.261 0.222

39162 39162

0.106 0.046

40295 40295

0.007 0.179

22687 22687 22687

0.047 0.093 0.170

Root Mean Square Error
0.021 0.122 0.023 0.062 0.020 0.060 0.042 0.070 0.621
0.105 0.079 0.030 0.073 0.058 0.064 0.113 0.200 1.119
0.233 0.240 0.090 0.041 0.036 0.181 0.048 0.092 0.218

Table 1: Results of various volume estimation techniques on the Athome2, Athome3, and Twitter collections.

Since these comprise our held-out test data, we only report the results of a single trial. In the table, rows are grouped in terms of different techniques at the same level of effort, e.g., the rows marked "" sample" denote accuracy with the same number of judged documents as the corresponding negative binomial or stratified condition. Due to the variability inherent in our sampling strategies, in our particular trial we observe greater error with M " 8 than with M " 4 using negative binomial sampling. This, however, is not inconsistent with the results in Figure 1.
Overall, the results on Athome2 (465k documents) are consistent with the results from Athome1, our training set. Negative binomial sampling becomes more accurate with increasing M and is more accurate than random sampling with the same level of effort. However, our stratified sampling technique provides comparable error at far less cost, beating both the HT estimator and random sampling.
Results on the Athome3 collection, which contains 902k documents, are quite poor. Table 2 shows why: for each topic in that collection, we list the total number of relevant documents, the effort expended in the active learning portion of our procedure, and the number of relevant documents found at that point. For five of the topics (those in bold), with active learning we've found either all or all but one of the relevant documents, which means that our termination condition for negative binomial sampling (e.g., with M " 2) is never met and hence the procedure forces us to examine the entire collection. In contrast, with stratified sampling we examine 1000 of the top 10000 documents, find zero relevant, and terminate.
The bottom of Table 1 shows our results for the Twitter

Topic
athome3089 athome3133 athome3226 athome3290 athome3357 athome3378 athome3423 athome3431 athome3481 athome3484

Rel Docs
255 113 2094 26 629 66 76 1111 2036 23

Knee Stop Effort
1105 1105 3478 2316 1526 1105 1232 1232 3478 1105

RelAtKnee
254 112 2022 26 599 66 40 1106 1924 23

Table 2: Relevant documents identified and effort when BMI terminates for Athome3.

collection. Once again, we report results from a single trial. Overall, the findings are consistent with the other collections: our stratified sampling technique clearly yields more accurate estimates than all other techniques. This gives us some degree of confidence that our algorithms, developed on email (Athome1), generalize to entirely different collections (tweets). We have no explanation as to why negative binomial sampling with M " 4 gives worse estimates than comparable random sampling, or why comparable random sampling with M " 8 gives such poor results. We purposely decided against error analysis in order to preserve the sanctity of this validation set.

6. CONCLUSION
Estimating the number of relevant documents in a collection presents an interesting twist to the high-recall retrieval problem. Our results show that actually finding the relevant documents is a good approach to counting the total volume. However, we develop and verify the insight that we should first identify the "easy to find" documents and then extrapolate via sampling on the rest. Our approach establishes a baseline for future work on an important real-world application, particularly in the social media space.
Acknowledgments. This work was supported in part by the U.S. National Science Foundation under awards IIS1218043 and CNS-1405688, the Natural Sciences and Engineering Research Council of Canada (NSERC), Google, and the University of Waterloo. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.

7. REFERENCES
[1] M. Bagdouri, W. Webber, D. Lewis, and D. Oard. Towards minimizing the annotation cost of certified text classification. CIKM, 2013.
[2] P. Bommannavar, J. Lin, and A. Rajaraman. Estimating topical volume in social media streams. SAC, 2016.
[3] G. Cormack and M. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv:1504.06868v1, 2015.
[4] G. Cormack and M. Grossman. Engineering quality and reliability in technology-assisted review. SIGIR, 2016.
[5] N. Johnson, A. Kemp, and S. Kotz. Univariate Discrete Distributions, 3rd Edition. Wiley, 2006.
[6] D. Oard and W. Webber. Information retrieval for e-discovery. FnTIR, 7(2­3):99­237, 2013.
[7] A. Roegiest, G. Cormack, M. Grossman, and C. Clarke. TREC 2015 Total Recall track overview. TREC, 2015.
[8] Y. Till´e. Sampling Algorithms. Springer Series in Statistics. Springer, 2006.

984


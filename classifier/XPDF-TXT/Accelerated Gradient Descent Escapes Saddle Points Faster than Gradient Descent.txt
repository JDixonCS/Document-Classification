arXiv:1711.10456v1 [cs.LG] 28 Nov 2017

Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent

Chi Jin University of California, Berkeley
chijin@cs.berkeley.edu

Praneeth Netrapalli Microsoft Research, India praneeth@microsoft.com

Michael I. Jordan University of California, Berkeley
jordan@cs.berkeley.edu

November 29, 2017

Abstract
Nesterov's accelerated gradient descent (AGD), an instance of the general family of "momentum methods," provably achieves faster convergence rate than gradient descent (GD) in the convex setting. However, whether these methods are superior to GD in the nonconvex setting remains open. This paper studies a simple variant of AGD, and shows that it escapes saddle points and finds a second-order stationary point in O~(1/7/4) iterations, faster than the O~(1/2) iterations required by GD. To the best of our knowledge, this is the first Hessian-free algorithm to find a second-order stationary point faster than GD, and also the first single-loop algorithm with a faster rate than GD even in the setting of finding a first-order stationary point. Our analysis is based on two key ideas: (1) the use of a simple Hamiltonian function, inspired by a continuous-time perspective, which AGD monotonically decreases per step even for nonconvex functions, and (2) a novel framework called improve or localize, which is useful for tracking the long-term behavior of gradient-based optimization algorithms. We believe that these techniques may deepen our understanding of both acceleration algorithms and nonconvex optimization.

1

1 Introduction
Nonconvex optimization problems are ubiquitous in modern machine learning. While it is NP-hard to find global minima of a nonconvex function in the worst case, in the setting of machine learning it has proved useful to consider a less stringent notion of success, namely that of convergence to a first-order stationary point (where f (x) = 0). Gradient descent (GD), a simple and fundamental optimization algorithm that has proved its value in large-scale machine learning, is known to find an -first-order stationary point (where f (x)  ) in O(1/2) iterations [Nesterov, 1998], and this rate is sharp [Cartis et al., 2010]. Such results, however, do not seem to address the practical success of gradient descent; first-order stationarity includes local minima, saddle points or even local maxima, and a mere guarantee of convergence to such points seems unsatisfying. Indeed, architectures such as deep neural networks induce optimization surfaces that can be teeming with such highly suboptimal saddle points [Dauphin et al., 2014]. It is important to study to what extent gradient descent avoids such points, particular in the high-dimensional setting in which the directions of escape from saddle points may be few.
This paper focuses on convergence to a second-order stationary point (where f (x) = 0 and 2f (x) 0). Second-order stationarity rules out many common types of saddle points (strict saddle points where min(2f (x)) < 0), allowing only local minima and higher-order saddle points. A significant body of recent work, some theoretical and some empirical, shows that for a large class of well-studied machine learning problems, neither higher-order saddle points nor spurious local minima exist. That is, all second-order stationary points are (approximate) global minima for these problems. Choromanska et al. [2014], Kawaguchi [2016] present such a result for learning multilayer neural networks, Bandeira et al. [2016], Mei et al. [2017] for synchronization and MaxCut, Boumal et al. [2016] for smooth semidefinite programs, Bhojanapalli et al. [2016] for matrix sensing, Ge et al. [2016] for matrix completion, and Ge et al. [2017] for robust PCA. These results strongly motivate the quest for efficient algorithms to find second-order stationary points.
Hessian-based algorithms can explicitly compute curvatures and thereby avoid saddle points (e.g., [Nesterov and Polyak, 2006, Curtis et al., 2014]), but these algorithms are computationally infeasible in the high-dimensional regime. GD, by contrast, is known to get stuck at strict saddle points [Nesterov, 1998, Section 1.2.3]. Recent work has reconciled this conundrum in favor of GD; Jin et al. [2017], building on earlier work of Ge et al. [2015], show that a perturbed version of GD converges to an -relaxed version of a second-order stationary point (see Definition 5) in O~(1/2) iterations. That is, perturbed GD in fact finds second-order stationary points as fast as standard GD finds first-order stationary point, up to logarithmic factors in dimension.
On the other hand, GD is known to be suboptimal in the convex case. In a celebrated paper, Nesterov [1983] showed that anaccelerated version of gradient descent (AGD) finds an -suboptimal point (see Section 2.2) in O(1/ ) steps, while gradient descent takes O(1/) steps. The basic idea of acceleration has been used to design faster algorithms for a range of other convex optimization problems [Beck and Teboulle, 2009, Nesterov, 2012, Lee and Sidford, 2013, Shalev-Shwartz and Zhang, 2014]. We will refer to this general family as "momentum-based methods."
Such results have focused on the convex setting. It is open as to whether momentum-based methods yield faster rates in the nonconvex setting, specifically when we consider the convergence criterion of second-order stationarity. We are thus led to ask the following question: Do momentum-based methods yield faster convergence than GD in the presence of saddle points?
2

Algorithm 1 Nesterov's Accelerated Gradient Descent (x0, , )
1: v0  0 2: for t = 0, 1, . . . , do 3: yt  xt + (1 - )vt 4: xt+1  yt - f (yt) 5: vt+1  xt+1 - xt

Algorithm 2 Perturbed Accelerated Gradient Descent (x0, , , , s, r, T )

1: v0  0 2: for t = 0, 1, . . . , do

3: if f (xt)   and no perturbation in last T steps then

4:

xt  xt + t t  Unif (B0(r))

5: yt  xt + (1 - )vt

6: xt+1  yt - f (yt)

7: vt+1  xt+1 - xt

8:

if f (xt)  f (yt) +

f (yt), xt - yt

-

 2

xt - yt 2 then

9:

(xt+1, vt+1)  Negative-Curvature-Exploitation(xt, vt, s)

Perturbation    AGD
Negative curvature exploitation

This paper answers this question in the affirmative. We present a simple momentum-based algorithm (PAGD for "perturbed AGD") that finds an -second order stationary point in O~(1/7/4) iterations, faster than the O~(1/2) iterations required by GD. The pseudocode of our algorithm is presented in Algorithm 2.1 PAGD adds two algorithmic features to AGD (Algorithm 1):
· Perturbation (Lines 3-4): when the gradient is small, we add a small perturbation sampled uniformly from a d-dimensional ball with radius r. The homogeneous nature of this perturbation mitigates our lack of knowledge of the curvature tensor at or near saddle points.
· Negative Curvature Exploitation (NCE, Lines 8-9; pseudocode in Algorithm 3): when the function becomes "too nonconvex" along yt to xt, we reset the momentum and decide whether to exploit negative curvature depending on the magnitude of the current momentum vt.
We note that both components are straightforward to implement and increase computation by a constant factor. The perturbation idea follows from Ge et al. [2015] and Jin et al. [2017], while NCE is inspired by [Carmon et al., 2017]. To the best of our knowledge, PAGD is the first Hessian-free algorithm to find a second-order stationary point in O~(1/7/4) steps. Note also that PAGD is a "single-loop algorithm," meaning that it does not require an inner loop of optimization of a surrogate function. It is the first single-loop algorithm to achieve a O~(1/7/4) rate even in the setting of finding a first-order stationary point.
1.1 Related Work
In this section, we review related work from the perspective of both nonconvex optimization and momentum/acceleration. For clarity of presentation, when discussing rates, we focus on the dependence on the accuracy  and the dimension d while assuming all other problem parameters are constant. Table 1 presents a comparison of the current work with previous work.
1See Section 3 for values of various parameters.
3

Guarantees Oracle Algorithm

Iterations Simplicity

First-order Stationary Point

Gradient

GD [Nesterov, 1998] AGD [Ghadimi and Lan, 2016] Carmon et al. [2017]

O(1/2) O(1/2) O 1/7/4

Single-loop Single-loop Nested-loop

Second-order Stationary Point

Hessian -vector
Gradient

Carmon et al. [2016] Agarwal et al. [2017]
Noisy GD [Ge et al., 2015] Perturbed GD [Jin et al., 2017] Perturbed AGD [This Work]

O(1/7/4 ) O(1/7/4 )
O(poly(d/)) O(1/2) O(1/7/4 )

Nested-loop Nested-loop
Single-loop Single-loop Single-loop

Table 1: Complexity of finding stationary points. O(·) ignores polylog factors in d and .

Convergence to first-order stationary points: Traditional analyses in this case assume only Lipschitz gradients (see Definition 1). Nesterov [1998] shows that GD finds an -first-order stationary point in O(1/2) steps. Ghadimi and Lan [2016] guarantee that AGD also converges in O 1/2 steps. Under the additional assumption of Lipschitz Hessians (see Definition 4), Carmon et al. [2017] develop a new algorithm that converges in O(1/7/4) steps. Their algorithm is a nested-loop algorithm, where the outer loop adds a proximal term to reduce the nonconvex problem to a convex subproblem. A key novelty in their algorithm is the idea of "negative curvature exploitation," which inspired a similar step in our algorithm. In addition to the qualitative and quantitative differences between Carmon et al. [2017] and the current work, as summarized in Table 1, we note that while Carmon et al. [2017] analyze AGD applied to convex subproblems, we analyze AGD applied directly to nonconvex functions through a novel Hamiltonian framework.
Convergence to second-order stationary points: All results in this setting assume Lipschitz conditions for both the gradient and Hessian. Classical approaches, such as cubic regularization [Nesterov and Polyak, 2006] and trust region algorithms [Curtis et al., 2014], require access to Hessians, and are known to find -second-order stationary points in O(1/1.5) steps. However, the requirement of these algorithms to form the Hessian makes them infeasible for highdimensional problems. A second set of algorithms utilize only Hessian-vector products instead of the explicit Hessian; in many applications such products can be computed efficiently. Rates of O(1/7/4) have been established for such algorithms [Carmon et al., 2016, Agarwal et al., 2017, Royer and Wright, 2017]. Finally, in the realm of purely gradient-based algorithms, Ge et al. [2015] present the first polynomial guarantees for a perturbed version of GD, and Jin et al. [2017] sharpen it to O(1/2). For the special case of quadratic functions, O'Neill and Wright [2017] analyze the behavior of AGD around critical points and show that it escapes saddle points faster than GD. We note that the current work is the first achieving a rate of O(1/7/4) for general nonconvex functions.
Acceleration: There is also a rich literature that aims to understand momentum methods; e.g., Allen-Zhu and Orecchia [2014] view AGD as a linear coupling of GD and mirror descent, Su et al. [2016] and Wibisono et al. [2016] view AGD as a second-order differential equation, and Bubeck et al. [2015] view AGD from a geometric perspective. Most of this work is tailored to the convex setting, and it is unclear and nontrivial to generalize the results to a nonconvex setting.

4

There are also several papers that study AGD with relaxed versions of convexity--see Necoara et al. [2015], Li and Lin [2017] and references therein for overviews of these results.

1.2 Main Techniques

Our results rely on the following three key ideas. To the best of our knowledge, the first two are

novel, while the third one was delineated in Jin et al. [2017].

Hamiltonian: A major challenge in analyzing momentum-based algorithms is that the objec-

tive function does not decrease monotonically as is the case for GD. To overcome this in the convex

setting, several Lyapunov functions have been proposed [Wilson et al., 2016]. However these Lya-

punov functions involve the global minimum x, which cannot be computed by the algorithm, and

is thus of limited value in the nonconvex setting. A key technical contribution of this paper is the

design of a function which is both computable and tracks the progress of AGD. The function takes

the form of a Hamiltonian:

Et

:=

f (xt)

+

1 2

vt 2 ;

(1)

i.e., a sum of potential energy and kinetic energy terms. It is monotonically decreasing in the

continuous-time setting. This is not the case in general in the discrete-time setting, a fact which

requires us to incorporate the NCE step.

Improve or localize: Another key technical contribution of this paper is in formalizing a

simple but powerful framework for analyzing nonconvex optimization algorithms. This framework

requires us to show that for a given algorithm, either the algorithm makes significant progress or

the iterates do not move much. We call this the improve-or-localize phenomenon. For instance,

when progress is measured by function value, it is easy to show that for GD, with proper choice of

learning rate, we have:

1 t-1 2  =0

x +1 - x

2  f (x0) - f (xt).

For AGD, a similar lemma can be shown by replacing the objective function with the Hamiltonian

(see Lemma 4). Once this phenomenon is established, we can conclude that if an algorithm does

not make much progress, it is localized to a small ball, and we can then approximate the objective

function by either a linear or a quadratic function (depending on smoothness assumptions) in

this small local region. Moreover, an upper bound on

t-1  =0

x +1 - x

2 lets us conclude that

iterates do not oscillate much in this local region (oscillation is a unique phenomenon of momentum

algorithms as can be seen even in the convex setting). This gives us better control of approximation

error.

Coupling sequences for escaping saddle points: When an algorithm arrives in the neigh-

borhood of a strict saddle point, where min(2f (x)) < 0, all we know is that there exists a direction of escape (the direction of the minimum eigenvector of 2f (x)); denote it by eesc. To

avoid such points, the algorithm randomly perturbs the current iterate uniformly in a small ball,

and runs AGD starting from this point x~0. As in Jin et al. [2017], we can divide this ball into a

"stuck region," Xstuck, starting from which AGD does not escape the saddle quickly, and its com-

plement from which AGD escapes quickly. In order to show quick escape from a saddle point, we

must show that the volume of Xstuck is very small compared to that of the ball. Though Xstuck may

be without an analytical form, one can control the rate of escape by studying two AGD sequences

that start from two realizations of perturbation, x~0 and x~0, which are separated along eesc by a

5

small distance r0. In this case, at least one of the sequences escapes the saddle point quickly, which proves that the width of Xstuck along eesc can not be greater than r0, and hence Xstuck has small volume.

2 Preliminaries
In this section, we will review some well-known results on GD and AGD in the strongly convex setting, and existing results on convergence of GD to second-order stationary points.

2.1 Notation
Bold upper-case letters (A, B) denote matrices and bold lower-case letters (x, y) denote vectors. For vectors · denotes the 2-norm. For matrices, · denotes the spectral norm and min(·) denotes the minimum eigenvalue. For f : Rd  R, f (·) and 2f (·) denote its gradient and Hessian respectively, and f  denotes its global minimum. We use O(·), (·), (·) to hide absolute constants, and O~(·), ~ (·), ~ (·) to hide absolute constants and polylog factors for all problem parameters.

2.2 Convex Setting
To minimize a function f (·), GD performs the following sequence of steps:

xt+1 = xt - f (xt).

The suboptimality of GD and the improvement achieved by AGD can be clearly illustrated for the case of smooth and strongly convex functions.

Definition 1. A differentiable function f (·) is -smooth (or -gradient Lipschitz) if:

f (x1) - f (x2)   x1 - x2  x1, x2.

The gradient Lipschitz property asserts that the gradient can not change too rapidly in a small local region.

Definition 2. A twice-differentiable function f (·) is -strongly convex if min(2f (x))  ,  x.
Let f  := miny f (y). A point x is said to be -suboptimal if f (x)  f  + . The following theorem gives the convergence rate of GD and AGD for smooth and strongly convex functions.

Theorem 1 (Nesterov [2004]). Assume that the function f (·) is -smooth and -strongly convex. Then, for any  > 0, the iteration complexities to find an -suboptimal point are as follows:

· GD with  = 1/: O((/) · log((f (x0) - f )/)) · AGD (Algorithm 1) with  = 1/ and  = /: O( / · log((f (x0) - f )/)).

The number of iterations of GD depends linearly on the ratio /, which is called the condition

number of f (·) since I 2f (x) I. Clearly    and hence condition number is always at

least one. Denoting the condition the momentum parameter satisfies

nu=mb1e/rbyan,dw(e2)hAigGhlDighimt ptwrooveims puporotnanGtDasbpyecatsfaocftoArGoDf :(1.)

6

Algorithm 3 Negative Curvature Exploitation(xt, vt, s)

1: if vt  s then

2: xt+1  xt;

3: else

4:  = s · vt/ vt

5:

xt+1  argminx{xt+,xt-} f (x)

6: return (xt+1, 0)

2.3 Nonconvex Setting
For nonconvex functions finding global minima is NP-hard in the worst case. The best one can hope for in this setting is convergence to stationary points. There are various levels of stationarity.
Definition 3. x is an -first-order stationary point of function f (·) if f (x)  .
As mentioned in Section 1, for most nonconvex problems encountered in practice, a majority of firstorder stationary points turn out to be saddle points. Second-order stationary points require not only zero gradient, but also positive semidefinite Hessian, ruling out most saddle points. Second-order stationary points are meaningful, however, only when the Hessian is continuous.
Definition 4. A twice-differentiable function f (·) is -Hessian Lipschitz if:
2f (x1) - 2f (x2)   x1 - x2  x1, x2.

Definition 5 (Nesterov and Polyak [2006]). For a -Hessian Lipschitz function f (·), x is an second-order stationary point if:
f (x)   and min(2f (x))  -.

The following theorem gives the convergence rate of a perturbed version of GD to second-order stationary points. See Jin et al. [2017] for a detailed description of the algorithm.

Theorem 2 ([Jin et al., 2017]). Assume that the function f (·) is -smooth and -Hessian Lipschitz. Then, for any  > 0, perturbed GD outputs an -second-order stationary point w.h.p in iterations:

O

(f (x0) - f ) 2

.

Note that this rate is essentially the same as that of GD for convergence to first-order stationary points. In particular, it only has polylogarithmic dependence on the dimension.

3 Main Result
In this section, we present our algorithm and main result. As mentioned in Section 1, the algorithm we propose is essentially AGD with two key differences (see Algorithm 2): perturbation and negative curvature exploitation (NCE). A perturbation is added when the gradient is small (to escape saddle points), and no more frequently than once in T steps. The perturbation t is sampled uniformly
7

from a d-dimensional ball with radius r. The specific choices of gap and uniform distribution are for technical convenience (they are sufficient for our theoretical result but not necessary).
NCE (Algorithm 3) is explicitly designed to guarantee decrease of the Hamiltonian (1). When it is triggered, i.e., when

f (xt)  f (yt) + f (yt), xt - yt

-

 2

xt - yt 2

(2)

the function has a large negative curvature between the current iterates xt and yt. In this case, if the momentum vt is small, then yt and xt are close, so the large negative curvature also carries over to the Hessian at xt due to the Lipschitz property. Assaying two points along ±(yt - xt) around xt gives one point that is negatively aligned with f (xt) and yields a decreasing function value and Hamiltonian. If the momentum vt is large, negative curvature can no longer be exploited, but fortunately resetting the momentum to zero kills the second term in (1), significantly decreasing
the Hamiltonian.

Setting of hyperparameters: Let  be the target accuracy for a second-order stationary point,

let  and  factor to be

bspeegcrifiaeddienlatt/eHr.esLseiatn-L:=ipsc/hitz,paarnadmseetters,

and

let

c, 

be

absolute

constant

and

log



=

1 4

,



=

1 4

,



=

2 

,

s

=

 4

,

 T =  · c,

r =  · -5c-8.

(3)

The following theorem is the main result of this paper.

Theorem 3. Assume that the function f (·) is -smooth and -Hessian Lipschitz. There exists an

absolute

constant

cmax

such

that

for

any



>

0,





2 

,

f



f (x0) - f ,

if



=

max{1, log

df 

},

c  cmax and such that if we run PAGD (Algorithm 2) with choice of parameters according to (3),

then with probability at least 1 - , one of the iterates xt will be an -second order stationary point

in the following number of iterations:

O

1/21/4(f (x0) 7/4

-

f

)

log6

df 

Theorem 3 says that when PAGD is run for the designated number of steps (which is poly-

logarithmic in dimension), at least one of the iterates is an -second-order stationary point. We

sfotactuisonoanrythpeocinatse(omf isnm(al2lf(x(i).)e., -2/)i)s

so that the nontrivial.

Hessian requirement for Note that 2f (x)  

itmhpe lie-sseco=nd-/order,

which can be viewed as a condition 3 with Theorem 2, PAGD, with a

number, akin to that in momentum parameter 

c=onve(x1/setti)n,ga. chCioemvepsar~in(g T)hbeoertetemr

iteration complexity compared to PGD.

Output -second order stationary point: Although Theorem 3 only guarantees that one of the iterates is an -second order stationary point, it is straightforward to identify one of them by adding a proper termination condition: once the gradient is small and satisfies the pre-condition to add a perturbation, we can keep track of the point xt0 prior to adding perturbation, and compare the Hamiltonian at t0 with the one T steps after. If the Hamiltonian decreases by E = ~ ( 3/), then the algorithm has made progress, otherwise xt0 is an -second-order stationary point according to Lemma 8. Doing so will add a hyperparameter (threshold E ) but does not increase complexity.

8

4 Overview of Analysis
In this section, we will present an overview of the proof of Theorem 3. Section 4.1 presents the Hamiltonian for AGD and its key property of monotonic decrease. This leads to Section 4.2 where the improve-or-localize lemma is stated, as well as the main intuition behind acceleration. Section 4.3 demonstrates how to apply these tools to prove Theorem 3. Complete details can be found in the appendix.

4.1 Hamiltonian

While GD guarantees decrease of function value in every step (even for nonconvex problems), the

biggest stumbling block to analyzing AGD is that it is less clear how to keep track of "progress."

Known Lyapunov functions for AGD [Wilson et al., 2016] are restricted to the convex setting and

furthermore are not computable by the algorithm (as they depend on x).

systTemo sdepeepresnpetchteiveu,nwdehresrteanwdeinfigxotfhAe GraDtioin~a=no/nconvteoxbseetaticnogn,swtaentin, swphecilte

it from letting

a 

dynamical  0. This

leads to an ODE which is the continuous limit of AGD [Su et al., 2016]:

x¨ + ~x + f (x) = 0,

(4)

where x¨ and x are derivatives with respect to time t. This equation is a second-order dynamical equation with dissipative forces -~x . Integrating both sides, we obtain:

f (x(t2))

+

1 2

x (t2)2

=

f (x(t1))

+

1 2

x (t1

)2

-

~

t2
x (t)2dt.
t1

(5)

Using physical language, f (x) is a potential energy while x 2/2 is a kinetic energy, and the sum

is a Hamiltonian. The integral shows that the Hamiltonian decreases monotonically with time t,

and the decrease is given

by the dissipation

term ~

t2 t1

x (t)2dt.

Note that (5) holds regardless

of

the convexity of f (·). This monotonic decrease of the Hamiltonian can in fact be extended to the

discretized version of AGD when the function is convex, or mildly nonconvex:

Lemma 4 (Hamiltonian decreases monotonically). Assume that the function f (·) is -smooth, the

learning

rate





1 2

,

and





[2,

1 2

]

in

AGD (Algorithm 1).

Then, for every iteration t where (2)

does not hold, we have:

f (xt+1)

+

1 2

vt+1

2



f (xt)

+

1 2

vt

2

-

 2

vt

2

-

 4

f (yt) 2 .

(6)

Denote

the

discrete

Hamiltonian

as

Et

:=

f (xt)+

1 2

vt 2, and note that in AGD, vt = xt-xt-1.

Lemma 4 tolerates nonconvexity with curvature at most  = (/). Unfortunately, when the

function becomes too nonconvex in certain regions (so that (2) holds), the analogy between the

continuous and discretized versions breaks and (6) no longer holds. In fact, standard AGD can even

increase the Hamiltonian in this regime (see Appendix A.1 for more details). This motivates us to

modify the algorithm by adding the NCE step, which addresses this issue. We have the following

result:

9

Lemma 5. Assume that f (·) is -smooth and -Hessian Lipschitz. For every iteration t of Algorithm 2 where (2) holds (thus running NCE), we have:

Et+1



Et

-

min{

s2 2

,

1 2

(

-

2s)s2}.

Lemmas 4 and 5 jointly assert that the Hamiltonian decreases monotonically in all situations, and are the main tools in the proof of Theorem 3. They not only give us a way of tracking progress, but also quantitatively measure the amount of progress.

4.2 Improve or Localize
One significant challenge in the analysis of gradient-based algorithms for nonconvex optimation is that many phenomena--for instance the accumulation of momentum and the escape from saddle points via perturbation--are multiple-step behaviors; they do not happen in each step. We address this issue by developing a general technique for analyzing the long-term behavior of such algorithms.
In our case, to track the long-term behavior of AGD, one key observation from Lemma 4 is that the amount of progress actually relates to movement of the iterates, which leads to the following improve-or-localize lemma:

Corollary 6 (Improve or localize). Under the same setting as in Lemma 4, if (2) does not hold for all steps in [t, t + T ], we have:

t+T  =t+1

x - x -1

2



2 

(Et

- Et+T ).

Corollary 6 says that the algorithm either makes progress in terms of the Hamiltonian, or the

iterates do not move much. In the second case, Corollary 6 allows us to approximate the dynamics of {x }t+=Tt with a quadratic approximation of f (·).
The acceleration phenomenon is rooted in and can be seen clearly for a quadratic, where the

function can be decomposed into eigen-directions. Consider an eigen-direction with eigenvalue ,

and

linear

term

g

(i.e.,

in

this

direction

f (x)

=

 2

x2

+

gx).

The GD update becomes x+1 =

(1 - )x - g, with µGD() := 1 -  determining the rate of GD. The update of AGD is

(x+1, x ) = (x , x-1)A - (g, 0) with matrix A defined as follows:

A :=

(2 - )(1 - ) 1

-(1 - )(1 - ) 0

.

The rate of AGD is determined by largest eigenvalue of matrix A, which is denoted by µAGD(). Recall the choice of parameter (3), and divide the eigen-directions into the following three categories.
· Strongly convex directions   [, ]: the slowest case is  = , where µGD() = 1 - (1/) while µAGD() = 1 - (1/ ), which results in AGD converging faster than GD.
· Flat directions   [-, ]: the representative case is  = 0 where AGD update becomes x+1 - x = (1 - )(x - x-1) - g. For   1/, we have |xt+ - xt| = ( ) for GD while |xt+ - xt| = ( 2) for AGD, which results in AGD moving along negative gradient directions faster than GD.

10

·

Strongly nonconvex slowest rate is for  =

d-irectiwohnesreµGD[-(,)-=1+]:

similar (1/)

to the strongly convex case, the while µAGD() = 1 + (1/ ),

which results in AGD escaping saddle point faster than GD.

Finally, the approximation error (from a quadratic) is also under control in this framework.

With appropriate choice of T and threshold for Et - Et+T in Corollary 6, by the Cauchy-Swartz

inequality we can restrict where both the gradient

iterates {x }t+=Tt and Hessian of

to all lie f (·) and

within a local its quadratic

ball around xt with radius /, approximation f~t(x) = f (xt) +

f (xt), x - xt

+

1 2

(x

-

xt)2f (xt)(x

-

xt)

are

close:

Fact. Assume f (·) is f (x) - f~t(x)  

-Hessian Lipschitz, then and 2f (x) - 2f~t(x) =

for all x 2f (x)

so that x - 2f (xt)

- 

xt.

/, we have

4.3 Main Framework

For simplicity of presentation, recall T

:=

 

·

c

=

~ ()

and

denote

E

:=

3/ · -5c-7 =

~ ( 3/), where c is sufficiently large constant as in Theorem 3. Our overall proof strategy will

be to show the following "average descent claim": Algorithm 2 decreases the Hamiltonian by E in

every set of T iterations as long as it does not reach an -second-order stationary point. Since the

Hamiltonian cannot decrease more than E0 - E = f (x0) - f , this immediately shows that it has to reach an -second-order stationary point in O((f (x0) - f )T /E ) steps, proving Theorem 3.

It can be verified by the choice of parameters (3) and Lemma 4 that whenever (2) holds so that

NCE is triggered, the Hamiltonian decreases by at least E in one step. So, if NCE step is performed

even once in each round of T steps, we achieve enough average decrease. The troublesome case

is when in some time interval of T steps starting with xt, only AGD steps are performed without

NCE. If xt is not an -second order stationary point, either the gradient is large or the Hessian has

a large negative direction. We prove the average decrease claim by considering these two cases.

Lemma 7 (Large gradient). Consider the setting of Theorem 3. If f (x )   for all   [t, t + T ], then by running Algorithm 2 we have Et+T - Et  -E .

-Lemm, aan8d(pNeergtuartibvaeticounrvhaatsunreo)t.

Consider the setting of Theorem been added in iterations   [t -

3. T

If f (xt) , t), then by

 , min(2f (xt)) running Algorithm

<

2, we have Et+T - Et  -E with high probability.

(EW) einnTote=th~a(tan)

important aspect of steps, which is faster

these two compared

lemmas to PGD

is that the Hamiltonian decreases which decreases the function value

by by

(E ) in T 2 = ~ () steps [Jin et al., 2017]. That is, the acceleration phenomenon in PAGD happens

in both cases. We also stress that under both of these settings, PAGD cannot achieve (E /T )

decrease in each step--it has to accumulate momentum over time to achieve (E /T ) amortized

decrease.

4.3.1 Large Gradient Scenario
For AGD, gradient and momentum interact, and both play important roles in the dynamics. Fortunately, according to Lemma 4, the Hamiltonian decreases sufficiently whenever the momentum vt is large; so it is sufficient to discuss the case where the momentum is small.

11

One difficulty in proving Lemma 7 lies in the difficulty of enforcing the precondition that

gradients of all iterates are large even with quadratic approximation. Intuitively we hope that

the large initial gradient f (xt)   suffices to give a sufficient decrease of the Hamiltonian.

iUnn[fortu,n]a,tceolyn,stishtiisngis

not true. Let S be the subspace of of all the strongly convex directions,

eigenvectors of 2f (xt) with eigenvalues and let Sc be the orthogonal subspace. It

turns out that the initial gradient component in S is not very helpful in decreasing the Hamiltonian

since AGD rapidly decreases the gradient in these directions. We instead prove Lemma 7 in two

steps.

Lemma 9. (informal) If vt is small, f (xt) not too large and Et+T /2 - Et  -E , then for all   [t + T /4, t + T /2] we have PS f (x )  /2.

Lemma 10. (informal) If vt is small and PScf (xt)  /2, then we have Et+T /4 - Et  -E .
See the formal versions, Lemma 16 and Lemma 17, for more details. We see that if the Hamiltonian does not decrease much (and so is localized in a small ball), the gradient in the strongly convex subspace PS f (x ) vanishes in T /4 steps by Lemma 9. Since the hypothesis of Lemma 7 guarantees a large gradient for all of the T steps, this means that PScf (xt) is large after T /4 steps, thereby decreasing the Hamiltonian in the next T /4 steps (by Lemma 10).

4.3.2 Negative Curvature Scenario

In this section, we will show that the volume of the set around a strict saddle point from which

AGD does not escape quickly is very small (Lemma 8). We do this using the coupling mechanism

introduced in Jin et al. [2017], which gives a fine-grained understanding of the geometry around saddle points. More concretely, letting the perturbation radius r = ~ (/) as specified in (3), we

show the following lemma.

Lemma 11. (informal) Suppose f (x~)   and min(2f (x~))  -. Let x0, x0 be at distance

at r0

most r from x~,  r/ d. Then

and x0 - x0 = r0e1 where for AGD starting at (x0, v)

e1 is the minimum eigen-direction and (x0, v), we have:

of

2f (x~)

and

min{ET - E, ET - E}  -E ,

where E, ET and ET are the Hamiltonians at (x~, v), (xT , vT ) and (xT , vT ) respectively.
See the formal version in Lemma 18. We note  in above Lemma is a small number characterize the failure probability of the algorithm (as defined in Theorem 3), and T has logarithmic dependence on  according to (3). Lemma 11 says that around any strict saddle, for any two points that are separated along the smallest eigen-direction by at least r/ d, PAGD, starting from at least one of those points, decreases the Hamiltonian, and hence escapes the strict saddle. This implies that the width of the region starting from where AGD is stuck has width at most r/ d, and thus has small volume.

5 Conclusions
In this paper, we show that a variant of AGD can escape saddle points faster than GD, demonstrating that momentum techniques can indeed accelerate convergence even for nonconvex optimization. Our algorithm finds an -second order stationary point in O 1/7/4 iterations, faster
12

than the O 1/2 iterations taken by GD. This is the first algorithm that is both Hessian-free and single-loop that achieves this rate. Our analysis relies on novel techniques that lead to a better understanding of momentum techniques as well as nonconvex optimization.
The results here also give rise to several questions. The first concerns lower bounds; is the rate of O 1/7/4 that we have established here optimal for gradient-based methods under the setting of gradient and Hessian-Lipschitz? We believe this upper bound is very likely sharp up to log factors, and developing a tight algorithm-independent lower bound will be necessary to settle this question. The second is whether the negative-curvature-exploitation component of our algorithm is actually necessary for the fast rate. To attempt to answer this question, we may either explore other ways to track the progress of standard AGD (other than the particular Hamiltonian that we have presented here), or consider other discretizations of the ODE (4) so that the property (5) is preserved even for the most nonconvex region. A final direction for future research is the extension of our results to the finite-sum setting and the stochastic setting.
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1195­1199. ACM, 2017.
Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537, 2014.
Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Conference on Learning Theory, pages 361­382, 2016.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183­202, 2009.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873­3881, 2016.
Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757­2765, 2016.
S´ebastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to Nesterov's accelerated gradient descent. arXiv preprint arXiv:1506.08187, 2015.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex optimization. arXiv preprint arXiv:1611.00756, 2016.
Yair Carmon, Oliver Hinder, John C Duchi, and Aaron Sidford. Convex until Proven Guilty: Dimension-free acceleration of gradient descent on non-convex functions. arXiv preprint arXiv:1705.02766, 2017.
13

Coralia Cartis, Nicholas Gould, and Ph L Toint. On the complexity of steepest descent, Newton's and regularized Newton's methods for nonconvex unconstrained optimization problems. Siam journal on optimization, 20(6):2833­2852, 2010.
Anna Choromanska, Mikael Henaff, Michael Mathieu, G´erard Ben Arous, and Yann LeCun. The loss surface of multilayer networks. arXiv:1412.0233, 2014.
Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a worst-case iteration complexity of o(-3/2) for nonconvex optimization. Mathematical Programming, pages 1­32, 2014.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems, pages 2933­2941, 2014.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. In Conference on Computational Learning Theory (COLT), 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973­2981, 2016.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59­99, 2016.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International Conference on Machine Learning (ICML), 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing Systems, pages 586­594, 2016.
Yin Tat Lee and Aaron Sidford. Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems. In Foundations of Computer Science (FOCS), pages 147­156. IEEE, 2013.
Huan Li and Zhouchen Lin. Provable accelerated gradient method for nonconvex low rank optimization. arXiv preprint arXiv:1702.04959, 2017.
Song Mei, Theodor Misiakiewicz, Andrea Montanari, and Roberto I Oliveira. Solving SDPs for synchronization and maxcut problems via the Grothendieck inequality. In Conference on Learning Theory (COLT), pages 1476­1515, 2017.
Ion Necoara, Yurii Nesterov, and Francois Glineur. Linear convergence of first order methods for non-strongly convex optimization. arXiv preprint arXiv:1504.06298, 2015.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). Soviet Mathematics Doklady, 27:372­376, 1983.
14

Yurii Nesterov. Introductory Lectures on Convex Programming Volume I: Basic course. Springer, 1998.
Yurii Nesterov. Introductory Lectures on Convex Optimization, volume 87. Springer Science & Business Media, 2004.
Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341­362, 2012.
Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance. Mathematical Programming, 108(1):177­205, 2006.
Michael O'Neill and Stephen J Wright. Behavior of accelerated gradient methods near critical points of nonconvex problems. arXiv preprint arXiv:1706.07993, 2017.
Cl´ement W Royer and Stephen J Wright. Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131, 2017.
Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In International Conference on Machine Learning (ICML), pages 64­72, 2014.
Weijie Su, Stephen Boyd, and Emmanuel J Candes. A differential equation for modeling Nesterov's accelerated gradient method: theory and insights. Journal of Machine Learning Research, 17 (153):1­43, 2016.
A. Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 133:E7351­E7358, 2016.
Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A Lyapunov analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635, 2016.
15

A Proof of Hamiltonian Lemmas

In this section, we prove Lemma 4, Lemma 5 and Corollary 6, which are presented in Section

4.1 and Section 4.2. In section A.1 we also give an example where standard AGD with negative

curvature exploitation can increase the Hamiltonian.

Recall

that

we

define

the

Hamiltonian

as

Et

:=

f (xt)

+

1 2

vt 2, where, for AGD, we define

vt = xt - xt-1. The first lemma shows that this Hamiltonian decreases in every step of AGD for

mildly nonconvex functions.

Lemma 4 (Hamiltonian decreases monotonically). Assume that the function f (·) is -smooth and

set

the

learning

rate

to

be





1 2

,





[2,

1 2

]

in

AGD (Algorithm 1). Then, for every iteration t

where (2) does not hold, we have:

Et+1



Et

-

 2

vt

2

-

 4

f (yt) 2 .

Proof. Recall that the update equation of accelerated gradient descent has following form:

xt+1  yt - f (yt) yt+1  xt+1 + (1 - )(xt+1 - xt).

By

smoothness,

with





1 2

:

f (xt+1)  f (yt) - 

f (yt)

2+

2 2

f (yt)

2



f (yt) -

3 4

f (yt) 2 ,

(7)

assuming that the precondition (2) does not hold:

f (xt)  f (yt) +

f (yt), xt - yt

-

 2

yt - xt 2 ,

(8)

and given the following update equation:

xt+1 - xt 2 = yt - xt - f (yt) 2

= (1 - )2 xt - xt-1 2 - 2 f (yt), yt - xt + 2 f (yt) 2 ,

(9)

we have:

f (xt+1)

+

1 2

xt+1 - xt 2 f (xt) + f (yt), yt - xt

-

3 4

f (yt) 2

+

1

+  2

(1

-

)2

xt - xt-1 2 -

f (yt), yt - xt

+

 2

f (yt) 2

f

(xt)

+

1 2

xt - xt-1

2

-

2

-

2

- (1 2

-

)2

vt

2

-

 4

f (yt) 2

f

(xt)

+

1 2

xt - xt-1

2

-

 2

vt

2

-

 4

f (yt) 2 .

The

last

inequality

uses

the

fact

that



 [2,

1 2

]

so

that

2



 2

and





 2

.

We

substitute

in

the

definition of vt and Et to finish the proof.

16

We see from this proof that (8) relies on approximate convexity of f (·), which explains why in all existing proofs, the convexity between xt and yt is so important. A perhaps surprising fact to note is that the above proof can in fact go through even with mild nonconvexity (captured in line 8 of Algorithm 2). Thus, high nonconvexity is the problematic situation. To overcome this, we need to slightly modify AGD so that the Hamiltonian is decreasing. This is formalized in the following lemma.

Lemma 5. Assume that f (·) is -smooth and -Hessian Lipschitz. For every iteration t of Algorithm 2 where (2) holds (thus running NCE), we have:

Et+1



Et

-

min{

s2 2

,

1 2

(

-

2s)s2}.

Proof. When we perform an NCE step, we know that (2) holds. In the first case ( vt  s), we set xt+1 = xt and set the momentum vt+1 to zero, which gives:

Et+1

=

f

(xt+1)

=

f (xt)

=

Et

-

1 2

vt

2



Et

-

s2 2

.

In the second case ( vt  s), expanding in a Taylor series with Lagrange remainder, we have:

f (xt) = f (yt) +

f (yt), xt - yt

+

1 2

(xt

-

yt)2f (t)(xt

-

yt),

where t = xt + (1 - )yt and   [0, 1]. Due to the certificate (2) we have

1 2

(xt

-

yt)2f

(t)(xt

-

yt)



-

 2

xt - yt 2 .

On the other hand, clearly min{ f (xt),  , f (xt), - }  0. WLOG, suppose f (xt),   0, then, by definition of xt+1, we have:

f (xt+1)  f (xt + ) = f (xt) +

f (xt), 

+

1 2

2f

(t)



f (xt)

+

1 2

2f

(t),

where t = xt +  and   [0, 1]. Since t - t  2s,  also lines up with yt - xt:

2f (t)  2f (t) + 2f (t) - 2f (t)  2  -  2 + 2s  2 .

Therefore, this gives

Et+1

=

f

(xt+1)



f (xt)

-

1 2

(

-

s)s2



Et

-

1 2

(

-

2s)s2,

which finishes the proof.

The Hamiltonian decrease has an important consequence: if the Hamiltonian does not decrease much, then all the iterates are localized in a small ball around the starting point. Moreover, the iterates do not oscillate much in this ball. We called this the improve-or-localize phenomenon.

Corollary 6 (Improve or localize). Under the same setting as in Lemma 4, if (2) does not hold for all steps in [t, t + T ], we have:

t+T  =t+1

x - x -1

2



2 

(Et

- Et+T ).

Proof. The proof follows immediately from telescoping the argument of Lemma 4.

17

A.1 AGD can increase the Hamiltonian under nonconvexity

In the previous section, we proved Lemma 4 which requires   2, that is,   /(2). In this section, we show Lemma 4 is almost tight in the sense that when   4/ in (2), we have:

f (xt)  f (yt) +

f (yt), xt - yt

-

 2

xt - yt 2 .

Monotonic decrease of the Hamiltonian may no longer hold, indeed, AGD can increase the Hamil-

tonian for those steps.

Consider

a

simple

one-dimensional

example,

f (x)

=

-

1 2



x2,

where

(2)

always

holds.

Define

the

initial condition x0 = -1, v0 = 1/(1 - ). By update equation in Algorithm 1, the next iterate will

be x1 = y0 = 0, and v1 = x1 - x0 = 1. By the definition of Hamiltonian, we have

E0

=f (x0)

+

1 2

|v0|2

=

-

 2

+

1 2(1 -

)2

E1

=f (x1)

+

1 2

|v1|2

=

1 2

,

since   1/4. It is not hard to verify that whenever   4/, we will have E1  E0; that is, the Hamiltonian increases in this step.
This fact implies that when we pick a large learning rate  and small momentum parameter  (both are essential for acceleration), standard AGD does not decrease the Hamiltonian in a very nonconvex region. We need another mechanism such as NCE to fix the monotonically decreasing property.

B Proof of Main Result

In this section, we set up the machinery needed to prove our main result, Theorem 3. We first present the generic setup, then, as in Section 4.3, we split the proof into two cases, one where gradient is large and the other where the Hessian has negative curvature. In the end, we put everything together and prove Theorem 3.
To simplify the proof, we introduce some notation for this section, and state a convention regarding absolute constants. Recall the choice of parameters in Eq.(3):



=

1 4

,



=

1 4

,



=

2 

=

 4

,

s

=

 4

=

1 16

 

,

r =  · -5c-8,

where



=

 , 

=

max{1, log

df 

},

and

c

is

a

sufficiently

large

constant

as

stated

in

the

precon-

dition of Theorem 3. Throughout this section, we also always denote

 T :=  · c,

E :=

3 

· -5c-7,

S :=

2T E 

=

2 

·

-2c-3,

M

:=

 

c-1,

which represent the special units for time, the Hamiltonian, the parameter space and the momentum. All the lemmas in this section hold when the constant c is picked to be sufficiently large. To avoid ambiguity, throughout this section O(·), (·), (·) notation only hides an absolute constant which is independent of the choice of sufficiently large constant c, which is

18

defined in the precondition of Theorem 3. That is, we will always make c dependence explicit in O(·), (·), (·) notation. Therefore, for a quantity like O(c-1), we can always pick c large enough so that it cancels out the absolute constant in the O(·) notation, and make O(c-1) smaller than
any fixed required constant.

B.1 Common setup
Our general strategy in the proof is to show that if none of the iterates xt is a SOSP, then in all T steps, the Hamiltonian always decreases by at least E . This gives an average decrease of E /T . In this section, we establish some facts which will be used throughout the entire proof, including the decrease of the Hamiltonian in NCE step, the update of AGD in matrix form, and upper bounds on approximation error for a local quadratic approximation.
The first lemma shows if negative curvature exploitation is used, then in a single step, the Hamiltonian will decrease by E .

Lemma 12. Under the same setting as Theorem 3, for every iteration t of Algorithm 2 where (2) holds (thus running NCE), we have:

Et+1 - Et  -2E .

Proof. It is also easy to check that the precondition of Lemma 5 holds, and by the particular choice of parameters in Theorem 3, we have:

min{

s2 2

,

1 2

(

-

2s)s2}



(E

c7)



2E

,

where the last inequality is by picking c in Theorem 3 large enough, which finishes the proof.

Therefore, whenever NCE is called, the decrease of the Hamiltonian is already sufficient. We thus only need to focus on AGD steps. The next lemma derives a general expression for xt after an AGD update, which is very useful in multiple-step analysis. The general form is expressed with respect to a reference point 0, which can be any arbitrary point (in many cases we choose it to be x0).
Lemma 13. Let 0 be an origin (which can be fixed at an arbitrary point). Let H = 2f (0). Then an AGD (Algorithm 1) update can be written as:

xt+1 xt

= At

x1 x0

t
-  At-
 =1

f (0) +  0

,

(10)

where  = f (y ) - f (0) - Hy , and

A=

(2 - )(I - H) I

-(1 - )(I - H) 0

.

Proof. Substituting for (yt, vt) in Algorithm 1, we have a recursive equation for xt:

xt+1 = (2 - )xt - (1 - )xt-1 - f ((2 - )xt - (1 - )xt-1).

(11)

19

By definition of  , we also have:

f (y ) = f (0) + Hy +  .

Therefore, in matrix form, we have:

xt+1 xt

=

(2 - )(I - H) I

-(1 - )(I - H) 0

=At

x1 x0

t
-  At-
 =1

f (0) +  0

,

which finishes the proof.

xt xt-1

-

f (0) + t 0

Clearly A in Lemma 13 is a 2d × 2d matrix, and if we expand A according to the eigenvector

directions of

H 0

0 H

, A can be reorganized as a block-diagonal matrix consisting of d 2 × 2

matrices. Let the jth eigenvalue of H be denoted j, and denote Aj as the jth 2 × 2 matrix with

corresponding eigendirections:

Aj =

(2 - )(1 - j) 1

-(1 - )(1 - j) 0

.

(12)

We note that the choice of reference point 0 is mainly to simplify mathmatical expressions involving
xt - 0. Lemma 13 can be viewed as update from a quadratic expansion around origin 0, and  is
the approximation error which marks the difference between true function and its quadratic ap-
proximation. The next lemma shows that when sequence x0, · · · , xt are all close to 0, then the approximation error is under control:

Proposition 14. Using the notation of Lemma 13, if for any   t, we have x  R, then for any   t, we also have

1.   O(R2);

2.  - -1  O(R)( xt - x-1 + x-1 - x-2 );

3.

t  =1

 - -1 2  O(2R2)

t  =1

x - x-1 2.

Proof. Let  = 01(2f (y ) - H)d. The first inequality is true because  =  y , thus:

 =  y   y =

1
(2f (y ) - H)d y

0

1



(2f (y ) - H) d · y   y 2   (2 - )x - (1 - )x-1 2  O(R2).

0

For the second inequality, we have:

 - -1 = f (y ) - f (y-1) - H(y - y-1) =  (y - y-1),

20

where  = 01(2f (y-1 + (y - y-1)) - H)d. As in the proof of the first inequality, we have:

 -  -1

 

y - y-1 =

1
(2f (y-1 + (y - y-1)) - H)d y - y-1
0

 max{ y , y-1 } y - y-1  O(R)( x - x-1 + x-1 - x-2 ).

Finally, since ( x - x-1 + x-1 - x-2 )2  2( x - x-1 2 + x-1 - x-2 2), the third inequality is immediately implied by the second inequality.

B.2 Proof for large-gradient scenario

We prove Lemma 7 in this subsection. Throughout this subsection, we let S be the subspace with

eigenvalues in (2/[(2 - )2], ], and let Sc be the be the corresponding projections. We note 2/[(2

-com)2p]le=men(tary)s,uabnsdpatcheis.

Also let PS and PSc particular choice lies

at the boundary between the real eigenvalues and complex eigenvalues of the matrix Aj, as shown

in Lemma 21.

The first lemma shows that if momentum or gradient is very large, then the Hamiltonian already

has sufficient decrease on average.

Lemma 15. Under the setting of Theorem 3, if vt  M or f (xt)  2M , and at time step t only AGD is used without NCE or perturbation, then:

Et+1 - Et  -4E /T .

Proof. When

vt



 10

,

by

Lemma

4,

we

have:

Et+1

-

Et



-

 2

vt 2  -

 

2 2

c-2

= -

2 2

c-2



E -(

c6)



- 4E

.

T

T

The last step is by picking c to be a large enough constant. When vt  M but f (xt)  2M , by the gradient Lipschitz assumption, we have:

f (yt)  f (xt) - (1 - ) vt  M .

Similarly, by Lemma 4, we have:

Et+1

-

Et



-

 4

f (yt)

2

 -( 2 c-2)  -( E



T

c6)  - 4E . T

Again the last step is by picking c to be a large enough constant, which finishes the proof.

Next, we show that if the initial momentum is small, but the initial gradient on the nonconvex subspace Sc is large enough, then within O(T ) steps, the Hamiltonian will decrease by at least E .

Lemma v0 

16 M,

(Formal Version of v0[PS2f (x0)PS

]Lve0mma210).MUn2,dearntdhefosrettting

of Theorem 3, [0, T /4] only

if PScf (x0) AGD steps are



 2

,

used

without NCE or perturbation, then:

ET /4 - E0  -E .

21

Proof. The high-level plan is a proof by contradiction. We first assume that the energy doesn't
decrease very much; that is, ET /4 - E0  -E for a small enough constant µ. By Corollary 6 and the Cauchy-Swartz inequality, this immediately implies that for all t  T , we have xt - x0 
2T E /(4) = S /2. In the rest of the proof we will show that this leads to a contradiction.
Given initial x0 and v0, we define x-1 = x0 - v0. Without loss of generality, set x0 as the origin 0. Using the notation and results of Lemma 13, we have the following update equation:

xt xt-1

=At

0 -v0

t-1
-  At-1-
 =0

f (0) +  0

.

Consider the j-th eigen-direction of H = 2f (0), recall the definition of the 2 × 2 block matrix Aj as in (12), and denote
(a(tj), - b(tj)) = 1 0 Atj.
Then we have for the j-th eigen-direction:

t-1

x(tj) =bt(j)v0(j) - 

a(t-j)1- (f (0)(j) + (j))

 =0

t-1

=-

a(j)

t-1

f (0)(j) +

p(j)(j) + qt(j)v0(j) ,

 =0

 =0

where

p(j) =

a(t-j)1-

t-1  =0

a(j

)

and

qt(j)

=

- 

b(tj)
t-1  =0

a(j)

.

Clearly

t-1  =0

p(j)

=

1.

For

j



Sc,

by

Lemma

25,

we

know

t-1  =0

a(j

)



(

1 2

).

We

can

thus

further

write the above equation as:

t-1

x(tj) = -

a(j)

 =0

f (0)(j) + ~(j) + v~(j) ,

where ~(j) the initial

=

t-1  =0

p(j)(j)

and

v~(j)

momentum respectively.

= qt(j)v0(j), coming from the Hessian For the remaining part, we would

Lipschitz assumption like to bound PSc ~

and and

PScv~ , and show that both of them are small compared to PScf (x0) .

First, for the PSc ~ term, we know by definition of the subspace Sc, and given that both eigenvalues of Aj are real and positive according to Lemma 21, such that p(j) is positive by Lemma 19, we have for any j  Sc:

t-1

t-1

|~(j)| =|

p(j)(j)| 

p(j)(|0(j)| + |(j) - 0(j)|)

 =0

 =0

t-1



p(j)

t-1

t-1

|0(j)| +

|(j) - (j-)1|  |0(j)| +

|(j) - (j-)1|.

 =0

 =1

 =1

22

By the Cauchy-Swartz inequality, this gives:





PSc ~ 2 =

|~(j)|2 

t-1
(|0(j)| + |(j) - (j-)1|)2  2 

|0(j)|2 +

t-1
( |(j) - (j-)1|)2

j S c

j S c

 =1





j S c

jSc  =1

t-1

t-1

2  |0(j)|2 + t

|(j) - (j-)1|2  2 0 2 + 2t

 - -1 2 .

j S c

jSc  =1

 =1

Recall that for t  T , we have xt  S /2. By Proposition 14, we know: 0  O(S 2), and by Corollary 6 and Proposition 14:

t-1

t-1

t

 - -1 2  O(2S 2)t

x - x-1 2  O(2S 4).

 =1

 =1

This gives PSc ~  O(S 2)  O( · c-6)  /10.

Next we consider the PScv~ term. By Lemma 25, we have

-qt(j) =

bt

t-1  =0

a

 O(1) max{,

|j |}.

This gives:

PSc v~

2

=

[qt(j)v0(j)]2
j S c



O(1)

j S c

max{|j 2

|,

2}

[v0(j)

]2.

(13)

Recall that we have assumed by way of contradiction that ET /4 - E0  -E . By the precondition that NCE is not used at t = 0, due to the certificate (2), we have:

1 2

v02f

(0)v0



-

 2

v0

2

=

 -8

v0 2 ,

where 0 = x0 + (1 - )y0 and   [0, 1]. Lipschitz property, it is easy to show that

Noting that we fix 2f (0) - H  

x0 y0

as the 

origin v0 

0M , bytheH.esTsihains

gives:

v0Hv0  -2 v0 2 .

Again letting j denote the eigenvalues of H, rearranging the above sum give:

|j |[v0(j)]2 O() v0 2 +

j [v0(j)]2

j:j 0

j :j >0
O() v0 2 +

j [v0(j)]2  O() v0 2 + v0[PSHPS ]v0.

j :j >2 /(2-)2

The second inequality uses the fact that 2/(2 - )2  O(). Substituting into (13) gives:

PSc v~

2



O(

1 

)



v0 2 + v0[PSHPS ]v0

 O(M 2) = O(2c-2)  2/100.

23

Finally, putting all pieces together, we have:

t-1

xt



PSc xt



min
j S c



=0

a(j)

PSc(f (0) + ~ + v~)

(

 2

)

PScf (0) - PSc~ - PScv~)



(

 2

)



(S

c3)



S

which contradicts the fact xt that remains inside the ball around 0 with radius S /2.

The next lemma shows that if the initial momentum and gradient are reasonably small, and the Hamitonian does not have sufficient decrease over the next T iterations, then both the gradient and momentum of the strongly convex component S will vanish in T /4 iterations.

Lemma 17 (Formal Version of Lemma 9). Under the setting of Theorem 3, suppose v0  M and f (x0)  2M , ET /2 - E0  -E , and for t  [0, T /2] only AGD steps are used, without NCE or perturbation. Then  t  [T /4, T /2]:

PS f (xt)



 2

and

vt[PS2f (x0)PS ]vt  M 2.

Proof. Since ET - E0  -E , by Corollary 6 and the Cauchy-Swartz inequality, we see that for all t  T we have xt - x0  2T E / = S .
Given initial x0 and v0, we define x-1 = x0 - v0. Without loss of generality, setting x0 as the origin 0, by the notation and results of Lemma 13, we have the update equation:

xt xt-1

=At

0 -v0

t-1
-  At-1-
 =0

f (0) +  0

.

(14)

First we prove the upper bound on the gradient:  t  [T /4, T ], we have

PS f (xt)



 2

.

Let t =

1 0

(2f

(xt

)

-

H)d.

According

to

(14),

we

have:

f (xt) =f (0) + (H + t)xt

=

I - H I

t-1

0

At-1-

I 0

 =0

f (0) + H I

g1

- H I

t-1

0

At-1-

t 0

+ txt .

 =0

g4

g3

0 At
g2

0 -v0

We will upper bound four terms g1, g2, g3, g4 separately. Clearly, for the last term g4, we have: g4   xt 2  O(S 2) = O(c-6)  /8.

Next, we show that the first two terms g1, g2 become very small for t  [T /4, T ]. Consider coordinate j  S and the 2 × 2 block matrix Aj. By Lemma 20 we have:

1 - j 1

t-1

0

Atj-1-

1 0

=1

0 Atj

1 1

.

 =0

24

Denote:

(a(tj), - b(tj)) = 1 0 Atj.

By Lemma 27, we know:

max
jS

|a(tj)|, |b(tj)|



(t

+

1)(1

-



)

t 2

.

This

immediately

gives

when

t



T

/4

=

(

c 

log

1 

)

for

c

sufficiently

large:

PS g1 2 = |(a(tj) - b(tj))f (0)(j)|2  (t + 1)2(1 - )t f (0) 2  2/64
jS
PS g2 2 = |j b(tj)v0(j)|2  2(t + 1)2(1 - )t v0 2  2/64.
jS

Finally, for g3, by Lemma 29, for all j  S, we have

t-1

t-1

|g3(j)| = j

a(j)t-1-  |t(-j)1| +

|(j) - (j-)1|.

 =0

 =1

By Proposition 14, this gives:

t-1

PS g3 2  2 t-1 2 + 2t

 - -1 2  O(2S 4)  O(2 · c-12)  2/64.

 =1

In sum, this gives for any fixed t  [T /4, T ]:

PS f (xt)



PS g1

+

PS g2

+

PS g3

+

g4



 2

.



t

We now provide a similar argument to  [T /4, T ], we show vt[PS2f (x0)PS

]vptrovetheMup2.peArccboorudnindgfotor

the momentum. (14), we have:

That is,

vt = 1 -1

xt xt-1

=1

-1 At

0 -v0

- 1

t-1

-1

At-1-

 =0

m1

m2

- 1

t-1

-1

At-1-

 0

.

 =0

f (0) 0

m3

Consider the j-th eigendirection, so that j  S, and recall the 2 × 2 block matrix Aj. Denoting

(a(tj), - b(tj)) = 1 0 Atj,

by

Lemma

19

and

27,

we

have

for

t



T

/4

=

(

c 

log

1 

)

with

c

sufficiently

large:

[PS

2f

(x0)PS

]

1 2

m1

2
=

1
|j2

(b(tj)-b(t-j)1)v0(j)|2



(t+1)2(1-)t

v0

2



O(

2 

c-3)



1 3

M

2.

jS

25

On the other hand, by Lemma 20, we have:

j 1

t-1

-1

Atj-1-

1 0

= j 1

t-1

0

(Atj-1- - Atj-2- )

1 0

=

1

0 (Atj - Atj-1)

1 1

.

 =0

 =0

This

gives,

for

t



T

/4

=

(

c 

log

1 

),

and

for

c

sufficiently

large:

[PS

2f

(x0

)PS

]

1 2

m2

2
=

|j-

1 2

(a(tj)

-

a(t-j)1

-

b(tj)

+

b(t-j)1)f

(0)(j)|2

jS

O(

1 

)(t

+

1)2(1

-

)t

f (0)

2



O(

2 

c-3)



1 3

M

2.

Finally, for any j  S, by Lemma 29, we have:

|(H

1 2

m3)(j

)|

=

1
|j2

t-1
(a

- a -1)t-1- |





 =0

Again by Proposition 14:

t-1

|t(-j)1| +

|(j) - (j-)1| .

 =1

[PS

2f

(x0)PS

]

1 2

m3

2
=

t-1
2 t-1 2 + 2t

 -  -1 2



O(2S 4)



O(

2 

c-6)



1 3

M

2

.

 =1

Putting everything together, we have:

vt[PS2f (x0)PS ]vt 

[PS

2

f

(x0)PS

]

1 2

m1

2
+

[PS

2f

(x0)PS

]

1 2

m2

2

+

[PS

2

f

(x0

)PS

]

1 2

m3

2  M 2.

This finishes the proof.

Finally, we are ready to prove the main lemma of this subsection (Lemma 7), which claims that if gradients in T iterations are always large, then the Hamiltonian will decrease sufficiently within a small number of steps.

Lemma 7 (Large gradient). Consider the setting of Theorem 3. If f (x )   for all   [0, T ], then by running Algorithm 2 we have ET - E0  -E .
Proof. Since f (x )   for all   [0, T ], according to Algorithm 2, the precondition to add perturbation never holds, so Algorithm will not add any perturbation in these T iterations.
Next, suppose there is at least one iteration where NCE is used. Then by Lemma 12, we know that that step alone gives E decrease in the Hamiltonian. According to Lemma 4 and Lemma 12 we know that without perturbation, the Hamiltonian decreases monotonically in the remaining steps. This means whenever at least one NCE step is performed, Lemma 7 immediately holds.
For the remainder of the proof, we can restrict the discussion to the case where NCE is never performed in steps   [0, T ]. Letting

1 = arg min {t | vt  M and f (xt)  2M } ,
t[0,T ]

26

we

know

in case

1



T 4

,

that

Lemma 15

ensures ET

- E0

 ET
4

- E0

 -E .

Thus, we

only

need

to

discuss

the

case

1



T 4

.

Again,

if

E1+T /2 - E1

 -E ,

Lemma

7

immediately

holds.

For

the

remaining case, E1+T /2 - E1  -E , we apply Lemma 17 starting at 1, and obtain

PS f (xt)



 2

and

vt[PS2f (x1 )PS ]vt  M 2.

T

T

t  [1 + 4 , 1 + 2 ].

Letting:

2 = arg min {t | vt  M } ,

t[1

+

T 4

,T

]

aabllyrseoLabedmyy mtghuaea1rpa5rnewtceoeenasdguiatffiiioncnieknontfowdLeecwmreemaosanel7yi,nnwteheeedkHntoaowmdiilstcounfsi(saxnt.h2)eTchaesne,,wwtehheucrlseearP2lySchafv1e(+xPT22)S;ofth(xe2r.w2)Oisne, twh2ee, other hand, since if the Hamiltonian does not decrease enough, E2 - E0  -E , by Lemma 6, we have x1 - x2  2S , by the Hessian Lipschitz property, which gives:
v2 [PS2f (x2 )PS ]v2  v2 [PS2f (x1 )PS ]v2 + 2f (x1 ) - 2f (x2 ) v2 2  2M 2.

Now x2 satisfies all the preconditions of Lemma 16, and by applying Lemma 16 we finish the proof.

B.3 Proof for negative-curvature scenario
We prove Lemma 8 in this section. We consider two trajectories, starting at x0 and x0, with v0 = v0 , where w0 = x0 - x0 = r0e1, where e1 is the minimum eigenvector direction of H, and where r0 is not too small. We show that at least one of the trajectories will escape saddle points efficiently.

Lemma f (x~)

18 

(Formal Version of and min(2f (x~))

Le-mma.

11). Let

Under the x0 and x0

same be at

setting as Theorem 3, suppose distance at most r from x~. Let

x0 - x0 = r0 · e1 and let v0 = v0 = v~ where e1 is the minimum eigen-direction of 2f (x~). Let

r0



E 2f

·

r .
d

Then,

running

AGD

starting

at

(x0, v0)

and

(x0, v0 )

respectively,

we

have:

min{ET - E, ET - E}  -E ,

where E, ET and ET are the Hamiltonians at (x~, v~), (xT , vT ) and (xT , vT ) respectively.
Proof. Assume none of the two sequences decrease the Hamiltonian fast enough; that is,
min{ET - E0, ET - E0 }  -2E , where E0 and E0 are the Hamiltonians at (x0, v0) and (x0, v0 ). Then, by Corollary 6 and the Cauchy-Swartz inequality, we have for any t  T :

max{ xt - x~ , xt - x~ }  r + max{ xt - x0 , xt - x0 }  r + 4T E /  2S .
Fix the origin 0 at x~ and let H be the Hessian at x~. Recall that the update equation of AGD (Algorithm 1) can be re-written as:

xt+1 =(2 - )xt - (1 - )xt-1 - f ((2 - )xt - (1 - )xt-1)

27

Taking the difference of two AGD sequences starting from x0, x0, and let wt = xt - xt, we have:

wt+1 =(2 - )wt - (1 - )wt-1 - f (yt) + f (yt ) =(2 - )(I - H - t)wt - (1 - )(I - H - t)wt-1,

where t =

1 0

(2f

(yt

+ (1 - )yt ) - H)d.

In

the

last

step,

we

used

f (yt) - f (yt ) = (H + t)(yt - yt ) = (H + t)[(2 - )wt - (1 - )wt-1].

We thus obtain the update of the wt sequence in matrix form:

wt+1 wt

=

(2 - )(I - H) I

-(1 - )(I - H) 0

wt wt-1

-

(2 - )twt - (1 - )twt-1 0

=A

wt wt-1

-

t 0

= At+1

w0 w-1

t
-  At-
 =0

 0

,

(15)

where t = (2 - )twt - (1 - )twt-1. Since v0 = v0 , we have w-1 = w0, and t   max{ xt - x~ , xt - x~ }  2S , as well as   6S ( w + w-1 ). According to (15):

wt = I

0 At

w0 w0

- I

t-1

0

At-1-

 =0

 0

.

Intuitively, we want to say that the first term dominates. Technically, we will set up an induction based on the following fact:

t-1

 I, 0

At-1-

 0

 =0



1 2

I, 0 At

w0 w0

.

It is easy to check the base case holds for t = 0. Then, assume that for all time steps less than or equal to t, the induction assumption hold. We have:

wt 

I

0 At

w0 w0

+ I

t-1

0

At-1-

 0

 =0

2

I

0 At

w0 w0

,

which gives:

t O(S )( wt + wt-1 )  O(S )

O(S )

I

0 At

w0 w0

,

I

0 At

w0 w0

+

I

0 At-1

w0 w0

where in the last inequality, we used Lemma 33 for monotonicity in t.

28

To prove that the induction assumption holds for t + 1 we compute:

 I, 0

t
At-

 0

t


I, 0 At-

I 0



 =0

 =0

t
O(S )
 =0

I, 0 At-

I 0

I

0 A

w0 w0

.

(16)

By the precondition we have min(H)  -. Without loss of generality, assume that the mini-

mum eigenvector direction of H is along he first coordinate e1, and denote the corresponding 2 × 2

matrix as A1 (as in the convention of (12). Let:

(a(t1), - b(t1)) = 1 0 At1.

We then see that (1) w0 is along the e1 direction, and (2) according to Lemma 32, the matrix

I, 0 At-

I 0

is a diagonal matrix, where the spectral norm is achieved along the first coordinate

which corresponds to the eigenvalue min(H). Therefore, using Equation (16), we have:

 I, 0

t
At-

 0

 =0

t
O(S ) a(t-1) (a(1) - b(1)) w0
 =0

O(S )

t

[

2 

+

(t

+

1)]|a(t+1)1

-

b(t+1)1|

w0

 =0

O(S T 2)

I, 0 At+1

w0 w0

,

where, in the second to last step, we used Lemma 31, and in the last step we used 1/  T . Finally, O(S T 2)  O(c-1)  1/2 by choosing a sufficiently large constant c. Therefore, we have proved
the induction, which gives us:

wt =

I

0 At

w0 w0

- I

t-1

0

At-1-

 0

 =0



1 2

I

0 At

w0 w0

.

Noting that min(H)  -, by applying Lemma 33 we have

1 2

I

0 At

w0 w0



 4

(1

+

())tr0,

which grows exponentially.

Therefore, for r0 

E 2f

·

r d

,

and

T

=

(

1 

·c)

where



=

max{1,

log

df 

},

where the constant c is sufficiently large, we have

xT - xT which contradicts the fact that:

= wT



 4

(1

+

())T

r0



4S

,

t  T , max{ xt - x~ , xt - x~ }  O(S ).

29

This means our assumption is wrong, and we can therefore conclude:

min{ET - E0, ET - E0 }  -2E .

On the other hand, by the precondition on x~ and the gradient Lipschitz property, we have:

max{E0

- E~, E0

- E~}



r

+

r2 2



E,

where the last step is due to our choice of r =  · -5c-8 in (3). Combining these two facts:

min{ET - E~, ET - E~}  min{ET - E0, ET - E0 } + max{E0 - E~, E0 - E~}  -E ,

which finishes the proof.

We are now ready to prove the main lemma in this subsection, which states with that random perturbation, PAGD will escape saddle points efficiently with high probability.

L-emm, aan8d(aNpeegrattuivrbeactuiornvahtausren)o. tCboenensidaedrdethdeinseittteinragtioofnTsheore[m-T3.,

If 0),

f (x0) then, by

 , min(2f (x0)) running Algorithm

<

2,

we

have

ET

- E0  -E

with

probability

at

least

1

-

E 2f

.

Proof. Since a perturbation has not been added in iterations   [-T , 0), according to PAGD (Algorithm 2), we add perturbation at t = 0, the Hamiltonian will increase by at most:

E



r

+

r2 2



E,

where the last step is due to our choice of r =  · -5c-8 in (3) with constant c sufficiently

large. Again by Algorithm 2, a perturbation will never be added in the remaining iterations,

and by Lemma 4 and Lemma 12 we know the Hamiltonian always decreases for the remaining

steps. Therefore, if at least one NCE step is performed in iteration   [0, T ], by Lemma 12

we will decrease 2E in that NCE step, and at most increase by E due to the perturbation. This

immediately gives ET - E0  -E .

Therefore, we only need to focus on the case where NCE is never used in iterations   [0, T ].

Let

B
x0

(r)

denote

the

ball

with

radius

r

around

x0.

According

to

algorithm

2,

we

know

the

iterate

after

adding

perturbation

to

x0

is

uniformly

sampled

from

the

ball

B
x0

(r).

Let

Xstuck



B
x0

(r)

be

the region where AGD is stuck (does not decrease the Hamiltonian E in T steps). Formally, for any

point x  Xstuck, let x1, · · · , xT be the AGD sequence starting at (x, v0), then ET - E0  -E . By

Lemma

18,

Xstuck

can

have

at

most

width

r0

=

E 2f

·

r d

along

the

minimum

eigenvalue

direction.

Therefore,

Vol(Xstuck) Vol(Bx(d0)(r))



r0

× Vol(B0(d-1)(r)) Vol(B0(d)(r))

=

r0 r

(

d 2

(

d 2

+ 1)

+

1 2

)



r0 r

·

d 2

+

1 2



E 2f

.

Thus,

with

probability

at

least

1

-

E f

,

the

perturbation

will

end

up

outside

of

Xstuck,

which

give

ET - E0  -E . This finishes the proof.

30

B.4 Proof of Theorem 3

Our main result is now easily obtained from Lemma 7 and Lemma 8.

Proof of Theorem 3. Suppose we never encounter any -second-order stationary point. Consider

the set T = { |  [0, T ] and f (x )  }, and two cases: (1) T = , in which case we know

all gradients are large and by Lemma 7 we have ET - E0  -E ; (2) T = . In this case, define

  = min T; i.e., the earliest an -second-order stationary

iteration where point, this gives

the gradient 2f (x) 

-issma, lal.ndSibnyceLbemy masasu8m, wpetiocnan,

x is not conclude

E+T - E0  E+T - E  -E . Clearly   + T  2T . That is, in either case, we will decrease

the Hamiltonian by E in at most 2T steps.

Then, for the the first case, we can repeat this argument starting at iteration T , and for the

second case, we can repeat the argument starting at iteration   + T . Therefore, we will continue

to obtain a decrease of the Hamiltonian by an average of E /(2T ) per step. Since the function f is

lower bounded, we know the Hamiltonian can not decrease beyond E0 - E = f (x0) - f , which

means

that

in

2(f (x0)-f )T E

steps, we must encounter an -second-order stationary point at least

once.

Finally,

in

2(f (x0)-f )T E

steps,

we

will

call

Lemma

8

at

most

2f E

times, and since Lemma 8

holds

with

probability

1

-

E 2f

,

by

a

union

bound,

we

know

that

the

argument

above

is

true

with

probability at least:

1-

E 2f

·

2f E

= 1 - ,

which finishes the proof.

C Auxiliary Lemma

In this section, we present some auxiliary lemmas which are used in proving Lemma 16, Lemma 17 and Lemma 18. These deal with the large-gradient scenario (nonconvex component), the largegradient scenario (strongly convex component), and the negative curvature scenario, respectively.
The first two lemmas establish some facts about powers of the structured matrices arising in AGD.

Lemma 19. Let the 2 × 2 matrix A have following form, for arbitrary a, b  R:

A=

a 1

b 0

.

Letting µ1, µ2 denote the two eigenvalues of A (can be repeated or complex eigenvalues), then, for any t  N:

t

1 0 At =

µi1µt2-i,

i=0

0 1 At = 1 0 At-1.

t-1

-µ1µ2

µi1µt2-1-i

i=0

31

Proof. When the eigenvalues µ1 and µ2 are distinct, the matrix A can be rewritten as

µ1 + µ2 1

-µ1µ2 0

,

and it is easy to check that the two eigenvectors have the form

µ1 1

and

µ2 1

.

Therefore, we

can write the eigen-decomposition as:

A

=

µ1

1 -

µ2

µ1 1

µ2 1

and the tth power has the general form:

µ1 0 0 µ2

1 -1

-µ2 µ1

,

At

=

µ1

1 - µ2

µ1 1

µ2 1

µt1 0 0 µt2

1 -µ2 -1 µ1

When there are two repeated eigenvalue µ1, the matrix

a 1

b 0

can be rewritten as

2µ1 1

-µ21 0

.

It is easy to check that A has the following Jordan normal form:

A=-

µ1 1

µ1 + 1 1

µ1 1 0 µ1

1 -1

-(µ1 + 1) µ1

,

which yields:

At = -

µ1 1

µ1 + 1 1

µt1 tµt1-1 0 µt1

1 -1

-(µ1 + 1) µ1

.

The remainder of the proof follows from simple linear algebra calculations for both cases.

Lemma 20. Under the same setting as Lemma 19, for any t  N:

(µ1 - 1)(µ2 - 1) 1

t-1

0

A

1 0

=1- 1

0 At

1 1

.

 =0

Proof. When µ1 and µ2 are distinct, we have:

1 0 At =

µt1+1 µ1

- -

µt2+1 µ2

,

-

µ1µ2(µt1 µ1 -

- µt2) µ2

.

When µ1, µ2 are repeated, we have:

1 0 At = (t + 1)µt1, -tµt1+1 .

The remainder of the proof follows from Lemma 22 and linear algebra.

The next lemma tells us when the eigenvalues of the AGD matrix are real and when they are complex.

Lemma

21.

Let





(0,

1 4

],

x



[-

1 4

,

1 4

]

and

define

the

2×2

matrix

A

as

follows:

A=

(2 - )(1 - x) 1

-(1 - )(1 - x) 0

32

Then the two eigenvalues µ1 and µ2 of A are solutions of the following equation: µ2 - (2 - )(1 - x)µ + (1 - )(1 - x) = 0.

Moreover,

when

x



[-

1 4

,

2 (2-)2

],

µ1

and

µ2

are

real

numbers,

and

when

x



(

2 (2-)2

,

1 4

],

µ1

and

µ2

are conjugate complex numbers.

Proof. An eigenvalue µ of the matrix A must satisfy the following equation: det(A - µI) = µ2 - (2 - )(1 - x)µ + (1 - )(1 - x) = 0.

The discriminant is equal to
 =(2 - )2(1 - x)2 - 4(1 - )(1 - x) =(1 - x)(2 - (2 - 2)x).

Then µ1 and µ2 are real if and only if   0, which finishes the proof. Finally, we need a simple lemma for geometric sums.
Lemma 22. For any  > 0 and fixed t, we have:

t-1
(

+

1)

=

1 - t (1 - )2

-

tt 1-

.

 =0

Proof. Consider the truncated geometric series:

Taking derivatives, we have:

t-1

 =0

=

1 1

- t -

.

t-1
(

+ 1)

=

d d

t-1
 +1

=

d d



·

1 - t 1-

=

1 - t (1 - )2

-

tt 1-

.

 =0

 =0

C.1 Large-gradient scenario (nonconvex component)

All the lemmas in this section are concerned with the behavior of the AGD matrix for eigendirections of the Hessian with eigenvalues being negative or small and positive, as used in proving Lemma 16. The following lemma bounds the smallest eigenvalue of the AGD matrix for those directions.

Lemma

23.

Under

the

same

setting

as

Lemma

21,

and

for

x



[-

1 4

,

2 (2-)2

],

where

µ1



µ2,

we

have:

µ2



1

-

1 2

max{,

|x|}.

33

Proof. The eigenvalues satisfy:

det(A - µI) = µ2 - (2 - )(1 - x)µ + (1 - )(1 - x) = 0.

Let µ = 1 + u. We have

(1 + u)2 - (2 - )(1 - x)(1 + u) + (1 - )(1 - x) = 0



u2 + ((1 - x) + 2x)u + x = 0.



Let f (u) = u2+ u + 2xu - xu + x. To prove µ2(A)  1 -

|x| 2

when

x



[-

1 4

,

-2],

we

only

need

to verify f (-

|x| 2

)



0:

f (-

|x| 2

)

=

|x| 4

-



|x| 2

+

|x|

|x| -

|x|

|x| 2

- |x|

|x|

|x|(1 -

 2

)

-

3|x| 4



0

The last inequality follows because For x  [-2, 0], we have:

|x|



1 4

by

assumption.

f

(-

 2

)

=

2 4

-

2 2

- x

+

x2 2

+x

=

-

2 4

+ x(1 - ) +

x2 2



0.

On the other hand, when x  [0, 2/(2 - )2], both eigenvalues are still real, and the midpoint of the two roots is:

u1

+ 2

u2

=

- (1

-

x) 2

+ 2x

=

-

+

(2 - 2

)x



-

 2

.

Combining

the

two

cases,

we

have

shown

that

when

x



[-2,

2/(2

-

)2]

we

have

µ2(A)



1

-

 2

.

In summary, we have proved that

µ2(A) 



1-

|x| 2

,

1

-

 2

.

x



[-

1 4

,

-2]

x  [-2, 2/(2 - )2],

which finishes the proof.

In the same setting as above, the following lemma bounds the largest eigenvalue.

Lemma

24.

Under

the

same

setting

as

Lemma

21,

and

with

x



[-

1 4

,

2 (2-)2

],

and

letting

µ1



µ2,

we have:

µ1



1

+

2

min{

|x| 

,

|x|}.

Proof. By Lemma 21 and Vieta's formula we have:

(µ1 - 1)(µ2 - 1) = µ1µ2 - (µ1 + µ2) + 1 = x.

An application of Lemma 23 finishes the proof.

34

The following lemma establishes some properties of the powers of the AGD matrix.

Lemma

25.

Consider

the

same

setting

as

Lemma

21,

and

let

x



[-

1 4

,

2 (2-)2

].

Denote:

(at, - bt) = 1 0 At.

Then,

for

any

t



2 

+

1,

we

have:

t-1

a

(

1 2

)

 =0

1 bt

t-1
a
 =0

(1) min

1 

,

1 |x|

.

Proof. We prove the two inequalities seperately. First Inequality: By Lemma 19:

t
1
 =0

0 A

1 0

t

=

µ1 -iµi2

 =0 i=0

=

t

(µ1

µ2)

 2

 =0

 i=0

(

µ1 µ2

)

 2

-i



t

[(1

-



)(1

-

x)]

 2

·



 =0

2

The last inequality holds because in

 i=0

(

µ1 µ2

)

 2

-i

at

least

 2

terms

are greater

than one.

Finally,

since x  2/(2 - )2  2  , we have 1 - x  1 - , thus:

t

[(1

-

)(1

-

x)]

 2

·





t

(1 - )

·





1/
(1 - )

·



2

2

2

 =0

 =0

 =0

1
(1 - ) 

1/

 2



(

1 2

),

 =0

which finishes the proof. Second Inequality: Without loss of generality, assume µ1  µ2. Again by Lemma 19:

t-1  =0

a

bt

t-1
=  =0 µ1µ2

 i=0

µi1

µ2 -i

t-1 i=0

µi1µ2t-1-i

=

1 µ1µ2

t-1  =0

 i=0

µi1

µ2 -i

t-1 i=0

µi1µt2-1-i



1 µ1µ2

t-1  =(t-1)/2

 i=0

µi1µ2 -i

t-1 i=0

µi1

µt2-1-i



1 µ1µ2

t-1  =(t-1)/2

1 2µt1-1-

=

1 2µ1µ2

1+

1 µ1

+

···

+

1 µ1(t-1)/2



1 2µ1µ2

1

+

1 µ1

+

···

+

1 µ11/

.

The second-to-last inequality holds because it is easy to check



t-1

2µ1t-1-

µi1µ2 -i 

µi1µt2-1-i,

i=0

i=0

35

for any   (t - 1)/2. Finally, by Lemma 24, we have

µ1



1

+

2

min{

|x| 

,

|x|}.

Since µ1 = (1), µ2 = (1), we have that when |x|  2,

t-1  =0

a

bt

 (1)

1

+

1 µ1

+

·

·

·

+

1 µ11/



(1)

·

1 

·

(1

1

+

)

1 



(

1 

).

When |x| > 2, we have:

t-1  =0

a

bt

 (1)

1

+

1 µ1

+

·

·

·

+

1 µ11/

=

1 (1)

-1
µ11/+1

1

-

1 µ1

=

(

µ1

1 -

1

)

=

(

1 ). |x|

Combining the two cases finishes the proof.

C.2 Large-gradient scenario (strongly convex component)

All the lemmas in this section are concerned with the behavior of the AGD matrix for eigendirections of the Hessian with eigenvalues being large and positive, as used in proving Lemma 17. The following lemma gives eigenvalues of the AGD matrix for those directions.

Lemma

26.

Under

the

same

setting

as

Lemma

21,

and

with

x



(

2 (2-)2

,

1 4

],

we

have

µ1

=

rei

and µ2 = re-i, where:

r = (1 - )(1 - x), sin  = ((2 - )2x - 2)(1 - x)/2r.

Proof. By Lemma 21, we know that µ1 and µ2 are two solutions of µ2 - (2 - )(1 - x)µ + (1 - )(1 - x) = 0.

This gives r2 = µ1µ2 = (1 - )(1 - x). On the other hand, discriminant is equal to

 =(2 - )2(1 - x)2 - 4(1 - )(1 - x) =(1 - x)(2 - (2 - 2)x).



Since Im(µ1) = r sin  =

- 2

,

the

proof

is

finished.

Under the same setting as above, the following lemma delineates some properties of powers of the AGD matrix.

Lemma

27.

Under

the

same

setting

as

in

Lemma

21,

and

with

x

(

2 (2-)2

,

1 4

],

denote:

(at, - bt) = 1 0 At.

Then, for any t  0, we have:

max{|at|,

|bt|}



(t

+

1)(1

-

)

t 2

.

36

Proof. By Lemma 19 and Lemma 26, using | · | to denote the magnitude of a complex number, we have:

t

t

|at| =

µi1µt2-i 

|µi1µt2-i|

=

(t

+

1)rt



(t

+

1)(1

-

)

t 2

i=0

i=0

t-1

t-1

|bt| = µ1µ2

µi1µ2t-1-i 

|µi1+1µt2-i|



trt+1



t(1

-

)

t+1 2

.

i=0

i=0

Reorganizing these two equations finishes the proof.

The following is a technical lemma which is useful in bounding the change in the Hessian by the amount of oscillation in the iterates.

Lemma 28. Under the same setting as Lemma 26, for any T  0, any sequence {t}, and any

0  [0, 2]:

T t=0

rt

sin(t

+

0)t



O(

1 sin

)

T
|0| + |t - t-1|
t=1

.

Proof. Let  = 2/ be the approximate period, and J = T /  be the number of periods that exist within time T . Then, we can group the summation by each period:





T

J min{(j+1) -1,T }

rt sin(t)t = 

rt sin(t + 0)t

t=0

j=0

t=j





J min{(j+1) -1,T }

=

rt sin(t + 0)[j + (t - j )]

j=0

t=j









J min{(j+1) -1,T }

J min{(j+1) -1,T }



rt sin(t + 0) j + 

rt|t - j | .

j=0

t=j

j=0

t=j

Term 1

Term 2

We prove the lemma by bounding the first term and the second term on the right-hand-side of this

equation separately.

Term 2: Since r  1, it is not hard to see:





J min{(j+1) -1,T }

Term 2 = 

rt|t - j |

j=0

t=j







J min{(j+1) -1,T }

min{(j+1) -1,T }



rt 

|t - t-1|

j=0

t=j



J min{(j+1) -1,T }

t=j +1

T

 

|t - t-1|   |t - t-1|.

j=0

t=j +1

t=1

37

Term 1: We first study the inner-loop factor,

(j+1) t=j

-1

rt

sin(t).

Letting



= 2 -   be

the

offset for each approximate period, we have that for any j < J:

(j+1) -1

 -1

rt sin(t + 0) = Im

rj +tei·[(j +t)+0]

t=j

t=0

rj

 -1
rtei·t
t=0

 rj

1 - r ei·(2-) 1 - rei·

=rj

(1 - r (1 - r

cos )2 cos )2

+ +

(r sin )2 (r sin )2

.

Combined with the fact that for all y  [0, 1] we have e-3y  1 - y  e-y, we obtain the following:

1 - r

=

1

-

[(1

-

)(1

-

x)]

 2

=

1 - e-((+x) )

=

(( + x) )

=



( + x) 

(17)

Also, for any a, b  [0, 1], we have (1 - ab)2  (1 - min{a, b})2  (1 - a2)2 + (1 - b2)2, and by definition of  , we immediately have   . This yields:

(1 - r cos )2 (1 - r cos )2

+ (r sin )2 + (r sin )2

 2(1 - r2 )2

+ 2(1 - cos2 )2 (r sin )2

+

(r

sin )2

O

1 sin2 

(

+ x)2 2

+

sin4



+

sin2



O

( + x)2 sin4 

The second last inequality used the fact that r = (1) (although note r is not (1)). The last inequality is true since by Lemma 26, we know ( + x)/ sin2   (1). This gives:

(j+1) -1
rt sin(t + 0)
t=j



rj

·

+ sin2

x, 

and therefore, we can now bound the first term:





J min{(j+1) -1,T }

J min{(j+1) -1,T }

Term 1 =

rt sin(t + 0)j = 

rt sin(t + 0) (0 + j - 0)

j=0

t=j

j=0

t=j

J -1
O(1)
j=0

rj

+ sin2

x 

T
(|0| + |j - 0|) + (|0| + |J - 0|)
t=J 

O(1)

1

1 - r

+ sin2

x 

+



·

T
|0| + |t - t-1|
t=1



O(

1 sin



)

+



·

T
|0| + |t - t-1|
t=1

.

The

second-to-last

inequality

used

Eq.(17).

In

conclusion,

since





2 



2 sin 

,

we

have:

T

rt sin(t + 0)t Term 1 + Term 2 

O(

1 sin



)

+

2

·

T
|0| +

|t - t-1|

t=0

t=1

O

1 sin 

T
|0| + |t - t-1| .
t=1

38

The following lemma combines the previous two lemmas to bound the approximation error in the quadratic.

Lemma

29.

Under

the

same

setting

as

Lemma

21,

and

with

x

(

2 (2-)2

,

1 4

],

denote:

(at, - bt) = 1 0 At.

Then,

for

any

sequence

{ },

any

t



(

1 

),

we

have:

t-1
a 
 =0

O(

1 x

)

t-1
|0| + | - -1|
 =1

t-1
(a
 =0

- a-1)

O( 1x )

t-1
|0| + | - -1|
 =1

.

Proof. We prove the two inequalities separately.

First

Inequality:

Since

x



(

2 (2-)2

,

1 4

],

we

further

split

the

analysis

into

two

cases:

Case

x



(

2 (2-)2

,

22 (2-)2

]:

By

Lemma

19,

we

can

expand

dthe

left-hand-side

as:

t-1

t-1

t-1

a   |a |(|0| + | - 0|) 

|a |

 =0

 =0

 =0

t-1
|0| + | - -1| .
 =1

Noting that in this case x = (2), by Lemma 27 and Lemma 22, we have for t  O(1/):

t-1
|a | 

t-1
(

+

1)(1

-

)

 2



O(

1 2

)

=

O(

1 x

).

 =0

 =0

Case

x



(

22 (2-)2

,

1 4

]:

Again,

we

expand

the

left-hand-side

as:

t-1
a 
 =0

=

t-1  =0

µ1+1 µ1

- -

µ2 +1 µ2



=

t-1  =0

r

+1

sin[( + r sin[]

1)]



.

Noting in this case that x = (sin2 ) by Lemma 26, then by Lemma 28 we have:

t-1
a 
 =0



O(

1 sin2



)

t-1
|0| + | - -1|
 =1



O(

1 x

)

t-1
|0| + | - -1|
 =1

.

Second Inequality: Using Lemma 19, we know:

a

-

a -1

= (µ1+1

-

µ2+1) µ1 -

- (µ1 µ2

-

µ2 )

= r+1

sin[(

+ 1)] - r sin[]

r

sin[ ]

= r

sin[

](r

cos



- 1) + r sin[]

r

+1

cos[

]

sin



=r

cos  r sin

- 

1

·

r

sin[ ]

+

r

cos[ ],

39

where we note r = (1) and the coefficient of the first term is upper bounded by the following:

r cos  - 1 r sin 



(1

-

cos2 ) + (1 r sin 

-

r2)



O

+x sin 

.

As in the proof of the first inequality, we split the analysis into two cases:

Case

x



(

2 (2-)2

,

22 (2-)2

]:

Again,

we

use

t-1

t-1

t-1

(a - a-1)  |a - a-1|(|0| + | - 0|) 

|a - a-1|

 =0

 =0

 =0

Noting

x

=

(2),

again

by

Lemma

22

and

|

sin   sin 

|



,

we

have:

t-1
|0| + | - -1| .
 =1

t-1
|a - a-1|
 =0

t-1

 O( + x)



(1

-

)

 2

t-1

+

(1

-

)

 2

 O( 1 ) = O( 1

).

 =0

 =0



x

Case

x



(

22 (2-)2

,

1 4

]:

From

the

above

derivation,

we

have:

t-1
(a
 =0

- a-1)

=

r

cos  r sin

- 

1

t-1
r
 =0

sin[ ]

t-1
+ r
 =0

cos[ ] .

According to Lemma 26, in this case x = (sin2 ), r = (1) and since (2)  x  O(1), we have:

r cos  - 1 r sin 

O

+x sin 

O

+xx

 O(1).

Combined with Lemma 28, this gives:

t-1
(a

- a-1)



O(

1 sin



)

t-1
|0| + | - -1|

 O( 1x )

t-1
|0| + | - -1|

.

 =0

 =1

 =1

Putting all the pieces together finishes the proof.

C.3 Negative-curvature scenario

In this section, we will prove the auxiliary lemmas required for proving Lemma 18. The first lemma lower bounds the largest eigenvalue of the AGD matrix for eigen-directions
whose eigenvalues are negative.

Lemma

30.

Under

the

same

setting

as

Lemma

21,

and

with

x



[-

1 4

,

0],

and

µ1



µ2,

we

have:

µ1



1

+

1 2

min{

|x| 

,

|x|}.

Proof. The eigenvalues satisfy:

det(A - µI) = µ2 - (2 - )(1 - x)µ + (1 - )(1 - x) = 0.

40

Let µ = 1 + u. We have

(1 + u)2 - (2 - )(1 - x)(1 + u) + (1 - )(1 - x) = 0



u2 + ((1 - x) + 2x)u + x = 0.



Let f (u) = u2 + u + 2xu - xu + x. To prove µ1(A)  1 +

|x| 2

when

x



[-

1 4

,

-2],

we

only

need

to verify f (

|x| 2

)



0:

f(

|x| 2

)

=

|x| 4

+



|x| 2

-

|x|

|x| + |x|

|x| 2

- |x|



|x| 2

-

3|x| 4

-

|x|

|x|(1

-

 2

)



0

The last inequality holds because   |x| in this case. For x  [-2, 0], we have:

f

(

|x| 2

)

=

|x|2 42

+

|x| 2

-

|x|2 

+

|x|2 2

-

|x|

=

|x|2 42

-

|x| 2

-

|x|2

(

1 

-

1 2

)



0,

where the last inequality is due to 2  |x|.

In summary, we have proved

µ1(A) 



1+

|x| 2

,

1

+

|x| 2

.

x



[-

1 4

,

-2]

x  [-2, 0],

which finishes the proof.

The next lemma is a technical lemma on large powers.

Lemma

31.

Under

the

same

setting

as

Lemma

21,

and

with

x



[-

1 4

,

0],

denote

(at, - bt) = 1 0 At.

Then, for any 0    t, we have

|a(t-1) ||a(1)

-

b(1) |



[

2 

+

(t

+

1)]|a(t+1)1

-

b(t+1)1|.

Proof.

Let µ1

and µ2

be the two eigenvalues

of the matrix A, where µ1  µ2.

Since

x



[-

1 4

,

0],

according

to

Lemma

21

and

Lemma

23,

we

have

0



µ2



1-

 2



1



µ1,

and

thus

expanding

41

both sides using Lemma 19 yields:

t-

 -1

LHS =

µ1t- -iµi2 (1 - µ2)

µ1 -iµi2 + µ2

i=0

i=0

t-

 -1

t-

=

µ1t- -iµi2 (1 - µ2)

µ1 -iµi2 +

µ1t- -iµi2 µ2

i=0

i=0

i=0

 -1

t-

(t -  + 1)µt1- (1 - µ2)

µ1 -iµi2 +

µ1t- -iµi2

i=0

i=0

(t + 1)(1 - µ2)

 -1
µt1+1-iµi2
i=0

+

2 

(1

-

µ2)

t-
µt1+1-iµi2

i=0

[

2 

+

(t

+

1)]

t
(1 - µ2)

µt1+1-iµi2 + µt2+1

= RHS,

i=0

which finishes the proof.

The following lemma gives properties of the (1, 1) element of large powers of the AGD matrix.

Lemma

32.

Let

the

2×2

matrix

A(x)

be

defined

as

follows

and

let

x



[-

1 4

,

0]

and





(0,

1 4

].

A(x) =

(2 - )(1 - x) 1

-(1 - )(1 - x) 0

.

For any fixed t > 0, letting g(x) =

1

0 [A(x)]t

1 0

, then we have:

1. g(x) is a monotonically decreasing function for x  [-1, 2/(2 - )2].

2. For any x  [2/(2 - )2, 1], we have g(x)  g(2/(2 - )2).

Proof. For x  [-1, 2/(2 - )2], we know that A(x) has two real eigenvalues µ1(x) and µ2(x), Without loss of generality, we can assume µ1(x)  µ2(x). By Lemma 19, we know:

g(x) =

1

0 [A(x)]t

1 0

t

t

=

[µ1(x)]i[µ2(x)]t-i

=

[µ1(x)µ2

(x)]

t 2

i=0

i=0

µ1(x) µ2(x)

t 2

-i

.

By

Lemma

21

and

Vieta's

formulas,

we

know

that

[µ1(x)µ2(x)]

t 2

=

[(1

-

)(1

-

x)]

t 2

is

monotonically

decreasing in x. On the other hand, we have that:

µ1(x) µ2(x)

+

µ2(x) µ1(x)

+2

=

[µ1(x) + µ2(x)]2 µ1(x)µ2(x)

=

(2

- )2(1 - x) 1-

is monotonically decreasing in x, implying that

t i=0

µ1 (x) µ2 (x)

t 2

-i

is

monotonically

decreasing

in

x.

Since both terms are positive, this implies the product is also monotonically decreasing in x, which

finishes the proof of the first part.

42

For x  [2/(2 - )2, 1], the two eigenvalues µ1(x) and µ2(x) are conjugate, and we have:

[µ1

(x)µ2

(x)]

t 2

=

[(1

-

)(1

-

x)]

t 2



[µ1(2/(2

-

)2)µ2(2/(2

-

)2)]

t 2

which yields:

t i=0

µ1(x) µ2(x)

t 2

-i



t i=0

µ1(x) µ2(x)

t 2

-i

t

i=0

µ1(x) µ2(x)

t 2

-i

t

=t+1=

i=0

µ1(2/(2 - )2) µ2(2/(2 - )2)

t 2

-i

,

and this finishes the proof of the second part.

The following lemma gives properties of the sum of the first row of large powers of the AGD matrix.

Lemma

33.

Under

the

same

setting

as

Lemma

21,

and

with

x



[-

1 4

,

0],

denote

(at, - bt) = 1 0 At.

Then we have and

|at+1 - bt+1|  |at - bt|

|at

-

bt|



 2

1

+

1 2

min{

|x| 

,

t
|x|} .

Proof. Since x < 0, we know that A has two distinct real eigenvalues. Let µ1 and µ2 be the two eigenvalues of A. For the first inequality, by Lemma 19, we only need to prove:

µt1+1 - µt2+1 - µ1µ2(µt1 - µt2)  µt1 - µt2 - µ1µ2(µt1-1 - µt2-1).

Taking the difference of the LHS and RHS, we have:

µt1+1 - µt2+1 - µ1µ2(µt1 - µt2) - (µt1 - µt2) + µ1µ2(µt1-1 - µt2-1) =µt1(µ1 - µ1µ2 - 1 + µ2) - µt2(µ2 - µ1µ2 - 1 + µ1) =(µt1 - µt2)(µ1 - 1)(1 - µ2).

According to Lemma 21 and Lemma 23, µ1  1  µ2  0, which finishes the proof of the first claim.
For the second inequality, again by Lemma 19, since both µ1 and µ2 are positive, we have:

t

t-1

t

at - bt = µi1µt2-i - µ1µ2 µi1µt2-1-i  (1 - µ2) µi1µt2-i  (1 - µ2)µt1.

i=0

i=0

i=0

By

Lemma

23

we

have

1

-

µ2



 2

,

By

Lemma

30

we

know

µ1



1

+

1 2

min{

|x| 

,

|x|}. Combining

these facts finishes the proof.

43


COMPUTATIONAL LEARNING THEORY
Sally A. Goldman, Washington University, St. Louis Missouri
1 Introduction
Since the late fties, computer scientists (particularly those working in the area of arti cial intelligence) have been trying to understand how to construct computer programs that perform tasks we normally think of as requiring human intelligence, and which can improve their performance over time by modifying their behavior in response to experience. In other words, one objective has been to design computer programs that can learn. For example, Samuels designed a program to play checkers in the early sixties that could improve its performance as it gained experience playing against human opponents. More recently, research on arti cial neural networks has stimulated interest in the design of systems capable of performing tasks that are di cult to describe algorithmically (such as recognizing a spoken word or identifying an object in a complex scene), by exposure to many examples.
As a concrete example consider the task of hand-written character recognition. A learning algorithm is given a set of examples where each contains a hand-written character as speci ed by a set of attributes (e.g. the height of the letter) along with the name (label) for the intended character. This set of examples is often called the training data. The goal of the learner is to e ciently construct a rule (often referred to as a hypothesis or classi er) that can be used to take some previously unseen character and with high accuracy determine the proper label.
Computational learning theory is a branch of theoretical computer science that formally studies how to design computer programs that are capable of learning, and identi es the computational limits of learning by machines. Historically, researchers in the arti cial intelligence community have judged learning algorithms empirically, according to their performance on sample problems. While such evaluations provide much useful information and insight, often it is hard using such evaluations to make meaningful comparisons among competing learning algorithms.
Computational learning theory provides a formal framework in which to precisely formulate
1

and address questions regarding the performance of di erent learning algorithms so that careful comparisons of both the predictive power and the computational e ciency of alternative learning algorithms can be made. Three key aspects that must be formalized are the way in which the learner interacts with its environment, the de nition of successfully completing the learning task, and a formal de nition of e ciency of both data usage (sample complexity) and processing time (time complexity). It is important to remember that the theoretical learning models studied are abstractions from real-life problems. Thus close connections with experimentalists are useful to help validate or modify these abstractions so that the theoretical results help to explain or predict empirical performance. In this direction, computational learning theory research has close connections to machine learning research. In addition to its predictive capability, some other important features of a good theoretical model are simplicity, robustness to variations in the learning scenario, and an ability to create insights to empirically observed phenomena.
The rst theoretical studies of machine learning were performed by inductive inference researchers (see Angluin and Smith, 1987]) beginning with the introduction of the rst formal de nition of learning given by Gold, 1967]. In Gold's model, the learner receives a sequence of examples and is required to make a sequence of guesses as to the underlying rule (concept) such that this sequence converges at some nite point to a single guess that correctly names the unknown rule. A key distinguishing characteristic is that Gold's model does not attempt to capture any notion of the e ciency of the learning process whereas the eld of computational learning theory emphasizes the computational feasibility of the learning algorithm.
Another closely related eld is that of pattern recognition (see Duda and Hart, 1973] and Devroye, Gyor , and Lugosi, 1996]). Much of the theory developed by learning theory researchers to evaluate the amount of data needed to learn directly adapts results from the elds of statistics and pattern recognition. A key distinguishing factor is that learning theory researchers study both the data (information) requirements for learning and the time complexity of the learning algorithm. (In contrast, pattern recognition researchers tend to focus on issues related to the data requirements.) Finally, there are a lot of close relations to work on arti cial neural networks. While much of neural network research is empirical, there is also a good amount of theoretical
2

work (see Devroye, Gyor , and Lugosi, 1996]). This chapter is structured as follows. In Section 2 we describe the basic framework of concept
learning and give notation that we use throughout the chapter. Next, in Section 3 we describe the PAC (distribution-free) model that began the eld of computational learning theory. An important early result in the eld is the demonstration of the relationships between the VC-dimension, a combinatorial measure, and the data requirements for PAC learning. We then discuss some commonly studied noise models and general techniques for PAC learning from noisy data.
In Section 4 we cover some of the other commonly studied formal learning models. In Section 4.1 we study an on-line learning model. Unlike the PAC model in which there is a training period, in the on-line learning model the learner must improve the quality of its predictions as it functions in the world. Next, in Section 4.2, we study the query model which is very similar to the on-line model except that the learner plays a more active role.
As in other theoretical elds, along with having techniques to prove positive results, another important component of learning theory research is to develop and apply methods to prove when a learning problem is hard. In Section 5 we describe some techniques used to show that learning problems are hard. Next, in Section 6, we explore a variation of the PAC model called the weak learning model, and study techniques for boosting the performance of a mediocre learning algorithm. Finally, we close with a brief overview of some of the many current research issues being studied within the eld of computational learning theory.
2 General Framework
For ease of exposition, we initially focus on concept learning in which the learner's goal is to infer how an unknown target function classi es (as positive or negative) examples from a given domain. In the character recognition example, one possible task of the learner would be to classify each character as a numeral or non-numeral. Most of the de nitions given here naturally extend to the general setting of learning functions with multiple-valued or real-valued outputs. Later in Section 4.1 we brie y discuss the more general problem of learning a real-valued function.
3

. . .

Figure 1: The left gure shows (in <2) a concept from Chalfspace and the right gure shows a

concept

for

C\s halfspace

for s

=

3.

The

points

from

<2

that

are

classi

ed

as

positive

are

shaded.

The unshaded points are classi ed as negative.

2.1 Notation

The instance space (domain) X is the set of all possible objects (instances) to be classi ed. Two

very common domains are the boolean hypercube f0 1gn, and the continuous n-dimensional

space <n. For example, if there are n boolean attributes then each example can be expressed as

an element of f0 1gn. Likewise, if there are n real-valued attributes, then <n can be used. A

concept f is a boolean function over domain X . Each x 2 X is referred to as an example (point).

A concept class C is a collection of subsets of X . That is C 2X . Typically (but not always),

it is assumed that the learner has prior knowledge of C. Examples are classi ed according to

membership of a target concept f 2 C. Often, instead of using this set theoretic view of C, a

functional view is used in which f(x) gives the classi cation of concept f 2 C for each example

x 2 X . An example x 2 X is a positive example of f if f(x) = 1 (equivalently x 2 f), or a

negative example of f if f(x) = 0 (or equivalently x 62 f). Often C is decomposed into subclasses

Cn according to some natural size measure n for encoding an example. For example, in the

boolean domain, n is the number of boolean attributes. Let Xn denote the set of examples to be

classi

ed

for each

problem

of size

n,

X

=

S
n

1 Xn.

We

use

the

class

C , halfspace

the

set

of

halfspaces

in

<n,

and

the

class

C , \s halfspace

the

set

of

all

the

intersections of up to s halfspaces in <d, to illustrate some of the basic techniques for designing

4

learning algorithms1 (see Figure 1). We now formally de ne a halfspace in <n and describe how

the points in <n are classi ed by it. Let ~x = (x1 : : : xn) denote an element of <n. A halfspace de ned by ~a 2 <n and b 2 < classi es as positive the set of points f~x j ~a ~x bg. For example,

in two-dimensions (i.e. n = 2), 5x1 ; 2x2 ;3 de nes a halfspace. The point (0 0) is a positive

example

and

(2

10)

is

a

negative

example.

For

C , \s halfspace

an

example

is

positive

exactly

when

it

is classi ed as positive by each of the halfspaces forming the intersection. Thus the set of positive

points forms a (possibly open) convex polytope in <n.

We also study some boolean concepts. For these concepts the domain is f0 1gn. Let x1 : : : xn
denote the n boolean attributes. A literal is either xi or xi where i = 1 : : : n. A term is a

conjunction of literals. Finally, a DNF formula is a disjunction of terms. One of the biggest

open problems of computational learning theory is whether or not the concept class of DNF

formulas is e ciently PAC learnable. Since the problem of learning general DNF formulas is a

long-standing open problem, several subclasses have been studied. A monotone DNF formula is

a DNF formula in which there are no negated variables. A read-once DNF formula is a DNF

formula in which each variable appears at most once. In addition to adding a restriction that

the formula be monotone and/or read-once, one can also limit either the size of each term or the

number of terms. A k-term DNF formula is a DNF formula in which there are at most k terms.

Finally, a k-DNF formula is a DNF formula in which at most k literals are used by each term.

As one would expect, the time and sample complexity of the learning algorithm depends on

the complexity of the underlying target concept. For example, the complexity for learning a

monotone DNF formula is likely to depend on the number of terms (conjuncts) in the formula.

To give a careful de nition of the size of a concept f 2 C, we associate a language RC with each concept class C that is used for representing concepts in C. Each r 2 RC denotes some f 2 C, and every f 2 C has at least one representation r 2 RC. Each concept f 2 Cn has a size denoted by jfj, which is the representation length of the shortest r 2 RC that denotes f. For ease of exposition, in the remainder of this chapter we use C and RC interchangeably.

To appreciate the signi cance of the choice of the representation class, consider the problem of

1As a convention, when the number of dimensions can be arbitrary we use n, and when the number of dimensions must be a constant we use d.

5

learning a regular language2. The question as to whether an algorithm is e cient depends heavily on the representation class. As de ned more formally below, an e cient learning algorithm must
have time and sample complexity polynomial in jfj where f is the target concept. The target
regular language could be represented as a deterministic nite-state automaton (DFA) or as a non-deterministic nite-state automaton (NFA). However, the length of the representation as a NFA can be exponentially smaller than the shortest representation as a DFA. Thus, the learner may be allowed exponentially more time when learning the class of regular languages as represented by DFAs versus when learning the class of regular languages as represented by NFAs. Often to make the representation class clear, the concept class names the representation. Thus instead of talking about learning a regular language, we talk about learning a DFA or an NFA. Thus the problem of learning a DFA is easier than learning an NFA. Similar issues arise when learning boolean functions. For example, whether the function is represented as a decision tree, a DNF formula, or a boolean circuit greatly a ects the representation size.
3 PAC Learning Model
The eld of computational learning theory began with Valiant's seminal work Valiant, 1984] in which he de ned the PAC3(distribution-free) learning model. In the PAC model examples
are generated according to an unknown probability distribution D, and the goal of a learning algorithm is to classify with high accuracy (with respect to the distribution D) any further
(unclassi ed) examples. We now formally de ne the PAC model. To obtain information about an unknown target
function f 2 Cn, the learner is provided access to labeled (positive and negative) examples of f, drawn randomly according to some unknown target distribution D over Xn. The learner is also given as input and such that 0 < < 1, and an upper bound k on jfj. The learner's goal is to output, with probability at least 1 ; , a hypothesis h 2 Cn that has probability at
2See Section 2.1 of Chapter 30 and Section 2 of Chapter 31 in this handbook for background on regular languages, DFAs and NFAs.
3PAC is an acronym coined by Dana Angluin for probably approximately correct.
6

most of disagreeing with f on a randomly drawn example from D (thus, h has error at most ). If such a learning algorithm A exists (that is, an algorithm A meeting the goal for any n 1, any target concept f 2 Cn, any target distribution D, any > 0, and any k jfj), then C is PAC learnable. A PAC learning algorithm is a polynomial-time (e cient) algorithm if the
number of examples drawn and the computation time are polynomial in n k, 1= and 1= . We note that most learning algorithms are really functions from samples to hypotheses (i.e. given a sample S the learning algorithm produces a hypothesis h). It is only for the analysis in which we say that for a given and , the learning algorithm is guaranteed with probability at least
1 ; to output a hypothesis with error at most given a sample whose size is a function of , ,
n and k. Thus, empirically, one can generally run a PAC algorithm on provided data and then empirically measure the error of the nal hypothesis. One exception is when trying to empirically use statistical query algorithms since, for most of these algorithms, the algorithm uses for more than just determining the desired sample size. (See Section 3.3 for a discussion of statistical query algorithms, and Goldman and Scott, 1996] for a discussion of their empirical use).
As originally formulated, PAC learnability also required the hypothesis to be a member of
the concept class Cn. We refer to this more stringent learning model as proper PAC-learnability.
The work of Pitt and Valiant, 1988] shows that a prerequisite for proper PAC-learning is the ability to solve the consistent hypothesis problem, which is the problem of nding a concept
f 2 C that is consistent with a provided sample. Their result implies that if the consistent
hypothesis problem is NP-hard for a given concept class (as they show is the case for the class of
k-term DNF formulas) and NP 6= RP, then the learning problem is hard4. A more general form
of learning in which the goal is to nd any polynomial-time algorithm that classi es instances accurately in the PAC sense is commonly called prediction. In this less stringent variation of the
PAC model, the algorithm need not output a hypothesis from the concept class C but instead
is just required to make its prediction in polynomial time. This idea of prediction in the PAC model originated in the paper of Haussler, Littlestone, and Warmuth, 1994], and is discussed in Pitt and Warmuth, 1990]. Throughout the remainder of this chapter, when referring to the
4For background on Complexity Classes see Chapter 33 (NP de ned) and Chapter 35, Section 3 (RP de ned) of this handbook.
7

PAC learning model we allow the learner to output any hypothesis that can be evaluated in

polynomial time. That is, given a hypothesis h and an example x, we require that h(x) can be

computed in polynomial time. We refer to the model in which the learner is required to output

a hypothesis h 2 Cn as the proper PAC learning model.

Many variations of the PAC model are known to be equivalent (in terms of what concept

classes are e ciently learnable) to the model de ned above. We now brie y discuss one of

these variations. In the above de nition of the PAC model, along with receiving , , and n

as input, the learner also receives k, an upperbound on jfj. The learner must be given and

. Further, by looking at just one example, the value of n is known. Yet giving the learner

knowledge of k may appear to make the problem easier. However, if the demand of polynomial-

time computation is replaced with expected polynomial-time computation, then the learning

algorithm need not be given the parameter k, but could \guess" it instead. We now brie y

review the standard doubling technique used to convert an algorithm A designed to have k as

input to an algorithm B that has no prior knowledge of k. Algorithm B begins with an estimate,

say 1, for its upperbound on k and runs algorithm A using this estimate to obtain hypothesis

h. Then algorithm B uses a hypothesis testing procedure to determine if the error of h is at

most . Since the learner can only gather a random sample of examples, it is not possible

to distinguish a hypothesis with error from one with error just greater than . However,

by drawing a su ciently large sample and looking at the empirical performance of h on that

sample, we can distinguish a hypothesis with error at most =2 from one with error more than

.

In

particular,

given

a

sample5

of

size

m=

l
32 ln

m
2,

if

h

misclassi

es

at

most

3 4

m examples

then B accepts h. Otherwise, algorithm B doubles its estimate for k and repeats the process.

For the technical details of the above argument as well as for a discussion the other equivalences,

see Haussler, Kearns, Littlestone, and Warmuth, 1991].

5Cherno bounds are used to compute the sample size so that the hypothesis testing procedure gives the desired

output with high probability.

8

-

-

-

+
+ -

-
+
+ +-

+

-

-

+
+

-

-

+

+
+ +

Figure 2: A demonstration that there are 3 points that are shattered by the class of twodimensional halfspaces (i.e. Chalfspace with n = 2). Notice that all 8 possible ways that the points
can be classi ed as positive or negative can be realized.
3.1 Sample Complexity Bounds, the VC-Dimension, and Occam's Razor
Although we are concerned with the time complexity of the learning algorithm, a fundamental question to rst answer is the sample complexity (data) requirements. Blumer, Ehrenfeucht, Haussler, and Warmuth, 1989] identi ed a combinatorial parameter of a class of functions de ned by Vapnik and Chervonenkis, 1971]. They give strong results relating this parameter, called the
VC-dimension, to information-theoretic bounds on the sample size needed to have accurate
generalization. Note that given a su ciently large sample there is still the computational problem of nding a \good" hypothesis.
We now de ne the VC-dimension. We say that a nite set S X is shattered by the concept class C if for each of the 2jSj subsets S0 S, there is a concept f 2 C that contains all of S0 and none of S ; S0. In other words, for any of the 2jSj possible labelings of S (where each example s 2 S is either positive or negative), there is some f 2 C that realizes the desired labeling (see Figure 2). We can now de ne the VC-dimension of a concept class C. The VC-dimension of C, denoted vcd(C), is the smallest d for which no set of d + 1 examples is shattered by C. Equivalently, vcd(C) is the cardinality of the largest nite set of points S X that is shattered by C.
So to prove that vcd(C) d it su ces to give d examples that can be shattered. However, to prove vcd(C) d one must show that no set of d + 1 examples can be shattered. Since the VC-
9

dimension is so fundamental in determining the sample complexity required for PAC learning, we now go through several sample computations of the VC-dimension.
Axis-parallel rectangles in <2. For this concept class the points lying on or inside the target
rectangle are positive, and points lying outside the target rectangle are negative. First, it
is easily seen that there is a set of four points (e.g. f(0 1) (0 ;1) (1 0) (;1 0)g) that can be shattered. Thus vcd(C) 4. We now argue that no set of ve points can be shattered.
The smallest bounding axis-parallel rectangle de ned by the ve points is in fact de ned by at most four of the points. For p a non-de ning point in the set, we see that the set cannot be shattered since it is not possible for p to be classi ed as negative while also classifying
the others as positive. Thus vcd(C) = 4.
Halfspaces in <2. Points lying in or on the halfspace are positive, and the remaining points are
negative. It is easily shown that any three non-collinear points (e.g. (0 1) (0 0) (1 0)) are
shattered by C (recall Figure 2). Thus vcd(C) 3. We now show that no set of size four can be shattered by C. If at least three of the points are collinear then there is no halfspace
that contains the two extreme points but does not contain the middle points. Thus the four points cannot be shattered if any three are collinear. Next, suppose that the points form a quadrilateral. There is no halfspace which labels one pair of diagonally opposite points positive and the other pair of diagonally opposite points negative. The nal case is that one point p is in the triangle de ned by the other three. In this case there is no halfspace which labels p di erently from the other three. Thus clearly the four points cannot be
shattered. Therefore we have demonstrated that vcd(C) = 3. Generalizing to halfspaces in <n, it can be shown that C vcd( ) halfspace = n + 1 (see Blumer, Ehrenfeucht, Haussler,
and Warmuth, 1989]).
Closed sets in <2. All points lying in the set or on the boundary of the set are positive, and all points lying outside the set are negative. Any set can be shattered by C, since a closed set can assume any shape in <2. Thus, the largest set that can be shattered by C is in nite, and hence vcd(C) = 1.
10

We now brie y discuss techniques that aid in computing the VC-dimension of more complex

concept classes.

Suppose

we

wanted

to

compute

the

VC-dimension

of

C , \s halfspace

the

class

of

intersections of up to s halfspaces over <d. We would like to make use of our knowledge of the

VC-dimension of C , halfspace the class of halfspaces over <d. Blumer, Ehrenfeucht, Haussler, and Warmuth, 1989] gave the following result: let C be a concept class with vcd(C) D. Then the

class de ned by the intersection of up to s concepts from C has VC-dimension6 at most 2Ds lg(3s).

In fact, the above result applies when replacing intersection with any boolean function. Thus

the

concept

class

C\s halfspace

(where

each

halfspace

is

de

ned

over

<d)

has

VC-dimension

at

most

2(d + 1)s lg(3s).

We now discuss the signi cance of the VC-dimension to the PAC model. One important

property is that for D = vcd(C), the number of di erent labelings that can be realized (also referred to as behaviors) using C for a set S is at most eDjSj D. Thus for a constant D we
have polynomial growth in the number of labelings versus the exponential growth of 2jSj. A key

result in the PAC model is that of Blumer, Ehrenfeucht, Haussler, and Warmuth, 1989] that

gives an upperbound on the sample complexity needed to PAC learn in terms of , , and the

VC-dimension of the hypothesis class. They proved that one can design a learning algorithm

A for concept class C using hypothesis space H in the following manner. Any concept h 2 H
consistent with a sample of size max 4 lg 2 8vcd(H) lg 13 has error at most with probability

at least 1 ; . To obtain a polynomial-time PAC learning algorithm what remains is to solve

the algorithmic problem of nding a hypothesis from H consistent with the labeled sample.

Furthermore, Ehrenfeucht and Haussler, 1989] proved an information-theoretic lower bound that
learning any concept class C requires 1 log 1 + vcd(C) examples in the worst case. As an

example application, see Figure 3.

A key limitation of this technique to design a PAC learning algorithm is that the hypothesis

must be drawn from some xed hypothesis class H. In particular, the complexity of the hypothesis

class must be independent of the sample size. However, often the algorithmic problem of nding

such a hypothesis from the desired class is NP-hard.

6Note that throughout this chapter, lg is used for the base-2 logarithm. When the base of the logarithm is not

signi cant (such as when using asymptotic notation), we use log.

11

PAC-learn-halfspace(n ) Draw a labeled sample S of size max 4 lg 2 8(n+1) lg 13 Use a polynomial-time linear programming algorithm to nd a halfspace h consistent with S Output h

Figure 3: A PAC algorithm to learn a concept from C . halfspace Recall that C vcd( ) halfspace = n+1.

As an example suppose we tried to modify PAC-learn-halfspace from Figure 3 to e ciently

PAC learn the class of intersections of at most s halfspaces in <d for d the number of dimensions

a constant. Suppose we are given a sample S of m example points labeled according to some

f

2

C . \s halfspace

The

algorithmic

problem

of

nding

a

hypothesis

from

C\s halfspace

can

be

formulated

as a set covering problem in the following manner. An instance of the set covering problem is a

set U and a family T of subsets of U such that (St2T t) = U. A solution is a smallest cardinality

subset G of T such that

S
g2G

g

= U. Consider any halfspace g that correctly classi es all

positive examples from S. We say that g covers all of the negative examples from S that are

also classi ed as negative by g. Thus U is the set of negative examples from S, and T is the set

of representative halfspaces (one for each behavior with respect to S) that correctly classify all

positive examples from S.

So

nding

a

minimum

sized

hypothesis

from

C\s halfspace

consistent

with

the

sample

S

is

exactly

the problem of nding the minimum number of halfspaces that are required to cover all negative

points in S. Given that the set covering problem is NP-complete, how can we proceed? We

can apply the greedy approximation algorithm that has a ratio bound of ln jUj + 1 to nd a

hypothesis consistent with S that is the intersection of at most ln jSj + 1 halfspaces. However,

since the VC-dimension of the hypothesis grows with the size of the sample, the basic technique

described above cannot be applied. In general, when using a set covering approach, the size of

the hypothesis often depends on the size of the sample.

Blumer, Ehrenfeucht, Haussler, and Warmuth, 1987 and 1989] extended this basic tech-

nique by showing that nding a hypothesis h consistent with a sample S for which the size

of h is sub-linear in jSj is su cient to guarantee PAC learnability. In other words, by ob-

taining su cient data compression one obtains good generalization. More formally, let HAk n m

12

PAC-learn-intersection-of-halfspaces(d s ) Draw a labeled sample S of size m = max 4 lg 2 192ds ;6 + 4 lg 3 + lg ds 3
Form a set T of halfspaces that are consistent with the positive examples
Let N be the negative examples from S Let h be the always true hypothesis Repeat until N =
Find a halfspace t 2 T consistent with the max. number of exs. from N Let h = h \ t
Remove the examples from N that were covered by t Output h

Figure

4:

An

Occam

algorithm

to

PAC

learn

a

concept

from

C . \s halfspace

be the hypothesis space used by algorithm A when each example has size n, the target con-

cept has size k, and the sample size is m. We say that algorithm A is an Occam algorithm

for concept class C if there exists a polynomial p(k n) and a constant , 0 < 1, such

that for any sample S with jSj = m 1 and any k n, A outputs a hypothesis7 h consistent

with S such that jhj p(k n)m . Let A be an Occam algorithm for concept class C that has

hypothesis space HAk n m. If vcd(HAk n m) p(k n)(lg m)` (so jhj p(k n)(lg m)`) for some

polynomia0l m = max @

p(k 4 lg

n) 2 and ` 2 2`+4p(k n)

1, then A is a PAC lg 8(2` + 2)`+1p(k n)

!le`a+r1n1ing A.

algorithm

for

C

using

sample

size

Figure

4

presents

an

Occam

algorithm

to

PAC

learn

a

concept

from

C\s halfspace

(i.e.

the

intersec-

tion

of

at

most

s

halfspaces

over

<d).

Since

C vcd(

) \s
halfspace

2(d+1)s lg(3s), and the hypothesis

consists of the intersection of at most s(1 + ln m) halfspaces, it follows that vcd(H) 2(d +

1)s(1 + ln m) lg(3s(1 + ln m)) 3ds ln2 m. The size of the sample S then follows by noting that

p(s d) = 3ds and ` = 3. By combining the fact that C vcd( ) halfspace = d+1 and the general upper

bound of

ejSj
D

D on the number of behaviors on a sample of size jSj for a class of VC-dimension

D, we get that jT j de+m1 d+1 = O(md+1). (For d constant this quantity is polynomial in the

size of the sample.) We can construct T in polynomial time by either using simple geometric ar-

guments or the more general technique of Blumer, Ehrenfeucht, Haussler, and Warmuth, 1989].
Finally, the time complexity of the greedy set covering algorithm is O(Pt2T jtj) = O(m md+1)

7Recall that we use hj j to denote the number of bits required to represent h.

13

which is polynomial in s (the number of halfspaces), , and for d (the number of dimensions) constant.
For the problem of learning the intersection of halfspaces the greedy covering technique provided substantial data compression. Namely, the size of our hypothesis only had a logarithmic dependence on the size of the sample. In general, only a sub-linear dependence is required as given by the following result of Blumer, Ehrenfeucht, Haussler, and Warmuth, 1987]. Let A be
an Occam algorithm for concept class C that has hypothesis space HAk n m. If vcd(HypAk n m) p(k n)m (so jhj p(k n)m ) for some polynomial p(k n) 2 and < 1, then A is a PAC learning algorithm for C using sample size m = max 2 ln 1 2 ln 2 p(k n) . 1;1 !
3.2 Models of Noise
The basic de nition of PAC learning assumes that the data received is drawn randomly from
D and properly labeled according to the target concept. Clearly, for learning algorithms to be
of practical use they most be robust to noise in the training data. In order to theoretically study an algorithm's tolerance to noise, several formal models of noise have been studied. In
the model of Random Classi cation Noise( Angluin and Laird, 1988]) with probability 1 ; ,
the learner receives the uncorrupted example (x `). However, with probability , the learner receives the example (x `). So in this noise model, learner usually gets a correct example, but some small fraction of the time the learner receives an example in which the label has been
inverted. In the model of Malicious Classi cation Noise( Sloan, 1988]) with probability 1; , the
learner receives the uncorrupted example (x `). However, with probability , the learner receives the example (x `0) in which x is unchanged, but the label `0 is selected by an adversary who has in nite computing power and has knowledge of the learning algorithm, the target concept,
and the distribution D. In the previous two models, only the labels are corrupted. Another noise model is that of Malicious Noise( Valiant, 1985]). In this model, with probability 1 ; ,
the learner receives the uncorrupted example (x `). However, with probability , the learner receives an example (x0 `0) about which no assumptions whatsoever may be made. In particular, this example (and its label) may be maliciously selected by an adversary. Thus in this model, the learner usually gets a correct example, but some small fraction of the time the learner gets
14

noisy examples and the nature of the noise is unknown.
We now de ne two noise models that are only de ned when the instance space is f0 1gn.
In the model of Uniform Random Attribute Noise ( Sloan, 1988]), the example (b1 bn `) is corrupted by a random process that independently ips each bit bi to bi with probability for 1 i n. Note that the label of the \true" example is never altered. In this noise model, the attributes' values are subject to noise, but that noise is as benign as possible. For example, the attributes' values might be sent over a noisy channel, but the label is not. Finally, in the model of Product Random Attribute Noise( Goldman and Sloan, 1995]), the example (b1 bn `) is corrupted by a random process of independently ipping each bit bi to bi with some xed probability i for each 1 i n. Thus unlike the model of uniform random attribute noise, the noise rate associated with each bit of the example may be di erent.
3.3 Gaining Noise Tolerance in the PAC Model
Some of the rst work on designing noise-tolerant PAC algorithms was done by Angluin and Laird, 1988]. They gave an algorithm for learning boolean conjunctions that tolerates random classi cation noise of a rate approaching the information-theoretic barrier of 1=2. Furthermore, they proved that the general technique of nding a hypothesis that minimizes disagreements with a su ciently large sample allows one to handle random classi cation noise of any rate approaching 1=2. However, they showed that even the very simple problem of minimizing disagreements (when there are no assumptions about the noise) is NP-hard. Until recently, there have been a small number of e cient noise-tolerant PAC algorithms, but no general techniques were available to design such algorithms, and there was little work to characterize which concept classes could be e ciently learned in the presence of noise.
The rst (computationally feasible) tool to design noise-tolerant PAC algorithms was pro-
vided by the statistical query model, rst introduced by Kearns, 1993], and since improved
by Aslam and Decatur, 1997]. In this model, rather than sampling labeled examples, the learner requests the value of various statistics. A relative-error statistical query Aslam and
Decatur, 1997] takes the form SQ( ) where is a predicate over labeled examples, is a relative error bound, and is a threshold. As an example, let to be \(h(x) = 1) ^ (` = 0)"
15

which is true when x is a negative example but the hypothesis classi es x as positive. So the

probability that is true for a random example is the false positive error of hypothesis h. For

target concept f, we de ne P = PrD (x f(x))) = 1] where PrD is used to denote that x is
drawn at random from distribution D. If P < then SQ( ) may return ?. If ? is not returned, then SQ( ) must return an estimate P^ such that P (1; ) P^ P (1+ ). The

learner may also request unlabeled examples (since we are only concerned about classi cation

noise).

Let's take our algorithm, PAC-learn-intersection-of-halfspaces and reformulate it as a relative-

error statistical query algorithm. First we draw an unlabeled sample Su that we use to generate
the set T of halfspaces to use for our covering. Similar to before, we place a halfspace in T

corresponding to each possible way in which the points of Su can be divided. Recall that before,
we only place a halfspace in T if it properly labeled the positive examples. Since we have an

unlabeled sample we cannot use such an approach. Instead, we use a statistical query (for each

potential halfspace) to check if a given halfspace is consistent with most of the positive examples.

Finally, when performing the greedy covering step we cannot pick the halfspace that covers the

most negative examples, but rather the one that covers the \most" (based on our empirical

estimate) negative weight. Figure 5 shows our algorithm in more depth.

We now cover the key ideas in proving that SQ-learn-intersection-of-halfspaces is correct. First,

we pick Su so that any hypothesis consistent with Su (if we knew the labels) would have error

at

most

6r

with

probability

1

;

2.

Since

our

hypothesis

class

is

C , \r halfspace

and

C vcd(

) \r
halfspace

2(d + 1)r lg(3r), we obtain the sample size for Su.

For each of the s halfspaces that form the target concept, there is some halfspace in T

consistent with that halfspace over Su, and thus that has error at most =(6r) on the positive region. For each such halfspace t our estimate P^ (for the probability that t(x) = 0 and ` = 1)

is at most 6r

3 2

=

4r .

Thus,

we

place

s

halfspaces

in

Tgood

that

produce

a

hypothesis

with

false

negative error =(6r). Let ei = Pr h(x) = 1 ^ ` = 0] (i.e. the false positive error) after i rounds

of the while loop. Since ei is our current error and =(6r) is a lower bound on the error we could

16

SQ-learn-intersection-of-halfspaces(d s )

Let r Draw

= 2s ln(3= ) an unlabeled

sample

Su

=

max

n

24r

lg

4

lg 96(d+1)r2 lg(3r) 78r o

Form the set T of halfspaces - one for each

TFgoorodeP^a=ch=tS2QT;t(x) = 0 ^ `
If P^ 4r or ? Then

=1
Tgood

1
2=

2Trgood

t

linear

separation

of

Su

into

two

sets

LWeht ihthlem=baSexQht=h\;ehat(rmaxglam)wx=aayxs1t2t^Trgu`oeo=dhSy0Qp19o;thh2(exs)i>s=491

and the loop has been executed

^ ` = 0 ^ t(x) = 0

1 3

370s

<

r

times

Output h

Figure

5:

A

relative-error

statistical

query

algorithm

to

learn

a

concept

from

C\s halfspace

over

<d.

achieve, by an averaging argument it follows that there is a t 2 Tgood for which

P = Pr (h(x) = 1) ^ (` = 0) ^ (t(x) = 0)]

ei ; 6r

1 s

ei ; 6

1 s

:

Thus for the halfspace tmax selected to add to h we know that the statistical query returns an

estimate P^

2 3

P

32s ;ei ; 6 . Thus for tmax we know that P

3 4

32s ;ei ; 6

= 21s(ei ; 6).

Solving the recurrence ei+1 ei ; 21s(ei ; 6) (with ei 1) yields that ei 6 + 1 ; 21s i. Picking

r = i = 2s ln 3 su ces to ensure that ei =2 as desired.

Once ei

2

=5,

we

exit

the

while

loop

(since

2 5

10 9

=

4 9

).

Thus

we

enter

the

loop

only

when

ei > 2 =5, and hence for tmax, P

1s (ei ; 6). So we choose

=

1s

(

2 5

;

6)

=

370s .

Finally,

when

we exit the loop Pr h(x) = 1 ^ ` = 0]

4 9

9 8

=

2.

Thus

the

total

error

is

at

most

as desired.

Notice that in the statistical query model there is not a con dence parameter . This is

because the SQ oracle guarantees that its estimates meet the given requirements. However,

when we use random labelled examples to simulate the statistical queries, we can only guarantee

that with high probability the estimates meet their requirements. Thus the results on converting

an SQ algorithm into a PAC algorithm re-introduce the con dence parameter .

By applying uniform convergence results and Cherno bounds it can be shown that if one

draws a su ciently large sample then a statistical query can be estimated by the fraction of the

sample that satis es the predicate. For example, in the relative-error SQ model with a set Q of

17

possible queries, Aslam and Decatur, 1997] show that a sample of size O

1
2

log jQj

su

ces to

appropriately answer ] for every 2 Q with probability at least 1 ; . (If vcd(Q) = q

then a sample of size O

q
2

log

1

+

1
2

log 1

su

ces.)

To handle random classi cation noise of any rate approaching 1=2 more complex methods

are used for answering the statistical queries. Roughly, using knowledge of the noise process

and a su ciently accurate estimate of the noise rate (which must itself be determined by the

algorithm), the noise process can be \inverted." The total sample complexity required to simulate

an SQ algorithm in the presence of classi cation noise of rate

b is O~

2

log(jQj= (1;2

)
b)2

where

(respectively, ) is the minimum value of (respectively, ) across all queries and 2

1], The soft-oh notation (O~) is similar to the standard big-oh notation except dominate

log factors are also removed. Alternatively, for the query space Q, the sample complexity is

O

2

vc(d1(;Q2) b)2

h
log

i
1(1;2 b) + log 1 . Notice that the amount by which b is less than

1=2

is

just

1=21;

b

=

1;22

b.

Thus

the

above

are

polynomial

as

long

as

1 2

;

b is at least one over

a polynomial.

4 Exact and Mistake Bounded Learning Models
The PAC learning model is a batch model|there is a separation between the training phase and the performance phase. In the training phase the learner is presented with labeled examples | no predictions are expected. Then at the end of the training phase the learner must output a hypothesis h to classify unseen examples. Also, since the learner never nds out the true classi cation for the unlabeled instances, all learning occurs in the training phase. In many settings, the learner does not have the luxury of a training phase but rather must learn as it performs. We now study two closely related learning models designed for such a setting.
4.1 On-line Learning Model
To motivate the on-line learning model (also known as the mistake-bounded learning model),
suppose that when arriving at work (in Boston) you may either park in the street or park in a garage. In fact, between your o ce building and the garage there is a street on which you

18

can always nd a spot. On most days, street parking is preferable since you avoid paying the $15 garage fee. Unfortunately, when parking on the street you risk being towed ($75) due to street cleaning, snow emergency, special events, etc. When calling the city to nd out when they tow, you are unable to get any reasonable guidance and decide the best thing to do is just learn from experience. There are many pieces of information that you might consider in making your prediction: e.g. the date, the day of the week, the weather. We make the following assumption: after you commit yourself to one choice or the other you learn of the right decision. In this example, the city has rules dictating when they tow you just don't know them. If you park on the street, at the end of the day you know if your car was towed otherwise when walking to the garage you see if the street is clear (i.e. you learn if you would have been towed). The on-line model is designed to study algorithms for learning to make accurate predictions in circumstances such as these. Note that unlike the problems addressed by many techniques from reinforcement learning, here there is immediate (versus delayed) feedback.
We now de ne the on-line learning model for the general setting in which the target function has a real-valued output (without loss of generality, scaled to be between 0 and 1). An on-line
learning algorithm for C is an algorithm that runs under the following scenario. A learning session consists of a set of trials. In each trial, the learner is given an unlabeled instance x 2 X . The
learner uses its current hypothesis to predict a value p(x) for the unknown (real-valued) target
concept f 2 C and then the learner is told the correct value for f(x). Several loss functions have
been considered to measure the quality of the learner's predictions. Three commonly used loss
functions are the following: the square loss de ned by `2(p(x) f(x)) = (f(x);p(x))2, the log loss de ned by `log(p(x) f(x)) = ;f(x) log p(x);(1;f(x)) log(1;p(x)), and the absolute loss de ned by `1(p(x) f(x)) = jf(x) ; p(x)j.
The goal of the learner is to make predictions so that total loss over all predictions is minimized. In this learning model, most often a worst-case model for the environment is assumed. There is some known concept class from which the target concept is selected. An adversary (with unlimited computing power and knowledge of the learner's algorithm) then selects both the target function and the presentation order for the instances. In this model there is no training
19

phase | Instead, the learner receives unlabeled instances throughout the entire learning session. However, after each prediction the learner \discovers" the correct value. This feedback can then be used by the learner to improve its hypothesis.
We now discuss the special case when the target function is boolean and correspondingly, predictions must be either 0 or 1. In this special case the loss function most commonly used is the absolute loss. Notice that if the prediction is correct then the value of the loss function is 0, and if the prediction is incorrect then the value of the loss function is 1. Thus the total loss of the learner is exactly the number of prediction mistakes. Thus, in the worst-case model we assume that an adversary selects the order in which the instances are presented to the learner and we evaluate the learner by the maximum number of mistakes made during the learning session. Our goal is to minimize the worst-case number of mistakes using an e cient learning algorithm (i.e. each prediction is made in polynomial time). Observe that such mistake bounds are quite strong in that the order in which examples are presented does not matter however, it is impossible to tell how early the mistakes will occur. Littlestone, 1988] has shown that in this learning model
vcd(C) is a lower bound on the number of prediction mistakes.
4.1.1 Handling Irrelevant Attributes
Here we consider the common scenario in which there are many attributes the learner could consider, yet the target concept depends on a small number of them. Thus most of the attributes are irrelevant to the target concept. We now brie y describe one early algorithm, Winnow of Littlestone, 1988], that handles a large number of irrelevant attributes. More speci cally, for Winnow, the number of mistakes only grows logarithmically with the number of irrelevant attributes. Winnow (or modi cations of it) have many nice features such as noise tolerance and the ability to handle the situation in which the target concept is changing. Also, Winnow can directly be applied to the agnostic learning model Kearns, Schapire, and Sellie, 1994]. In the agnostic learning model no assumptions are made about the target concept. In particular, the learner is unaware of any concept class that contains the target concept. Instead, we compare the performance of an agnostic algorithm (typically in terms of the number of mistakes or based on some other loss function) to the performance of the best hypothesis selected from a comparison
20

Winnow( ) For i = 1 : : : n, initialize wi = 1

To

pIErfelsP deicnipt=r1ethdweiicxtvia0l>ue

of x = , then

(x1 : : : predict

xn) 1

2

X

:

Let be the correct prediction

If the prediction was 1 and = 0 then

If

If the

pxrie=dic1titohnenwlaest

0wain=d

wi==

1

then

If xi = 1 then let wi = wi

Figure 6: The algorithm Winnow.

or \touchstone" class where the best hypothesis from the touchstone class is the one that incurs the minimum loss over all functions in the touchstone class.
Winnow is similar to the classical perceptron algorithm Rosenblatt, 1958], except that it uses a multiplicative weight-update scheme that permits it to perform much better than classical perceptron training algorithms when many attributes are irrelevant. The basic version of Winnow is designed to learn the concept class of a linearly separable boolean function which is a map
f : f0 1gn ! f0 1g such that there exists a hyperplane in <n that separates the inverse images
f;1(0) and f;1(1) (i.e. the hyperplane separates the points on which the function is 1 from those on which it is 0). An example of a linearly separable function is any monotone disjunction: if
f(x1,: : :,xn) = xi1 _ _xik, then the hyperplane xi1 + +xik = 1=2 is a separating hyperplane.
For each attribute xi there is an associated weight wi. There are two parameters, which determines the threshold for predicting 1 (positive), and which determines the adjustment made to the weight of an attribute that was partly responsible for a wrong prediction. The pseudo-code for Winnow is shown in Figure 6.
Winnow has been successfully applied to many learning problems. It has the very nice property that one can prove that the number of mistakes only grows logarithmically in the number of variables (and linearly in the number of relevant variables). For example, it can be shown that for the learning of monotone disjunctions of at most k literals, if Winnow is run with > 1 and 1= , then the total number of mistakes is at most k(log + 1) + n= . Observe that

21

Winnow need not have prior knowledge of k although the number of mistakes depends on k. Littlestone, 1988]) showed how to optimally choose and if an upperbound on k is known a priori. Also, while Winnow is designed to learn a linearly separable class, reductions (discussed in Section 5.1) can be used to apply Winnow to classes for which the positive and negative points are not linearly separable, e.g. k-DNF.
4.1.2 The Halving Algorithm and Weighted Majority Algorithm
We now discuss some of the key techniques for designing good on-line learning algorithms for the special case of concept learning (i.e. learning boolean-valued functions). If one momentarily ignores the issue of computation time, then the halving algorithm performs very well. It works
as follows. Initially all concepts in the concept class C are candidates for the target concept.
To make a prediction for instance x, the learner takes a majority vote based on all remaining candidates (breaking a tie arbitrarily). Then when the feedback is received, all concepts that disagree are removed from the set of candidates. It can be shown that at each step the number of candidates is reduced by a factor of at least 2. Thus, the number of prediction mistakes made
by the halving algorithm is at most lg jCj.
Clearly the halving algorithm will perform poorly if the data is noisy. We now brie y discuss the weighted majority algorithm which is one of several multiplicative weight-update schemes for generalizing the halving algorithm to tolerate noise. Also, the weighted majority algorithm provides a simple and e ective method for constructing a learning algorithm A that is provided with a pool of \experts", one of which is known to perform well, but A does not know which one. Associated with each expert is a weight that gives A's con dence in the accuracy of that expert. When asked to make a prediction, A predicts by combining the votes from its experts based on their associated weights. When an expert suggests the wrong prediction, A passes that information to the given expert and reduces its associated weight using a multiplicative weightupdating scheme. Namely, the weight associated with each expert that mispredicts is multiplied by some weight 0 < 1. By selecting > 0 this algorithm is robust against noise in the data. Figure 7 shows the weighted majority algorithm in more depth.
We now brie y discuss some learning problems in which weighted majority is applicable.
22

Weighted Majority Algorithm (WM)

Let wi be the weight associated with algorithm Ai

For 1 i n

To

Let wi make a

=1 prediction

for

an

instance

x

2

X:

If

a

LLLLPmeeeerittttesdtAAqqai01ck01t==ebb0ieesP PimttAAhhqaeeii220dssAAeee:01tt

of of wi qw1i

algorithms algorithms

that that

predicted predicted

0 1

Let be the prediction

For 1 i n

If Ai's prediction for x is

Let wi = wi (where 0 < 1)

Inform Ai of the mistake

Figure 7: The weighted majority algorithm.

Suppose one knows that the correct prediction comes from some target concept selected from a

known concept class. Then one can apply the weighted majority algorithm where each concept in

the class is one of the algorithms in the pool. For such situations, the weighted majority algorithm

is a robust generalization of the halving algorithm. (In fact, the halving algorithm corresponds

to the special case where = 0.) As another example, the weighted majority algorithm can often

be applied to help in situations in which the prediction algorithm has a parameter that must be

selected and the best choice for the parameter depends on the target. In such cases one can build

the pool of algorithms by choosing various values for the parameter.

We now describe some of the results known about the performance of the weighted majority

algorithm. If the best algorithm in the pool A makes at most m mistakes, then the worst case

number of mistakes made by the weighted majority algorithm is O(log jAj+m) where the constant

hidden within the big-oh notation depends on . Speci cally, the number of mistakes made by

the weighted majority algorithm is

m mistakes,

log jAkj +m log 1

log

2 1+

if there

makes at most m mistakes, and log

at most:

log jAj+m log 1

log

2 1+

if

one

algorithm

in

A

makes at most

exists a set of k algorithms in A such that each algorithm

jAk j + mk log 1

log

2 1+

if

the

total

number

of

mistakes

made

by

a

set

of

k algorithms in A is m.

23

When jAj is not polynomial, the weighted majority algorithm (when directly implemented),
is not computationally feasible. Recently, Maass and Warmuth, 1995] introduced what they call the virtual weight technique to implicitly maintain the exponentially large set of weights so that the time to compute a prediction and then update the \virtual" weights is polynomial. More speci cally, the basic idea is to simulate Winnow by grouping concepts that \behave alike" on seen examples into blocks. For each block only one weight has to be computed and one constructs the blocks so that the number of concepts combined in each block as well as the weight for the block can be e ciently computed. While the number of blocks increases as new counterexamples are received, the total number of blocks is polynomial in the number of mistakes. Thus all predictions and updates can be performed in time polynomial in the number of blocks, which is in turn polynomial in the number of prediction mistakes of Winnow. Many variations of the basic weighted majority algorithm have also been studied. The results of Cesa-Bianchi, et al., 1993] demonstrate how to tune as a function of an upper bound on the noise rate.
4.2 Query Learning Model
A very well-studied formal learning model is the membership and equivalence query model developed by Angluin Angluin, 1988]. In this model (often called the exact learning model) the learner's goal is to learn exactly how an unknown (boolean) target function f, taken from some
known concept class C, classi es all instances from the domain. This goal is commonly referred
to as exact identi cation. The learner has available two types of queries to nd out about f:
one is a membership query, in which the learner supplies an instance x from the domain and is told f(x). The other query provided is an equivalence query in which the learner
presents a candidate function h and either is told that h f (in which case learning is com-
plete), or else is given a counterexample x for which h(x) 6= f(x). There is a very close
relationship between this learning model and the on-line learning model (supplemented with membership queries) when applied to a classi cation problem. Using a standard transformation Angluin, 1988, Littlestone, 1988], algorithms that use membership and equivalence queries can easily be converted to on-line learning algorithms that use membership queries. Under such a transformation the number of counterexamples provided to the learner in response to the learner's
24

equivalence queries directly corresponds to the number of mistakes made by the on-line algorithm. In this model a number of interesting polynomial time algorithms are known for learning
deterministic nite automata Angluin, 1987], Horn sentences Angluin, Frazier, and Pitt, 1992], read-once formulas Angluin, Hellerstein, and Karpinski, 1993], read-twice DNF formulas Aizenstein and Pitt, 1991], decision trees Bshouty and Mansour, 1995], and many others. It is easily shown that membership queries alone are not su cient for e cient learning of these classes, and Angluin has developed a technique of \approximate ngerprints" to show that equivalence queries alone are also not enough Angluin, 1990]. (In both cases the arguments are information theoretic, and hold even when the computation time is unbounded.) The work of Bshouty, Goldman, Hancock and Matar, 1996] extended Angluin's results to establish tight bounds on how many equivalence queries are required for a number of these classes. Maass and Turan studied upper and lower bounds on the number of equivalence queries required for learning (when computation time is unbounded), both with and without membership queries Maass and Turan, 1992].
It is known that any class learnable exactly from equivalence queries can be learned in the PAC setting Angluin, 1988]. At a high level the exact learning algorithm is transformed to a PAC algorithm by having the learner use random examples to \search" for a counterexample to the hypothesis of the exact learner. If a counterexample is found, it is given as a response to the equivalence query. Furthermore, if a su ciently large sample was drawn and no counterexample
was found then the hypothesis has error at most (with probability at least 1 ; ). The converse
does not hold Blum, 1990]. That is, there are concept classes that are e ciently PAC learnable but cannot be e ciently learned in the exact model.
We now describe Angluin's algorithm for learning monotone DNF formulas (see Figure 8). A prime implicant t of a formula f is a satis able conjunction of literals such that t implies f but no
proper subterm of t implies f. For example, f = (a ^ c) _ (b ^ c) has prime implicants a ^ c, b ^ c, and a^b. The number of prime implicants of a general DNF formula may be exponentially larger
than the number of terms in the formula. However, for monotone DNF the number of prime implicants is no greater than the number of terms in the formula. The key to the analysis is to show that at each iteration, the term t is a new prime implicant of f, the target concept. Since
25

Learn-Monotone-DNF

h false

Do forever

Make equivalence query with h

If \yes," output h and halt

Else let Let For

x t i

= = =

Vb1b2 1 b:i:=:1

yibn n

be

the

counterexample

Let

If If h

=b\iyhe=s_,1"tpt e=rftornmfymigemanbderxsh=ipbq1uerybion

x with bn

ith

bit

ipped

Figure 8: An algorithm that uses membership and equivalence queries to exactly learn an unknown monotone DNF formula over the domain f0 1gn. Let fy1 y2 : : : yng be the boolean
variables and let h be the learner's hypothesis.

the loop iterates exactly once for each prime implicant there are at most m counterexamples where m is the number of terms in the target formula. Since at most n membership queries are performed during each iteration there are at most nm membership queries overall.

5 Hardness Results
In order to understand what concept classes are learnable, it is essential to develop techniques to prove when a learning problem is hard. Within the learning theory community, there are two basic type of hardness results that apply to all of the models discussed here. There are
representation-dependent hardness results in which one proves that one cannot e ciently learn C using a hypothesis class of H. These hardness results typically rely on some complexity theory assumption8 such as RP 6= NP. For example, given that RP 6= NP, Pitt and Warmuth,
1990] showed that k-term-DNF is not learnable using the hypothesis class of k-term-DNF. While such results provide some information, what one would really like to obtain is a hardness result for learning a concept class using any reasonable (i.e. polynomially evaluatable) hypothesis class. (For example, while we have a representation-dependent hardness result for learning k-
8For background on Complexity Classes see Chapter 33 (NP de ned) and Chapter 35, Section 3 (RP de ned) of this handbook.

26

term-DNF, there is a simple algorithm to PAC learn the class of k-term-DNF formulas using
a hypothesis class of k-CNF formulas.) Representation-independent hardness results meet
this more stringent goal. However, they depend on cryptographic (versus complexity theoretic) assumptions. Kearns and Valiant, 1989] gave representation-independent hardness results for learning several concept classes such as Boolean formulas, deterministic nite automata, and constant-depth threshold circuits (a simpli ed form of \neural networks"). These hardness results are based on assumptions regarding the intractability of various cryptographic schemes such as factoring Blum integers and breaking the RSA function.
5.1 Prediction-Preserving Reductions
Given that we have some representation-independent hardness result (assuming the security of various cryptographic schemes) one would like a \simple" way to prove that other problems are hard in a similar fashion as one proves a desired algorithm is intractable by reducing a known NP-complete problem to it9. Such a complexity theory for predictability has been provided by Pitt and Warmuth, 1990]. They formally de ne a prediction preserving reduction from con-
cept class C over domain X to concept class C0 over domain X 0 (denoted by C C0) that allows an e cient learning algorithm for C0 to be used to obtain any e cient learning algorithm for C. The requirements for such a prediction-preserving reduction are: (1) an e cient instance transformation g from X to X 0, and (2) the existence of an image concept. The instance
transformation g must be polynomial time computable. Hence if g(x) = x0 then the size of x0
must be polynomially related to the size of x. So for x 2 Xn, g(x) 2 Xp(n) where p(n) is some
polynomial function of n. It is also important that g be independent of the target function. We
now de ne what is meant by the existence of an image concept. For every f 2 Cn there must exist some f0 2 Cp0(n) such that for all x 2 Xn, f(x) = f0(g(x)) and the number of bits to represent f0
is polynomially related to the number of bits to represent f.
As an example, let C be the class of DNF formulas over X = f0 1gn, and C0 be the class of monotone DNF formulas over X 0 = f0 1g2n. We now show that C C0. Let y1 : : : yn be
9See Chapter 34 of this handbook for background on reducibility and completeness.
27

the variables for each concept from C. Let y10 : : : y20n be the variables for C0. The intuition
behind the reduction is that variable yi for the DNF problem is associated with variable y2i;1 for the monotone DNF problem. And variable yi for the DNF problem is associated with variable
y2i for the monotone DNF problem. So for example x = b1b2 bn where each bi 2 f0 1g and g(x) = b1(1 ; b1)b2(1 ; b2) bn(1 ; bn). Given a target concept f 2 C, the image concept f0 is
obtained by replacing each non-negated variable yi from f with y20i;1 and each negated variable yi from f with y20i. It is easily con rmed that all required conditions are met.
If C C0, what implications are there with respect to learnability? Observe that if we are given a polynomial prediction algorithm A0 for C0, one can use A0 to obtain a polynomial prediction algorithm A for C as follows. If A0 requests a labeled example then A can obtain a labeled example x 2 X from its oracle and give g(x) to A0. Finally, when A0 outputs hypothesis h0, A can make a prediction for x 2 X using h(g(x)). Thus if C is known not to be learnable then C0 also is not learnable. Thus the reduction given above implies that the class of monotone
DNF formulas is just as hard to learn in the PAC model as the class of arbitrary DNF formulas. Equivalently, if there were an e cient PAC algorithm for the class of monotone DNF formulas, then there would also be an e cient PAC algorithm for arbitrary DNF formulas.
Pitt and Warmuth, 1990] gave a prediction preserving reduction from the class of boolean formulas to class of DFAs. Thus since Kearns and Valiant, 1989] proved that boolean formula are not predictable (under cryptographic assumptions), it immediately follows that DFAs are not predictable. In other words, DFAs cannot be e ciently learned from random examples alone. Recall that any algorithm to exactly learn using only equivalence queries can be converted into an e cient PAC algorithm. Thus if DFAs are not e ciently PAC learnable (under cryptographic assumptions), it immediately follows that DFAs are not e ciently learnable from only equivalence queries (under cryptographic assumptions). Contrasting this negative result, recall that DFAs are exactly learnable from membership and equivalence queries Angluin, 1987], and thus are PAC learnable with membership queries.
Notice that for C C0, the result that an e cient learning algorithm for C0 also provides an e cient algorithm for C relies heavily on the fact that membership queries are NOT allowed. The
28

problem created by membership queries (whether in the PAC or exact model) is that algorithm
A0 for C0 may make a membership query on an example x0 2 X 0 for which g;1(x0) is not in X .
For example, the reduction described earlier demonstrates that if there is an e cient algorithm to PAC learn monotone DNF formulas then there is an e cient algorithm to PAC learn DNF formulas. Notice that we already have an algorithm Learn-Monotone-DNF that exactly learns this class with membership and equivalence queries (and thus can PAC learn the class when provided with membership queries). Yet, the question of whether or not there is an algorithm with access to a membership oracle to PAC learn DNF formulas remains one of the biggest open questions within the eld. We now describe the problem with using the algorithm Learn-Monotone-DNF to obtain an algorithm for general DNF formulas. Suppose that we have 4 boolean variables (thus
the domain is f0 1g4). The algorithm Learn-Monotone-DNF could perform a membership query on the example 00100111. While this example is in the domain f0 1g8 there is no x 2 f0 1g4 for
which g(x) = 00100111. Thus the provided membership oracle for the DNF problem cannot be used to respond to the membership query posed by Learn-Monotone-DNF.
We now de ne a more restricted type of reduction C wmq C0 that yields results even when
membership queries are allowed. For these reductions, we just add the following third condition
to the two conditions already described: for all x0 2 X 0, if x0 is not in the image of g (i.e. there is no x 2 X such that g(x) = x0), then the classi cation of x0 for the image concept must always be positive or always be negative. As an example, we show that C wmq C0 where C is the class of DNF formulas and C0 is the class of read-thrice DNF formulas (meaning that each literal can
appear at most three times). Thus learning a read-thrice DNF formula (even with membership queries) is as hard as learning general DNF formula (with membership queries). Let s be the number of literals in f. (If s is not known a priori the standard doubling technique can be
applied.) The mapping g maps from an x 2 f0 1gn to an x0 2 f0 1gsn. More speci cally, g(x)
simply repeats, s times, each bit of x. To see that there is an image concept, note that we have
s variables for each concept in C0 associated with each variable for a concept in C. Thus we can rewrite f 2 C as a formula f0 2 C0 in which each variable only appears once. At this point we have shown C C0 but still need to do more to satisfy the new third condition. We want to
29

ensure that the s variables (call them `01 : : : `0s) for f0 associated with one literal `i of f all take
the same value. We do this by de ning our nal image concept as: f0 ^ ;1 ^ ^ ;n where n is the number of variables and ;i is of the form (`01 ! `02) ^ ^ (`0s;1 ! `0s) ^ (`0s ! `01). This
formula evaluates to true exactly when `01 : : : `0s have the same value. Thus an x0 for which no g(x) = x0 will not satisfy some ;i and thus we can respond \no" to a membership query on x0. Finally, f0 is a read-thrice DNF formula. This reduction also proves that boolean formulas wmq read-thrice boolean formulas.
6 Weak Learning and Hypothesis Boosting
As originally de ned, the PAC model requires the learner to produce, with arbitrarily high probability, a hypothesis that is arbitrarily close to the target concept. While for many problems it is easy to nd simple algorithms (\rules-of-thumb") that are often correct, it seems much harder
to nd a single hypothesis that is highly accurate. Informally, a weak learning algorithm is
one that outputs a hypothesis that has some advantage over random guessing. (To contrast this sometimes a PAC learning algorithm is called a strong learning algorithm.) This motivates the question: are there concept classes for which there is an e cient weak learner, but there is no e cient PAC learner? Somewhat surprisingly the answer is no Schapire, 1990]. The technique used to prove this result is to take a weak learner and transform (boost) it into a PAC learner. The general method of converting a rough rule-of-thumb (weak learner) into a highly accurate
prediction rule (PAC learner) is referred to as hypothesis boosting10.
We now formally de ne the weak learning model Kearns and Valiant, 1989, Schapire, 1990].
As in the PAC model, there is the instance space X and the concept class C. Also the examples are drawn randomly and independently according to a xed but unknown distribution D on X . The learner's hypothesis h must be a polynomial time function that given an x 2 X returns a prediction of f(x) for f 2 C, the unknown target concept. The accuracy requirements for the
10We note that one can easily boost the con dence by rst designing an algorithm A that works for say = 1=2 and then running A several times taking a majority vote. For an arbitrary > 0 the number of runs of A needed are polynomial in lg 1= .
30

hypothesis of a weak learner are as follows. There is a polynomial function p(n) and algorithm A
such that, for all n 1, f 2 Cn, for all distributions D, and for all 0 < 1, algorithm A, given n, , and access to labeled examples from D, outputs a hypothesis h such that, with probability
1 ; , errorD(h) is at most (1=2 ; 1=p(n)). Algorithm A should run in time polynomial in n
and 1= .
If C is strongly learnable, then it is weakly learnable|just x = 1=4 (or any constant less
than 1=2). The converse (weak learnability implying strong learnability) is not at all obvious. In fact, if one restricts the distributions under which the weak learning algorithm runs then weak learnability does not imply strong learnability. Namely, Kearns and Valiant, 1989] have shown that under a uniform distribution, monotone boolean functions are weakly, but not strongly, learnable. Thus it is important to take advantage of the requirement that the weak learning algorithm must work for all distributions. Using this property, Schapire, 1990] proved the converse
result: if concept class C is weakly learnable, then it is strongly learnable.
Proving that weak learnability implies strong learnability has also been called the hypothesis boosting problem, because a way must be found to boost the accuracy of slightly-better-than-half hypotheses to be arbitrarily close to 1. There have been several boosting algorithms proposed since the original work of Schapire, 1990]. Figure 9 describes AdaBoost Freund and Schapire, 1996] that has shown promise in empirical use. The key to forcing the weak learner to output hypotheses that can be combined to create a highly accurate hypothesis is to create di erent distributions on which the weak learner is trained.
Freund and Schapire Freund and Schapire, 1996] have done some experiments showing that by applying AdaBoost to some simple rules of thumb or using C4.5 Quinlan, 1993] as the weak learner they can perform better than previous algorithms on some standard benchmarks. Also, Freund and Schapire have presented a more general version for AdaBoost for the situation in which the predictions are real-valued (versus binary). Some of the practical advantages of AdaBoost are that is is fast, simple and easy to program, requires no parameters to tune (besides T, the number of rounds), no prior knowledge is needed about the weak learner, it is provably e ective, and it is exible since you can combine it with any classi er that nds weak hypotheses.
31

AdaBoost

For i = 1 : : : m

For

tD=1(i1)

= ::

1=m :T

Call WeakLearn providing Calculate the error of ht :

it with t=

tX he distDribt(uit)ion

Dt

to

obtain

the

hypothesis

ht

If t Let

>
t

1=2, set T
= t=(1 ;

= t)

t

;

1

and

abi:ohrt(txti)h6=eyiloop

Update distribution Dt:

Let Zt be a normalization

If ht(xi) = yi Let Dt+1
Else Let Dt+1

= =

DZt(ti) DZt(ti)

t

constant

so

Dt+1

is

a

valid

probability

distribution

Output the hfin(x) =

nal hypothesis:

arg

my2aYx

X
t:ht(x)=y

log

1
t

Figure 9: The procedure AdaBoost to boost the accuracy of a mediocre hypothesis (created by WeakLearn to a very accurate hypothesis. The input is a sequence h(x1 y1) : : : (xm ym)i of labeled examples where each label comes from the set Y = f1 : : : kg.

32

7 Research Issues and Summary
In this chapter, we have described the fundamental models and techniques of computational learning theory. There are many interesting research directions besides those discussed here. One general direction of research is in de ning new, more realistic models, and models that capture di erent learning scenarios. Here are a few examples. Often, as in the problem of weather prediction, the target concept is probabilistic in nature. Thus Kearns and Schapire, 1990] introduced the p-concepts model. Also there has been a lot of work in extending the VC theory to real-valued domains (e.g. Haussler, 1992]). In both the PAC and on-line models, many algorithms use membership queries. While most work has assumed that the answers provided to the membership queries are reliable, in reality a learner must be able to handle inconclusive and noisy results from the membership queries. See Blum, Chalasani, Goldman, and Slonim, 1995] for a summary of several models introduced to address this situation. As one last example, there has been much work recently in exploring models of a \helpful teacher," since teaching is often used to assist human learning (e.g. Goldman and Mathias, 1996, Angluin and Krikis, 1997]).
Finally, there has been work to bridge the computational learning research with the research from other elds such as neural networks, natural language processing, DNA analysis, inductive logic programming, information retrieval, expert systems, and many others.
8 De ning Terms
Classi cation Noise A model of noise in which the the label may be reported incorrectly.
In the random classi cation noise model, with probability the label is inverted. In the malicious classi cation noise model, with probability an adversary can choose the label.
In both models with probability 1 ; the example is not corrupted. The quantity is
referred to as the noise rate.
Concept Class A set of rules from which the target function is selected.
Counterexample A counterexample x to a hypothesis h (where the target concept is f) is
either an example for which f(x) = 1 and h(x) = 0 (a positive counterexample) or for
33

which f(x) = 0 and h(x) = 1 (a negative counterexample).
Equivalence Query A query to an oracle in which the learner provides a hypothesis h and is
either told that h is logically equivalent to the target or otherwise given a counterexample.
Hypothesis Boosting The process of taking a weak learner that predicts correctly just over half
of the time and transforming (boosting) it into a PAC (strong) learner whose predictions are as accurate as desired.
Hypothesis Class The class of functions from which the learner's hypothesis is selected.
Membership Query The learner supplies the membership oracle with an instance x from the
domain and is told the value of f(x).
Occam Algorithm An algorithm that draws a sample S and then outputs a hypothesis con-
sistent with the examples in S such that the size of the hypothesis is sublinear in S (i.e. it performs some data compression).
On-Line Learning Model The learning session consists of a set of trials where in each trial, the learner is given an unlabeled instance x 2 X . The learner uses its current hypothesis
to predict a value p(x) for the unknown (real-valued) target and is then told the correct value for f(x). The performance of the learner is measured in terms of the sum of the loss over all predictions.
PAC Learning This is a batch model in which rst there is a training phase in which the learner sees examples drawn randomly from an unknown distribution D, and labeled according to the unknown target concept f drawn from a known concept class C. The learner's goal is to output a hypothesis that has error at most with probability at least 1; . (The learner
receives and as inputs.) In the proper PAC learning model the learner must output
a hypothesis from C. In the non-proper PAC learning model the learner can output any
hypothesis h for which h(x) can be computed in polynomial time.
Representation-Dependent Hardness Result A hardness result in which one proves that one cannot e ciently learn C using a hypothesis class of H. These hardness results typically
34

rely on some complexity theory assumption such as RP 6= NP.
Representation-Independent Hardness Result A hardness result in which no assumption
is made about the hypothesis class used by the learner (except the hypothesis can be evaluated in polynomial time). These hardness results typically rely on cryptographic assumptions such as the di culty of breaking RSA.
Statistical Query Model A learning model in which the learner does not have access to labeled
examples, but rather only can ask queries about statistics about the labeled examples and receive unlabeled examples. Any statistical query algorithm can be converted to a PAC algorithm that can handle random classi cation noise (as well as some other types of noise).
Vapnik-Chervonenkis (VC) Dimension A nite set S X is shattered by C if for each of the 2jSj subsets S0 S, there is a concept f 2 C that contains all of S0 and none of S;S0. In other words, for any of the 2jSj possible labelings of S (where each example s 2 S is either positive or negative), there is some f 2 C that realizes the desired labeling (see Figure 2). The Vapnik-Chervonenkis dimension of C, denoted as vcd(C), is the smallest d for which no set of d + 1 examples is shattered by C. Equivalently, vcd(C) is the cardinality of the largest nite set of points S X that is shattered by C.
Weak Learning Model A variant of the PAC model in which the learner is only required to output a hypothesis that with probability 1 ; , has error at most (1=2 ; 1=p(n)). I.e., it
does noticeably better than random guessing. Recall in the standard PAC (strong learning) model the learner must output a hypothesis with arbitrarily low error.
References
Aizenstein and Pitt, 1991] Aizenstein, H. and Pitt, L. 1991. Exact learning of readtwice DNF formulas. In Proc. 32th Annu. IEEE Sympos. Found. Comput. Sci. (1991). IEEE Computer Society Press, pp. 170{179.
35

Angluin, 1987] Angluin, D. 1987. Learning regular sets from queries and counterexamples. Inform. Comput. 75, 2 (Nov.), 87{106.
Angluin, 1988] Angluin, D. 1988. Queries and concept learning. Machine Learning 2, 4 (April), 319{342.
Angluin, 1990] Angluin, D. 1990. Negative results for equivalence queries. Machine Learning 5, 121{150.
Angluin, Frazier, and Pitt, 1992] Angluin, D., Frazier, M., and Pitt, L. 1992. Learning conjunctions of Horn clauses. Machine Learning 9, 147{164.
Angluin, Hellerstein, and Karpinski, 1993] Angluin, D., Hellerstein, L., and Karpinski, M. 1993. Learning read-once formulas with queries. J. ACM 40, 185{210.
Angluin and Krikis, 1997] Angluin, D., Krikis, M. 1997. Teachers, learners and black boxes. In Proc. 10th Annu. Conf. on Comput. Learning Theory (1997). ACM Press, New York, NY, pp. 285{297.
Angluin and Laird, 1988] Angluin, D. and Laird, P. 1988. Learning from noisy examples. Machine Learning 2, 4, 343{370.
Angluin and Smith, 1987] Angluin, D. and Smith, C. 1987. Inductive inference. In Encyclopedia of Arti cial Intelligence, pp. 409{418. J. Wiley and Sons, New York.
Anthony and Biggs, 1992] Anthony, M. and Biggs, N. 1992. Computational Learning Theory, Cambridge Tracts in Theoretical Computer Science (30). Cambridge Tracts in Theoretical Computer Science (30). Cambridge University Press.
Aslam and Decatur, 1997] Aslam, J. A. and Decatur, S. E. 1997. Speci cation and simulation of statistical query algorithms for e ciency and noise tolerance. Journal of Computer and System Sciences. To appear.
36

Blum, 1990] Blum, A. 1990. Separating distribution-free and mistake-bound learning models over the Boolean domain. In Proc. of the 31st Symposium on the Foundations of Comp. Sci. (1990). IEEE Computer Society Press, Los Alamitos, CA, pp. 211{218.
Blum, Chalasani, Goldman, and Slonim, 1995] Blum, A., Chalasani, P., Goldman, S. A., and Slonim, D. K. 1995. Learning with unreliable boundary queries. In Proc. 8th Annu. Conf. on Comput. Learning Theory (1995). ACM Press, New York, NY, pp. 98{107.
Blumer, Ehrenfeucht, Haussler, and Warmuth, 1987] Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. 1987. Occam's razor. Inform. Proc. Lett. 24, 377{380.
Blumer, Ehrenfeucht, Haussler, and Warmuth, 1989] Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. 1989. Learnability and the Vapnik-Chervonenkis dimension. J. ACM 36, 4, 929{965.
Bshouty, Goldman, Hancock, and Matar, 1996] Bshouty, N., Goldman, S., Hancock, T., and Matar, S. 1996. Asking questions to minimize errors. Journal of Computer and System Sciences 52, 2 (April), 268{286.
Bshouty and Mansour, 1995] Bshouty, N. H. and Mansour, Y. 1995. Simple learning algorithms for decision trees and multivariate polynomials. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science (1995). IEEE Computer Society Press, Los Alamitos, CA, pp. 304{311.
Cesa-Bianchi, Freund, Helmbold, Haussler, Schapire, and Warmuth, 1993] Cesa-Bianchi, N.,
Freund, Y., Helmbold, D. P., Haussler, D., Schapire, R. E., and Warmuth, M. K.
1993. How to use expert advice. In Proc. 25th Annu. ACM Sympos. Theory Comput. (1993). ACM Press, New York, NY, pp. 382{391. Expanded version in Univ. of Calif. Computer Research Lab TR UCSC-CRL-94-33, From Santa Cruz, CA.
Devroye, Gyor , and Lugosi, 1996] Devroye, L., Gyorfi, L., and Lugosi, G. 1996. A Probabalistic Theory of Pattern Recognition, Applications of Mathematics. Applications of Mathematics. Springer-Verlag, New York.
37

Duda and Hart, 1973] Duda, R. O. and Hart, P. E. 1973. Pattern Classi cation and Scene Analysis. Wiley.
Ehrenfeucht and Haussler, 1989] Ehrenfeucht, A. and Haussler, D. 1989. A general lower bound on the number of examples needed for learning. Inform. Comput. 82, 3 (Sept.), 247{251.
Freund and Schapire, 1996] Freund, Y. and Schapire, R. 1996. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning (1996). Morgan Kaufmann, pp. 148{156.
Gold, 1967] Gold, E. M. 1967. Language identi cation in the limit. Inform. Control 10, 447{474.
Goldman and Mathias, 1996] Goldman, S. and Mathias, D. 1996. Teaching a smarter learner. J. of Comput. Syst. Sci. 52, 2 (April), 255{267.
Goldman and Sloan, 1995] Goldman, S. and Sloan, R. 1995. Can PAC learning algorithms tolerate random attribute noise? Algorithmica 14, 1 (July), 70{84.
Goldman and Scott, 1996] Goldman, S. A. and Scott, S. D. 1996. A theoretical and empirical study of a noise-tolerant algorithm to learn geometric patterns. In Proceedings of the Thirteenth International Conference on Machine Learning (1996). Morgan Kaufmann, pp. 191{199.
Haussler, 1992] Haussler, D. 1992. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Inform. Comput. 100, 1 (Sept.), 78{150.
Haussler, Kearns, Littlestone, and Warmuth, 1991] Haussler, D., Kearns, M., Littlestone, N., and Warmuth, M. K. 1991. Equivalence of models for polynomial learnability. Inform. Comput. 95, 2 (Dec.), 129{161.
Haussler, Littlestone, and Warmuth, 1994] Haussler, D., Littlestone, N., and War-
muth, M. K. 1994. Predicting f0 1g functions on randomly drawn points. Inform.
Comput. 115, 2, 284{293.
38

Kearns, 1993] Kearns, M. 1993. E cient noise-tolerant learning from statistical queries. In Proc. 25th Annu. ACM Sympos. Theory Comput. (1993). ACM Press, New York, NY, pp. 392{401.
Kearns and Valiant, 1989] Kearns, M. and Valiant, L. G. 1989. Cryptographic limitations on learning Boolean formulae and nite automata. In Proc. of the 21st Symposium on Theory of Computing (1989). ACM Press, New York, NY, pp. 433{444.
Kearns and Vazirani, 1994] Kearns, M. and Vazirani, U. 1994. An Introduction to Computational Learning Theory. MIT Press, Cambridge, Massachusetts.
Kearns and Schapire, 1990] Kearns, M. J. and Schapire, R. E. 1990. E cient distribution-free learning of probabilistic concepts. In Proc. of the 31st Symposium on the Foundations of Comp. Sci. (1990). IEEE Computer Society Press, Los Alamitos, CA, pp. 382{391.
Kearns, Schapire, and Sellie, 1994] Kearns, M. J., Schapire, R. E., and Sellie, L. M. 1994. Toward e cient agnostic learning. Machine Learning 17, 2/3, 115{142.
Kodrato and Michalski, 1990] Kodratoff, Y. and Michalski, R. S., Eds. 1990. Machine Learning: An Arti cial Intelligence Approach, vol. III. Morgan Kaufmann, Los Altos, California.
Littlestone, 1988] Littlestone, N. 1988. Learning when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning 2, 285{318.
Maass and Turan, 1992] Maass, W. and Turan, G. 1992. Lower bound methods and separation results for on-line learning models. Machine Learning 9, 107{145.
Maass and Warmuth, 1995] Maass, W. and Warmuth, M. K. 1995. E cient learning with virtual threshold gates. In Proc. 12th International Conference on Machine Learning (1995). Morgan Kaufmann, pp. 378{386.
Natarajan, 1991] Natarajan, B. K. 1991. Machine Learning: A Theoretical Approach. Morgan Kaufmann, San Mateo, CA.
39

Pitt and Valiant, 1988] Pitt, L. and Valiant, L. 1988. Computational limitations on learning from examples. J. ACM 35, 965{984.
Pitt and Warmuth, 1990] Pitt, L. and Warmuth, M. K. 1990. Prediction preserving reducibility. J. of Comput. Syst. Sci. 41, 3 (Dec.), 430{467. Special issue of the for the Third Annual Conference of Structure in Complexity Theory (Washington, DC., June 88).
Quinlan, 1993] Quinlan, J. R. 1993. C4.5: Programs for machine learning. Morgan Kaufmann.
Rosenblatt, 1958] Rosenblatt, F. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psych. Rev. 65, 386{407. (Reprinted in Neurocomputing (MIT Press, 1988).).
Schapire, 1990] Schapire, R. E. 1990. The strength of weak learnability. Machine Learning 5, 2, 197{227.
Sloan, 1988] Sloan, R. 1988. Types of noise in data for concept learning. In Proc. 1st Annu. Workshop on Comput. Learning Theory (San Mateo, CA, 1988). Morgan Kaufmann, pp. 91{96.
Valiant, 1984] Valiant, L. G. 1984. A theory of the learnable. Commun. ACM 27, 11 (Nov.), 1134{1142.
Valiant, 1985] Valiant, L. G. 1985. Learning disjunctions of conjunctions. In Proceedings of the 9th International Joint Conference on Arti cial Intelligence, vol. 1 (Los Angeles, California, 1985). International Joint Committee for Arti cial Intelligence, pp. 560{566.
Vapnik and Chervonenkis, 1971] Vapnik, V. N. and Chervonenkis, A. Y. 1971. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probab. and its Applications 16, 2, 264{280.
40

9 Further Information
Good introductions to computational learning theory (along with pointers to relevant literature) can be found in such textbooks as Kearns and Vazirani, 1994], Natarajan, 1991], and Anthony and Biggs, 1992]. Many recent results can be found in the proceedings from the following conferences: ACM Conference on Computational Learning Theory (COLT), European Conference on Computational Learning Theory (EuroCOLT), International Workshop on Algorithmic Learning Theory (ALT), International Conference on Machine Learning (ICML), IEEE Symposium on Foundations of Computer Science (FOCS), ACM Symposium on Theoretical Computing (STOC), and Neural Information Processing Conference (NIPS).
Some major journals in which many learning theory papers can be found are: Machine Learning, Journal of the ACM, SIAM Journal of Computing, Information and Computation, and Journal of Computer and System Sciences. See Kodrato and Michalski, 1990] for background information on machine learning, and see Angluin and Smith, 1987] for a discussion of inductive inference models and research.
41


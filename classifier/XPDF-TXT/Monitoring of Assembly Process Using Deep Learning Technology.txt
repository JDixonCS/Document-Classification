sensors
Article
Monitoring of Assembly Process Using Deep Learning Technology
Chengjun Chen 1,2,*, Chunlin Zhang 1,2, Tiannuo Wang 1,2, Dongnian Li 1,2, Yang Guo 1,2, Zhengxu Zhao 1,2 and Jun Hong 3
1 School of Mechanical and Automotive Engineering, Qingdao University of Technology, Qingdao 266520, China; zhchl123654@163.com (C.Z.); goodtn1993@163.com (T.W.); dongnianli@qut.edu.cn (D.L.); guoyang@qut.edu.cn (Y.G.); Zhaozhengxu@qut.edu.cn (Z.Z.)
2 Key Lab of Industrial Fluid Energy Conservation and Pollution Control, Ministry of Education, Qingdao University of Technology, Qingdao 266520, China
3 School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an 711049, China; jhong@mail.xjtu.edu.cn * Correspondence: chencj@qut.edu.cn; Tel.: +86-532-6805-2755
Received: 18 June 2020; Accepted: 23 July 2020; Published: 29 July 2020
Abstract: Monitoring the assembly process is a challenge in the manual assembly of mass customization production, in which the operator needs to change the assembly process according to different products. If an assembly error is not immediately detected during the assembly process of a product, it may lead to errors and loss of time and money in the subsequent assembly process, and will affect product quality. To monitor assembly process, this paper explored two methods: recognizing assembly action and recognizing parts from complicated assembled products. In assembly action recognition, an improved three-dimensional convolutional neural network (3D CNN) model with batch normalization is proposed to detect a missing assembly action. In parts recognition, a fully convolutional network (FCN) is employed to segment, recognize different parts from complicated assembled products to check the assembly sequence for missing or misaligned parts. An assembly actions data set and an assembly segmentation data set are created. The experimental results of assembly action recognition show that the 3D CNN model with batch normalization reduces computational complexity, improves training speed and speeds up the convergence of the model, while maintaining accuracy. Experimental results of FCN show that FCN-2S provides a higher pixel recognition accuracy than other FCNs.
Keywords: monitoring of assembly process; assembly action recognition; segmentation of assembled products; 3D CNN; batch normalization; fully convolutional network

1. Introduction
During the assembly process of product, if an assembly error is not immediately detected, it may lead to errors and loss of time and money in the subsequent assembly process, and will affect product quality. Using computer vision technology to monitor the assembly process can reduce operating costs, shorten product production cycles, and reduce the rate of defective products. This is especially the case for mass customization production, when assembly lines are often restructured to produce different products. In a changing production environment, assembly quality can often be affected by missed operation steps or by irregular operations of workers. This paper considers the use of computer vision-based monitoring of assembly process, with the aim of quickly and accurately recognizing the assembly action of the workers, and recognizing different parts from complicated assembled products. In this way, assembly efficiency and quality of the products can be improved. Therefore, the research issues of this paper include assembly action recognition and parts recognition from complicated assembled products.

Sensors 2020, 20, 4208; doi:10.3390/s20154208

www.mdpi.com/journal/sensors

Sensors 2020, 20, 4208

2 of 18

ASenssosrhs o20w20n, 20in, x FFOigRuPrEeER1,RtEhVeIEWassembly action recognition process falls within the resear2choffi18eld of human action recognition. Existing human action recognition methods are mainly divided into two
categoriesAas cschoorwdnining Ftoigudrieff1e,rethnet afsesaetmubrelyeaxcttrioanctrieocnogmniettiohnodpsro: caerstsififacllisalwfiethaitnurtheeerxetsreaacrtcihonfiealdndofdeep learnihnugm. aInn aarcttiiofinciraelcfoegantuitrioene.xEtrxaiscttiinognh, aucmtiaonnafcetaiotunrreescoogf nimitiaognems oerthvoiddseoarferammaienslyardeievxidtreadcitnetdo, tawnod then classicfiaetedgoursieins gacdcoiffrdeirnegnttokidnidffesroenf tclfaeastsuifireeresx.trFacotrioenxammetphloed,sB: oabrtiicfikciealt faela.t[u1r]eperxetsraecntitoend aanvdiedwee-pbased approleaacrhnitnog.thInearretpifirceisael nfetaattuiroeneoxtfraaccttiioonn,.acFtiirosntlfye,amturoetsioonf iemnaegregsyomr vaidpesoofframmoetsioanrefeexattruacrteesda, arendcathlceunlated, and thcalpeanpssrmiofiaeocdthioutnsoicnltgahsedsiifrfifeecpraertenisotenknitinasdtpisoenroffoocrflmaasecsditfiiobenrys..mFFiaortsrctlheyxi,namgmwpolteiito,hnBsotebonirceekrdgeyatcmatilo.apn[1st]eopmfrepmsleaonttteieosd.nWafeevaiinteuwlare-nbsdasaeertdeal. [2] introdcaulccueldatMedo, tainodn tHheisntomroytiVoonluclmasesif(iMcaHtioVn)iasspaecrftoiormn efedabtuyrme afotcrhaincgtiownitrhesctoogrenditiaocnti.onThteims fpelaatteusr.e can avoidWtheienliannfldueetnacl.e[2o]fidntirffoedruecnetdhMumotiaonn bHoisdtyorfyeVatouluremseo(nMaHcVtio) ansraecctioognnfietaiotunr.eDfoarlaacltieotnarle. c[o3g] nuistieodn.HOG featuTrehsistofedateusrcericbaenhauvmoiadnthfeatinufrleusenacnedocfladsifsfiefryentht ehmumwanithboSdVyMf.eaCtuhraeus dohnryacettioanl. r[e4c]ogrenpitrioense. nted
each Dfraalmal eetofaal. v[3id] euoseudsiHngOGa Hfeiasttuorgersatmo odfesOcrriibeentheudmOanptfiecaatluFrelsowan(dHcOlaOssFif)yanthdemrecwoigthnizSVe Mhu. man
actionCshbauydchlarysseitfyailn. [g4]HrOepOreFsetnimtede-seeacrihesfr.aSmcehuofldatveitdaelo. [u5s]incgonasHtriustcotgerdamvidoef oOreiepnrteesdeOntpattiicoanl sFlionwterms of loc(aHlOspOaFc)ea-ntdimreecofegantiuzerehsuamnadn ianctteiognrsabtey sculacshsifryeipngreHseOnOtaFttiiomnes-sweriitehs.SSVchMulcdltaestsaifil.c[5a]ticoonnsstcruhcetmedes for recogvniidteioonr.epWreasnegnteattiaoln.s[6in] itnertmrosdoufcleocthalesIpDaTce-atligmoeriftehamtu.reTshaendIDinTteaglgraotreitshumch irsepcurersreennttaltyiownsidweiltyh used recogSnVitMionclaaslsgifoicraitthiomn sbcahseemdeos nfoarrrteificocgianlitdioens.igWnafnegateut raels. .[6T]hinetarolgdourcieththme IuDsTesadlgeonrsitehmtr.ajTehcetoIrDieTs and motioaanllggoobrroiittuhhnmmduaissreycsudrdreeesnncstrelyiptrwtaojierdcsetlotyoriuersseepadnredrsemeconogttnivoitniidobenooualfngedoaartriutyhrmedsesbfcoarsripeadtocrotsinotnoarrrteiefpcicroieagslnenditteisoviingdn.eofAefarettauitfiruecrsie.aslTfhfoeerature extraacctitoionn-breacsoegdniaticotnio. nArrteifcicoigalnfietiaotunremeextthraocdtisonu-sbuasaeldlyarcetiqounirreecoagcnoitmionplmexetdhoadtas upsrueaplrlyocreesqsuiinreg astage, and rceocmogplneixtiodnataacpcruerparcoyceassnindgeffistacgiee,nacnydcarencobgensitiigonnifiacccaunrtalcyyinaflndueenffciecidenbcyy fceaantubree ssieglneicfitciaonnt.lyDeep learniinnfglu-benacseeddbmy efetahtoudres seenleacbtiloena. dDaepeptivleearfneiantgu-rbeasledarmnienthgowdsitehnasbimlepadleapdtaivtae pferaetpurreocleeasrsniinngg awnitdhhave
recenstilmy pbleeednadtaepvreelporpoecdesasnindg uanseddhainvethreecaenretlayobfeecnomdepvuetloepr evdisainodn.uCsehdeinn ethteaal.re[7a]osftcuodmiepdutreercvoigsinointi.on of
repetCitihveenaestsaelm. [7b]lystuadctiieodnrsectogmniotinointoorf trheepeatsitsievme abslsyemprbolyceascstiofnswtormkeornsitaonrdthperaesvsemntbalysspermocbelsys qofuality problwemorskcearsusaendd bpyreivrreengtualsasremopbelyraqtiuoanliotyf wproorbkleemrss. TcahueseYdObLyOivrr3eaglugloarritohpmera[t8i]onis oafppwloierkdetros.juTdhege the assemYbOlyLOtovo3lsalagnodritrhemco[g8n] iizseapthpeliewdotrokejurdsgeastsheemabssleymabcltyiotno.olTshaendporseecoegsntiimzeatthioenwaolrgkoerristhamsseCmPbMly [9] is emplaocytieodn.toThreecpoogsne iezsetimthaetihounmalagnorjiothinmt CwPhMich[9]isissuembspelqoyueednttloyruecsoegdnitzoejuthdegheutmhaenojpoeinrtatwinhgichtimis es of repetistthuievbseperaqosubseleenmmtlybolufysaaesdcsteitmoonbjusly.dgTaechttiihsoenpoareppceeorrgaatnidintdgiortneimsbsaeessseodtfhoreenppderetoiebtiplveelmeaasrosnfeimnagsbs,leaymnadcbtpliyoronapsc.otTisoehnsisarpenaceopugerrnaailtdniodenrtwebsosaersksed on deepmleoadrnelinfogr,aasnsedmpbrloypaocstieosnarenceougrnaitlionnetawnodrmkomniotdoreilnfgo. r assembly action recognition and monitoring.

FiFgiguurere11.. AAsssseemmbbllyyaacctitoion.n.
In mIanssmcaussstcoumstoizmaitzioatnio, na, awwidiederarannggee ooff ppeerrssoonnaalilzizeeddpprordoudcutsc,tsw, iwthitlahrglaerdgieffedrieffnecreesnicnetshein the assemafrsboslmeymptbhrleoyacpserssoesmc,eabsrslye, aspreerqopudreuondccuee,dcme.diT.shsTiehnregerpfeofaorrretes,,,amannisaaasslssigeemnmebbdllypymamrotsoninistiontroeinreidgnemgd.emtThehotedhreotfhdoartteh,dapettaedrcteststredeccetovsgiadnteiitovioninastions fromitshneecaesssseamrybfloyr smeoqnuietonrcine,gmasissesminbglypparrotcse, sms.iCsaolmigpnuetderpvaisritosni-sbnaseeeddaesdse. mThbleyremfoonreit,oprianrgtsisrekceoygtonition is necimespsraorvyinfgorthmeoenffiitcoiernincyg aansdsetmheblqyuaplritoyceosfsm. aCnoumalpaussteemr vbliys.ioKnim-baesteadl. apsrsoepmosbeldy amvoisniiotno-rbiansgedis key to imspyrsotevminfgorthmeoenffiitocriienngctyheanbldoctkheasqseumalbitlyy ionfsmhiapnbuuailldainssge. mThbeliyr.sKysitmemectaanl.epxtrroapctoasreedasaovf ibslioocnk-sb; ased systetmhefoexrtmracotneditobrlioncgksthareebtlhoecnkiadsesnetimfiebdlyanindschoimppbaureilddiwnigth. TChAeDir dsaytsateinmacnaenffeoxrtrtaoctesatrimeaasteofthbelocks; the exastsreamctbeldy bplroocgkressasre[10th].eAnlitdhoeungtihfiethdeaanbdovceommepntairoendedwrietsheaCrcAhDredaalitzaesinthaenmeffonoirttortiongesotifmthaete the assemabssleympbrloygprerossgr[e1s0s],.iAt dltoheosungoht pthroevaibdoevpeamrternectioognneitdiorne,speaarrtchporesiatlioizneisngthaenmd oasnsietomrbinlygroefcothgeniatisosnembly progrienssth, eit odvoeersalnl optropcreosvs.idZeidpeakrtetreacl.o[g1n1i]ticoonn,dpuacrtetdpoexsipteiorinminegntasnrdegaasrdseinmgbtlhyeruecsoe gonf ictioonnvoinlutthioenoalverall procensesu. rZalidneektweotrakls.([C1N1]Nc)otnodauchciteevdeeaxrpoebruismt iednetnstirfeicgaatirodninogf stthaneduasred oasfsceomnbvloylpuatriotsn(asul cnheausrasclrnewetsw, orks (CNNnlo)ucttaost)eaadcnhwdieiftvehaeintuaarregosrb.oHuusoptwoidfevoeenvret,irfitlhacpeapteiixonpngerpoiafmrstetsnaatnsnddsahfroodwr asthshsinaetymtshbuelryafappcpaerrsotwsach(hsiucfhacihslhsaotsowsdcreretefelwectcstp,ioannrutsst.swA) shanischdhofawernaetures. HowienvFeirg, uthree2e,xdpifeferirmenetnfrtosmshtohewidthenattiftihcaetiaopnporfosaccahttefraeidlsptaortds ebteefoctrepaasrstesmwblhinicgh, tahreealsosecmatbeldy wofiathin a group of overlapping parts and for shiny surfaces which show reflections. As shown in Figure 2, different from the identification of scattered parts before assembling, the assembly of a product usually

Sensors 2020, 20, 4208

3 of 18

Sensors 2020, 20, x FOR PEER REVIEW

3 of 18

wcohnitcahpdinrubsoerdmitnuougcotlstcuidcpsliuulffieasilpcolyunaclr,totwisnehwtsaictihonhiscbdmhreiutnoelgvtcsietprdalleiafwpfpiacheuroatlsclteihwesphotaitochrhtdeofrer.vtoeSecmrotlmaaapwecephoacomahlreptospltehaaxerrtera.fosSrosnoemmlymeapbpcalaoyrrm.ttlsypalereexxpoanoslssyeemdpabdrltuyly.e etxopoocsceldusion,

FigFuirgeur2e. 2P.aPratrstsrerceocoggnniittiioonn ffrroommaasssseemmbbleldedprpordoudcutsc.ts.
The mThaeinmmaiontmivoattiivoantioonf othf itshipsappaeprerisistotommoonniittoorr tthheeaasssseemmblbylyprporcoecssesbsybryecroegcnoizginnigzainssgemasbsleymbly actionacocatninotrdnibuarenticdoongrsenociofzgtihnneigzpinpregasreptnsatrfstsrtuofdmryomacroecmoamspflpoicllilacotawetedsd: aasssseemmbblleedd pprroodduuctcst.s.ThTehmeaminaiinnnoinvnatoiovnastioannds and contributions of the present study are as follows:
(1) We propose a three-dimensional convolutional neural network (3D CNN) model with batch (1) We pnroorpmoasliezaatiothnretoe-dreimcoegnnsizioenaaslsecmonbvlyoluacttiioonnas.l nTehue raplronpeotswedork3D(3CDNCNNmNo)dmelowdeitlhwbiathtchbatch
normnaolrimzaatliioznattioonrecoangneifzfecatsivsemlybrleydauccteiotnhse. Tnuhme pbreorpoofsetrdai3nDinCgNpNarammoedteerlswainthdbiamtcphronvoirnmg atlhizeation can ecfofencvteirvgeelnycreesdpueceed.the number of training parameters and improving the convergence speed. (2) T(2h) e Tfuhellyfuclloyncvoonlvuotliuotnioanl anl entewtworokrkss (F(FCCNN)) iiss eemmppllooyyededfofrorsesgemgemnteinngtindgiffderiffenetrepnatrtspafrrotsmfrom compcfroloimmcaptcleiocdmatpaedlsisceaasmtesedbmlaebsdsleedmprbpolredodudpcutrc.ot.dAAufcfttteesrrisppcaaorrnttsdsussecetggemdmetneontcathtaieotcinko,ntth,heetharseesceromegcbnoligytinoseintqiuooefnndcoieffffedorireffnmetirspesaninrttgsparts fromocrommispalliicganteedd paasrstesm. Abslefdarparsowduecktsnoiswc,owneduarcetetdhetofircshtetcokatphpelyasdseepmthbliymsaegqeuseengmceenfotartmionissing or mtiescahlnigonloegdy ptoatrhtes.apApslicfaatrioans owf emkonniotowri,nwg eofaarsesetmheblfiyrpsrtotcoesas.pply depth image segmentation technology to the application of monitoring of assembly process.
This paper is organized as follows: Section 2 summarizes the state of the art. Section 3 outlines a
Tnheuisraplanpeetrwiosrokrmgaondiezlefdorasasfsoelmlobwlys:rSeceocgtinointio2n.suSmectmioanri4zedsetshcreibsetsatteheofFtChNe aermt.pSleocyteidonfo3r othuetlines a neusreaml annettiwc soergkmmenotadteiolnfoorf aasssseemmbbleldy prreocdougcntsit.iSoenc.tioSnec5teioxpnla4indeedstchreibpersoctehses FofCcNreaetmingpldoaytaedseftso.r the semanExtipcesreimgmenetsntaantdionanoaflyassessemthabtleddemproondsturacttes. tSheecetifofenct5iveexnpeslsainanedd tehffeicpiernoccyesosf oofucrremaetitnhogddaartea sets. ExperpimroevnidtsedanindSaenctailoynse6s. Stehcattiodne7mcoonnstatrinasteotuhreceoffneclcutsivioennseasnsdanfudtuerffie cwieonrkc.y of our method are provided
in Sec2t.ioRnel6a.teSdeWctioornk 7 contains our conclusions and future work.
2. RelatedWWithoirnk the research field of human action recognition based on deep learning, Feichtenhofer et al. [12] proposed a convolutional two-stream network fusion for video action Wreictohginnitthioenr,efsuesainrcghCfoienlvdNoefthtuowmearns baocttihosnpraetciaolglynaitniodntebmaspeodraollny.dAeespinlgelaerfnrianmge, Fiseuicshetdenashtohfeerineptuatl. [12]
propotsoedthae csopnatviaollustrioeanmal tCwoon-vsNtreeta,mwhnieletwmourkltif-ufrsaimone fooprtviciadlefoloawctiosnthreeciongpnuittitont,hfeutseinmgpCoroanl vstNreeatmto.wers
both sTphaetitawlolystarnedamtsemarpeofruasleldy. bAy asi3nDglfeiltferramthaet iiss uabsleedtoasleathrne cionrpreustptoondthenecesspabteitawlesetnretahme hCigohnlyvNet, whileambsutrlatic-tffreaamtuereospotfictahleflspoawtiaisl stthreeaimnpauntdttohethteemtepmorpaol rsatrlesatmre.aTmhi.sTmheethtowdohsatsreaahmigshaarcecufurasceyd, bbuyt a 3D filter bthecaatuisseaobfltehteonleeeadrtnoceoxrtrraecstpthoenodpetniccaelsflboewtwcheaernactthereishtiicgsholfythaebvstidraecotinfeaadtuvarnesceo, ftrtahineinspgaistisalol wst,ream and tahnedtethme pmoertahlodstirsenamot .suTithaibslemfoerthloondg-htiamseavhidiegohfraacmcuesr.acy, but because of the need to extract the opticaTlSflNoWwneatcnwhgoaerrktaacilts.e[er1si3tsa]tibpclrsiosohpfeodstheodenavttihedemebopaoisnreaaol dsfetvgwamone-csnettr, entareamtwincoionrnkgv(oiTslSusNtlio)owfnoa,rlavnniedduetroha-lbenamseetewdthoacrotkdi.oIninsrnaedcoodtgistnuioiitntioatnbo.le for long-tuisminegvoipdteicoalfrfalomwegsr.aph as input, TSN uses RGB difference and warped optical flow graph as input.
WTraanngeet taal.l.[1[143] ]pprorpoopsoesdeda aC3teDm(pCoornavlosluegtimoneanl t3nDe)tawpoprrkoa(cThSNfo)r fsopratvioidteemo-pboarsaeldfeaactutiroenleraercnoignngition. TSN nuesitnwgodrekepistehsretaeb-dliismheendsioonnatlhceonbvaosleutoiofntwal on-esutrraelanmetwcoonrkvso(l3uDtioCnNaNl n) [e1u5r].aTl hniestmweotrhko.dIins saidmdpiltei,on to usingaonpdteicaasyl fltootwraignraanpdhuasse.inDpuuett,aTl.S[N16]upsreospRoGseBd adirffeceurrernecnet paonsde-wataternpteiodnonpetwicoarlkfl(oRwPAgNra).pRhPaAsNinput.
Tran eistaanl.en[1d4-t]op-ernodproesceudrreantCn3eDtw(oCrko.nTvhoislumteiothnoadl u3sDes) tahpeproostaucrhalfaottresnptiaotniomtecmhapnoirsaml afnedatcuarnelelearanrning usingsdomeeephtuhmreaen-dfeiamtuernessiobnyaslhcaorinnvgopluartaiomneatel rnseounrahlunmeatwn joorikntss.(3TDheCnNthNes)e[1fe5a]t.uTrehsisarme efetdhoindtoistshiemple, and eaagsgyretogattriaoinnlaaynedr utoseco. nDsturuecttathl.e[1p6o]stpurroepcoorsreedlaatiornecrueprrreensetnptaotsioen-aftoterntetmiopnonraeltwmootrikon(RmPoAdNeli)n. gR.PAN is an Denonda-thou-een[1d7]repcruoprroesnedt naeltowngo-rtker.mThreicsumrsievtehocodnuvosleustitohnenpeotwstourrka(lLaRtCteNn)t.ioInn tmhiescmhaondiesl,mCNanNd can learnfseoatmureesheuxmtraacntefdeaintutriemsebsyeqsuheanrciengarepaursaemd easteirnspount ohfuLmSTaMn jnoeitnwtso.rkT, hwehnichthceasne pferoactuesrsestimaree fed
into the aggregation layer to construct the posture correlation representation for temporal motion
modeling. Donahue [17] proposed a long-term recursive convolution network (LRCN). In this model,

Sensors 2020, 20, 4208

4 of 18

CNN features extracted in time sequence are used as input of LSTM network, which can process time information better. Xu et al. [18] presented a region convolutional 3D network (R-C3D) model. R-C3D firstly extracts features from the network by using the features of C3D network, then obtains the time regions that may contain activities according to C3D features, and finally obtains the actual activity types in the regions according to C3D features and recommended areas. R-C3D can handle any length of video input.
Human action recognition is widely studied, but there are few existing studies relating to assembly action recognition in industry, and there is no public data set for industrial assembly action. Assembly action recognition therefore requires higher recognition accuracy, higher recognition speed, and good adaptability to the working environment, such as changes in light or texture. Most of the traditional artificial feature extraction methods are problematic because of complicated preprocessing, low speed, and poor stability, so are unsuitable for industrial applications.
Common action recognition models based on deep learning include the LSTM-based LRCN model [17], the two-stream convolutional model [10], and the C3D model [14]. The recognition accuracy of these three models on the public data set UCF-101 [19] is similar, but the C3D model is fastest, reaching 313 fps [14], while the two-stream convolutional model is 1.2 fps [12]. This result is due to the fact that the C3D model is relatively simple in its data preprocessing and network structure; the two-stream convolutional model not only needs to process image sequence but also needs to extract optical flow information, slowing it down. Due to the difficulty of the parallelism required for the RNN network, the LRCN model is also slow. Using a 3D CNN for assembly action recognition has the advantages of simple data preprocessing, fast training speed and high recognition accuracy, making it more suitable for industrial field applications.
It is difficult to avoid the influence of changes in illumination intensity on recognition accuracy when simply using the 3D CNN to process RGB video sequences. Under the complex production environment of a factory, it is important to lessen the effect of the environment and improve recognition speed. This paper therefore considers the effects of depth image, binary image and gray image on training speed and accuracy. A 3D CNN model based on the dimensional transformation of single-channel gray video sequences is designed. In addition, the 3D CNN model is improved by introducing the batch normalization layer [20] into the model, which improves the performance of the neural network. Neither single-channel gray images nor three-channel RGB images can affect the understanding of the motion, and gray images can reduce the sensitivity of the model to different illumination conditions. The improved model is shown to effectively reduce the number of training data parameters and to accelerate the convergence and training speeds of the model, while maintaining accuracy.
Semantic segmentation or image segmentation is a computer vision method judging to which object each pixel in image belongs. Shotton et al. [21] mapped the difficult pose estimation problem into a simpler per-pixel classification problem, and a depth comparison feature is presented and used to represent features of each pixel in depth image. Joo et al. [22] proposed a method to detect the hand region in real-time using the feature of depth difference. Long et al. [23] presented fully convolutional networks (FCN) for end-to-end semantic segmentation. FCN has become the cornerstone of deep learning to solve the segmentation problem. Ronneberger et al. [24] presented a U-Net network, which includes an encoding network that extracts context information and a decoding network that accurately locates its symmetric recovery target. The U-Net can achieve end-to-end training using a small amount of data sets. In addition, it has achieved good results in biomedical image segmentation. Zhao et al. [25] proposed a pyramid scene parsing network (PSPNet), which implemented the function of capturing the global context information by fusing the up and down of different regions through the pyramid pool module. The PSPNet network has a good performance in scene analysis tasks. Peng et al. [26] explored the role of large convolution kernels (and effective acceptance domains) in facing simultaneous classification and localization tasks, and proposed a global convolutional network in which the atrous convolution can expand receptive field without reducing the resolution. Chen et al. [27] combined

Sensors 2020, 20, 4208

5 of 18

the atrous convolution with the pyramid pool module to propose a new spatial pyramid aggregation algorithm (ASPP). The ASPP can segment the target object at multiple scales.
Li et al. [28] used graph convolution in semantic segmentation, and improved the Laplace algorithm to be suitable for semantic segmentation tasks. Zhong et al. [29] presented a model that uses a novel squeeze and attention module composition (SANet). In order to make full use of the interdependence of spatial channels, the pixel group attention is introduced into the attention convolution channel through the SA module and imposed on the conventional convolution. The outputs of SANet's four stratification stages are combined. Huang et al. [30] presented the Criss-Cross network (CCNet). The network proposes a crisscross attention module to obtain the context information of all pixels on different paths. Fu et al. [31] presented the stacked deconvolutional network. To fuse context information and restore location information, the network superimposes multiple modular shallow deconvolution networks (called SDN units) one by one. Artacho et al. [32] presented a network based on "waterfall" Atrous space pooling, which not only achieves improved accuracy, but also reduces network parameters and memory footprint. Sharma et al. [33] presented a method which used the DeconvNet as a pre-training network in order to solve the problem of differences between networks in the process of transfer learning.
From the abovementioned research, we can see that the image segmentation technology is mainly used in pose estimation, biomedical image segmentation, scene analysis, face classification and so on. As far as we know, there are few applications of image segmentation in the monitoring of assembled products. In contrast to an identification of scattered parts before assembling, the assembled product usually contains multiple parts which overlap each other. Thus, some parts are only partly visible due to occlusions. For this reason, it is difficult to detect complete parts within a complex assembled product. As shown in Figure 2, compared with a color image, a depth image is less affected by light and texture conditions. Therefore, it is more suitable for the recognition of metal parts. To monitor the assembly process, this paper performs semantic segmentation, which is also known as pixel-wise classification, on the depth image of the assembled product, to determine to which part each pixel belongs. We propose a depth image segmentation method employing FCN [23] to recognize parts from complicated assembled products.
3. Three-Dimensional CNN Model with Batch Normalization
The 3D CNN is an extension of the 2D CNN, which adds a time dimension to the base of the 2D CNN. Since there is no need for complex processing of the input sample data, the processing speed of the 3D CNN is faster, making it more suitable for the application of assembly operations. The conventional 3D CNN model [15] consists of input layer, three-dimensional convolutional layer, pooling layer, fully connected layer, and output layer. The input is usually the original RGB video frame or optical flow. Due to the large sample size, the training time is long and the training result is unstable.
In this paper, based on 3D CNN [15] and batch normalization [20], a batch normalization layer is added between the three-dimensional convolutional layer and the activation function on the base of the 3D CNN. The batch normalization layer preprocesses the output of 3D convolutional layer so that its mean value is 0 and its variance is 1, which speeds up the training speed and convergence speed, and improves the generalization of the model. The structure of the improved 3D CNN is shown in Figure 1. Firstly, the continuous video frames are transferred to the three-dimensional convolutional layer, and then the inactivated features obtained from the convolutional layer are transferred to the batch normalization layer. Finally, the features are activated by the ReLu function [34] and transferred to the three-dimensional pooling layer. The features obtained by the last pooling layer are transferred to the softmax function through the fully connected layer for classification and output.
The improved 3D CNN model is different from the 3D CNN model [15] by inserting batch normalization layer after Conv1, Conv2 and Conv3. In addition, this paper investigates the effects of gray image, binary image and depth image on training results in addition to the RGB image.

Sensors 2020, 20, 4208

6 of 18

Experiments show that the RGB video frame can be transformed into a single-channel gray image by

image processing, and its array can be dimensionally transformed to conform to the input requirements

of the 3D CNN. Under the 3D CNN model with the batch normalization model proposed in this paper,

the training speed can be improved and the convergence time of the network can be reduced while

accuracy is guaranteed. The detailed network structure is shown in Figure 3.
Sensors 2020, 20, x FOR PEER REVIEW

6 of 18

Figure 3. 3D CNN model with batch normalization. Figure 3. 3D CNN model with batch normalization.

The three-dimensional convolutional layer is shown in the blue part of Figure 3. The video frame sTehqueetnhcreeei-sdiumseednsaios nianlpcuotnovfoltuhtrieoen-dailmlaeynesironisalshcoownvnoliuntiothnealbllauyeerp, aartndof tFhirgeue-rdeim3.enTsihoenavlideo framceonsveoqluuetinocnealiskeursneedl (aass sihnopwunt oinf tthhereger-edeinmpeanrtsioofnFailgucoren3v)oilsuutisoendatlolacoynevr,olaunted tthhereinep-dutimviednesoional convforalumtieo. nTahl ekesrinzel (oafs tshheowdantainitnhpeugttreedeninptaorttohfreFei-gduimree3n)siosnuasledcotnovcooluntvionluatleltahyeerinipsut1vi×deo2 ×frame.
The si3z(eleonfgth,ehdeaigtahti,nwpuidttehd), itnhteontuhmrebee-dr iomf etnhesiochnaanl nceolnivsocl,uatinodnathl elaysiezre iosfa1th×e at2hr×eea-3d(ilmenengstiho,nhael ight, widtcho)n, vtholeuntiuonmabl ekreronfetlhies ch×ann×elis, ca,nadntdhethceonsivzoeluotfiothnektehrnreeel -ddiimmeennssioionniasl co×nvo×lut×ion.alIfktehrenel is

f × fn×umf b, earnodf t3hDeccoonnvvoolultuiotinoanl kkeerrnneelsl idsinm, tehnesniothneiosuftp×utf N× afft×erc.thIef ctohnevnoulumtiobnearloofp3eDratcioonncvaonlubteional kerneexlspriesssne,dtahsesnhotwhen ionuEtpquuattiNona(f1t)e. r the convolutional operation can be expressed as shown in

Equation (1).

 = (1 -  + 1) × (2 -  + 1) × (3 -  + 1) × 

(1)

The batch normalizNati=on(laa1ye-r ifs+sh1o)w×n (ina2th-e fre+d p1)ar×t o(fa3Fi-gufre+31. )L×ikenthe convolutional layer, (1)

tThhe epboaotlicnhgnloayrmeraalnizdataiofnulllayyceorninseschteodwlnayienr,thbeatrcehdnpoarrmt aolfizFaitgiounreca3n. Laliskoe btheeucsoendvaosluatinoenuarlallayer,

the pnoeotwlinogrklalayyeerra. nWdhaenfuelalychcolanyneercotfedthleanyeetrw, boartkchis ninoprmuta, laiznaotriomnalciaznatiaolnsolabyeerusisedinasesratende, uwrhailcnheitswork

layere.qWuihvaelnenetatcohplareyperroocfestshiengnethtwe doraktaisobintapinuet,dafrnoomrmeaaclihzacotinovnollauytieorniasl ilnasyeerrt,eadn,dwthheicnheinsteerqinugivtahleent to prepnrreoecxcoetnsslsatirnyuegcrttihooenf dmthaeettahnooedbtstwatihonraektdatrfoeroummseadeinawtcahiilnlcnotonhtvedoedlsuattrtiaooynbtaehltewladeyiesentrr,ib0auntadinotdnhoe1fn.tehTnehtfeeeraittnruagrnetsshfoleeranmrenaxettidolnabyyaetnrhdeof the netwcoornkvtoolumtioaninatlalianyethr.eTdhaetfaobrmetuwlaeefonr0baantcdh 1n.oTrmhealtirzaantisofnorismaastsiohnowanndinreEcqounastitorunc(t2i)o.n methods that are

used will not destroy the distribution of the features learned by the convolutional layer. The formula

for batch normalization is as shown in Equa=tion((2))-.[[(())]],

(2)

where () represents the neuron parameter, x(k[)-()E] rxe(pk)resents the mean, and Var[()] represents

the variance. The formula

for

transforming

x^ =

,

and reconstrVuactri[nxg(k)b]atch

normalized

parameters

is

shown

in

(2)

Equation (3), where (k) and (k) are learnable transformation and reconstruction parameters.

where x(k) represents the variance.

the

neuron

param(e)te=r,

E[(x()x)(]r)e+pres(en) ts

the

mean,

and

Var

x(k)

rep(3r)esents

TheTfhorreme-udliamefonrsiotrnaanl spfooromlininggoapnerdatrioencos nasrteruicntcinlugdebda.tcUhsunaolrlym, apliozoeldingpaorpaemraettieornss isincslhuodwe n in Equamtiaoxnim(3u)m, wphoeorleing(k()taaknindg(tkh)ealroeclaelamrnaaxbimleutmra)nasnfodrmmaetainonpoaonldinrgec(otanksitnrgucthtieonlopcaalrammeeatne)r. sT. he
pooling operation can effectively reduce the number of features and reduce the amount of calculation,

while also retaining local features. Theym(k)ax=imu(mk)x^p(ko)o+ling(ko)peration is adopted in this model. The (3)
pooling operations were carried out after the first, second and fourth batch-normalized layers and

aTftherreteh-edtihmirednsainodnfailftphocoolninvgolouptieornaatilolanysearrse. included. Usually, pooling operations include maximum pooling (Ttahkeinfuglltyhecolnoncaecltmedaxlaiymeur mis )shaonwdnmienatnhepgoroelyinpga(rttaokfinFgiguthree 3lo. cTahlemmeaainn).fuTnhcetiopnooolfinthgeofuplelryation can ectffhoeencnctheivactreealdyctleraeryidseturiccisevtatohluaeecnst uoasfmtahbbeerrciodongfvefeobaleuttutwiroeenesanal tnlhadyeerhreiaddndudecnethlteahypeeoaromalnloaduytnehrte)oaofnucdtaptluhcutenllaatytoieortnr(aw,nwhsmihcihiltecthaaenlsrfoelasrtuetteltansining

to the output layer for classification. The dropout processing is often carried out in the fully connected

layer, and some nodes are randomly hidden to prevent over-fitting. Another method to prevent over-

fitting is L2 regularization, which is shown in Equation (4).

Sensors 2020, 20, 4208

7 of 18

local features. The maximum pooling operation is adopted in this model. The pooling operations were carried out after the first, second and fourth batch-normalized layers and after the third and fifth convolutional layers.
The fully connected layer is shown in the grey part of Figure 3. The main function of the fully connected layer is to act as a bridge between the hidden layer and the output layer (which can flatten the characteristic values of the convolutional layer and the pool layer) and then to transmit the results to the output layer for classification. The dropout processing is often carried out in the fully connected layer, and some nodes are randomly hidden to prevent over-fitting. Another method to prevent over-fitting is L2 regularization, which is shown in Equation (4).

Sensors 2020, 20, x FOR PEER REVIEW

J() ()

= =

1 21m 2

m



 i=1

(h(xi)

-

yi )2

+

[(() - )2

 n   j=1 2j  +  ( 2)]

where

m i=1

(h(xi)

-

yi )2

is

the

loss

functio=n1, 

is

the

parameters

oft=h1e

CNN

model,



7 of 18
(4)

(4)

n j=1

2j

is the

regulawr theerrme ,a=n1d((isr)e-gul)a2riizsatthioenlocsosefffiuncciteinont.,  is the parameters of the CNN model, (=1 2) is

Tthhee oreugtupluatr ltaerymer, aisndclasissifireegdulbayrizthateiosnofctomefafixcifeunnt.ction.

The output layer is classified by the softmax function.

4. FCN for Semantic Segmentation of Assembled Product

4. FCN for Semantic Segmentation of Assembled Product As can be seen in Figure 4, we use the FCN for the semantic segmentation of a depth image of an assemAbs lceadn pberosdeeuncti.n TFihgeurFeC4N, wceaunsbe ethdeiFvCidNedfoirntthoetswemoasnttaigc esse:gmtheentfaetaiotunroeflaeadrenpitnhgimstaaggeeoaf nd the semananastsicemsebglemdepnrtoadtuiocnt. sTthaegeF.CINn cthane fbeeadtuivriedleedarinntiongtwsotasgtaeg, etsh:ethVeGfeGatculraessleifiarcnaitnigonstnageetsa[n3d5]thweere reintersrpeeirmnetateenrdtpicraessteefgdumlalysenfcutoallntyivocnoolnusvttaoigoluen.taiIolnnnatelhtnse.eftWesa. etWufreuefrutlehratehrrneurinsugesetshttahegetetr,raatnhnsesffeeVrrGlleeGaarrcnnliainnsgsgi[f3[ic36a6]ta]iopanpprpnoreaotcsahc[t3ho5tr]oewtrreearitnerain the patrhaempeatrearms eotferths eofctohnevcoolnuvtoioluntiloanylearyseorsf othf ethVe GVGGGwwitihthththeeddeepptthh iimmaaggeessaannddppixiexlellablaebleedleidmiamgeasges of theoafstsheemabslseedmbplreoddpurcotdsu. cTtsh. eThseemseamnatnicticsesgemgmenentatattiioonn ssttaaggee iiss ccoommppoosesdedofofa asksikpiparcahricthecittuecretu, re,
whichwchoimchbicnoemsbcinoeasrsceo,arhsieg,hhilgahyelrayienrfoinrmforamtiaotnionwiwthithfinfien,e,lolwow-l-alayyeerr iinnffoorrmmaattiioonn. .ThTehecocmobminbeidned

semansteicmiannfotircminaftoiormn aistiounp-isaumpp-slaemd ptoletdhetodtihmeednismioensoiofnthoefitnhpeuitnpimutagime augseinugsitnhge tdheecdonecvoonlvuotilountiolanyer.

Therefloaryee,r.aTlhaebreelfoprree,dailcatbioenl pfroerdeicatciohnpfoixreelaicshgpeinxelraistegdenwerhaitleedpwrehsileerpvrinesgertvhiengspthaetisapl arteisaol lruestoioluntioofnthe

input oimf tahgeein. pUustiinmgatghee. Upsriendgitchteiopnrsedoifctaiollnps ioxfeallsl,paixseelsm, aansetmicasnetgicmseegnmtaetniotantioonf tohfethdeedpetphthimimaaggeeoof an

assemabnleadsspermobdluedctpirsoodbutcatiinseodb.tained.

224*224*3(Image) pool1

pool2

pool3

VGG feature learning module

pool4

pool5 4096

4096

1000

256

128 64
Deconv

2X

Deconv 2X

512 Deconv 2X

512 Deconv 2X

Deconv

Heat Map

7*7*4096 7*7*4096 7*7*15

Deconv 2X

FCN-2S

FCN-4S

Deconv 2X

Deconv 4X

FCN-8S Deconv 8X

16X upsampled prediction(FCN-16S)
Deconv 16X

Skip ARchitecture (semantic segmentation module)

224*224*15

224*224*15

Conv+relu

224*224*15

224*224*15

Pool

Fully connected

Deconv+Pool

Figure 4. The FCN structure. Figure 4. The FCN structure.
The lower the selected layer is, the more refined the obtained semantics are. Therefore, on the basis of theTFhCeNlo-w8Serntehtes,sleolweceterdlalyayeersr wis,etrheeumseodreforreftihneedupth-esaombtpaliinnegdtsoegmeannetricastearme.oTreherreefifnoreed, osenmthaentic
basis of the FCN-8S nets, lower layers were used for the up-sampling to generate more refined semantic segmentations. We further defined FCN-4S and FCN-2S nets as well as FCN-16S nets to compare the effects of semantic segmentation. All above mentioned nets are shown in Figure 4.

5. Creating Data Sets

5.1. Creating the Data Set for Assembly Action Recognition

Sensors 2020, 20, x FOR PEER REVIEW

8 of 19

results to obtain the appropriate 3D CNN model with batch normalization. The process is shown in
Figure 5.
Sensors 2020, 20, 4208

8 of 18

segmentations. We further defined FCN-4S and FCN-2S nets as well as FCN-16S nets to compare the effects of semantic segmentation. All above mentioned nets are shown in Figure 4.

5. Creating Data Sets

S5e.n1s.oCrsr2e0a2t0i,n2g0,txheFODRatPaEESRetRfEoVr IAEWssembly Action Recognition

8 of 18

Training a 3D CNN requires a data set with enough sample, followed by the creation and training roefstuhletsmtoodoeblt.aTinhtehneeatwpporrokpsrtirautect3uDreCisNtNhenmcoodnetlinwuitohusblaytcahdjnuosrtmedaltihzraotuiognh. aTnhaelypsrioscoefssthise srhesouwltns itno Foibgtuairne t5h.e appropriate 3D CNN model with batch normalization. The process is shown in Figure 5.

FiFFgiuiggruuerr5ee. F55l..oFFwllocohwwacrchthfaaorrrttcffrooerractcirrneegaatthiinneggdtathhtaeesddeatattfaoarssaeesttsffeoomrrbaalssysseaecmmtibbolnlyyraeaccottiigoonnnitrrioeencc.ooggnniittiioonn.. TTThhheeeccrcerraeetaaittoiinoonnofooaffnaaannssaaessmsseebmmlybballcyytiaaoccnttidiooanntaddsaaetttaaisssereettqiiussirrreeeqdquubiierrfeeoddrebbteheffeoonrreeutthrhaeel nneetuuwrraoalrlknnceeattwnwbooerrktkrcacaiannnedbb.eettrraaiinneedd.. TTThhheeerrreeeiisissccucururrrerrneetnnlyttllynyonnaoossaeasmssesbmelmybablycltyiaocantcidotianotandsadettaa,tssaoestt,hesitso, rsteohsietsahrriceshsreienascrelcuahdricenhsctlihunedccleursedatethisoentchroeefascturicoehantaoiosfenstu.oTcfhhseaucshet.aTsheet. RTRhGGeBBRvvGididBeeovoiadnaendodtahtnehdedetdhpeethpdtvheipdvtehiodvewiodereweoeswrimeeurseiltmasniumelotuaulnstaleynoerueosculoysrldryeedcreobcroydreaddeKdbinybeycataKdKeinpinethcetcctaddmeepeptrhath, cacanamdmetehrraea,,aanndd tthhee vviidddeeeooo ffrraammmeesesswwereeeresepseapraaattreeallytyeleyxttreraaxccttrteeaddctfferrdoommfrtohmee twthoe vvtiwiddeoosv.idAesosese.mmAbblslyyseaamccttibioolnyn iasscdtiioffffnereisnttdffirfofemrecnotmfmroomn cwtcacwtahuuahhloosscussaheshmestmeummeiieitdcotmamcommpThoanTThlomlbihoonsbinhyhollsineslylagsnee(saryuushy,daecuduchssdsuatcparubseasaiutcmateouedeitaewatmowtaadnliaaamlndoitsynlssnaesmlnteieasgynffw(drat(styeeesace,araotio.(cptybrgseunsicrffeeoteocep.dtsbiaa,nniawrnfidetoserentsfsegaisu,inswatfdgld(et,ewsfinsese,nemimnosa.tndfiurg(gewusfpeebbtis,.)etm,nont.piillirugennyrytosgterbua,s.gwgtlo,,nacsiilna,nnyijprohatcicnunnsu,ptggtruittmoifisainnpo,ooioatofancnognrnnprttpwtogl,weumtsisiippnoji,onhutniaccraoghrsngitmrfsirocttlw,aaoese,siihpnptjimoraaesuenimcgrtitnqmisnemesfmira,tuaaodegdiihsbontstpaeras,aielpfmerofgtstiyamooteenimqna,linrrradtsugnmhsabogitotsas,gttailhoe,heeoftysm,aurtiotldmiqnisilesastsnrnsmuo.tpebrobiggcroeasMfytele.e,,uuhlsty)assterwt.e1slaiae.itilse2tapIcnnsnafMorftr.ooreggaup)yrcoci.rape,l,ewhsuhlInatrsosfeptsmaryosiee.ptiineiasprccsanMnlapfaeco.erercumgs)limpeclg.asun(,lwuchiaetnInndlbshofdtmiyroyipnglegeeziiayr,lbnrssnsuiapy`sanlpricanmpspypznyeulgascipriuiicanpearntanpnaeotidcmipegyeoiagnrstgceeinkispa,ilbobnsrknpeysissanlioinbtsmapyinznsiauendondndrimirbaatpdntyasngiessrclnypgyb',eeoytm)sgoleibm.fpaiyso,afrkmroTicsbancomuaiocbvsoirtnvicsoesnmuioeliteedohlammaitmsdamnoinmrsihrrnmynnsbee.ibubogngo.lnomuysurng,,ftnteittb,,c,moaactlrwvcsahouitsemlmehisaemohimrtpmcnoiehibn.bonnouglligtnysst,,, dcslaaawtmaisnpegitndagin,vdsearsfiwiltiiynngagn)adenatdochfeinloihnfagwn)cheiatcchhhe igosefonweprhearilcaizhtaetidisonboypche1ra2aratpecdeteorbpisyltei1cs(2tohpfeetoh`ppealaersts(ictehimpeba`lnpytasar'ct)ti.icoiTnposa,netwtnsso')u.orTreodeantasusreet tddhairvteaeerstsoeitotyldsaianvredrptsoritoyevniadhneadncttoeo fteihnneihsghaenenaceecrhathalisezsaegmteionbnelyrcaahlciatziraoatnci,otecnrhiocsshteiacnrsabocyftetthhrieesptaiacsrssteiocmifpbtahlnyetsaa.csWtsieohmnensb, rltyewcoaorcdotiirontnghsr,etewtoooolrs ttahhrereepveirdtooevooi,ldstehaderetpoparfirotnivciiispdhaenedtatcpoherfafionsrsimsehms ebtahlcyehacacostrirsoeensmp, obcnhlydoiasnecgntiaobsnys,etmchhbeolyspeanarctbtiicyoinpthawenittpsh.arrWteischpiepencatnrtteosc.ohWridshionewngnrtheceovrdidineog, utthhneedepvraisdrtateincodi,pinathgnetofppteahrreftoiarccmitpiosannt.htTehpceeorarfsroseresmmposbnlytdhtieonogclsoaarsrrseeesmsphboolnwydnainicntgiFoaingsuwsreeimth6b. rleyspaecctitotno hwisitohwrnesupnedctertsotahnidsinogwonf uthnedaecrtsitoann.dTinhge oafsstehme baclytiotono. lTshaereasssheomwbnlyintoFoiglsuarere6s. hown in Figure 6.

FFiigguurree 66.. AAsssseemmbbllyy ttoooollss..
The video for each type action for each participant was edited and divided into three or four video data samples, which were each associated with one of the nine assembly action classification labels. Each action category contained 36 segments of video data samples, each of which ranged between 3 and 6 s in duration. Both deep video and RGB video adopted the same processing method. The RGB images were converted into gray images and binary images, respectively. Accordingly, four

Sensors 2020, 20, 4208

9 of 18

The video for each type action for each participant was edited and divided into three or four

video data samples, which were each associated with one of the nine assembly action classification

labels. Each action category contained 36 segments of video data samples, each of which ranged

between 3 and 6 s in duration. Both deep video and RGB video adopted the same processing method.

The RGB images were converted into gray images and binary images, respectively. Accordingly,

four types of data set were obtained: RGB video sequence, depth image video sequence, gray image

video sequence, and binary image video sequence. Figure 7 shows the four different types of data sets

Simensaogrse2s0c2o0,r2r0e,sxpFoOnRdPinEgERtoREthVIeEsWame assembly action.

9 of 18

(a)

(b)

(c)

(d)

Figure 7. CCoommppaarriissoonn ooff tthhee ffoouurr iimmaage types of data set. ((aa))RRGGBB iimmaaggee;; ((bb)) DDeepptthh iimmagge; (c) Binary image; (d) Gray image.

The RGB image is a color image consisting of three primary colors of red, green and blue. Each picttuurreeccoonntataininssththrereeechcahnannenlselosfoinf fionrfmoramtioanti,oann,datnhde tvhaeluveasloufeesaocfhecahcahnnchelaanrnee0l­a2r5e50. T­2h5e5R. GTBheimRaGgBe ismraicgheiins croicnhteinnt,cbounteitnht,absutht riet ehcaosltohrrceheacnonleolrscahnadninseslesnasnitdiviestsoecnhsaitnivgestoinclhigahntgiensteinslitgyh. t intensity.
The depth image with depth information is obtained using the Kinect depth sensor; the pposition information contained in each pixel reflflects the distance from the sensor..
The binary graph is obtaineedd byy biinnaarriizziinngg tthhee RRGGBB iimmaaggee,, uussiinngg oonnllyy tthhee vvaalluueess oof 0 and 1 for each pixel. IItt rreessuullttss iinn lloossss ooff iimmaaggee iinnffoorrmmaattiioonn ttoo vvaarryyiinngg ddeeggrreeeess,, aanndd iitt iiss ddiifffificult to distinguish what the experimenter is doing with a single-frame image.
The gray image is a single channel image, which no longer contains color information, and the range of values for each pixel is 0­255. TThhee ggrraayyiimmaaggee ccaannrreedduuccee tthhee aammoouunntt ooff ddaattaa wwhhiillee eennssuurriing
the integrity of the image information.

55..22.. CCrreeaattiinngg tthhee DDaattaa SSeett ffoorr IImmaaggee SSeeggmmeennttaattiioonn ooff AAsssseemmbblleedd PPrroodduuccttss
AAss sshhoowwnn iinn FFiigguurree 88,, wwee ddeessiiggnn aa fflloowwcchhaarrtt ttoo ccrreeaattee tthhee ssaammppllee sseett ffoorr ttrraaiinniinngg FFCCNN mmooddeell ttoo rreeccooggnniizzee ppaarrttss ffrroomm ccoommpplliiccaatteedd aasssseemmbblleedd pprroodduuccttss.. TThhee pprroocceessss ffoorr ccoommppuutteerr ggeenneerraattiinngg ddeepptthh iimmaaggeess aanndd llaabbeelllliinngg RRGGBB iimmaaggeess iiss aass ffoolllloowwss::
(1) CCoommmmeerrcciiaall CCAADDssooftfwtwaareresuscuhchasaSsolSidolWidoWrkosrikssseilsecsteeldecttoedbutioldbthueildCAthDemCoAdDel omf othdeelproofdtuhcet parnodduthcet aCnAdDthme oCdAelDomf tohdeeplroofdtuhcetpisrosadvuecdt iisnsoabvjefdorinmoabt.j format.
(2) MMuuttiiggeennCCrreeaattoorrmmooddeelliinngg ssooffttwwaarree iiss uusseed to lload the assembly model in obj format. Each part iinntthheeaassseemmbblylymmooddeel lisislalbaebleeldedwwithithonoenue nuinqiuqeuceoclorlo. rT.hTehreefroerfeo,rde,ifdfeiffreenretnptaprtasrctsorcroersrpeospndontdo dtoiffderieffnetreRntGRBGvBalvuaelsu. eTs.heThaessaesmsebmlyblymomdoedlselsfofrordidffieffreernetntasassesmemblbylystsataggesesaarree ssaavveedd in OOppeennFFlliigghhttffoorrmmaatt..
(3) TThhee OOppeenn SScceennee GGrraapphh ((OOSSGG)) 33DD rreennddeerriinngg eennggiinnee iiss uusseedd ttoo desiiggnn an assembly labeling ssooffttwwaarree,,wwhhiicchhccaannllooaaddaanndd rreennddeerr tthhee asssseemmbbllyy mmooddeell in OpenFlight format, and establish tthheeddeepptthhccaammeerraaiimmaaggiinnggmmooddeell aanndd RRGGBB ccaammeerraa imagingg model. BByy cchhaannggiinngg tthhee viewpooint oorriieennttaattiioonnooff tthhee ddeepptthh ccaammeerraa iimmaaggiinngg mmooddeell aanndd RRGGBB caammeerraa imaaggiinngg mooddeell,, thee depth iimmaaggeess aanndd RGB images ooff pprroodduuccttiinnddiffifefererennt tasassesmemblbylystsatgaegseasnadnddidffiefrfenretnpteprseprespcteivcteisvecsancabne bseynsythnethsiezseidzebdybaycaomcopmupteurt.er.
Using the above process, the data set for image segmentation of assembled products can be synthesized by computer without using the physical assembly. Therefore, it is suitable for training FCN model to recognize parts from personalized products, which is usually not produced before monitoring.

(3) The Open Scene Graph (OSG) 3D rendering engine is used to design an assembly labeling software, which can load and render the assembly model in OpenFlight format, and establish the depth camera imaging model and RGB camera imaging model. By changing the viewpoint orientation of the depth camera imaging model and RGB camera imaging model, the depth
Sensorism20a2g0,e2s0,a4n2d08RGB images of product in different assembly stages and different perspective1s0 ocfa1n8 be synthesized by a computer.
Using the above process, the data set for image segmentation of assembledd productss can be synthesizeedd by computer without using the physical assembly. Therefore, it is suitable for training FCN model to recognize parrttss ffrroomm peerrssoonnaalliizzeeddpprroodduuccttss,,wwhhiicchhiissuussuuaallylynnoottpprroodduucceeddbbeefoforreemmoonnitiotorirningg. .

FFiigguurree 88.. FFlloowwcchhaarrtt ooff ccrreeaattiinngg iimmaaggee sseeggmmeennttaattiioonn ddaattaa sseett.. 6. Experiments and Results Analysis
The system used in this experiment is Ubuntu 16.04 (64 bits), the graphics card is NVIDIA QuadroM4000 and the CPU is Intel E5-2630 V4 @ 2.20 GHz × 20, and 64G RAM. Experiments for both recognizing assembly action and recognizing parts from complicated assembled products are conducted.
6.1. Assembly Action Recognition Experiments and Results Analysis

6.1.1. Assembly Action Recognition Experiments
The Adam optimization algorithm [37] is used. The basic network structure in the experiment was first determined based on the training results of the RGB data set of assembly action. Subsequently, the batch normalization layer was introduced and tested on different data set images to adjust the network structure. Ultimately, the four data sets were compared and evaluated. The sample size of all data sets is identical, with the same comprising 16-frame sequence images in the sub-folder under each action classification. That is, the input is 16 × 112 × 112 × 3 or 16 × 112 × 112 × 1, where 3 and 1 are the number of channels. Table 1 shows the settings of the parameters and functions using in the CNN model. Three quarters of each data set were randomly selected as the training set, with 20% of them being the validation set. The rest quarter of each data set is the test set.

Method 3D CNN

Table 1. 3D CNN parameter configuration.

Crop Size Loss Function 112 × 112 Cross_entropy

Optimizer Adam

Learning Rate
0.0001

Batch Size
10

Decay Rate
0.5

Decay Steps
2

First, the 3D CNN model is built based on the RGB data set. The model structure adopts the structure of the C3D model [14], comprising only a stack of a three-dimensional convolutional layer and a three-dimensional pooling layer. Figure 9 shows the comparison between the training results of different convolutional layers. When the number of three-dimensional convolutional layers is four and five, the accuracy of the test set deviates greatly from that of the training set, and the training result is in an under-fitting state. When the number of three-dimensional convolutional layers is seven and eight, the deviation between the test set accuracy and the training set accuracy is gradually increased, and the phenomenon of over-fitting appears. When the depth of convolutional layer is six, the 3D CNN model achieves better results. In the absence of any preprocessing of the data set, the accuracy of the test set is 82.85%.
Next, the structure of the 3D CNN model is finally determined by introducing the batch normalization layer, adjusting the model, and testing and optimizing on different types of data set. As shown in Figure 3, the 3D CNN model with batch normalization consists of five three-dimensional

structure of the C3D model [14], comprising only a stack of a three-dimensional convolutional layer and a three-dimensional pooling layer. Figure 9 shows the comparison between the training results of different convolutional layers. When the number of three-dimensional convolutional layers is four and five, the accuracy of the test set deviates greatly from that of the training set, and the training result is in an under-fitting state. When the number of three-dimensional convolutional layers is sSeenvseorns 2a0n2d0, e20ig, 4h2t0,8the deviation between the test set accuracy and the training set accuracy is grad1u1 aofll1y8 increased, and the phenomenon of over-fitting appears. When the depth of convolutional layer is six, tchoenv3oDluCtiNonNalmlaoydeerls,afichveietvherseeb-edtitmerernessiuolntas.l Ipnoothliengablasyenercse, othfraeneybaptrcehpnroocremssailnizgatoiof nthleaydeartsaasnedt,ttwheo afucclluyrcaocynnoefcttheedtleasytesrest. is 82.85%.

100

80

Accuracy rate

60 train acc(%)
40 test acc(%)
20

Sensors 2020, 20, x FOR PEER R0EVIEW

11 of 18

4

5

6

7

8

Number of convolutional layers

As shown in Figure 3, the 3D CNN model with batch normalization consists of five three-dimensional

convolutional layers, five three-diFmigeunrseio9n. Calopmmoppoaalriinsgonlaoyfenrest,wthorrkeedbepatthch. normalization layers and two fully connected layers.
ThNTeheexdtid,mitmehneesnisostinrousncsotufortfehtehoefsisnthignelgel3ecDhcahCnanNnenNledl madtaoatdaseelstesitssarfaeirnetahltelhynendtretartneasrnfmsofrionmremeddebdtyotocinocntorfonodfromurcmitnogtothttheheeinbipnauptctuht renrqeouqriumrieramelmieznaentsitosfnofrolarthytehre3,D3aDdCjuCNsNNtiNn. Fg. oFtrohrexemaxmaomdpeplell,eta,hnthedestiszeisezteoinfogtfhtaehnegdrgaoryapyitmimimaigazegineigsis1o11n21d2×i×1ff1e12r1,e2an, tawttyowp-ode-sidmoimfendesanitosainosanelat.l mmataritxrixwwhihchichcacnannontobtebeusuesdedasatshteheinipnuptuttotothtehe3D3DCCNNNN. T. hTehedidmimenesniosinonofotfhtehegrgaryayimimagaegeisisthtuhsus trtarnasnfsofromrmededinitnoto111212× ×11121×2 1×. 1.
FiFnianlalyll,yt,htehefofuorurtytpyepsesofodf adtaatasesteot foafsassesmemblbylyacatciotinonaraereusuesdedasaisnipnuptuttotothtehiemimprporvoevded3D3DCNCNNN mmodoedle. lT. hTehteratrinaiinnginrgesreuslutsltasraerceocmompapreadreadnadnadnaanlyazlyezdewd iwthitrhesrepsepcet cttottohtehfeofuorucrrictreirtiearioafosftasbtailbitiyli,ty, trtarianiinnigngtimtime,ec,ocnovnevregregnecnecespsepeededanadndacaccucruarcayc.y.

6.61..12..2A. nAanlaylsyissiosfoEfxEpxepreimrimenetnatlaRl eRseuslutslts
CComompapraisriosnonofoSf tSatbaibliitlyityanadndCConovnevregregnecneceSpSepeeded aaa"ncnbW"oaaaltasWdrbcnicyimotstdiiitenhFcstoharhisi.otnF,lsegahoTiusr.izueugeaaihtTasrsutbeatiehptsBBisrbsh(oeeceNNatsencish1()cst"a"0leisilp1os)vaa­c0lassypeuc1oa­rsaule3sart1yrvrsvarr.oe3,svtaervfhrelseotsaeuthoshfshltseweuhhotpeshseoewenhfasewtciunonhgtafitmiuwduetnhvghmrdeebtureehleetbrsytrdhesrehee.suiseearsffholubstefoaohdsslrbwfttceoirssofitnwacfsraboeciisacntserbaaiacseintcbuinainanicsergnciiuetatnsegwdhrcsetayeahesdbcfmaeetrcyernmofotptarcwmrmtoilpoahnememlipesienetn.arsrpnigatrnTe.raiisghsasnrTtuotiiehisheslnnntetope(iseg,nbnspiw(r)g,nbuoseiis)rssntwouhhiusIrnsowhthlaieIgtwinotsnhrcegawdhtirchltwaoihtwteotlhhsiointtisoeefthhsohnstcoefiuooormtacorudimuonmretirdmddpnsteahi.adapsntTetrw.aaaaeihtbTstrsiaseeotiahashesnt"seortcoa,eWsehn"urti,tWsenontihtifotonheiwthtrfhahamBewhseacNsBaiachsceclNusb"himiceczrau"ahmaattbnrhcctalatibhdyyenohcldnyye

Accuracy

1.0

0.8

0.6

0.4

With BN Without

0.2

0.0 100.0 300.0 500.0 700.0 900.0
Iteration (a) Accuraticmy reaste comparison

Loss

With BN

2.0

Without

With BN

Without

1.0

0.0 100.0

300.0 500.0 700.0 900.0

(b) LoIstsecraotmiopnarison

Figure 10. Comparison curves for the RGB image. Figure 10. Comparison curves for the RGB image. From Figures 10­13, we can see that the introduction of the batch normalization layer improves
the convergence speed for the RGB video sequence, the binary image video sequence, the gray image
1.0

curacy

0.8

2.0

With BN

Without

Loss

0.6

With BN

L

Acc

0.4

With BN

1.0

Without

0.2

S0e.0nsors120002.00, 20,340200.80 500.0 700.0

900.0

0.0 100.0

300.0 500.0 700.0 900.0 12 of 18

Iteration

video

se(aq)uAenccceu,raatnicmyd rethasteedceopmthpaimriasogne

video

sequence,

and

the

(b) LoIstsecraotmiopnarison improved 3D CNN model

provides

better stability on the training set. Figure 14 shows a comparison of the training results of the batch

normalization layer moFidgeulroen10t.hCeofmouprardisaotna csuertsv.es for the RGB image.

1.0

Accuracy

0.8

2.0

Loss

0.6

0.4

With BN

1.0

Without

With BN Without

0.2 0.0 100.0 300.0 500.0 700.0

900.0

0.0 100.0

300.0 500.0 700.0 900.0

Iteration Sensors 2020, 2(a0,)xAFOccRuPrEaEcRytiRrmEaVteeIEscWomparison

Iteration (b) Loss comparison 12 of 18

Sensors 2020, 20, x FOR PEER REVIEW Figure 11. Comparison curves for the binarycimomagpe.arison

12 of 18

1.0

Figure 11. Comparison curves for the binary image.

Accuracy rate

Accuracy rate

1.0 0.8

0.8 0.6 0.6 0.4 0.4 0.2

With BN
Without BN With BN
Without BN

0.2 0.0 100.0 300.0 500.0 700.0 900.0

0.0

100.0

300.0
(a)

Ac5cI0tu0er.r0aactyior7an0t0e.0comp9a0r0is.0on

Iteration

(a) Accuracy rate comparison

Loss

Loss

2.0 2.0
1.0

With BN
Without BN With BN
Without BN

1.0

0.0

0.0

100.0 300.0 500.0 700.0 900.0

100.0 300.0 50It0e.0ratio7n00ti.0mes 900.0

(Ibte) rLaotisosnctoimmepsarison

(b) Loss comparison

Figure 12. Comparison curves for the gray image. Figure 12. Comparison curves for the gray image.

Figure 12. Comparison curves for the gray image.

Loss

Accuracy rate

1.0 1.0 0.8 0.8 0.6

2.0 2.0

With BN
Without BN With BN
Without BN

Accuracy rate

0.6 0.4 0.4 0.2 0.2 0.0

100.0

300.0

With BN
Without BN With BN
Without BN
500.0 700.0 900.0

0.0 100.0 300.0 5I0t0e.r0atio7n00.0 900.0 (a) AcIctuerraactyiorante comparison

Loss

1.0
1.0 0.0 100.0 300.0 500.0 700.0 900.0
0.0
100.0 300.0 50It0e.0ratio7n00ti.0mes 900.0 (bIt)erLaotisosncotimmpesarison

(a) Accuracy rate compFairgiusoren13. Comparison curves for the d(ebp)tLhoimssacgoem. parison
Figure 13. Comparison curves for the depth image.

imagFTerohamerecFosilngigvuherrteglsye1ns0lco­ew1Fs3ipeg,reuwetrhdeeas1cn3oa.nftChtsroaeamteionptfihantrahgitseoutnhRseiGcnuigBnrvttirehmosedafvoguriecdtthaeioeonnddseeotpqhftuethheginemrcabaeyagsteiocm.hf atnhgoeer,bmainnaadlirzytahtieimoenaffgeleacytaenorfditmthhepebrdoinevpaertshy tvFhiirmdeoeamcogoenFsveiigsqeuruthgreeeensnwcc1eeo0, ­rsas1pnt3e.d,ewdthefeocrdatenhpestehReGitmhBaavtgitedheveoiidnseteorqousdeeuqncucteeio,ntncheoe,fbatinhndearbtyhaeticmihmangpoerrovmvidaeedlioz3asDteiqoCunNelnaNcyeem,rtihomde peglrroapvyreoismvidagees the cboentvteerrgsetanbcielistpyeoend tfhoer tthraeinRiGnBg sveidt.eFoigseuqreue1n4cseh, othwesbaincaormy ipmaraigsoenviodfetohseetqruaiennicneg, trheesuglrtasyoifmthaegebatch videonosremquaelinzcaeti,oanndlaytheer mdeopdtehl oimnatghee fvoiduerodasteaquseetnsc. e, and the improved 3D CNN model provides better stability on the training set. Figure 14 shows a comparison of the training results of the batch

normalizat1i.o0n layer model on the four data sets.

RGB image

Gray image

e

Figure 13. Comparison curves for the depth image.
From Figures 10­13, we can see that the introduction of the batch normalization layer improves the convergence speed for the RGB video sequence, the binary image video sequence, the gray image video sequence, and the depth image video sequence, and the improved 3D CNN model provides bSeetntseorrss2t0a2b0,il2i0t,y42o0n8 the training set. Figure 14 shows a comparison of the training results of the b13atocfh18 normalization layer model on the four data sets.

Accuracy rate Loss

1.0

0.8

0.6

RGB image

Gray image

0.4

Depth image

0.2

Binary image

0.0 100.0 300.0 500.0 700.0 900.0
Iteration
(a) Accuracy rate comparison with BN

RGB image

Gray image

2.0

Depth image

Binary image

1.0

0.0 100.0 300.0 500.0 700.0 900.0
Iteration times (b) Loss comparison with BN

Figure 14. Comparison of training for four data sets. Figure 14. Comparison of training for four data sets. Comparison of Accuracy and Training Time
The training results of the common 3D CNN model and the improved 3D CNN model on the four different types of data sets are compared and tested with the test set. The introduction of the batch normalization into 3D CNN model [15] should improve the initial learning rate and improve the training speed. For the sake of fairness, the initial learning rates of the common 3D CNN and improved 3D CNN are set to be the same to avoid the impact of the learning rate on the training speed. Table 2 shows the comparison of accuracy and training time for different test sets. The accuracy and of each data sets is the average of 10 tests.

Table 2. Comparison results for the four data sets.

Data Set Type
Accuracy (without BN) Accuracy (with BN)
Training Time (without BN) Training Time (with BN)

RBG Image
82.85% 83.70% 50 m 37 s 51 m 10 s

Binary Image
79.78% 79.88% 54 m 34 s 54 m 35 s

Gray Image
80.86% 81.89% 46 m 45 s 48 m 3 s

Depth Image
70% 68.75% 55 m 9 s 55 m 50 s

In comparing the four data sets, the training times for the binary image and the depth image are slower, confirming the result shown in Figure 14; that is, that the convergence speeds of the binary image and the depth image are slower than that of the other two types. The training speed can be improved significantly by transforming the RGB image into gray image through image processing. In addition, the introduction of the batch normalization layer does not directly improve the training speed, but since the batch normalization layer can improve the convergence speed, the training time can be reduced by decreasing the number of training iterations.
The accuracy of the RGB video sequences is highest owing to the abundant picture information, followed by the gray video sequence, but there is little difference between these two video sequences. In gray image data set, the identifying accuracy of screw twisting, nut twisting, hammering, tape wrapping, spraying, brushing, clamping, sawing, and filing is 75%, 80%, 78%, 80%, 87.5%, 78%, 88%, 75% and 90%, respectively. The average speed for recognition of an action is about 18.8 fps.
However, both the depth image and the binary image will lose image information to varying degrees, resulting in low test results. This is particularly the case for the depth image, when there may be serious misjudgments. When the depth image is acquired, the true depth value can be recorded. The depth value (700­4000 mm) when represented by a gray scale of 0­255 will bring a depth error of about 15 mm. Figures 15 and 16 show the depth video frames of the assembly actions of screw twisting and brushing, respectively. It is difficult to see from the picture which tool is in hand and what the participant is doing.

Sensors 2020, 20, 4208 SSeennssoorrss 22002200,, 2200,, xx FFOORR PPEEEERR RREEVVIIEEWW

14 of 18 1144 ooff 1188

FFiigguurree 1155.. SSccrreeww ttwwiissttiinngg..

FFFiiiggguuurrreee 111666... BBBrrruuussshhhiiinnnggg...

Tncwcsncwpooaooheennnerrellemmvvll,,sdeeAeAAaiaaaffrrnilnnnlnnggesiigadazdazeecilllalantnmeyyyttittcc-rvrsiisscpaeaeooiieihssisirnnlssnnoayoppiivnoofnneeccrenfftgegeaaedheddnndssetltw..hpuhpgeTTeeeceeehxrhheeeffaipffddleeeyeeeeetxxccshsrsiietptscpsiiieiminnnaeveviigsgmlmrrneeeeuilillnlumemeppyryit--mmirraccneeoolhhnnrrgbavvreeaattgeeeetaaddnnrersddllununauvoiewcewclrrnitfeelleedsihhggssntehiiruurrttglloaahhaeaelltitsypyeesnseseesssrnnicechqnnnsshhaacouguuuuialalwseeerrmsmsipniioniinnmcmbabnssggtereehh.haaarrttocoaggrmrawwataeeoonieitnnnnvvffhtpeiiiiennddrrtttt3sregrgeehhaaDsooaappaiientntnssrrrCiieeveednnNttqcqcehhggiiuuNisseeimeemiioopnpnwp33nnacacaDDirg..eerrtoaaheccvmmaacCCfeunoneeNNsntttppeiehNNtorrrreensseensscbtwweeoaaawrrnnniitvvttcddevhheehlelii,rnmmiiffgmmauuoeaanrssppnggmiidoorrceeoaonnetccrlvviooaszeebbninpanaatttetietetitenhhoccnndhhngeett.

666...222... PPPaaarrrtttsss RRReeecccooogggnnniiitttiiiooonnn EEExxxpppeeerrriiimmmeeennntttsss aaannnddd RRReeesssuuullltttsss AAAnnnaaalllyyysssiiisss

AAAsss ssshhhooowwwnnn iiinnn FFFiiiggguuurrreee 222,,, aaa gggeeeaaarrr rrreeeddduuuccceeerrr cccooonnnsssiiissstttiiinnnggg ooofff 111444 pppaaarrrtttsss aaannnddd aaa wwwooorrrmmm gggeeeaaarrr rrreeeddduuuccceeerrr cccooonnnsssiiissstttiiinnnggg

ooofff ssseeevvveeennnpppaaarrrtttsssaaarreereuuusseesddedaassatsthhteehaaessasseesmsmebbmlleebddleppdrroopddrouudccttu..cTTthh. eeTggheeeaarrgrreeeaddruurcceeedrruaacnneddr atthhneedwwthooerrmmwggoeeraamrr rrgeeeddauurccreeerrdwwueecrreeer

wmmeoorddeeemlleeddoduuessliienndggu33sDDinmmg oo3ddDeellmiinnoggdsseoolfifnttwwgaasrroeef..tEEwaaaccrhhe.ppEaarrattcoohffptthhareetaaossfsseethmmebballeesddseppmrrboodldeuudccttpiirssodmmuaacrrtkkieesddmwwaiirttkhheaaduuwnniiitqqhuueea

cucoonlliooqrru.. e33DDcommlooro.dd3eeDllss mooffodddiiefflffseerroeefnndtt iaaffssessreeemmntbbllayyssppehhmaabsseelyss pwwheearrseeerrseewnnddeereerreerddenuudsseiinnreggdtthhueesiOnOgppeethnneSSOcceepnneeenGGSrcraaeppnhhe G((OOraSSpGGh))

(rrOeennSddGee)rriirnneggndeeennrgginiinngeee.. nUUgssiininnegg. Uaa sddineegppttahhdbbeuupfftffheerrbutteeffccehhrnntooellcoohggnyyo,, lddoeegppy,tthhdeiipmmtaahggieemss awwgeiittshhwddiiitffhffeedrreieffnnettrevvniieetwwvppieoowiinnpttossinffootrsr

dfdoiirffffdeerrieffennettreaansstsseeammssbebmllyybpplyhhaapssheeascceaanncabbneebggeeegnneeenrraaetrteeaddte..dFF.ooFrroeeraaeccahhchpprrpoorddouduccuttc,,t,iinninttotoottataalllooofff 111888000 cccooommmpppuuuttteeerrr---gggeeennneeerrraaattteeeddd

dddeeepppttthhh iiimmmaaagggeeesss wwweeerrreee ooobbbtttaaaiiinnneeeddd;;; 111222000 cccooommmpppuuuttteeerrr---gggeeennneeerrraaattteeeddd dddeeepppttthhh iiimmmaaagggeeesss cccooonnntttrrriiibbbuuuttteee tttooo ttthhheee tttrrraaaiiinnniiinnnggg ssseeettt aaannnddd

666000 cccooommmpppuuuttteeerrr---gggeeennneeerrraaattteeeddddddeeepppttthhhiiimmmaaagggeeessscccooonnnttrrtiribbibuuutteetettoototthhtheeevvvaallaiilddidaattaiitooinnonsseestte..tII.nnIanaddadddiidttiiiootnnio,,n11,001dd0eedppetthhptiihmmiaamggeeassgeoosff

aoafppahhpyyhssiiyccsaaillcaaalssssaeesmmsebbmlleebddleoodbbojjeebccjttecaatrreearuuessueesddedaassatsthhteehetteetssettstsseestte..t.IInnInttrrtaaraiinniniinningggtthhtheeeFFFCCCNNN,, ,ttthhheeetttrrraaaiiinnniiinnnggg ssseeettt aaannnddd ttthhheee

vvvaaallliiidddaaatttiiiooonnnssseeetttwwwereeerreeiniicnnrcecrraeesaaessdeetddo t4too0544a00n55daa1nn3dd5,11r33e55sp,, errceetssippveeecclytt,iivvbeeyllyyth,, ebbdyyattthhaeeauddgaamttaaenaatuuaggtimmoneennmttaaetttiihooonnd.mmAeettthhraoondds..feAAr

tlterraaannrnssffineerrgllseetaarrrannteiinngggy sswttrraaastteeeggmyypwwloaaysseeedmmtoppllioonyyiteeiddalittzooeiinnthiitteiiaaFllCiizzNee .tthhee FFCCNN..

IIInnn ooorrrdddeeerrr tttooo eeevvvaaallluuuaaattteee ttthhheee pppeeerrrfffooorrrmmmaaannnccceee ooofff ttthhheee ppprrrooopppooossseeeddd mmmeeettthhhooodddsss,,, ttthhheee aaaccccccuuurrraaacccyyy ooofff ttthhheee pppiiixxxeeelll

ccclllaaassssssiiififfiicccaaatttiiiooonnn(a((asassshssohhwoonwwinnn EiinnquEEaqqtiuuoanatt(iio5o)nn) is((55u))s))eidissauus ssoeendde oaafssthooenneeevaoolffuattthhioeeneecvvriaatlleuuraaiatti.iooTnnheccrrpiiittxeeerrliiaac..laTTsshhieeficpaptiiixxoeenll

cacllcaacssussriiffaiicccyaattPiiooAnniaasccdcceuufirranacceyydPPaAAs fiiossllddoeewffiisnn:eedd

aass

ffoolllloowwss:: PPPAAA=

PPYYY PPNNN

, ,

((55(5)) )

wwhheerree PPYYY iiss tthhee nnuummbbeerr ooff ccoorrrreeccttllyy pprreeddiicctteedd ppiixxeellss,, PPNNN iiisss ttthhheee tttoootttaaalll nnnuuummmbbbeeerrr ooofff pppiiixxeellss.. TTaabbllee 33 ssshhhooowwwsss ttthhheee FFFCCCNNNnnneeetttwwwooorrrkkkpppaaarrraaammmeeettteeerrr cccooonnnfiffiiggguuurrraaattiioonn aaanndd ttthhheee nnnuuummmbbbeeerrr ooofff pppaaarrraaammmeeettteeerrrsss.. FFCCNN--22SS
hhaass oonnllyy aa sslliigghhtt iinnccrreeaassee iinn FFCCNN--22SS ccoommppaarreedd ttoo FFCCNN--88ss iinn tteerrmmss ooff tthhee nnuummbbeerr ooff nneettwwoorrkkk ppaarrraaammmeeettteeerrrsss...

Sensors 2020, 20, 4208 Sensors 2020, 20, x FOR PEER REVIEW

15 of 18 15 of 18

MMeeththoodd
FCFCNN-1-166SS FCFCNN-8-8SS FCFCNN-4-4SS FCFCNN-2-2SS

TaTbalbel3e. 3N. eNtwetowrkorpkapraamraemteertecrocnofingfiugruatriaotnioannadnndunmubmebreorfopfapraarmameteertes.rs.

ImImaaggeeSSiizzee
222244**222244 22244**222244 222244**222244 222244**222244

LoLsossFsuFnucntcitoionn
CrCosross_se_netnrtoroppyy CrCosross_se_netnrtoroppyy CrCosross_se_netnrtoroppyy CrCosross_se_netnrtoroppyy

OptOimptiizmeirzer LearLneianrnginRgaRteate

AdaAmdam
AdaAmdam AdaAmdam AdaAmdam

0.0000.0001001
0.0000.0001001 0.0000.0001001 0.0000.0001001

Batch SBizaetch

SizeoTfhPeaNorTafuhmPemaeNrbtaeumermrsebteerrs

1 1
1 1 1 1 1 1

145251946512549614
139551832935588238 140131949091389998 140161346011643614

FigFuigruer1e717shsohwoswtshtehpe ipxeixl ecllacslasisfsiicfiactiaotnionacaccucruacraycyofotfhtehFeCFNC-N8S-8Snentwetowrokrkanadndthtehneunmumbebreorfof iteirteartiaotniosn. sW. Withitahnainncinrecaresaesienitnhtehieteirtaetriaotniotnimtiem, eth, tehpeixpeixl eclacslasisfsiicfiactiaotnioancaccucruarcaycoyfobfobthotthhtehteratrinainging sesteatnadndthtehveavliadliadtiaotniosnesteitmimprporvoev. eA. fAtefrte6r06000i0teirtaetriaotniosn, tsh, ethfeinfianl aplipxeixl eclacslasisfsiicfiactiaotnioancacuccruacraycoyfotfhtehe vavliadliadtiaotnionsesteits iassahsihgihghasa9s69.61.%1%. T. hTehoenolninleinteratrinaingintgotooko5k.851.81h.hT. hTehaebaobvoevreesreuslutsltsshsohwowthtahtatthtehe prporpoopsoesdedmmetehtohdodbabsaesdedononFCFNCNfofroarsassesmemblbylymmonointoitroinrigngacahciheiveevseas agogoododpeprefrofromrmanacnec.e.

Accuracy % 1.00 0.980

0.960 0.940 0.920 0.900 0.880 0.860

Validation set acc% Train set acc%

0.00 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000
Iteration times
FigFuigruer1e71. 7T.raTirnainnginagccauccruacryacoyfoFfCFNC.N.

TaTbalbele44sshhoowwss tthhee ccoommppaarirsiosonnofotfhethpeixpelixcelal scsliafiscsaitfiiocnataiocncuraacccyurfaocrythfeotrestthseettaenstdstehte avnaldidtahtieon vasleidt.aFtiroonmsetth.eFcroommpthaericsoomn opfarthiseonpioxfetlhcelapsisxifielcacltaiossnifaiccacutiroancyacacnudratchyeannedtwthoerknettrwaionrinkgtrtaiimnienbgettiwmeeen beFtwCNeesnwFitChNdsiffwerietnht oduiftfpeuretnsttruocuttupruets,sittrcuacntubreesse,eint tchaant tbhee FsCeeNn-2tShahtasththeeFhCigNh-e2sSt ahccausrtahcey rheiggahredsitng acpcuixrealccylarsesgiafircdaitniognpoixnetlhcelatsessitfisceatt.ioTnheonavthereatgeestrusent.tTimhee oavf epriaxgeel crluansstiifimcaetoiofnpfixoerlocnlaesdsiefpictahtiiomnafgoer is onaebdouept t0h.1i7m3asg. e is about 0.173 s.

TaTbalbel4e. 4C.oCmopmapriasroinsoonf opfixpeilxecllacslsaisfsicifiatciaotnioanccauccruacryac. y.

MetMhoedthod
FCNFF-C1C6NNS--186SS FCNF-C8SN-4S FCNF-C4SN-2S FCNF-C2SN-2S FCN-2S

Data SeDt ata SPetixel(PCAla)Ps(isGxiefel(iPacCAarltaR)iso(eGsndifieuAaccracetRciroue)ndrauAcccyecru) racPy(iPxAel)PC((iPxWlAealso) Cs(rWimlfaioscGrsamitfieiacoGarnetRiaoArencRdAcueudccrcueaurccr)eyarc)y

validavtiaolindation set

93.84

%

93.84% 96.10%

96.10 % 97.72%

97.699477%..8634%% 97.8938%.59%

set

97.72 % 98.80%

98.5999%.53%

test set

98.80 % 94.95%

99.5936%.52%

test set

94.95 %

96.52%

As shown in Table 4, the experimental results of the gear reducer show that the FCN-4S and FCANs-2sShaorwen1.6in2%Taabnled 42,.7t%heheigxpheerritmheanntFalCrNes-8uSltsinopf itxheel agcecaurrraecdy.uTcehresphioxwel athccaut rtahceyFoCf NFC-4NS-a1n6Sd is FC2N.2-62%S laorwe 1e.r6t2h%ananthda2t .o7f%FChiNgh-8eSr.tFhoarnfFeaCtNur-e8Sleianrnpiinxgel, tahcecuFrCacNy.uTsehsetphiexeinl vaaccriuarnatcmy oetfhFoCdNo-f1s6pSaitsial 2.2tr6a%nslfoowrmerattihoann, wthhaitcohfaFlsCoNli-m8Sit.sFtohrefsepaatutirael alecacurnraincgy,otfhtehFeCoNbjeucts.eTs htheeloinwvearr-lieavnetlmcoenthvoodluotifosnpaaltliaayler trahnassfoarcmcuartaitoenl,owcahtiicohn ianlsfoorlmimatiitosnt.hIenstphaetilaolwaecrc-ulervaeclycoofnvthoeluotibojencatl. lTayheer,lothweeFr-CleNvenlectwonovroklcuatinolneaalrn layer has accurate location information. In the lower-level convolutional layer, the FCN network can learn accurate location information, thereby improving network performance. The FCN-2S network

Sensors 2020, 20, 4208 Sensors 2020, 20, x FOR PEER REVIEW

16 of 18 16 of 18

Sensors 2020, 20, x FOR PEER REVIEW

16 of 18

ahcacsurreaatechloedca9ti8o.8n0i%nfpoirxmelaaticocnu,rtahceyr,eabnyditmheprtoesvtinpgixneleatwccourrkacpyerhfaosrmreanchced. T9h4.e95F%CN. T-h2Se enxeptweroimrkehnatasl

hrreaeasscurheletasdcho9e8fd.8th90e8%.8wp0io%xrempl aixcgeceluaarracccryue,draaunccyde,rtahnaeldsteotshtsephtioexwsetl pathcixcauetlrFaacCcycNuhr-a2ascSryehhaacashseardceha9ci4eh.v9ee5dd%9.4thT.9eh5e%bee.xsTptherreeismeuxelptnset,raiwlmrieetshnutlaatlsn roaefcstuchuletrsawcooyfrmrthategeewaorof rr9em9d.u5g3cee%arr,awlrseohdisuchhcoewirsat1hls.a7ot%FshChoNigw-h2eStrhhatathsaFanCchNFiCe-2vNSe-d8hStah.seTabhceehsitteervesestdupltitsxh,eewl biatechcsutanrraeacscyucluhtrsaa,scwyriertahctehaeondf a99c96c.u.553r2a%%c,y.wIrnhatisceuhomifsm91a9.7r.5%y3,%hthi,gewhFehCrictNhh-a2insSF1nC.7eNt%w-8ohSri.kgThhhearesttehascathnpieiFxvCeNlda-tc8hcSue.rbTaechsyet hrtaeessturelptasixcihenledtahc9ec6u.d5raa2ct%ay.shIenatssfourremaimcmhaaergdye, 9tsh6e.e5g2Fm%CeN.nIt-na2tSisonunemtowmfoamrrkye,hchathasenaicFchaCileNva-se2sdSemthnbeeltbywe. soFtrikrgeushuraelstssa1icn8htaihenvededd1a9ttahsseheotbwfeosrtthirmeesasugelgetsmseiegnnmttaheteniotdantaiotafn sdoeeftpmftohercihmimanaagigceaesl. saTesghsmemleenbfttlyaf.tigiFouingreuorsfehmso1we8cshatanhndeicd1a9elpsahtshosewimmtabhgleyes.ienFgpimguutetrnientsagt1io8nntaonotdfhde1e9FpCtshhNoi-mw2Satgmheeos.dsTeeglh.meTlheenefttmafitiigdoudnrleeosffhidgouewpresthtshhiemowdaegspettshh.e Tiomhueatpgleeufttinfoipgf uuFtCrteiNnshg-2oiSwn.tTsohttheheerdigFehCptNtfhi-g2imuSramegioesditnehple.ugTtrthioneugmnidnidttodrultethhefiogFfuCtrNhee-s2lheSofmtwdosedpthetlhe. Toimhuetapgmueit.dodflFeCfiNgu-2reS.shTohwe sritghhet ofiugtupruetiosfthFeCgNr-o2uSn. dThtreurtihghotf ftihgeulreeftisdtehpethgriomuangde.truth of the left depth image.

Figure 18. Depth image segmentation of gear reducer. FFiigguurree1188..DDeepptthhimimaaggeesseeggmmeennttaattiioonnooffggeeaarrrreedduucceerr..

FFiigguurree1199.. Depptth iimmaaggee sseeggmmeennttaattiioonn ooff wwoorrmm ggeeaarrrreedduucceerr..
77.. CCoonncclluussiioonnss aanndd FFuuFttiuugrureereWW1o9o.rrkDkepth image segmentation of worm gear reducer. 7. ConIInnclttuhhsiiissoppnaasppaeenrrd,, aaF3u3DDtuCrCeNNWNNomrmkooddelewl withithbabtachtcnhonromramliazlaiztiaotnioins pisropproospeods.eAdn. aAsnseamsbselymabcltyioancstidoantsa snipsrpcpwpmpwTpdniaahnmoIamsnipsrpcpwpmpseepomtoarrulmsnsnaooiheaepyoicataoharrulmooeogrssrtiprircatadrtrosepmooyceshraertemiedvppnrpahamiomyctrlheagpemdvnprdmmmieimeIioruoaoeFeigpmdndomdastnbnlianucoom,ialdssvoscdiaCnbmeleblncvb,eslaielcvclesctieoitwtelteruitttlaieioltc,cszeltNaioelitzwthieuietzaytndpeecoeusztdaiooloiendaitheadrictoiaunrhdpdoosie-aiintvhdnstfnigahrstttishtxb3etitnnidcdntfioi3hlso.,esic,tietio3eoponideDepanltl.o,msgaDseontaeorruDehnnTnrlnragsiorarshaineinnTnremroiendenlorpoeahachciCndsagreimiCiiaddlbenhdCoslitgisagrmndgssiieeaiiddhaunNasnisngeulmnssNeemhrhgasoeNuiemtteusdcmegasahggm,tonescFieNrettnmsaeceaNmenrmpcbFleaeeNtreafstCositaiotrmpcsaloebmmaasifitChtltsoyztsionc3cthbnoamnaaNhelttlyiztm.afeecg,hcomnpaDNeecnhiabtlicsdneiea,ttadpuenohe-rotocddnseiFnibieliootTiboo-ttoaadxnyaeciobnreiCeotm.diiCibtoirodxnahoonsroayote.dcnmogiincnpIiepNaorienscnN,specghlnrnmtbcenpIoeannliresplerritylbboaatnocgiliarrNmontgso-eimp.dwhepnatsociarlcilyawmohnbgsneddwhgcsanyxlcaayeTcuesnecp,daagegbaidemosecupinuresap,tcahtidtoslustsfumlsftaudstehetfahrcsutderlehuyofsohsefidehoettchscsmyotitreaofdreiahraustetdnsmiestobtotueai,afsepicbahauaoesiitsctbn.eimrapapefecacdsmyorsiistcactfp.enarapaantomyemltsrtiomhosTttea.nyetttaFinoearcsetsohymTatenwt.cmisyoutFphcoTtshynleaCsianotatmisphsahaaiacTpnheCiroduetiamtincphsstoniaabcnNnloehtneimnbaheonnsiblunNnttloheenmlssnbdndtbedlsiyloacymet-lsscsescaondcbeeloaxciybl-obocseproeurtmogecnorytxbloaocppRrmca,eseaagmafthonymesfmfpdRmcrsaoeuamfrtassselctemGptfsdeieouedereacsgecsupeeonGtprtrdtaeRrecagxapieehsoeltBrhrioadaamameflldlrnoociaoimslBuitadaimeamtiGrlcctzfnoooiotmauntcnsztesmaiccxezishlyyjisaaacndtneeimziutBexaomhblytinisa,easmgtdeeststnsmeabtngaitapedsriletiimsttynesnbtsiiaioeasyatpmoitimneinatmatayosamgsitaonaa,ottianmingnatnooatadrdgrnfia,ninilgpniteiqanoaderredeirntnmnafimoileecomdceeluirtnmumoftmrcgagttiimsmnacoanhpatmireahzinnanttaaiamsesapnshacinveahpineiswesrgasestgnalpsecnnstermseogupoitettesiaeltesmcsesitrorfisehupohtifedceesmgrlcsysmsiraeofoehstatoieffvtooymgrleoss.rachffeemuhano.fsgfstvttnocuo.eetbndernedefsagTieshnteceecuimTuhebdneedaerTptcdlf.ttiidtmuuotcheryticteesarshlfsttitdsiuirhettcrydheoitehsicBiestcttstdrtaiyoieettcpvabhenaoicethmttsednveqyoaomv.nsarveop,ecimtnueremtqedee.en.trupue,etnaaeaxomteoaeleTtmxttuushel.gayxFioanlacenttiprTtimnstpynmenpyhtoiazhnctprhimanuslnTpdhoehdtgoiszcitgrooaeleatniperohedattshrthiroleinoeeeatnsmpeirfteunytromrtilseinjsenudeitcieftny3Fnumiraodamirjtriospmoydinde3iummiasmruDeteitCroei3tydsu3nnmeemnruDeingeateti.sdct.cDisneebDwgeinedege-chanout.tAuceiebpBng2deCaudietnitsgtseenpBtrlooshCnutSphiCttrognhCtsattptttveNhnpthatrroaohnsrioogahtptatatelNeoshsNeeokoNneialooghnhlatdledtsNeryeiorametehsnwdsridtneNetdstmmorNueeNsnncwdesteihstsnemtuencsndthitehhcetawbsnteicmegazaecsuhuahhctomftdtumeurmmegeomaaouuayhsammtirotnepcmoleeaotomhnsnmlisotnttm3elpththbottoonruioFhssidteno,3hedvtmhsnDbtFkorsledbdencdhdCsrnhoDbte.eyeesotececCsrinsupnahee.eesaehltvnertcFulp-rruCvhamcarohuslstgrclFue2ls-orCoeusmrpewsooprtece2rooNskaoeSduwswnronowwopteeteoNtsaemwSranfcnitswsoueeitifvvsmcNeogahfcir.sthiouenidccvomNterotteahpittrrthtdhendertroenthrheedfiheerahstetrgrhprrvalmzteeuiheasmtsrcrrgscoipaesvambznwiwaaeatbbccpegcfiiecahpbnestnwdiwoaeaansritfrtiabhsaaatnetlotnoaoesirioilrtttttdaiauittitittroonnooioiiohhciottttedrncroenhhtccnmccoonioehhlchnnrnkgcrkdaeneemsdhhinthegeeeeelhnnnkgkeeess,l.. sAiynustbheomotrh,Cetovansektnrsicb. oumtiobnins:eCaosnsecempbtulyalaizcattiioonn,mCo.Cn.itaonrdinJ.gHa.;nmdestehmodaonlotigcys, eCg.Cm.eanntdatZio.Zn.;tsoofitmwpareo,vCe.Zth.,eTr.Wes.ualntsd iwCnA.rCubitt.oi;hntvohgar--ltiCadrsoaektnvisioter.niwb, Tua.tnWido.,neYsd.:GiCt.inoangn,cdeCCp.Ct.Cu.a.A;liflzolaramtuiotahnl o,arCnsa.Chlya.svaisne,drTe.JWa.Hd.,.a;Dnm.dLe.atahgnordedeoCdl.oCtgo.y;tw,hCeri.ptCiun. bgal--nisdhorZeidg.Zivn.;easrlosdifortwanfatorpfert,heCpea.mZra.a,tniTou.nWs,cC.rai.pCntd..; ACCFN2CCm0..uuCCo..1anCCt..;7hn;d.;Gvo;u5iwvarn1Gswalc4rgCiXlirrd7i:tiiod25iatpnTni0a2ttngih3tt5.ogir0--i1onis--0,bn,3rr5u,eTre)1.eTtvs.i7Wvei.oW0eianew5.r,ws.2c,Y:h7aYC3.anw.)GondGnaod. nc.reaekedadnpdnidwtttidhiutnaCieanCsg.lCgKi,.czC,o.Cea;.C-y;tf.siCof.uoCRor.npmre.,AmpsAaCeolallal.rlClartaeacn.audhnaautalnhtb&ylhdoyysorsiDJssrti.hss,He,heTvh.Ta;.eNWav.mlWvoea.epet.DrtimoheDr.enaoLe.adLdan.dl.otaaNalnPaonndnragddotdyCuga,Crr.agCCaa.gCrlm.;erC;Swees.wcedraidoeirnttfiniotdtnSicongehZtght--a.Fh--Zenoeo.du;porposirnugoinudbgifgbnlatiinwlatsPiiaslhoarlhedorneddervd,oariaCfvnftfvecC.tZperephsrr.,s(ierioGniTeponpar.anWaar(onarGo.fattfairttoNianhtonnhdeon,te., mAFacuknnundsoicwnrgilpe: tdT. ghmisernetsse:aWrcehthwaonrkkowpearsactoor-ssu(wpphoorsteendambyesthweerNeaatsikonedaltoNbaetukreaplt Ssecciernetc)epFarotuicnipdaatteiodninotfhCe hexinpaer(iGmreanntt FtNounoe.dsti5an1bg4li:7s5Th2ht5ih1se,r5de1sa7etaa0sr5ec2ht7.3w) oarnkdwthaes cKoe-ysuRpepsoeratrecdh b&y DtheevNeloatpiomneanltNPartougrraalmSscieonf cSehFanoudnodngatiPornovoifnCcehi(nGara(GntraNnot . N2o0.175G14G7X5225013,005317).05273) and the Key Research & Development Programs of Shandong Province (Grant No. 2017GGX203003).

Sensors 2020, 20, 4208

17 of 18

Conflicts of Interest: The authors declare no conflict of interest.
References
1. Bobick, A.; Davis, J. An appearance-based representation of action. In Proceedings of the 13th International Conference on Pattern Recognition, Vienna, Austria, 25­29 August 1996; pp. 307­312.
2. Weinland, D.; Ronfard, R.; Boyer, E. Free viewpoint action recognition using motion history volumes. Comput. Vis. Image Underst. 2006, 104, 249­257. [CrossRef]
3. Dalal, N.; Triggs, B. Histograms of oriented gradients for human detection. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), San Diego, CA, USA, 20­25 June 2005; pp. 886­893.
4. Chaudhry, R.; Ravichandran, A.; Hager, G.; Vidal, R. Histograms of oriented optical flow and Binet-Cauchy kernels on nonlinear dynamical systems for the recognition of human actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20­25 June 2009; pp. 1932­1939.
5. Schuldt, C.; Laptev, I.; Caputo, B. Recognizing human actions: A local SVM approach. In Proceedings of the 17th International Conference on Pattern Recognitio, Cambridge, UK, 26 August 2004; Volume 3, pp. 32­36.
6. Wang, H.; Kläser, A.; Schmid, C.; Liu, C.L. Dense trajectories and motion boundary descriptors for action recognition. Int. J. Comput. Vis. 2013, 103, 60­79. [CrossRef]
7. Chen, C.; Wang, T.; Li, D.; Hong, J. Repetitive assembly action recognition based on object detection and pose estimation. J. Manuf. Syst. 2020, 55, 325­333. [CrossRef]
8. Redmon, J.; Farhadi, A. Yolov3: An incremental improvement. arXiv 2018, arXiv:1804.02767. 9. Wei, S.E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y. Convolutional pose machines. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27­30 June 2016; pp. 4724­4732. 10. Kim, M.; Choi, W.; Kim, B.C.; Kim, H.; Seol, J.H.; Woo, J.; Ko, K.H. A vision-based system for monitoring block assembly in shipbuilding. Comput. Aided Des. 2015, 59, 98­108. [CrossRef] 11. Zidek, K.; Hosovsky, A.; Pitel', J.; Bednár, S. Recognition of Assembly Parts by Convolutional Neural Networks. In Advances in Manufacturing Engineering and Materials; Lecture Notes in Mechanical Engineering; Springer: Cham, Switzerland, 2019; pp. 281­289. 12. Feichtenhofer, C.; Pinz, A.; Zisserman, A. Convolutional two-stream network fusion for video action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27­30 June 2016; pp. 1933­1941. 13. Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.; Van Gool, L. Temporal segment networks: Towards good practices for deep action recognition. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11­14 October 2016; Lecture Notes in Computer Science. Volume 9912, pp. 20­36. 14. Tran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; Paluri, M. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 7­13 December 2015; pp. 4489­4497. 15. Ji, S.; Xu, W.; Yang, M.; Yu, K. 3D convolutional neural networks for human action recognition. IEEE Trans. Pattern Anal. Mach. Intell. 2013, 35, 221­231. [CrossRef] [PubMed] 16. Du, W.; Wang, Y.; Qiao, Y. RPAN: An end-to-end recurrent pose-attention network for action recognition in videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22­29 October 2017; pp. 3745­3754. 17. Donahue, J.; Hendricks, L.A.; Guadarrama, S.; Rohrbach, M.; Venugopalan, S.; Darrell, T.; Saenko, K. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7­12 June 2015; pp. 2625­2634. 18. Xu, H.; Das, A.; Saenko, K. R-C3D: Region Convolutional 3D Network for Temporal Activity Detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22­29 October 2017; pp. 5783­5792. 19. Soomro, K.; Zamir, A.R.; Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv 2012, arXiv:1212.0402.

Sensors 2020, 20, 4208

18 of 18

20. Ioffe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 11 February 2015.
21. Shotton, J.; Fitzgibbon, A.; Cook, M.; Sharp, T.; Finocchio, M.; Moore, R.; Kipman, A.; Blake, A. Real-time human pose recognition in parts from single depth images. In Proceedings of the 24th IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 20­25 June 2011; pp. 1297­1304.
22. Joo, S.I.; Weon, S.H.; Hong, J.M.; Choi, H.I. Hand detection in depth images using features of depth difference. In Proceedings of the International Conference on Image Processing, Computer Vision, and Pattern Recognition (IPCV). The Steering Committee of the World Congress in Computer Science, Computer Engineering and Applied Computing (World Comp), Las Vegas, NV, USA, 22­25 July 2013; Volume 1.
23. Long, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks for semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017, 39, 640­651.
24. Ronneberger, O.; Fischer, P.; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, Munich, Germany, 5­9 October 2015; Springer: Cham, Switzerland; pp. 234­241.
25. Zhao, H.; Shi, J.; Qi, X.; Wang, X.; Jia, J. Pyramid scene parsing network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21­26 July 2017; pp. 2881­2890.
26. Peng, C.; Zhang, X.; Yu, G.; Luo, G.; Sun, J. Large kernel matters--Improve semantic segmentation by global convolutional network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21­26 July 2017; pp. 4353­4361.
27. Chen, L.C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; Yuille, A.L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell. 2017, 40, 834­848. [CrossRef] [PubMed]
28. Li, X.; Yang, Y.; Zhao, Q.; Shen, T.; Lin, Z.; Liu, H. Spatial pyramid based graph reasoning for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 16­18 June 2020; pp. 8950­8959.
29. Zhong, Z.; Lin, Z.Q.; Bidart, R.; Hu, X.; Daya, I.B.; Li, Z.; Wong, A. Squeeze-and-attention networks for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 16­18 June 2020; pp. 13065­13074.
30. Huang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y.; Liu, W. Ccnet: Criss-cross attention for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, Seoul, Korea, 27 October­3 November 2019; pp. 603­612.
31. Fu, J.; Liu, J.; Wang, Y.; Zhou, J.; Wang, C.; Lu, H. Stacked deconvolutional network for semantic segmentation. IEEE Trans. Image Process. 2019. [CrossRef] [PubMed]
32. Artacho, B.; Savakis, A. Waterfall atrous spatial pooling architecture for efficient semantic segmentation. Sensors 2019, 19, 5361. [CrossRef] [PubMed]
33. Sharma, S.; Ball, J.E.; Tang, B.; Carruth, D.W.; Doude, M.; Islam, M.A. Semantic segmentation with transfer learning for off-road autonomous driving. Sensors 2019, 19, 2577. [CrossRef] [PubMed]
34. Glorot, X.; Bordes, A.; Bengio, Y. Deep sparse rectifier neural networks. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, Naha, Okinawa, Japan, 16­18 April 2019; pp. 315­323.
35. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv 2014, arXiv:1409.1556.
36. Yosinski, J.; Clune, J.; Bengio, Y.; Lipson, H. How transferable are features in deep neural networks? In Proceedings of the 27th International Conference on Neural Information Processing Systems, Montreal, QC, Canada, 8­13 December 2014; pp. 3320­3328.
37. Kingma, D.; Ba, J. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations, ICLR, San Diego, CA, USA, 7­9 May 2015.
© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).


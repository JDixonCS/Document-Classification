930

IEEE TRANSACTIONS ON INFORMAI10N THEORY, VOL. 39, NO.3, MAY 1993

Universal Approximation Bounds for Superpositions of a Sigmoidal Function
Andrew R. Barron, Member, IEEE

Abstract- Approximation properties of a class of artificial

neural networks are established. It is shown that feedforward

networks with one layer of sigmoidal nonlinearities achieve inte
grated squared error of order O(l/n), where n is the number

of nodes. The function appruximated is assumed to have a

bound on the first moment of the magnitude distribution of

the Fourier transform. The nonlinear parameters associated

with the sigmoidal nodes, as well as the parameters of linear

combination, are adjusted in the approximation. In contrast, it

is shown that for series expansions with n terms, in which only

the parameters of linear combination are adjusted, the integrated

s1q/una2r/edd

approximation error cannot be made smaller than order uniformly for functions satisfying the same smoothness

assumption, where d is the dimension of the input to the function.

For the class of functions examined here, the approximation rate

and the parsimony of the parameterization of the networks are

surprisingly advantageous in high-dimensional settings.

Index Terms- Artificial neural networks, approximation of
functions, Fourier analysis, Kolmogorov n-widths.

A smoothness property of the function to be approximated is
expressed in terms of its Fourier representation. In particular,
an average of the norm of the frequency vector weighted by the
Fourier magnitude distribution is used to measure the extent
to which the function oscillates. In this Introduction, the result is presented in the case that the Fourier distribution has a density that is integrable as well as having a finite first moment. Somewhat greater generality is permitted in the theorem stated and proven in Sections III and IV.
Consider the class of functions f on Rd for which there is a Fourier representation of the form

f(x)

=

r JRd

eiw.x j(w)

dw,

(2)

for some complex-valued function j(w) for which wj(w) is integrable, and define

1. INTRODUCTION
APPROXIMATION bounds for a class of artificial neural networks are derived. Continuous functions on compact subsets of Rd can bc uniformly well approximated by linear combinations of sigmoidal functions as independently shown by Cybenko [1] and Hornik, Stinchcombe, and White [2]. The purpose of this paper is to examine how the approximation error is related to the number of nodes in the network.
As in [1], we adopt the definition of a sigmoidal function
¢;(z) as a bounded measurable function on the real line for
which ¢;(z) --* 1 as z --* 00 and ¢;(z) --+ 0 as z --* -00. Feedforward neural network models with one layer of sigmoidal units implement functions on Rd of the form

n

fn(x)

=

l:>k¢;(ak k=1

.

x

+

bk) +

Co

(1)

parameterized by ak E Rd and bk, Ck E R, where a . x
denotes the inner product of vectors in Rd. The total number of parameters of the network is (el + 2)'1,1 + 1 .

Manuscript received February 19, 1991. This work was supported b y ONR under Contract N00014-S9-J-1SI1. Material in this paper was presented at the IEEE International Symposium on Information Theory, Budapest, Hungary, June 1991.
The author was with the Department of Statistics, the Dcpartment of Electrical and Computer Engineering, the Coordinated Science Laboratory, and the Beckman Institute, University of Illinois at Urbana-Champaign. He is now with the Department of Statistics, Yale University, Box 2179, Yale Station, New Haven, CT 06520.
IEEE Log Number 920696fi.

Of = lRdlwllj(w)1 dw,

(3)

where Iwl = (w . w) 1/2. For each 0> 0, let rc be the set of functions f such that Cf ::; 0,

Functions with Cf finite are continuously differentiable on

Rd and the gradient of f has the Fourier representation

6.f(.7:) = jeiw'X6.f(W)dW,

(4)

where 6.f(w) = iwj(w). Thus, condition (3) may be in terpreted as the integrability of the Fourier transform of the gradient of the function f. In Section III, functions are permitted to be defined on domains (such as Boolean functions
on {O, I}d) for which it does not make sense to refer to differentiability on iliat domain. Nevertheless, the conditions
imposed imply that the function has an extension to Rd with a gradient that possesses an integrable Fourier representation.
The following approximation bound is representative of
the results obtained in this paper for approximation by linear combinations of a sigmoidal function. The approximation error is measured by the integrated squared error with respect to an arbitrary probability measure J.L on the ball Br = {x: Ixl ::; r} of radius r > O. The function ((1(z) is an arbitrary fixed sigmoidal function.
Proposition 1: For every function f with Of finite, and every n  1, there exists a linear combination of sigmoidal functions fn(x) of the form (1), such that

: r (f(X)
.fEr

- f,,(X))2J.L(dx)::;

c'

'

(5)

0018-9448/93$03.00 © 1993 IEEE

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSITIONS OF A SIGMOIDAL FUNCTION

931

where cj = (2rCf)2. For functions in rG, the coefficients
of the linear combination in (1) may be restricted to satisfy
2::=1 ICkl ::; 2rC, and Co = f(O).
Extensions of this result are also given to handle Fourier distributions that are not absolutely continuous, to bound the approximation error on arbitrary bounded sets, to restrict the parameters ak and bk to be bounded, to handle certain infinite-dimensional cases, and to treat iterative optimization of the network approximation. Examples of functions for which bounds can be obtained for Cf are given in Section IX.
A lower bound on the integrated squared error is given in Section X for approximations by linear combinations of fixed
basis functions. For dimensions d ;:: 3, the bounds demonstrate
a striking advantage of adjustable basis functions (such as used in sigmoidal networks) when compared to fixed basis functions
for the approximation of functions in rc.
II. DISCUSSION
The approximation bound shows that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated
squared error of order 0(1/11,), where 11, is the number of
nodes, uniformly for functions in the given smoothness class. A surprising aspect of this result is that the approximation
bound of order 0(1/11,) is achieved using networks with
a relatively small number of parameters compared to the exponential number of parameters required by traditional poly nomia' spline, and trigonometric expansions. These traditional expansions take a linear combination of a set of fixed basis functions. It is shown in Section X that there is no choice of 11, fixed basis' functions such that linear combinations of them achieve integrated squared approximation error of
smaller order than (1/n)C2/d) uniformly for functions in rc, in
agreement with the theory of Kolmogorov n-widths for other similar classes of functions (see, e.g., [3, pp. 232-233]). This
vanishingly small approximation rate (2/d instead of 1 in the exponent of 1/11,) is a "curse of dimensionality" that does not
apply to the methods of approximation advocated here for functions in the given class.
Roughly, the idea behind the proof of the lower bound result is that there are exponentially many orthonormal functions
with the same magnitude of the frequency w. Unless all of
these orthonormal functions are used in the fixed basis, there will remain functions in rG that are not well approximated. This problem is avoided by tuning or adapting thc parameters of the basis functions to fit the target function as in the case of sigmoidal networks. The idea behind the proof of the
upper bound result (Proposition 1) is that if the function has
an integrable representation in terms of parameterized basis functions, then a random sample of the parameters of the basis functions from the right distribution leads to an accurate approximation.
Jones [4] has obtained similar approximation properties for linear combinations of sinusoidal functions, where the frequency variables are the nonlinear parameters. The class
of functions he examines are those for which J li(w)ldw
is bounded, which places less of a restriction on the high frequency components of the function (but more of a restric-

tion on low-frequency components) than does the integrability

of Iwlli(w)l. In the course of our proof, it is seen that the integrability of Iw 11.1(w)I is also sufficient for a linear combina tion of sinusoidal functions to achieve the 1/11, approximation

rate. Siu and Brunk [5] have obtained similar approximation

results for neural networks in the case of Boolean functions on .
{O, l}d. Independently, they developed similar probabilistic

arguments for the existence of accurate approximations in their

setting.

It is not surprising that sinusoidal functions are at least as

well suited for approximation as are sigmoidal functions, given

that the smoothness properties of the function are formulated

in terms of the Fourier transform. The sigmoidal functions

are studied here not because of any unique qualifications in

achieving the desired approximation properties, but rather to

answer the question as to what bounds can be obtained for this

commonly used class of neural network models.

There are moderately good approximation rate properties in

high dimensions for other classes of functions that involve a

high degree of smoothness. In particular, for functions with

.r li(w)12IwI2s dw bounded, the best approximation rate for
the integrated squared error achievable by traditional basis

function expansions using order md parameters is of order

0(I/m)2S for m = 1, 2"", for instance, see [3] (for polynomial methods m is the degree, and for spline methods m is the number of knots per coordinate). If 8 = d/2 and n is of order md, then the approximation rates in the

two settings match. However, the exponential number of

parameters required for the series methods still prevent their

direct use when d is large.
Unlike the condition J lj(w)i2lwI2s dw < 00, which by Par
seval's identity is equivalent to the square integrability of all

partial derivatives of order 8, the condition J li(w)llwldw <

00 is not directly related to a condition on derivatives of the function. It is necessary (but not sufficient) that all first

order partial derivatives be bounded. It is sufficient (but not

necessary) that all partial derivatives of order less than or equal to . be square-integrable on Rd, where s is the least integer

greater than 1+ d/2, as shown in example 15 of Section IX. In

the case of approximation on a ball of radius r, if the partial

derivatives of order 8 are bounded on Br' for some r' > r,

then there is a smooth extension of f for which the partial derivatives of order 8 are square integrable on Rd, thereby

permitting the approximation bounds to be applied to this case.

Another class of functions with good approximation prop

erties in moderately high dimensions is the set of functions

with a bound on J(osdf(x)/ox ...OXd)2 dx (or equivalent,

.r IWl12s
rate of

· · 'IWdI2sli(wW order 0(1/11,)28

dw). For this
is achieved

class, an approximation
using 0(n(10gn)d-1)

parameters, corresponding to a special subset of terms in

a Fourier expansion (see Korobov [6] and Wahba [7, pp.
145-146]). Nevertheless, the (logn)d- 1 factor still rules out

practical use of these methods in dimensions of, say, 10 or

more.

Thus far in the discussion, attention is focused on the

comparison of the rate of convergence. In this respect, methods

that adapt the basis functions (such as sigmoidal networks) are
shown to be superior in dimensions d ;:: 3 for the class rc for

932

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 39, NO. 3, MAY 1993

any value of C, no matter how large. Now it must be pointed out that the dimension d can also appear indirectly through the constant Cf. Dependence of the constant on d does not affect the convergence rate as an exponent of lin. Nevertheless, if Cf is exponentially large in d, then an exponentially large value of n would be required for CJIn to be small for approximation by sigmoidal networks. If C is exponentially large, then approximation by traditional expansions can be even worse. Indeed, since the lower bound developed in Section X is of the form C2In(2/d), a superexponentially large number of terms n would be necessary to obtain a small value of the integrated squared error for some functions in re.
The constant Cf involves a d-dimensional integral, and it is not surprising that often it can be exponentially large in d. Standard smoothness properties such as the existence of enough bounded derivatives guarantee that Cf is finite (as dis cussed above), but alone they are not enough to guarantee that Cf is not exponentially large. In Section IX, a large number of examples are provided for which Cf is only moderately large, e.g., O(d1/2) or Oed), together with certain closure properties for translation, scaling, linear combination, and composition of functions. Since in engineering and scientific contexts it is not unusual for functions to be built up in this way, the results
suggest that fe may be a suitable class for treating many
functions that arise in such contexts. Otller classes of functions may ultimately provide better
characterizations of the approximation capabilities of artificial
neural networks. The class re is provided as a first step in
the direction of identifying those classes of functions for which artificial neural networks provide accurate approximations.
Some improvements to the bound may be possible. Note that there can be more than one extension of a function outside of a bounded set B that possesses a gradient with an integrable transform. Each such extension provides an upper bound for tile approximation error. An interesting open question is how to solve for the extension of a function outside of Br that yields the smallest value for IlwIIRw)ldw.
For small d, the bound (2rC)2In on the integrated squared error in Proposition 1 is not the best possible. In particular, for d = 1, the best bound for approximation by step functions is (rCt/n)2 (which can be obtained by standard methods using
the fact that, for functions in re, the absolute value of the derivative is bounded by C). For d > 1, it is recently shown
in [20] that the rate for sigmoidal networks cannot be better
than (1/n)1+(2/d) in the worse case for functions in Ie. Note
that the gap between the upper and lower bounds on the rates vanishes in the limit of large dimension. Determination of the exact rate for each dimension is an open problem.
The bound in the proposition assumes that {l is a probability measure. More generally, if {l is a measure for which {l(Br) is finite, it follows from Proposition 1 that
(6)
In particular, with the choice of It equal to Lebesgue measure, the bound is of order O(l/n), which is independent of d, but the constant {l(Br) is equal to the volume of the ball in d dimensions, which grows exponentially in d for r > 1.

In the case that the function is observed at sites

Xl, X
bound

2o,n.

. . , XN restricted the training error

to

Bro

Proposition

1

provides

a

I
N

N
 0 (t(Xi)
i=l

-

A
!n(X

i)) 2

-:;-c:'j;; ,

(7)

where the estimate in = in, N of the form (1) is chosen to
minimize the sum of squared errors (or to achieve a somewhat
simpler iterative minimization given in Section VIII). In this case, the integral in Proposition 1 is taken to be with respect to the empirical distribution.
The implications for the generalization capability of sig
moidal networks estimated from data are discussed briefly.
There are contributions to the total mean squared error IBr (fin)2 dlt from the mean squared error of approximation IBr (t fn)2 d{l and the mean squared error of estimation IBr (tn -
in)2 d{l. An index of resolvability provides a bound to the total mean squared error in terms of the approximation error and the model complexity according to a theorem in [8] and [9] (see also [10] for related results). In [11], the approximation result obtained here is used to evaluate tIlis index of resolvability
for neural network estimates of functions in r, assuming a smoothness condition for the sigmoid. There it is concluded
that statistically estimated sigmoidal networks achieve mean squared error bounded by a constant multiple of CJIn+ (ndIN)logN. In particular, with n  Cj(NI(dlogNW/2, the bound on the mean squared error is a constant times
Cj«dIN) logN)1/2.ln the theory presented in [11], a bound of the same form is also obtained when the number of units n is not preset as a function of tile sample size N, but rather it is optimized from the data by the use of a complexity
regularization criterion.
Other relevant work on the statistical estimation of sig moidal networks is in White [12] and Haussler [13] where metric entropy bounds play a key role in characterizing the
estimation error. For these metric entropy calculations and for the complexity bounds in [11], it is assumed that domain bounds are imposed for the parameters of the sigmoidal net
work. In order that the approximation theory can be combined with such statistical results, the approximation bounds are
refined in Section VI under constraints on the magnitudes of
the parameter values.. The size of the parameter domains for the sigmoids grows with n to preserve the same approximation rate as in the unbounded case.
For the practitioner, the theory provides the guidance to choose the number of variables d, the number of network nodes n, and the sample size N, such that lin and (ndIN)log N are small. But there are many otller practical issues that must be addressed to successfully estimate network
functions in high dimensions. Some of these issues include
the iterative search for parameters, the selection of subsets of
terms input to each node, the possible selection of higher order
terms, and the automatic selection of the number of nodes on the basis of a suitable model selection criterion. See Barron and
Barron [14] for an examination of some of these issues and the relationship between neural network methods and other

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSITIONS OF A SIGMOIDAL PUNCTION

933

methods developed in statistics for the approximation and
estimation of functions.
After the initial manuscript was distributed to colleagues, the methods and results of this paper have found application
to approximation by hinged hyperplanes (Breiman [15]), slide
, functions (Tibshirani [16]) projection pursuit regression (Zhao , [17)) radial basis functions (Girosi and Anzellotli [18)), and
the convergence rate for neural net classification error (Farago
and Lugosi [19]). Moreover, the results have been refined to
give approximation bounds for network approximation in Lp
norms, 1 < p < 00 (Darken et at. [31], in the Loo norm
(Barron [20], Yukich [32]) and in Sobolev norms (Hornick et
at. [21)).
Approximation rates for the sigmoidal networks have re
cently been developed in McGaffrey and Gallant [22],Mhaskar

(9) is finite,

is used instead of (8) since then the required

integrability follows from leiw.x - 11 ::;: 21w . xl ::;: 2lwllxl.

(See the Appendix for the characterization of the Fourier
representation in this context.) The class of functions on Rd

J for which Cf = IwIF(dw) is finite is denoted by r.

of

Functions are their domain

aipnprRodx.imLaettedBonbeboaunbdoeudndmeedassuertabinleRsudbstheatst

contains the point x = 0, and let fB be the set of functions

f on B for which the representation (9) holds for x E B for

some complex-valued measure F(dw) for which .r IwIF(dw)

is finite, where F is the magnitude distribution corresponding

to F. (The right side of (9) then defines an extension of

r, the function 1 from B to Rd that is contained in

and

F may be interpreted as the Fourier distribution of an a

continuously differentiable extension of 1 from B to Rd. Each

and Micchelli [23], and Kurkova [33] in the settings of more traditional smoothness classes that are subject to the
[ ] curse of dimensionality. Reference 22 also gives implications
for statistical convergence rates of neural networks in these
] settings. Jones [24 gives convergence rates and a set of "good

such extension provides a possible Fourier representation of 1

on B.)

I(x) ,

of

Linifnienaitreflyundcitfifoenresntiable=fuan·cxtioannsdo, nmRorde

generally, the class are not contained in

r, but they are contained inrB when restricted to any bounded

weights" to use in the estimation of almost periodic functions.
1 Zhao [ 7] gives conditions such that uniformly distributed
weight directions are sufficient for accurate approximation. A challenging problem for network estimation is the opti
mization of the parameters in high-dimensional settings. In Section VIII, a key lemma due to Jones [4] is presented that permits the parameters of the network to be optimized one node at a time, while still achieving the approximation

set B (because such functions can be modified outside of the

r; bseotunodf efdunscettiotonsproonduRcde

a function in see Section IX). The with this property of containment in

rB for every bounded set of B is denoted for convenience
by r. = nBrB.

> 0, For each C

let ra B be the set of all functions 1 in

fB  such that for some F eprescnting 1 on B,

bound of Proposition 1. This result considerably reduces the computational task of the parameter search. Nevertheless, it is

jIWIBF(dW) ::;: C,

(10)

not known whether there is a computational algorithm that can be proven to produce accurate estimates in polynomial time as a function of the number of variables for the class of functions studied here. We have avoided the effects of the curse of dimensionality in terms of the accuracy of approximation but not in terms of computational complexity.

where IwlB =
{:r,: Ixl :::; r},

SUPxEB Iw
this norm

. xl· In the case of simplifies to IwlBr

the =

ball Br =
rlwl. (See

Section V for the form of the bound for certain other domains

such as cubes.)

The main theorem follows; a bound is given for the inte

grated squared error for approximation by linear combinations

III. CONTEXT AND STATEMENT OF THE THEOREM
In this section, classes of functions are defined and then the main result is stated for the approximation of these functions by sigmoidal networks. The context of Fourier distribution permits both series and integral cases. A number of interesting

of a sigmoidal function.
Theorem 1: For every function f in rB, a,every sigmoidal function ,p, every probability measure j.L, and every n :2: 1,
fn(x) there exists a linear combination of sigmoidal functions
of the form (1), such that

examples make use of the Fourier distribution, as will be seen

in Sections VII and IX.
The Fourier distribution of a function f(x) on gt is a unique complex-valued measure F(dw) = ei8(c;) F(dw), where F(dw) denotes the magnitude distribution and B(w) denotes the phase at the frequency w, such that

The coefficients of the linear combination in (1) may be
restricted to satisfy 2:=1 ICkl :::; 2C, and Co = 1(0).

(8)

or, more generally,

f(x) = f(O) + j(eiW'X -l)F(dw),

(9)

for all x E Rd. If J F(dw) is finite, then both (8) and (9) are valid and (9) follows from (8). Assuming only that J IwlF(dw)

IV. ANALYSIS Denote the set of bounded multiples of a sigmoidal function composed with linear functions by
G¢ = b¢(a ·x+ b): hi::;: 2C, a E Rd, bE R}. (12)
r For functions 1 in a, B, Theorem 1 bounds the error in the
approximation of the function J(x) = I(x) - 1(0) by convex
combinations of functions in the set G¢.

934

IEEE TRANSACfIONS ON INFORMATION THEORY. VOL 39, NO, 3, MAY 1993

Proofof Theorem 1: The proof of Theorem 1 is based on the following fact about convex combinations in a Hilbert space, which is attributed to Maurey in Pisier [25]. We denote the norm of the Hilbert space by II . II.
Lemma 1: If 7 is in the closure of the convex hull of a set G in a Hilbert space, with Ilgll ::; b for each 9 E G, then for every n  1, and every c' > b2-117112, there is an In in the convex hull of 71, points in G such that
(13)

fo = Re (eiwx -l)eiOCw) F(dw)

fo= (cos (w· x+B(w»- cos (B(w»)F(dw) 1- - Il -GW 1 -f,1B-B (cos(w.7. :+B(w»

fo- cos (B(w»)A(dw)
= g(x, w)A(dw),

(14)

Proof" A proof of this lemma by use of an iterative approximation, in which the points of the convex combination are optimized one at a time, is due to Jones [4]. A slight

for xE B, where Gf,B = J IwIBF(dw) ::; G is tile integral
assumed to be bounded; A(dw) = IwIBF(dw)/Gf,B is a probability distribution; IwlB = sUPxEBIw.xl; and

refinement of his iterative Hilbert space approximation the orem is in Section VIII. The noniterative proof of Lemma 1

g(x,

w) =

Gf,B IwlB

(cos

(w·

x

+

B(w» -

cos

(B(w»).

(15)

(credited to Maurey) is based on a law of large numbers bound

as follows. Given n  1 and 8 > 0, let 1* be a point in the

convex hull of G with 117-1* II ::; 8/71,.Thus, 1* is of the

form some

lsu:fZf'=ic1ie'nYktlygk

with large

gk
m.

E G,
Let 9

Yk  0, l:Z'=1 'Yk be randomly drawn

= 1, from

for the

Note that these functions are bounded by Ig(x, w) 1 ::; Glw, xl/lwlB ::; G for x in Band w =J O.
The integral in (14) represents 7 as an infinite convex
combination of functions in the class

{  set {g, ...,g::'} with P{g = gk} = 'Yk; let gl, g2," ',gn
be independently drawn from the same distribution as g;

Geos =

lwB (cos (w· x+ b) - cos (b»: w=J 0,

and let In = (l/n)l:=1 g; be the sample average. Then

I'YI ::; G, bE R}. (16)

Efn = 1*, and the expected value of the squared norm of

the error is Eil/n - 1*112 = (l/n)Ellg-1*112, which equals (l/n)(EllgI12-11I'112)and is bounded by (1/n)(b2-111*112).

Since the expected value is bounded in this way, there must

e1x11i*st112g).l,Ugsi2n,'g" til,egt"riafnogr lewihniecqhuaIlIiIt"y

-1*112 ::; and 117-f*

(1/n)(b2II ::; 8/n, the

proof of Lemma 1 is completed by the choice of a sufficiently

It follows that 7 is in the closure of tile convex hull of Geos.

This can be seen by Riemann-Stieltjes integration theory in

the case that F has a continuous density function on Rd.

More generally, it follows form an L2 law of large numbers.

Iinnddeepeden, diefnWtlyI,dWra2w,.n..fr,oWmn

is the

a random sample of n points, distribution A, then by Fubini's

small 8.

0 Theorem the expected square of the L2(/-l, Br) norm is

Fix a bounded measurable set B that contains the point x = 0 and a positive constant G. If it is shown that for functions in the class rC,B, the function 7(x) = I(x)-1(0)
is in the closure of the convex hull of G.p in L2(p" E), then it
will follow by Lemma 1 that there exists a convex combination
of nsigmoidal functions such that the square of the L2(p" B) norm is bounded by a constant divided by n. Therefore, the
main task is to demonstrate the following theorem.

Theorem 2: For every function I in rc, B, and every sigmoidal function 1;, the function f(x) - 1(0) is in the
closure of the convex hull of G1>' where the closure is taken in TLhe2(m/-l,eBtho».d used here to prove Theorem 2 is motivated by the techniques used in Jones [4] to prove convergence rate results for projection pursuit approximation, and in Jones [261 to prove the denseness property of sigmoidal networks in the space of continuous functions.

Proof" Let F(dw) = ei&(wJF(dw) denote the magnitude

and phase decomposition in the Fourier representation of an
cxtension of the function I on Bfor which J IwIBF(dw)::; G.
Let n = {w E Rd:w =J O}.

From the Fourier representation (9) and the fact that f(7. :)

jeeiW'X is real-valued, it follows that

f(.7:) - f(O) = Re

-l)F(dw)

n

(17)

Thus, the mean value of convex combination of n

tphoeinstqsuianreGd eLos2c(/o-l,nB ver)gneosrmto

of a zero

as n --> 00. (Note that it converges at rate O(l/n) in

accordance with Lemma 1.) Therefore, there exists a sequence

of convex combinations of points in Geos that converges to 7

in

LLe2m(/m-l, aB2:).

We have For each

proven the following. I in LC,B, the function

f(x)

-

f(O)

is in the closure of the convex hull of Geos. Next it is shown that functions in Geos are in the closure
of the convex hull of G</;. The case that 1; is the unit step

function is treated first.

Each function in Geoe is the composition of a one dimensional sinusoidal function g(z) = 'Y /lwIB(cOS (lwlBz+ b)-cos (b))and a linear function z = a·x, where a = w/lwlB
for some w=J O. For x in E, the variable z = a·xtakes values

in a subset of [-1, 1]. Therefore, it suffices to examine tile

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSITIONS OF A SIGMOIDAL FUNCTION

935

approximation of the sinusoidal function 9 on [-1, 1]. Note that 9 has derivative bounded by h'l  C. Now since 9 is uniformly continuous on [-1, 1], it follows that it is uniformly

well approximated by piecewise constant functions, for any
sequence of partitions of [-1, 1] into intervals of maximum

width tending to zero. Such piecewise constant functions may

be represented as linear combinations of unit step functions.

Moreover, it can be arranged that the sum of the absolute

yalues of the coefficients of the linear combination are bounded
by 2C. In particular, consider first the function g(z) restricted to
o  z  1, and note that 9 = (0) = O. For a partition o = to < tl < ... < tk = 1, define

k-l

gk,+(Z) = )9(ti) - g(ti-d)l{zt,}.

(18)

i=1

This piecewise constant function interpolates the function
9 at the points ti for i  k - 1. Note that gk, + is a
linear combination of step functions. Now since the deriva
tive of 9 is bounded by C on [0, 1], it follows that the sum of the absolute values of the coefficients Li Ig(ti) g(ti-l)1 is bounded by C. In a similar way, define gk, _(z) = L:ll(g(-t;)-g(-ti-d)l{z:O;-t,}. Adding these components gn, -(z) + gn, +(z) yields a sequence of piecewise constant functions on [-1, 1] that are uniformly dose to 9(z) (as
the maximum interval width tends to zero), and each of
these approximating functions is a linear combination of
step functions with the sum of the absolute values of the
coefficients bounded by 2C. It follows that the functions g(z)
are in the closure of the convex hull of the set of functions 'Y
step (z-t) and'Y step (-z-t) with I'YI  2C and It I  1, where step (z) = l{zo} denotes the unit step function. Defining

Gstep = bstep (0:' X - t): I'YI  2C, lodB = 1, ItI  I}, (19)
the following lemma has been demonstrated, where the closure
property holds with the supremum norm on B, and hence with respect to L2(/1, B).
Lemma 3: Gcos is in the closure of the convex hull of
Gstep.

It can be seen that Lemma 3 continues to work if, for each 0:, the parameter t is restricted to a subset T", that is dense in [-1,1]. In particular, restrict t to the continuity points of the
distribution of z = 0: . x induced by the measure /1 on Rd. Let
Gtep be the subset of step functions in G.tep with locations
t restricted in this way. Then the following result holds.
Lemma 31: Gcos is in the closure of the convex hull of

Gep·

Functions in Gep are in the closure sigmoidal functions, taking the closure in

of the class of
L2(/1, B). This

follows by taking the sequence of sigmoidal functions ¢>(IaI(0:'

- t» X

with lal --+ 00. This sequence has pointwise limit

. - equal . 0: x

to
-

step
t =

(0: 0,

x t) (except possibly for x
which has p, measure zero by

in the set with the restriction

imposed on t). Consequently, by the dominated convergence

theorem, the limit also holds in L2(p" B). Thus the desired

closure property holds.

Lemma 4: Gep is in the closure of Gq,. Together, Lemmas 2, 31, and 4 show that in L2(/1, B),

r c coGcos C coGtep C coGq"

(20)

where coG denotes the closure in L2(/1, B) of the convex hull

r& r of G, and

is the set of functions in e with I(0) = O.

Here the fact is used that the closure of a convex set is convex,

so that it is not necessary to take the convex hull operation

twice when combining the Lemmas. This completes the proof

of Theorem 2.

D

The proof of Theorem 1 is completed by using Lemma 1

abnedtaTkheneotroemequ2a. lT(o2Cse)e2

that the constant in Theorem 1 can
for functions I in re, B, proceed as

follows. The approximation bound is trivially true
where f(x) - 1(0,) for then I(x) is equal to a

if 11111 =
constant

0,
/1-

> almost everywhere on B. So now suppose that 11111 O. If

the sigmoidal function is bounded by one, then the functions in

Gq, (2Ca)r2e-bo11u1n11d2edis

by a

b = 2C. Consequently, any cl greater than
valid choice for the application of Lemma

1. The conclusion is that there is a convex combination of n

Gq, functions in

for which the square of the L2(/1, B) norm

of the approximation error is bounded by d/n.

If the sigmoidal function ¢>(x) is not bounded by one, first use Lemma 1 and the conclusion that r& c coGtep to obtain

a convex combination of n functions in Gtep for which the

sbqyu(a2r0ed)2L-2((/11,/2B)11)1no112rmdiovfidthede

approximation error is bounded by n. Then, using Lemma 4, by

a suitable choice of scale of the sigmoidal function, sufficiently

accurate replacements to the step functions can be obtained

such that the resulting convex combination
Gq, yields a square L2(/1, B) norm bounded

of by

(n20fu)n2c/tnio.nTshiins

completes the proof of Theorem 1.

D

Note that the argument given above simplifies slightly in the
. . case that the distribution of 0: x is continuous for every a,
for then Gstcp can be used in place of Gtep and there would
be no need for Lemma 31·

A variant of the theory just developed is to replace the

function step(z) with the function stepq,(z), which is the same

as the unit step function! except at z = 0 where it is set to equal

1>(0). By a modification of the proof of Lemma 3, it can be

G Gstep shown that cos is in the closure of the convex hull of

.

The advantage of this variant is that Gstep/> is in the closure

of Gq" without any restriction on the location of the points

tin [-1,1]. But if I¢>(O)I > 1, then an additional argument

would still be needed (as above, where t is restricted to the

clcontinuity can be

points of a taken to be

. x)
not

in order to larger than

(s2h0ow)2.that

the

constant

V. APPROXIMATION ON OTHER BOUNDED DOMAINS IN Rd
In thi brief section, the form of the constant Cf, B = J IwIBIII(w)ldw in the approximation bound is determined for
various choices of the bounded set B other than the Euclidean
ball of radius r mentioned in the Introduction.
Recall that, by definition, IwlB = 8UP"EB Iw . xl. The interpretation is that IwlB bounds the domain of the trigono-

936

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 39, NO.3, MAY 1993

metric component eiw.x that has frequency w in the Fourier representation of the function restricted to x in B.
Clearly, if B is contained in a ball of radius r, that is, if
Ixl ::; r for x in B, then, by the Cauchy-Schwarz inequality, IwlB ::; rlwl. Thus,

(21)

However, for some natural sets B, a fairly large radius ball

would be required for application of the bound in that form.
It is better to determine IwlB directly in some cases. If B is a

multiple of a unit ball with respect to some norm on Rd, then

IwIB is determined by the dual norm. In particular, if B = Boo, r = {x: Ixloo ::; r} is the 1= ball

of radius r (the cube centered then IwlB = rlwll where Iwll

at x = 0 with
is the h norm

sidelength and

2r),

(22)

More generally, if B = Bp, r = {x: Ixlp ::; r} is the lp ball of radius r, then IwlB = rlwlq where l/p + 1/q = 1 (as can be
seen by a standard application of HOlder's inequality). Here the
Ip norm is given by Ixlp = CL:=l IXiIP)l/p for 1 ::; p < x, and Ixloo = maxi IXiI for p = x. Thus,

(23)

The approximation bound becomes
.Lv. r(f(x) - fn(X))2MCdx)
::; (2:)2 (./lwlqIJ(w)1 dW) 2, (24)
for some network fn of the form (1).
Note also that the center of the domain of integration may
be taken to be any point Xo in B not necessarily equal to O.
(This follows by a simple argument, since the magnitude of the Fourier transform is unchanged by translation.) In particular, for any cube C of side length s, the result becomes

for some network fn of the form (1). In like manner, a scaling
argument shows that if Rect is any rectangle with side lengths
SI, S2,"', Sd, then there is a network fn such that
.Lect(f(x) - fnCX»2MCdx) : ;  (S}IWiIIJ(w)1 dW) 2 (26)
In general, for a bounded set fl, the point Xo to take for the centering that would lead to the smallest approximation bound
is one such that Cf, B, Xu = J IwIB, Xo IJ(w)ldw is minimized where IwlB,xo = sUPxEB Iw· (x - xo)l. In this context , the
./ representation (4) would become fCx) = f(xo)+ (eiO!x - eiw.xO)J(w) dw.

VI. REFINEMENT
In the above analysis , the approximation results were proved
by allowing the magnitude of the parameters ak to be ar bitrarily large. The absence of restrictions on lakI yields a
difficult problem of searching an unbounded domain. Large
values of IakI contribute to large gradients of the sigmoidal
function which can also lead to difficulties of computation.
In this section, we control the growth of lak I and bound
the effect on the approximation error. Knowledge of the relationship between the magnitude of the parameters and the accuracy of the network makes it possible to bound the index of resolvability of sigmoidal networks as in [11]. Bounds on the parameters are also required in the metric entropy computations as in White [12, Lemma 4.3] and Haussler [13] .
Given r > 0, C > 0 and a bounded set B, let
G¢,.,. = b4>(r(a· x + b)): hi::; 2C, lalB ::; 1, Ibl ::; I}. (27)

This is the class of bounded multiples of a sigmoidal function,
with the scale parameter of the sigmoid not larger than r. We
desire to bound the approximation error achievable by convex
combinations of n functions in Gq".,..

Theorem 3: For every f E re, B, r > 0, n 2: 1, every

probability measure po, and every sigmoidal function 4> with
°S 4>(x) S 1, there is a function fn in the convex hull of n
functions in G¢,.,. such that

( D.,.) ; 111- fnll ::; 2C 71, /2 +

(28)

where 11·11 denotes the L2(M, B) norm, lCx) = f(x) - f(O),
and

Here, D.,. is a distance between the unit step function and the
scaled sigmoidal function . Note that D.,. --; 0 as r --; 00.
If 4> is the unit step function, then b.,. = 0 for all r > 0, and Theorem 3 reduces to Theorem 1.
If 4> is the logistic sigmoidal function

1

4>(z) = l+e-z'

(30)

then 14>(rz)+I{z>o}1 S e-TE for Izl2: E. Setting f = (lnr)/r
yields

(c}.,.S - 1+r2- 1nr-

(31)

Therefore, if we set r 2: n1/21n 71" then from Theorem 3, for
functions f in rc, B,

(32)

Similar size of

rcnornecqluusirieodnstohporldesefrovreotthheerorsdigerm(oli/dnal)1f/u2ncatpipornosx. iTmhae

tion depends on the rate at which the sigmoid approach the

limits of 0 and 1 as x--; ±oo.

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSITIONS OF A SIGMOIDAL FUNCTION

937

The proof of Theorem 3 is based on the following result
for the univariate case. Let g(z) be a function with a bounded derivative on [- 1, 1]. Assume that 0 is in the range of the function g. Let 9r denote a function in the convex hull of
Gq"r'
Lemma 5: If 9 is a function on [-1,1] with derivative
r bounded by a constant C, then for every > 0,

infgTEeoG·. T sUPlzlr Ig(z) - gr(z)1  2C or.

(33)

Proof: The proof of Lemma 5 is as follows. Given
o < t  1/2, let k be an integer satisfying (lit) - 1 < k  liE. Partition [-1, 1] into k intervals of width 21k. Then
approximate the function 9 by a linear combination of k unit
step functions as in the proof of Lemma 3. Since the derivative
is bounded by C, the error of the approximation is bounded
by 2C/k for all z in [-1, 1]. Replacing each step function
l{ztd or l{z$td by the sigmoidal function </>(r(z - til) or </>(-r(z - til), respectively, one obtains a function gr in the
convex hull of k functions in Gq" r, which has error bounded
by

Ig(z)

- gT(z)1



2C T

+2CsIJPIYI1/k

i</>(ry)

i -l{y>o} ,
(34)

for every z in [- 1, 1], where 2CIk bounds the contribution

to the error from the replacement of the unit step function

by the z, and

sigmoidal function centered at
2CsuPIYI1/k I</>(ry) -I{y>o}1

the point ti closest to
bounds the cumulative

errors from the other sigmoidal functions (each of which is

centered at distance at least 11k from the point z). Since

E  11k  E/(l - f), it follows that

Ig(z)

-

gT(z)1



2C 1

t
_

E

+2CsuPIYIrt i</>(ry) -l{y>ol i · (35)

Using E/(I - t)  2dor O < t  1/2, and taking the infimum,

completes the proof of Lemma 5.

0

Proof of Theorem 3: The proof of Theorem 3 is as fol

1 lows. From Lemma 2, is in the closure of the convex hull
of functions in Geos. The functions in Geos are univariate
z functions g(z) evaluated at a linear combination = a· x with
Ig'(z)1  C and Izl  1. Each such function 9 is approximated
by a function gr as in Lemma 5 with supremum error bounded

by a quantity arbitrarily close to 2COr. It follows that there is a

function fT in the closure of the convex hull of Gq" r such that 111- fTIl  2CoT. From Lemma 1 there is an fn in the convex
i hull of n points in Gq" T such that l fT - fnll  2Cln1/2· By
the triangle inequality, this completes the proof of Theorem

3.

0

VII. EXTENSION

An extension of the theory is to replace Rd by a (possibly
infinite dimensional) Hilbert space H, where now w· x denotes

the Hilbert space inner product, and I . I denotes the Hilbert

H space norm. For instance, may be the space L2 of square

integrable signals (x(t), 0  t  1) with inner product
( w . x = I01 w(t)x t) dt. A real-valued function f(x) of the

E H signal x

is to be approximated. The Fourier representation

we require is that there is a complex-valued measure F(dw) = ei8(w)F(dw) on H such that f(x) = JH eiw.ItF(dw) or f(x) = f(O)+ JH(eiw.1t - I)F(dw).

Theorem 4: Let lex), x E H be a function on a Hilbert

space every

HsigmwiotihdaCl ffu= ncJtiHonIw</l>Fo(ndRw)1

< 00; then for every r > 0,
, every probability measure

jJ, on H, and every n  1, there is a linear combination of sigmoidal functions fn(x) = L=l ck</>(ak . x+ bk)+ co, such that JnJf(x) - fn(x»2jJ,(dx)  (2rCf)2/n, where
} Br = {x E H: Ixl  r is the Hilbert space ball of radius r.

The parameters ak take. values in the Hilbert space, while

the other parameters are realcvalued. With modification to the

approximation bound, the norms of the parameters may be

restricted in the same way as in Theorem 3.

For an interesting class of examples in this Hilbert space

setting, let R(t, s) be a positive definite function on [0, 1]2 (a

valid covariance function for a Gaussian process on [0, 1],and suppose for simplicity that R(t, t) = 1. Let f be the function

defined by

{- } f(x) =exp 101101x(s)x(t)R(s, t) ds dt/2

(36)

for square-integrable x on [0, 1]. Note that f(x) is the characteristic fimction of the Gaussian process (w(t), 0  t  1) with mean zero and covariance R(s, t) = E(w(s)w(t)):
jthat is, if F is the Gaussian meas)lre on w, eiwoxF(dw) = E[eiwolt] { } =exp -1011o\;(S)x(t)R(s, t) ds dt/2

= fCc).

(37)

Now, from the identity E(w2(t)) = R(t, t) = 1, it follows
that Elwl2 = J01 Ew2(t) dt = 1. Therefore, the constant Cf
in the approximation bound satisfies
j Cf = lwIF(dw) = Elwl

 (ElwI2)1/2

=1.

(38)

Thus, for function

<a/>noynprRobla, biitliftoylmloewassufrreojmJ,

on x
the

and for any sigmoidal theorem that for this

infinite-dimensional example there exists fn(x) such that

r (I(x)
J{lxlSll

-

fn(X»)2jJ,(dx)



n.

(39)

An even more general context may be treated in which

the nonlinear functions are defined on a normed linear space.

Vt B be a bounded subset of a normed linear space X, and

let w take values in a set of bounded linear operators on X

(the dual space of X). Now w 0 x denotes the operator w applied to x and IwlB = 8UPxEB Iw . xl denotes the nanD. of the operator restricted to B. If there is a meflSurable set

of w's and some complex-valued measure F(dw) on this

set, such that
J eiwx F(dw)

the or

function
f(x) =

f has the representai.?n I(x) = f(O) + J(eiwox I)F(dw) with

938

IEEE TRANSACfIONS ON fNFORMAIlON THEORY, VOL. 39, NO. 3, MAY 1993

Gf, B = f lw IB IF(dw) 1 finite, then for every n > 1, every probability measure {t on X, and every sigmoidal function cP, there will exist In = 2:=1 ckcP(ak ' x + bk) + Co such that

r
JB

(I

(x)

-

In (x))2{t(dx)

::;

(2Gfn, B )2 ,

(40)

where now the ak 's take values in the dual space of X.

One context for this more general result is the case that

X is the set of bounded signals {x: supt lx(t) l ::; r}, and w · x

(x
=

(

tI),o1

0 w

::; (t

)

t x

(

::; t)

1) dt,

,

B = where

the bounded functions on

linear operators w [0, 1]. Then Iw l ll

are
=

ridfeolntIwifi(etd) 1

with integrable dt, the Fourier

distribution F(dw) would be a measure supported on the set

of w in Llo and the ak 's would be integrable functions on

[0, 1].

VIII. ITERATIVE APPROXIMATION
In this section, it is seen that the bounds in Theorems
1, 3, and 4 can be achieved by an iterative sequence of
approximations taking the form

lI aln-l +ag - III . The iterations (42) are initialized with 0:1 = 0, so that h is a point gl in G that achieves a nearly
minimal value for Ilgl - I II. Note that In> as defined in (42), is in the convex hull of the points g1 , . . . , g".
Jones [4] showed that if I is in the closure of the convex
hull of G, then Illn - 11 12 ::; O(l/n ), for the sequence of
approximations defined as in (42). Here Jones' theorem and proof are presented with a minor refinement. The constant in the approximation bound is improved so as to agree with
the constant in the noniterative version (Lemma 1). As noted by Jones, the error Ilaln-l +(5g - III need not be exactly
minimized; here it is shown that it is enough to achieve a
value within O(1/n)2 of the infimum on each iteration.
Theorem 5: Suppose I is in the closure of the convex hull of a set G in a Hilbert space, with Ilgll ::; b for each 9 E G.
Set bJ = b2 - 1 1 / 1 12. Suppose that h is chosen to satisfy
I l h - 1 1 1 2 ::; infgEG IIg - 1 1 1 2 + El and, iteratively, In is chosen to satisfy
I l fn - f l 1 2 ::; info::;o9 infgEG lIaln- l + (5g - 1 112 + En (43)
where (5 = 1 - a, c' 2: bJ, p = c'lbJ - 1 , and

The optimization is restricted to the parameters an, "fn, an, and bn of the nth node, with the parameter values from ear lier nodes held fixed. This iterative formulation considerably

reduces the complexity of the surface to be optimized at each

step.

This reduction in the complexity of the surface is particu

larly useful in the case that the function I is only observed

taitvesitaepsprXoxli,mXat2i,o"n

'

, XN theory

in a bounded set n. The itera shows that to find an estimate

wi.th average squared error bounded by(11N) 2:;r=v 1 ( l eXi) -

In (Xi))2 ::; c'ln, it suffices to optimize the parameters of the

network one node at a time, Avoiding global optimization has

computational benefits. The error surface is still multimodal

as a function of the parameters of the nth node, but there is

a reduction in the dimensionality of the search problem by

optimizing one node at a time.

A recent result of Jones [4] on iterative approximation in a

Hilbert space is the key to the iterative approximation bound

in the neural network case. As in the noniterative case, the
applicability of Jones' Theorem is based on our demonstration
that functions in r are in the closure of the convex. hull of

G¢.
To avoid cluttering the notation in this section, the notation
I (instead of ]) is used to denote the point to be approximated
by elements of the convex hull. As before, for the application

to the approximation by sigmoidal networks of functions in

rC, one subtracts off the value of the function at x = ° to obtain the function in r which is approximated by functions

in the convex hull of G¢. Let G be a subset of a Hilbert space. Let In be a sequence
of approximations to an element I that take the form

(42)

where an = (1 - un) and 0 ::; an ::; 1 and gn E C. Here
an and gn are chosen to achieve a nearly minimal value for

En

::;

pc' n(n + p )"

(44)

Then for every n > 1,

III

-

In l1 2

::;

c'
n -

.

(45)

Proof' The proof of Theorem 5 is as follows. We show
that if I is in the closure of the convex hull of G, then, for
any given In- l and 0 ::; a ::; 1 ,

giEnGf I l aln- l +ag - 1 11 2

= infgEG Ila(ln- l - I) + (5(g - 1) 112

::; a2 1 1 /n_ l - 1 1 1 2 + ( 1 - a)2bJ .

(46)

The proof is then completed by setting a = b}/(b} + II In- l -
1 1 12 ) to get

I I ln

-

III 2

::;

bJ l l
bJ +

/n I l /n

l -

-1 l _

112 111

2

+En

,

(47)

or, equivalently,

- Illn---11-11,2-, ,--- - En

2:

1 I l /n-l - III 2

+

1 2 bf '

(48)

Equation (48) provides what is needed to verify that Illn 1 1 12 ::; c'ln by an induction argument. Indeed, (46) with Q; = 0 shows that the desired inequality is true for n = 1. Suppose Il/n-1 - 1 1 1 2 ::; c'/(n - 1), then plugging this into (48) and using c' = b} ( 1 + p) yields

- Illn---11-112-- -- E-n

2:

1
Illn- l -

1 III 2 +bf2

>-

-n (- ;-, 1

+

1
-b;'

n+p

(49)

c'

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSlTlONS OF A SIGMOIDAL FUNCTION

939

En Reciprocating and using the assumed bound on yields

IIfn - fl12 :s; - n c+' p + En

-

c'
-n

-

n

C'p (n +

p)

+

E n

C' <- -n .'

(50)

as desired.
0, r Thus, it remains to verify (46). Given 8 > let be a point in the convex hull of G with Ilf - 1* 11 :s; 8. Thus r is of the form L;;'=l "'(k9Z with gZ E G, rk ;:: 0, L;;'=l rk = 1,
for some sufficiently large m. Then,

lIa(fn-1 - 1) + a(g - 1)11 :s; Ila(fn-l - 1) + Ci(g - r)1I + 8. (51)

Expanding the square yields

lIa(fn-1 - 1) + a(g - 1*)112

= a2l1fn_1 - fl12 + a211g - 1*112

+ 2aa(fn-1 - f, 9 - f*),

(52)

where (., .) denotes the inner product. Now the average value
9 of the last two terms is, for E {gi, " " g;',},

m
Lrk(a21lgk - f*1I2 + 2aa(fn-l - f, gk - f*» k=l
m
= a2Lrkllg;: - f*112 + 0
( ) k=l
= a2 lkI19kIl2 _ 111*112

:s; a2(b2 - 111*112).

(53)

Since the average value is bounded in iliis way, there must
exist 9 E {gi, ' ' ' ' g;;'}, such that

lIa(fn-l - 1) + a(g - f*)112 :s; a2l1fn_l - fl12 + a2(b2 - 1If*112). (54)

Now by the triangle inequality, 111* II > IIfll - 8. So using (51) 0, and letting fj -> it follows that
infgEG lIa(fn-l - f) + a(g - 1)112 :s; a2l1fn_l - fl12 + a2(b2 - llfIl2), (55)

as desired. This completes ilie proof of Theorem 5.

0

Inspection of the proof shows an alternative optimization

that may provide further simplification in some cases. Instead

of minimizing the sum of squares Il afn-l + ag - fl12 at each

9 G iteration, one may instead choose E ·to maximize the inner

product (f - fn-l , g). (In this case, one can derive the bound

IIf- fn
finding

ll -:;:
the

p(a2rba)m2/ente.r)sFaonr

sigmoids, the
and bn such

search task reduces to that the inner product

of ¢(a · x + b) and f - fn-l is maximized. The function fn an Cn depends linearly on the other parameters and in (41),

so they may be determined by ordinary least squares.

IX. PROPERTIES AND EXAMPLES OF fuNCTIONS IN r

In this section, several properties and examples of functions

f(x) are presented for which the Fourier integral Cj = J IwIF(dw) is evaluated or bounded, where F(dw) is the

magnitude distribution of the Fourier transform. Note that

Cf = Cj, B in the case that B is the unit ball centered at zero. Examples are also given of classes of functions in r*, that is, functions on R2 that are contained in rB when restricted

to any bounded set B. The simpler facts are stated wiiliout

proof.

1) f(x) Translation: If E re, then f(.'E + b) E re.

2) Scaling: If f(x) E re, then f(ax) E rlale.

3)
4)

Combination: If /;(x) E re" then L ;3;f;

f(x) /2, Gaussian: If

= e- lxl2 then Of :s;

jew) = (27r)-d/2e-lwI2/2 and OJ =

E r d1/2 f Iw

L Ipi lei · Indeed, lj(w) dw

which is bounded by (J Iwl2jew) dw)I/2 = d1/2.

5) Positive Definite Functions: Of -:;: (-f(0)\72f(0» 1/2.

f A positive definite function ( x) is one such iliat Li, j

X;Xjf(Xi - Xi) is nonnegative for all Xl, X2, ' " , Xk

in Rd. Positive definite functions arise as covariance

functions for random fields and as characteristic functions for probability distributions on Rd. The

essential property (due to Bochner) is that contin

uous positive definite functions are characterized as

f(x) functions that have a Fourier representation

=

F(dw) f e;w,,,,

in terms a positive real-valued measure

F. f If is a twice continuously differentiable pos

itive definite function, then by the Cauchy-Schwarz
inequality J Iw\F(dw) :s; (J F(dw) f Iwl2F(dw» 1/2 = (-f(0)\72f(0» 1/2, where \72f(x) =L=l 82f(;r;)/8x;. 0, (Positive definite functions have a maximum at X = so \72f(O) :s; 0.) What is noteworthy about iliis class
of functions for our approximation purposes is that the

Cj is bounded in terms of behavior at a single point
X = O. Moreover, since \72f(O) is a sum of d terms, it
is plausible to model a moderate behavior of the constant
d1/2, OJ, such as order for positive definite functions of
many variables.

6) Integral Representations: Suppose f(·7;) = f K(a(x + b) G(da, db) for some location and scale mixture of a a 0 b function K( x) in r, for > and E Rd. (For instance, K(x) may be a Gaussian density or other positive
. definite kernel on Rd.) Then OJ :s; OK f laI IGI(da, db).
In the same way, if the function has a representative
f(x) = J K(a . 'E + b)G(da, db), for a E Rd and b K (z) E Rd, for some on Rl and some signed measure

G(da, db), then OJ :s; CK J laIIGI(da, db). 7) Ridge Functions: If f(x) = g(a . x) for some direction
a lal E Rd with = 1 and some univariate function g(z) g with integrable Fourier transform on R\ then

f has a Fourier representation in terms of a singular distribution F(dw) concentrated on the set of w in Rd in the direction a, iliat is, f (x) = f eita.xg(t) dt. In this case, Of = 09 = fR' Itl lg(t)1 dt. If f(x) = g(a . x) a 1aI for some E Rd with = 1 and the derivative of

9 is a continuous positive definite function on Rl, then

940

IEEE TRANSACfIONS ON INPORMATION THEORY, VOL. 39, NO. 3, MAY 1993

Cf = Cg = gl (O). Note that Ct is independent of the

dimension d. 8) Sigmoidal Functions on Rd: These are ridge functions of
the form I(x) = </JCa ' x + b) for some a and b in Rd, for some sigmoidal function ¢(z) on Rl . Generally, such

sigmoidal functions do not have an integrable Fourier

transform. Nevertheless, typical choices of smooth sig

moidal functions </J(x) have a derivative </JI (Z) with an integrable transform /(t). In this case, I is in r with Ct = l al J 1¢/Ct) 1 dt. Using the closure properties for

linear combinations and compositions, it is seen that

certain multiple-layer sigmoidal networks are also in r.

9) Radial Functions: Suppose I(x) = g(lxl) is a function

that depends on x only through the magnitude Ixl (i.e.,

the angular components of I(x) are constant), and that I has a Fourier representation I(x) = J eiw.x l(w) dw.

Then
g(lwl

)

lfoCwr )soims ealfsuoncatiorand9ialonfuRncl t.ioInnt;egthraattinigs,

I

lCw) wI IF(

=
w)

using polar coordinates yields Cf = 8d Jo= ,,.d l9(r) I dr,

where sphere

iSn d Rids .

the The

d-1-dimensional volume of the factor rd in the integrand suggests

unit that

CJ is typically exponentially large in d; for an exception,

twhiethGaCuJssiandf1u/n2c. tion in example (4) is a radial function

1 0)

SigmoidalApproximation with an AugmentedInput Vec
tor: For a d-dimensional input x, let .7:1 in R2d consist

of the coordinates of x and the squares of these

coordinates. Then with the unit step function for </J,

the terms in the approximation In (xl ) = L Ck</J(ak .

Xl + bk) with ak>
ellipsoidal regions.

bk E R2d Functions

consist of indicators of
I(x) on Rd can have a

significantly smaller value for Cf when represented as a function of Xl on R2d. In particular, consider the functions of the form I(x) = geL aixT) on Rd

with L aT = 1. These functions include the radial

functions and may be interpreted as a ridge function in

the squared
transform 9

coonmRplo,ntehnetns.

In this there is

case, if 9 has a Fourier a representation of the

function I as a function on R2d, with CJ given by the

one-dimensional integral JIl' Itl lg(t) 1 dt. This potential

improvement in the constant in the approximation

bound helps justify the common practice of including

the squares of the inputs in the sigmoidal network.

For the following examples, let rca, c) C rc be
the class of functions 1(:1:) on Rd for which there is

a Fourier representation I(x) = J eiw·x F(dw) with

magnitude distribution F(dw) satisfying J F(dw)  a

and J IwlF(dw)  c.

1 1 ) Products of Functions in r: If It E rca] , cd and

h Er function

(a2 in

' r((2a)

,
l

then a2 , al

the product
c2 + CL2cd .

11 (x)h (x) is a This follows by

applying Young's convolution inequality to the Fourier

representations of the product I(x )g(x) and its gradient

f(x)Vg(x) + g(x)V/(x). Together with property (3),

this shows that the class of functions in r for which

both J F(dw) and J Iw IF (dw) are finite forms an

algebra of functions closed under linear combinations

and products.

12) Composition with Polynomials: If 9 E rca, c),
then (g(x» k is in rcak , kak-1c). It follows that if I(z) is a polynomial function of one variable and

9 E rca, c), then the composition f(g(x» is in

r(fabs(a), cfbs (a» ), where labs is the polynomial

obtained from I by replacing each coefficient with its

absolute value. A similar statement can be made for

the composition of a polynomial function of severable

variables with functions in rca, c).

13) Composition with Analytic Functions: If 9 E rca, c)

and 1(:1;) is an analytic function represented by a power
series J(z) = L;;=' o akzk with radius of absolute

convergence
in r(fabs (a)

,

r > a, then the
c/bs(a) where

composition I
labs (z) = Lk

(lga(kxl z) k

.

is

The next examples concern functions in r.-that is,

functions which can be modified outside of bounded

sets B to produce functions in r. For I in r. , let

Cthja,t

B = infg Cg,
agree with I

B, where the on the set B

infimum
, eg, B =

is
J

over 9 in r
IwIBG(dw),

and G is the magnitude distribution in the Fourier
representation of 9 on Rd . For functions in r., the ap

bpyro(x2imCajt, iBon)2eIrnro, rfoJrBs(ofm(xe

) - 1,,(x) 2t£(dx) is bounded sigmoidal network In of the

form (1).

14) Linear Functions and Other Polynomials: If I(x)

a . x,
every

then J is in r ·. set n contained

Moreover, in the ball

Cj, {x:

B
Ix

l

 

l

air, r},

for for

every radius r > O. This is shown first in the case

that d = 1 and f(x) = x on [-r, r], by considering

certain extrapolations hex) . In particular, let h(x) =

hb(:.r:) have derivative hl(x) that is equal to 1 for

Ixl  r, equal to 0 for Ixl ::: r + b, and interpolates

linearly between 1 and 0 for r  Ixl  r + b, for

some b > O. Then hl(x) has a Fourier transform that can be calculated to be 2 sin (wb12) sin (w (r + bI2» /(w27rb). By a change of variables (t = wbl2)

and an application of the dominated convergence the

orem, it is seen that as b tends to infinity, Chb =

J Iw l lhb (w) dw
converges to J

= J (sin (

Isin
t» 2

Ct) sin /(t27r)

(t( dt

l +
=

21-jb)) II
1. (This

(t27r) dt
matches

the intuition that as b tends to infinity, hb(x) approaches

I (x) which has a constant derivative equal to one, so

"thdeelFtaoufurinecrtitorann"sfaonrdmthoefinhte(gxr)alooufghIht"t1oshapopurldoxbiemcaltoesae to oney Consequently, Cj, [-r, r]  limboo rChb = r. For I(x) = a.x on Rd, let gb (x) = lalhb (a·x) where a = al lal. Then for sets B in the ball Br of radius r,

Cj, B  lalinfb Chb , [-r, r]  lair. Other extrapolations of I(:!:) = x on [-r, r] can be constructed for which

g(w), as well as wg(w), are integrable. Together with

property (12), this implies that polynomial functions

(in one or several variables) are contained in ro.

Manageable bounds on the constraints Cj, B can be

obtained in the case of sparse polynomials and in the

1 For an alternative treatment in which
alized to allow for functions with a l inear

the Fourier component

representation is gener

Rd, on

sec the remarks

in the Appendix.

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSITIONS OF A SIGMOIDAL FUNCtION

941

case of multiple-layer polynomials networks, which are

rare for this periodic extension to be continuous or to

polynomials defined in terms of a restricted number of

possess a continuous derivative on the boundary points

elementary compositions (sums and products).

of [0, l]d . (It is for this reason that in this paper the

15) Functions with Derivatives of Sufficiently High Order:

f(x) d If the partial derivatives of

of order s = l /2J + 2

f are continuous on Rd, then is in r . . Consider first

the case that the partial derivatives of order less than

or equal to s are square-integrable on Rd. In this case,

f is
aIw(w12)k

)in=1/ 2r,(.1wI+nhdereIweedIk2,k=w) -r1ist/e2-

li(w)llwl and b(w)
1. By the

= a(w)b(w) with = li(w)llwl(1+
Cauchy-Schwarz

a(w)b(w) dw inequality, Gf = I

is bounded by the

product of (J the integral I

a2(w) a2(w)

dw) dw

1=/21a(n1d+(JI wb2I 2(kw) )-d1 wdw) 1/i2s.

Now finite

k d for 2 > and, by Parseval's identity, the integral

I b2(w) dw = I li(wW(lwI2 + IwI2S) dw is finite when

the partial derivatives of order s and of order 1 are

square-integrable on Rd. This demonstrates that f is in
r. Now suppose that the partial derivatives of order s

are continuous, but not necessarily square-integrable on

p(x) Rd. Given r > 0, let

be an s-times continuously

differentiable function that is equal to 1 on Br =

{x: Ixl Ixl ::: r} and equal to ° for

 r' for some

p(x) P1 ( lxl) r' > r. (In particular, we can take

=

p1 (Z) where

equals 1 for z. ::: r, ° for z  r',

and interpolates by a (piecewise polynomial) spline

of order s for r ::: z ::: r'.) Consider the function

fr (x) = I(x)p(x), which agrees with f(x) on Br. It

has continuous partial derivatives for order s which are

Fourier distribution has not been restricted exclusively

w to such a lattice; allowing the coordinates of to take

arbitrary values relaxes the boundary constraints.) In

some cases, it is possible to take a function defined

on a subset of [0, l] d and extend it in a continuously

differentiable way to a function that is zero and has

zero gradient on the boundary of [0, l]d, so that the

requirement of a continuously differentiable periodic

extension is satisfied. Examples of this are similar to

those given in 14) and 15).

17) Functions ofIntegers: Functions defined on Zd are in

rB for any finite subset B. of Zd. Indeed, given such a

iB (W) set w for

B, set in [-7r,

7r]

d

to equal (1/27r)d L:xEB e-iw,xf(x) and to equal ° otherwise.2 Then the

Fourier representation f(x) = I[-1C, 7r]d eiw,xiB (W) dw

iB (W) holds for x in B. Now

is a continuous func

liB (W) 1 tion on the set [-7r, 7r]d, which implies that is bounded and Gf, B = k", 7rjd IwIB liB (W) l dw is

B. B t finite. Consequently, .f is in Moreover, Gf, may

be bounded in terms of the L1 norm of the Fourier

B liB (W)l dw, transform: that is Gf, ::: 7rSd _7r, 1CJd

maxxEB Ixloo. d where s =

As a practical matter, if

is large, then additional structure needs to be assumed

of the function to have a moderate value of Gf, B.

18) Boolean Functions: Here, we consider functions

defined on B

{O, l }d and note that, for

Ixl equal to zero for > r', and hence are integrable on
Rd. Consequently, for each r > 0, the function fr (x) is in r. It follows that f is in r·. A disadvantage of

characterizing approximation capabilities in terms of

high-order differentiability properties is that the bound

on the constant Gf can be quite large. Indeed, the

li(w)i2lwI2s dw, integral I

as characterized by Parse

val's identity, involves a sum of exponentially many

terms--one for each partial derivative of order s.
The last three examples involve functions in r which
have a discrete domain for either of the variables x or
w.

16) Absolutely Convergent Fourier Series: Suppose f is a

e-i21rk.x continuous function on [0, l]d; let A = (27r)-d 1[0, 1jd I(x) d x b e the Fourier series coefficients o f f ,

k kik asenqduseunpcpesosfeorthatEAZadn, dwhereaZrcd

absolutely is the set

summable of vectors

f with integer coordinates. Then f(x) ik the Fourier series representation

is

a

function = L:k

eini27rrkw.xith

k Ikllik l · and Gf = 27r L:

Thus, f has a Fourier dis

w tribution F restricted to the lattice of points of the

({27rk ik form 27rk, with F

}) = k In this case, for

ki and k to be absolutely summable, it is necessary that

Boolean functions, rc, B is related to a class of functions recently examined by Siu and Bruck [5] and

Kushilevitz and Mansour [27t This leads to a number

of additional interesting examples of functions with

B . not excessively large values of Gf, For functions

(x) {O, I} f on

d, the Fourier distribution may be

w k restricted to {O, k I}d (for

the set of of the form then the functions ei7rk.x ,

7r k

with E {O,

E
I} d

are orthogonal functions that span the 2dcdimensional

linear space of real-valued functions on {O, 1 }d).

f(x) ei1rk.x onsequently,

=

fk e x). B = 2-d "L.,.xE{O, l}d

L: -,,,

kk,Ex{fO(,

l

}d

iko where Here, Gf, =

k Ikl1 1fkl 7r L: E{O, l}d

which is bounded above by

lik l 7rdL(.f), where L(.f) = L:kE{O, l}d is the spectral

norm. Now the class PL of Boolean j;'unctions, for

d which L(f) is bounded by a polynomial function of

de (that is, L(f) ::: for some fixed c  1), is examined

in [5] and [27].3 In particular, Siu and Bruck [5]

show, among other examples, that the Boolean function
{O, on 1 Fd defined by the comparison of two d-bit

f(x) 2If it happens that

is an absolutely summable function n Zd, then the

E transfonn J(w) = (1/21r)d

f(x), x e-iw ' x

w E [-11', 1I'jd, may be used

in place of fB (W),

the function f(x) possess a continuously differentiable

periodic extension to Rd. It is a simple matter to

periodically extend a function defined on [0, l]d such

(x k) I(x) that I + =

for all vectors of integers

k; however, for functions that arise in practice, it is

3 The cited references express the Fourier transform in terms of a polynomial
rr1= 1 (eiUj )kj basis that turns out to be identical to the Fourier basis used here. Indeed, for x
restricted to {a, 1 }d, the Fourier basis functions eik.x =

nd= l ,li , may be expressed in the polynomial fonn

where Xj = 1 - 2xj

l i equals I, - 1 for xj equal to 0, 1, respective y (in greement with the values

assigned by eiXl ).

942

IEEE TRANSACTIONS ON INFORMArION THEORY, VOL. 39, NO. 3, MAY 1993

integers and the functions defined by (each bit ot) the
addition of two such integers are functions in P [, with
L(f) = d+ 1. It follows that C/, B .:; 27rd(d+ 1) for the
comparison and addition functions. On the other hand,
they show that the majority function 1 { L::=1 xj -d/2}
(which has a simple network representation) is not in the class PL. Kushilevitz and Mansour [27] show
that a class of binary decision trees represent Boolean functions satisfying L(f) .:; Tn, where Tn is the nnmber of nodes of the tree. It follows that C/, B .:; nnd for such decision trees. Bellare [28] generalizes the results
of [27] by allowing decision trees with more general PL functions implemented at the nodes of the tree. He gives bounds on L(f) in terms of spectral norms
of the node functions, from which bounds on C/, B follow for the classes of decision trees he considers. The implication of polynomial bounds on C/, B, as a
consequence of the bound 2C/, B /yin from Theorem 1,
is that a polynomial rather than an exponential number
of nodes n is sufficient for accurate approximation by
sigmoidal networks.

X. LoWER BOUNDS FOR ApPROXIMATION
BY LINEAR SUBSPACES

The purpose of this section is to present and derive a lower

bound on the best approximation error for linear combinations

of any fixed basis functions for functions inrc. These results,

taken together with Theorem 1, show that fixed basis function

expansion must have a worst-case performance that is much

worse that that which is proven to be achievable by certain

adaptable basis function methods (such as neural nets).

Let /L cube B

be
=

th[eO,uIn]idf,ormandprolebtabdil(ifty,

distribution on the unit g) = U[o. l]' ( f (.x) 

g (x))2 For a

dx) I/2 be the distance function f and a set

obfetfwuenecntiofunns ctGio, nlsetindL( f2

(/L, B). , G) =

infYEG d(f, g) . For a collection of basis functions h1 , h2, " ' , hn

(56)

denotes the error in the approximation of a function f by

the best linear where H" =

combination span (hI, h2

,

of ' .

"thhe nf)u. nTcthioensbehh1a,vhio2r,

" " of

hn , this

approximation error for functions in rc = rc, B may be

characterized (in the worse case) by

(57)

Here, a lower bound to this approximation error is determined
that holds uniformly over all choices of fixed basis functions. In this formulation, the functions hi are not allowed to depend on f (in contrast, sigmoidal basis functions have nonlinear parameters that are allowed to be adjusted in the fit to f). Let

Wn = infh" . . ,hn SUP/Ere d(f, span (hI , h2 , " ' , hn)) . (58)
This is the Kolmogorov n-width of the class of functionsrc.

Theorem 6: For every choice of fixed basis functions hI , h2 , " ' , hn,
(59)

where r;, is a universal constant not smaller than 1 /(81re7r - 1 ) .

Thus, the Kolmogorov n-width o f the class o f functions rc

satisfies

( ) Wn

>-

C r;, -d

-l
n

l/d

(60)

The proof of Theorem 6 is based on the following Lemma. Lemma 6: No linear subspace of dimension n can have
squared distance less than 1/2 from every basis function in an orthonormal basis of a 2n-dimensional space.

Proof' For the proof of Lemma 6, it is to be shown

,that
{gt

if
..

e1, . .
. , g,, }

.is, ea2nliniesaransuobrstphaocneoormf dailmbeanssisionannd,

Gn = span then there is

an Cj such that the squared distance d2 (ej , Gn)  1/2. Indeed,

Ie! P denote projection onto Gn . Then, d2 (ej , Gn) = Ilej 

Pej 1 1 2 = Il ej 1 1 2 - I IPej Il2 = 1 - 1 1 Pej 112 . Thus it is equivalent

to show that there is an ej such that the norm squared of the

projection satisfies II Pej l1 2 .:; 1/2. Without loss of generality,
take g1 , . . . , gn to be an orthonormal basis of Gn . Then the

projection Pej takes the form L:=1 (ej , gi)gi . So the norm

squared of the projection satisfies IIPej l 1 2 = L:=I (ej , gi )2 .

Taking the average for .i = 1 " " , 2n, exchanging the order of

summation, and using Ilgi ll = 1, yields

-
2n 2

(61)

Since the average value of the norm squared of the projection

IIPej l1 2 is equal to 1/2, there must exist a choice of the basis
function ej for some 1 .:; .i .:; 2n for which II Pej 1 1 2  1/2.

This completes the proof of Lemma 6.

D

. . Proof of Theorem 6: Let ht, h , . . . be the functions
cos (w . x) for w = 27rk for k E {a, 1, . }d ordered in terms
of increasing h norm Ik l 1 = L:=l lh l · Let H2n denote the span of h! , . . . , h;n ' We proceed as follows. First reduce the

supremum overrc by restricting to functions in H2n , then

lower bound further by replacing the arbitrary basis functions h i , . . . , hn with their projections onto H2n, which we denote by .111 , . . . , gn ' Then gl , . . . , gn span an n-dimensional linear subspace of H2n and a lower bound is obtained by taking
the infimum over all n-dimensional linear subspaces Gn . The supremum is then restricted to multiples of the orthogonal
functions hi that belong torc, which permits application of

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSITIONS OF A SIGMOIDAL FUNCTION

943

the lemma. Thus, putting it all together,

· Wn = h" i.n··f,h" JsEurpe d(f, span (hI , h2, · · , hn))



h , i, .n· ·f, h,,

sup JEH;n n rc

d(f,

span

(h1

,

h2 ,

·

·

:

,

hn ) )

 h , ,i.n··f,h,, JEHs;u" pn rc d(f, span (gl, g2 , ' " , gn))



inf sup Gn JEHin n rc

d(f,

Gn)

() 

inf Gn

JE{

(C/IWj

l)

su(Wpj "x), j=l, .
cos

· , 2n }

dU,

Gn )

-> j=m1,.i·n·,2n 27rlkj l

. ( . ) infGn sUPJE{COS (Wj .x), j=1,. .,2n} d(f, Gn)
()  >- j=m1,.i.n,2n 27rlkj l 2

>-  47rm '

(62)

for m satisfying (m;jd)  2n (such that the number of

multiindices with norm Ikl ::: m Stirling's formula yields (m;jd)

is at least 2n). A  (m/Td)d for

bound from a universal

constant T :::> e,,-l. Setting m = ITdnl/dl and adjusting the

constant to account for the rounding of m to an integer, the

desired bound is obtained, namely,

-- (-) Wn

>-

C 87rTd

l n

l/d

(63)

This completes the proof of Theorem 6.

o

XI. CONCLUSION
The error in the approximation of functions by artificial neural networks is bounded. For an artificial neural network with one layer of n sigmoidal nodes, the integrated squared error of approximation, integrating on a bounded subset of d variables, is bounded by cf/n, where cf depends on a norm of the Fourier transform of the function being ap proximated. This rate of approximation is achieved under growth constraints on the magnitudes of the parameters of the network. The optimization of a network to achieve these bounds may proceed one node at a time. Because of the economy of number of parameters, order nd instead of nd, these approximation rates permit satisfactory estimators of functions using artificial neural networks even in moderately high-dimensional problems.

APPENDIX
In this appendix, equivalent characterizations of the class of functions r are given in the context of general Fourier distributions on Rd. This appendix is not needed for t!i.e proofs of the theorems in the paper. It is intended to supplement the understanding of the class of functions for which the approximation bounds are obtained.
Recall that r is defined (in Section III) as the class of functions I on Rd such that I(x) = 1(0) + lRd (eiW X -

I)F(dw) for some complex-valued measure F(dw) for which lRd IwIIF(dw)l· Complex-valued measures take the form
ei8(w)F(dw), for some real-valued measure F(dw) = IF(dw)1
called the magnitude distribution and some function (J(w)

called the phase (see, for instance, Rudin [29, theorem 6.12]). A complex vector-valued measure G(dw) on Rd is a vector of complex-valued measures (G1 (dw), " ' , Gd(dw)). Let IG(dw)h = L:=l IGk(dw)1 denote the sum of the
magnitude distributions of the coordinate measures.

Proposition: The following are equivalent for a function I on Rd.

a) The gradient of I has the Fourier representation

\lI(x) = 1 eiw xG(dw) for some complex vector-valued . measure G with I IG(dw)1 < 00 and G({O}) = 0 (in

which case it follows that G(dw) = iwF(dw) for some

complex scalar-valued measure F).

b) The function I has the representation I(x) = 1(0) + 1(eiw x - 1)F(dw) for x E Rd, for some complex-valued

measure F with l lw I IF(dw ) 1 < DC.

c) The increments of the function I of the form !h(:r;) =

I(x + h) 1 eiw'X (eiw.

hI(-x)l

have a Fourier representation Ih (X) = )F(dw), x E , for each h E Rd, for

some complex-valued measure F with l lwIF(dw)1 <

DC.
If any one of a), b), or c) is satisfied for some F, then the other two representations hold with the same F.

Proof'
recall that l

eiTwh.he

proof - 1 1 is

of this proposition bounded by 2hlwl,

is as so 1

follows. First, IwI IF(dw) 1 <

00 implies the absolute integrability of the representations in

b) and c). Now, b) implies c) since the difference of the integrands at x and x + h is integrable, and c) implies b) by taking a specific choice of x and h; consequently, b) and

c) are equivalent. Next, a) follows from c) by the dominated

convergence theorem; c) follows from a) by plugging the

1F0o1uhrie. rVreIp(xres+entthat)iodnt

of the gradient into I(x + h) and applying Fubini's theorem.

I(x)

=

It remains to show that in a), if the gradient of I has

an absolutely integrable Fourier representation \lI (x) = 1 eiw.xG(dw), and if G assigns no mass to the point w = 0, then G(dw) is proportional to w (that is, the measures (l/wk)Gk (dw) are the same for k = 1 , 2, . . . , d). Now,

if the gradient of I has an absolutely integrable Fourier

representation, 101 h . \lI(:r;

then so do the + th) dt =

increments
J h . lRd

fehiw. .I(nxd+etehd) G, !(hd(wx))

= dt,

and Jd

eiinWteXg«raetiiWngh

first with respect to t _ l)/iw . h)h · G(dw) (the

yields Ih(x) = exchange in order

of of

ienitte:.;g· hraitsio(neiiws. hva-lidl )b/ iywF.uhb,inwi 'hsicthhehoaresmmasginnciteudtheebionutengdreadl

by 2). Thus, II. has a Fourier distribution

It is argued that the factor h . G(dw)/h . w .determines a
measure that does not depend on h (from which it follows that G(dw) is proportional to w). Now, the increments of I satisfy !h (X + Y) = ly+h (X) - Iy (x), so it follows that their

944

IEEE TRANSACTIO:-lS ON INFORMAI10N THEORY, VOL. 39, NO. 3, MAY 1993

Fourier distributions satisfy

eiw,yA (dw) = Fy+h (dw) + Fy (dw)

(65)

for all y, h E Rd, Examination of this identity suggests that A (dw) must be of the form (eiw'h - l )F(dw) for some measure l' which does not depend on h. Indeed, by (64), the measures Fh are dominated by IGII for all h, so (64) and (65) may be reexpressed as identities involving the densities
of these measures with respect to IGl l ' Consequently,

eiw

Y ( eiw ' h

_

1)

h

· g(w) h·w

=

( e iw , (y - h )

_

1

)

(y + h) · (y + h)

g(w) ·w

+

(eiw,y

-

l)

y

' g(w) y·w

,

(66)

where g(w) is a complex vector-valued function such that

G(dw) = g(w)IG(dw) l l ' (For each y and h in Rd, (66)

holds-except possibly for a set of w of measure zero with

respect to IGl l-so if y and h are restricted to a countable dense set, then there is one I G1 1 -null set outside of which

(66) holds for all such y and h.) Now take a derivative in

(66), replacing h with th, dividing both sides by t, and letting

i -t 0 (along a countable sequence of values with th restricted

to the dense set). The identity that results from this derivative

( ) ( ) calculation, after a rearrangement of the terms, is

W.h

eiW,y w·

y

1

_

ieiW,y

h ' 9(W) _ y , g(w)

h·w

y·w

= 0. (67)

Therefore, h . g(w)/h · w = y . g(w)/y . w, whenever h . w

and y . w are not equal to zero (for y and h in the countable

dense set and for almost every w). Let p(w) = y . g( w)/y . w

denote the common value of this ratio for all such y (for w

outside of the null set). Then, y . (g(w) - wp(w») = 0; so

taking d points y which span Rd, it follows that yew) = wp(w)

for almost every w. Consequently, G(dw) = wp(wllG(dw) l l ,

which may be expressed in the form G(dw) = iwF(dw) for

some complex-valued measure l' on Rd. This completes the

proof of the proposition.

0

The usefulness of the above proposition is that it provides
a Fourier characterization of l' for functions in r in the case that I 11'(dw) I is not necessarily finite. It is the unique
complex-valued measure such that G(dw) = iwF(dw), where G is the Fourier distribution of the gradient of the function.
For several of the examples in Section IX of functions in r,
including sigmoidal functions, the function f does not possess an integrable Fourier representation (in the traditional form
f(x) = I eiw,xF(dw» , but the gradient of f does possess
an integrable Fourier representation, and in this way F is determined for the mQdified Fourier representation f(.T) = f(O) + I(eiw.", - l)F(dw) .
A Remark Concerning Functions with a Linear Component:
If the Fourier distribution G of the gradient of the function f has G( {O}) # 0, that is, if the gradient has a nonzero constant
component, then (strictly speaking) the function f is not inr, Nevertheless, it is possible to treat this more general situation
by using the representation f(x) = f(O) + a · x + I(ei,"'x -

l)F(dw), where a = G({O}), and F(dw) is characterized by G(dw) = iwF(dw) on Rd - {O}. The component a . x is approximated by linear combinations of sigmoidal functions
in the same way as the sinusoidal components as in the proof
of Theorem 1. Now let Cj· B = I IG(dw)IB, where IGIB is the measure that assigns mass IG(to}) I B = lalB at w = 0,
and that equals IG(dw) I B = IwIB I IF(dw)1 when restricted to Rd - {O} (recall that, by definition, lalB = SUPXEB la . xj). It can be shown in this context that there is a linear combination
of n sigmoidal functions fn(x) of the form (1), such that the L2 (p" B) norm of the error f - fn is bounded by 2Cj, B/..;n. The same bound can also be obtained by the extrapolation method in example (14).
Additional Remarks: In the case that the distribution l' has an integrable Fourier density jew), there is a forward transform characterization in terms of Gaussian summability,
that is,

for almost every w (see, for instance, Stein and Weiss [30]). In the same way, iwJcw) is determined as the Gauss-Fourier transform of \lf(x) for functions in r in the case that Fourier distribution of the gradient is absolutely
continuous. If f(x) or \lf(x), respectively, is integrable on Rd, then j(w) is determined by an ordinary forward transform, that is, ](w) = (27f) -d I e-iW'Xf(x) dx or iwJ(w) =
(27r)-d I e-i"" X\lf(x) dx for almost every w.
Note Added in Proof: A factor of two improvement in the
constant in the approximation bound can be obtained.Indeed, if rj;(z) is a given sigmoid with range in [0,1 ], then subtracting
a constant of 1/2 yields a sigmoid with range in [-1/2, 1/2. Allowing for a change in the additive constant Co, the class of
functions represented by superpositions of this new sigmoid
is the same as represented by superpositions of the original
sigmoid. Therefore, an approximation bound using the new
one also is achievable by the original sigmoid. Now the new
sigmoid has norm bounded by 1/2 instead ofl. Applying this fact in the proof of Theorem 1 yields the existence of a network function fn (x) of the form (1) such that

J (f(x) - fn(x) 2 ti(dx)   ch .

(69)

B

Other scales for the magnitude of the sigmoid are also permit ted, including popular choices with for whieh rj;(z) has limits ±1 as z -t ±oo. In that case, the bound (69) holds with the
constraint on the coefficients of fn that :L:=1 I Ck I C,
provided the spectral norm satifies Cj,B  C,

ACKNOWLEDGMENT
The author is grateful for tlte collaboration of L. Jones. As he points out in [4], together we observed that his theorem applies to artificial neural networks, provided it could be shown that the function to be approximated is in the closure of the convex hull of bounded multiples of sigmoidal functions. This motivated the work presented here.

BARRON: UNIVERSAL APPROXIMATION BOUNDS FOR SUPERPOSlTlONS OF A SIGMOIDAL FUNCTION

945

R. Dudley pointed out [25] where the result of Lemma 1 is attributed to Maurey. E. Sontag noted the necessity of the restriction to continuity points of the distribution for the validity of Lemma 4. B. Hajek suggested the extension to functions on a Hilbert space. H. White encouraged the
development of the lower bounds as in Theorem 6, and D.
Haussler and J. Zinn contributed to the proof of Lemma 6.
REFERENCES
[1) G. Cybenko, "Approximations by superpositions of a sigmoidal func tion," Math Contr. Signals, Syst, vol. 2, pp. 303-3 1 4, 1989.
(2) K Hornik, M. Stinchcombe, and H. White, "Multi-layer feedforward networks are universal approximators," Neural Networks, vol. 2, pp. 359-366, 1989.
(3) A. Pinkus, n·Widths in Approximation Theory. New York: Springer· Verlag, 1985.
[4] L. K. Jones, "A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training," Ann. Stati,1., vol. 20, pp. 608-613, Mar. 1992.
[5] K-Y. Siu and J. Brunk, "On the power threshold circuits with small weights," SIAM J. Discrete Math., 1991.
[6] N. M. Korobov, Nllmber Theoretic Methods of Approximate 1\.nalysis. Moscow: Fiznatgiz, 1963.
[1) G. Wahba, Splinl' Models for Observational Data. Philadelphia, PA: SIAM, 1990.
[8) A. R. Barron, "Statistical properties of artificial neural networks," in Proc. IEEE Int. Cont Decision Contr. , Tampa, FL, Dec. 13-15, 1989, pp. 280-285.
[9] __ , "Complexity regularization with applications to artificial neural
networks," in Nonparametric Functional Estimatiqn, G. Roussas, Ed.
Dordrecht, The Netherlands: K1uwer Academic, 1991, pp. 61-516. [ 10) A. R. Barron and t. M. Cqver, "Minimum complexity density estima
lion," IEEE Trans. Inform. Theory, vol. IT-31, pp. 1034-lQ53, 1991. [11) A. R. Barron, "Approximtion and estimation bounds for artificial neural
networks," in Proc. 4th Annu. Workshop Computation. Learning Theory,
San Mateo: Morgan Kauffman, Aug. 1991, pp. 243-249. (Also to appear
in Mach/ne Learning.)
[12) H. White, "Connectionists nonparametric regression: Multilayer feed·
forward networks can learn arbitrary mappings," Neural NetworkS, voL
3, no. 5, pp. 535-550, 1990. fB) D. Haussler, "Decision theoretic generalizations of the PAC model for
neural net and other learning applications," Computer Res. Lab. Tech. Rep. UCSC-CRL·91-02. Vniv. of California, Santa Cruz, 1991. [(4) A. R. Barron and R. L. Barron, "Statistical lcarning networks: A unifying
view," in Computing Science and Statistics: Proc. 21sl Symp. Interface,
Alexandria: American Statistical Assoc., 1988, pp. 192-203.

[15) L. Breiman, "Hinging hyperplanes for regression, classification, and function approximation," IEEE Trans. Inform. Theory, vol. 39, pp. 999-1013, May 1993.
[16) R. Tibshirani, "Slide functions for projection pursuit regression and neural networks," Dep. Stat. Tech. Rep. 9205, Univ. Toronto, 1992.
[17] Y. Zhao, "On projection pursuit learning," Ph.D. dissertation, Dept. Math. Art. Intell. Lab., M.LT., 1992.
[18] F. Girosi and G. Anzcllotti, "Convergence rates of approximation by !ranslates," Art. Intell. Lab. Tech. Rep. 1288, Mass. Inst. Techno!., 1992.
[19] A. Farago and G. Lugosi, "Strong universal consistency of neural
network classifiers," Dep. Elect. Eng. Tech. Rep., Tech. Vniv. Budapest,
1992. (To appear in the IEEE Trans. Inform. Theory.)
N [20] A R. Barron, "Neural net apprOximation," in Proc. Yale Workshop Adaptive Learning Syst., K arendra, Ed., Yale Vniv., May 1992. [21] K. Hornik, M. Stinchcombe, H. White, and P. Auer, "Degree of
approximation results for feedforward networks approximating unknown
mappings and their derivatives," preprint, 1992. [22] D. F. McCaffrey and A R. Gallant, "Convergence rates for sil)gle hidden
layer feedforward networks," Tech. Rep. RAND Corp., Santa Monica, CA, and Dept. Statist., North Carolina State Vniv., 1 992. [23] H. N. Mhaskar and C. A. Micchelli, "Approximation by superposition of
a sigmoidal function," Advances in Appl. Math., voL 13, pp. 350-373, 1992. [24) L. K. Jones, "Good weights and hyperbolic kernels for neural networks,
projection pursuit, and pattern classification: Fourier strategies for
extracting information from high·dimensional data," Thch. Rep., Dept Math. Sci., Univ. of Massachusetts, Lowell, 1991. (To apPear in IEEE
Trans. Inform. Theory.)
[25] G. Pisier, "Remarqucs sur un resultat non publie de B. Maurey,"
presented at the Seminaire d' analyse fonctionelle 1980-1981, Ecole
Polytechnique, Centre de Mathematiques, Palaiseau. [26] L. K. Jones, "Constructive approximations for neural networks by sill
moidal functions," Proc. IEEE: Special Issue on Neural Networks, vol. 78, pp. 1586-1589, 1991. [27] E. Kushilevitz and Y. Mansour, "Learning decisiol) trees using the Fourier spectrum," in Proc. 23rd ACM Symp Theory Comput., 1991, pp. 455-464. [28] M. Bellare, "The spectral norm of finite functions," MIT'LCS Tach. Rep. TR-465, 1991. [29] W. Rudin,Realand ComplexAnalysis. New York: McGraw-Hilt, 1984. [30) E. M. Stein and G. Weiss, Introduction to Fourier Analysis on Euclidean
Spaces. Princeton, NJ: Princeton Vniv. Press, 1911. [311 C. Darken, M. Donahue, L. Gurvits, and E. Sontag, "Rate of approxi
mation results motivated b robust neu\1ll network learning," to appear
in Proc. Sixth ACM Wrokshop on Computat. Learning Theory. [32) J. Yukich, "Sup norm approximation bounds for networks via Yap·
nik-Chervoncnkis classes," Notes, Dept. of Math., Lehigh Vniv., Betil
lehem, PA, Jan. 1993. [33) V. Kdrkova, "Kolmogorov's theorem and multilayer neural networJ<:s,"
Neural Net. , vol. 5, pp. 501-506, 1992.


A VIEW ON DEEP REINFORCEMENT LEARNING IN SYSTEM OPTIMIZATION

arXiv:1908.01275v3 [cs.LG] 4 Sep 2019

Ameer Haj-Ali 1 2 Nesreen K. Ahmed 1 Ted Willke 1 Joseph E. Gonzalez 2 Krste Asanovic 2 Ion Stoica 2

ABSTRACT
Many real-world systems problems require reasoning about the long term consequences of actions taken to configure and manage the system. These problems with delayed and often sequentially aggregated reward, are often inherently reinforcement learning problems and present the opportunity to leverage the recent substantial advances in deep reinforcement learning. However, in some cases, it is not clear why deep reinforcement learning is a good fit for the problem. Sometimes, it does not perform better than the state-of-the-art solutions. And in other cases, random search or greedy algorithms could outperform deep reinforcement learning. In this paper, we review, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. We propose a set of essential metrics to guide future works in evaluating the efficacy of using deep reinforcement learning in system optimization. Our evaluation includes challenges, the types of problems, their formulation in the deep reinforcement learning setting, embedding, the model used, efficiency, and robustness. We conclude with a discussion on open challenges and potential directions for pushing further the integration of reinforcement learning in system optimization.

1 INTRODUCTION
Reinforcement learning (RL) is a class of learning problems framed in the context of planning on a Markov Decision Process (MDP) (Bellman, 1957), when the MDP is not known. In RL, an agent continually interacts with the environment (Kaelbling et al., 1996; Sutton et al., 2018). In particular, the agent observes the state of the environment, and based on this observation takes an action. The goal of the RL agent is then to compute a policy­a mapping between the environment states and actions­that maximizes a long term reward. There are multiple ways to extrapolate the policy. Non-approximation methods usually fail to predict good actions in states that were not visited in the past, and require storing all the action-reward pairs for every visited state, a task that incurs a huge memory overhead and complex computation. Instead, approximation methods have been proposed. Among the most successful ones is using a neural network in conjunction with RL, also known as deep RL. Deep models allow RL algorithms to solve complex problems in an end-to-end fashion, handle unstructured environments, learn complex functions, or predict actions in states that have not been visited in the past. Deep RL is gaining wide interest recently due to its success in robotics, Atari games, and superhuman capabilities (Mnih et al., 2013; Doya, 2000; Kober et al., 2013; Peters et al., 2003). Deep RL was the key technique behind defeating the human European champion in the game of Go, which has long been viewed as the most challenging of classic games for artificial intelligence (Silver et al., 2016).
1Intel Labs 2University of California, Berkeley. Correspondence to: Ameer Haj-Ali <ameerh@berkeley.edu>.

Many system optimization problems have a nature of delayed, sparse, aggregated or sequential rewards, where improving the long term sum of rewards is more important than a single immediate reward. For example, an RL environment can be a computer cluster. The state could be defined as a combination of the current resource utilization, available resources, time of the day, duration of jobs waiting to run, etc. The action could be to determine on which resources to schedule each job. The reward could be the total revenue, jobs served in a time window, wait time, energy efficiency, etc., depending on the objective. In this example, if the objective is to minimize the waiting time of all jobs, then a good solution must interact with the computer cluster and monitor the overall wait time of the jobs to determine good schedules. This behavior is inherent in RL. The RL agent has the advantage of not requiring expert labels or knowledge and instead the ability to learn directly from its own interaction with the world. RL can also learn sophisticated system characteristics that a straightforward solution like first come first served allocation scheme cannot. For instance, it could be better to put earlier long-running arrivals on hold if a shorter job requiring fewer resources is expected shortly.
In this paper, we review different attempts to overcome system optimization challenges with the use of deep RL. Unlike previous reviews (Hameed et al., 2016; Mahdavinejad et al., 2018; Krauter et al., 2002; Wang et al., 2018; Ashouri et al., 2018; Luong et al., 2019) that focus on machine learning methods without discussing deep RL models or applying them beyond a specific system problem, we focus on deep RL in system optimization in general. From reviewing prior work, it is evident that standardized metrics for assessing

A View on Deep Reinforcement Learning in System Optimization
posed (Mnih et al., 2016; Ross et al., 2011; Sutton et al., 2000; Schulman et al., 2017; Lillicrap et al., 2015).
Policy Gradient (PG) (Sutton et al., 2000), for example, uses a neural network to represent the policy. This policy is updated directly by differentiating the term in Equation 1 as follows:

Figure 1. RL environment example. By observing the state of the environment (the cluster resources and arriving jobs' demands), the RL agent makes resource allocation actions for which he receives rewards as revenues. The agent's goal is to make allocations that maximize cumulative revenue.

deep RL solutions in system optimization problems are lacking. We thus propose quintessential metrics to guide future work in evaluating the use of deep RL in system optimization. We also discuss and address multiple challenges that faced when integrating deep RL into systems.

2 BACKGROUND

One of the promising machine learning approaches is reinforcement learning (RL), in which an agent learns by continually interacting with an environment (Kaelbling et al., 1996). In RL, the agent observes the state of the environment, and based on this state/observation takes an action as illustrated in figure 1. The ultimate goal is to compute a policy­a mapping between the environment states and actions­that maximizes expected reward. RL can be viewed as a stochastic optimization solution for solving Markov Decision Processes (MDPs) (Bellman, 1957), when the MDP is not known. An MDP is defined by a tuple with four elements: S, A, P (s, a), r(s, a) where S is the set of states of the environment, A describes the set of actions or transitions between states, s P (s, a) describes the probability distribution of next states given the current state and action and r(s, a) : S × A  R is the reward of taking action a in state s. Given an MDP, the goal of the agent is to gain the largest possible cumulative reward. The objective of an RL algorithm associated with an MDP is to find a decision policy (a|s) : s  A that achieves this goal for that MDP:

 = arg max E() [ ] =


(1)

arg max E()

r(st, at) ,



t

where  is a sequence of states and actions that define a single episode, and T is the length of that episode. Deep RL leverages a neural network to learn the policy (and sometimes the reward function). Over the past couple of years, a plethora of new deep RL techniques have been pro-

J = E()

r(st, at)

t

= E ( ) 1N
 N
i=1

 log (at |st )
t
 log  (ai,t |si,t )
t

r(st, at)
t
r(si,t, ai,t)
t
(2)

and updating the network parameters (weights) in the direction of the gradient:

   + J,

(3)

Proximal Policy Optimization (PPO) (Schulman et al., 2017) improves on top of PG for more deterministic, stable, and robust behavior by limiting the updates and ensuring the deviation from the previous policy is not large.

In contrast, Q-Learning (Watkins et al., 1992), state-actionreward-state-action (SARSA) (Rummery et al., 1994) and deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) are temporal difference methods, i.e., they update the policy on every timestep (action) rather than on every episode. Furthermore, these algorithms bootstrap and, instead of using a neural network for the policy itself, they learn a Q-function, which estimates the long term reward from taking an action. The policy is then defined using this Q-function. In Q-Learning the Q-function is updated as follows:

Q(st, at)  Q(st, at)+r(st, at)+maxat [Q(st, at)]. (4)
In other words, the Q-function updates are performed based on the action that maximizes the value of that Q-function. On the other hand, in SARSA, the Q-function is updated as follows:

Q(st, at)  Q(st, at) + r(st, at) + Q(st+1, at+1). (5)
In this case, the Q-function updates are performed based on the action that the policy would select given state st. DDPG fits multiple neural networks to the policy, including the Q-function and target time-delayed copies that slowly track the learned networks and greatly improve stability in learning.

A View on Deep Reinforcement Learning in System Optimization

Algorithms such as upper-confidence-bound and greedy can then be used to determine the policy based on the Qfunction (Auer, 2002; Sutton et al., 2018). The reviewed works in this paper focus on the epsilon greedy method where the policy is defined as follows:

(at|st) =

arg maxat Q(st, at), random action,

w.p. 1 - w.p.

(6)

A method is considered to be on-policy if the new policy is computed directly from the decisions made by the current policy. PG, PPO, and SARSA are thus on-policy while DDPG and Q-Learning are off-policy. All the mentioned methods are model-free: they do not require a model of the environment to learn, but instead learn directly from the environment by trial and error. In some cases, a model of the environment could be available. It may also be possible to learn a model of the environment. This model could be used for planning and enable more robust training as less interaction with the environment may be required.
Most RL methods considered in this review are structured around value function estimation (e.g., Q-values) and using gradients to update the policy. However, this is not always the case. For example, genetic algorithms, simulated annealing, genetic programming, and other gradient-free optimization methods - often called evolutionary methods (Sutton et al., 2018) - can also solve RL problems in a manner analogous to the way biological evolution produces organisms with skilled behavior. Evolutionary methods can be effective if the space of policies is sufficiently small, the policies are common and easy to find, and the state of the environment is not fully observable. This review considers only the deep versions of these methods, i.e., using a neural network in conjunction with evolutionary methods typically used to evolve and update the neural network parameters or vice versa.
Multi-armed bandits (Berry et al., 1985; Auer et al., 2002) simplify RL by removing the learning dependency on state and thus providing evaluative feedback that depends entirely on the action taken (1-step RL problems). The actions usually are decided upon in a greedy manner by updating the benefit estimates of performing each action independently from other actions. To consider the state in a bandit solution, contextual bandits may be used (Chu et al., 2011). In many cases, a bandit solution may perform as well as a more complicated RL solution or even better. Many Bandit algorithms enjoy stronger theoretical guarantees on their performance even under adversarial settings. These bounds would likely be of great value to the systems world as they suggest in the limit that the proposed algorithm would be no worse than using the best fixed system configuration in hindsight.

2.1 Prior RL Works With Alternative Approximation Methods
Multiple prior works have proposed to use non-deep neural network approximation methods for RL in system optimization. These works include reliability and monitoring (Das et al., 2014; Zhu et al., 2007; Zeppenfeld et al., 2008), memory management (Ipek et al., 2008; Andreasson et al., 2002; Peled et al., 2015; Diegues et al., 2014) in multicore systems, congestion control (Li et al., 2016; Silva et al., 2016), packet routing (Choi et al., 1996; Littman et al., 2013; Boyan et al., 1994), algorithm selection (Lagoudakis et al., 2000), cloud caching (Sadeghi et al., 2017), energy efficiency (Farahnakian et al., 2014) and performance (Peng et al., 2015; Jamshidi et al., 2015; Barrett et al., 2013; Arabnejad et al., 2017; Mostafavi et al., 2018). Instead of using a neural network to approximate the policy, these works used tables, linear approximations, and other approximation methods to train and represent the policy. Tables were generally used to store the Q-values, i.e., one value for each action, state pair, which are used in training, and this table becomes the ultimate policy. In general, deep neural networks allowed for more complex forms of policies and Q functions (Lin, 1993), and can better approximate good actions in new states.
3 RL IN SYSTEM OPTIMIZATION
In this section, we discuss the different system challenges tackled using RL and divide them into two categories: Episodic Tasks, in which the agent-environment interaction naturally breaks down into a sequence of separate terminating episodes, and Continuing Tasks, in which it does not. For example, when optimizing resources in the cloud, the jobs arrive continuously and there is not a clear termination state. But when optimizing the order of SQL joins, the query has a finite number of joins, and thus after enough steps the agent arrives at a terminating state.
3.1 Continuing Tasks An important feature of RL is that it can learn from sparse reward signals, does not need expert labels, and the ability to learn direction from its own interaction with the world. Jobs in the cloud arrive in an unpredictable and continuous manner. This might explain why many system optimization challenges tackled with RL are in the cloud (Mao et al., 2016; He et al., 2017a;b; Tesauro et al., 2006; Xu et al., 2017; Liu et al., 2017; Xu et al., 2012; Rao et al., 2009). A good job scheduler in the cloud should make decisions that are good in the long term. Such a scheduler should sometimes forgo short term gains in an effort to realise greater long term benefits. For example, it might be better to delay a long running job if a short running job is expected to arrive soon. The scheduler should also adapt to variations in the underlying resource performance and scale in the presence of new or unseen workloads combined with large numbers of resources.

A View on Deep Reinforcement Learning in System Optimization

These schedulers have a variety of objectives, including minimizing average performance of jobs and optimizing the resource allocation of virtual machines (Mao et al., 2016; Tesauro et al., 2006; Xu et al., 2012; Rao et al., 2009), optimizing data caching on edge devices and base stations (He et al., 2017a;b), and maximizing energy efficiency (Xu et al., 2017; Liu et al., 2017). The RL algorithms used for addressing each system problem are listed in Table 1 lists the RL algorithms used for addressing each problem.
Interestingly, for cloud challenges most works are driven by Q-learning (or the very similar SARSA). In the absence of a complete environmental model, model-free Q-Learning can be used to generate optimal policies. It is able to make predictions incrementally by bootstrapping the current estimate with previous estimates and provide good sample efficiency (Jin et al., 2018). Q-Learning is also characterized by inherent continuous temporal difference behavior where the policy can be updated immediately after each step (not the end of trajectory); something that might be very useful for online adaptation.
3.2 Episodic Tasks Due to the sequential nature of decision making in RL, the order of the actions taken has a major impact on the rewards the RL agent collects. The agent can thus learn these patterns and select more rewarding actions. Previous works took advantage of this behavior in RL to optimize congestion control (Jay et al., 2019; Ruffy et al., 2018), decision trees for packet classification (Liang et al., 2019), sequence to SQL/program translation (Zhong et al., 2017; Guu et al., 2017; Liang et al., 2016), ordering of SQL joins (Krishnan et al., 2018; Ortiz et al., 2018; Marcus et al., 2018; 2019), compiler phase ordering (Huang et al., 2019; Kulkarni et al., 2012) and device placement (Addanki et al., 2019; Paliwal et al., 2019).
After enough steps in these problems, the agent will always arrive at a clear terminating step. For example, in query join order optimization, the number of joins is finite and known from the query. In congestion control ­ where the routers need to adapt the sending rates to provide high throughput without comprising fairness ­ the updates are performed on a fixed number of senders/receivers known in advance. These updates combined define one episode. This may explain why there is a trend towards using PG methods for these types of problems, as they don't require a continuous temporal difference behavior and can often operate in batches of multiple queries. Nevertheless, in some cases, Q-learning is still used, mainly for sample efficiency as the environment step might take a relatively long time.
To improve the performance of PG methods, it is possible to take advantage of the way the gradient computation is performed. If the environment is not needed to generate the observation, it is possible to save many environment

steps. This is achieved by rolling out the whole episode from interacting only with the policy and performing one environment step at the very end. The sum of rewards will be the same as the reward received from this environment step. For example, in query optimization, since the observations are encoded directly from the actions, and the environment is mainly used to generate the rewards, it will be possible to repeatedly perform an action, form the observation directly from this action, and feed it to the policy network. After the end of the episode, the environment can be triggered to get the final reward, which would be the sum of the intermediate rewards. This can significantly reduce the training time.
3.3 Discussion: Continuous vs. Episodic Continuous policies can handle both continuous and episodic tasks, while episodic policies cannot. So, for example, Q-Learning can handle all the tasks mentioned in this work, while PG based methods cannot directly handle it without modification. For example, in (Mao et al., 2016), the authors limited the the scheduler window of jobs to M , allowing the agent in every time step to schedule up to M jobs out of all arrived jobs. The authors also discussed this issue of "bounded time horizon" and hoped to overcome it by using a value network to replace the timedependent baseline. It is interesting to note that prior work on continuous system optimization tasks using non deep RL approaches (Choi et al., 1996; Littman et al., 2013; Boyan et al., 1994; Peng et al., 2015; Jamshidi et al., 2015; Barrett et al., 2013; Arabnejad et al., 2017; Sadeghi et al., 2017; Farahnakian et al., 2014) used Q-Learning.
One solution for handling continuing problems without episode boundaries with PG based methods is to define performance in terms of the average rate of reward per time step (Sutton et al., 2018) (Chapter 13.6). Such approaches can help better fit the continuous problems to episodic RL algorithms.
4 FORMULATING THE RL ENVIRONMENT
Table 1 lists all the works we reviewed and their problem formulations in the context of RL, i.e., the model, observations, actions and rewards definitions. Among the major challenges when formulating the problem in the RL environment is properly defining the system problem as an RL problem, with all of the required inputs and outputs, i.e., state, action spaces and rewards. The rewards are generally sparse and behave similarly for different actions, making the RL training ineffective due to bad gradients. The states are generally defined using hand engineered features that are believed to encode the state of the system. This results in a large state space with some features that are less helpful than others and rarely captures the actual system state. Using model-based RL can alleviate this bottleneck and provide more sample efficiency. (Liu et al., 2017) used auto-

A View on Deep Reinforcement Learning in System Optimization

Table 1. Problem formulation in the deep RL setting. The model abbreviations are: fully connected neural networks (FCNN), convolutional

neural network (CNN), recurrent neural network (RNN), graph neural network (GNN), gated recurrent unit (GRU), and long short-term

memory (LSTM).

Description

Reference

State/Observation

Action

Reward

Objective

Algorithm

Model

congestion control

(Jay et al., 2019)1 (Ruffy et al., 2018)2

histories of sending rates and resulting changes to sending rate statistics (e.g., loss rate)

throughput and negative of
latency or loss rate

maximize throughput while maintaining PPO1,2/PG2/DDPG2
fairness

FCNN

packet classification (Liang et al., 2019)

(Krishnan et al., 2018)1

SQL join

(Ortiz et al., 2018)2

order optimization (Marcus et al., 2019)3

(Marcus et al., 2018)4

encoding of the tree node, e.g.,
split rules
encoding of current join plan

cutting a classification classification time build optimal decision

tree node or partitioning /memory

tree for packet

PPO

a set of rules

footprint

classification

FCNN

next relation to join

negative cost1-3, minimize execution

1/cost4

time

Q-Learning1-3/PPO4

tree conv.3, FCNN1-4

sequence to SQL

(Zhong et al., 2017)

SQL vocabulary, question, column
names

query corresponding to the token

-2 invalid query, -1 valid but wrong, +1 valid and right

tokens in the WHERE clause

PG

LSTM

language to program translation

(Guu et al., 2017)

natural language utterances

a sequence of program tokens

1 if correct result generate equivalent

0 otherwise

program

PG

LSTM, FCNN

semantic parsing (Liang et al., 2016)

embedding of the words

a sequence of program tokens

positive if correct generate equivalent

0 otherwise

program

PG

current allocation of

i(

-1 Ti

)

for

resource allocation in the cloud

(Mao et al., 2016)

cluster resources & resource profiles of

next job to schedule

all jobs in the minimize average system (Ti is the job slowdown

PG

waiting jobs

duration of job i)

RNN, GRU
FCNN

resource allocation

(He et al., 2017a;b)

status of edge devices, base stations,
content caches

which base station, to offload/cache or not

total revenue

maximize total revenue

Q-Learning

CNN

resource allocation in the cloud

(Tesauro et al., 2006)

current allocation & demand

next resource to allocate

payments

maximize revenue

Q-Learning

FCNN

resource allocation in cloud radio access networks

(Xu et al., 2017)

active remote radio heads & user demands

which remote radio heads to activate

negative power consumption

power efficiency

Q-Learning

FCNN

cloud resource allocation & power management

(Liu et al., 2017)

current allocation & demand

next resource to allocate

linear combination of total power , VM latency, & reliability metrics

power efficiency

Q-Learning

autoencoder, weight sharing
& LSTM

automate virtual machine (VM) configuration process

(Rao et al., 2009) (Xu et al., 2012)

compiler phase (Kulkarni et al., 2012)1

ordering

(Huang et al., 2019)2

current resource allocations
program features

increase/decrease CPU/time/memory
next optimization pass

throughput -response time

maximize performance

Q-Learning

FCNN, model-based

performance improvement

minimize execution Evolutionary Methods1/

time

Q-Learning2/PG2

FCNN

device placement

(Paliwal et al., 2019)1 (Addanki et al., 2019)2

computation graph

placement/schedule of graph node

speedup

maximize performance & minimize peak memory

PG1,2/ Evolutionary Methods1

GNN/FCNN

distributed instruction placement

(Coons et al., 2008)

instruction features

instruction placement location

speedup maximize performance Evolutionary Methods FCNN

encoders to help reduce the state dimensionality. The action space is also large but generally represents actions that are directly related to the objective. Another challenge is the environment step. Some tasks require a long time for the environment to perform one step, significantly slowing the learning process of the RL agent.
Interestingly, most works focus on using simple out-of-thebox FCNNs, while some works that targeted parsing and translation ((Liang et al., 2016; Guu et al., 2017; Zhong et al., 2017)) used RNNs (Graves et al., 2013) due to their ability to parse strings and natural language. While FCNNs are simple and easy to train to learn a linear and non-linear function policy mappings, sometimes having a more complicated network structure suited for the problem could further improve the results.

4.1 Evaluation Results Table 2 lists training, and evaluation results of the reviewed works. We consider the time it takes to perform a step in the environment, the number of steps needed in each iteration of training, number of training iterations, total number of steps needed, and whether the prior work improves the state of the art and compares against random search/bandit solution.
The total number of steps and the the cost of each environment step is important to understand the sample efficiency and practicality of the solution, especially when considering RLs inherent sample inefficiency (Schaal, 1997; Hester et al., 2018). For different workloads, the number of samples needed varies from thousands to millions. The environment step time also varies from milliseconds to minutes. In multiple cases, the interaction with the environment is very slow. Note that in most cases when the environment step time was a few milliseconds, it was because it was a simulated environment, not a real one. We observe that for faster

A View on Deep Reinforcement Learning in System Optimization

Work

Problem

Table 2. Evaluation results.

Environment Step Time

Number of StepsNumber of training

Per Iteration

Iterations

Total Of

Number Steps

ImopfrothveesASrttateCBoamnpdSaietr/aeRsrcaAhngdaoimnst

packet classification

(Liang et al., 2019)

20-600ms

up to 60,000

up to 167

1,002,000

(18%)



congestion control

(Jay et al., 2019)

50-500ms

8192

1200

9,830,400

(similar)

congestion control

(Ruffy et al., 2018)

0.5s

N/A

N/A

50,000-100,000





resource allocation

(Mao et al., 2016)

10-40ms

20,000

1000

20,000,000

(10-63%)

resource (He et al., 2017a) allocation (He et al., 2017b)

N/A

N/A

20,000

N/A no comparison



resource allocation

(Tesauro et al., 2006)

N/A

N/A

10,000-20,000

N/A no comparison

resource allocation

(Xu et al., 2017)

N/A

N/A

N/A

N/A no comparison

resource allocation

(Liu et al., 2017) 1-120 minutes

100,000

20

2,000,000 no comparison



resource (Rao et al., 2009) allocation (Xu et al., 2012)

N/A

N/A

N/A

N/A no comparison



SQL Joins

(Krishnan et al., 2018)

10ms

640

100

64,000

(70%)

SQL joins

(Ortiz et al., 2018)

N/A

N/A

N/A

N/A no comparison



SQL joins

(Marcus et al., 2019) 250ms

100-8,000

100

10,000-80,000 (10-66%)

SQL joins

(Marcus et al., 2018)

1.08s

N/A

N/A

10,000

(20%)

sequence to SQL

(Zhong et al., 2017)

N/A

80,654

300

24,196,200

(similar)



language to program trans.

(Guu et al., 2017)

N/A

N/A

N/A

13,000

(56%)



semantic parsing

(Liang et al., 2016)

N/A

3,098

200

619,600

(3.4%)



phase ordering

(Huang et al., 2019)

1s

N/A

N/A

1,000-10,000 (similar)

phase ordering

(Kulkarni et al., 2012)

13.2 days for all steps

N/A

N/A

N/A





device placement

(Addanki et al., 2019) N/A (seconds)

N/A

N/A

1,600-94,000

(3%)

device placement

(Paliwal et al., 2019) N/A (seconds)

N/A

N/A

400,000

(5%)

instruction placement

(Coons et al., 2008) N/A (minutes)

N/A

200

N/A (days)





environment steps more training samples were gathered to leverage that and further improve the performance. This excludes (Liu et al., 2017) where a cluster was used and thus more samples could be processed in parallel.
As listed in Table 2, many works did not provide sufficient data to reproduce the results. Reproducing the results is necessary to further improve the solution and enable future evaluation and comparison against it.
4.2 Frameworks and Toolkits A few RL benchmark toolkits for developing and comparing reinforcement learning algorithms, and providing a faster simulated system environment, were recently pro-

posed. OpenAI Gym (Brockman et al., 2016) supports an environment for teaching agents everything, from walking to playing games like Pong or Pinball. Iroko (Ruffy et al., 2018) provides a data center emulator to understand the requirements and limitations of applying RL in data center networks. It interfaces with the OpenAI Gym and offers a way to evaluate centralized and decentralized RL algorithms against conventional traffic control solutions.
Park (Mao et al., 2019) proposes an open platform for easier formulation of the RL environment for twelve real world system optimization problems with one common easy to use API. The platform provides a translation layer between

A View on Deep Reinforcement Learning in System Optimization

the system and the RL environment making it easier for RL researchers to work on systems problems. That being said, the framework lacks the ability to change the action, state and reward definitions, making it harder to improve the performance by easily modifying these definitions.
5 CONSIDERATIONS FOR EVALUATING DEEP RL IN SYSTEM OPTIMIZATION
In this section, we propose a set of questions that can help system optimization researchers determine whether deep RL could be an effective tool in solving their systems optimization challenges.
Can the System Optimization Problem Be Modeled by an MDP? The problem of RL is the optimal control of an MDP. MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards but also future states and rewards. This involves delayed rewards and the trade-off between delayed and immediate reward. In MDPs, the new state and new reward are dependent only on the preceding state and action. Given a perfect model of the environment, an MDP can compute the optimal policy.
MDPs are typically a straightforward formulation of the system problem, as an agent learns by continually interacting with the system to achieve a particular goal, and the system responds to these interactions with a new state and reward. The agent's goal is to maximize expected reward over time.
Is It a Reinforcement Learning Problem? What distinguishes RL from other machine learning approaches is the presence of self exploration and exploitation, and the tradeoff between them. For example, RL is different from supervised learning. The latter is learning from a training set with labels provided by an external supervisor that is knowledgeable. For each example the label is the correct action the system should take. The objective of this kind of learning is to act correctly in new situations not present in the training set. However, supervised learning is not suitable for learning from interaction, as it is often impractical to obtain examples representative of all the cases in which the agent has to act.
Are the Rewards Delayed? RL algorithms do not maximize the immediate reward of taking actions but, rather, expected reward over time. For example, an RL agent can choose to take actions that give low immediate rewards but that lead to higher rewards overall, instead of taking greedy actions every step that lead to high immediate rewards but low rewards overall. If the objective is to maximize the immediate reward or the actions are not dependent, then other simpler approaches, such as bandits and greedy algorithms, will perform better than deep RL, as their objective is to maximize the immediate reward.

What is Being Learned? It is important to provide insights on what is being learned by the agent. For example, what actions are taken in which states and why? Can the knowledge learned be applied to new states/tasks? Is there a structure to the problem being learned? If a brute-force solution is possible for simpler tasks, it will also be helpful to know how much better the performance of the RL agent is than the brute force solution. In some cases, not all hand-engineered features are useful. Using all of them can result in high variance and prolonged training. Feature analysis can help overcome this challenge. For example, in (Coons et al., 2008) significant performance gaps were shown for different feature selection.
Does It Outperform Random Search and a Bandit Solution? In some cases, the RL solution is just another form of a improved random search. In some cases, good RL results were achieved merely by chance. For instance, if the features used to represent the state are not good or do not have a pattern that could be learned. In such cases, random search might perform as well as RL, or even better, as it is less complicated. For example, in (Huang et al., 2019), the authors showed 10% improvement over the baseline by using random search. In some cases the actions are independent and a greedy or bandit solution can achieve the optimal or near-optimal solution. Using a bandit method is equivalent using a 1-step RL solution, in which the objective is to maximize the immediate reward. Maximizing the immediate reward could deliver the overall maximum reward and, thus, a comparison against a bandit solution can help reveal this.
Are the Expert Actions Observable? In some cases it might be possible to have access to expert actions, i.e., optimal actions. For example, if a brute force search is plausible and practical then it is possible to outperform deep RL by using it or using imitation learning (Schaal, 1999), which is a supervised learning approach that learns by imitating expert actions.
Is It Possible to Reproduce/Generalize Good Results? The learning process in deep RL is stochastic and thus good results are sometimes achieved due to local maxima, simple tasks, and chance. In (Haarnoja et al., 2018) different results were generated by just changing the random seeds. In many cases, good results cannot be reproduced by retraining, training on new tasks, or generalizing to new tasks.
Does It Outperform the State of the Art? The most important metric in the context of system optimization in general is outperforming the state of the art. Improving the state of the art includes different objectives, such as efficiency, performance, throughput, bandwidth, fault tolerance, security, utilization, reliability, robustness, complexity, and energy. If the proposed approach does not perform better than the state of the art in some metric then

A View on Deep Reinforcement Learning in System Optimization

it is hard to justify using it. Frequently, the state of the art solution is also more stable, practical, and reliable than deep RL. In many prior works listed in Table 2 a comparison against the state of the art is not available or deep RL performs worse. In some cases deep RL can perform as good as the state of the art or slightly worse, but still be a useful solution as it achieves an improvement on other metrics.
6 RL METHODS AND NEURAL NETWORK MODELS
Multiple RL methods and neural network models can be used. RL frameworks like RLlib (Liang et al., 2017), Intel's Coach (Caspi et al., 2017), TensorForce (Kuhnle et al., 2017), Facebook Horizon (Gauci et al., 2018), and Google's Dopamine (Castro et al., 2018) can help the users pick the right RL model, as they provide implementations of many policies and models for which a convenient interface is available.
As a rule of thumb, we rank RL algorithms based on sample efficiency as follows: model-based approaches (most efficient), temporal difference methods, PG methods, and evolutionary algorithms (least efficient). In general, many RL environments run in a simulator. For example (Paliwal et al., 2019; Mao et al., 2019; 2016), run in a simulator as the real environment's step would take minutes or hours, which significantly slows down the training. If this simulator is fast enough or training time is not constrained then PG methods can perform well. If the simulator is not fast enough or training time is constrained then temporal difference methods can do better than PG methods as they are more sample efficient.
If the environment is the real one, then temporal difference can do well, as long as interaction with the environment is not slow. Model-based RL performs better if the environment is slow. Model-based methods require a model of the environment (that can be learned) and rely mainly on planning rather than learning (Deisenroth et al., 2011; Guo et al., 2014). Since planning is not done in the actual environment, but in much faster simulation steps within the model, it requires less samples from the real environment to learn. Many real-world system problems have well established and often highly accurate models, which model-based methods can leverage. That being said, model-free methods are often used as they are simpler to deploy and have the potential to generalize better from exploration in a real environment.
If good policies are easy to find and if either the space of policies is small enough or time is not a bottleneck for the search, then evolutionary methods can be effective. Evolutionary methods also have advantages when the learning agent cannot observe the complete state of the environment. As mentioned earlier, bandit solutions are good if the prob-

lem can be viewed as a one-step RL problem.
PG methods are in general more stable than methods like QLearning that do not directly use and derive a neural network to represent the agent's policy. The greedy nature of directly deriving the policy and moving the gradient in the direction of the objective also make PG methods easier to reason about and often more reliable. However, Q-Learning can be applied to data collected from a running system more readily than PG, which must interact with the system during training.
The RL methods may be implemented using any number of deep neural network architectures. The preferred architecture depends on the the nature of the observation and action spaces. CNNs that efficiently capture spatially-organized observation spaces lend themselves visual data (e.g., images or video). Networks designed for sequential learning, such as RNNs, are appropriate for observation spaces involving sequence data (e.g., code, queries, temporal event streams). Otherwise, FCNNs are preferred for their general applicability and ease of use, although they tend to be the most computationally-intensive choice. Finally, GNNs or other networks that capture structure within observations can be used in the less frequent case that the designer has a priori knowledge of the representational structure. In this case, the model can even generate structured action spaces (e.g., a query plan tree or computational graph).
7 CHALLENGES
In this section, we discuss the primary challenges that face the application of deep RL in system optimization.
Interactions with Real Systems Can Be Slow. Generalizing from Faster Simulated Environments Can Be Restrictive. Unlike the case with simulated environments that can run fast, when running on a real system, performing an action can trigger a reward after a lengthy delay. For example, when scheduling jobs on a cluster of nodes, some jobs might require hours to run, and thus improving their performance by monitoring job execution time will be very slow. To speed up this process, some works use simulators as cost models instead of the actual system. These simulators often do not fully capture the actual behavior of the real system and thus the RL agent may not work as well in practice. More comprehensive environment models can aid generalization from simulated environments. RL methods that are more sample efficient will speed up training in real system environments.
Instability and High Variance. This is a common problem which leads to bad policies when tackling system problems with deep RL. Such policies can generate a large performance gap when trained multiple times and behave in an unpredictable manner. This is mainly due to poor formula-

A View on Deep Reinforcement Learning in System Optimization

tion of the problem as an RL problem, limited observation of the state, i.e., the use of embeddings and input features that are not sufficient/meaningful, and sparse or similar rewards. Sparse rewards can be due to bad reward definition or the fact that some rewards cannot be computed directly and are known only at the end of the episode. For example, in (Liang et al., 2019), where deep RL is used to optimize decision trees for packet classification, the reward (the performance of the tree) is known only when the whole tree is built, or after approximately 15,000 steps. In some cases using more robust and stable policies can help. For example, Q-learning is known to have good sample efficiency but unstable behavior. SARSA, double Q-learning (Van Hasselt et al., 2016) and policy gradient methods, on the other hand, are more stable. Subtracting a bias in PG can also help reduce variance (Greensmith et al., 2004).
Lack of Reproducibility. Reproducibility is a frequent challenge with many recent works in system optimization that rely on deep RL. It becomes difficult to reproduce the results due to restricted access to the resources, code, and workloads used, lack of a detailed list of the used network hyperparameters and lack of stable, predictable, and scalable behavior of the different RL algorithms. This challenge prevents future deployment, incremental improvements, and proper evaluation.
Defining Appropriate Rewards, Actions and States. The proper definition of states, actions, and rewards is the key, since otherwise the RL solution is not useful. In the general use case of deep RL, defining the states, actions and rewards is much more straightforward than in the case in system optimization. For example, in atari games, the state is an image representing the current status of the game, the rewards are the points collected while playing and the actions are moves in the game. However, often in system optimization, it is not clear what are the appropriate definitions. Furthermore, in many cases the rewards are sparse or similar, the states are not fully observable to capture the whole system state and have limited features that capture only a small portion of the system state. This results in unstable and inadequate policies. Generally, the action and state spaces are large, requiring a lot of samples to learn and resulting in instability and large variance in the learned network. Therefore, retraining often fails to generate the same results.
Lack of Generalization. The lack of generalization is an issue that deep RL solutions often suffer from. This might be beneficial when learning a particular structure. For example, in NeuroCuts (Liang et al., 2019), the target is to build the best decision tree for fixed set of predefined rules and thus the objective of the RL agent is to find the optimal fit for these rules. However, lack of generalization sometimes results in a solution that works for a particular workload or setting but overall, across various workloads, is not very

good. This problem manifests when generalization is important and the RL agent has to deal with new states that it did not visit in the past. For example, in (Paliwal et al., 2019; Addanki et al., 2019), where the RL agent has to learn good resource placements for different computation graphs, the authors avoided the possibility of learning only good placements for particular computation graphs by training and testing on a wide range graphs.
Lack of Standardized Benchmarks, Frameworks and Evaluation Metrics. The lack of standardized benchmarks, frameworks and evaluation metrics makes it very difficult to evaluate the effectiveness of the deep RL methods in the context of system optimization. Thus, it is crucial to have proper standardized frameworks and evaluation metrics that define success. Moreover, benchmarks are needed that enable proper training, evaluation of the results, measuring the generalization of the solution to new problems and performing valid comparisons against baseline approaches.

8 AN ILLUSTRATIVE EXAMPLE

We put all the metrics (from Section 5) to work and further

highlight the challenges (from Section 7) of implementing

deep RL solutions using DeepRM (Mao et al., 2016) as

an illustrative example. In DeepRM, the targeted system

problem is resource allocation in the cloud. The objective is

to avoid job slowdown, i.e., the goal is to minimize the wait

time for all jobs. DeepRM uses PG in conjunction with a

simulated environment rather than a real cloud environment.

This significantly improves the step time but can result in

restricted generalization when used in a real environment.

Furthermore, since all the simulation parameters are known,

the full state of the simulated environment can be captured.

The actions are defined as selecting which job should be

scheduled next. The state is defined as the current allocation

of cluster resources, as well as the resource profiles of jobs

waiting to be scheduled. The reward is defined as the sum of

of

job

slowdowns:

i

(

-1 Ti

)

where

Ti

is

the

pure

execution

time of job i without considering the wait time. This reward

basically gives a penalty of -1 for jobs that are waiting to

be scheduled. The penalty is divided by Ti to give a higher priority to shorter jobs.

The state, actions and reward clearly define an MDP and a reinforcement learning problem. Specifically, the agent interacts with the system by making sequential allocations, observing the state of the current allocation of resources and receiving delayed long-term rewards as overall slow downs of jobs. The rewards are delayed because the agent cannot know the effect of the current allocation action on the overall slow down at any particular time step; the agent would have to wait until all the other jobs are allocated to assess the full impact. The agent then learns which jobs to allocate in the current time step to minimize the average job slowdown, given the current resource allocation in the

A View on Deep Reinforcement Learning in System Optimization

cloud. Note that DeepRM also learns to withhold larger jobs to make room for smaller jobs to reduce the overall average job slowdown. DeepRM is shown to outperform random search.
Expert actions are not available in this problem as there are no methods to find the optimal allocation decision at any particular time step. During training in DeepRM, multiple examples of job arrival sequences were considered to encourage policy generalization and robust decisions1. DeepRM is also shown to outperform the state-of-the-art by 10­63%1.
Clearly, in the case of DeepRM, most of the challenges mentioned in Section 7 are manifested. The interaction with the real cloud environment is slow and thus the authors opted for a simulated environment. This has the advantage of speeding up the training but may result in a policy that does not generalize to the real environment. Unfortunately, generalization tests in the real environment were not provided. The instability and high variance were addressed by subtracting a bias in the PG equation. The bias was defined as the average of job slowdowns taken at a single time step across all episodes. The implementation of DeepRM was open sourced allowing others to reproduce the results. The rewards, actions, and states defined allowed the agent to learn a policy that performed well in the simulated environment. Note that defining the state of the system was easier because the environment was simulated. The solution also considered multiple reward definitions. For example, -|J|, where J is the number of unfinished jobs in the system. This reward definition optimizes the average job completion time. The jobs evaluated in DeepRM were considered to arrive online according to a Bernoulli process. In addition, the jobs were chosen randomly and it is unclear whether they represent real workload scenarios or not. This emphasizes the need for standardized benchmarks and frameworks to evaluate the effectiveness of deep RL methods in scheduling jobs in the cloud.
9 FUTURE DIRECTIONS
We see multiple future directions for the deployment of deep RL in system optimization tasks. The general assumption is that deep RL may be useful in every system problem where the problem can be formulated as a sequential decision making process, and where meaningful action, state, and reward definitions can be provided. The objective of deep RL in such systems may span a wide range of options, such as energy efficiency, power, reliability, monitoring, revenue, performance, and utilization. At the processor level, deep RL could be used in branch prediction, memory prefetching, caching, data alignment, garbage collection, thread/task scheduling, power management, reliability, and monitoring.
1Results provided were only in the simulated system.

Compilers may also benefit from using deep RL to optimize the order of passes (optimizations), knobs/pragmas, unrolling factors, memory expansion, function inlining, vectorizing multiple instructions, tiling and instruction selection. With advancement of in- and near-memory processing, deep RL can be used to determine which portions of a workload should be performed in/near memory and which outside the memory.
At a higher system level, deep RL may be used in SQL/pandas query optimization, cloud computing, scheduling, caching, monitoring (e.g., temperature/failure) and fault tolerance, packet routing and classification, congestion control, FPGA allocation, and algorithm selection. While some of this has already been done, we believe there is big potential for improvement. It is necessary to explore more benchmarks, stable and generalizable learners, transfer learning approaches, RL algorithms, model-based RL and, more importantly, to provide better encoding of the states, actions and rewards to better represent the system and thus improve the learning. For example, with SQL/pandas join order optimization, the contents of the database are critical for determining the best order, and thus somehow incorporating an encoding of these contents may further improve the performance.
There is room for improvement in the RL algorithms as well. Some action and state spaces can dynamically change with time. For example, when adding a new node to a cluster, the RL agent will always skip the added node and it will not be captured in the environment state. Generally, the state transition function of the environment is unknown to the agent. Therefore, there is no guarantee that if the agent takes a certain action, a certain state will follow in the environment. This issue was presented in (Kulkarni et al., 2012), where compiler optimization passes were selected using deep RL. The authors mentioned a situation where the agent is stuck in an infinite loop of repeatedly picking the same optimization (action) back to back. This issue arose when a particular optimization did not change the features that describe the state of the environment, causing the neural network to apply the same optimization. To break this infinite loop, the authors limited the number of repetitions to five, and then instead, applied the second best optimization. This was done by taking the actions that corresponds to the second highest probability from the neural network's probability distribution output.
10 CONCLUSION
In this work, we reviewed and discussed multiple challenges in applying deep reinforcement learning to system optimization problems and proposed a set of metrics that can help evaluate the effectiveness of these solutions. Recent applications of deep RL in system optimization are mainly in

A View on Deep Reinforcement Learning in System Optimization

packet classification, congestion control, compiler optimization, scheduling, query optimization and cloud computing. The growing complexity in systems demands learning based approaches. Deep RL presents unique opportunity to address the dynamic behavior of systems. Applying deep RL to systems proposes new set of challenges on how to frame and evaluate deep RL techniques. We anticipate that solving these challenges will enable system optimization with deep RL to grow.
REFERENCES
Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H., and Alizadeh, M. Placeto: Learning generalizable device placement algorithms for distributed machine learning. arXiv preprint arXiv:1906.08879, 2019.
Andreasson, E., Hoffmann, F., and Lindholm, O. To collect or not to collect? machine learning for memory management. In Java Virtual Machine Research and Technology Symposium, pp. 27­39. Citeseer, 2002.
Arabnejad, H., Pahl, C., Jamshidi, P., and Estrada, G. A comparison of reinforcement learning techniques for fuzzy cloud auto-scaling. In Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, pp. 64­73. IEEE Press, 2017.
Ashouri, A. H., Killian, W., Cavazos, J., Palermo, G., and Silvano, C. A survey on compiler autotuning using machine learning. ACM Computing Surveys (CSUR), 51(5): 96, 2018.
Auer, P. Using confidence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3(Nov):397­422, 2002.
Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235­256, 2002.
Barrett, E., Howley, E., and Duggan, J. Applying reinforcement learning towards automating resource allocation and application scalability in the cloud. Concurrency and Computation: Practice and Experience, 25(12):1656­ 1674, 2013.
Bellman, R. A markovian decision process. In Journal of Mathematics and Mechanics, pp. 679­684, 1957.
Berry, D. A., and Fristedt, B. Bandit problems: sequential allocation of experiments (monographs on statistics and applied probability). London: Chapman and Hall, 5: 71­87, 1985.
Boyan, J. A., and Littman, M. L. Packet routing in dynamically changing networks: A reinforcement learning

approach. In Advances in neural information processing systems, pp. 671­678, 1994.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym, 2016.
Caspi, I., Leibovich, G., Novik, G., and Endrawis, S. Reinforcement learning coach, December 2017. URL https: //doi.org/10.5281/zenodo.1134899.
Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http: //arxiv.org/abs/1812.06110.
Choi, S. P., and Yeung, D.-Y. Predictive q-routing: A memory-based reinforcement learning approach to adaptive traffic control. In Advances in Neural Information Processing Systems, pp. 945­951, 1996.
Chu, W., Li, L., Reyzin, L., and Schapire, R. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 208­214, 2011.
Coons, K. E., Robatmili, B., Taylor, M. E., Maher, B. A., Burger, D., and McKinley, K. S. Feature selection and policy optimization for distributed instruction placement using reinforcement learning. In Proceedings of the 17th international conference on Parallel architectures and compilation techniques, pp. 32­42. ACM, 2008.
Das, A., Shafik, R. A., Merrett, G. V., Al-Hashimi, B. M., Kumar, A., and Veeravalli, B. Reinforcement learningbased inter-and intra-application thermal optimization for lifetime improvement of multicore systems. In Proceedings of the 51st Annual Design Automation Conference, pp. 1­6. ACM, 2014.
Deisenroth, M. P., Rasmussen, C. E., and Fox, D. Learning to control a low-cost manipulator using data-efficient reinforcement learning. Robotics: Science and Systems V, pp. 57­64, 2011.
Diegues, N., and Romano, P. Self-tuning intel transactional synchronization extensions. In 11th International Conference on Autonomic Computing ({ICAC} 14), pp. 209­219, 2014.
Doya, K. Reinforcement learning in continuous time and space. Neural computation, 12(1):219­245, 2000.
Farahnakian, F., Liljeberg, P., and Plosila, J. Energy-efficient virtual machines consolidation in cloud data centers using reinforcement learning. In 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, pp. 500­507. IEEE, 2014.

A View on Deep Reinforcement Learning in System Optimization

Gauci, J., Conti, E., Liang, Y., Virochsiri, K., He, Y., Kaden, Z., Narayanan, V., and Ye, X. Horizon: Facebook's open source applied reinforcement learning platform. arXiv preprint arXiv:1811.00260, 2018.
Graves, A., Mohamed, A.-r., and Hinton, G. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645­6649. IEEE, 2013.
Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov): 1471­1530, 2004.
Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In Advances in neural information processing systems, pp. 3338­3346, 2014.
Guu, K., Pasupat, P., Liu, E. Z., and Liang, P. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. arXiv preprint arXiv:1704.07926, 2017.
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
Hameed, A., Khoshkbarforoushha, A., Ranjan, R., Jayaraman, P. P., Kolodziej, J., Balaji, P., Zeadally, S., Malluhi, Q. M., Tziritas, N., Vishnu, A., et al. A survey and taxonomy on energy efficient resource allocation techniques for cloud computing systems. Computing, 98(7):751­774, 2016.
He, Y., Yu, F. R., Zhao, N., Leung, V. C., and Yin, H. Software-defined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach. IEEE Communications Magazine, 55(12):31­37, 2017a.
He, Y., Zhao, N., and Yin, H. Integrated networking, caching, and computing for connected vehicles: A deep reinforcement learning approach. IEEE Transactions on Vehicular Technology, 67(1):44­55, 2017b.
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Osband, I., et al. Deep q-learning from demonstrations. In ThirtySecond AAAI Conference on Artificial Intelligence, 2018.
Huang, Q., Haj-Ali, A., Moses, W., Xiang, J., Stoica, I., Asanovic, K., and Wawrzynek, J. Autophase: Compiler phase-ordering for hls with deep reinforcement learning. In 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pp. 308­308. IEEE, 2019.

Ipek, E., Mutlu, O., Mart´inez, J. F., and Caruana, R. Selfoptimizing memory controllers: A reinforcement learning approach. In ACM SIGARCH Computer Architecture News, volume 36, pp. 39­50. IEEE Computer Society, 2008.
Jamshidi, P., Sharifloo, A. M., Pahl, C., Metzger, A., and Estrada, G. Self-learning cloud controllers: Fuzzy qlearning for knowledge evolution. In 2015 International Conference on Cloud and Autonomic Computing, pp. 208­ 211. IEEE, 2015.
Jay, N., Rotman, N., Godfrey, B., Schapira, M., and Tamar, A. A deep reinforcement learning perspective on internet congestion control. In International Conference on Machine Learning, pp. 3050­3059, 2019.
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. Is q-learning provably efficient? In Advances in Neural Information Processing Systems, pp. 4863­4873, 2018.
Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: A survey. volume 4, pp. 237­285, 1996.
Kober, J., Bagnell, J. A., and Peters, J. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238­1274, 2013.
Krauter, K., Buyya, R., and Maheswaran, M. A taxonomy and survey of grid resource management systems for distributed computing. Software: Practice and Experience, 32(2):135­164, 2002.
Krishnan, S., Yang, Z., Goldberg, K., Hellerstein, J., and Stoica, I. Learning to optimize join queries with deep reinforcement learning. arXiv preprint arXiv:1808.03196, 2018.
Kuhnle, A., Schaarschmidt, M., and Fricke, K. Tensorforce: a tensorflow library for applied reinforcement learning. Web page, 2017. URL https://github. com/tensorforce/tensorforce.
Kulkarni, S., and Cavazos, J. Mitigating the compiler optimization phase-ordering problem using machine learning. In ACM SIGPLAN Notices, volume 47, pp. 147­162. ACM, 2012.
Lagoudakis, M. G., and Littman, M. L. Algorithm selection using reinforcement learning. In ICML, pp. 511­518. Citeseer, 2000.
Li, W., Zhou, F., Meleis, W., and Chowdhury, K. Learningbased and data-driven tcp design for memory-constrained iot. In 2016 International Conference on Distributed Computing in Sensor Systems (DCOSS), pp. 199­205. IEEE, 2016.

A View on Deep Reinforcement Learning in System Optimization

Liang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020, 2016.
Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gonzalez, J., Goldberg, K., and Stoica, I. Ray rllib: A composable and scalable reinforcement learning library. arXiv preprint arXiv:1712.09381, 2017.
Liang, E., Zhu, H., Jin, X., and Stoica, I. Neural packet classification. arXiv preprint arXiv:1902.10319, 2019.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Lin, L.-J. Reinforcement learning for robots using neural networks. 1993.
Littman, M., and Boyan, J. A distributed reinforcement learning scheme for network routing. In Proceedings of the international workshop on applications of neural networks to telecommunications, pp. 55­61. Psychology Press, 2013.
Liu, N., Li, Z., Xu, J., Xu, Z., Lin, S., Qiu, Q., Tang, J., and Wang, Y. A hierarchical framework of cloud resource allocation and power management using deep reinforcement learning. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS), pp. 372­382. IEEE, 2017.
Luong, N. C., Hoang, D. T., Gong, S., Niyato, D., Wang, P., Liang, Y.-C., and Kim, D. I. Applications of deep reinforcement learning in communications and networking: A survey. IEEE Communications Surveys & Tutorials, 2019.
Mahdavinejad, M. S., Rezvan, M., Barekatain, M., Adibi, P., Barnaghi, P., and Sheth, A. P. Machine learning for internet of things data analysis: A survey. Digital Communications and Networks, 4(3):161­175, 2018.
Mao, H., Alizadeh, M., Menache, I., and Kandula, S. Resource management with deep reinforcement learning. In Proceedings of the 15th ACM Workshop on Hot Topics in Networks, pp. 50­56. ACM, 2016.
Mao, H., Negi, P., Narayan, A., Wang, H., Yang, J., Wang, H., Marcus, R., Addanki, R., Khani, M., He, S., et al. Park: An open platform for learning augmented computer systems. 2019.
Marcus, R., and Papaemmanouil, O. Deep reinforcement learning for join order enumeration. In Proceedings of the First International Workshop on Exploiting Artificial

Intelligence Techniques for Data Management, pp. 3. ACM, 2018.
Marcus, R., Negi, P., Mao, H., Zhang, C., Alizadeh, M., Kraska, T., Papaemmanouil, O., and Tatbul, N. Neo: A learned query optimizer. arXiv preprint arXiv:1904.03711, 2019.
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928­ 1937, 2016.
Mostafavi, S., Ahmadi, F., and Sarram, M. A. Reinforcement-learning-based foresighted task scheduling in cloud computing. arXiv preprint arXiv:1810.04718, 2018.
Ortiz, J., Balazinska, M., Gehrke, J., and Keerthi, S. S. Learning state representations for query optimization with deep reinforcement learning. arXiv preprint arXiv:1803.08604, 2018.
Paliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M., Kohli, P., and Vinyals, O. Regal: Transfer learning for fast optimization of computation graphs. arXiv preprint arXiv:1905.02494, 2019.
Peled, L., Mannor, S., Weiser, U., and Etsion, Y. Semantic locality and context-based prefetching using reinforcement learning. In 2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA), pp. 285­297. IEEE, 2015.
Peng, Z., Cui, D., Zuo, J., Li, Q., Xu, B., and Lin, W. Random task scheduling scheme based on reinforcement learning in cloud computing. Cluster computing, 18(4): 1595­1607, 2015.
Peters, J., Vijayakumar, S., and Schaal, S. Reinforcement learning for humanoid robotics. In Proceedings of the third IEEE-RAS international conference on humanoid robots, pp. 1­20, 2003.
Rao, J., Bu, X., Xu, C.-Z., Wang, L., and Yin, G. Vconf: a reinforcement learning approach to virtual machines autoconfiguration. In Proceedings of the 6th international conference on Autonomic computing, pp. 137­146. ACM, 2009.
Ross, S., Gordon, G., and Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online

A View on Deep Reinforcement Learning in System Optimization

learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627­635, 2011.
Ruffy, F., Przystupa, M., and Beschastnikh, I. Iroko: A framework to prototype reinforcement learning for data center traffic control. arXiv preprint arXiv:1812.09975, 2018.
Rummery, G. A., and Niranjan, M. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, England, 1994.
Sadeghi, A., Sheikholeslami, F., and Giannakis, G. B. Optimal and scalable caching for 5g using reinforcement learning of space-time popularities. IEEE Journal of Selected Topics in Signal Processing, 12(1):180­190, 2017.
Schaal, S. Learning from demonstration. In Advances in neural information processing systems, pp. 1040­1046, 1997.
Schaal, S. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3(6):233­242, 1999.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Silva, A. P., Obraczka, K., Burleigh, S., and Hirata, C. M. Smart congestion control for delay-and disruption tolerant networks. In 2016 13th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), pp. 1­9. IEEE, 2016.

Wang, Z., and OBoyle, M. Machine learning in compiler optimization. Proceedings of the IEEE, 106(11):1879­ 1901, 2018.
Watkins, C. J., and Dayan, P. Q-learning. Machine learning, 8(3-4):279­292, 1992.
Xu, C.-Z., Rao, J., and Bu, X. Url: A unified reinforcement learning approach for autonomic cloud management. Journal of Parallel and Distributed Computing, 72 (2):95­105, 2012.
Xu, Z., Wang, Y., Tang, J., Wang, J., and Gursoy, M. C. A deep reinforcement learning based framework for powerefficient resource allocation in cloud rans. In 2017 IEEE International Conference on Communications (ICC), pp. 1­6. IEEE, 2017.
Zeppenfeld, J., Bouajila, A., Stechele, W., and Herkersdorf, A. Learning classifier tables for autonomic systems on chip. GI Jahrestagung (2), 134:771­778, 2008.
Zhong, V., Xiong, C., and Socher, R. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.
Zhu, Q., and Yuan, C. A reinforcement learning approach to automatic error recovery. In 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07), pp. 729­738. IEEE, 2007.

Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.

Sutton, R. S., and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.

Tesauro, G., Das, R., and Jong, N. K. Online performance management using hybrid reinforcement learning. Proceedings of SysML, 2006.

Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence, 2016.


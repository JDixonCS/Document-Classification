arXiv:1905.09866v1 [cs.CL] 23 May 2019

Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor

Malvina Nissim
University of Groningen
Rob van der Goot
University of Groningen

Rik van Noord
University of Groningen

Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also exposed how strongly human biases are encoded in vector spaces built on natural language. While finding that queen is the answer to man is to king as woman is to X leaves us in awe, papers have also reported finding analogies deeply infused with human biases, like man is to computer programmer as woman is to homemaker, which instead leave us with worry and rage. In this work we show that, often unknowingly, embedding spaces have not been treated fairly. Through a series of simple experiments, we highlight practical and theoretical problems in previous works, and demonstrate that some of the most widely used biased analogies are in fact not supported by the data. We claim that rather than striving to find sensational biases, we should aim at observing the data "as is", which is biased enough. This should serve as a fair starting point to properly address the evident, serious, and compelling problem of human bias in word embeddings.
1. Introduction
Word embeddings are distributed representations of texts which capture similarities between words. Beside improving a wide variety of NLP tasks, the power of word embeddings is often also tested intrinsically. Together with the idea of training word embeddings, Mikolov et al. (2013a) introduced the idea of testing the soundness of embedding spaces via the analogy task. Proportional analogies are equations of the form A : B :: C : D, or simply A is to B as C is to D. Given the terms A, B, C, the model must return the word that correctly stands for D in the given analogy. A most classic example is man is to king as woman is to X, where the model is expected to return queen, by subtracting "manness" from the concept of king to obtain some general royalty, and then re-adding some "womanness" to obtain the concept of queen (king - man + woman = queen).
Beside this kind of magical power, however, embeddings have been shown to carry worrying biases present in our society and thus encoded in language. Recent studies (Bolukbasi et al. 2016; Manzini et al. 2019b) found that embeddings yield biased analogies such as the classic man is to doctor as woman is to nurse, or man is to computer programmer as woman is to homemaker.
Attempts at reducing bias, either via postprocessing (Bolukbasi et al. 2016) or directly in training (Zhao et al. 2018) have nevertheless left two outstanding issues: bias is still encoded implicitly (Gonen and Goldberg 2019), and it is debat-

© 2019 Association for Computational Linguistics

Computational Linguistics

Volume 0, Number 0

able whether we should aim at removal or rather at transparency and awareness (Caliskan, Bryson, and Narayanan 2017; Gonen and Goldberg 2019).
With an eye to transparency, we took a closer look at the analogy structure. In the original proportional analogy implementation, all terms of the equation A : B :: C : D are distinct (Mikolov et al. 2013a; Goldberg 2017, p. 138). In other words, the model is forced to return a different concept than the original ones. Given an analogy of the form A : B :: C : D, the model is not allowed to yield any term D such that D == B, or D == A, or D == C, since the code explicitly prevents this. While this constraint is helpful when all terms of the analogy are expected to be different, it becomes a problem, and even a dangerous artifact, when the terms could or even should be the same.
We investigate this issue using the original analogy test set (Mikolov et al. 2013a), and examples from the literature. We test all examples on different embedding spaces built for English, using two settings for the analogy code: when all terms must be different (as in the original, widely used, implementation), and without this constraint, meaning that any word, including the input terms, can be returned. As far as we know, this is the first work that evaluates and reports analogies in an unrestricted fashion, since the analogy code is always used as is. Our experiments and results suggest that the mainstream examples as well as the use of the analogy task itself as a tool to detect bias should be revised and reconsidered. Warning This work does not mean at all to downplay the presence and danger of human biases in word embeddings. On the contrary: embeddings do encode human biases, and we believe that this issue deserves the full attention of the field. However, we also believe that overemphasising and specifically seeking biases to achieve sensational results is not beneficial. It is also not necessary: what we observe naturally is worrying and sensational enough. Rather, we should aim at transparency and experimental clarity so as to ensure the fairest and most efficient treatment of the problem.
2. Experimental details
For both word2vec (Mikolov et al. 2013a) and gensim (R ehu° rek and Sojka 2010) we adapted the code so that the input terms of the analogy query are allowed to be returned. Throughout this article, we use two different embedding spaces. The first is the widely used representation built on GoogleNews1 (Mikolov et al. 2013b). The second is taken from (Manzini et al. 2019b), and was trained on a Reddit dataset (Rabinovich, Tsvetkov, and Wintner 2018).
We test analogies using the code with and without modification, with the aim of showing the drawbacks and dangers of constraining (and selecting) the output of analogy queries to word embeddings. The analogies we use in this article come from three sources: the original analogy dataset proposed by Mikolov et al. (2013a) (Section 3), a small selection of additional analogies to highlight the need to be able to return input vectors (Section 3), and a collection of examples found in papers that address the problem of (human) biases in word embeddings (Section 4). We follow Mikolov et al. (2013a), Bolukbasi et al. (2016) and Manzini et al. (2019b) by using 3COSADD to calculate the analogies, as shown in Equation 1:

argmax(cos(d, c) - cos(d, a) + cos(d, b))

(1)

d

1 https://code.google.com/archive/p/word2vec/

2

M. Nissim, R. van Noord and R. van der Goot

Fair is Better than Sensational

Table 1: Performance on the standard analogy test set (Mikolov et al. 2013a) using the original and the fair versions of the analogy code. The fair version allows for any term in the vocabulary to be returned, including the input terms, while the original one does not allow any of the input terms to be returned.

Category
Semantic
capital-common-countries capital-world currency city-in-state
Morpho-syntactic
family gram1-adjective-to-adverb gram2-opposite gram3-comparative gram4-superlative gram5-present-participle gram6-nationality-adjective gram7-past-tense gram8-plural gram9-plural-verbs

orig
83.20 79.13 27.37 70.90
84.58 27.72 42.73 90.84 87.34 78.22 89.93 64.49 86.04 67.93

fair
44.47 25.82 18.13 10.21
32.61 2.02 1.72 24.70 11.05 6.91 73.80 8.59 4.73 12.18

All the examples used in this article, plus any new query, can be tested on any of the embeddings in the original and modified analogy code2, and through our online demo.3
3. Not all analogies are the same
The original, widely used, analogy test set introduced by Mikolov et al. (2013a) consists of two main categories: semantic analogies (Paris is to France as Tokyo is to Japan) and morpho-syntactic analogies (car is to cars as table is to tables). Within these, examples are classified in more specific sub-categories, as shown in the left column of Table 1. In the same table we report two scores based on the Google News embeddings as well as for the reddit embeddings from (Manzini et al. 2019b). Under "orig" we report the score obtained using the original analogy code, and under "fair" we report the score yielded by our altered version, where the query terms (A, B, C) can be returned.
The results show a drastic drop in performance for the fair setting. In most cases, this is because the second term is returned as answer (man is to king as woman is to king, thus D == B), but in some cases it is the third term that gets returned (big is to bigger as cold is to cold, thus D == C). Results are over 50% lower in the semantic set, and the drop is even more serious in the syntactic examples, with the exception of "nationality-adj".
In the default evaluation set, and in the extended set proposed by Gladkova, Drozd, and Matsuoka (2016) to cover additional linguistic relations,
2 https://bitbucket.org/robvanderg/w2v/ 3 www.robvandergoot.com/embs
3

Computational Linguistics

Volume 0, Number 0

Table 2: Results of analogy task on examples where one of the original words is the correct answer. Bold indicates the correct answer.

Analogy
Homographs
decrease decreased bet eat ate split banana bananas fish Athens Greece Luxembourg Berlin Germany Singapore
Hypernyms, Holonyms, Orders
cat animal dog beer drink wine finger hand nail branch tree leaf sophomore junior freshman silver gold bronze

1st
bet split fish Luxembourg Singapore
animal wine nail leaf junior bronze

2nd
bets splitting lobsters Belgium Malaysia
dog drink hand tree freshman bronze_medal

there are no word-pairs for which the gold target word is one of the three query words, in other words: A, B or C is the correct answer. Thus one might deem it a reasonable decision that the original analogy code does not let any of the original vectors to be returned. However, these conditions do exist, and this choice has consequences. The major consequence we observe is discussed in Section 4, and has to do with analogies affected by human bias. But even for the analogy types of Table 1, there are cases where this constraint is undesirable, due to homography. Additionally, there are other analogy types for which such constraint is utterly counterproductive, such as is-a or part-of relations.
Homographs. Because most out-of-the-box word embeddings have no notion of senses, homographs are modelled as one unit. For example, the infinitive form and the past tense of the verb "to read", will be represented by one single vector for the word "read". A consequence of this is that for certain examples, one needs the model to return a term identical to one of the input terms, though they would be conceptually different.
Morpho-syntactically, this happens often with strong verbs, where infinitive and simple past are homographs (e.g. split/split). But other analogy types are also affected. For example, there are countries or regions that are homographs with their capitals (e.g. Singapore/Singapore). In all such cases, not allowing the model to return any of the original vectors makes it impossible to obtain the correct answer to the analogy query.
We just tested a few examples of this kind using the modified code on the GoogleNews embeddings, and report them in the top part of Table 2.
Hypernyms, Holonyms, Orders. Examples of other cases where one of the original words from the query should be returned are shown in the bottom part of Table 2. These include "is-a" relations (hypernyms), "part-of" relations (holonyms) and cases where a natural ordering of concepts exists. Differently than what observed for the homograph examples, these relations seems to be less explicitly encoded in the embedding space.
4

M. Nissim, R. van Noord and R. van der Goot

Fair is Better than Sensational

These are obviously only a couple of examples, and no conclusions can be drawn over the power of embeddings in expressing such relations in general. But they serve to clearly illustrate the existence of cases where input terms are desired as answer, further highlighting the drawbacks of imposing constraints on the analogy code.
However, these are still examples in the ballpark of factually `correct or wrong'. The issue becomes worryingly more irksome when analogies are aimed at uncovering bias.

4. Let women be doctors
One of the most well known analogies brought as example of human bias in word embeddings is man is to doctor as woman is to nurse (Bolukbasi et al. 2016; Manzini et al. 2019b). This heavily biased analogy reflecting gendered stereotypes in our society, is however truly meaningful only if the system were allowed to yield "doctor" (arguably the expected answer in absence of bias) instead of "nurse", and it doesn't. But we know that the system isn't allowed to return this candidate, since the original analogy code rules out the possibility of returning as D any of the query terms A, B, C, making it impossible to obtain man is to doctor as woman is to doctor (where D == B).
This means that the bias isn't necessarily (or at least not only) in the representations themselves, rather in the way we query them. So, what do the embedding spaces actually tell if you let them return any word in the vocabulary?
We took a selection of mainstream, striking examples from the literature on embedding bias, and tested them fairly, without posing any constraint on the returned term, exactly as we did for all analogies in Section 3. In Table 3 we report these examples, organised by the papers which discussed them, together with the returned term as reported in the paper itself, and the top two terms returned when using our modified code (1st and 2nd, respectively). Each example is tested over the same embedding space used in the corresponding paper.

4.1 Constraining the output
What immediately stands out is that, bar a few exceptions, we do not obtain the term reported in the respective paper. One reason for this is that the model is now allowed to return the input vectors, and in most cases it does just that (especially D == B).
In Section 3, we saw how this heavily affects the results on the original analogy test, and we also discussed why it would nevertheless be beneficial to impose no restrictions on the returned answer. When analogies are used to study human bias, though, the problem is more serious: How can we claim the model is biased because it does not return doctor if the model is simply not allowed to return doctor?
As a further constraint to the allowed output, Bolukbasi et al. (2016) add an empirical threshold to Equation 1 to ensure that terms that are too similar to B are excluded. Consequences are non-trivial. By not allowing the returned vector to be too close to the input vectors, this method basically skips potentially valid, unbiased answers until a potentially more biased answer is found. It isn't necessarily the case that more distance corresponds to more bias, but it is usually the case that less distance is akin to less bias (for example, gynecologist is a less biased answer than nurse to the query man is to doctor as woman is to X).
5

Computational Linguistics

Volume 0, Number 0

Table 3: Fair results of embedding analogies. We use the same embedding set for our analysis as is used in the respective papers. "Index" denotes the position where the reported biased answer was actually found in our experiments. 1st answer and 2nd answer report what we actually find in first and second position. In brackets: the index of the reported answer as obtained using the swapped query, C is to B as A is to X.

Analogy

Reported

Index 1st answer

2nd answer

Mikolov et al. (2013a)

man king woman Paris France Tokyo brother sister grandson big bigger cold Einstein scientist Picasso

queen

2

Japan

1

granddaughter 1

colder

2

painter

1

king Japan granddaughter cold painter

queen Tokyo niece colder scientist

Bolukbasi et al. (2016)

man computer_programmer woman homemaker

2

he doctor she

nurse

2

she interior_designer he

architect

2

she feminism he

conservatism 4

she lovely he

brilliant

10

she sewing he

carpentry

4

computer_programmer homemaker

doctor

nurse

interior_designer

architect

feminism

liberalism

lovely

magnificent

sewing

woodworking

Manzini et al. (2019b)

black criminal caucasian caucasian lawful black caucasian hillbilly asian asian yuppie caucasian asian engineer black black killer asian christian conservative jew jew liberal christian muslim terrorist jew jew journalist muslim christian conservative muslim muslim regressive christian

lawful

13

criminal

2

yuppie

3

hillbilly

2

killer

39

engineer

7

liberal

4

conservative 2

journalist

4

terrorist

2

regressive

53

conservative 13

legal lawful hillbilly yuppie operator killer centrist liberal hacker purportedly moderate regressive

statutory criminal hippy hillbilly jockey impostor democrat conservative protestor terrorist conservative progressive

4.2 First or twentieth is not the same
A closer look at the results makes things even more worrying. If the top answer yielded by our unrestricted code is one of the input vectors (e.g. doctor), the original code would not have shown it. It would have instead yielded what we obtain as our second answer. This is what we should see in the reported analogies. However, Table 3 (column Index) shows that this is not always the case.
The threshold method of Bolukbasi et al. (2016) described in Section 4.1 is the cause for this outcome in their examples, as vectors higher in the rank have been excluded as too close to the input vector. Unfortunately, while surely successful over standard, factual analogy cases, this strategy turns out to be essentially a way of selecting the output. For example, their strategy not only excludes lovely (input term), but also magnificent as a possible answer for she is to lovely as he is to X, since the vector for magnificent is not distant enough from the vector of input term lovely. As can be seen in Table 3, lovely and magnificent would be the first and second words returned otherwise. The term brilliant is only returned in 10th position by an unrestricted search. While aiming at returning a
6

M. Nissim, R. van Noord and R. van der Goot

Fair is Better than Sensational

Table 4

(a) Top 10 answers for the asian is to engineer as black is to X for the Reddit embeddings of (Manzini et al. 2019b) and the Google News embeddings.

(b) Top 10 answers for both the usual and the inverse results for the analogy man is to doctor as woman is to, using the Google News embeddings.

Manzini et al. (2019b)

GoogleNews

1. operator 2. jockey 3. technician 4. welder 5. stingray 6. wizard 7. navigator 8. provocateur 9. thief 10. painter

1. engineer 2. electrical_engineer 3. mechanical_engineer 4. Engineer 5. engineers 6. black 7. electrician 8. technician 9. engineeer 10. engineering

man : doctor woman : X

woman : doctor man : X

1. doctor

1. doctor

2. gynecologist

2. physician

3. nurse

3. doctors

4. doctors

4. surgeon

5. physician

5. dentist

6. pediatrician

6. cardiologist

7. nurse_practitioner 7. neurologist

8. obstetrician

8. neurosurgeon

9. ob_gyn

9. urologist

10. midwife

10. Doctor

vector distant enough from the input term might be desirable for some of the analogies, this threshold-based strategy is not fair when researching bias, as it potentially forces the exclusion of unbiased terms (in this case, after magnificent, one would find the following terms before encountering brilliant: marvelous, splendid, nice, fantastic, delightful, terrific, wonderful). In the example she is to sewing as he is to X., the threshold was strong enough to even exclude a potentially biased answer (woodworking).
Manzini et al. (2019b) also use the analogy test to demonstrate bias, starting from a pre-selection of terms to construct their queries from a variety of sources. In addition to using the original analogy code, thus missing out on what the actual returned term would be, they state that rather than reporting the top term, they hand-pick an example from the returned top-N words. While qualitatively observing and weighing the bias of a large set of returned answers makes sense, it can be misleading to cherry-pick and report very biased terms in sensitive analogies. At the very least, when reporting termN, one should report the top-N terms to provide a more accurate picture. In Table 4a, we report the top-10 candidates for asian is to engineer as black is to X in both the Reddit embeddings of (Manzini et al. 2019b) as well as GoogleNews, for completeness. Similarly, we now know that an unrestricted analogy search for man is to doctor as woman is to X returns doctor, but this does not provide a complete picture. Reporting the top-10 for this query as well as the top-10 for the inverted query (Table 4b) surely allows for a much better informed analysis rather than simply reporting doctor, or picking nurse.
4.3 Computer programmer or just programmer?
If the analogy really is a symptom of a biased vector space, we should find similar biases for synonyms or closely related words to the input word B. However, with computer_programmer for example, this does not seem to be the case. If we use the term programmer instead of computer_programmer, homemaker is not very close (165), while for
7

Computational Linguistics

Volume 0, Number 0

coder (13,374), developer (26,117)4 and hacker (56,646) it does not even appear in the top 10,000. Also, when using white instead of the less frequent and more specialised (and in a way less parallel to black) caucasian in the analogy of black is to criminal as caucasian is to X, lawful is found at position 40 instead of 13.
In a way, examples are always cherry-picked, but when making substantial claims on observed biases, the fact that the results we obtain are due to a carefully chosen word (rather than a similar one, possibly even more frequent), should not be overlooked.
5. Please, use analogies fairly, and with care
If we do not operate any manipulations on the returned vectors, neither by setting constraints nor by cherry-picking the output, we observe that in many cases, independently of the analogy type and the query terms, the model simply returns one of the input terms, and in particular D == B. Perhaps, this is a weakness of embeddings in modelling certain relations, or the analogy task as such is not apt at capturing them.
Such observations relate to two points raised in previous work. First, the suggestive power of analogies should not be overestimated. It has been argued that what is observed through the analogy task might be mainly due to irrelevant neighborhood structure rather than to the vector offset that supposedly captures the analogy itself (Linzen 2016; Rogers, Drozd, and Li 2017). Indeed, Drozd, Gladkova, and Matsuoka (2016) have also shown that the 3COSADD method is not able to capture all linguistic regularities present in the embeddings. Interestingly, the analogy task has not been recently used anymore to evaluate the soundness of contextualised embeddings (Devlin et al. 2018; Peters et al. 2018).5 Second, bias isn't fully captured anyway via the analogy task. In fact,Gonen and Goldberg (2019) suggest that analogies are not quite reliable diagnostics for uncovering bias in word embeddings, since bias is anyway often encoded implicitly. As a side note, we would like to mention that in an earlier version of their paper, Manzini et al. (2019a) accidentally searched for the inverse of the intended query, and still managed to find biased examples.6 This seems to be a further, strong, indication that strategies like this are not fully suitable to demonstrate the presence of bias in embeddings.
If analogies might not be the most appropriate tool to capture certain relations, surely matters have been made worse by selecting results in order to prove (and emphasise) the presence of human bias. Using such sensational "party tricks" (Gonen and Goldberg 2019) is harmful, as they get easily propagated both in science itself (Jha and Mamidi 2017; Gebru et al. 2018; Mohammad et al. 2018), even outside NLP and AI (McQuillan 2018), as well as in popularised articles of the calibre of Nature (Zou and Schiebinger 2018). This is even more dangerous, because of the widened pool of readers, and because such readers are usually in no position to verify the reliability of such examples.
In any case, anyone who constructs and uses analogies to uncover human biases must do this fairly and transparently, and be aware of their limitations. In this sense,

4 Likely also due to the different senses of developer. 5 Recent work also suggests that, compared to word embeddings, contextualised embeddings (Devlin et al.
2018; Peters et al. 2018) might be less biased (Zhao et al. 2019; Basta, Costa-jussà, and Casas 2019). 6 Instead of asking the model A is to B as C is to X, they queried C is to B as A is to X. So while reporting
results for black is to criminal as caucasian is to X showing how they supported the hypothesis that there is some cultural bias against the black, they had in fact searched for caucasian is to criminal as black is to X. We confirmed this with the authors.
8

M. Nissim, R. van Noord and R. van der Goot

Fair is Better than Sensational

it is admirable that Caliskan, Bryson, and Narayanan (2017) try to better understand their results by checking them against actual job distributions between the two genders. Aiming primarily at scientific discovery rather than sensational findings is a strict prerequisite to truly understand how and to what extent embeddings encode and reflect the biases of our society, and how to cope with this, both socially and computationally.

9

Computational Linguistics

Volume 0, Number 0

References
Basta, Christine, Marta Ruiz Costa-jussà, and Noe Casas. 2019. Evaluating the underlying gender bias in contextualized word embeddings. CoRR, abs/1904.08783.
Bolukbasi, Tolga, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems 29, pages 4349­4357, Curran Associates, Inc.
Caliskan, Aylin, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183­186.
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (to appear in NAACL 2019).
Drozd, Aleksandr, Anna Gladkova, and Satoshi Matsuoka. 2016. Word embeddings, analogies, and machine learning: Beyond king - man + woman = queen. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3519­3530, The COLING 2016 Organizing Committee, Osaka, Japan.
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumeé III, and Kate Crawford. 2018. Datasheets for datasets. arXiv preprint arXiv:1803.09010 V4.
Gladkova, Anna, Aleksandr Drozd, and Satoshi Matsuoka. 2016. Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't. In Proceedings of the NAACL Student Research Workshop, pages 8­15, Association for Computational Linguistics, San Diego, California.
Goldberg, Yoav. 2017. Neural network methods for natural language processing. Synthesis Lectures on Human Language Technologies, 10(1):1­309.
Gonen, Hila and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them.
Jha, Akshita and Radhika Mamidi. 2017. When does a compliment become sexist? analysis and classification of ambivalent sexism using Twitter data. In Proceedings of the Second Workshop on NLP and

Computational Social Science, pages 7­16, Association for Computational Linguistics, Vancouver, Canada. Linzen, Tal. 2016. Issues in evaluating semantic spaces using word analogies. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 13­18, Association for Computational Linguistics, Berlin, Germany. Manzini, Thomas, Yao Chong Lim, Yulia Tsvetkov, and Alan W Black. 2019a. Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. Manzini, Thomas, Yao Chong Lim, Yulia Tsvetkov, and Alan W Black. 2019b. Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. McQuillan, Dan. 2018. People's councils for ethical machine learning. Social Media+ Society, 4(2). Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111­3119. Mohammad, Saif, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. SemEval-2018 task 1: Affect in tweets. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 1­17, Association for Computational Linguistics, New Orleans, Louisiana. Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227­2237, Association for Computational Linguistics, New Orleans, Louisiana. Rabinovich, Ella, Yulia Tsvetkov, and Shuly Wintner. 2018. Native language cognate effects on second language lexical choice. Transactions of the Association for Computational Linguistics, 6:329­342.

10

M. Nissim, R. van Noord and R. van der Goot
R ehu° rek, Radim and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45­50, ELRA, Valletta, Malta.
Rogers, Anna, Aleksandr Drozd, and Bofang Li. 2017. The (too many) problems of analogical reasoning with word vectors. In Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 135­148, Association for Computational Linguistics, Vancouver, Canada.
Zhao, Jieyu, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender bias in contextualized word embeddings. arXiv preprint arXiv:1904.03310 (to appear in NAACL 2019).
Zhao, Jieyu, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-Wei Chang. 2018. Learning gender-neutral word embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847­4853, Association for Computational Linguistics, Brussels, Belgium.
Zou, James and Londa Schiebinger. 2018. AI can be sexist and racist-it's time to make it fair. Nature, 559(7714):324.

Fair is Better than Sensational

11


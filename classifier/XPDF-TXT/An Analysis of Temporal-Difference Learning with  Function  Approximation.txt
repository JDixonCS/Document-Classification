674

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

An Analysis of Temporal-Difference Learning with Function Approximation
John N. Tsitsiklis, Member, IEEE, and Benjamin Van Roy

Abstract-- We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an infinite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a finite or infinite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.
In addition to proving new and stronger positive results than those previously available, we identify the significance of online updating and potential hazards associated with the use of nonlinear function approximators. First, we prove that divergence may occur when updates are not based on trajectories of the Markov chain. This fact reconciles positive and negative results that have been discussed in the literature, regarding the soundness of temporal-difference learning. Second, we present an example illustrating the possibility of divergence when temporaldifference learning is used in the presence of a nonlinear function approximator.
Index Terms-- Dynamic programming, function approximation, Markov chains, neuro-dynamic programming, reinforcement learning, temporal-difference learning.
I. INTRODUCTION
THE PROBLEM of predicting the expected long-term future cost (or reward) of a stochastic dynamic system manifests itself in both time-series prediction and control. An example in time-series prediction is that of estimating the net present value of a corporation as a discounted sum of its future cash flows, based on the current state of its operations. In control, the ability to predict long-term future cost as a function of state enables the ranking of alternative states in order to guide decision-making. Indeed, such predictions constitute the cost-to-go function that is central to dynamic programming and optimal control [1].
Temporal-difference learning, originally proposed by Sutton [2], is a method for approximating long-term future cost as a function of current state. The algorithm is recursive, efficient, and simple to implement. A function approximator is used to approximate the mapping from state to future cost.
Manuscript received March 20, 1996; revised November 11, 1996. Recommended by Associate Editor, E. K. P. Chong. This work was supported by the NSF under Grant DMI-9625489 and the ARO under Grant DAAL-03-92G-0115.
The authors are with the Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail: jnt@mit.edu).
Publisher Item Identifier S 0018-9286(97)03437-5.

Parameters of the function approximator are updated upon each observation of a state transition and the associated cost. The objective is to improve approximations of long-term future cost as more and more state transitions are observed. The trajectory of states and costs can be generated either by a physical system or a simulated model. In either case, we view the system as a Markov chain. Adopting terminology from dynamic programming, we will refer to the function mapping states of the Markov chain to expected long-term cost as the cost-to-go function.
Though temporal-difference learning is simple and elegant, a rigorous analysis of its behavior requires significant sophistication. Several previous papers have presented positive results about the algorithm. These include [2]­[7], all of which only deal with cases where the number of tunable parameters is the same as the cardinality of the state space. Such cases are not practical when state spaces are large or infinite. The more general case, involving the use of function approximation, is addressed by results in [8]­[12]. The latter three establish convergence with probability one. However, their results only apply to a very limited class of function approximators and involve variants of a constrained version of temporaldifference learning, known as TD(0). Dayan [8] establishes convergence in the mean for the general class of linear function approximators, i.e., function approximators involving linear combinations of fixed basis functions, where the weights of the basis functions are tunable parameters. However, this form of convergence is rather weak, and the analysis used in the paper does not directly lead to approximation error bounds or interpretable characterizations of the limit of convergence. Schapire and Warmuth [9] carry out a (nonprobabilistic) worst case analysis of an algorithm similar to temporal-difference learning. Fewer assumptions are required by their analysis, but the end results do not imply convergence and establish error bounds that are weak relative to those that can be deduced in the standard probabilistic framework.
In addition to the positive results, counterexamples to variants of the algorithm have been provided in several papers; these include [10], [11], [13], and [14]. As suggested by Sutton [15], the key feature that distinguishes these negative results from their positive counterparts is that the variants of temporaldifference learning used do not employ on-line state sampling. In particular, sampling is done by a mechanism that samples states with frequencies independent from the dynamics of the underlying system. Our results shed light on these counterexamples by showing that for linear function approximators, convergence is guaranteed if states are sampled according

0018­9286/97$10.00 © 1997 IEEE

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

675

to the steady-state probabilities, while divergence is possible when states are sampled from distributions independent of the dynamics of the Markov chain of interest. Given that the steady-state probabilities are usually unknown, the only viable approach to generating the required samples is to perform online sampling. By this we mean that the samples should consist of an actual sequence of visited states obtained either through simulation of a Markov chain or observation of a physical system.
In addition to the analysis of temporal-difference learning in conjunction with linear function approximators, we provide an example demonstrating that the algorithm may diverge when a nonlinear function approximator is employed. This example should be viewed as a warning rather than a ban on all nonlinear function approximators. In particular, the function approximator used in the example is somewhat contrived, and it is not clear whether or not divergence can occur with specific classes of nonlinear function approximators such as neural networks.
In this paper, we focus on the application of temporaldifference learning to infinite-horizon discounted Markov chains with finite or infinite state spaces. Though absorbing (and typically finite state) Markov chains have been the dominant setting for past analyses, we find the infinite-horizon framework to be the most natural and elegant setting for temporal-difference learning. Furthermore, the ideas used in our analysis can be applied to the simpler context of absorbing Markov chains. Though this extension is omitted from this paper, it can be found in [16], which also contains a more accessible version of the results in this paper for the case of finite state spaces.
The contributions in this paper are as follows.
1) Convergence (with probability one) is established for the case where approximations are generated by linear combinations of (possibly unbounded) basis functions over a (possibly infinite) state space. This is the first such result that handles the case of "compact representations" of the cost-to-go function, in which there are fewer parameters than states. (In fact, convergence of on-line TD( ) in the absence of an absorbing state had not been established even for the case of a lookup table representation.)
2) The limit of convergence is characterized as the solution to a set of interpretable linear equations, and a bound is placed on the resulting approximation error.
3) Our methodology leads to an interpretation of the limit of convergence and hence new intuition on temporaldifference learning and the dynamics of weight updating.
4) We reconcile positive and negative results concerning temporal-difference learning by proving a theorem that identifies the importance of on-line sampling.
5) We provide an example demonstrating the possibility of divergence when temporal-difference learning is used in conjunction with a nonlinear function approximator.
At about the same time that this paper was initially submitted, Gurvits [17] independently established convergence with probability one in the context of absorbing Markov chains.

Also, Pineda [18] derived a stable differential equation for the "mean field" of temporal-difference learning, in the case of finite-state absorbing Markov chains. He also suggested a convergence proof based on a weighted maximum norm contraction property, which, however, is not satisfied in the presence of function approximation. (The proof was corrected after the paper became available.)
This paper is organized as follows. In Section II, we provide a precise definition of the algorithm that we will be studying. Sections III­IX deal only with the use of linear function approximators. In Section III, we recast temporal-difference learning in a way that sheds light into its mathematical structure. Section IV contains our main convergence result together with our assumptions. We develop some mathematical machinery in Section V, which captures the fundamental ideas involved in the analysis. Section VI presents a proof of the convergence result, which consists primarily of the technicalities required to integrate the machinery supplied by Section V. Our analysis is valid for general state spaces, subject to certain technical assumptions. In Section VII, we show that these technical assumptions are automatically valid for the case of irreducible aperiodic finite-state Markov chains. In Section VIII, we argue that the class of infinite-state Markov chains that satisfy our assumptions is broad enough to be of practical interest. Section IX contains our converse convergence result, which establishes the importance of on-line sampling. Section X departs from the setting of linear function approximators, presenting a divergent example involving a nonlinear function approximator. Finally, Section XI contains some concluding remarks.

II. DEFINITION OF TEMPORAL-DIFFERENCE LEARNING

In this section, we define precisely the nature of temporal-

difference learning, as applied to approximation of the cost-to-

go function for an infinite-horizon discounted Markov chain.

While the method as well as our subsequent results are

applicable to Markov chains with a fairly general state space,

we restrict our attention to the case where the state space

is countable. This allows us to work with relatively simple

notation; for example, the Markov chain can be defined in

terms of an (infinite) transition probability matrix as opposed

to a transition probability kernel. The extension to the case of

general state spaces requires the translation of the matrix no-

tation into operator notation, but is otherwise straightforward.

We consider an irreducible aperiodic Markov chain whose

states lie in a finite or countably infinite space . By indexing

the states with positive integers, we can view the state space as

the set

, where is possibly infinite. Note that

the positive integers only serve as indexes here. In particular,

each state might actually correspond to some other entity such

as a vector of real numbers describing the state of a physical

system. In such a case, the actual state space would comprise

of a countable subset of a Euclidean space.

The sequence of states visited by the Markov chain is

denoted by

. The dynamics of the

Markov chain are described by a (finite or infinite) transition

probability matrix whose th entry, denoted by , is

676

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

the probability that

given that

. For any pair

, we are given a scalar

that represents the cost

of a transition from to . (Extensions to the case where the

one-stage costs are random is discussed in our conclusions

section.) Finally, we let

be a discount factor.

The cost-to-go function

associated with this

Markov chain is defined by

where is viewed as an is equal to ; that is

matrix whose th column ...

assuming that this expectation is well-defined. It is often

convenient to view as a vector instead of a function (its

dimension is infinite if is infinite).

We consider approximations of

using a function

, which we refer to as a function approximator.

To approximate the cost-to-go function one usually tries to

choose a parameter vector

so as to minimize some

error metric between the functions

and .

Suppose that we observe a sequence of states generated

according to the transition probability matrix and that at

time the parameter vector has been set to some value

. We define the temporal difference corresponding to the

transition from to by

Note that the gradient vector here is given by

and we have

where

is the Jacobian matrix whose th column is equal

to

.

In the case of linear function approximators, a more con-

venient representation of TD( ) is obtained by defining a

sequence of eligibility vectors (of dimension ) by

Then, for

the temporal-difference learning

method updates according to the formula

With this new notation, the TD( ) updates are given by

where is initialized to some arbitrary vector, is a sequence of scalar step sizes, is a parameter in , and the gradient
is the vector of partial derivatives with respect to the components of . Since temporal-difference learning is actually a continuum of algorithms, parameterized by , it is often referred to as TD( ).
In the special case of linear function approximators, the function takes the form

Here,

is the parameter vector and each

is a fixed scalar function defined on the state space .

The functions can be viewed as basis functions (or as

vectors of dimension ), while each

can be viewed

as the associated weight.

It is convenient to define a vector-valued function

by letting

. With this notation,

the approximation can also be written in the form

or

and the eligibility vectors can be updated according to

initialized with

.

In the next few sections, we focus on temporal-difference

learning as used with linear function approximators. Only

in Section X do we return to the more general context of

nonlinear function approximators.

III. UNDERSTANDING TEMPORAL-DIFFERENCE LEARNING
Temporal-difference learning originated in the field of reinforcement learning. A view commonly adopted in the original setting is that the algorithm involves "looking back in time and correcting previous predictions." In this context, the eligibility vector keeps track of how the parameter vector should be adjusted in order to appropriately modify prior predictions when a temporal-difference is observed. In this paper, we take a different view which involves examining the "steady-state" behavior of the algorithm and arguing that this characterizes the long-term evolution of the parameter vector. In the remainder of this section, we introduce this view of TD( ) and provide an overview of the analysis that it leads to, in the context of linear function approximators. Our goal is to convey some intuition about how the algorithm works, and in this spirit we maintain the discussion at an informal level, omitting technical assumptions and other details required to

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

677

formally prove the statements we make. These technicalities will be addressed in subsequent sections, where formal proofs are presented.

A. Inner Product Space Concepts and Notation

We begin by introducing some notation that will make our

discussion here, as well as the analysis later in the paper,

more concise. Let

denote the steady-state

probabilities for the process . We assume that

for

all . We define an

diagonal matrix with diagonal

entries

. It is easy to see that

satisfies the requirements for an inner product. We denote

the norm on the associated inner product space by

and the set of vectors

by

. As we will later prove, lies in

,

and it is in this inner product space that the approximations

evolve. Regarding notation, we will also keep

using

, without a subscript, to denote the Euclidean

norm on finite-dimensional vectors or the Euclidean-induced

norm on finite matrices. (That is, for any matrix , we have

.)

We will assume that each basis function is an element

of

so that

. For any pair

of functions

, we say that is -orthogonal

to (denoted by

) if and only if

. For any

, there exists a unique element

minimizing

over

. This is

referred to as the projection of on

with respect

to

. We define a "projection matrix" (more precisely,

projection operator) that generates such a when applied to

. Assuming that the basis functions

are linearly

independent, the projection matrix is given by

(1)

(Note that

is a

matrix.) For any

,

we then have

Furthermore,

is the unique element of

such that

for all

. In other

words, the difference between and is -orthogonal to the

space spanned by the basis functions.

The projection

is a natural approximation to ,

given the fixed set of basis functions. In particular,

is

the solution to the weighted linear least-squares problem of

minimizing

B. The TD( ) Operator

To streamline our analysis of TD( ) we introduce an oper-

ator that is useful in characterizing the algorithm's dynamics.

This operator, which we will refer to as the TD( ) operator,

is indexed by a parameter

and is denoted by

. It is defined by

for

, and

for

, so that

(under

some technical conditions). The fact that maps

into

will be established in a later section. To

interpret the TD( ) operator in a meaningful manner, note

that for each , the term

is the expected cost to be incurred over transitions plus an

approximation to the remaining cost to be incurred, based on

. This sum is sometimes called the " -stage truncated cost-

to-go." Intuitively, if is an approximation to the cost-to-go

function, the -stage truncated cost-to-go can be viewed as an

improved approximation. Since

is a weighted average

over the -stage truncated cost-to-go values,

can also

be viewed as an improved approximation to . In fact, we

will prove later that is a contraction on

, whose

fixed point is . Hence,

is always closer to than

is, in the sense of the norm

.

C. Dynamics of the Algorithm

To clarify the fundamental structure of TD( ), we construct

a process

. It is easy to see that is a

Markov process. In particular, and are deterministic

functions of and the distribution of only depends on

. Note that at each time , the random vector , together

with the current parameter vector , provides all necessary

information for computing . By defining a function with

where

, we can rewrite the TD( ) algorithm as

with respect to . Note that the error associated with each state is weighed by the frequency with which the state is visited. (If the state space were continuous instead of countable, this sum would be replaced by an integral.)

As we will show later, for any ,

has a well-defined

"steady-state" expectation, which we denote by

.

Intuitively, once reaches steady state, the TD( ) algorithm,

in an "average" sense, behaves like the following deterministic

algorithm:

678

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

Under some technical assumptions, the convergence of this deterministic algorithm implies convergence of TD( ), and both algorithms share the same limit of convergence. Our study centers on an analysis of this deterministic algorithm.
It turns out that
and thus the deterministic algorithm takes the form

with respect to . If step sizes are appropriately chosen,

will converge to

, where is the projection matrix

with respect to the inner product

. On the other hand, if

we replace with in the TD( ) algorithm for

, the

algorithm might not converge at all! We will formally illustrate

this phenomenon in Section IX.

To get a better grasp on the issues involved here, let us

consider the following variant of the algorithm:

(3)

As a side note, observe that the execution of this deterministic algorithm would require knowledge of transition probabilities and the transition costs between all pairs of states, and when the state space is large or infinite, this is not feasible. Indeed, stochastic approximation algorithms like TD( ) are motivated by the need to alleviate such stringent information and computational requirements. We introduce the deterministic algorithm solely for conceptual purposes and not as a feasible alternative for practical use.
To gain some additional insight about the evolution of , we rewrite the deterministic algorithm in the form

Note that by letting

, we recover the deterministic

variant of TD( ). Each iteration given by (3) can be thought

of as a steepest descent iteration on an error function given by

(The variable being optimized is , while remains fixed.)

Note that the minimum of this (time-varying) error function

at time is given by

. Hence, letting

,

we might think of

as a "target vector," given a

current vector . We can define an algorithm of the form

(2)

(4)

Note that in the case of

, this becomes

which is a steepest descent iteration for the problem of minimizing

with respect to . It is easy to show that if the step sizes are

appropriately chosen, will converge to .

In the case of

, we can think of each iteration of the

deterministic algorithm as that of a steepest descent method

for minimizing

with respect to , given a fixed . Note, however, that the error

function changes from one time step to the next, and therefore

it is not a true steepest descent method. Nevertheless, if we

think of

as an approximation to , the algorithm

makes some intuitive sense. However, some subtleties are

involved here.

To illustrate this, consider a probability distribution

over the state-space that is different from the steady-state

distribution . Define a diagonal matrix with diagonal

entries

. If we replace the matrix in the

deterministic variant of TD(1) with the matrix , we obtain

which is a steepest descent method that minimizes

which moves directly to the target, given a current vector .

Intuitively, the iteration of (3) can be thought of as an

incremental form of (4). Hence, one might expect the two

algorithms to have similar convergence properties. In fact,

they do. Concerning convergence of the algorithm given by

(4), note that if is a contraction of the norm

, then

the composition

is also a contraction of the norm

, since the projection is a nonexpansion of that norm.

However, there is no reason to believe that the projection

will be a nonexpansion of the norm

if

.

In this case,

may not be a contraction and might

even be an expansion. Hence, convergence guarantees for the

algorithms of (3) and (4) rely on a relationship between

and . This idea captures the issue that arises with variants

of TD( ) that sample states with frequencies independent of

the dynamics of the Markov process. In particular, the state

sampling frequencies are reflected in the matrix , while the

dynamics of the Markov process make

a contraction

with respect to

. When states are sampled on-line, we

have

, while there is no such promise when states are

sampled by an independent mechanism.

For another perspective on TD( ), note that the determinis-

tic variant, as given by (2), can be rewritten in the form

for some matrix and vector . As we will show later, the

contraction property of and the fact that is a projection

with respect to the same norm imply that the matrix is

negative definite. From this fact, it is easy to see that the

iteration converges, given appropriate step-size constraints.

However, it is difficult to draw an intuitive understanding

from the matrix , as we did for the operators

and .

Nevertheless, for simplicity of proof, we use the representation

in terms of and when we establish that TD( ) has the

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

679

properties required for application of the available machinery from stochastic approximation. This machinery is what allows us to deduce convergence of the actual (stochastic) algorithm from that of the deterministic counterpart.
IV. CONVERGENCE RESULT
In this section we present the main result of this paper, which establishes convergence and characterizes the limit of convergence of temporal-difference learning, when linear function approximators are employed. We begin by stating the required assumptions.
The first assumption places constraints on the underlying infinite-horizon discounted Markov chain. Essentially, we assume that the Markov chain is irreducible and aperiodic and that the steady-state variance of transition costs is finite. The formal statement follows.
Assumption 1: 1) The Markov chain is irreducible and aperiodic. Fur-
thermore, there is a unique distribution that satisfies

2) Any

, there exists a constant such that for all

Implicit in the statement of this assumption is that certain expectations are finite. It will be seen later that their finiteness is a consequence of earlier assumptions.
Our final assumption places fairly standard constraints on the sequence of step sizes.
Assumption 4: The step sizes are positive, nonincreasing, and predetermined (chosen prior to execution of the algorithm). Furthermore, they satisfy

and

with

for all ; here, is a finite or infinite

vector, depending on the cardinality of . Let

stand for expectation with respect to this distribution.

2) Transition costs

satisfy

Our second assumption ensures that the basis functions used for approximation are linearly independent and do not grow too fast.
Assumption 2:

1) The matrix has full column rank; that is, the basis

functions

are linearly indepen-

dent.

2) For every , the basis function satisfies

The next assumption essentially requires that the Markov

chain has a certain "degree of stability." As will be shown in Section VI, this assumption is always satisfied when the state-space is finite. It is also satisfied in many situations of practical interest when the set is infinite. Further discussion

can be found in Section VII.

Assumption 3: There exists a function

(the

range is the set of nonnegative reals) satisfying the following

requirements.

1) For all and

and

The main result of this paper follows.

Theorem 1: Under Assumptions 1­4, the following hold.

1) The cost-to-go function is in

.

2) For any

, the TD( ) algorithm with linear func-

tion approximators, as defined in Section II, converges

with probability one.

3) The limit of convergence is the unique solution of

the equation

4) Furthermore, satisfies1

In order to place Theorem 1 in perspective, let us discuss

its relation to available results. If one lets be the th unit

vector for each , and if we assume that is finite, we are

dealing with a lookup table representation of the cost-to-go

function. In that case, we recover a result similar to those in

[5] (actually, that paper dealt with the on-line TD( ) algorithm

only for Markov chains involving a termination state). With

a lookup table representation, the operator

is easily

shown to be a maximum norm contraction, the projection

operator is simply the identity matrix, and general results

on stochastic approximation methods based on maximum

norm contractions [4], [5] become applicable. However, once

function approximation is introduced, the composition

need not be a maximum norm contraction, and this approach

does not extend.

Closer to our results is the work of Dayan [8] who con-

sidered TD( ) for the case of linear function approximators

and established a weak form of convergence (convergence

1 It has been brought to our attention by V. Papavassiliou that this bound

can be improved to

k8r3 0 J3kD 

(1

0

1 0  )(1 + 

0

2)

k5J3

0

J

3kD:

680

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

in the mean). Finally, the work of Dayan and Sejnowski [6] contains a sketch of a proof of convergence with probability one. However, it is restricted to the case where the vectors
are linearly independent, which is essentially equivalent to having a lookup table representation. (A more formal proof, for this restricted case, has been developed in [7].) Some of the ideas in our method of proof originate in the work of Sutton [2] and Dayan [8]. Our analysis also leads to an interpretation of the limit of convergence. In particular, Theorem 1 offers an illuminating fixed-point equation, as well as a graceful bound on the approximation error. Previous works lack interpretable results of this kind.

, and

Proof: If the Markov chain starts in steady state, it remains in steady state, and therefore

where we are using the Tonelli­Fubini theorem to interchange

the expectation and the summation, as well as Assumption

1-2). Since

, it follows that

V. PRELIMINARIES

In this section we present a series of lemmas that provide

the essential ideas behind Theorem 1. Lemma 1 states a

general property of Markov chains that is central to the

analysis of TD( ). Lemma 2 ensures that our assumptions

are sufficient to have a well-defined cost-to-go function

in

. Lemmas 3­6 deal with properties of the TD( )

operator and the composition

, as well as their fixed

points. Lemma 7 characterizes the steady-state expectations

of various variables involved in the dynamics of TD( ), and

these results are used in the proof of Lemma 8, which deals

with the steady-state dynamics. Lemma 9 establishes that these

dynamics lead to convergence of the deterministic version

of the algorithm. Finally, we state a theorem concerning

stochastic approximation that will be used in Section VI, along

with the lemmas, to establish convergence of the stochastic

algorithm.

We begin with the fundamental lemma on Markov chains.

Lemma 1: Under Assumption 1-1), for any

,

we have

.

Proof: The proof involves Jensen's inequality, the

Tonelli­Fubini theorem, and the property

Since

for all , the expectation defining

is well

defined and finite.

Using the Tonelli­Fubini theorem to switch the order of

expectation and summation in the definition of , we obtain

and it follows that

To show that is in

, we have

where the second inequality follows from Lemma 1. Note that we have

Our first use of Lemma 1 will be in showing that is in

. In particular, we have the following result, where

we use the notation to denote the vector of dimension

whose th component is equal to

.

Lemma 2: Under Assumptions 1-1) and 2),

is well

defined and finite for every

. Furthermore, is in by Assumption 1-2). It follows that is in

.

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

681

The next lemma states that the operator

maps

into itself and provides a formula for evaluating

.

Lemma 3: Under Assumption 1, for any

and

,

is in

, and for

, we

have

The next lemma states that is the unique fixed point of

.

Lemma 5: Under Assumption 1, for any

, the

cost-to-go function uniquely solves the system of equations

given by

Proof: We have

Proof: For the case of

, the result follows directly

from the definition of . For

, the fact that is a

fixed point follows from Lemmas 2 and 3, the Tonelli­Fubini

theorem, and some simple algebra:

and the formula in the statement of the lemma follows.

We have shown in Lemma 2 that

. Thus, for

, we can use Lemma 1 to obtain

Similarly

for any

, by Lemma 1. This completes the proof.

Lemma 1 can also be used to show that is a contraction

on

. This fact, which is captured by the next lemma,

will be useful for establishing error bounds.

Lemma 4: Under Assumption 1-1), for any

and

, we have

Proof: The case of

is trivial. For

, the result

follows from Lemmas 1 and 3. In particular, we have

The contraction property (Lemma 4) implies that the fixed

point is unique.

The next lemma characterizes the fixed point of the com-

position

. This fixed point must lie in the range of

, which is the space

[note that this is a

subspace of

, because of Assumption 2-2)]. The

lemma establishes existence and uniqueness of this fixed point,

which we will denote by . Note that in the special case

of

the lemma implies that

, in agreement

with the definition of .

Lemma 6: Under Assumptions 1 and 2,

is a

contraction and has a unique fixed point which is of the form

for a unique choice of . Furthermore, satisfies the

following bound:

Proof: Lemma 4 ensures that into itself, and from
Lemma 5. Note that for nian­Pythagorean theorem we have

is a contraction from is its fixed point by
, by the Babylo-

since

. It follows that is nonexpansive,

and thus the composition

is a contraction. Hence,

has a unique fixed point of the form , for some

. Because the functions

are assumed to be linearly

independent, it follows that the choice of is unique.

Using the fact that is in

(Lemma 2) and is

the fixed point of

(Lemma 5), we establish the desired

682
bound. In particular, we have

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

Proof: We first observe that for any

,

we have

and it follows that

(Note that

, by Lemma 1, and using the

Cauchy­Schwartz inequality,

is finite.) By special-

izing to the case where we are dealing with vectors of the

form

and

(these vectors are in

as

a consequence of Assumption 2), we obtain

We next set out to characterize the expected behavior of

the steps taken by the TD( ) algorithm in "steady state."

In particular, we will get a handle on

for any

given . While this expression can be viewed as a limit of

as goes to infinity, it is simpler to view it as

an expectation referring to a process that is already in steady

state. We therefore make a short digression to construct a

stationary process .

We proceed as follows. Let be a Markov chain that

evolves according to the transition probability matrix and

is in steady state, in the sense that

for

all and all . Given any sample path of this Markov chain,

we define

Since the vectors and are arbitrary, it follows that
We place a bound on the Euclidean-induced matrix norm as follows. We have

(5)

Note that is constructed by taking the stationary process

, whose variance is finite (Assumption 2), and passing it

through an exponentially stable linear time invariant system.

It is then well known that the output of this filter is finite

with probability one and has also finite variance. With so

constructed, we let

and note that this is

a Markov process with the same transition probabilities as

the Markov process that was constructed in the middle

of Section III (the evolution equation is the same). The only

difference is that the process of Section III was initialized

with

, whereas here we have a stationary process .

We can now identify

with the expectation with respect

to this invariant distribution.

Prior to studying

, let us establish a few pre-

liminary relations in the next lemma.

Lemma 7: Under Assumptions 1 and 2, the following re-

lations hold.

1)

, for

2) There exists a finite constant

, for all

3)

4)

5)

. such that .

Furthermore, each of the above expressions is well defined and finite.

which is a finite constant , by Assumption 2-2). We have

used here the notation to indicate the th column of

the matrix , with entries

. Note that the

second inequality above follows from the Cauchy­Schwartz

inequality.

We have so far verified parts 1) and 2) of the lemma. We

now begin with the analysis for part 3). Note that

is the same for all , and it suffices to prove the result for the

case

. We have

where the interchange of summation and expectation is justi-

fied by the dominated convergence theorem. The desired result

follows by using the result of part 1).

The results of parts 4) and 5) are proved by entirely similar

arguments, which we omit.

With the previous lemma at hand, we are ready to charac-

terize

. This is done in the following lemma.

Lemma 8: Under Assumptions 1 and 2, we have

which is well defined and finite for any finite .

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

683

Proof: By applying Lemma 7, we have

For

, it follows that

[19] follow from the assumptions imposed in the result below.

We do not show here the assumptions of [19] because the list

is long and would require a lot in terms of new notation.

However, we note that in our setting here, the potential

function that would be required to satisfy the assumptions

of the theorem from [19] is given by

.

Theorem 2: Consider an iterative algorithm of the form

Note that for

and any

Hence, for

, we have

, we have

by Lemma 3. Each expression is finite and well defined by Lemma 7.
The next lemma shows that the steps taken by TD( ) tend to move toward .
Lemma 9: Under Assumptions 1 and 2, we have

where

1) the (predetermined) step-size sequence is positive,

nonincreasing, and satisfies

and

;

2) is a Markov process with a unique invariant distri-

bution, and there exists a mapping from the states of

the Markov process to the positive reals, satisfying the

remaining conditions. Let

stand for expectation

with respect to this invariant distribution;

3)

and are matrix and vector valued functions,

respectively, for which

and

are well defined and finite;

4) the matrix is negative definite;

5) there exist constants and such that for all

and

Proof: We have

6) for any all

there exists a constant such that for

Then, converges to , with probability one, where is

where the last equality follows because

[see the unique vector that satisfies

.

(1)]. As shown in the beginning of the proof of Lemma

5,

is a contraction with fixed point

contraction factor is no larger than . Hence

, and the

The step

VI. PROOF OF THEOREM 1 involved in the update of is

and using the Cauchy­Schwartz inequality, we obtain

Hence,

takes the form

where

Since

, the result follows.

We now state without proof a result concerning stochastic

approximation which will be used in the proof of Theorem 1.

This is a special case of a very general result on stochastic

approximation algorithms [19, Th. 17, p. 239]. It is straight-

forward to check that all of the assumptions in the result of

and

By Lemma 7,

and

well defined and finite.

By Lemma 6, we have

also have

. Hence,

are both
. From (1), we .

684
We now compare with the formula for by Lemma 8, and conclude that

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

, as given for some constants and and any . Hence

. It follows that

It follows from Lemma 9 that

for some constants and . Next, we deal with the second summation. Letting
be defined by

for any

, and thus is negative definite.

We will use Theorem 2 to show that converges. Our

analysis thus far ensures validity of all conditions except for

5) and 6). We now show that Assumption 3 is sufficient to

ensure validity of these two conditions.

We begin by bounding the summations involved in 5).

Letting

, recall that

we have

Let us concentrate on the term , we have

. Using the formula for

for some constant , where the inequality follows from

Assumption 3-1).

Finally, recalling that

, for some

absolute constant (Lemma 7), we have

Using the triangle inequality, we obtain

Given these bounds, it follows that there exist positive constants and such that

We will individually bound the magnitude of each summation in the right-hand side.
First we have

In other words, the summation above is bounded by a polyno-

mial function of , , and . An identical argument

can be carried out for the terms

and

,

which we omit to avoid repetition. Using these arguments, we

can place bounds that are polynomial in , , and ,

on the summations in Condition 5) of Theorem 2. We can thus

satisfy the condition with a function

(

)

that is polynomial in , , and . The fact that such

a function would satisfy Condition 6) then follows from

Assumption 3-2).

We now have all the conditions needed to apply Theorem 2.

It follows that converges to , which solves

.

Since

, Lemma 8 implies that

where the second inequality follows from the fact that . Assumption 3-1) implies that

By Lemma 6 along with the fact that

has full row

rank [by virtue of Assumption 2-1)], uniquely satisfies this

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

685

equation and is the unique fixed point of

. Lemma 6 also

provides the desired error bound. This completes the proof to

Theorem 1.

VII. THE CASE OF A FINITE STATE SPACE

In this section, we show that Assumptions 1-2), 2-2), and

3 are automatically true whenever we are dealing with an

irreducible aperiodic Markov chain with a finite state space.

This tremendously simplifies the conditions required to apply

Theorem 1, reducing them to a requirement that the basis

functions be linearly independent [Assumption 2a)]. Actually,

even this assumption can be relaxed if Theorem 1 is stated

in a more general way. This assumption was adopted for the

sake of simplicity in the proof.

Let us now assume that is an irreducible aperiodic finite-

state Markov chain [Assumption 1-1)]. Assumptions 1-2) and

2-2) are trivially satisfied when the state space is finite. We

therefore only need to prove that Assumption 3 is satisfied.

It is well known that for any irreducible aperiodic finite-state

Markov chain, there exist scalars

and such that

Let us fix . We define a sequence of

diagonal matrices

with the th diagonal element equal to

. Note

that

VIII. INFINITE STATE SPACES

The purpose of this section is to shed some light on the

nature of our assumptions and to suggest that our results

apply to infinite-state Markov chains of practical interest. For

concreteness, let us assume that the state space is a countable

subset of . Each state

is associated with an integer

index

and denoted by .

Let us first assume that the state space is a bounded subset of

and that the mappings defined by

and

are continuous functions on

and

. Then, Assumptions 1-2) and 2-2) are automatically valid

because continuous functions are bounded on bounded sets.

Assumption 3-1) basically refers to the speed with which the

Markov chain reaches steady state. Let

be a diagonal

matrix whose th entry is

. Then Assumption 3-

3) is satisfied by a function

if we impose a condition

of the form

for some finite constant . In other words, we want the -step

transition probabilities to converge fast enough to the steady-

state probabilities (for example,

could drop at the

rate of ). In addition, we need this convergence to be

uniform in the initial state.

As a special case, suppose that the Markov chain has a

distinguished state, say state zero, and that for some

It is then easy to show that

the proof being essentially the same as in Lemma 7-1). We then have

Note that all entries of are bounded by one, and therefore

there exists a constant such that

for all . We

then have

The first part of Assumption 3-1) is thus satisfied by a function that is equal to a constant for all . An analogous
argument, which we omit, can be used to establish that the same is true for the second part of Assumption 3-1). Assumption 3-2) follows from the fact that is constant.

Then,

converges to exponentially fast, and uniformly

in , and Assumption 3-1) is satisfied with

. Validity

of Assumption 3-2) easily follows.

Let us now consider the case where the state space is an

unbounded subset of . For many stochastic processes of

practical interest (e.g., those that satisfy a large deviations

principle), the tails of the probability distribution

exhibit exponential decay; let us assume that this is the case.

For the purposes of Assumption 3, it is natural in this context

to employ a function

, for some and

. Assumption 3-2) is essentially a stability condition; given

our definition of , it states that

is not expected to

grow too rapidly, and this is satisfied by most stable Markov

chains of practical interest. Note that by taking the steady-state

limit we obtain

for all , which in essence

says that the tails of the steady-state distribution decay

faster than any polynomial (e.g., exponentially).

Assumption 3-1) is the most complex one. Recall that it

deals with the speed of convergence of certain functions of

the Markov chain to steady state. Whether it is satisfied has

to do with the interplay between the speed of convergence of

to and the growth rate of the functions

and

. Note that the assumption allows the rate of convergence

to get worse as

increases; this is captured by the term

in the right-hand side.

We close with a concrete illustration, related to queueing

theory. Let be a Markov chain that takes values in the

686

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

nonnegative integers, and let its dynamics be

where the are independent, identically distributed nonneg-

ative integer random variables with a "nice" distribution; e.g.,

assume that the tail of the distribution of asymptotically

decays at an exponential rate. (This Markov chain corresponds

to an M/G/1 queue which is observed at service completion

times, with being the number of new arrivals while serving

a customer.) Assuming that

, this chain has a

"downward drift," is "stable," and has a unique invariant

distribution [20]. Furthermore, there exists some

such

that

, for sufficiently large. Let

so

that the cost function basically counts the number of customers

in queue. Let us introduce the basis functions

,

. Then, Assumptions 1 and 2 are satisfied.

Assumption 3-2) can be shown to be true for functions of the

form

by exploiting the downward drift

property (in this example, it is natural to simply let

).

Let us now discuss Assumption 3-1). The key is again

the speed of convergence of

to . Starting from

state , with large, the Markov chain has a negative drift

and requires

steps to enter (with high probability) the

vicinity of state zero [21], [22]. Once the vicinity of state zero

is reached, it quickly reaches steady state. Thus, if we con-

centrate on

, the difference

is of the order of for

time

steps and afterwards decays at a fast rate. This suggests

that Assumption 3-1) is satisfied by a function that grows

polynomially with

.

Our discussion in the preceding example was far from

rigorous. Our objective was not so much to prove that our

assumptions are satisfied by specific examples, but rather

to demonstrate that their content is plausible. Furthermore,

while the M/G/1 queue is too simple an example, we ex-

pect that stable queueing networks that have a downward

drifting Lyapunov function should also generically satisfy our

assumptions.

IX. THE IMPORTANCE OF ON-LINE SAMPLING
In the introduction, we claimed that on-line sampling plays an instrumental role in ensuring convergence of TD( ). In particular, when working with a simulation model, it is possible to define variants of TD( ) that do not sample states with the frequencies natural to the Markov chain and, as a result, do not generally converge. Many papers, including [10], [11], [13], and [14], present such examples as counterexamples to TD( ). In this section, we provide some insight into this issue by exploring the behavior of a variant of TD(0). More generally, variants of TD( ) can be defined in a similar manner, and the same issues arise in that context. We limit our discussion to TD(0) for ease of exposition.
We consider a variant of TD(0) where states are sampled independently from a distribution over , and successor states are generated by sampling according to
. Each iteration of the algorithm takes the form

Let us refer to this algorithm as -sampled TD(0). Note that

this algorithm is closely related to the original TD(0) algorithm

as defined in Section II. In particular, if is generated by the

Markov chain and

, we are back to the original

algorithm. It is easy to show, using a subset of the arguments

required to prove Theorem 1, that this algorithm converges

when

for all , and Assumptions 1, 2, and 4 are

satisfied. However, results can be very different when is

arbitrary. This is captured by the following Theorem.

Theorem 3: Let be a probability distribution over a

countable set with at least two elements. Let the discount

factor be constrained to the open interval

. Let

the sequence satisfy Assumption 4. Then, there exists a

stochastic matrix , a transition cost function

, and a

matrix , such that Assumptions 1 and 2 are satisfied, and

execution of the -sampled

algorithm leads to

for some unique vector .

Proof: Without loss of generality, we will assume

throughout this proof that

and

.

We define a probability distribution

satisfying

and

for all . The fact that

ensures that such a probability distribution exists. We define

the transition probability matrix with each row equal to

. In other words, we have

... . . . ...

Finally, we define the transition cost function to be

for all and . Assumption 1 is trivially satisfied by our choice

of and

, and the invariant distribution of the Markov

chain is . Note that

, since no transition incurs any

cost.

Let be an matrix, defined by a single scalar function

with

if if otherwise.

Note that, implicit from our definition of , is scalar, and

Assumption 2 is trivially satisfied. We let

so that

.

In general, we can express

in terms of a recurrence

of the form

where

is the diagonal matrix with diagonal elements .

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

687

Specializing to our choice of parameters, the recurrence becomes

For shorthand notation, let be defined by

Since

and

, we have

and since

, there exists some

such that

It follows that and since

, we have

= Fig. 1. Example of divergence with a nonlinear function approximator. The
plot is of points in the plane fJ 2 <3je0J 0g.

, where

, requiring that

unique solution to the linear differential equation

be the (6)

if

.

where is the

identity matrix, is a small positive

constant, and is given by

X. DIVERGENCE WITH A NONLINEAR APPROXIMATOR

Our analysis of temporal-difference learning up until now

has focused on linear function approximators. In many situa-

tions, it may be natural to employ classes of nonlinear function

approximators. Neural networks present one popular example.

One might hope that the analysis we have provided for the

linear case generalizes to nonlinear parameterizations, perhaps

under some simple regularity conditions. Unfortunately, this

does not seem to be the case. To illustrate potential difficulties,

we present an example for which TD(0) diverges due to the

structure of a nonlinear function approximator. (By divergence

here, we mean divergence of both the approximate cost-to-

go function and the parameters.) For the sake of brevity, we

limit our study to a characterization of steady-state dynamics,

rather than presenting a rigorous proof, which would require

arguments formally relating the steady-state dynamics to the

actual stochastic algorithm.

We consider a Markov chain with three states (

), all transition costs equal to zero, and a discount

factor

. The cost-to-go function

is therefore

given by

. Let the function approximator

be parameterized by a single scalar . Let the form of be

defined by letting

be some nonzero vector satisfying

Given our definition of , it is easy to show that all functions

representable by lie on the plane

.

Furthermore, the set of functions

forms a spiral

that diverges as grows to infinity (see Fig. 1).

We let the transition probability matrix of the Markov chain

be

Since all transition costs are zero, the TD(0) operator is given

by

, for all

. It turns out that there is

an acute angle and a scalar

such that for any ,

is equal to the vector scaled by and rotated by

degrees in the plane

. The points labeled

and

in Fig. 1 illustrate the nature of this transformation.

Before discussing divergence of TD(0), let us motivate the

underlying intuition by observing the qualitative behavior of

a simpler algorithm. In particular, suppose we generated a

sequence of approximations , where each satisfies

688

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

(Note that the steady-state distribution is uniform so that the

Euclidean norm is the appropriate one for this context.) In

Fig. 1, the point on the spiral closest to

is further from

the origin than , even though

is closer to the origin

than (the origin is located at the center of the circle in the

diagram). Therefore, if

, then

.

Furthermore, since each application of induces the same

degree of rotation and scaling, we might expect that each

subsequent iteration takes the approximation further from the

origin in a completely analogous way. Hence, the underlying

dynamics suggest that divergence is conceivable.

Let us now more concretely identify divergent behavior

in the steady-state dynamics of TD(0). The TD(0) algorithm

applies the update equation

where is the state visited by the trajectory at time . Since the steady-state distribution resulting from is uniform, the steady-state expectation of the update direction, within a factor of three, is given by

This is the inner product of the vector

, which

is

, with the vector with components

, which is the vector

.

As the step size becomes extremely small, we can think of

the deterministic version of the algorithm as an approximation

to a differential equation. Given the average direction of

motion of the parameter , the appropriate differential equation

for our example is

sufficiently small. The combination of this inequality and the fact that

implies that both and

diverge to infinity.

XI. CONCLUSIONS

We have established the convergence of on-line temporal-

difference learning with linear function approximators when

applied to irreducible aperiodic Markov chains. We note

that this result is new even for the case of lookup table

representations (i.e., when there is no function approximation),

but its scope is much greater. Furthermore, in addition to

covering the case where the underlying Markov chain is

finite, the result also applies to Markov chains over a general

(infinite) state space, as long as certain technical conditions

are satisfied.

The key to our development was the introduction of the

norm

and the property

. Furthermore, our

development indicates that the progress of the algorithm can

be monitored in two different ways: 1) we can keep track of

the magnitude of the approximation error

; the natural

norm for doing so is

, or 2) we can keep track of the

parameter error

; the natural norm here is the Euclidean

norm, as made clear by our convergence proof.

To reinforce the central ideas in the proof, let us revisit

the TD(0) method, for the case where the costs per stage

are identically zero. In this case,

is simply . The

deterministic counterpart of the algorithm, as introduced in

Section III, takes the form

For any vector , we have

For

, we have

where the first equality follows from the fact that , for any . Note that

This shows that the matrix

is negative definite,

hence

is also negative definite and convergence

of this deterministic iteration follows.

Besides convergence, we have also provided bounds on the

distance of the limiting function from the true cost-to-

go function . These bounds involve the expression

, which is natural because no approximation could have

error smaller than this expression (when the error is measured

in terms of

). What is interesting is the factor of

which is easily verified to be positive definite. Hence, there exists a positive constant such that
(7)
By a continuity argument, this inequality remains true (possibly with a smaller positive constant ) if is positive but

This expression is one when

. For every

, it

is larger than one, and the bound actually deteriorates as

decreases. The worst bound, namely

is obtained when

. Although this is only a bound, it

strongly suggests that higher values of are likely to produce

more accurate approximations of . This is consistent with

the examples that have been constructed by Bertsekas [23].

TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING

689

The sensitivity of the error bound to raises the question

of whether or not it ever makes sense to set to values less

than one. Experimental results [2], [24], and [25] suggest that

setting to values less than one can often lead to significant

gains in the rate of convergence. Such acceleration may be

critical when computation time and/or data (in the event

that the trajectories are generated by a physical system) are

limited. A full understanding of how influences the rate

of convergence is yet to be found. Furthermore, it might

be desirable to tune as the algorithm progresses, possibly

initially starting with

and approaching

(although

the opposite has also been advocated). These are interesting

directions for future research.

In many applications of temporal-difference learning, one

deals with a controlled Markov chain and at each stage

a decision is "greedily" chosen, by minimizing the right-

hand side of Bellman's equation and using the available

approximation in place of . Our analysis does not apply

to such cases involving changing policies. Of course, if the

policy eventually settles into a limiting policy, we are back to

the case studied in this paper and convergence is obtained.

However, there exist examples for which the policy does

not converge [16]. It remains an open problem to analyze

the limiting behavior of the parameters and the resulting

approximations for the case where the policy does not

converge.

On the technical side, we mention a few straightforward

extensions of our results. First, the linear independence of the

basis functions is not essential. In the linearly dependent

case, some components of and become linear combina-

tions of the other components and can be simply eliminated,

which takes us back to the linearly independent case. A second

extension is to allow the cost per stage

to be

noisy, as opposed to being a deterministic function of

and . In particular, we can replace the Markov process

that was constructed for the purposes of

our analysis with a process

, where

is the cost associated with the transition from to . Then,

as long as the distribution of the noise only depends on the

current state and its moments are such that the assumptions of

Theorem 2 are still satisfied, our proof can easily be modified

to accommodate this situation. Finally, the assumption that

the Markov chain was aperiodic can be alleviated. No part of

our convergence proof truly required this assumption--it was

introduced merely to simplify the exposition.

Our results in Section IX have elucidated the importance

of sampling states according to the steady-state distribution of

the Markov chain under consideration. In particular, variants

of TD( ) that sample states otherwise can lead to divergence

when function approximators are employed. As a parting

note, we point out that a related issue arises when one

"plays" with the evolution equation for the eligibility vector

. (For example Singh and Sutton [24] have suggested an

alternative evolution equation for known as the "replace

trace.") A very general class of such mechanisms can be

shown to lead to convergent algorithms for the case of lookup

table representations [16]. However, different mechanisms for

adjusting the coefficients lead to a change in the steady-

state average value of

, affect the matrix , and the

negative definiteness property can be easily lost.

Finally, the example of Section X identifies the possibil-

ity of divergence when TD( ) is used in conjunction with

nonlinear function approximators. However, the example is

somewhat contrived, and it is unclear whether divergence can

occur with special classes of function approximators, such

as neural networks. This presents an interesting question for

future research.

ACKNOWLEDGMENT
The authors would like to thank R. S. Sutton for starting
them on the path that led to this work by pointing out that the
counterexample in [10] would no longer be a counterexample
if on-line state sampling was used. They also thank him for
suggesting an algebraic simplification to the original expres-
sion for the error bound in Theorem 1, which resulted in its
current form. The authors would like to thank the reviewers
for their feedback, especially the one who provided them with
four pages of detailed corrections and useful comments.
REFERENCES
[1] D. P. Bertsekas, Dynamic Programming and Optimal Control. Belmont, MA: Athena Scientific, 1995.
[2] R. S. Sutton, "Learning to predict by the methods of temporal differences," Mach. Learning, vol. 3, pp. 9­44, 1988.
[3] C. J. C. H. Watkins and P. Dayan, "Q-learning," Mach. Learning, vol.
8, pp. 279­292, 1992.
[4] J. N. Tsitsiklis, "Asynchronous stochastic approximation and Q-
learning," Mach. Learning, vol. 16, pp. 185­202, 1994. [5] T. Jaakkola, M. I. Jordan, and S. P. Singh, "On the convergence of
stochastic iterative dynamic programming algorithms," Neural Comp., vol. 6, no. 6, pp. 1185­1201, 1994.
[6] P. D. Dayan and T. J. Sejnowski, "TD() converges with probability
1," Mach. Learning, vol. 14, pp. 295­301, 1994. [7] L. Gurvits, L. J. Lin, and S. J. Hanson, "Incremental learning of
evaluation functions for absorbing Markov chains: New methods and theorems," 1994, preprint.
[8] P. D. Dayan, "The convergence of TD() for general ," Mach.
Learning, vol. 8, pp. 341­362, 1992. [9] R. E. Schapire and M. K. Warmuth, "On the worst-case analysis of
temporal-difference learning algorithms," Mach. Learning, vol. 22, pp. 95­122, 1996. [10] J. N. Tsitsiklis and B. Van Roy, "Feature-based methods for large scale dynamic programming," Mach. Learning, vol. 22, pp. 59­94, 1996. [11] G. J. Gordon, "Stable function approximation in dynamic programming," Carnegie Mellon Univ., Tech. Rep. CMU-CS-95-103, 1995. [12] S. P. Singh, T. Jaakkola, and M. I. Jordan, "Reinforcement learning with soft state aggregation," in Advances in Neural Information Processing Systems, vol. 7, G. Tesauro, D. S. Touretzky, and T. K. Leen, Eds. Cambridge, MA: MIT Press, 1995. [13] L. C. Baird, "Residual algorithms: Reinforcement learning with function approximation," in Machine Learning: Proceedings 12th Int. Conf., July 9­12, Prieditis and Russell, Eds. San Francisco, CA: Morgan Kaufman, 1995. [14] J. A. Boyan and A. W. Moore, "Generalization in reinforcement learning: Safely approximating the value function," in Advances in Neural Information Processing Systems, vol. 7. MIT Press, 1995. [15] R. S. Sutton, "On the virtues of linear learning and trajectory distributions," in Proc. Wkshp. Value Function Approximation, Mach. Learning Conf., Carnegie Mellon Univ., Tech. Rep. CMU-CS-95-206, 1995. [16] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Belmont, MA: Athena Scientific, 1996. [17] L. Gurvits, 1996, private communication.
[18] F. Pineda, "Mean-field analysis for batched TD()," 1996, preprint.
[19] A. Benveniste, M. Me´tivier, and P. Prioret, Adaptive Algorithms and Stochastic Approximations. Berlin: Springer-Verlag, 1990.
[20] J. Walrand, An Introduction to Queueing Networks. Englewood Cliffs, NJ: Prentice Hall, 1988.

690

IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997

[21] G. D. Stamoulis and J. N. Tsitsiklis, "On the settling time of the congested GI/G/1 queue," Adv. Appl. Probability, vol. 22, pp. 929­956, 1990.
[22] P. Konstantopoulos and F. Baccelli, "On the cut-off phenomenon in some queueing systems," J. Appl. Probability, vol. 28, pp. 683­694, 1991.
[23] D. P. Bertsekas, "A counterexample to temporal-difference learning," Neural Comp., vol. 7, pp. 270­279, 1994.
[24] S. P. Singh and R. S. Sutton, "Reinforcement learning with replacing eligibility traces," Mach. Learning, vol. 22, pp. 123­158, 1996.
[25] R. S. Sutton, "Generalization in reinforcement learning: Successful examples using sparse coarse coding," in Advances in Neural Information Processing Systems, vol. 8, D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, Eds. Cambridge, MA: MIT Press, 1996.

Benjamin Van Roy was born in Bangkok, Thailand, in 1971. He received the B.S. degree in computer science in 1993 and the M.S. degree in electrical engineering and computer science in 1995, both from the Massachusetts Institute of Technology, Cambridge, MA, where he is currently a Ph.D. candidate.
He is a coauthor of Solving Pattern Recognition Problems (1995), and he consults regularly with private industry.
Mr. Van Roy has been a recipient of a Digital Equipment Corporation Scholarship, the MIT George C. Newton Award for "the best undergraduate electrical engineering laboratory project," and the MIT Morris J. Levin Memorial Award for "an outstanding Master's thesis presentation."

John N. Tsitiklis (S'81­M'83) was born in Thessaloniki, Greece, in 1958. He received the B.S. degree in mathematics in 1980 and the B.S., M.S., and Ph.D. degrees in electrical engineering, all from the Massachusetts Institute of Technology, Cambridge, MA, in 1980, 1981, and 1984, respectively.
During the academic year 1983­1984, he was an Acting Assistant Professor of Electrical Engineering at Stanford University, Stanford, CA. Since 1984, he has been with the Massachusetts Institute of Technology, where he is currently Professor of Electrical Engineering. His research interests include systems and control theory, neural networks, and operations research. He has written more than 70 journal papers on these subjects and is a coauthor of Parallel and Distributed Computation: Numerical Methods (1989), Neuro-Dynamic Programming (1996), and Introduction to Linear Optimization (1997). Dr. Tsitiklis has been a recipient of an IBM Faculty Development Award (1983), an NSF Presidential Young Investigator Award (1986), an Outstanding Paper Award by the IEEE Control Systems Society, the MIT Edgerton Faculty Achievement Award (1989), and the Bodossakis Foundation Prize (1994). He was a plenary speaker at the 1992 IEEE Conference on Decision and Control. He is an associate editor of Applied Mathematics Letters and has been an associate editor of the IEEE TRANSACTIONS ON AUTOMATIC CONTROL and Automatica.


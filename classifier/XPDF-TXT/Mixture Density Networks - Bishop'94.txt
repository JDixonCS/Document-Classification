Mixture Density Networks
Christopher M. Bishop
Neural Computing Research Group Dept. of Computer Science and Applied Mathematics
Aston University Birmingham. B4 7ET, U.K.
C.M.Bishop@aston.ac.uk
February, 1994 Neural Computing Research Group Report: NCRG/94/0041
Available from: http://www.ncrg.aston.ac.uk/
Abstract
Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classi cations problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the e ectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.
1Previously issued as NCRG/94/4288

1 Introduction

Ntwuosheiuniacrghaslaettnheeontfawitiomoerukstiepsmtutootofdlveeealaxsrrainaamrbaeplelwtsersiadt,newslfyhoriucfmshtea1d;wt:ieio:nn:d;aeftprncopgom.ltiecaaIbtnsiyoepntfrsxoafiqcn;tivnticoqpeglu,v,tisnwvughacrehairsaesnboeqlcetiswa=toxirv1ke;s:mf:ax:ar;1epn;p:,ti:rna:agn;insxdedindgq
labels the particular training pattern. The central goal in network training is not to
mtsroaeimtnheoadrtizntehettewheboretksrtaiispnoisnsugsbibsdelaeqtuape,rnebtdulyitctprioarentshseenrfotetrodtmhweiotdhoeulatpthnueetwuvnevcdatelorurleyitfnogcragnxe.nbeeTrahmtoearmdoeof swtthhgeeendnaettrhaae,l ssafaopmnnraddactlechcl.aoernmeTdghpbaioiletsenatede(xpepodnrixesneis;tstcysrteiftopdu)tnliiiinocsentgtisoieoovnrmfemnstepshwbeeoyhcfgiepter(henexseei;rtnptah)rtatoohtbrextahobifenitlptpi,hturayeotn-bddtdaaeabrnitsgiasleinitttioysyrsamoppf(asaxcatl;eaidzt:tea)isRdtitanpitco(patxohlg;ieinotvnt)jeoed(uixxn(n;tWdittti)nhp=fpiartuolel1bt,i-.na1tgba9Nir8ilgon9itet)yaet,
that, if the generator of the data itself evolves with time, then we must consider time as an additional variable in the joint probability density. In this paper we shall limit our attention to static distributions, although the models which we shall introduce can be extended to non-stationary problems, provided the models are treated as continuously adaptive.

For associative mapping problems of the kind we are considering, it is convenient to

decompose the joint probability density into the product of the conditional density of the

target data, conditioned on the input data, and the unconditional density of input data

p(x; t) = p(t j x)p(x)

(1)

wThheeredepn(stitjyxp)(dxe)n=oteRs

pt(hxe;

tp)rodbtaobfiliintypudtendsaittay

of t given that x takes a
plays an important role

particular value. in validating the

predictions predictions

of of

ttrfaoirnendewnevtawluoersksof(Bxi,sihtoips,

t1h9e94co).ndHitioowneavledre, nfsoirtythpe(tpjuxrp)owsehsicohf

making we need

to model.

As we shall see in the next section, the conventional neural network technique of minimizing a sum-of-squares error leads to network functions which approximate the conditional average of the target data. A similar result holds for the cross-entropy error function. For classi cation problems in which the target variables have a 1-of-N coding scheme, these conditional averages represent the posterior probabilities of class membership, and so can be regarded as providing an optimal solution. For problems involving the prediction of continuous variables, however, the conditional average represents a very limited description of the statistical properties of the target data, and for many applications will be wholly inadequate. Such applications include the important class of inverse problems, for which the target data is frequently multi-valued. In such cases the conventional least-squares approach can give completely erroneous results, as we shall show.

In this paper we introduce a new class of neural network models, called Mixture Density Networks (MDNs), which overcome these limitations and which provide a completely general framework for modelling conditional density functions. These networks, obtained by combining a conventional neural network with a mixture density model, contain the conventional least-squares approach as a special case. Indeed, their implementation in

1

software represents a straightforward modi cation of standard neural network models. We demonstrate the e ectiveness of MDNs using both a toy problem in two variables, and a problem involving robot inverse kinematics.

2 Conventional Least Squares

The usual approach to network training involves the minimization of a sum-of-squares error, de ned over a set of training data, of the form

ES(w)

=

1 2

Xn q=1

Xc k=1

fk(xq; w)

?

tqk]2

(2)

where tk sponding

represent the components of the outputs of the network mapping

ftuanrcgteitonv,ewcthoirc,hainsdpafrka(mx;ewtr)izdeednboyteanthaerrcaoyrrwe-

adaptive parameters (the weights and biases).

We begin by considering the nature of the solutions found by least-squares techniques. In a practical application we must deal with a nite data set. This in turn means that we must limit the complexity of the network model (for instance by limiting the number of hidden units or by introducing regularization terms) in order to control the balance between bias and variance (Geman et al., 1992; Bishop, 1995). The use of successively larger data sets allows the models to be more exible (i.e. have less bias) without overtting the data (i.e. without leading to increased variance). In the limit of an in nite data set both bias and variance can be reduced to zero, and the optimal least squares solution is obtained. In this limit we can replace the nite sum over data points in the sum-of-squares error function (2) by an integral over the joint probability density

ES

=

nl!im1

1 2n

Xn q=1

Xc k=1

fk(xq;

w)

?

tqk

]2

(3)

=

1 Xc Z Z 2 k=1

fk(x; w) ? tk]2 p(t; x) dt dx

(4)

wtmhhienecirmoeriarzenesptehxoentrdeairnrogovrenrfeautlnwlcoftarikcotnmorboyodfeflu1fn=knc(txiho;anwsa)lbideseipneeirrnemtnritotitadetudicotenodwbienittvhoerr(ey3s)peefoxcrtibctlooen,fwvkee(nxci;eawnnc)feo.rmSianlclye

ES
fk(x; w)

=

0

(5)

Substituting (4) into (5), and using (1), we obtain the following expression for the mini-

mizing function

fk(x; w ) = htk j xi

(6)

where tional

awverraegpereosfeantqsutahnetictoyrrQes(pt)onbdying

set

of

weight

values.

We

have

de

ned

the

condi-

hQ j xi Z Q(t) p(t j x) dt

(7)

2

Thus, the network function is given by the conditional average of the target data, conditioned on the input vector. This key result is illustrated in Figure 1, and has many important implications for practical applications of neural networks (Bishop, 1995).
t t|x0
p(t | x0)

x0

x

Figure 1: A schematic illustration of a set of data points (black dots) consisting of values of

the input variable x and corresponding target variable t. Also shown is the optimal least-

squares function (solid curve), which is given by the conditional average of the target data.

Thus, for a given by the average of

vtawluitehorfexsp, escutchtoatshteheprvoablaubeixli0ty,

the least-squares function ht density p(t j x) at the given

j xi is given value of x.

We By we

can also derive this result

adding obtain

and

subtracting

htk

bj yxireinwsriidteintghethsequsaurme-borf-ascqkueatrseisne(r4r)o,rainndaagdaiineruesnitngfo(r1m),.

ES

=

1 Xc Z 2 k=1

fk(x; w) ? htk j xi]2 p(x) dx

(8)

+

1 2

Xc k=1

Z

ht2k j xi ? htk j xi2] p(x) dx

(9)

We note that the error function only depends on the network weights through the rst term (8), whose minimum occurs when (6) is satis ed. Thus, we again see the result that the global minimum of the error function is given by the conditional average of the target data. The second term (9) gives the residual value of the error function at its global minimum, and is seen to correspond to the average variance of the target data around its conditional average value.

Thus, the result of training a standard neural network by least squares is that we have ap-

copofrnotxhdieimtiionanpteaudltatvvweeocrtasogtraext,ig,sitgviicevsnenobfybtythhefekt(raxersg;iwedtud)a,al atvana,ldunetahmoefeatlvhyeetreharegreocrovnafurdiniatcnitocinoeanolfaattvhietersadmgaetinaaisamrauofumunn.dcItftihwoines

kdnatoawbtyheasGe tawuosssiatantfisutniccst,iotnhehnavwinegcaancreenptrrees(ewnhtitchhedceopnednidtisononalxd)isgtirviebnutbioynfko(fxt;hwe

target ) and

3

a global variance parameter determined by the residual error. The use of a least squares approach does not assume or require the distribution of the target data to be Gaussian, but neither can it distinguish between a Gaussian distribution and any other distribution having the same (conditional) mean and (global) variance.

Conversely, if we assume that the conditional distribution of the target data is indeed Gaussian, then we can obtain the least-squares formalism using maximum likelihood, as follows. We assume that the target data is governed by the following conditional probability density function

p(tk

j x) =

(2

1 )1=2

exp

(
?

Fk(x)
2

?
2

tk]2 )

(10)

witnohdeberpeeeandgeiesnntaelryaglldoifbsutanrlicbtvuiaotrneiadon.fcHexe.praeTrFahmke(excto)enrid,s iatthnioednmawlehdaeenrneosfitthtyehoeofuttathrpgeuetctovvmaarrpiialaebbtlleeesttaakrrgaeenttdrveieascttteaodkreainss
then given by

p(t j x) = Yc p(tk k=1

j x) =

(2

1 )c=2

c

exp

(
?

1 2

2

Xc k=1

)
Fk(x) ? tk]2

(11)

dTofofe,hrtmtgeehiruvemfenksGid(neuxaersu;,lwyssasii)ninca.gconeFmgeaekpensnldseoeu-rtwfmaeoltrepdowdrtegaisfoecrundrnoi)pcf.ntteFiiWuooknnr(eaxFol)tfkn,h(teetxhtorw)eegfeioodstrrahkuetesnarmkogwnoeoeidntwrehelnrata,Fhptakieano(rxdnvta)iipclsuubrtoeylhacoreealfsybsptahpa(sewroicawiumtqenhruekifatnunrnloittzwhciethendyofipwfrcuaaeenmrasfcoeemtreiwokeotnthrteakoerl
parametrized function since they can e ciently represent non-linear multivariate functions with in principle no serious limitations on the functional forms which they can approximate (Hornik et al., 1989).

tTrhaienvinaglueexsafmorpltehsefpxaqr;atmqge.teTrshiws ciannfbke(xa;cwhi)evmedusbtybemdaxetimerimziinngedthferolmiketlhiheoodnittehastetthoef
model gave rise to the particular set of data points. If we assume that the training data are drawn independently from the distribution given by (11), then the likelihood of the data set is given by the product of the likelihoods for each of the data points

L = Yn p(tq; xq) = Yn p(tq j xq)p(xq)

(12)

q=1

q=1

where we have used (1). The determine appropriate values

fqourawntitbyy

LmiasxaimfiuznacttioionnooffLt.heInpaprraamcteitceer,siwt i,sacnodnvweeniceannt

instead to minimize the negative logarithm of L, which is usually called an error function

E = ? ln L

(13)

Minimizing E is equivalent to function. Using (11), (12) and the form

m(1a3x)i,mainzdinmg LodseilnlicnegtFhek(nxe)gbaytivfek(lxog; war)it,hwme

is a can

monotonic write E in

E = nc ln

+

nc 2

ln(2

)

+

1 2

2

Xn q=1

Xc k=1

fk(xq;

w)

?

tqk]2

+

Xn q=1

ln

p(xq)

(14)

4

Note that only the third term in (14) depends on the parameters w and so their values

can be determined by minimizing only this term. omitted since it has no e ect on the minimization

In addition, with respect

ttohew.facTthoirs

1= 2 gives

can rise

be to

the standard sum-of-squares error function commonly used in neural network training

ES

=

1 2

Xn q=1

Xc k=1

fk(xq;

w)

?

tqk]2

(15)

owbfehEetrhSee.thcInaespgeerfeno-efrraacalt,omfrkuo(lfxti1;-=lwa2y)hewrasipllberebceeenparterntooannin-nleiedntewfaoorrrkcfuonfnocvrteinioninesntoacfnetcwhe.ehepTnahcruaomsm, epttuheterisnmgwidn,eiamrsivizwaatotiuvioledns
of ES represents a problem in non-linear optimization, for which there exists a range of standard techniques (Press et al., 1992; Bishop, 1995).

Having found values for the parameters w , the optimum value for can then by found
by minimization of E in (14) with respect to . This minimization is easily performed analytically with the explicit, and intuitive, result

2

=

1 nc

Xn Xc q=1 k=1

fk(xq; w

)

?

tqk]2

(16)

which says that the optimum value of 2 is given by the residual value of the sum-ofsquares error function at its minimum, as shown earlier.

dsFciohsrtercmilbaeusstwiiohncearotefiboytnartpqkgreo=tbvleakmllusfeostrhiesantthairengnpeutgtivvapelnautebtseyranrexgqebneerloanllgyincghotsoencltaosshaCvl.e

a 1-of-N coding The probability

p(tk j x) = Xc (tk ? kl)P (Cl j x)

(17)

l=1

twhheenregiPve(sCl

j

x)

is

the

probability that x belongs to class fk(x; w ) = P(Ck j x)

Cl.

Substituting

(17)

into (6) (18)

and so the network outputs represent the Bayesian posterior probabilities of membership of the corresponding classes. In this sense the network outputs can be regarded as optimal since the posterior probabilities allow minimum risk classi cations to be made, once the appropriate loss matrix has been speci ed. For instance, if the goal is to minimize the number of misclassi cations, corresponding to loss matrix given by the unit matrix, then each new input should be assigned to the class having the largest posterior probability.

It should be noted that the cross-entropy error function also leads to network outputs which approximate the conditional average of the target data. For a nite data set, the cross-entropy error function can be written as

E

=

?

Xn q=1

Xc k=1

ftqk

ln

fk(xq

;

w)

+

(1

?

tqk

)

ln(1

?

fk(xq;

w))g

(19)

In the in nite-data limit this can be written as

E = ? Xc Z Z ftk ln fk(x; w) + (1 ? tk) ln(1 ? fk(x; w))g p(t; x) dt dx (20) k=1

5

By functional di erentiation as before, and making use of (1) and (7), we obtain

fk(x; w ) = htk j xi

(21)

so that the network outputs again represent the conditional averages of the target data.

cmMoeonasdntit,cigooinnvveeednnotbniyotnfhakel(xainp;pwpulitc)av, tewicohtnoicsrh.oWfapnepeuhroraxavliemnseaettewensotrthkhaseto,cnofolnyrdcmitlaiaosknseialucasaevtieoorfnatgphereoopbfrlteehmdeisct,taitrohgniestfroderpattrhaee-,
sents the optimal solution. However, for problems involving the prediction of continuous variables, the conditional average represents only a very limited statistic. For many applications, there is considerable bene t in obtaining a much more complete description of the probability distribution of the target data. We therefore introduce the Mixture Density Network which can in principle represent arbitrary conditional distributions, in the same way that a conventional neural network can represent arbitrary non-linear functions.

3 Mixture Density Networks

As we have already seen, the conventional least-squares technique can be derived from maximum likelihood on the assumption of Gaussian distributed data. This motivates the idea of replacing the Gaussian distribution in (11) with a mixture model (McLachlan and Basford, 1988), which has the exibility to model completely general distribution functions. The probability density of the target data is then represented as a linear combination of kernel functions in the form

p(t j x) = X m i(x) i(t j x)

(22)

i=1

wmttahihir(exegtiremnjetgximvx)ceionrcieegtspotcrrcheotiseeeehnnnattcusvi,mteihnnabegntesdcrboaeocnreafednncittogiabmoekennepearnorelangdtteaoeenrnddtbsseeifdtriyonfuamostnfhctptethhrieiomeonitristaxhprtogcurfoeorbtmeth.avpebeToiclinhtintoeepinreuptsttao(frvofcaoremtcnhttedheotieretmirioxstihnx. tekudiTe(rrxheon).eneNlaf.xruoe)Vnteaocctrfatiilhootlhenuadetss

choices for the kernel functions are possible. For the purposes of this paper, however, we

shall restrict attention to kernel functions which are Gaussian of the form

i(t j x) =

(2

1 )c=2

i(x)c

exp

(
?

kt

?
2

i(xi()x2)k2

)

(23)

(w2h3e)rewethheavveectaosrsumi(exd)trheaptretsheentcsomthpeocneennttrse

of of

the the

ith kernel, with components output vector are statistically

ik. In inde-

pvinaegrnidafuenlncltecwoviit(ahxrii)an.necTaehcmhisacatosrmsicupemos npfoetnriotenaocfchatnhGebadeuissrsteiralianbxuektdeiorinnn,elaa, nasdttrtcahaigenhebtxfeoprdewneasscrerdiobfweaadymbbyoyraeinccotormmodmpuloecnx-

formalism. In principle, however, such a complication is not necessary, since a Gaussian

mixture model, with kernels given by (23), can approximate any given density function to

arbitrary accuracy, provided the mixing coe cients and the Gaussian parameters (means

and variances) are correctly chosen (McLachlan and Basford, 1988). Thus, the represen-

tation given by (22) and (23) is completely general. In particular, it does not assume

6

that the components of t are statistically independent, in contrast to the single-Gaussian
representation in (11).

Feramalnolomidrndeagettnlhelayeirnnsgvgioaavtfrrehbiatneihntmrvecaaeurmlsuysiiexncitogo(uxfntr)dxhe,i,ettmitooohounebdtaepemllu,gditxeensntnaueosmrrfiteaeayllmyc(fucootonndhnvceettleiinmn(o2tunii2oox)punin(saptg)lronfjcvueoxinued)cre.tasicloWiannenesegtteowsnnfooewxrrika(.lxtwaTf)ok,hhreitmicshhtaehilsteimasakmveceaahsrfniioexosrvueamsdsiop(bixdatys)--

input. The combined structure of a feed-forward network and a mixture model we refer

to as a Mixture Density Network (MDN), and its basic structure is indicated in Figure 2.

By choosing a mixture model with a su cient number of kernel functions, and a neural

network with a su cient number as desired any conditional density

opf(thijdxd)e.n

units,

the

MDN

can

approximate

as

closely

conditional probability
density

p(t|x)

mixture model

parameter vector

z

neural network

input vector

x

Figure 2: The Mixture Density Network consists of a feed-forward neural network whose outputs determine the parameters in a mixture density model. The mixture model then represents the conditional probability density function of the target variables, conditioned on the input vector to the neural network.

The neural network element of the MDN can be any standard feed-forward structure with

universal approximation capabilities. In this paper we consider a standard multi-layer

perceptron, with a single hidden layer of sigmoidal units and an output layer of linear

units, and of network

we shall outputs

iussgeivzejntobyde(cno+te2)the

output variables. Note that the m, as compared with the usual c

total number outputs for a

7

network used in the conventional manner.

It is important to note that the mixing coe cients i(x) must satisfy the constraint

Xm i(x) = 1

(24)

i=1

This is achieved by choosing i(x) to be related to the networks outputs by a `softmax'
function (Bridle, 1990; Jacobs et al., 1991)

i = PMje=x1pe(xzpi()zj )

(25)

awlhizearteiozni

represent the corresponding network outputs. of the usual logistic sigmoid, and ensures that

(0; 1) and sum to unity, as required for probabilities.

This can be regarded as a generthe quantities i lie in the range

The variances i represent scale parameters and so it is convenient to represent them in terms of the exponentials of the corresponding network outputs

i = exp(zi )

(26)

which, in a Bayesian framework, would correspond to the choice of an un-informative

Bayesian prior, assuming the corresponding network outputs distributions (Jacobs et al., 1991; Nowlan and Hinton, 1992).

Tzhi isharedpurensiefonrtmatipornoablasboihliatys

the additional bene t of avoiding pathological con gurations in which one or more of the

variances goes to zero. The centers un-informative prior suggests that

thiesreepbreesreenptrleosceanttieodn

parameters, and the notion of an directly by the network outputs

ik = zik

(27)

As before, we can construct a likelihood function using (12), and then de ne an error function by taking the negative logarithm of the likelihood, as in (13), to give the error function for the Mixture Density Network in the form

E = XEq

(28)

q

where the contribution to the error from pattern q is given by

Eq = ? ln (Xm

i(xq)

)
i(tq j xq)

(29)

i=1

twhiethpari(atmjexte)rgsiovfenthbeym(2ix3t)u. rWe me hodavele,

dropped the and hence is

tienrdmepPenqdpe(nxtq)ofasthite

is independent of network weights.

Note that (29) is formally equivalent to the error function used in the `competing lo-

cal experts' model of Jacobs et al. (Jacobs et al., 1991). The interpretation presented

here, however, is quite di erent. Instead of seeking to impose soft competition between a

number of competing simpler network modules, the aim is to model the complete condi-

tional probability density of the output variables. From this density function, any desired

statistic involving the output variables can in principle be computed.

In order to minimize the error function, we need to calculate the derivatives of the error E with respect to the weights in the neural network. These can be evaluated by using

8

the standard `back-propagation' procedure, provided we obtain suitable expressions for the derivatives of the error with respect to the activations of the output units of the wpnnaehutditcreahtrhlncen,aedwntewerboicevraaknbt.aicvScoekinsn-spcoierdfoetEphraetbgheayertrsedoudermrtfimhuvarniotnciutgvigeoohsnvte(kqhr2e8=a)lnl@iespEtawcqtoo=tmr@ekrzpnkotsosf.oeTdr nhaodefpatdahresteruiicmvduaeltroaiifvrveatpestarivtmktqeessar,ncwotniqtaehasfnor`deerrstrephoaereccshnt' to the network weights. This is discussed further in Section 4. Standard optimization algorithms, such as conjugate gradients or quasi-Newton methods, can then be used to nd a minimum of E. Alternatively, if an optimization algorithm such as stochastic gradient descent is to be used, the weight updates can be applied after presentation of each pattern separately.

We have already remarked that the i can be regarded as conditional density functions, pwriothbapbriliiotrieps,rowbhaibchiliwtieesobtia. inItusiisngcoBnavyeensietnhteotroemintroduce the corresponding posterior

i(x; t) = Pmj=1i

i j

j

(30)

as this leads to some simpli cation of the subsequent analysis. Note that the posterior

probabilities sum to unity:

Xm i = 1

(31)

i=1

Consider rst the derivatives with respect to those network outputs which correspond to the mixing coe cients i. Using (29) and (30) we obtain

@Eq @i

=

?

i i

(32)

We now note that, as a result of the softmax activation function (25), the value of i depends on all of the network outputs which contribute to the priors, and so we have

@i @zk

=

ik

i?

i

k

(33)

From the chain rule we have

@Eq @zk

=

X
i

@Eq @i

@i @zk

(34)

Combining (32), (33) and (34) we then obtain

@Eq @zk

=

k?

k

(35)

where we have used (31).

For the derivatives corresponding to the together with (23), to give

i parameters we make use of (29) and (30),

@Eq @i

=?

(kt ?

i

i3

ik2

?

c)
i

(36)

9

Using (26) we have

@i @zi

=

i

(37)

Combining these together we then obtain

@Eq @zi

=?

(kt ?

i

i2

ik2

?

)
c

(38)

Finally, since the parameters using (29) and (30), together

wiikthar(e23g)i,ven

directly

by

the

zik

network

outputs,

we

have,

@Eq @zik

=

((
i

ik ? tk)) i2

(39)

The derivatives of the error function with respect to the network outputs, given by (35), (38) and (39), can be used in standard optimization algorithms to nd a minimum of the error. For the results presented in this paper, the optimization of the network weights was performed using the BFGS quasi-Newton algorithm (Press et al., 1992).

In the previous section we considered the properties of the standard least-squares network

model in the limit of an in nite data set. We now perform the corresponding analysis for

the Mixture Density Network. Taking the in nite data set limit of (28) and (29), we can

write the error function in the form

E =?ZZ

ln (Xm

i(x)

)
i(t j x) p(x; t) dx dt

(40)

i=1

If we set the obtain, after

functional derivatives of E with some algebraic rearrangement,

rtehsepefocltlotwo iznig(xc)o,nzdiit(ixo)nasnwdhzici h(xa)rteoszaetriso

we ed

by the mixture model parameters at the minimum of the error function

i(x) = h i j xi

(41)

i(x)

=

h i t j xi h i j xi

(42)

i2(x)

=

h ik

i

(x)
hi

? tk2 j xi

j

x)i

(43)

Tw(4hh1ee)sreeshroeiwsuslttshahi(taxvt;hete)a,pvareniordyrswnahite(urxre)altahrienetgceoirvpnerdneittbaiotynioatnhl.eavFceoorrraregesaepscohanrvdeainldugee

onfedthbeyin(p7u)tavsecbteofrorxe.,
posterior probabilities,

averaged over the conditional density of the target data. Similarly, the centers (42)

are given by the conditional average of the target data, weighted by the corresponding

posterior probabilities. Finally, the variance parameters (43) are given by the conditional

average of the variance of the target data around the corresponding kernel centre, again

weighted by the posterior probability of that kernel.

Once an MDN has been trained it can predict the conditional density function of the target data for any given value of the input vector. This conditional density represents a

10

complete description of the generator of the data, so far as the problem of predicting the value of the target vector is concerned. From this density function we can calculate more speci c quantities which may be of interest in di erent applications. Here we discuss some of the possibilities.

One of the simplest statistics is the mean, corresponding to the conditional average of the target data, given by

ht j xi = X i(x) Z t i(t j x) dt

(44)

i

=

X
i

i(x)

i(x)

(45)

where we have used (23). This is equivalent to the function computed by a standard network trained by least-squares. Thus, MDNs contain the conventional least-squares result as a special case. We can likewise evaluate the variance of the density function about the conditional average, to give

s2(x) = Dkt ? ht j xik2 j xE

(46)

8

=

X
i

i(x)

>< >:

i(x)2 +

i(x) ? X j

j(x)

xj(

)

29>= >;

(47)

which is more general than the allowed to be a general function

coof rxre. sSpiomndilianrgrelseualstts-scqaunarbees

result since obtained for

this variance is other moments

of the conditional distribution.

For many problems we vector. The most likely by the maximum of the

mcvoainlgudheittfibooernatilnhtedeeroneusstittpeyudtpi(nvtejctnxod)r.i,nSfgoinrocnaeegtsihvpiesencdieinncpsviutatyluvfeuecnftcootrriotxnh,eiissorugetippvreuent-

sented by a mixture model, the location of its global maximum is a problem in non-linear

optimization. While standard techniques exist for solving such problems (Press et al.,

1992), these are iterative in nature and are therefore computationally costly. For applica-

tions where speed of processing for new data is important, we may need to nd a faster,

approximate, approach.

If we assume that the overlapping, then to a

component kernels of the density very good approximation the most

function are likely value

onfott

too will

strongly be given

by the centre of the highest component. From (22) and (23), we see that the component

with the largest central value is given by

(
miax

ii((xx))c )

(48)

and the corresponding centre approximation. Alternatively,

wei

represents the most may wish to consider

likely output vector, the total `probability

to a mass'

good asso-

ciated with each of the mixture components. This would be appropriate for applications

involving multi-valued mappings with a nite number of distinct branches, in which we are

interested in nding a representative vector corresponding to the most probable branch.

11

(This approach is also less susceptible to problems due to arti cially small values of i arising from regions of sparse data). An example of such a problem, involving the kine-
mmasaisxuttimcusrineogmf torhodebeocl otismanpromornmse,aniltsiszedadirs,ecRuwsesile(ldtsejipnxa)rtdahttee=dne1ax,nttdhsheeacmvtieoosnnt.epgrlSiogibnibacbleeleeoavbcerhralnaccpoh,miospf ogtnihveeennstoblouyftitohne,

miax f i(x)g

(49)

The required value of t is then given by the corresponding centre i.

A whole variety of other statistics can be computed from the conditional probability density, as appropriate to the particular application.

4 Software Implementation

The implementation of the MDN in software is very straightforward, and for large-scale problems will typically not lead to a signi cant computational overhead compared with the standard least-squares approach. Consider a multi-layer perceptron network trained by minimizing a sum-of-squares error function using a standard optimization procedure (such as gradient descent or quasi-Newton). The only modi cation to the software which is required arises from the modi ed de nition of the error function, with all other aspects
acpoosfarttartheee`srmpnimo,ondapdunlieldnemg'aewltnsahotraigctthheiotentvadrkeeecermtsiovaaaritnntiivqenetgaswnuodnrqkcwohofhaunticthghpeeudcet.arrnvIoenrrcegtwtoeurnitrehznrqartle(h,fsoewpreevcaactlauptneaorrotteihfgceauthrlndaeerttewhprearootretrkrerEroonurqtqfpfu)ounratcnstthdizoaqnat.
This is illustrated in Figure 3. The derivatives of the error function with respect to one of the weights w in the network is obtained by use of the chain rule

@Eq @w

=

X
i

@Eq @zi

@ @

zi w

=X
i

iq

@zi @w

(50)

where the quantities propagated through

thiqe=ne@tEwoq=rk@.zi

can For

be interpreted the particular

as `errors' which are to be case of the sum-of-squares

backerror

function we have

Eq

=

1 2

jzq

?

tqj2

(51)

q = zq ? tq

(52)

In order to modify the software to implement the MDN, (51) and (52) must be replaced by

the appropriate expressions. The error function for a particular pattern is given by (29),

while the elements of the vector are given by (35), (38) and (39). The implementation

of an MDN is particularly simple and natural in an object oriented language such as

C++, since the error module can be represented as a class, with methods to set the

mixture parameters for a given set of network outputs, and to return the error function

or its derivatives. The error the value of the conditional

function class can also probability density for

be provided given values

owfitxhamndetht,odors

to to

return return

12

zq

error

Eq

tq

function



q =

Eq
z

Figure 3: For the purposes of software implementation, an error function can be regarded as a module which takes a network output vector zq (for a particular pattern q) and a corresponding target vector tq and which can return the value of the error Eq for that pattern, as well as the derivatives of the error with respect to the network outputs, q = rzEq.

other statistics derived from the conditional probability density (such as the centre vector corresponding to the most probable kernel).

For applications involving large numbers of input variables, the computational requirements for the MDN need not be signi cantly greater than with a standard network trained using a sum-of-squares error function, since much of the computational cost lies in the forward and backward propagation of signals through the network itself. For networks with a large number of inputs (and hence a large number of weights in the rst layer) this will exceed the cost of evaluating the error function and its derivatives.

In any algorithm which uses gradient-based methods to perform error minimization, a

very powerful check on the software can be made by comparing the error derivatives

obtained from the analytic expressions with those calculated using nite di erences. Close

agreement between these two approaches demonstrates that a high proportion of the code

has been implemented correctly. Note that substantially improved accuracy is obtained if

symmetric central di erences are used, rather than simple nite di erences, since in the

latter case we have

E(w +

) ? E(w)

=

@E @w

+ O(

)

(53)

where is a small parameter, whereas central di erences give

E(w +

) ? E(w ?
2

)

=

@E @w

+ O(

2)

(54)

for which the correction terms are much smaller. Of course, for use in error minimization, the analytic expressions should be used in preference to the nite di erence formulae since, not only are they more accurate, but they are substantially more computationally e cient (Bishop, 1995).

5 A Simple Inverse Problem
Many potential applications of neural networks fall into the category of inverse problems. Examples include the control of industrial plant, analysis of spectral data, tomographic reconstruction, and robot kinematics. For such problems there exists a well de ned forward problem which is characterized by a functional (i.e. single-valued) mapping. Often
13

this corresponds to causality in a physical system. In the case of spectral reconstruction, for example, the forward problem corresponds to the prediction of the spectrum when the parameters (locations, widths and amplitudes) of the spectral lines are prescribed. For practical applications, however, we generally have to solve the corresponding inverse problem in which the roles of input and output variables are interchanged. In the case of spectral analysis, this corresponds to the determination of the spectral line parameters from an observed spectrum. For inverse problems, the mapping can be often be multivalued, with values of the input for which there are several valid values for the output. For example, there may be several choices for the spectral line parameters which give rise to the same observed spectrum (corresponding, for example, to the exchange of width parameters for two co-located lines). If a standard neural network is applied to such inverse problems, it will approximate the conditional average of the target data, and this will frequently lead to extremely poor performance. (The average of several solutions is not necessarily itself a solution). This problem can be overcome in a natural and e ective way by appropriate use of a Mixture Density Network instead.

In order to illustrate the application of the MDN, we begin by considering a simple example of an inverse problem involving a mapping between a single input variable and a single output variable. Consider the mapping from t (regarded here as an input variable) to x (regarded as an output variable) de ned by

x = t + 0:3 sin(2 t) +

(55)

where is a random variable with uniform distribution in the interval (?0:1; 0:1). The
mapping from t to x provides an example of a forward problem. In the absence of the noise term , this mapping is single-valued, so that each value of t gives rise to a unique value of x. Figure 4 shows a data set of 1,000 points generated by sampling (55) at equal intervals of t in the range (0:0; 1:0). Also shown is the mapping represented by a standard multi-layer perceptron after training using this data set. The network had 1 input, 5 hidden units with `tanh' activation functions, and 1 linear output unit, and was trained for 1,000 complete cycles of the BFGS quasi-Newton algorithm. It can be seen that the network, which is approximating the conditional average of the target data, gives an excellent representation of the underlying generator of the data. This result is insensitive to the choice of network structure, the initial values for the network weights, and the details of the training procedure.

We now consider the corresponding inverse problem in which we use exactly the same data set as before, but we try to nd a mapping from the x variable to the t variable. The result of training a neural network using least-squares is shown in Figure 5. Again the network tries to approximate the conditional average of the target data, but now this corresponds to a very poor representation of the process (55) which generated the data. The precise form of the neural network mapping is now more sensitive to network architecture, weight initialization, etc., than was the case for the forward problem. The mapping shown in Figure 5 is the best result obtained after some careful optimization (with the network often nding signi cantly poorer solutions). The network in this case had 20 hidden units and was trained for 1,000 cycles of the BFGS algorithm. It is clear that a conventional network, trained by minimizing a sum-of-squares error function, cannot give a good representation of the generator of this data.

We next apply an MDN to the same inverse problem, using the same data set as before.

14

1.0
t
0.5

0.0 0.0

0.5

x 1.0

Figure 4: A simple example of a forward problem, showing 1,000 data points (the circles) generated from the mapping x = t + 0:3 sin(2 t) + where is a random variable having a uniform distribution in the range (?0:1; 0:1). The solid curve shows the result of training a multi-layer perceptron network with 5 hidden units using a sum-of-squares error function. The network approximates the conditional average of the target data, which gives a good representation of the generator of the data.

1.0
t
0.5

0.0 0.0

0.5

x 1.0

Figure 5: This shows precisely the same data set as in Figure 4, but with the roles of input and target variables interchanged. The solid curve shows the result of training a standard neural network using a sum-of-squares error. This time the network gives a very poor t, as it tries to represent the conditional average of the target data.

15

For clarity we restrict attention to MDNs with 3 kernel functions, as this is the minimum

number needed to give good solutions for this problem (since the inverse mapping has 3

branches at intermediate values of x, as is clear from Figure 5). In practice, the appropri-

ate number of kernels will not be known in advance and must be addressed as part of the

model order selection problem. Experiments with 5 kernel functions on this same problem

give almost identical results to those obtained using 3 kernels. We shall discuss the prob-

lem of selecting the appropriate number of kernel functions in Section 7. The network

component of the MDN was a multi-layer perceptron with 1 input, 20 hidden units with

`tanh' activation functions, and 9 output units (corresponding to the 3 parameters for

each of the 3 Gaussian kernel functions). This network structure has not been optimized

to any degree since the main purpose of this exercise is to illustrate the operation of the

MDN. The MDN was trained with 1,000 cycles of the BFGS algorithm. Once trained,

the MDN predicts the conditional probability density of t for each value of x presented

to the input of the network. Figure 6 shows contours of p(t j x) as a function of t and x.

It is clear that the MDN has captured the underlying structure in the data set, despite

the multi-valued nature of the inverse problem. Notice that the contour values are much

higher in fact that

regions
p(t j x)

of x satis

wesheRrpe(tthjexd)datta=is1

single-valued at each value

in of

t. x,

This is a consequence and can be seen more

of the clearly

in Figure 7 which shows plots of p(t j x) versus t for 3 values of x. Note particularly that,

for x = 0:5, the MDN has correctly captured the tri-modal nature of the mapping.

1.0

t

0.5

0.0 0.0

0.5 x 1.0

Figure 6: Plot of the contours of the conditional probability density of the target data obtained from a Mixture Density Network trained using the same data as in Figure 5. The network has 3 Gaussian kernel functions, and 5 sigmoidal units in the hidden layer.

The outputs of the neural network part of the MDN, and hence the parameters in the mixture model, are necessarily continuous single-valued functions of the input variables. The MDN is able to produce a conditional density which is unimodal for some values of x and trimodal for other values, as in Figure 6, by modulating the amplitudes of the mixture components. This can be seen in Figure 8 which shows plots of the 3 priors i(x) as functions of x. It can be seen that for x = 0:2 and x = 0:8 only one of the 3 kernels has a signi cant prior probability. At x = 0:5, however, all 3 kernels have comparable priors.

16

20
p(t | x)
10

x = 0.2

x = 0.8

x = 0.5

0

0.0

0.5

t

1.0

Figure 7: Plot of the conditional probability densities of the target data, for various values of x, obtained by taking vertical slices through the contours in Figure 6, for x = 0:2, x = 0:5 and x = 0:8. It is clear that the Mixture Density Network is able to capture correctly the multi-modal nature of the target data density function at intermediate values of x.

1.0
i
0.5

0.0 0.0

0.5

x

1.0

Figure 8: Plot of the priors i(x) as a function of x for the 3 kernel functions from the same Mixture Density Network as was used to plot Figure 6. At both small and large values of x, where the conditional probability density of the target data is unimodal, only one of the kernels has a prior probability which di ers signi cantly from zero. At intermediate values of x, where the conditional density is tri-modal, the three kernels have comparable priors.

17

Having obtained a good representation for the conditional density of the target data, it is then in principle straightforward to calculate any desired statistic from that distribution.
We consider rst the evaluation of the conditional mean of the target data ht j xi, given
by (45), and the squared variance s2(x) of the target data around this mean, given by
(47). Figure 9 shows a plot of ht j xi against x for the MDN used to plot Figure 6, together with plots of ht j xi s(x). This representation corresponds to the assumption
of a single Gaussian distribution for the target data, but with a variance parameter which is a function of x. While this is more general that the standard least-squares approach (which assumes a constant variance) it still gives a poor representation of the data in the multi-valued region. Notice that, in the regions where the data is single valued, the MDN gives a much smoother and more accurate representation of the conditional average of the target data than was obtained from the standard least-squares neural network as shown in Figure 5. This can be attributed to the fact that the standard network is having to make a single global t to the whole data set, whereas the MDN uses di erent kernels for the di erent branches of the mapping.
1.0
t
0.5

0.0 0.0

0.5

x 1.0

Figure 9: This shows a plot of ht j xi against x (solid curve) calculated from the MDN used to plot Figure 6, together with corresponding plots of ht j xi s(x) (dashed curves). Notice that for small and large values of x, where the mapping is single-valued, the MDN actually gives a better representation of the conditional average than the standard leastsquares approach, as can be seen by comparison with Figure 5. This can be attributed to the fact that the standard network is having to make a single global t to the whole data set, whereas the MDN uses di erent kernels for the di erent branches of the mapping.

We can also consider the evaluation of the centre of the most probable kernel according to (49), which gives the result shown in Figure 10. This now represents a discontinuous functional mapping from x to t, such that, at each value of x, the MDN make a good prediction for the value of t, which lies well within one of the branches of the data. It can be seen that the discontinuities correspond to the crossing points in Figure 8 which separate the regions in which di erent priors have the largest value. Comparison with the corresponding mapping obtained with the standard neural network approach, given in Figure 5, shows that the MDN gives substantially improved predictions for the inverse

18

mapping.
1.0
t
0.5

0.00.0

0.5

x 1.0

Figure 10: Plot of the central value of the most probable kernel as a function of x from the Mixture Density Network used to plot Figure 6. This gives a (discontinuous) functional mapping from x to t which at every value of x gives an accurate representation of the data. The diagram should be compared with the corresponding result obtained from a conventional neural network, shown in Figure 5.

6 Robot Kinematics

As our second application of Mixture Density Networks, we consider the kinematics of a simple 2-link robot arm, as shown in Figure 11. For given values of the joint angles ( 1; 2), the end e ector is moved to a position given by the Cartesian coordinates

x1 = L1 cos( 1) ? L2 cos( 1 + 2)

(56)

x2 = L1 sin( 1) ? L2 sin( 1 + 2)

(57)

where L1 and L2 are the lengths of the two links of the robot arm. Here we consider

a particular restricted to ffroormpr(ac1t;ic2a)l

ttrchoooebn(xorat1gn;ucxgore2an)t(tii0roso:n3lk,;no1tohf:w2er)noeabnanosddttehf2oeerifcsotwrroewhrsiatmcrrhiductsLketi1dnbete=momat0hto:ie8vcesra,dannatdngoedLp(ir2se=ss=i2cn;rgi30bl:ee2=-dv2aa)lnlo.udcTeadthw.ieohHnmesroaewapnepvd1ieniirgts,

is therefore necessary to nd corresponding values for the joint angles. This is called the

inverse kinematics and the inverse kinematics

icsonrroetspaosnidnsglteo-vtahlueemd amppapinpginfgro. mTh(xis1;ixs2i)llutost(ra1t;ed2)i.n

In general, Figure 12,

where we see that there are two con gurations of the joint angles, known as `elbow up'

and `elbow down', which both give rise to the same end e ector position. The extent

of the elbow up and elbow-down regions, for the particular con guration of robot arm

considered here, is shown in Figure 13. We see that there are regions (A and C) which

are accessible using only one of the two con gurations, and for end e ector positions in

19

either of these regions, the inverse kinematics will be single valued. There is also a region (B) in which end e ector positions can be accessed by both elbow-up and elbow-down con gurations, and in this region the inverse kinematics is double-valued.
( x1 , x2 ) L2

2

L1

1

Figure 11: Schematic illustration of a two-link robot arm in two dimensions. For given values

of the joint coordinates

angles (x1; x2),

1isaunndiqu2,eltyhdeepteorsmitiionnedo.f

the end e In practice,

ector, described by the Cartesian control of the robot arm requires

the solution of the inverse kinematics problem in which the end e ector position is speci ed

and the joint angles 1 and 2 must be found.

We rst consider the use of a standard neural network, trained by minimizing a sum-ofsquares error function, to learn the inverse kinematics mapping. A training set of 1,000 points was generated by selecting pairs of joint angles at random with uniform distribution within the allowed limits, and computing the corresponding end e ector coordinates using (56) and (57). A test set, also of 1,000 points, was generated in a similar way, but with a di erent random selection of joint angles. A standard multi-layer perceptron network having 2 inputs, N hidden units with `tanh' activation functions, and 2 linear output units was used. Here N was set to 5, 10, 15, 20, 25 and 30, and in each case the network was trained for 3,000 complete cycles of the BFGS algorithm. The performance of the network was assessed by presenting the test set end e ector coordinates as inputs and using the corresponding values of joint angles predicted by the network to calculate the achieved end e ector position using (56) and (57). The RMS Euclidean distance between the desired and achieved end e ector positions is used as a measure of performance. This measure is evaluated using the test set, after every 10 cycles of training using the training set, and the network having the smallest RMS error is saved. All the networks gave very similar performance. Figure 14 shows the positioning errors achieved by the best network (20 hidden units) for all of the points in the test set. Comparison with Figure 13 shows that the positioning errors are largest in region B where the inverse kinematics mapping is double valued. In this region the end e ector positions achieved by the robot lie at the outer boundary of the accessible region, corresponding to a value of 2 = .

20

elbow up

( x1 , x2 )
elbow down

Figure 12: This diagram shows why the inverse kinematics mapping for the robot arm is multi-valued. For the given position (x1; x2) of the end e ector, there are two solutions for the joint angles, corresponding to `elbow up' and `elbow down'.

1.0

A

x2

elbow

down

B

0.5

elbow

C

up

0.0 0.0

0.5

x1

1.0

Figure 13: For the particular geometry of robot arm considered, the end e ector is able to reach all points in regions A and B in the `elbow down' con guration, and all points in regions B and C in the `elbow up' con guration. Thus, the inverse kinematics will correspond to a single-valued mapping for positions in regions A and C, and to a double-valued mapping for positions in region B. The base of the robot arm is at (0:0; 0:0).

21

Examination of Figure 12 shows that this is indeed the result we would expect, since the average of the joint angles for an elbow-up con guration and the corresponding elbowdown con guration always gives this value for 2.
1.0
x2
0.5

0.0 0.0

0.5

x1

1.0

Figure 14: This shows the result of training a conventional neural network, using a sum-ofsquares error function, on the inverse kinematics problem for the robot arm corresponding to Figure 13. For each of the 1,000 points in the test set, the positioning error of the end e ector is shown as a straight line connecting the desired position (which forms the input to the network) to the actual position achieved by the robot when the joint angles are set to the values predicted by the outputs of the network. Note that the errors are everywhere large, but are smaller for positions corresponding to regions A and C in Figure 13 where the inverse kinematics is single valued, and larger for positions corresponding to the double valued region B.

The same datasets were also used to train an MDN having two kernel functions in the mixture model. The network component of the MDN was a standard multi-layer perceptron having two inputs, N hidden units and 8 output units, and the same optimization procedure was used as for the previous network trained by least squares. In this case the best network had 10 hidden units, and the corresponding position errors are plotted in Figure 15. It is clear that the positioning errors are reduced dramatically compared to the least-squares results shown in Figure 14. A comparison of the RMS positioning errors for the two approaches is given in Table 1, which shows that the MDN gives an order of magnitude reduction in RMS error compared to the least-squares approach.

Model RMS positioning error

Least squares

0.0578

MDN

0.0053

Table 1: Comparison of RMS positioning errors for the robot end e ector, measured using the test set, for a standard neural network trained by least-squares, and for a Mixture Density Network.

22

1.0
x2
0.5

0.0 0.0

0.5

x1

1.0

Figure 15: As in Figure 14, but showing the corresponding results obtained using a Mixture Density Network. The RMS error in positioning the end e ector is reduced by an order of magnitude compared with the conventional network, and the errors remain small even in the region where the inverse kinematics is double-valued.

7 Discussion

In this paper we have introduced a new class of networks, called Mixture Density Networks, which can model general conditional probability densities. By contrast, the conventional network approach, involving the minimization of a sum-of-squares error function, only permits the determination of the conditional average of the target data, together with a single global variance parameter. We have illustrated the use of Mixture Density Networks for a simple 1-input 1-output mapping, and for a robot inverse kinematics problem. In both of these examples the required mapping is multi-valued and so is poorly represented by the conditional average. There are many other approaches to dealing with the problem of learning multi-valued mappings from a set of example data. In general, however, these are concerned with generating one speci c solution (i.e. one branch of the multi-valued mapping). The Mixture Density Network, by contrast, is concerned with modelling the complete conditional density function of the output variables, and so gives a completely general description of the required mapping. From the conditional density, more speci c information can be extracted. In particular, we have discussed methods for evaluating moments of the conditional density (such as the mean and variance), as well as for selecting a particular branch of a multi-valued mapping. Implementation of Mixture Density Networks is straightforward, and corresponds to a modi cation of the error function, together with a di erent interpretation for the network outputs. One aspect of the MDN which is more complex than with standard models is the problem of model order selection. In applying neural networks to nite data sets,

23

the degree of complexity of the model must be optimized to give the best generalization

performance. This might be done by varying the number of hidden units (and hence

the number of adaptive parameters) as was done for the simulations in this paper. It

could also be done through the use of regularization terms added to the error function, or

through the use of `early stopping' during training to limit the e ective number of degrees

of freedom in the network. The same problem of model complexity must also be addressed

for MDNs. However, there is in addition the problem of selecting the appropriate number

of kernel functions. Changes to the number of kernels leads to changes in the number

of adaptive parameters in the network through changes to the number of output units

(for a given number of hidden units), and so the two problems are somewhat interrelated.

For problems involving discrete multi-valued mappings it is important that the number

of kernel functions is at least equal to the maximum number of branches of the mapping.

However, it is likely that the use of a greater number of kernel functions than this will

have little ill e ect, since the network always has the option either of `switching o '

redundant kernels by setting the corresponding priors to small values, or of `combining'

kernels by giving them similar problems discussed in this paper

iinvanoldvinig

parameters. a surplus of

Preliminary experiments on the kernels indicates that there is no

signi cant reduction in network performance. Future research will be concerned with the

automation of model order selection for Mixture Density Networks, as well as with the

performance of these networks in a range of large-scale applications.

Acknowledgements
I would like to thank Pierre Baldi, David Lowe, Richard Rohwer and Andreas Weigend for providing helpful comments on an earlier draft of this report.

24

References

Bishop, ings:

C. M. (1994). Vision, Image

Novelty detection and and Signal Processing

1n4eu1ra(4l )n,e2t1w7o{r2k22v.alSidpaetciioanl .isIsEuEe

Proceedon appli-

cations of neural networks.

Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford University Press.

Bridle, J. S. (1990). Probabilistic interpretation of feedforward classi cation network outputs, with relationships to statistical pattern recognition. In F. Fogelman Soulie and J. Herault (Eds.), Neurocomputing: Algorithms, Architectures and Applications, pp. 227{236. New York: Springer-Verlag.

Geman, S., E. bias/variance

Bienenstock, dilema. Neural

aCnodmpRu.taDtioonurs4at(1)(,119{9528)..

Neural

networks

and

the

Hornik, K., M. are universal

Satpipnrcohxciommabtoe,rsa.nNdeuHr.alWNheittweo(r1k9s829)(.5M), u3l5t9il{a3y6e6r.

feedforward

networks

Jacobs, R. of local

eAx.p, eMrt.sI..NJeourdraalnC, So.mJp.uNtaotwiolann3,

and (1),

G. E. Hinton 79{87.

(1991).

Adaptive

mixtures

McLachlan, G. J. and K. E. Basford (1988). Mixture Models: Inference and Applications to Clustering. New York: Marcel Dekker.

Nowlan, S. sharing.

J. and Neural

CGo.mEp.utHatiinotnon4 ((149)9,24)7. 3S{i4m93p.lifying

neural

networks

by

soft

weight

Press, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery (1992). Numerical Recipes in C: The Art of Scienti c Computing (Second ed.). Cambridge University Press.

WhCitoem, Hpu.t(a1t9io89n).1L(e4a)r,n4in25g{i4n6a4r.ti cial neural networks: a statistical perspective. Neural

25

